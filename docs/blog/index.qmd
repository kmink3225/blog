---
title: Blog
subtitle: Engineering, Data QC, Data Processing, Modeling, and Visualization
listing:
  sort: "date desc" # 날짜 내림차순
  contents: "posts" # 포스트 폴더
  type: default # default, table, grid
  sort-ui: true # 정렬 UI
  filter-ui: true # 필터 UI 
  categories: true # 카테고리 표시
  feed: true  #RSS 피드
margin-header: signup.html
title-block-banner: true
title-block-banner-color: body
search: true
---

## The Evolution from Statistics to AI

데이터 분석의 여정은 통계학에서 시작하여 인공지능까지 자연스럽게 발전해왔다. 각 단계는 이전 단계의 한계를 극복하면서 더 복잡하고 현실적인 문제를 해결할 수 있는 방법론으로 진화했다.

**통계학(Statistics)**은 수학적으로 엄격한 기반 위에서 데이터를 분석한다. 확률 이론과 추론 통계를 통해 데이터에서 의미 있는 패턴을 발견하고, 가설을 검증하며, 불확실성을 정량화한다. 잘 설계된 실험과 데이터에 대해서는 수학적으로 매우 높은 설명력을 제공한다. 그러나 정규성(normality), 독립성(independence), 등분산성(homoscedasticity) 등 많은 가정을 필요로 하며, 이러한 가정이 충족되지 않는 현실 데이터에서는 한계를 보인다.

**데이터 과학(Data Science)**은 통계학의 수학적 엄격함을 유지하면서도, 컴퓨터 과학의 계산 능력과 도메인 지식을 결합한다. 통계적 가정을 완전히 만족하지 않는 대규모 데이터도 처리할 수 있고, 데이터 전처리, 탐색, 시각화, 의사소통까지 포괄하는 전체 파이프라인을 다룬다. 실제 비즈니스 문제 해결에 초점을 맞춘다.

**머신러닝(Machine Learning)**은 명시적인 통계적 가정 없이도 데이터에서 패턴을 자동으로 학습한다. 전통적 통계 모델이 요구하는 복잡한 전제 조건들을 완화하고, 비선형적이고 고차원적인 관계도 포착할 수 있다. 모델이 데이터로부터 스스로 학습하므로, 사람이 일일이 특성을 설계하지 않아도 된다.

**딥러닝(Deep Learning)**은 인간의 뇌 구조에서 영감을 받은 다층 신경망을 통해 더욱 복잡한 패턴과 추상적인 표현을 학습한다. 이미지, 음성, 텍스트와 같은 비정형 데이터에서 뛰어난 성능을 보이며, 전통적인 특성 공학(feature engineering)의 필요성을 크게 줄였다.

**인공지능(Artificial Intelligence)**은 이 모든 기술을 통합하여, 인간의 인지 능력을 모방하고 자율적으로 의사결정을 내릴 수 있는 시스템을 구축한다. 단순한 예측을 넘어 추론, 계획, 자연어 이해 등 복합적인 지능적 행동을 수행한다.

## Statistics: Mathematical Foundation with Strict Assumptions

통계학은 데이터 분석의 수학적 기초를 제공하며, 엄격한 이론적 프레임워크를 통해 신뢰할 수 있는 추론을 가능하게 한다.

### 핵심 강점
- **수학적 엄밀성**: 확률 이론에 기반한 정확한 추론
- **불확실성 정량화**: p-value, 신뢰구간을 통한 통계적 유의성 측정
- **해석 가능성**: 명확한 인과관계와 변수 간 관계 설명
- **작은 샘플에서도 효과적**: 적절한 실험 설계로 적은 데이터로도 신뢰할 수 있는 결론 도출

### 주요 한계
- **많은 가정 요구**: 정규성, 독립성, 등분산성, 선형성 등
- **현실 데이터와의 괴리**: 실제 데이터는 가정을 위반하는 경우가 많음
- **고차원 데이터 처리 어려움**: 변수가 많아지면 다중공선성, 차원의 저주 문제
- **비선형 관계 모델링 한계**: 복잡한 비선형 패턴을 포착하기 어려움
- **수동 특성 선택**: 분석가가 어떤 변수를 사용할지 미리 결정해야 함

> 이러한 한계로 인해 더 유연하고 자동화된 방법론이 필요하게 되었다.

## Statistics: Key Concepts

### 중심극한정리 (Central Limit Theorem)

중심극한정리는 확률론과 통계학의 핵심 개념이다. 모집단에서 충분히 큰 표본을 추출하면, 원래 모집단의 분포 형태와 무관하게 표본 평균들의 분포는 근사적으로 정규분포를 따른다는 정리다.

더 쉽게 말하면, 표본 크기가 증가할수록 표본 평균은 모집단 평균을 더 정확하게 대표하게 되며, 표본 평균들의 분포는 종 모양의 대칭적인 정규분포에 가까워진다는 것이다.

중심극한정리가 중요한 이유는 모집단 분포의 정확한 형태를 모르더라도 표본을 기반으로 모집단에 대한 통계적 추론을 할 수 있게 해주기 때문이다. 또한 많은 통계 검정과 모델에서 사용되는 핵심 모수인 표본 평균과 표준편차의 행동을 이해하는 데 도움을 준다.

#### 예시

**예시 1: 모집단의 평균 체중**

한 국가의 모든 사람의 평균 체중을 알고 싶다고 가정하자. 모든 사람의 체중을 측정하는 것은 비현실적이므로, 1000명의 표본을 추출한다. 중심극한정리에 따르면 크기 1000인 표본을 충분히 많이(30개 이상) 추출하면, 이 표본들의 평균은 정규분포를 따른다. 이를 통해 우리의 표본 평균을 기반으로 모집단 평균에 대한 추론을 할 수 있다.

**예시 2: 품질 관리**

공장에서 대량의 동일한 제품을 생산하며, 각 제품의 무게는 평균 500그램, 표준편차 10그램의 정규분포를 따른다고 가정하자. 공장은 100개 제품 배치의 평균 무게가 498그램에서 502그램 사이에 있는지를 95% 신뢰수준으로 확인하고자 한다.

중심극한정리를 사용하여 모집단에서 100개 제품의 무작위 표본을 추출하고, 각 표본의 평균 무게를 계산하여 평균들의 표본분포를 만들 수 있다. 표본 크기가 충분히 크므로(n > 30), 중심극한정리에 따르면 평균들의 표본분포는 평균 500그램, 표준편차 1그램(모집단 표준편차를 표본 크기의 제곱근으로 나눈 값)의 근사적 정규분포를 따른다.

이 정보를 사용하여 100개 제품 배치의 평균 무게가 원하는 범위 내에 있을 확률을 계산할 수 있다. 표준정규분포표나 소프트웨어를 사용하여 범위의 각 끝점(498과 502)에 대한 z-점수를 찾고, 무작위로 선택된 100개 제품 표본의 평균 무게가 이 범위 내에 있을 확률을 계산한다. 확률이 최소 95%라면, 공장의 품질 관리 기준이 충족된다고 결론지을 수 있다.

### 표본추출 (Sampling)

표본추출의 목적은 더 큰 모집단에 대한 추론이나 결론을 도출하는 데 사용할 수 있는 대표적인 부분집합을 얻는 것이다.

#### 확률적 표본추출 (Probabilistic Sampling)

* **단순 무작위 추출 (Simple Random Sampling)**: 모집단의 각 개인이나 단위가 표본으로 선택될 동등한 기회를 갖는다. 보통 난수 생성기나 난수표를 사용한다.
* **층화 추출 (Stratified Sampling)**: 모집단을 특성(연령, 성별, 지리적 위치 등)에 따라 하위 그룹(층)으로 나누고, 각 층에서 무작위 표본을 선택한다. 이는 관심 특성과 관련하여 표본이 모집단을 대표하도록 보장한다.
* **집락 추출 (Cluster Sampling)**: 모집단을 가구나 학교 같은 집락으로 나누고, 조사할 집락의 무작위 표본을 선택한다. 선택된 각 집락 내의 모든 개인이나 단위를 조사한다.
* **계통 추출 (Systematic Sampling)**: 모집단에서 무작위 시작점을 선택하고, n번째마다 개인이나 단위를 선택한다. 여기서 n은 미리 정해진 간격이다.

#### 비확률적 표본추출 (Non-Probabilistic Sampling)

* **편의 추출 (Convenience Sampling)**: 공식적인 표본추출 기법보다는 편의성이나 이용 가능성에 기반하여 개인이나 단위를 선택한다. 편리하지만 편향된 결과를 초래할 수 있으며 연구 목적으로는 일반적으로 권장되지 않는다.
* **눈덩이 추출 (Snowball Sampling)**: 접근하기 어렵거나 제한적인 모집단을 연구할 때 주로 사용된다. 특정 기준을 충족하는 작은 그룹으로 시작하여, 그들이 동일한 기준을 충족하는 다른 개인을 추천하게 한다. 원하는 표본 크기에 도달할 때까지 이 과정을 계속한다.

### 제1종 오류 vs 제2종 오류 (Type I vs Type II Error)

| 진실 | $H_{null}$ 기각 | $H_{null}$ 기각 실패 |
|:-----------------:|:---------------------:|:------------------------------:|
| $H_{null}$ = 참 | 제1종 오류, $\alpha$, 거짓 양성| 올바른 결정 (1 - $\alpha$), 참 음성|
| $H_{null}$ = 거짓| 올바른 결정 (검정력, 1-$\beta$), 참 양성 | 제2종 오류, $\beta$, 거짓 음성|

가설 검정에서 제1종 오류는 실제로 참인 귀무가설을 기각할 때 발생한다. 즉, 실제로는 유의한 효과나 차이가 없는데 있다고 결론 내리는 것이다. 이 오류는 거짓 양성(false positive)이라고도 한다.

반면, 제2종 오류는 실제로 거짓인 귀무가설을 기각하지 못할 때 발생한다. 즉, 실제로는 유의한 효과나 차이가 있는데 없다고 결론 내리는 것이다. 이 오류는 거짓 음성(false negative)이라고도 한다.

### 선형회귀 (Linear Regression)

선형회귀는 종속변수(반응변수)와 하나 이상의 독립변수(예측변수 또는 설명변수) 간의 관계를 모델링하는 데 사용되는 통계 기법이다. 선형회귀의 목표는 종속변수의 값을 예측하는 데 사용할 수 있는 종속변수와 독립변수 간의 최적의 선형 관계를 찾는 것이다.

선형회귀에서 p-value, 계수, R-squared 값은 모델의 통계적 유의성과 예측력을 평가하는 데 도움이 되는 중요한 구성요소다:

**P-value**: p-value는 귀무가설이 참이라고 가정할 때, 관찰된 것만큼 극단적인 결과를 얻을 확률을 측정한다. 선형회귀의 맥락에서 귀무가설은 독립변수의 계수가 0이라는 것, 즉 독립변수와 종속변수 간에 선형 관계가 없다는 것이다. 낮은 p-value(일반적으로 0.05 미만)는 독립변수와 종속변수 간의 관계가 통계적으로 유의하며, 귀무가설을 기각할 수 있음을 나타낸다.

**계수 (Coefficient)**: 계수는 독립변수와 종속변수 간의 선형 관계를 나타내는 직선의 기울기다. 다른 모든 변수를 일정하게 유지하면서, 독립변수의 한 단위 변화로 인한 종속변수의 변화량을 측정한다. 양의 계수는 독립변수와 종속변수 간의 양의 관계를 나타내고, 음의 계수는 음의 관계를 나타낸다.

**R-squared 값**: R-squared 값은 독립변수의 변동으로 설명되는 종속변수의 변동 비율을 측정한다. 모델의 적합도를 나타내는 척도이며, 0에서 1 사이의 값을 갖는다. 높은 R-squared 값은 모델이 종속변수의 변동 중 큰 비율을 설명하며, 종속변수의 좋은 예측변수임을 나타낸다.

선형회귀에서 각 구성요소의 중요성:
- **P-value**: 독립변수와 종속변수 간의 관계가 통계적으로 유의한지, 모델을 유효한 예측에 사용할 수 있는지 판단하는 데 도움을 준다.
- **계수**: 독립변수와 종속변수 간 선형 관계의 방향과 강도를 결정하고, 독립변수의 변화에 따른 종속변수 예측을 수행하는 데 도움을 준다.
- **R-squared 값**: 모델의 전반적인 적합도를 평가하고, 여러 모델을 비교하여 종속변수의 최적 예측변수를 결정하는 데 도움을 준다.

### 선택 편향 (Selection Bias)

선택 편향은 연구나 분석에 참여할 개인이나 그룹을 선택하는 과정이 무작위적이지 않거나 관심 모집단을 대표하지 못할 때 발생하는 편향의 한 유형이다. 선택 편향은 모집단 모수의 편향된 추정을 초래하고 연구의 내적 및 외적 타당도에 영향을 미칠 수 있다.

선택 편향은 연구나 분석의 다양한 단계에서 발생할 수 있다:

**표본추출 편향 (Sampling bias)**: 표본이 관심 모집단을 대표하지 못할 때 발생한다. 예를 들어, 질병 유병률에 대한 연구가 특정 인구통계학적 또는 지리적 지역의 참가자만 포함한다면, 결과를 전체 모집단에 일반화할 수 없다.

**무응답 편향 (Non-response bias)**: 응답률이 낮거나, 참여를 거부한 참가자가 참여에 동의한 참가자와 다를 때 발생한다. 이는 대표성 없는 표본과 편향된 추정으로 이어질 수 있다.

**생존 편향 (Survivorship bias)**: 표본이 추적 연구와 같은 특정 선택 과정을 거쳐 생존하거나 지속된 개인이나 그룹만 포함할 때 발생한다. 이는 생존율이나 다른 결과의 과대평가로 이어질 수 있다.

**버크슨 편향 (Berkson's bias)**: 표본이 노출이나 관심 결과와 관련된 조건에 기반하여 선택될 때 발생한다. 예를 들어 병원 기반 표본의 경우다. 이는 노출과 결과 간 연관성의 과소평가로 이어질 수 있다.

선택 편향은 무작위 표본추출 기법을 사용하고 표본이 관심 모집단을 대표하도록 보장함으로써 최소화할 수 있다. 또한 결과가 올바르게 해석되도록 연구의 잠재적 편향과 한계를 분석하고 보고하는 것이 중요하다.

## Data Science: Bridging Statistics and Computation

통계학의 한계를 극복하기 위해 등장한 데이터 과학은 통계적 방법론에 컴퓨터 과학의 계산 능력을 결합하여 더 큰 규모와 복잡도의 데이터를 다룰 수 있게 한다.

### 핵심 강점
- **대규모 데이터 처리**: 분산 컴퓨팅과 병렬 처리로 빅데이터 분석 가능
- **실용적 접근**: 통계적 가정을 완벽히 만족하지 않아도 유용한 인사이트 도출
- **End-to-End 파이프라인**: 데이터 수집, 전처리, 분석, 시각화, 배포까지 전체 과정 포괄
- **도메인 지식 통합**: 비즈니스 문제를 이해하고 실제 가치 창출에 집중
- **다양한 도구 활용**: Python, R, SQL, Hadoop, Spark 등 다양한 기술 스택

### 데이터 과학의 정의

데이터 과학은 통계적, 계산적, 도메인 특화 지식과 기법을 사용하여 데이터로부터 인사이트와 지식을 추출하는 학제간 분야다. 통계학, 컴퓨터 과학, 수학, 도메인 지식의 방법과 개념을 결합하여 복잡한 데이터 관련 문제를 해결한다.

통계학과 컴퓨터 과학은 데이터 과학의 두 가지 중요한 구성요소다. 통계학은 데이터 분석과 모델링을 위한 기초적인 방법을 제공하고, 컴퓨터 과학은 데이터 저장, 처리, 분석을 위한 계산 인프라와 도구를 제공한다. 또한 컴퓨터 과학은 대규모 데이터셋에서 인사이트와 지식을 추출하는 데 사용할 수 있는 데이터 시각화, 머신러닝, 인공지능 기법을 제공한다.

데이터 과학은 데이터 수집, 정제 및 전처리, 탐색적 데이터 분석, 모델링 및 예측, 데이터 시각화, 결과 커뮤니케이션 등 광범위한 활동을 포함한다. 통계학, 프로그래밍, 도메인 지식에 대한 탄탄한 기초와 데이터 관리, 데이터 시각화, 머신러닝에 대한 전문 지식이 필요하다.

### 여전히 남는 한계
- **모델 선택의 어려움**: 어떤 모델을 사용할지 여전히 전문가의 판단 필요
- **특성 공학 부담**: 좋은 특성을 설계하는 것이 성능에 결정적이지만 시간 소모적
- **해석과 예측의 트레이드오프**: 복잡한 모델은 정확하지만 해석하기 어려움
- **수동 하이퍼파라미터 튜닝**: 모델 성능 최적화를 위한 반복적 실험 필요

> **이러한 한계를 극복하기 위해 데이터로부터 자동으로 학습하는 머신러닝이 발전했다.**

## Machine Learning: Learning Patterns from Data

머신러닝은 명시적으로 프로그래밍하지 않고도 데이터에서 패턴을 자동으로 학습하여 예측과 의사결정을 수행한다.

### 핵심 강점
- **자동 패턴 학습**: 데이터에서 스스로 규칙과 패턴을 발견
- **비선형 모델링**: 복잡한 비선형 관계도 효과적으로 포착
- **고차원 데이터 처리**: 수천, 수만 개의 특성도 처리 가능
- **유연한 가정**: 엄격한 통계적 가정 없이도 작동
- **지속적 개선**: 새로운 데이터로 모델을 계속 업데이트하고 개선

### 주요 알고리즘 유형
- **Supervised Learning**: 입력과 정답 쌍으로 학습 (회귀, 분류)
  - Linear/Logistic Regression, Decision Trees, Random Forest, SVM, Gradient Boosting
- **Unsupervised Learning**: 정답 없이 데이터 구조 발견 (군집화, 차원축소)
  - K-Means, Hierarchical Clustering, PCA, t-SNE
- **Ensemble Methods**: 여러 모델을 결합하여 성능 향상
  - Bagging, Boosting, Stacking

### 한계
- **대량의 데이터 필요**: 좋은 성능을 위해 많은 학습 데이터 필요
- **특성 공학 여전히 중요**: 좋은 특성 설계가 성능에 큰 영향
- **해석 가능성 낮음**: 특히 앙상블 모델은 블랙박스처럼 작동
- **비정형 데이터 처리 한계**: 이미지, 음성, 텍스트 같은 원시 데이터에는 수동 전처리 필요

> **특히 비정형 데이터 처리와 자동 특성 추출을 위해 딥러닝이 등장했다.**

## Deep Learning: Hierarchical Representation Learning

딥러닝은 다층 신경망을 통해 데이터의 계층적 표현을 자동으로 학습하며, 특히 비정형 데이터에서 혁신적인 성능을 달성했다.

### 핵심 강점
- **자동 특성 추출**: 원시 데이터에서 자동으로 유용한 특성 학습
- **계층적 표현**: 저수준 특성에서 고수준 추상 개념까지 단계적 학습
- **비정형 데이터 탁월**: 이미지, 음성, 텍스트 처리에서 인간 수준 성능
- **End-to-End 학습**: 전처리에서 예측까지 하나의 통합 모델로 학습
- **전이 학습**: 한 작업에서 학습한 지식을 다른 작업에 재사용

### 주요 아키텍처
- **Convolutional Neural Networks (CNN)**: 이미지 처리, 공간적 패턴 인식
- **Recurrent Neural Networks (RNN/LSTM/GRU)**: 시계열, 자연어 처리
- **Transformer**: 자연어 이해, 번역, 생성 (BERT, GPT)
- **Generative Models**: GAN, VAE - 새로운 데이터 생성
- **Graph Neural Networks**: 그래프 구조 데이터 처리

### 한계
- **막대한 데이터 요구**: 수백만~수십억 개의 샘플 필요
- **계산 자원 집약적**: 강력한 GPU/TPU와 많은 전력 소비
- **블랙박스 특성**: 왜 그런 예측을 했는지 설명하기 매우 어려움
- **과적합 위험**: 모델이 너무 복잡하면 훈련 데이터에만 과도하게 맞춰짐
- **하이퍼파라미터 민감**: 학습률, 배치 크기 등 설정에 따라 성능 크게 변동

> **딥러닝의 성능을 더욱 발전시키고 추론, 계획 등 고차원 인지 능력을 구현하기 위해 현대 AI가 발전하고 있다.**

## Artificial Intelligence: Towards Human-like Intelligence

인공지능은 머신러닝과 딥러닝을 기반으로, 인간의 인지 능력을 모방하고 자율적으로 의사결정을 내릴 수 있는 시스템을 구현한다.

### AI의 발전 단계

#### Rule-Based Systems (Expert Systems) - 초기 AI (1970-80년대)

초기 AI는 전문가의 지식을 if-then 규칙으로 명시적으로 코딩하는 방식으로 시작했다.

**작동 방식:**
```
IF 환자의 체온 > 38도 AND 기침 = 있음 THEN 감기 가능성 높음
IF 고객의 구매이력 > 10회 AND 최근방문 < 30일 THEN VIP 등급 부여
```

**강점:**
- **완전한 제어**: 시스템의 모든 동작을 정확히 예측 가능
- **해석 가능성**: 왜 그런 결론에 도달했는지 명확히 설명 가능
- **도메인 지식 활용**: 전문가의 경험과 지식을 직접 시스템에 반영
- **데이터 불필요**: 학습 데이터 없이도 작동

**치명적 한계:**
- **확장성 없음**: 규칙이 수백, 수천 개로 늘어나면 관리 불가능
- **규칙 충돌**: 여러 규칙이 상충할 때 해결 방법 불명확
- **예외 처리 어려움**: 모든 상황을 규칙으로 만들 수 없음
- **변화 적응 불가**: 새로운 패턴이나 상황에 자동으로 대응 못함
- **암묵적 지식 표현 한계**: 전문가도 설명 못하는 직관적 판단을 코딩할 수 없음

**실패 사례: AI의 겨울**

1980년대 후반, 많은 기업이 expert system에 막대한 투자를 했지만 실용성 부족으로 실패했다. 이는 "AI의 겨울"로 이어졌고, 데이터로부터 학습하는 머신러닝 접근법의 필요성을 입증했다.

> **규칙을 수동으로 만드는 것의 한계가 명확해지면서, 데이터로부터 자동으로 패턴을 학습하는 머신러닝과 딥러닝이 주류가 되었다.**

#### Machine Learning & Deep Learning Era (1990-2020년대)

통계적 학습과 신경망 기반의 머신러닝이 AI의 중심이 되었다. (앞서 Machine Learning과 Deep Learning 섹션 참조)

#### Modern AI: Integration and Emergence (2020년대~)

현대 AI는 대규모 데이터와 컴퓨팅 파워를 활용하여 인간 수준의 능력을 보이기 시작했다.

### 핵심 발전 방향
- **Large Language Models (LLMs)**: GPT, Claude 등 대규모 언어 모델
  - 자연어 이해, 생성, 추론, 대화
  - Few-shot/Zero-shot Learning: 적은 예제 또는 예제 없이도 새 작업 수행
- **Multimodal AI**: 텍스트, 이미지, 음성을 통합 처리
  - CLIP, GPT-4V 등 여러 모달리티 동시 이해
- **Reinforcement Learning**: 환경과 상호작용하며 최적 전략 학습
  - AlphaGo, OpenAI Five - 게임, 로봇 제어
- **Explainable AI (XAI)**: 블랙박스 모델의 의사결정 과정 설명
  - SHAP, LIME, Attention Visualization
- **AutoML**: 모델 선택과 하이퍼파라미터 튜닝 자동화

### AI의 현재와 미래
- **현재 달성한 것**:
  - 이미지 분류, 객체 인식에서 인간 수준 성능
  - 자연어 이해와 생성에서 놀라운 능력
  - 특정 게임과 작업에서 인간 능가
  - 의료 진단, 약물 발견, 과학 연구 지원

- **여전히 어려운 것**:
  - 상식 추론과 인과관계 이해
  - 소수 샘플로 학습 (Few-shot Learning 개선 필요)
  - 신뢰성과 안전성 보장
  - 편향과 공정성 문제 해결
  - 에너지 효율성 개선

### AI 윤리와 책임
- **공정성**: 편향되지 않은 데이터와 알고리즘
- **투명성**: 의사결정 과정의 설명 가능성
- **프라이버시**: 개인정보 보호와 데이터 거버넌스
- **책임성**: AI 시스템의 결정에 대한 책임 소재

## Conclusion: The Continuous Evolution

통계학에서 AI까지의 여정은 각 단계가 이전의 한계를 극복하면서 발전한 자연스러운 진화 과정이다:

1. **통계학**: 수학적으로 엄밀하지만 많은 가정과 제약
2. **데이터 과학**: 계산 능력으로 대규모 데이터 처리, 하지만 여전히 수동 작업 많음
3. **머신러닝**: 자동 패턴 학습으로 유연성 확보, 하지만 특성 공학 필요
4. **딥러닝**: 자동 특성 추출로 비정형 데이터 정복, 하지만 해석 어려움
5. **AI 발전**:
   - 초기 Rule-based: 완전한 제어와 해석 가능성, 하지만 확장성 없음
   - 현대 Learning-based: 데이터로부터 학습하여 복잡한 패턴 포착, 인간 수준 인지 능력 지향

각 방법론은 서로를 대체하는 것이 아니라 보완하는 관계다. 실제로 많은 현대 AI 시스템은 딥러닝과 규칙 기반 시스템을 결합한다:
- **Hybrid Systems**: 딥러닝으로 패턴 학습 + 규칙으로 안전장치와 제약조건 구현
- **Neural-Symbolic AI**: 신경망의 학습 능력 + 기호적 추론의 논리성

문제의 특성, 데이터의 양과 질, 해석 가능성의 중요도, 계산 자원, 안전성 요구사항 등을 고려하여 적절한 방법을 선택하고 결합하는 것이 중요하다.

미래의 데이터 과학자와 AI 연구자는 이 모든 단계를 이해하고, 각각의 강점과 한계를 파악하여, 실제 문제에 가장 적합한 접근법을 선택할 수 있어야 한다.
