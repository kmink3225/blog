
## 프로젝트 개요

**과제명**: AI Agent 기반 지능형 기술 지식 플랫폼 구축

### 문제 배경
- 지식 파편화(사일로화), 정보 탐색 비효율, 인력 이탈로 인한 업무 병목
- 암묵지 의존으로 인수인계 부하 및 기술 부채 증가

### 3가지 핵심 개발 목표

| # | 기능 | 지표 |
|---|------|------|
| 1 | **지식 QnA Chatbot** - 기존 문서 행간 해석 AI | 데이터 표준화 원칙 100% 명시화 |
| 2 | **데이터 표준화 도우미 Agent** - 메타데이터 자동화 | 물리명/설명/데이터타입 3종 추천 |
| 3 | **인실리코 알고리즘 분석 Agent** - 코드 투명성 확보 | 인실리코 코드 분석 100% 커버 |

### 시스템 아키텍처

```
[FE] → [LB] → [Backend/Java] → [AI Engine (K8s/Docker)]
                                        ↓
                              RAG (FastAPI)
                              ├── Azure OpenAI Embeddings
                              ├── Azure Document Intelligence (문서 파싱)
                              ├── Vector Store (Azure AI Search)
                              ├── Rule-Based Engine (AST/CFG Parser 등)
                              └── Data-Driven Engine (DL/ML 모델)
                                        ↓
                              LLM (Azure OpenAI, OpenAI, HuggingFace 등)
                                        ↓
                              Data Storage (SQL DW, MySQL, Data Lake)

[Monitoring] Azure Monitor + Log Analytics + Cost Management
```

### 현재 워크스페이스와의 연관성

이 아키텍처의 **Streamlit 기반 프로토타입**으로 독립적으로 개발하고 후에 추상화하여 통합할 예정:
- `1_QnA_Chatbot.py` → 목표 1 (지식 QnA)
- `2_Data_Standardizer.py` → 목표 2 (데이터 표준화 Agent)
- `3_Code_Analyzer.py` → 목표 3 (인실리코 분석 Agent)

## 과제 규모 추정

**중대형 엔터프라이즈 AI 프로젝트** (6~18개월, 팀 5~10명 수준)

### 개발 범위
- **3개 AI Agent** (QnA Chatbot, 데이터 표준화, 코드 분석)
- **Full-Stack**: FE + Java Backend + AI Engine(Python) + Data Pipeline
- **현재 프로토타입** → **프로덕션 전환** 수준의 작업

## 필요 인프라

### Azure 핵심 서비스

| 서비스 | 용도 | 예상 티어 |
|--------|------|-----------|
| **Azure OpenAI** | LLM + Embeddings (text-embedding-3-large) | PTU 또는 Standard |
| **Azure AI Search** | Vector Store (RAG 근거 검색) | Standard S1+ |
| **Azure Document Intelligence** | PDF/문서 파싱 | S0+ |
| **Azure Kubernetes Service (AKS)** | AI Engine 컨테이너 오케스트레이션 | Standard B/D 시리즈 |
| **Azure Container Registry** | Docker 이미지 관리 | Basic/Standard |
| **Azure SQL / Synapse** | Data Warehouse | General Purpose |
| **Azure Data Lake Storage** | 비정형 데이터 저장 | Gen2 |
| **Azure Monitor + Log Analytics** | 모니터링 | Pay-as-you-go |
| **Azure Cost Management** | 비용 추적 | 무료 |

### 추가 인프라

```
Apache Airflow       - 데이터 파이프라인 오케스트레이션
Load Balancer        - 트래픽 분산 (Azure Application Gateway)
Java Backend         - Spring Boot 기반 API Gateway
FastAPI              - AI Engine REST API
MySQL                - 운영 DB (메타데이터 등)
```

## 리소스 요구사항

### 컴퓨팅 (AKS 기준)

| 컴포넌트 | 스펙 | 이유 |
|----------|------|------|
| AI Engine Pod | CPU 4코어 / 16GB RAM | FastAPI + RAG 처리 |
| Rule-Based Engine | CPU 8코어 / 32GB RAM | 메타데이터 파싱 부하 |
| Data-Driven Engine | GPU (A100/T4) | DL 모델 추론 |
| Backend (Java) | CPU 2코어 / 8GB RAM | API 처리 |

### 비용 추정 (월간, Azure Korea 기준)

```
Azure OpenAI (GPT-4o)       ~$500~2,000  (사용량에 따라 변동)
Azure AI Search (S1)        ~$250
AKS 클러스터                ~$300~800
Azure SQL / ADLS             ~$200
Document Intelligence        ~$150
기타 (모니터링, 네트워크 등) ~$200
─────────────────────────────────────
총계 (추정)                  ~$1,600~3,600/월
```

## 현재 PoC 상태 vs 목표 Gap

| 항목 | 현재 (프로토타입) | 목표 (프로덕션) |
|------|-----------------|----------------|
| UI | Streamlit | React/Vue FE + Java Backend |
| 배포 | 로컬 | AKS (K8s + Docker) |
| LLM | 단일 Provider | Multi-LLM (OpenAI, HuggingFace 등) |
| 문서 파싱 | 기본 | Azure Document Intelligence |
| 파이프라인 | 없음 | Apache Airflow |
| 모니터링 | 없음 | Azure Monitor + Log Analytics |


### 현재 PoC 상태

(2026-02-20 기준)

src/agent/
├── app.py                    ✅ 완성 (메인 진입점, Streamlit multipage)
├── styles.css                ✅ 완성
├── config/                   ✅ 완성도 높음
│   ├── settings.py           ✅ RAG 실험 설정 전체 구현 (461줄)
│   ├── env_loader.py         ✅ Azure/Local 환경변수 중앙 관리
│   └── llm_config.py         ✅
├── shared/                   ✅ 핵심 인프라 완성
│   ├── llm/provider.py       ✅ Azure OpenAI ↔ Local(Ollama) 추상화
│   ├── rag/
│   │   ├── chatbot_retriever.py    ✅ 문서 로더(PDF/Markdown), 청킹, 검색 (331줄)
│   │   └── vectorstore_provider.py ✅ Azure AI Search ↔ FAISS 추상화
│   ├── metrics/experiment_metrics.py ✅ RAG 성능 측정 메트릭 구현 (394줄)
│   ├── ui/                   ❌ 비어있음
│   └── utils/                ❌ 비어있음
├── pages/
│   ├── 1_QnA_Chatbot.py      ✅ 가장 완성도 높음 (1306줄) - RAG 완전 구현 (추상화 전)
│   ├── 2_Data_Standardizer.py ❌ UI 껍데기만 (분석 기능 미구현) 
│   └── 3_Code_Analyzer.py    ❌ UI 껍데기만 (분석 기능 미구현)
├── agent_qna_chatbot/
│   └── modules/
│       └── docstring-generator.py ❌ 빈 파일
├── prompts/                  ✅
│   ├── chatbot_data_standardization_rules.yml ✅
│   ├── data_standardizer.yml  ✅
│   └── code_analyzer.yml      ✅
└── tests/                    ❌ __init__.py만 존재 (테스트 없음)

### 기능별 완성도

| 기능 | 완성도 | 상태 |
|------|--------|------|
| **인프라 (LLM/VectorStore 추상화)** | 90% | Azure/Local 전환 가능, 잘 설계됨 |
| **QnA Chatbot (Page 1)** | 70% | RAG 동작, 실험 메트릭 수집, 문서 선택 UI |
| **데이터 표준화 (Page 2)** | 5% | UI 껍데기만, 로직 전무 |
| **코드 분석기 (Page 3)** | 5% | UI 껍데기만, 로직 전무 |
| **Docstring Generator** | 0% | 빈 파일 |
| **테스트 코드** | 0% | 없음 |
| **shared/ui, shared/utils** | 0% | 없음 |

## 개발 로드맵

| 원칙 | 의미 |
|------|------|
| **POC 우선** | 추상화는 나중에, 지금은 기능 검증 |
| **계약 기반 확장** | Phase 2 계약대로 agent를 찍어냄 |
| **Multi-repo Monolithic** | repo는 분리, 각 repo 내부는 monolithic |
| **Sub-agent 허용** | main agent 아래 evaluator/helper 다수 가능 |
| **오버스펙 방지** | 실제 필요한 것만 공통화 |

### Phase 1 - POC (현재 ~ 3개월)  

- 3개 Agent **독립적으로** 개발 (공유 의존성 최소화)
- **공유 자원**: Streamlit UI만
- 목표: 기능 검증, 오버스펙 방지

```
pages/              ← UI만 공유
├── 1_QnA_Chatbot    ← 독립 POC
|   └── sub-agents (evaluator, helper 등)
├── 2_Data_Std       ← 독립 POC
|   └── sub-agents (evaluator, helper 등)  
└── 3_Code_Analyzer  ← 독립 POC
    └── sub-agents (evaluator, helper 등)
```

### Phase 2 - 추상화 (POC 결과 기반)
- 3개 POC에서 **실제로 반복된 패턴**만 공통 모듈로 추출
- Agent 생애주기(lifecycle) 계약 정의
- 성능 평가 지표 표준화

```
contract/
├── BaseAgent         ← 모든 Agent가 따르는 계약
├── AgentLifecycle    ← 공통 생애주기
└── EvalMetrics       ← 공통 성능 평가
```

### Phase 3 - Multi-Repo Monolithic 확장
```
repo: agent-qna/          ← Main Agent 1
  └── sub-agents/
      ├── retriever-agent
      ├── reranker-agent
      └── evaluator-agent

repo: agent-data-std/     ← Main Agent 2
  └── sub-agents/
      └── validator-agent

repo: agent-code-analyzer/ ← Main Agent 3
  └── sub-agents/
      ├── ast-agent
      └── docstring-agent

repo: agent-commons/      ← Phase 2에서 추출한 공통 계약/모듈
```

## 현재 PoC기준 Azure 자원 이용 현황

### Resource Group: 

`rg-aipoc-test-krc-001` (테스트 구독)

#### 구축된 리소스 전체 목록

| 리소스명 | 타입 | 리전 | 용도 |
|---------|------|------|------|
| `test-agent` | VM (D8as v5) | Korea Central | Streamlit 앱 실행 서버 |
| `aipoc-code-analyzer-krc-001` | Azure OpenAI | Korea Central | LLM (주) |
| `aipoc-code-analyzer-krc-002` | Azure OpenAI | Sweden Central | LLM (백업/분산) |
| `aipoc-code-analyzer-krc-003` | Azure OpenAI | East US | LLM (백업/분산) |
| `di-aipoc-test-krc-001` | Document Intelligence | Korea Central | 문서 파싱 |
| `vdb-aipoc-test-krc-001` | Search service | Korea Central | Vector DB (Azure AI Search) |
| `saaipoctestkrc001` | Storage account | Korea Central | 파일 저장 |
| `privatelink.openai.azure.com` | Private DNS zone | Global | OpenAI Private 통신 |
| `pe-aipoc-code-analyzer-krc-001` | Private endpoint | Korea Central | OpenAI 보안 접속 |
| `pe-aipoc-code-analyzer-krc-003` | Private endpoint | East US | OpenAI 보안 접속 |
| `vnet-aipoc-test-krc-insilico-001` | Virtual network | Korea Central | 주 네트워크 |
| `vnet01~vnet01-4` | Virtual networks | 다중 리전 | 리전간 네트워크 피어링 |

### 아키텍처

```
인터넷
  │
  ▼
[VM: test-agent]  Public IP: 20.196.144.16
  │  (Ubuntu 24.04, 8vCPU/32GB)
  │  → Streamlit 앱 구동
  │
  └── Private Network (172.16.0.4)
        │
        ├── [Private Endpoint] → Azure OpenAI (KRC)  ← 주 LLM
        │                      → Azure OpenAI (EUS)  ← 분산 (특정 LLM의 배포유형이 서비스 지역에 따라 배포 제한이 있음)
        │                      (Sweden는 Public 접속)
        │
        ├── [Azure AI Search]   → Vector DB (RAG용)
        ├── [Document Intelligence] → 문서 파싱
        └── [Storage Account]   → 파일/데이터 저장
```

### 핵심 포인트 설명

**1. Azure OpenAI를 3개 리전에 배포한 이유**
- Azure OpenAI는 **리전별 TPM(분당 토큰) 쿼터 제한**이 있는 것으로 파악됨
- Korea Central 할당량이 부족하거나 너무 지연시간이 길면 Sweden/East US로 분산
- provider.py의 `LLM_PROVIDER` 환경변수로 전환 가능하게 설계할 것

**2. Private Endpoint를 쓰는 이유**
- Azure OpenAI를 인터넷 노출 없이 VM → Private IP로 직접 통신
- `privatelink.openai.azure.com` DNS zone이 이를 내부 라우팅 처리
- **보안 강화** + 네트워크 트래픽 비용 절감

**3. VM 스펙 (D8as v5: 8vCPU/32GB)**
- Streamlit + RAG 파이프라인 동시 처리에 충분한 스펙
- GPU는 없음 → 현재는 Azure OpenAI API 호출만, 로컬 모델 추론은 불가

### POC 관점에서 이 인프라 평가

| 항목 | 평가 |
|------|------|
| LLM 다중화 | ✅ 3리전으로 쿼터 문제 대비 |
| 보안 | ✅ Private Endpoint 적용 |
| VM 스펙 | ✅ POC에 충분 |
| Vector DB | ✅ Azure AI Search 준비 완료 |
| 문서 파싱 | ✅ Document Intelligence 준비 완료 |
| GPU | ❌ 없음 (로컬 모델 불가, API 전용) |
| K8s/AKS | ❌ 없음 (향후 필요) |

## 워크샵 내용

### 현재 PoC 공유할 상황

```
프로젝트: AI Agent 기반 지능형 기술 지식 시스템
단계:     Phase 1 POC 
팀:       소규모 (Seegene, Core개발팀)

[예정]
- Streamlit 기반 멀티페이지 앱
- LLM Provider 추상화 (Azure ↔ Local 전환)
- RAG 파이프라인 (QnA Chatbot, Azure AI Search + FAISS)
- Data Standardizer: 미구현
- Code Analyzer: 미구현
- Azure OpenAI 다중 리전 fallback 로직 구현 예정
- Storage Account 연동 로직 구체화

[목표]
- 추상화 후 Multi-repo 확장
```

### 현재 Azure 자원 이용도 현황

| 리소스 | 이용도 | 진단 |
|--------|--------|------|
| **VM (test-agent)** | ✅ 적극 사용 | Streamlit 앱 구동|
| **Azure OpenAI (KRC-001)** | ✅ 주력 사용 | LLM + Embedding 모두 이 엔드포인트 |
| **Azure OpenAI (KRC-002, 003)** | ⚠️ 준비만 됨 | 다중화 목적이나 코드에 fallback 로직 구현 예정 |
| **Azure AI Search** | ✅ 주력 사용 | vectorstore_provider에 연결됨. 실제 인덱스 운영 예정 |
| **Document Intelligence** | ❌ 미사용 | 필요시 사용 |
| **Storage Account** | ✅ 사용 | 주기적으로 사용 예정 |
| **Private Endpoint/DNS** | ✅ 인프라 구성 | 보안 통신 설정됨 |
| **VNet 다중구성** | ⚠️ 과잉 구성 | vnet01~4 (4개 리전) - 네트워킹 개념 스터디 필요 |


### Azure 활용 아키텍처

* 방향성 및 보완 사항 검토 요청  
  - **모든 Agent**가 동일한 Azure OpenAI 엔드포인트를 호출  
  - **모든 Agent**가 Azure AI Search를 공유 (인덱스 이름으로 논리 격리)  
  - **모든 Agent**의 성능 로그가 동일한 Monitor/App Insights로 수집  
* Agent별 리소스 격리 전략과 쿼터 관리에 대한 질문하기
    - 후에 Agent가 늘어날수록 공유 리소스의 쿼터 경쟁과 인덱스 충돌이 생길 수 있을까?

```
[각 Agent Repo]
      │
      ▼
[Azure API Management]   ← 인증, 라우팅, 쿼터 관리, 버전 관리
      │
      ├── Agent 1 (Container Apps)──┐  ┌── [공유] Azure OpenAI (다중 리전 LB)
      ├── Agent 2 (Container Apps)──┤──├── [공유] Azure AI Search 
      └── Agent 3 (Container Apps)──┘  └── [공유] Azure Monitor + App Insights    
                                          
                              │
                              ▼
                    [대시보딩 (Grafana, Power BI, etc.)]
                    (Agent별 성능 비교)
```

## 질문 리스트

### 상황 설명 (워크샵에서 먼저 전달할 컨텍스트)

* "3개의 AI Agent POC를 단일 Streamlit 앱으로 개발하려합니다. 이후 각 Agent를 독립 repository로 분리하고, 새로운 도메인이 생길 때마다 동일한 계약 기반으로 Agent를 계속 추가하고 싶습니다. 각 Agent는 sub-agent를 가질 수 있고, 모든 Agent의 성능(RAG 품질, 응답시간, 토큰 비용 등)을 중앙에서 관리하고 싶습니다. **이런 'Agent 플랫폼'을 Azure에서 어떻게 설계해야 하는지 자문을 받고 싶습니다.**"

* *"3개의 AI Agent POC를 Azure OpenAI + AI Search 로 구축 예정이며, 인프라는 준비됐지만 코드와의 연결은 진행중에 있습니다. 오늘 가장 확인하고 싶은 건 **다중 리전 OpenAI fallback 전략**, **Document Intelligence 도입 가치**, **현재 VNet 비용 최적화** 3가지입니다."*

### 아키텍처 결정에 영향이 있는 질문

**Q1. Azure OpenAI 다중 리전 전략**
> "Korea Central, Sweden Central, East US에 OpenAI 인스턴스를 3개 배포했습니다. LLM의 Deployment Type이 특정 Region에 제한을 받는 것이 파악되었고, 쿼터 초과 시 자동 failover 하려면 Provider 또는 LangChain 레벨에서 직접 구현해야 하나요, 아니면 Azure API Management나 다른 managed 방법이 있나요?"

**Q2. Document Intelligence vs LangChain PDFPlumber**
> "DI(Document Intelligence)가 비정형 이미지나 복잡한 도면에서는 성능이 떨어지거나 이용성 면에서 적절치 않다고 판단했습니다. DI가 어떤 문서 유형(1개의 노드에 수십개의 복잡한 엣지가 연결된 지식그래프, 수많은 그림이 포함된 문서처럼 사용되는 엑셀파일, 계약서, 표 중심 문서, 반정형 보고서 등)에서 실질적인 이점이 있나요? 그리고 DI를 선택적으로 적용할 때 문서 타입을 사전 분류하는 전략을 어떻게 설계하면 좋을까요?"

**Q3. Azure AI Search Hybrid Search 설정**
> "`gpt-4.1-mini`에서 `search_type=similarity`를 설정할 경우 RAG의 성능이 저조한 것을 관찰했습니다. `search_type=hybrid`로 설정하여 성능이 향상되는 것을 확인했는데, 어떤 상황에서 Semantic Ranker(월 $250 추가 과금)를 함께 써야 효과가 극대화되나요? Semantic Ranker는 별도 과금인데 POC 단계에서 활성화할 가치가 있나요?"

### 인프라/비용 질문

**Q4. VM vs Container Apps for Streamlit/REACT**
> "현재 D8as v5 VM에서 Streamlit을 직접 실행 중입니다. POC 단계에서 Azure Container Apps로 전환하면 비용이나 관리 측면에서 이점이 있나요? 추후 서비스 배포시 (내부 조직 약 100명 규모), AKS로의 마이그레이션 경로도 고려해야 하나요?"

**Q5. VNet 구성 최적화**
> "현재 네트워킹 지식이 부족해서 azure resource에서 자동 추천되는 옵션을 선택해서 vnet01~vnet01-4까지 4개 리전에 VNet이 만들어졌습니다. POC 목적에서 이 구성이 필요한지 또는 최적화할 수 있는지 검토해주실 수 있나요? 불필요한 비용이 발생하고 있지는 않은지 이해하고 싶습니다."

**Q6. Private Endpoint + Streamlit 접속**
> "VM은 Public IP로 Streamlit에 접속하고, Azure OpenAI는 Private Endpoint로 통신합니다. 현재 Streamlit에 인증이 없는데, Azure AD 기반 접근 제어를 가볍게 붙이는 방법이 있나요? (Azure Static Web App Auth, App Gateway + OIDC 등)"

**Q7. LLM Latency 질문**
> "현재 PoC단계이기 때문에 Korea Central Region을 사용하고 있다. 그런데, 개인적으로 다른 용도로 OpenAI에서 api key를 받아 직접 LLM을 연결할떄는 latency가 azure openai에 비해 적은 것을 체감했습니다. 실제로 그런 이유가 있나 아니면 저의 개인적인 느낌일까요?" latency가 긴 이유가 있다면 다음의 원인 때문일까요? 
- 직접 OpenAI API: 개인 PC → OpenAI CDN (전세계 분산) → 가장 가까운 서버 선택됨
- Azure OpenAI: 개인 PC → **Azure 리전 (Korea Central) → Azure 내부** → OpenAI 모델
- Private Endpoint 추가 경유: VM → VNet → **Private DNS 조회 및 Internal Routing** → Private Endpoint → Azure OpenAI
- Azure 내부 안전장치 레이어: Content Filtering & Responsible AI 정책 검토

맞다면 어떻게 개선해야할까요?

### 향후 플랫폼화 대비 질문

**Q7. Multi-repo Monolithic Agent 아키텍처에서의 Azure 전략**
> "추후 3개의 Main Agent들을 독립 repository로 Agent를 분리하고, 각 Repository에 sub-agents를 다수 만들 계획입니다. 각 Agent가 동일한 Azure AI Search와 Azure OpenAI를 공유할 예정인데 리소스 격리(인덱스 네이밍 규칙, API Key 분리 등)를 어떻게 설계하는 게 베스트 프랙티스인가요?"

**Q8. Azure AI Foundry / AI Studio 활용**
> "실험 메트릭을 자체 코드기반 로그 데이터로 수집하려 하는데, Azure AI Foundry의 Evaluation 기능이 RAG 성능 평가에 도움이 될 수 있나요? Prompt Flow와의 통합도 고려해야 할까요?"

**Q9. 비용 최적화**
> "GPT-4.1, GPT-5-mini 등 최신 모델 deployment가 설정에 있는데, POC에서 비용 대비 성능이 가장 좋은 모델 선택 기준을 조언해주실 수 있나요? PTU(Provisioned Throughput) 전환 시점은 언제가 적절한가요?"


### 세부 질문 5가지

**Q1. Agent별 리소스 격리 전략**
> "Agent가 늘어날 때 Azure OpenAI, AI Search 인덱스, Storage를 Agent별로 분리해야 할까요, 아니면 공유 리소스에 네이밍 규칙으로 논리적 격리만 해도 충분할까요? 리소스 격리 수준(Resource Group 분리 vs 인덱스 네이밍 분리)의 기준이 뭔가요?"

**Q2. 중앙 성능 관리 구조**
> "각 Agent의 응답시간, 토큰 사용량, RAG 품질 점수(정확도, 관련성)를 중앙에서 수집하고 비교하려 합니다. Azure Monitor + Log Analytics로 충분한가요, 아니면 Azure AI Foundry의 Evaluation이나 Application Insights를 함께 써야 하나요? Agent별 성능 대시보드를 만들려면 어떤 구성이 권장되나요?"

**Q3. Agent 배포 파이프라인**
> "Agent마다 독립 repository를 가지는 Multi-repo 구조입니다. 각 Agent를 독립적으로 배포하되, 공통 계약(인터페이스)의 버전은 맞춰야 합니다. Azure DevOps나 GitHub Actions로 이런 구조의 CI/CD를 어떻게 설계하면 좋을까요? 공통 라이브러리 变更 시 downstream Agent 전체를 테스트하는 방법도 궁금합니다."

**Q4. Agent 트래픽 관리 및 확장**
> "Agent별로 사용량이 다를 수 있고, 특정 Agent에 트래픽이 몰릴 수 있습니다. Azure API Management를 Agent 앞단에 두고 라우팅, Rate Limiting, 인증을 통합 관리하는 구조가 맞나요? Agent를 Container Apps vs AKS 중 어디서 운영하는 게 이 규모에 적합한가요?"

**Q5. Azure OpenAI 쿼터 관리 (멀티 Agent 환경)**
> "Agent가 늘어나면 Azure OpenAI 쿼터 경쟁이 생깁니다. 현재 3개 리전에 OpenAI 인스턴스가 있는데, Agent별로 쿼터를 할당하거나 우선순위를 두는 방법이 있나요? APIM의 Load Balancing 정책으로 다중 OpenAI 엔드포인트를 관리하는 패턴을 써야 하나요?"


# 정리해야할 사항

LLM Failover 전략 구현 위치: `provider.py` vs LangChain 레벨

짧게 정리하자면 — 둘 다 가능하지만 책임 구분 때문에 `provider.py`(인프라/엔드포인트 선택 책임)에 failover를 두는 게 권장됩니다. 이유와 권장 구현 요점은 아래와 같습니다.

- 왜 LangChain에서 묻나: LangChain은 LLM 호출과 재시도/콜백을 쉽게 제어하는 레이어라서 체인(비즈니스 로직) 단에서 재시도·로컬 폴백 정책을 붙이기 편함. 그래서 설계 문서에 LangChain 옵션도 적어둔 것임.
- 권장 아키텍처: 핵심은 중앙화된 엔드포인트/토큰·헬스체크 관리 → `provider.py`에 담당시킴. LangChain 쪽에는 옵셔널(체인별) 재시도/타임아웃 정책만 둠.
- 이유: 중앙화
  - 인증·프라이빗 엔드포인트 관리 한곳에서 처리
  - 로깅/메트릭/쿼터 제어 일관성 확보
  - 네트워크·리전별 접근성(Private Endpoint) 제어 쉬움
- LangChain의 역할: 호출 레벨에서 짧은 재시도, 체인별 타임아웃·로그 캡처, 사용자 친화적 콜백(모니터링) 제공
- 반드시 고려할 사항: 리전별 API 키/엔드포인트, 헬스체크(주기적), 지연시간 기반 라우팅, 지표(토큰 비용/latency), 세션 일관성(상태 있는 요청 주의)

간단한 `provider.py` 패턴 (의사코드):

```python
class MultiRegionProvider:
    def __init__(self, endpoints):  # endpoints = [("krc", url, key), ("swe", ...), ...]
        self.endpoints = endpoints
        self.healthy = {e[0]: True for e in endpoints}

    def call(self, payload):
        for region, url, key in ordered_endpoints_by_health_and_latency():
            try:
                return send_request(url, key, payload, timeout=5)
            except (QuotaError, TimeoutError, HTTPError) as e:
                mark_unhealthy(region)
                log_warning(...)
                continue
        raise RuntimeError("All endpoints failed")
```

권장 단계 (작업 리스트)
1. `provider.py`에 엔드포인트 리스트 + 헬스체크 구현  
2. 실패시 백오프 + circuit-breaker 간단 구현  
3. LangChain 설정에는 짧은 재시도/체인 타임아웃만 남김  
4. 모니터링(토큰 사용량·latency) 연결

원하면 `provider.py`용 구체적 코드 패치(테스트 포함)를 작성해 드릴게요. 진행할까요?