{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"GPU와 CUDA를 활용한 딥러닝 환경 구축\"\n",
        "subtitle: \"GPU 기초부터 CUDA 설치까지 완벽 가이드\" \n",
        "description: |\n",
        "  딥러닝을 위한 GPU와 CUDA의 개념을 이해하고, 실제 환경 구축 방법을 알아본다.\n",
        "  NVIDIA GPU를 활용한 딥러닝 가속화 환경을 단계별로 설정하는 방법을 다룬다.\n",
        "categories:\n",
        "  - Engineering\n",
        "  - Infrastructure\n",
        "author: Kwangmin Kim\n",
        "date: 05/01/2023\n",
        "format: \n",
        "  html:\n",
        "    page-layout: full\n",
        "    code-fold: true\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "comments: \n",
        "  utterances: \n",
        "    repo: ./docs/comments\n",
        "draft: False\n",
        "execute: \n",
        "  eval: false\n",
        "---"
      ],
      "id": "4836e708"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPU란 무엇인가?  \n",
        "\n",
        "## CPU의 정의  \n",
        "\n",
        "* **CPU(Central Processing Unit)**는 컴퓨터의 중앙 처리 장치  \n",
        "* 모든 연산과 제어를 담당하는 핵심 부품이다.   \n",
        "* 복잡한 명령어를 순차적으로 처리하는 데 최적화되어 있으며, 일반적으로 4-32개의 고성능 코어를 가지고 있다.  \n",
        "* 그냥 컴퓨터의 두뇌라고 생각하면 된다.   \n",
        "* 컴퓨터의 일반적이고 기본적인 모든 연산은 CPU가 처리한다.  \n",
        "\n",
        "### CPU (Central Processing Unit)  \n",
        "\n",
        "* **설계 철학**: 복잡한 명령어를 빠르게 순차 처리  \n",
        "* **코어 구조**: 적은 수의 강력한 코어 (4-32개)  \n",
        "* **캐시 메모리**: 대용량 캐시로 지연 시간 최소화  \n",
        "* **분기 예측**: 복잡한 제어 흐름 처리에 최적화  \n",
        "\n",
        "## GPU의 정의  \n",
        "\n",
        "* **GPU(Graphics Processing Unit)**는 원래 그래픽 처리를 위해 설계된 전용 프로세서이다.   \n",
        "* 하지만 현재는 딥러닝과 같은 병렬 연산이 필요한 작업에서 CPU보다 훨씬 뛰어난 성능을 보여준다.  \n",
        "\n",
        "### GPU (Graphics Processing Unit)  \n",
        "\n",
        "* **설계 철학**: 단순한 연산을 대량으로 병렬 처리  \n",
        "* **코어 구조**: 많은 수의 단순한 코어 (수백-수천개)  \n",
        "* **메모리**: 높은 대역폭의 전용 메모리(VRAM)  \n",
        "* **SIMD 구조**: 같은 명령을 여러 데이터에 동시 적용  \n",
        "\n",
        "\n",
        "## 딥러닝에서 GPU를 사용하는 이유  \n",
        "\n",
        "### 행렬 연산의 병렬성  \n",
        "\n",
        "딥러닝의 핵심인 행렬 곱셈은 본질적으로 병렬 처리가 가능:  \n",
        "\n",
        "$$  \n",
        "C_{ij} = \\sum_{k=1}^{n} A_{ik} \\times B_{kj}  \n",
        "$$  \n",
        "\n",
        "각 $C_{ij}$ 원소는 독립적으로 계산 가능하므로 GPU의 수천 개 코어가 동시에 처리할 수 있다.  \n",
        "\n",
        "### 대용량 데이터 처리  \n",
        "\n",
        "* **배치 처리**: GPU의 수천 개 코어가 동시에 연산 수행, 특히 합성곱(Convolution) 연산에서 큰 성능 향상  \n",
        "  * 합성곱: 이미지나 신호 처리에서 사용되는 수학적 연산으로, 필터(커널)를 입력 데이터 위에서 슬라이딩하며 각 위치에서 요소별 곱셈과 합을 수행하는 연산. CNN에서 특징 추출의 핵심 연산이며, 각 필터 연산이 독립적이어서 GPU의 병렬 처리에 매우 적합함  \n",
        "* **높은 메모리 대역폭**: CPU 대비 10-20배 빠른 메모리 접근  \n",
        "* **병렬 데이터 로딩**: 여러 데이터를 동시에 GPU 메모리로 전송  \n",
        "* **전용 메모리(VRAM)**: GPU 전용 메모리로 빠른 데이터 접근, CPU-GPU 간 데이터 전송 최소화  \n",
        "\n",
        "### 성능 비교  \n",
        "\n",
        "| 특성 | CPU | GPU |  \n",
        "|------|-----|-----|  \n",
        "| 코어 수 | 적음 (4-32개) | 많음 (수백-수천개) |  \n",
        "| 연산 방식 | 순차 처리 | 병렬 처리 |  \n",
        "| 메모리 대역폭 | 낮음 | 높음 |  \n",
        "| 딥러닝 성능 | 느림 | 빠름 |  \n",
        "| 전력 효율성 | 높음 | 낮음 |  \n",
        "\n",
        "\n",
        "# CUDA란 무엇인가?  \n",
        "\n",
        "## CUDA의 정의  \n",
        "\n",
        "* **CUDA(Compute Unified Device Architecture)**는 NVIDIA에서 개발한 병렬 컴퓨팅 플랫폼 및 프로그래밍 모델  \n",
        "* 딥러닝 모델을 가속화하기 위해 사용되는 프로그래밍 모델  \n",
        "\n",
        "## CUDA의 핵심 개념  \n",
        "\n",
        "### 병렬 프로그래밍 모델  \n",
        "\n",
        "```python  \n",
        "# CPU 코드 (순차 처리)  \n",
        "for i in range(1000000):  \n",
        "    result[i] = a[i] + b[i]  \n",
        "\n",
        "# GPU 코드 개념 (병렬 처리)  \n",
        "# 1000000개의 스레드가 동시에 실행  \n",
        "```  \n",
        "\n",
        "### 메모리 계층 구조  \n",
        "\n",
        "- **스레드(Thread)**: GPU에서 실제 연산을 수행하는 최소 실행 단위  \n",
        "  - CPU의 스레드와 달리 GPU 스레드는 매우 가벼움 (컨텍스트 스위칭 비용이 거의 없음)  \n",
        "  - 수천 개의 스레드가 동시에 실행되어 병렬 처리 수행  \n",
        "  - 각 스레드는 고유한 ID를 가지며, 이를 통해 처리할 데이터를 구분  \n",
        "- **글로벌 메모리**: 모든 스레드가 접근 가능, 느림  \n",
        "- **공유 메모리**: 블록 내 스레드 공유, 빠름  \n",
        "- **레지스터**: 개별 스레드 전용, 가장 빠름  \n",
        "\n",
        "### 스레드 계층 구조  \n",
        "\n",
        "```  \n",
        "Grid (전체 작업)  \n",
        "├── Block 1  \n",
        "│   ├── Thread 1  \n",
        "│   ├── Thread 2  \n",
        "│   └── ...  \n",
        "├── Block 2  \n",
        "└── ...  \n",
        "```  \n",
        "\n",
        "# GPU와 CUDA의 관계  \n",
        "\n",
        "## GPU (하드웨어)  \n",
        "\n",
        "- **물리적 장치**: 실제 그래픽 카드에 탑재된 프로세서  \n",
        "- **병렬 처리 능력**: 수천 개의 코어로 동시 연산 수행  \n",
        "- **하드웨어 자원**: 메모리, 연산 유닛 등 물리적 자원 제공  \n",
        "\n",
        "## CUDA (소프트웨어 플랫폼)  \n",
        "\n",
        "- **프로그래밍 도구**: GPU의 병렬 처리 능력을 활용할 수 있게 해주는 소프트웨어  \n",
        "- **개발 환경**: GPU 프로그래밍을 위한 컴파일러, 라이브러리, API 제공  \n",
        "- **NVIDIA 전용**: NVIDIA GPU에서만 동작  \n",
        "\n",
        "## 딥러닝에서의 역할  \n",
        "\n",
        "1. **GPU**: 실제 행렬 연산을 병렬로 처리  \n",
        "2. **CUDA**: PyTorch, TensorFlow 등이 GPU를 쉽게 사용할 수 있도록 지원  \n",
        "\n",
        "* **GPU는 하드웨어이고 CUDA는 그 하드웨어를 활용하기 위한 소프트웨어 플랫폼**  \n",
        "* CUDA가 있어야 딥러닝 프레임워크들이 GPU의 성능을 제대로 활용할 수 있다.  \n",
        "\n",
        "\n",
        "# 딥러닝에서 GPU 활용  \n",
        "\n",
        "## 주요 딥러닝 프레임워크의 GPU 지원  \n",
        "\n",
        "### PyTorch  \n",
        "\n",
        "```python  \n",
        "import torch  \n",
        "\n",
        "# GPU 사용 가능 여부 확인  \n",
        "print(torch.cuda.is_available())  \n",
        "\n",
        "# 텐서를 GPU로 이동  \n",
        "tensor = torch.randn(1000, 1000)  \n",
        "tensor_gpu = tensor.cuda()  \n",
        "\n",
        "# 모델을 GPU로 이동  \n",
        "model = MyModel()  \n",
        "model = model.cuda()  \n",
        "```  \n",
        "\n",
        "### TensorFlow  \n",
        "\n",
        "```python  \n",
        "import tensorflow as tf  \n",
        "\n",
        "# GPU 사용 가능 여부 확인  \n",
        "print(tf.config.list_physical_devices('GPU'))  \n",
        "\n",
        "# GPU 메모리 증가 설정  \n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')  \n",
        "if gpus:  \n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)  \n",
        "```  \n",
        "\n",
        "# GPU 환경 구축 가이드  \n",
        "\n",
        "## 하드웨어 요구사항 확인  \n",
        "\n",
        "### GPU 확인 방법  \n",
        "\n",
        "```bash  \n",
        "# Windows  \n",
        "nvidia-smi  \n",
        "\n",
        "# Linux  \n",
        "lspci | grep -i nvidia  \n",
        "```  \n",
        "\n",
        "* 이 명령어가 실행되면 GPU 정보와 CUDA 버전이 표시된다.  \n",
        "* 만약 \"nvidia-smi is not recognized\" 오류가 나면 NVIDIA 드라이버를 설치해야 한다.  \n",
        "\n",
        "```bash  \n",
        "# 나의 예시  \n",
        "+-----------------------------------------------------------------------------------------+  \n",
        "| NVIDIA-SMI 561.19                 Driver Version: 561.19         CUDA Version: 12.6     |  \n",
        "|-----------------------------------------+------------------------+----------------------+  \n",
        "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |  \n",
        "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |  \n",
        "|                                         |                        |               MIG M. |  \n",
        "|=========================================+========================+======================|  \n",
        "|   0  NVIDIA GeForce RTX 2070 ...  WDDM  |   00000000:01:00.0  On |                  N/A |  \n",
        "| N/A   53C    P8             10W /   89W |     727MiB /   8192MiB |      5%      Default |  \n",
        "|                                         |                        |                  N/A |  \n",
        "+-----------------------------------------+------------------------+----------------------+  \n",
        "\n",
        "+-----------------------------------------------------------------------------------------+  \n",
        "| Processes:                                                                              |  \n",
        "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |  \n",
        "|        ID   ID                                                               Usage      |  \n",
        "|=========================================================================================|  \n",
        "|    0   N/A  N/A      2340    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |  \n",
        "|    0   N/A  N/A     14556    C+G   C:\\Windows\\explorer.exe                     N/A      |  \n",
        "|    0   N/A  N/A     15068    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe      N/A      |  \n",
        "|    0   N/A  N/A     19280    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |  \n",
        "|    0   N/A  N/A     20256    C+G   ..._v10z8vjag6ke6\\OMENAudioControl.exe      N/A      |  \n",
        "|    0   N/A  N/A     20340    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |  \n",
        "|    0   N/A  N/A     21916    C+G   ...ta\\Local\\Programs\\cursor\\Cursor.exe      N/A      |  \n",
        "|    0   N/A  N/A     23780    C+G   ...on\\137.0.3296.83\\msedgewebview2.exe      N/A      |  \n",
        "|    0   N/A  N/A     25400    C+G   ...812_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |  \n",
        "|    0   N/A  N/A     26072    C+G   ...\\HncUtils\\Service\\HncUpdateTray.exe      N/A      |  \n",
        "|    0   N/A  N/A     26104    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |  \n",
        "|    0   N/A  N/A     26248    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |  \n",
        "|    0   N/A  N/A     29228    C+G   ...on\\137.0.3296.83\\msedgewebview2.exe      N/A      |  \n",
        "|    0   N/A  N/A     31772    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |  \n",
        "|    0   N/A  N/A     32572    C+G   ...812_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |  \n",
        "|    0   N/A  N/A     35516    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |  \n",
        "|    0   N/A  N/A     35628    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |  \n",
        "|    0   N/A  N/A     38204    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |  \n",
        "|    0   N/A  N/A     40680    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |  \n",
        "+-----------------------------------------------------------------------------------------+  \n",
        "```  \n",
        "\n",
        "* 모델: RTX 2070 (아마도 RTX 2070 Super로 보임)  \n",
        "* VRAM: 8GB (8192MiB)  \n",
        "* 현재 메모리 사용량: 727MB / 8192MB (약 9% 사용 중)  \n",
        "* GPU 사용률: 5% (거의 유휴 상태)  \n",
        "* 전력 소모: 10W / 89W (매우 낮음, 절전 모드)  \n",
        "\n",
        "### 권장 GPU 사양  \n",
        "\n",
        "- **입문용**: GTX 1660 Super (6GB VRAM)  \n",
        "  * 할 수 있는 것들:  \n",
        "    * 간단한 CNN 모델 (MNIST, CIFAR-10)  \n",
        "    * 작은 데이터셋으로 실습  \n",
        "    * 기본적인 PyTorch/TensorFlow 튜토리얼  \n",
        "    * 배치 크기 16-32 정도  \n",
        "  * 제한사항:  \n",
        "    * 큰 모델(ResNet-50, Transformer) 훈련 어려움  \n",
        "    * ImageNet 같은 대용량 데이터셋 처리 힘듦  \n",
        "    * 배치 크기를 작게 해야 함 (메모리 부족)  \n",
        "- **중급용**: RTX 3070 (8GB VRAM)  \n",
        "  * 할 수 있는 것들:  \n",
        "    * 중간 크기 모델 훈련 (ResNet-34, EfficientNet-B0)  \n",
        "    * 전이학습(Transfer Learning) 실험  \n",
        "    * 개인 프로젝트, 캐글 대회 참여  \n",
        "    * 배치 크기 32-64  \n",
        "  * 제한사항:  \n",
        "    * 매우 큰 모델(GPT, BERT Large) 훈련 어려움  \n",
        "    * 긴 시퀀스 처리 제한적  \n",
        "    * 상용 서비스 수준의 모델 훈련은 힘듦  \n",
        "- **고급용**: RTX 4090 (24GB VRAM)  \n",
        "  * 할 수 있는 것들:  \n",
        "    * 대형 모델 훈련 (BERT, GPT-2 Small)  \n",
        "    * 고해상도 이미지 처리  \n",
        "    * 복잡한 연구 실험  \n",
        "    * 배치 크기 128-256  \n",
        "    * 상용 서비스 프로토타입  \n",
        "    * 가능한 실험:  \n",
        "      * 대용량 데이터셋 처리  \n",
        "      * 복잡한 모델 훈련  \n",
        "      * 최신 모델 실험  \n",
        "      * 빠른 추론 최적화  \n",
        "\n",
        "| GPU | 가격대 | 전력 소모 | 성능/가격 비율 |  \n",
        "|-----|--------|-----------|----------------|  \n",
        "| GTX 1660 Super | 30-40만원 | 125W | 높음 |  \n",
        "| RTX 3070 | 60-80만원 | 220W | 중간 |  \n",
        "| RTX 4090 | 200-250만원 | 450W | 낮음 |  \n",
        "\n",
        "## NVIDIA 드라이버 설치  \n",
        "\n",
        "### Windows  \n",
        "1. [NVIDIA 공식 사이트](https://www.nvidia.com/drivers)에서 드라이버 다운로드  \n",
        "2. 설치 파일 실행 후 지시에 따라 설치  \n",
        "3. 재부팅 후 `nvidia-smi` 명령어로 확인  \n",
        "\n",
        "### Linux (Ubuntu)  \n",
        "```bash  \n",
        "# 자동 설치 (권장)  \n",
        "sudo ubuntu-drivers autoinstall  \n",
        "\n",
        "# 수동 설치  \n",
        "sudo apt update  \n",
        "sudo apt install nvidia-driver-525  \n",
        "sudo reboot  \n",
        "\n",
        "# 설치 확인  \n",
        "nvidia-smi  \n",
        "```  \n",
        "\n",
        "# Python 딥러닝 환경 구축  \n",
        "\n",
        "\n",
        "##  Conda 환경 생성  \n",
        "```bash  \n",
        "# 새 환경 생성  \n",
        "conda create -n nblog python=3.11  \n",
        "conda activate nblog  \n",
        "```  \n",
        "\n",
        "## CUDA Toolkit 설치  \n",
        "\n",
        "### 버전 호환성 확인  \n",
        "\n",
        "| PyTorch 버전 | CUDA 버전 | Python 버전 |  \n",
        "|-------------|-----------|-------------|  \n",
        "| 2.1.x | 11.8, 12.1 | 3.8-3.11 |  \n",
        "| 2.0.x | 11.7, 11.8 | 3.8-3.11 |  \n",
        "| 1.13.x | 11.6, 11.7 | 3.7-3.10 |  \n",
        "\n",
        "\n",
        "### CUDA와 Pytorch 설치   \n"
      ],
      "id": "d33fe132"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#  \n",
        "## 1. PyTorch 설치 (CUDA 12.1 권장)  \n",
        "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  \n",
        "#  \n",
        "## 2. transformers 및 관련 패키지  \n",
        "#pip install transformers  \n",
        "#pip install accelerate  \n",
        "#pip install datasets  # 데이터셋 사용시  \n",
        "\n",
        "import torch  \n",
        "\n",
        "print(f\"PyTorch 버전: {torch.__version__}\")  \n",
        "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")  \n",
        "print(f\"CUDA 버전 (PyTorch): {torch.version.cuda}\")  \n",
        "print(f\"GPU 이름: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")  "
      ],
      "id": "c0a1b166",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorFlow 설치  "
      ],
      "id": "ad1fdada"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install tensorflow[and-cuda]  \n",
        "\n",
        "# pytorch 설치 후 텐서플로우 설치 시 제거  \n",
        "#pip uninstall tensorflow tensorflow-gpu tensorflow-cpu  \n",
        "#pip uninstall tensorboard tensorboard-plugin-wit  \n",
        "## 캐시 정리  \n",
        "#pip cache purge  "
      ],
      "id": "612023ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 설치 확인  \n"
      ],
      "id": "fd9668fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch  \n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "    device = torch.device(\"cuda\")  \n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")  \n",
        "    \n",
        "    # 간단한 텐서 연산  \n",
        "    a = torch.randn(1000, 1000).to(device)  \n",
        "    b = torch.randn(1000, 1000).to(device)  \n",
        "    \n",
        "    # GPU에서 행렬 곱셈  \n",
        "    c = torch.matmul(a, b)  \n",
        "    \n",
        "    print(\"✅ GPU 텐서 연산 성공!\")  \n",
        "    print(f\"결과 shape: {c.shape}\")  \n",
        "    print(f\"GPU 메모리 사용량: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")  \n",
        "    \n",
        "    # 메모리 정리  \n",
        "    torch.cuda.empty_cache()  "
      ],
      "id": "76f0c58f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 더 가벼운 모델로 테스트  \n",
        "model_name = \"prajjwal1/bert-tiny\"  # 매우 작은 모델  \n",
        "\n",
        "try:  \n",
        "    from transformers import AutoTokenizer, AutoModel  \n",
        "    \n",
        "    print(\"모델 다운로드 중...\")  \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)  \n",
        "    model = AutoModel.from_pretrained(model_name).to(device)  \n",
        "    \n",
        "    print(\"GPU에서 추론 실행 중...\")  \n",
        "    text = \"GPU 테스트입니다\"  \n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")  \n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}  \n",
        "    \n",
        "    with torch.no_grad():  \n",
        "        outputs = model(**inputs)  \n",
        "    \n",
        "    print(\"✅ 작은 모델로 GPU 테스트 성공!\")  \n",
        "    print(f\"출력 shape: {outputs.last_hidden_state.shape}\")  \n",
        "    print(f\"GPU 메모리 사용량: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")  \n",
        "    \n",
        "except Exception as e:  \n",
        "    print(f\"여전히 오류: {e}\")  "
      ],
      "id": "317a23fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch  \n",
        "from transformers import AutoModel, AutoTokenizer  \n",
        "\n",
        "print(\"=== PyTorch GPU 테스트 ===\")  \n",
        "print(f\"PyTorch 버전: {torch.__version__}\")  \n",
        "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")  \n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "    print(f\"CUDA 버전: {torch.version.cuda}\")  \n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")  \n",
        "    \n",
        "    # transformers 테스트  \n",
        "    device = torch.device(\"cuda\")  \n",
        "    model_name = \"distilbert-base-uncased\"  \n",
        "    \n",
        "    try:  \n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)  \n",
        "        model = AutoModel.from_pretrained(model_name).to(device)  \n",
        "        \n",
        "        text = \"GPU test with transformers\"  \n",
        "        inputs = tokenizer(text, return_tensors=\"pt\")  \n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}  \n",
        "        \n",
        "        with torch.no_grad():  \n",
        "            outputs = model(**inputs)  \n",
        "        \n",
        "        print(\"✅ transformers GPU 테스트 성공!\")  \n",
        "        print(f\"출력 shape: {outputs.last_hidden_state.shape}\")  \n",
        "        print(f\"GPU 메모리 사용량: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")  \n",
        "        \n",
        "    except Exception as e:  \n",
        "        print(f\"❌ 오류: {e}\")  \n",
        "else:  \n",
        "    print(\"❌ CUDA를 사용할 수 없습니다.\")  "
      ],
      "id": "a2251197",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 성능 최적화 팁  \n"
      ],
      "id": "a2037e58"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 배치 크기 조정  \n",
        "\n",
        "# VRAM에 따른 권장 배치 크기  \n",
        "# 6GB VRAM: batch_size = 16-32  \n",
        "# 8GB VRAM: batch_size = 32-64  \n",
        "# 12GB VRAM: batch_size = 64-128  \n",
        "\n",
        "batch_size = 32  # VRAM 용량에 맞게 조정  \n",
        "\n",
        "# 혼합 정밀도 훈련  \n",
        "# PyTorch AMP 사용  \n",
        "from torch.cuda.amp import autocast, GradScaler  \n",
        "\n",
        "scaler = GradScaler()  \n",
        "\n",
        "for data, target in dataloader:  \n",
        "    optimizer.zero_grad()  \n",
        "    \n",
        "    with autocast():  \n",
        "        output = model(data)  \n",
        "        loss = criterion(output, target)  \n",
        "    \n",
        "    scaler.scale(loss).backward()  \n",
        "    scaler.step(optimizer)  \n",
        "    scaler.update()  "
      ],
      "id": "8e372637",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU 메모리 관리  "
      ],
      "id": "6802a704"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PyTorch 메모리 정리  \n",
        "torch.cuda.empty_cache()  \n",
        "\n",
        "# 메모리 사용량 확인  \n",
        "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")  \n",
        "print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")  "
      ],
      "id": "7d784d19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 실제 성능 벤치마크  \n",
        "\n",
        "## 간단한 벤치마크 코드  "
      ],
      "id": "891e2705"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch  \n",
        "import time  \n",
        "\n",
        "def benchmark_gpu():  \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "    \n",
        "    # 큰 행렬 생성  \n",
        "    a = torch.randn(5000, 5000).to(device)  \n",
        "    b = torch.randn(5000, 5000).to(device)  \n",
        "    \n",
        "    # 성능 측정  \n",
        "    start_time = time.time()  \n",
        "    for _ in range(100):  \n",
        "        c = torch.mm(a, b)  \n",
        "    end_time = time.time()  \n",
        "    \n",
        "    print(f\"Device: {device}\")  \n",
        "    print(f\"Time: {end_time - start_time:.2f} seconds\")  \n",
        "    print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")  \n",
        "\n",
        "benchmark_gpu()  "
      ],
      "id": "8c789591",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 실제 딥러닝 모델 성능 비교  "
      ],
      "id": "e594a97d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch  \n",
        "import torch.nn as nn  \n",
        "import time  \n",
        "\n",
        "class SimpleModel(nn.Module):  \n",
        "    def __init__(self):  \n",
        "        super().__init__()  \n",
        "        self.layers = nn.Sequential(  \n",
        "            nn.Linear(1000, 2000),  \n",
        "            nn.ReLU(),  \n",
        "            nn.Linear(2000, 1000),  \n",
        "            nn.ReLU(),  \n",
        "            nn.Linear(1000, 10)  \n",
        "        )  \n",
        "    \n",
        "    def forward(self, x):  \n",
        "        return self.layers(x)  \n",
        "\n",
        "def train_benchmark(device):  \n",
        "    model = SimpleModel().to(device)  \n",
        "    optimizer = torch.optim.Adam(model.parameters())  \n",
        "    criterion = nn.CrossEntropyLoss()  \n",
        "    \n",
        "    # 더미 데이터  \n",
        "    data = torch.randn(1000, 1000).to(device)  \n",
        "    targets = torch.randint(0, 10, (1000,)).to(device)  \n",
        "    \n",
        "    start_time = time.time()  \n",
        "    \n",
        "    for epoch in range(100):  \n",
        "        optimizer.zero_grad()  \n",
        "        outputs = model(data)  \n",
        "        loss = criterion(outputs, targets)  \n",
        "        loss.backward()  \n",
        "        optimizer.step()  \n",
        "    \n",
        "    end_time = time.time()  \n",
        "    print(f\"{device} training time: {end_time - start_time:.2f} seconds\")  \n",
        "\n",
        "# CPU vs GPU 비교  \n",
        "train_benchmark('cpu')  \n",
        "train_benchmark('cuda')  "
      ],
      "id": "0f41da53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 결론  \n",
        "\n",
        "GPU와 CUDA를 활용한 딥러닝 환경 구축은 다음과 같은 이점을 제공합니다:  \n",
        "\n",
        "1. **훈련 시간 단축**: 10-100배 빠른 모델 훈련  \n",
        "2. **더 큰 모델 실험**: 메모리 효율성으로 복잡한 모델 실험 가능  \n",
        "3. **실시간 추론**: 빠른 GPU 연산으로 실시간 서비스 구현  \n",
        "\n",
        "딥러닝을 시작하는 분들에게는 적절한 GPU 환경 구축이 필수적입니다. 하드웨어 선택부터 소프트웨어 설치까지 체계적으로 접근하면 효율적인 딥러닝 개발 환경을 구축할 수 있습니다.  "
      ],
      "id": "b014a885"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "airflow",
      "language": "python",
      "display_name": "Python (airflow)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}