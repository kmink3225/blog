## 요약

이 글은 Azure AI Search에서의 Relevance(관련도)와 Ranking(랭킹) 전략을 실무 관점에서 정리한다. 특히 Agentic Retrieval과 Retrieval-Augmented Generation(RAG)의 설계·비교·운영 포인트에 집중한다. 핵심은 다음과 같다.

- Agentic Retrieval: LLM을 쿼리 플래너로 사용해 복합 질의를 하위 쿼리로 분해하고 병렬 실행으로 결과를 통합하는 파이프라인이다.
- Classic RAG: 단일 쿼리를 실행해 검색 결과를 LLM으로 전달해 응답을 생성하는 전통적인 패턴이다.
- 실무 고려사항: 콘텐츠 전처리(청크화·벡터화), 인덱스 설계, 하이브리드 쿼리 전략, 시맨틱 랭킹, 비용 추정, 보안·거버넌스.

이 글은 개념, 아키텍처, 색인·쿼리 설계, 비용·성능 튜닝, 보안, 마이그레이션 전략, 예제 코드까지 포함한다.

## 목적과 독자

이 문서는 다음 독자를 위해 작성한다.

- RAG 또는 Agentic Retrieval 도입을 고려하는 솔루션 엔지니어
- Azure AI Search를 사용해 챗봇·에이전트 백엔드를 설계하는 개발자
- 검색·랭킹 시스템의 비용·성능·보안 트레이드오프를 설계해야 하는 아키텍트

읽는 후 얻을 수 있는 가치:

- Agentic Retrieval의 내부 동작과 구성요소를 이해한다.
- Classic RAG와의 차이를 실무 판단 기준으로 활용한다.
- 색인과 쿼리 파라미터를 어떻게 설계하고 튜닝할지 실무적 체크리스트를 얻는다。

---

## 1. 핵심 개념 정리

### 1.1 Retrieval-Augmented Generation(RAG)이란

RAG는 LLM의 생성 능력을 고유 데이터로 보강하기 위한 패턴이다. LLM 모델은 사전학습 지식만으로는 최신·사내 문서 기반의 정확한 답변을 만들기 어려우므로, 검색 시스템이 관련 문서를 제공하면 LLM이 이를 근거로 응답을 생성한다. 이때 검색의 품질이 RAG 응답 품질을 결정한다.

주요 구성요소:

- 검색(검색엔진, 인덱스)
- 임베딩·벡터 검색
- 시맨틱 랭킹(semantic reranker)
- LLM(응답 생성 또는 쿼리 플래닝)

### 1.2 Agentic Retrieval이란

Agentic Retrieval은 RAG를 한 단계 발전시킨 형태로, LLM을 단순 생성기로 쓰지 않고 쿼리 플래너로 활용한다. 사용자의 복합 질의를 LLM이 분석해 여러 하위 쿼리로 분해하고, 이 하위 쿼리들을 병렬로 실행해 통합 응답을 만든다. 결과는 근거 문서, 실행 로그(activity plan), 그리고 병합된 증거(grounding data)를 포함한다。

핵심 효과:

- 복합 질문(여러 조건, 의도 혼합)을 분해하고 각 조건에 최적화된 검색을 수행한다。
- 대화 맥락을 쿼리 플래닝에 반영해 질의 의도를 더 잘 파악한다。

### 1.3 언제 Agentic Retrieval을 선택하나

- 챗봇·에이전트가 복잡한 질문을 받아 처리해야 할 때
- 정밀한 증거·참조(traceability)가 필요한 경우
- 다수의 지식 소스(인덱스 + 원격 소스)를 통합해 질의해야 할 때

단, 프리뷰(미리보기) 기능 제약과 추가 비용을 감안해 도입 결정을 한다。

---

## 2. 아키텍처와 워크플로(상세)

아래는 Agentic Retrieval의 전형적 워크플로다。

1) 클라이언트(챗 UI, 에이전트)가 `retrieve` 호출을 Knowledge Base에 보낸다. 호출에는 현재 질문과 대화 이력이 포함된다。
2) Knowledge Base는 LLM(쿼리 플래닝 모델)에 질의와 대화 이력을 전달한다. LLM은 질의를 분석해 하위 쿼리(예: 키워드 쿼리, 개념 쿼리, 필터 포함 쿼리)로 분해한다。
3) 생성된 하위 쿼리들을 병렬로 실행한다。 각 쿼리에 대해 텍스트 검색, 벡터 검색, 하이브리드 검색 등을 병행할 수 있다。
4) 각 쿼리 결과에 대해 시맨틱 랭커가 재정렬한다。 재랭킹에는 문맥 점수, 메타데이터 가중치, 사용자 지정 스코어가 반영될 수 있다。
5) 통합 모듈이 결과를 병합하고, 출처(reference) 및 쿼리 실행 로그(activity plan)를 포함한 구조화된 응답을 만든다。
6) 애플리케이션은 구조화된 grounding data를 사용해 최종 LLM(또는 같은 LLM)으로 응답을 생성한다。

아래는 각 구성요소의 책임이다。

- Knowledge Base: 파이프라인 오케스트레이션, LLM 연동, 요청 파라미터 관리
- LLM(쿼리 플래닝): 대화 이해, 쿼리 분해 및 재작성, 맞춤형 확장(동작 지침)
- Knowledge Source: 인덱스 설정, 원격 쿼리 설정(SharePoint 등)
- Search Service: 텍스트+벡터 검색 엔진, 시맨틱 랭킹

---

## 3. 구성요소별 실무 설계 가이드

### 3.1 색인 설계와 콘텐츠 준비

1) 청크화(Chunking)

- 문서를 의미 단위로 청크화한다。 일반적으로 문단 수준(1~3문단) 또는 의미 단위가 좋다。
- 청크 크기는 토큰 수·문맥 길이에 맞춰 조정한다(예: 200~500 토큰 권장)。 LLM 입력 토큰 한계를 고려해 균형을 잡는다。

2) 벡터화(Embeddings)

- 각 청크에 임베딩을 생성해 벡터 필드로 저장한다。
- 임베딩 모델은 일관되게 선택한다(문서와 쿼리 모두 동일 모델로 임베딩)。 Azure OpenAI 임베딩 또는 다른 사설 모델을 사용한다。

3) 메타데이터 포함

- 문서 id, 원본 경로, 권한 메타데이터, 문서 유형, 날짜 등 검색·필터링에 사용할 필드를 포함한다。

4) OCR·이미지 처리

- 이미지·PDF는 텍스트 추출(이미지 음성화, OCR)과 이미지 설명(vision→text) 파이프라인을 거쳐 청크화한다。

5) 인덱스 필드 전략

- 텍스트 필드(검색가능), 벡터 필드(임베딩), 필터용 키워드 필드, 정렬·스코어용 숫자 필드를 균형 있게 설계한다。

### 3.2 하이브리드 쿼리 전략

하이브리드 쿼리는 키워드 기반 검색과 벡터 기반 유사도 검색을 병행한다。 이 전략은 용어 불일치(term mismatch)를 보완하면서 정확도를 높인다。

- 입력을 텍스트와 임베딩의 두 형태로 동시에 검색하도록 구성한다。
- 결과 병합은 RRF나 CombSUM 같은 퓨전 기법을 활용한다(자세한 내용은 퓨전 섹션 참조)。

튜닝 포인트：

- 벡터 가중치: 벡터 점수와 텍스트 점수의 상대 가중치를 실험해 최적값을 찾는다。
- 임계값(threshold): 유사도 점수가 낮은 결과를 필터링해 잡음 제거。

### 3.3 시맨틱 랭킹(semantic reranker)

시맨틱 랭커는 초기 검색 결과를 L2·cross-encoder 방식으로 재랭킹해 응답의 품질을 높인다。 Azure는 내장 시맨틱 랭킹을 제공한다。

- 재랭킹은 비용이 들므로 Top-K(예: 50)만 재랭킹하도록 제한한다。
- 재랭킹 모델은 문맥을 반영해 정확도를 올리지만、 지연과 비용 트레이드오프를 고려해야 한다。

---

## 4. 쿼리 플래닝과 Reasoning Effort

Agentic Retrieval의 핵심 차별점은 LLM을 쿼리 플래너로 사용하는 것이다。 이때 `reasoning effort`라는 개념으로 LLM의 처리 강도를 조정한다。

---
title: "Relevance and Ranking"
subtitle: "Relevance scoring, hybrid ranking and semantic reranking in Azure Search"
description: |
  Relevance scoring과 ranking 알고리즘(BM25, vector, RRF, semantic reranker)과 실무적 하이브리드 패턴을 정리한다. 파라미터 튜닝, fusion 전략, 성능·비용 고려사항을 다룬다.
categories:
  - AI
  - Cloud
  - Azure
author: Kwangmin Kim
date: 12/26/2025
format: 
  html:
    code-fold: true
    toc: true
    number-sections: true
draft: false
---


