---
title: "Prompt Structure"
subtitle: 프롬프트의 개념과 구성 요소
description: |
  프롬프트의 개념과 구성 요소를 파악한다.
categories:
  - AI
  - RAG
  - LangChain
  - Prompt Engineering
author: Kwangmin Kim
date: 01/16/2025
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
execute:
    eval: false
---

## 프롬프트의 이론적 기초 (Theoretical Foundation)

### 프롬프트 엔지니어링의 정의와 위상

* **프롬프트 엔지니어링**: 자연어 인터페이스를 통해 대규모 언어모델(LLM)의 행동을 제어하고 최적화하는 체계적 방법론
* **핵심 구성:**
  * 프롬프트 엔지니어링 = 언어학(Linguistics) + 인지과학(Cognitive Science) + 컴퓨터과학(Computer Science) + 통계학(Statistics)

### LLM의 수학적 작동 원리

- AI (Artificial Intelligence)의 한 종류
- 초기 프롬프트나 문맥을 기반으로 다음 단어 분포를 예측하여 데이터를 생성
- **Autoregression** 사용: 과거 데이터 값을 입력으로 받아 다음 시점의 값을 예측

#### Autoregressive Language Modeling Mechanism

- GPT-4는 본질적으로 **Transformer** 기반
- 주어진 텍스트에서 다음 토큰(단어 또는 하위 단어)을 예측하도록 훈련
- **Autoregressive** 모델 구조로 일관되고 문맥적으로 관련된 텍스트 생성
  - LLM은 다음 토큰의 조건부 확률 분포를 학습:

$$P(x_t | x_{<t}) = \text{softmax}(W \cdot h_t + b)$$

여기서:
- $x_t$: 시점 $t$의 토큰
- $x_{<t}$: 이전 모든 토큰들 ($x_1, x_2, ..., x_{t-1}$)
- $h_t$: Transformer의 hidden state
- $W, b$: 학습 가능한 파라미터

**전체 시퀀스 생성 확률:**

$$P(x_1, x_2, ..., x_n) = \prod_{t=1}^{n} P(x_t | x_{<t})$$

#### Transformer 아키텍처의 핵심

**Self-Attention 메커니즘:**

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

여기서:
- $Q$ (Query): 현재 토큰의 질의 벡터
- $K$ (Key): 모든 토큰의 키 벡터
- $V$ (Value): 모든 토큰의 값 벡터
- $d_k$: 키 벡터의 차원

**의미:**
- 각 토큰이 문맥 내 다른 토큰들과의 관계를 학습
- $\sqrt{d_k}$로 스케일링하여 gradient vanishing 방지

### 주요 LLM 모델 비교 (2024-2025)

#### 모델 발전 연대기 (파라미터 기준)

| 연도 | 모델 | 파라미터 | 주요 혁신 |
|------|------|----------|-----------|
| 2018 | ELMo | 94M | 문맥 기반 임베딩 |
| 2019 | BERT-Large | 340M | 양방향 인코더 |
| 2020 | GPT-3 | 175B | Few-shot learning |
| 2021 | Megatron-Turing NLG | 530B | 대규모 분산 학습 |
| 2023 | GPT-4 | ~1.76T (추정) | Multimodal 통합 |
| 2024 | Claude 3 Opus | - | Constitutional AI |
| 2025 | GPT-4o | - | Real-time multimodal |

#### 현존 3대 주요 LLM모델 특성 비교 (2024~2025)

**OpenAI GPT-4o:**
- **강점**: 실시간 멀티모달, API 비용 효율성 (50% 절감), 빠른 응답 속도, 비용 50% 절감, 멀티모달, 정확한 결과, 실시간 비디오/오디오 지원
- **아키텍처**: Transformer + RLHF + Constitutional AI
- **최적 용도**: 범용 텍스트 생성, 복잡한 추론, 코딩
- **Chat Interface의 핵심 기능:**
  - 이전 대화 내용 기억 (Remember what user said earlier in the conversation)
  - 후속 수정 허용 (Allow user to provide follow-up corrections)
  - 부적절한 요청 거부 학습 (Trained to decline inappropriate requests)
**Anthropic Claude 3:**
- **강점**: Constitutional AI를 통한 안전성, 200K 토큰 컨텍스트, 다국어 지원, 비전 및 이미지 처리, 헌법적 AI
- **핵심 기술**: 
  - Feature-Tuning: 모델 내부 활성화 패턴 직접 제어
  - Interpretability Research: 뉴런 활성화의 개념적 매핑
- **최적 용도**: 장문 분석, 윤리적 추론, 구조화된 대화
- "Mapping the Mind of A Large Language Model":
  - 모델의 뉴런 활성화 패턴을 인간이 해석 가능한 수백만 개의 개념으로 식별
  - 언어 모델 내부 상태에 대한 개념 지도 생성 및 시각화
**Google Gemini:**
- **강점**: 멀티모달 통합 (텍스트/이미지/오디오/비디오), 대부분 벤치마크에서 뛰어난 성능, 텍스트/이미지/오디오/비디오 지원
- **벤치마크**: MMLU, BBH 등 대부분 벤치마크 우위
- **최적 용도**: 멀티모달 분석, 과학적 추론

#### Model Tuning 방법

- fine-tuning: 사전 학습된 모델을 특정 작업이나 도메인에 맞게 추가 학습시키는 방법. 가중치를 직접 업데이트하여 모델을 특화시킴.
- **prompt-engineering**: 모델의 가중치 변경 없이 입력 프롬프트 설계를 통해 원하는 출력을 유도하는 방법. 가장 비용 효율적이고 빠른 접근법.
- feature-tuning: 모델 내부의 특정 뉴런이나 활성화 패턴을 직접 제어하여 모델 행동을 조정하는 방법. Claude의 Constitutional AI에서 사용되는 고급 기법.

### 프롬프트와 프롬프트 엔지니어링

- **프롬프트** = 새로운 대화 도구
- **프롬프트 엔지니어링** = 자연어로 컴퓨터와 상호작용하는 방법
  - 프롬프트 엔지니어링은 모델의 출력을 제어하고 원하는 방향으로 유도
- 프롬프트 엔지니어링의 중요성: 언어 모델 성능 향상을 위한 연구에 필요
  - 출력제어: 모델의 응답 형식, 톤, 스타일을 정밀하게 조정하여 일관된 결과 생성
  - 정확성: 명확한 지시와 맥락 제공으로 사실 기반의 정확한 정보 도출
  - 편향완화: 중립적이고 균형 잡힌 프롬프트 설계로 모델의 편향된 응답 최소화
  - 적응성: 다양한 도메인과 작업에 맞게 프롬프트를 조정하여 범용성 확보
  - 문맥이해: 충분한 배경 정보 제공으로 모델의 문맥 파악 능력 향상
  - 경제성: Fine-tuning 없이 프롬프트만으로 성능 개선, 시간과 비용 절감
  - 윤리적 사용: 안전장치와 가이드라인을 프롬프트에 내장하여 책임 있는 AI 활용

## 프롬프트 요소 (Element of Prompt)

### 프롬프트 vs 프롬프트 엔지니어링 비교

| 비교사항 | 프롬프트 | 프롬프트 엔지니어링 |
|---------|---------|-------------------|
| **정의** | AI 모델에게 응답을 생성하도록 입력하는 텍스트, 문장 | AI 모델로부터 원하는 출력을 얻기 위해 프롬프트를 설계하고 개선하는 과정 |
| **목적** | AI 모델로부터 응답이나 출력을 유도하기 위함 | 프롬프트를 설계하여 AI 모델의 응답을 최적화하고 제어하기 위함 |
| **예시** | 용에 대한 이야기 들려줘 | 용감한 용이 마을을 구하는 짧고 상상력 넘치는 이야기를 생생한 묘사와 대화를 사용하여 들려줘 |
| **필요 사항** | AI 모델의 입력 요구 사항에 대한 기본적인 이해 | AI 모델의 내부 작동 방식, 창의성, 언어의 미묘한 차이에 대한 이해 |

### 프롬프트 엔지니어링의 구성 요소

* 프롬프트 엔지니어링 = 프롬프트(인문학) + 엔지니어링(테크놀로지의 융합)
* 프롬프트의 **4가지 핵심 요소:**
  1. 지시 (Instructions): 모델이 수행할 특정 작업 또는 지시
  2. 문맥 (Context): 모델이 수행할 특정 작업에 대한 참고 지식이나 배경
  3. 입력 데이터 (Input Data): 응답에 참고할 입력 값
  4. 출력 지시문 (Output Indicator): 응답 형식이나 결과 포맷
  * 예시:

  ```
  지시(Prompt): Sentiment Analysis를 해야 해.
  문맥(Context): 아래 텍스트를 긍정, 중립, 부정 중에서 구분해줘.
  입력 데이터(Input Text): 그 음식점 마라탕 맛이 그저 그랬어.

  {sentiment}: {부정}
  ```

* 프롬프트 타입 (Type)
  * Type A (기본): 지시문 + 출력문 
    * 예시: 구름 색깔은?
  * Type B (Type A + 맥락): 지시문 + **맥락** + 출력문 
    * 예시: **해가 질때** 구름 색깔은?
  * Type C (Type B + 예시): 지시문 + 맥락 + **예시** + 출력문
    * 예시: 
    
    ```
    해가 질 때 구름 색깔과 종류는? 예시 구조처럼 대답해. 
    #예시
    1. 권운: 하얀 섬유 모양의 구름으로 얼음 결정으로 되어 있다. 
    2. 고적운: 고적운은 중층운으로 양떼구름, 높쌘구름이라고도 한다
    ```

  * Type D (Type C + 입력값): 지시문 + **맥락 + 예시 + 입력값** + 출력문 
    * 예시:

    ``` 
    구름의 종류를 설명해줘.
    아래 [[입력값]]을 활용해서, 내용을 보충해줘.
    한 문단으로 완성해.
    [[입력값]]
    여러가지 다양한 모양의 구름을 최초로 나눈 사람은 영국의 기상학자 루크하우드 입니다.
    그는 1803년, 구름을 네가지 이름으로 구분했습니다.
    층운: 넓고, 얇게 펼쳐진 구름
    권운: 가느다랗고 새털처럼 생긴 구름
    적운: 쌓아 올려진 모양의 구름
    난운: 회색빛의 먹구름
    지금은 이보다 더 다양한 10개의 기본형으로 나눕니다.
    ```

  ```
  # Type A 예시 출력물
  구름 자체는 색깔을 가지고 있지 않습니다. 구름은 작은 물방울이나
  얼음 결정으로 이루어져 있으며, 보통 하얗게 보이는데, 이는 태양
  빛이 구름의 물방울에 의해 분산되어 그렇게 보이기 때문입니다.
  그러나, 때때로 구름은 환경적인 요인에 따라 다른 색을 띠기도
  합니다.

  # Type B 예시 출력물
  해가 질 때 구름의 색깔은 주로 빨강, 주황, 분홍색과 같은 따뜻한
  톤으로 변합니다. 이 현상은 􃑄레일리 산란􃑄이라고 불리는 광학적
  원리 때문입니다. 낮 동안, 태양 빛은 대기를 통과하면서 파란색
  빛을 산란시키며, 이것이 하늘이 파랗게 보이는 이유입니다. 그러나
  해가 지면서 태양 빛이 지평선에서 더 긴 경로를 거치게 되면서,
  파란색과 녹색 빛은 더 많이 산란되고, 빨강과 주황색 빛이
  우세해집니다. 이때 구름이 이 빛을 반사하면서 빨강, 주황, 분홍색
  등으로 아름답게 물드는 것을 볼 수 있습니다.

  # Type C 예시 출력물
  해가 지는 시간에 나타나는 구름의 색깔과 종류를 설명해 드리겠습니다.
  1) 고층운: 해가 지는 시간에 하늘에서 빛나는 얇고, 희미한 구름으로, 얼음 결정으로
  이루어져 있다. 일반적으로 높은 고도􃏙약 5􃎿13km􃏚에 위치하며, 구름이 얇아서 태양의
  빛을 통과시켜 아름다운 색감을 만들어낸다.
  2) 적란운: 해가 질 무렵에 종종 보이는 두꺼운 구름으로, 하늘 전체를 덮을 수 있다. 대기
  중 낮은 고도􃏙약 2km 이하􃏚에서 형성되며, 태양의 낮은 각도로 인해 붉은색 또는
  주황색으로 물드는 경우가 많다.

  이러한 구름은 일몰 시 특유의 아름다운 하늘색을 만들어내며, 대기 상태와 날씨 변화에
  따라 다양한 형태와 색상을 보여줍니다.
    
  # Type D 예시 출력물
  구름의 종류를 최초로 나눈 사람은 영국의 기상학자 루크 하워드로, 그는 1803년에 구름을
  네 가지 이름으로 구분했습니다: 층운, 권운, 적운, 난운입니다. 층운은 넓고 얇게 펼쳐진
  구름으로, 하늘을 덮는 특징이 있습니다. 권운은 가느다랗고 새털처럼 생긴 구름으로,
  고도가 높은 곳에서 형성됩니다. 적운은 쌓아 올려진 모양의 구름으로, 대개 맑은 날씨를
  나타냅니다. 난운은 회색빛의 먹구름으로, 비나 눈을 예고하는 경우가 많습니다. 현대에는
  이러한 기본 분류에서 더 나아가 10개의 기본형으로 구름을 나누어 분류합니다. 이러한
  름의 분류는 기상학적 연구와 일기 예보에 중요한 기초 자료가 됩니다.
  ```

### 예시 출력물 분석

#### Type A (기본): `지시: 구름 색깔은?` (가장 기본적인 질문)

* 응답의 일반성: 질문에 대한 가장 **일반적이고 보편적인 사실**을 설명. "구름 자체는 색깔을 가지고 있지 않다"는 기본 전제를 알려줌.
* 기본 지식 활용: 모델이 학습한 일반적인 지식에서 '구름의 색깔이 하얗게 보이는 이유'와 '때때로 다른 색을 띠기도 한다'는 기본적인 설명을 제공.
* 정보량: 적절하지만 깊이가 얕고, 추가적인 배경 지식 없이 질문 자체에만 초점을 맞춘 답변
* 형식: 특별한 제약이 없으므로 자유로운 서술형 문단 형식으로 답변.

#### Type B (Type A + 맥락): `지시: 구름 색깔은?`, `맥락: 해가 질 때` (특정 상황을 제시)

* 응답의 구체성 향상: `해가 질 때`라는 **특정 문맥에 초점을 맞춰 답변이 훨씬 구체적**으로 변함. 단순한 색깔 언급을 넘어 빨강, 주황, 분홍색 등 따뜻한 톤으로 변하는 현상을 설명.
* 심화된 설명: 일반적인 상식 수준을 넘어 `레일리 산란`이라는 **과학적인 원리**까지 함께 제시하며 배경 설명을 풍부하게 제공.
* 정보량: Type A보다 정보량이 많고, 주어진 맥락과 관련된 전문적인 지식까지 포함되어 깊이가 더해져.
* 형식: 여전히 자유로운 서술형 문단이지만, 내용 구성이 맥락에 맞춰 보다 논리적이고 풍부.

#### Type C (Type B + 예시): `지시: 해가 질 때 구름 색깔과 종류는?`, `맥락: 해가 질 때`, `예시: 1. 권운 2. 고적운` (특정 상황과 답변 형식 제시)

* 응답의 형식 제어: **프롬프트 내에 포함된 `예시 구조처럼`이라는 지시와 함께 제시된 `예시` 포맷**을 준수. numbered list, 구름 이름, 설명의 구조를 완벽하게 재현.
* 정보의 구체성 유지: Type B에서와 같이 `해가 지는 시간`이라는 문맥을 유지하며, 해당 시간대에 관찰될 수 있는 `고층운`과 `적란운` 같은 **구체적인 구름 종류와 그 특징**을 설명
* 정보 활용의 유연성: 프롬프트에 제공된 예시 구름 종류(권운, 고적운)를 직접적으로 사용하지 않고도, '해가 질 때'라는 맥락에 더 적합한 다른 구름 종류를 모델이 스스로 선택하여 예시와 같은 형식으로 설명. **형식을 이해하고 적용**하는 능력 존재
* 정보량: Type B와 유사하거나 약간 더 많은 정보량을 제공하지만, 깔끔하게 정리된 형식 덕분에 가독성이 높음

### Type D (Type A + 입력값): `지시: 구름의 종류를 설명.`, `맥락: [[입력값]]을 활용해서 내용을 보충.`, `예시: 권운, 층운 등`, `입력 데이터: 루크 하워드의 구름 분류와 설명`

* 외부 데이터의 완벽한 활용: 제공된 `[[입력값]]`에 있는 **모든 정보를 그대로 반영하고 활용**하여 답변을 구성. "루크 하워드", "1803년", "네 가지 이름(층운, 권운, 적운, 난운)과 각각의 설명", "지금은 10개의 기본형으로 나눈다"는 내용이 모두 포함
* 명확한 출력 형식: `한 문단으로 완성해`라는 지시에 따라 **하나의 긴 문단**으로 깔끔하게 정리
* 자기 생성 능력과 외부 지식 융합: 모델이 `[[입력값]]`의 정보를 바탕으로 답변을 구성했지만, 마지막 문장("이러한 구름의 분류는 기상학적 연구와 일기 예보에 중요한 기초 자료가 됩니다.")처럼 **모델이 스스로 가진 배경 지식을 추가하여 답변을 더욱 풍부**. 문맥에 맞춰 의미를 부여하고 통합하는 능력.
* 응답의 정밀도와 신뢰성: 외부 데이터를 명확히 제공함으로써, 모델이 **'어떤 내용을 기반으로 답변했는지'** 명확하게 알 수 있으며, 따라서 정보의 정확도나 신뢰도 향상


1.  **지시 (Instructions):** 모델이 **'무엇을 할지'**를 결정.
2.  **문맥 (Context):** 모델이 **'어떤 상황에서 답변할지'**를 설정하여 답변의 구체성과 깊이를 더함.
3.  **입력 데이터 (Input Data):** 모델이 **'어떤 정보를 기반으로 답변할지'**를 제시하여 답변의 사실적 정확도와 풍부함을 보장.
4.  **출력 지시문 (Output Indicator):** 모델이 **'어떤 형식으로 답변할지'**를 지시하여 응답의 가독성과 활용성을 높임.

이러한 요소들을 잘 조합하여 프롬프트를 작성할수록, 우리는 모델로부터 원하는 목적에 훨씬 더 부합하고 고품질의 응답을 얻을 수 있다.