---
title: "RAG과 LangChain 소개"
subtitle: "Retrieval-Augmented Generation과 LangChain 프레임워크 이해"
description: |
  LangChain 프레임워크와 RAG(Retrieval-Augmented Generation) 기술의 개념, 필요성, 구현 방법에 대해 알아본다.
categories:
  - AI
  - RAG
  - LangChain
author: Kwangmin Kim
date: 01/01/2025
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
execute:
  echo: true
  eval: false
---

# RAG 소개

## RAG이란?

* Retrieve = "검색하다", "가져오다", "찾아오다", **검색하여 찾아오다** 라고 이해하면 된다. IT 분야에선 대충 아래와 같은 의미를 갖는다.
  * 데이터 검색: 데이터베이스에서 정보를 찾아오다
  * 파일 복구: 삭제된 파일을 되살리다
  * 정보 추출: 시스템에서 필요한 정보를 가져오다
* RAG(Retrieval-Augmented Generation)는 **검색 증강 생성** 기술로, 대규모 언어 모델(LLM)의 생성 능력과 외부 지식 검색을 결합한 AI 기술이다.
  * 검색(Retrieval): 질문과 관련된 정보를 외부 데이터베이스나 문서에서 검색
  * 증강(Augmented): 검색된 정보를 LLM의 입력에 추가하여 컨텍스트 (또는 문맥) 강화
  * 생성(Generation): 증강된 정보를 바탕으로 더 정확하고 근거 있는 답변 생성

## 왜 RAG가 필요한가?

### Chat GPT 도래 및 성공

- **2022년 11월 출시**: OpenAI ChatGPT 공개 후 폭발적 인기
- **사용자 급증**: 출시 2개월 만에 1억 사용자 돌파
- **산업 변화**: AI 기반 대화형 인터페이스의 새로운 표준 제시
- **기술 혁신**: Transformer 기반 대화형 AI의 상용화 성공
- **생성형 AI 붐**: GPT 성공으로 생성형 AI 시장 급성장

### Chat GPT의 문제점

- **할루시네이션(Hallucination)**: 그럴듯하지만 잘못된 정보 생성
- **지식 컷오프**: 학습 데이터 시점 이후 정보 부족
- **일관성 부족**: 같은 질문에 다른 답변 제공 가능
- **도메인 특화 한계**: 전문 분야 지식의 정확도 부족
- **출처 불명**: 답변 근거가 되는 정보 출처 제공 불가

### RAG vs 일반 LLM

| 구분 | 일반 LLM | RAG |
|------|----------|-----|
| 지식 소스 | outdated 학습 데이터만 활용 | 학습 데이터 + 외부 검색 |
| 정보 업데이트 | 재학습 필요 | 문서 추가만으로 가능 |
| 답변 근거 | 불명확 | 검색된 문서 기반 |
| 도메인 특화 | 제한적 | 전문 문서 활용 가능 |
| 할루시네이션 | 높은 위험 | 상대적으로 낮음 |

* 가령, 최근 뉴스 동향에 대한 질문을 하면 검색기능이 없는 일반 LLM은 학습 데이터 시점 이후의 정보를 가지고 있지 않기 때문에 답변을 할 수 없다. 
* 하지만, RAG를 사용하면 최신 정보나 전문적인 내용이 담긴 문서 (또는 텍스트)를 제공해주면 더 구체적이고 최신의 정보를 검색하여 답변을 할 수 있다.
* 따라서, RAG을 사용하면 아래와 같은 문제를 해결할 수 있다.
  * LLM의 한계 극복: Hallucination 문제 해결 및 학습 데터 시점 이후 정보 부족 문제 해결
  * 실시간 정보 활용: 최신 정보를 동적으로 검색하여 활용
  * 도메인 특화: 특정 분야의 전문 지식 기반 답변 생성
  * 비용 효율성: 전체 모델 재학습 없이 지식 확장 가능
  * 신뢰성 향상: 검색된 문서 기반으로 답변의 근거 제공
  * 근거 기반 답변: 검색된 문서를 바탕으로 답변 생성
  * 일관성 개선: 동일한 문서 소스 기반으로 일관된 답변
  * 출처 추적: 답변에 사용된 문서 출처 명시

## RAG 적용 방법

Chat GPT의 할루시네이션을 줄이고 방대한 지식 기반으로 답변하는 도메인 특화 chatbot 구축 가능

즉, RAG란 chat gpt에게 잘 정제된 데이터를 제공하여 더 정확하고 신뢰할 수 있는 답변을 제공하는 방법이다.

**주요 과제들:**
- Chat GPT의 RAG 과정은 비공개되어 user가 통제할 수 없는 부분이기 때문에 사용자들은 문서를 chat gpt가 잘 검색할 수 있는 형태로 변경하는 것이 중요
- 어려운 점은 각 문서마다 파일 형식이 다르고 이를 gpt가 처리가능한 형태로 전처리하는 과정이 공수가 많이 들어감
- 고유의 RAG를 만들어줘야 함

### 일반 LLM Process

```
사용자 질문 → 프롬프트 → LLM (with Black Box의 RAG 처리과정) → 답변 생성
```

* 특징
  * 통제할수 없는 RAG 구조: 질문을 직접 LLM에 입력하여 자체 Black Box의 RAG 처리과정을 통해 답변 생성
  * 학습 데이터 의존: 검색 대상의 데이터가 모델이 사전에 학습한 데이터
  * 방대한 학습데이터 의존: 너무 방대한 학습 데이터로 질문과 연관된 데이터를 검색하는데 한계가 있음, 전문적이고 디테일한 질문일수록 검색 결과의 질이 떨어짐
  * 정적 지식: 학습 시점 이후의 새로운 정보 반영 불가
  * 제한된 컨텍스트: 입력 토큰 길이 제한으로 긴 문서를 업로드하여 질문하면 긴 문서의 처리가 어려움
  * 할루시네이션 위험: 학습되지 않은 정보에 대해 그럴듯한 거짓 답변 생성 가능
* 따라서, RAG은 이 일반 LLM Process에 최신 정보 또는 추가 정보를 제공하여 Context(문맥)을 강화하여 더 정확하고 신뢰할 수 있는 답변을 제공하는 방법이다.

### RAG Process

```
query -> RAG(Retriever -> Similiarity Search -> Vector Store (DB) Pre-Processed Chunks-> Most Similar Chunk -> Retriver -> Promptanswer
```



1. 질문 또는 쿼리 입력
2. Retriever (검색기): 전처리된 Vector Store에서 유사도 검색을 통해 가장 유사한 문서 청크 검색
3. Pre-Process
  1. document upload
  2. text chunk split: 문서를 검색 가능한 단위로 분할
  3. embedding: 텍스트를 숫자 벡터로 변환
  4. vector store (DB)에 embedding된 벡터를 DB에 저장, 매번 검색할때마다 임베딩을 하면 비용이 증가하기 때문에 벡터 스토어에 저장된 벡터와 쿼리의 벡터를 비교하여 유사도가 높은 벡터를 찾아오는 과정
4. 검색(Retrieval): 보통 유사도 검색으로 코사인 유사도와 같은 방법으로 쿼리와 유사한 문서 청크 검색하여 검색 결과를 찾아오는 과정
5. prompt engineering: 검색 결과를 포함한 프롬프트 구성
6. LLM
7. answer: LLM이 최종 답변 생성




* 이 고유의 RAG를 만들어주는 것이 중요하지만 매우 고되고 어려운 과정이다. 내가 구현하려고 하는 답변 기능이 안되는 이유는 정말 수백가지에 달하기 때문이다. 
* 하지만, 분명한건 RAG를 잘 적용하여 원하는 기능을 구현하는 사례들이 많이 나오고 있고 효과적인 방법론들이 존재한다:
* 고급 RAG 기법들
  * 코사인 유사도 최적화: 벡터 검색 정확도 개선
  * HyDE Retrieval: 가상 문서 생성을 통한 검색 성능 향상
  * FT Embedding: 도메인 특화 임베딩 모델 파인튜닝
  * Chunk Embedding 실험: 최적 청킹 전략 탐색
  * Reranking: 검색 결과 재순위화
  * Classification Step: 쿼리 유형 분류를 통한 검색 최적화
  * Prompt Engineering: 효과적인 프롬프트 설계
  * Tool Use: 외부 도구 활용 확장
  * Query Expansion: 쿼리 확장 및 개선

## RAG 구현 난이도

RAG 구현은 사실상 LLM을 Tuning하는 것과 같다.

**LLM Tuning 방법 난이도 비교:**
- **Prompt Engineering** (매우 쉬움): 프롬프트만 수정하여 성능 개선
- **RAG** (쉬움): 외부 지식 소스 연결하여 답변 품질 향상
- **PEFT** (어려움): Parameter-Efficient Fine-Tuning 적용
- **Full Fine Tuning** (매우 어려움): 전체 모델 파라미터 재학습

**RAG의 장점:**
- 상대적으로 구현 난이도가 낮음
- 기존 모델 파라미터 수정 불필요
- 지식 업데이트가 용이함
- 비용 효율적인 성능 개선 방법

## RAG 구현 방법

### 기본 RAG 파이프라인 구축

**문서 처리:**
- 다양한 형식(PDF, DOCX, TXT, HTML) 문서 로딩
- 텍스트 추출 및 전처리
- 의미 있는 단위로 청킹 분할

**벡터화 및 저장:**
- OpenAI Embeddings 또는 오픈소스 임베딩 모델 사용
- FAISS, Chroma, Pinecone 등 벡터 데이터베이스 구축
- 효율적인 유사도 검색 인덱스 생성

### 검색 시스템 구현

**검색 전략:**
- 코사인 유사도 기반 벡터 검색
- 키워드 기반 하이브리드 검색
- 의미적 유사도와 키워드 매칭 결합

**검색 최적화:**
- Top-K 검색 결과 개수 조정
- 검색 임계값(threshold) 설정
- 문서 메타데이터 활용 필터링

### 프롬프트 엔지니어링

**프롬프트 구조:**
- 시스템 메시지: 역할 및 지침 명시
- 컨텍스트: 검색된 문서 내용 포함
- 질문: 사용자 쿼리
- 답변 형식: 원하는 출력 형태 지정

**프롬프트 개선:**
- Few-shot 예시 추가
- 체인 오브 생각(Chain of Thought) 적용
- 답변 검증 및 출처 표기 요구

### 평가 및 개선

**성능 평가:**
- 답변 정확도 측정
- 검색 정밀도(Precision) 및 재현율(Recall)
- 사용자 만족도 조사

**지속적 개선:**
- A/B 테스트를 통한 파라미터 최적화
- 사용자 피드백 기반 모델 업데이트
- 새로운 문서 데이터 정기 추가

# Lang Chain 소개

## LangChain이란?

- **정의**: LLM(Large Language Model) 기반 애플리케이션 개발을 위한 오픈소스 프레임워크
- **주요 목적**: 복잡한 LLM 애플리케이션을 쉽게 구축할 수 있도록 지원

## LangChain의 핵심 기능

- **체인(Chain)**: 여러 컴포넌트를 연결하여 복잡한 워크플로우 구성
- **프롬프트 템플릿**: 동적으로 프롬프트 생성 및 관리
- **메모리**: 대화 히스토리 및 컨텍스트 유지
- **에이전트**: 도구 사용 및 의사결정 자동화
- **문서 로더**: 다양한 형식의 문서 처리
- **벡터 스토어**: 임베딩 기반 검색 시스템

## LangChain 사용 사례

- **챗봇**: 문맥을 이해하는 대화형 AI 시스템
- **문서 QA**: 문서 기반 질의응답 시스템  
- **데이터 분석**: 자연어로 데이터 분석 수행
- **코드 생성**: 자연어 명령으로 코드 자동 생성
- **RAG 시스템**: 검색 기반 답변 생성 시스템






