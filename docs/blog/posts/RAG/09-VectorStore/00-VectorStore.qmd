---
title: "Vector Store 개요"
subtitle: 벡터 스토어
description: |
  임베딩 벡터를 저장하고 검색하는 벡터 데이터베이스를 다룬다.
categories:
  - AI
  - RAG
  - LangChain
author: Kwangmin Kim
date: 04/15/2025
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
execute:
    eval: false
---


## VectorStore란?

* VectorStore(벡터 스토어)는 임베딩 벡터를 효율적으로 저장하고 검색하는 특수한 데이터베이스다.
* RAG(Retrieval-Augmented Generation) 시스템에서 VectorStore는 문서 검색의 핵심 구성 요소로, 의미 기반 검색(Semantic Search)을 가능하게 한다.
* 문서 load > 문서 split > Embedding > Store
* Vector Store는 Vector DB 검색 기능은 없고 데이터만 저장하는 공간 Vector Store에 따라 검색 성능은 차이가 나지 않음. 
* 하지만, 실제로 Vector DB에 따라 검색 성능이 차이가 나는데 검색 알고리즘이 차이가 나기 때문이다.
    * 검색 알고리즘
        * Semantic Search: 거의 모든 Vector DB가 갖고 있는 검색 기능
        * Keyword Search: 일부 Vector DB가 지원하는 기능
* 로컬 DB는 일정 용량 이상이면 검색 성능이 급격히 저하되는 확장성 문제가 있다.
  - 예: FAISS, Chroma (로컬 모드)는 메모리 기반으로 동작하여 대규모 데이터셋에서 한계 존재
  - 단일 서버의 리소스(메모리, CPU)에 의존하므로 수평 확장(horizontal scaling)이 어려움 
* **FAISS (Facebook AI Similarity Search)**
  - Meta AI에서 개발한 고성능 벡터 검색 라이브러리
  - 주요 특징:
    - CPU/GPU 모두 지원하여 하드웨어 가속 활용 가능
    - 다양한 인덱스 알고리즘 제공 (Flat, IVF, HNSW, PQ 등)
    - 10억 개 이상의 벡터도 효율적으로 검색 가능
  - 장점:
    - 매우 빠른 검색 속도 (특히 GPU 사용 시)
    - 메모리 효율적인 압축 기법 (Product Quantization)
    - 오픈소스이며 Python/C++ 인터페이스 제공
  - 단점:
    - 기본적으로 인메모리 저장 (영구 저장은 수동으로 save/load 필요)
    - 실시간 업데이트가 어려움 (인덱스 재구축 필요)
    - 분산 환경 지원 부족
  - 적합한 사용 사례:
    - 프로토타이핑 및 실험 단계
    - 로컬 개발 환경
    - 배치 처리 중심의 검색 시스템
    - 초고속 검색이 필요하지만 업데이트가 적은 경우
* **Chroma**
  - AI 네이티브 오픈소스 임베딩 데이터베이스
  - **주요 특징**:
    - 내장 DB로 별도 서버 설치 불필요
    - SQLite 기반으로 자동 영구 저장
    - Python과 JavaScript 클라이언트 지원
    - 간단한 API로 빠른 시작 가능
  - 장점:
    - 설치 및 사용이 매우 간편 (`pip install chromadb`만으로 시작)
    - 자동으로 디스크에 데이터 저장 (별도 save/load 불필요)
    - 메타데이터 필터링 강력 (복잡한 쿼리 지원)
    - 멀티모달 지원 (텍스트 + 이미지)
    - 로컬과 클라우드 모두 지원 (Chroma Cloud)
  - 단점:
    - FAISS 대비 검색 속도가 느림
    - 대규모 데이터셋(수백만 이상)에서 성능 저하
    - GPU 가속 미지원
    - 프로덕션 환경에서는 제한적
  - 적합한 사용 사례:
    - 빠른 프로토타이핑이 필요한 경우
    - 소규모 프로젝트 (10만 개 이하 문서)
    - 개발/테스트 환경
    - 멀티모달 검색이 필요한 경우 (이미지)
    - 간단한 RAG 애플리케이션
  * 클라우드 DB는 확장성에 최적화되어 있다
    - 자동 스케일링: 데이터 증가에 따라 자동으로 인프라 확장 (예: Pinecone, Weaviate Cloud)
    - 분산 아키텍처: 여러 노드에 데이터를 분산 저장하여 병렬 처리 가능 (예: Milvus, Qdrant)
    - 관리형 서비스: 백업, 복제, 모니터링 등의 운영 작업을 자동으로 처리
    - 고가용성(High Availability): 장애 발생 시에도 서비스 중단 없이 운영 가능

### 기존 데이터베이스와의 차이점

| 구분 | 기존 DB (관계형/NoSQL) | VectorStore |
|------|----------------------|-------------|
| **저장 방식** | 텍스트, 숫자, 구조화된 데이터 | 고차원 벡터 (임베딩) |
| **검색 방식** | 키워드 매칭, 정확한 일치 | 유사도 기반 검색 (코사인 유사도, 유클리드 거리) |
| **인덱싱** | B-Tree, Hash Index | HNSW, IVF, Product Quantization |
| **사용 사례** | 트랜잭션 처리, CRUD 작업 | 의미 검색, 추천 시스템, RAG |

## RAG 파이프라인에서의 위치

VectorStore는 RAG 시스템의 4단계 중 네 번째 단계에 해당한다:

1. 문서 로딩(Document Loading): 다양한 형식의 문서를 불러온다
2. 텍스트 분할(Text Splitting): 문서를 작은 청크(chunk)로 나눈다
3. 임베딩 생성(Embedding): 텍스트 청크를 벡터로 변환한다
4. 벡터 저장(Vector Store): 임베딩 벡터를 VectorStore에 저장한다 ← **현재 단계**
5. 검색(Retrieval): 질의와 유사한 벡터를 찾아 관련 문서를 반환한다

## VectorStore의 필요성

### 빠른 검색 속도

- 수백만 개의 벡터 중에서도 밀리초 단위로 유사 벡터를 찾는다
- 근사 최근접 이웃(ANN, Approximate Nearest Neighbor) 알고리즘 활용
- 대규모 데이터셋에서도 실시간 검색 가능

### 의미 기반 검색(Semantic Search)

기존 키워드 검색의 한계를 극복한다:

**키워드 검색의 문제점:**
```
질문: "모바일 디바이스 상에서 동작하는 인공지능 기술을 소개한 기업명은?"
키워드 매칭: "모바일", "디바이스", "인공지능" 등의 단어가 정확히 포함된 문서만 검색
→ 유의어나 문맥을 고려하지 못함
```

**의미 기반 검색:**
```
질문: "모바일 디바이스 상에서 동작하는 인공지능 기술을 소개한 기업명은?"
벡터 유사도: 질문과 의미적으로 유사한 문서를 검색
→ "스마트폰 AI", "온디바이스 머신러닝" 등 유의어가 포함된 문서도 검색 가능
```

### 스케일러빌리티(Scalability)

- 데이터가 지속적으로 증가해도 성능 저하 없이 관리 가능
- 분산 저장 및 병렬 처리 지원
- 인덱스 최적화를 통한 효율적인 메모리 사용

## 주요 VectorStore 비교

| VectorStore | 배포 방식 | 라이선스 | 특징 | 장점 | 단점 | 적합한 사용 사례 |
|-------------|----------|---------|------|------|------|-----------------|
| **FAISS** | 로컬 | 오픈소스 (MIT) | Meta AI 개발, CPU/GPU 지원,벡터 검색 전용
 | 빠른 검색 속도, 다양한 인덱스 옵션 | 메모리 내 저장, 영구 저장 추가 설정 필요 | 프로토타이핑, 로컬 개발 |
| **Chroma** | 로컬/클라우드 | 오픈소스 (Apache 2.0) | 내장 DB, 멀티모달 지원, 벡터 검색 전용
 | 쉬운 사용, 자동 영구 저장 | 상대적으로 느린 속도 | 소규모 프로젝트, 빠른 프로토타이핑 |
| **Pinecone** | 클라우드 | 유료 (Freemium) | 완전 관리형, 서버리스 | 스케일링 자동화, 안정성, 관리 불필요 | 비용 발생, 온프레미스 불가, 벤더 종속 | 프로덕션 환경, 대규모 서비스 |
| **Qdrant** | 로컬/클라우드 | 오픈소스 (Apache 2.0) | Rust 기반, 빠른 성능, 벡터 검색 전용 | 필터링 강력, 메모리 효율적, RESTful API | 상대적으로 작은 커뮤니티 | 실시간 추천, 시맨틱 검색 |
| **Weaviate** | 로컬/클라우드 | 오픈소스 (BSD-3) | GraphQL API, 하이브리드 검색 | 풍부한 기능, 필터링 강력, 멀티테넌시 | 설정 복잡도 높음 | 복잡한 검색 요구사항, 엔터프라이즈 |
| **Milvus** | 로컬/클라우드 | 오픈소스 (Apache 2.0) | 분산 아키텍처, 엔터프라이즈급 | 높은 성능, 확장성 우수, GPU 지원 | 설치/관리 복잡 | 대규모 엔터프라이즈, 프로덕션 |
| **Azure AI Search** | 클라우드 (Azure) | 유료 | Azure 네이티브, 하이브리드 검색 | Azure 생태계 통합, 벡터+키워드 검색, 보안 | Azure 종속, 비용, 설정 복잡 | Azure 환경, 엔터프라이즈 검색 |
| **pgvector** | 로컬/클라우드 | 오픈소스 (PostgreSQL) | PostgreSQL 확장, 하이브리드 검색  | 기존 DB 활용, SQL 쿼리, 트랜잭션 지원 | PostgreSQL 성능 의존, 대규모 제한적 | 기존 PostgreSQL 사용, 하이브리드 앱 |
| **Elasticsearch** | 로컬/클라우드 | 오픈소스/유료 (Elastic License) | 검색 엔진 기반, 벡터 검색 추가 | 강력한 전문 검색, 분석 기능, 성숙한 생태계 | 리소스 사용 많음, 복잡한 설정 | 로그 분석, 하이브리드 검색 |
| **Redis** | 로컬/클라우드 | 오픈소스/유료 (Redis Stack) | 인메모리 DB, RediSearch 모듈, 하이브리드 검색  | 매우 빠른 속도, 익숙한 Redis 인터페이스 | 메모리 비용, 영구 저장 제한적 | 실시간 검색, 캐싱과 검색 통합 |

## VectorStore 사용 예시

### 기본 사용법 (FAISS)

```{python}
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter

# 1. 텍스트 분할
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
documents = text_splitter.create_documents([
    "LangChain은 대규모 언어 모델을 활용한 애플리케이션 개발 프레임워크다.",
    "VectorStore는 임베딩 벡터를 효율적으로 저장하고 검색한다.",
    "RAG 시스템은 외부 지식을 활용하여 LLM의 답변 품질을 향상시킨다."
])

# 2. 임베딩 생성 및 VectorStore 구축
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)

# 3. 유사도 검색
query = "RAG 시스템이란 무엇인가?"
results = vectorstore.similarity_search(query, k=2)

for doc in results:
    print(doc.page_content)
```

### 메타데이터와 함께 저장

```{python}
from langchain_core.documents import Document

# 메타데이터 포함 문서 생성
documents = [
    Document(
        page_content="LangChain은 LLM 애플리케이션 개발 프레임워크다.",
        metadata={"source": "docs", "page": 1, "category": "framework"}
    ),
    Document(
        page_content="FAISS는 효율적인 벡터 검색 라이브러리다.",
        metadata={"source": "docs", "page": 2, "category": "vectorstore"}
    ),
]

vectorstore = FAISS.from_documents(documents, embeddings)

# 메타데이터 필터링과 함께 검색
results = vectorstore.similarity_search(
    query="벡터 검색",
    k=1,
    filter={"category": "vectorstore"}
)
```

### VectorStore 저장 및 로드

```{python}
# VectorStore 저장
vectorstore.save_local("./vector_db")

# VectorStore 로드
loaded_vectorstore = FAISS.load_local(
    "./vector_db",
    embeddings,
    allow_dangerous_deserialization=True
)
```

## 검색 방법 비교

### Similarity Search (유사도 검색)

가장 기본적인 검색 방법으로, 코사인 유사도를 기반으로 상위 k개 문서를 반환한다.

```{python}
results = vectorstore.similarity_search(query="RAG 시스템", k=3)
```

### Similarity Search with Score

유사도 점수와 함께 문서를 반환한다.

```{python}
results = vectorstore.similarity_search_with_score(query="RAG 시스템", k=3)
for doc, score in results:
    print(f"Score: {score}, Content: {doc.page_content}")
```

### Max Marginal Relevance (MMR)

다양성을 고려한 검색으로, 관련성은 높지만 서로 다른 내용의 문서를 반환한다.

```{python}
results = vectorstore.max_marginal_relevance_search(
    query="RAG 시스템",
    k=3,
    fetch_k=10,  # 후보 문서 수
    lambda_mult=0.5  # 0: 다양성 최대, 1: 관련성 최대
)
```

## 성능 최적화 팁

### 적절한 청크 크기 설정

- 너무 작은 청크: 문맥 손실, 검색 정확도 저하
- 너무 큰 청크: 관련 없는 정보 포함, 토큰 낭비
- 권장 크기: 500-1000 토큰 (사용 사례에 따라 조정)

### 임베딩 모델 선택

- 속도 우선: `text-embedding-3-small`
- 정확도 우선: `text-embedding-3-large`
- 비용 절감**: 캐시 활용 (`CacheBackedEmbeddings`)

### 인덱스 타입 선택 (FAISS)

```{python}
# Flat Index: 정확하지만 느림
vectorstore = FAISS.from_documents(documents, embeddings)

# IVF Index: 빠르지만 근사치
import faiss
index = faiss.IndexIVFFlat(dimension, nlist=100)
vectorstore = FAISS(embeddings, index)
```

## 참고 자료

### 공식 문서 및 GitHub

**LangChain & 통합:**
- [LangChain VectorStores 공식 문서](https://python.langchain.com/docs/modules/data_connection/vectorstores/)
- [LangChain VectorStore 통합 목록](https://python.langchain.com/docs/integrations/vectorstores/)

**개별 VectorStore:**
- [FAISS GitHub Repository](https://github.com/facebookresearch/faiss)
- [FAISS Wiki 문서](https://github.com/facebookresearch/faiss/wiki)
- [Chroma 공식 문서](https://docs.trychroma.com/)
- [Chroma GitHub Repository](https://github.com/chroma-core/chroma)
- [Pinecone 공식 문서](https://docs.pinecone.io/)
- [Weaviate 공식 문서](https://weaviate.io/developers/weaviate)
- [Milvus 공식 문서](https://milvus.io/docs)
- [Qdrant 공식 문서](https://qdrant.tech/documentation/)
- [Azure AI Search 문서](https://learn.microsoft.com/en-us/azure/search/)
- [pgvector GitHub](https://github.com/pgvector/pgvector)
- [Elasticsearch Vector Search](https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html)

### 벤치마크 및 비교

**성능 비교:**
- [VectorDBBench - Vector Database Benchmark](https://github.com/zilliztech/VectorDBBench)
- [ANN Benchmarks](https://ann-benchmarks.com/) - 근사 최근접 이웃 알고리즘 벤치마크
- [Vector Database Comparison 2024](https://benchmark.vectorview.ai/)

**선택 가이드:**
- [Choosing a Vector Database](https://www.pinecone.io/learn/vector-database/)
- [Vector Database Comparison Guide](https://thedataquarry.com/posts/vector-db-1/)

### 학습 자료

**개념 및 이론:**
- [What is a Vector Database?](https://www.pinecone.io/learn/vector-database/) - Pinecone 학습 자료
- [Vector Search Explained](https://weaviate.io/blog/vector-search-explained) - Weaviate 블로그
- [Understanding HNSW](https://www.pinecone.io/learn/series/faiss/hnsw/) - HNSW 알고리즘 설명

**실습 튜토리얼:**
- [Building a RAG System with LangChain](https://python.langchain.com/docs/tutorials/rag/)
- [FAISS Tutorial](https://www.pinecone.io/learn/series/faiss/)
- [Chroma Cookbook](https://cookbook.chromadb.dev/)

### 연구 논문

**핵심 알고리즘:**
- [Billion-scale similarity search with GPUs (FAISS)](https://arxiv.org/abs/1702.08734) - Facebook AI Research, 2017
- [Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs (HNSW)](https://arxiv.org/abs/1603.09320) - 2016
- [Product Quantization for Nearest Neighbor Search](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf) - 2011

**RAG 시스템:**
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) - Meta AI, 2020
- [In-Context Retrieval-Augmented Language Models](https://arxiv.org/abs/2302.00083) - 2023

### 커뮤니티 및 포럼

- [LangChain Discord](https://discord.gg/langchain)
- [Chroma Discord](https://discord.gg/MMeYNTmh3x)
- [Weaviate Community Forum](https://forum.weaviate.io/)
- [r/MachineLearning - Vector Database 토론](https://www.reddit.com/r/MachineLearning/)

### 블로그 및 기술 아티클

- [Vector Databases: A Beginner's Guide](https://www.datacamp.com/tutorial/vector-databases-guide) - DataCamp
- [The Illustrated Vector Database](https://www.youtube.com/watch?v=dN0lsF2cvm4) - YouTube 강의
- [LangChain Blog - Vector Stores](https://blog.langchain.dev/tag/vector-stores/)
- [Towards Data Science - Vector Database Articles](https://towardsdatascience.com/tagged/vector-database)

