{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        self.dweights = np.dot(self.inputs.T, grad_output)\n",
    "        self.dbiases = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        return np.dot(grad_output, self.weights.T)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.layer1 = Layer(input_size, hidden_size)\n",
    "        self.layer2 = Layer(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.h1 = relu(self.layer1.forward(x))\n",
    "        self.y_pred = self.layer2.forward(self.h1)\n",
    "        return self.y_pred\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_h2 = self.layer2.backward(grad_output)\n",
    "        grad_h1 = grad_h2 * relu_derivative(self.h1)\n",
    "        self.layer1.backward(grad_h1)\n",
    "        \n",
    "    def get_params(self):\n",
    "        return np.concatenate([self.layer1.weights.ravel(), self.layer1.biases.ravel(),\n",
    "                               self.layer2.weights.ravel(), self.layer2.biases.ravel()])\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        idx = 0\n",
    "        self.layer1.weights = params[idx:idx+self.layer1.weights.size].reshape(self.layer1.weights.shape)\n",
    "        idx += self.layer1.weights.size\n",
    "        self.layer1.biases = params[idx:idx+self.layer1.biases.size].reshape(self.layer1.biases.shape)\n",
    "        idx += self.layer1.biases.size\n",
    "        self.layer2.weights = params[idx:idx+self.layer2.weights.size].reshape(self.layer2.weights.shape)\n",
    "        idx += self.layer2.weights.size\n",
    "        self.layer2.biases = params[idx:idx+self.layer2.biases.size].reshape(self.layer2.biases.shape)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "def f_alpha(alpha, fun, x, s, args):\n",
    "    return fun(x + alpha * s, *args)\n",
    "\n",
    "def search_golden_section(fun, dfun, x, s, args=(), delta=1.0e-2, tol=1e-15):\n",
    "    gr = (np.sqrt(5) + 1) / 2\n",
    "        \n",
    "    AL = 0.\n",
    "    FL = f_alpha(AL, fun, x, s, args)\n",
    "    AA = delta\n",
    "    FA = f_alpha(AA, fun, x, s, args)\n",
    "    while FL < FA:\n",
    "        delta = 0.1*delta\n",
    "        AA = delta\n",
    "        FA = f_alpha(AA, fun, x, s, args)\n",
    "    \n",
    "    j = 1\n",
    "    AU = AA + delta * (gr**j)\n",
    "    FU = f_alpha(AU, fun, x, s, args)\n",
    "    while FA > FU:\n",
    "        AL = AA\n",
    "        AA = AU\n",
    "        FL = FA\n",
    "        FA = FU\n",
    "        \n",
    "        j += 1\n",
    "        AU = AA + delta * (gr**j)\n",
    "        FU = f_alpha(AU, fun, x, s, args)\n",
    "\n",
    "    AB = AL + (AU - AL) / gr\n",
    "    FB = f_alpha(AB, fun, x, s, args)\n",
    "    \n",
    "    while abs(AA - AB) > tol:\n",
    "        if f_alpha(AA, fun, x, s, args) < f_alpha(AB, fun, x, s, args):\n",
    "            AU = AB\n",
    "        else:\n",
    "            AL = AA\n",
    "\n",
    "        AA = AU - (AU - AL) / gr\n",
    "        AB = AL + (AU - AL) / gr\n",
    "\n",
    "    return (AU + AL) / 2,\n",
    "\n",
    "def objective(params, model, X, Y):\n",
    "    model.set_params(params)\n",
    "    y_pred = model.forward(X)\n",
    "    return mse_loss(Y, y_pred)\n",
    "\n",
    "def gradient(params, model, X, Y):\n",
    "    model.set_params(params)\n",
    "    y_pred = model.forward(X)\n",
    "    grad_output = mse_derivative(Y, y_pred)\n",
    "    model.backward(grad_output)\n",
    "    return np.concatenate([model.layer1.dweights.ravel(), model.layer1.dbiases.ravel(),\n",
    "                           model.layer2.dweights.ravel(), model.layer2.dbiases.ravel()])\n",
    "\n",
    "def compute_conjugate_gradient(f, df, x, args=(), eps=1.0e-7, max_iter=2500, verbose=False, callback=None):\n",
    "    if verbose:\n",
    "        print(\"#####START OPTIMIZATION#####\")\n",
    "        print(\"INIT POINT : {}, dtype : {}\".format(x, x.dtype))\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        c = df(x, *args)\n",
    "        if np.linalg.norm(c) < eps :\n",
    "            if verbose:\n",
    "                print(\"Stop criterion break Iter: {:5d}, x: {}\".format(k, x))\n",
    "                print(\"\\n\")\n",
    "            break\n",
    "\n",
    "        if k == 0 :\n",
    "            d = -c\n",
    "        else:\n",
    "            beta = (np.linalg.norm(c) / np.linalg.norm(c_old))**2\n",
    "            d = -c + beta*d\n",
    "        \n",
    "        alpha = search_golden_section(f, df, x, d, args=args)[0]\n",
    "        x = x + alpha * d\n",
    "        c_old = c.copy()    \n",
    "\n",
    "        if callback :\n",
    "            callback(x)    \n",
    "\n",
    "    else:\n",
    "        print(\"Stop max iter:{:5d} x:{}\".format(k, x)) \n",
    "\n",
    "    return x\n",
    "\n",
    "# 데이터 생성\n",
    "np.random.seed(42)\n",
    "t = np.linspace(0, 4*np.pi, 200)\n",
    "y = np.sin(t) + 0.1 * np.random.randn(200)\n",
    "\n",
    "# 시계열 데이터 준비\n",
    "window_size = 5\n",
    "X, Y = [], []\n",
    "for i in range(len(y) - window_size):\n",
    "    X.append(y[i:i+window_size])\n",
    "    Y.append(y[i+window_size])\n",
    "X, Y = np.array(X), np.array(Y).reshape(-1, 1)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터 분리\n",
    "split = 150\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "Y_train, Y_test = Y[:split], Y[split:]\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP(window_size, 10, 1)\n",
    "\n",
    "# 학습 과정을 기록하기 위한 리스트\n",
    "losses = []\n",
    "\n",
    "# 콜백 함수 정의\n",
    "def callback(params):\n",
    "    loss = objective(params, model, X_train, Y_train)\n",
    "    losses.append(loss)\n",
    "    if len(losses) % 100 == 0:\n",
    "        print(f\"Iteration {len(losses)}, Loss: {loss}\")\n",
    "\n",
    "# Conjugate Gradient 최적화\n",
    "initial_params = model.get_params()\n",
    "optimized_params = compute_conjugate_gradient(\n",
    "    f=objective,\n",
    "    df=gradient,\n",
    "    x=initial_params,\n",
    "    args=(model, X_train, Y_train),\n",
    "    eps=1e-6,\n",
    "    max_iter=1000,\n",
    "    verbose=True,\n",
    "    callback=callback\n",
    ")\n",
    "\n",
    "# 최적화된 파라미터 설정\n",
    "model.set_params(optimized_params)\n",
    "\n",
    "# 45개의 새로운 데이터 포인트 예측\n",
    "predictions = []\n",
    "last_window = X_test[-1]\n",
    "\n",
    "for _ in range(45):\n",
    "    next_pred = model.forward(last_window.reshape(1, -1))\n",
    "    predictions.append(next_pred[0, 0])\n",
    "    last_window = np.roll(last_window, -1)\n",
    "    last_window[-1] = next_pred[0, 0]\n",
    "\n",
    "print(\"45개의 예측된 데이터 포인트:\", predictions)\n",
    "\n",
    "# 학습 과정 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss over Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
