{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_conjugate_gradient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33272\\2495437327.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[0minitial_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m optimized_params = compute_conjugate_gradient(\n\u001b[0m\u001b[0;32m     91\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_conjugate_gradient' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 준비\n",
    "data = np.array([10345.95344093, 10254.57894458, 10198.42042604, 10160.22832452, 10117.95232831, \n",
    "                 10096.21754303, 10068.98950732, 10048.45509888, 10029.97294258, 10012.6500211, \n",
    "                 9992.32002722, 9986.8216521, 9967.30002051, 9953.65734963, 9944.32238966, \n",
    "                 9924.22764642, 9918.56652301, 9906.78991636, 9896.77347545, 9881.72663955, \n",
    "                 9870.3843831, 9866.06461084, 9857.33460291, 9847.63331086, 9839.89791331, \n",
    "                 9829.26714378, 9819.53021078, 9803.83869316, 9802.0457347, 9792.98681928, \n",
    "                 9785.25389664, 9770.42734065, 9762.52207987, 9747.74130149, 9745.77012212, \n",
    "                 9734.95618568, 9731.59571095, 9728.38748027, 9713.43131064, 9706.91520543, \n",
    "                 9698.41219488, 9695.5943976, 9685.78235559, 9677.83567665, 9678.25361304])\n",
    "\n",
    "# 데이터 정규화\n",
    "data_normalized = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "# 입력 데이터 생성\n",
    "X = np.arange(len(data)).reshape(-1, 1) / len(data)\n",
    "Y = data_normalized.reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "def search_golden_section(fun, dfun, x, s, args=(), delta=1.0e-2, tol=1e-15):\n",
    "    \"\"\"\n",
    "    https://en.wikipedia.org/wiki/Golden-section_search and [arora]\n",
    "    \n",
    "    fun   : Original objective function\n",
    "    dfun  : Objective function gradient which is not used\n",
    "    x     : Start point\n",
    "    s     : 1D search directin\n",
    "    args  : Tuple extra arguments passed to the objective function\n",
    "    delta : Init. guess interval determining initial interval of uncertainty\n",
    "    tol   : stop criterion\n",
    "    \"\"\"\n",
    "    gr = (np.sqrt(5) + 1) / 2\n",
    "        \n",
    "    AL = 0.\n",
    "    FL = f_alpha(AL, fun, x, s, args)\n",
    "    AA = delta\n",
    "    FA = f_alpha(AA, fun, x, s, args)\n",
    "    while  FL < FA :\n",
    "        delta = 0.1*delta\n",
    "        AA = delta\n",
    "        FA = f_alpha(AA, fun, x, s, args)\n",
    "    \n",
    "    j = 1\n",
    "    AU = AA + delta * (gr**j)\n",
    "    FU = f_alpha(AU, fun, x, s, args)\n",
    "    while FA > FU :\n",
    "        AL = AA\n",
    "        AA = AU\n",
    "        FL = FA\n",
    "        FA = FU\n",
    "        \n",
    "        j += 1\n",
    "        AU = AA + delta * (gr**j)\n",
    "        FU = f_alpha(AU, fun, x, s, args)\n",
    "\n",
    "    AB = AL + (AU - AL) / gr\n",
    "    FB = f_alpha(AB, fun, x, s, args)\n",
    "    \n",
    "    while abs(AA - AB) > tol:\n",
    "        if f_alpha(AA, fun, x, s, args) < f_alpha(AB, fun, x, s, args):\n",
    "            AU = AB\n",
    "        else:\n",
    "            AL = AA\n",
    "\n",
    "        # we recompute both c and d here to avoid loss of precision \n",
    "        # which may lead to incorrect results or infinite loop\n",
    "        AA = AU - (AU - AL) / gr\n",
    "        AB = AL + (AU - AL) / gr\n",
    "\n",
    "    return ( (AU + AL) / 2, )\n",
    "\n",
    "\n",
    "def compute_conjugate_gradient(f, df, x, args=(), eps=1.0e-7, max_iter=2500, verbose=False, callback=None):\n",
    "    \"\"\"\n",
    "    f       : 최적화 할 함수\n",
    "    df      : 최적화할 함수의 도함수\n",
    "    x       : 초기값\n",
    "    args    : f(x, *args)로 f를 호출하기 위한 추가 매개변수\n",
    "    eps     : 정기 기준\n",
    "    max_iter: 최대 반복수\n",
    "    verbose : 실행 정보를 텍스트로 출력 \n",
    "    callback: x에 대한 기록을 위한 콜백함수\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"#####START OPTIMIZATION#####\")\n",
    "        print(\"INIT POINT : {}, dtype : {}\".format(x, x.dtype))\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        c = df(x, *args)\n",
    "        if np.linalg.norm(c) < eps :\n",
    "            if verbose:\n",
    "                print(\"Stop criterion break Iter: {:5d}, x: {}\".format(k, x))\n",
    "                print(\"\\n\")\n",
    "            break\n",
    "\n",
    "        if k == 0 :\n",
    "            d = -c\n",
    "        else:\n",
    "            beta = (np.linalg.norm(c) / np.linalg.norm(c_old))**2\n",
    "            d = -c + beta*d\n",
    "        \n",
    "        alpha = search_golden_section(f, df, x, d, args=args)[0]\n",
    "        x = x + alpha * d\n",
    "        c_old = c.copy()    \n",
    "\n",
    "        if callback :\n",
    "            callback(x)    \n",
    "\n",
    "    else:\n",
    "        print(\"Stop max iter:{:5d} x:{}\".format(k, x)) \n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# 신경망 모델 정의\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        self.layers = len(hidden_sizes) + 1\n",
    "        self.W = [np.random.randn(input_size, hidden_sizes[0]) * 0.01]\n",
    "        self.b = [np.zeros((1, hidden_sizes[0]))]\n",
    "        \n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            self.W.append(np.random.randn(hidden_sizes[i-1], hidden_sizes[i]) * 0.01)\n",
    "            self.b.append(np.zeros((1, hidden_sizes[i])))\n",
    "        \n",
    "        self.W.append(np.random.randn(hidden_sizes[-1], output_size) * 0.01)\n",
    "        self.b.append(np.zeros((1, output_size)))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.a = [X]\n",
    "        for i in range(self.layers):\n",
    "            z = np.dot(self.a[i], self.W[i]) + self.b[i]\n",
    "            a = np.tanh(z)\n",
    "            self.a.append(a)\n",
    "        return self.a[-1]\n",
    "\n",
    "    def get_params(self):\n",
    "        return np.concatenate([w.ravel() for w in self.W] + [b.ravel() for b in self.b])\n",
    "\n",
    "    def set_params(self, params):\n",
    "        start = 0\n",
    "        for i in range(self.layers):\n",
    "            end = start + self.W[i].size\n",
    "            self.W[i] = params[start:end].reshape(self.W[i].shape)\n",
    "            start = end\n",
    "            end = start + self.b[i].size\n",
    "            self.b[i] = params[start:end].reshape(self.b[i].shape)\n",
    "            start = end\n",
    "\n",
    "# 목적 함수 및 그래디언트 정의\n",
    "def objective(params, nn, X, Y):\n",
    "    nn.set_params(params)\n",
    "    predictions = nn.forward(X)\n",
    "    return np.mean((predictions - Y) ** 2)\n",
    "\n",
    "def gradient(params, nn, X, Y):\n",
    "    nn.set_params(params)\n",
    "    m = X.shape[0]\n",
    "    predictions = nn.forward(X)\n",
    "    \n",
    "    grad = []\n",
    "    delta = predictions - Y\n",
    "    for i in range(nn.layers - 1, -1, -1):\n",
    "        dW = np.dot(nn.a[i].T, delta) / m\n",
    "        db = np.sum(delta, axis=0, keepdims=True) / m\n",
    "        grad = [dW.ravel(), db.ravel()] + grad\n",
    "        \n",
    "        if i > 0:\n",
    "            delta = np.dot(delta, nn.W[i].T) * (1 - nn.a[i]**2)\n",
    "    \n",
    "    return np.concatenate(grad)\n",
    "\n",
    "# Helper function for search_golden_section\n",
    "def f_alpha(alpha, fun, x, s, args):\n",
    "    return fun(x + alpha * s, *args)\n",
    "\n",
    "# search_golden_section 및 compute_conjugate_gradient 함수는 이미 정의되어 있다고 가정합니다.\n",
    "\n",
    "# 모델 학습\n",
    "nn = NeuralNetwork(input_size=1, hidden_sizes=[32, 32], output_size=1)\n",
    "initial_params = nn.get_params()\n",
    "\n",
    "optimized_params = compute_conjugate_gradient(\n",
    "    f=objective,\n",
    "    df=gradient,\n",
    "    x=initial_params,\n",
    "    args=(nn, X, Y),\n",
    "    eps=1e-6,\n",
    "    max_iter=1000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "nn.set_params(optimized_params)\n",
    "\n",
    "# 예측\n",
    "predictions_normalized = nn.forward(X)\n",
    "predictions = predictions_normalized * (np.max(data) - np.min(data)) + np.min(data)\n",
    "\n",
    "# 결과 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data, label='Actual')\n",
    "plt.plot(predictions, label='Predicted', linestyle='--')\n",
    "plt.title('Time Series Prediction with Conjugate Gradient Optimization')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 성능 평가\n",
    "mse = np.mean((data - predictions.flatten()) ** 2)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
