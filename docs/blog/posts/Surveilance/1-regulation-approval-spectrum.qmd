---
title: "의료 AI 규제 실전 가이드"
subtitle: "알고리즘 유형별 승인 전략과 생성형 AI의 규제 장벽"
description: |
    실제 FDA/EMA 승인을 받은 AI 의료기기 사례를 분석하고, Rule-based부터 딥러닝까지의 승인 전략을 제시한다. 생성형 AI가 의료기기로 승인되지 못하는 구조적 이유를 규제 관점에서 상세히 다룬다.
categories:
    - AI
    - Surveilance
    - Regulation
    - Healthcare
author: Kwangmin Kim
date: 01/08/2025
format: 
    html:
        code-fold: true
        toc: true
        number-sections: true
draft: false
execute:
    eval: false
bibliography: references.bib
---

## 실제로 승인된 알고리즘의 스펙트럼

의료 AI 규제는 이론이 아닌 **실전 승인 사례**로 이해해야 한다. FDA는 2025년 12월 기준 945개 이상의 AI 의료기기를 승인했으며[@FDAAIML2025], 이들을 분석하면 명확한 패턴이 보인다.

### Rule-based와 통계 모델: 가장 안전한 규제 경로

#### 특징과 승인 현황

Rule-based AI와 통계 모델은 **규제 리스크가 거의 없다**. 이들은 의료 현장에서 수십 년간 사용되어 왔으며, 대부분 Class I (낮은 위험) 또는 Class II (중간 위험)로 분류된다[@FDA2026CDS].

**대표 사례:**

1. **CHA₂DS₂-VASc Score (심방세동 뇌졸중 위험 예측)**
   - 점수 기반 규칙: 나이, 성별, 고혈압, 당뇨병 등
   - 계산식: 각 요인에 점수 부여 → 합산
   - 규제 상태: 510(k) 면제 (정보 제공만)
   - 근거: [2010 ESC Guidelines](https://www.escardio.org/static-file/Escardio/Guidelines/publications/NO%20REFguidelines-afib-slides-2010.pdf)

2. **APACHE II Score (중환자실 사망률 예측)**
   - 12개 생리학적 변수 + 나이 + 만성 질환
   - 통계적 회귀 모델 (Logistic Regression)
   - 1985년부터 사용[@Knaus1985APACHE]
   - 규제: Clinical Decision Support로 분류 → 의료기기 아님

3. **Wells Score (심부정맥혈전증 확률)**
   - 임상 징후 기반 점수표
   - 사전 검사 확률(Pre-test Probability) 계산
   - 규제: 정보 제공 도구 → 510(k) 면제

**승인이 쉬운 이유:**

- **완벽한 투명성**: 모든 계산 과정 추적 가능
- **임상적 검증**: 수십 년의 임상 사용 데이터
- **결정론적**: 동일 입력 → 동일 출력 100% 보장
- **문서화 용이**: 규칙 자체가 검증 문서

복잡한 의료 AI를 개발 중이라면, **Phase 1에서 Rule-based 버전을 먼저 승인**받는 전략을 고려하라. 예:
- Phase 1: "혈압 140 이상이면 고혈압" (Rule-based, 빠른 승인)
- Phase 2: 딥러닝 모델 추가 (업그레이드 승인)

### 전통 ML: 이미 주류 의료기기

#### 승인 통계와 주요 알고리즘

* 2025년 12월 기준, FDA 승인 AI 의료기기의 **약 65%가 전통 ML 기반**이다[@FDAAIML2025]. 
* Logistic Regression, SVM, Random Forest, XGBoost, Gradient Boosting 등이 대표적이다.

**주요 승인 사례:**

1. **Viz.ai Contact (뇌졸중 CT 분석, 2018)**[@FDA2018Viz]
   - **알고리즘**: Random Forest + CNN (하이브리드)
   - **기능**: 대혈관 폐색(LVO) 자동 감지 → 뇌졸중 전문의에게 SMS 알림
   - **Class**: II (510(k) K173542)
   - **임상 검증**: 
     - Sensitivity 94%, Specificity 96%
     - Door-to-treatment time 96분 → 52분 단축
   - **핵심 전략**: "의사를 대체하지 않고 알림만 제공" → Class II 유지

2. **EarlySense InSight (패혈증 조기 경고, 2020)**[@FDA2020EarlySense]
   - **알고리즘**: Random Forest
   - **입력**: 10개 생체 신호 (심박수, 호흡수, 혈압 등)
   - **출력**: 패혈증 발생 위험도 (0-100%)
   - **Class**: II (510(k) K201884)
   - **검증**: 3개 병원, 12,000명 환자 데이터
   - **Feature Importance**: SHAP 사용하여 각 변수 기여도 시각화

3. **Dascena Previse Sepsis (2022)**
   - **알고리즘**: Gradient Boosting Machine (XGBoost 변형)
   - **기능**: 패혈증 4시간 전 예측
   - **Class**: II (510(k) K213582)
   - **특징**: 
     - Feature engineering 상세 문서화
     - 인종/성별별 Subgroup Analysis 제출
     - Prospective study (전향적 연구) 포함

4. **CureMetrix cmAssist (유방 X-ray 분석)**
   - **알고리즘**: SVM + Hand-crafted Features
   - **기능**: 유방암 의심 병변 표시
   - **Class**: II (510(k) K183282)
   - **Reader Study**: 방사선 전문의 12명 vs AI 비교

**전통 ML의 승인 전략:**

1. **Feature Engineering 문서화**
   - 어떤 변수를 왜 선택했는지 임상적 근거 제시
   - 예: "HbA1c는 당뇨병 진단의 Gold Standard이므로..."

2. **Explainability 강화**
   - **SHAP (SHapley Additive exPlanations)**: 각 예측에 대한 변수 기여도
   - **Feature Importance**: 모델 전체에서 중요한 변수 순위
   - **Partial Dependence Plot**: 변수 변화에 따른 예측 변화

3. **Validation Protocol**
   - **K-fold Cross-validation**: 데이터 부족 시
   - **External Validation**: 다른 병원 데이터로 검증 (FDA 강력 선호)
   - **Temporal Validation**: 시간에 따른 성능 변화 분석

4. **Subgroup Analysis (편향 검증)**
   - 인종별 성능 (백인, 흑인, 아시아인, 히스패닉 등)
   - 성별 성능
   - 연령대별 성능
   - FDA는 "공정성(Fairness)" 문서화 요구[@FDA2021Bias]

**주로 아래와 같은 요소들을 정량화:**

- **Confidence Score 제공**: "이 예측의 신뢰도는 85%"
- **Out-of-Distribution 감지**: "이 환자는 학습 데이터 범위 밖입니다"
- **Uncertainty Quantification**: 베이지안 방법으로 예측 불확실성 정량화

### 딥러닝: 조건부 승인의 시대

* **가장 큰 오해**: "딥러닝은 블랙박스라 의료기기로 승인 안 됨"
* **진실**: **딥러닝은 이미 300건 이상 FDA 승인됨** (2025년 12월 기준, 전체의 약 32%)[@FDAAIML2025]. 
* 단, **조건이 까다롭다**.

#### 승인된 딥러닝 사례 심층 분석

**1. IDx-DR (Digital Diagnostics, 2018) - 역사적 의미**[@Abramoff2018IDxDR]

- **최초의 자율 진단 AI**: 의사 확인 없이 진단 가능
- **알고리즘**: CNN (Convolutional Neural Network)
- **기능**: 당뇨망막병증 자동 진단
- **Class**: II (De Novo DEN180001)
- **입력**: 망막 안저 촬영 이미지 (Topcon NW400)
- **출력**: "More than mild diabetic retinopathy" 유/무

**임상 검증 프로토콜:**[@Abramoff2018IDxDR]
- **연구 설계**: Prospective, multi-site (10개 1차 진료소)
- **환자 수**: 900명
- **Gold Standard**: 안과 전문의 3명의 합의 진단
- **성능**:
  - Sensitivity: 87.2% (95% CI: 82.7-91.0%)
  - Specificity: 90.7% (95% CI: 88.3-92.7%)
- **실패 케이스 분석**: 저화질 이미지 10.8%는 "판독 불가" 처리

**FDA 승인 조건:**
- 특정 카메라만 사용 (Topcon NW400)
- 50세 이상 당뇨병 환자만
- 망막 수술 이력 있으면 제외
- 연 1회 성능 모니터링 보고서 제출

**Explainability 전략:**
- **Grad-CAM**: 병변 위치 히트맵 표시
- **임상적 설명**: "출혈, 미세동맥류, 삼출물 패턴 감지"
- **실패 모드 정의**: "화질 불량 시 자동 거부"

**2. Viz.ai ContaCT (2020) - 응급 알림 시스템**[@FDA2020Viz]

- **알고리즘**: CNN + 3D 영상 처리
- **기능**: 뇌 CT에서 대혈관 폐색(LVO) 감지 → 뇌졸중 전문의에게 SMS
- **Class**: II (510(k) K192287)
- **핵심 전략**: "진단이 아닌 트리아지(Triage)"로 포지셔닝

**임상적 영향:**
- Time to treatment: **96분 → 52분** (44분 단축)
- 결과: 환자 예후 개선, 병원 수익 증가

**규제 전략:**
- "의사에게 알림만 제공" → Class II 유지
- "최종 진단은 의사가 수행" 명시
- 알림 후 의사가 확인하지 않으면 책임은 의사에게

**3. Arterys Cardio DL (2017) - 최초의 클라우드 기반 AI**[@FDA2017Arterys]

- **알고리즘**: CNN (U-Net 변형)
- **기능**: 심장 MRI에서 좌심실 용적 자동 측정
- **Class**: II (De Novo DEN170022)
- **특징**: **최초의 클라우드 기반 의료기기** (HIPAA 준수)

**기술적 혁신:**
- 4D Flow MRI 혈류 분석
- 자동 세그멘테이션 (심장 벽 경계 추출)
- 기존 수동 측정 대비 시간 90% 단축

**검증 방법:**
- Gold Standard: 방사선 전문의의 수동 측정
- Bland-Altman Plot: 측정 일치도 분석
- Inter-observer Variability보다 낮은 오차

**4. Paige Prostate (2024) - 병리 진단 AI**[@FDA2024Paige]

- **알고리즘**: Transformer 기반 Vision Model
- **기능**: 전립선암 병리 슬라이드 분석
- **Class**: II (De Novo DEN210034)
- **최신 사례**: 대규모 병리 AI 승인

**임상 검증 규모:**
- **16개 기관**, 2,100명 환자
- **Reader Study**: 병리학자 16명 vs AI
- **결과**: 
  - AI 보조 시 민감도 **7.1% 향상**
  - Gleason score 정확도 향상
  - 검사 시간 30% 단축

**Whole Slide Imaging (WSI)의 규제 도전:**
- 슬라이드 스캔 → 디지털 이미지 → AI 분석
- 각 단계마다 품질 관리 필요
- FDA는 WSI 장비와 AI를 별도 승인

#### 딥러닝 승인의 공통 조건

모든 승인된 딥러닝 AI는 다음을 만족한다:

**1. Locked Model (고정 모델)**
- 승인 후 가중치 변경 금지
- 버전 관리 엄격 (v1.0, v1.1 등)
- 업데이트 시 재승인 필요

**2. 입력 제한 (Narrow Scope)**
- 특정 장비: "Topcon NW400만"
- 특정 포맷: "DICOM, 512x512 이상"
- 특정 환자군: "50세 이상, 당뇨병 있음"
- 범위 밖 입력 → 자동 거부

**3. 대규모 임상 검증**
- 최소 수백~수천 명 환자
- 다기관 연구 (Multicenter)
- Prospective study 선호
- Gold Standard와 비교

**4. Explainability 기법**
- **Grad-CAM**: 어디를 봤는지
- **Attention Map**: 무엇에 집중했는지
- **Saliency Map**: 중요한 픽셀 표시
- **임상적 해석**: "출혈 패턴 감지"

**5. Failure Mode 정의**
- "저화질 이미지 → 거부"
- "환자 움직임 → 경고"
- "Out-of-distribution → 사용 금지"

**6. Post-Market Surveillance**
- 승인 후 성능 모니터링
- 실패 사례 보고 의무
- 연간 안전성 보고서


#### 딥러닝 승인 전략 요약

| 단계 | 핵심 작업 | FDA 요구사항 |
|-----|---------|------------|
| **1. 개발** | Dataset 수집 | 다양성, 대표성, IRB 승인 |
| **2. 학습** | 모델 훈련 | Train/Val/Test 명확히 분리 |
| **3. 검증** | 성능 평가 | External validation 필수 |
| **4. 해석** | Explainability | Grad-CAM, Attention 등 |
| **5. 임상시험** | Reader Study | 전문의 vs AI 비교 |
| **6. 문서화** | 510(k) 작성 | 2,000페이지 이상 |
| **7. 승인 후** | 모니터링 | 연간 보고서, 실패 추적 |

**실전 조언:**

- **작게 시작**: "모든 암 진단"이 아닌 "전립선암만"
- **비교 대상 찾기**: 510(k)는 Predicate device 필요
- **의사 협력**: 알고리즘 설계부터 의사 참여 필수
- **규제 컨설팅**: FDA Pre-Submission Meeting 활용 (수수료 $4,000)

**비용 현실:**

- 딥러닝 의료기기 승인 비용: **$500만 ~ $2,000만**
- 기간: 2-5년
- 임상시험 비용이 대부분 (80%)

> 딥러닝은 이미 의료기기로 승인 가능하다. 단, **엄청난 비용과 시간, 그리고 엄격한 조건**이 따른다.

## 생성형 AI의 규제 장벽 - 왜 승인이 거의 불가능한가

### 현실: 의료기기로 승인된 LLM은 거의 없다

* 2025년 12월 기준, **의료 진단/치료 의사결정에 직접 사용되는 생성형 AI 의료기기는 전 세계적으로 손에 꼽을 정도**다[@FDA2025FoundationModel]. 
* FDA AI 의료기기 목록 945건 중 LLM 기반은 15건 미만 (1.6%)이며, 대부분은:

- 진료 기록 자동 요약 (행정 업무)
- 환자 교육 자료 생성 (정보 제공)
- 의사용 참고 자료 검색 (CDS로 면제)

**핵심**: 진단·치료 의사결정을 하는 생성형 AI는 **사실상 승인 불가능**하다.

### 4가지 구조적 장벽

#### 비결정성 (Non-deterministic Output)

* **문제의 본질:** LLM은 본질적으로 **확률적(Stochastic)** 시스템이다. 동일한 입력에 대해 다른 출력을 생성할 수 있다.
* 실제로, chat gpt에 같은 질문을 2번 입력하면 답변이 다르게 나오는 것을 확인할 수 있다.
* **규제 관점의 문제:** FDA는 **"동일 입력 → 동일 출력"**을 요구한다[@FDA2021AIML]. 의료기기는 재현 가능(reproducible)해야 한다. 그래야 임상시험 결과가 의미 있다.
* **Temperature = 0으로 설정하면?** 여전히 완벽한 결정론을 보장하지 못한다:
- 하드웨어 부동소수점 연산 차이
- 병렬 처리 순서 차이
- API 버전 업데이트 영향

**FDA의 입장:**

> "비결정론적 출력은 임상시험 프로토콜 설계를 불가능하게 만든다. 어떤 버전의 모델을 승인하는가? 내일 다른 답을 한다면?" - FDA Software as a Medical Device Working Group, 2024


#### 근거 추적 불가 (Lack of Traceability)

* **문제의 본질:** LLM이 **왜 그런 답을 했는지** 재구성하기 거의 불가능하다.
* **구체적 사례:**  
  - **질문**: "55세 남성, 공복혈당 132, HbA1c 6.7%. 당뇨병인가?"
  - **LLM 답변**: "당뇨병으로 진단됩니다. 메트포르민 500mg 하루 2회 처방 권장합니다."
* **규제자의 질문:**
  1. "왜 126이 아닌 132가 당뇨병인가?" (기준은 126)
  2. "어떤 가이드라인을 참조했는가?"
  3. "메트포르민 용량 근거는?"
  4. "환자의 신기능은 확인했는가?" (메트포르민 금기)
* **LLM의 답변 불가:**
  - 학습 데이터의 어느 부분이 영향을 미쳤는지 모름
  - 토큰 생성은 확률적 → 특정 근거 추출 불가
  - Attention weight는 있지만 인과관계 설명 안 됨
  - 또는 잘몬된 정보를 주는 Hallucination 발생
* Attention은 "어디를 봤는가"만 보여줄 뿐, "왜"를 설명하지 못한다.
* **FDA의 요구사항:** EU MDR 2017/745 Annex I[@EUMDR2017]은 명시한다:

> "제조업체는 소프트웨어의 의사결정 프로세스를 문서화하고, 각 출력의 근거를 추적 가능하게 해야 한다." LLM은 이를 만족시킬 수 없다.

#### 학습 데이터 불명확 (Data Provenance Issues)

* **문제의 본질:** LLM의 학습 데이터는 **출처, 품질, 편향이 불명확**하다.

1. **의료 데이터 포함 여부 불명**
   - GPT-4 학습 데이터: 공개 안 됨
   - 의료 논문이 몇 % 포함되었는가?
   - 어떤 가이드라인이 포함되었는가?

2. **오래된 정보 문제**
   - 학습 데이터 Cutoff: 2023년 4월 (GPT-4 기준)
   - 의료 가이드라인은 매년 업데이트
   - 예: 2024년 고혈압 기준 변경 → LLM은 구 기준 사용

3. **저작권 문제**
   - UpToDate, PubMed 논문이 무단 학습되었는가?
   - 의료 데이터베이스 라이선스 위반 가능성
   - 규제 승인 시 학습 데이터 목록 제출 필수 → LLM 불가능

4. **편향 문제**
   - 영어권 의료 정보 편중
   - 특정 인종/성별 데이터 부족
   - 희귀 질환 정보 부족

**FDA Bias 가이드라인 (2021):**[@FDA2021Bias]

> "AI 의료기기는 인종, 성별, 연령별 성능을 입증해야 한다. 학습 데이터의 인구통계적 분포를 명시하라."

**LLM의 불가능:**
- GPT 학습 데이터 인구통계: 비공개
- 의료 데이터 비율: 비공개
- Subgroup 성능 검증: 불가능

**실제 사례 - 편향 문제:**

연구 결과[@Omiye2023LLMBias]:
- LLM이 흑인 환자에게 진통제를 덜 처방
- 여성 환자의 심장마비 증상을 "불안증"으로 오진
- 이유: 학습 데이터의 역사적 편향 반영

#### 의료 책임 귀속 불가 (Liability Issues)

* **문제의 본질:** LLM이 오진했을 때 **누가 책임지는가?**
* **복잡한 책임 사슬:**

```
환자 ← 의사 ← 병원 ← AI 앱 개발사 ← LLM 제공사 (OpenAI/Anthropic) ← 학습 데이터 제공자
```

* **가상 시나리오:**
  - **사건**: LLM이 "메트포르민 처방"을 권장 → 환자의 신장 기능 저하 간과 → 젖산산증 발생 → 사망
  - **책임 소재**:
    1. **의사**: "LLM을 맹신했다"
    2. **병원**: "AI 검증 없이 도입"
    3. **AI 앱 개발사**: "우리는 API만 호출, LLM은 OpenAI 책임"
    4. **OpenAI**: "우리는 도구만 제공, 의료 판단은 의사 책임"
    5. **학습 데이터 제공자**: "우리는 단순 데이터 제공"

**현행법의 공백:**

미국 제조물책임법(Product Liability):
- 전통 의료기기: 제조사 책임 명확
- LLM: "제품"인가 "서비스"인가? 불명확
- API 형태 제공 시 책임 경계 모호

**FDA의 입장:**

FDA는 2025년 현재 **"Foundation Model 기반 의료기기의 책임 소재"** 가이드라인을 작성 중이지만 발표되지 않았다[@FDA2025FoundationModel].

**보험 문제:**

- 의료 과오 보험: LLM 오류 커버 안 함
- LLM 제공사: 의료 책임 면책 조항
- 결국 의사/병원만 책임

### Hallucination - 의료 AI의 치명적 결함

* **정의:** Hallucination은 LLM이 **존재하지 않는 정보를 그럴듯하게 생성**하는 현상이다.
* **의료에서의 실제 사례:**

1. **가짜 논문 인용**
   - 질문: "당뇨병 치료 최신 연구는?"
   - LLM 답변: "Smith et al. (2024) NEJM 연구에 따르면..."
   - 문제: 해당 논문 존재하지 않음

2. **약물 상호작용 오류**
   - 질문: "와파린 + 아스피린 병용 가능?"
   - LLM: "안전합니다. 용량 조절 불필요"
   - 진실: 출혈 위험 증가, 엄격한 모니터링 필수

3. **용량 오류**
   - 질문: "소아 아목시실린 용량"
   - LLM: "50mg/kg/day"
   - 진실: 40mg/kg/day (10mg 차이 = 잠재적 독성)

**발생률:**

연구 결과[@Ji2023Hallucination]:
- 의료 질문에서 Hallucination 발생률: **8-15%**
- 문제: **어느 8%인지 알 수 없음**
- 일관성 없음: 같은 질문도 다른 답

**FDA의 입장:**

> "Hallucination rate를 정량화할 수 없다면 임상시험 설계가 불가능하다. False Negative Rate를 계산할 수 없으면 위험도 평가가 불가능하다." — FDA Digital Health Center, 2024

### 현재 생성형 AI가 허용되는 영역

생성형 AI는 **의료기기가 아닌** 다음 영역에서만 활용된다:

#### Administrative (행정 업무)

- **진료 기록 자동 요약**
  - 예: Nuance DAX Copilot
  - 기능: 의사-환자 대화 → SOAP Note 생성
  - 규제 우회: "의사가 최종 검토·수정 필수" → 비의료기기

- **청구 코드 자동 제안**
  - ICD-10, CPT 코드 추천
  - 의사가 최종 승인

- **문서 자동화**
  - 퇴원 요약 작성
  - 의뢰서 초안 생성

**핵심 조건:**
- "초안(Draft)" 생성만
- 의사가 100% 검토
- 환자 진료에 직접 영향 없음


#### Clinical Decision Support – 참고용

**허용 조건:**[@FDA2026CDS]

FDA CDS 가이던스 (2026)의 면제 조건:
1. **정보 출처 명시**: "UpToDate 2024년판 참조"
2. **의사 독립 판단**: "참고용이며 최종 결정은 의사"
3. **개별 환자 데이터 미사용**: 일반 가이드라인만

**실제 사례: UpToDate + ChatGPT**
- UpToDate가 GPT-4 통합 검토 중
- 하지만 "진단 도구"가 아닌 "검색 도구"로만 제한
- 의사가 읽고 판단

#### 환자 커뮤니케이션

- **건강 정보 챗봇**
  - "당뇨병이란?"
  - "혈압약 부작용은?"
  - 조건: 일반 정보만, "의사와 상담 필요" 경고

- **환자 교육 자료 생성**
  - 진단 설명
  - 치료 계획 설명
  - 의사가 검토 후 전달

- **예약/문의 응대**
  - "언제 병원 와야 하나요?"
  - 진단/치료 조언 제외

**위험 사례:**

- **Babylon Health (2020)**
  - AI 챗봇이 환자 증상 평가
  - "응급실 갈 필요 없음" 판단
  - 실제: 심근경색 → 사망
  - 결과: 소송, 규제 강화

### 생성형 AI의 미래 제한적 승인 시나리오

#### Hybrid Approach (하이브리드)

**컨셉**: LLM + Rule-based 검증
**장점**: Rule로 명백한 오류 차단  
**단점**: 여전히 LLM의 비결정성 문제

#### Narrow Domain Fine-tuning

**컨셉**: 특정 질병/절차만 하는 전문 LLM
  - **예**: "대장암 병기 결정만" 하는 LLM
  - **학습 데이터**: NCCN 가이드라인 + 병리 보고서만
  - **출력 형식**: Stage I/II/III/IV (정형화)
  - **검증**: 기존 병기 결정과 비교
**장점**: 범위 제한으로 검증 가능  
**단점**: 여전히 Hallucination 위험

#### Human-in-the-Loop (HITL)

**컨셉**: 모든 출력을 의사가 검토

```
환자 → LLM → (초안) → 의사 검토 → 최종 결정
```

**규제 포지셔닝**: "의사의 생산성 도구"
**문제**:
- 의사가 LLM을 맹신하면?
- "Automation Bias": 사람은 AI 결과를 과신하는 경향

**연구 결과[@Gaube2023AutomationBias]:**
- 의사 + AI: 틀린 AI 제안을 **88% 수용**
- AI 없는 의사: 같은 문제를 **72% 정답**
- 결론: AI가 오히려 성능 저하

### FDA의 2025년 입장 - Foundation Model 규제

**발표 예정 (2025년 상반기):**[@FDA2025FoundationModel]

1. **Foundation Model Transparency Label**
   - "이 제품은 LLM 기반입니다" 명시
   - 사용자가 인지하도록

2. **Regulatory Sandbox**
   - 제한된 환경에서 LLM 실험 허용
   - 단, 의료 의사결정에서 제외

3. **Post-Market Surveillance 강화**
   - 모든 LLM 출력 로그 저장
   - Hallucination 발생 시 즉시 보고

**결론적 입장:**

> "현재 기술로는 생성형 AI를 진단/치료 의사결정 의료기기로 승인하기 어렵다. 추가 연구와 기술 발전이 필요하다." - FDA Commissioner Statement, 2024

### 생성형 AI를 의료에 쓰려면?

1. **의료기기 아닌 방향으로 포지셔닝**
   - "진단 도구" ❌
   - "의사 참고 자료 생성 도구" ✅

2. **Human-in-the-Loop 필수**
   - 모든 출력을 의사가 검토
   - "초안" 개념 명확히

3. **책임 경계 명확히**
   - 이용약관에 "의료 조언 아님" 명시
   - 의료 과오 책임은 사용자(의사)에게

4. **규제 샌드박스 활용**
   - EU AI Act Sandbox (유럽)
   - FDA Pre-Cert Program (미국, 제한적)

5. **단기적으로는 행정 업무에 집중**
   - 진료 기록 요약
   - 문서 자동화
   - 청구 코드 제안

**장기 전망 (5-10년):**

생성형 AI가 의료기기로 승인되려면:
- Hallucination 해결
- 완벽한 결정론 보장
- 학습 데이터 투명성
- 책임 소재 법적 정리

현재 기술로는 **거의 불가능**하다.

## 참고문헌 및 추가 자료

### 주요 규제 문서

- **FDA AI/ML 의료기기 목록** (945+ 제품, 2025): <https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-enabled-medical-devices>
- **FDA Clinical Decision Support Guidance** (2026): <https://www.fda.gov/regulatory-information/search-fda-guidance-documents/clinical-decision-support-software>
- **FDA AI/ML Action Plan** (2021): <https://www.fda.gov/media/145022/download>
- **EU MDR 2017/745**: <https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:02017R0745-20250110>
- **EU AI Act** (2024): <https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32024R1689>

### 학술 논문

1. **Abramoff, M. D., et al. (2018).** "Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic retinopathy." *NPJ Digital Medicine*, 1(39). <https://doi.org/10.1038/s41746-018-0040-6>

2. **Knaus, W. A., et al. (1985).** "APACHE II: A severity of disease classification system." *Critical Care Medicine*, 13(10), 818-829.

3. **Omiye, J. A., et al. (2023).** "Large language models propagate race-based medicine." *NPJ Digital Medicine*, 6(1), 195. <https://doi.org/10.1038/s41746-023-00939-z>

4. **Ji, Z., et al. (2023).** "Survey of hallucination in natural language generation." *ACM Computing Surveys*, 55(12), 1-38.

5. **Gaube, S., et al. (2023).** "Non-task expertise influences on algorithmic decision-making." *Nature Human Behaviour*, 7(3), 403-411.

### 실제 승인 사례 문서

- **IDx-DR De Novo**: <https://www.accessdata.fda.gov/cdrh_docs/reviews/DEN180001.pdf>
- **Viz.ai 510(k)**: <https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?ID=K192287>
- **Paige Prostate De Novo**: <https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/denovo.cfm?ID=DEN210034>

### 추가 리소스

- **FDA Good Machine Learning Practice**: <https://www.fda.gov/medical-devices/software-medical-device-samd/good-machine-learning-practice-medical-device-development-guiding-principles>
- **FDA Bias in AI**: <https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-software-medical-device>
- **IMDRF ML Guidelines**: <https://www.imdrf.org/documents/machine-learning-enabled-medical-devices-key-terms-and-definitions>
