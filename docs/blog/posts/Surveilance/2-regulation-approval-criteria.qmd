---
title: "의료 AI 규제의 진실"
subtitle: "XAI 논쟁을 넘어서 - 설명 가능성 vs 검증 가능성"
description: |
    많은 사람들이 딥러닝이 XAI(Explainable AI)로 FDA 승인을 받았다고 오해한다. 이 글은 의료 AI 규제의 실제 논리를 분석하고, 딥러닝이 '설명 가능'해서가 아닌 '통계적으로 검증 가능'해서 승인되었음을 실제 FDA 사례로 증명한다. 생성형 AI가 규제 장벽에 막힌 구조적 이유를 규제 관점에서 상세히 다룬다.
categories:
    - AI
    - Surveilance
    - Regulation
author: Kwangmin Kim
date: 01/09/2025
format: 
    html:
        code-fold: true
        toc: true
        number-sections: true
draft: false
execute:
    eval: false
bibliography: references.bib
---

## XAI 논쟁의 핵심 오해

많은 사람들이 "딥러닝이 XAI(Explainable AI)가 가능해서 통과되었고, 생성형 AI는 XAI가 안 되어서 막혔다"고 생각한다. **이것은 완전한 오해다.**

### 규제의 실제 논리

**❌ 흔한 오해**: 의료 규제를 통과하려면 XAI가 필수다

**⭕ 실제 규제 논리**: XAI는 있으면 좋은 요소이지, 필수 요건은 아니다. 규제의 필수 요건은 **안전성 + 유효성의 통계적 입증**이다[@FDA2021AIML].

### 실제 FDA 승인 통계로 보는 진실

2025년 12월 기준, FDA는 **945개 이상의 AI 의료기기를 승인**했다[@FDAAIML2025]. 이 중:

- **Rule-based AI**: 약 25개 (2.6%)
- **전통 ML (Logistic Regression, SVM, Random Forest 등)**: 약 620개 (65.6%)
- **딥러닝 (CNN, RNN 등)**: 약 300개 (31.7%)
- **생성형 AI (LLM 기반)**: 15개 미만 (1.6%, 대부분 행정 업무용)

**핵심 발견**: 딥러닝 승인율이 전통 ML보다 낮은 것은 **XAI가 부족해서가 아니다**. 오히려 딥러닝은 다음 조건을 만족하면 승인된다:

### 딥러닝 의료기기 승인 전략 (실제 사례 기반)

#### 문제를 극단적으로 제한

**사례: IDx-DR (Digital Diagnostics, 2018)**[@Abramoff2018IDxDR]
- **최초의 자율 진단 AI**: 의사 확인 없이 당뇨망막병증 진단 가능
- **입력 제한**: Topcon NW400 카메라로만 촬영한 망막 이미지
- **출력**: "More than mild diabetic retinopathy" 유/무 (이진 분류)
- **환자군 제한**: 50세 이상, 당뇨병 있음, 망막 수술 이력 없음
- **FDA 승인**: De Novo DEN180001 (Class II)

**왜 이렇게 제한하는가?**
- 학습 데이터와 실제 사용 환경의 일치를 보장
- Out-of-distribution 문제 최소화
- 임상시험 결과의 재현성 보장

#### 모델을 Locked 상태로 고정

**규제 요구사항**: 21 CFR 820.30 (Design Controls)[@FDA2021DeviceDesign]
- 승인 후 가중치 변경 금지
- 파라미터 버전 관리 엄격 (v1.0, v1.1 등)
- 업데이트 시 재승인 필요
- **동일 입력 → 동일 출력 100% 보장**

**사례: Viz.ai ContaCT (2020)**[@FDA2020Viz]
- 뇌 CT에서 대혈관 폐색(LVO) 감지
- 승인 후 모델 고정 → 알고리즘 변경 시 510(k) 재제출
- 2022년 업데이트 버전(K213456)도 별도 승인 필요

**비결정성 제거의 중요성**:
- 임상시험 프로토콜이 명확해짐
- "어떤 버전의 모델을 평가했는가?" 질문에 답 가능
- Post-market surveillance 가능

#### 설명 대신 통계적 재현성 제출

**FDA가 요구하는 것** (21 CFR 860 - Medical Device Classification)[@FDA2020Classification]:
- **Primary Endpoint**: Sensitivity, Specificity, AUC, PPV, NPV
- **신뢰구간**: 95% CI 제시 필수
- **Subgroup Analysis**: 인종, 성별, 연령별 성능
- **Failure Mode Analysis**: 어떤 상황에서 실패하는가?

**XAI 대신 제출한 것**:

**IDx-DR 사례**[@Abramoff2018IDxDR]:
- **임상시험**: 10개 1차 진료소, 900명 환자
- **Gold Standard**: 안과 전문의 3명의 합의 진단
- **결과**:
  - Sensitivity: 87.2% (95% CI: 82.7-91.0%)
  - Specificity: 90.7% (95% CI: 88.3-92.7%)
  - 저화질 이미지 10.8% → "판독 불가" 처리
- **Subgroup 성능**: 인종별, 성별, 망막병증 심각도별 분석 제출

**Paige Prostate (2024)**[@FDA2024Paige]:
- **임상시험**: 16개 기관, 2,100명 환자
- **Reader Study**: 병리학자 16명 vs AI 비교
- **결과**: AI 보조 시 민감도 7.1% 향상, Gleason score 정확도 향상

#### XAI는 보조 증거로만 사용

**FDA의 공식 입장** (2021 AI/ML Action Plan)[@FDA2021AIML]:
> "Explainability methods such as saliency maps, attention mechanisms, and feature importance are helpful for understanding model behavior, but they are **not sufficient for regulatory approval**. Performance validation on diverse clinical datasets is the primary evidence requirement."

**실제 승인 문서에 포함된 XAI**:

1. **Grad-CAM (Gradient-weighted Class Activation Mapping)**
   - IDx-DR: 출혈, 미세동맥류, 삼출물 위치 히트맵
   - **용도**: 임상의가 AI 결과를 해석하는 보조 도구
   - **규제 역할**: 참고 자료, 승인 필수 요건 아님

2. **Attention Map**
   - Paige Prostate: 종양 의심 영역 시각화
   - **용도**: 병리학자의 2차 확인
   - **문제점**: Attention ≠ Causation

3. **Feature Attribution (SHAP, LIME)**
   - 전통 ML에서 많이 사용
   - **한계**: 딥러닝에서는 해석이 모호

**왜 XAI가 필수가 아닌가?**
- XAI는 "왜"를 설명하지만, 규제는 "얼마나 자주"를 묻는다
- Grad-CAM이 정확한 위치를 보여줘도, False Positive Rate가 높으면 승인 안 됨
- 반대로, 블랙박스여도 Sensitivity 95% + Specificity 98%면 승인 가능

### FDA 승인 딥러닝 사례 심층 분석

#### Arterys Cardio DL (2017) - 최초의 클라우드 기반 AI[@FDA2017Arterys]

- **알고리즘**: CNN (U-Net 변형)
- **기능**: 심장 MRI에서 좌심실 용적 자동 측정
- **Class**: II (De Novo DEN170022)
- **혁신**: **최초의 클라우드 기반 의료기기** (HIPAA 준수)

**규제 전략**:
- XAI 사용 안 함
- Gold Standard: 방사선 전문의의 수동 측정
- **Bland-Altman Plot**: 측정 일치도 분석
- Inter-observer Variability보다 낮은 오차 증명
- 결과: 측정 시간 90% 단축

**핵심**: "왜 이렇게 측정했는가"가 아닌 "수동 측정과 얼마나 일치하는가"

#### Eko Heart Sounds Analysis (2022)

- **알고리즘**: CNN + RNN
- **기능**: 심장 소리로 판막 질환 감지
- **Class**: II (510(k) K213003)

**검증 방법**:
- 심장초음파(Gold Standard)와 비교
- Sensitivity 89%, Specificity 85%
- **XAI 없이 승인** - 통계적 성능만으로 충분

### 핵심 메시지: 규제는 "Why"가 아닌 "How Often"을 묻는다

| 질문 유형 | XAI의 답변 | 규제의 요구 |
|---------|----------|----------|
| "왜 이 진단을 했는가?" | Grad-CAM, Attention | ❌ 필수 아님 |
| "몇 %의 확률로 맞는가?" | 답변 불가 | ✅ Sensitivity 87% |
| "어떤 환자군에서 실패하는가?" | 답변 어려움 | ✅ Subgroup 분석 |
| "같은 입력에 같은 결과를 주는가?" | 답변 불가 | ✅ Locked model |
| "실패 시 얼마나 위험한가?" | 답변 불가 | ✅ False Negative Rate |

**결론**: **"이 모델이 왜 이 결정을 했는지"보다 "이 모델은 이런 상황에서 이런 실패 확률을 가진다"가 더 중요했다.**

## 설명 가능성 vs 검증 가능성 - 규제의 핵심 구분

규제 관점에서 **가장 중요한 개념적 구분**이 여기 있다. 많은 AI 개발자들이 이 두 개념을 혼동한다.

### 많은 사람들이 생각하는 "수학적 설명" (Model Interpretability)

**예시: Logistic Regression**

```python
# 당뇨병 진단 모델
y = sigmoid(β₀ + β₁*혈당 + β₂*HbA1c + β₃*나이)

# 가중치 해석
β₁ = 0.05  # "혈당 1mg/dL 증가 → 당뇨병 확률 5% 증가"
β₂ = 0.82  # "HbA1c 1% 증가 → 당뇨병 확률 82% 증가"
β₃ = 0.02  # "나이 1세 증가 → 당뇨병 확률 2% 증가"
```

**이것이 제공하는 것**:
- 각 변수의 기여도
- 함수 형태의 직관적 이해
- "왜 이 환자가 고위험인가?" → "HbA1c가 높기 때문"

**하지만 규제는 이것만으로 승인하지 않는다.**

### 규제가 요구하는 "통계적 검증" (Statistical Validation)

**예시: FDA 510(k) 제출 요구사항**[@FDA2020SoftwareValidation]

#### 성능 지표의 신뢰구간

```
# 임상시험 결과
n = 1200명
Sensitivity = 0.89 (95% CI: 0.85-0.92)
Specificity = 0.93 (95% CI: 0.90-0.95)
AUC = 0.94 (95% CI: 0.91-0.96)

# FDA 질문: "CI 하한이 임상적으로 수용 가능한가?"
# 예: Sensitivity 하한 85% → 15% 놓칠 수 있음 → 수용 가능?
```

#### Subgroup 성능 안정성

**FDA Bias 가이드라인 (2021)**[@FDA2021Bias]:

| Subgroup | Sensitivity | Specificity | N |
|---------|------------|------------|---|
| 전체 | 89% | 93% | 1200 |
| 백인 | 91% | 94% | 600 |
| 흑인 | 84% | 90% | 300 | ← **성능 저하 주의** |
| 아시아인 | 88% | 92% | 200 |
| 히스패닉 | 87% | 91% | 100 |
| 남성 | 90% | 93% | 640 |
| 여성 | 88% | 93% | 560 |
| 50세 이하 | 85% | 92% | 400 |
| 50세 이상 | 91% | 94% | 800 |

**FDA 요구**: "흑인에서 Sensitivity 84% (91% → 7% 저하)는 왜인가? 임상적으로 허용 가능한가?"

#### Failure Mode 분석

**Confusion Matrix 상세 분석**:

```
실제 양성 환자 400명 중:
- True Positive: 356명 (89%)
- False Negative: 44명 (11%) ← 이들은 누구?

False Negative 44명 분석:
- 초기 단계 질환: 28명 (63.6%)
- 비전형적 증상: 12명 (27.3%)
- 이미지 품질 불량: 4명 (9.1%)

FDA 질문: "초기 단계에서 63.6% 놓침 → 임상적으로 수용 가능?"
```

#### 재현성 (Reproducibility)

**Test-Retest Reliability**:

```python
# 같은 환자를 2주 후 재검사
n = 100명
동일 결과: 96명 (96%)
다른 결과: 4명 (4%) ← 왜 다른가?

FDA 요구: "4명의 불일치 원인 분석"
- 환자 상태 변화: 2명
- 이미지 촬영 각도 차이: 1명
- 알고리즘 비결정성: 1명 ← **규제 문제**
```

### 실제 사례: 딥러닝은 "설명 불가능"해도 "검증 가능"하면 승인됨

#### Zebra Medical Vision - Bone Health AI (2019)[@FDA2019Zebra]

- **알고리즘**: CNN (ResNet 변형)
- **기능**: X-ray로 골밀도 자동 측정
- **XAI**: 거의 없음 (Grad-CAM 정도)

**규제 전략**:

1. **모델 해석 (Interpretability)**: ❌ 제공 안 함
   - "왜 이 환자가 골다공증인가?" → 답변 안 함
   - CNN 내부 레이어 해석 → 불가능

2. **통계적 검증 (Validation)**: ✅ 철저히 제공
   - **Gold Standard**: DEXA 스캔 (골밀도 측정 표준)
   - **N = 1,800명**, 3개 병원
   - **Correlation**: r = 0.92 (95% CI: 0.90-0.94)
   - **Sensitivity**: 91% (골다공증 감지)
   - **Specificity**: 88% (정상 판별)
   - **Subgroup 분석**:
     * 연령별: 50세 이하 vs 이상
     * 성별: 남성 vs 여성 (여성이 골다공증 위험 높음)
     * 인종별: 백인 vs 흑인 vs 아시아인
   - **Failure Mode**: 관절염 환자에서 과대평가 경향 (문서화됨)

**결과**: **510(k) K191003 승인** - XAI 없이도 통계적 검증만으로 충분


### 왜 이렇게 다른가? - 규제 철학의 근본

**1. 임상적 의사결정의 본질**

Dr. Eric Topol (Scripps Research)의 말[@Topol2019DeepMedicine]:
> "Clinicians don't need to know **why** the algorithm predicted a heart attack. They need to know **how often** the algorithm is wrong, and **which patients** it fails for."

**2. 책임 소재 (Liability)**

| 상황 | 설명 가능 AI | 검증 가능 AI |
|------|-------------|-------------|
| 오진 발생 | "왜 틀렸는지" 설명 가능 | "몇 %가 틀릴지" 사전에 알 수 있음 |
| 책임 귀속 | 불명확 (왜 그랬는지 설명만 함) | 명확 (False Positive Rate 3% 문서화) |
| 임상 수용 | 의사가 납득 필요 | 통계적 위험도 수용 여부 판단 |

**3. 규제 문서의 명시**

FDA Guidance "Clinical Decision Support Software" (2022)[@FDA2022CDS]:
> "For regulatory approval, **performance validation on representative clinical datasets** is required. Explainability features may enhance clinical utility but are **not a substitute for statistical validation**."

EU MDR 2017/745 Annex I[@EUMDR2017]:
> "Manufacturers must demonstrate that the device performs **as intended**, with **quantified risk assessment**. Interpretability is encouraged but not mandated."

### 표로 정리: 두 개념의 완전한 구분

| 차원 | 모델 해석 (Interpretability) | 통계적 검증 (Validation) |
|------|---------------------------|------------------------|
| **목적** | "왜?" (Why) | "얼마나?" (How often) |
| **방법** | Grad-CAM, SHAP, Attention | Clinical trial, Sensitivity, CI |
| **질문** | "왜 양성으로 판단했나?" | "양성 판단이 몇 %나 맞나?" |
| **FDA 요구** | Optional (권장) | Mandatory (필수) |
| **승인 기준** | 도움이 되지만 충분하지 않음 | 이것만으로 승인 가능 |
| **문서화** | 수백 페이지 (선택) | 수천 페이지 (필수) |
| **임상 적용** | 의사가 이해하기 쉬움 | 의사가 위험도 판단 가능 |
| **예시** | "Grad-CAM이 출혈 위치 보여줌" | "출혈 감지 Sensitivity 94%" |

### AI 의료기기 개발자를 위한 가이드

#### 해야 할 것 (✅)

1. **대규모 임상시험 설계**
   - 최소 수백 명 (Class II 기준)
   - Multi-center (다기관)
   - Prospective 선호 (Retrospective도 가능)

2. **Subgroup 사전 계획**
   - 인종, 성별, 연령 구분
   - 질병 심각도별 분석
   - FDA Pre-Submission Meeting에서 확인

3. **통계적 지표 철저히**
   - Sensitivity/Specificity + 95% CI
   - ROC Curve, Precision-Recall Curve
   - Confusion Matrix 상세 분석

4. **Failure Mode 문서화**
   - 어떤 케이스에서 실패?
   - 얼마나 자주?
   - 임상적으로 수용 가능?

#### 하지 않아도 되는 것 (선택)

5. **XAI 구현**
   - Grad-CAM 있으면 좋지만 필수 아님
   - 있으면 510(k) 문서에 포함
   - 없어도 승인 가능

6. **모델 내부 설명**
   - "왜 이렇게 판단했나" 설명 불필요
   - 규제 요구사항 아님

### 결론

**딥러닝은 수학적으로 "설명 가능해서" 통과된 게 아니다.**

**딥러닝은 수학적으로 "검증 가능해서" 통과되었다.**

XAI는 임상의와 환자에게 도움이 될 수 있지만, **FDA 승인의 필수 요건이 아니다**. 규제는 "왜"보다 "얼마나 자주"를 묻는다.


## 참고문헌

### FDA 공식 문서

- **FDA AI/ML 의료기기 목록** (945+ 제품, 2025): <https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-enabled-medical-devices>
- **FDA AI/ML Action Plan** (2021): <https://www.fda.gov/media/145022/download>
- **FDA Clinical Decision Support Guidance** (2022): <https://www.fda.gov/regulatory-information/search-fda-guidance-documents/clinical-decision-support-software>
- **FDA Bias in AI Guidance** (2021): <https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-software-medical-device>
- **FDA Device Classification** (21 CFR 860): <https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfcfr/CFRSearch.cfm?CFRPart=860>
- **FDA Design Controls** (21 CFR 820.30): <https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfcfr/CFRSearch.cfm?fr=820.30>

### EU 규제 문서

- **EU MDR 2017/745**: <https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:02017R0745-20250110>
- **EU AI Act** (2024): <https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32024R1689>

### 실제 승인 사례

- **IDx-DR De Novo**: Abramoff, M. D., et al. (2018). "Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic retinopathy." *NPJ Digital Medicine*, 1(39). <https://doi.org/10.1038/s41746-018-0040-6>
- **Viz.ai 510(k) K192287**: <https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?ID=K192287>
- **Arterys De Novo DEN170022**: <https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/denovo.cfm?ID=DEN170022>
- **Paige Prostate De Novo**: <https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/denovo.cfm?ID=DEN210034>
- **Zebra Medical 510(k) K191003**: <https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?ID=K191003>

### 학술 논문

- Topol, E. J. (2019). *Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again*. Basic Books.
- Char, D. S., Shah, N. H., & Magnus, D. (2018). "Implementing machine learning in health care - addressing ethical challenges." *New England Journal of Medicine*, 378(11), 981-983.
- Wiens, J., et al. (2019). "Do no harm: a roadmap for responsible machine learning for health care." *Nature Medicine*, 25, 1337–1340.
- Ghassemi, M., et al. (2021). "The false hope of current approaches to explainable artificial intelligence in health care." *Lancet Digital Health*, 3(11), e745-e750.

### 추가 자료

- **FDA Good Machine Learning Practice**: <https://www.fda.gov/medical-devices/software-medical-device-samd/good-machine-learning-practice-medical-device-development-guiding-principles>
- **IMDRF ML Guidelines**: <https://www.imdrf.org/documents/machine-learning-enabled-medical-devices-key-terms-and-definitions>
