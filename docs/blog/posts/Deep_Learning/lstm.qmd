---
title: "Long Short-Term Memory (LSTM) Networks Basic"
subtitle: Understanding LSTM Architecture, Functionality, and Applications
description: |
  A comprehensive overview of LSTM networks, their components, and use cases in deep learning
categories:
  - Deep Learning
author: Kwangmin Kim
date: 05/01/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
comments: 
  utterances: 
    repo: ./docs/comments
draft: False
---

## 기본 개념

* Long Short-Term Memory(LSTM)는 순환 신경망(RNN)의 한 종류로, 시퀀스 데이터에서 장기 의존성을 학습할 수 있도록 설계되었음. 
*기존 RNN이 가진 기울기 소실 문제를 해결하기 위해 1997년 Hochreiter와 Schmidhuber에 의해 제안되었음.
* LSTM(Long Short-Term Memory)에 대해 이해하기 위해서는 다음 개념들을 알면 도움이 됨:

1. 인공 신경망(ANN)의 기본 구조
  - 뉴런과 활성화 함수의 개념
  - 순방향 전파(Forward Propagation)와 역전파(Backpropagation)
  - 손실 함수와 최적화 알고리즘
2. 순환 신경망(RNN)의 개념과 한계점
  - 시퀀스 데이터 처리의 특성
  - 은닉 상태(Hidden State)의 개념
  - 시간에 따른 정보 전달 구조
  - 기울기 소실/폭발 문제
3. 시퀀스 데이터와 시계열 분석
4. 기울기 소실/폭발 문제
5. 게이트 메커니즘
6. 셀 상태와 은닉 상태
7. 시퀀스 모델링 응용 분야(자연어 처리, 음성 인식 등)

## LSTM의 구조

LSTM의 핵심은 셀 상태(Cell State)와 이를 제어하는 세 가지 게이트로 구성됨:

### 망각 게이트(Forget Gate)

- 이전 정보를 얼마나 유지할지 결정
- 시그모이드 함수를 통해 0~1 사이의 값 출력

### 입력 게이트(Input Gate)

- 새로운 정보를 얼마나 저장할지 결정
- 시그모이드 함수와 tanh 함수 사용

### 출력 게이트(Output Gate)

- 셀 상태의 어떤 부분을 출력할지 결정
- 필터링된 정보를 다음 시간 단계로 전달

## LSTM vs 기본 RNN

LSTM은 기본 RNN과 비교하여 다음과 같은 장점이 있음:

- 장기 의존성 학습 가능
- 기울기 소실/폭발 문제 완화
- 정보의 선택적 기억과 망각

## 주요 응용 분야

LSTM은 다양한 시퀀스 데이터 처리에 활용됨:

1. 자연어 처리(NLP)
  * 단어 순서에 따라 문장의 의미가 완전히 달라짐 (예: "개가 사람을 물었다" vs "사람이 개를 물었다")
  * 이전 단어들이 다음에 올 단어의 문맥을 제공함
  * 문법적 구조와 의미적 관계가 순서에 의존함
    * 기계 번역
    * 감성 분석
    * 텍스트 생성
2. 시계열 예측
  * 주가 예측
  * 날씨 예측
3. 음성 인식 및 처리
4. 비디오 분석 및 행동 인식

## LSTM의 변형

- GRU(Gated Recurrent Unit): 게이트가 더 적은 단순화된 구조
- Bi-directional LSTM: 양방향 정보 흐름 고려
- Attention 메커니즘과 LSTM의 결합

## 코드 예제 (PyTorch)

```python
import torch
import torch.nn as nn
import torch.optim as optim

class LSTM(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super(LSTM, self).__init__()
    self.hidden_size = hidden_size
    self.lstm = nn.LSTM(input_size, hidden_size)
    self.fc = nn.Linear(hidden_size, output_size)
```
