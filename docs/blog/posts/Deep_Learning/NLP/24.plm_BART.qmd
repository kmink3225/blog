---
title: "BART: Bidirectional and Auto-Regressive Transformers"
subtitle: "인코더-디코더 구조로 NLU와 NLG를 통합한 혁신적 언어 모델"
description: |
  BART는 Facebook AI Research에서 2019년 발표한 혁신적인 사전 학습 모델로, BERT의 양방향 이해 능력과 GPT의 생성 능력을 결합한 encoder-decoder 구조를 특징으로 한다. 다양한 노이즈 함수를 사용한 denoising autoencoder 방식의 사전 학습을 통해 자연어 이해와 생성 모두에서 뛰어난 성능을 보여준다. 텍스트 요약, 기계번역, 질의응답 등 다양한 생성 태스크에서의 활용과 성능을 분석한다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-24
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

# 요약

BART(Bidirectional and Auto-Regressive Transformers)는 2019년 Facebook AI Research에서 발표한 혁신적인 사전 학습 언어 모델이다. 기존 BERT의 이해 능력과 GPT의 생성 능력을 하나의 모델에서 통합한 encoder-decoder 구조를 통해 자연어 이해(NLU)와 자연어 생성(NLG) 모두에서 탁월한 성능을 보여준다.

주요 특징과 혁신 사항은 다음과 같다:

* **Encoder-Decoder 통합 구조**:
  - BERT 스타일의 bidirectional encoder로 입력 텍스트의 풍부한 표현 학습
  - GPT 스타일의 autoregressive decoder로 순차적 텍스트 생성
  - 양방향 이해와 일방향 생성의 최적 결합
  - Cross-attention을 통한 encoder-decoder 간 정보 전달

* **다양한 노이즈 함수를 활용한 사전 학습**:
  - **Denoising Autoencoder**: 손상된 텍스트를 원본으로 복원하는 방식으로 학습
  - **Token Masking**: BERT와 유사하지만 더 다양한 마스킹 패턴 적용
  - **Token Deletion**: 임의 토큰 삭제 후 복원
  - **Text Infilling**: 연속된 토큰 스팬을 하나의 마스크로 대체
  - **Sentence Permutation**: 문장 순서 무작위 셔플 후 원래 순서 복원
  - **Document Rotation**: 문서의 시작점을 임의로 회전시킨 후 복원

* **범용적 생성 능력**:
  - 텍스트 요약, 기계번역, 질의응답, 대화 생성 등 다양한 생성 태스크에 적용
  - Fine-tuning을 통한 태스크별 최적화
  - 긴 텍스트 생성에서의 일관성과 품질 향상

* **성능과 효율성**:
  - BART-Base: 140M 파라미터, BART-Large: 400M 파라미터
  - CNN/DailyMail 요약 태스크에서 SOTA 달성
  - WMT 기계번역에서 경쟁력 있는 성능
  - ConvAI2 대화 생성에서 뛰어난 성능

BART는 단일 모델로 이해와 생성을 모두 잘 수행할 수 있는 가능성을 보여주었으며, 이후 T5, PEGASUS 등 encoder-decoder 기반 모델들의 발전에 중요한 영향을 미쳤다.

# NLP 모델 발전 과정

```
RNN Language Model
├── Seq2Seq
├── Beam Search
├── Subword Tokenization
├── Attention
├── Transformer Encoder (Vaswani et al., 2017)
|   ├── Positional Encoding
|   ├── Multi-Head Attention
|   └── Feed Forward Neural Network
|
├── Transformer Decoder (Vaswani et al., 2017)
|
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
|
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
|
├── BERT 변형 모델들
|   ├── RoBERTa (Facebook, 2019)
|   ├── ALBERT (Google, 2019)
|   ├── DistilBERT (Hugging Face, 2019)
|   └── ELECTRA (Google, 2020)
|
└── 후속 발전 모델들
    ├── T5, XLNet, DeBERTa
    └── GPT-2/3/4, ChatGPT, PaLM 등
```

# BART 이전 모델들의 한계점

## 기존 언어 모델의 문제점

**BERT의 한계**:
- Encoder-only 구조로 인한 생성 능력 부족
- Masked Language Model은 이해에는 강하지만 자연스러운 텍스트 생성에는 부적합
- 순차적 디코딩이 불가능하여 autoregressive 생성 태스크에 활용 어려움
- 긴 텍스트 생성 시 일관성과 품질 저하

**GPT의 한계**:
- Decoder-only 구조로 인한 양방향 문맥 이해 부족
- Causal masking으로 인해 미래 정보 활용 불가
- 이해 태스크에서 BERT 대비 상대적으로 낮은 성능
- 입력 문맥의 전체적 이해보다 순차적 예측에 집중

**Seq2Seq 모델의 한계**:
- 사전 학습 없이 태스크별 학습으로 인한 데이터 효율성 문제
- 작은 규모의 모델로 인한 표현력 한계
- Transfer learning의 혜택을 충분히 활용하지 못함
- 복잡한 언어 패턴 학습의 어려움

**통합적 접근의 필요성**:
- NLU와 NLG를 모두 잘 수행하는 단일 모델의 부재
- 이해와 생성 태스크를 위해 별도 모델 필요
- 효율적인 전이 학습을 위한 범용적 사전 학습 방법 부족

# BART (Bidirectional and Auto-Regressive Transformers)

## 개요와 기본 개념

* BART는 **양방향 인코더와 자기회귀 디코더를 결합한 혁신적인 언어 모델**
* 2019년 Facebook AI Research에서 발표되었다. 
* 기존 BERT의 강력한 이해 능력과 GPT의 뛰어난 생성 능력을 하나의 모델에서 통합
* 자연어 이해(NLU)와 자연어 생성(NLG) 모두에서 탁월한 성능을 보여준다.
* 모델 크기는 BART-Base (140M), BART-Large (400M) 두 가지가 있다.

### 핵심 아이디어

* **최고의 결합**: BERT의 bidirectional encoder + GPT의 autoregressive decoder
* **Denoising 사전 학습**: 다양한 노이즈로 손상된 텍스트를 원본으로 복원하며 학습
* **범용성**: 하나의 모델로 분류, 생성, 번역, 요약 등 다양한 태스크 수행
* **효율성**: Encoder-decoder 구조의 장점을 최대한 활용

## 아키텍처 상세

### Encoder-Decoder 구조

```
입력 텍스트 (노이즈 추가)
    ↓
BERT-style Bidirectional Encoder
├── Multi-Head Self-Attention (양방향)
├── Feed-Forward Network
└── Layer Normalization
    ↓
Encoder 표현 (풍부한 문맥 정보)
    ↓
GPT-style Autoregressive Decoder
├── Masked Multi-Head Self-Attention (인과적)
├── Cross-Attention (Encoder 참조)
├── Feed-Forward Network
└── Layer Normalization
    ↓
복원된 원본 텍스트
```

### 핵심 구성 요소

**Bidirectional Encoder**:
- BERT와 동일한 구조로 양방향 문맥 처리
- 입력 시퀀스의 모든 위치를 동시에 참조
- 풍부한 표현 학습으로 깊은 이해 능력 제공
- 손상된 입력에서도 robust한 표현 추출

**Autoregressive Decoder**:
- GPT와 유사한 구조로 순차적 생성
- 이전 생성 토큰들만 참조하는 causal masking
- Cross-attention을 통해 encoder 정보 활용
- 자연스럽고 일관성 있는 텍스트 생성

**Cross-Attention 메커니즘**:
- Decoder의 각 층에서 encoder 출력 참조
- Query는 decoder, Key와 Value는 encoder에서 생성
- 입력 문맥 정보를 생성 과정에 효과적으로 반영
- Attention visualization으로 해석 가능성 제공

## 사전 학습 방법론

### Denoising Autoencoder 패러다임

**기본 원리**:
$$\text{BART}(\text{corrupt}(x)) = x$$

- 원본 텍스트 x에 다양한 노이즈 함수 적용
- 손상된 입력에서 원본 복원을 학습 목표로 설정
- 모델이 언어의 구조와 의미를 깊이 이해하도록 유도
- 다양한 downstream 태스크에 효과적으로 전이

### 다양한 노이즈 함수

**1. Token Masking**
```
원본: "The quick brown fox jumps over the lazy dog"
마스킹: "The quick [MASK] fox jumps [MASK] the lazy dog"
```
- BERT의 MLM과 유사하지만 더 유연한 패턴
- 임의의 토큰을 [MASK]로 대체
- 문맥을 통한 단어 의미 추론 능력 학습

**2. Token Deletion**
```
원본: "The quick brown fox jumps over the lazy dog"
삭제: "The brown fox jumps the lazy dog"
```
- 임의의 토큰들을 완전히 제거
- 모델이 누락된 정보를 추론하여 복원
- 압축된 정보에서 전체 의미 재구성 능력 향상

**3. Text Infilling**
```
원본: "The quick brown fox jumps over the lazy dog"
Infilling: "The quick [MASK] jumps over [MASK] dog"
```
- 연속된 토큰 스팬을 하나의 [MASK]로 대체
- 다양한 길이의 스팬을 무작위로 선택
- 긴 구문의 의미와 구조 학습에 효과적

**4. Sentence Permutation**
```
원본: "First sentence. Second sentence. Third sentence."
셔플: "Third sentence. First sentence. Second sentence."
```
- 문장 순서를 무작위로 섞은 후 원래 순서 복원
- 문서 구조와 논리적 흐름 이해 능력 향상
- 긴 텍스트의 일관성 유지 학습

**5. Document Rotation**
```
원본: "A B C D E F G H"
회전: "D E F G H A B C" (토큰 D부터 시작)
```
- 문서의 시작점을 임의로 설정한 후 원래 시작점 찾기
- 문서 전체의 구조적 이해 능력 향상
- 시작과 끝의 구분 없이 전체 맥락 파악

### 노이즈 함수 조합 및 효과

**Text Infilling + Sentence Permutation (최적 조합)**:
- BART에서 가장 효과적인 것으로 확인된 조합
- Text infilling: 지역적 언어 이해 능력
- Sentence permutation: 전역적 문서 구조 이해
- 두 방식의 시너지로 최고 성능 달성

**다른 조합들과의 비교**:
- Token masking only: BERT와 유사한 성능
- Token deletion only: 정보 손실로 인한 성능 저하
- 모든 노이즈 함수 조합: 과도한 복잡성으로 최적화 어려움

## 학습 과정

### 사전 학습 세부사항

**데이터셋**:
- 16GB의 diverse text corpus 사용
- Book corpus, English Wikipedia, CC-News, OpenWebText
- 도메인 다양성을 통한 robust한 표현 학습

**학습 설정**:
- 모델 크기: BART-Base (140M), BART-Large (400M)
- Batch size: 8,000 sequences
- Learning rate: 3e-4 (Adam optimizer)
- 500,000 스텝 학습 (약 250 epochs)

**토큰화**:
- GPT-2 스타일의 BPE (Byte Pair Encoding)
- 50,265개의 vocabulary
- Subword 단위로 효율적 처리

### Fine-tuning 전략

**Generation Tasks**:
- 전체 encoder-decoder 구조 그대로 사용
- Task-specific 출력 형식에 맞게 조정
- 요약, 번역, 질의응답 등에 직접 적용

**Classification Tasks**:
- Encoder 출력을 classification head에 연결
- [CLS] 토큰 또는 전체 시퀀스 평균 사용
- BERT와 유사한 방식으로 fine-tuning

## 주요 특징과 혁신

### 1. 양방향 이해 + 순차적 생성

**Encoder의 양방향 처리**:
- 입력의 모든 위치를 동시에 참조
- 문맥의 완전한 이해를 통한 rich representation
- BERT 수준의 깊은 언어 이해 능력

**Decoder의 순차적 생성**:
- 이전 토큰들만 참조하는 autoregressive 생성
- 자연스럽고 일관성 있는 출력 생성
- GPT 수준의 유창한 텍스트 생성 능력

### 2. 유연한 입력-출력 매핑

**다양한 태스크 형태**:
- Sequence-to-Sequence: 번역, 요약, 질의응답
- Sequence-to-Label: 분류, 개체명 인식
- Conditional Generation: 조건부 텍스트 생성

**길이 변화 처리**:
- 입력보다 짧은 출력: 요약, 압축
- 입력보다 긴 출력: 확장, 상세화
- 동일 길이 출력: 번역, 패러프레이징

### 3. Robust한 표현 학습

**노이즈에 강한 인코딩**:
- 다양한 노이즈 함수로 훈련되어 robust함
- 불완전한 입력에서도 의미 추출 가능
- 실제 사용 환경의 noisy input에 효과적

**Transfer Learning 효과**:
- 사전 학습의 풍부한 언어 지식 활용
- 적은 양의 태스크별 데이터로도 high performance
- Domain adaptation에 효과적

## 성능 및 벤치마크

### 텍스트 요약

**CNN/DailyMail 데이터셋**:
- ROUGE-1: 44.16 (당시 SOTA)
- ROUGE-2: 21.28 
- ROUGE-L: 40.90
- 기존 모델들 대비 평균 2-3점 향상

**XSum 데이터셋**:
- ROUGE-1: 45.14
- ROUGE-2: 22.27
- ROUGE-L: 37.25
- Abstractive summarization에서 특히 강점

### 기계번역

**WMT16 English-German**:
- BLEU: 35.0 (Transformer baseline 대비 +2.3)
- 특히 긴 문장에서 품질 향상 확인
- Encoder-decoder 구조의 장점 활용

**WMT16 English-French**:
- BLEU: 41.5
- Cross-attention을 통한 정확한 alignment
- 문맥 정보 활용도 개선

### 대화 생성

**ConvAI2 데이터셋**:
- Perplexity: 16.3 (당시 최고 성능)
- F1 score: 20.3
- 일관성 있는 긴 대화 생성 능력

### 독해 및 질의응답

**SQuAD 1.1**:
- F1: 88.8
- EM: 84.9
- BERT와 경쟁적 성능 달성

**SQuAD 2.0**:
- F1: 86.1
- EM: 83.2
- 답변 생성의 자연스러움 향상

# 결론

BART는 자연어 처리 분야에서 **이해와 생성을 통합한 새로운 패러다임**을 제시한 혁신적인 모델이다. 2019년 Facebook AI Research에서 발표된 이후, encoder-decoder 기반 사전 학습 모델의 가능성을 보여주며 NLP 분야에 중요한 영향을 미쳤다.

## BART의 핵심 기여

* **이해와 생성의 통합**: BERT의 양방향 이해 능력과 GPT의 순차적 생성 능력을 하나의 모델에서 효과적으로 결합하여, 단일 모델로 다양한 NLP 태스크를 해결할 수 있는 가능성을 제시했다.

* **혁신적인 사전 학습 방법**: 다양한 노이즈 함수를 활용한 denoising autoencoder 방식으로 언어의 구조와 의미를 깊이 학습하여, 기존 MLM보다 더 robust하고 효과적인 표현을 학습할 수 있음을 보여주었다.

* **Encoder-Decoder 사전 학습의 효과성**: Transformer의 전체 encoder-decoder 구조를 사전 학습에 활용하여, sequence-to-sequence 태스크에서 탁월한 성능을 달성할 수 있음을 입증했다.

* **유연한 적용성**: 텍스트 요약, 기계번역, 질의응답, 대화 생성 등 다양한 생성 태스크뿐만 아니라 분류 태스크에서도 경쟁력 있는 성능을 보여주었다.

## NLP 발전에 미친 영향

**Encoder-Decoder 모델의 부흥**: BART의 성공은 이후 T5, PEGASUS, mT5 등 다양한 encoder-decoder 기반 사전 학습 모델들의 개발을 촉진했다. 특히 생성 태스크에서 encoder-decoder 구조의 우수성을 입증했다.

**사전 학습 방법의 다양화**: 단순한 token masking을 넘어 text infilling, sentence permutation 등 다양한 노이즈 함수를 활용한 사전 학습 방법의 중요성을 보여주었다. 이는 이후 모델들에서 더욱 창의적인 사전 학습 방법들이 개발되는 계기가 되었다.

**생성 태스크의 성능 향상**: 텍스트 요약, 기계번역 등 생성 태스크에서 기존 모델들을 크게 앞서는 성능을 보여주어, 실용적인 NLP 응용 분야의 발전을 가속화했다.

**통합 모델의 가능성**: 하나의 모델로 이해와 생성을 모두 잘 수행할 수 있다는 가능성을 보여주어, 범용 언어 모델 개발의 방향성을 제시했다.

## 실용적 활용과 파급 효과

**산업 응용**: BART는 뉴스 요약, 문서 번역, 챗봇 개발 등 다양한 산업 분야에서 실제로 활용되기 시작했다. 특히 자동 요약 시스템에서는 BART 기반 모델들이 널리 사용되고 있다.

**연구 도구**: 연구자들이 다양한 sequence-to-sequence 태스크를 실험할 수 있는 강력한 baseline을 제공하여, NLP 연구의 효율성을 높였다.

**오픈 소스 생태계**: Hugging Face Transformers 등을 통해 쉽게 사용할 수 있게 되어, 학계와 산업계 모두에서 광범위하게 활용되고 있다.

## 한계와 개선 영역

**계산 복잡도**: Encoder-decoder 구조로 인해 encoder-only나 decoder-only 모델에 비해 더 많은 계산 자원이 필요하다. 특히 inference 시에 decoder의 순차적 생성으로 인한 속도 제약이 있다.

**긴 시퀀스 처리**: Transformer의 quadratic attention complexity로 인해 매우 긴 문서 처리에는 여전히 한계가 있다. 메모리 사용량과 계산 시간이 시퀀스 길이에 따라 급격히 증가한다.

**도메인 특화**: 특정 도메인에 특화된 성능을 위해서는 여전히 상당한 양의 domain-specific 데이터와 fine-tuning이 필요하다.

**해석 가능성**: 복잡한 encoder-decoder 구조로 인해 모델의 의사결정 과정을 이해하고 해석하기가 어렵다.

## 미래 발전 방향

**효율성 개선**: Sparse attention, linear attention 등을 활용한 더 효율적인 BART 변형들이 개발될 것이다. 또한 model compression과 knowledge distillation을 통한 경량화 연구도 활발해질 것이다.

**다중 모달 확장**: 텍스트뿐만 아니라 이미지, 음성 등 다양한 모달리티를 함께 처리할 수 있는 multimodal BART의 개발이 진행될 것이다.

**도메인 특화**: 의료, 법률, 과학 등 특정 도메인에 특화된 BART 모델들이 개발되어 전문 분야에서의 활용도가 높아질 것이다.

**Few-shot Learning**: GPT-3와 같이 few-shot learning 능력을 갖춘 대규모 BART 모델들이 개발되어, 적은 데이터로도 새로운 태스크에 적응할 수 있게 될 것이다.

**실시간 응용**: 더 빠른 inference를 위한 최적화 기술들이 개발되어, 실시간 번역, 실시간 요약 등의 응용이 가능해질 것이다.

## 역사적 의미와 전망

BART는 NLP 역사에서 **이해와 생성의 통합**이라는 중요한 이정표를 세웠다. BERT가 이해 태스크에서, GPT가 생성 태스크에서 각각 혁신을 가져왔다면, BART는 이 두 능력을 하나의 모델에서 효과적으로 결합할 수 있음을 보여주었다.

이는 현재 우리가 목표로 하는 **범용 인공지능(AGI)**의 관점에서도 중요한 의미를 가진다. 인간이 언어를 이해하고 생성하는 능력을 하나의 통합된 시스템에서 수행하듯이, AI도 이해와 생성을 분리하지 않고 통합적으로 처리할 수 있어야 한다.

앞으로 BART의 핵심 아이디어들은 더욱 발전되어, 인간 수준의 언어 이해와 생성 능력을 갖춘 AI 시스템 개발의 중요한 기초가 될 것이다. 특히 대화형 AI, 창작 보조 도구, 자동 번역 시스템 등에서 BART의 영향은 계속해서 확대될 것으로 예상된다.

BART는 단순한 기술적 개선을 넘어, AI가 언어를 통해 인간과 더욱 자연스럽고 효과적으로 소통할 수 있는 미래를 열어가는 중요한 발걸음이었다.
