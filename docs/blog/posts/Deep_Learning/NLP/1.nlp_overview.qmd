---
title: "자연어 처리(NLP) 개요"
subtitle: 자연어 처리의 기본 개념과 한국어/영어 처리의 특성
description: |
  자연어 처리의 기본 개념과 응용 분야, 그리고 한국어와 영어의 언어적 특성이 자연어 처리에 미치는 영향을 살펴본다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-01
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

# 자연어 처리의 개요

자연어 처리(NLP)는 컴퓨터가 인간의 언어를 이해하고 처리하는 기술 분야다. 여기서 자연어란 우리가 일상에서 사용하는 한국어, 영어, 중국어, 일본어 같은 언어를 말한다.

## 자연어 처리의 구성 요소

- NLP = 전처리 + NLU + NLG
  - 전처리 (pre-processing)
    - tokenization: 텍스트 데이터를 정제하고 토큰화하는 과정
    - vectorization: 텍스트 데이터를 벡터 형태로 변환하는 과정 (텍스트를 숫자로 변환)
    - word embedding: 단어를 밀집 벡터(dense vector) 공간에 매핑하는 기법
  - 자연어 이해(NLU, Natural Language Understanding)
    - encoder 기반의 알고리즘으로 텍스트를 이해하는 부분
    - 텍스트를 생성하는데 사용되진 않는다.
  - 자연어 생성(NLG, Natural Language Generation)
    - decoder 기반의 알고리즘으로 텍스트를 생성하는 부분
    - 텍스트를 이해할 수 있지만 비효율적인 방법

## 자연어 처리의 응용 분야

- 음성 인식
- 텍스트 요약
- 기계 번역
- 감성 분석
- 텍스트 분류
- 질의 응답 시스템
- 챗봇
- 등등

최근 딥러닝을 활용한 자연어 처리 기술이 폭발적으로 발전하면서 괄목할만한 성과를 이루고 있다. 이 분야는 종종 '텍스트 분석'이라고도 불리지만, '자연어 처리'라고 하면 인공지능 기술을 활용한다는 의미가 더 강하게 내포된다.

# 자연어 처리의 기회

자연어 처리는 현재 IT 업계에서 매우 유망한 분야다. 그 이유는 다음과 같다:

1. **산업 전반에 걸친 수요**: 특정 도메인에 국한되지 않고 거의 모든 산업 분야에서 자연어 처리 기술에 대한 수요가 있다.

2. **인력 부족**: 현재 실무에서 PLM(Pre-trained Language Model)을 활용할 수 있는 중급 이상의 인력이 부족해 기회가 많다.

3. **오픈소스의 발전**: 성능 좋은 오픈소스 모델과 라이브러리가 지속적으로 공개되고 있어, 자연어 처리 기술을 익히면 손쉽게 높은 성능의 모델을 활용할 수 있다.

# 자연어 처리 학습 순서

자연어 처리를 배우기 위한 일반적인 학습 경로는 다음과 같다:

1. 자연어 처리의 기본 개념 이해
2. 통계적 방식의 자연어 처리 학습
3. 초기 딥러닝 기반 자연어 처리 방법론 (2010년대 초반~중반)
4. 현대의 딥러닝 자연어 처리 - PLM(Pre-trained Language Models) 중심

세부 학습 단계

```
전처리 (Pre-processing)
├── Tokenization (※ 명시되지 않았지만 기본 전제)
├── Vectorization (수치화)
│   ├── BoW (통계 기반, 1950s~)
│   ├── DTM (통계 기반, 1950s~)
│   └── TF-IDF (정보 검색 분야, 1972)
├── Word Embedding (의미 임베딩)
│   ├── Embedding Layer (딥러닝 공통, 2010s~)
│   ├── Word2Vec (Google, 2013)
│   ├── FastText (Facebook AI, 2016)
│   ├── GloVe (Stanford, 2014)
│   └── 기타: Swivel (Google, 2016), LexVec (UNSW, 2016)

자연어 이해 (NLU: Natural Language Understanding)
├── RNN 기반 문맥 임베딩
│   ├── LSTM (Hochreiter & Schmidhuber, 1997)
│   ├── GRU (U. of Montreal, 2014)
│   └── ELMo (Allen Institute, 2018)
├── Attention 메커니즘
│   ├── Basic Attention (Bahdanau et al., 2014)
│   ├── Self-Attention (Google, 2017)
│   └── Multi-Head Attention (Google, 2017)
├── Transformer 구조 자체 (Google, 2017)
├── BERT 시리즈
│   ├── BERT (Google, 2018)
│   ├── RoBERTa (Facebook AI, 2019)
│   └── ALBERT (Google & Toyota Research Institute, 2019)
├── 한국어 특화 NLU 모델
│   ├── KoBERT (Kakao Brain, 2019)
│   └── KLU-BERT (KLUE 연구진, 2021)
└── 기타 NLU 모델
    └── ELECTRA (Google, 2020), XLNet (Google & CMU, 2019)

자연어 생성 (NLG: Natural Language Generation)
├── Transformer 구조 자체 (Google, 2017)
├── GPT 시리즈 (GPT-1~4, ChatGPT) (OpenAI, 2018~2023)
├── KoGPT (Kakao Brain, 2021)
├── T5 (Google, 2019)
├── LaMDA (Google, 2021), PaLM (Google, 2022), Gemini (Google DeepMind, 2023), Claude (Anthropic, 2023)
└── 기타 생성 모델

```

# 주요 도구와 프레임워크

## PyTorch

PyTorch는 Facebook AI Research(FAIR)에서 개발한 딥러닝 프레임워크다. 연구용 프로토타입부터 상용 제품까지 빠르게 개발할 수 있는 유연성을 제공하며, 현재 자연어 처리 분야에서 가장 보편적으로 사용되는 프레임워크 중 하나다.

## Transformers

Hugging Face[(https://huggingface.co/docs/transformers/index)](https://huggingface.co/docs/transformers/index) 에서 개발한 Transformers 라이브러리는 다양한 트랜스포머 계열의 모델과 관련 모듈을 제공하는 Data Science Hub 플랫폼이다. 현대 자연어 처리에서 PLM을 활용할 때 대부분 이 라이브러리를 사용한다. BERT, GPT, T5 등 최신 언어 모델을 쉽게 불러와 사용할 수 있게 해준다.

# 한국어 자연어 처리의 난이도와 특성

한국어는 영어에 비해 자연어 처리가 더 어려운 특성을 가지고 있다. 이러한 특성들은 언어 모델의 성능에 직접적인 영향을 미친다.

## 한국어의 언어적 특성

### 교착어로서의 특성
- **정의**: 실질적인 의미를 가진 어간에 조사나 어미와 같은 문법 형태소들이 결합하여 문법적 기능이 부여되는 언어
- **예시**: '사람은', '사람이', '사람을', '사람에게', '사람과', '사람의', '사람에', '사람으로부터'와 같이 같은 명사에 다양한 조사가 결합
- **문제점**: 띄어쓰기 단위로 토큰화할 경우 이들이 모두 다른 단어로 간주됨

### 어순의 유연성

- 한국어는 정황어로서 조사나 토씨만으로도 문장의 의미를 파악할 수 있음
- 문장 성분의 위치가 바뀌어도 의미 전달에 큰 문제가 없음
- **예시**:
  - 나는 오늘 저녁에 친구와 함께 영화를 보러 간다.
  - (나는) 친구와 함께 오늘 저녁에 영화를 보러 간다.
  - (나는) 영화를 보러 오늘 저녁에 친구와 함께 간다.
  - 간다 (나는) 영화를 보러 오늘 저녁에 친구와 함께.

### 주어 생략 현상

- 한국어는 문맥상 이해 가능한 경우 주어를 자주 생략함
- 때로는 주어와 서술어가 모두 생략되는 경우도 있음
- **예시**: "(나는) 오늘 저녁에 친구와 함께 영화를 보러 (간다)."

### 띄어쓰기 규칙의 비일관성

- 한국어는 띄어쓰기가 엄격하게 지켜지지 않는 경향이 있음
- 띄어쓰기를 하지 않더라도 문장 이해가 가능함
- **예시**: "동해물과백두산이마르고닳도록하느님이보우하사우리나라만세"
- 반면 영어는 띄어쓰기가 없으면 읽기 어려움: "tobeornottobethatisthequestion"

### 한자어의 특성

- 하나의 음절이 다양한 의미를 가질 수 있음
- 동음이의어가 많아 문맥 파악이 중요함

## 한국어 자연어 처리의 어려움

### 모델링 관점의 어려움

- 주어 생략과 자유로운 어순으로 인해 언어 모델의 예측 성능이 저하됨
- 교착어 특성으로 인한 어휘 다양성 증가로 모델 복잡도 증가

### 리소스 부족

- 영어에 비해 데이터와 언어에 특화된 모델이 상대적으로 부족함
- 한국어 특성을 고려한 전처리 도구와 평가 방법론 개발 필요

이러한 한국어의 특성들은 자연어 처리 모델의 성능에 직접적인 영향을 미치며, 한국어 자연어 처리를 위해서는 이러한 특성을 고려한 접근 방식이 필요하다.

## 영어의 언어적 특성

### 고립어로서의 특성
- **정의**: 단어의 형태가 거의 변하지 않고, 어순과 전치사 등을 통해 문법적 관계를 표현하는 언어
- **예시**: 'person', 'the person', 'to the person', 'with the person', 'of the person'과 같이 명사 자체는 변하지 않고 전치사나 관사가 추가됨
- **특징**: 단어 경계가 명확하여 토큰화가 상대적으로 용이함

### 엄격한 어순

- 영어는 주어-동사-목적어(SVO) 구조를 기본으로 하는 엄격한 어순을 가짐
- 어순이 바뀌면 문장의 의미가 크게 달라지거나 비문법적이 됨
- **예시**:
  - "I love you" (나는 너를 사랑한다)
  - "You love I" (비문법적)
  - "Love I you" (비문법적)

### 주어 필수 현상

- 영어는 거의 모든 문장에서 주어가 필수적으로 요구됨
- 주어가 없는 문장은 명령문이나 특수한 경우를 제외하고는 비문법적임
- 의미상 주어가 없을 때도 형식적 주어(it, there)를 사용함
- **예시**: "It is raining." (비인칭 주어 it 사용)

### 띄어쓰기의 중요성

- 영어는 띄어쓰기가 의미 구분에 필수적인 역할을 함
- 띄어쓰기가 없으면 단어 경계 식별이 어려워 문장 이해가 불가능함
- **예시**: "Iloveyou" vs "I love you"

### 굴절 현상의 제한성

- 영어는 한국어에 비해 굴절(inflection) 현상이 제한적임
- 명사는 복수형, 소유격 정도만 변화
- 동사는 시제, 인칭에 따라 제한적으로 변화
- **예시**: 
  - 명사: dog → dogs (복수형)
  - 동사: walk → walks, walked, walking

### 동음이의어와 다의어

- 영어도 동음이의어와 다의어가 많아 문맥 파악이 중요함
- **예시**: 
  - "bank"(은행 또는 강둑)
  - "light"(가벼운 또는 빛)

## 영어 자연어 처리의 특징

### 모델링 관점의 이점

- 어순이 고정되어 있어 언어 모델의 예측 성능이 상대적으로 높음
- 단어 형태 변화가 적어 어휘 다양성이 한국어보다 낮음
- 띄어쓰기가 명확하여 토큰화가 용이함

### 풍부한 리소스

- 방대한 양의 텍스트 데이터와 사전 학습된 모델이 존재함
- 다양한 자연어 처리 도구와 평가 데이터셋이 개발되어 있음
- 영어 기반 연구가 선행되어 참고할 수 있는 자료가 많음

영어와 한국어의 이러한 언어적 차이는 자연어 처리 접근 방식에 직접적인 영향을 미치며, 각 언어의 특성에 맞는 전처리 방법과 모델링 전략이 필요하다.

# 사전 학습 언어 모델(PLM)의 발전과 다국어 처리

## 다국어 처리의 혁신

최근 자연어 처리 분야에서는 사전 학습 언어 모델(Pre-trained Language Models, PLM)의 급속한 발전으로 인해 언어 간 차이에 대한 우려가 크게 감소하고 있다. 이러한 모델들은 다양한 언어의 특성을 효과적으로 학습하여 언어 간 격차를 줄이고 있다.

### 다국어 사전 학습 모델의 등장

- **다국어 BERT, XLM, mT5** 등의 모델은 100개 이상의 언어를 동시에 처리할 수 있는 능력을 갖추고 있음
- 이러한 모델들은 각 언어의 고유한 특성(어순, 형태소, 문법 구조 등)을 대규모 말뭉치를 통해 자동으로 학습
- 한국어와 같은 교착어도 효과적으로 처리할 수 있는 능력을 보여줌

### 언어 간 전이 학습의 효과

- 영어 등 리소스가 풍부한 언어에서 학습된 지식이 리소스가 적은 언어로 효과적으로 전이됨
- 적은 양의 한국어 데이터로도 영어 수준에 근접한 성능을 달성할 수 있게 됨
- 언어 간 공통된 의미적, 구문적 패턴을 모델이 포착하여 활용

## 한국어 자연어 처리의 현재

### 한국어 특화 모델의 발전

- **KoBERT, KoGPT, KoELECTRA** 등 한국어에 최적화된 사전 학습 모델들이 개발되어 공개됨
- 이러한 모델들은 한국어의 특성을 고려한 토큰화 방식과 학습 방법을 적용
- 한국어 위키피디아, 뉴스, 웹 문서 등 대규모 한국어 말뭉치로 학습되어 우수한 성능을 보임

### 실용적 관점에서의 변화

- 과거에는 한국어 형태소 분석기 선택과 최적화가 핵심 과제였으나, 현재는 적절한 사전 학습 모델 선택이 더 중요해짐
- 토큰화, 형태소 분석 등의 전처리 작업이 모델 내부에서 자동으로 처리되어 개발자의 부담이 크게 감소
- 대부분의 NLP 태스크에서 언어 특성에 대한 깊은 이해 없이도 준수한 성능 달성 가능

## 실무적 시사점

### 개발 효율성 향상

- 언어별 특화된 전처리 파이프라인 구축 필요성이 감소
- 동일한 아키텍처와 접근 방식으로 다양한 언어 처리 가능
- 개발 및 유지보수 비용 절감

### 다국어 서비스 구현 용이성

- 하나의 모델로 여러 언어를 지원하는 서비스 구현 가능
- 언어 간 일관된 성능으로 사용자 경험 향상
- 새로운 언어 추가가 상대적으로 용이해짐

결론적으로, 현대 PLM의 발전으로 인해 한국어와 영어 간의 언어적 차이가 자연어 처리에 미치는 영향은 크게 감소했다. 물론 각 언어의 특성을 이해하는 것은 여전히 중요하지만, 기술적 장벽은 상당히 낮아졌으며, 이는 다양한 언어에 대한 NLP 응용 프로그램 개발을 크게 촉진하고 있다.
