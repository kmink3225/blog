---
title: "Attention 메커니즘의 종류와 발전"
subtitle: "Self-Attention에서 Multi-Head Attention까지의 체계적 분류"
description: |
  자연어 처리에서 혁신을 가져온 Attention 메커니즘의 다양한 종류와 발전 과정을 체계적으로 분석한다. Self-Attention, Cross-Attention, Multi-Head Attention 등의 핵심 개념과 작동 원리, 그리고 각각의 특징과 활용 분야를 살펴본다. RNN의 순차적 처리 한계를 극복하고 Transformer 아키텍처의 핵심이 된 Attention 메커니즘이 현대 NLP에 미친 영향을 다룬다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-19
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

# 요약

Attention 메커니즘은 자연어 처리에서 가장 중요한 혁신 중 하나로, 기존 RNN의 순차적 처리 한계를 극복하고 현대 NLP의 핵심 기술이 되었다. 이 문서는 다양한 Attention 메커니즘의 종류와 특징을 체계적으로 분류하고 분석한다.

주요 내용은 다음과 같다:

* **Attention 메커니즘의 기본 개념**:
  - 시퀀스의 모든 위치를 동시에 참조하여 중요한 정보에 집중하는 메커니즘
  - RNN의 순차적 처리와 장기 의존성 문제를 해결하는 핵심 기술
  - "어디에 주의를 기울일 것인가"를 자동으로 학습하는 가중치 메커니즘

* **참조 대상에 따른 분류**:
  - **Self-Attention**: 같은 시퀀스 내 요소들 간의 관계 모델링
  - **Cross-Attention**: 서로 다른 시퀀스 간의 대응 관계 학습
  - 각각의 용도와 활용 분야의 차이점

* **처리 방식에 따른 분류**:
  - **Single-Head Attention**: 하나의 관점에서 관계 계산
  - **Multi-Head Attention**: 여러 관점에서 동시에 관계를 포착하여 더 풍부한 표현 학습
  - 병렬 처리를 통한 계산 효율성과 표현력 향상

* **방향성에 따른 분류**:
  - **Bidirectional Attention**: 양방향으로 모든 위치 참조 (BERT 방식)
  - **Causal/Unidirectional Attention**: 과거 정보만 참조 (GPT 방식)
  - 각각의 장단점과 적용 분야

* **현대 NLP에서의 활용**:
  - Transformer 아키텍처의 핵심 구성 요소
  - BERT, GPT 등 사전 학습 모델의 기반 기술
  - 기계번역, 텍스트 생성, 문서 이해 등 다양한 태스크에 적용

Attention 메커니즘의 이해는 현대 NLP 모델의 작동 원리를 파악하는 데 필수적이며, 향후 더욱 발전된 언어 모델 설계의 기초가 된다.

# 텍스트 인코딩 및 벡터화

```
텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention

RNN Language Model
├── Seq2Seq
├── Beam Search
├── Subword Tokenization
├── Attention
├── Transformer Encoder (Vaswani et al., 2017)
|   ├── Positional Encoding
|   ├── Multi-Head Attention
|   └── Feed Forward Neural Network
|
├── Transformer Decoder (Vaswani et al., 2017)
|
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
|
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
|
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등
```


# Attention 메커니즘의 등장 배경

## RNN의 한계점

**순차적 처리의 문제**:
- RNN은 시퀀스를 왼쪽에서 오른쪽으로 순서대로 처리
- 병렬 처리가 불가능하여 계산 속도가 느림
- GPU의 병렬 연산 능력을 제대로 활용하지 못함

**장기 의존성 문제**:
- 문장이 길어질수록 초기 정보가 점점 희석됨
- "그 강아지는 공원에서... (중간에 여러 단어) ...즐거워했다"
- "강아지"와 "즐거워했다"의 연결 관계가 약화됨
- Gradient vanishing 문제로 인한 학습 한계

**고정된 표현의 한계**:
- 마지막 은닉 상태 하나로 전체 시퀀스 표현
- 중간 과정의 중요한 정보 손실
- 문맥에 따른 동적 표현 부족

## Attention의 혁신

**전체 시퀀스 동시 참조**:
- 모든 위치를 한 번에 보면서 중요한 부분 식별
- 거리에 상관없이 관련성 높은 정보에 집중
- 병렬 처리로 계산 속도 대폭 향상

**동적 가중치 계산**:
- 각 단어를 처리할 때마다 전체 시퀀스에서 관련성 점수 계산
- 문맥에 따라 같은 단어도 다른 중요도 부여
- 학습을 통해 최적의 주의 패턴 자동 발견

# Attention 메커니즘의 기본 원리

## 핵심 개념

**"어디에 주의를 기울일 것인가?"**
- 인간이 긴 글을 읽을 때 중요한 부분에 집중하는 것과 유사
- 전체 정보 중에서 현재 처리 중인 요소와 관련성이 높은 부분 찾기
- 관련성을 수치로 계산하여 가중 평균으로 정보 통합

## 기본 수식

**Attention 기본 형태**:
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**구성 요소**:
- **Query (Q)**: "무엇을 찾고 있는가?" - 현재 처리 중인 요소
- **Key (K)**: "각 위치의 특징은 무엇인가?" - 참조할 수 있는 모든 위치들
- **Value (V)**: "실제 정보는 무엇인가?" - 가져올 실제 정보들

**작동 과정**:
1. **유사도 계산**: Query와 각 Key 간의 유사도 측정 (QK^T)
2. **스케일링**: 차원 수의 제곱근으로 나누어 안정화 (√d_k)
3. **확률화**: Softmax로 가중치를 확률 분포로 변환
4. **가중 합**: 가중치를 Value에 적용하여 최종 출력 계산

## 직관적 이해

**도서관에서 책 찾기 비유**:
- Query: "머신러닝에 대한 정보가 필요해"
- Key: 각 책의 제목과 키워드들
- Value: 각 책의 실제 내용
- Attention: 제목/키워드 매칭으로 관련도 계산 → 관련 있는 책들을 가중치에 따라 참조

# Attention 메커니즘의 종류

## 1. 참조 대상에 따른 분류

### Self-Attention

**정의**: 같은 시퀀스 내에서 각 요소가 다른 모든 요소들과의 관계를 계산

**특징**:
- Query, Key, Value가 모두 같은 시퀀스에서 생성
- 문장 내 단어들 간의 상호작용 모델링
- 각 단어가 문장의 다른 모든 단어들을 "참조"

**구체적 예시**:
```
문장: "그 강아지는 공원에서 뛰어다니며 즐거워했다"

Self-Attention 결과:
- "강아지" 처리 시: "뛰어다니며"(0.4), "즐거워했다"(0.3), "공원에서"(0.2), "그"(0.1)
- "뛰어다니며" 처리 시: "강아지"(0.5), "공원에서"(0.3), "즐거워했다"(0.2)
- "즐거워했다" 처리 시: "강아지"(0.6), "뛰어다니며"(0.3), "공원에서"(0.1)
```

**장점**:
- 장거리 의존성 효과적 포착
- 병렬 처리 가능
- 문맥 정보 풍부하게 활용

**활용 분야**:
- 문서 이해 (BERT)
- 텍스트 생성 (GPT)
- 기계번역의 인코더

### Cross-Attention

**정의**: 서로 다른 시퀀스 간의 관계를 계산

**특징**:
- Query는 한 시퀀스, Key와 Value는 다른 시퀀스에서 생성
- 두 시퀀스 간의 대응 관계 학습
- 정보를 한 시퀀스에서 다른 시퀀스로 전달

**구체적 예시**:
```
기계번역: "I love dogs" → "나는 개를 좋아한다"

Cross-Attention:
- "나는" 생성 시: "I"(0.8), "love"(0.1), "dogs"(0.1)
- "개를" 생성 시: "dogs"(0.7), "love"(0.2), "I"(0.1)  
- "좋아한다" 생성 시: "love"(0.6), "dogs"(0.3), "I"(0.1)
```

**활용 분야**:
- 기계번역 (원문 ↔ 번역문)
- 이미지 캡셔닝 (이미지 ↔ 텍스트)
- 질의응답 (질문 ↔ 문서)
- 요약 (원문 ↔ 요약문)

## 2. 처리 방식에 따른 분류

### Single-Head Attention

**특징**:
- 하나의 Attention Head만 사용
- 단일 관점에서 관계 계산
- 계산량이 적지만 표현력 제한

**한계점**:
- 복잡한 관계 패턴 포착의 어려움
- 다양한 유형의 관계를 동시에 모델링 불가
- 정보의 다면적 측면 반영 부족

### Multi-Head Attention

**정의**: 여러 개의 Attention Head를 병렬로 사용하여 다양한 관점에서 관계 포착

**핵심 아이디어**:
- 각 Head가 서로 다른 종류의 관계에 특화
- 여러 관점을 종합하여 더 풍부한 표현 생성
- 전문가들이 각자 다른 관점에서 분석 후 종합하는 것과 유사

**수식**:
$$ \text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O $$
$$ \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$

**각 Head의 역할 예시**:

```
8개 Head를 가진 Multi-Head Attention:
- Head 1: 문법적 관계 (주어-동사, 형용사-명사)
- Head 2: 의미적 관계 (유의어, 반의어) 
- Head 3: 위치적 관계 (인접한 단어들)
- Head 4: 장거리 의존성 (문장 시작과 끝)
- Head 5: 개체 관계 (사람, 장소, 시간)
- Head 6: 감정적 관계 (긍정/부정 표현)
- Head 7: 논리적 관계 (원인-결과, 조건)
- Head 8: 화제 관계 (주제와 관련 단어들)
```

**장점**:

- 다양한 종류의 관계 동시 포착
- 표현력 대폭 향상
- 각 Head의 특화를 통한 효율적 정보 처리
- 해석 가능성 향상 (각 Head의 역할 분석 가능)

## 3. 방향성에 따른 분류

### Bidirectional Attention

**정의**: 양방향으로 모든 위치를 참조할 수 있는 Attention

**특징**:
- 현재 위치에서 과거와 미래 모든 위치 참조 가능
- 전체 문맥 정보를 활용한 풍부한 표현
- 이해 태스크에 적합

**활용 예시**:
```
문장: "그 강아지는 공원에서 뛰어다니며 즐거워했다"

"공원에서" 처리 시:
- 과거 참조: "그", "강아지는" 
- 미래 참조: "뛰어다니며", "즐거워했다"
- 모든 정보를 종합하여 "공원에서"의 의미 결정
```

**대표 모델**: BERT, RoBERTa, ALBERT

### Causal/Unidirectional Attention

**정의**: 현재 위치 이전의 정보만 참조할 수 있는 Attention

**특징**:
- 미래 정보 차단으로 순차적 생성 모델링
- 텍스트 생성 시 "부정행위" 방지
- Attention mask를 통한 구현

**마스킹 메커니즘**:

```
Attention Score Matrix (4×4 예시):
[[ 0.2, -∞,  -∞,  -∞],     →     [[1.0, 0.0, 0.0, 0.0],
 [ 0.1, 0.3, -∞,  -∞],      →      [0.4, 0.6, 0.0, 0.0],
 [ 0.4, 0.2, 0.1, -∞],      →      [0.5, 0.3, 0.2, 0.0],
 [ 0.3, 0.1, 0.2, 0.5]]     →      [0.2, 0.1, 0.2, 0.5]]

-∞ 부분이 Softmax 후 0이 되어 미래 정보 차단
```

**대표 모델**: GPT 시리즈, 대부분의 디코더 모델

## 구조적 분류

### Scaled Dot-Product Attention

**정의**: 현재 가장 널리 사용되는 표준 Attention 방식

**특징**:
- 내적(Dot Product)을 통한 유사도 계산
- 차원으로 스케일링하여 안정성 확보
- 계산 효율성과 성능의 균형

**장점**:
- 구현이 간단하고 직관적
- 행렬 연산 최적화 가능
- GPU에서 빠른 계산
- 메모리 효율적

### Additive Attention

**정의**: 초기 seq2seq 모델에서 사용된 Attention 방식

**수식**:
$$ e_{ij} = v^T \tanh(W_1 h_i + W_2 s_j) $$

**특징**:
- Neural Network를 통한 유사도 계산
- 더 많은 파라미터와 계산량 필요
- 현재는 잘 사용되지 않음

### Sparse Attention

**정의**: 계산 효율성을 위해 일부 위치만 참조하는 Attention

**필요성**:
- 긴 시퀀스에서 O(n²) 복잡도 문제
- 모든 위치를 참조할 필요가 없는 경우
- 메모리와 계산 자원 절약

**종류**:
- **Local Attention**: 주변 k개 위치만 참조
- **Strided Attention**: 일정 간격으로 참조
- **Random Attention**: 무작위로 선택된 위치 참조

# 현대 NLP에서의 Attention 활용

## Transformer 아키텍처

**Encoder의 Self-Attention**:
- 입력 시퀀스 내 모든 위치 간 관계 모델링
- 양방향 정보 처리로 풍부한 표현 생성
- 병렬 처리로 빠른 인코딩

**Decoder의 Multi-Attention**:
1. **Masked Self-Attention**: 생성된 부분 내에서의 관계
2. **Cross-Attention**: 인코더 출력과의 관계
3. 순차적 생성을 위한 미래 정보 차단

## 사전 학습 모델에서의 활용

### BERT 계열 (Bidirectional)

- **Self-Attention**: 양방향 문맥 이해
- **Multi-Head**: 다양한 언어학적 관계 포착
- **Deep Architecture**: 12-24층의 Attention 레이어

### GPT 계열 (Causal)

- **Causal Self-Attention**: 순차적 텍스트 생성
- **Multi-Head**: 다양한 생성 패턴 학습
- **Scaling**: 모델 크기 증가에 따른 성능 향상

### T5 (Text-to-Text)

- **Encoder-Decoder**: 양방향 이해 + 순차적 생성
- **Cross-Attention**: 입력과 출력 간 관계
- **다양한 태스크**: 하나의 구조로 모든 NLP 태스크

## 특화된 Attention 변형

### Longformer

- **Sliding Window**: 지역적 Attention
- **Global Attention**: 특정 토큰에 전역 Attention
- **긴 문서**: 수천 개 토큰 처리 가능

### BigBird

- **Random Attention**: 무작위 위치 참조
- **Window Attention**: 지역적 참조
- **Global Attention**: 중요 토큰 전역 참조

### Linformer

- **Linear Complexity**: O(n) 복잡도로 축소
- **Projection**: Key, Value를 저차원으로 투영
- **효율성**: 긴 시퀀스에서 메모리 절약

# 결론

Attention 메커니즘은 자연어 처리 분야에서 **패러다임의 전환점**이 된 가장 중요한 기술적 혁신 중 하나다. RNN의 순차적 처리 한계를 극복하고 현대 NLP의 기반을 마련했다.

## Attention 메커니즘의 핵심 기여

* **계산 효율성 혁신**: 순차적 처리에서 병렬 처리로 전환하여 학습 속도를 획기적으로 향상시켰다. GPU의 병렬 연산 능력을 최대한 활용할 수 있게 되었다.
* **장기 의존성 해결**: 시퀀스 길이에 관계없이 모든 위치 간 직접적 연결을 통해 멀리 떨어진 요소들 간의 관계를 효과적으로 포착한다.
* **표현력 향상**: Multi-Head Attention을 통해 문법적, 의미적, 위치적 관계 등 다양한 종류의 언어학적 패턴을 동시에 학습할 수 있다.
* **유연성 제공**: Self-Attention과 Cross-Attention의 조합으로 다양한 NLP 태스크에 적용 가능한 범용적 구조를 제공한다.

## 현대 AI에 미친 영향

* **Transformer 혁명**: Attention 메커니즘을 기반으로 한 Transformer는 자연어 처리의 표준 아키텍처가 되었다. "Attention is All You Need"라는 제목처럼 실제로 Attention만으로도 충분히 강력한 언어 모델을 만들 수 있음을 증명했다.
* **사전 학습 모델의 기반**: BERT, GPT 등 현재 널리 사용되는 모든 대규모 언어 모델의 핵심 구성 요소가 되었다. 이들 모델의 놀라운 성능은 Attention 메커니즘의 강력함을 보여준다.
* **다중 모달 확장**: 텍스트를 넘어 이미지, 음성, 비디오 등 다양한 모달리티를 처리하는 모델들도 Attention 메커니즘을 활용하고 있다.

## 한계와 도전 과제

* **계산 복잡도**: 시퀀스 길이의 제곱에 비례하는 O(n²) 복잡도로 인해 매우 긴 시퀀스 처리에 한계가 있다. 이를 해결하기 위한 Sparse Attention, Linear Attention 등의 연구가 활발히 진행되고 있다.

* **해석 가능성**: Attention 가중치가 모델의 의사결정을 완전히 설명하지는 못한다는 연구 결과들이 있다. 모델이 실제로 무엇에 "주의"를 기울이는지 정확히 파악하기는 여전히 어렵다.

* **편향성 문제**: 학습 데이터의 편향이 Attention 패턴에 반영될 수 있으며, 이는 공정성 문제로 이어질 수 있다.

## 미래 전망

* **효율성 개선**: Sparse Attention, Linear Attention, Flash Attention 등 계산 효율성을 높이는 다양한 변형들이 개발되고 있다. 이를 통해 더 긴 시퀀스를 처리하거나 더 적은 자원으로 같은 성능을 달성할 수 있을 것이다.
* **특화된 Attention**: 특정 도메인이나 태스크에 최적화된 Attention 메커니즘들이 계속 개발될 것이다. 예를 들어, 그래프 구조나 계층적 구조를 고려한 Attention 등이 있다.
* **다중 모달 통합**: 텍스트, 이미지, 음성, 비디오 등을 통합적으로 처리하는 다중 모달 Attention 메커니즘이 더욱 발전할 것이다.
**하드웨어 최적화**: Attention 연산에 특화된 하드웨어나 최적화 기법들이 개발되어 더욱 빠르고 효율적인 처리가 가능해질 것이다.

## 결론적 의미

Attention 메커니즘의 등장은 단순한 기술적 개선을 넘어 **AI가 정보를 처리하는 방식 자체를 바꾸었다**. 순차적이고 제한적인 처리에서 전역적이고 동적인 처리로의 전환은 현재 우리가 경험하고 있는 대화형 AI, 창작 AI, 번역 AI 등 모든 혁신의 기반이 되었다.

특히 "어디에 주의를 기울일 것인가"라는 Attention의 핵심 아이디어는 인간의 인지 과정과 매우 유사하다. 이는 AI가 인간의 사고 과정에 한 걸음 더 가까워졌음을 의미하며, 앞으로도 더욱 자연스럽고 효과적인 인간-AI 상호작용의 기초가 될 것이다.

Attention 메커니즘의 이해는 현대 NLP 모델의 작동 원리를 파악하는 데 필수적이며, 향후 AI 발전의 방향을 예측하고 새로운 모델을 설계하는 데 중요한 기초 지식이 된다.