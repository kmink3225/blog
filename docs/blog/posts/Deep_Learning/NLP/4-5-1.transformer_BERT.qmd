---
title: "텍스트 벡터화: 신경망 기반 방법론"
subtitle: "Word2Vec, GloVe, FastText부터 ELMo, BERT, SBERT까지 문맥을 이해하는 벡터 표현 소개"
description: |
  정적 임베딩의 한계를 넘어, 단어의 문맥적 의미를 동적으로 포착하는 ELMo, BERT, GPT, SBERT와 같은 주요 문맥 기반 임베딩 모델들의 원리, 특징, 혁신적인 기여를 살펴본다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-16
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False # 발행 시 False로 변경
---

# 요약

이 문서는 단어의 의미가 문맥에 따라 변하는 현상을 효과적으로 다루기 위해 등장한 **동적/문맥적 임베딩(Contextualized Embedding)** 방법론을 탐구한다. 정적 임베딩의 한계를 지적하고, 이를 극복하기 위한 주요 모델들의 핵심 아이디어와 특징을 소개한다.

주요 내용은 다음과 같다.

*   **정적 임베딩 vs. 동적 임베딩**:
    *   정적 임베딩(예: Word2Vec, GloVe)은 단어마다 고정된 벡터를 할당하여 문맥에 따른 의미 변화(다의성)를 포착하지 못하는 한계가 있다.
    *   동적 임베딩은 동일한 단어라도 문맥에 따라 다른 벡터 표현을 생성하여 이러한 문제를 해결한다.
*   **주요 문맥 기반 임베딩 모델**:
    *   **ELMo (Embeddings from Language Models)**: 양방향 LSTM(BiLSTM)의 각 계층에서 얻은 내부 상태들을 가중합하여 문맥 정보를 풍부하게 담은 임베딩을 생성한다. 문자 단위 표현부터 시작하여 다양한 수준의 정보를 결합한다.
    *   **BERT (Bidirectional Encoder Representations from Transformers)**: 트랜스포머(Transformer)의 인코더 구조를 활용하여 문장 내 모든 단어의 양방향 문맥을 동시에 고려한다. 'Masked Language Model(MLM)'과 'Next Sentence Prediction(NSP)'이라는 두 가지 혁신적인 사전 학습(pre-training) 목표를 통해 깊은 언어 이해 능력을 학습한다. 문서 전체의 표현으로는 `[CLS]` 토큰의 출력을 사용하거나 토큰 출력들의 풀링(pooling) 결과를 활용한다.
    *   **SBERT (Sentence-BERT)**: BERT의 출력을 문장 수준의 의미론적 벡터로 효율적으로 변환하기 위해 Siamese 또는 Triplet 네트워크 구조를 사용한다. 이를 통해 문장 간 유사도 계산 및 대규모 검색 작업의 효율성을 크게 향상시킨다.
    *   **GPT (Generative Pre-trained Transformer)**: 트랜스포머의 디코더 구조를 기반으로 하는 단방향(autoregressive) 언어 모델이다. 이전 단어들을 바탕으로 다음 단어를 예측하도록 학습하며, 이 과정에서 문맥을 이해하고 생성하는 능력을 키운다. 특히, 가중치 업데이트 없이 프롬프트에 몇 가지 예시(few-shot)를 제공하는 것만으로 새로운 작업을 수행하는 'In-context Learning' 능력으로 주목받았다. 문서 표현으로는 첫 번째 토큰([BOS])의 출력을 활용하기도 한다.
*   **실용적 응용 및 평가**:
    *   이러한 모델들은 문서 분류, 정보 검색, 질의응답, 기계 번역 등 다양한 NLP 태스크에서 혁신적인 성능 향상을 가져왔다.
    *   모델 평가는 단어 유사도나 관계 유추 같은 내재적 평가(intrinsic evaluation)와 실제 다운스트림 태스크에서의 성능을 측정하는 외재적 평가(extrinsic evaluation)로 이루어진다.

이 문서를 통해 독자는 문맥을 이해하는 동적 임베딩 기술의 발전 과정과 핵심 원리를 파악하고, 다양한 NLP 문제 해결에 이를 어떻게 활용할 수 있는지에 대한 통찰을 얻을 수 있다.

# 텍스트 인코딩 및 벡터화

```
텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention
 
Transformer 이후 생성형 모델 발전 계열
├── Transformer 구조 (Vaswani et al., 2017)
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등
```

## 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩

#### ELMo (Embedding from Language Models, 2018)

* ELMo 수식: $\text{ELMo}_k^{task} = \gamma^{task} \sum_{j=0}^L s_j^{task} \mathbf{h}_{k,j}^{LM}$
   * **$\mathbf{h}_{k,j}^{LM}$: 각 레이어의 hidden state**

   ```python
   # 예시: 3층 BiLSTM에서 "bank" 단어 (k번째 위치)
   h_{bank,0} = character_embedding("bank")     # 레이어 0 (입력)
   h_{bank,1} = first_LSTM_layer_output        # 레이어 1  
   h_{bank,2} = second_LSTM_layer_output       # 레이어 2
   h_{bank,3} = third_LSTM_layer_output        # 레이어 3 (최상위)
   ```
   * **$s_j^{task}$: 학습 가능한 가중치**
      * 각 레이어의 중요도를 태스크별로 학습
      * 문법적 태스크 → 낮은 레이어 중시
      * 의미적 태스크 → 높은 레이어 중시
   * **$\gamma^{task}$: 전체 스케일 조정**
      * ELMo 벡터의 전체적인 크기 조정
* 계산 예시

```python
# "bank" 단어의 ELMo 벡터 (감정 분석 태스크)
h_0 = [0.1, 0.2, 0.3]  # 문자 레벨
h_1 = [0.4, 0.5, 0.6]  # 낮은 레벨 (문법적)  
h_2 = [0.7, 0.8, 0.9]  # 높은 레벨 (의미적)

s_0 = 0.1  # 문자 레벨 가중치 (낮음)
s_1 = 0.3  # 문법 레벨 가중치  
s_2 = 0.6  # 의미 레벨 가중치 (높음)

ELMo_bank = γ × (s_0×h_0 + s_1×h_1 + s_2×h_2)
          = 2.0 × (0.1×[0.1,0.2,0.3] + 0.3×[0.4,0.5,0.6] + 0.6×[0.7,0.8,0.9])
          = 2.0 × [0.55, 0.65, 0.75]
          = [1.1, 1.3, 1.5]
```

* 양방향 정보의 중요성
   * **Forward만 사용할 경우:**

   ```
   "The bank was closed because of ___"
   → "bank"를 이해할 때 "The"만 참고
   ```

   * **Backward까지 사용할 경우:**  

   ```
   "The bank was closed because of ___"
   → "bank"를 이해할 때 "was closed" 정보도 참고
   → 금융 기관으로 해석 가능성 증가
   ```

#### BERT (Bidirectional Encoder Representations from Transformers, 2018)

* 양방향 문맥 동시 고려
   * 15% 단어를 마스킹하여 예측
   * 문장 간 관계 학습
* **핵심 혁신:**
   * **Transformer 기반**: 양방향 문맥 동시 고려
      * **기존 RNN의 한계:**

         ```python
         # RNN은 순차적 처리 (병렬화 어려움)
         h_1 = RNN(x_1)
         h_2 = RNN(x_2, h_1)      # h_1이 완료되어야 시작 가능
         h_3 = RNN(x_3, h_2)      # h_2가 완료되어야 시작 가능
         ```

      * **Transformer의 장점:**

         ```python
         # 모든 위치를 동시에 처리 (병렬화 가능)
         attention_weights = compute_attention(all_words)
         all_representations = apply_attention(all_words, attention_weights)
         ```

   * **Masked Language Model**: 15% 단어를 마스킹하여 예측
      * BERT의 핵심 학습 방법
         * **기본 아이디어**: 일부 단어를 숨기고 맞추게 하기

         ```python
         # 원본 문장
         "나는 [MASK]를 좋아한다"

         # 모델이 학습하는 것
         P("사과" | "나는 [MASK]를 좋아한다") = 0.7
         P("바나나" | "나는 [MASK]를 좋아한다") = 0.2  
         P("컴퓨터" | "나는 [MASK]를 좋아한다") = 0.01
         ```

         * **15% 마스킹 전략:**

         ```python
         입력 문장의 15% 단어에 대해:
         - 80%: [MASK] 토큰으로 교체
         - 10%: 랜덤한 다른 단어로 교체  
         - 10%: 원래 단어 그대로 유지
         ```

         * **왜 이렇게 하는가?**
            ```python
            # 80% [MASK]: 메인 학습 목적
            "나는 [MASK]를 좋아한다"

            # 10% 랜덤 교체: 노이즈에 강한 표현 학습
            "나는 컴퓨터를 좋아한다"  # 원래는 "사과"

            # 10% 원본 유지: 실제 사용 시와 동일한 조건
            "나는 사과를 좋아한다"
            ```

   * **Next Sentence Prediction**: 문장 간 관계 학습
   
   ```python
   # 실제 연속된 문장 (Positive)
   문장A: "나는 아침에 일어났다"
   문장B: "그리고 아침 식사를 했다"
   Label: IsNext = True

   # 랜덤하게 조합된 문장 (Negative)  
   문장A: "나는 아침에 일어났다"
   문장B: "축구는 재미있는 스포츠다"
   Label: IsNext = False
   ```

* **BERT의 문서 벡터화 방법:**
   * **[CLS] 토큰**: 문장/문서 전체 표현

   ```python
   입력: "[CLS] 문장 내용 [SEP]"
   출력: [CLS]_벡터가 전체 문장의 의미를 담음

   # 예시
   input_tokens = ["[CLS]", "나는", "사과를", "좋아한다", "[SEP]"]
   bert_output = bert_model(input_tokens)
   sentence_vector = bert_output[0]  # [CLS] 위치의 벡터
   ```

   * **Pooling 전략**: 
      * Mean pooling: $\frac{1}{n}\sum_{i=1}^n \mathbf{h}_i$
      * Max pooling: $\max(\mathbf{h}_1, ..., \mathbf{h}_n)$

      ```python
      # 모든 토큰의 BERT 출력
      token_representations = [
         [0.1, 0.2, 0.3],  # [CLS]
         [0.4, 0.5, 0.6],  # "나는"  
         [0.7, 0.8, 0.9],  # "사과를"
         [0.2, 0.3, 0.4],  # "좋아한다"
         [0.5, 0.6, 0.7],  # [SEP]
      ]

      # Mean Pooling
      mean_vector = mean(token_representations[1:-1])  # [CLS], [SEP] 제외
      = (0.4+0.7+0.2)/3, (0.5+0.8+0.3)/3, (0.6+0.9+0.4)/3
      = [0.43, 0.53, 0.63]

      # Max Pooling  
      max_vector = max(token_representations[1:-1])  # 각 차원별 최댓값
      = [0.7, 0.8, 0.9]
      ```
*   **특징**:
    *   단어의 의미적, 문법적 정보를 벡터 공간에 학습.
    *   벡터 간 연산을 통해 단어 간 유사도, 유추 등 관계 표현 가능 (예: "king" - "man" + "woman" ≈ "queen").
*   **중요성**: 현대 NLP 딥러닝 모델의 핵심 구성 요소로, 성능 향상에 크게 기여.



#### SBERT (Sentence-BERT)

* 최근 가장 보편적인 문장 또는 문서 임베딩 방법으로 SBERT가 이용된다.
* 문서의 유사도를 구할 때는 SBERT 사용을 권장
* 문장 벡터화 전략
   * 문장 간 유사도 계산
   * 문장 간 유사도 계산 시 문장 임베딩 사용
* 기존 BERT의 한계: 문장 유사도 계산의 비효율성

   ```python
   # 1000개 문장의 유사도를 모두 구하려면
   sentences = ["문장1", "문장2", ..., "문장1000"]

   # 기존 BERT 방식 (비효율적)
   for i in range(1000):
      for j in range(i+1, 1000):
         combined = f"[CLS] {sentences[i]} [SEP] {sentences[j]} [SEP]"
         similarity = bert_classifier(combined)  # 매번 BERT 실행
         
   # 총 계산 횟수: 1000 × 999 / 2 = 499,500번!
   ```

   * BERT로 문장 유사도를 계산하려면:
      * 두 문장을 [SEP]로 연결
      * BERT에 입력하여 분류
      * $O(n^2)$ 시간 복잡도 (n개 문장 비교 시)
* SBERT의 해결책: Siamese Network 구조

```python
# SBERT 방식 (효율적)

문장 A → BERT → Pooling → Vector A
문장 B → BERT → Pooling → Vector B
유사도 = cosine_similarity(Vector A, Vector B)

# 1단계: 모든 문장을 미리 벡터화
sentence_vectors = []
for sentence in sentences:
    vector = sbert_model(sentence)  # 각 문장마다 1번씩만 실행
    sentence_vectors.append(vector)

# 2단계: 벡터 간 코사인 유사도로 빠른 계산
for i in range(1000):
    for j in range(i+1, 1000):
        similarity = cosine_similarity(sentence_vectors[i], sentence_vectors[j])
        
# 총 SBERT 실행 횟수: 1000번 (대폭 감소!)
```

* **학습 목적 함수:**
   * **Classification**: $\mathcal{L} = -\sum_{i} y_i \log(\text{softmax}(W[\mathbf{u}; \mathbf{v}; |\mathbf{u}-\mathbf{v}|]))$

   ```python
   # 두 문장의 SBERT 벡터
   u = sbert("나는 사과를 좋아한다")      # [0.2, 0.4, 0.1, ...]
   v = sbert("나는 바나나를 좋아한다")    # [0.3, 0.5, 0.2, ...]

   # 특성 벡터 구성
   concat = [u; v]                    # 연결: [0.2, 0.4, 0.1, 0.3, 0.5, 0.2, ...]
   abs_diff = |u - v|                # 절댓값 차이: [0.1, 0.1, 0.1, ...]
   features = [u; v; abs_diff]        # 최종 특성 벡터

   # 분류 (유사/비유사)
   logits = W @ features + b
   probability = softmax(logits)
   loss = cross_entropy(probability, true_label)
   ```

   * **Regression**: $\mathcal{L} = \text{MSE}(\text{cosine\_sim}(\mathbf{u}, \mathbf{v}), \text{label})$
   
   ```python
   # 예측 유사도
   predicted_sim = cosine_similarity(u, v) = 0.85

   # 실제 라벨 (0~1 점수)
   true_sim = 0.9  # 사람이 평가한 유사도

   # 손실 계산
   loss = (predicted_sim - true_sim)² = (0.85 - 0.9)² = 0.0025
   ```

* **성능 개선:**
   * 시간 복잡도: $O(n^2) \rightarrow O(n)$

   ```python
   # 시간 복잡도 비교
   기존_BERT_시간 = O(n²) = 1000² = 1,000,000
   SBERT_시간 = O(n) = 1000

   속도_향상 = 1,000,000 / 1000 = 1000배!
   ```
   * 의미적 유사도 정확도 대폭 향상
   * 대규모 문서 검색 시스템
   * 실시간 문장 유사도 계산
   * 추천 시스템에서의 텍스트 매칭

#### GPT(Generative Pre-trained Transformer)

* 단방향 언어 모델의 핵심 개념
* BERT vs GPT의 근본적 차이
* **BERT (양방향)**:
   
   ```
   입력: "나는 [MASK]를 좋아한다"
   모델이 보는 정보: "나는" + "를 좋아한다" (양쪽 모두)
   예측: [MASK] = "사과"
   ```

* **GPT (단방향)**:
   
   ```
   입력: "나는 사과를"
   모델이 보는 정보: "나는 사과를" (왼쪽만)
   예측: 다음 단어 = "좋아한다"
   ```

* 왜 단방향일까?
   * **생성 태스크의 특성**:

      ```python
      # 실제 텍스트 생성 시
      "안녕하세요, 오늘 날씨가"
      → 모델: "좋네요" (미래 정보는 알 수 없음)

      # 만약 양방향이라면?
      "안녕하세요, 오늘 날씨가 [미래정보] 입니다"
      → 실제 생성 시에는 미래 정보가 없으므로 불일치
      ```

* **GPT의 학습 방식: Autoregressive Language Modeling**
   * 이전 토큰들로 다음 토큰 예측
   * 수학적 목적 함수: $P(\text{문장}) = \prod_{t=1}^T P(w_t | w_1, w_2, ..., w_{t-1})$
      * 문장의 확률 = 각 단어가 이전 단어들 조건 하에 나타날 확률의 곱
   * 구체적 학습 예시
      * **훈련 문장**: "나는 사과를 좋아한다"

      ```python
      # 학습 데이터 구성
      입력 → 정답
      "나는" → "사과를"
      "나는 사과를" → "좋아한다"  
      "나는 사과를 좋아한다" → "<끝>"

      # 손실 함수
      loss = -log P("사과를" | "나는") 
            -log P("좋아한다" | "나는 사과를")
            -log P("<끝>" | "나는 사과를 좋아한다")
      ```

   * Causal Masking (인과 마스킹)
      * Attention에서 미래 정보 차단
      ```python
      # Attention Matrix (4개 단어 예시)
              나는  사과를  좋아한다  <끝>
      나는     ✓     ✗      ✗      ✗
      사과를    ✓     ✓      ✗      ✗  
      좋아한다  ✓     ✓      ✓      ✗
      <끝>     ✓     ✓      ✓      ✓
      
      # ✓: 참고 가능, ✗: 마스킹 (참고 불가)
      ```

   * **코드 구현**:

      ```python
      # 마스킹 행렬
      mask = torch.tril(torch.ones(seq_len, seq_len))
      # 상삼각 부분을 -무한대로 설정
      attention_scores.masked_fill_(mask == 0, -1e9)
      attention_weights = softmax(attention_scores)
      ```

* **첫 번째 토큰을 문서 표현으로 활용**
   * 정보 흐름의 특성
      * 입력: "[BOS] 문장 내용들..."
      * 각 토큰이 보는 정보량
      * 토큰1 ([BOS]): 자기 자신만
      * 토큰2: [BOS] + 토큰2  
      * 토큰3: [BOS] + 토큰2 + 토큰3
* **왜 첫 번째 토큰인가?**
   * 정보 흐름의 특성
      * 입력: "[BOS] 문장 내용들..."
      * 각 토큰이 보는 정보량
      * 토큰1 ([BOS]): 자기 자신만
      * 토큰2: [BOS] + 토큰2  
      * 토큰3: [BOS] + 토큰2 + 토큰3
      * 마지막토큰: [BOS] + 전체 문장
      * 역설적으로, [BOS]는 전체 문장을 "예측"해야 하므로
      * 전체 문장 정보를 압축한 표현을 학습하게 됨
   * 구체적 메커니즘
      * 학습 과정에서의 압축

      ```python
      # GPT가 학습하는 것
      P(전체_문장 | [BOS]) = P(w1|[BOS]) × P(w2|[BOS],w1) × ... × P(wn|[BOS],w1,...,wn-1)

      # [BOS] 토큰은 "이 문장이 어떤 내용일까?"를 예측해야 함
      # → 문장의 주제, 감정, 스타일 등을 함축하는 표현을 학습
      ```

* **실제 활용 예시**:

   ```python
   # 문서 분류
   document = "[BOS] 이 영화는 정말 재미있었다. 스토리도 좋고..."
   gpt_output = gpt_model(document)
   document_vector = gpt_output[0]  # [BOS] 위치의 벡터
   classification = classifier(document_vector)  # 긍정/부정 분류
   ```

* **In-context Learning 심화 분석**
   * 기존 학습 방식과의 차이
      * **전통적 학습 (Fine-tuning)**:

         ```python
         # 1단계: 새로운 태스크 데이터로 모델 가중치 업데이트
         model.train()
         for batch in task_data:
            loss = compute_loss(model(batch.input), batch.target)
            loss.backward()
            optimizer.step()

         # 2단계: 추론
         prediction = model(new_input)
         ```

      * **In-context Learning**:

         ```python
         # 가중치 업데이트 없이, 입력에 예시를 포함
         context = """
         번역 예시:
         영어: Hello → 한국어: 안녕하세요
         영어: Thank you → 한국어: 감사합니다  
         영어: Good morning → 한국어: 좋은 아침

         영어: How are you? → 한국어:
         """

         result = gpt_model(context)  # "어떻게 지내세요?" 출력
         ```

* **왜 In-context Learning이 가능한가?**
   * 패턴 인식 능력
      * GPT가 학습 중 본 패턴들
      * "A는 B이다. C는 D이다. E는"  → F 예측
      * "1+1=2, 2+2=4, 3+3=" → 6 예측
      * "cat→고양이, dog→개, bird→" → 새 예측
   * 메타 학습 (Learning to Learn)
    
      ```python
      # 다양한 패턴을 학습하면서 "학습하는 방법"을 학습
      패턴1: 번역 (A→B 형태)
      패턴2: 수학 (계산 규칙)  
      패턴3: 분류 (라벨링 규칙)

      # 새로운 패턴이 주어져도 빠르게 적응
      ```

* **실제 In-context Learning 예시**
   * **감정 분석 태스크**:

      ```python
      prompt = """
      다음은 리뷰와 감정을 분류한 예시입니다:

      리뷰: "이 영화 정말 재미있어요!" 감정: 긍정
      리뷰: "시간 낭비였습니다." 감정: 부정
      리뷰: "그냥 그래요." 감정: 중립

      리뷰: "배우들 연기가 훌륭했습니다!" 감정:
      """

      # GPT 출력: "긍정"
      ```
   * **번역 태스크**:
   
      ```python
      prompt = """
      English to Korean translation:

      English: I love programming
      Korean: 나는 프로그래밍을 좋아합니다

      English: The weather is nice today  
      Korean: 오늘 날씨가 좋네요

      English: What time is it now?
      Korean:
      """

      # GPT 출력: "지금 몇 시인가요?"
      ```

* **GPT 발전사와 특징**
   * **GPT-1 (2018)**
      * 크기: 117M 파라미터
      * 특징: Transformer 디코더만 사용
      * 성능: 간단한 텍스트 생성
   * **GPT-2 (2019)**
      * 크기: 1.5B 파라미터
      * 특징: 스케일 확장의 효과 입증
      * 성능: 일관성 있는 긴 텍스트 생성
   * **GPT-3 (2020)**
      * 크기: 175B 파라미터  
      * 특징: In-context Learning의 강력한 능력
      * 성능: Few-shot Learning으로 다양한 태스크 수행
   * **GPT-4 (2023)**
      * 크기: 공개되지 않음 (추정 수조 개)
      * 특징: 멀티모달 (텍스트 + 이미지)
      * 성능: 인간 수준에 근접한 성능

* **GPT vs BERT 비교 정리**

| 측면 | GPT | BERT |
|------|-----|------|
| **방향성** | 단방향 (왼쪽→오른쪽) | 양방향 |
| **학습 목표** | 다음 토큰 예측 | 마스킹된 토큰 예측 |
| **주요 용도** | 생성 태스크 | 이해 태스크 |
| **문서 벡터** | 첫 번째 토큰 | [CLS] 토큰 |
| **특별 능력** | In-context Learning | Fine-tuning 효율성 |

* **결론**: GPT는 "다음에 올 단어를 예측"하는 단순한 목표로 학습하지만, 이 과정에서 언어의 패턴, 의미, 추론 능력까지 학습하게 되어 강력한 생성 및 추론 모델이 되었다.

### 실용적 응용 및 평가

#### 평가 지표

**Intrinsic Evaluation (내재적 평가):**
- **단어 유사도**: WordSim-353, SimLex-999 - 사람이 평가한 단어 유사도와 모델 예측의 상관관계 측정
- **단어 관계**: "king - man + woman = queen" - 벡터 연산으로 의미 관계 포착 정도 평가

**Extrinsic Evaluation (외재적 평가):**
- **문서 분류 정확도**: 실제 분류 태스크에서의 성능
- **정보 검색 성능**: NDCG, MAP - 검색 결과의 관련성 및 순위 정확도
- **의미적 텍스트 유사도**: STS benchmark - 문장 간 의미적 유사성 예측 성능

#### 모델 선택 가이드

- **소규모 데이터**: FastText (OOV 처리)
- **대규모 문서 분류**: BERT fine-tuning
- **실시간 유사도 계산**: SBERT
- **창작/생성 태스크**: GPT 계열

#### 통계적 해석

임베딩 공간에서의 기하학적 관계:
$$\cos(\mathbf{v}_{\text{similar words}}) > \cos(\mathbf{v}_{\text{dissimilar words}})$$

**시각화 도구**: t-SNE/UMAP을 통한 의미적 클러스터링 확인


## 결론

본 문서에서는 단어의 고정된 의미 표현을 넘어, 문맥에 따라 유연하게 변화하는 의미를 포착하는 동적 임베딩 방법론들을 심층적으로 살펴보았다. ELMo에서 시작하여 BERT, GPT, SBERT에 이르기까지, 이러한 문맥 기반 임베딩 모델들은 자연어 처리(NLP) 분야에 혁명적인 발전을 가져왔다.

주요 내용을 다시 한번 정리하면 다음과 같다.

*   **정적 임베딩의 한계 극복**: 초기의 워드 임베딩(Word2Vec, GloVe 등)은 단어의 의미를 단일 벡터로 표현하여 문맥에 따른 다의성을 반영하지 못했다. 동적 임베딩은 이 한계를 극복하고, 동일한 단어라도 문맥에 따라 다른 벡터 표현을 생성함으로써 보다 정교한 의미 이해를 가능하게 했다.

*   **주요 모델들의 혁신과 기여**:
    *   **ELMo**: 양방향 LSTM을 통해 문맥 정보를 통합하고, 여러 계층의 표현을 활용하여 풍부한 임베딩을 제공했다.
    *   **BERT**: 트랜스포머 아키텍처와 Masked Language Model, Next Sentence Prediction과 같은 혁신적인 사전 학습 방식을 도입하여 양방향 문맥 이해의 새로운 지평을 열었다. 이는 다양한 NLP 다운스트림 태스크에서 SOTA(State-of-the-Art) 성능을 달성하는 데 크게 기여했다.
    *   **GPT**: 단방향 트랜스포머 디코더를 기반으로 강력한 텍스트 생성 능력을 보여주었으며, 특히 GPT-3 이후 모델들은 In-context Learning이라는 새로운 패러다임을 제시하며 모델 활용의 유연성을 크게 확장했다.
    *   **SBERT**: 기존 BERT 모델을 문장 임베딩 생성에 효율적으로 사용할 수 있도록 Siamese 및 Triplet 네트워크 구조를 활용하여, 의미적으로 유사한 문장 벡터를 효과적으로 생성하고 문장 간 유사도 비교 작업의 속도와 정확도를 크게 향상시켰다.

*   **패러다임의 전환과 LLM의 토대**: 이러한 문맥 기반 임베딩 모델들의 발전은 단순한 특징 추출기를 넘어, 언어 자체를 깊이 이해하고 생성할 수 있는 대규모 언어 모델(Large Language Models, LLMs) 시대로 나아가는 핵심적인 발판이 되었다. 사전 학습과 미세 조정(fine-tuning) 패러다임, 그리고 최근의 프롬프트 기반 학습은 모델의 활용 범위를 크게 넓혔다.

*   **적절한 전략 선택의 지속적 중요성**: 해결하고자 하는 특정 문제의 요구사항, 가용 데이터의 특성, 계산 자원 등을 고려하여 가장 적합한 임베딩 전략과 모델을 선택하는 것은 여전히 중요하다. 실용적인 응용을 위해서는 모델의 성능뿐만 아니라 효율성, 해석 가능성 등도 함께 고려해야 한다.

문맥을 이해하는 텍스트 벡터화 기술은 앞으로도 계속 발전하여, 기계가 인간의 언어를 더욱 정교하게 이해하고 상호작용하는 미래를 앞당길 것이다. 이러한 기술의 발전은 정보 검색, 질의응답, 창작, 교육 등 사회 여러 분야에 걸쳐 혁신적인 변화를 주도할 잠재력을 지니고 있다.
