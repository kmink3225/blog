---
title: "텍스트 벡터화: Attention 메커니즘의 이해"
subtitle: "시퀀스 모델링의 혁신, Attention의 구조와 원리"
description: |
  기존 Seq2Seq 모델의 한계를 극복하기 위해 제안된 Attention 메커니즘의 핵심 원리와 작동 방식을 설명한다. Attention이 어떻게 시퀀스 데이터의 장기 의존성 문제를 해결하고, 입력과 출력 간의 관계를 효과적으로 모델링할 수 있는지 살펴본다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-16
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False # 발행 시 False로 변경
---

# 요약

이 문서는 기존 Seq2Seq 모델의 한계를 극복하기 위해 제안된 Attention 메커니즘의 기본 원리와 구조를 소개한다. Attention은 입력 시퀀스의 각 요소에 대한 중요도를 동적으로 계산하여 출력 시퀀스 생성 시 필요한 정보를 효과적으로 활용할 수 있게 해주는 혁신적인 접근 방식이다.

주요 내용은 다음과 같다.

*   **Seq2Seq 모델의 한계와 Attention의 등장**:
    *   기존 Seq2Seq 모델은 입력 문장의 모든 정보를 고정된 크기의 컨텍스트 벡터에 압축하는 과정에서 정보 손실이 발생한다.
    *   Attention은 이러한 한계를 극복하기 위해 입력 시퀀스의 각 요소에 대한 중요도를 동적으로 계산하여 필요한 정보를 선택적으로 활용한다.
*   **Attention의 핵심 구성 요소 및 작동 원리**:
    *   **Query, Key, Value**: 입력 시퀀스의 각 요소를 Query, Key, Value로 변환하여 유사도와 중요도를 계산한다.
    *   **Attention Score**: Query와 Key의 유사도를 계산하여 각 입력 요소의 중요도를 결정한다.
    *   **Attention Weight**: Attention Score를 정규화하여 각 입력 요소에 대한 가중치를 생성한다.
    *   **Context Vector**: Attention Weight와 Value를 결합하여 최종 컨텍스트 벡터를 생성한다.
*   **Attention의 장점**:
    *   입력 시퀀스의 길이에 상관없이 모든 정보를 효과적으로 활용할 수 있다.
    *   출력 시퀀스 생성 시 필요한 정보를 선택적으로 집중할 수 있다.
    *   모델의 해석 가능성을 높이고, 장기 의존성 문제를 효과적으로 해결한다.
*   **의의**: Attention은 자연어 처리, 컴퓨터 비전 등 다양한 분야에서 혁신적인 성능을 보여주며, Transformer와 같은 최신 모델의 기반이 되었다.

이 문서를 통해 독자는 Attention이 어떻게 시퀀스 데이터의 장기 의존성 문제를 해결하고, 입력과 출력 간의 관계를 효과적으로 모델링할 수 있는지에 대한 기본적인 이해를 얻을 수 있다.

# 텍스트 인코딩 및 벡터화

```
텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention
 
Transformer 이후 생성형 모델 발전 계열
├── Transformer 구조 (Vaswani et al., 2017)
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등
```

# Attention

## 기존 Sequence to Sequence, Seq2Seq의 한계

- 입력 문장의 길이와 상관없이 고정된 크기의 벡터에 정보를 모두 압축한다.
- Bottleneck 문제: 입력 문장의 길이가 길어질수록 고정된 크기의 벡터에 정보가 다 압축되지 않아 정보 손실이 발생한다.
- Encoder -> Context Vector -> Decoder
    - 단어를 Embedding 후 Encoder에서 벡터화되어 Context Vector가 됨
    - 즉, Context Vector는 Encoder의 LSTM의 마지막 hidden state의 출력값 (벡터 크기가 고정)
    - 이 고정된 크기에 정보가 모두 압축되지 않는다면 정보 손실이 발생
    - 벡터 크기가 고정되어 있기 때문에 입력 문장의 길이가 길어질수록 정보 손실이 발생한다.
    - 입력 문장의 길이가 길어질수록 정보 손실이 발생한다.
    - 정보 손실이 일어난 Context Vector가 Decoder의 Input 벡터가 됨 
- RNN 자체 문제: RNN 계열의 고질적인 장기 의존성 문제로 초기 정보가 손실되며 전달된다.
    - LSTM과 GRU가 장기 손실을 줄이기 위해 고안된 모델이지만 여전히 장기 의존성 문제가 발생한다.
    - 전체 한 문장도 잘 기억 못함 (even with LSTM, GRU)
    - Carnegie Mellon University의 연구 : BLEU Score 측정 

## Attention의 정의

- 사전적 의미: 주의, 집중
- NLP에서의 의미: 번역 문장을 만드는 과정에서 기존 문장에서 주용한 단어를 집중(Attention)
- 예: I am a good student를 한글로 번역할 때 각 문장에서 주요 단어에 집중하여 번역
    1. `I` 에 집중 -> 나는    
    2. `good` 에 집중 -> 나는 좋은
    3. `student` 에 집중 -> 나는 좋은 학생
    4. `am` 에 집중 -> 나는 좋은 학생이다.
- 단어를 생성할 때 기존에 선택한 단어의 유사도와 문맥을 고려하여 다음 단어를 선택

## 원리

### Key Value 형태로 학습

* Attention 메커니즘: Attention(Q, K, V) = Attention Value
    - Q: Query, K: Key, V: Value
    - Q,K,V를 입력받아 Attention Value를 출력
    
    1. 어텐션 함수는 주어진 쿼리(Q)에 대해서 모든 키(K)와의 유사도를 각 각 계산한다.
    2. 구해낸 이 유사도를 키(K)와 맵핑되어있는 각각의 값(V)에 곱하여 반영해준다.
    3. 유사도가 반영된 값을 모두 더해서 리턴한다.
    4. 이렇게 출력된 값을 어테션 값 (attention value)이라고 한다.
    5. 이 어테션 값을 출력으로 사용한다.
* 예를 들어 Q 1개, Key 3개, Value 3개가 있다면
    - Q와 K1, K2, K3의 유사도를 계산: $Q \cdot K1$, $Q \cdot K2$, $Q \cdot K3$ 
    - 유사도를 V1, V2, V3에 곱하여 반영: $V'1= K1 \cdot V1$, $V'2= K2 \cdot V2$, $V'3= K3 \cdot V3$
    - attention score = 유사도가 반영된 값 $V'1$ , $V'2$ , $V'3$
    - softmax([$V'1$ , $V'2$ , $V'3$])을 구함
    - attention value, a1 = 위의 3값과 hidden state의 값을 내적하여 모두 더함
    - 즉, a1은 decoder의 예측 단어와 입력단어들의 유사도 정보가 있음. (유사도가 높으면 가중치가 높게 부여되어 반영됨)           
    - a1과 decoder의 마지막 시점의 hidden state를 내적하여 tanh를 취하여 출력
    - $y_t = \text{softmax}(W_y\tilde{s_t}+b_y)$
* 참고로 Seq2Seq의 hidden state가 Key이자 Value가 역할을 한다.

### 수식

1. $\text{score}_i = Q \cdot K_i$
2. $\alpha_i = \text{softmax}(\text{score}_i)$
3. $a_t = \sum_i \alpha_i V_i$
4. $\tilde{s_t} = \tanh(W_c [a_t; s_t])$
5. $y_t = \text{softmax}(W_y \tilde{s_t} + b_y)$

1. **유사도(Score) 계산: Query와 Key의 내적**

$$
\text{score}_i = Q \cdot K_i \quad \text{for } i = 1, 2, 3
$$

혹은 전체를 벡터화하면:
$$
\text{scores} = Q K^T \quad \text{(Q: 1×d, K: 3×d → scores: 1×3)}
$$

> Scaled Dot-Product Attention에서는 보통 다음과 같이 스케일 조정:

$$
\text{scores} = \frac{Q K^T}{\sqrt{d_k}}
$$

2. **Softmax로 유사도 정규화 (Attention Weights)**

$$
\alpha_i = \frac{\exp(\text{score}_i)}{\sum_{j=1}^{3} \exp(\text{score}_j)}
\quad \text{(i = 1, 2, 3)}
$$

또는 벡터 전체:

$$
\boldsymbol{\alpha} = \text{softmax}(Q K^T)
$$

---

3. **각 Value 벡터에 가중치를 곱해 합산 (Attention Output)**

$$
\text{Attention}(Q, K, V) = \sum_{i=1}^{3} \alpha_i V_i = \boldsymbol{\alpha} \cdot V
$$

즉, 전체 수식은:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
$$


4. **디코더 hidden state와 결합 후 출력 계산**

* context vector (a₁)와 디코더의 hidden state $s_t$ 결합:

$$
\tilde{s_t} = \tanh(W_c [a_t; s_t])
$$

* 최종 출력 (예측 단어 확률 분포):

$$
y_t = \text{softmax}(W_y \tilde{s_t} + b_y)
$$


## 강점

* RNN계열 Seq2Seq 구조에 도입되어 기계 번역의 성능을 상당 부분 개선
* 후에, attention으로 모든 state에 접근하여 더 나은 성능을 보임 = Attention만으로도 성능 월등
* 결국, RNN은 필요하지 않게 되었음 = 모든 정보를 벡터화하여 저장하는 것이 아니라 중요한 정보만 저장하고 있으면 됨
* 후에 이를 바탕으로 발전된 기술이 Transformer (Attention is all you need.)

## 결론

본 문서에서는 Attention 메커니즘의 기본 원리와 작동 방식을 살펴보았다. Attention은 시퀀스 데이터 처리에서 중요한 정보에 집중할 수 있게 해주는 핵심적인 기술로, 기계 번역을 비롯한 다양한 자연어 처리 태스크에서 혁신적인 성능 향상을 가져왔다.

* **Attention의 핵심 원리 요약**:
    * Attention은 Query, Key, Value 세 가지 요소를 기반으로 작동하며, Query와 Key의 유사도를 계산하여 Value에 대한 가중치를 결정한다.
    * Softmax를 통해 정규화된 가중치를 사용하여 중요한 정보에 더 집중할 수 있게 해주며, 이를 통해 문맥에 따른 적절한 정보 선택이 가능해진다.
* **RNN 기반 모델과의 관계 및 장점**:
    * 기존 RNN 기반 Seq2Seq 모델의 한계를 극복하여, 긴 시퀀스에서도 중요한 정보를 효과적으로 포착할 수 있게 되었다.
    * 모든 입력 정보에 직접 접근할 수 있어 장기 의존성 문제를 해결하고, 더 정확한 번역과 생성이 가능해졌다.
* **Transformer로의 발전**:
    * Attention 메커니즘의 성공은 RNN을 완전히 대체하는 Transformer 아키텍처의 등장으로 이어졌다.
    * "Attention is all you need"라는 명제가 증명되었듯이, Attention만으로도 뛰어난 성능을 보일 수 있음을 보여주었다.

결론적으로, Attention 메커니즘은 자연어 처리 분야에서 혁신적인 변화를 가져온 핵심 기술이다. RNN의 한계를 극복하고 Transformer의 등장을 이끌어냄으로써, 현대 자연어 처리의 기반을 마련했으며, 이는 BERT, GPT와 같은 혁신적인 모델들의 등장으로 이어졌다.
