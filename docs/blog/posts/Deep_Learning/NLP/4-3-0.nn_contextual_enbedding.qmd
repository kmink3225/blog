---
title: "텍스트 벡터화: 문맥 기반 임베딩의 이해"
subtitle: "ELMo, BERT, GPT, SBERT 등 주요 모델 소개"
description: |
  단어의 의미를 문맥에 따라 동적으로 표현하는 문맥 기반 임베딩(Contextual Embedding)의 기본 원리를 소개한다. ELMo, BERT, GPT, SBERT 등 주요 모델들의 핵심 아이디어와 특징을 간략히 살펴본다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-09
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False # 발행 시 False로 변경
---

# 요약

이 문서는 단어의 의미가 문맥에 따라 달라지는 점을 반영하는 **동적/문맥적 임베딩(Contextualized Embedding)** 방법론의 기본 개념을 소개한다. 정적 임베딩의 한계를 간략히 언급하고, 초기 RNN 기반의 시도부터 ELMo, 그리고 트랜스포머 기반의 BERT, GPT, SBERT 등 주요 모델들의 핵심 아이디어와 특징을 살펴본다.

주요 내용은 다음과 같다.

*   **정적 임베딩 vs. 동적 임베딩**:
    *   정적 임베딩(예: Word2Vec)은 단어마다 하나의 고정된 벡터를 할당하여, 같은 단어라도 문맥에 따라 의미가 달라지는 다의성을 처리하기 어렵다.
    *   동적 임베딩은 문맥을 고려하여 동일한 단어라도 상황에 맞는 다른 벡터 표현을 생성한다. 이는 순차적 데이터 처리에 강점을 가진 RNN 계열 모델들을 통해 처음 시도되었고, 이후 ELMo나 트랜스포머 기반 모델들로 발전했다.
*   **주요 문맥 기반 임베딩 모델 소개**:
    *   **ELMo (Embeddings from Language Models)**: 순방향과 역방향 LSTM(Bidirectional LSTM, BiLSTM)을 독립적으로 학습시킨 후, 각 계층에서 얻은 내부 상태들을 문자 단위 표현부터 단어 수준의 의미까지 종합적으로 활용하여 문맥 정보를 풍부하게 담은 임베딩을 생성한다. 이는 깊은(deep) 언어 표현 학습의 가능성을 보여주었다.
    *   **BERT (Bidirectional Encoder Representations from Transformers)**: 트랜스포머(Transformer)의 인코더 구조를 사용하여 문장 내 모든 단어의 양방향 문맥을 동시에, 그리고 깊게 고려한다. 특히 'Masked Language Model(MLM)'을 통해 단어를 예측하고, 'Next Sentence Prediction(NSP)'으로 문장 간의 관계를 학습하는 혁신적인 사전 학습(pre-training) 목표를 통해 뛰어난 언어 이해 능력을 보여준다. 문장 전체의 표현으로는 특수 토큰 `[CLS]`의 최종 출력을 사용하거나, 모든 토큰 출력의 평균/최대 풀링(pooling) 등을 활용한다.
    *   **SBERT (Sentence-BERT)**: BERT와 같은 트랜스포머 모델의 출력을 문장 수준의 고정된 크기 의미 벡터로 효율적으로 변환하기 위해 Siamese 또는 Triplet 네트워크 구조를 사용한다. 이를 통해 문장 간 유사도 계산이나 의미 검색 작업의 효율성과 정확성을 크게 향상시킨다.
    *   **GPT (Generative Pre-trained Transformer)**: 트랜스포머의 디코더 구조를 기반으로 하는 단방향(autoregressive) 언어 모델이다. 이전 단어들을 바탕으로 다음 단어를 순차적으로 예측하도록 학습하며, 이 과정에서 문맥을 이해하고 자연스러운 텍스트를 생성하는 능력을 키운다. 특히, 별도의 미세 조정 없이 프롬프트에 몇 가지 예시(few-shot)를 제공하는 것만으로도 새로운 작업을 수행할 수 있는 'In-context Learning' 능력으로 큰 주목을 받았다.
*   **활용**: 이러한 모델들은 문서 분류, 질의응답, 기계 번역 등 다양한 NLP 태스크에서 기존 방법론들의 성능을 크게 뛰어넘는 결과를 가져왔다.

이 문서를 통해 독자는 문맥 기반 임베딩 기술의 발전 과정과 주요 모델들의 기본적인 아이디어 및 특징을 이해할 수 있다.

# 텍스트 인코딩 및 벡터화

텍스트 벡터화

```
텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention
 
Transformer 이후 생성형 모델 발전 계열
├── Transformer 구조 (Vaswani et al., 2017)
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등
```

## 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩

정적 임베딩은 단어의 의미를 하나의 고정된 벡터로 표현하기 때문에, 문맥에 따라 달라지는 단어의 다양한 의미(다의성, 동음이의어)를 제대로 포착하지 못하는 근본적인 한계를 지닌다. 예를 들어, "사과"라는 단어가 과일인지, 아니면 누군가의 사과(apology)인지 문맥 없이는 알 수 없지만, 정적 임베딩에서는 항상 같은 벡터로 표현된다. 이러한 문제를 해결하기 위해 등장한 것이 바로 문맥 기반 동적 임베딩이다.

### 초기 시도: RNN 기반 문맥 표현

동적 임베딩의 초기 아이디어는 순차적인 정보를 처리하는 데 강점을 가진 RNN(Recurrent Neural Network)과 그 변형인 LSTM(Long Short-Term Memory), GRU(Gated Recurrent Unit) 등을 활용하는 것에서 시작되었다. 이 모델들은 단어 시퀀스를 입력으로 받아 각 단어의 은닉 상태(hidden state)를 계산하는데, 이 은닉 상태가 해당 단어까지의 문맥 정보를 요약한다고 보았다.

*   **작동 방식**: 문장을 순차적으로 읽으면서 각 단어의 정보를 누적하여 현재 단어의 문맥적 의미를 표현하려 했다. 예를 들어, "나는 [사과]를 먹었다"에서 "사과"의 벡터를 만들 때, "나는"이라는 이전 단어의 정보가 영향을 미치는 식이다.
*   **한계**: RNN 계열 모델은 문장 앞부분의 정보가 뒤로 갈수록 희석되는 장기 의존성 문제(long-term dependency problem)가 있었고, 문장 전체의 양방향 문맥을 동시에 고려하는 데에도 한계가 있었다. 또한, 주로 특정 다운스트림 태스크에 대해 처음부터 학습되어 일반적인 언어 표현 자체를 학습했다고 보기는 어려웠다.

이러한 초기 시도들은 문맥의 중요성을 인식하게 했지만, 더 정교하고 일반화된 문맥 임베딩 방법론의 필요성을 야기했다.

### ELMo (Embeddings from Language Models, 2018)

ELMo는 '언어 모델로부터 얻는 임베딩'이라는 이름에서 알 수 있듯이, 깊은 양방향 언어 모델(deep BiLSTM)을 사전 학습하여 문맥에 따라 단어의 의미를 동적으로 결정하는 임베딩을 제안했다.

*   **핵심 아이디어**: 정방향 LSTM과 역방향 LSTM을 각각 독립적으로 학습한 후, 이 두 모델의 각 계층(layer)에서 나오는 은닉 상태들을 결합하여 사용한다. 특히, 문자 단위 합성곱 신경망(Character CNN)을 통해 단어의 철자 정보(형태론적 특징)까지 포착하여 OOV(Out-of-Vocabulary) 문제에도 강인함을 보였다.
*   **동적 표현**: 특정 단어의 ELMo 임베딩은 해당 단어 주변의 전체 문맥을 고려하여 계산되며, 단순히 미리 정의된 벡터를 가져오는 것이 아니라 실제 입력 문장에 따라 동적으로 생성된다. 또한, 사전 학습된 BiLSTM의 여러 계층에서 나온 벡터들을 가중합(weighted sum)하여 사용함으로써, 구문론적 정보(낮은 계층)부터 의미론적 정보(높은 계층)까지 다양한 수준의 정보를 활용할 수 있게 했다.
*   **의의**: ELMo는 사전 학습된 깊은 신경망을 통해 문맥 의존적인 단어 표현을 효과적으로 생성할 수 있음을 보여주었고, 이후 등장하는 트랜스포머 기반 모델들의 중요한 영감을 제공했다.

### 정적 vs 동적 임베딩

* 문제 상황: 동음이의어와 다의어
   * **영어 예시: "bank"**

   ```
   문장1: "I went to the bank to deposit money"  (은행)
   문장2: "The river bank was muddy"            (강둑)
   ```
   * **한국어 예시: "배"**

   ```
   문장1: "배가 고파서 밥을 먹었다"  (배 = 위장)
   문장2: "배를 타고 바다에 나갔다"  (배 = 선박)  
   문장3: "달콤한 배를 먹었다"      (배 = 과일)
   ```

* 정적 임베딩의 한계
   * **Word2Vec/GloVe 방식:**
      * 서로 다른 의미임에도 같은 벡터 사용
      * 문맥 정보를 활용하지 못함
      * 의미 구분이 불가능

   ```python
   # 단어별로 고정된 하나의 벡터만 존재
   embedding_table = {
      "bank": [0.2, -0.4, 0.7, 0.1, ...],  # 항상 같은 벡터
      "배": [0.3, 0.1, -0.2, 0.8, ...],    # 항상 같은 벡터
   }

   # 문맥에 관계없이 항상 같은 벡터 반환
   vector_bank_1 = embedding_table["bank"]  # 은행 문맥
   vector_bank_2 = embedding_table["bank"]  # 강둑 문맥
   # vector_bank_1 == vector_bank_2 (문제!)
   ```

* 동적 임베딩의 해결책
   * **BERT/ELMo 방식:**

   ```python
   # 같은 단어라도 문맥에 따라 다른 벡터 생성
   sentence1 = "I went to the bank to deposit money"
   sentence2 = "The river bank was muddy"

   vector_bank_1 = contextual_embedding(sentence1, word_position=4)  
   # → [0.8, 0.2, -0.1, ...]  (은행 의미)

   vector_bank_2 = contextual_embedding(sentence2, word_position=2)  
   # → [-0.3, 0.9, 0.4, ...]  (강둑 의미)

   # vector_bank_1 ≠ vector_bank_2 (해결!)
   ```

### BERT (Bidirectional Encoder Representations from Transformers, 2018)

BERT는 트랜스포머(Transformer)의 인코더(Encoder) 구조만을 활용하여, 문장 내 모든 단어의 양방향 문맥을 동시에 그리고 깊게 학습하는 혁신적인 모델이다.

*   **핵심 아이디어**: 기존 언어 모델들이 주로 단방향(왼쪽에서 오른쪽 또는 오른쪽에서 왼쪽)으로 문맥을 학습했던 것과 달리, BERT는 'Masked Language Model (MLM)'이라는 새로운 사전 학습 방식을 통해 진정한 의미의 양방향 학습을 가능하게 했다. MLM은 문장 내 일부 단어를 가리고 (마스킹하고), 주변 단어들만을 이용해 가려진 단어를 예측하도록 하는 방식이다. 또한, 'Next Sentence Prediction (NSP)'을 통해 두 문장이 이어지는 관계인지 예측하도록 학습하여 문장 간의 관계 이해 능력도 키웠다.
*   **문맥 표현**: 특정 단어의 BERT 임베딩은 해당 단어뿐만 아니라 문장 전체의 모든 단어와의 관계를 동시에 고려하여 생성된다. 입력의 시작 부분에 추가되는 `[CLS]` 토큰의 최종 은닉 상태는 문장 전체를 대표하는 벡터로 활용되어 분류(classification) 문제 등에 사용될 수 있다.
*   **의의**: BERT는 다양한 NLP 벤치마크에서 압도적인 성능 향상을 보여주며, 사전 학습-미세 조정(pre-training and fine-tuning) 패러다임을 NLP 분야의 표준으로 만들었다. 이후 RoBERTa, ALBERT, ELECTRA 등 수많은 변형 모델들이 등장하는 계기가 되었다.

### GPT (Generative Pre-trained Transformer, 2018 이후)

GPT 계열 모델들은 트랜스포머의 디코더(Decoder) 구조를 기반으로 하며, 주로 텍스트 생성(generation) 작업에 강력한 성능을 보인다.

*   **핵심 아이디어**: GPT는 이전 단어들(또는 토큰들)이 주어졌을 때 다음 단어를 예측하는 전통적인 단방향(autoregressive) 언어 모델링 방식으로 학습된다. 이 과정에서 문맥을 이해하고 일관성 있는 긴 텍스트를 생성하는 능력을 학습한다.
*   **In-context Learning**: GPT-2부터 두드러지기 시작하여 GPT-3에서 크게 발전한 특징으로, 모델의 가중치를 직접 업데이트하는 미세 조정(fine-tuning) 없이, 프롬프트(prompt)에 작업 설명과 몇 가지 예시(few-shot)를 함께 제공하는 것만으로도 모델이 새로운 작업을 수행할 수 있는 능력을 의미한다. 이는 모델 활용의 유연성을 크게 높였다.
*   **의의**: GPT는 대규모 데이터와 큰 모델 크기를 통해 놀라운 생성 능력과 일반화 성능을 보여주었으며, 챗봇, 요약, 번역, 코드 생성 등 매우 광범위한 응용 가능성을 열었다. 현재 대규모 언어 모델(LLM)의 대표적인 아키텍처 중 하나이다.

### SBERT (Sentence-BERT, 2019)

SBERT는 BERT와 같은 사전 학습된 트랜스포머 모델을 문장이나 짧은 단락 수준의 임베딩 생성에 효과적으로 사용하기 위해 제안된 방법론이다.

*   **핵심 아이디어**: BERT의 출력을 그대로 사용하여 문장 벡터를 만들 경우(예: `[CLS]` 토큰 출력 또는 토큰 임베딩 평균 풀링), 의미적으로 유사한 문장을 잘 찾아내지 못하는 경우가 있다. SBERT는 Siamese 또는 Triplet 네트워크 구조를 사용하여 BERT를 미세 조정함으로써, 문장 간 의미 유사도를 잘 반영하는 고정된 크기의 문장 임베딩을 생성한다.
    *   **Siamese 네트워크**: 두 개의 문장을 각각 동일한 BERT 모델에 통과시켜 나온 문장 임베딩 간의 유사도를 계산한다.
    *   **Triplet 네트워크**: 기준 문장(anchor), 긍정 문장(positive), 부정 문장(negative) 세 쌍을 이용하여, 기준 문장이 긍정 문장과는 가깝고 부정 문장과는 멀어지도록 학습한다.
*   **효율성**: SBERT를 통해 얻은 문장 임베딩은 코사인 유사도 등으로 매우 빠르게 비교할 수 있어, 대규모 문장 데이터셋에서의 의미 검색, 클러스터링, 정보 검색 등의 작업에 매우 효율적이다.
*   **의의**: 복잡한 계산 없이도 고품질의 문장 임베딩을 생성하고 활용할 수 있는 실용적인 방법을 제시하여, 문장 수준의 의미 이해가 중요한 다양한 NLP 응용 분야에 널리 사용된다.

### 실용적 응용 및 평가

#### 평가 지표

**Intrinsic Evaluation (내재적 평가):**
- **단어 유사도**: WordSim-353, SimLex-999 - 사람이 평가한 단어 유사도와 모델 예측의 상관관계 측정
- **단어 관계**: "king - man + woman = queen" - 벡터 연산으로 의미 관계 포착 정도 평가

**Extrinsic Evaluation (외재적 평가):**
- **문서 분류 정확도**: 실제 분류 태스크에서의 성능
- **정보 검색 성능**: NDCG, MAP - 검색 결과의 관련성 및 순위 정확도
- **의미적 텍스트 유사도**: STS benchmark - 문장 간 의미적 유사성 예측 성능

#### 모델 선택 가이드

- **소규모 데이터**: FastText (OOV 처리)
- **대규모 문서 분류**: BERT fine-tuning
- **실시간 유사도 계산**: SBERT
- **창작/생성 태스크**: GPT 계열

#### 통계적 해석

임베딩 공간에서의 기하학적 관계:
$$ \cos(\mathbf{v}_{\text{similar words}}) > \cos(\mathbf{v}_{\text{dissimilar words}}) $$

**시각화 도구**: t-SNE/UMAP을 통한 의미적 클러스터링 확인


## 결론

본 문서에서는 단어의 고정된 벡터 표현을 넘어, 문맥에 따라 의미가 유동적으로 변하는 것을 포착하는 동적 임베딩 방법론의 기본 개념과 주요 모델들을 살펴보았다. ELMo부터 BERT, GPT, SBERT에 이르기까지, 이러한 문맥 기반 임베딩 모델들은 자연어 처리(NLP) 분야에 큰 발전을 가져왔다.

주요 내용을 다시 정리하면 다음과 같다.

*   **정적 임베딩의 한계점 보완**: 기존의 Word2Vec, GloVe 같은 정적 임베딩은 단어의 다의성을 반영하기 어려웠다. 동적 임베딩은 이 문제를 해결하여, 같은 단어라도 문맥에 따라 다른 벡터를 생성함으로써 보다 정확한 의미 표현을 가능하게 했다.

*   **주요 모델들의 아이디어와 기여**:
    *   **ELMo**: 양방향 LSTM을 통해 문맥 정보를 임베딩에 통합했다.
    *   **BERT**: 트랜스포머와 MLM, NSP 학습 방식을 통해 양방향 문맥 이해의 새로운 기준을 제시했고, 다양한 NLP 문제에서 높은 성능을 보였다.
    *   **GPT**: 단방향 트랜스포머 디코더를 통해 강력한 텍스트 생성 능력을 보여주었고, In-context Learning이라는 유연한 활용 가능성을 제시했다.
    *   **SBERT**: BERT를 문장 임베딩 생성에 효율적으로 적용하여, 문장 간 유사도 비교 작업의 성능을 높였다.

*   **LLM 발전의 기반**: 문맥 기반 임베딩 모델들의 발전은 언어를 깊이 이해하고 생성할 수 있는 대규모 언어 모델(Large Language Models, LLMs) 시대로 나아가는 중요한 발판이 되었다. 사전 학습과 미세 조정, 그리고 프롬프트 기반 학습 방식은 모델 활용의 폭을 넓혔다.

*   **적절한 모델 선택의 중요성**: 문제의 특성, 데이터, 자원 등을 고려하여 적합한 임베딩 전략과 모델을 선택하는 것은 여전히 중요하다.

문맥을 이해하는 텍스트 벡터화 기술은 기계가 인간의 언어를 더 잘 이해하고 상호작용하는 데 기여하며 지속적으로 발전하고 있다. 이러한 기술은 사회 여러 분야에 혁신을 가져올 잠재력을 가지고 있다.
