---
title: "텍스트 벡터화: GRU의 이해"
subtitle: "LSTM의 대안, Gated Recurrent Unit의 구조와 원리"
description: |
  LSTM(Long Short-Term Memory)의 복잡성을 줄이면서 유사한 성능을 목표로 개발된 GRU(Gated Recurrent Unit)의 핵심 원리와 두 가지 게이트(리셋 게이트, 업데이트 게이트)의 작동 방식을 설명한다. GRU가 어떻게 장기 의존성 문제를 해결하고 다양한 시퀀스 모델링 작업에 효과적으로 사용될 수 있는지 살펴본다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-12
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False # 발행 시 False로 변경
---

# 요약

이 문서는 순환 신경망(RNN)의 한계점인 장기 의존성 문제를 효과적으로 해결하기 위해 제안된 GRU(Gated Recurrent Unit)의 기본 원리와 구조를 소개한다. GRU는 LSTM(Long Short-Term Memory)과 유사한 성능을 보이면서도 내부 구조를 단순화하여 계산 효율성을 높인 모델이다.

주요 내용은 다음과 같다.

*   **RNN의 장기 의존성 문제와 GRU의 등장**:
    *   기존 RNN은 시퀀스 길이가 길어질수록 과거의 중요 정보가 손실되는 장기 의존성 문제를 겪는다.
    *   GRU는 이러한 문제를 해결하기 위해 LSTM과 마찬가지로 게이트 메커니즘을 사용하지만, 더 적은 수의 게이트로 구성된다.
*   **GRU의 핵심 구성 요소 및 작동 원리**:
    *   **리셋 게이트 (Reset Gate, $r_t$)**: 이전 시점의 은닉 상태($h_{t-1}$)에서 어떤 정보를 무시하고 현재 입력($x_t$)과 함께 새로운 후보 은닉 상태를 만들지 결정한다.
    *   **업데이트 게이트 (Update Gate, $z_t$)**: 이전 은닉 상태($h_{t-1}$)의 정보를 얼마나 유지하고, 현재 계산된 후보 은닉 상태($\tilde{h}_t$)의 정보를 얼마나 반영하여 새로운 은닉 상태($h_t$)를 만들지 결정한다.
    *   이 두 게이트는 시그모이드 함수를 통해 0과 1 사이의 값을 출력하며, 이를 통해 정보 흐름을 정교하게 제어한다. GRU는 LSTM과 달리 별도의 셀 상태(Cell State)를 사용하지 않고 은닉 상태(Hidden State)만으로 정보를 전달한다.
*   **LSTM과의 비교**:
    *   GRU는 LSTM에 비해 게이트 수가 적고(2개 vs 3개), 파라미터 수도 적어 계산 비용이 낮고 학습 속도가 빠를 수 있다.
    *   많은 경우 LSTM과 비슷한 성능을 보이며, 데이터셋의 크기가 작거나 특정 문제에서는 GRU가 더 나은 결과를 보이기도 한다.
*   **의의**: GRU는 장기 의존성 문제를 완화하여 긴 시퀀스에서도 효과적인 학습을 가능하게 하며, 자연어 처리, 음성 인식 등 다양한 분야에서 RNN 계열 모델의 중요한 선택지로 활용된다.

이 문서를 통해 독자는 GRU가 어떻게 게이트 메커니즘을 통해 정보의 흐름을 제어하고 장기 기억을 가능하게 하는지에 대한 기본적인 이해를 얻을 수 있다.

# 텍스트 인코딩 및 벡터화

```
텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
        ├── GRU
    │   └── ELMo
    ├── Attention 메커니즘
    │   ├── Basic Attention
    │   ├── Self-Attention
    │   └── Multi-Head Attention
    └── Transformer 계열
        ├── BERT, RoBERTa, ALBERT
        ├── GPT 시리즈
        ├── KoBERT, KoGPT 등 한국어 특화
        └── 기타 모델: T5, LaMDA, PaLM, XLNet, ELECTRA
```

# 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩

## GRU (Gated Recurrent Unit)

GRU(Gated Recurrent Unit)는 2014년 조경현 교수 등이 제안한 순환 신경망의 한 종류로, LSTM(Long Short-Term Memory)과 마찬가지로 기존 RNN의 장기 의존성 문제를 해결하기 위해 설계되었다. GRU는 LSTM의 복잡한 구조를 단순화하면서도 유사한 성능을 내는 것을 목표로 하며, LSTM보다 적은 수의 게이트를 사용하여 계산 효율성을 높였다.

주요 특징은 다음과 같다:

*   LSTM과 마찬가지로 장기 의존성 문제에 강인한 모습을 보인다.
*   LSTM에는 3개의 게이트(입력, 삭제, 출력 게이트)와 별도의 셀 상태(Cell State)가 있었던 반면, GRU는 **리셋 게이트(Reset Gate)**와 **업데이트 게이트(Update Gate)**라는 2개의 게이트만을 사용하며, 별도의 셀 상태 없이 은닉 상태(Hidden State)를 통해 정보를 전달한다.
*   이로 인해 LSTM보다 파라미터 수가 적어 일반적으로 학습 속도가 빠르고, 계산량이 적으며, 특히 데이터가 적은 경우 과적합(overfitting)에 대한 강점을 가질 수 있다.

GRU의 핵심 아이디어는 각 시점에서 이전 정보를 얼마나 '리셋'할지, 그리고 새로운 정보를 얼마나 '업데이트'할지를 게이트를 통해 학습하여 조절하는 것이다.

### GRU의 구조와 작동 원리

GRU는 현재 입력 $x_t$ 와 이전 시점의 은닉 상태 $h_{t-1}$ 을 받아 현재 시점의 은닉 상태 $h_t$ 를 출력한다. 이 과정은 다음의 두 가지 주요 게이트와 후보 은닉 상태 계산을 통해 이루어진다.

#### 1. 리셋 게이트 (Reset Gate, $r_t$ )

리셋 게이트는 이전 은닉 상태 $h_{t-1}$ 의 정보를 얼마나 '잊을지' 또는 '무시할지'를 결정한다. 이 게이트는 현재 입력 $x_t$와 이전 은닉 상태 $h_{t-1}$ 을 사용하여 계산된다.

$r_t = \sigma(W_r x_t + U_r h_{t-1} + b_r)$

*   $W_r, U_r$ : 리셋 게이트의 가중치 행렬
*   $b_r$ : 리셋 게이트의 편향 벡터
*   $\sigma$ : 시그모이드 함수 (출력값을 0과 1 사이로 제한하여 게이트의 열림/닫힘 정도를 결정)

리셋 게이트의 출력 $r_t$ 는 0에 가까울수록 이전 은닉 상태의 정보를 많이 잊고(즉, 새로운 후보값 생성 시 이전 정보의 영향력을 줄임), 1에 가까울수록 많이 기억(활용)하게 된다. 이 $r_t$ 는 후보 은닉 상태 $\tilde{h}_t$ 를 계산할 때 이전 은닉 상태 $h_{t-1}$ 에 요소별 곱(element-wise product, $\odot$ )으로 적용되어, $h_{t-1}$ 의 어떤 부분을 새로운 후보 은닉 상태 계산에 사용할지 결정한다.

#### 2. 업데이트 게이트 (Update Gate, $z_t$ )

업데이트 게이트는 이전 은닉 상태 $h_{t-1}$ 의 정보를 얼마나 현재 은닉 상태 $h_t$로 가져올지, 그리고 새로 계산된 후보 은닉 상태 $\tilde{h}_t$ 의 정보를 얼마나 반영할지를 결정한다. LSTM의 삭제 게이트와 입력 게이트의 역할을 동시에 수행한다고 볼 수 있다.

$z_t = \sigma(W_z x_t + U_z h_{t-1} + b_z)$

*   $W_z, U_z$ : 업데이트 게이트의 가중치 행렬
*   $b_z$ : 업데이트 게이트의 편향 벡터

업데이트 게이트의 출력 $z_t$ 는 $h_t$ 를 계산할 때 $\tilde{h}_t$ 에 곱해지는 가중치 역할을 하며, $(1-z_t)$ 는 $h_{t-1}$ 에 곱해지는 가중치 역할을 한다. 즉, $z_t$ 가 1에 가까우면 후보 은닉 상태 $\tilde{h}_t$ 의 정보를 많이 반영하고, $z_t$ 가 0에 가까우면 이전 은닉 상태 $h_{t-1}$ 의 정보를 많이 유지한다.

#### 3. 후보 은닉 상태 (Candidate Hidden State, $\tilde{h}_t$ )

후보 은닉 상태는 현재 시점의 정보를 담고 있는 새로운 은닉 값의 '후보'이다. 이는 현재 입력 $x_t$와 리셋 게이트에 의해 조절된 이전 은닉 상태 $(r_t \odot h_{t-1})$ 를 사용하여 계산된다.

$\tilde{h}_t = \tanh(W_h x_t + U_h (r_t \odot h_{t-1}) + b_h)$

*   $W_h, U_h$ : 후보 은닉 상태 계산을 위한 가중치 행렬
*   $b_h$ : 후보 은닉 상태 계산을 위한 편향 벡터
*   $\tanh$ : 하이퍼볼릭 탄젠트 함수 (출력값을 -1과 1 사이로 제한)

리셋 게이트 $r_t$ 가 $h_{t-1}$ 에 곱해짐으로써, $h_{t-1}$ 중 과거 정보 중 현재 필요한 부분만 선택적으로 활용하여 현재 입력 $x_t$ 와 함께 새로운 정보 $\tilde{h}_t$ 를 구성하는 데 사용된다.

#### 4. 최종 은닉 상태 (Final Hidden State, $h_t$ )

최종적으로 현재 시점의 은닉 상태 $h_t$ 는 업데이트 게이트 $z_t$ 에 의해 이전 은닉 상태 $h_{t-1}$ 과 후보 은닉 상태 $\tilde{h}_t$ 가 조합되어 결정된다.

$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$

*   이 식은 $h_{t-1}$ 과 $\tilde{h}_t$ 사이의 가중 평균과 유사한 형태로, $z_t$ 가 $\tilde{h}_t$ 를 얼마나 반영할지, 그리고 $(1-z_t)$ 가 $h_{t-1}$ 을 얼마나 유지할지를 결정한다.
*   $z_t$ 가 1이면 $h_t = \tilde{h}_t$ (이전 상태 무시, 새 정보만 반영)
*   $z_t$ 가 0이면 $h_t = h_{t-1}$ (이전 상태 유지, 새 정보 무시)

이러한 방식으로 GRU는 두 개의 게이트를 통해 정보의 흐름을 효과적으로 제어하며 장기 의존성 문제를 해결하려고 시도한다. LSTM에서 셀 상태와 은닉 상태를 분리했던 것과 달리, GRU는 은닉 상태 $h_t$ 하나로 이 두 가지 역할을 어느 정도 통합하여 수행한다.

### 장기 의존성 문제 해결

GRU의 게이트 메커니즘은 RNN의 기울기 소실/폭주 문제를 완화하여 장기 의존성을 포착하는 데 도움을 준다.

*   **업데이트 게이트 ( $z_t$ )**: $z_t$ 값을 통해 이전 시점의 정보를 얼마나 유지할지 학습한다. 시퀀스에서 중요한 정보가 먼 과거에 있더라도, 네트워크는 $z_t$ 를 0에 가깝게 유지하여 해당 정보를 $h_t$ 로 계속 전달할 수 있다. 이는 장기적인 의존성을 유지하는 데 기여한다.
*   **리셋 게이트 ( $r_t$ )**: $r_t$ 값을 통해 과거 정보 중 현재 예측에 불필요한 부분을 효과적으로 리셋(무시)할 수 있다. 이를 통해 현재 필요한 정보에 집중하고, 과거의 덜 중요한 정보로 인해 학습이 방해받는 것을 줄인다.

이처럼 필요한 정보는 오래 유지하고, 불필요한 정보는 적절히 리셋함으로써 GRU는 긴 시퀀스에서도 안정적인 학습을 가능하게 한다.

### LSTM과의 비교

GRU는 LSTM과 유사한 목적을 가지고 있지만 몇 가지 주요 차이점이 있다.

| 특징          | LSTM (Long Short-Term Memory)                                  | GRU (Gated Recurrent Unit)                                       |
|---------------|----------------------------------------------------------------|------------------------------------------------------------------|
| **게이트 수**   | 3개 (입력 게이트, 삭제 게이트, 출력 게이트)                        | 2개 (리셋 게이트, 업데이트 게이트)                                   |
| **셀 상태**    | 별도의 셀 상태 ( $C_t$ )를 가짐 (장기 기억 담당)                      | 별도의 셀 상태 없음 (은닉 상태 $h_t$ 가 장기 기억과 현재 출력을 모두 담당) |
| **파라미터 수** | 일반적으로 GRU보다 많음                                          | 일반적으로 LSTM보다 적음                                           |
| **계산 복잡도** | GRU보다 높음                                                   | LSTM보다 낮음 (학습 속도 빠를 수 있음)                               |
| **성능**       | 다양한 NLP 작업에서 강력한 성능 입증. 일반적으로 복잡한 데이터셋에서 유리 | 많은 경우 LSTM과 유사한 성능. 데이터가 적거나 특정 작업에서 더 나을 수 있음 |
| **정보 흐름**  | 셀 상태와 은닉 상태를 통해 정보 흐름 제어                          | 은닉 상태와 두 게이트를 통해 정보 흐름 제어                            |

*   **구조적 차이**: LSTM은 명시적인 메모리 셀( $C_t$ )을 사용하여 장기 정보를 저장하고, 세 개의 게이트로 이 셀과 은닉 상태( $h_t$ )를 제어한다. 반면 GRU는 셀 상태 없이 은닉 상태( $h_t$ )만으로 정보를 전달하며, 리셋 게이트와 업데이트 게이트 두 개로 정보 흐름을 제어한다. 업데이트 게이트가 LSTM의 삭제 게이트와 입력 게이트의 역할을 통합한 것으로 볼 수 있다.
*   **효율성**: GRU는 파라미터 수가 적기 때문에 계산 효율성이 높고 학습 속도가 빠를 수 있으며, 특히 데이터셋의 크기가 작을 때 과적합을 피하는 데 유리할 수 있다.
*   **성능**: 어떤 모델이 항상 우수하다고 단정하기는 어렵다. 문제의 성격, 데이터셋의 크기 및 복잡성, 하이퍼파라미터 튜닝 등에 따라 결과가 달라질 수 있다. 많은 연구에서 두 모델이 유사한 성능을 보인다고 보고되지만, 때로는 GRU가, 때로는 LSTM이 약간 더 나은 성능을 보이기도 한다.
*   **선택 기준**: 일반적으로는 LSTM이 더 많은 표현력을 가질 수 있다고 여겨져 복잡한 문제에 우선 시도될 수 있지만, 계산 자원이 제한적이거나 빠른 학습이 필요할 경우, 또는 데이터가 충분하지 않을 경우에는 GRU가 좋은 대안이 될 수 있다. 실제 적용 시에는 두 모델을 모두 실험해보고 성능을 비교하는 것이 좋다.

### 양방향 GRU (Bidirectional GRU)

양방향 LSTM(BiLSTM)과 마찬가지로, 양방향 GRU(BiGRU)도 시퀀스 데이터 처리 시 과거와 미래의 문맥을 모두 고려하기 위해 사용된다. BiGRU는 순방향 GRU와 역방향 GRU 두 개를 병렬로 구성한다.

*   **순방향 GRU**: 입력 시퀀스를 처음부터 끝까지 순서대로 처리한다.
*   **역방향 GRU**: 입력 시퀀스를 끝에서부터 처음까지 역순으로 처리한다.

각 시점에서 두 GRU의 은닉 상태는 보통 연결(concatenation)되어 해당 시점의 최종적인 특징 표현으로 사용된다. 이를 통해 특정 시점의 단어나 요소에 대한 이해도를 높일 수 있다. 예를 들어, 문장에서 단어의 의미를 파악할 때 해당 단어의 앞뒤 단어들이 모두 중요한 역할을 하는 경우 BiGRU가 효과적이다.

## 결론

본 문서에서는 RNN의 장기 의존성 문제를 해결하기 위해 설계된 GRU(Gated Recurrent Unit)의 핵심적인 구조와 작동 방식을 살펴보았다. GRU는 LSTM의 대안으로 제시되었으며, 더 단순한 구조와 적은 파라미터로도 유사하거나 때로는 더 나은 성능을 제공할 수 있음을 보여주었다.

*   **GRU의 핵심 원리 요약**:
    *   GRU는 **리셋 게이트**와 **업데이트 게이트**라는 두 가지 게이트 메커니즘을 통해 정보의 흐름을 정교하게 제어한다. 리셋 게이트는 과거 정보 중 현재 예측에 덜 중요한 부분을 무시하도록 하고, 업데이트 게이트는 과거 정보와 현재 후보 정보 사이의 균형을 조절하여 다음 은닉 상태를 결정한다.
    *   별도의 셀 상태 없이 은닉 상태만으로 장기 정보를 효과적으로 전달하며, 이는 LSTM에 비해 구조적 단순성과 계산 효율성을 가져다준다.
*   **LSTM과의 관계 및 장점**:
    *   GRU는 LSTM보다 파라미터 수가 적어 학습 속도가 빠르고, 특히 데이터가 적은 환경에서 과적합의 위험을 줄일 수 있는 장점이 있다.
    *   많은 자연어 처리 및 시퀀스 모델링 문제에서 LSTM과 대등한 성능을 보여주어, 모델 선택 시 중요한 고려 대상이 된다.
*   **문맥 이해와 NLP에서의 중요성**:
    *   GRU는 장기 의존성을 효과적으로 처리할 수 있어 긴 시퀀스 데이터의 문맥적 의미를 파악하는 데 중요한 역할을 한다.
    *   자연어 이해, 기계 번역, 음성 인식 등 다양한 NLP 태스크에서 순환 신경망의 핵심 구성 요소로 활용되며, 특히 계산 효율성이 중요하거나 빠른 프로토타이핑이 필요할 때 유용하다.

결론적으로, GRU는 LSTM과 함께 시퀀스 데이터, 특히 장기 의존성을 가진 데이터 처리에 있어 중요한 진보를 이룬 모델이다. 단순화된 게이트 메커니즘을 통해 정보의 선택적 기억과 업데이트를 가능하게 함으로써, 복잡한 패턴 학습 능력을 효과적으로 제공하며 다양한 응용 분야에서 그 가치를 입증하고 있다.
