---
title: "텍스트 벡터화: GRU의 이해"
subtitle: "LSTM의 대안, Gated Recurrent Unit의 구조와 원리"
description: |
  LSTM(Long Short-Term Memory)의 복잡성을 줄이면서 유사한 성능을 목표로 개발된 GRU(Gated Recurrent Unit)의 핵심 원리와 두 가지 게이트(리셋 게이트, 업데이트 게이트)의 작동 방식을 설명한다. GRU가 어떻게 장기 의존성 문제를 해결하고 다양한 시퀀스 모델링 작업에 효과적으로 사용될 수 있는지 살펴본다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-13
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False # 발행 시 False로 변경
---

# 요약

이 문서는 순환 신경망(RNN)의 한계점인 장기 의존성 문제를 효과적으로 해결하기 위해 제안된 GRU(Gated Recurrent Unit)의 기본 원리와 구조를 소개한다. GRU는 LSTM(Long Short-Term Memory)과 유사한 성능을 보이면서도 내부 구조를 단순화하여 계산 효율성을 높인 모델이다.

주요 내용은 다음과 같다.

*   **RNN의 장기 의존성 문제와 GRU의 등장**:
    *   기존 RNN은 시퀀스 길이가 길어질수록 과거의 중요 정보가 손실되는 장기 의존성 문제를 겪는다.
    *   GRU는 이러한 문제를 해결하기 위해 LSTM과 마찬가지로 게이트 메커니즘을 사용하지만, 더 적은 수의 게이트로 구성된다.
*   **GRU의 핵심 구성 요소 및 작동 원리**:
    *   **리셋 게이트 (Reset Gate, $r_t$)**: 이전 시점의 은닉 상태($h_{t-1}$)에서 어떤 정보를 무시하고 현재 입력($x_t$)과 함께 새로운 후보 은닉 상태를 만들지 결정한다.
    *   **업데이트 게이트 (Update Gate, $z_t$)**: 이전 은닉 상태($h_{t-1}$)의 정보를 얼마나 유지하고, 현재 계산된 후보 은닉 상태($\tilde{h}_t$)의 정보를 얼마나 반영하여 새로운 은닉 상태($h_t$)를 만들지 결정한다.
    *   이 두 게이트는 시그모이드 함수를 통해 0과 1 사이의 값을 출력하며, 이를 통해 정보 흐름을 정교하게 제어한다. GRU는 LSTM과 달리 별도의 셀 상태(Cell State)를 사용하지 않고 은닉 상태(Hidden State)만으로 정보를 전달한다.
*   **LSTM과의 비교**:
    *   GRU는 LSTM에 비해 게이트 수가 적고(2개 vs 3개), 파라미터 수도 적어 계산 비용이 낮고 학습 속도가 빠를 수 있다.
    *   많은 경우 LSTM과 비슷한 성능을 보이며, 데이터셋의 크기가 작거나 특정 문제에서는 GRU가 더 나은 결과를 보이기도 한다.
*   **의의**: GRU는 장기 의존성 문제를 완화하여 긴 시퀀스에서도 효과적인 학습을 가능하게 하며, 자연어 처리, 음성 인식 등 다양한 분야에서 RNN 계열 모델의 중요한 선택지로 활용된다.

이 문서를 통해 독자는 GRU가 어떻게 게이트 메커니즘을 통해 정보의 흐름을 제어하고 장기 기억을 가능하게 하는지에 대한 기본적인 이해를 얻을 수 있다.

# 텍스트 인코딩 및 벡터화


```
텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
        ├── GRU
    │   └── ELMo
    ├── Attention 메커니즘
    │   ├── Basic Attention
    │   ├── Self-Attention
    │   └── Multi-Head Attention
    └── Transformer 계열
        ├── BERT, RoBERTa, ALBERT
        ├── GPT 시리즈
        ├── KoBERT, KoGPT 등 한국어 특화
        └── 기타 모델: T5, LaMDA, PaLM, XLNet, ELECTRA
```

# 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩

## ELMo (Embedding from Language Models, 2018)

* ELMo 수식: $\text{ELMo}_k^{task} = \gamma^{task} \sum_{j=0}^L s_j^{task} \mathbf{h}_{k,j}^{LM}$
   * **$\mathbf{h}_{k,j}^{LM}$: 각 레이어의 hidden state**

   ```python
   # 예시: 3층 BiLSTM에서 "bank" 단어 (k번째 위치)
   h_{bank,0} = character_embedding("bank")     # 레이어 0 (입력)
   h_{bank,1} = first_LSTM_layer_output        # 레이어 1  
   h_{bank,2} = second_LSTM_layer_output       # 레이어 2
   h_{bank,3} = third_LSTM_layer_output        # 레이어 3 (최상위)
   ```
   * **$s_j^{task}$: 학습 가능한 가중치**
      * 각 레이어의 중요도를 태스크별로 학습
      * 문법적 태스크 → 낮은 레이어 중시
      * 의미적 태스크 → 높은 레이어 중시
   * **$\gamma^{task}$: 전체 스케일 조정**
      * ELMo 벡터의 전체적인 크기 조정
* 계산 예시

```python
# "bank" 단어의 ELMo 벡터 (감정 분석 태스크)
h_0 = [0.1, 0.2, 0.3]  # 문자 레벨
h_1 = [0.4, 0.5, 0.6]  # 낮은 레벨 (문법적)  
h_2 = [0.7, 0.8, 0.9]  # 높은 레벨 (의미적)

s_0 = 0.1  # 문자 레벨 가중치 (낮음)
s_1 = 0.3  # 문법 레벨 가중치  
s_2 = 0.6  # 의미 레벨 가중치 (높음)

ELMo_bank = γ × (s_0×h_0 + s_1×h_1 + s_2×h_2)
          = 2.0 × (0.1×[0.1,0.2,0.3] + 0.3×[0.4,0.5,0.6] + 0.6×[0.7,0.8,0.9])
          = 2.0 × [0.55, 0.65, 0.75]
          = [1.1, 1.3, 1.5]
```

* 양방향 정보의 중요성
   * **Forward만 사용할 경우:**

   ```
   "The bank was closed because of ___"
   → "bank"를 이해할 때 "The"만 참고
   ```

   * **Backward까지 사용할 경우:**  

   ```
   "The bank was closed because of ___"
   → "bank"를 이해할 때 "was closed" 정보도 참고
   → 금융 기관으로 해석 가능성 증가
   ```
## 결론

본 문서에서는 RNN의 장기 의존성 문제를 해결하기 위해 설계된 GRU(Gated Recurrent Unit)의 핵심적인 구조와 작동 방식을 살펴보았다. GRU는 LSTM의 대안으로 제시되었으며, 더 단순한 구조와 적은 파라미터로도 유사하거나 때로는 더 나은 성능을 제공할 수 있음을 보여주었다.

*   **GRU의 핵심 원리 요약**:
    *   GRU는 **리셋 게이트**와 **업데이트 게이트**라는 두 가지 게이트 메커니즘을 통해 정보의 흐름을 정교하게 제어한다. 리셋 게이트는 과거 정보 중 현재 예측에 덜 중요한 부분을 무시하도록 하고, 업데이트 게이트는 과거 정보와 현재 후보 정보 사이의 균형을 조절하여 다음 은닉 상태를 결정한다.
    *   별도의 셀 상태 없이 은닉 상태만으로 장기 정보를 효과적으로 전달하며, 이는 LSTM에 비해 구조적 단순성과 계산 효율성을 가져다준다.
*   **LSTM과의 관계 및 장점**:
    *   GRU는 LSTM보다 파라미터 수가 적어 학습 속도가 빠르고, 특히 데이터가 적은 환경에서 과적합의 위험을 줄일 수 있는 장점이 있다.
    *   많은 자연어 처리 및 시퀀스 모델링 문제에서 LSTM과 대등한 성능을 보여주어, 모델 선택 시 중요한 고려 대상이 된다.
*   **문맥 이해와 NLP에서의 중요성**:
    *   GRU는 장기 의존성을 효과적으로 처리할 수 있어 긴 시퀀스 데이터의 문맥적 의미를 파악하는 데 중요한 역할을 한다.
    *   자연어 이해, 기계 번역, 음성 인식 등 다양한 NLP 태스크에서 순환 신경망의 핵심 구성 요소로 활용되며, 특히 계산 효율성이 중요하거나 빠른 프로토타이핑이 필요할 때 유용하다.

결론적으로, GRU는 LSTM과 함께 시퀀스 데이터, 특히 장기 의존성을 가진 데이터 처리에 있어 중요한 진보를 이룬 모델이다. 단순화된 게이트 메커니즘을 통해 정보의 선택적 기억과 업데이트를 가능하게 함으로써, 복잡한 패턴 학습 능력을 효과적으로 제공하며 다양한 응용 분야에서 그 가치를 입증하고 있다.
