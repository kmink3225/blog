---
title: "텍스트 벡터화: 신경망 기반 방법론"
subtitle: "Word2Vec, GloVe, FastText을 이용한 벡터 표현 소개"
description: |
  자연어 처리(NLP)에서 텍스트의 의미와 문맥을 벡터로 표현하는 신경망 기반의 고급 벡터화 방법들을 심층적으로 탐구한다. 정적 워드 임베딩(Word2Vec, GloVe, FastText)과 동적 문맥 임베딩(ELMo, BERT, SBERT)의 원리, 특징, 활용 방안을 다룬다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-06
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False # 발행 시 False로 변경
---

# 요약

이 문서는 통계 기반 벡터화의 한계를 넘어 텍스트 데이터로부터 풍부한 의미론적, 문맥적 정보를 추출하는 신경망 기반 벡터화 방법론을 소개한다.

*   **DTM 방식의 한계와 신경망 접근법의 등장**:
    *   전통적인 DTM(문서-단어 행렬) 방식의 문제점(차원의 저주, 희소성, 의미 관계 표현 불가)을 지적하고, 이를 극복하기 위한 신경망 기반 밀집 벡터 표현(워드 임베딩)의 필요성을 설명한다.
*   **워드 임베딩 (Word Embedding) - 정적 임베딩**:
    *   **핵심 원리**: "같은 문맥에 나타나는 단어는 비슷한 의미를 가진다"는 분포 가설에 기반하여 단어를 저차원 밀집 벡터로 표현한다.
    *   **Embedding Layer**: 정수 인코딩된 단어를 밀집 벡터로 변환하는 신경망의 핵심 구성 요소로, 그 구조와 Look-up Table 방식을 설명한다.
    *   **주요 모델**:
        *   **Word2Vec (CBOW, Skip-gram)**: 주변 단어로 중심 단어를 예측(CBOW)하거나 중심 단어로 주변 단어를 예측(Skip-gram)하며 벡터를 학습한다.
        *   **GloVe**: 전체 단어 동시 등장 통계 정보를 직접 활용하여 벡터를 학습한다.
        *   **FastText**: 단어를 문자 n-gram으로 분해하여 벡터를 학습함으로써 OOV(Out-of-Vocabulary) 문제에 강직한 형태론적 특징을 포착한다.
*   **실용적 응용 및 평가**:
    *   임베딩 모델의 성능을 평가하는 내재적 평가(단어 유사도, 관계 유추)와 외재적 평가(다운스트림 태스크 성능) 방법을 소개한다.
*   **결론**: 신경망 기반 벡터화 기법들의 발전 과정과 그 의의를 요약한다.

# 텍스트 인코딩 및 벡터화

```
텍스트 벡터화
├── 전통적 방법 (통계 기반)
│   ├── DTM (Document Term Matrix)
│   ├── BoW (Bag of Words)  
│   └── TF-IDF
└── 신경망 기반 방법
    ├── Embedding Layer (핵심 구성 요소)
    └── 구체적 모델들
        ├── Word2Vec (CBOW, Skip-gram)
        ├── GloVe
        ├── FastText
        └── 문맥 기반 모델 (BERT, GPT 등)
```

### 신경망 사용 (2008~2018)

### DTM 방식의 한계와 신경망 접근법의 등장

**DTM 방식의 문제점:**
- **차원의 저주**: 어휘집 크기 = 벡터 차원 (예: 50,000개 단어 → 50,000차원)
- **희소성**: 대부분의 값이 0인 sparse vector
- **의미적 관계 부재**: "왕"과 "여왕"의 관계를 벡터가 표현하지 못함
- **문제 상황:**

```python
# 기존 방식 (원-핫 인코딩)
"사과" = [1, 0, 0, 0, 0, ...]  # 50,000차원 벡터
"바나나" = [0, 1, 0, 0, 0, ...]
"과일" = [0, 0, 1, 0, 0, ...]
```

- 모든 단어가 서로 똑같이 멀어 보임 (유클리드 거리 = √2)
- "사과"와 "바나나"가 비슷한 과일이라는 정보가 없음
- 메모리 낭비 (대부분이 0)

**신경망 접근법의 혁신:**
- **밀집 벡터(Dense Vector)**: 고정된 낮은 차원 (예: 300차원)에 0/1 값이 아닌 실수 값을 가짐.
- **의미적 유사도**: 벡터 간 거리로 단어 유사도 측정 가능
- **문맥 학습**: 주변 단어들을 통해 의미 학습

```python
# 신경망 모델: 워드 임베딩 (300차원)
"사과" = [0.2, -0.4, 0.7, 0.1, ...]     # 300개 실수
"바나나" = [0.3, -0.5, 0.6, 0.2, ...]   # 비슷한 값들
"과일" = [0.25, -0.45, 0.65, 0.15, ...] # 과일 카테고리
```

- 문맥 고려 방법 (Neural / Context-dependent)
- 신경망을 통해 단어의 의미를 주변 문맥을 고려하여 학습하고, 이를 밀집 벡터(Dense Vector)로 표현.


### 워드 임베딩 (Word Embedding) 

* 문맥 속에서 각 단어가 어떻게 사용되는지까지 신경망을 통해 벡터값을 구해 벡터에 담아내려 시도.
* 학습 후에는 각 단어 벡터 간의 유사도(의미반영)를 계산할 수 있다.
* 즉, 신경망 기반의 벡터화라는 것은 벡터의 값이 학습에 의해 결정된다는 것을 의미.
* 워드 임베딩 모델의 예시
  * Word2Vec, GloVe, FastText, 모델 내 `Embedding` Layer 사용.      
* Embedding Layer는 모델 내에 있는 레이어로, 입력 데이터를 밀집 벡터로 변환하는 역할을 한다.
   * 딥러닝 자연어 처리 시 거의 항상 하게 되는 작업
   * 단어 -> 정수 인코딩 -> Embedding Layer -> 임베딩 벡터(=밀집 벡터)
   * 자연어 처리에서 단어를 정수로 바꿔주는 이유가 Embedding Layer를 통해 밀집 벡터로 변환하기 위해서이다.
   * Look up table: 정수 인코딩을 밀집 벡터로 변환하는 테이블
* 핵심 원리
  * **분포 가설(Distributional Hypothesis):**
    * "같은 문맥에서 나타나는 단어들은 유사한 의미를 가진다"
    * 수학적으로 표현하면: $\text{similarity}(w_i, w_j) \propto \text{context\_overlap}(w_i, w_j)$
   * **"같은 문맥에 나타나는 단어들은 비슷한 의미를 가진다"**
   * **예시:**
   ```
   문장1: "나는 사과를 먹었다"
   문장2: "나는 바나나를 먹었다"  
   문장3: "나는 딸기를 먹었다"
   ```
   
   → "사과", "바나나", "딸기"는 같은 위치(문맥)에 나타남
   → 비슷한 벡터를 가져야 함

* 임베딩 벡터의 의미
   * 벡터 간 유사도

      ```python
      cosine_similarity(v_사과, v_바나나) = 0.8  # 높음 (비슷함)
      cosine_similarity(v_사과, v_컴퓨터) = 0.1  # 낮음 (다름)
      ```
   
      * 수식: $\text{similarity}(\mathbf{v}_1, \mathbf{v}_2) = \frac{\mathbf{v}_1 \cdot \mathbf{v}_2}{||\mathbf{v}_1|| \cdot ||\mathbf{v}_2||}$
      * 직관적 해석
         * 1에 가까울수록 비슷한 의미
         * 0에 가까울수록 관련 없음
         * -1에 가까울수록 반대 의미
   * 벡터 연산의 마법
      * 유명한 예시: 
         * $\vec{\text{king}} - \vec{\text{man}} + \vec{\text{woman}} \approx \vec{\text{queen}}$
         * $\vec{\text{king}} - \vec{\text{man}}$: "남성성"을 제거 → "왕권" 개념만 남음
         * $+ \vec{\text{woman}}$: "여성성" 추가
         * 결과: "여성 + 왕권" → "여왕"
      * 수학적 설명:
         * 각 벡터를 의미 성분들의 조합으로 생각:
         ```
         king = [왕권: 0.9, 남성: 0.8, 권력: 0.7, ...]
         man = [남성: 0.9, 성인: 0.6, ...]  
         woman = [여성: 0.9, 성인: 0.6, ...]
         ```
         * 연산 후:
         ```
         king - man + woman ≈ [왕권: 0.9, 여성: 0.9, 권력: 0.7, ...]
         ```
         → "queen"과 가장 유사!
* 실제 학습 예시
   * 초기 상태 (랜덤)
      
      ```
      "사과" = [0.1, -0.3, 0.7, ...]  (랜덤)
      "바나나" = [-0.8, 0.2, -0.1, ...] (랜덤)
      ```
      
      * 서로 전혀 관련 없어 보임
   * 학습 진행
      
      ```
      문장들을 계속 보면서:
      "사과를 먹었다", "바나나를 먹었다", "딸기를 먹었다"
      ...

      * 점차 비슷한 벡터로 수렴:
      
      ```
      "사과" = [0.2, -0.4, 0.6, ...]
      "바나나" = [0.3, -0.5, 0.7, ...]  
      "딸기" = [0.25, -0.45, 0.65, ...]
      ```

   * 학습 완료 후
      * 의미가 비슷한 단어들 → 벡터 공간에서 가까운 위치
      * 반대 의미 단어들 → 먼 위치 또는 반대 방향
      * 유추 관계 → 벡터 연산으로 표현 가능

* **핵심**: 신경망이 "문맥"이라는 단서를 통해 **단어의 의미**를 수치로 학습

#### Embedding Layer 구조 분석

* Embedding Layer란?
   * 정수 인코딩 → 밀집 벡터 변환기

```python
# 이런 변환을 해주는 것
15 → [0.2, -0.4, 0.7, 0.1, -0.3]  # 300차원 벡터
23 → [0.1, 0.3, -0.2, 0.8, 0.5]   # 300차원 벡터  
7  → [-0.1, 0.6, 0.4, -0.2, 0.9]  # 300차원 벡터
```

* 왜 정수 인코딩이 필요한가?
   * 문제: 컴퓨터는 "사과"라는 글자를 직접 처리할 수 없음
   * 해결 과정:
   
```python
# 1단계: 단어 → 정수 (정수 인코딩)
"사과" → 15
"바나나" → 23  
"딸기" → 7

# 2단계: 정수 → 벡터 (Embedding Layer)
15 → [0.2, -0.4, 0.7, ...]
23 → [0.1, 0.3, -0.2, ...]
7  → [-0.1, 0.6, 0.4, ...]
```

* Look-up Table의 구체적 동작
   * Embedding Matrix 구조: $\mathbf{E} \in \mathbb{R}^{V \times d}$
   * 실제 예시:

   ```python
   # V = 5 (어휘 크기), d = 3 (임베딩 차원)
   embedding_matrix = [
      [0.1, 0.2, 0.3],    # 단어 ID 0의 벡터
      [0.4, 0.5, 0.6],    # 단어 ID 1의 벡터  
      [0.7, 0.8, 0.9],    # 단어 ID 2의 벡터
      [0.2, -0.1, 0.4],   # 단어 ID 3의 벡터
      [0.5, 0.3, -0.2],   # 단어 ID 4의 벡터
   ]
   ```

* 행렬의 의미:
   * 행(row): 각 단어의 임베딩 벡터
   * 열(column): 임베딩 벡터의 각 차원
   * 전체: 모든 단어의 벡터를 저장하는 "사전"

* Look-up 연산 과정

   * 입력: `input_ids = [2, 0, 4]`

   ```python
   # 1단계: 각 ID에 해당하는 행을 추출
   embedding_matrix[2] → [0.7, 0.8, 0.9]    # ID 2의 벡터
   embedding_matrix[0] → [0.1, 0.2, 0.3]    # ID 0의 벡터  
   embedding_matrix[4] → [0.5, 0.3, -0.2]   # ID 4의 벡터

   # 2단계: 결과 (3개 벡터)
   output = [
      [0.7, 0.8, 0.9],     # 첫 번째 단어
      [0.1, 0.2, 0.3],     # 두 번째 단어
      [0.5, 0.3, -0.2]     # 세 번째 단어  
   ]
   ```

* 수학적 의미

   * $\text{embedding}(i) = \mathbf{E}[i, :]$
      * $i$: 단어의 정수 ID
      * $\mathbf{E}[i, :]$: 행렬 E의 i번째 행 전체
      * 결과: i번째 단어의 임베딩 벡터
   * 구체적 예시:

   ```python
   i = 2  # "딸기"의 ID라고 가정
   embedding(2) = E[2, :] = [0.7, 0.8, 0.9]  # 2번째 행
   ```

* 실제 PyTorch 코드

```python
import torch
import torch.nn as nn

# 1. Embedding Layer 생성
vocab_size = 1000      # 어휘 크기
embedding_dim = 300    # 벡터 차원
embedding = nn.Embedding(vocab_size, embedding_dim)

# 2. 내부 구조 확인
print(embedding.weight.shape)  # torch.Size([1000, 300])
# → 1000×300 크기의 look-up table

# 3. 입력 데이터
input_ids = torch.tensor([15, 23, 7])  # 3개 단어의 ID

# 4. 임베딩 변환
output = embedding(input_ids)
print(output.shape)  # torch.Size([3, 300])
# → 3개 단어 × 300차원 벡터
```

* Look-up Table이 학습되는 과정
   * 초기화 (랜덤)

   ```python
   # 처음에는 랜덤 값들
   embedding_matrix = torch.randn(vocab_size, embedding_dim)
   ```

* 학습 과정

```python
# 예: "사과는 맛있다"라는 문장 학습
input_ids = [15, 23, 7]  # [사과는, 맛있다, <END>]

# 1. 현재 임베딩으로 예측
embeddings = embedding_matrix[input_ids]  # Look-up
prediction = model(embeddings)

# 2. 손실 계산
loss = criterion(prediction, target)

# 3. 역전파로 embedding_matrix 업데이트  
loss.backward()  # embedding_matrix의 gradient 계산
optimizer.step()  # embedding_matrix 값들 업데이트
```

* 핵심: 학습이 진행되면서 embedding_matrix의 각 행(단어 벡터)이 점점 더 의미 있는 값으로 변함!

* 왜 "Look-up Table"이라고 부르는가?
   * 전통적인 사전과 비교

   ```python
   # 일반 사전
   사전 = {
      "사과": "빨간 과일",
      "바나나": "노란 과일", 
      "컴퓨터": "전자 기기"
   }
   의미 = 사전["사과"]  # "빨간 과일"

   # Embedding Table  
   임베딩_테이블 = {
      15: [0.2, -0.4, 0.7, ...],   # "사과"
      23: [0.1, 0.3, -0.2, ...],   # "바나나"
      78: [-0.5, 0.8, 0.1, ...]    # "컴퓨터"
   }
   벡터 = 임베딩_테이블[15]  # [0.2, -0.4, 0.7, ...]
   ```

   * 차이점:
      * 일반 사전: 단어 → 설명 (텍스트)
      * 임베딩 테이블: 단어 ID → 숫자 벡터

* 전체 과정 정리

```python
# 전체 파이프라인
"사과는 맛있다" 
→ ["사과는", "맛있다"]           # 토큰화
→ [15, 23]                      # 정수 인코딩  
→ [[0.2, -0.4, 0.7],           # Embedding Layer (Look-up)
   [0.1, 0.3, -0.2]]
→ 신경망 처리                    # 후속 레이어들
```

* 핵심
   * Embedding Layer는 단순히 "정수 ID를 인덱스로 사용해서 미리 저장된 벡터를 가져오는" 매우 단순한 연산. 
   * 하지만 이 벡터들이 학습을 통해 의미 있는 값으로 변하기 때문에 강력한 도구가 되는 것


#### 주요 모델별 특징

* **Word2Vec (2013)**의 두 가지 아키텍처: 
   * **CBOW (Continuous Bag of Words)**
      * 주변 단어들로 중심 단어 예측
      * 목적 함수: $\max \sum_{w \in V} \log P(w|context(w))$
      * 기본 구조
         * **목표**: 주변 단어들을 보고 가운데 단어를 맞추기

         ```
         입력: [나는] [___] [먹었다]  
         출력: [사과를]
         ```

      * 수학적 모델링
         * **1단계: 입력 표현**
            * 문맥 단어들: $w_{-2}, w_{-1}, w_{+1}, w_{+2}$ (윈도우 크기 2)
            * 각 단어의 원-핫 벡터: $\mathbf{x}_{w} \in \{0,1\}^V$ (V = 어휘 크기)

         * **2단계: 임베딩 변환**
            * $\mathbf{v}_w = \mathbf{W}_{\text{in}} \mathbf{x}_w$
            * 여기서:
               * $\mathbf{W}_{\text{in}} \in \mathbb{R}^{d \times V}$: 입력 임베딩 행렬
               * $d$: 임베딩 차원 (예: 300)
               * $\mathbf{v}_w \in \mathbb{R}^d$: 단어의 임베딩 벡터
            * **직관적 해석**: 
               * $\mathbf{W}_{\text{in}}$ 는 "단어 ID → 의미 벡터" 변환표
               * 원-핫 벡터와의 곱은 단순히 해당 행을 선택하는 것

         * **3단계: 문맥 벡터 계산**
            * $\mathbf{h} = \frac{1}{C} \sum_{c \in \text{context}} \mathbf{v}_c$
            * **직관적 해석**:
               * 주변 단어들의 평균 벡터
               * "이 위치에 올 수 있는 단어의 특징"을 나타냄
         * **4단계: 출력 확률 계산**
            * $P(w_{\text{center}}|\text{context}) = \frac{\exp(\mathbf{u}_{w_{\text{center}}}^T \mathbf{h})}{\sum_{w'=1}^V \exp(\mathbf{u}_{w'}^T \mathbf{h})}$
            * **직관적 해석**:   
               * 분자: 정답 단어가 이 문맥에 얼마나 적합한지
               * 분모: 모든 단어 중에서 정규화 (확률의 합 = 1)

            * 예시
               * **예시 문장**: "나는 사과를 먹었다"

               ```
               Step 1: 문맥 = ["나는", "먹었다"], 정답 = "사과를"
               Step 2: 현재 모델이 "바나나를" 높은 확률로 예측
               Step 3: 손실 계산 → "사과를"의 확률을 높이도록 가중치 업데이트
               Step 4: 반복 학습 후 → "나는 ___ 먹었다" 문맥에서 과일 단어들이 높은 확률
               ```

               * **학습 결과**:
                  * 비슷한 문맥에 나타나는 단어들 → 비슷한 벡터
                  * "사과", "바나나", "딸기" → 가까운 위치의 벡터
   * **Skip-gram:**
      * 중심 단어로 주변 단어들 예측  
      * 목적 함수: $\max \sum_{w \in V} \sum_{c \in context(w)} \log P(c|w)$
      * 특징:
         * 선형 관계 학습: $\vec{king} - \vec{man} + \vec{woman} \approx \vec{queen}$
         * 속도가 빠름
         * 단어별 고정된 하나의 벡터
      * 예시

         ```
         입력: [사과를]
         출력: [나는], [먹었다] (주변 모든 단어들)
         "중심 단어로 문맥 예측하기"
         ```

      * 수학적 모델
         * 목적 함수: $\max \sum_{w \in V} \sum_{c \in context(w)} \log P(c|w)$
         * 단계별 해석:
            * **1단계**: $w \in V$ (모든 단어에 대해)
               * 말뭉치의 모든 단어를 중심 단어로 한 번씩 사용
            * **2단계**: $c \in context(w)$ (각 중심 단어의 모든 문맥 단어에 대해)
               * 윈도우 크기만큼 주변 단어들을 문맥으로 설정
            * **3단계**: $\log P(c|w)$ (확률의 로그값)
               * 중심 단어 w가 주어졌을 때 문맥 단어 c가 나타날 확률
         * 예시
            * **문장**: "나는 사과를 정말 좋아한다" (윈도우 크기 = 2)

            ```
            중심 단어: "사과를"
            문맥 단어들: ["나는", "정말", "좋아한다"] (앞뒤 2개씩)

            Skip-gram이 학습하는 것:
            P("나는"|"사과를")     → 높아야 함
            P("정말"|"사과를")     → 높아야 함  
            P("좋아한다"|"사과를") → 높아야 함
            P("컴퓨터"|"사과를")   → 낮아야 함 (문맥에 없음)
            ```

         * 확률 계산: $P(c|w) = \frac{\exp(\mathbf{u}_c^T \mathbf{v}_w)}{\sum_{c'=1}^V \exp(\mathbf{u}_{c'}^T \mathbf{v}_w)}$
            * $\mathbf{v}_w$ : 중심 단어 w의 입력 벡터 (우리가 원하는 임베딩)
            * $\mathbf{u}_c$ : 문맥 단어 c의 출력 벡터
            * $\mathbf{u}_c^T \mathbf{v}_w$ : 두 단어가 "같이 나타날 가능성" 점수
            * Softmax로 정규화하여 확률로 변환
         * 학습 과정 예시
            * 초기 상태 (랜덤 벡터)

               ```python
               v_사과 = [0.1, -0.2, 0.3]  # 중심 단어 벡터
               u_나는 = [0.4, 0.1, -0.1]  # 문맥 단어 벡터
               u_컴퓨터 = [-0.2, 0.5, 0.2]
               ```

            * 점수 계산

               ```python
               score_나는 = dot(u_나는, v_사과) = 0.4*0.1 + 0.1*(-0.2) + (-0.1)*0.3 = -0.01
               score_컴퓨터 = dot(u_컴퓨터, v_사과) = -0.2*0.1 + 0.5*(-0.2) + 0.2*0.3 = -0.08
               ```

            * 확률 계산 (간단히)

               ```python
               P(나는|사과) = exp(-0.01) / (exp(-0.01) + exp(-0.08) + ...) ≈ 0.3
               P(컴퓨터|사과) = exp(-0.08) / (exp(-0.01) + exp(-0.08) + ...) ≈ 0.2
               ```

            * 학습 업데이트
               * 목표: P(나는|사과)는 높이고, P(컴퓨터|사과)는 낮추기

               ```python
               # 실제로 "나는"이 문맥에 있었으므로
               # v_사과와 u_나는을 더 비슷하게 만들기
               v_사과 += learning_rate * u_나는  # 벡터를 가까워지게
               u_나는 += learning_rate * v_사과

               # "컴퓨터"는 문맥에 없었으므로  
               # v_사과와 u_컴퓨터를 더 멀게 만들기
               v_사과 -= learning_rate * u_컴퓨터  # 벡터를 멀어지게
               u_컴퓨터 -= learning_rate * v_사과
               ```

* **GloVe (2014)**
   * 전역 통계 정보 활용
   * 단어 동시 출현 행렬(Co-occurrence Matrix) 기반
   * 목적 함수: $\min \sum_{i,j=1}^V f(X_{ij})(\vec{w_i}^T \vec{w_j} + b_i + b_j - \log X_{ij})^2$
   * 여기서 $X_{ij}$ 는 단어 $i$ 와 $j$ 의 동시 출현 빈도
   * Word2Vec의 한계: 
      * 지역적 문맥 정보만 사용 (윈도우 크기 내)
      * 전체 말뭉치의 통계 정보를 충분히 활용하지 못함
   * GloVe의 해결책:
      * 전체 말뭉치의 동시 출현 통계를 미리 계산
      * 이 통계 정보를 직접 활용하여 벡터 학습
   * 동시 출현 행렬 (Co-occurrence Matrix)
   * 구체적 예시
      * 말뭉치:

         ```
         문장1: "나는 사과를 좋아한다"
         문장2: "사과는 맛있는 과일이다"  
         문장3: "바나나도 좋은 과일이다"
         ```

      * 윈도우 크기 2로 동시 출현 계산:

         |       | 나는 | 사과를/사과는 | 좋아한다 | 맛있는 | 과일이다 | 바나나도 | 좋은 |
         |-------|------|---------------|----------|--------|----------|----------|------|
         | 나는     | 0    | 1             | 1        | 0      | 0        | 0        | 0    |
         | 사과를/사과는 | 1    | 0             | 1        | 1      | 1        | 0        | 0    |
         | 좋아한다   | 1    | 1             | 0        | 0      | 0        | 0        | 0    |
         | 맛있는    | 0    | 1             | 0        | 0      | 1        | 0        | 0    |
         | 과일이다   | 0    | 1             | 0        | 1      | 0        | 1        | 1    |
         | 바나나도   | 0    | 0             | 0        | 0      | 1        | 0        | 1    |
         | 좋은     | 0    | 0             | 0        | 0      | 1        | 1        | 0    |

      * 해석: $X_{ij}$ = 단어 i와 j가 윈도우 내에서 함께 나타난 횟수
   * GloVe 목적 함수 해부
      * $\min \sum_{i,j=1}^V f(X_{ij})(\mathbf{w}_i^T \mathbf{w}_j + b_i + b_j - \log X_{ij})^2$
      * 각 항목의 의미
         * $\mathbf{w}_i^T \mathbf{w}_j$ : 단어 벡터들의 내적
            * 두 단어의 유사도를 나타냄
            * 자주 함께 나타나는 단어들은 높은 내적값을 가져야 함
         * $b_i + b_j$ : 편향(bias) 항
            * 각 단어의 전반적인 빈도를 조정
            * 자주 나타나는 단어는 높은 편향값
         * $\log X_{ij}$ : 실제 동시 출현 빈도의 로그
            * 목표값 (우리가 맞추려는 값)
            * 로그를 취하는 이유: 빈도의 분포가 매우 치우쳐 있어서
         * $f(X_{ij})$ : 가중치 함수
            * 너무 희귀한 동시 출현은 신뢰도가 낮음 → 낮은 가중치
   
   $$
   f(x) = \begin{cases} 
   (\frac{x}{x_{max}})^{\alpha} & \text{if } x < x_{max} \\
   1 & \text{otherwise}
   \end{cases}
   $$

         * 가중치 함수의 역할:
            * 너무 희귀한 동시 출현은 신뢰도가 낮음 → 낮은 가중치
            * 너무 흔한 동시 출현도 정보량이 적음 → 가중치 제한
            * 적당한 빈도의 동시 출현에 높은 가중치

   * 학습 과정 예시
      * 예시: "사과"와 "과일" 관계 학습

      ```python
      # 실제 동시 출현: X_사과_과일 = 10번
      # 목표: w_사과^T * w_과일 + b_사과 + b_과일 ≈ log(10) = 2.3

      # 초기 (랜덤)
      w_사과 = [0.1, 0.2]
      w_과일 = [0.3, -0.1]  
      내적 = 0.1*0.3 + 0.2*(-0.1) = 0.01
      편향합 = 0.5 + 0.3 = 0.8
      예측값 = 0.01 + 0.8 = 0.81

      # 손실: (0.81 - 2.3)^2 = 2.22 (크다!)

      # 업데이트 후
      w_사과 = [0.4, 0.5]  # 더 큰 값들로
      w_과일 = [0.6, 0.2]
      내적 = 0.4*0.6 + 0.5*0.2 = 0.34
      편향합 = 1.2 + 0.7 = 1.9
      예측값 = 0.34 + 1.9 = 2.24  # 목표 2.3에 가까워짐!
      ```

* **FastText (2017)**
   * Sub-word 정보 활용
   * 단어를 character n-gram으로 분해
   * 예: "apple" → {"ap", "pp", "pl", "le"} + "apple"
   * 장점: 
      * OOV(Out-of-Vocabulary) 문제 해결
      * 형태학적 정보 포착
      * 한국어와 같은 교착어에 효과적
   * 기존 방법의 한계
      * Word2Vec/GloVe 문제:

         ```python
         # 훈련 데이터에 없는 단어 (OOV)
         "사과" → [0.2, 0.4, 0.1, ...]  ✓ (학습됨)
         "사과들" → ???  ✗ (학습 안됨)
         "사과나무" → ???  ✗ (학습 안됨)
         ```

      * 한국어의 특별한 어려움:
         
         ```
         "먹다" → "먹는다", "먹었다", "먹고", "먹어서", "먹지만", ...
         수천 가지 변형이 가능하지만 모두 같은 어근 "먹"을 공유
         ```

      * FastText의 해결책: Subword 분해
         * Character n-gram 분해
            * 예시: "사과" (n=2,3으로 설정)

         ```python
         "사과" 분해:
         - 2-gram: "<사", "사과", "과>"  # <, >는 단어 경계 표시
         - 3-gram: "<사과", "사과>"
         - 전체 단어: "사과"

         최종 n-gram 집합: {"<사", "사과", "과>", "<사과", "사과>", "사과"}
         ```

      * 벡터 표현
         * 기존 Word2Vec:

            ```python
            vector("사과") = lookup_table["사과"]  # 하나의 벡터
            ```

         * FastText:

            ```python
            vector("사과") = vector("<사") + vector("사과") + vector("과>") + 
                vector("<사과") + vector("사과>") + vector("사과")
                # n-gram 벡터들의 합
            ```

      * OOV 문제 해결 과정
         * 새로운 단어 처리

         ```python
         # 훈련 시 보지 못한 단어: "사과나무"

         # 1단계: n-gram 분해
         "사과나무" → {"<사", "사과", "과나", "나무", "무>", "<사과", "사과나", "과나무", "나무>", "사과나무"}

         # 2단계: 학습된 n-gram 벡터 찾기
         "<사": [0.1, 0.2, ...]     ✓ (있음)
         "사과": [0.3, 0.1, ...]    ✓ (있음)  
         "과나": [0.0, 0.0, ...]    ✗ (없음 → 0벡터)
         "나무": [0.2, 0.4, ...]    ✓ (있음)
         "무>": [0.1, 0.1, ...]     ✓ (있음)

         # 3단계: 합계 계산
         vector("사과나무") = sum(존재하는_ngram_벡터들) / 개수
         ```

         * 결과: "사과나무"는 "사과"와 "나무"의 의미를 모두 반영한 벡터를 얻음!

      * 형태학적 정보 포착
         * 한국어 예시

         ```python
         훈련 데이터:
         "먹는다" → {"<먹", "먹는", "는다", "다>", ...}
         "먹었다" → {"<먹", "먹었", "었다", "다>", ...}  
         "먹고" → {"<먹", "먹고", "고>", ...}

         # 공통 n-gram "<먹", "먹"이 반복 학습됨
         # → "먹" 관련 의미가 강화됨

         새로운 단어 "먹거나":
         {"<먹", "먹거", "거나", "나>", ...}
         # "<먹" n-gram을 통해 "먹다"와 관련된 의미를 자동으로 얻음!
         ```

         * 영어 예시

         ```python
         "running" ↔ "run"
         "running" → {"ru", "un", "nn", "ni", "in", "ng", "run", "unn", "nni", ...}
         "run" → {"ru", "un", "run", ...}

         공통 부분: {"ru", "un", "run"}을 통해 관계 학습
         ```

      * 실제 성능 비교
         * 한국어 형태소 분석 없이도 효과적
         * **Word2Vec**:

         ```python
         "좋다": [0.2, 0.4, 0.1, ...]
         "좋은": ??? (없으면 <UNK>)
         "좋아서": ??? (없으면 <UNK>)
         ```

         * **FastText**:

         ```python
         "좋다": [0.2, 0.4, 0.1, ...]
         "좋은": [0.18, 0.38, 0.12, ...]  # "좋" n-gram으로 유추
         "좋아서": [0.19, 0.39, 0.11, ...]  # "좋" n-gram으로 유추
         ```

         * 결과: 형태소 분석기 없이도 어근의 의미를 공유하는 벡터들을 얻을 수 있음!
         * Trade-off:
            * 장점: OOV 해결, 형태학적 정보 포착
            * 단점: n-gram 개수만큼 파라미터 증가
* Word2Vec: 단어 수 × 벡터 차원
* FastText: (단어 수 + 모든_ngram_수) × 벡터 차원


**실용적 해결책**: 빈도가 낮은 n-gram은 제외하여 크기 조절

## 결론

본 문서에서는 자연어 처리(NLP) 분야에서 텍스트 데이터의 의미를 효과적으로 포착하기 위해 통계 기반 방법의 한계를 넘어, 신경망은 단어와 문맥의 복잡한 관계를 학습하여 풍부한 정보를 담은 벡터 표현을 생성한다.

*   **워드 임베딩의 발전**:
    *   **정적 임베딩 (Word2Vec, GloVe, FastText)**: '분포 가설'에 기반하여 단어를 저차원 밀집 벡터로 표현함으로써 단어 간 의미적 유사성과 관계(예: 유추)를 포착했다. `Embedding Layer`는 이러한 변환의 핵심이며, FastText는 하위 단어(subword) 정보를 활용하여 OOV 문제와 형태론적 특징 처리에 강점을 보였다.
    *   이러한 초기 신경망 기반 방법들은 단어의 의미를 고정된 벡터로 표현하여 NLP 성능을 크게 향상시켰다.
*   **벡터화 방법 선택의 중요성**:
    *   단순한 단어 유사도 측정부터 복잡한 문서 이해 및 생성에 이르기까지, 해결하고자 하는 문제의 특성, 데이터의 규모와 성격, 그리고 사용하려는 모델의 요구사항을 종합적으로 고려하여 적절한 벡터화 전략을 선택하는 것이 중요하다.
    *   이러한 신경망 기반 벡터화 기법들은 현대 대규모 언어 모델(LLM) 발전의 핵심적인 토대가 되었으며, 자연어 이해 및 생성 능력의 비약적인 발전을 이끌고 있다.
