---
title: "텍스트 벡터화: 신경망 기반 방법론"
subtitle: "Word2Vec, GloVe, FastText부터 ELMo, BERT, SBERT까지 문맥을 이해하는 벡터 표현 소개"
description: |
  정적 임베딩의 한계를 넘어, 단어의 문맥적 의미를 동적으로 포착하는 ELMo, BERT, GPT, SBERT와 같은 주요 문맥 기반 임베딩 모델들의 원리, 특징, 혁신적인 기여를 살펴본다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-06
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False # 발행 시 False로 변경
---

# 요약

이 문서는 단어의 의미가 문맥에 따라 변하는 현상을 효과적으로 다루기 위해 등장한 **동적/문맥적 임베딩(Contextualized Embedding)** 방법론을 탐구한다. 정적 임베딩의 한계를 지적하고, 이를 극복하기 위한 주요 모델들의 핵심 아이디어와 특징을 소개한다.

주요 내용은 다음과 같다.

*   **정적 임베딩 vs. 동적 임베딩**:
    *   정적 임베딩(예: Word2Vec, GloVe)은 단어마다 고정된 벡터를 할당하여 문맥에 따른 의미 변화(다의성)를 포착하지 못하는 한계가 있다.
    *   동적 임베딩은 동일한 단어라도 문맥에 따라 다른 벡터 표현을 생성하여 이러한 문제를 해결한다.
*   **주요 문맥 기반 임베딩 모델**:
    *   **ELMo (Embeddings from Language Models)**: 양방향 LSTM(BiLSTM)의 각 계층에서 얻은 내부 상태들을 가중합하여 문맥 정보를 풍부하게 담은 임베딩을 생성한다. 문자 단위 표현부터 시작하여 다양한 수준의 정보를 결합한다.
    *   **BERT (Bidirectional Encoder Representations from Transformers)**: 트랜스포머(Transformer)의 인코더 구조를 활용하여 문장 내 모든 단어의 양방향 문맥을 동시에 고려한다. 'Masked Language Model(MLM)'과 'Next Sentence Prediction(NSP)'이라는 두 가지 혁신적인 사전 학습(pre-training) 목표를 통해 깊은 언어 이해 능력을 학습한다. 문서 전체의 표현으로는 `[CLS]` 토큰의 출력을 사용하거나 토큰 출력들의 풀링(pooling) 결과를 활용한다.
    *   **SBERT (Sentence-BERT)**: BERT의 출력을 문장 수준의 의미론적 벡터로 효율적으로 변환하기 위해 Siamese 또는 Triplet 네트워크 구조를 사용한다. 이를 통해 문장 간 유사도 계산 및 대규모 검색 작업의 효율성을 크게 향상시킨다.
    *   **GPT (Generative Pre-trained Transformer)**: 트랜스포머의 디코더 구조를 기반으로 하는 단방향(autoregressive) 언어 모델이다. 이전 단어들을 바탕으로 다음 단어를 예측하도록 학습하며, 이 과정에서 문맥을 이해하고 생성하는 능력을 키운다. 특히, 가중치 업데이트 없이 프롬프트에 몇 가지 예시(few-shot)를 제공하는 것만으로 새로운 작업을 수행하는 'In-context Learning' 능력으로 주목받았다. 문서 표현으로는 첫 번째 토큰([BOS])의 출력을 활용하기도 한다.
*   **실용적 응용 및 평가**:
    *   이러한 모델들은 문서 분류, 정보 검색, 질의응답, 기계 번역 등 다양한 NLP 태스크에서 혁신적인 성능 향상을 가져왔다.
    *   모델 평가는 단어 유사도나 관계 유추 같은 내재적 평가(intrinsic evaluation)와 실제 다운스트림 태스크에서의 성능을 측정하는 외재적 평가(extrinsic evaluation)로 이루어진다.

이 문서를 통해 독자는 문맥을 이해하는 동적 임베딩 기술의 발전 과정과 핵심 원리를 파악하고, 다양한 NLP 문제 해결에 이를 어떻게 활용할 수 있는지에 대한 통찰을 얻을 수 있다.

# 텍스트 인코딩 및 벡터화

텍스트 벡터화

```
├── 전통적 방법 (통계 기반)
│   ├── DTM (Document Term Matrix)
│   ├── BoW (Bag of Words)  
│   └── TF-IDF
└── 신경망 기반 방법
    ├── Embedding Layer (핵심 구성 요소)
    └── 구체적 모델들
        ├── Word2Vec (CBOW, Skip-gram)
        ├── GloVe
        ├── FastText
        └── 문맥 기반 모델 (BERT, GPT 등)
```
## 문맥을 고려한 벡터화 (2018-현재)

### 정적 vs 동적 임베딩

* 문제 상황: 동음이의어와 다의어
   * **영어 예시: "bank"**

   ```
   문장1: "I went to the bank to deposit money"  (은행)
   문장2: "The river bank was muddy"            (강둑)
   ```
   * **한국어 예시: "배"**

   ```
   문장1: "배가 고파서 밥을 먹었다"  (배 = 위장)
   문장2: "배를 타고 바다에 나갔다"  (배 = 선박)  
   문장3: "달콤한 배를 먹었다"      (배 = 과일)
   ```

* 정적 임베딩의 한계
   * **Word2Vec/GloVe 방식:**
      * 서로 다른 의미임에도 같은 벡터 사용
      * 문맥 정보를 활용하지 못함
      * 의미 구분이 불가능

   ```python
   # 단어별로 고정된 하나의 벡터만 존재
   embedding_table = {
      "bank": [0.2, -0.4, 0.7, 0.1, ...],  # 항상 같은 벡터
      "배": [0.3, 0.1, -0.2, 0.8, ...],    # 항상 같은 벡터
   }

   # 문맥에 관계없이 항상 같은 벡터 반환
   vector_bank_1 = embedding_table["bank"]  # 은행 문맥
   vector_bank_2 = embedding_table["bank"]  # 강둑 문맥
   # vector_bank_1 == vector_bank_2 (문제!)
   ```

* 동적 임베딩의 해결책
   * **BERT/ELMo 방식:**

   ```python
   # 같은 단어라도 문맥에 따라 다른 벡터 생성
   sentence1 = "I went to the bank to deposit money"
   sentence2 = "The river bank was muddy"

   vector_bank_1 = contextual_embedding(sentence1, word_position=4)  
   # → [0.8, 0.2, -0.1, ...]  (은행 의미)

   vector_bank_2 = contextual_embedding(sentence2, word_position=2)  
   # → [-0.3, 0.9, 0.4, ...]  (강둑 의미)

   # vector_bank_1 ≠ vector_bank_2 (해결!)
   ```



### 주요 모델별 분석

#### RNN (Recurrent Neural Network)

* FFNN(Feed Forward Neural Network): 행렬과 벡터 연산으로 이루어진다.
* RNN: 행렬과 벡터 연산 + 자기 자신의 출력을 다시 입력으로 사용한다.
   * 연속적인 시퀀스를 처리하기 위한 신경망
   * 사람은 이전 단어들에 대한 이해를 바탕으로 다음 단어를 이해한다.
   * 기존의 MLP에 비해서 RNN은 이러한 이슈를 다루며, 내부에 정보를 지속하는 루프로 구성된 신경망
   * 단순한 행렬과 벡터 연산을 넘어, **이전 시점의 은닉 상태(hidden state)를 현재 시점의 입력으로 다시 활용**하는 순환 구조
   * 이러한 "기억" 메커니즘 덕분에 RNN은 시간의 흐름에 따른 연속적인 데이터(시퀀스 데이터) 처리에 매우 효과적
   * **핵심 원리**: 신경망 내부에 루프(loop)를 만들어 정보가 지속되도록 함으로써, 마치 사람이 이전 대화 내용을 기억하며 다음 문장을 이해하는 것과 유사한 방식으로 작동
   * RNN은 입력의 길이만큼 신경망이 펼쳐진다. (unrolled)
   * 이때, 입력 받는 각 순간을 시점(time step)이라고 한다.
   * 시점 $t$ 에서 입력 $x_t$ 와 이전 시점의 은닉 상태 $h_{t-1}$ 을 받아 현재 시점의 은닉 상태 $h_t$ 를 계산
   * 매시점마다 새로운 입력값을 받고 은닉층에서 이전 시점의 정보를 다음 시점의 은닉층에 전달하는데 이것을 시간순대로 쭉 나열하여 도식화하면 그림이 너무 길어져 은닉층을 하나의 loop형태로 표현한다.
   * RNN은 FFNN (or MLP)에 시점을 도입한 개념과 같다.
   * RNN의 입력과 출력은 모두 기본적으로 벡터 단위를 가정한다. 따라서, 일반 RNN다이어 그램에선 입력층, 은닉층과 출력층이 소문자로 되어 있지만 모두 벡터라고 생각해야한다.
   * NLP에서 각 시점(time step)은 주로 단어 하나 (단어 벡터값) 또는 형태소 (한국어) 하나가 (형태소 벡터)가 된다.
* RNN의 설계
   * RNN의 구조는 설계하기 나름이지만 다음과 같은 유형을 갖는다.
   * One to Many
      * Image Captioning
      * 이미지를 첫 시점에서 입력받아 각 시점에서 출력
   * Many to One
      * 단어를 각 시점에서 입력받아 맨 마지막 시점의 은닉 상태를 출력
      * Text Classification
         * 단어들을 입력 받아 이것이 스펨메일인지 아닌지 맨 마지막 시점에서 분류
   * Many to Many
      * 각 시점에서 입력받은 단어를 각 시점에서 출력
      * sequence labeling: 각 단어에 대해 특정 레이블을 할당하는 작업으로, 주로 품사 태깅이나 개체명 인식과 같은 태스크에 사용된다.
         * 개체명 인식(Named Entity Recognition, NER): 문장에서 인물, 장소, 조직 등과 같은 고유명사를 식별하고 분류하는 작업
* Basic Architecture of RNN
   * Cell: 은닉층에 있는 RNN의 처리 단위 도식상에서 부르는 명칭, 보통 cell 이나 hidden state 구분없이 부르기도 한다.
   * Hidden State: Cell의 출력, RNN에서 부르는 명칭
   * RNN은 시점(time step)마다 입력을 받는데 현재 시점의 hidden state인 $h_t$ 연산을 위해 직전 시점의 hidden state인 $h_{t-1}$ 을 입력받는다.
   * 이게 RNN이 과거의 정보를 기억하는 원리이다.
   * 이러한 구조 덕분에 RNN은 시퀀스 데이터를 처리하는 데 매우 효과적이다.
   * 문장 내 각 단어는 시점(time step)이 되며, 각 단어는 벡터 형태로 입력된다.
   * 각 시점에서 입력된 벡터와 이전 시점의 hidden state를 받아 현재 시점의 hidden state를 계산한다.
   * 이렇게 계산된 hidden state는 다음 시점의 입력을 받을 때 사용된다.
   * 이 과정을 모든 시점에 반복하여 수행하면 문장 전체에 대한 정보를 효과적으로 표현할 수 있다.
   * 입력층: $x_t$
   * 은닉층 (cell): $h_t = \tanh(W_{h}h_{t-1} + W_{x}x_t + b_h)$ 
      * ex) $h_t = \tanh(W_{h}h_{t-1} + W_{x}x_t + b_h)$
   * 출력층 (output): $y_t = activation(W_{y}h_t + b)$ 
      * ex) $y_t = \text{softmax}(W_{y}h_t + b)$
   * 도식화
   ```{dot}
      digraph RNN_Cell {
         rankdir=TB;
         node [shape=box, style=filled];
         
         // 맨 위: y_t
         y_t [label="y_t", fillcolor=lightgray];
         
         // 중간: 빈 녹색 사각형(왼쪽)과 Cell(가운데)
         h_prev [label="", fillcolor=lightgreen];  // 빈 녹색 사각형
         cell [label="Cell", fillcolor=lightgreen, shape=box];
         
         // 맨 아래: x_t
         x_t [label="x_t", fillcolor=lightgray];
         
         // 연결선들과 라벨들
         h_prev -> cell [label="W_h", color=red, fontcolor=red, fontsize=10];
         x_t -> cell [label="W_x", color=red, fontcolor=red, fontsize=10];
         cell -> y_t [label="W_y", color=red, fontcolor=red, fontsize=10];
         cell -> h_prev [label="h_{t-1}", color=red, fontcolor=red, fontsize=10, dir=back];
         
         // 레이아웃 조정 - 수직 배치
         {rank=source; y_t;}  // 맨 위
         {rank=same; h_prev; cell;}  // 중간, 같은 레벨
         {rank=sink; x_t;}  // 맨 아래
         
         // 왼쪽-오른쪽 순서 조정
         h_prev -> cell [weight=10];
      }
   ```

![RNN matrix](../../../../../images/rnn/rnn_matrix.PNG)

* d: t time step의 단어 벡터의 차원
* $D_h$: hidden state의 차원 (RNN의 주요 파라미터)
* $W_h$: hidden state에 대한 가중치, 역전파로 최적화되는 파라미터
* $W_x$: 입력에 대한 가중치, 역전파로 최적화되는 파라미터
* $b_h$: hidden state에 대한 편향, 역전파로 최적화되는 파라미터
* $W_y$: 출력에 대한 가중치, 역전파로 최적화되는 파라미터
* $b$: 출력에 대한 편향

* tanh: hyperbolic tangent, RNN에서 주로 사용되는 활성화 함수
   * sigmoid함수와 달리 -1~1 사이의 값을 가지며, 이는 모델의 출력이 sigmoid 함수(0~1)보다 더 넓은 범위의 값을 가지게 됨을 의미한다.
   * tanh 함수는 출력 범위가 -1에서 1로 넓어, 시그모이드 함수의 0에서 1 범위보다 기울기 소실 문제를 줄여준다. 이는 학습 시 더 안정적이고 빠른 수렴을 가능하게 하여 은닉층에서 연산적으로 유리하다.
   * 따라서, tanh 함수는 시그모이드 함수보다 더 안정적이고 빠른 수렴을 가능하게 하여 은닉층에서 연산적으로 유리하다.
#### LSTM (Long Short-Term Memory)

* 기존 RNN의 한계: 장기 의존성 문제
   * 장기 의존성 문제 (Long-Term Dependency Problem): 기존의 RNN은 시점이 깅러지면 앞에 있던 정보가 소실되는 장기 의존성 문제를 갖고 있다
   * 즉, 너무 오래 전의 정보를 기억하지 못함. 이것은 RNN의 고질적인 문제입니다.
   * 기울기 소실 문제 (Gradient Vanishing Problem): 너무 오래 전의 정보를 기억하지 못함
   * Vanilla RNN: 기존의 RNN, 아이스크림 맛중에서 vanilla가 가장 기본적인 맛이기 때문에 vanilla RNN이라고 부른다.
* LSTM
   * LSTM은 기존의 RNN의 장기 의존성 문제를 해결하기 위해 은닉층에 추가적인 메커니즘을 도입한다.
   * 추가 메커니즘은 기억력을 증가시키는 것이다.
   * industry에서 RNN을 썼다고 하면 대부분 LSTM이나 GRU를 쓰는 것을 의미한다.
   * 기존의 RNN은 이전 시점의 hidden state의 정보를 현재 시점의 hidden state에 전달하면서 현재 시점의 출력값도 만들어내는 구조였다.
   * 기존의 RNN은 cell state가 없었다.
   * LSTM은 이전의 hidden state와 cell state 둘 모두를 다음 시점의 hidden state와 cell state의 정보를 전달한다.
   * cell state에 gate라는 구조를 통해서 정보를 더하거나 빼는 등의 통제를 한다.
* input gate: 현재 정보를 기억하기 위한 gate이다.   
   * cell state: $x_t$ 와 $h_{t-1}$ 을 받아서 현재 시점의 선택된 기억할 값을 정한다.
      * sigmoid 함수를 통해 [0,1]을 반환
      * tanh 함수를 통해 [-1,1]을 반환
      * 이 두 종류의 값을 갖고 cell state에서 이번에 선택된 기억할 값을 정한다. 즉,
      * $i_t = \sigma(W_{x_{i}}x_t + W_{h_i}h_{t-1} + b_i)$ 
      * $g_t = \tanh(W_{x_g} x_t + W_{h_g} h_{t-1} + b_g)$
      * 이때, $i_t$ 는 0~1 사이의 값을 가지며, $g_t$ 는 -1~1 사이의 값을 가진다.
      * 이 두 값을 곱해서 현재 시점의 cell state를 결정한다.
* forget gate: 이전 정보를 잊기 위한 gate이다.   
   * 기억을 삭제하기 위한 gate이다.
   * sigmoid 함수를 지나 [0,1]을 반환된 값으로 0에 가까울수록 정보가 많이 삭제된 것이며, 1에 가까울수록 정보를 온전히 기억한 셈이다. 즉, 
   * $f_t=\sigma(W_{x_f}x_t+W_{h_f}h_{t-1}+b_f)$
   * 이렇게, 일부 기억을 소실하고 입력 게이트의 $i_t$ 와 $g_t$ 의 정보를 조합하여 elementwise product를 수행하여 더해서 이번에 기억할 값을 결정
      * elementwise product: 각 요소별 곱셈을 의미한다.
      * 즉, 현재 시점의 cell state는 이전의 cell state와 현재 시점의 정보를 조합하여 결정된다.
      * $C_t = f_t \odot C_{t-1} + i_t \odot g_t$
      * 이때, $\odot$ 는 요소별 곱셈을 의미한다.
      * $i_t \odot g_t$ = input gate의 값, 현재 시점의 기억할 값 
      * $C_{t-1}$ = 이전 시점의 cell state
      * $f_t \odot C_{t-1}$ = forget gate의 값으로 과거의 기억을 삭제한 값
      * 즉, 현재 시점의 cell state는 이전의 cell state와 현재 시점의 정보를 조합하여 결정된다.
* input gate와 forget gate의 영향력
   * $f_t$ 가 0이 되면 이전 시점의 cell state가 완전히 삭제되고 오직 input gate만이 현재 시점의 cell state값을 결정한다.
      * 이를 forget gate가 닫히고 input gate만 열린 상태라고 한다.
   * 반면, $i_t$ 가 0이 되면 이전 시점의 cell state를 완전히 삭제하고 오직 forget gate만이 현재 시점의 cell state값을 결정한다.
      * 이를 input gate가 닫히고 forget gate만 열린 상태라고 한다.
   * 따라서, input gate는 현재 시점의 입력을 얼마나 반영할지 결정
   * 반면, forget gate는 이전 시점의 기억을 얼마나 유지할지 결정
* output gate
   * 현재 시점의 출력을 결정하기 위한 gate이다.
   * Hidden state를 연산하는 일에 쓰이며 Cell state와 비교하여 단기 상태라고도 부른다.
   * 이전 시점의 cell state는 현재 시점의 cell 내부에 기억력을 돕기위한 역할만 하고 현재 시점의 출력엔 영향을 주지 않는다.
   * 현재 시점의 출력은 이전 시점의 hidden state에 영향을 받은 현재 시점의 hidden state에 의해서만 결정된다.
   * sigmoid 함수를 통해 [0,1]을 반환
   * 이 값을 현재 시점의 cell state에 곱해서 현재 시점의 출력을 결정한다.즉,
   * $o_t = \sigma(W_{x_o}x_t + W_{h_o}h_{t-1} + b_o)$
   * $h_t = o_t \odot \tanh(C_t)$
* 양방향 LSTM (Bidirectional LSTM)
   * 기본 LSTM의 한계: 단방향 처리로 인한 정보 손실
      * 기본 LSTM은 과거에서 미래로만 정보를 처리한다
      * 현재 시점에서 미래의 정보를 활용할 수 없어 맥락 이해가 제한적이다
      * 예: "그는 은행에 갔다"에서 "은행"이 금융기관인지 강둑인지 앞뒤 문맥을 모두 봐야 판단 가능
   * 양방향 LSTM은 순방향과 역방향 두 개의 LSTM을 동시에 사용한다
   * Forward LSTM과 Backward LSTM이 독립적으로 작동하여 전체 시퀀스의 맥락을 모두 활용한다
   * 자연어 처리, 음성 인식 등에서 단방향 LSTM보다 우수한 성능을 보인다
   * Forward LSTM (순방향)
      * 일반적인 LSTM과 동일하게 시점 1부터 시점 T까지 순차적으로 처리한다
      * 각 시점에서 과거 정보를 현재로 전달한다
      * Forward hidden state: $\overrightarrow{h_t}$, Forward cell state: $\overrightarrow{C_t}$
      * Gate 연산:
         * $\overrightarrow{i_t} = \sigma(W_{x_i}x_t + W_{h_i}\overrightarrow{h_{t-1}} + b_i)$ (input gate)
         * $\overrightarrow{f_t} = \sigma(W_{x_f}x_t + W_{h_f}\overrightarrow{h_{t-1}} + b_f)$ (forget gate)
         * $\overrightarrow{g_t} = \tanh(W_{x_g}x_t + W_{h_g}\overrightarrow{h_{t-1}} + b_g)$ (candidate values)
         * $\overrightarrow{C_t} = \overrightarrow{f_t} \odot \overrightarrow{C_{t-1}} + \overrightarrow{i_t} \odot \overrightarrow{g_t}$ (cell state)
         * $\overrightarrow{o_t} = \sigma(W_{x_o}x_t + W_{h_o}\overrightarrow{h_{t-1}} + b_o)$ (output gate)
         * $\overrightarrow{h_t} = \overrightarrow{o_t} \odot \tanh(\overrightarrow{C_t})$ (hidden state)
   * Backward LSTM (역방향)
      * 시점 T부터 시점 1까지 역순으로 처리한다
      * 각 시점에서 미래 정보를 현재로 전달한다 ($t+1$ 시점에서 $t$ 시점으로)
      * Backward hidden state: $\overleftarrow{h_t}$, Backward cell state: $\overleftarrow{C_t}$
      * Gate 연산 (별도의 가중치 매개변수 $W'$ 사용):
         * $\overleftarrow{i_t} = \sigma(W'_{x_i}x_t + W'_{h_i}\overleftarrow{h_{t+1}} + b'_i)$ (input gate)
         * $\overleftarrow{f_t} = \sigma(W'_{x_f}x_t + W'_{h_f}\overleftarrow{h_{t+1}} + b'_f)$ (forget gate)
         * $\overleftarrow{g_t} = \tanh(W'_{x_g}x_t + W'_{h_g}\overleftarrow{h_{t+1}} + b'_g)$ (candidate values)
         * $\overleftarrow{C_t} = \overleftarrow{f_t} \odot \overleftarrow{C_{t+1}} + \overleftarrow{i_t} \odot \overleftarrow{g_t}$ (cell state)
         * $\overleftarrow{o_t} = \sigma(W'_{x_o}x_t + W'_{h_o}\overleftarrow{h_{t+1}} + b'_o)$ (output gate)
         * $\overleftarrow{h_t} = \overleftarrow{o_t} \odot \tanh(\overleftarrow{C_t})$ (hidden state)
   * 최종 출력 결합
      * 각 시점에서 Forward와 Backward의 hidden state를 결합하여 최종 출력을 생성한다
      * 가장 일반적인 방법은 연결(concatenation)이다: $h_t = [\overrightarrow{h_t}; \overleftarrow{h_t}]$
      * 최종 hidden state의 차원은 단방향 LSTM의 2배가 된다
      * 다른 결합 방식들:
         * 합계: $h_t = \overrightarrow{h_t} + \overleftarrow{h_t}$
         * 평균: $h_t = \frac{\overrightarrow{h_t} + \overleftarrow{h_t}}{2}$
         * 가중합: $h_t = \alpha \overrightarrow{h_t} + (1-\alpha) \overleftarrow{h_t}$
   * 양방향 LSTM의 장점과 한계
      * 장점: 전체 시퀀스의 맥락 정보를 모두 활용하여 더 정확한 표현 학습이 가능하다
      * 단점: 전체 시퀀스가 필요하므로 실시간 처리가 불가능하다
      * 계산 복잡도가 단방향 LSTM의 약 2배로 증가한다
      * 파라미터 수와 메모리 사용량이 증가한다
#### GRU (Gated Recurrent Unit)

* 뉴욕대학교 조경현 교수가 제안
* LSTM와 마찬가지로 장기 의존성 문제 비해 Vanilla RNN에 Robust
* 3개의 gate 있었던 LSTM과 달리 udpate gate, reset gate 2개로 줄였다.
* GRU는 LSTM의 복잡한 구조를 단순화한 모델이다.
* LSTM의 각 게이트를 하나의 게이트로 축소하여 모델을 간단하게 만든다.
* 기본적인 구조는 LSTM과 유사하다.
* 하지만, 더 적은 파라미터를 사용하여 더 빠른 학습을 가능하게 한다.
* 또한, 더 적은 메모리를 사용하여 더 빠른 처리를 가능하게 한다.
* $z_t=\sigma(W_z \dot [h_{t-1},x_t])$ : update gate
* $r_t=\sigma(W_r \dot [h_{t-1},x_t])$ : reset gate
* $\tilde{h}_t=\tanh(W \dot [r_t \odot h_{t-1},x_t])$ : candidate hidden state
* $h_t=(1-z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$ : hidden state
* 이때, $z_t$ 는 0~1 사이의 값을 가지며, $r_t$ 는 0~1 사이의 값을 가진다.
* $z_t$ 가 0에 가까울수록 이전 시점의 hidden state를 많이 유지하고, $z_t$ 가 1에 가까울수록 현재 시점의 candidate hidden state를 많이 반영한다.
* $r_t$ 가 0에 가까울수록 이전 시점의 hidden state를 많이 잊고, $r_t$ 가 1에 가까울수록 현재 시점의 candidate hidden state를 많이 기억한다.
* 이렇게 하여 장기 의존성 문제를 해결한다.
* LSTM과 GRU중 일반적으로 LSTM이 좀 더 많이 사용된다. 그 이유는 다음과 같다.
   * LSTM은 더 많은 파라미터를 가지고 있어 더 많은 정보를 저장할 수 있다.
   * 또한, LSTM은 더 많은 연산을 필요로 하여 더 많은 시간이 걸린다.
   * 하지만, LSTM은 더 나은 성능을 보인다.
   * 일반적으로 GRU는 LSTM보다 더 빠르게 학습되지만, LSTM은 더 나은 성능을 보인다.
   * 따라서, 일반적으로 LSTM이 더 많이 사용된다.

#### Deep Bidirectional RNN

* Deep RNN
   * RNN의 은닉층을 여러 개 쌓은 모델
   * 각 은닉층은 이전 은닉층의 출력을 입력으로 받아 현재 은닉층의 출력을 생성
   * 이렇게 하여 더 복잡한 패턴을 학습할 수 있다.
* Bidirectional RNN
   * 기본적인 구조는 RNN과 유사하다.
* Deep Bidirectional RNN
   * 기본적인 구조는 RNN과 유사하다.
   * 하지만, 더 많은 레이어를 가지고 있어 더 많은 정보를 저장할 수 있다.
* 또한, 더 많은 연산을 필요로 하여 더 많은 시간이 걸린다.
* 하지만, 더 나은 성능을 보인다.
* 일반적으로 GRU는 LSTM보다 더 빠르게 학습되지만, LSTM은 더 나은 성능을 보인다.
* 따라서, 일반적으로 LSTM이 더 많이 사용된다.

#### ELMo (Embedding from Language Models, 2018)

* ELMo 수식: $\text{ELMo}_k^{task} = \gamma^{task} \sum_{j=0}^L s_j^{task} \mathbf{h}_{k,j}^{LM}$
   * **$\mathbf{h}_{k,j}^{LM}$: 각 레이어의 hidden state**

   ```python
   # 예시: 3층 BiLSTM에서 "bank" 단어 (k번째 위치)
   h_{bank,0} = character_embedding("bank")     # 레이어 0 (입력)
   h_{bank,1} = first_LSTM_layer_output        # 레이어 1  
   h_{bank,2} = second_LSTM_layer_output       # 레이어 2
   h_{bank,3} = third_LSTM_layer_output        # 레이어 3 (최상위)
   ```
   * **$s_j^{task}$: 학습 가능한 가중치**
      * 각 레이어의 중요도를 태스크별로 학습
      * 문법적 태스크 → 낮은 레이어 중시
      * 의미적 태스크 → 높은 레이어 중시
   * **$\gamma^{task}$: 전체 스케일 조정**
      * ELMo 벡터의 전체적인 크기 조정
* 계산 예시

```python
# "bank" 단어의 ELMo 벡터 (감정 분석 태스크)
h_0 = [0.1, 0.2, 0.3]  # 문자 레벨
h_1 = [0.4, 0.5, 0.6]  # 낮은 레벨 (문법적)  
h_2 = [0.7, 0.8, 0.9]  # 높은 레벨 (의미적)

s_0 = 0.1  # 문자 레벨 가중치 (낮음)
s_1 = 0.3  # 문법 레벨 가중치  
s_2 = 0.6  # 의미 레벨 가중치 (높음)

ELMo_bank = γ × (s_0×h_0 + s_1×h_1 + s_2×h_2)
          = 2.0 × (0.1×[0.1,0.2,0.3] + 0.3×[0.4,0.5,0.6] + 0.6×[0.7,0.8,0.9])
          = 2.0 × [0.55, 0.65, 0.75]
          = [1.1, 1.3, 1.5]
```

* 양방향 정보의 중요성
   * **Forward만 사용할 경우:**

   ```
   "The bank was closed because of ___"
   → "bank"를 이해할 때 "The"만 참고
   ```

   * **Backward까지 사용할 경우:**  

   ```
   "The bank was closed because of ___"
   → "bank"를 이해할 때 "was closed" 정보도 참고
   → 금융 기관으로 해석 가능성 증가
   ```

#### BERT (Bidirectional Encoder Representations from Transformers, 2018)

* 양방향 문맥 동시 고려
   * 15% 단어를 마스킹하여 예측
   * 문장 간 관계 학습
* **핵심 혁신:**
   * **Transformer 기반**: 양방향 문맥 동시 고려
      * **기존 RNN의 한계:**

         ```python
         # RNN은 순차적 처리 (병렬화 어려움)
         h_1 = RNN(x_1)
         h_2 = RNN(x_2, h_1)      # h_1이 완료되어야 시작 가능
         h_3 = RNN(x_3, h_2)      # h_2가 완료되어야 시작 가능
         ```

      * **Transformer의 장점:**

         ```python
         # 모든 위치를 동시에 처리 (병렬화 가능)
         attention_weights = compute_attention(all_words)
         all_representations = apply_attention(all_words, attention_weights)
         ```

   * **Masked Language Model**: 15% 단어를 마스킹하여 예측
      * BERT의 핵심 학습 방법
         * **기본 아이디어**: 일부 단어를 숨기고 맞추게 하기

         ```python
         # 원본 문장
         "나는 [MASK]를 좋아한다"

         # 모델이 학습하는 것
         P("사과" | "나는 [MASK]를 좋아한다") = 0.7
         P("바나나" | "나는 [MASK]를 좋아한다") = 0.2  
         P("컴퓨터" | "나는 [MASK]를 좋아한다") = 0.01
         ```

         * **15% 마스킹 전략:**

         ```python
         입력 문장의 15% 단어에 대해:
         - 80%: [MASK] 토큰으로 교체
         - 10%: 랜덤한 다른 단어로 교체  
         - 10%: 원래 단어 그대로 유지
         ```

         * **왜 이렇게 하는가?**
            ```python
            # 80% [MASK]: 메인 학습 목적
            "나는 [MASK]를 좋아한다"

            # 10% 랜덤 교체: 노이즈에 강한 표현 학습
            "나는 컴퓨터를 좋아한다"  # 원래는 "사과"

            # 10% 원본 유지: 실제 사용 시와 동일한 조건
            "나는 사과를 좋아한다"
            ```

   * **Next Sentence Prediction**: 문장 간 관계 학습
   
   ```python
   # 실제 연속된 문장 (Positive)
   문장A: "나는 아침에 일어났다"
   문장B: "그리고 아침 식사를 했다"
   Label: IsNext = True

   # 랜덤하게 조합된 문장 (Negative)  
   문장A: "나는 아침에 일어났다"
   문장B: "축구는 재미있는 스포츠다"
   Label: IsNext = False
   ```

* **BERT의 문서 벡터화 방법:**
   * **[CLS] 토큰**: 문장/문서 전체 표현

   ```python
   입력: "[CLS] 문장 내용 [SEP]"
   출력: [CLS]_벡터가 전체 문장의 의미를 담음

   # 예시
   input_tokens = ["[CLS]", "나는", "사과를", "좋아한다", "[SEP]"]
   bert_output = bert_model(input_tokens)
   sentence_vector = bert_output[0]  # [CLS] 위치의 벡터
   ```

   * **Pooling 전략**: 
      * Mean pooling: $\frac{1}{n}\sum_{i=1}^n \mathbf{h}_i$
      * Max pooling: $\max(\mathbf{h}_1, ..., \mathbf{h}_n)$

      ```python
      # 모든 토큰의 BERT 출력
      token_representations = [
         [0.1, 0.2, 0.3],  # [CLS]
         [0.4, 0.5, 0.6],  # "나는"  
         [0.7, 0.8, 0.9],  # "사과를"
         [0.2, 0.3, 0.4],  # "좋아한다"
         [0.5, 0.6, 0.7],  # [SEP]
      ]

      # Mean Pooling
      mean_vector = mean(token_representations[1:-1])  # [CLS], [SEP] 제외
      = (0.4+0.7+0.2)/3, (0.5+0.8+0.3)/3, (0.6+0.9+0.4)/3
      = [0.43, 0.53, 0.63]

      # Max Pooling  
      max_vector = max(token_representations[1:-1])  # 각 차원별 최댓값
      = [0.7, 0.8, 0.9]
      ```
*   **특징**:
    *   단어의 의미적, 문법적 정보를 벡터 공간에 학습.
    *   벡터 간 연산을 통해 단어 간 유사도, 유추 등 관계 표현 가능 (예: "king" - "man" + "woman" ≈ "queen").
*   **중요성**: 현대 NLP 딥러닝 모델의 핵심 구성 요소로, 성능 향상에 크게 기여.



#### SBERT (Sentence-BERT)

* 최근 가장 보편적인 문장 또는 문서 임베딩 방법으로 SBERT가 이용된다.
* 문서의 유사도를 구할 때는 SBERT 사용을 권장
* 문장 벡터화 전략
   * 문장 간 유사도 계산
   * 문장 간 유사도 계산 시 문장 임베딩 사용
* 기존 BERT의 한계: 문장 유사도 계산의 비효율성

   ```python
   # 1000개 문장의 유사도를 모두 구하려면
   sentences = ["문장1", "문장2", ..., "문장1000"]

   # 기존 BERT 방식 (비효율적)
   for i in range(1000):
      for j in range(i+1, 1000):
         combined = f"[CLS] {sentences[i]} [SEP] {sentences[j]} [SEP]"
         similarity = bert_classifier(combined)  # 매번 BERT 실행
         
   # 총 계산 횟수: 1000 × 999 / 2 = 499,500번!
   ```

   * BERT로 문장 유사도를 계산하려면:
      * 두 문장을 [SEP]로 연결
      * BERT에 입력하여 분류
      * $O(n^2)$ 시간 복잡도 (n개 문장 비교 시)
* SBERT의 해결책: Siamese Network 구조

```python
# SBERT 방식 (효율적)

문장 A → BERT → Pooling → Vector A
문장 B → BERT → Pooling → Vector B
유사도 = cosine_similarity(Vector A, Vector B)

# 1단계: 모든 문장을 미리 벡터화
sentence_vectors = []
for sentence in sentences:
    vector = sbert_model(sentence)  # 각 문장마다 1번씩만 실행
    sentence_vectors.append(vector)

# 2단계: 벡터 간 코사인 유사도로 빠른 계산
for i in range(1000):
    for j in range(i+1, 1000):
        similarity = cosine_similarity(sentence_vectors[i], sentence_vectors[j])
        
# 총 SBERT 실행 횟수: 1000번 (대폭 감소!)
```

* **학습 목적 함수:**
   * **Classification**: $\mathcal{L} = -\sum_{i} y_i \log(\text{softmax}(W[\mathbf{u}; \mathbf{v}; |\mathbf{u}-\mathbf{v}|]))$

   ```python
   # 두 문장의 SBERT 벡터
   u = sbert("나는 사과를 좋아한다")      # [0.2, 0.4, 0.1, ...]
   v = sbert("나는 바나나를 좋아한다")    # [0.3, 0.5, 0.2, ...]

   # 특성 벡터 구성
   concat = [u; v]                    # 연결: [0.2, 0.4, 0.1, 0.3, 0.5, 0.2, ...]
   abs_diff = |u - v|                # 절댓값 차이: [0.1, 0.1, 0.1, ...]
   features = [u; v; abs_diff]        # 최종 특성 벡터

   # 분류 (유사/비유사)
   logits = W @ features + b
   probability = softmax(logits)
   loss = cross_entropy(probability, true_label)
   ```

   * **Regression**: $\mathcal{L} = \text{MSE}(\text{cosine\_sim}(\mathbf{u}, \mathbf{v}), \text{label})$
   
   ```python
   # 예측 유사도
   predicted_sim = cosine_similarity(u, v) = 0.85

   # 실제 라벨 (0~1 점수)
   true_sim = 0.9  # 사람이 평가한 유사도

   # 손실 계산
   loss = (predicted_sim - true_sim)² = (0.85 - 0.9)² = 0.0025
   ```

* **성능 개선:**
   * 시간 복잡도: $O(n^2) \rightarrow O(n)$

   ```python
   # 시간 복잡도 비교
   기존_BERT_시간 = O(n²) = 1000² = 1,000,000
   SBERT_시간 = O(n) = 1000

   속도_향상 = 1,000,000 / 1000 = 1000배!
   ```
   * 의미적 유사도 정확도 대폭 향상
   * 대규모 문서 검색 시스템
   * 실시간 문장 유사도 계산
   * 추천 시스템에서의 텍스트 매칭

#### GPT(Generative Pre-trained Transformer)

* 단방향 언어 모델의 핵심 개념
* BERT vs GPT의 근본적 차이
* **BERT (양방향)**:
   
   ```
   입력: "나는 [MASK]를 좋아한다"
   모델이 보는 정보: "나는" + "를 좋아한다" (양쪽 모두)
   예측: [MASK] = "사과"
   ```

* **GPT (단방향)**:
   
   ```
   입력: "나는 사과를"
   모델이 보는 정보: "나는 사과를" (왼쪽만)
   예측: 다음 단어 = "좋아한다"
   ```

* 왜 단방향일까?
   * **생성 태스크의 특성**:

      ```python
      # 실제 텍스트 생성 시
      "안녕하세요, 오늘 날씨가"
      → 모델: "좋네요" (미래 정보는 알 수 없음)

      # 만약 양방향이라면?
      "안녕하세요, 오늘 날씨가 [미래정보] 입니다"
      → 실제 생성 시에는 미래 정보가 없으므로 불일치
      ```

* **GPT의 학습 방식: Autoregressive Language Modeling**
   * 이전 토큰들로 다음 토큰 예측
   * 수학적 목적 함수: $P(\text{문장}) = \prod_{t=1}^T P(w_t | w_1, w_2, ..., w_{t-1})$
      * 문장의 확률 = 각 단어가 이전 단어들 조건 하에 나타날 확률의 곱
   * 구체적 학습 예시
      * **훈련 문장**: "나는 사과를 좋아한다"

      ```python
      # 학습 데이터 구성
      입력 → 정답
      "나는" → "사과를"
      "나는 사과를" → "좋아한다"  
      "나는 사과를 좋아한다" → "<끝>"

      # 손실 함수
      loss = -log P("사과를" | "나는") 
            -log P("좋아한다" | "나는 사과를")
            -log P("<끝>" | "나는 사과를 좋아한다")
      ```

   * Causal Masking (인과 마스킹)
      * Attention에서 미래 정보 차단
      ```python
      # Attention Matrix (4개 단어 예시)
              나는  사과를  좋아한다  <끝>
      나는     ✓     ✗      ✗      ✗
      사과를    ✓     ✓      ✗      ✗  
      좋아한다  ✓     ✓      ✓      ✗
      <끝>     ✓     ✓      ✓      ✓
      
      # ✓: 참고 가능, ✗: 마스킹 (참고 불가)
      ```

   * **코드 구현**:

      ```python
      # 마스킹 행렬
      mask = torch.tril(torch.ones(seq_len, seq_len))
      # 상삼각 부분을 -무한대로 설정
      attention_scores.masked_fill_(mask == 0, -1e9)
      attention_weights = softmax(attention_scores)
      ```

* **첫 번째 토큰을 문서 표현으로 활용**
   * 정보 흐름의 특성
      * 입력: "[BOS] 문장 내용들..."
      * 각 토큰이 보는 정보량
      * 토큰1 ([BOS]): 자기 자신만
      * 토큰2: [BOS] + 토큰2  
      * 토큰3: [BOS] + 토큰2 + 토큰3
* **왜 첫 번째 토큰인가?**
   * 정보 흐름의 특성
      * 입력: "[BOS] 문장 내용들..."
      * 각 토큰이 보는 정보량
      * 토큰1 ([BOS]): 자기 자신만
      * 토큰2: [BOS] + 토큰2  
      * 토큰3: [BOS] + 토큰2 + 토큰3
      * 마지막토큰: [BOS] + 전체 문장
      * 역설적으로, [BOS]는 전체 문장을 "예측"해야 하므로
      * 전체 문장 정보를 압축한 표현을 학습하게 됨
   * 구체적 메커니즘
      * 학습 과정에서의 압축

      ```python
      # GPT가 학습하는 것
      P(전체_문장 | [BOS]) = P(w1|[BOS]) × P(w2|[BOS],w1) × ... × P(wn|[BOS],w1,...,wn-1)

      # [BOS] 토큰은 "이 문장이 어떤 내용일까?"를 예측해야 함
      # → 문장의 주제, 감정, 스타일 등을 함축하는 표현을 학습
      ```

* **실제 활용 예시**:

   ```python
   # 문서 분류
   document = "[BOS] 이 영화는 정말 재미있었다. 스토리도 좋고..."
   gpt_output = gpt_model(document)
   document_vector = gpt_output[0]  # [BOS] 위치의 벡터
   classification = classifier(document_vector)  # 긍정/부정 분류
   ```

* **In-context Learning 심화 분석**
   * 기존 학습 방식과의 차이
      * **전통적 학습 (Fine-tuning)**:

         ```python
         # 1단계: 새로운 태스크 데이터로 모델 가중치 업데이트
         model.train()
         for batch in task_data:
            loss = compute_loss(model(batch.input), batch.target)
            loss.backward()
            optimizer.step()

         # 2단계: 추론
         prediction = model(new_input)
         ```

      * **In-context Learning**:

         ```python
         # 가중치 업데이트 없이, 입력에 예시를 포함
         context = """
         번역 예시:
         영어: Hello → 한국어: 안녕하세요
         영어: Thank you → 한국어: 감사합니다  
         영어: Good morning → 한국어: 좋은 아침

         영어: How are you? → 한국어:
         """

         result = gpt_model(context)  # "어떻게 지내세요?" 출력
         ```

* **왜 In-context Learning이 가능한가?**
   * 패턴 인식 능력
      * GPT가 학습 중 본 패턴들
      * "A는 B이다. C는 D이다. E는"  → F 예측
      * "1+1=2, 2+2=4, 3+3=" → 6 예측
      * "cat→고양이, dog→개, bird→" → 새 예측
   * 메타 학습 (Learning to Learn)
    
      ```python
      # 다양한 패턴을 학습하면서 "학습하는 방법"을 학습
      패턴1: 번역 (A→B 형태)
      패턴2: 수학 (계산 규칙)  
      패턴3: 분류 (라벨링 규칙)

      # 새로운 패턴이 주어져도 빠르게 적응
      ```

* **실제 In-context Learning 예시**
   * **감정 분석 태스크**:

      ```python
      prompt = """
      다음은 리뷰와 감정을 분류한 예시입니다:

      리뷰: "이 영화 정말 재미있어요!" 감정: 긍정
      리뷰: "시간 낭비였습니다." 감정: 부정
      리뷰: "그냥 그래요." 감정: 중립

      리뷰: "배우들 연기가 훌륭했습니다!" 감정:
      """

      # GPT 출력: "긍정"
      ```
   * **번역 태스크**:
   
      ```python
      prompt = """
      English to Korean translation:

      English: I love programming
      Korean: 나는 프로그래밍을 좋아합니다

      English: The weather is nice today  
      Korean: 오늘 날씨가 좋네요

      English: What time is it now?
      Korean:
      """

      # GPT 출력: "지금 몇 시인가요?"
      ```

* **GPT 발전사와 특징**
   * **GPT-1 (2018)**
      * 크기: 117M 파라미터
      * 특징: Transformer 디코더만 사용
      * 성능: 간단한 텍스트 생성
   * **GPT-2 (2019)**
      * 크기: 1.5B 파라미터
      * 특징: 스케일 확장의 효과 입증
      * 성능: 일관성 있는 긴 텍스트 생성
   * **GPT-3 (2020)**
      * 크기: 175B 파라미터  
      * 특징: In-context Learning의 강력한 능력
      * 성능: Few-shot Learning으로 다양한 태스크 수행
   * **GPT-4 (2023)**
      * 크기: 공개되지 않음 (추정 수조 개)
      * 특징: 멀티모달 (텍스트 + 이미지)
      * 성능: 인간 수준에 근접한 성능

* **GPT vs BERT 비교 정리**

| 측면 | GPT | BERT |
|------|-----|------|
| **방향성** | 단방향 (왼쪽→오른쪽) | 양방향 |
| **학습 목표** | 다음 토큰 예측 | 마스킹된 토큰 예측 |
| **주요 용도** | 생성 태스크 | 이해 태스크 |
| **문서 벡터** | 첫 번째 토큰 | [CLS] 토큰 |
| **특별 능력** | In-context Learning | Fine-tuning 효율성 |

* **결론**: GPT는 "다음에 올 단어를 예측"하는 단순한 목표로 학습하지만, 이 과정에서 언어의 패턴, 의미, 추론 능력까지 학습하게 되어 강력한 생성 및 추론 모델이 되었다.

### 실용적 응용 및 평가

#### 평가 지표

**Intrinsic Evaluation (내재적 평가):**
- **단어 유사도**: WordSim-353, SimLex-999 - 사람이 평가한 단어 유사도와 모델 예측의 상관관계 측정
- **단어 관계**: "king - man + woman = queen" - 벡터 연산으로 의미 관계 포착 정도 평가

**Extrinsic Evaluation (외재적 평가):**
- **문서 분류 정확도**: 실제 분류 태스크에서의 성능
- **정보 검색 성능**: NDCG, MAP - 검색 결과의 관련성 및 순위 정확도
- **의미적 텍스트 유사도**: STS benchmark - 문장 간 의미적 유사성 예측 성능

#### 모델 선택 가이드

- **소규모 데이터**: FastText (OOV 처리)
- **대규모 문서 분류**: BERT fine-tuning
- **실시간 유사도 계산**: SBERT
- **창작/생성 태스크**: GPT 계열

#### 통계적 해석

임베딩 공간에서의 기하학적 관계:
$$\cos(\mathbf{v}_{\text{similar words}}) > \cos(\mathbf{v}_{\text{dissimilar words}})$$

**시각화 도구**: t-SNE/UMAP을 통한 의미적 클러스터링 확인


## 결론

본 문서에서는 단어의 고정된 의미 표현을 넘어, 문맥에 따라 유연하게 변화하는 의미를 포착하는 동적 임베딩 방법론들을 심층적으로 살펴보았다. ELMo에서 시작하여 BERT, GPT, SBERT에 이르기까지, 이러한 문맥 기반 임베딩 모델들은 자연어 처리(NLP) 분야에 혁명적인 발전을 가져왔다.

주요 내용을 다시 한번 정리하면 다음과 같다.

*   **정적 임베딩의 한계 극복**: 초기의 워드 임베딩(Word2Vec, GloVe 등)은 단어의 의미를 단일 벡터로 표현하여 문맥에 따른 다의성을 반영하지 못했다. 동적 임베딩은 이 한계를 극복하고, 동일한 단어라도 문맥에 따라 다른 벡터 표현을 생성함으로써 보다 정교한 의미 이해를 가능하게 했다.

*   **주요 모델들의 혁신과 기여**:
    *   **ELMo**: 양방향 LSTM을 통해 문맥 정보를 통합하고, 여러 계층의 표현을 활용하여 풍부한 임베딩을 제공했다.
    *   **BERT**: 트랜스포머 아키텍처와 Masked Language Model, Next Sentence Prediction과 같은 혁신적인 사전 학습 방식을 도입하여 양방향 문맥 이해의 새로운 지평을 열었다. 이는 다양한 NLP 다운스트림 태스크에서 SOTA(State-of-the-Art) 성능을 달성하는 데 크게 기여했다.
    *   **GPT**: 단방향 트랜스포머 디코더를 기반으로 강력한 텍스트 생성 능력을 보여주었으며, 특히 GPT-3 이후 모델들은 In-context Learning이라는 새로운 패러다임을 제시하며 모델 활용의 유연성을 크게 확장했다.
    *   **SBERT**: 기존 BERT 모델을 문장 임베딩 생성에 효율적으로 사용할 수 있도록 Siamese 및 Triplet 네트워크 구조를 활용하여, 의미적으로 유사한 문장 벡터를 효과적으로 생성하고 문장 간 유사도 비교 작업의 속도와 정확도를 크게 향상시켰다.

*   **패러다임의 전환과 LLM의 토대**: 이러한 문맥 기반 임베딩 모델들의 발전은 단순한 특징 추출기를 넘어, 언어 자체를 깊이 이해하고 생성할 수 있는 대규모 언어 모델(Large Language Models, LLMs) 시대로 나아가는 핵심적인 발판이 되었다. 사전 학습과 미세 조정(fine-tuning) 패러다임, 그리고 최근의 프롬프트 기반 학습은 모델의 활용 범위를 크게 넓혔다.

*   **적절한 전략 선택의 지속적 중요성**: 해결하고자 하는 특정 문제의 요구사항, 가용 데이터의 특성, 계산 자원 등을 고려하여 가장 적합한 임베딩 전략과 모델을 선택하는 것은 여전히 중요하다. 실용적인 응용을 위해서는 모델의 성능뿐만 아니라 효율성, 해석 가능성 등도 함께 고려해야 한다.

문맥을 이해하는 텍스트 벡터화 기술은 앞으로도 계속 발전하여, 기계가 인간의 언어를 더욱 정교하게 이해하고 상호작용하는 미래를 앞당길 것이다. 이러한 기술의 발전은 정보 검색, 질의응답, 창작, 교육 등 사회 여러 분야에 걸쳐 혁신적인 변화를 주도할 잠재력을 지니고 있다.
