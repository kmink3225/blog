---
title: "텍스트 인코딩 및 벡터화: NLP 숫자 변환의 모든 것"
subtitle: "정수 인코딩, OOV 처리, 패딩부터 원-핫, BoW, TF-IDF까지 핵심 개념 총정리"
description: |
  자연어 처리(NLP)에서 텍스트 데이터를 기계가 이해하고 처리할 수 있는 숫자 형태로 변환하는 인코딩 및 벡터화의 주요 개념과 방법들을 살펴본다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-04
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

# 이 문서 한눈에 보기

이 문서는 자연어 처리(NLP)를 위해 텍스트 데이터를 기계가 이해할 수 있는 숫자 형태로 변환하는 주요 과정인 **인코딩(Encoding)**과 **벡터화(Vectorization)**에 대해 심층적으로 다룬다.

주요 내용은 다음과 같다.

*   **인코딩 (Encoding)**:
    *   **정의 및 필요성**: 텍스트(문자열)를 모델이 처리할 수 있는 형태로 바꾸는 모든 과정으로 텍스트를 숫자(주로 정수 인덱스)로 변환하는 과정이다.
    *   **정수 인코딩**: 각 단어에 고유 정수를 부여하는 방법과 어휘 집합 크기 결정, 그리고 학습 데이터에 없는 단어(OOV: Out-of-Vocabulary)를 `UNK` 토큰으로 처리하는 방법을 설명한다.
*   **패딩 (Padding)**:
    *   **정의 및 필요성**: 길이가 다른 텍스트 시퀀스들을 모델 입력을 위해 동일한 길이로 맞춰주는 작업으로, `PAD` 토큰을 사용한다.
*   **벡터화 (Vectorization)**:
    *   **정의**: 정수 인코딩된 데이터를 숫자 벡터로 변환하여 텍스트의 의미나 통계적 정보를 표현한다.
    *   **통계적 방법**:
        *   **원-핫 인코딩**: 각 단어를 고유한 희소 벡터로 표현하며, 장단점(차원의 저주)을 다룬다.
        *   **빈도 기반 방법 (DTM, BoW, TF-IDF)**: 문서 내 단어 빈도를 기반으로 문서를 벡터화하는 DTM(Document Term Matrix), Bag-of-Words(BoW), TF-IDF 기법의 개념과 특징, 활용 방안을 설명한다.
    *   **벡터 표현의 단위**: 단어는 벡터로, 문서는 벡터 또는 행렬로 표현될 수 있음을 설명한다.
*   **결론**: 효과적인 인코딩 및 벡터화 전략 선택의 중요성을 강조한다.

이 문서를 통해 텍스트 데이터가 NLP 모델에서 어떻게 처리될 수 있도록 준비되는지에 대한 기본적인 이해를 얻을 수 있다.

# 텍스트 인코딩 및 벡터화

```
텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention
 
Transformer 이후 생성형 모델 발전 계열
├── Transformer 구조 (Vaswani et al., 2017)
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등
```
## 인코딩(Encoding)

*   **인코딩 (Encoding)**:
    *   **정의**: 자연어 처리(NLP)에서 **텍스트 데이터를 기계가 이해하고 처리할 수 있는 숫자 형태로 변환하는 과정** 전반을 의미. 컴퓨터는 텍스트를 직접 이해할 수 없으므로, 토큰화된 각 구성 요소(단어 등)를 숫자 표현(주로 정수 인덱스)으로 바꾸는 단계.
    *   **핵심**: 토큰화 후, 토큰들을 숫자로 매핑.
*   **주요 기법**
   * 정수 인코딩: 보통 사전 정의된 정수, 의미 보존 x, 비학습, 코사인 유사도 측정 불가
      * 예: 원-핫 인코딩, Indexing, 빈도 기반 인코딩 등
   * embedding: 학습 가능, 코사인 유사도 측정 가능, 문맥 표현 가능
      * 예: word2vec, fasttext, glove 등.

### 정수 인코딩 (Integer Encoding)

*   **개념**: 어휘 집합(Vocabulary) 내 각 고유 토큰에 **고유한 정수 인덱스를 부여**.
*   **과정**:
    1.  어휘 집합 구축: 전체 텍스트에서 고유 토큰 추출 
      * 빈도, 최대 크기 고려하여 어휘 집합 크기 제한하는 것이 현실적인 전략
    2.  각 토큰에 정수 할당
      * 빈도 높은 단어에 낮은 숫자 할당
    3.  텍스트의 토큰 시퀀스를 정수 시퀀스로 변환
* **예시**:
   * 어휘 집합 구축:
       *   예시 문장: `"I love natural language processing"`
       *   토큰화 결과: `["i", "love", "natural", "language", "processing"]`
       *   초기 어휘 집합 (`word_to_index`)의 정수 인코딩
         * `{"i": 1, "love": 2, "natural": 3, "language": 4, "processing": 5}`
       *   정수 시퀀스: `[1, 2, 3, 4, 5]`
       *   이때 어휘 집합의 크기는 5이다.
*   **한계**
    *   텍스트간 유사도 측정 불가: 숫자 값 자체가 단어 간 의미/관계 표현 못 함 (모델 오해 가능성).

### OOV (Out-of-Vocabulary) 문제

*   **정의**: 학습 시 구축된 어휘 집합에 **포함되지 않은 단어**가 입력될 때 발생.
*   **원인**: 신조어, 전문 용어, 오타, 제한된 어휘 크기 등.
*   **영향**: 정보 손실, 잘못된 예측, 모델 성능 저하.
*   **`UNK` (Unknown) 토큰 처리**:
    *   OOV 단어를 미리 정의된 `UNK` 토큰의 인덱스로 일괄 대체.
    *   모든 OOV가 동일 토큰으로 매핑되어 원래 단어의 고유 정보는 손실.
*   **어휘 집합 크기 및 신규 단어(OOV) 처리 예시 (영문)**:
    *   어휘 집합의 크기는 모델의 성능과 효율성에 영향을 미치며, 너무 작으면 OOV 문제가, 너무 크면 계산 비용 및 과적합 문제가 발생할 수 있다.
    *   **신규 단어(OOV) 발생 시 `UNK` 토큰 처리**:
        *   기존 예시 문장: `"I love natural language processing"`
        *   새로운 문장: `"I also love deep learning"` 이 입력되었다고 가정
        *   이 문장의 토큰: `["i", "also", "love", "deep", "learning"]`
        *   여기서 "also", "deep", "learning"은 기존 어휘 집합에 없는 새로운 단어(OOV)이다.
        *   이런 OOV 단어를 처리하기 위해, 특별 토큰인 `"UNK"` (Unknown)를 어휘 집합에 추가하고, 이 `"UNK"` 토큰에 어휘 집합의 **가장 마지막 다음 번호**를 부여
        *   업데이트된 어휘 집합 (`word_to_index`): `{"i": 1, "love": 2, "natural": 3, "language": 4, "processing": 5, "UNK": 6}`
        *   어휘 집합의 크기: 6 (`UNK` 토큰 포함).
        *   새로운 문장 `"I also love deep learning"`의 정수 시퀀스
         *   OOV 단어인 "also", "deep", "learning"은 `"UNK"` 토큰의 인덱스인 6으로 매핑되어 `[1, 6, 2, 6, 6]` 이 된다.
        *   이렇게 `UNK` 토큰을 사용하면 모델이 학습하지 않은 단어에 대해서도 일관된 처리가 가능하지만, 모든 OOV 단어가 하나의 인덱스로 매핑되므로 원래 단어의 정보는 일부 손실된다.
*   **근본적 해결 시도**: 서브워드 토큰화 (BPE, WordPiece 등)는 단어를 더 작은 단위로 나눠 OOV 발생 빈도를 크게 줄임.

## 패딩 (Padding)

*   **개념**: 입력되는 텍스트 길이가 다르기 때문에 토큰화 후 서로 다른 길이의 정수 시퀀스들의 **길이를 동일하게 맞춰주는** 작업.
*   **필요성**: 딥러닝 모델의 고정된 입력 크기 요구 충족, 배치 단위 병렬 처리 효율 증대.
*   **`PAD` 토큰 사용**: 특별한 `PAD` 토큰 (주로 인덱스 0)을 사용.
*   **방법**:
    *   `post-padding` (뒷부분 채움): 시퀀스 뒤에 `PAD` 인덱스 추가. (일반적)
        *   예: `[[1,2,3,4], [5,6]]` -> `maxlen=4` 가정 시 `[[1,2,3,4], [5,6,0,0]]`, 집합의 크기는 4
    *   `pre-padding` (앞부분 채움): 시퀀스 앞에 `PAD` 인덱스 추가. (RNN 계열에서 마지막 정보 중요시할 때)
        *   예: `[[1,2,3,4], [5,6]]` -> `maxlen=4` 가정 시 `[[1,2,3,4], [0,0,5,6]]`, 집합의 크기는 4
*   **어휘 집합 크기**: `PAD` 토큰 사용 시 어휘 집합 크기에 영향
*   **주의**: 과도한 패딩은 실제 정보 비율 낮춰 학습에 부정적 영향 가능.

## 벡터화 (Vectorization)

*   **정의**: 정수 인코딩 등 숫자 표현을 바탕으로, 각 텍스트 단위(단어, 문장, 문서)를 **숫자 벡터(Numeric Vector)로 변환**하는 과정.
*   **목적**: 기계 학습 모델 처리 가능 형태 변환, 텍스트의 의미/문맥 정보 표현, 데이터 효율적 처리.

### 통계적 방법

* 신경망 미사용: 신경망을 사용하지 않는 전통적 통계 방식으로 벡터화
* 문맥 미고려 방법 (Non-neural / Context-independent)
* 각 단어를 주변 문맥과 독립적으로 고정된 벡터로 표현 (Sparse Vector). 
    
#### 단어 벡터 표현 방법: 원-핫 인코딩 (One-Hot Encoding)

*   어휘 집합 크기의 벡터에서, 해당 단어의 정수 인덱스 위치만 1이고 나머지는 모두 0인 벡터로 표현.
*   예: 어휘집 `{"apple":0, "banana":1, "cherry":2, "PAD":3}` (4은 패딩용 가정), `vocab_size=4`
*   `apple` (인덱스 0) -> `[1, 0, 0, 0]`
*   `banana` (인덱스 1) -> `[0, 1, 0, 0]`
*   `cherry` (인덱스 2) -> `[0, 0, 1, 0]`
*   `PAD` (인덱스 3) -> `[0, 0, 0, 1]`
*   장점: 단어 간 순서/크기에 의한 관계 없음 명확히 표현.
*   단점:
   * 차원의 저주: 어휘 집합 크면 벡터 차원 매우 커짐 (희소 벡터) -> 계산 비효율, 데이터 부족 문제.
      * 차원의 저주란? 고차원 공간(많은 feature 또는 여기서는 매우 큰 어휘 집합으로 인한 고차원 벡터)으로 갈수록 데이터 포인트들이 해당 공간을 매우 드문드문(희소하게, sparsely) 채우게 되는 현상
      * 희소성의 문제점:
         * **거리 계산의 무의미화**: 고차원 공간에서는 대부분의 데이터 포인트들이 서로 멀리 떨어져 있게 되어, 유클리드 거리와 같은 전통적인 거리 척도가 의미를 잃어갑니다. 즉, 가장 가까운 이웃과 가장 먼 이웃 간의 거리 차이가 거의 없어지거나, 모든 점이 샘플링된 점들의 껍질(hull)에 가깝게 위치한다. 이는 최근접 이웃(Nearest Neighbor)과 같은 거리 기반 알고리즘의 성능을 저하시킵니다.
         * **데이터 부족 심화**: 동일한 밀도로 데이터를 채우기 위해서는 차원이 증가할수록 기하급수적으로 더 많은 데이터가 필요합니다. 예를 들어, 1차원에서 10개의 구간을 커버하는데 10개의 데이터 포인트가 필요했다면, 10차원에서는 각 차원마다 10개의 구간을 커버하기 위해 \(10^{10}\)개의 데이터 포인트가 필요하게 된다. 현실적으로 이만큼의 데이터를 확보하기는 매우 어렵다.
         * **과적합(Overfitting) 가능성 증가**: 제한된 데이터로 고차원 모델을 학습시키면, 모델이 실제 데이터의 분포보다는 학습 데이터의 노이즈에 과도하게 적응하여 새로운 데이터에 대한 일반화 성능이 떨어질 수 있다.
         * **모델 학습의 어려움**: 데이터가 희소해지면, 의미 있는 패턴을 찾거나 변수 간의 관계를 모델링하는 것이 더욱 어려워지고, 모델의 복잡도에 비해 학습할 수 있는 정보가 부족해집니다.
         * **단어 간 유사도 표현 불가**: 모든 단어 벡터 직교.
*   **최근 동향**: 단점들로 인해 NLP 딥러닝에서는 단어 임베딩으로 대체되는 추세.

#### 문서 벡터 표현 방법: 빈도 기반 방법 (Frequency-based Methods)

* Document Term Matrix (DTM): 텍스트 마이닝과 자연어처리에서 문서 내 단어 빈도를 표현하는  **문서-단어 행렬의 데이터 구조**이다.
   * 벡터가 단어 집합의 크기를 가지며 대부분의 원소가 0을 가진다.
   * DTM의 기본 구조
      * **행(rows)**: 각 문서 (document), 즉, 문서가 행벡터가 된다.
      * **열(columns)**: 어휘집(vocabulary)의 각 단어 (term)
      * **셀 값**: 해당 문서에서 해당 단어의 빈도 또는 가중치
   * 수학적으로 표현하면 $M \in \mathbb{R}^{d \times v}$ 형태의 행렬이며, 여기서 $d$ 는 문서 수, $v$ 는 어휘집 크기
* DTM(행렬)의 셀 값을 채우는 방법
   * **Bag-of-Words (BoW)**:
      * Raw Count (단순 빈도): 단어를 한 가방에 넣고 흔들면 단어의 순서는 무의미해지기 때문에 단어가 등장한 빈도수를 벡터화하는 이론
      * 표현: 각 문서는 어휘 집합 크기의 벡터로, 각 차원은 해당 단어의 빈도수 (또는 존재 유무 0/1).
      * 예: 문서1: "나는 바나나 사과 바나나", 어휘집: `{"나는":0, "바나나":1, "사과":2}` -> `[1, 2, 1]`
      * 장점: 단순하고 구현 용이.
      * 단점: 어순 무시로 문맥 정보 손실, 단어 의미 모호성 해결 불가, 차원이 크고 희소(sparse)할 수 있음.
      * DTM[i,j] = count(word_j in document_i)
   * Binary (이진)
      * DTM[i,j] = 1 if word_j appears in document_i, else 0
      * {"데이터": 1, "과학": 1, "분석": 1}
   * **TF (Term Frequency)**
      * 특정 문서 내 특정 단어의 등장 빈도를 정규화하여 BoW로 표현된 벡터에 가중치를 주는 방법
      * DTM[i,j] = TF(word_j, document_i)
      * {"데이터": 2/4=0.5, "과학": 1/4=0.25, "분석": 1/4=0.25}
   * **TF-IDF (Inverse Document Frequency)**  
      * 여러 문서가 있을때 단어의 변별력을 측정하는 지표
      * 문서의 유사도, 검색 시스템에서 검색 결과의 순위 등을 구하는데 사용
      * 단어 빈도(TF)와 역문서 빈도(IDF)를 곱하여, 특정 문서 내 단어의 상대적 중요도를 가중치로 부여. 
      * 통계적 방법 (신경망 미사용)론 중 여전히 실무에서 괴장히 많이 쓰이는 벡터화 방법.           
      * 결과값이 벡터이므로 신경망의 입력값으로도 사용될 수 있다.
      * 문서를 벡터화 한다면 문서 간 유사도 구할 수 있다.
      * 문서 간 유사도 구하면 가능한 작업
         * 문서 클러스터링, 유사한 문서 찾기, 문서 분류 문제
      * IDF: 특정 문서에만 자주 나오는 단어일수록 높은 값을 가짐. 전체 문서 중 해당 단어가 등장한 문서 수의 역수
      * 예: "the" 같이 여러 문서에 자주 나오는 단어는 낮은 TF-IDF, 특정 주제 문서에만 나오는 전문용어는 높은 TF-IDF.
      * 장점: BoW보다 단어의 중요도를 더 잘 반영 (예: 불용어의 영향력 감소).
      * 단점: 어순 무시, 의미적 유사도 표현에는 여전히 한계.
      * $\text{IDF}(w_j) = \log\left(\frac{N}{1+\text{df}(w_j)}\right)$
      * 여기서:
         * $N$: 전체 문서 수
         * $\text{df}(w_j)$: 단어 $w_j$가 등장한 문서 수
         * 특정 단어가 문서2,3에서 각 각 등장할 때 특정단어가 문서2에서 10000번 등장했더라도 df의 값은 2가 된다.
         * log 스케일: df값이 매우 크거나 작을 때 IDF값의 범위를 줄이기 위해 사용 (IDF값이 기하급수적으로 커질 수 있음)
         * 불용어 등과 같이 자주 쓰이는 단어들은 비교적 자주 쓰이지 않는 단어들보다 최소 수십배 더 자주 등장한다.
         * 비교적 자주 쓰이지 않는 단어들조차 희귀 단어들과 비교하면 최소 수백 배는 더 자주 등장하는 편이다. (is, a, I, the, and, ...)
         * log를 씌우지 않으면 희귀 단어들에 엄청난 가중치가 부여될 위험이 있다.
      * 높은 IDF: 적은 문서에만 등장 → 희귀하고 특별한 단어
      * 낮은 IDF: 많은 문서에 등장 → 일반적이고 흔한 단어
* 계산 예시

```
문서1: "머신러닝 알고리즘 연구"
문서2: "딥러닝 모델 개발" 
문서3: "데이터 과학 연구"
문서4: "인공지능 연구 동향"

"연구": df=4 → IDF = log(4/4) = 0  (낮음 - 흔한 단어)
"머신러닝": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)
"딥러닝": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)
"데이터": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)
"알고리즘": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)

TF만 사용할 경우 (문서1 기준):
"연구": TF = 1/3 = 0.333
"머신러닝": TF = 1/3 = 0.333
"알고리즘": TF = 1/3 = 0.333   

IDF (문서1 기준):
"연구": IDF = log(4/4) = 0
"머신러닝": IDF = log(4/2) = 0.693
"알고리즘": IDF = log(4/2) = 0.693         

TF-IDF 사용할 경우 (문서1 기준):
"연구": TF × IDF = 0.333 × 0 = 0  (낮음 - 일반적 단어)
"머신러닝": TF × IDF = 0.333 × 0.693 = 0.231  (높음 - 문서의 특징)
"알고리즘": TF × IDF = 0.333 × 0.693 = 0.231  (높음 - 문서의 특징)
→ 문서를 특징짓는 단어들이 강조됨
```   

* 구체적 예시
   * **DTM with BoW:**
      * 문서 $D_i$ 에 대해 BoW 벡터 $\mathbf{x}_i = [x_{i1}, x_{i2}, ..., x_{iv}]$ 는 다음과 같이 정의:
      $x_{ij} = \text{count}(w_j, D_i)$

      * 여기서 $\text{count}(w_j, D_i)$는 문서 $D_i$에서 단어 $w_j$의 등장 횟수이다.
      * **문서 집합:**
         - 문서1: "데이터 과학은 흥미로운 분야다"
         - 문서2: "머신러닝은 데이터 과학의 핵심이다"  
         - 문서3: "딥러닝도 머신러닝의 한 분야다"
      * **어휘집 구성:** {"데이터", "과학은", "흥미로운", "분야다", "머신러닝은", "과학의", "핵심이다", "딥러닝도", "머신러닝의", "한"}
      * **DTM (BoW):**
    
      ```
               데이터 과학은 흥미로운 분야다 머신러닝은 과학의 핵심이다 딥러닝도 머신러닝의 한
      문서1        1     1      1      1       0      0      0      0       0    0
      문서2        1     0      0      0       1      1      1      0       0    0  
      문서3        0     0      0      1       0      0      0      1       1    1
      ```

      * **통계적 특성**
         * **희소성(Sparsity)**: 대부분의 셀이 0인 희소 행렬
         * **차원의 저주**: 어휘집 크기가 클수록 벡터 차원이 증가
         * **코사인 유사도**: 문서 간 유사도 측정에 자주 사용
      $$\text{cosine\_similarity}(D_i, D_j) = \frac{\mathbf{x}_i \cdot \mathbf{x}_j}{||\mathbf{x}_i|| \cdot ||\mathbf{x}_j||}$$

   * **DTM with TF-IDF**
      * TF-IDF는 두 구성요소의 곱으로 정의:
         * $\text{TF-IDF}(w_j, D_i) = \text{TF}(w_j, D_i) \times \text{IDF}(w_j)$

      * Term Frequency (TF)
         * $\text{TF}(w_j, D_i) = \frac{\text{count}(w_j, D_i)}{\sum_{k=1}^{|D_i|} \text{count}(w_k, D_i)}$
         * 또는 로그 정규화 (가장 보편적으로 사용):
            * $\text{TF}(w_j, D_i) = \log(1 + \text{count}(w_j, D_i))$
      * Inverse Document Frequency (IDF)
         * $\text{IDF}(w_j) = \log\left(\frac{N}{\text{df}(w_j)}\right)$
         * 여기서:
            * $N$ : 전체 문서 수
            * $\text{df}(w_j)$ : 단어 $w_j$가 등장한 문서 수
      * 예시 (위의 동일한 문서 집합 사용):
         * **1단계: TF 계산 (로그 정규화 사용)**
            * TF("데이터") = 1/4 = 0.25
            * TF("과학은") = 1/4 = 0.25  
            * TF("흥미로운") = 1/4 = 0.25
            * TF("분야다") = 1/4 = 0.25
            * TF("머신러닝은") = 1/4 = 0.25
            * TF("데이터") = 1/4 = 0.25
            * TF("과학의") = 1/4 = 0.25  
            * TF("핵심이다") = 1/4 = 0.25
            * TF("딥러닝도") = 1/4 = 0.25
            * TF("머신러닝의") = 1/4 = 0.25
            * TF("한") = 1/4 = 0.25
            * TF("분야다") = 1/4 = 0.25

               ```
                        데이터  과학은  흥미로운  분야다  머신러닝은  과학의  핵심이다  딥러닝도  머신러닝의  한
               문서1      0.25    0.25    0.25    0.25     0       0       0       0       0       0
               문서2      0.25    0       0       0      0.25     0.25    0.25     0        0      0
               문서3       0       0       0      0.25     0        0       0     0.25    0.25  0.25
               ```

         * **2단계: IDF 계산**
            * "데이터": 문서1, 문서2 → df = 2
            * "분야다": 문서1, 문서3 → df = 2  
            * "과학은": 문서1만 → df = 1
            * "흥미로운": 문서1만 → df = 1
            * "머신러닝은": 문서2만 → df = 1
            * "과학의": 문서2만 → df = 1
            * "핵심이다": 문서2만 → df = 1
            * "딥러닝도": 문서3만 → df = 1
            * "머신러닝의": 문서3만 → df = 1  
            * "한": 문서3만 → df = 1
            * IDF("데이터") = log(3/(1+2)) = log(1) = 0.000
            * IDF("분야다") = log(3/(1+2)) = log(1) = 0.000
            * IDF("과학은") = log(3/(1+1)) = log(1.5) = 0.405
            * IDF("흥미로운") = log(3/(1+1)) = log(1.5) = 0.405  
            * IDF("머신러닝은") = log(3/(1+1)) = log(1.5) = 0.405
            * IDF("과학의") = log(3/(1+1)) = log(1.5) = 0.405
            * IDF("핵심이다") = log(3/(1+1)) = log(1.5) = 0.405
            * IDF("딥러닝도") = log(3/(1+1)) = log(1.5) = 0.405
            * IDF("머신러닝의") = log(3/(1+1)) = log(1.5) = 0.405
            * IDF("한") = log(3/(1+1)) = log(1.5) = 0.405
         
         * **3단계: TF-IDF 최종 계산**
            
            ```
                     데이터   과학은   흥미로운  분야다   머신러닝은  과학의   핵심이다  딥러닝도  머신러닝의   한
            문서1      0     0.101    0.101   0.101      0        0        0       0        0      0
            문서2      0       0        0       0       0.081    0.081    0.081    0        0      0
            문서3      0       0        0       0        0        0        0     0.081    0.081  0.081
            ```

      * TF-IDF의 통계적 해석
         * **정보 이론적 관점**: IDF는 단어의 정보량(information content)을 측정
            * $\text{Information}(w_j) = -\log P(w_j) \approx -\log\left(\frac{\text{df}(w_j)}{N}\right) = \text{IDF}(w_j)$
         * **가중치 효과**: 
            * 높은 TF: 해당 문서에서 **중요한 단어**
            * 높은 IDF: 전체 문서 집합에서 **희귀한 단어**
            * 높은 TF-IDF: 특정 문서의 **특징을 잘 나타내는 중요한 단어**
         * **정규화 효과**: 문서 길이에 따른 편향을 줄임

   * 실용적 고려사항
      * **불용어 처리**: "은", "는", "이" 등은 보통 전처리 단계에서 제거
      * **최소 문서 빈도**: 너무 희귀한 단어들을 필터링하여 차원 축소
      * **최대 문서 빈도**: 너무 일반적인 단어들도 제외 가능

## 결론

본 문서에서는 자연어 처리(NLP)의 핵심 전처리 단계인 텍스트 인코딩과 벡터화 기법들을 살펴보았다. 주요 내용을 요약하면 다음과 같다.

*   **텍스트 인코딩의 중요성**: 기계가 텍스트를 이해하기 위한 첫걸음으로, 정수 인코딩과 같은 방법을 통해 토큰을 숫자 표현으로 변환합니다. 이때 어휘 집합에 없는 단어(OOV) 처리가 중요하며, `UNK` 토큰 사용이 일반적인 해결책이다.
*   **일관된 입력 처리를 위한 패딩**: 모델의 고정된 입력 크기 요구를 충족시키기 위해, 다양한 길이의 시퀀스를 `PAD` 토큰을 사용하여 동일한 길이로 맞추는 패딩 작업이 필수적이다.
*   **다양한 벡터화 기법**:
    *   **통계 기반 방법**: 원-핫 인코딩은 간단하지만 차원의 저주 문제가 있으며, 단어 빈도 기반의 DTM, BoW, TF-IDF 등은 문서 수준의 벡터 표현에 효과적이다. 특히 TF-IDF는 단어의 중요도를 반영하여 널리 사용된다.
    *   **벡터 표현의 의미**: 이러한 기법들은 단어를 벡터로, 문서를 벡터 또는 행렬로 변환하여 기계 학습 모델이 처리할 수 있도록 한다.
*   **방법 선택의 기준**: 최적의 인코딩 및 벡터화 전략은 당면한 문제의 특성, 데이터의 양과 질, 그리고 사용하려는 모델 등을 종합적으로 고려하여 선택해야 한다.
