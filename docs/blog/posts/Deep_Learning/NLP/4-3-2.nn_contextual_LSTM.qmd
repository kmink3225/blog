---
title: "텍스트 벡터화: LSTM의 이해"
subtitle: "RNN의 장기 의존성 문제 해결을 위한 Long Short-Term Memory"
description: |
  RNN의 한계인 장기 의존성 문제를 해결하기 위해 등장한 LSTM(Long Short-Term Memory)의 기본 원리와 구조를 소개한다. LSTM의 핵심 구성 요소인 셀 상태와 세 가지 게이트(입력, 삭제, 출력)가 어떻게 정보를 효과적으로 제어하고 장기 기억을 가능하게 하는지 살펴본다. 양방향 LSTM의 개념도 간략히 다룬다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-11
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

# 요약

이 문서는 기존 RNN(Recurrent Neural Network)의 주요 한계점인 장기 의존성 문제(Long-Term Dependency Problem)를 해결하기 위해 제안된 LSTM(Long Short-Term Memory) 네트워크의 기본적인 작동 원리와 구조를 설명한다.

*   **기존 RNN의 한계와 LSTM의 등장 배경**:
    *   바닐라 RNN은 시퀀스가 길어질수록 초기 정보가 소실되어 먼 과거의 정보를 현재까지 전달하기 어렵다는 장기 의존성 문제를 가진다. 이는 기울기 소실/폭주 문제와도 관련된다.
    *   LSTM은 이러한 문제를 해결하기 위해 셀 내부에 '게이트(gate)'라는 정교한 정보 제어 메커니즘을 도입했다.
*   **LSTM의 핵심 구성 요소 및 작동 원리**:
    *   **셀 상태 (Cell State, $C_t$)**: LSTM의 핵심으로, 컨베이어 벨트처럼 정보를 비교적 오래 기억하고 전달하는 역할을 한다. 게이트들을 통해 정보가 추가되거나 제거된다.
    *   **게이트 (Gates)**: 세 가지 주요 게이트가 셀 상태와 은닉 상태(hidden state, $h_t$)를 제어한다.
        *   **삭제 게이트 (Forget Gate, $f_t$)**: 이전 셀 상태($C_{t-1}$)에서 어떤 정보를 버릴지 결정한다.
        *   **입력 게이트 (Input Gate, $i_t$)**: 현재 입력($x_t$)과 이전 은닉 상태($h_{t-1}$)를 바탕으로 어떤 새로운 정보를 셀 상태에 저장할지 결정한다. 후보 값($g_t$)과 함께 사용된다.
        *   **출력 게이트 (Output Gate, $o_t$)**: 현재 셀 상태를 바탕으로 어떤 정보를 현재 시점의 은닉 상태($h_t$)로 출력할지 결정한다.
    *   이러한 게이트들은 시그모이드(sigmoid) 함수를 통해 0과 1 사이의 값을 출력하여 정보의 흐름을 조절하고, 요소별 곱셈(element-wise product)을 통해 정보를 선택적으로 통과시키거나 차단한다.
*   **양방향 LSTM (Bidirectional LSTM, BiLSTM)**:
    *   단방향 LSTM의 정보 흐름(과거→미래) 한계를 극복하기 위해, 순방향 LSTM과 역방향 LSTM을 함께 사용하여 과거와 미래의 문맥을 모두 고려한다. 최종 출력은 두 LSTM의 은닉 상태를 결합하여 생성된다.
*   **의의**: LSTM은 장기 의존성 문제를 효과적으로 완화하여 더 긴 시퀀스에서도 의미 있는 정보를 학습할 수 있게 만들었다. 이는 자연어 처리, 음성 인식, 기계 번역 등 다양한 분야에서 RNN 계열 모델의 성능을 크게 향상시키는 데 기여했으며, ELMo와 같은 초기 문맥 기반 임베딩 모델의 중요 구성 요소로 활용되었다.

이 문서를 통해 독자는 LSTM이 어떻게 게이트 메커니즘을 통해 정보의 흐름을 제어하고 장기 기억을 가능하게 하는지에 대한 기본적인 이해를 얻을 수 있다.

# 텍스트 인코딩 및 벡터화

```
텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    └── RNN 계열
        ├── LSTM
        ├── GRU
        └── ELMo
```

## 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩

### 기존 RNN의 한계: 장기 의존성 문제

* 장기 의존성 문제 (Long-Term Dependency Problem): 기존의 RNN은 시점이 깅러지면 앞에 있던 정보가 소실되는 장기 의존성 문제를 갖고 있다
* 즉, 너무 오래 전의 정보를 기억하지 못함. 이것은 RNN의 고질적인 문제입니다.
* 기울기 소실 문제 (Gradient Vanishing Problem): 너무 오래 전의 정보를 기억하지 못함
* Vanilla RNN: 기존의 RNN, 아이스크림 맛중에서 vanilla가 가장 기본적인 맛이기 때문에 vanilla RNN이라고 부른다.


### LSTM (Long Short-Term Memory. 1997)
   
* LSTM은 기존의 RNN의 장기 의존성 문제를 해결하기 위해 은닉층에 추가적인 메커니즘을 도입한다.
* 추가 메커니즘은 기억력을 증가시키는 것이다.
* industry에서 RNN을 썼다고 하면 대부분 LSTM이나 GRU를 쓰는 것을 의미한다.
* 기존의 RNN은 이전 시점의 hidden state의 정보를 현재 시점의 hidden state에 전달하면서 현재 시점의 출력값도 만들어내는 구조였다.
* 기존의 RNN은 cell state가 없었다.
* LSTM은 이전의 hidden state와 cell state 둘 모두를 다음 시점의 hidden state와 cell state의 정보를 전달한다.
* cell state에 gate라는 구조를 통해서 정보를 더하거나 빼는 등의 통제를 한다.

#### input gate

* 현재 정보를 기억하기 위한 gate이다.   
* cell state: $x_t$ 와 $h_{t-1}$ 을 받아서 현재 시점의 선택된 기억할 값을 정한다.
   * sigmoid 함수를 통해 [0,1]을 반환
   * tanh 함수를 통해 [-1,1]을 반환
   * 이 두 종류의 값을 갖고 cell state에서 이번에 선택된 기억할 값을 정한다. 즉,
   * $i_t = \sigma(W_{x_{i}}x_t + W_{h_i}h_{t-1} + b_i)$ 
   * $g_t = \tanh(W_{x_g} x_t + W_{h_g} h_{t-1} + b_g)$
   * 이때, $i_t$ 는 0~1 사이의 값을 가지며, $g_t$ 는 -1~1 사이의 값을 가진다.
   * 이 두 값을 곱해서 현재 시점의 cell state를 결정한다.

#### forget gate

* 이전 정보를 잊기 위한 gate이다.   
* 기억을 삭제하기 위한 gate이다.
* sigmoid 함수를 지나 [0,1]을 반환된 값으로 0에 가까울수록 정보가 많이 삭제된 것이며, 1에 가까울수록 정보를 온전히 기억한 셈이다. 즉, 
* $f_t=\sigma(W_{x_f}x_t+W_{h_f}h_{t-1}+b_f)$
* 이렇게, 일부 기억을 소실하고 입력 게이트의 $i_t$ 와 $g_t$ 의 정보를 조합하여 elementwise product를 수행하여 더해서 이번에 기억할 값을 결정
   * elementwise product: 각 요소별 곱셈을 의미한다.
   * 즉, 현재 시점의 cell state는 이전의 cell state와 현재 시점의 정보를 조합하여 결정된다.
   * $C_t = f_t \odot C_{t-1} + i_t \odot g_t$
   * 이때, $\odot$ 는 요소별 곱셈을 의미한다.
   * $i_t \odot g_t$ = input gate의 값, 현재 시점의 기억할 값 
   * $C_{t-1}$ = 이전 시점의 cell state
   * $f_t \odot C_{t-1}$ = forget gate의 값으로 과거의 기억을 삭제한 값
   * 즉, 현재 시점의 cell state는 이전의 cell state와 현재 시점의 정보를 조합하여 결정된다.

#### input gate와 forget gate의 영향력

* $f_t$ 가 0이 되면 이전 시점의 cell state가 완전히 삭제되고 오직 input gate만이 현재 시점의 cell state값을 결정한다.
   * 이를 forget gate가 닫히고 input gate만 열린 상태라고 한다.
* 반면, $i_t$ 가 0이 되면 이전 시점의 cell state를 완전히 삭제하고 오직 forget gate만이 현재 시점의 cell state값을 결정한다.
   * 이를 input gate가 닫히고 forget gate만 열린 상태라고 한다.
* 따라서, input gate는 현재 시점의 입력을 얼마나 반영할지 결정
* 반면, forget gate는 이전 시점의 기억을 얼마나 유지할지 결정

#### output gate

* 현재 시점의 출력을 결정하기 위한 gate이다.
* Hidden state를 연산하는 일에 쓰이며 Cell state와 비교하여 단기 상태라고도 부른다.
* 이전 시점의 cell state는 현재 시점의 cell 내부에 기억력을 돕기위한 역할만 하고 현재 시점의 출력엔 영향을 주지 않는다.
* 현재 시점의 출력은 이전 시점의 hidden state에 영향을 받은 현재 시점의 hidden state에 의해서만 결정된다.
* sigmoid 함수를 통해 [0,1]을 반환
* 이 값을 현재 시점의 cell state에 곱해서 현재 시점의 출력을 결정한다.즉,
* $o_t = \sigma(W_{x_o}x_t + W_{h_o}h_{t-1} + b_o)$
* $h_t = o_t \odot \tanh(C_t)$

### 양방향 LSTM (Bidirectional LSTM. 2005)
   
* 기본 LSTM의 한계: 단방향 처리로 인한 정보 손실
   * 기본 LSTM은 과거에서 미래로만 정보를 처리한다
   * 현재 시점에서 미래의 정보를 활용할 수 없어 맥락 이해가 제한적이다
   * 예: "그는 은행에 갔다"에서 "은행"이 금융기관인지 강둑인지 앞뒤 문맥을 모두 봐야 판단 가능
* 양방향 LSTM은 순방향과 역방향 두 개의 LSTM을 동시에 사용한다
* Forward LSTM과 Backward LSTM이 독립적으로 작동하여 전체 시퀀스의 맥락을 모두 활용한다
* 자연어 처리, 음성 인식 등에서 단방향 LSTM보다 우수한 성능을 보인다
* Forward LSTM (순방향)
   * 일반적인 LSTM과 동일하게 시점 1부터 시점 T까지 순차적으로 처리한다
   * 각 시점에서 과거 정보를 현재로 전달한다
   * Forward hidden state: $\overrightarrow{h_t}$, Forward cell state: $\overrightarrow{C_t}$
   * Gate 연산:
      * $\overrightarrow{i_t} = \sigma(W_{x_i}x_t + W_{h_i}\overrightarrow{h_{t-1}} + b_i)$ (input gate)
      * $\overrightarrow{f_t} = \sigma(W_{x_f}x_t + W_{h_f}\overrightarrow{h_{t-1}} + b_f)$ (forget gate)
      * $\overrightarrow{g_t} = \tanh(W_{x_g}x_t + W_{h_g}\overrightarrow{h_{t-1}} + b_g)$ (candidate values)
      * $\overrightarrow{C_t} = \overrightarrow{f_t} \odot \overrightarrow{C_{t-1}} + \overrightarrow{i_t} \odot \overrightarrow{g_t}$ (cell state)
      * $\overrightarrow{o_t} = \sigma(W_{x_o}x_t + W_{h_o}\overrightarrow{h_{t-1}} + b_o)$ (output gate)
      * $\overrightarrow{h_t} = \overrightarrow{o_t} \odot \tanh(\overrightarrow{C_t})$ (hidden state)
* Backward LSTM (역방향)
   * 시점 T부터 시점 1까지 역순으로 처리한다
   * 각 시점에서 미래 정보를 현재로 전달한다 ($t+1$ 시점에서 $t$ 시점으로)
   * Backward hidden state: $\overleftarrow{h_t}$, Backward cell state: $\overleftarrow{C_t}$
   * Gate 연산 (별도의 가중치 매개변수 $W'$ 사용):
      * $\overleftarrow{i_t} = \sigma(W'_{x_i}x_t + W'_{h_i}\overleftarrow{h_{t+1}} + b'_i)$ (input gate)
      * $\overleftarrow{f_t} = \sigma(W'_{x_f}x_t + W'_{h_f}\overleftarrow{h_{t+1}} + b'_f)$ (forget gate)
      * $\overleftarrow{g_t} = \tanh(W'_{x_g}x_t + W'_{h_g}\overleftarrow{h_{t+1}} + b'_g)$ (candidate values)
      * $\overleftarrow{C_t} = \overleftarrow{f_t} \odot \overleftarrow{C_{t+1}} + \overleftarrow{i_t} \odot \overleftarrow{g_t}$ (cell state)
      * $\overleftarrow{o_t} = \sigma(W'_{x_o}x_t + W'_{h_o}\overleftarrow{h_{t+1}} + b'_o)$ (output gate)
      * $\overleftarrow{h_t} = \overleftarrow{o_t} \odot \tanh(\overleftarrow{C_t})$ (hidden state)
* 최종 출력 결합
   * 각 시점에서 Forward와 Backward의 hidden state를 결합하여 최종 출력을 생성한다
   * 가장 일반적인 방법은 연결(concatenation)이다: $h_t = [\overrightarrow{h_t}; \overleftarrow{h_t}]$
   * 최종 hidden state의 차원은 단방향 LSTM의 2배가 된다
   * 다른 결합 방식들:
      * 합계: $h_t = \overrightarrow{h_t} + \overleftarrow{h_t}$
      * 평균: $h_t = \frac{\overrightarrow{h_t} + \overleftarrow{h_t}}{2}$
      * 가중합: $h_t = \alpha \overrightarrow{h_t} + (1-\alpha) \overleftarrow{h_t}$
* 양방향 LSTM의 장점과 한계
   * 장점: 전체 시퀀스의 맥락 정보를 모두 활용하여 더 정확한 표현 학습이 가능하다
   * 단점: 전체 시퀀스가 필요하므로 실시간 처리가 불가능하다
   * 계산 복잡도가 단방향 LSTM의 약 2배로 증가한다
   * 파라미터 수와 메모리 사용량이 증가한다

#### Deep Bidirectional RNN (2013~2015)

* Deep RNN
   * RNN의 은닉층을 여러 개 쌓은 모델
   * 각 은닉층은 이전 은닉층의 출력을 입력으로 받아 현재 은닉층의 출력을 생성
   * 이렇게 하여 더 복잡한 패턴을 학습할 수 있다.
* Bidirectional RNN
   * 기본적인 구조는 RNN과 유사하다.
* Deep Bidirectional RNN
   * 기본적인 구조는 RNN과 유사하다.
   * 하지만, 더 많은 레이어를 가지고 있어 더 많은 정보를 저장할 수 있다.
* 또한, 더 많은 연산을 필요로 하여 더 많은 시간이 걸린다.
* 하지만, 더 나은 성능을 보인다.
* 일반적으로 GRU는 LSTM보다 더 빠르게 학습되지만, LSTM은 더 나은 성능을 보인다.
* 따라서, 일반적으로 LSTM이 더 많이 사용된다.



## 결론

본 문서에서는 RNN의 장기 의존성 문제를 해결하기 위해 설계된 LSTM(Long Short-Term Memory) 네트워크의 핵심적인 구조와 작동 방식을 살펴보았다. LSTM은 셀 상태와 세 가지 주요 게이트(입력, 삭제, 출력)를 통해 정보의 흐름을 정교하게 제어함으로써 장기 기억을 효과적으로 수행한다.

*   **LSTM의 핵심 원리 요약**:
    *   LSTM의 중심에는 장기적인 정보를 저장하는 '셀 상태(Cell State)'가 있으며, 이 셀 상태는 게이트들에 의해 선택적으로 정보가 추가되거나 제거되면서 업데이트된다.
    *   **삭제 게이트**는 과거 정보 중 불필요한 것을 잊도록 하고, **입력 게이트**는 현재 정보 중 중요한 것을 셀 상태에 추가하며, **출력 게이트**는 현재 셀 상태를 바탕으로 다음 은닉 상태(단기 기억) 및 출력을 결정한다.
    *   이러한 게이트 메커니즘 덕분에 LSTM은 기울기 소실 문제를 완화하고, 시퀀스 내의 멀리 떨어진 정보 간의 의존성을 학습할 수 있다.
*   **양방향 LSTM의 활용**:
    *   단일 방향 LSTM이 과거의 문맥만을 고려하는 한계를 보완하기 위해, 순방향과 역방향 LSTM을 결합한 양방향 LSTM(BiLSTM)이 널리 사용된다. BiLSTM은 특정 시점의 양쪽 문맥 정보를 모두 활용하여 보다 풍부한 표현을 학습할 수 있게 한다.
*   **문맥 이해와 NLP에서의 중요성**:
    *   LSTM은 복잡한 시퀀스 데이터를 모델링하는 데 강력한 성능을 보여주었으며, 특히 자연어 처리 분야에서 문장이나 문서의 문맥적 의미를 파악하는 데 중요한 역할을 했다.
    *   ELMo와 같은 초기 문맥 기반 워드 임베딩 모델의 기반 구조로 사용되었으며, 이후 등장한 트랜스포머 아키텍처 이전까지 다양한 NLP 태스크에서 핵심적인 구성 요소로 활용되었다.

결론적으로, LSTM은 RNN의 한계를 극복하고 시퀀스 데이터, 특히 장기 의존성을 가진 데이터 처리에 있어 중요한 진보를 이룬 모델이다. 정교한 내부 메커니즘을 통해 정보의 선택적 기억과 망각을 가능하게 함으로써, 복잡한 패턴 학습 능력을 크게 향상시켰다.
