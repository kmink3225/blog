---
title: "Hugging Face: PLM ìƒíƒœê³„ì˜ ì¤‘ì‹¬"
subtitle: "ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì˜ í—ˆë¸Œ"
description: |
  Hugging FaceëŠ” í˜„ì¬ NLP ë¶„ì•¼ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ì í”Œë«í¼ì´ë‹¤. ìˆ˜ë§Œ ê°œì˜ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ì œê³µí•˜ë©°, ëª‡ ì¤„ì˜ ì½”ë“œë§Œìœ¼ë¡œ ìµœì‹  PLMì„ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤. í† í¬ë‚˜ì´ì €ë¶€í„° íŒŒì¸íŠœë‹, ë°°í¬ê¹Œì§€ ì „ì²´ ML ì›Œí¬í”Œë¡œìš°ë¥¼ ì§€ì›í•˜ëŠ” Hugging Faceì˜ í•µì‹¬ ê¸°ëŠ¥ë“¤ê³¼ ì‹¤ë¬´ í™œìš© ì „ëµì„ ìƒì„¸íˆ ë¶„ì„í•œë‹¤.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-27
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

# ìš”ì•½

Hugging FaceëŠ” í˜„ì¬ **NLP ë¶„ì•¼ì˜ ì‚¬ì‹¤ìƒ í‘œì¤€**ì´ ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ì í”Œë«í¼ì´ë‹¤. PyTorchì™€ TensorFlow ëª¨ë‘ë¥¼ ì§€ì›í•˜ë©°, ìˆ˜ë§Œ ê°œì˜ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ì œê³µí•˜ëŠ” ê±°ëŒ€í•œ ìƒíƒœê³„ë¥¼ êµ¬ì¶•í–ˆë‹¤.

## í•µì‹¬ ê°€ì¹˜ ì œì•ˆ

* **ì ‘ê·¼ì„± í˜ëª…**: ëª‡ ì¤„ì˜ ì½”ë“œë¡œ ìµœì‹  PLM ì‚¬ìš© ê°€ëŠ¥
* **í‘œì¤€í™”**: ëª¨ë“  ëª¨ë¸ì´ ë™ì¼í•œ APIë¡œ í†µì¼
* **ì™„ì „í•œ ì›Œí¬í”Œë¡œìš°**: ì „ì²˜ë¦¬ë¶€í„° ë°°í¬ê¹Œì§€ ì›ìŠ¤í†± ì§€ì›
* **ê±°ëŒ€í•œ ì»¤ë®¤ë‹ˆí‹°**: ì „ ì„¸ê³„ ê°œë°œìë“¤ì´ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ê³µìœ 

## ì£¼ìš” êµ¬ì„± ìš”ì†Œ

* **Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬**: í•µì‹¬ ëª¨ë¸ êµ¬í˜„ì²´
* **Model Hub**: 10ë§Œ+ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ì €ì¥ì†Œ
* **Datasets**: í‘œì¤€í™”ëœ ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
* **Tokenizers**: ê³ ì„±ëŠ¥ í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
* **Accelerate**: ë¶„ì‚° í•™ìŠµ ë° ìµœì í™” ë„êµ¬
* **Gradio**: ë¹ ë¥¸ ë°ëª¨ ë° í”„ë¡œí† íƒ€ì… êµ¬ì¶•
* **Spaces**: ëª¨ë¸ ë°°í¬ ë° ê³µìœ  í”Œë«í¼

## ì‹¤ë¬´ì—ì„œì˜ ê°•ë ¥í•¨

**Before Hugging Face** (2019ë…„ ì´ì „):
```python
# ë³µì¡í•œ ëª¨ë¸ êµ¬í˜„ê³¼ ì „ì²˜ë¦¬ í•„ìš”
class CustomBERTModel(nn.Module):
    def __init__(self):
        # ìˆ˜ë°± ì¤„ì˜ êµ¬í˜„ ì½”ë“œ...
        pass
    
# í† í¬ë‚˜ì´ì € ì§ì ‘ êµ¬í˜„
# ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì½”ë“œ ì‘ì„±
# ê° ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¸ API
```

**After Hugging Face**:
```python
# 3ì¤„ë¡œ ë
from transformers import pipeline
classifier = pipeline("sentiment-analysis")
result = classifier("ì´ ì˜í™” ì •ë§ ì¬ë°Œì—ˆì–´!")
```

ì´ëŸ¬í•œ **ì½”ë“œ ë‹¨ìˆœí™”**ëŠ” NLP ê¸°ìˆ ì˜ ë¯¼ì£¼í™”ë¥¼ ì´ëŒì—ˆìœ¼ë©°, ì—°êµ¬ìë¿ë§Œ ì•„ë‹ˆë¼ ì¼ë°˜ ê°œë°œìë“¤ë„ ì‰½ê²Œ ìµœì‹  AI ê¸°ìˆ ì„ í™œìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì—ˆë‹¤.

# Hugging Face ìƒíƒœê³„ ì „ì²´ êµ¬ì¡°

## í”Œë«í¼ ì•„í‚¤í…ì²˜

```mermaid
graph TD
    A[Hugging Face Hub] --> B[Models]
    A --> C[Datasets] 
    A --> D[Spaces]
    
    B --> E[Transformers Library]
    C --> F[Datasets Library]
    D --> G[Gradio/Streamlit]
    
    E --> H[PyTorch]
    E --> I[TensorFlow]
    E --> J[JAX]
    
    K[ì‚¬ìš©ì] --> L[Pipeline API]
    K --> M[AutoModel API]
    K --> N[Trainer API]
    
    L --> E
    M --> E
    N --> E
```

## í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤

### 1. Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬
**ì—­í• **: ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì˜ í†µí•© êµ¬í˜„ì²´
```python
# ì§€ì›í•˜ëŠ” ì£¼ìš” ëª¨ë¸ë“¤
models = [
    "BERT", "GPT-2", "T5", "BART", "RoBERTa",
    "ALBERT", "DistilBERT", "ELECTRA", "DeBERTa",
    "GPT-Neo", "GPT-J", "OPT", "BLOOM", "LLaMA"
]

# ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ë“¤
tasks = [
    "text-classification", "token-classification",
    "question-answering", "text-generation",
    "summarization", "translation", "fill-mask"
]
```

### 2. Model Hubì˜ ê·œëª¨
```python
# 2024ë…„ ê¸°ì¤€ í†µê³„
hub_stats = {
    'total_models': 100000+,
    'organizations': 10000+,
    'downloads_per_month': '10ì–µ+',
    'supported_languages': 100+,
    'korean_models': 1000+
}
```

### 3. ì§€ì›í•˜ëŠ” í”„ë ˆì„ì›Œí¬
```python
# ë©€í‹° í”„ë ˆì„ì›Œí¬ ì§€ì›
frameworks = {
    'PyTorch': 'ê¸°ë³¸ ì§€ì›, ê°€ì¥ ë§ì€ ëª¨ë¸',
    'TensorFlow': 'TF 2.x ì™„ì „ ì§€ì›',
    'JAX/Flax': 'Google ì—°êµ¬íŒ€ í˜‘ì—…',
    'ONNX': 'ì¶”ë¡  ìµœì í™” ì§€ì›'
}
```

# í† í°í™”ì™€ ì „ì²˜ë¦¬

## í† í¬ë‚˜ì´ì €ì˜ ì¤‘ìš”ì„±

**í•µì‹¬ ì›ì¹™**: ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ëŠ” í•­ìƒ ìŒìœ¼ë¡œ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

```python
# ì˜¬ë°”ë¥¸ ì‚¬ìš©ë²• âœ…
from transformers import AutoTokenizer, AutoModel

model_name = "klue/bert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
```

```python
# ì˜ëª»ëœ ì‚¬ìš©ë²• âŒ - ë‹¤ë¥¸ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")  # ì˜ì–´
model = AutoModel.from_pretrained("klue/bert-base")  # í•œêµ­ì–´
# â†’ ì™„ì „íˆ ë‹¤ë¥¸ vocabularyë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜
```

## í† í°í™” ê³¼ì • ìƒì„¸ ë¶„ì„

```python
from transformers import BertTokenizer

# í•œêµ­ì–´ BERT í† í¬ë‚˜ì´ì €
tokenizer = BertTokenizer.from_pretrained("klue/bert-base")

# ë‹¨ê³„ë³„ í† í°í™” ê³¼ì •
text = "ì•ˆë…•í•˜ì„¸ìš”. ìì—°ì–´ ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤."

# 1ë‹¨ê³„: í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• 
tokens = tokenizer.tokenize(text)
print(f"í† í°í™”: {tokens}")
# ['ì•ˆë…•', '##í•˜', '##ì„¸ìš”', '.', 'ìì—°ì–´', 'ì²˜ë¦¬ë¥¼', 'ê³µë¶€', '##í•©ë‹ˆë‹¤', '.']

# 2ë‹¨ê³„: í† í°ì„ IDë¡œ ë³€í™˜
token_ids = tokenizer.convert_tokens_to_ids(tokens)
print(f"í† í° ID: {token_ids}")
# [2374, 8910, 4567, 119, 15234, 9876, 3456, 7890, 119]

# 3ë‹¨ê³„: íŠ¹ìˆ˜ í† í° ì¶”ê°€ ë° íŒ¨ë”©
encoded = tokenizer(text, padding=True, truncation=True, return_tensors="pt")
print(f"ìµœì¢… ì¸ì½”ë”©: {encoded}")
# {'input_ids': tensor([[101, 2374, 8910, ..., 102]]), 
#  'attention_mask': tensor([[1, 1, 1, ..., 1]])}
```

## ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì € ì¢…ë¥˜

```python
# 1. WordPiece (BERT ê³„ì—´)
wordpiece_tokenizer = BertTokenizer.from_pretrained("klue/bert-base")
result1 = wordpiece_tokenizer.tokenize("ìì—°ì–´ì²˜ë¦¬")
print(f"WordPiece: {result1}")  # ['ìì—°ì–´', '##ì²˜ë¦¬']

# 2. BPE (GPT ê³„ì—´)
bpe_tokenizer = GPT2Tokenizer.from_pretrained("skt/kogpt2-base-v2")
result2 = bpe_tokenizer.tokenize("ìì—°ì–´ì²˜ë¦¬")
print(f"BPE: {result2}")  # ['ìì—°ì–´', 'ì²˜ë¦¬']

# 3. SentencePiece (T5, ALBERT ê³„ì—´)
sp_tokenizer = T5Tokenizer.from_pretrained("KETI-AIR/ke-t5-base")
result3 = sp_tokenizer.tokenize("ìì—°ì–´ì²˜ë¦¬")
print(f"SentencePiece: {result3}")  # ['â–ìì—°ì–´', 'ì²˜ë¦¬']
```

## í† í¬ë‚˜ì´ì € ì„±ëŠ¥ ìµœì í™”

```python
# ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ
texts = ["ë¬¸ì¥ 1", "ë¬¸ì¥ 2", "ë¬¸ì¥ 3"] * 1000

# ëŠë¦° ë°©ë²• âŒ
slow_results = []
for text in texts:
    result = tokenizer(text)
    slow_results.append(result)

# ë¹ ë¥¸ ë°©ë²• âœ… (10-100ë°° ë¹ ë¦„)
fast_results = tokenizer(texts, padding=True, truncation=True)

# Fast Tokenizer ì‚¬ìš© (Rust êµ¬í˜„)
fast_tokenizer = AutoTokenizer.from_pretrained(
    "klue/bert-base", 
    use_fast=True  # Rust ê¸°ë°˜ ê³ ì† í† í¬ë‚˜ì´ì €
)
```

# ëª¨ë¸ ë¡œë”©ê³¼ í™œìš©

## AutoModel ê³„ì—´ì˜ ê°•ë ¥í•¨

```python
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification

# ìë™ìœ¼ë¡œ ì ì ˆí•œ ëª¨ë¸ í´ë˜ìŠ¤ ì„ íƒ
model_name = "klue/bert-base"

# ë²”ìš© ì¸ì½”ë”
encoder = AutoModel.from_pretrained(model_name)

# ë¶„ë¥˜ìš© ëª¨ë¸ (ë¶„ë¥˜ í—¤ë“œ ìë™ ì¶”ê°€)
classifier = AutoModelForSequenceClassification.from_pretrained(
    model_name, 
    num_labels=3
)

# ì§ˆì˜ì‘ë‹µìš© ëª¨ë¸
qa_model = AutoModelForQuestionAnswering.from_pretrained(model_name)
```

## ëª¨ë¸ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
from transformers import BertConfig, BertForSequenceClassification

# ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸° ë° ìˆ˜ì •
config = BertConfig.from_pretrained("klue/bert-base")
config.num_labels = 5  # ë¶„ë¥˜ í´ë˜ìŠ¤ ìˆ˜ ë³€ê²½
config.dropout = 0.2   # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ì¡°ì •
config.attention_probs_dropout_prob = 0.1

# ìˆ˜ì •ëœ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ìƒì„±
model = BertForSequenceClassification.from_pretrained(
    "klue/bert-base",
    config=config
)

# ëª¨ë¸ êµ¬ì¡° í™•ì¸
print(model)
```

## ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëª¨ë¸ ë¡œë”©

```python
# 1. ì ˆë°˜ ì •ë°€ë„ ì‚¬ìš© (ë©”ëª¨ë¦¬ 50% ì ˆì•½)
model = AutoModel.from_pretrained(
    "klue/bert-base",
    torch_dtype=torch.float16
)

# 2. CPU ì˜¤í”„ë¡œë”© (í° ëª¨ë¸ìš©)
model = AutoModel.from_pretrained(
    "microsoft/DialoGPT-large",
    device_map="auto",  # ìë™ìœ¼ë¡œ GPU/CPU ë°°ì¹˜
    low_cpu_mem_usage=True
)

# 3. 8ë¹„íŠ¸ ì–‘ìí™” (ë©”ëª¨ë¦¬ 75% ì ˆì•½)
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True)
model = AutoModel.from_pretrained(
    "facebook/opt-6.7b",
    quantization_config=quantization_config
)
```

# Pipeline API: ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ NLP

## ê¸°ë³¸ Pipeline ì‚¬ìš©ë²•

```python
from transformers import pipeline

# 1. ê°ì • ë¶„ì„
sentiment_analyzer = pipeline(
    "sentiment-analysis",
    model="klue/bert-base-en-ko-cased",
    return_all_scores=True
)

result = sentiment_analyzer("ì´ ì˜í™” ì •ë§ ì¬ë°Œì—ˆì–´!")
print(result)
# [{'label': 'POSITIVE', 'score': 0.9998}]

# 2. í…ìŠ¤íŠ¸ ìƒì„±
generator = pipeline(
    "text-generation",
    model="skt/kogpt2-base-v2",
    max_length=100,
    do_sample=True,
    temperature=0.8
)

result = generator("ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ”")
print(result[0]['generated_text'])

# 3. ì§ˆì˜ì‘ë‹µ
qa_pipeline = pipeline(
    "question-answering",
    model="klue/bert-base"
)

context = "íŒŒì´ì¬ì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ê°œë°œí•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë‹¤."
question = "íŒŒì´ì¬ì„ ê°œë°œí•œ ì‚¬ëŒì€ ëˆ„êµ¬ì¸ê°€?"

result = qa_pipeline(question=question, context=context)
print(f"ë‹µ: {result['answer']}, ì‹ ë¢°ë„: {result['score']:.4f}")
```

## ê³ ê¸‰ Pipeline ì„¤ì •

```python
# ë°°ì¹˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
classifier = pipeline(
    "text-classification",
    model="klue/bert-base",
    device=0,  # GPU ì‚¬ìš©
    batch_size=16,  # ë°°ì¹˜ í¬ê¸°
    max_length=512,
    truncation=True
)

# ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ ì²˜ë¦¬
texts = ["í…ìŠ¤íŠ¸ 1", "í…ìŠ¤íŠ¸ 2"] * 1000
results = classifier(texts)

# ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_function(examples):
    # ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ë¡œì§
    return tokenizer(examples, truncation=True, padding=True)

# íŒŒì´í”„ë¼ì¸ì— ì»¤ìŠ¤í…€ í•¨ìˆ˜ ì ìš©
custom_pipeline = pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    preprocessing=preprocess_function
)
```

## ì‹¤ë¬´ìš© Pipeline ìµœì í™”

```python
class OptimizedPipeline:
    def __init__(self, model_name, task, device="cuda"):
        self.pipeline = pipeline(
            task,
            model=model_name,
            device=0 if device == "cuda" else -1,
            batch_size=32,
            return_all_scores=False
        )
        
    def batch_predict(self, texts, batch_size=32):
        """ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ íš¨ìœ¨ì  ì²˜ë¦¬"""
        results = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            batch_results = self.pipeline(batch)
            results.extend(batch_results)
        return results
    
    def predict_with_confidence(self, text, threshold=0.8):
        """ì‹ ë¢°ë„ ê¸°ë°˜ ì˜ˆì¸¡"""
        result = self.pipeline(text, return_all_scores=True)
        max_score = max(result[0], key=lambda x: x['score'])
        
        if max_score['score'] >= threshold:
            return max_score
        else:
            return {"label": "UNCERTAIN", "score": max_score['score']}

# ì‚¬ìš© ì˜ˆì‹œ
classifier = OptimizedPipeline("klue/bert-base", "text-classification")
results = classifier.batch_predict(["í…ìŠ¤íŠ¸1", "í…ìŠ¤íŠ¸2", "í…ìŠ¤íŠ¸3"])
```

# íŒŒì¸íŠœë‹ê³¼ í•™ìŠµ

## Trainer API í™œìš©

```python
from transformers import Trainer, TrainingArguments
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from torch.utils.data import Dataset

# 1. ë°ì´í„°ì…‹ ì¤€ë¹„
class CustomDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

# 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì¤€ë¹„
model_name = "klue/bert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3
)

# 3. í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_accuracy",
    greater_is_better=True
)

# 4. í‰ê°€ í•¨ìˆ˜
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = predictions.argmax(axis=-1)
    
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='weighted'
    )
    
    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# 5. í•™ìŠµ ì‹¤í–‰
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics
)

trainer.train()
```

## íš¨ìœ¨ì  íŒŒì¸íŠœë‹ ê¸°ë²•

### LoRA (Low-Rank Adaptation)

```python
from peft import LoraConfig, get_peft_model, TaskType

# LoRA ì„¤ì •
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=16,  # ë­í¬
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["query", "value"]  # ì ìš©í•  ë ˆì´ì–´
)

# LoRA ì ìš©
model = get_peft_model(model, lora_config)

# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
model.print_trainable_parameters()
# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: 294,912 (ì „ì²´ì˜ 0.27%)
```

### Gradient Checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)

```python
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì„ (ì†ë„ëŠ” ì•½ê°„ ëŠë ¤ì§)
model.gradient_checkpointing_enable()

training_args = TrainingArguments(
    # ... ê¸°íƒ€ ì„¤ì •
    gradient_checkpointing=True,
    dataloader_pin_memory=False,  # ë©”ëª¨ë¦¬ ì ˆì•½
    remove_unused_columns=True
)
```

### DeepSpeed í†µí•© (ëŒ€ê·œëª¨ ëª¨ë¸)

```python
# deepspeed_config.json
deepspeed_config = {
    "train_batch_size": 16,
    "gradient_accumulation_steps": 1,
    "fp16": {
        "enabled": True
    },
    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": True
        }
    }
}

training_args = TrainingArguments(
    # ... ê¸°íƒ€ ì„¤ì •
    deepspeed=deepspeed_config,
    fp16=True
)
```

# ì‹¤ë¬´ í™œìš© ì‚¬ë¡€ì™€ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

## í•œêµ­ì–´ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

```python
# íƒœìŠ¤í¬ë³„ ì¶”ì²œ í•œêµ­ì–´ ëª¨ë¸
korean_models = {
    # ë²”ìš© ì¸ì½”ë”
    'general_encoder': [
        "klue/bert-base",           # ê°€ì¥ ì•ˆì •ì 
        "klue/roberta-large",       # ë†’ì€ ì„±ëŠ¥
        "monologg/kobert",          # ê²½ëŸ‰í™”
        "beomi/kcbert-base"         # ëŒ“ê¸€/ë¦¬ë·° íŠ¹í™”
    ],
    
    # í…ìŠ¤íŠ¸ ìƒì„±
    'text_generation': [
        "skt/kogpt2-base-v2",       # ì¼ë°˜ì  ìš©ë„
        "kakaobrain/kogpt",         # ê³ í’ˆì§ˆ ìƒì„±
        "EleutherAI/polyglot-ko-12.8b"  # ëŒ€ê·œëª¨ ëª¨ë¸
    ],
    
    # ìš”ì•½
    'summarization': [
        "eenzeenee/t5-base-korean-summarization",
        "psyche/KoT5-summarization"
    ],
    
    # ë²ˆì—­
    'translation': [
        "Helsinki-NLP/opus-mt-ko-en",   # í•œâ†’ì˜
        "Helsinki-NLP/opus-mt-en-ko"    # ì˜â†’í•œ
    ]
}
```

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

```python
import time
import torch
from transformers import pipeline

def benchmark_model(model_name, texts, task="text-classification"):
    """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    # GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    torch.cuda.empty_cache()
    
    # ëª¨ë¸ ë¡œë“œ ì‹œê°„
    start_time = time.time()
    pipe = pipeline(task, model=model_name, device=0)
    load_time = time.time() - start_time
    
    # ì¶”ë¡  ì‹œê°„ (ë‹¨ì¼)
    start_time = time.time()
    single_result = pipe(texts[0])
    single_inference_time = time.time() - start_time
    
    # ì¶”ë¡  ì‹œê°„ (ë°°ì¹˜)
    start_time = time.time()
    batch_results = pipe(texts)
    batch_inference_time = time.time() - start_time
    
    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    memory_usage = torch.cuda.max_memory_allocated() / 1024**3  # GB
    
    return {
        'model': model_name,
        'load_time': load_time,
        'single_inference_time': single_inference_time,
        'batch_inference_time': batch_inference_time,
        'throughput': len(texts) / batch_inference_time,
        'memory_usage_gb': memory_usage
    }

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
test_texts = ["í…ŒìŠ¤íŠ¸ ë¬¸ì¥"] * 100
models_to_test = [
    "klue/bert-base",
    "monologg/distilkobert",
    "beomi/kcbert-base"
]

for model in models_to_test:
    result = benchmark_model(model, test_texts)
    print(f"{model}: {result}")
```

## í”„ë¡œë•ì…˜ ë°°í¬ ì „ëµ

```python
# 1. ëª¨ë¸ ìµœì í™” ë° ì €ì¥
class ProductionModel:
    def __init__(self, model_path):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_path,
            torch_dtype=torch.float16,  # ë°˜ì •ë°€ë„
            return_dict=True
        )
        self.model.eval()  # í‰ê°€ ëª¨ë“œ
        
    @torch.no_grad()
    def predict(self, texts):
        """ì¶”ë¡  ì „ìš© ë©”ì„œë“œ"""
        inputs = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            return_tensors="pt",
            max_length=512
        )
        
        outputs = self.model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        return predictions.cpu().numpy()

# 2. API ì„œë²„ ì˜ˆì‹œ (FastAPI)
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List

app = FastAPI()
model = ProductionModel("./trained_model")

class PredictionRequest(BaseModel):
    texts: List[str]

@app.post("/predict")
async def predict(request: PredictionRequest):
    predictions = model.predict(request.texts)
    return {
        "predictions": predictions.tolist(),
        "model_info": "klue/bert-base",
        "timestamp": time.time()
    }

# 3. ë„ì»¤ ì»¨í…Œì´ë„ˆí™”
dockerfile_content = """
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
"""
```

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

```python
import logging
from datetime import datetime
import json

class ModelMonitor:
    def __init__(self, model_name):
        self.model_name = model_name
        self.logger = self._setup_logger()
        self.prediction_count = 0
        self.error_count = 0
        
    def _setup_logger(self):
        logger = logging.getLogger(f"model_{self.model_name}")
        handler = logging.FileHandler(f"model_logs_{self.model_name}.log")
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
        return logger
    
    def log_prediction(self, input_text, prediction, confidence, latency):
        """ì˜ˆì¸¡ ê²°ê³¼ ë¡œê¹…"""
        self.prediction_count += 1
        
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "input_length": len(input_text),
            "prediction": prediction,
            "confidence": confidence,
            "latency_ms": latency * 1000,
            "prediction_id": self.prediction_count
        }
        
        self.logger.info(json.dumps(log_data))
        
        # ë‚®ì€ ì‹ ë¢°ë„ ê²½ê³ 
        if confidence < 0.7:
            self.logger.warning(f"Low confidence prediction: {confidence}")
    
    def log_error(self, error_msg, input_text):
        """ì˜¤ë¥˜ ë¡œê¹…"""
        self.error_count += 1
        self.logger.error(f"Error: {error_msg}, Input: {input_text[:100]}...")
    
    def get_stats(self):
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        return {
            "total_predictions": self.prediction_count,
            "total_errors": self.error_count,
            "error_rate": self.error_count / max(self.prediction_count, 1)
        }

# ì‚¬ìš© ì˜ˆì‹œ
monitor = ModelMonitor("sentiment_classifier")

def monitored_predict(text):
    start_time = time.time()
    try:
        result = model.predict([text])[0]
        latency = time.time() - start_time
        
        prediction = result.argmax()
        confidence = result.max()
        
        monitor.log_prediction(text, prediction, confidence, latency)
        return {"prediction": prediction, "confidence": confidence}
        
    except Exception as e:
        monitor.log_error(str(e), text)
        raise
```

# í•œêµ­ì–´ NLP íŠ¹í™” ê³ ë ¤ì‚¬í•­

## í•œêµ­ì–´ í† í°í™”ì˜ íŠ¹ìˆ˜ì„±

```python
# í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ì™€ ì¡°í•©
from konlpy.tag import Mecab
from transformers import AutoTokenizer

class KoreanPreprocessor:
    def __init__(self, model_name):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.mecab = Mecab()  # í˜•íƒœì†Œ ë¶„ì„ê¸°
    
    def preprocess_korean(self, text):
        """í•œêµ­ì–´ íŠ¹í™” ì „ì²˜ë¦¬"""
        
        # 1. ì •ê·œí™”
        text = text.strip()
        text = re.sub(r'[ã„±-ã…ã…-ã…£]+', '', text)  # ììŒ/ëª¨ìŒë§Œ ìˆëŠ” ê²½ìš° ì œê±°
        text = re.sub(r'\s+', ' ', text)  # ì—°ì† ê³µë°± ì œê±°
        
        # 2. ì´ëª¨ì§€/íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
        text = re.sub(r'[^\w\sê°€-í£]', '', text)
        
        # 3. í˜•íƒœì†Œ ë¶„ì„ (ì„ íƒì )
        # morphs = self.mecab.morphs(text)
        # text = ' '.join(morphs)
        
        return text
    
    def tokenize_korean(self, text):
        """í•œêµ­ì–´ í† í°í™”"""
        preprocessed = self.preprocess_korean(text)
        tokens = self.tokenizer(
            preprocessed,
            max_length=512,
            truncation=True,
            padding=True,
            return_tensors="pt"
        )
        return tokens

# ì‚¬ìš© ì˜ˆì‹œ
preprocessor = KoreanPreprocessor("klue/bert-base")
result = preprocessor.tokenize_korean("ì•ˆë…•í•˜ì„¸ìš”!!! ğŸ˜Š í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤ã…‹ã…‹ã…‹")
```

## í•œêµ­ì–´ ë°ì´í„° ì¦ê°•

```python
import random

class KoreanDataAugmentation:
    def __init__(self):
        # ìœ ì˜ì–´ ì‚¬ì „ (ì‹¤ì œë¡œëŠ” ë” í° ì‚¬ì „ í•„ìš”)
        self.synonyms = {
            "ì¢‹ë‹¤": ["í›Œë¥­í•˜ë‹¤", "ë©‹ì§€ë‹¤", "ê´œì°®ë‹¤", "ë‚˜ì˜ì§€ì•Šë‹¤"],
            "ë‚˜ì˜ë‹¤": ["ë³„ë¡œë‹¤", "ì•ˆì¢‹ë‹¤", "í˜•í¸ì—†ë‹¤"],
            "í¬ë‹¤": ["ê±°ëŒ€í•˜ë‹¤", "í°", "ëŒ€í˜•ì˜"],
            "ì‘ë‹¤": ["ì†Œí˜•ì˜", "ì‘ì€", "ë¯¸ë‹ˆ"]
        }
    
    def synonym_replacement(self, text, n=1):
        """ìœ ì˜ì–´ ì¹˜í™˜"""
        words = text.split()
        new_words = words.copy()
        
        random_word_list = list(set([word for word in words if word in self.synonyms]))
        random.shuffle(random_word_list)
        
        num_replaced = 0
        for random_word in random_word_list:
            if num_replaced < n:
                synonyms = self.synonyms[random_word]
                synonym = random.choice(synonyms)
                new_words = [synonym if word == random_word else word for word in new_words]
                num_replaced += 1
        
        return ' '.join(new_words)
    
    def random_insertion(self, text, n=1):
        """ì„ì˜ ì‚½ì…"""
        words = text.split()
        
        for _ in range(n):
            new_word = self._get_random_synonym()
            random_idx = random.randint(0, len(words))
            words.insert(random_idx, new_word)
        
        return ' '.join(words)
    
    def _get_random_synonym(self):
        """ì„ì˜ ìœ ì˜ì–´ ì„ íƒ"""
        word = random.choice(list(self.synonyms.keys()))
        return random.choice(self.synonyms[word])

# ì‚¬ìš© ì˜ˆì‹œ
augmenter = KoreanDataAugmentation()
original = "ì´ ì˜í™”ëŠ” ì •ë§ ì¢‹ë‹¤"
augmented = augmenter.synonym_replacement(original)
print(f"ì›ë³¸: {original}")
print(f"ì¦ê°•: {augmented}")
```

# ê²°ë¡ 

Hugging FaceëŠ” **NLP ë¶„ì•¼ì˜ ê²Œì„ì²´ì¸ì €**ë¡œì„œ, ì—°êµ¬ì™€ ì‹¤ë¬´ ì‚¬ì´ì˜ ê°„ê²©ì„ í˜ì‹ ì ìœ¼ë¡œ ì¤„ì˜€ë‹¤. 

## í•µì‹¬ ì„±ê³¼

* **ê¸°ìˆ  ë¯¼ì£¼í™”**: ë³µì¡í•œ PLMì„ ëª‡ ì¤„ì˜ ì½”ë“œë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦
* **í‘œì¤€í™”**: ëª¨ë“  ëª¨ë¸ì´ ë™ì¼í•œ APIë¡œ í†µì¼ë˜ì–´ í•™ìŠµ ê³¡ì„  ë‹¨ì¶•
* **ìƒíƒœê³„ êµ¬ì¶•**: 10ë§Œ+ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ê³µìœ í•˜ëŠ” ê±°ëŒ€í•œ ì»¤ë®¤ë‹ˆí‹° í˜•ì„±
* **ê°œë°œ ê°€ì†í™”**: í”„ë¡œí† íƒ€ì…ë¶€í„° í”„ë¡œë•ì…˜ê¹Œì§€ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì§€ì›

## ì‹¤ë¬´ì—ì„œì˜ ê°€ì¹˜

**Before Hugging Face** ì‹œëŒ€ì—ëŠ” ìƒˆë¡œìš´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë…¼ë¬¸ì„ ì½ê³ , ì½”ë“œë¥¼ ì§ì ‘ êµ¬í˜„í•˜ë©°, ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì•„ í—¤ë§¤ì•¼ í–ˆë‹¤. **After Hugging Face** ì‹œëŒ€ì—ëŠ” `pipeline("task", model="model_name")`ë§Œìœ¼ë¡œ ìµœì‹  ê¸°ìˆ ì„ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤.

ì´ëŸ¬í•œ ì ‘ê·¼ì„± í–¥ìƒì€ ë‹¨ìˆœí•œ í¸ì˜ì„±ì„ ë„˜ì–´ì„œ, **AI ê¸°ìˆ ì˜ ì§„ì… ì¥ë²½ì„ ë‚®ì¶° ë” ë§ì€ ê°œë°œìë“¤ì´ NLP ë¶„ì•¼ì— ì°¸ì—¬í•  ìˆ˜ ìˆê²Œ** ë§Œë“¤ì—ˆë‹¤. ìŠ¤íƒ€íŠ¸ì—…ë¶€í„° ëŒ€ê¸°ì—…ê¹Œì§€, ì—°êµ¬ìë¶€í„° ì‹¤ë¬´ ê°œë°œìê¹Œì§€ ëª¨ë“  ë ˆë²¨ì—ì„œ í˜œíƒì„ ë°›ê³  ìˆë‹¤.

## ë¯¸ë˜ ì „ë§

Hugging FaceëŠ” ê³„ì†í•´ì„œ ë°œì „í•˜ê³  ìˆë‹¤:

* **ë” í° ëª¨ë¸ë“¤**: LLaMA, Falcon ë“± ì˜¤í”ˆì†ŒìŠ¤ ëŒ€ê·œëª¨ ëª¨ë¸ ì§€ì› í™•ëŒ€
* **ë©€í‹°ëª¨ë‹¬**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ìŒì„±ì„ í†µí•©í•˜ëŠ” ëª¨ë¸ë“¤
* **íš¨ìœ¨ì„±**: ì–‘ìí™”, í”„ë£¨ë‹, LoRA ë“± íš¨ìœ¨ì  í•™ìŠµ/ì¶”ë¡  ê¸°ë²•
* **AutoML**: ìë™ ëª¨ë¸ ì„ íƒê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
* **Edge ë°°í¬**: ëª¨ë°”ì¼ê³¼ ì„ë² ë””ë“œ í™˜ê²½ ì§€ì› ê°•í™”

## ì‹¤ë¬´ì§„ì„ ìœ„í•œ ì¡°ì–¸

1. **Pipelineë¶€í„° ì‹œì‘**: ë³µì¡í•œ êµ¬í˜„ë³´ë‹¤ëŠ” Pipeline APIë¡œ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… ì œì‘
2. **í•œêµ­ì–´ ëª¨ë¸ í™œìš©**: KLUE ë“± ê²€ì¦ëœ í•œêµ­ì–´ ëª¨ë¸ ìš°ì„  ê³ ë ¤
3. **ë‹¨ê³„ì  ì ‘ê·¼**: API â†’ íŒŒì¸íŠœë‹ â†’ ì»¤ìŠ¤í…€ ëª¨ë¸ ìˆœìœ¼ë¡œ ì ì§„ì  ë°œì „
4. **ì»¤ë®¤ë‹ˆí‹° í™œìš©**: Model Hubì˜ í‰ê°€ì™€ ë¦¬ë·°ë¥¼ ì°¸ê³ í•œ í˜„ëª…í•œ ëª¨ë¸ ì„ íƒ
5. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë°˜ë“œì‹œ ì„±ëŠ¥ê³¼ í’ˆì§ˆ ì¶”ì 

Hugging FaceëŠ” **"ì—°êµ¬ì‹¤ì˜ ìµœì‹  ê¸°ìˆ ì„ í˜„ì‹¤ì˜ ë¬¸ì œ í•´ê²°ì— ë°”ë¡œ ì ìš©"**í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê°•ë ¥í•œ ë„êµ¬ë‹¤. ì´ì œ ì¤‘ìš”í•œ ê²ƒì€ ê¸°ìˆ  ìì²´ê°€ ì•„ë‹ˆë¼, **ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•  ê²ƒì¸ê°€**ì™€ **ì–´ë–»ê²Œ ì‚¬ìš©ìì—ê²Œ ê°€ì¹˜ë¥¼ ì œê³µí•  ê²ƒì¸ê°€**ì´ë‹¤. Hugging FaceëŠ” ê·¸ ì—¬ì •ì„ ìœ„í•œ ìµœê³ ì˜ ë™ë°˜ìê°€ ë  ê²ƒì´ë‹¤.

