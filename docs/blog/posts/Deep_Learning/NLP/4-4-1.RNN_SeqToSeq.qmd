---
title: "RNN 기반 언어 모델과 Seq2Seq"
subtitle: "순환 신경망을 이용한 언어 모델링과 기계 번역"
description: |
  RNN을 활용한 언어 모델의 구조와 작동 원리, 그리고 Seq2Seq 모델을 통한 기계 번역의 발전 과정을 다룬다. Teacher Forcing 학습 기법과 Encoder-Decoder 구조의 실제 동작 원리를 상세히 설명한다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-15
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False # 발행 시 False로 변경
---

# 요약

이 문서는 RNN(순환 신경망)을 활용한 언어 모델의 기본 원리와 Seq2Seq 모델을 통한 기계 번역 시스템의 구현 방법을 설명한다. 언어 모델은 이전 단어들의 문맥을 바탕으로 다음에 올 단어를 예측하는 모델로, 자연어 생성과 기계 번역에서 핵심적인 역할을 한다.

주요 내용은 다음과 같다:

* **언어 모델의 기본 개념**: 
  - 주어진 단어 시퀀스에서 다음 단어의 확률 분포를 예측하는 모델
  - 수식: $P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^{T} P(w_t | w_1, w_2, \ldots, w_{t-1})$
  - 문맥 정보를 활용한 조건부 확률 모델링이 핵심

* **RNN 언어 모델 구조**:
  - 가변 길이 입력 처리 가능한 순환 구조
  - 각 시점에서 hidden state가 문맥 정보를 누적하여 전달
  - Embedding → Hidden → Output 레이어로 구성된 표준 구조

* **Seq2Seq와 기계 번역**:
  - Encoder-Decoder 구조를 통한 시퀀스 간 변환
  - Context Vector를 통한 소스 언어 정보의 압축 전달
  - 1950년대 규칙 기반부터 2020년대 Transformer까지의 발전 과정

* **Teacher Forcing 학습 기법**:
  - 훈련 시 실제 정답 토큰을 디코더 입력으로 사용하는 방법
  - 오류 누적 방지와 학습 안정성 확보
  - 훈련과 테스트 단계의 차이점과 하이퍼파라미터로서의 활용

RNN 언어 모델과 Seq2Seq는 현대 자연어 처리의 기초가 되며, GPT와 같은 최신 언어 모델들의 토대를 제공한다.

# 텍스트 인코딩 및 벡터화

```
RNN Language Model
├── Seq2Seq
├── Beam Search
├── Subword Tokenization
├── Attention
├── Transformer Encoder (Vaswani et al., 2017)
|   ├── Positional Encoding
|   ├── Multi-Head Attention
|   └── Feed Forward Neural Network
|
├── Transformer Decoder (Vaswani et al., 2017)
|
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
|
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
|
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등
```

# RNN 기반 언어 모델

## NLP

* NLP = NLU + NLG
  * NLU: 자연어 이해
  * NLG: 자연어 생성
    * Image Captioning
    * Text Summarization
    * Text Generation
    * Text Classification
    * Chatbot
    * **Neural Machine Translation**    

### Machine Translation

* 입력문장(source)을 번역한 출력 문장(target text)을 생성해내는 Task
* Brief History
  * RBMT(Rule-based Machine Translation): 1950s, if-statement 기반 알고리즘, 성능 안좋음
  * EBMT(Example-based Machine Translation): 1980s, 예제 기반 알고리즘, 성능 안좋음
  * SMT(Statistical Machine Translation): 1990s, 통계 기반 알고리즘, 성능 보통
  * NMT(Neural Machine Translation): 2010s, 신경망 기반 알고리즘, 성능 비약적 상승
  * Transformer(Attention is all you need): 2017, 신경망 기반 알고리즘, 성능 매우 좋음
  * GPT-3(Generative Pre-trained Transformer 3): 2020, 성능 좋음
  * ChatGPT(Generative Pre-trained Transformer 3): 2022, 성능 매우 좋음
  * GPT-4(Generative Pre-trained Transformer 4): 2023, 성능 매우 좋음
* NMT(Neural Machine Translation) 성능 향상 이유
  * word embedding으로 인한 continuous representation 활용
  * 기존 SMT가 여러 모듈이 결합된 결과였다면 이제는 end-to-end 모델의 시대
    * 모듈 간 의존성 제거: upstream 모듈이 에러가 나면 전체 모델이 에러가 나는 문제 발생
    * end-to-end 모델 구조 도입: 하나의 모델이 수많은 파라미터 기반으로 오차함수에 의해 알고리즘이 일제히 업데이트 되는 구조
  * Attention 으로 인해 길이가 긴 문장 또한 좋은 성능을 보이기 시작

### Translation 모델 종류

* Seq2Seq: 2014, 입력 문장을 임베딩 벡터로 변환하여 출력 문장을 생성하는 모델
* Transformer: 2017, 입력 문장을 임베딩 벡터로 변환하여 출력 문장을 생성하는 모델
* BERT: 2018, 입력 문장을 임베딩 벡터로 변환하여 출력 문장을 생성하는 모델
* GPT: 2018, 입력 문장을 임베딩 벡터로 변환하여 출력 문장을 생성하는 모델

## Seq2Seq (Sequence to Sequence)

* 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력한다.
* 구조: Encoder + Decoder
  * Encoder
    * NLU 모델: 입력 시퀀스를 임베딩 벡터로 변환
    * RNN으로 구성
  * Decoder
    * NLG 모델: 출력 시퀀스를 출력 문장으로 변환
    * RNN으로 구성
* 동작 방식    
  * Sequence to Sequence Learning with Neural Networks 논문 참조
    * 본격적인 신경망 기계 번역기를 제시
    * 서로 다른 2개의 LSTM 아키텍쳐를 각각 인코더-디코더로 사용
  * 예문: 저는 학생입니다.
   

    ```
    Encoder-Decoder LSTM 번역 과정
    │
    ├── Encoder Phase (인코딩 단계)
    │   ├── Step 1: "저는"
    │   │   ├── Input: "저는"
    │   │   ├── Initial State: h0=0, c0=0
    │   │   ├── Encoder LSTM1 연산
    │   │   └── Output: h1, c1 → 다음 단계로 전달
    │   │
    │   ├── Step 2: "학생"  
    │   │   ├── Input: "학생"
    │   │   ├── Previous State: h1, c1 (from LSTM1)
    │   │   ├── Encoder LSTM2 연산
    │   │   └── Output: h2, c2 → 다음 단계로 전달
    │   │
    │   └── Step 3: "입니다"
    │       ├── Input: "입니다"
    │       ├── Previous State: h2, c2 (from LSTM2)
    │       ├── Encoder LSTM3 연산
    │       └── Output: Context Vector (h3, c3) ★ → Decoder로 전달
    │
    ├── Context Transfer (문맥 전달)
    │   └── Context Vector (h3, c3) → Decoder 초기 상태 (h_d0, c_d0)
    │
    └── Decoder Phase (디코딩 단계)
        ├── Step 1: 번역 시작
        │   ├── Input: EOS (번역 시작)
        │   ├── Initial State: h_d0=h3, c_d0=c3 (Context Vector)
        │   ├── Decoder LSTM1 연산
        │   ├── Output: "I"
        │   └── Hidden State: h_d1, c_d1 → 다음 단계로 전달
        │
        ├── Step 2: 첫 번째 단어 생성 후
        │   ├── Input: "I"
        │   ├── Previous State: h_d1, c_d1 (from Decoder LSTM1)
        │   ├── Decoder LSTM2 연산
        │   ├── Output: "am"
        │   └── Hidden State: h_d2, c_d2 → 다음 단계로 전달
        │
        ├── Step 3: 두 번째 단어 생성 후
        │   ├── Input: "am"
        │   ├── Previous State: h_d2, c_d2 (from Decoder LSTM2)
        │   ├── Decoder LSTM3 연산
        │   ├── Output: "a"
        │   └── Hidden State: h_d3, c_d3 → 다음 단계로 전달
        │
        ├── Step 4: 세 번째 단어 생성 후
        │   ├── Input: "a"
        │   ├── Previous State: h_d3, c_d3 (from Decoder LSTM3)
        │   ├── Decoder LSTM4 연산
        │   ├── Output: "student"
        │   └── Hidden State: h_d4, c_d4 → 다음 단계로 전달
        │
        ├── Step 5: 네 번째 단어 생성 후
        │   ├── Input: "student"
        │   ├── Previous State: h_d4, c_d4 (from Decoder LSTM4)
        │   ├── Decoder LSTM5 연산
        │   ├── Output: "."
        │   └── Hidden State: h_d5, c_d5 → 다음 단계로 전달
        │
        └── Step 6: 번역 종료
            ├── Input: "."
            ├── Previous State: h_d5, c_d5 (from Decoder LSTM5)
            ├── Decoder LSTM6 연산
            ├── Output: EOS (번역 종료) ★
            └── Final State: h_d6, c_d6 (번역 완료)
    ```

    * 각 단어는 워드 임베딩이 된 벡터로 LSTM에 들어가지만 편의상 한글 단어로 표현
    * context vector
      * encoder 마지막 시점의 hidden state    
      * 인코다가 입력 문장의 모든 단어들을 수찬적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 context vector로 만든다.
      * 즉, context vector는 decoder RNN 셀의 첫 번째 hidden state로 사용된다.
* Teacher Forcing
  * Encoder RNN (학습방식) 과 Decoder RNN (Test 방식)의 동작 방식이 다르다.
  * 파파고나 구글 번역기의 기초 동작 메커니즘
  * 훈련 단계에서 Teacher Forcing 을 무조건 하는 것이 아니라 비율을 정해서 수행
  * 이 비율이 hyper parameter 이다.
  * 이 비율을 높게 설정할 수록 빠른 학습이 가능해지지만 overfit되어 테스트 단계에서 악영향을 줄 수 있음
  * test 단계에서는 teacher forcing을 사용하지 않으며 현 시점의 출력을 다음 시점의 입력으로 사용한다.



## 결론

RNN 기반 언어 모델과 Seq2Seq 구조는 자연어 처리 분야에서 패러다임 전환을 이끈 핵심 기술이다. 이들은 기계 번역의 성능을 획기적으로 개선시켰으며, 현대 언어 모델들의 기초 개념을 제공했다.

* **기술적 의의**:
  - 가변 길이 시퀀스 처리 능력으로 자연어의 본질적 특성 반영
  - Encoder-Decoder 구조를 통한 서로 다른 도메인 간 매핑 실현
  - Context Vector 개념으로 긴 문장의 의미 압축 표현 가능

* **Teacher Forcing의 혁신**:
  - 훈련과 추론 단계의 차이를 체계적으로 관리하는 방법론 제시
  - 오류 누적 문제 해결로 안정적인 시퀀스 생성 학습 실현
  - 하이퍼파라미터 조정을 통한 성능 최적화 전략 제공

* **기계 번역의 발전**:
  - 규칙 기반(1950s) → 통계 기반(1990s) → 신경망 기반(2010s)의 발전 과정
  - End-to-end 학습으로 모듈 간 의존성 문제 해결
  - Word embedding과 연속 표현의 활용으로 성능 비약적 향상

* **현대적 의미**:
  - LSTM, GRU 등 개선된 RNN 변형들의 토대 제공
  - Attention 메커니즘 도입의 동기와 배경 이해
  - Transformer, BERT, GPT 등 최신 모델들의 개념적 기초

비록 현재는 Transformer 기반 모델들이 주류를 이루고 있지만, RNN 언어 모델과 Seq2Seq의 핵심 아이디어들은 여전히 많은 NLP 시스템에서 활용되고 있다. 특히 실시간 처리가 중요한 애플리케이션이나 제한된 자원 환경에서는 RNN 기반 모델들이 여전히 유용한 선택지가 되고 있으며, 언어 모델의 기본 원리를 이해하는 데 필수적인 개념으로 남아있다.

## 언어 모델의 수학적 정의

언어 모델은 주어진 단어 시퀀스의 확률을 계산하는 모델이다. $T$개의 단어로 구성된 문장 $w_1, w_2, \ldots, w_T$에 대해:

$$P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^{T} P(w_t | w_1, w_2, \ldots, w_{t-1})$$

여기서 $w_i$ 는 $i$ 번째 단어이고, $P(w_i | w_1, w_2, \ldots, w_{i-1})$ 는 이전 단어들이 주어졌을 때 현재 단어가 나올 조건부 확률이다.

## RNN을 이용한 구현

RNN 언어 모델에서는 각 시점의 hidden state $h_t$가 지금까지의 모든 단어 정보를 압축하여 저장한다:

$$h_t = f(h_{t-1}, x_t)$$
$$P(w_{t+1} | w_1, \ldots, w_t) = \text{softmax}(W_o h_t + b_o)$$

각 시점 $t$ 에서의 계산 과정:

1. **입력 임베딩**: $e_t = \text{Embedding}(x_t)$
2. **Hidden State 업데이트**: $h_t = \tanh(W_{hh} h_{t-1} + W_{xh} e_t + b_h)$
3. **출력 계산**: $o_t = W_{ho} h_t + b_o$
4. **확률 분포**: $P(w_{t+1}) = \text{softmax}(o_t)$

### 실제 동작 예시

문장 "what will the side effects be?"를 처리하는 과정:

- $x_1 = \text{"what"}$ → $h_1$ 계산 → $P(\text{next word} | \text{"what"})$ → $y_1 = \text{"will"}$
- $x_2 = \text{"will"}$ → $h_2$ 계산 → $P(\text{next word} | \text{"what will"})$ → $y_2 = \text{"the"}$
- $x_3 = \text{"the"}$ → $h_3$ 계산 → $P(\text{next word} | \text{"what will the"})$ → $y_3 = \text{"side"}$
- $x_4 = \text{"side"}$ → $h_4$ 계산 → $P(\text{next word} | \text{"what will the side"})$ → $y_4 = \text{"effects"}$

이 과정에서 $h_t$ 는 시점 $t$까지의 모든 이전 단어들의 문맥 정보를 압축적으로 담고 있다.
     
## Teacher Forcing (교사 강요)

### 개념과 필요성

Teacher Forcing은 sequence-to-sequence 모델의 훈련 과정에서 사용되는 기법이다:

- **훈련 시**: 실제 정답(ground truth) 토큰을 디코더의 다음 입력으로 사용
- **추론 시**: 모델이 예측한 토큰을 다음 입력으로 사용

이는 다음과 같은 문제를 해결한다:
- **오류 누적(Error Accumulation)**: 잘못된 예측이 연쇄적으로 더 큰 오류를 만드는 문제
- **학습 불안정성**: 초기 학습 단계에서 무작위 예측으로 인한 학습 어려움
- **수렴 속도**: 올바른 패턴 학습을 위한 가이드 제공

### 구현 시 고려사항

**Teacher Forcing 비율 조정**:
- 초기 학습: 높은 비율(0.8-1.0)로 안정적 학습
- 후기 학습: 점진적으로 비율 감소(0.3-0.5)
- 실제 추론 상황과의 차이를 줄이기 위한 점진적 조정

**Scheduled Sampling**:
- 훈련 중에도 가끔 예측값을 사용하여 exposure bias 완화
- 커리큘럼 학습의 한 형태로 활용

### 각 레이어의 상세 구조

#### Embedding Layer
- **역할**: 단어 인덱스를 고정 크기의 벡터로 변환
- **수식**: $e_t = E[w_t]$, 여기서 $E \in \mathbb{R}^{V \times d}$
- **파라미터**: 
  - $V$: 어휘 크기 (일반적으로 10,000-50,000개)
  - $d$: 임베딩 차원수 (일반적으로 100-300차원)

#### RNN Hidden Layer  
- **역할**: 시퀀스의 문맥 정보를 누적하여 표현
- **수식**: $h_t = \tanh(W_{hh} h_{t-1} + W_{xh} e_t + b_h)$
- **파라미터**: 
  - $W_{hh} \in \mathbb{R}^{H \times H}$: hidden-to-hidden 가중치
  - $W_{xh} \in \mathbb{R}^{H \times d}$: input-to-hidden 가중치
  - $H$: 은닉 상태 차원수 (일반적으로 128-512차원)

#### Output Layer
- **역할**: 은닉 상태를 어휘 크기의 로짓 벡터로 변환
- **수식**: $o_t = W_{ho} h_t + b_o$
- **파라미터**: $W_{ho} \in \mathbb{R}^{V \times H}$, $b_o \in \mathbb{R}^{V}$

#### Softmax & Loss
- **확률 분포**: $P(w_t | w_{<t}) = \text{softmax}(o_t) = \frac{e^{o_t^{(i)}}}{\sum_{j=1}^{V} e^{o_t^{(j)}}}$
- **손실 함수**: $\mathcal{L} = -\sum_{t=1}^{T} \log P(w_t^* | w_{<t})$

### 모델 성능 최적화

**기울기 소실 문제 해결**:
- LSTM, GRU 같은 게이트 메커니즘 도입
- Residual Connection 활용
- 적절한 가중치 초기화 (Xavier, He 초기화)

**계산 효율성 개선**:
- 배치 처리를 통한 병렬화
- 적응적 softmax (hierarchical softmax)
- 어휘 크기 축소 기법 (subword tokenization)
