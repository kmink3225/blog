---
title: "텍스트 벡터화: GloVe의 이해"
subtitle: "전역 통계 정보를 활용한 단어 임베딩"
description: |
  자연어 처리(NLP)에서 단어를 벡터로 표현하는 GloVe(Global Vectors for Word Representation) 모델을 심층적으로 탐구한다. GloVe가 전체 말뭉치의 단어 동시 등장 통계 정보를 어떻게 활용하여 벡터를 학습하는지, 그 원리와 목적 함수, Word2Vec과의 차이점을 다룬다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-07
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False # 발행 시 False로 변경
---

# 요약

이 문서는 Word2Vec과 같은 예측 기반 방식과는 다른 접근법을 사용하는 GloVe(Global Vectors for Word Representation) 모델을 심층적으로 소개한다. GloVe는 단어 임베딩을 학습하기 위해 전체 말뭉치의 전역적인 단어 동시 등장 통계 정보를 직접 활용한다.

*   **Word2Vec의 한계와 GloVe의 등장 배경**:
    *   Word2Vec이 지역적 문맥 정보(local context)만을 활용하는 반면, GloVe는 말뭉치 전체의 통계 정보를 보다 직접적으로 임베딩에 반영하고자 한다.
*   **GloVe (Global Vectors for Word Representation)**:
    *   **핵심 원리**: 단어 동시 등장 행렬(Co-occurrence Matrix)을 기반으로, 단어 벡터 간의 관계가 전체 말뭉치에서 해당 단어들이 함께 등장할 확률의 비율을 근사하도록 학습한다.
    *   **목적 함수**: $\sum_{i,j=1}^V f(X_{ij})(\mathbf{w}_i^T \mathbf{\tilde{w}}_j + b_i + \tilde{b}_j - \log X_{ij})^2$ 형태의 손실 함수를 최소화하는 방식으로 학습이 진행된다. 여기서 $X_{ij}$는 단어 $i$와 $j$의 동시 등장 빈도, $f(X_{ij})$는 가중치 함수이다.
    *   학습 과정을 통해 단어 벡터는 전역적인 통계적 특성을 반영하게 되며, 이는 단어 간의 의미론적 관계를 포착하는 데 효과적이다.
*   **결론**: GloVe가 Word2Vec과 같은 예측 기반 모델과 어떻게 다르며, 전역 통계 정보를 활용함으로써 얻는 장점과 정적 임베딩 분야에서의 의의를 요약한다.

# 텍스트 인코딩 및 벡터화

```
텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    └── RNN 계열
        ├── LSTM
        ├── GRU
        └── ELMo
```

## 신경망 사용 (2008~2018): Static Word Embedding

### GloVe (2014)

* 전역 통계 정보 활용
* 단어 동시 출현 행렬(Co-occurrence Matrix) 기반
* 목적 함수: $\min \sum_{i,j=1}^V f(X_{ij})(\mathbf{w}_i^T \mathbf{\tilde{w}}_j + b_i + \tilde{b}_j - \log X_{ij})^2$
* 여기서 $X_{ij}$ 는 단어 $i$ 와 $j$ 의 동시 출현 빈도
* Word2Vec의 한계: 
   * 지역적 문맥 정보만 사용 (윈도우 크기 내)
   * 전체 말뭉치의 통계 정보를 충분히 활용하지 못함
* GloVe의 해결책:
   * 전체 말뭉치의 동시 출현 통계를 미리 계산
   * 이 통계 정보를 직접 활용하여 벡터 학습
* 동시 출현 행렬 (Co-occurrence Matrix)
* 구체적 예시
   * 말뭉치:

      ```
      문장1: "나는 사과를 좋아한다"
      문장2: "사과는 맛있는 과일이다"  
      문장3: "바나나도 좋은 과일이다"
      ```

   * 윈도우 크기 2로 동시 출현 계산:

      |       | 나는 | 사과를/사과는 | 좋아한다 | 맛있는 | 과일이다 | 바나나도 | 좋은 |
      |-------|------|---------------|----------|--------|----------|----------|------|
      | 나는     | 0    | 1             | 1        | 0      | 0        | 0        | 0    |
      | 사과를/사과는 | 1    | 0             | 1        | 1      | 1        | 0        | 0    |
      | 좋아한다   | 1    | 1             | 0        | 0      | 0        | 0        | 0    |
      | 맛있는    | 0    | 1             | 0        | 0      | 1        | 0        | 0    |
      | 과일이다   | 0    | 1             | 0        | 1      | 0        | 1        | 1    |
      | 바나나도   | 0    | 0             | 0        | 0      | 1        | 0        | 1    |
      | 좋은     | 0    | 0             | 0        | 0      | 1        | 1        | 0    |

   * 해석: $X_{ij}$ = 단어 i와 j가 윈도우 내에서 함께 나타난 횟수

#### GloVe 목적 함수 해부

* $\min \sum_{i,j=1}^V f(X_{ij})(\mathbf{w}_i^T \mathbf{w}_j + b_i + b_j - \log X_{ij})^2$
* 각 항목의 의미
   * $\mathbf{w}_i^T \mathbf{w}_j$ : 단어 벡터들의 내적
      * 두 단어의 유사도를 나타냄
      * 자주 함께 나타나는 단어들은 높은 내적값을 가져야 함
   * $b_i + b_j$ : 편향(bias) 항
      * 각 단어의 전반적인 빈도를 조정
      * 자주 나타나는 단어는 높은 편향값
   * $\log X_{ij}$ : 실제 동시 출현 빈도의 로그
      * 목표값 (우리가 맞추려는 값)
      * 로그를 취하는 이유: 빈도의 분포가 매우 치우쳐 있어서
   * $f(X_{ij})$ : 가중치 함수
      * 너무 희귀한 동시 출현은 신뢰도가 낮음 → 낮은 가중치

      $$
      f(x) = \begin{cases} 
      (\frac{x}{x_{max}})^{\alpha} & \text{if } x < x_{max} \\
      1 & \text{otherwise}
      \end{cases}
      $$

      * 가중치 함수의 역할:
         * 너무 희귀한 동시 출현은 신뢰도가 낮음 → 낮은 가중치
         * 너무 흔한 동시 출현도 정보량이 적음 → 가중치 제한
         * 적당한 빈도의 동시 출현에 높은 가중치

* 학습 과정 예시
   * 예시: "사과"와 "과일" 관계 학습

   ```python
   # 실제 동시 출현: X_사과_과일 = 10번
   # 목표: w_사과^T * w_과일 + b_사과 + b_과일 ≈ log(10) = 2.3

   # 초기 (랜덤)
   w_사과 = [0.1, 0.2]
   w_과일 = [0.3, -0.1]  
   내적 = 0.1*0.3 + 0.2*(-0.1) = 0.01
   편향합 = 0.5 + 0.3 = 0.8
   예측값 = 0.01 + 0.8 = 0.81

   # 손실: (0.81 - 2.3)^2 = 2.22 (크다!)

   # 업데이트 후
   w_사과 = [0.4, 0.5]  # 더 큰 값들로
   w_과일 = [0.6, 0.2]
   내적 = 0.4*0.6 + 0.5*0.2 = 0.34
   편향합 = 1.2 + 0.7 = 1.9
   예측값 = 0.34 + 1.9 = 2.24  # 목표 2.3에 가까워짐!
   ```

# 결론

본 문서에서는 자연어 처리(NLP) 분야에서 단어의 의미를 효과적으로 벡터 공간에 표현하기 위한 또 다른 핵심적인 정적 임베딩 방법론인 GloVe(Global Vectors for Word Representation)를 심층적으로 살펴보았다. GloVe는 Word2Vec과 같이 '분포 가설'에 기반하지만, 학습 방식에서 차별점을 보인다.

*   **GloVe의 핵심**:
    *   **전역 통계 정보 활용**: GloVe의 가장 큰 특징은 전체 말뭉치에서 단어들이 함께 등장하는 빈도, 즉 동시 등장 행렬(Co-occurrence Matrix) 정보를 직접적으로 사용하여 단어 임베딩을 학습한다는 점이다. 이는 지역적 문맥(local context window)에 의존하는 Word2Vec과 대비된다.
    *   **목적 함수 최적화**: GloVe는 특정 목적 함수를 최소화하는 과정을 통해 단어 벡터를 학습한다. 이 목적 함수는 두 단어 벡터의 내적이 두 단어의 동시 등장 확률에 비례하도록 설계되어, 단어 간의 의미론적 관계를 전역적인 통계치로부터 학습할 수 있게 한다.
    *   학습된 GloVe 벡터는 Word2Vec과 마찬가지로 단어 유추(analogy)와 같은 작업에서 좋은 성능을 보이며, 단어 간의 선형적인 관계를 포착할 수 있다.
*   **GloVe의 의의**:
    *   GloVe는 예측 기반(prediction-based) 방법론인 Word2Vec과 계수 기반(count-based) 방법론의 장점을 결합하려는 시도로 볼 수 있다. 전역적인 통계 정보를 손실 함수에 직접 통합함으로써, 보다 효율적으로 단어 간의 의미 관계를 학습할 수 있음을 보여주었다.
    *   Word2Vec과 함께 GloVe는 정적 워드 임베딩의 중요한 축을 이루며, 이후 등장하는 문맥 기반 동적 임베딩 모델들의 발전에 영향을 미쳤다. 다양한 NLP 다운스트림 태스크에서 강력한 초기 임베딩 레이어로 활용되며 그 유효성을 입증했다.

결론적으로, GloVe는 전체 말뭉치의 통계 정보를 명시적으로 활용하여 고품질의 단어 벡터를 생성하는 효과적인 방법론이다. 이는 단어의 의미를 고정된 벡터로 표현하는 데 있어 Word2Vec과는 다른 접근 방식을 제공하며, NLP 연구 및 응용 분야에서 중요한 도구로 자리매김했다.
