---
title: "텍스트 벡터화: Word2Vec의 이해"
subtitle: "CBOW와 Skip-gram을 이용한 신경망 기반 단어 임베딩"
description: |
  자연어 처리(NLP)에서 단어를 벡터로 표현하는 핵심 방법론인 Word2Vec을 심층적으로 탐구한다. CBOW 및 Skip-gram 모델의 원리, 수학적 배경, 학습 과정, 그리고 Negative Sampling 기법을 상세히 다룬다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-06
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False # 발행 시 False로 변경
---

# 요약

이 문서는 전통적인 DTM 방식의 한계를 넘어, 단어의 의미론적 정보를 벡터 공간에 표현하는 신경망 기반의 핵심 방법론인 Word2Vec을 심층적으로 소개한다.

*   **DTM 방식의 한계와 신경망 접근법의 등장**:
    *   전통적인 DTM(문서-단어 행렬) 방식의 문제점(차원의 저주, 희소성, 의미 관계 표현 불가)을 지적하고, 이를 극복하기 위한 신경망 기반 밀집 벡터 표현(워드 임베딩)의 필요성을 설명한다.
*   **Word2Vec (CBOW, Skip-gram)**:
    *   **핵심 원리**: "같은 문맥에 나타나는 단어는 비슷한 의미를 가진다"는 분포 가설에 기반하여 단어를 저차원 밀집 벡터로 표현한다.
    *   **CBOW (Continuous Bag-of-Words)**: 주변 단어들을 이용하여 중심 단어를 예측하는 모델의 구조와 수학적 원리를 설명한다.
    *   **Skip-gram**: 중심 단어를 이용하여 주변 단어들을 예측하는 모델의 구조와 수학적 원리를 설명한다. 특히, 연산 효율성을 높이기 위한 Negative Sampling (SGNS) 기법을 상세히 다룬다.
    *   두 모델의 학습 과정과 임베딩 벡터가 단어의 의미를 포착하는 방식을 예시와 함께 설명한다.
*   **결론**: Word2Vec이 신경망 기반 단어 임베딩의 기초를 어떻게 마련했는지, 그리고 그 의의를 요약한다.

# 텍스트 인코딩 및 벡터화

```
텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    └── RNN 계열
        ├── LSTM
        ├── GRU
        └── ELMo
```

## 신경망 사용 (2008~2018): Static Word Embedding

### Word2Vec (2013)

#### **Word2Vec (2013)**의 두 가지 아키텍처: 
   
* 전제: 같은 문잭에서의 단어들은 유사한 단어들이 주로 출현하게 된다.
   * 강아지 문맥: 귀여운, 개, 놀이터, 애견, 등
   * 법률 문맥: 판사, 변호사, 검사, 소송, 법정 등
* [한국어 word2vec 예시](https://w.elnn.kr/search)   
* **CBOW (Continuous Bag of Words)**
   * 주변 단어들로 중심 단어 예측
   * 목적 함수: $\max \sum_{w \in V} \log P(w|context(w))$
   * 기본 구조
      * **목표**: 주변 단어들을 보고 가운데 단어를 맞추기

      ```
      입력: [나는] [___] [먹었다]  
      출력: [사과를]
      ```

#### 수학적 모델링

* **1단계: 입력 표현**
   * 문맥 단어들: $w_{-2}, w_{-1}, w_{+1}, w_{+2}$ (윈도우 크기 2)
   * 각 단어의 원-핫 벡터: $\mathbf{x}_{w} \in \{0,1\}^V$ (V = 어휘 크기)

* **2단계: 임베딩 변환**
   * $\mathbf{v}_w = \mathbf{W}_{\text{in}} \mathbf{x}_w$
   * 여기서:
      * $\mathbf{W}_{\text{in}} \in \mathbb{R}^{d \times V}$: 입력 임베딩 행렬
      * $d$: 임베딩 차원 (예: 300)
      * $\mathbf{v}_w \in \mathbb{R}^d$: 단어의 임베딩 벡터
   * **직관적 해석**: 
      * $\mathbf{W}_{\text{in}}$ 는 "단어 ID → 의미 벡터" 변환표
      * 원-핫 벡터와의 곱은 단순히 해당 행을 선택하는 것

* **3단계: 문맥 벡터 계산**
   * $\mathbf{h} = \frac{1}{C} \sum_{c \in \text{context}} \mathbf{v}_c$
   * **직관적 해석**:
      * 주변 단어들의 평균 벡터
      * "이 위치에 올 수 있는 단어의 특징"을 나타냄
* **4단계: 출력 확률 계산**
   * $P(w_{\text{center}}|\text{context}) = \frac{\exp(\mathbf{u}_{w_{\text{center}}}^T \mathbf{h})}{\sum_{w'=1}^V \exp(\mathbf{u}_{w'}^T \mathbf{h})}$
   * **직관적 해석**:   
      * 분자: 정답 단어가 이 문맥에 얼마나 적합한지
      * 분모: 모든 단어 중에서 정규화 (확률의 합 = 1)

   * 예시
      * **예시 문장**: "나는 사과를 먹었다"

      ```
      Step 1: 문맥 = ["나는", "먹었다"], 정답 = "사과를"
      Step 2: 현재 모델이 "바나나를" 높은 확률로 예측
      Step 3: 손실 계산 → "사과를"의 확률을 높이도록 가중치 업데이트
      Step 4: 반복 학습 후 → "나는 ___ 먹었다" 문맥에서 과일 단어들이 높은 확률
      ```

      * **학습 결과**:
         * 비슷한 문맥에 나타나는 단어들 → 비슷한 벡터
         * "사과", "바나나", "딸기" → 가까운 위치의 벡터

#### **CBOW:**

* (생략 - 구버전이라 알필요없다고 판단) 

#### **Skip-gram:**

* CBoW보다 Skip-gram이 더 좋은 성능을 보임
* 구버전(중심 단어로 주변 단어들 예측)과 신버전(주변 단어들로 중심 단어 예측)으로 나뉨      
* 중심 단어로 주변 단어들 예측, window size (중심 단어를 중심으로 앞뒤 몇 개의 단어를 문맥으로 설정) 존재.
* window가 슬라이딩하면서 모든 단어에 대해 중심 단어로 주변 단어들 예측
* window size가 크면 (7~25) 문맥이 넓어지고 작으면(2~7) 문맥이 좁아짐
* 윈도우 사이즈가 작으면 상호 교환할 수 있을 정도의 높은 유사도를 가진다.
* 여기서 상호 교환이 가능하다는 것은 반의어도 포함될 수 있다.
* 반면, 윈도우 사이즈가 크면 관련 단어들을 군집하는 효과가 있다.
* 목적 함수: $\max \sum_{w \in V} \sum_{c \in context(w)} \log P(c|w)$
* 특징:
   * 선형 관계 학습: $\vec{king} - \vec{man} + \vec{woman} \approx \vec{queen}$
   * 속도가 빠름
   * 단어별 고정된 하나의 벡터
* 구버전 예시

```
문장: Thou shalt not make a machine in the likeness of a human mind.
Input word: [not] 일 때,
Target word: [thou], [shalt], [make], [a]
따라서, 4개의 다중 클래스 분류 문제로 딥러닝을 써서 학습

"중심 단어로 문맥 예측하기": 슬라이딩 하면서 중심단어를 바꿔가며 학습
Input words: [Thou], [shalt], [not], [make], [a], [machine], [in], [the], [likeness], [of], [a], [human], [mind]

예를 들어, 중심 단어가 [not]일 때, 주변 단어들은 [thou], [shalt], [make], [a]가 되도록 학습
softmax 와 cross entropy 손실 함수를 사용하여 학습
input layer, projection layer, output layer 3개의 층으로 구성된 신경망
소프트맥스 함수를 지난 예측값과 실제값으로부터 오차를 구한다.
하지만 이 구버전으로 사용하지 않는 방법이다.
그 이유는 속도가 너무 느리고 단어 집합의 크기에 대해서 softmax + cross entropy 손실 함수를 사용하면 연산이 너무 무겁다.
통상적으로, 단어 집합의 크기는 일반적으로 수 만개 이상이다.
```

* 신버전 예시

```
Skip-gram with Negative Sampling (SGNS)
다중 클래스 분류를 이진분류 문제로 바꾸어 연산량을 줄임
중심단어와 주변 단어를 입력값으로 주고 이 둘의 내적으로부터 0,1로 이진 분류: 1이면 이웃단어, 0이면 비이웃단어
중심 단어 데이터셋에 레이블을 1로 할당하고 Negative Sampling을 통해 비이웃 단어 샘플들도 추가
Negative Sampling: 비이웃 단어 샘플들을 전체 데이터셋에서 랜덤하게 추출한다.
2개의 Embedding Table을 사용
   * 하나는 임베딩 테이블 (size = 단어의 개수를 행으로 갖고 embedding dimension을 열로 갖는 행렬): 임베딩 디멘션은 사용자가 지정
   * 하나는 주변 단어 정보를 갖는 context 테이블 (임베딩 테이블의 동일한 크기를 갖는 테이블)

문장: Thou shalt not make a machine in the likeness of a human mind.
Input word: [not, not, not] = [0.2, 0.2, 0.2]
output words: [thou, taco, you] = [0.1, 0.7, -0.6]
target = [1, 0, 0]
input x output = [0.02 0.14 -0.12]
sigmoid(x) = [0.505 0.535 0.47]
error = target - sigmoid(x) = [1-0.505, 0-0.535, 0-0.47] = [0.495, -0.535, -0.47]
역전파를 통해 error를 최소화하는 input word와 output word의 임베딩 벡터를 학습

embedding vector의 차원을 정하는 것은 사용자의 몫
Negative Sampling의 비율 또한 성능에 영향을 주는 결정 요소
논문에서는 5-20을 최적의 숫자로 제안한다.
데이터가 방대하다면 2-5로도 충분하다.
```

* 수학적 모델
* 목적 함수: $\max \sum_{w \in V} \sum_{c \in context(w)} \log P(c|w)$
* 단계별 해석:
   * **1단계**: $w \in V$ (모든 단어에 대해)
      * 말뭉치의 모든 단어를 중심 단어로 한 번씩 사용
   * **2단계**: $c \in context(w)$ (각 중심 단어의 모든 문맥 단어에 대해)
      * 윈도우 크기만큼 주변 단어들을 문맥으로 설정
   * **3단계**: $\log P(c|w)$ (확률의 로그값)
      * 중심 단어 w가 주어졌을 때 문맥 단어 c가 나타날 확률
* 예시
   * **문장**: "나는 사과를 정말 좋아한다" (윈도우 크기 = 2)

   ```
   중심 단어: "사과를"
   문맥 단어들: ["나는", "정말", "좋아한다"] (앞뒤 2개씩)

   Skip-gram이 학습하는 것:
   P("나는"|"사과를")     → 높아야 함
   P("정말"|"사과를")     → 높아야 함  
   P("좋아한다"|"사과를") → 높아야 함
   P("컴퓨터"|"사과를")   → 낮아야 함 (문맥에 없음)
   ```

* 확률 계산: $P(c|w) = \frac{\exp(\mathbf{u}_c^T \mathbf{v}_w)}{\sum_{c'=1}^V \exp(\mathbf{u}_{c'}^T \mathbf{v}_w)}$
   * $\mathbf{v}_w$ : 중심 단어 w의 입력 벡터 (우리가 원하는 임베딩)
   * $\mathbf{u}_c$ : 문맥 단어 c의 출력 벡터
   * $\mathbf{u}_c^T \mathbf{v}_w$ : 두 단어가 "같이 나타날 가능성" 점수
   * Softmax로 정규화하여 확률로 변환
* 학습 과정 예시
   * 초기 상태 (랜덤 벡터)

      ```python
      v_사과 = [0.1, -0.2, 0.3]  # 중심 단어 벡터
      u_나는 = [0.4, 0.1, -0.1]  # 문맥 단어 벡터
      u_컴퓨터 = [-0.2, 0.5, 0.2]
      ```

   * 점수 계산

      ```python
      score_나는 = dot(u_나는, v_사과) = 0.4*0.1 + 0.1*(-0.2) + (-0.1)*0.3 = -0.01
      score_컴퓨터 = dot(u_컴퓨터, v_사과) = -0.2*0.1 + 0.5*(-0.2) + 0.2*0.3 = -0.08
      ```

   * 확률 계산 (간단히)

      ```python
      P(나는|사과) = exp(-0.01) / (exp(-0.01) + exp(-0.08) + ...) ≈ 0.3
      P(컴퓨터|사과) = exp(-0.08) / (exp(-0.01) + exp(-0.08) + ...) ≈ 0.2
      ```

   * 학습 업데이트
      * 목표: P(나는|사과)는 높이고, P(컴퓨터|사과)는 낮추기

      ```python
      # 실제로 "나는"이 문맥에 있었으므로
      # v_사과와 u_나는을 더 비슷하게 만들기
      v_사과 += learning_rate * u_나는  # 벡터를 가까워지게
      u_나는 += learning_rate * v_사과

      # "컴퓨터"는 문맥에 없었으므로  
      # v_사과와 u_컴퓨터를 더 멀게 만들기
      v_사과 -= learning_rate * u_컴퓨터  # 벡터를 멀어지게
      u_컴퓨터 -= learning_rate * v_사과
      ```

* Word2Vec: 단어 수 × 벡터 차원


# 결론

본 문서에서는 자연어 처리(NLP) 분야에서 단어의 의미를 효과적으로 벡터 공간에 표현하기 위한 핵심적인 신경망 기반 방법론인 Word2Vec을 살펴보았다. Word2Vec은 '분포 가설'에 착안하여, 단어가 사용되는 문맥을 학습함으로써 단어 간의 의미론적 유사성과 관계를 밀집 벡터 형태로 포착한다.

*   **Word2Vec의 핵심**:
    *   **CBOW와 Skip-gram**: Word2Vec은 주변 단어로부터 중심 단어를 예측하는 CBOW 방식과 중심 단어로부터 주변 단어를 예측하는 Skip-gram 방식을 통해 단어 임베딩을 학습한다. 각 방식은 서로 다른 관점에서 문맥 정보를 활용하며, 특히 Skip-gram은 Negative Sampling과 같은 최적화 기법을 통해 대규모 데이터셋에서도 효율적으로 학습할 수 있다.
    *   학습된 임베딩 벡터는 단어의 의미적 유사성(예: '왕' - '남자' + '여자' ≈ '여왕')을 벡터 공간에서의 관계로 나타낼 수 있으며, 이는 정적 임베딩 방식의 중요한 성과이다.
*   **Word2Vec의 의의**:
    *   Word2Vec은 단어의 의미를 고정된 벡터로 표현하는 정적 임베딩 방식의 대표적인 예로, 이후 등장하는 다양한 문맥 기반 동적 임베딩 방법론들의 중요한 기초가 되었다.
    *   이러한 신경망 기반의 단어 표현 방식은 기존의 통계 기반 방법론들의 한계를 극복하고 NLP 분야의 성능을 크게 향상시키는 데 기여했으며, 현대 대규모 언어 모델(LLM) 발전의 핵심적인 토대를 마련했다.

선택하는 벡터화 전략은 해결하고자 하는 문제의 특성, 데이터의 규모와 성격, 그리고 사용하려는 모델의 요구사항을 종합적으로 고려해야 하지만, Word2Vec은 그 자체로도 여전히 유용하며, 더 복잡한 모델을 이해하는 데 있어 중요한 개념적 기반을 제공한다.
