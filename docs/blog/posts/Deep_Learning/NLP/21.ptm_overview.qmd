---
title: "사전 학습 모델의 발전"
subtitle: "ELMo부터 LLaMA까지: 현대 NLP의 혁신적 변화"
description: |
  ELMo, BERT, GPT, T5, LLaMA 등 주요 사전 학습 모델들의 발전 과정과 핵심 원리를 다룬다. 문맥 기반 임베딩부터 대규모 언어 모델까지, 각 모델의 혁신적 기여와 특징을 상세히 설명한다.
categories:
  - NLP
  - Deep Learning
author: Kwangmin Kim
date: 2025-01-21
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False # 발행 시 False로 변경
---

# 요약

이 문서는 자연어 처리 분야에서 패러다임 전환을 이끈 **사전 학습 모델(Pre-trained Model)**들의 발전 과정과 핵심 원리를 탐구한다. 2015년 Google의 LSTM 사전 학습 실험부터 2023년 LLaMA까지, 각 모델이 가져온 혁신적 변화와 기술적 특징을 상세히 설명한다.

주요 내용은 다음과 같다:

* **사전 학습의 개념과 발전**:
  - 초기에는 Word2Vec, GloVe 같은 정적 임베딩에서 시작
  - Google의 LSTM 사전 학습 실험(2015)이 사전 학습의 효과를 입증
  - 대규모 데이터로 미리 학습한 모델이 무작위 초기화보다 우수한 성능 확인

* **문맥 기반 임베딩의 등장**:
  - **ELMo (2017)**: BiLSTM 기반 양방향 문맥 임베딩의 선구자
  - 동일한 단어라도 문맥에 따라 다른 벡터 표현 생성
  - "bank"가 금융기관과 강가에서 서로 다른 의미로 표현되는 혁신

* **Transformer 아키텍처의 혁명**:
  - **Transformer (2017)**: Self-Attention과 Position Encoding으로 순차 처리 방식 탈피
  - 병렬 처리 가능, 장거리 의존성 포착 능력 향상
  - 현대 모든 대규모 언어 모델의 기초 구조 제공

* **특화 모델들의 분화**:
  - **GPT (2018)**: Transformer 디코더 기반 생성 특화 모델
  - **BERT (2018)**: Transformer 인코더 기반 이해 특화 모델
  - **BART (2019)**: 인코더-디코더 결합으로 이해와 생성 모두 강화
  - **T5 (2020)**: 모든 NLP 태스크를 텍스트-투-텍스트로 통합

* **최신 발전 동향**:
  - **LLaMA (2023)**: 효율적인 대규모 언어 모델의 새로운 표준
  - **UL2 (2023)**: 다양한 사전 학습 방식의 융합
  - **FLAN (2022)**: Instruction Tuning을 통한 명령어 이해 능력 강화

각 모델의 핵심 아이디어, 학습 방식, 활용 분야를 통해 현대 NLP 기술의 발전 궤적과 미래 방향성을 이해할 수 있다.

# 텍스트 인코딩 및 벡터화

```
RNN Language Model
├── Seq2Seq
├── Beam Search
├── Subword Tokenization
├── Attention
├── Transformer Encoder (Vaswani et al., 2017)
|   ├── Positional Encoding
|   ├── Multi-Head Attention
|   └── Feed Forward Neural Network
|
├── Transformer Decoder (Vaswani et al., 2017)
|
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
|
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
|
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등
```

# 사전 학습 모델 (Pre-trained Model)

* 원래는 언어모델보다는 word embedding에서 사용되던 개념이었음
* 사전 훈련된 임베딩 (Word2Vec, GloVe)은 대용량 텍스트의 단어들의 동시 등장 통계로부터 훈련시키는 방법
* 미리 학습시켜 놓은 모델을 가리고 새로운 문제를 풀었었음
* Google의 LSTM 사전학습 실험 (Semi-Supervised Sequence Learning, 2015)
   * LSTM 언어 모델을 사전 학습한 후에 텍스트 분류에 적용해봄
   * LSTM을 사전 학습하지 않은 상태에서 텍스트 분류를 학습한 것과 성능 비교
   * 사전 학습된 LSTM이 random 초기화된 LSTM보다 성능이 좋았음

## ELMo(Embeddings from Language Models. 2017)

* 사전 학습된 LSTM 언어 모델 2가지를 결합하여 좋은 임베딩 벡터값을 얻는 방법론
* 사전 학습된 언어 모델이 NLP에서 좋은 성능을 얻을 수 있다는 강한 인상을 줌
* ELMo는 문장에서 단어의 의미를 상황에 맞게 다르게 표현해주는 단어 임베딩 기법
* 예를 들어,
  * "He went to the bank to deposit money."
  * "She sat by the bank of the river."
  * 여기서 bank는 같은 철자지만 의미가 전혀 다르지만 기존 방식(Word2Vec, GloVe)은 이걸 같은 의미로 취급
  * 그런데 ELMo는 문맥을 보고 각각 다른 벡터로 표현
* 동작 방식
  * 문장을 왼쪽에서 읽는 모델 + 오른쪽에서 읽는 모델 두 개를 활용하여 해당 단어가 문장 속에서 어떤 의미로 쓰였는지를 분석
  * 문장을 양방향으로 읽어 단어의 문맥 정보를 파악함
    * 앞에서부터 → 순방향 LSTM
    * 뒤에서부터 → 역방향 LSTM
    * 순방향에서의 bank에 대한 hidden state와 역방향에서의 bank에 대한 hidden state를 결합하여 임베딩을 생성
  * 모든 단어의 의미는 "문맥 기반"
    * "bank"가 앞뒤에 어떤 단어들과 쓰였는지 보면서 이게 '돈 관련 은행'인지, '강가'인지 판단함
  * 그리고, 임베딩을 뽑음
    * 이렇게 판단한 의미를 벡터(숫자 집합)로 표현
* ELMo는 등장하자마자 기존 NLP 모델들의 정확도를 확 뛰어넘었다
* 개체명 인식(NER), 질의응답(QA), 문장 분류 등에 광범위하게 쓰였다
* 지금은 BERT, GPT 같은 트랜스포머가 주도하고 있지만, ELMo는 문맥 기반 임베딩의 출발점이었음

## Transformer (2017)

* 기존의 RNN, LSTM과 달리 **순서를 따라 처리하지 않고**, 문장 전체를 **한꺼번에 보고** 이해할 수 있도록 만든 모델이다.
* Transformer는 두 가지 핵심 구조
   * 1. **Self-Attention**
      * 문장 안에서 **각 단어가 다른 단어들과 얼마나 중요한 관계가 있는지를 계산**한다.
      * 예를 들어, "The cat sat on the mat"에서 "sat"와 "cat" 사이의 연결이 중요하다면, 모델은 그 둘의 관계를 더 강하게 본다.
   * 2. **Position Encoding**
      * Transformer는 순서를 따라 읽지 않기 때문에, **각 단어의 위치 정보**를 따로 추가해줘야 한다.
      * 위치 정보를 더해줘서 "첫 번째 단어", "두 번째 단어" 등의 순서를 인식하게 한다.
* Transformer의 구성요소
  * **인코더(Encoder)**: 입력 문장을 이해하고 벡터로 변환함
  * **디코더(Decoder)**: 그 벡터를 바탕으로 결과(예: 번역, 요약 등)를 생성함
  * 예를 들어, 영어 문장을 프랑스어로 번역하는 경우,
    * 인코더는 영어 문장을 이해하고
    * 디코더는 그것을 프랑스어로 바꾸어 생성한다.
* Transformer는 다음과 같은 이유로 혁신적이다:
   * **병렬처리 가능**: RNN처럼 순서대로 처리하지 않기 때문에 연산 속도가 빠르다.
   * **문맥 파악 능력 향상**: 문장의 멀리 떨어진 단어들 간의 관계도 잘 이해한다.
   * **기반 기술로 발전**: BERT, GPT, T5, LLaMA 등 오늘날의 대부분의 대형 언어모델은 Transformer 기반이다.
* 오늘날 대부분의 LLM은 이 구조를 바탕으로 하고 있다.
* ELMo는 RNN 구조 기반이고,
  * Transformer는 그 한계를 뛰어넘기 위해 등장한 구조라는 점에서
  * 이 둘의 **패러다임 자체가 다르다**는 것도 같이 기억해두면 좋겠다.

## GPT(Generative Pre-trained Transformer. 2018)

* GPT는 문장을 생성하는 모델이다.
* 기존 모델들이 문장을 이해하는 데 집중했다면, GPT는 문장을 생성하는 데 집중한다.
* OpenAI는 Google의 Transformer을 보고 STM이 사전 학습되어 사용되면 성능이 좋은 것을 확인.  
* Transformer의 디코더 구조를 분석하고 다음 단어를 예측하는 모듈에 해당하는 디코더로 사전 학습 언어 모델을 구현했다.
* Transformer의 encoder를 버리고 decoder만 사용하여 문장을 생성하는 모델을 만들었다.
* 동작 방식
  * GPT는 **Transformer의 디코더 구조만** 사용한다.
  * 즉, 문장을 생성하는 데 특화되어 있으며,
  * 문장을 이해하는 인코더 구조는 없다.
  * 핵심 동작 방식
    * **좌→우 방향의 언어 생성 학습**
      * 문장을 왼쪽에서 오른쪽으로 읽으면서,
      * 다음에 올 단어를 예측하는 방식으로 학습한다.
      * 예:
        * 입력: "I want to eat"
        * 목표: 다음 단어가 무엇일까? → "pizza"
        * 이런 식으로 엄청나게 많은 문장 데이터를 통해 **패턴을 학습**한다.
   * **사전학습 후 활용**
     * 대규모 텍스트로 먼저 학습(Pre-training)
     * 이후 별도 작업(챗봇, 작문, 번역 등)에 맞게 사용(Fine-tuning or Prompting)
* 특징
   * **텍스트 생성 능력**이 매우 뛰어남
   * 다양한 태스크를 **명시적 미세조정 없이 프롬프트만으로 해결 가능**
     * 이게 GPT 계열 모델의 큰 장점이다 (Few-shot, Zero-shot, etc.)
   * **문장 완성, 요약, 번역, 창작, 대화 등**
     * 생성형 작업에 모두 강하다
* **GPT는 문장을 생성하는 데 특화된 모델**이다.


## BERT(Bidirectional Encoder Representations from Transformers. 2018)

* Google이 OpenAI의 GPT 모델을 보고 반대로 문장을 이해하는 데 집중하는 모델을 만들었다.
* Transformer의 인코더 구조를 사용하여 문장을 이해하는 모델을 만들었다.
* NLU가 특화된 부분이기 때문에 Text 분류에서 GPT보다 더 나은 성능을 보인다.
* BERT는 문장을 **양방향으로 이해하는** 언어 모델이다.
* 기존 모델들이 **왼쪽에서 오른쪽** 혹은 **오른쪽에서 왼쪽**으로만 문장을 해석했다면,
* BERT는 문장을 **양쪽 방향에서 동시에** 해석한다.
* 그래서 더 깊은 문맥 이해가 가능하다.
* 동작 방식
  * BERT는 **Transformer의 인코더 구조만** 사용한다.
  * 즉, 문장을 이해하고 벡터로 바꾸는 데 특화되어 있으며,
  * 문장을 새로 생성하는 디코더 구조는 없다.
  * 핵심 동작 방식은 두 가지 학습 방식으로 이루어진다:
    * **Masked Language Modeling (MLM)**
      * 입력 문장 중 일부 단어를 가려놓고, 그 단어가 무엇인지 맞히도록 학습한다.
      * 예: "The cat sat on the \[MASK]." → 모델은 'mat'이라고 예측해야 한다.
      * 이를 통해 문장의 앞뒤 **모든 문맥**을 참고해서 단어를 이해하는 법을 배운다.
    * **Next Sentence Prediction (NSP)**
      * 두 문장을 입력한 뒤, 두 번째 문장이 첫 번째 문장의 **진짜 다음 문장인지** 판단하게 학습한다.
      * 예:
        * 문장1: "She opened the door."
        * 문장2: "She picked up the package." → 연결된 문장 (True)
        * 문장2: "The sun is a star." → 무관한 문장 (False)
      * 문장 간의 관계 이해 능력을 키우기 위한 학습이다.
* 문맥 기반 단어 임베딩 제공
  * 같은 단어라도 문맥에 따라 다른 벡터로 표현됨
* 사전학습 + 미세조정 구조 (Pretraining + Fine-tuning)
  * 대규모 텍스트로 미리 학습해두고,
  * 이후 실제 태스크(NER, 분류, QA 등)에 맞게 추가 학습만 하면 됨.
* BERT는 다양한 NLP 태스크에서 **기록적인 성능 향상**을 이끌었다.
* 이후 등장한 RoBERTa, ALBERT, DistilBERT, DeBERTa 등은 BERT의 변형이다.
* 현재 GPT 계열이 생성에 특화되어 있다면,
  * **BERT는 이해(이해 기반 태스크)에 특화된 모델**이라고 보면 된다.
* BERT는 **Transformer 인코더 기반의 양방향 문맥 이해 모델**이다.
* **단어를 문맥 안에서 정확하게 해석**하기 위해 만들어졌다.
* **문장 분류, 질의응답, 개체명 인식 등 NLP의 다양한 작업에 폭넓게 활용**된다.

## BART(Bidirectional and Auto-Regressive Transformers. 2019)

* Encoder-Decoder 가 결합된 구조를 사용하여 문장을 이해하고 생성하는 모델을 만들었다.
* BART는 **BERT + GPT의 장점**을 결합한 모델로, **이해와 생성 모두에 강한 모델**이다.
* BART는 **문장을 이해하고 → 그것을 기반으로 문장을 생성하는 모델**이다.
* 즉, **BERT처럼 입력을 양방향으로 이해**하고, **GPT처럼 자연스럽게 문장을 생성**한다.
* 구조적으로는 **Transformer 인코더 + 디코더**를 모두 사용한다.
* 쉽게 말해 **BERT + GPT를 합친 하이브리드 모델**이다.
* 동작 방식
   * **Denoising Autoencoder**: BART의 학습은 **"노이즈 추가 → 원문 복원"** 방식으로 이루어진다.
   * 입력 문장에 노이즈를 준다
   * 예:
      * 원래 문장: `The cat sat on the mat.`
      * 망가뜨린 문장: `The [MASK] on the mat.` 또는 `sat the mat on the cat.` (순서 뒤섞기)
   * 모델은 이 망가진 문장을 보고 **원래 문장을 복원**한다. 즉, 문장을 이해하고, 적절한 형태로 **다시 생성**할 수 있어야 한다.
   * 이 과정을 통해 BART는 **이해 능력**과 **생성 능력**을 동시에 학습하게 된다.
* 구성
  * **인코더**는 BERT처럼 **양방향 문맥 이해**
  * **디코더**는 GPT처럼 **왼쪽→오른쪽 순서대로 문장 생성**
* 이 구조 덕분에 **복잡한 입력을 해석하고**, 그에 맞는 **정확하고 자연스러운 출력**을 만들어낼 수 있다.
* 활용 분야
  * BART는 다음과 같은 **생성 기반 작업**에 특히 강하다:
    * 텍스트 요약 (Summarization)
    * 문장 생성 (Text Generation)
    * 기계 번역 (Machine Translation)
    * 문법 오류 수정 (Grammatical Error Correction)
    * 질문 생성 (Question Generation)
* 특히 **요약 모델로서 매우 뛰어난 성능**을 보여주었고, Facebook AI에서 개발한 이후 HuggingFace에서도 적극적으로 채택되었다.


## T5 (Text-to-Text Transfer Transformer. 2020)

* T5는 모든 NLP 문제를 텍스트 → 텍스트 문제로 바꾸자는 발상에서 출발했다.
* 즉, 입력도 텍스트, 출력도 텍스트로 통일된 프레임워크를 사용한다.
* T5는 구글이 제안한 범용 NLP 모델이다.
* 자연어처리에서 벌어지는 거의 모든 작업을 **텍스트 입력 → 텍스트 출력**으로 통합하는 것이 핵심이다.
* 요약, 번역, 문장 분류, 질문 생성, 질의 응답 등 모두 동일한 구조에서 처리 가능하다.
* 동작 방식
   * T5는 BART처럼 **Transformer 인코더 + 디코더 구조**를 사용한다.
   * 하지만 BART와 달리, **모든 태스크를 통일된 방식으로 표현**하는 철학이 핵심이다.
* 예시:
   * 문장 분류
      * 입력: `"sst2 sentence: I love this movie."`
      * 출력: `"positive"`
   * 문장 요약
      * 입력: `"summarize: The cat sat on the mat. It was sleepy."`
      * 출력: `"The cat was sleepy."`
   * 질의 응답
      * 입력: `"question: Where is the Eiffel Tower? context: The Eiffel Tower is in Paris."`
      * 출력: `"Paris"`
   * 이처럼 **작업을 구분하는 태그 + 텍스트 입력**을 넣으면 **디코더가 원하는 정답 텍스트를 생성**한다.
* 사전학습 방식
  * T5도 BART처럼 **Denoising Autoencoder** 방식으로 학습된다.
  * 하지만 T5는 자체적으로 만든 **Span Corruption**이라는 방식을 쓴다:
  * 문장에서 일부 구간(span)을 가리고, 그 구간을 `<extra_id_0>`, `<extra_id_1>` 같은 토큰으로 대체
  * 모델이 이 빈칸들을 복원하게 함
  * 이 과정을 통해 **문장 이해 + 생성 능력**을 동시에 기른다.
* 특징
  * **모든 NLP 태스크를 텍스트 → 텍스트 문제로 통일**
  * 다양한 태스크를 하나의 모델로 처리 가능
  * 프롬프트 기반으로 유연하게 태스크 전환
  * 학습 구조는 BART와 유사하지만, **설계 철학은 더 범용적**임
* **T5는 BART와 구조는 유사하나, 태스크 프레임워크가 다르다.**
* **모든 입력과 출력을 텍스트로 다루며, 태스크 이름을 붙여 명시함**
* 따라서 여러 NLP 작업을 하나의 파이프라인에서 처리할 수 있다.
* 대표적인 범용 자연어 처리 모델 중 하나로, 후속 버전으로 T5.1.1, UL2, FLAN-T5 등이 있다.


## LLaMA(Large Language Model Meta AI. 2023)

* LLaMA는 Meta(구 Facebook)에서 제안한 **대규모 언어 모델 시리즈**이다.
* 이름은 **Large Language Model Meta AI**의 약자이다.
* LLaMA는 범용 텍스트 생성 능력을 목표로 하는 **GPT 계열 디코더 기반 모델**이다.
* 고성능 언어 모델을 **비교적 작은 파라미터 수로도 구현 가능**하다는 점을 증명하고자 설계되었다.
* 논문 및 모델은 공개되어 **학계, 오픈소스 커뮤니티에서 널리 활용되고 있음**.
* LLaMA는 1\~2단계 모델 학습을 통해 \*\*사전학습된 기반 모델 (Base LM)\*\*만 제공하며,
  **대화, 요약, 추론 등에 맞게 파인튜닝은 사용자가 직접 수행**하도록 설계되어 있다.
* 동작 방식
  * LLaMA는 GPT처럼 **Transformer 디코더 구조**만 사용한다.
  * 입력을 **왼쪽에서 오른쪽**으로 읽으며 **다음 토큰을 예측**하는 방식으로 작동한다.
  * 학습 시점에 **자기회귀 언어모델 (Autoregressive Language Modeling)** 방식 사용.
* 특징
  * GPT처럼 텍스트 생성 중심의 모델이지만,
    **효율성과 학습 품질 향상에 집중된 다양한 설계 전략**을 포함한다.
  * 예:
    * **Norm 위치 변경 (Pre-normalization)**
    * **GEGLU 활성화 함수**
    * **더 긴 시퀀스 학습 (최대 2,048 토큰)**
    * **높은 품질의 텍스트 코퍼스만 선별하여 학습**
  * 성능 대비 **모델 크기 효율이 매우 우수**하여,
    작은 파라미터 수로도 **GPT-3 수준의 성능**을 낼 수 있음.
* 버전별 주요 모델
  * **LLaMA 1 (2023 초)**
    * 파라미터 크기: 7B, 13B, 33B, 65B
    * 공개 후 오픈소스 생태계에서 광범위한 활용이 시작됨
  * **LLaMA 2 (2023 중반)**
    * 성능 개선 및 다양한 크기: 7B, 13B, 70B
    * **LLaMA 2-Chat**: 대화용으로 미세조정된 모델
  * **LLaMA 3 (2024 출시)**
    * Meta가 직접 **SOTA급 성능**을 표방하며 출시
    * 8B, 70B 모델 제공, 대화형 fine-tuning 포함
    * 상용 사용 가능, HuggingFace 등에서 공개됨
* 활용 방식
  * LLaMA는 **기반 모델만 제공**하기 때문에,
    대화형 모델로 쓰려면 **Alpaca, Vicuna, OpenChat, Zephyr** 등의
    **LoRA 파인튜닝** 모델을 함께 사용하는 경우가 많다.
  * 특히 LLaMA는 **프롬프트 기반 제어**, **추론**, **코드 생성** 등
    다양한 태스크에서 활용 가능하며,
    **고품질 오픈소스 AI 개발의 중심**으로 자리 잡았다.
* LLaMA는 GPT 계열의 디코더 언어 모델이다.
* **작은 모델 크기로도 높은 성능**을 발휘할 수 있도록 최적화됨.
* 다양한 오픈소스 프로젝트에서 핵심 기반 모델로 활용됨.
* 후속 시리즈인 **LLaMA 2, 3**는 대화형 파인튜닝 모델과 함께 상용 및 학술용으로 널리 쓰이고 있음.

좋다. 이번에는 **UL2**와 **FLAN**에 대해 각각 T5 형식에 맞춰 설명하겠다.
둘 다 **구글이 T5 이후에 개발한 모델 또는 학습 기법**이며,
**텍스트 생성과 이해를 모두 강화**하기 위한 방향성을 가진다.

## FLAN (Fine-tuned LAnguage Net, 2022)

* **FLAN은 "Instruction Tuning"의 대표적 구현**으로,
  **UL2와 같은 사전학습된 언어 모델에 다양한 명령어(Task Prompt)를 학습시키는 과정**이다.
* FLAN은 T5 또는 UL2-T5에 적용되어 등장했으며,
  FLAN-T5, FLAN-UL2 같은 이름으로 모델이 배포된다.
* 목표는 **명령어(prompt)를 이해하고 정확히 수행하는 능력 강화**이다.
* 동작 방식
  * 먼저 T5 또는 UL2-T5 모델을 준비한다.
  * 그런 다음 **Instruction Tuning**을 수행한다.
    → 다양한 NLP 작업(요약, 번역, 추론, QA 등)을 명시적인 지시문 형식으로 학습
* 예시:
  * 입력: `"Translate English to French: I am happy."`
  * 출력: `"Je suis heureux."`
  * 입력: `"Summarize: The sun is hot. It rises in the east."`
  * 출력: `"The sun is hot and rises in the east."`
* 특징
  * 다양한 명령어와 태스크를 학습시켜, **프롬프트 이해 능력을 대폭 향상**시킴
  * **Zero-shot / Few-shot** 능력이 크게 향상됨
  * 단순한 텍스트 생성 모델이 아닌 **명령어 기반의 범용 도우미로 진화**
* 요약
  * FLAN은 모델이 아니라 **Instruction tuning 과정 또는 결과 모델 이름**이다.
  * 일반 언어모델에 **명령어 기반 태스크 학습을 추가한 것**
  * 대표적인 모델: **FLAN-T5**, **FLAN-UL2**
  * 현재 Gemini, PaLM, ChatGPT 등의 **Instruction Following 능력의 기초가 된 전략**

## UL2(Unifying Language Learning. 2023)

* **UL2는 "Unifying Language Learning"의 약자**로, 구글이 제안한 새로운 **사전학습 방식**이다.
* 목적은 기존 T5, BERT, GPT 계열 모델들의 **단점을 보완하고 장점을 융합**하는 것에 있다.
* **이해 중심 태스크와 생성 중심 태스크 모두에 잘 작동**하는 범용 사전학습 전략이다.
* UL2는 기존 T5 구조를 사용하지만, **학습 방식(프리트레이닝)이 완전히 다르다.**
* 동작 방식
  * UL2는 한 가지 방식이 아닌 **세 가지 프리트레이닝 모드**를 혼합하여 학습한다:
    1. **R-denoising (Random span masking)**: T5처럼 일부 span을 가리고 복원
    2. **X-denoising (Extreme masking)**: 전체 문장을 거의 다 가리고 생성
    3. **Causal LM**: GPT처럼 왼쪽에서 오른쪽으로 생성 (Autoregressive)
  → 이 세 가지 모드를 비율에 따라 섞어 학습시킴 (Multi-task 사전학습)
* 예시:
  * 입력: `fill in the blanks: The <extra_id_0> sat on the <extra_id_1>.`
  * 출력: `cat`, `mat`
  * 또는 GPT처럼 입력: `"The cat sat on"` → 출력: `" the mat."`
* 특징
  * **세 가지 방식의 장점을 융합**하여
    → 문장 이해 + 생성 모두에 강함
  * 기존 T5는 디코더에서도 마스킹된 부분을 전부 본다 (비자연스러움)
    → UL2는 자연스러운 생성을 위해 **Causal 방식도 병행**
  * 다양한 태스크에 잘 작동하도록 설계됨
* 요약
  * UL2는 모델이 아니라 **사전학습 전략**이다.
  * 기존 Transformer 구조에 적용할 수 있음 (예: T5에 적용하면 UL2-T5)
  * **다양한 프리트레이닝 모드를 섞어 범용성과 자연스러움 극대화**
  * 이후 FLAN 및 다양한 구글 LLM 개발의 기반이 됨

## 결론

사전 학습 모델의 발전은 자연어 처리 분야에서 가장 중요한 패러다임 변화 중 하나다. 2015년 Google의 LSTM 실험부터 시작된 이 여정은 현재 우리가 사용하는 ChatGPT, Claude, Gemini 등 모든 대규모 언어 모델의 토대가 되었다.

* **기술적 진화의 핵심**:
  - **정적 → 동적**: Word2Vec에서 ELMo로의 전환으로 문맥 기반 표현 실현
  - **순차 → 병렬**: Transformer의 등장으로 Self-Attention 기반 병렬 처리 가능
  - **단일 → 통합**: T5의 텍스트-투-텍스트 프레임워크로 모든 NLP 태스크 통합
  - **일반 → 특화**: GPT(생성), BERT(이해), BART(양방향) 등 목적별 특화

* **패러다임의 변화**:
  - **Pre-training + Fine-tuning**: 대규모 데이터로 사전 학습 후 특정 태스크 미세조정
  - **Transfer Learning**: 학습된 지식을 다양한 하위 태스크로 전이
  - **Few-shot Learning**: GPT 계열에서 보여준 예시 기반 학습 능력
  - **Instruction Following**: FLAN으로 대표되는 명령어 이해 및 수행 능력

* **각 모델의 독특한 기여**:
  - **ELMo**: 문맥 기반 임베딩의 가능성 입증
  - **Transformer**: 현대 AI의 기초 아키텍처 제공
  - **BERT**: 양방향 문맥 이해의 혁신적 접근
  - **GPT**: 생성형 AI와 In-context Learning의 선구자
  - **T5**: 통합 프레임워크를 통한 범용성 확보
  - **LLaMA**: 효율성과 성능의 균형점 제시

* **현재와 미래의 의미**:
  - 이들 모델은 단순한 기술적 발전을 넘어 AI와 인간의 상호작용 방식을 근본적으로 변화시켰다
  - ChatGPT의 성공으로 이어진 생성형 AI 붐의 기술적 토대 제공
  - 언어 이해와 생성 능력의 비약적 향상으로 다양한 실용적 응용 가능

* **지속되는 혁신**:
  - 모델 크기와 성능의 지속적 확장
  - 효율성과 접근성 개선을 위한 경량화 연구
  - 다중 모달(텍스트, 이미지, 음성) 통합 모델로의 발전
  - 더 나은 Instruction Following과 안전성 확보

사전 학습 모델의 발전은 여전히 진행 중이며, 각 모델이 제시한 핵심 아이디어들은 미래 AI 시스템의 기초가 되고 있다. 이러한 기술적 토대 위에서 더욱 강력하고 유용한 AI 시스템들이 계속 등장할 것으로 예상된다.
