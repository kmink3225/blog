---
title: "Active-Prompt: ë¶ˆí™•ì‹¤í•œ ì˜ˆì‹œë¥¼ ì„ ë³„í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸°"
subtitle: Active Learningì„ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì— ì ìš©í•˜ì—¬ ìµœì†Œ ì–´ë…¸í…Œì´ì…˜ìœ¼ë¡œ ìµœëŒ€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ê¸°ë²•
description: |
  Active-Promptì˜ ì •ì˜ë¶€í„° ì‹¤ì „ êµ¬í˜„ê¹Œì§€ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•œë‹¤.
  Diao et al. (2023) "Active Prompting with Chain-of-Thought for Large Language Models" ì—°êµ¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ
  Active Learningì˜ ì›ë¦¬ë¥¼ Few-shot í”„ë¡¬í”„íŒ…ì— ì ìš©í•˜ëŠ” ë°©ë²•, ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ë©”ì»¤ë‹ˆì¦˜(Self-Consistency, Entropy ê¸°ë°˜), 4ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤(ë¶ˆí™•ì‹¤ì„± ì¸¡ì •, ì„ íƒ, ì–´ë…¸í…Œì´ì…˜, ì¶”ë¡ )ë¥¼ ë¶„ì„í•œë‹¤.
  GSM8K, SVAMP, AQuA ë“± ìˆ˜í•™ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ Random Selection ëŒ€ë¹„ ìµœëŒ€ 5.2% ì„±ëŠ¥ í–¥ìƒ ê²°ê³¼ë¥¼ ì œì‹œí•˜ê³ ,
  ì–´ë…¸í…Œì´ì…˜ ë¹„ìš© 50% ì ˆê°í•˜ë©´ì„œ ì„±ëŠ¥ 2% í–¥ìƒ ë‹¬ì„± ì‚¬ë¡€, ìˆ˜í•™ ë¬¸ì œ í’€ì´/ë³µì¡í•œ ì¶”ë¡  íƒœìŠ¤í¬ ë“± ì‹¤ë¬´ ì˜ˆì‹œì™€
  ë¹„ìš©-íš¨ìœ¨ì„± íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„, ì–´ë…¸í…Œì´ì…˜ ë¶€ë‹´ ìµœì†Œí™” ì „ëµ,
  ëŠ¥ë™ì  ì˜ˆì‹œ ì„ íƒ vs ë¬´ì‘ìœ„ ì„ íƒ ë¹„êµ, ë°˜ë³µì  ê°œì„  íŒ¨í„´ì„ ì œì‹œí•œë‹¤.
categories:
  - Prompt Engineering
  - LLM
  - AI
  - Agent
author: Kwangmin Kim
date: 02/04/2025
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

## ë“¤ì–´ê°€ë©°

Few-shot í”„ë¡¬í”„íŒ…ì˜ íš¨ê³¼ëŠ” ì˜ ì•Œë ¤ì ¸ ìˆë‹¤. ëª‡ ê°œì˜ ì˜ˆì‹œë§Œìœ¼ë¡œë„ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì¤‘ìš”í•œ ì§ˆë¬¸ì´ ìˆë‹¤:

> "ì–´ë–¤ ì˜ˆì‹œë¥¼ ì„ íƒí•´ì•¼ ê°€ì¥ íš¨ê³¼ì ì¸ê°€?"

ëŒ€ë¶€ë¶„ì˜ ê²½ìš°, ì‚¬ëŒë“¤ì€ **ë¬´ì‘ìœ„ë¡œ** ì˜ˆì‹œë¥¼ ì„ íƒí•˜ê±°ë‚˜, **ì§ê´€ì ìœ¼ë¡œ** "ëŒ€í‘œì ì¼ ê²ƒ ê°™ì€" ì˜ˆì‹œë¥¼ ê³ ë¥¸ë‹¤. í•˜ì§€ë§Œ ì´ê²ƒì´ ìµœì„ ì¼ê¹Œ?

**Active-Prompt**ëŠ” Active Learningì˜ ì•„ì´ë””ì–´ë¥¼ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì— ì ìš©í•œë‹¤. ëª¨ë¸ì´ **ê°€ì¥ ë¶ˆí™•ì‹¤í•´í•˜ëŠ” ì˜ˆì‹œ**ë¥¼ ì„ ë³„í•˜ì—¬, ê·¸ ì˜ˆì‹œë“¤ì— ëŒ€í•´ì„œë§Œ ì‚¬ëŒì´ ì–´ë…¸í…Œì´ì…˜ì„ ì œê³µí•œë‹¤. ì´ë¥¼ í†µí•´ ìµœì†Œí•œì˜ ì–´ë…¸í…Œì´ì…˜ìœ¼ë¡œ ìµœëŒ€ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•œë‹¤.

## Active Learningì˜ ê¸°ë³¸ ê°œë…

### ì „í†µì  í•™ìŠµ vs Active Learning

**ì „í†µì  í•™ìŠµ (Passive Learning)**:
```
1. ì‚¬ëŒì´ ë¬´ì‘ìœ„ë¡œ ì˜ˆì‹œ ì„ íƒ
2. ëª¨ë“  ì˜ˆì‹œì— ì–´ë…¸í…Œì´ì…˜
3. ëª¨ë¸ í•™ìŠµ

ë¬¸ì œì : ì‰¬ìš´ ì˜ˆì‹œì—ë„ ë™ì¼í•œ ë…¸ë ¥ íˆ¬ì…
```

**Active Learning**:
```
1. ëª¨ë¸ì´ ë¶ˆí™•ì‹¤í•œ ì˜ˆì‹œ ì‹ë³„
2. ê·¸ ì˜ˆì‹œë“¤ë§Œ ì‚¬ëŒì´ ì–´ë…¸í…Œì´ì…˜
3. ëª¨ë¸ ì¬í•™ìŠµ
4. ë°˜ë³µ

ì¥ì : ì–´ë…¸í…Œì´ì…˜ ë¹„ìš© ìµœì†Œí™”, í•™ìŠµ íš¨ìœ¨ ê·¹ëŒ€í™”
```

### ì‹œê°ì  ë¹„êµ

```
Passive Learning:
[ì˜ˆì‹œ 100ê°œ] â†’ [ë¬´ì‘ìœ„ 20ê°œ ì„ íƒ] â†’ [ëª¨ë‘ ì–´ë…¸í…Œì´ì…˜] â†’ ì„±ëŠ¥ 85%
ë¹„ìš©: 20ê°œ ì–´ë…¸í…Œì´ì…˜

Active Learning:
[ì˜ˆì‹œ 100ê°œ] â†’ [ë¶ˆí™•ì‹¤í•œ 10ê°œ ì„ íƒ] â†’ [10ê°œë§Œ ì–´ë…¸í…Œì´ì…˜] â†’ ì„±ëŠ¥ 87%
ë¹„ìš©: 10ê°œ ì–´ë…¸í…Œì´ì…˜ (50% ì ˆê°, ì„±ëŠ¥ì€ +2%)
```

## Active-Promptë€?

Diao et al. (2023)ì´ ì œì•ˆí•œ **Active-Prompt**ëŠ” Active Learningì„ Few-shot í”„ë¡¬í”„íŒ…ì— ì ìš©í•œ ê¸°ë²•ì´ë‹¤.

### í•µì‹¬ ì•„ì´ë””ì–´

```
Question Set
    â†“
[ëª¨ë¸ë¡œ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •]
    â†“
ê°€ì¥ ë¶ˆí™•ì‹¤í•œ Kê°œ ì„ íƒ
    â†“
[ì‚¬ëŒì´ CoT ì–´ë…¸í…Œì´ì…˜]
    â†“
Few-shot í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©
    â†“
ìµœì¢… ë‹µë³€
```

### ì™œ íš¨ê³¼ì ì¸ê°€?

**ì˜ˆì‹œ**:

```
ì§ˆë¬¸ 100ê°œì—ì„œ 5ê°œë¥¼ Few-shot ì˜ˆì‹œë¡œ ì„ íƒí•´ì•¼ í•œë‹¤.

Random Selection (ë¬´ì‘ìœ„):
- ì‰¬ìš´ ì§ˆë¬¸ 3ê°œ: ëª¨ë¸ì´ ì´ë¯¸ ì˜ í‘¸ëŠ” ë¬¸ì œ
- ì¤‘ê°„ ì§ˆë¬¸ 2ê°œ: ì•½ê°„ ë„ì›€ë¨
â†’ ì„±ëŠ¥ í–¥ìƒ: +5%

Active-Prompt (ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜):
- ì–´ë ¤ìš´ ì§ˆë¬¸ 5ê°œ: ëª¨ë¸ì´ í—·ê°ˆë ¤í•˜ëŠ” ë¬¸ì œ
â†’ ì„±ëŠ¥ í–¥ìƒ: +12%

ì´ìœ : ëª¨ë¸ì´ ë°°ì›Œì•¼ í•  ê²ƒì„ ì •í™•íˆ ê°€ë¥´ì¹¨
```

## Active-Promptì˜ 4ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤

### Step 1: Uncertainty Estimation (ë¶ˆí™•ì‹¤ì„± ì¸¡ì •)

ê° ì§ˆë¬¸ì— ëŒ€í•´ ëª¨ë¸ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •í•œë‹¤.

#### ë°©ë²• 1: Self-Consistency ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±

ê°€ì¥ ì¼ë°˜ì ì´ê³  íš¨ê³¼ì ì¸ ë°©ë²•ì´ë‹¤.

```python
import anthropic
from typing import List, Dict
from collections import Counter

class ActivePrompt:
    """
    Active-Prompt êµ¬í˜„
    """
    
    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.model = "claude-sonnet-4-20250514"
    
    def estimate_uncertainty_self_consistency(
        self,
        question: str,
        num_samples: int = 10
    ) -> float:
        """
        Self-Consistency ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •
        
        ì›ë¦¬: ì—¬ëŸ¬ ë²ˆ ìƒ˜í”Œë§í–ˆì„ ë•Œ ë‹µì´ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ë¶ˆí™•ì‹¤í•¨
        
        Args:
            question: í‰ê°€í•  ì§ˆë¬¸
            num_samples: ìƒ˜í”Œë§ íšŸìˆ˜
        
        Returns:
            ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ (0~1, ë†’ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤)
        """
        # CoT í”„ë¡¬í”„íŠ¸
        cot_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ì„¸ìš”.

        ì§ˆë¬¸: {question}

        ë‹¨ê³„ë³„ ì¶”ë¡ :"""
        
        answers = []
        
        # ì—¬ëŸ¬ ë²ˆ ìƒ˜í”Œë§
        for _ in range(num_samples):
            message = self.client.messages.create(
                model=self.model,
                max_tokens=500,
                temperature=0.7,  # ë‹¤ì–‘ì„±ì„ ìœ„í•´ > 0
                messages=[{"role": "user", "content": cot_prompt}]
            )
            
            response = message.content[0].text
            
            # ìµœì¢… ë‹µë³€ ì¶”ì¶œ (ë§ˆì§€ë§‰ ì¤„ ë˜ëŠ” "ë‹µ:" ì´í›„)
            answer = self._extract_final_answer(response)
            answers.append(answer)
        
        # ë‹µë³€ ë¶„í¬ ê³„ì‚°
        answer_counts = Counter(answers)
        most_common_count = answer_counts.most_common(1)[0][1]
        
        # ë¶ˆí™•ì‹¤ì„± = 1 - (ìµœë‹¤ ë‹µë³€ ë¹„ìœ¨)
        # ì˜ˆ: 10ë²ˆ ì¤‘ 9ë²ˆ ê°™ì€ ë‹µ â†’ ë¶ˆí™•ì‹¤ì„± 0.1 (í™•ì‹¤)
        #     10ë²ˆ ì¤‘ 5ë²ˆ ê°™ì€ ë‹µ â†’ ë¶ˆí™•ì‹¤ì„± 0.5 (ë¶ˆí™•ì‹¤)
        consistency = most_common_count / num_samples
        uncertainty = 1 - consistency
        
        return uncertainty
    
    def _extract_final_answer(self, response: str) -> str:
        """
        ì‘ë‹µì—ì„œ ìµœì¢… ë‹µë³€ ì¶”ì¶œ
        
        íœ´ë¦¬ìŠ¤í‹±:
        - "ë‹µ:", "Answer:", "ë”°ë¼ì„œ" ë“±ì˜ í‚¤ì›Œë“œ ì´í›„
        - ë§ˆì§€ë§‰ ë¬¸ì¥
        - ìˆ«ìê°€ ìˆìœ¼ë©´ ìˆ«ì
        """
        # ê°„ë‹¨í•œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•˜ê²Œ)
        lines = response.strip().split('\n')
        
        # "ë‹µ:" ê°™ì€ í‚¤ì›Œë“œ ì°¾ê¸°
        for line in reversed(lines):
            if any(keyword in line for keyword in ['ë‹µ:', 'ë‹µì€', 'Answer:', 'ë”°ë¼ì„œ']):
                # í‚¤ì›Œë“œ ì´í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                for keyword in ['ë‹µ:', 'ë‹µì€', 'Answer:', 'ë”°ë¼ì„œ']:
                    if keyword in line:
                        answer = line.split(keyword)[-1].strip()
                        return answer
        
        # í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ì¤„
        return lines[-1].strip() if lines else ""
```

**Self-Consistency ë¶ˆí™•ì‹¤ì„± ì˜ˆì‹œ**:

```python
# í™•ì‹¤í•œ ì§ˆë¬¸
question_certain = "2 + 2ëŠ”?"

# 10ë²ˆ ìƒ˜í”Œë§ ê²°ê³¼:
# "4", "4", "4", "4", "4", "4", "4", "4", "4", "4"
# ìµœë‹¤ ë‹µë³€ ë¹„ìœ¨: 10/10 = 1.0
# ë¶ˆí™•ì‹¤ì„±: 1 - 1.0 = 0.0 (ë§¤ìš° í™•ì‹¤)

# ë¶ˆí™•ì‹¤í•œ ì§ˆë¬¸
question_uncertain = "AëŠ” Bë³´ë‹¤ í¬ê³ , CëŠ” Aë³´ë‹¤ í¬ë‹¤. Bì™€ C ì¤‘ ëˆ„ê°€ ë” í°ê°€?"

# 10ë²ˆ ìƒ˜í”Œë§ ê²°ê³¼:
# "C", "C", "B", "C", "C", "B", "C", "B", "C", "C"
# ìµœë‹¤ ë‹µë³€ ë¹„ìœ¨: 7/10 = 0.7
# ë¶ˆí™•ì‹¤ì„±: 1 - 0.7 = 0.3 (ë‹¤ì†Œ ë¶ˆí™•ì‹¤)
```

#### ë°©ë²• 2: Entropy ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±

í™•ë¥  ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ì‚¬ìš©í•œë‹¤.

```python
    def estimate_uncertainty_entropy(
        self,
        question: str,
        answer_choices: List[str]
    ) -> float:
        """
        Entropy ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •
        
        ì›ë¦¬: ë‹µë³€ ì„ íƒì§€ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ê°€ ê· ì¼í•˜ë©´ ë¶ˆí™•ì‹¤í•¨
        
        Args:
            question: í‰ê°€í•  ì§ˆë¬¸
            answer_choices: ê°€ëŠ¥í•œ ë‹µë³€ë“¤ (ì˜ˆ: ["A", "B", "C", "D"])
        
        Returns:
            ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ (0~1, ë†’ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤)
        """
        import math
        
        # ê° ì„ íƒì§€ì— ëŒ€í•œ í™•ë¥  ê³„ì‚°
        probabilities = []
        
        for choice in answer_choices:
            prompt = f"""ì§ˆë¬¸: {question}

            ë‹¤ìŒ ì¤‘ ì •ë‹µì€?
            {chr(65 + answer_choices.index(choice))}. {choice}

            ì´ ë‹µì´ ì •ë‹µì¼ í™•ë¥ ì€? (0.0-1.0 ì‚¬ì´ ìˆ«ìë¡œë§Œ ë‹µí•˜ì„¸ìš”)"""
            
            message = self.client.messages.create(
                model=self.model,
                max_tokens=20,
                temperature=0,
                messages=[{"role": "user", "content": prompt}]
            )
            
            try:
                prob = float(message.content[0].text.strip())
                prob = max(0.0, min(1.0, prob))  # 0-1 ë²”ìœ„ë¡œ ì œí•œ
            except:
                prob = 1.0 / len(answer_choices)  # ê¸°ë³¸ê°’: ê· ë“± ë¶„í¬
            
            probabilities.append(prob)
        
        # ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
        total = sum(probabilities)
        if total > 0:
            probabilities = [p / total for p in probabilities]
        
        # Entropy ê³„ì‚°
        # H = -Î£ p(x) * log(p(x))
        entropy = 0
        for p in probabilities:
            if p > 0:
                entropy -= p * math.log2(p)
        
        # ì •ê·œí™” (0-1 ë²”ìœ„)
        max_entropy = math.log2(len(answer_choices))
        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
        
        return normalized_entropy
```

**Entropy ë¶ˆí™•ì‹¤ì„± ì˜ˆì‹œ**:

```python
question = "í”„ë‘ìŠ¤ì˜ ìˆ˜ë„ëŠ”?"
choices = ["íŒŒë¦¬", "ëŸ°ë˜", "ë² ë¥¼ë¦°", "ë§ˆë“œë¦¬ë“œ"]

# í™•ë¥  ë¶„í¬:
# íŒŒë¦¬: 0.90
# ëŸ°ë˜: 0.05
# ë² ë¥¼ë¦°: 0.03
# ë§ˆë“œë¦¬ë“œ: 0.02

# Entropy = -(0.90*log(0.90) + 0.05*log(0.05) + 0.03*log(0.03) + 0.02*log(0.02))
#         = 0.47
# Normalized = 0.47 / 2.0 = 0.235
# â†’ ë‚®ì€ ë¶ˆí™•ì‹¤ì„± (í™•ì‹¤í•¨)

question_uncertain = "ë‹¤ìŒ ì¤‘ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€?"
choices = ["ììœ ", "í‰ë“±", "ì •ì˜", "ì‚¬ë‘"]

# í™•ë¥  ë¶„í¬:
# ììœ : 0.28
# í‰ë“±: 0.26
# ì •ì˜: 0.24
# ì‚¬ë‘: 0.22

# Entropy â‰ˆ 1.99
# Normalized = 1.99 / 2.0 = 0.995
# â†’ ë†’ì€ ë¶ˆí™•ì‹¤ì„± (ë¶ˆí™•ì‹¤í•¨)
```

### Step 2: Selection (ì˜ˆì‹œ ì„ íƒ)

ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ Kê°œì˜ ì§ˆë¬¸ì„ ì„ íƒí•œë‹¤.

```python
    def select_uncertain_questions(
        self,
        questions: List[str],
        k: int = 5,
        method: str = "self_consistency",
        num_samples: int = 10
    ) -> List[Dict]:
        """
        ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ ì§ˆë¬¸ ì„ íƒ
        
        Args:
            questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            k: ì„ íƒí•  ì§ˆë¬¸ ìˆ˜
            method: "self_consistency" ë˜ëŠ” "entropy"
            num_samples: Self-Consistency ìƒ˜í”Œë§ íšŸìˆ˜
        
        Returns:
            ì„ íƒëœ ì§ˆë¬¸ë“¤ (ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ í¬í•¨)
        """
        print(f"ğŸ“Š {len(questions)}ê°œ ì§ˆë¬¸ì˜ ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ì¤‘...")
        print(f"   ë°©ë²•: {method}")
        print(f"   ì„ íƒí•  ê°œìˆ˜: {k}\n")
        
        question_uncertainties = []
        
        for i, question in enumerate(questions, 1):
            if method == "self_consistency":
                uncertainty = self.estimate_uncertainty_self_consistency(
                    question, 
                    num_samples=num_samples
                )
            elif method == "entropy":
                # Entropy ë°©ë²• (ì„ íƒì§€ê°€ í•„ìš”)
                # ê°„ë‹¨íˆ í•˜ê¸° ìœ„í•´ ì—¬ê¸°ì„œëŠ” ìƒëµ
                uncertainty = 0.5
            
            question_uncertainties.append({
                'question': question,
                'uncertainty': uncertainty
            })
            
            if i % 10 == 0:
                print(f"   {i}/{len(questions)} ì™„ë£Œ")
        
        print(f"âœ… ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ì™„ë£Œ\n")
        
        # ë¶ˆí™•ì‹¤ì„± ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        question_uncertainties.sort(
            key=lambda x: x['uncertainty'], 
            reverse=True  # ë†’ì€ ë¶ˆí™•ì‹¤ì„±ë¶€í„°
        )
        
        # ìƒìœ„ kê°œ ì„ íƒ
        selected = question_uncertainties[:k]
        
        print(f"ğŸ¯ ì„ íƒëœ ì§ˆë¬¸ (ë¶ˆí™•ì‹¤ì„± ë†’ì€ ìˆœ):")
        for i, item in enumerate(selected, 1):
            print(f"   [{i}] ë¶ˆí™•ì‹¤ì„±: {item['uncertainty']:.3f}")
            print(f"       ì§ˆë¬¸: {item['question']}")
        print()
        
        return selected
```

**ì„ íƒ ì˜ˆì‹œ**:

```
100ê°œ ì§ˆë¬¸ì—ì„œ 5ê°œ ì„ íƒ:

ë¶ˆí™•ì‹¤ì„± ìˆœìœ„:
1. Q47: ë¶ˆí™•ì‹¤ì„± 0.82 â† ì„ íƒ
2. Q23: ë¶ˆí™•ì‹¤ì„± 0.79 â† ì„ íƒ
3. Q91: ë¶ˆí™•ì‹¤ì„± 0.76 â† ì„ íƒ
4. Q15: ë¶ˆí™•ì‹¤ì„± 0.73 â† ì„ íƒ
5. Q68: ë¶ˆí™•ì‹¤ì„± 0.71 â† ì„ íƒ
6. Q34: ë¶ˆí™•ì‹¤ì„± 0.68
...
100. Q7: ë¶ˆí™•ì‹¤ì„± 0.05
```

### Step 3: Annotation (ì–´ë…¸í…Œì´ì…˜)

ì„ íƒëœ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ì‚¬ëŒì´ CoT ì–´ë…¸í…Œì´ì…˜ì„ ì‘ì„±í•œë‹¤.

```python
    def collect_annotations(
        self,
        selected_questions: List[Dict]
    ) -> List[Dict]:
        """
        ì„ íƒëœ ì§ˆë¬¸ì— ëŒ€í•œ ì–´ë…¸í…Œì´ì…˜ ìˆ˜ì§‘
        
        ì‹¤ì œë¡œëŠ” ì‚¬ëŒì´ ì§ì ‘ ì‘ì„±í•˜ì§€ë§Œ,
        ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•´ LLMì´ ìƒì„±
        """
        print("âœï¸  ì–´ë…¸í…Œì´ì…˜ ìˆ˜ì§‘ ì¤‘...\n")
        
        annotated = []
        
        for i, item in enumerate(selected_questions, 1):
            question = item['question']
            
            print(f"[{i}/{len(selected_questions)}] ì§ˆë¬¸: {question}")
            
            # ì‹¤ì œë¡œëŠ”: ì‚¬ëŒì´ ì§ì ‘ CoT ì‘ì„±
            # human_cot = input("CoT ì¶”ë¡  ê³¼ì •ì„ ì…ë ¥í•˜ì„¸ìš”: ")
            
            # ì‹œë®¬ë ˆì´ì…˜: LLMì´ CoT ìƒì„±
            cot_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì •ì„ ì‘ì„±í•˜ì„¸ìš”.

            ì§ˆë¬¸: {question}

            ë‹¨ê³„ë³„ ì¶”ë¡ :"""
            
            message = self.client.messages.create(
                model=self.model,
                max_tokens=500,
                temperature=0.7,
                messages=[{"role": "user", "content": cot_prompt}]
            )
            
            cot_reasoning = message.content[0].text
            
            # ìµœì¢… ë‹µë³€ ì¶”ì¶œ
            final_answer = self._extract_final_answer(cot_reasoning)
            
            annotated.append({
                'question': question,
                'reasoning': cot_reasoning,
                'answer': final_answer
            })
            
            print(f"   CoT: {cot_reasoning[:100]}...")
            print(f"   ë‹µ: {final_answer}\n")
        
        print(f"âœ… {len(annotated)}ê°œ ì–´ë…¸í…Œì´ì…˜ ì™„ë£Œ\n")
        
        return annotated
```

**ì–´ë…¸í…Œì´ì…˜ ì˜ˆì‹œ**:

```
ì§ˆë¬¸: "Johnì€ ì‚¬ê³¼ 3ê°œë¥¼ ê°€ì§€ê³  ìˆì—ˆê³ , Maryì—ê²Œ 2ê°œë¥¼ ì£¼ì—ˆë‹¤. 
       ê·¸ í›„ Tomìœ¼ë¡œë¶€í„° 5ê°œë¥¼ ë°›ì•˜ë‹¤. Johnì€ ì´ì œ ì‚¬ê³¼ë¥¼ ëª‡ ê°œ ê°€ì§€ê³  ìˆëŠ”ê°€?"

ì‚¬ëŒì´ ì‘ì„±í•œ CoT:
"1. Johnì˜ ì´ˆê¸° ì‚¬ê³¼: 3ê°œ
 2. Maryì—ê²Œ ì¤€ í›„: 3 - 2 = 1ê°œ
 3. Tomìœ¼ë¡œë¶€í„° ë°›ì€ í›„: 1 + 5 = 6ê°œ
 ë”°ë¼ì„œ Johnì€ ì´ì œ 6ê°œì˜ ì‚¬ê³¼ë¥¼ ê°€ì§€ê³  ìˆë‹¤."

ë‹µ: 6
```

### Step 4: Inference (ì¶”ë¡ )

ì–´ë…¸í…Œì´ì…˜ëœ ì˜ˆì‹œë“¤ì„ Few-shot í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ì§ˆë¬¸ì— ë‹µí•œë‹¤.

```python
    def inference_with_annotated_examples(
        self,
        test_question: str,
        annotated_examples: List[Dict]
    ) -> str:
        """
        ì–´ë…¸í…Œì´ì…˜ëœ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•œ Few-shot ì¶”ë¡ 
        
        Args:
            test_question: ë‹µí•  ì§ˆë¬¸
            annotated_examples: ì–´ë…¸í…Œì´ì…˜ëœ ì˜ˆì‹œë“¤
        
        Returns:
            ë‹µë³€
        """
        # Few-shot í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        few_shot_examples = ""
        
        for i, example in enumerate(annotated_examples, 1):
            few_shot_examples += f"""ì˜ˆì‹œ {i}:
            ì§ˆë¬¸: {example['question']}
            ì¶”ë¡ : {example['reasoning']}
            ë‹µ: {example['answer']}

            """
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸
        prompt = f"""{few_shot_examples}ì´ì œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

        ì§ˆë¬¸: {test_question}
        ì¶”ë¡ :"""
                
        message = self.client.messages.create(
            model=self.model,
            max_tokens=500,
            temperature=0,
            messages=[{"role": "user", "content": prompt}]
        )
        
        return message.content[0].text
```

## ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬í˜„

```python
    def active_prompt_pipeline(
        self,
        question_pool: List[str],
        test_questions: List[str],
        k: int = 8,
        num_samples: int = 10
    ) -> Dict:
        """
        Active-Prompt ì „ì²´ íŒŒì´í”„ë¼ì¸
        
        Args:
            question_pool: ì–´ë…¸í…Œì´ì…˜ í›„ë³´ ì§ˆë¬¸ë“¤
            test_questions: í‰ê°€í•  ì§ˆë¬¸ë“¤
            k: ì„ íƒí•  ì˜ˆì‹œ ìˆ˜
            num_samples: Self-Consistency ìƒ˜í”Œë§ íšŸìˆ˜
        
        Returns:
            ê²°ê³¼ ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­
        """
        print("="*80)
        print("Active-Prompt Pipeline")
        print("="*80)
        print(f"ì§ˆë¬¸ í’€: {len(question_pool)}ê°œ")
        print(f"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {len(test_questions)}ê°œ")
        print(f"ì„ íƒí•  ì˜ˆì‹œ: {k}ê°œ\n")
        
        # Step 1: ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ë° ì„ íƒ
        print("Step 1: ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ì˜ˆì‹œ ì„ íƒ")
        print("-"*80)
        selected = self.select_uncertain_questions(
            question_pool,
            k=k,
            num_samples=num_samples
        )
        
        # Step 2: ì–´ë…¸í…Œì´ì…˜
        print("Step 2: ì–´ë…¸í…Œì´ì…˜ ìˆ˜ì§‘")
        print("-"*80)
        annotated = self.collect_annotations(selected)
        
        # Step 3: í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì— ëŒ€í•œ ì¶”ë¡ 
        print("Step 3: í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì¶”ë¡ ")
        print("-"*80)
        
        results = []
        for i, test_q in enumerate(test_questions, 1):
            print(f"\n[{i}/{len(test_questions)}] {test_q}")
            
            answer = self.inference_with_annotated_examples(
                test_q,
                annotated
            )
            
            print(f"ë‹µë³€: {answer[:100]}...")
            
            results.append({
                'question': test_q,
                'answer': answer
            })
        
        print("\n" + "="*80)
        print("íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        print("="*80)
        
        return {
            'selected_examples': selected,
            'annotated_examples': annotated,
            'test_results': results
        }


# ì‚¬ìš© ì˜ˆì‹œ
def main():
    # Active-Prompt ì´ˆê¸°í™”
    active_prompt = ActivePrompt(api_key="your-api-key")
    
    # ì§ˆë¬¸ í’€ (ì–´ë…¸í…Œì´ì…˜ í›„ë³´)
    question_pool = [
        "7 + 8 = ?",
        "25 - 13 = ?",
        "ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œ...",
        # ... ì´ 50ê°œ
    ]
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_questions = [
        "15 + 9 = ?",
        "31 - 17 = ?",
        # ... ì´ 10ê°œ
    ]
    
    # Active-Prompt ì‹¤í–‰
    result = active_prompt.active_prompt_pipeline(
        question_pool=question_pool,
        test_questions=test_questions,
        k=8,
        num_samples=10
    )
    
    # ê²°ê³¼ ë¶„ì„
    print("\nì„ íƒëœ ì˜ˆì‹œ:")
    for ex in result['selected_examples']:
        print(f"  - {ex['question']} (ë¶ˆí™•ì‹¤ì„±: {ex['uncertainty']:.3f})")


if __name__ == "__main__":
    main()
```

## ì‹¤í—˜ ê²°ê³¼ ë¶„ì„

Diao et al. (2023)ì˜ ë…¼ë¬¸ ê²°ê³¼ë¥¼ ë¶„ì„í•´ë³´ì.

### ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥

#### GSM8K (ìˆ˜í•™ ë¬¸ì œ)

**ì‹¤í—˜ ì„¤ì •**:
- ëª¨ë¸: GPT-3.5
- ì§ˆë¬¸ í’€: 200ê°œ
- Few-shot ì˜ˆì‹œ: 8ê°œ ì„ íƒ
- í…ŒìŠ¤íŠ¸: 500ê°œ

**ê²°ê³¼**:

| ë°©ë²• | ì •í™•ë„ |
|-----|-------|
| Zero-shot | 57.2% |
| Random Few-shot (8ê°œ) | 71.3% |
| **Active-Prompt (8ê°œ)** | **76.8%** |
| Human-selected (8ê°œ) | 73.1% |

**ê°œì„ í­**:
- Random ëŒ€ë¹„: +5.5%
- Human ëŒ€ë¹„: +3.7%

#### CommonsenseQA

**ê²°ê³¼**:

| ë°©ë²• | ì •í™•ë„ |
|-----|-------|
| Zero-shot | 62.8% |
| Random Few-shot (8ê°œ) | 68.4% |
| **Active-Prompt (8ê°œ)** | **72.1%** |

**ê°œì„ í­**: +3.7%

### ì˜ˆì‹œ ê°œìˆ˜ì™€ ì„±ëŠ¥ì˜ ê´€ê³„

```python
# ì‹¤í—˜ ë°ì´í„° (GSM8K)
num_examples = [0, 2, 4, 6, 8, 10, 12, 16]

random_accuracy = [57.2, 63.1, 66.8, 69.2, 71.3, 72.5, 73.1, 73.8]
active_accuracy = [57.2, 66.2, 71.4, 74.3, 76.8, 78.2, 78.9, 79.3]

# ì‹œê°í™”
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(num_examples, random_accuracy, marker='o', label='Random', linewidth=2)
plt.plot(num_examples, active_accuracy, marker='s', label='Active-Prompt', linewidth=2)
plt.xlabel('Number of Few-shot Examples', fontsize=12)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.title('Active-Prompt vs Random Selection (GSM8K)', fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('active_prompt_performance.png')
```

**ê´€ì°°**:
- ì˜ˆì‹œê°€ ì ì„ìˆ˜ë¡ Active-Promptì˜ ì´ì ì´ í¼
- 8ê°œ: +5.5% ì°¨ì´
- 16ê°œ: +5.5% ì°¨ì´ (ì—¬ì „íˆ ìœ ì§€)
- Active-PromptëŠ” **ë°ì´í„° íš¨ìœ¨ì„±**ì´ ë†’ìŒ

### ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ë°©ë²• ë¹„êµ

| ë°©ë²• | GSM8K ì •í™•ë„ | ì¸¡ì • ë¹„ìš© |
|-----|------------|---------|
| Random (baseline) | 71.3% | 0 |
| Entropy | 74.1% | ì¤‘ê°„ |
| **Self-Consistency** | **76.8%** | ë†’ìŒ |
| Perplexity | 73.5% | ë‚®ìŒ |

**ê²°ë¡ **: Self-Consistencyê°€ ê°€ì¥ íš¨ê³¼ì ì´ì§€ë§Œ ë¹„ìš©ì´ ë†’ë‹¤. ì˜ˆì‚°ì´ ì œí•œì ì´ë©´ Perplexityë„ ê³ ë ¤í•  ë§Œí•˜ë‹¤.

## Random vs Active-Prompt ìƒì„¸ ë¹„êµ

### ì¼€ì´ìŠ¤ ìŠ¤í„°ë””

**ì‹œë‚˜ë¦¬ì˜¤**: ìˆ˜í•™ ë¬¸ì œ í’€ì´ì—ì„œ 8ê°œ ì˜ˆì‹œ ì„ íƒ

#### Random Selectionì´ ì„ íƒí•œ ì˜ˆì‹œë“¤:

```
1. "5 + 3 = ?" (ì‰¬ì›€, ë¶ˆí™•ì‹¤ì„±: 0.02)
2. "12 - 7 = ?" (ì‰¬ì›€, ë¶ˆí™•ì‹¤ì„±: 0.05)
3. "9 Ã— 4 = ?" (ì¤‘ê°„, ë¶ˆí™•ì‹¤ì„±: 0.15)
4. "24 Ã· 6 = ?" (ì‰¬ì›€, ë¶ˆí™•ì‹¤ì„±: 0.03)
5. "15 + 18 = ?" (ì‰¬ì›€, ë¶ˆí™•ì‹¤ì„±: 0.08)
6. "45 - 23 = ?" (ì¤‘ê°„, ë¶ˆí™•ì‹¤ì„±: 0.12)
7. "(3 + 5) Ã— 2 = ?" (ì¤‘ê°„, ë¶ˆí™•ì‹¤ì„±: 0.25)
8. "100 Ã· 5 = ?" (ì‰¬ì›€, ë¶ˆí™•ì‹¤ì„±: 0.04)

í‰ê·  ë¶ˆí™•ì‹¤ì„±: 0.093
ê²°ê³¼: í…ŒìŠ¤íŠ¸ ì •í™•ë„ 71.3%
```

#### Active-Promptê°€ ì„ íƒí•œ ì˜ˆì‹œë“¤:

```
1. "((12 + 3) Ã— 4 - 7) Ã· 9 = ?" (ì–´ë ¤ì›€, ë¶ˆí™•ì‹¤ì„±: 0.78)
2. "2^3 + 3^2 - 4 = ?" (ì–´ë ¤ì›€, ë¶ˆí™•ì‹¤ì„±: 0.72)
3. "ë¶„ìˆ˜ ê³„ì‚°: 3/4 + 2/5 = ?" (ì–´ë ¤ì›€, ë¶ˆí™•ì‹¤ì„±: 0.81)
4. "ë°±ë¶„ìœ¨: 15ëŠ” 75ì˜ ëª‡ %?" (ì–´ë ¤ì›€, ë¶ˆí™•ì‹¤ì„±: 0.69)
5. "ë‹¤ë‹¨ê³„ ë¬¸ì œ: John..." (ì–´ë ¤ì›€, ë¶ˆí™•ì‹¤ì„±: 0.85)
6. "ë¹„ìœ¨ ë¬¸ì œ: 3:5 = x:15" (ì–´ë ¤ì›€, ë¶ˆí™•ì‹¤ì„±: 0.74)
7. "ì¡°í•© ë¬¸ì œ: nCr ê³„ì‚°" (ì–´ë ¤ì›€, ë¶ˆí™•ì‹¤ì„±: 0.88)
8. "ìˆœì—´: nê°œ ì¤‘ rê°œ ì„ íƒ" (ì–´ë ¤ì›€, ë¶ˆí™•ì‹¤ì„±: 0.79)

í‰ê·  ë¶ˆí™•ì‹¤ì„±: 0.778
ê²°ê³¼: í…ŒìŠ¤íŠ¸ ì •í™•ë„ 76.8%
```

**ë¶„ì„**:
- Random: ì‰¬ìš´ ë¬¸ì œê°€ ë§ì•„ ëª¨ë¸ì´ ì´ë¯¸ ì•„ëŠ” ê²ƒë§Œ ë°˜ë³µ í•™ìŠµ
- Active: ì–´ë ¤ìš´ ë¬¸ì œë¡œ ëª¨ë¸ì˜ ì•½ì ì„ ì •í™•íˆ ë³´ê°•

### ì–´ë…¸í…Œì´ì…˜ ë¹„ìš© ì ˆê°

**ì‹œë‚˜ë¦¬ì˜¤**: 80% ì •í™•ë„ ëª©í‘œ

```
Random Selection:
- 16ê°œ ì˜ˆì‹œ í•„ìš”
- ì–´ë…¸í…Œì´ì…˜ ë¹„ìš©: 16 Ã— $2 = $32
- ë‹¬ì„± ì •í™•ë„: 80.1%

Active-Prompt:
- 10ê°œ ì˜ˆì‹œ í•„ìš”
- ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ë¹„ìš©: $5 (API)
- ì–´ë…¸í…Œì´ì…˜ ë¹„ìš©: 10 Ã— $2 = $20
- ì´ ë¹„ìš©: $25
- ë‹¬ì„± ì •í™•ë„: 80.3%

ì ˆê°ì•¡: $32 - $25 = $7 (21.9% ì ˆê°)
```

```qmd
## Active-Promptì˜ ë³€í˜• ë° í™•ì¥

### ë³€í˜• 1: Iterative Active-Prompt

ë‹¨ì¼ ë¼ìš´ë“œê°€ ì•„ë‹Œ **ë°˜ë³µì **ìœ¼ë¡œ ì˜ˆì‹œë¥¼ ì¶”ê°€í•œë‹¤.

```python
class IterativeActivePrompt(ActivePrompt):
    """
    ë°˜ë³µì  Active-Prompt
    
    ì—¬ëŸ¬ ë¼ìš´ë“œì— ê±¸ì³ ì ì§„ì ìœ¼ë¡œ ì˜ˆì‹œ ì¶”ê°€
    """
    
    def iterative_selection(
        self,
        question_pool: List[str],
        test_questions: List[str],
        examples_per_round: int = 3,
        num_rounds: int = 3,
        target_accuracy: float = 0.85
    ) -> Dict:
        """
        ë°˜ë³µì  ì˜ˆì‹œ ì„ íƒ ë° í‰ê°€
        
        Args:
            question_pool: í›„ë³´ ì§ˆë¬¸ë“¤
            test_questions: í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
            examples_per_round: ë¼ìš´ë“œë‹¹ ì¶”ê°€í•  ì˜ˆì‹œ ìˆ˜
            num_rounds: ìµœëŒ€ ë¼ìš´ë“œ ìˆ˜
            target_accuracy: ëª©í‘œ ì •í™•ë„
        """
        print("ğŸ”„ Iterative Active-Prompt ì‹œì‘")
        print(f"   ë¼ìš´ë“œë‹¹ ì˜ˆì‹œ: {examples_per_round}ê°œ")
        print(f"   ìµœëŒ€ ë¼ìš´ë“œ: {num_rounds}íšŒ")
        print(f"   ëª©í‘œ ì •í™•ë„: {target_accuracy:.1%}\n")
        
        annotated_examples = []
        remaining_pool = question_pool.copy()
        accuracy_history = []
        
        for round_num in range(1, num_rounds + 1):
            print(f"{'='*80}")
            print(f"Round {round_num}/{num_rounds}")
            print(f"{'='*80}")
            
            # Step 1: í˜„ì¬ ë¶ˆí™•ì‹¤ì„± ì¸¡ì • (ë‚¨ì€ ì§ˆë¬¸ë“¤ ëŒ€ìƒ)
            print(f"ğŸ“Š ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ì¤‘... (ë‚¨ì€ ì§ˆë¬¸: {len(remaining_pool)}ê°œ)")
            
            selected = self.select_uncertain_questions(
                remaining_pool,
                k=examples_per_round,
                num_samples=10
            )
            
            # Step 2: ì–´ë…¸í…Œì´ì…˜
            print(f"âœï¸  ì–´ë…¸í…Œì´ì…˜ ìˆ˜ì§‘ ì¤‘...")
            new_annotations = self.collect_annotations(selected)
            annotated_examples.extend(new_annotations)
            
            # Step 3: ì„ íƒëœ ì§ˆë¬¸ì„ í’€ì—ì„œ ì œê±°
            selected_questions = [s['question'] for s in selected]
            remaining_pool = [q for q in remaining_pool if q not in selected_questions]
            
            # Step 4: í˜„ì¬ ì„±ëŠ¥ í‰ê°€
            print(f"ğŸ“ˆ ì„±ëŠ¥ í‰ê°€ ì¤‘... (í˜„ì¬ ì˜ˆì‹œ: {len(annotated_examples)}ê°œ)")
            
            correct = 0
            for test_q in test_questions:
                answer = self.inference_with_annotated_examples(
                    test_q,
                    annotated_examples
                )
                # ì‹¤ì œë¡œëŠ” ì •ë‹µê³¼ ë¹„êµ
                # correct += (answer == ground_truth)
                correct += 1  # ì‹œë®¬ë ˆì´ì…˜
            
            accuracy = correct / len(test_questions)
            accuracy_history.append({
                'round': round_num,
                'num_examples': len(annotated_examples),
                'accuracy': accuracy
            })
            
            print(f"\nâœ… Round {round_num} ì™„ë£Œ")
            print(f"   ëˆ„ì  ì˜ˆì‹œ: {len(annotated_examples)}ê°œ")
            print(f"   í˜„ì¬ ì •í™•ë„: {accuracy:.3f}\n")
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
            if accuracy >= target_accuracy:
                print(f"ğŸ¯ ëª©í‘œ ì •í™•ë„ ë‹¬ì„±! ({accuracy:.3f} >= {target_accuracy:.3f})")
                break
        
        return {
            'annotated_examples': annotated_examples,
            'accuracy_history': accuracy_history,
            'final_accuracy': accuracy_history[-1]['accuracy']
        }
```

**ì‹¤í–‰ ì˜ˆì‹œ**:

```
Round 1: 3ê°œ ì¶”ê°€ â†’ ëˆ„ì  3ê°œ â†’ ì •í™•ë„ 68.2%
Round 2: 3ê°œ ì¶”ê°€ â†’ ëˆ„ì  6ê°œ â†’ ì •í™•ë„ 73.5%
Round 3: 3ê°œ ì¶”ê°€ â†’ ëˆ„ì  9ê°œ â†’ ì •í™•ë„ 77.8%
Round 4: 3ê°œ ì¶”ê°€ â†’ ëˆ„ì  12ê°œ â†’ ì •í™•ë„ 80.1%
Round 5: 3ê°œ ì¶”ê°€ â†’ ëˆ„ì  15ê°œ â†’ ì •í™•ë„ 81.2%

ëª©í‘œ ì •í™•ë„(80%) ë‹¬ì„±: Round 4
```

**ì¥ì **:
- âœ… ë” ì •í™•í•œ ë¶ˆí™•ì‹¤ì„± ì¸¡ì • (ê° ë¼ìš´ë“œë§ˆë‹¤ ì¬í‰ê°€)
- âœ… ëª©í‘œ ì •í™•ë„ ë‹¬ì„± ì‹œ ì¡°ê¸° ì¢…ë£Œë¡œ ë¹„ìš© ì ˆê°
- âœ… ì ì§„ì  ê°œì„ ìœ¼ë¡œ ì•ˆì •ì 

**ë‹¨ì **:
- âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹œê°„ ì¦ê°€
- âŒ ì—¬ëŸ¬ ë¼ìš´ë“œì˜ ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ë¹„ìš©

### ë³€í˜• 2: Diverse Active-Prompt

ë¶ˆí™•ì‹¤ì„±ë¿ë§Œ ì•„ë‹ˆë¼ **ë‹¤ì–‘ì„±**ë„ ê³ ë ¤í•œë‹¤.

```python
    def select_diverse_uncertain_questions(
        self,
        questions: List[str],
        k: int = 8,
        diversity_weight: float = 0.3
    ) -> List[Dict]:
        """
        ë¶ˆí™•ì‹¤ì„± + ë‹¤ì–‘ì„±ì„ ëª¨ë‘ ê³ ë ¤í•œ ì„ íƒ
        
        Args:
            questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            k: ì„ íƒí•  ê°œìˆ˜
            diversity_weight: ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜ (0~1)
        
        Returns:
            ì„ íƒëœ ì§ˆë¬¸ë“¤
        """
        print(f"ğŸ¯ ë‹¤ì–‘ì„± ê³ ë ¤ ì„ íƒ (ê°€ì¤‘ì¹˜: {diversity_weight})")
        
        # Step 1: ëª¨ë“  ì§ˆë¬¸ì˜ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •
        uncertainties = []
        embeddings = []
        
        for question in questions:
            uncertainty = self.estimate_uncertainty_self_consistency(
                question,
                num_samples=10
            )
            uncertainties.append(uncertainty)
            
            # ì„ë² ë”© ìƒì„± (ë‹¤ì–‘ì„± ì¸¡ì •ìš©)
            embedding = self._get_embedding(question)
            embeddings.append(embedding)
        
        # Step 2: íƒìš•ì  ì„ íƒ (Greedy Diversity Selection)
        selected_indices = []
        
        # ì²« ë²ˆì§¸: ê°€ì¥ ë¶ˆí™•ì‹¤í•œ ê²ƒ
        first_idx = uncertainties.index(max(uncertainties))
        selected_indices.append(first_idx)
        
        # ë‚˜ë¨¸ì§€: ë¶ˆí™•ì‹¤ì„± + ë‹¤ì–‘ì„± ê· í˜•
        for _ in range(k - 1):
            best_score = -float('inf')
            best_idx = None
            
            for i, (uncertainty, embedding) in enumerate(zip(uncertainties, embeddings)):
                if i in selected_indices:
                    continue
                
                # ë¶ˆí™•ì‹¤ì„± ì ìˆ˜
                uncertainty_score = uncertainty
                
                # ë‹¤ì–‘ì„± ì ìˆ˜ (ì„ íƒëœ ê²ƒë“¤ê³¼ì˜ í‰ê·  ê±°ë¦¬)
                if selected_indices:
                    distances = []
                    for selected_idx in selected_indices:
                        distance = self._cosine_distance(
                            embedding,
                            embeddings[selected_idx]
                        )
                        distances.append(distance)
                    
                    diversity_score = sum(distances) / len(distances)
                else:
                    diversity_score = 1.0
                
                # ì¢…í•© ì ìˆ˜
                combined_score = (
                    (1 - diversity_weight) * uncertainty_score +
                    diversity_weight * diversity_score
                )
                
                if combined_score > best_score:
                    best_score = combined_score
                    best_idx = i
            
            selected_indices.append(best_idx)
        
        # ì„ íƒëœ ì§ˆë¬¸ ë°˜í™˜
        selected = []
        for idx in selected_indices:
            selected.append({
                'question': questions[idx],
                'uncertainty': uncertainties[idx]
            })
        
        print(f"âœ… {k}ê°œ ì„ íƒ ì™„ë£Œ (ë‹¤ì–‘ì„± ë³´ì¥)\n")
        
        return selected
    
    def _get_embedding(self, text: str) -> List[float]:
        """
        í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        """
        import openai
        
        client = openai.OpenAI(api_key="your-openai-key")
        
        response = client.embeddings.create(
            input=text,
            model="text-embedding-3-large"
        )
        
        return response.data[0].embedding
    
    def _cosine_distance(self, emb1: List[float], emb2: List[float]) -> float:
        """
        ì½”ì‚¬ì¸ ê±°ë¦¬ ê³„ì‚°
        """
        import numpy as np
        
        emb1 = np.array(emb1)
        emb2 = np.array(emb2)
        
        cosine_sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        
        return 1 - cosine_sim  # ê±°ë¦¬ë¡œ ë³€í™˜
```

**íš¨ê³¼**:

```
ë¶ˆí™•ì‹¤ì„±ë§Œ ê³ ë ¤ (diversity_weight=0.0):
- ì„ íƒëœ ì˜ˆì‹œë“¤ì´ ìœ ì‚¬í•œ ìœ í˜•
- ì˜ˆ: ëª¨ë‘ ë¶„ìˆ˜ ê³„ì‚° ë¬¸ì œ
- ë‹¤ë¥¸ ìœ í˜•(ë°±ë¶„ìœ¨, ë¹„ìœ¨)ì—ëŠ” ì•½í•¨

ë¶ˆí™•ì‹¤ì„± + ë‹¤ì–‘ì„± (diversity_weight=0.3):
- ë¶ˆí™•ì‹¤í•˜ë©´ì„œë„ ë‹¤ì–‘í•œ ìœ í˜• ì„ íƒ
- ì˜ˆ: ë¶„ìˆ˜ 1ê°œ, ë°±ë¶„ìœ¨ 1ê°œ, ë¹„ìœ¨ 1ê°œ, ì¡°í•© 1ê°œ...
- ì „ë°˜ì ì¸ ì„±ëŠ¥ í–¥ìƒ
```

### ë³€í˜• 3: Confidence-Calibrated Active-Prompt

ëª¨ë¸ì˜ **calibration** (ë³´ì •)ì„ ê³ ë ¤í•œë‹¤.

```python
    def calibrated_uncertainty(
        self,
        question: str,
        num_samples: int = 10
    ) -> Dict[str, float]:
        """
        Calibrationì„ ê³ ë ¤í•œ ë¶ˆí™•ì‹¤ì„±
        
        ëª¨ë¸ì´ ìì‹ ì˜ í™•ì‹ ë„ë¥¼ ì •í™•íˆ í‘œí˜„í•˜ëŠ”ì§€ ë³´ì •
        """
        # Self-Consistencyë¡œ ë‹µë³€ ìˆ˜ì§‘
        answers = []
        confidences = []
        
        for _ in range(num_samples):
            prompt = f"""ì§ˆë¬¸: {question}

ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ê³  ë‹µí•˜ì„¸ìš”.
ë§ˆì§€ë§‰ì— í™•ì‹ ë„ë¥¼ 0.0-1.0 ì‚¬ì´ë¡œ í‘œí˜„í•˜ì„¸ìš”.

ë‹µë³€:"""
            
            message = self.client.messages.create(
                model=self.model,
                max_tokens=500,
                temperature=0.7,
                messages=[{"role": "user", "content": prompt}]
            )
            
            response = message.content[0].text
            
            # ë‹µë³€ ë° í™•ì‹ ë„ ì¶”ì¶œ
            answer = self._extract_final_answer(response)
            confidence = self._extract_confidence(response)
            
            answers.append(answer)
            confidences.append(confidence)
        
        # ì‹¤ì œ ì¼ê´€ì„± (Self-Consistency)
        from collections import Counter
        answer_counts = Counter(answers)
        most_common_count = answer_counts.most_common(1)[0][1]
        actual_consistency = most_common_count / num_samples
        
        # ëª¨ë¸ì˜ í‰ê·  í™•ì‹ ë„
        avg_confidence = sum(confidences) / len(confidences)
        
        # Calibration error: |ëª¨ë¸ í™•ì‹ ë„ - ì‹¤ì œ ì¼ê´€ì„±|
        calibration_error = abs(avg_confidence - actual_consistency)
        
        # Calibrated uncertainty
        # ëª¨ë¸ì´ ê³¼ì‹ í•˜ë©´(calibration_error í¬ë©´) ë¶ˆí™•ì‹¤ì„± ì¦ê°€
        raw_uncertainty = 1 - actual_consistency
        calibrated_uncertainty = raw_uncertainty + calibration_error
        
        return {
            'raw_uncertainty': raw_uncertainty,
            'calibrated_uncertainty': calibrated_uncertainty,
            'calibration_error': calibration_error,
            'avg_confidence': avg_confidence,
            'actual_consistency': actual_consistency
        }
    
    def _extract_confidence(self, response: str) -> float:
        """
        ì‘ë‹µì—ì„œ í™•ì‹ ë„ ì¶”ì¶œ
        """
        import re
        
        # "í™•ì‹ ë„: 0.8" ê°™ì€ íŒ¨í„´ ì°¾ê¸°
        patterns = [
            r'í™•ì‹ ë„[:\s]+([0-9.]+)',
            r'confidence[:\s]+([0-9.]+)',
            r'certainty[:\s]+([0-9.]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response.lower())
            if match:
                try:
                    confidence = float(match.group(1))
                    return max(0.0, min(1.0, confidence))
                except:
                    pass
        
        # ê¸°ë³¸ê°’: 0.5 (ì¤‘ë¦½)
        return 0.5
```

## ë‹¤ì–‘í•œ ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ë°©ë²• ì‹¬í™”

### ë°©ë²• ë¹„êµí‘œ

| ë°©ë²• | ì›ë¦¬ | ì¥ì  | ë‹¨ì  | ë¹„ìš© |
|-----|------|------|------|------|
| **Self-Consistency** | ì—¬ëŸ¬ ìƒ˜í”Œë§ ê²°ê³¼ ì¼ì¹˜ë„ | ë†’ì€ ì •í™•ë„ | ë¹„ìš© ë†’ìŒ | $$$ |
| **Entropy** | í™•ë¥  ë¶„í¬ ì—”íŠ¸ë¡œí”¼ | ì´ë¡ ì  ê·¼ê±° ëª…í™• | ì„ íƒì§€ í•„ìš” | $$ |
| **Perplexity** | í† í° ì˜ˆì¸¡ ë‚œì´ë„ | ë¹ ë¦„ | ì •í™•ë„ ë‚®ìŒ | $ |
| **Confidence** | ëª¨ë¸ ìì²´ í™•ì‹ ë„ | ë§¤ìš° ë¹ ë¦„ | ë³´ì • í•„ìš” | $ |
| **Ensemble** | ì—¬ëŸ¬ ëª¨ë¸ í•©ì˜ë„ | ê²¬ê³ í•¨ | ì—¬ëŸ¬ ëª¨ë¸ í•„ìš” | $$$$ |

### ë°©ë²• 1: Perplexity ê¸°ë°˜

```python
    def estimate_uncertainty_perplexity(
        self,
        question: str
    ) -> float:
        """
        Perplexity ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
        
        ì›ë¦¬: ì§ˆë¬¸ì´ ì–´ë ¤ìš°ë©´ ëª¨ë¸ì´ ë‹µì„ ìƒì„±í•  ë•Œ perplexityê°€ ë†’ìŒ
        """
        prompt = f"""ì§ˆë¬¸: {question}

ë‹µë³€:"""
        
        # APIì—ì„œ log probabilities ê°€ì ¸ì˜¤ê¸°
        # (Anthropic APIëŠ” í˜„ì¬ ì§€ì› ì•ˆ í•¨, OpenAI ì˜ˆì‹œ)
        import openai
        
        client = openai.OpenAI(api_key="your-openai-key")
        
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            logprobs=True,
            max_tokens=100
        )
        
        # Log probabilities ì¶”ì¶œ
        logprobs = []
        for token_data in response.choices[0].logprobs.content:
            logprobs.append(token_data.logprob)
        
        # Perplexity ê³„ì‚°
        import math
        avg_logprob = sum(logprobs) / len(logprobs)
        perplexity = math.exp(-avg_logprob)
        
        # ì •ê·œí™” (0~1 ë²”ìœ„)
        # ì‹¤ì œë¡œëŠ” ê²½í—˜ì ìœ¼ë¡œ ê²°ì •
        normalized = min(1.0, perplexity / 100.0)
        
        return normalized
```

**íŠ¹ì§•**:
- âš¡ ë¹ ë¦„ (ë‹¨ì¼ API í˜¸ì¶œ)
- ğŸ’° ì €ë ´
- âš ï¸ ì •í™•ë„ëŠ” Self-Consistencyë³´ë‹¤ ë‚®ìŒ

### ë°©ë²• 2: Ensemble Disagreement

ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ê²¬ ë¶ˆì¼ì¹˜ë„ë¥¼ ì¸¡ì •í•œë‹¤.

```python
    def estimate_uncertainty_ensemble(
        self,
        question: str,
        models: List[str] = None
    ) -> float:
        """
        Ensemble ë¶ˆí™•ì‹¤ì„±
        
        ì—¬ëŸ¬ ëª¨ë¸ì´ ë‹¤ë¥¸ ë‹µì„ ë‚´ë©´ ë¶ˆí™•ì‹¤í•¨
        """
        if models is None:
            models = [
                "claude-sonnet-4-20250514",
                "claude-opus-4-20250514",
                "claude-haiku-4-20250514"
            ]
        
        prompt = f"""ì§ˆë¬¸: {question}

ê°„ë‹¨íˆ ë‹µí•˜ì„¸ìš”:"""
        
        answers = []
        
        for model in models:
            message = self.client.messages.create(
                model=model,
                max_tokens=100,
                temperature=0,
                messages=[{"role": "user", "content": prompt}]
            )
            
            answer = message.content[0].text.strip()
            answers.append(answer)
        
        # ì˜ê²¬ ë¶ˆì¼ì¹˜ë„ ê³„ì‚°
        from collections import Counter
        answer_counts = Counter(answers)
        most_common_count = answer_counts.most_common(1)[0][1]
        
        agreement = most_common_count / len(answers)
        disagreement = 1 - agreement
        
        return disagreement
```

### ë°©ë²• 3: Query Complexity

ì§ˆë¬¸ ìì²´ì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•œë‹¤.

```python
    def estimate_uncertainty_complexity(
        self,
        question: str
    ) -> float:
        """
        ì§ˆë¬¸ ë³µì¡ë„ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
        
        ë³µì¡í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ë¶ˆí™•ì‹¤í•¨
        """
        # ì—¬ëŸ¬ ë³µì¡ë„ ì§€í‘œ ê³„ì‚°
        
        # 1. ê¸¸ì´ (í† í° ìˆ˜)
        import tiktoken
        encoding = tiktoken.get_encoding("cl100k_base")
        tokens = encoding.encode(question)
        length_score = min(1.0, len(tokens) / 100)
        
        # 2. ì¤‘ì²© ê¹Šì´ (ê´„í˜¸, ì ˆ ë“±)
        nesting_depth = question.count('(') + question.count('[')
        nesting_score = min(1.0, nesting_depth / 5)
        
        # 3. ìˆ˜í•™ ì—°ì‚°ì ê°œìˆ˜
        math_operators = ['+', '-', '*', '/', '^', '=']
        operator_count = sum(question.count(op) for op in math_operators)
        operator_score = min(1.0, operator_count / 10)
        
        # 4. ì¡°ê±´ë¬¸/ë‹¤ë‹¨ê³„ ì§€ì‹œì–´
        complexity_keywords = ['ë§Œì•½', 'if', 'ê·¸ ë‹¤ìŒ', 'then', 'ë¨¼ì €', 'first']
        keyword_count = sum(1 for kw in complexity_keywords if kw in question.lower())
        keyword_score = min(1.0, keyword_count / 3)
        
        # ì¢…í•© ì ìˆ˜
        complexity = (
            0.2 * length_score +
            0.3 * nesting_score +
            0.3 * operator_score +
            0.2 * keyword_score
        )
        
        return complexity
```

### í•˜ì´ë¸Œë¦¬ë“œ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •

ì—¬ëŸ¬ ë°©ë²•ì„ ê²°í•©í•œë‹¤.

```python
    def estimate_uncertainty_hybrid(
        self,
        question: str,
        methods: List[str] = None,
        weights: Dict[str, float] = None
    ) -> Dict[str, float]:
        """
        ì—¬ëŸ¬ ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ë°©ë²• ê²°í•©
        
        Args:
            question: ì§ˆë¬¸
            methods: ì‚¬ìš©í•  ë°©ë²•ë“¤
            weights: ê° ë°©ë²•ì˜ ê°€ì¤‘ì¹˜
        
        Returns:
            ê° ë°©ë²•ì˜ ì ìˆ˜ ë° ìµœì¢… ì ìˆ˜
        """
        if methods is None:
            methods = ['self_consistency', 'complexity']
        
        if weights is None:
            # ê¸°ë³¸ ê°€ì¤‘ì¹˜
            weights = {
                'self_consistency': 0.7,
                'complexity': 0.3,
                'perplexity': 0.0,
                'ensemble': 0.0
            }
        
        scores = {}
        
        # ê° ë°©ë²•ìœ¼ë¡œ ì¸¡ì •
        if 'self_consistency' in methods:
            scores['self_consistency'] = self.estimate_uncertainty_self_consistency(
                question,
                num_samples=10
            )
        
        if 'complexity' in methods:
            scores['complexity'] = self.estimate_uncertainty_complexity(question)
        
        if 'perplexity' in methods:
            scores['perplexity'] = self.estimate_uncertainty_perplexity(question)
        
        if 'ensemble' in methods:
            scores['ensemble'] = self.estimate_uncertainty_ensemble(question)
        
        # ê°€ì¤‘ í‰ê· 
        final_score = sum(
            scores.get(method, 0) * weights.get(method, 0)
            for method in methods
        )
        
        scores['final'] = final_score
        
        return scores
```

## ì‹¤ë¬´ ì ìš© ì „ëµ

### ì „ëµ 1: ë‹¨ê³„ë³„ ë„ì…

```python
class GradualActivePrompt:
    """
    ì ì§„ì  Active-Prompt ë„ì…
    """
    
    def phase_1_baseline(self, questions: List[str]) -> Dict:
        """
        Phase 1: ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì • (Random Few-shot)
        """
        print("Phase 1: ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì •")
        
        # ë¬´ì‘ìœ„ë¡œ 8ê°œ ì„ íƒ
        import random
        random_examples = random.sample(questions, 8)
        
        # ì–´ë…¸í…Œì´ì…˜ ë° í‰ê°€
        # ...
        
        return {'accuracy': 0.71, 'method': 'random'}
    
    def phase_2_simple_active(self, questions: List[str]) -> Dict:
        """
        Phase 2: ê°„ë‹¨í•œ Active-Prompt (Complexity ê¸°ë°˜)
        """
        print("Phase 2: Complexity ê¸°ë°˜ ì„ íƒ")
        
        # ë³µì¡ë„ ê¸°ë°˜ ì„ íƒ (ë¹ ë¥´ê³  ì €ë ´)
        scored = []
        for q in questions:
            complexity = self.estimate_uncertainty_complexity(q)
            scored.append((q, complexity))
        
        scored.sort(key=lambda x: x[1], reverse=True)
        selected = [q for q, _ in scored[:8]]
        
        # ì–´ë…¸í…Œì´ì…˜ ë° í‰ê°€
        # ...
        
        return {'accuracy': 0.74, 'method': 'complexity'}
    
    def phase_3_full_active(self, questions: List[str]) -> Dict:
        """
        Phase 3: Full Active-Prompt (Self-Consistency)
        """
        print("Phase 3: Self-Consistency ê¸°ë°˜ ì„ íƒ")
        
        # Self-Consistency ê¸°ë°˜ ì„ íƒ (ì •í™•í•˜ì§€ë§Œ ë¹„ìŒˆ)
        selected = self.select_uncertain_questions(
            questions,
            k=8,
            method='self_consistency',
            num_samples=10
        )
        
        # ì–´ë…¸í…Œì´ì…˜ ë° í‰ê°€
        # ...
        
        return {'accuracy': 0.77, 'method': 'self_consistency'}
    
    def gradual_rollout(self, questions: List[str]) -> Dict:
        """
        ì „ì²´ ë‹¨ê³„ë³„ ë„ì…
        """
        results = []
        
        # Phase 1
        result1 = self.phase_1_baseline(questions)
        results.append(result1)
        print(f"  ê²°ê³¼: {result1['accuracy']:.3f}\n")
        
        # Phase 2
        result2 = self.phase_2_simple_active(questions)
        results.append(result2)
        improvement = result2['accuracy'] - result1['accuracy']
        print(f"  ê²°ê³¼: {result2['accuracy']:.3f} (+{improvement:.3f})\n")
        
        # Phase 3ë¡œ ì§„í–‰ ì—¬ë¶€ ê²°ì •
        if improvement >= 0.02:  # 2% ì´ìƒ ê°œì„ ë˜ë©´
            print("âœ… Phase 2ì—ì„œ ì¶©ë¶„í•œ ê°œì„  â†’ Phase 3 ì§„í–‰")
            result3 = self.phase_3_full_active(questions)
            results.append(result3)
            print(f"  ê²°ê³¼: {result3['accuracy']:.3f}\n")
        else:
            print("âš ï¸  ê°œì„  í­ ì‘ìŒ â†’ Phase 3 ìŠ¤í‚µ")
        
        return {'results': results}
```

### ì „ëµ 2: ì˜ˆì‚° ì œì•½ í•˜ ìµœì í™”

```python
def budget_constrained_active_prompt(
    questions: List[str],
    max_budget_usd: float = 10.0,
    annotation_cost_per_example: float = 2.0
) -> Dict:
    """
    ì˜ˆì‚° ì œì•½ í•˜ì—ì„œ Active-Prompt
    
    Args:
        questions: ì§ˆë¬¸ë“¤
        max_budget_usd: ìµœëŒ€ ì˜ˆì‚°
        annotation_cost_per_example: ì˜ˆì‹œë‹¹ ì–´ë…¸í…Œì´ì…˜ ë¹„ìš©
    """
    print(f"ğŸ’° ì˜ˆì‚°: ${max_budget_usd}")
    print(f"   ì–´ë…¸í…Œì´ì…˜ ë¹„ìš©: ${annotation_cost_per_example}/ì˜ˆì‹œ\n")
    
    # ì˜ˆì‚° ë¶„ë°°
    # 70%: ì–´ë…¸í…Œì´ì…˜
    # 30%: ë¶ˆí™•ì‹¤ì„± ì¸¡ì •
    annotation_budget = max_budget_usd * 0.7
    measurement_budget = max_budget_usd * 0.3
    
    # ì–´ë…¸í…Œì´ì…˜ ê°€ëŠ¥í•œ ì˜ˆì‹œ ìˆ˜
    max_examples = int(annotation_budget / annotation_cost_per_example)
    
    print(f"ğŸ“Š ìµœëŒ€ ì–´ë…¸í…Œì´ì…˜ ê°€ëŠ¥: {max_examples}ê°œ")
    print(f"   ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ì˜ˆì‚°: ${measurement_budget}\n")
    
    # ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ë°©ë²• ì„ íƒ
    cost_per_measurement = {
        'complexity': 0.0,  # ë¬´ë£Œ (ë¡œì»¬ ê³„ì‚°)
        'perplexity': 0.01,  # ì €ë ´
        'self_consistency': 0.10  # ë¹„ìŒˆ (10íšŒ ìƒ˜í”Œë§)
    }
    
    # ì˜ˆì‚°ì— ë§ëŠ” ë°©ë²• ì„ íƒ
    if measurement_budget / len(questions) >= cost_per_measurement['self_consistency']:
        method = 'self_consistency'
        print("âœ… Self-Consistency ì‚¬ìš© ê°€ëŠ¥")
    elif measurement_budget / len(questions) >= cost_per_measurement['perplexity']:
        method = 'perplexity'
        print("âš ï¸  Perplexity ì‚¬ìš© (ì˜ˆì‚° ì œì•½)")
    else:
        method = 'complexity'
        print("âš ï¸  Complexity ì‚¬ìš© (ì˜ˆì‚° ì œì•½)")
    
    # Active-Prompt ì‹¤í–‰
    active_prompt = ActivePrompt(api_key="your-api-key")
    
    selected = active_prompt.select_uncertain_questions(
        questions,
        k=max_examples,
        method=method
    )
    
    # ì‹¤ì œ ë¹„ìš© ê³„ì‚°
    measurement_cost = len(questions) * cost_per_measurement[method]
    annotation_cost = max_examples * annotation_cost_per_example
    total_cost = measurement_cost + annotation_cost
    
    print(f"\nğŸ’µ ì‹¤ì œ ë¹„ìš©:")
    print(f"   ë¶ˆí™•ì‹¤ì„± ì¸¡ì •: ${measurement_cost:.2f}")
    print(f"   ì–´ë…¸í…Œì´ì…˜: ${annotation_cost:.2f}")
    print(f"   ì´: ${total_cost:.2f} (ì˜ˆì‚° ë‚´: ${max_budget_usd})")
    
    return {
        'selected': selected,
        'method': method,
        'num_examples': max_examples,
        'total_cost': total_cost
    }
```

### ì „ëµ 3: ë„ë©”ì¸ ì ì‘

```python
def domain_adaptive_active_prompt(
    domain: str,
    questions: List[str],
    k: int = 8
) -> Dict:
    """
    ë„ë©”ì¸ì— ë§ê²Œ Active-Prompt ì¡°ì •
    
    Args:
        domain: 'math', 'commonsense', 'code', 'reasoning' ë“±
        questions: ì§ˆë¬¸ë“¤
        k: ì„ íƒí•  ê°œìˆ˜
    """
    print(f"ğŸ¯ ë„ë©”ì¸: {domain}\n")
    
    # ë„ë©”ì¸ë³„ ì„¤ì •
    config = {
        'math': {
            'method': 'self_consistency',
            'num_samples': 15,  # ìˆ˜í•™ì€ ì •í™•ë„ ì¤‘ìš”
            'diversity_weight': 0.2  # ë‹¤ì–‘ì„± ëœ ì¤‘ìš”
        },
        'commonsense': {
            'method': 'self_consistency',
            'num_samples': 10,
            'diversity_weight': 0.4  # ë‹¤ì–‘ì„± ì¤‘ìš”
        },
        'code': {
            'method': 'complexity',  # ì½”ë“œëŠ” ë³µì¡ë„ ì¢‹ì€ ì§€í‘œ
            'num_samples': 5,
            'diversity_weight': 0.3
        },
        'reasoning': {
            'method': 'self_consistency',
            'num_samples': 12,
            'diversity_weight': 0.3
        }
    }
    
    domain_config = config.get(domain, config['commonsense'])
    
    print(f"ì„¤ì •:")
    print(f"   ë°©ë²•: {domain_config['method']}")
    print(f"   ìƒ˜í”Œë§: {domain_config['num_samples']}íšŒ")
    print(f"   ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜: {domain_config['diversity_weight']}\n")
    
    # Active-Prompt ì‹¤í–‰
    active_prompt = ActivePrompt(api_key="your-api-key")
    
    selected = active_prompt.select_diverse_uncertain_questions(
        questions,
        k=k,
        diversity_weight=domain_config['diversity_weight']
    )
    
    return {
        'selected': selected,
        'config': domain_config
    }
```

## í•œê³„ì  ë° ì£¼ì˜ì‚¬í•­

### í•œê³„ 1: Cold Start ë¬¸ì œ

**ë¬¸ì œ**: ì´ˆê¸°ì—ëŠ” ëª¨ë¸ì´ ëª¨ë“  ì§ˆë¬¸ì„ ë¶ˆí™•ì‹¤í•˜ê²Œ ëŠë‚Œ

```
ì²˜ìŒ Active-Prompt ì ìš© ì‹œ:
- ëŒ€ë¶€ë¶„ì˜ ì§ˆë¬¸ì´ ë†’ì€ ë¶ˆí™•ì‹¤ì„±
- êµ¬ë¶„ì´ ì–´ë ¤ì›€

í•´ê²°ì±…:
1. ë„ë©”ì¸ ë³µì¡ë„ ë¨¼ì € ê³ ë ¤
2. ì†Œìˆ˜ ì˜ˆì‹œë¡œ warm-up
3. ì ì§„ì  ì ‘ê·¼
```

### í•œê³„ 2: ë¶„í¬ í¸í–¥

**ë¬¸ì œ**: ì–´ë ¤ìš´ ì˜ˆì‹œë§Œ ì„ íƒí•˜ë©´ ì‰¬ìš´ ì˜ˆì‹œ ë¶€ì¡±

```python
def balanced_active_prompt(
    questions: List[str],
    k: int = 8,
    difficulty_distribution: Dict[str, float] = None
) -> List[Dict]:
    """
    ë‚œì´ë„ ë¶„í¬ë¥¼ ê³ ë ¤í•œ ì„ íƒ
    
    Args:
        difficulty_distribution: {'easy': 0.2, 'medium': 0.3, 'hard': 0.5}
    """
    if difficulty_distribution is None:
        difficulty_distribution = {
            'easy': 0.1,
            'medium': 0.3,
            'hard': 0.6
        }
    
    # ê° ë‚œì´ë„ë³„ ê°œìˆ˜ ê³„ì‚°
    num_easy = int(k * difficulty_distribution['easy'])
    num_medium = int(k * difficulty_distribution['medium'])
    num_hard = k - num_easy - num_medium
    
    print(f"ë‚œì´ë„ ë¶„í¬: ì‰¬ì›€ {num_easy}, ì¤‘ê°„ {num_medium}, ì–´ë ¤ì›€ {num_hard}")
    
    # ë¶ˆí™•ì‹¤ì„± ì¸¡ì •
    uncertainties = []
    for q in questions:
        u = estimate_uncertainty(q)
        uncertainties.append((q, u))
    
    uncertainties.sort(key=lambda x: x[1])
    
    # ë‚œì´ë„ë³„ ë¶„í• 
    # ì‰¬ì›€: í•˜ìœ„ 30%
    # ì¤‘ê°„: ì¤‘ê°„ 40%
    # ì–´ë ¤ì›€: ìƒìœ„ 30%
    n = len(uncertainties)
    easy_pool = uncertainties[:int(n*0.3)]
    medium_pool = uncertainties[int(n*0.3):int(n*0.7)]
    hard_pool = uncertainties[int(n*0.7):]
    
    # ê° í’€ì—ì„œ ì„ íƒ
    import random
    selected = []
    selected.extend(random.sample(easy_pool, num_easy))
    selected.extend(random.sample(medium_pool, num_medium))
    selected.extend(random.sample(hard_pool, num_hard))
    
    return selected
```

### í•œê³„ 3: ì–´ë…¸í…Œì´ì…˜ í’ˆì§ˆ

**ë¬¸ì œ**: ì‚¬ëŒì˜ ì–´ë…¸í…Œì´ì…˜ í’ˆì§ˆì´ ì¼ì •í•˜ì§€ ì•ŠìŒ

```python
def annotation_quality_check(
    annotated_examples: List[Dict],
    validator: callable = None
) -> List[Dict]:
    """
    ì–´ë…¸í…Œì´ì…˜ í’ˆì§ˆ ê²€ì‚¬
    
    Args:
        annotated_examples: ì–´ë…¸í…Œì´ì…˜ëœ ì˜ˆì‹œë“¤
        validator: ê²€ì¦ í•¨ìˆ˜
    """
    print("ğŸ” ì–´ë…¸í…Œì´ì…˜ í’ˆì§ˆ ê²€ì‚¬ ì¤‘...\n")
    
    validated = []
    issues = []
    
    for i, example in enumerate(annotated_examples, 1):
        # ê¸°ë³¸ ì²´í¬
        checks = {
            'has_question': bool(example.get('question')),
            'has_reasoning': bool(example.get('reasoning')),
            'has_answer': bool(example.get('answer')),
            'reasoning_length': len(example.get('reasoning', '')) > 20,
            'answer_not_empty': len(example.get('answer', '').strip()) > 0
        }
        
        # ì»¤ìŠ¤í…€ ê²€ì¦ (ìˆë‹¤ë©´)
        if validator:
            checks['custom'] = validator(example)
        
        # ëª¨ë“  ì²´í¬ í†µê³¼ ì—¬ë¶€
        all_pass = all(checks.values())
        
        if all_pass:
            validated.append(example)
            print(f"âœ… ì˜ˆì‹œ {i}: í†µê³¼")
        else:
            issues.append({
                'index': i,
                'example': example,
                'failed_checks': [k for k, v in checks.items() if not v]
            })
            print(f"âŒ ì˜ˆì‹œ {i}: ì‹¤íŒ¨ - {[k for k, v in checks.items() if not v]}")
    
    print(f"\nê²°ê³¼: {len(validated)}/{len(annotated_examples)} í†µê³¼")
    
    if issues:
        print(f"âš ï¸  {len(issues)}ê°œ ì˜ˆì‹œ ì¬ì‘ì—… í•„ìš”")
    
    return validated
```

### í•œê³„ 4: ì‹œê°„ ì§€ì—°

**ë¬¸ì œ**: ë¶ˆí™•ì‹¤ì„± ì¸¡ì •ì— ì‹œê°„ì´ ê±¸ë¦¼

```
Self-Consistency (10 ìƒ˜í”Œ):
- ì§ˆë¬¸ë‹¹ ì•½ 20ì´ˆ
- 100ê°œ ì§ˆë¬¸ = 33ë¶„

í•´ê²°ì±…:
1. ë³‘ë ¬ ì²˜ë¦¬
2. ìºì‹±
3. ìƒ˜í”Œ ìˆ˜ ì¡°ì •
```

**ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„**:

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

def parallel_uncertainty_estimation(
    questions: List[str],
    max_workers: int = 5
) -> List[Dict]:
    """
    ë³‘ë ¬ë¡œ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •
    """
    print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (workers: {max_workers})")
    
    start_time = time.time()
    
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ì‘ì—… ì œì¶œ
        future_to_question = {
            executor.submit(
                estimate_uncertainty_self_consistency,
                q,
                num_samples=10
            ): q
            for q in questions
        }
        
        # ì™„ë£Œëœ ì‘ì—… ìˆ˜ì§‘
        for future in as_completed(future_to_question):
            question = future_to_question[future]
            try:
                uncertainty = future.result()
                results.append({
                    'question': question,
                    'uncertainty': uncertainty
                })
                print(f"âœ“ {len(results)}/{len(questions)}")
            except Exception as e:
                print(f"âœ— ì˜¤ë¥˜: {question[:30]}... - {e}")
    
    elapsed = time.time() - start_time
    print(f"\nì™„ë£Œ: {elapsed:.1f}ì´ˆ (í‰ê·  {elapsed/len(questions):.1f}ì´ˆ/ì§ˆë¬¸)")
    
    return results
```

## ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. íŒŒì¼ëŸ¿ í…ŒìŠ¤íŠ¸ë¡œ ì‹œì‘

```python
def pilot_test(
    small_question_pool: List[str],  # 20-30ê°œ
    test_questions: List[str],  # 10ê°œ
    k: int = 5
) -> Dict:
    """
    ì†Œê·œëª¨ íŒŒì¼ëŸ¿ í…ŒìŠ¤íŠ¸
    
    ëª©ì : Active-Promptê°€ í•´ë‹¹ ë„ë©”ì¸ì—ì„œ íš¨ê³¼ì ì¸ì§€ ê²€ì¦
    """
    print("ğŸ§ª íŒŒì¼ëŸ¿ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # Baseline: Random
    print("1. Random ì„ íƒ í…ŒìŠ¤íŠ¸")
    import random
    random_examples = random.sample(small_question_pool, k)
    random_accuracy = evaluate_examples(random_examples, test_questions)
    print(f"   ì •í™•ë„: {random_accuracy:.3f}\n")
    
    # Active-Prompt
    print("2. Active-Prompt í…ŒìŠ¤íŠ¸")
    active_prompt = ActivePrompt(api_key="your-api-key")
    selected = active_prompt.select_uncertain_questions(
        small_question_pool,
        k=k,
        method='self_consistency',
        num_samples=10
    )
    active_accuracy = evaluate_examples(selected, test_questions)
    print(f"   ì •í™•ë„: {active_accuracy:.3f}\n")
    
    # ê²°ê³¼ ë¶„ì„
    improvement = active_accuracy - random_accuracy
    is_effective = improvement >= 0.03  # 3% ì´ìƒ ê°œì„ 
    
    print("="*60)
    if is_effective:
        print(f"âœ… Active-Prompt íš¨ê³¼ì  (+{improvement:.1%})")
        print("   â†’ ì „ì²´ ë°ì´í„°ì…‹ì— ì ìš© ê¶Œì¥")
    else:
        print(f"âš ï¸  Active-Prompt íš¨ê³¼ ì œí•œì  (+{improvement:.1%})")
        print("   â†’ Random ì„ íƒ ë˜ëŠ” ë‹¤ë¥¸ ë°©ë²• ê³ ë ¤")
    
    return {
        'random_accuracy': random_accuracy,
        'active_accuracy': active_accuracy,
        'improvement': improvement,
        'is_effective': is_effective
    }
```

### 2. ë¬¸ì„œí™” ë° ì¶”ì 

```python
import json
from datetime import datetime

class ActivePromptTracker:
    """
    Active-Prompt ì‹¤í–‰ ì´ë ¥ ì¶”ì 
    """
    
    def __init__(self, project_name: str):
        self.project_name = project_name
        self.history = []
    
    def log_run(
        self,
        run_config: Dict,
        selected_examples: List[Dict],
        performance: Dict
    ):
        """
        ì‹¤í–‰ ê¸°ë¡
        """
        record = {
            'timestamp': datetime.now().isoformat(),
            'config': run_config,
            'num_selected': len(selected_examples),
            'selected_questions': [ex['question'] for ex in selected_examples],
            'uncertainties': [ex['uncertainty'] for ex in selected_examples],
            'performance': performance
        }
        
        self.history.append(record)
        
        # íŒŒì¼ë¡œ ì €ì¥
        filename = f"active_prompt_{self.project_name}_history.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.history, f, indent=2, ensure_ascii=False)
    
    def get_best_run(self) -> Dict:
        """
        ìµœê³  ì„±ëŠ¥ ì‹¤í–‰ ë°˜í™˜
        """
        if not self.history:
            return None
        
        return max(
            self.history,
            key=lambda x: x['performance'].get('accuracy', 0)
        )
    
    def analyze_trends(self):
        """
        íŠ¸ë Œë“œ ë¶„ì„
        """
        if len(self.history) < 2:
            print("ì¶©ë¶„í•œ ë°ì´í„° ì—†ìŒ")
            return
        
        accuracies = [h['performance']['accuracy'] for h in self.history]
        
        print(f"ì‹¤í–‰ íšŸìˆ˜: {len(self.history)}")
        print(f"í‰ê·  ì •í™•ë„: {sum(accuracies)/len(accuracies):.3f}")
        print(f"ìµœê³  ì •í™•ë„: {max(accuracies):.3f}")
        print(f"ìµœì € ì •í™•ë„: {min(accuracies):.3f}")
        
        # ì‹œê°„ì— ë”°ë¥¸ ê°œì„ 
        if accuracies[-1] > accuracies[0]:
            improvement = accuracies[-1] - accuracies[0]
            print(f"âœ… ê°œì„ : +{improvement:.3f}")
        else:
            decline = accuracies[0] - accuracies[-1]
            print(f"âš ï¸  í•˜ë½: -{decline:.3f}")


# ì‚¬ìš©
tracker = ActivePromptTracker("sentiment_classification")

tracker.log_run(
    run_config={'k': 8, 'method': 'self_consistency'},
    selected_examples=selected,
    performance={'accuracy': 0.768}
)
```

### 3. A/B í…ŒìŠ¤íŒ…

```python
def ab_test_active_prompt(
    question_pool: List[str],
    test_set: List[str],
    k: int = 8,
    num_trials: int = 5
) -> Dict:
    """
    Active-Prompt vs Random A/B í…ŒìŠ¤íŠ¸
    """
    print("ğŸ”¬ A/B í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"   ì‹œë„ íšŸìˆ˜: {num_trials}íšŒ\n")
    
    random_scores = []
    active_scores = []
    
    for trial in range(1, num_trials + 1):
        print(f"Trial {trial}/{num_trials}")
        
        # A: Random
        import random
        random_examples = random.sample(question_pool, k)
        random_score = evaluate(random_examples, test_set)
        random_scores.append(random_score)
        print(f"  Random: {random_score:.3f}")
        
        # B: Active-Prompt
        active_prompt = ActivePrompt(api_key="your-api-key")
        selected = active_prompt.select_uncertain_questions(
            question_pool,
            k=k
        )
        active_score = evaluate(selected, test_set)
        active_scores.append(active_score)
        print(f"  Active: {active_score:.3f}\n")
    
    # í†µê³„ ë¶„ì„
    import numpy as np
    from scipy import stats
    
    mean_random = np.mean(random_scores)
    mean_active = np.mean(active_scores)
    
    # t-test
    t_stat, p_value = stats.ttest_rel(active_scores, random_scores)
    
    print("="*60)
    print("ê²°ê³¼:")
    print(f"  Random í‰ê· : {mean_random:.3f} (Â±{np.std(random_scores):.3f})")
    print(f"  Active í‰ê· : {mean_active:.3f} (Â±{np.std(active_scores):.3f})")
    print(f"  ê°œì„ : +{mean_active - mean_random:.3f}")
    print(f"  p-value: {p_value:.4f}")
    
    if p_value < 0.05:
        print("  âœ… í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ê°œì„  (p < 0.05)")
    else:
        print("  âš ï¸  í†µê³„ì  ìœ ì˜ì„± ì—†ìŒ (p >= 0.05)")
    
    return {
        'random_scores': random_scores,
        'active_scores': active_scores,
        'mean_random': mean_random,
        'mean_active': mean_active,
        'improvement': mean_active - mean_random,
        'p_value': p_value,
        'is_significant': p_value < 0.05
    }
```

## Active Learningê³¼ì˜ ì°¨ì´ì 

### ì „í†µì  Active Learning

```
ëª©ì : ëª¨ë¸ íŒŒë¼ë¯¸í„° í•™ìŠµ
í”„ë¡œì„¸ìŠ¤:
1. ì´ˆê¸° ëª¨ë¸ í•™ìŠµ
2. ë¶ˆí™•ì‹¤í•œ ë°ì´í„° ì„ íƒ
3. ì–´ë…¸í…Œì´ì…˜
4. ëª¨ë¸ ì¬í•™ìŠµ â† í•µì‹¬
5. ë°˜ë³µ
```

### Active-Prompt

```
ëª©ì : í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ ì„ íƒ
í”„ë¡œì„¸ìŠ¤:
1. ì‚¬ì „í•™ìŠµëœ LLM ì‚¬ìš©
2. ë¶ˆí™•ì‹¤í•œ ë°ì´í„° ì„ íƒ
3. ì–´ë…¸í…Œì´ì…˜
4. Few-shot í”„ë¡¬í”„íŠ¸ êµ¬ì„± â† í•µì‹¬
5. (ëª¨ë¸ í•™ìŠµ ì—†ìŒ)
```

**í•µì‹¬ ì°¨ì´**:
- Active Learning: ëª¨ë¸ì„ **í•™ìŠµ**ì‹œí‚´
- Active-Prompt: í”„ë¡¬í”„íŠ¸ë¥¼ **êµ¬ì„±**í•¨

**ê³µí†µì **:
- ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ì„ íƒ
- ì–´ë…¸í…Œì´ì…˜ ë¹„ìš© ìµœì†Œí™”
- ë°ì´í„° íš¨ìœ¨ì„±

## ì •ë¦¬ ë° ë‹¤ìŒ í¬ìŠ¤íŠ¸ ì˜ˆê³ 

### í•µì‹¬ ìš”ì•½

**Active-Prompt**:
- ëª¨ë¸ì´ ë¶ˆí™•ì‹¤í•œ ì˜ˆì‹œë¥¼ ì„ ë³„
- ê·¸ ì˜ˆì‹œë“¤ë§Œ ì‚¬ëŒì´ ì–´ë…¸í…Œì´ì…˜
- Few-shot í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©
- Random ëŒ€ë¹„ +3~6% ì„±ëŠ¥ í–¥ìƒ
- ì–´ë…¸í…Œì´ì…˜ ë¹„ìš© 30-50% ì ˆê°

**ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ë°©ë²•**:
- Self-Consistency: ê°€ì¥ ì •í™•, ë¹„ìš© ë†’ìŒ
- Entropy: ì´ë¡ ì  ê·¼ê±° ëª…í™•
- Perplexity: ë¹ ë¥´ê³  ì €ë ´
- Complexity: ë¬´ë£Œ, ì •í™•ë„ ì œí•œì 
- Hybrid: ì—¬ëŸ¬ ë°©ë²• ê²°í•©

**ì–¸ì œ ì‚¬ìš©í•  ê²ƒì¸ê°€**:
- âœ… ì–´ë…¸í…Œì´ì…˜ ë¹„ìš©ì´ ì¤‘ìš”í•  ë•Œ
- âœ… ì¶©ë¶„í•œ ì§ˆë¬¸ í’€ì´ ìˆì„ ë•Œ (50+)
- âœ… ëª…í™•í•œ í‰ê°€ ë©”íŠ¸ë¦­
- âœ… Few-shot í•™ìŠµì´ íš¨ê³¼ì ì¸ íƒœìŠ¤í¬

**ì–¸ì œ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒì¸ê°€**:
- âŒ ì§ˆë¬¸ í’€ì´ ë§¤ìš° ì ì„ ë•Œ (<20)
- âŒ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
- âŒ ë¶ˆí™•ì‹¤ì„± ì¸¡ì • ë¹„ìš©ì´ ê³¼ë„í•  ë•Œ

### ì‹¤ë¬´ ê¶Œì¥ì‚¬í•­

1. **íŒŒì¼ëŸ¿ í…ŒìŠ¤íŠ¸ë¡œ ê²€ì¦**
2. **ì˜ˆì‚°ì— ë§ëŠ” ë°©ë²• ì„ íƒ**
3. **ë¬¸ì„œí™” ë° ì¶”ì **
4. **A/B í…ŒìŠ¤íŒ…ìœ¼ë¡œ íš¨ê³¼ í™•ì¸**
5. **ì ì§„ì  ë„ì…** (Random â†’ Simple â†’ Full)

### ë‹¤ìŒ í¬ìŠ¤íŠ¸ ì˜ˆê³ 

ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„œëŠ” **Directional Stimulus Prompting**ì„ ë‹¤ë£¬ë‹¤:

- Policy LM + Stimulus LM êµ¬ì¡°
- íŠ¹ì • ë°©í–¥ìœ¼ë¡œ ì‘ë‹µ ìœ ë„
- íŒíŠ¸ ìƒì„± ë° í™œìš©
- Black-box ìµœì í™”
- ì‹¤ì „ êµ¬í˜„ ë° ì‚¬ë¡€

## ì°¸ê³ ë¬¸í—Œ

1. Diao, S., Wang, P., Lin, Y., & Zhang, T. (2023). **Active prompting with chain-of-thought for large language models.** *arXiv preprint arXiv:2302.12246*.

2. Settles, B. (2009). **Active learning literature survey.** *University of Wisconsin-Madison Department of Computer Sciences*.

3. Zhang, T., et al. (2022). **Active learning for natural language processing.** *EMNLP 2022 Tutorial*.

*ì´ í¬ìŠ¤íŠ¸ëŠ” ìµœì‹  ì—°êµ¬ì™€ ì‹¤ë¬´ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆë‹¤. Active-PromptëŠ” ì–´ë…¸í…Œì´ì…˜ ë¹„ìš©ì´ ì¤‘ìš”í•œ ìƒí™©ì—ì„œ ë§¤ìš° íš¨ê³¼ì ì´ë©°, íŠ¹íˆ Few-shot í•™ìŠµì´ ì˜ ì‘ë™í•˜ëŠ” íƒœìŠ¤í¬ì— ê°•ë ¥íˆ ê¶Œì¥ëœë‹¤.*

**í•µì‹¬ ë©”ì‹œì§€**: Active-PromptëŠ” ì–´ë…¸í…Œì´ì…˜ ë¹„ìš©ì„ 30-50% ì ˆê°í•˜ë©´ì„œë„ ì„±ëŠ¥ì„ 3-6% í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” íš¨ìœ¨ì ì¸ ê¸°ë²•ì´ë‹¤. íŠ¹íˆ Few-shot í•™ìŠµì´ íš¨ê³¼ì ì¸ íƒœìŠ¤í¬ì—ì„œ ê°•ë ¥íˆ ê¶Œì¥ëœë‹¤.
