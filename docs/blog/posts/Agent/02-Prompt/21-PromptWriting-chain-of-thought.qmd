---
title: "Few-Shot Prompting"
subtitle: 적은 예시로 큰 효과를 만드는 In-Context Learning의 원리와 실전
description: |
  Few-Shot Prompting의 정의부터 실전 활용까지 체계적으로 설명한다.
  GPT-3 논문(Brown et al. 2020)의 핵심 발견을 통해 모델 크기와 예시 개수의 상관관계,
  In-Context Learning의 메커니즘을 분석한다.
  "What Makes In-Context Learning Work?"(Min et al. 2022) 연구를 바탕으로
  예시의 품질(정답 라벨 vs 랜덤 라벨), 입력-라벨 형식의 중요성,
  4가지 핵심 요소(Format, Label Space, Input Distribution, Mapping)를 상세히 다룬다.
  감정 분류, 새로운 단어 학습, 일기 작성 등 실무 예시와 
  복잡한 추론 과제에서의 한계를 통해 Few-Shot의 적절한 활용 시나리오를 제시한다.
categories:
  - Prompt Engineering
  - LLM
  - AI
  - Agent
author: Kwangmin Kim
date: 01/28/2025
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---


# Chain-of-Thought (CoT) Prompting

## 들어가며

Few-shot prompting은 예시를 통해 모델의 성능을 향상시킬 수 있었지만, 복잡한 추론(reasoning) 문제에서는 여전히 한계가 있었습니다. 이번 글에서는 이러한 한계를 극복하기 위한 Chain-of-Thought (CoT) Prompting 기법에 대해 알아보겠습니다.

## Few-Shot Prompting의 한계

먼저 Few-shot prompting이 복잡한 추론 문제에서 어떤 한계를 보이는지 확인해보겠습니다.

**문제:** 이 그룹의 홀수들을 더하면 짝수가 됩니다: 15, 32, 5, 13, 82, 7, 1.

### Zero-shot prompting 시도
```
Q: 이 그룹의 홀수를 더하면 짝수가 된다. 15, 32, 5, 13, 82, 7, 1.
A: 네, 이 그룹의 홀수들을 더하면 107이 되는데, 이는 짝수입니다.
```
❌ 오답 (107은 홀수)

### Few-shot prompting 시도
```
이 그룹의 홀수들을 더하면 짝수가 됩니다: 4, 8, 9, 15, 12, 2, 1.
답: 거짓입니다.

이 그룹의 홀수들을 더하면 짝수가 됩니다: 17, 10, 19, 4, 8, 12, 24.
답: 참입니다.

이 그룹의 홀수들을 더하면 짝수가 됩니다: 16, 11, 14, 4, 8, 13, 24.
답: 참입니다.

이 그룹의 홀수들을 더하면 짝수가 됩니다: 17, 9, 10, 12, 13, 4, 2.
답: 거짓입니다.

이 그룹의 홀수들을 더하면 짝수가 됩니다: 15, 32, 5, 13, 82, 7, 1.
답:
```

**모델 출력:** 정답입니다.

❌ 여전히 오답 (15+5+13+7+1=41, 홀수이므로 거짓)

이처럼 복잡한 산술 추론 문제에서는 단순히 예시를 제공하는 것만으로는 충분하지 않습니다.

## Chain-of-Thought (CoT) Prompting이란?

Chain-of-Thought Prompting은 **모델이 최종 답에 도달하는 과정에서 중간 추론 단계를 거치도록 유도**하는 기법입니다.

### 핵심 아이디어
- 복잡한 과제 수행을 위해 LLM에게 더 자세한 안내 문구 제공
- 중간 추론 단계를 명시적으로 보여주는 예시 사용
- 답이 나오는 과정을 단계별로 설명하도록 유도

### CoT의 적용 분야
- 산술 문제 (Arithmetic reasoning)
- 상식 추론 (Commonsense reasoning)
- 기호 추론 (Symbolic reasoning)

## Standard Prompting vs CoT Prompting

같은 문제를 두 가지 방식으로 비교해보겠습니다.

### Standard Prompting

**입력:**
```
Q: Roger는 테니스공 5개를 가지고 있습니다. 
   그는 테니스공 3개가 들어있는 캔을 2개 더 삽니다. 
   Roger는 이제 테니스공을 몇 개 가지고 있습니까?

A: 정답은 11입니다.

Q: 카페테리아에 사과가 23개 있었습니다. 
   그들은 20개를 점심을 만드는 데 사용했고 6개를 더 샀습니다. 
   이제 사과가 몇 개 있습니까?
```

**출력:** 정답은 27입니다. ❌

### Chain-of-Thought Prompting

**입력:**
```
Q: Roger는 테니스공 5개를 가지고 있습니다. 
   그는 테니스공 3개가 들어있는 캔을 2개 더 삽니다. 
   Roger는 이제 테니스공을 몇 개 가지고 있습니까?

A: Roger는 5개의 공으로 시작했습니다. 
   테니스공 3개가 들어있는 캔 2개를 더 샀습니다.
   따라서 3 × 2 = 6개의 공을 더 샀습니다.
   5 + 6 = 11개. 정답은 11입니다.

Q: 카페테리아에 사과가 23개 있었습니다. 
   그들은 20개를 점심을 만드는 데 사용했고 6개를 더 샀습니다. 
   이제 사과가 몇 개 있습니까?
```

**출력:** 
```
카페테리아에 원래 사과가 23개 있었습니다. 
그들은 20개를 점심을 만드는 데 사용했습니다. 
그래서 23 - 20 = 3개가 남았습니다. 
그들은 6개를 더 샀기 때문에, 이제 3 + 6 = 9개가 있습니다. 
정답은 9입니다.
```
✅ 정답!

## CoT Prompting의 다양한 적용 사례

CoT는 산술 문제뿐만 아니라 다양한 추론 문제에 적용될 수 있습니다.

### 1. 산술 문제 (Math Word Problems)
- GSM8K 데이터셋의 초등학교 수준 수학 문제
- 단계별 계산 과정 표시

### 2. 상식 추론 (Commonsense Reasoning)
**예시:** "Sammy는 사람들이 많은 곳에 가고 싶어 합니다. 어디로 가야 할까요?"
- 선택지: (a) 경마장 (b) 인구 밀집 지역 (c) 사막 (d) 아파트 (e) 도로

**CoT 답변:** "답은 인구 밀집 지역이어야 합니다. 경마장, 사막, 아파트, 도로는 항상 많은 사람들이 있는 것은 아니지만, 인구 밀집 지역은 그렇습니다."

### 3. 기호 추론 (Symbolic Reasoning)
- 논리적 추론 단계를 명시
- 중간 결과를 단계별로 도출

### 4. 전략적 추론 (Strategy QA)
**예시:** "생쥐가 물에 빠지면 가라앉을까요?"
**CoT 답변:** "생쥐 한 마리의 밀도는 물보다 약간 높아서 물에서 가라앉습니다. 그래서 답은 '예'입니다."

### 5. 날짜 이해 (Date Understanding)
- 상대적 날짜 계산
- 시간 경과에 대한 추론

### 6. 스포츠 이해 (Sports Understanding)
**예시:** "다음 문장이 그럴듯한가요? 'Joao Moutinho가 NFC 챔피언십에서 스크린 패스를 잡았다.'"
**CoT 답변:** "Joao Moutinho는 축구 선수입니다. NFC 챔피언십은 미식축구의 일부입니다. 따라서 답은 '아니오'입니다."

## 연구 결과: Chain-of-Thought의 효과

### 핵심 연구: Wei et al. (2022)

Google Research의 연구팀은 "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" 논문에서 CoT의 효과를 체계적으로 분석했습니다.

### GSM8K 벤치마크 성능

GSM8K(Grade School Math 8K)는 초등학교 수준의 수학 단어 문제 8,500개로 구성된 데이터셋입니다.

**성능 비교:**
- Fine-tuned GPT-3 175B: 55%
- PaLM 540B (standard prompting): 18%
- **PaLM 540B (chain-of-thought prompting): 57%** ⭐

CoT를 사용했을 때 standard prompting 대비 **3배 이상의 성능 향상**을 보였습니다.

### 모델 크기에 따른 효과

CoT는 **모델의 파라미터가 충분히 클 때 효과적**입니다.

```
모델 크기별 GSM8K 정확도:
- 0.3B 파라미터: Standard ~0%, CoT ~0%
- 1.3B 파라미터: Standard ~0%, CoT ~5%
- 6.7B 파라미터: Standard ~5%, CoT ~20%
- 175B 파라미터: Standard ~20%, CoT ~60%
```

**주요 발견:**
- 작은 모델(~1B)에서는 CoT 효과가 미미
- 약 100B 이상의 파라미터에서 CoT가 큰 효과 발휘
- 일반적인 스케일링 곡선보다 더 가파른 성능 향상

### 다양한 추론 과제에서의 성능

**산술 추론 (Arithmetic Reasoning):**
- MultiArith: 17.7% → 78.7%
- GSM8K: 10.4% → 40.7%
- SVAMP: 실질적 성능 향상

**상식 추론 (Commonsense Reasoning):**
- CSQA: 약 20% 성능 향상
- StrategyQA: 지속적 향상
- Date Understanding: 명확한 개선

## 실습: CoT Prompting 적용하기

앞서 실패했던 홀수 합 문제를 CoT로 다시 시도해보겠습니다.

### 실습 1: 기본 CoT 적용

```
이 그룹의 홀수의 합이 짝수가 된다: 4, 8, 9, 15, 12, 2, 1.
A: 모든 홀수(9, 15, 1)를 더하면 25입니다.
   정답은 "거짓"입니다.

이 그룹의 홀수의 합이 짝수가 된다: 17, 10, 19, 4, 8, 12, 24.
A: 모든 홀수(17, 19)를 더하면 36입니다.
   정답은 "참"입니다.

이 그룹의 홀수의 합이 짝수가 된다: 16, 11, 14, 4, 8, 13, 24.
A: 모든 홀수(11, 13)를 더하면 24입니다.
   정답은 "참"입니다.

이 그룹의 홀수의 합이 짝수가 된다: 17, 9, 10, 12, 13, 4, 2.
A: 모든 홀수(17, 9, 13)를 더하면 39입니다.
   정답은 "거짓"입니다.

이 그룹의 홀수의 합이 짝수가 된다: 15, 32, 5, 13, 82, 7, 1.
A:
```

**모델 출력:**
```
모든 홀수(15, 5, 13, 7, 1)를 더하면 41입니다. 
홀수입니다. 
따라서 정답은 "거짓"입니다.
```

✅ **성공!** CoT를 통해 올바른 추론과 답을 도출했습니다.

### 실습 2: 더 복잡한 문제

OpenAI Playground에서 temperature를 0.1-0.3으로 설정하고 다음 프롬프트를 시도해보세요:

```
Q: 재닛의 오리들은 하루에 알을 16개 낳습니다. 
   재닛은 매일 아침 3개의 알을 먹고, 
   매일 4개의 알을 머핀을 굽는데 사용합니다. 
   나머지 알은 하나에 2달러씩 판매합니다. 
   재닛은 매일 얼마를 벌까요?

단계적으로 생각해봅시다.
```

**기대 출력:**
```
1. 재닛의 오리들은 하루에 16개의 알을 낳습니다.
2. 재닛은 3개를 먹고 4개를 머핀에 사용하므로, 3 + 4 = 7개를 사용합니다.
3. 남은 알은 16 - 7 = 9개입니다.
4. 각 알을 2달러에 팔므로, 9 × 2 = 18달러를 벌게 됩니다.

정답: 18달러
```

## CoT Prompting의 장점과 한계

### 장점

1. **복잡한 추론 문제 해결 능력 향상**
   - 다단계 계산이 필요한 문제에서 특히 효과적
   - 논리적 추론 과정을 명시적으로 표현

2. **모델의 추론 과정 가시화**
   - 어떻게 답에 도달했는지 이해 가능
   - 오류 지점 파악 용이

3. **범용성**
   - 산술, 상식, 기호 추론 등 다양한 영역에 적용
   - 특별한 모델 학습 없이 프롬프트만으로 구현

### 한계점

1. **모델 크기 의존성**
   - 약 100B 파라미터 이상에서만 효과적
   - 작은 모델에서는 성능 향상이 미미하거나 없음

2. **수작업 부담**
   - 사람이 직접 사고 과정을 문장으로 작성해야 함
   - 고품질 예시 작성에 시간과 노력 필요

3. **프롬프트 품질 의존성**
   - 프롬프트가 명확하지 않으면 성능 저하
   - 도메인별로 적절한 추론 단계 설계 필요

4. **추론 비용 증가**
   - 중간 단계를 모두 생성하므로 토큰 사용량 증가
   - 응답 시간 증가

## 마무리

Chain-of-Thought Prompting은 대형 언어 모델의 복잡한 추론 능력을 크게 향상시키는 획기적인 기법입니다. 특히 다단계 사고가 필요한 산술 문제나 논리적 추론 과제에서 탁월한 성능을 보입니다.

하지만 CoT에는 여전히 개선의 여지가 있습니다:
- 매번 수작업으로 추론 단계를 작성해야 하는가?
- 작은 모델에서도 효과적으로 사용할 수 있는 방법은?
- 프롬프트 작성 부담을 줄일 수 있는 방법은?

다음 글에서는 이러한 한계를 극복하기 위한 **Zero-Shot Chain-of-Thought** 기법에 대해 알아보겠습니다. "단계적으로 생각해봅시다"라는 간단한 문구 하나로 어떻게 CoT의 효과를 얻을 수 있는지 살펴보겠습니다.

## 참고문헌

- Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ... & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. *Advances in Neural Information Processing Systems, 35*, 24824-24837.

