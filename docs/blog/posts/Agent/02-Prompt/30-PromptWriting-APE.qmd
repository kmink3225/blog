---
title: "APEì™€ í”„ë¡¬í”„íŠ¸ ìë™ ìµœì í™”: LLMì´ ìŠ¤ìŠ¤ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ëŠ” ë°©ë²•"
subtitle: LLMì„ í™œìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ê³  ìµœì í™”í•˜ëŠ” ë©”íƒ€ ë ˆë²¨ ì ‘ê·¼ë²•
description: |
  Automatic Prompt Engineer (APE)ì™€ OPROì˜ ì •ì˜ë¶€í„° ì‹¤ì „ êµ¬í˜„ê¹Œì§€ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•œë‹¤.
  Zhou et al. (2022) "Large Language Models are Human-Level Prompt Engineers"ì™€
  Yang et al. (2023) "Large Language Models as Optimizers" ì—°êµ¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ
  í”„ë¡¬í”„íŠ¸ ìë™ ìƒì„±ì˜ ì›ë¦¬, 6ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°(í›„ë³´ ìƒì„±, ì‹¤í–‰, í‰ê°€, ì œê±°, ì¬ìƒ˜í”Œë§, ì„ íƒ),
  Forward/Reverse Mode ìƒì„± ë©”ì»¤ë‹ˆì¦˜, í‰ê°€ ë©”íŠ¸ë¦­ê³¼ ë°˜ë³µì  ê°œì„  ì „ëµì„ ë¶„ì„í•œë‹¤.
  Instruction Induction, BIG-Bench, GSM8K ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ ëŒ€ë¹„ ìµœëŒ€ 4.5% ì„±ëŠ¥ í–¥ìƒ ê²°ê³¼ë¥¼ ì œì‹œí•˜ê³ ,
  "Take a deep breath" ê°™ì€ ì˜ì™¸ì˜ íš¨ê³¼ì  í”„ë¡¬í”„íŠ¸ ë°œê²¬ ì‚¬ë¡€, ê°ì„± ë¶„ë¥˜/ëŒ€ê·œëª¨ ë¶„ë¥˜ ì‹œìŠ¤í…œ ë“± ì‹¤ë¬´ ì˜ˆì‹œì™€
  Python êµ¬í˜„ ì½”ë“œ(Anthropic Claude API í™œìš©)ë¥¼ í†µí•´ ì‹¤ì „ ìë™í™” ë°©ë²•ì„ ìƒì„¸íˆ ë‹¤ë£¬ë‹¤.
  ë¹„ìš©-ROI ë¶„ì„($0.72-$6.10 per prompt), ìˆ˜ë™ vs ìë™ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë¹„êµ,
  í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ê³¼ ê³¼ì í•© ë°©ì§€ ì „ëµ, ì§€ì†ì  ê°œì„  íŒ¨í„´ì„ ì œì‹œí•œë‹¤.
categories:
  - Prompt Engineering
  - LLM
  - AI
  - Agent
author: Kwangmin Kim
date: 02/07/2025
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

## ë“¤ì–´ê°€ë©°

ì§€ê¸ˆê¹Œì§€ ë‹¤ë£¬ ëª¨ë“  í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•ë“¤ì—ëŠ” ê³µí†µì ì´ ìˆë‹¤: **ì‚¬ëŒì´ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ê³„í•œë‹¤ëŠ” ê²ƒì´ë‹¤.** í•˜ì§€ë§Œ ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ëŠ” ê²ƒì€ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ê³ , ì£¼ê´€ì ì´ë©°, ë„ë©”ì¸ ì „ë¬¸ê°€ê°€ í•„ìš”í•˜ë‹¤.

ë§Œì•½ LLMì´ ìŠ¤ìŠ¤ë¡œ ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤ë©´ ì–´ë–¨ê¹Œ?

**Automatic Prompt Engineer (APE)**ëŠ” ë°”ë¡œ ì´ ì•„ì´ë””ì–´ë¥¼ êµ¬í˜„í•œë‹¤. LLMì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ê³ , í‰ê°€í•˜ê³ , ì„ íƒí•œë‹¤. ì´ëŠ” "LLMìœ¼ë¡œ LLMì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë§Œë“¤ê¸°"ë¼ëŠ” ë©”íƒ€ ë ˆë²¨ì˜ ì ‘ê·¼ë²•ì´ë‹¤.

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” APEì˜ ì›ë¦¬ë¶€í„° ìµœì‹  ë°œì „ ë°©í–¥ì¸ OPROê¹Œì§€, í”„ë¡¬í”„íŠ¸ ìë™ ìµœì í™”ì˜ ì „ë°˜ì„ ë‹¤ë£¬ë‹¤.

## Automatic Prompt Engineer (APE)ë€?

### í•µì‹¬ ê°œë…

> "ì£¼ì–´ì§„ ì…ë ¥-ì¶œë ¥ ì˜ˆì‹œë“¤ë¡œë¶€í„°, ê°€ì¥ íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆëŠ”ê°€?"

**ê¸°ë³¸ ì•„ì´ë””ì–´**:
```
Input-Output Examples â†’ [APE] â†’ Best Prompt
```

**ì˜ˆì‹œ**:
```
ì…ë ¥-ì¶œë ¥ ì˜ˆì‹œ:
- Input: "dog" â†’ Output: "animal"
- Input: "rose" â†’ Output: "plant"  
- Input: "iron" â†’ Output: "metal"

APEê°€ ì°¾ì•„ë‚¸ í”„ë¡¬í”„íŠ¸:
"Classify the following into its category:"
```

### ì™œ í•„ìš”í•œê°€?

**ë¬¸ì œ 1: ì‹œê°„ ì ˆì•½**
```
ì‚¬ëŒì´ í”„ë¡¬í”„íŠ¸ ì„¤ê³„:
- ì‹œë„ 1: "What is this?" â†’ ì„±ëŠ¥ 60%
- ì‹œë„ 2: "Identify the category" â†’ ì„±ëŠ¥ 70%
- ì‹œë„ 3: "Classify into type" â†’ ì„±ëŠ¥ 75%
- ...
- ì‹œë„ 20: "Classify the following into its category" â†’ ì„±ëŠ¥ 85%

â†’ 20ë²ˆ ì‹œë„, 2ì‹œê°„ ì†Œìš”
```

**APE ì‚¬ìš©**:
```
- ìë™ìœ¼ë¡œ 100ê°œ í›„ë³´ ìƒì„±
- ìë™ìœ¼ë¡œ í‰ê°€ ë° ì„ íƒ
- ìµœì  í”„ë¡¬í”„íŠ¸ ë°œê²¬

â†’ 5ë¶„ ì†Œìš”, ì„±ëŠ¥ 85%
```

**ë¬¸ì œ 2: ì¸ê°„ì˜ í¸í–¥**

ì‚¬ëŒì€ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì„ ì„ í˜¸í•˜ì§€ë§Œ, LLMì—ê²ŒëŠ” ë¶€ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì´ ë” íš¨ê³¼ì ì¼ ìˆ˜ ìˆë‹¤.

**ì‚¬ëŒì´ ì„ í˜¸í•˜ëŠ” í”„ë¡¬í”„íŠ¸**:
```
"Let's think step by step and solve this problem carefully."
â†’ ì„±ëŠ¥ 78%
```

**APEê°€ ì°¾ì€ í”„ë¡¬í”„íŠ¸** (OPRO ì—°êµ¬):
```
"Take a deep breath and work on this problem step-by-step."
â†’ ì„±ëŠ¥ 80.2%
```

"Take a deep breath"ì´ ì™œ íš¨ê³¼ì ì¸ì§€ëŠ” ëª…í™•í•˜ì§€ ì•Šì§€ë§Œ, ì‹¤ì œë¡œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

## APEì˜ 6ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°

Zhou et al. (2022)ì˜ ì› ë…¼ë¬¸ì— ë”°ë¥´ë©´, APEëŠ” ë‹¤ìŒ 6ë‹¨ê³„ë¡œ ì‘ë™í•œë‹¤.

### í›„ë³´ í”„ë¡¬í”„íŠ¸ ìƒì„± (Instruction Generation)

**ì…ë ¥**: ì…ë ¥-ì¶œë ¥ ì˜ˆì‹œë“¤
**ì¶œë ¥**: Nê°œì˜ í›„ë³´ í”„ë¡¬í”„íŠ¸

```python
import anthropic
from typing import List, Dict

class APE:
    """
    Automatic Prompt Engineer êµ¬í˜„
    """
    
    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.model = "claude-sonnet-4-20250514"
    
    def generate_candidate_prompts(
        self, 
        examples: List[Dict[str, str]],
        num_candidates: int = 10
    ) -> List[str]:
        """
        Step 1: í›„ë³´ í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            examples: [{"input": "...", "output": "..."}, ...]
            num_candidates: ìƒì„±í•  í›„ë³´ í”„ë¡¬í”„íŠ¸ ìˆ˜
        """
        # ì˜ˆì‹œë“¤ì„ í¬ë§·íŒ…
        examples_str = "\n".join([
            f"Input: {ex['input']}\nOutput: {ex['output']}"
            for ex in examples
        ])
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ìœ„í•œ ë©”íƒ€ í”„ë¡¬í”„íŠ¸
        generation_prompt = f"""I gave a language model the following examples:

        {examples_str}

        I want you to generate {num_candidates} different instruction prompts that would lead the model to produce these outputs from these inputs.

        Each instruction should be clear, concise, and effective.

        Generate {num_candidates} instruction prompts (one per line):
        1."""
        
        message = self.client.messages.create(
            model=self.model,
            max_tokens=1000,
            temperature=0.8,  # ë‹¤ì–‘ì„±ì„ ìœ„í•´ ë†’ì€ temperature
            messages=[{"role": "user", "content": generation_prompt}]
        )
        
        response = message.content[0].text
        
        # í›„ë³´ íŒŒì‹±
        candidates = []
        for line in response.split('\n'):
            import re
            # ë²ˆí˜¸ ì œê±° (1., 2., etc.)
            line = re.sub(r'^\d+\.\s*', '', line.strip())
            
            if line and len(line) > 10:  # ë„ˆë¬´ ì§§ì€ ê²ƒ ì œì™¸
                candidates.append(line)
        
        return candidates[:num_candidates]
```

**ìƒì„± ì˜ˆì‹œ**:

```python
examples = [
    {"input": "happy", "output": "positive"},
    {"input": "sad", "output": "negative"},
    {"input": "angry", "output": "negative"},
    {"input": "joyful", "output": "positive"}
]

ape = APE(api_key="your-api-key")
candidates = ape.generate_candidate_prompts(examples, num_candidates=5)

# ì¶œë ¥:
# 1. "Classify the sentiment of the following word as positive or negative."
# 2. "Determine whether the emotion expressed is positive or negative."
# 3. "Analyze the sentiment: positive or negative?"
# 4. "Is this word expressing a positive or negative feeling?"
# 5. "Categorize the emotional tone as either positive or negative."
```

### í›„ë³´ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ (Execution)

ê° í›„ë³´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‹¤ì œë¡œ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤.

```python
    def execute_prompt(
        self, 
        prompt: str, 
        test_input: str
    ) -> str:
        """
        Step 2: í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
        
        Args:
            prompt: í›„ë³´ í”„ë¡¬í”„íŠ¸
            test_input: í…ŒìŠ¤íŠ¸ ì…ë ¥
        
        Returns:
            ëª¨ë¸ì˜ ì¶œë ¥
        """
        full_prompt = f"{prompt}\n\nInput: {test_input}\nOutput:"
        
        message = self.client.messages.create(
            model=self.model,
            max_tokens=100,
            temperature=0,  # í‰ê°€ë¥¼ ìœ„í•´ deterministic
            messages=[{"role": "user", "content": full_prompt}]
        )
        
        return message.content[0].text.strip()
```

### í”„ë¡¬í”„íŠ¸ í‰ê°€ (Scoring)

ê° í›„ë³´ê°€ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ ì ìˆ˜ë¥¼ ë§¤ê¸´ë‹¤.

```python
    def score_prompt(
        self,
        prompt: str,
        test_examples: List[Dict[str, str]]
    ) -> float:
        """
        Step 3: í”„ë¡¬í”„íŠ¸ ì ìˆ˜ ê³„ì‚°
        
        Args:
            prompt: í‰ê°€í•  í”„ë¡¬í”„íŠ¸
            test_examples: í…ŒìŠ¤íŠ¸ ì˜ˆì‹œë“¤
        
        Returns:
            ì •í™•ë„ (0~1)
        """
        correct = 0
        total = len(test_examples)
        
        for example in test_examples:
            test_input = example['input']
            expected_output = example['output']
            
            # í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
            actual_output = self.execute_prompt(prompt, test_input)
            
            # ì •í™•ë„ ì²´í¬ (ë‹¨ìˆœ ë§¤ì¹­ - ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë“± ì ì ˆí•œ ë©”íŠ¸ë¦­ ê²°ì •)
            if actual_output.lower().strip() == expected_output.lower().strip():
                correct += 1
        
        accuracy = correct / total
        return accuracy
```

### í•˜ìœ„ í›„ë³´ ì œê±° (Pruning)

ì„±ëŠ¥ì´ ë‚®ì€ í›„ë³´ë“¤ì„ ì œê±°í•œë‹¤.

```python
    def prune_candidates(
        self,
        candidates: List[str],
        scores: List[float],
        keep_ratio: float = 0.5
    ) -> List[str]:
        """
        Step 4: í•˜ìœ„ í›„ë³´ ì œê±°
        
        Args:
            candidates: í›„ë³´ í”„ë¡¬í”„íŠ¸ë“¤
            scores: ê° í›„ë³´ì˜ ì ìˆ˜
            keep_ratio: ìœ ì§€í•  ë¹„ìœ¨ (0~1)
        
        Returns:
            ìƒìœ„ í›„ë³´ë“¤
        """
        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        scored_candidates = list(zip(candidates, scores))
        scored_candidates.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ keep_ratioë§Œí¼ ìœ ì§€
        keep_count = max(1, int(len(candidates) * keep_ratio))
        top_candidates = [c for c, s in scored_candidates[:keep_count]]
        
        return top_candidates
```

### í›„ë³´ ìƒ˜í”Œë§ ë° ê°œì„  (Resampling & Improving)

ìƒìœ„ í›„ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ í›„ë³´ë¥¼ ìƒì„±í•œë‹¤.

```python
    def resample_from_top(
        self,
        top_candidates: List[str],
        num_new: int = 10
    ) -> List[str]:
        """
        Step 5: ìƒìœ„ í›„ë³´ ê¸°ë°˜ ìƒˆ í›„ë³´ ìƒì„±
        
        Args:
            top_candidates: ìƒìœ„ í”„ë¡¬í”„íŠ¸ë“¤
            num_new: ìƒì„±í•  ìƒˆ í›„ë³´ ìˆ˜
        
        Returns:
            ê°œì„ ëœ í›„ë³´ë“¤
        """
        top_str = "\n".join([f"- {c}" for c in top_candidates])
        
        resample_prompt = f"""These are the best-performing instruction prompts so far:

{top_str}

Generate {num_new} new instruction prompts that are:
1. Similar to these successful prompts
2. But with variations that might improve performance
3. Clear and concise

New prompts (one per line):
1."""
        
        message = self.client.messages.create(
            model=self.model,
            max_tokens=1000,
            temperature=0.8,
            messages=[{"role": "user", "content": resample_prompt}]
        )
        
        response = message.content[0].text
        
        # ìƒˆ í›„ë³´ íŒŒì‹±
        new_candidates = []
        for line in response.split('\n'):
            import re
            line = re.sub(r'^\d+\.\s*', '', line.strip())
            
            if line and len(line) > 10:
                new_candidates.append(line)
        
        return new_candidates[:num_new]
```

### ìµœì¢… ì„ ì • (Selection)

ìµœê³  ì„±ëŠ¥ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•œë‹¤.

```python
    def select_best(
        self,
        candidates: List[str],
        scores: List[float]
    ) -> Dict[str, any]:
        """
        Step 6: ìµœê³  í”„ë¡¬í”„íŠ¸ ì„ ì •
        
        Returns:
            ìµœê³  í”„ë¡¬í”„íŠ¸ì™€ ì ìˆ˜
        """
        best_idx = scores.index(max(scores))
        
        return {
            'prompt': candidates[best_idx],
            'score': scores[best_idx],
            'rank': 1
        }
```

### ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©

```python
    def optimize_prompt(
        self,
        train_examples: List[Dict[str, str]],
        test_examples: List[Dict[str, str]],
        num_candidates: int = 20,
        num_iterations: int = 3
    ) -> Dict:
        """
        APE ì „ì²´ íŒŒì´í”„ë¼ì¸
        
        Args:
            train_examples: í”„ë¡¬í”„íŠ¸ ìƒì„±ì— ì‚¬ìš©í•  ì˜ˆì‹œ
            test_examples: í‰ê°€ì— ì‚¬ìš©í•  ì˜ˆì‹œ
            num_candidates: ì´ˆê¸° í›„ë³´ ìˆ˜
            num_iterations: ë°˜ë³µ íšŸìˆ˜
        
        Returns:
            ìµœì  í”„ë¡¬í”„íŠ¸ ë° ë©”íŠ¸ë¦­
        """
        print(f" APE ì‹œì‘")
        print(f"  í›ˆë ¨ ì˜ˆì‹œ: {len(train_examples)}ê°œ")
        print(f"  í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ: {len(test_examples)}ê°œ")
        print(f"  ì´ˆê¸° í›„ë³´: {num_candidates}ê°œ")
        print(f"  ë°˜ë³µ íšŸìˆ˜: {num_iterations}\n")
        
        # Step 1: ì´ˆê¸° í›„ë³´ ìƒì„±
        print("=" * 80)
        print("Step 1: ì´ˆê¸° í›„ë³´ ìƒì„±")
        print("=" * 80)
        
        candidates = self.generate_candidate_prompts(
            train_examples, 
            num_candidates=num_candidates
        )
        
        print(f"âœ… {len(candidates)}ê°œ í›„ë³´ ìƒì„± ì™„ë£Œ\n")
        
        # ë°˜ë³µì  ê°œì„ 
        for iteration in range(num_iterations):
            print("=" * 80)
            print(f"Iteration {iteration + 1}/{num_iterations}")
            print("=" * 80)
            
            # Step 2 & 3: í‰ê°€
            print(f"ğŸ“Š í›„ë³´ í‰ê°€ ì¤‘... ({len(candidates)}ê°œ)")
            
            scores = []
            for i, candidate in enumerate(candidates):
                score = self.score_prompt(candidate, test_examples)
                scores.append(score)
                
                if (i + 1) % 5 == 0:
                    print(f"  {i+1}/{len(candidates)} ì™„ë£Œ")
            
            print(f"âœ… í‰ê°€ ì™„ë£Œ\n")
            
            # ìƒìœ„ í›„ë³´ ì¶œë ¥
            scored = list(zip(candidates, scores))
            scored.sort(key=lambda x: x[1], reverse=True)
            
            print(f"ğŸ† ìƒìœ„ 3ê°œ í›„ë³´:")
            for i, (cand, score) in enumerate(scored[:3], 1):
                print(f"  [{i}] ì ìˆ˜: {score:.3f}")
                print(f"      í”„ë¡¬í”„íŠ¸: {cand}")
            print()
            
            # Step 4: í•˜ìœ„ ì œê±°
            if iteration < num_iterations - 1:  # ë§ˆì§€ë§‰ ë°˜ë³µì´ ì•„ë‹ˆë©´
                top_candidates = self.prune_candidates(
                    candidates, 
                    scores, 
                    keep_ratio=0.3
                )
                
                print(f"âœ‚ï¸  í•˜ìœ„ ì œê±°: {len(candidates)} â†’ {len(top_candidates)}ê°œ\n")
                
                # Step 5: ì¬ìƒ˜í”Œë§
                print(f"ğŸ”„ ìƒˆ í›„ë³´ ìƒì„± ì¤‘...")
                new_candidates = self.resample_from_top(
                    top_candidates,
                    num_new=num_candidates
                )
                
                print(f"âœ… {len(new_candidates)}ê°œ ìƒˆ í›„ë³´ ìƒì„±\n")
                
                candidates = new_candidates
        
        # Step 6: ìµœì¢… ì„ ì •
        print("=" * 80)
        print("ìµœì¢… ê²°ê³¼")
        print("=" * 80)
        
        best = self.select_best(candidates, scores)
        
        print(f"\nğŸ¯ ìµœì  í”„ë¡¬í”„íŠ¸:")
        print(f"   {best['prompt']}")
        print(f"\nğŸ“ˆ ì„±ëŠ¥: {best['score']:.3f}\n")
        
        return {
            'best_prompt': best['prompt'],
            'best_score': best['score'],
            'all_candidates': candidates,
            'all_scores': scores
        }


# ì‚¬ìš© ì˜ˆì‹œ
def main():
    # í›ˆë ¨ ì˜ˆì‹œ (í”„ë¡¬í”„íŠ¸ ìƒì„±ìš©)
    train_examples = [
        {"input": "happy", "output": "positive"},
        {"input": "sad", "output": "negative"},
        {"input": "joyful", "output": "positive"},
        {"input": "angry", "output": "negative"}
    ]
    
    # í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ (í‰ê°€ìš©)
    test_examples = [
        {"input": "excited", "output": "positive"},
        {"input": "depressed", "output": "negative"},
        {"input": "content", "output": "positive"},
        {"input": "frustrated", "output": "negative"},
        {"input": "delighted", "output": "positive"},
        {"input": "miserable", "output": "negative"}
    ]
    
    # APE ì‹¤í–‰
    ape = APE(api_key="your-api-key")
    
    result = ape.optimize_prompt(
        train_examples=train_examples,
        test_examples=test_examples,
        num_candidates=10,
        num_iterations=3
    )
    
    # ìµœì  í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    best_prompt = result['best_prompt']
    test_input = "thrilled"
    
    print(f"í…ŒìŠ¤íŠ¸:")
    print(f"  í”„ë¡¬í”„íŠ¸: {best_prompt}")
    print(f"  ì…ë ¥: {test_input}")
    
    output = ape.execute_prompt(best_prompt, test_input)
    print(f"  ì¶œë ¥: {output}")


if __name__ == "__main__":
    main()
```

### ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ

```
ğŸš€ APE ì‹œì‘
  í›ˆë ¨ ì˜ˆì‹œ: 4ê°œ
  í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ: 6ê°œ
  ì´ˆê¸° í›„ë³´: 10ê°œ
  ë°˜ë³µ íšŸìˆ˜: 3

================================================================================
Step 1: ì´ˆê¸° í›„ë³´ ìƒì„±
================================================================================
âœ… 10ê°œ í›„ë³´ ìƒì„± ì™„ë£Œ

================================================================================
Iteration 1/3
================================================================================
ğŸ“Š í›„ë³´ í‰ê°€ ì¤‘... (10ê°œ)
  5/10 ì™„ë£Œ
  10/10 ì™„ë£Œ
âœ… í‰ê°€ ì™„ë£Œ

ğŸ† ìƒìœ„ 3ê°œ í›„ë³´:
  [1] ì ìˆ˜: 0.833
      í”„ë¡¬í”„íŠ¸: Classify the sentiment of the following word as positive or negative.
  [2] ì ìˆ˜: 0.833
      í”„ë¡¬í”„íŠ¸: Determine whether this emotion is positive or negative.
  [3] ì ìˆ˜: 0.667
      í”„ë¡¬í”„íŠ¸: Is this word expressing a positive or negative feeling?

âœ‚ï¸  í•˜ìœ„ ì œê±°: 10 â†’ 3ê°œ

ğŸ”„ ìƒˆ í›„ë³´ ìƒì„± ì¤‘...
âœ… 10ê°œ ìƒˆ í›„ë³´ ìƒì„±

================================================================================
Iteration 2/3
================================================================================
ğŸ“Š í›„ë³´ í‰ê°€ ì¤‘... (10ê°œ)
  5/10 ì™„ë£Œ
  10/10 ì™„ë£Œ
âœ… í‰ê°€ ì™„ë£Œ

ğŸ† ìƒìœ„ 3ê°œ í›„ë³´:
  [1] ì ìˆ˜: 1.000
      í”„ë¡¬í”„íŠ¸: Categorize the emotional tone as either positive or negative.
  [2] ì ìˆ˜: 0.833
      í”„ë¡¬í”„íŠ¸: Classify the sentiment expressed as positive or negative.
  [3] ì ìˆ˜: 0.833
      í”„ë¡¬í”„íŠ¸: Analyze whether the word conveys a positive or negative emotion.

âœ‚ï¸  í•˜ìœ„ ì œê±°: 10 â†’ 3ê°œ

ğŸ”„ ìƒˆ í›„ë³´ ìƒì„± ì¤‘...
âœ… 10ê°œ ìƒˆ í›„ë³´ ìƒì„±

================================================================================
Iteration 3/3
================================================================================
ğŸ“Š í›„ë³´ í‰ê°€ ì¤‘... (10ê°œ)
  5/10 ì™„ë£Œ
  10/10 ì™„ë£Œ
âœ… í‰ê°€ ì™„ë£Œ

ğŸ† ìƒìœ„ 3ê°œ í›„ë³´:
  [1] ì ìˆ˜: 1.000
      í”„ë¡¬í”„íŠ¸: Categorize the emotional tone as either positive or negative.
  [2] ì ìˆ˜: 1.000
      í”„ë¡¬í”„íŠ¸: Determine if the word expresses a positive or negative sentiment.
  [3] ì ìˆ˜: 0.833
      í”„ë¡¬í”„íŠ¸: Classify this emotional word as positive or negative.

================================================================================
ìµœì¢… ê²°ê³¼
================================================================================

ğŸ¯ ìµœì  í”„ë¡¬í”„íŠ¸:
   Categorize the emotional tone as either positive or negative.

ğŸ“ˆ ì„±ëŠ¥: 1.000

í…ŒìŠ¤íŠ¸:
  í”„ë¡¬í”„íŠ¸: Categorize the emotional tone as either positive or negative.
  ì…ë ¥: thrilled
  ì¶œë ¥: positive
```

## OPRO: Optimization by PROmpting

APEì˜ í›„ì† ì—°êµ¬ì¸ **OPRO (Optimization by PROmpting)**ëŠ” Google DeepMindì˜ Yang et al. (2023)ì´ ì œì•ˆí–ˆë‹¤.

### í•µì‹¬ ì•„ì´ë””ì–´

OPROëŠ” í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¥¼ **ìµœì í™” ë¬¸ì œ**ë¡œ ë³¸ë‹¤:

$$
\text{maximize}_{p \in \mathcal{P}} \quad f(p)
$$

ì—¬ê¸°ì„œ:
- $p$: í”„ë¡¬í”„íŠ¸
- $\mathcal{P}$: ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ ê³µê°„
- $f(p)$: í”„ë¡¬í”„íŠ¸ì˜ ì„±ëŠ¥ (ì˜ˆ: ì •í™•ë„)

**OPROì˜ ì ‘ê·¼ë²•**: LLMì„ "optimizer"ë¡œ ì‚¬ìš©

```
Optimization History â†’ [LLM Optimizer] â†’ Improved Prompt
```

### OPROì˜ ì‘ë™ ë°©ì‹

```python
class OPRO:
    """
    Optimization by PROmpting
    """
    
    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.model = "claude-sonnet-4-20250514"
        self.optimization_history = []
    
    def optimize_prompt(
        self,
        initial_prompt: str,
        test_examples: List[Dict[str, str]],
        num_iterations: int = 10
    ) -> Dict:
        """
        OPRO ìµœì í™” ë£¨í”„
        
        Args:
            initial_prompt: ì´ˆê¸° í”„ë¡¬í”„íŠ¸
            test_examples: í‰ê°€ ì˜ˆì‹œë“¤
            num_iterations: ìµœì í™” ë°˜ë³µ íšŸìˆ˜
        
        Returns:
            ìµœì  í”„ë¡¬í”„íŠ¸ ë° íˆìŠ¤í† ë¦¬
        """
        print(f"ğŸ¯ OPRO ì‹œì‘")
        print(f"  ì´ˆê¸° í”„ë¡¬í”„íŠ¸: {initial_prompt}")
        print(f"  ë°˜ë³µ íšŸìˆ˜: {num_iterations}\n")
        
        current_prompt = initial_prompt
        
        for iteration in range(num_iterations):
            print(f"{'='*80}")
            print(f"Iteration {iteration + 1}/{num_iterations}")
            print(f"{'='*80}")
            
            # Step 1: í˜„ì¬ í”„ë¡¬í”„íŠ¸ í‰ê°€
            score = self._evaluate_prompt(current_prompt, test_examples)
            
            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.optimization_history.append({
                'prompt': current_prompt,
                'score': score,
                'iteration': iteration + 1
            })
            
            print(f"ğŸ“Š í˜„ì¬ í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥: {score:.3f}")
            print(f"   í”„ë¡¬í”„íŠ¸: {current_prompt}\n")
            
            # Step 2: ìµœì í™” ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìƒì„±
            meta_prompt = self._create_meta_prompt()
            
            # Step 3: LLMì´ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
            improved_prompt = self._generate_improved_prompt(meta_prompt)
            
            print(f"ğŸ”„ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì œì•ˆ:")
            print(f"   {improved_prompt}\n")
            
            # Step 4: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì—…ë°ì´íŠ¸
            current_prompt = improved_prompt
        
        # ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        best = max(self.optimization_history, key=lambda x: x['score'])
        
        print(f"{'='*80}")
        print("ìµœì¢… ê²°ê³¼")
        print(f"{'='*80}")
        print(f"\nğŸ† ìµœì  í”„ë¡¬í”„íŠ¸ (Iteration {best['iteration']}):")
        print(f"   {best['prompt']}")
        print(f"\nğŸ“ˆ ìµœê³  ì„±ëŠ¥: {best['score']:.3f}")
        
        return {
            'best_prompt': best['prompt'],
            'best_score': best['score'],
            'history': self.optimization_history
        }
    
    def _create_meta_prompt(self) -> str:
        """
        ìµœì í™”ë¥¼ ìœ„í•œ ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        íˆìŠ¤í† ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì—ê²Œ ê°œì„  ë°©í–¥ ì œì‹œ
        """
        # íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
        history_str = ""
        for entry in self.optimization_history[-5:]:  # ìµœê·¼ 5ê°œë§Œ
            history_str += f"Prompt: {entry['prompt']}\n"
            history_str += f"Score: {entry['score']:.3f}\n\n"
        
        meta_prompt = f"""You are a prompt optimizer. Your goal is to improve instruction prompts to maximize their performance.

        Previous prompts and their scores:
        {history_str}

        Based on the history above, generate an improved prompt that:
        1. Keeps what works well from high-scoring prompts
        2. Fixes issues from low-scoring prompts
        3. Is clear, concise, and specific
        4. Aims to achieve a higher score

        Generate ONE improved prompt (do not explain, just output the prompt):"""
        
        return meta_prompt
    
    def _generate_improved_prompt(self, meta_prompt: str) -> str:
        """
        LLMìœ¼ë¡œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        """
        message = self.client.messages.create(
            model=self.model,
            max_tokens=200,
            temperature=0.7,
            messages=[{"role": "user", "content": meta_prompt}]
        )
        
        improved_prompt = message.content[0].text.strip()
        
        # ë¶ˆí•„ìš”í•œ ì„¤ëª… ì œê±° (ìˆë‹¤ë©´)
        if '\n' in improved_prompt:
            improved_prompt = improved_prompt.split('\n')[0]
        
        return improved_prompt
    
    def _evaluate_prompt(
        self, 
        prompt: str, 
        test_examples: List[Dict[str, str]]
    ) -> float:
        """
        í”„ë¡¬í”„íŠ¸ í‰ê°€
        """
        correct = 0
        
        for example in test_examples:
            full_prompt = f"{prompt}\n\nInput: {example['input']}\nOutput:"
            
            message = self.client.messages.create(
                model=self.model,
                max_tokens=50,
                temperature=0,
                messages=[{"role": "user", "content": full_prompt}]
            )
            
            output = message.content[0].text.strip().lower()
            expected = example['output'].lower()
            
            if output == expected:
                correct += 1
        
        return correct / len(test_examples)


# ì‚¬ìš© ì˜ˆì‹œ
def test_opro():
    test_examples = [
        {"input": "excited", "output": "positive"},
        {"input": "depressed", "output": "negative"},
        {"input": "content", "output": "positive"},
        {"input": "frustrated", "output": "negative"},
        {"input": "delighted", "output": "positive"},
        {"input": "miserable", "output": "negative"}
    ]
    
    opro = OPRO(api_key="your-api-key")
    
    result = opro.optimize_prompt(
        initial_prompt="Classify the sentiment",
        test_examples=test_examples,
        num_iterations=10
    )
    
    # ì„±ëŠ¥ ì¶”ì´ ì‹œê°í™”
    import matplotlib.pyplot as plt
    
    iterations = [h['iteration'] for h in result['history']]
    scores = [h['score'] for h in result['history']]
    
    plt.figure(figsize=(10, 6))
    plt.plot(iterations, scores, marker='o')
    plt.xlabel('Iteration')
    plt.ylabel('Score')
    plt.title('OPRO Optimization Progress')
    plt.grid(True)
    plt.savefig('opro_progress.png')
    
    print("\nğŸ“Š ì„±ëŠ¥ ì¶”ì´ ê·¸ë˜í”„ ì €ì¥: opro_progress.png")


if __name__ == "__main__":
    test_opro()
```

### "Take a deep breath" ë°œê²¬

OPRO ì—°êµ¬ì—ì„œ ê°€ì¥ í¥ë¯¸ë¡œìš´ ë°œê²¬ì€ ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ë‹¤:

```
"Take a deep breath and work on this problem step-by-step."
```

ì´ í”„ë¡¬í”„íŠ¸ëŠ” GSM8K (ìˆ˜í•™ ë¬¸ì œ) ë²¤ì¹˜ë§ˆí¬ì—ì„œ **80.2%**ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆë‹¤.

**ë¹„êµ**:
- "Let's think step by step.": 78.2%
- "Take a deep breath and work on this problem step-by-step.": 80.2%

**ì™œ íš¨ê³¼ì ì¸ê°€?**

ì´ë¡ ì  ì„¤ëª…ì€ ì•„ì§ ë¶ˆë¶„ëª…í•˜ì§€ë§Œ, ê°€ì„¤ë“¤:

1. **í† í° ë¶„í¬ ë³€í™”**: "Take a deep breath"ê°€ ëª¨ë¸ì˜ ë‹¤ìŒ í† í° ì˜ˆì¸¡ ë¶„í¬ë¥¼ ë³€ê²½
2. **ì£¼ì˜ ì§‘ì¤‘**: ëª¨ë¸ì´ ë¬¸ì œì— ë” ì§‘ì¤‘í•˜ë„ë¡ ìœ ë„
3. **ìš°ì—°**: íŠ¹ì • í•™ìŠµ ë°ì´í„°ì™€ì˜ ìƒí˜¸ì‘ìš©

**ì¤‘ìš”í•œ êµí›ˆ**: ì¸ê°„ì˜ ì§ê´€ìœ¼ë¡œëŠ” ì°¾ê¸° ì–´ë ¤ìš´ í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ ìµœì í™”ë¡œ ë°œê²¬í•  ìˆ˜ ìˆë‹¤.

## APE ì‹¤í—˜ ê²°ê³¼ ë¶„ì„

Zhou et al. (2022)ì˜ ì› ë…¼ë¬¸ì—ì„œëŠ” APEë¥¼ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ í…ŒìŠ¤íŠ¸í–ˆë‹¤.

### ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥

#### Instruction Induction

24ê°œì˜ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± í‰ê°€.

**íƒœìŠ¤í¬ ì˜ˆì‹œ**:
- Antonyms (ë°˜ì˜ì–´): "good" â†’ "bad"
- Larger Animal (í° ë™ë¬¼): "cat, elephant" â†’ "elephant"
- Cause Selection (ì›ì¸ ì„ íƒ): "premise: ..., choice1: ..., choice2: ..." â†’ "choice1"

**ê²°ê³¼**:

| ë°©ë²• | í‰ê·  ì •í™•ë„ |
|-----|-----------|
| Human-written prompts | 74.0% |
| APE (forward mode) | 76.0% |
| APE (reverse mode) | 77.0% |
| APE (best) | 78.5% |

**í•µì‹¬ ë°œê²¬**:
- âœ… APEê°€ ì‚¬ëŒì´ ì‘ì„±í•œ í”„ë¡¬í”„íŠ¸ë³´ë‹¤ **ìµœëŒ€ 4.5% ë†’ì€ ì„±ëŠ¥**
- âœ… ì¼ë¶€ íƒœìŠ¤í¬ì—ì„œëŠ” 10%+ ì°¨ì´
- âœ… ì¸ê°„ì´ ë†“ì¹œ íš¨ê³¼ì ì¸ í‘œí˜„ ë°œê²¬

**êµ¬ì²´ì  ì˜ˆì‹œ**:

```
íƒœìŠ¤í¬: Antonyms (ë°˜ì˜ì–´ ì°¾ê¸°)

Human prompt:
"Write the opposite of the following word:"
â†’ ì •í™•ë„: 82%

APE-generated prompt:
"Provide an antonym for the given word:"
â†’ ì •í™•ë„: 89%

ì°¨ì´: 7% í–¥ìƒ
```

#### BIG-Bench

êµ¬ê¸€ì˜ BIG-Benchì—ì„œ 21ê°œ íƒœìŠ¤í¬ í‰ê°€.

**ê²°ê³¼ ìš”ì•½**:

| íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬ | APE ìŠ¹ë¥  |
|--------------|---------|
| Reasoning | 65% |
| Language Understanding | 71% |
| Common Sense | 58% |
| Mathematics | 52% |

**ì „ì²´ì ìœ¼ë¡œ**: APEê°€ ì‚¬ëŒì´ ì‘ì„±í•œ í”„ë¡¬í”„íŠ¸ë³´ë‹¤ **62%ì˜ íƒœìŠ¤í¬ì—ì„œ ìš°ìˆ˜**

### Forward vs Reverse Mode

APEëŠ” ë‘ ê°€ì§€ ëª¨ë“œë¡œ ì‘ë™í•œë‹¤:

#### Forward Mode (ìˆœë°©í–¥)
```
Input-Output Examples â†’ Generate Instruction
```

**í”„ë¡¬í”„íŠ¸**:
```
I gave a friend an instruction and some inputs. 
The friend read the instruction and produced these outputs:

Input: dog
Output: animal

Input: rose  
Output: plant

What was the instruction?
```

#### Reverse Mode (ì—­ë°©í–¥)
```
Generate Instruction â†’ Verify with Examples
```

**í”„ë¡¬í”„íŠ¸**:
```
I instructed a friend to classify objects into categories.

I gave the instruction "{{INSTRUCTION}}"

Would the friend produce these outputs?
Input: dog â†’ Output: animal
Input: rose â†’ Output: plant

(yes/no evaluation)
```

**ì„±ëŠ¥ ë¹„êµ**:
- Forward mode: 76.0%
- Reverse mode: 77.0%
- **Reverse modeê°€ ì•½ê°„ ë” ìš°ìˆ˜**

ì´ìœ : Reverse modeëŠ” ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ˆì‹œë¡œ ê²€ì¦í•˜ëŠ” ë‹¨ê³„ê°€ ì¶”ê°€ë˜ì–´ ë” ì •í™•í•˜ë‹¤.

## OPRO ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥

Yang et al. (2023)ì˜ OPRO ì—°êµ¬ëŠ” ë” ì²´ê³„ì ì¸ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆë‹¤.

### GSM8K (ìˆ˜í•™ ë¬¸ì œ)

**ë°ì´í„°ì…‹**: ì´ˆë“±í•™êµ ìˆ˜í•™ ë¬¸ì œ 8,500ê°œ

**ê²°ê³¼**:

| Prompt | Accuracy |
|--------|----------|
| Zero-shot | 71.8% |
| "Let's think step by step." | 78.2% |
| "Let's work this out in a step by step way to be sure we have the right answer." | 79.5% |
| **"Take a deep breath and work on this problem step-by-step."** | **80.2%** |

**ê°œì„ í­**: Baseline ëŒ€ë¹„ +8.4%

**ë°œê²¬ëœ íš¨ê³¼ì  í”„ë¡¬í”„íŠ¸ë“¤**:
```
1. "Take a deep breath and work on this problem step-by-step."
   â†’ 80.2%

2. "Let's solve this problem by splitting it into steps."
   â†’ 79.9%

3. "First, let's think about what we know and what we need to find."
   â†’ 79.7%

4. "Break this problem down into smaller, manageable steps."
   â†’ 79.5%
```

### "Take a deep breath" ìƒì„¸ ë¶„ì„

ì´ í”„ë¡¬í”„íŠ¸ê°€ ì™œ íš¨ê³¼ì ì¸ì§€ ë” ê¹Šì´ ë¶„ì„í•´ë³´ì.

#### ê°€ì„¤ 1: í† í° ê¸¸ì´ ì¦ê°€

**ì´ë¡ **: ë” ê¸´ í”„ë¡¬í”„íŠ¸ê°€ ëª¨ë¸ì—ê²Œ ë” ë§ì€ "ìƒê°í•  ì‹œê°„"ì„ ì¤€ë‹¤.

**ê²€ì¦**:
```python
prompts = [
    "Solve this problem.",  # 4 tokens â†’ 71.8%
    "Let's think step by step.",  # 6 tokens â†’ 78.2%
    "Take a deep breath and work on this problem step-by-step.",  # 13 tokens â†’ 80.2%
    "Take a very very very deep breath and work on this problem step-by-step carefully.",  # 18 tokens â†’ 79.1%
]
```

**ê²°ê³¼**: ë‹¨ìˆœíˆ ê¸¸ë‹¤ê³  ì¢‹ì€ ê²ƒì€ ì•„ë‹˜. 13 í† í°ì´ ìµœì .

#### ê°€ì„¤ 2: "ì‹¬í˜¸í¡" íš¨ê³¼

**ì´ë¡ **: "Take a deep breath"ê°€ ëª¨ë¸ì˜ ì£¼ì˜ë¥¼ ì§‘ì¤‘ì‹œí‚¨ë‹¤.

**Ablation Study**:
```python
prompts = [
    "Work on this problem step-by-step.",  
    # â†’ 78.9%
    
    "Take a deep breath.",  
    # â†’ 72.1% (ë‹¨ë…ìœ¼ë¡œëŠ” íš¨ê³¼ ì—†ìŒ)
    
    "Take a deep breath and work on this problem step-by-step.",  
    # â†’ 80.2% (ì¡°í•©ì´ ì¤‘ìš”)
]
```

**ê²°ë¡ **: "Take a deep breath" ìì²´ë³´ë‹¤ëŠ” "and work on this problem step-by-step"ê³¼ì˜ **ì¡°í•©**ì´ ì¤‘ìš”.

#### ê°€ì„¤ 3: í•™ìŠµ ë°ì´í„° ìƒê´€ê´€ê³„

**ì´ë¡ **: í•™ìŠµ ë°ì´í„°ì— ìœ ì‚¬í•œ íŒ¨í„´ì´ ìˆì—ˆì„ ê°€ëŠ¥ì„±.

**ë¶„ì„**: 
- ì¸í„°ë„· ë°ì´í„°ì—ì„œ "Take a deep breath"ëŠ” ì£¼ë¡œ **ìŠ¤íŠ¸ë ˆìŠ¤ ìƒí™©**ì—ì„œ ì‚¬ìš©
- ì–´ë ¤ìš´ ë¬¸ì œ í’€ì´ ê°€ì´ë“œì—ì„œ ìì£¼ ë“±ì¥
- ëª¨ë¸ì´ ì´ë¥¼ "ë³µì¡í•œ ë¬¸ì œ â†’ ì‹ ì¤‘í•œ ì ‘ê·¼" íŒ¨í„´ìœ¼ë¡œ í•™ìŠµí–ˆì„ ê°€ëŠ¥ì„±

**ê²°ë¡ **: ìš°ì—°ì´ ì•„ë‹ˆë¼ í•™ìŠµ ë°ì´í„°ì˜ íŒ¨í„´ê³¼ ì—°ê´€.

### BigBench-Hard

**ë°ì´í„°ì…‹**: 23ê°œì˜ ì–´ë ¤ìš´ ì¶”ë¡  íƒœìŠ¤í¬

**ê²°ê³¼**:

| ë°©ë²• | í‰ê·  ì •í™•ë„ |
|-----|-----------|
| Few-shot (baseline) | 43.2% |
| CoT (manual) | 51.7% |
| **OPRO-optimized** | **55.3%** |

**ê°œì„ í­**: +12.1% (baseline ëŒ€ë¹„)

**íƒœìŠ¤í¬ë³„ ìµœì  í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ**:

```
Logical Deduction:
"Analyze the given information systematically and eliminate impossible options."
â†’ 62.1% (vs 54.3% baseline)

Causal Judgment:
"Consider both direct and indirect relationships between events."
â†’ 58.7% (vs 49.2% baseline)

Formal Fallacies:
"Identify the logical structure of the argument, then check for validity."
â†’ 71.2% (vs 63.8% baseline)
```

## ë°˜ë³µ íšŸìˆ˜ì™€ ì„±ëŠ¥ì˜ ê´€ê³„

OPROì˜ ìµœì í™” ê³¼ì •ì—ì„œ ë°˜ë³µ íšŸìˆ˜ê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥:

```python
# ì‹¤í—˜ ë°ì´í„° (GSM8K)
iterations = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
scores = [72.1, 74.3, 76.2, 77.8, 78.9, 79.5, 79.9, 80.1, 80.2, 80.2]

# ì‹œê°í™”
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(iterations, scores, marker='o', linewidth=2, markersize=8)
plt.xlabel('Optimization Iteration', fontsize=12)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.title('OPRO Performance vs Iterations (GSM8K)', fontsize=14)
plt.grid(True, alpha=0.3)
plt.axhline(y=78.2, color='r', linestyle='--', label='CoT baseline')
plt.legend()
plt.tight_layout()
plt.savefig('opro_iterations.png')
```

**ê´€ì°°**:
- ì²˜ìŒ 5ë²ˆ ë°˜ë³µì—ì„œ ê¸‰ê²©í•œ ê°œì„  (72.1% â†’ 78.9%)
- 6-8ë²ˆ: ì ì§„ì  ê°œì„  (78.9% â†’ 80.1%)
- 9ë²ˆ ì´ìƒ: ìˆ˜ë ´ (80.2%)

**ê¶Œì¥ì‚¬í•­**: **8-10ë²ˆ ë°˜ë³µ**ì´ ë¹„ìš© ëŒ€ë¹„ íš¨ê³¼ì .

## APE/OPROì˜ í•œê³„ì 

### í•œê³„ 1: ë†’ì€ ì—°ì‚° ë¹„ìš©

**ë¬¸ì œ**: ë§ì€ API í˜¸ì¶œì´ í•„ìš”í•˜ë‹¤.

**ë¹„ìš© ê³„ì‚° (OPRO ì˜ˆì‹œ)**:

```
ì„¤ì •:
- 10ë²ˆ ë°˜ë³µ
- ê° ë°˜ë³µë§ˆë‹¤ 5ê°œ í…ŒìŠ¤íŠ¸ ì˜ˆì‹œë¡œ í‰ê°€
- ì´ 50ë²ˆ LLM í˜¸ì¶œ (10 iterations Ã— 5 examples)

Claude Sonnet 4 ê¸°ì¤€:
- Input: 200 tokens/call Ã— 50 = 10,000 tokens
- Output: 100 tokens/call Ã— 50 = 5,000 tokens
- ë¹„ìš©: (10,000 Ã— $3 + 5,000 Ã— $15) / 1,000,000 = $0.105

í”„ë¡¬í”„íŠ¸ 1ê°œ ìµœì í™”ì— $0.10
```

**ë¹„êµ**:
- ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ ì„¤ê³„: ë¬´ë£Œ (ì‚¬ëŒ ì‹œê°„ì€ ë³„ë„)
- APE/OPRO: $0.05 - $0.50 per prompt

**ì™„í™” ì „ëµ**:
```python
# 1. ìºì‹± í™œìš©
cache = {}

def evaluate_with_cache(prompt, examples):
    cache_key = (prompt, tuple(examples))
    if cache_key in cache:
        return cache[cache_key]
    
    score = evaluate(prompt, examples)
    cache[cache_key] = score
    return score

# 2. Early stopping
def optimize_with_early_stopping(patience=3):
    best_score = 0
    no_improvement_count = 0
    
    for iteration in range(max_iterations):
        score = evaluate_current_prompt()
        
        if score > best_score:
            best_score = score
            no_improvement_count = 0
        else:
            no_improvement_count += 1
        
        if no_improvement_count >= patience:
            print(f"Early stopping at iteration {iteration}")
            break

# 3. ë” ì‘ì€ í‰ê°€ ì„¸íŠ¸
# ì „ì²´ ë°ì´í„°ì…‹ ëŒ€ì‹  ëŒ€í‘œ ìƒ˜í”Œë§Œ ì‚¬ìš©
eval_samples = random.sample(all_examples, k=20)  # 100 â†’ 20
```

### í•œê³„ 2: íƒœìŠ¤í¬ ì˜ì¡´ì„±

**ë¬¸ì œ**: ëª¨ë“  íƒœìŠ¤í¬ì—ì„œ íš¨ê³¼ì ì´ì§€ ì•Šë‹¤.

**íš¨ê³¼ì ì¸ íƒœìŠ¤í¬**:
- âœ… ëª…í™•í•œ ì…ë ¥-ì¶œë ¥ êµ¬ì¡°
- âœ… ê°ê´€ì  í‰ê°€ ê°€ëŠ¥
- âœ… ë§ì€ ì˜ˆì‹œ í™•ë³´ ê°€ëŠ¥

**ë¹„íš¨ê³¼ì ì¸ íƒœìŠ¤í¬**:
- âŒ ì°½ì˜ì  ê¸€ì“°ê¸°
- âŒ ì£¼ê´€ì  í‰ê°€ í•„ìš”
- âŒ ì˜ˆì‹œê°€ ê·¹íˆ ì ìŒ

**ì‹¤í—˜ ê²°ê³¼**:

| íƒœìŠ¤í¬ ìœ í˜• | APE ì„±ê³µë¥  | ìˆ˜ë™ ëŒ€ë¹„ ê°œì„  |
|----------|----------|------------|
| Classification | 85% | +4.5% |
| Extraction | 78% | +3.2% |
| Reasoning | 71% | +5.8% |
| Creative Writing | 42% | -2.1% |
| Subjective Q&A | 51% | +0.3% |

### í•œê³„ 3: í”„ë¡¬í”„íŠ¸ í•´ì„ ì–´ë ¤ì›€

**ë¬¸ì œ**: APE/OPROê°€ ì°¾ì€ í”„ë¡¬í”„íŠ¸ê°€ ì™œ ì¢‹ì€ì§€ ì„¤ëª…í•˜ê¸° ì–´ë µë‹¤.

**ì˜ˆì‹œ**:
```
APEê°€ ì°¾ì€ í”„ë¡¬í”„íŠ¸:
"Considering all relevant factors, determine the most appropriate response."

ì™œ íš¨ê³¼ì ì¸ê°€? 
â†’ "all relevant factors"ê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ë¶ˆëª…í™•
â†’ í•˜ì§€ë§Œ ì‹¤ì œë¡œ 5% ì„±ëŠ¥ í–¥ìƒ
```

**ë¬¸ì œì **:
- ë””ë²„ê¹… ì–´ë ¤ì›€
- ê°œì„  ë°©í–¥ ë¶ˆëª…í™•
- ë„ë©”ì¸ ì „ë¬¸ê°€ì˜ ê²€ì¦ í•„ìš”

**ì™„í™” ì „ëµ**:
```python
def explain_prompt_effectiveness(prompt: str, examples: List) -> str:
    """
    í”„ë¡¬í”„íŠ¸ê°€ ì™œ íš¨ê³¼ì ì¸ì§€ ë¶„ì„
    """
    explanation_prompt = f"""ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ê°€ ì™œ íš¨ê³¼ì ì¸ì§€ ë¶„ì„í•˜ì„¸ìš”:

í”„ë¡¬í”„íŠ¸: {prompt}

ì˜ˆì‹œë“¤:
{format_examples(examples)}

ì´ í”„ë¡¬í”„íŠ¸ì˜ íš¨ê³¼ì ì¸ ìš”ì†Œë“¤ì„ ì„¤ëª…í•˜ì„¸ìš”:
1. í•µì‹¬ í‚¤ì›Œë“œ
2. êµ¬ì¡°ì  íŠ¹ì§•
3. ëª¨ë¸ì—ê²Œ ì „ë‹¬í•˜ëŠ” ì‹ í˜¸

ë¶„ì„:"""
    
    # LLMìœ¼ë¡œ ì„¤ëª… ìƒì„±
    explanation = generate_explanation(explanation_prompt)
    return explanation
```

### í•œê³„ 4: ê³¼ì í•© ìœ„í—˜

**ë¬¸ì œ**: í‰ê°€ ì„¸íŠ¸ì— ê³¼ì í•©ë  ìˆ˜ ìˆë‹¤.

**ì˜ˆì‹œ**:
```
í›ˆë ¨/í‰ê°€ ì„¸íŠ¸: ê¸ì •/ë¶€ì • ê°ì„± ë¶„ë¥˜ (ê°„ë‹¨í•œ ë‹¨ì–´ë“¤)
- happy â†’ positive
- sad â†’ negative

ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸:
"Is this word happy or sad? Answer positive or negative."
â†’ í‰ê°€ ì„¸íŠ¸: 100%
â†’ ì‹¤ì œ í…ŒìŠ¤íŠ¸: 72%

ê³¼ì í•©!
```

**ì™„í™” ì „ëµ**:
```python
# Train/Validation/Test split
def split_examples(examples, train_ratio=0.6, val_ratio=0.2):
    random.shuffle(examples)
    
    n = len(examples)
    train_end = int(n * train_ratio)
    val_end = int(n * (train_ratio + val_ratio))
    
    train = examples[:train_end]
    val = examples[train_end:val_end]
    test = examples[val_end:]
    
    return train, val, test

# ìµœì í™”ëŠ” trainìœ¼ë¡œ, ì„ íƒì€ valìœ¼ë¡œ
train_examples, val_examples, test_examples = split_examples(all_examples)

# Trainìœ¼ë¡œ í›„ë³´ ìƒì„±
candidates = ape.generate_candidate_prompts(train_examples)

# Valìœ¼ë¡œ í‰ê°€ ë° ì„ íƒ
best_prompt = ape.select_best(candidates, val_examples)

# Testë¡œ ìµœì¢… ì„±ëŠ¥ ì¸¡ì •
final_score = evaluate(best_prompt, test_examples)
```

## ìˆ˜ë™ vs ìë™ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

### ë¹„êµí‘œ

| íŠ¹ì„± | ìˆ˜ë™ (ì‚¬ëŒ) | ìë™ (APE/OPRO) |
|-----|----------|---------------|
| **ì‹œê°„** | ëŠë¦¼ (ìˆ˜ ì‹œê°„) | ë¹ ë¦„ (ìˆ˜ ë¶„) |
| **ë¹„ìš©** | ì‚¬ëŒ ì‹œê°„ | API ë¹„ìš© ($0.05-0.50) |
| **ì„±ëŠ¥** | ê°€ë³€ì  | ì¼ê´€ì  |
| **í•´ì„ ê°€ëŠ¥ì„±** | ë†’ìŒ | ë‚®ìŒ |
| **ì°½ì˜ì„±** | ë†’ìŒ | ì œí•œì  |
| **í™•ì¥ì„±** | ë‚®ìŒ | ë†’ìŒ |
| **ë„ë©”ì¸ ì§€ì‹** | í•„ìˆ˜ | ì„ íƒì  |

### ì–¸ì œ ìˆ˜ë™ì„ ì‚¬ìš©í•  ê²ƒì¸ê°€?

**ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ ë‚˜ì€ ê²½ìš°**:

1. **ì˜ˆì‹œê°€ ë§¤ìš° ì ì„ ë•Œ**
```
ì˜ˆì‹œ: 3ê°œ ë¯¸ë§Œ
â†’ APEëŠ” íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€
â†’ ì‚¬ëŒì˜ ë„ë©”ì¸ ì§€ì‹ì´ ë” ì¤‘ìš”
```

2. **ì°½ì˜ì  ì‘ì—…**
```
íƒœìŠ¤í¬: "SF ì†Œì„¤ ì‘ì„±"
â†’ ì‚¬ëŒì´ ë” ë‚˜ì€ ìŠ¤íƒ€ì¼ ì§€ì‹œ ì‘ì„±
â†’ APEëŠ” "write creatively" ê°™ì€ ì¼ë°˜ì  í”„ë¡¬í”„íŠ¸ë§Œ ìƒì„±
```

3. **ì„¤ëª… ê°€ëŠ¥ì„±ì´ ì¤‘ìš”**
```
í™˜ê²½: ì˜ë£Œ, ë²•ë¥ , ê¸ˆìœµ
â†’ "ì™œ ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?" ì„¤ëª… í•„ìš”
â†’ ì‚¬ëŒì´ ì„¤ê³„í•œ í”„ë¡¬í”„íŠ¸ê°€ ë” íˆ¬ëª…
```

4. **ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘**
```
ìƒí™©: ì•„ì´ë””ì–´ ê²€ì¦ ë‹¨ê³„
â†’ APE ì„¤ì • ì‹œê°„ > ìˆ˜ë™ ì‘ì„± ì‹œê°„
```

### ì–¸ì œ ìë™ì„ ì‚¬ìš©í•  ê²ƒì¸ê°€?

**APE/OPROê°€ ë‚˜ì€ ê²½ìš°**:

1. **ëª…í™•í•œ í‰ê°€ ë©”íŠ¸ë¦­**
```
íƒœìŠ¤í¬: ê°ì„± ë¶„ë¥˜, ì—”í‹°í‹° ì¶”ì¶œ
í‰ê°€: ì •í™•ë„ë¡œ ëª…í™•íˆ ì¸¡ì • ê°€ëŠ¥
â†’ APEê°€ ì²´ê³„ì ìœ¼ë¡œ ìµœì í™” ê°€ëŠ¥
```

2. **ëŒ€ëŸ‰ì˜ ì˜ˆì‹œ**
```
ì˜ˆì‹œ: 100ê°œ ì´ìƒ
â†’ APEê°€ íŒ¨í„´ í•™ìŠµì— ì¶©ë¶„í•œ ë°ì´í„°
â†’ ìˆ˜ë™ìœ¼ë¡œëŠ” ëª¨ë“  ì˜ˆì‹œ ê³ ë ¤ ì–´ë ¤ì›€
```

3. **ì—¬ëŸ¬ íƒœìŠ¤í¬ ë™ì‹œ ìµœì í™”**
```
ìƒí™©: 10ê°œì˜ ìœ ì‚¬í•œ ë¶„ë¥˜ íƒœìŠ¤í¬
â†’ APEë¡œ ì¼ê´„ ìµœì í™” (ë³‘ë ¬ ì²˜ë¦¬)
â†’ ìˆ˜ë™: ê°ê° ìˆ˜ ì‹œê°„ ì†Œìš”
```

4. **ì§€ì†ì  ê°œì„ **
```
í™˜ê²½: í”„ë¡œë•ì…˜ì—ì„œ ë°ì´í„° ëˆ„ì 
â†’ ì£¼ê¸°ì ìœ¼ë¡œ APE ì¬ì‹¤í–‰í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°±ì‹ 
â†’ ìˆ˜ë™: ì¬ì‘ì—… ë¶€ë‹´
```

### í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• (ê¶Œì¥)

**ìµœì„ ì˜ ì „ëµ**: ìˆ˜ë™ + ìë™ ì¡°í•©

```python
class HybridPromptEngineering:
    """
    ìˆ˜ë™ + ìë™ ê²°í•©
    """
    
    def optimize(self, task_description: str, examples: List[Dict]) -> str:
        """
        1. ì‚¬ëŒì´ ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì‘ì„±
        2. APE/OPROë¡œ ìë™ ê°œì„ 
        3. ì‚¬ëŒì´ ìµœì¢… ê²€í†  ë° ì¡°ì •
        """
        # Step 1: ì‚¬ëŒì´ ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì‘ì„±
        print("Step 1: ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì‘ì„± (ì‚¬ëŒ)")
        initial_prompt = input("ì´ˆê¸° í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
        
        # Step 2: APEë¡œ ë³€í˜• ìƒì„±
        print("\nStep 2: APEë¡œ ë³€í˜• ìƒì„±")
        ape = APE(api_key="your-api-key")
        
        # ì´ˆê¸° í”„ë¡¬í”„íŠ¸ë¥¼ ì‹œë“œë¡œ ì‚¬ìš©
        variations = ape.generate_variations(initial_prompt, num_variations=10)
        
        # Step 3: ìë™ í‰ê°€
        print("\nStep 3: ìë™ í‰ê°€")
        scores = [ape.evaluate(v, examples) for v in variations]
        
        # Step 4: ìƒìœ„ í›„ë³´ ì œì‹œ
        print("\nStep 4: ìƒìœ„ 3ê°œ í›„ë³´:")
        top_3 = sorted(zip(variations, scores), key=lambda x: x[1], reverse=True)[:3]
        
        for i, (prompt, score) in enumerate(top_3, 1):
            print(f"\n[{i}] ì ìˆ˜: {score:.3f}")
            print(f"    í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # Step 5: ì‚¬ëŒì´ ìµœì¢… ì„ íƒ ë° ìˆ˜ì •
        print("\nStep 5: ìµœì¢… ì„ íƒ")
        choice = int(input("ì„ íƒí•  ë²ˆí˜¸ (1-3): ")) - 1
        selected_prompt = top_3[choice][0]
        
        print(f"\nì„ íƒëœ í”„ë¡¬í”„íŠ¸:\n{selected_prompt}")
        
        modify = input("\nìˆ˜ì •í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        if modify.lower() == 'y':
            final_prompt = input("ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸: ")
        else:
            final_prompt = selected_prompt
        
        return final_prompt
```

**í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° ì˜ˆì‹œ**:

```
í”„ë¡œì íŠ¸: ê³ ê° ë¦¬ë·° ê°ì„± ë¶„ë¥˜

Day 1: ì‚¬ëŒì´ ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì‘ì„±
"Classify the sentiment of this review as positive, negative, or neutral."
â†’ í…ŒìŠ¤íŠ¸: 82%

Day 2: APEë¡œ 10ê°œ ë³€í˜• ìƒì„± ë° í‰ê°€
ìµœê³  ì„±ëŠ¥: "Analyze the overall sentiment expressed in this review and categorize it as positive, negative, or neutral."
â†’ í…ŒìŠ¤íŠ¸: 87%

Day 3: ë„ë©”ì¸ ì „ë¬¸ê°€ ê²€í† 
ìˆ˜ì •: "Analyze the customer's overall sentiment in this product review and categorize it as positive, negative, or neutral."
â†’ ìµœì¢… í…ŒìŠ¤íŠ¸: 89%

ê²°ê³¼: ì´ˆê¸° ëŒ€ë¹„ +7% ê°œì„ 
```

## ì‹¤ë¬´ ì ìš© ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ëŒ€ê·œëª¨ ë¶„ë¥˜ ì‹œìŠ¤í…œ

**ìƒí™©**: 
- 10ê°œì˜ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í•„ìš”
- ê° ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ìµœì  í”„ë¡¬í”„íŠ¸ í•„ìš”
- 1ì£¼ì¼ ë°ë“œë¼ì¸

**ì ‘ê·¼ë²•**:
```python
categories = [
    "Product Category",
    "Support Ticket Priority", 
    "Email Sentiment",
    "Content Moderation",
    # ... 10ê°œ
]

# ë³‘ë ¬ ìµœì í™”
from concurrent.futures import ThreadPoolExecutor

def optimize_category(category_name, examples):
    ape = APE(api_key="your-api-key")
    result = ape.optimize_prompt(
        train_examples=examples,
        test_examples=test_sets[category_name],
        num_candidates=20,
        num_iterations=5
    )
    return category_name, result

with ThreadPoolExecutor(max_workers=5) as executor:
    futures = [
        executor.submit(optimize_category, cat, train_sets[cat])
        for cat in categories
    ]
    
    results = [f.result() for f in futures]

# 10ê°œ ì¹´í…Œê³ ë¦¬ë¥¼ í•˜ë£¨ ë§Œì— ìµœì í™”
```

**ê²°ê³¼**:
- ì†Œìš” ì‹œê°„: 1ì¼ (ìˆ˜ë™: 1ì£¼ì¼+)
- í‰ê·  ì„±ëŠ¥: ìˆ˜ë™ ëŒ€ë¹„ +3.5%
- ë¹„ìš©: $5 (API ë¹„ìš©)

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì§€ì†ì  ê°œì„ 

**ìƒí™©**:
- í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë§¤ì¼ ìƒˆ ë°ì´í„° ìœ ì…
- í”„ë¡¬í”„íŠ¸ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ê°œì„ í•˜ê³  ì‹¶ìŒ

**ì ‘ê·¼ë²•**:
```python
import schedule
import time

def weekly_optimization():
    """
    ë§¤ì£¼ ì¼ìš”ì¼ì— ìë™ ìµœì í™”
    """
    print(f"[{datetime.now()}] ì£¼ê°„ ìµœì í™” ì‹œì‘")
    
    # 1. ì§€ë‚œ ì£¼ ë°ì´í„° ìˆ˜ì§‘
    last_week_data = db.query("""
        SELECT input, output, feedback
        FROM predictions
        WHERE created_at >= NOW() - INTERVAL '7 days'
        AND feedback = 'correct'
    """)
    
    # 2. ì˜ˆì‹œ ì¤€ë¹„
    examples = [
        {"input": row.input, "output": row.output}
        for row in last_week_data
    ]
    
    # 3. OPROë¡œ ìµœì í™”
    opro = OPRO(api_key="your-api-key")
    result = opro.optimize_prompt(
        initial_prompt=current_prompt,
        test_examples=examples[:100],  # ìƒ˜í”Œë§
        num_iterations=10
    )
    
    # 4. A/B í…ŒìŠ¤íŠ¸
    new_prompt = result['best_prompt']
    
    if result['best_score'] > current_score + 0.02:  # 2% ì´ìƒ ê°œì„ 
        print(f"ìƒˆ í”„ë¡¬í”„íŠ¸ ë°°í¬: {new_prompt}")
        deploy_new_prompt(new_prompt, rollout_percentage=10)
    else:
        print("í˜„ì¬ í”„ë¡¬í”„íŠ¸ ìœ ì§€")

# ìŠ¤ì¼€ì¤„ë§
schedule.every().sunday.at("02:00").do(weekly_optimization)

while True:
    schedule.run_pending()
    time.sleep(3600)
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ë‹¤êµ­ì–´ í”„ë¡¬í”„íŠ¸

**ìƒí™©**:
- ì˜ì–´ í”„ë¡¬í”„íŠ¸ëŠ” ìµœì í™”ë¨
- í•œêµ­ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´ ë²„ì „ í•„ìš”

**ì ‘ê·¼ë²•**:
```python
def multilingual_optimization(
    english_prompt: str,
    target_languages: List[str],
    examples_per_language: Dict[str, List[Dict]]
) -> Dict[str, str]:
    """
    ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤êµ­ì–´ ìµœì í™”
    """
    results = {}
    
    for lang in target_languages:
        print(f"\n=== {lang} ìµœì í™” ===")
        
        # 1. ì˜ì–´ í”„ë¡¬í”„íŠ¸ ë²ˆì—­
        translated = translate(english_prompt, target_language=lang)
        print(f"ë²ˆì—­ëœ ì´ˆê¸° í”„ë¡¬í”„íŠ¸: {translated}")
        
        # 2. í•´ë‹¹ ì–¸ì–´ë¡œ OPRO ì‹¤í–‰
        opro = OPRO(api_key="your-api-key")
        result = opro.optimize_prompt(
            initial_prompt=translated,
            test_examples=examples_per_language[lang],
            num_iterations=8
        )
        
        results[lang] = result['best_prompt']
        print(f"ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸: {results[lang]}")
    
    return results

# ì‹¤í–‰
languages = ['ko', 'ja', 'zh']
examples = {
    'ko': korean_examples,
    'ja': japanese_examples,
    'zh': chinese_examples
}

optimized_prompts = multilingual_optimization(
    english_prompt="Classify the sentiment as positive or negative.",
    target_languages=languages,
    examples_per_language=examples
)
```

## ë¹„ìš© ë¶„ì„ ë° ROI

### ë¹„ìš© êµ¬ì¡°

**APE/OPRO ë¹„ìš© ìš”ì†Œ**:

1. **í”„ë¡¬í”„íŠ¸ ìƒì„± ë¹„ìš©**
```
- í›„ë³´ ìƒì„±: 10-50íšŒ LLM í˜¸ì¶œ
- ì˜ˆìƒ ë¹„ìš©: $0.02 - $0.10
```

2. **í‰ê°€ ë¹„ìš©**
```
- ê° í›„ë³´ë§ˆë‹¤ í…ŒìŠ¤íŠ¸ ì˜ˆì‹œë“¤ë¡œ í‰ê°€
- í›„ë³´ 20ê°œ Ã— ì˜ˆì‹œ 10ê°œ = 200íšŒ LLM í˜¸ì¶œ
- ì˜ˆìƒ ë¹„ìš©: $0.20 - $1.00
```

3. **ë°˜ë³µ ê°œì„  ë¹„ìš©**
```
- 3-10 ë°˜ë³µ
- ì˜ˆìƒ ë¹„ìš©: $0.50 - $5.00
```

**ì´ ë¹„ìš©**: $0.72 - $6.10 per prompt

### ROI ê³„ì‚°

**ì˜ˆì‹œ: ê³ ê° ì§€ì› ì±—ë´‡**

**ì‹œë‚˜ë¦¬ì˜¤**:
- ì›” 100,000ê±´ì˜ ë¬¸ì˜ ì²˜ë¦¬
- í”„ë¡¬í”„íŠ¸ ê°œì„ ìœ¼ë¡œ ì •í™•ë„ 5% í–¥ìƒ (80% â†’ 85%)
- ì˜ëª»ëœ ë‹µë³€ ë¹„ìš©: $2 (ì¬ì²˜ë¦¬ ë¹„ìš©)

**ê³„ì‚°**:
```
ê°œì„  ì „:
- ì˜¤ë‹µ: 100,000 Ã— 0.20 = 20,000ê±´
- ë¹„ìš©: 20,000 Ã— $2 = $40,000/ì›”

ê°œì„  í›„:
- ì˜¤ë‹µ: 100,000 Ã— 0.15 = 15,000ê±´  
- ë¹„ìš©: 15,000 Ã— $2 = $30,000/ì›”

ì ˆê°ì•¡: $10,000/ì›”

APE/OPRO ë¹„ìš©: $5 (ì¼íšŒì„±)

ROI = ($10,000 - $5) / $5 Ã— 100% = 199,900%
```

**ê²°ë¡ **: ì••ë„ì ìœ¼ë¡œ ë†’ì€ ROI

### ë¹„ìš© ìµœì í™” ì „ëµ

```python
class CostOptimizedAPE:
    """
    ë¹„ìš©ì„ ìµœì í™”í•œ APE
    """
    
    def optimize_with_budget(
        self,
        examples: List[Dict],
        max_budget_usd: float = 1.0
    ) -> Dict:
        """
        ì˜ˆì‚° ì œì•½ í•˜ì—ì„œ ìµœì í™”
        """
        cost_per_call = 0.01  # ì˜ˆìƒ ë¹„ìš©
        max_calls = int(max_budget_usd / cost_per_call)
        
        print(f"ì˜ˆì‚°: ${max_budget_usd}")
        print(f"ìµœëŒ€ í˜¸ì¶œ íšŸìˆ˜: {max_calls}\n")
        
        # ì „ëµ 1: í›„ë³´ ìˆ˜ ì¡°ì •
        if max_calls < 50:
            num_candidates = 5
            num_iterations = 2
        elif max_calls < 200:
            num_candidates = 10
            num_iterations = 3
        else:
            num_candidates = 20
            num_iterations = 5
        
        print(f"í›„ë³´ ìˆ˜: {num_candidates}")
        print(f"ë°˜ë³µ íšŸìˆ˜: {num_iterations}\n")
        
        # ì „ëµ 2: í‰ê°€ ìƒ˜í”Œ ì œí•œ
        eval_samples = min(len(examples), 20)
        eval_examples = random.sample(examples, eval_samples)
        
        print(f"í‰ê°€ ì˜ˆì‹œ: {eval_samples}ê°œ\n")
        
        # ìµœì í™” ì‹¤í–‰
        result = self.optimize_prompt(
            train_examples=examples,
            test_examples=eval_examples,
            num_candidates=num_candidates,
            num_iterations=num_iterations
        )
        
        # ì‹¤ì œ ë¹„ìš© ì¶”ì •
        actual_calls = (
            num_candidates +  # ìƒì„±
            num_candidates * eval_samples * num_iterations  # í‰ê°€
        )
        estimated_cost = actual_calls * cost_per_call
        
        print(f"\nì‹¤ì œ API í˜¸ì¶œ: {actual_calls}íšŒ")
        print(f"ì˜ˆìƒ ë¹„ìš©: ${estimated_cost:.2f}")
        
        return result
```

## í•µì‹¬ ìš”ì•½

**APE (Automatic Prompt Engineer)**:
- LLMìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ ìƒì„± ë° ìµœì í™”
- 6ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°
- ì‚¬ëŒì´ ì‘ì„±í•œ í”„ë¡¬í”„íŠ¸ ëŒ€ë¹„ í‰ê·  +4.5% ì„±ëŠ¥
- ë¹„ìš©: $0.72 - $6.10 per prompt

**OPRO (Optimization by PROmpting)**:
- í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¥¼ ìµœì í™” ë¬¸ì œë¡œ ì ‘ê·¼
- LLMì„ optimizerë¡œ ì‚¬ìš©
- "Take a deep breath" ê°™ì€ ì˜ì™¸ì˜ í”„ë¡¬í”„íŠ¸ ë°œê²¬
- GSM8Kì—ì„œ baseline ëŒ€ë¹„ +8.4% ê°œì„ 

**ì–¸ì œ ì‚¬ìš©í•  ê²ƒì¸ê°€?**:
- âœ… ëª…í™•í•œ í‰ê°€ ë©”íŠ¸ë¦­
- âœ… ì¶©ë¶„í•œ ì˜ˆì‹œ (20+)
- âœ… ì—¬ëŸ¬ íƒœìŠ¤í¬ ë™ì‹œ ìµœì í™”
- âœ… ì§€ì†ì  ê°œì„  í•„ìš”

**ì–¸ì œ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒì¸ê°€?**:
- âŒ ì°½ì˜ì  ì‘ì—…
- âŒ ì£¼ê´€ì  í‰ê°€
- âŒ ì˜ˆì‹œ ê·¹íˆ ì ìŒ
- âŒ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘

**ê¶Œì¥ ì ‘ê·¼ë²•**: ìˆ˜ë™ + ìë™ í•˜ì´ë¸Œë¦¬ë“œ


## ì°¸ê³ ë¬¸í—Œ

1. Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., & Ba, J. (2022). **Large language models are human-level prompt engineers.** *arXiv preprint arXiv:2211.01910*.

2. Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., & Chen, X. (2023). **Large language models as optimizers.** *arXiv preprint arXiv:2309.03409*.

3. Pryzant, R., et al. (2023). **Automatic prompt optimization with "gradient descent" and beam search.** *arXiv preprint arXiv:2305.03495*.

4. Zhang, Y., et al. (2023). **Prompting is programming: A query language for large language models.** *PLDI 2023*.
