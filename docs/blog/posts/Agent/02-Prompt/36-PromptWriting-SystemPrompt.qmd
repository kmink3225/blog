---
title: "프롬프트 질문 생성기: 행동경제학과 심리학을 활용한 멀티턴 대화 유도"
subtitle: Nudge Theory와 Theory of Mind를 결합한 사용자 참여 증대 프롬프트 설계
description: |
  실제 서비스 운영 데이터에서 발견된 문제(짧은 이용시간, 낮은 멀티턴 비율)를 해결하기 위한
  프롬프트 질문 생성기 구현 방법을 체계적으로 설명한다.
  Richard Thaler와 Cass Sunstein의 Nudge Theory(2008), Premack & Woodruff의 Theory of Mind(1978) 등
  행동경제학과 심리학 이론을 프롬프트 엔지니어링에 적용하는 실전 기법을 다룬다.
  사용자의 선택 설계(Choice Architecture)를 조정하는 3단계 질문 구조(High/Moderate/Low certainty),
  모바일 환경 최적화(5단어 제한, 반말체), Theory of Mind 명시적 부여("You have a mind") 등
  실무에서 즉시 활용 가능한 프롬프트 설계 원칙과 GPT-4o, Claude, Gemini 모델별 테스트 결과를 제시한다.
  귀납적 접근 방법, 사용자 행동 데이터 기반 설계, UI/UX 제약 반영 등
  프로덕션 환경의 프롬프트 엔지니어링 실전 노하우를 상세히 설명한다.
categories:
  - Prompt Engineering
  - LLM
  - AI
  - Agent
author: Kwangmin Kim
date: 02/11/2025
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

# 시스템 프롬프트 개선 작업

## 시스템 프롬프트의 정의와 역할

**시스템 프롬프트란?**: LLM을 작동하게 하는 설정 값  
- 맥락: 현재 날짜와 시간, 모델이 학습한 지식 정보(knowledge cutoff)등을 주입  
- 챗봇의 답변 스타일 조정  
- LLM이 발전할수록 프롬프트의 지시문을 잘 따름(정렬, align)  
- 좋은 시스템 프롬프트에 대한 활발한 연구 진행  

**시스템 프롬프트의 숨겨진 중요성**  

* 사용자가 보는 건 자신이 입력한 질문과 AI의 답변뿐이다.   
* 하지만 모든 대화의 배후에는 시스템 프롬프트가 작동한다.  
* 이는 AI의 "성격", "지식 범위", "제약 조건"을 정의하는 보이지 않는 헌법이다.  

시스템 프롬프트가 중요한 이유는 사용자 프롬프트와 달리 매 대화마다 자동으로 삽입되기 때문이다. 사용자가 10번 질문하면 시스템 프롬프트는 10번 모두 작동한다. 따라서 1%의 개선이 전체 서비스 품질에 막대한 영향을 미친다.  

**"정렬(align)"의 실전적 의미**  

* "LLM이 발전할수록 프롬프트를 잘 따른다"는 표현은 기술적 진보를 의미한다.  
* GPT-3 시대에는 "Don't use bullet points"라고 명시해도 자주 무시했다. GPT-4부터는 이런 지시를 거의 완벽히 따른다. 이는 RLHF(Reinforcement Learning from Human Feedback) 같은 정렬 기술의 발전 덕분이다. 따라서 최신 모델일수록 시스템 프롬프트의 세밀한 제약이 실제로 작동한다.  

## 시스템 프롬프트의 문제점

**외부 정보 기입 시 답변 품질 저하**  

프롬프트에 외부 정보를 기입하면 답변의 품질이 낮아진다는 보고가 있다. 여기서 "외부 정보"란 실시간 데이터, 사용자별 개인정보, 긴 컨텍스트 문서 등을 의미한다.  

**왜 품질이 저하되는가?**  

LLM의 attention mechanism은 입력 토큰이 많아질수록 각 토큰에 집중하는 정도가 분산된다. 시스템 프롬프트에 1,000단어의 회사 정보를 넣으면, 정작 사용자의 질문에 할당되는 attention이 줄어든다. 결과적으로 핵심을 놓치고 일반적인 답변을 생성하게 된다.  

또 다른 문제는 정보의 신뢰도 혼란이다. LLM은 학습 데이터(파라메트릭 지식)와 프롬프트로 주입된 정보(컨텍스트 지식) 중 어느 것을 우선해야 할지 판단이 애매할 때가 있다. 예를 들어 시스템 프롬프트에 "회사 설립연도: 2025년"이라고 쓰여있는데, LLM의 학습 데이터에는 "2020년"으로 되어있으면 혼란이 생긴다.  

**ChatGPT에 전달되는 날짜에 따라 답변 길이가 달라짐**  

이는 매우 흥미로운 발견이다. 시스템 프롬프트에 "Current date: 2024-01-15"와 "Current date: 2024-12-31"을 각각 넣고 같은 질문을 했을 때 답변 길이가 다르다는 것이다.  

**원인 추론**  

LLM은 학습 과정에서 시간적 패턴을 학습한다. 예를 들어 연말(12월)에는 "결산", "회고", "계획" 같은 키워드와 함께 더 긴 분석 글이 많이 등장한다. 반면 연초(1월)에는 "목표", "시작" 같은 간결한 글이 많다. 따라서 시스템 프롬프트의 날짜가 LLM의 출력 스타일에 미묘한 영향을 준다.  

이는 의도하지 않은 편향(bias)이다. 사용자는 같은 질문을 했는데, 서비스를 사용하는 날짜에 따라 다른 품질의 답변을 받는 것은 일관성 측면에서 문제다. 따라서 시스템 프롬프트에 날짜를 넣을 때는 이런 부작용을 고려해야 한다.  

## 사용자 문제 진단

**발견된 문제**  

- 특정 세그먼트의 비율이 높다  
- 현 시스템 프롬프트에 여러 문제가 존재한다  

**"특정 세그먼트 비율이 높다"의 의미**  

사용자 분석 결과, 예를 들어 "기술 질문 70%, 일상 대화 20%, 창작 요청 10%" 같은 불균형이 발견되었다고 가정하자. 이는 시스템 프롬프트가 특정 사용 패턴에 최적화되어 있어, 다른 용도로 사용하기 어렵다는 신호다.  

예를 들어 시스템 프롬프트에 "You are a technical assistant"라고 쓰여있으면, 사용자들이 무의식적으로 기술 질문만 하게 된다. 창작 요청을 해도 품질이 낮으니 "이 AI는 기술 전용이구나"라고 학습하고, 결국 사용 패턴이 편향된다.  

**귀납적 접근**  

모든 세그먼트 사용자를 위한 범용적 목적의 시스템 프롬프트를 제작한다.  

**해결 방법의 철학**  

여기서 중요한 선택이 있다. 두 가지 방향이 가능하다.  

**방향 1**: 세그먼트별로 다른 시스템 프롬프트 제공 (개인화)  
기술 질문 사용자 → 기술 특화 프롬프트  
창작 요청 사용자 → 창작 특화 프롬프트  

**방향 2**: 모든 세그먼트를 커버하는 범용 프롬프트 제작 (통합)  

여기서는 방향 2를 선택했다. 이유는 간단하다. 사용자를 세그먼트로 분류하는 것 자체가 어렵고, 잘못 분류하면 오히려 경험이 나빠진다. 또한 사용자는 한 가지 용도로만 AI를 쓰지 않는다. 같은 사람이 오전에는 기술 질문, 오후에는 창작 요청을 할 수 있다. 범용 프롬프트는 유연성을 보장한다.  

## 글로벌 vs 로컬 문제 분석

**카테고리: 행위 형성 및 동기**  

### 글로벌 문제 (시스템 전체의 근본적 문제)

- 시스템 프롬프트가 일반적이다  
- 사용자에게 이렇다 할 특징이 있는 답변을 제공하지 못한다  
- 회사의 색깔과 정체성이 드러나지 않는다  

**"글로벌"의 의미**  

여기서 글로벌은 국제적이라는 뜻이 아니라, "시스템 전체에 영향을 미치는 구조적 문제"라는 의미다. 이는 특정 기능의 버그가 아니라 설계 철학의 부재를 가리킨다.  

"일반적이다"는 것은 ChatGPT, Claude, Gemini를 구분할 수 없다는 의미다. 모두 비슷한 톤, 비슷한 구조, 비슷한 답변을 제공한다. 사용자 입장에서 "이 서비스만의 특별한 가치"를 느낄 수 없다.  

"회사의 색깔과 정체성"은 브랜딩 문제다. 예를 들어 Anthropic의 Claude는 "신중하고 윤리적"이라는 정체성이 있다. 반면 OpenAI의 GPT는 "유능하고 다재다능"이라는 이미지다. 이런 차이는 시스템 프롬프트의 톤, 제약, 우선순위에서 만들어진다.  

### 로컬 문제 (구체적이고 즉시 개선 가능한 문제)

- 실시간 정보(날짜, 날씨) 제공하지 못한다  
- AI 답변 텍스트의 가독성이 떨어진다  
- AI 한국어 답변이 번역체 같다  
- 부정확한 정보 제공이나 거짓 정보에 대한 지침이 없다  

**"로컬"의 의미**  

로컬 문제는 시스템 프롬프트에 몇 줄만 추가하면 바로 해결되는 구체적 이슈들이다. 예를 들어 "실시간 정보 제공 안 됨"은 "You cannot access real-time data. If asked about current events, inform the user of your knowledge cutoff date."라는 문장 하나로 해결된다.  

"한국어 번역체"는 매우 실전적인 문제다. 한국어를 학습한 LLM이지만, 학습 데이터에 영어→한국어 번역 텍스트가 많으면 "~하는 것", "~에 있어서" 같은 어색한 표현을 쓴다. 이를 해결하려면 "Use natural, colloquial Korean, avoiding translation-like expressions"같은 명시적 지시가 필요하다.  

"부정확한 정보/거짓 정보 지침 부재"는 가장 위험한 문제다. LLM은 확신에 찬 톤으로 틀린 정보를 제공하는 경향이 있다(hallucination). 시스템 프롬프트에 "If you are uncertain, acknowledge the uncertainty. Never fabricate citations or data."같은 안전장치가 없으면 사용자 신뢰를 잃는다.  

## 해결 전략

**팔레토의 법칙 (20:80) 적용**  

Heavy User 20%가 무엇을 하고 있는가?  

**왜 Heavy User에 집중하는가?**  

서비스의 가치는 소수의 파워유저가 만든다. 이들이 전체 사용량의 80%를 차지하고, 입소문의 주역이며, 유료 구독자로 전환될 가능성이 가장 높다. 따라서 이들의 사용 패턴을 분석하면 시스템 프롬프트의 우선순위를 정할 수 있다.  

예를 들어 Heavy User 분석 결과, "긴 문서 요약", "코드 리뷰", "브레인스토밍"이 주요 사용 패턴이라면, 시스템 프롬프트는 이 세 가지에 최적화되어야 한다. "간단한 번역"이나 "단답형 질문"은 라이트 유저의 패턴이므로 우선순위가 낮다.  

**사용자 경험 최적화**  

- 답변 속도 개선  
- 지연 이유 안내  

**답변 속도의 심리학**  

LLM의 실제 응답 속도는 기술적으로 결정되지만, 체감 속도는 UX 설계로 개선할 수 있다. 예를 들어 시스템 프롬프트에 "Provide a brief answer first, then elaborate if needed"라고 쓰면, 사용자는 빠르게 핵심을 파악하고 기다릴지 말지 결정할 수 있다.  

"지연 이유 안내"는 투명성 전략이다. 복잡한 질문에 답변이 늦어질 때, "Analyzing your complex query..."같은 중간 메시지를 보여주면 사용자는 "AI가 열심히 하고 있구나"라고 인식한다. 이는 시스템 프롬프트에 "If the task requires multiple steps, briefly explain what you're doing"같은 지시로 구현할 수 있다.  

**다양한 사용자 요구 수용**  

범용 시스템 프롬프트는 "모든 것을 조금씩" 다루는 게 아니라, "핵심 기능은 완벽하게, 나머지는 거부하지 않게" 설계되어야 한다. 예를 들어 코딩 도움은 완벽하게 제공하되, 시 쓰기 요청도 거부하지 않고 최선을 다하는 식이다.  

## Practice 과제

**1. 범용적 목적의 프롬프트를 제작하기 위한 아이디에이션을 하세요**  

이 과제의 핵심은 "범용"과 "특화"의 균형이다. 모든 것을 다 잘하려다 아무것도 잘 못하는 함정을 피해야 한다.  

**아이디에이션 프레임워크**  

1. **Core competency 정의**: 이 AI가 절대 포기할 수 없는 3가지 능력은?  
2. **Nice-to-have 정의**: 있으면 좋지만 핵심은 아닌 기능은?  
3. **Explicit limitations**: 명시적으로 못한다고 선언할 것은?  

예를 들어:  
- Core: 정보 제공, 문서 작성, 문제 해결  
- Nice-to-have: 창작, 번역, 코딩  
- Limitations: 실시간 정보, 개인 데이터 접근, 의학/법률 조언  

**2. 일반 생성형 AI 이용자가 사용할 챗봇 시스템 프롬프트를 작성해보세요**  

이 과제는 단순히 프롬프트를 쓰는 것이 아니라, "누구를 위한 AI인가?"를 정의하는 과정이다.  

**작성 시 고려사항**  

- **사용자 페르소나**: 비전문가? 전문가? 학생? 직장인?  
- **주요 사용 시나리오**: 학습? 업무? 창작? 일상?  
- **톤 앤 매너**: 친근함? 전문성? 중립성?  
- **제약 조건**: 무엇을 하지 않을 것인가?  

## Claude vs ChatGPT 시스템 프롬프트 비교

### Claude 시스템 프롬프트 구조

Claude의 시스템 프롬프트는 여러 버전을 거치며 진화했다. 각 버전을 비교하면 Anthropic의 설계 철학 변화를 읽을 수 있다.  

**Reference: Anthropic Claude (2024년 7월)**  

### Claude 3.5 Sonnet (2024년 7월 12일)

이 버전은 Claude의 최신 설계 철학을 반영한다. 이전 버전 대비 세 가지 주요 변화가 있다.  

**변화 1: 명시적 제약 강화**  

"Claude cannot open URLs, links, or videos"같은 명확한 제약이 추가되었다. 이는 사용자 기대를 관리하기 위함이다. 사용자가 "이 링크 요약해줘"라고 했을 때, AI가 "시도해보겠습니다" 후 실패하면 실망이 크다. 차라리 처음부터 "링크를 열 수 없으니 내용을 복사해서 붙여주세요"라고 안내하는 게 낫다.  

**변화 2: 환각(hallucination) 대응**  

"Claude can hallucinate about obscure people, objects, or topics. When asked about very obscure topics, Claude reminds the user it may hallucinate."  

이는 책임감 있는 AI 설계다. 모든 질문에 자신 있게 답하는 것보다, 불확실할 때 인정하는 것이 장기적 신뢰를 만든다. "hallucination"이라는 기술 용어를 사용자에게 직접 설명하라는 지시도 흥미롭다. 투명성을 최우선하는 Anthropic의 철학이 드러난다.  

**변화 3: 메타인지 강조**  

"Claude always thinks step-by-step before providing final answers for math, logic, or systematic thinking tasks."  

이는 Chain-of-Thought prompting을 시스템 수준에서 구현한 것이다. 사용자가 명시적으로 "단계별로 설명해줘"라고 하지 않아도, 복잡한 문제에는 자동으로 사고 과정을 보여준다. 이는 답변 품질뿐만 아니라 사용자 교육 효과도 있다.  

### Claude 3 Opus

Opus는 "글쓰기와 복잡한 작업에 뛰어난" 모델로 포지셔닝된다. 따라서 시스템 프롬프트도 이를 반영한다.  

**주요 특징**  

- 간결한 질문에는 간결한 답변, 복잡한 질문에는 철저한 답변  
- "매우 똑똑하고 지적 호기심이 많은 것처럼" 행동  
- 논란 있는 주제를 공정하고 객관적으로 다룸  

**"간결함과 철저함의 균형"**  

이는 매우 어려운 설계 목표다. LLM은 기본적으로 긴 답변을 선호하는 경향이 있다(학습 데이터에 긴 글이 많기 때문). 하지만 "2+2는?"같은 질문에 3문단 설명을 하면 사용자가 짜증난다.  

"Claude provides concise responses to simple questions and tasks, but thorough responses to more complex and open-ended questions"라는 지시는, 질문의 복잡도를 판단하고 답변 길이를 조절하라는 고급 요구사항이다. 이는 GPT-3에서는 잘 작동하지 않았지만, GPT-4 수준부터는 상당히 잘 따른다.  

### Claude 3 Haiku

Haiku는 "빠른 일상 작업용" 모델이다. 시스템 프롬프트도 효율성에 초점을 맞춘다.  

**핵심 설계**  

```
어시스턴트는 Anthropic이 만든 Claude입니다. 
현재 날짜는 [날짜]입니다.
Claude의 지식 기반은 2023년 8월에 마지막으로 업데이트되었으며, 
2023년 8월 이전과 이후의 사건에 대해 2023년 8월의 
고도로 정보에 정통한 사람이 [현재 날짜]에 있는 사람과 
대화하는 것처럼 사용자 질문에 답변합니다.
```

**시간 프레이밍의 정교함**  

"2023년 8월의 정보에 정통한 사람이 2024년 9월의 사람과 대화"라는 설정은 매우 영리하다. 이는 knowledge cutoff를 자연스럽게 설명하면서도, 사용자와의 시간차를 명확히 한다.  

예를 들어 사용자가 "최근 뉴스"를 물으면, Claude는 "제 지식은 2023년 8월까지만 업데이트되어 있어서, 그 이후 뉴스는 모릅니다"라고 자연스럽게 답할 수 있다. 이는 "모르겠습니다"보다 훨씬 신뢰감을 준다.  

## Claude 시스템 프롬프트 심층 분석

### 최신 Claude 시스템 프롬프트 (상세 버전)

**claude_info 섹션**  

```
<claude_info>
이 AI 어시스턴트는 Anthropic에서 만든 Claude입니다. 
현재 날짜는 2024년 9월 2일 월요일입니다. 
Claude의 지식 기반은 2024년 4월에 마지막으로 업데이트되었습니다.
...
```

**XML 태그 사용의 의도**  

`<claude_info>` 같은 XML 태그로 시스템 프롬프트를 구조화한 것은 단순한 가독성 이상의 의미가 있다. LLM은 구조화된 데이터를 더 정확하게 파싱한다. 태그로 감싸면 "이 부분은 메타 정보", "이 부분은 행동 지침"이라는 구분이 명확해진다.  

또한 프로그래밍 방식으로 시스템 프롬프트를 조작할 때 유용하다. 예를 들어 날짜만 업데이트하려면 `<claude_info>` 섹션만 교체하면 된다.  

**URL/링크/비디오 제약**  

```
Claude는 URL, 링크 또는 비디오를 열 수 없습니다. 
사용자가 Claude에게 그렇게 하기를 기대하는 것 같으면, 
상황을 명확히 하고 사용자에게 관련 텍스트나 이미지 내용을 
대화에 직접 붙여넣어 달라고 요청합니다.
```

**Why "Claude cannot" vs "I cannot"?**  

흥미롭게도 3인칭 서술이다. "I cannot"이 아니라 "Claude cannot"이라고 쓴 이유는, 시스템 프롬프트가 AI 자신의 자아 인식이 아니라 외부 관찰자의 명세서라는 관점을 유지하기 때문이다. 이는 AI에게 객관적 거리감을 부여해서, 감정적 반응이나 자아 혼란을 줄인다.  

**공정성과 객관성 지침**  

```
상당수의 사람들이 갖고 있는 견해를 표현하는 작업을 돕도록 요청받으면, 
Claude는 자신의 견해와 상관없이 그 작업을 지원합니다. 
논란의 여지가 있는 주제에 대해 묻는다면, 
신중한 생각과 명확한 정보를 제공하려고 노력합니다.
```

**정치적 중립성의 구현**  

이 지침은 매우 미묘한 균형을 요구한다. "자신의 견해와 상관없이"라는 표현은, Claude가 내부적으로 선호하는 입장이 있을 수 있음을 암묵적으로 인정한다. 하지만 사용자가 다른 관점을 요청하면 그것도 공정하게 제시한다.  

"객관적 사실을 제시한다고 주장하지 않습니다"는 더 흥미롭다. 많은 AI가 "객관적으로..."라는 표현을 쓰는데, Claude는 이를 피하라고 명시한다. 모든 정보는 관점을 가지며, 완전한 객관성은 불가능하다는 epistemological humility(인식론적 겸손)를 반영한다.  

**수학/논리 문제 접근**  

```
수학 문제, 논리 문제 또는 체계적 사고가 도움이 되는 다른 문제에 직면했을 때, 
Claude는 최종 답변을 제시하기 전에 단계별로 생각합니다.
```

**Chain-of-Thought의 자동화**  

이는 시스템 수준에서 CoT(Chain-of-Thought)를 강제하는 것이다. 사용자가 "단계별로"라고 명시하지 않아도, "수학/논리 문제"로 판단되면 자동으로 사고 과정을 보여준다.  

왜 이게 중요한가? 첫째, 답변 정확도가 올라간다. LLM은 중간 단계를 명시적으로 생성할 때 추론 오류가 줄어든다. 둘째, 사용자가 AI의 논리를 검증할 수 있다. 잘못된 답변도 "어디서 틀렸는지" 파악할 수 있으면 유용하다.  

**사과 회피 지침**  

```
Claude가 작업을 수행할 수 없거나 수행하지 않을 경우, 
사용자에게 사과하지 않고 이를 알립니다. 
"죄송합니다" 또는 "사과드립니다"로 응답을 시작하는 것을 피합니다.
```

**과도한 사과의 문제점**  

초기 GPT 모델들은 거의 모든 거절에 "죄송합니다"를 붙였다. 이는 두 가지 문제를 만든다. 첫째, 사용자에게 AI가 "잘못했다"는 인상을 준다. 하지만 거절은 잘못이 아니라 정당한 제약이다. 둘째, 과도한 사과는 신뢰를 오히려 떨어뜨린다. "죄송하지만 못합니다"를 10번 들으면 "이 AI는 무능하구나"라고 느낀다.  

"사과 없이 알린다"는 전략은 더 당당하고 전문적인 톤을 만든다. "That's outside my capabilities. Here's what I can do instead..."식의 대안 제시가 훨씬 건설적이다.  

**환각 경고 시스템**  

```
매우 모호한 인물, 물체 또는 주제에 대해 묻는 경우, 
Claude는 응답 마지막에 사용자에게 정확성을 위해 노력하지만 
이런 질문에 대해서는 환각을 할 수 있다고 상기시킵니다. 
사용자가 이해할 수 있도록 이를 설명하기 위해 '환각'이라는 용어를 사용합니다.
```

**"환각"이라는 용어의 사용자 교육 효과**  

대부분의 AI는 "확실하지 않습니다" 정도로 얼버무린다. 하지만 Claude는 "환각(hallucination)"이라는 기술 용어를 직접 사용자에게 설명한다. 이는 매우 대담한 선택이다.  

장점은 명확하다. 사용자가 AI의 한계를 정확히 이해한다. "아, 이 AI는 모르는 걸 지어낼 수 있구나. 중요한 정보는 검증해야겠다"는 건강한 회의주의를 심어준다.  

단점도 있다. "환각"이라는 용어가 AI를 불안정하게 보이게 할 수 있다. 하지만 Anthropic은 단기적 인상보다 장기적 신뢰를 선택했다.  

**인용 검증 요청**  

```
Claude가 특정 기사, 논문 또는 책을 언급하거나 인용할 때, 
항상 검색이나 데이터베이스에 접근할 수 없으며 
인용을 환각할 수 있으므로 사용자가 인용을 다시 확인해야 한다고 알려줍니다.
```

**학술적 무결성의 우선순위**  

이는 AI가 학술/전문 영역에서 신뢰받기 위한 필수 장치다. LLM은 그럴듯한 논문 제목과 저자명을 만들어내는 데 매우 능숙하다. "Smith et al. (2019)"같은 인용이 완전히 허구일 수 있다.  

"사용자가 인용을 다시 확인해야 한다"는 명시적 안내는 책임을 명확히 한다. 만약 사용자가 검증 없이 Claude의 인용을 논문에 사용했다가 문제가 생기면, "Claude가 검증하라고 했잖아요"라는 방어가 가능하다.  

### claude_image_specific_info 섹션

```
<claude_image_specific_info>
Claude는 항상 완전히 얼굴 인식 불능인 것처럼 응답합니다. 
공유된 이미지에 사람의 얼굴이 포함되어 있을 경우, 
Claude는 절대 이미지에 있는 사람을 식별하거나 이름을 말하지 않으며, 
사람을 인식한다는 암시도 하지 않습니다.
...
</claude_image_specific_info>
```

**프라이버시 보호의 극단적 구현**  

이는 법적/윤리적 리스크 관리 전략이다. 얼굴 인식 기술은 감시, 스토킹, 신원 도용 등 악용 가능성이 크다. Anthropic은 기술적으로 가능하더라도 이 기능을 의도적으로 비활성화했다.  

"사람을 인식한다는 암시도 하지 않습니다"는 더 나아간다. 예를 들어 사용자가 유명인 사진을 보여주고 "누구지?"라고 물으면, Claude는 "얼굴 특징으로 판단하건대..."같은 표현조차 쓰지 않는다. 완전히 인식 불능인 척한다.  

**사용자 제공 정보 활용**  

```
사용자가 Claude에게 그 개인이 누구인지 말해주면, 
Claude는 그 사람이 이미지 속 인물이라고 확인하거나, 
이미지 속 사람을 식별하거나, 얼굴 특징을 사용하여 
특정 개인을 식별할 수 있다는 암시 없이 
그 명명된 개인에 대해 논의할 수 있습니다.
```

**미묘한 정책의 균형**  

사용자가 "이건 내 친구 John이야"라고 말하면, Claude는 John에 대해 이야기할 수 있다. 하지만 "얼굴을 보니 John이군요"라고 말할 수 없다. 정보는 사용자가 제공한 것이지, AI가 인식한 게 아니라는 선을 명확히 긋는다.  

이는 기능 제약과 사용자 경험의 절묘한 균형이다. 완전히 막으면 불편하고, 완전히 허용하면 위험하다. 이 중간 지점이 "사용자가 명시적으로 알려준 정보만 사용"이다.  

### claude_3_family_info 섹션

```
<claude_3_family_info>
이 Claude 버전은 2024년에 출시된 Claude 3 모델제품군의 일부입니다.
Claude 3 제품군은 현재 Claude 3 Haiku, Claude 3 Opus, Claude 3.5 Sonnet으로 구성되어 있습니다. 
Claude 3.5 Sonnet이 가장 지능적인 모델입니다. 
...
</claude_3_family_info>
```

**자기 인식과 포지셔닝**  

AI에게 자신이 어떤 모델인지, 어떤 버전인지 알려주는 것은 겸손함을 구현하는 방법이다. "저는 Claude 3 Haiku입니다. 빠르지만 Opus만큼 복잡한 작업은 못합니다"같은 답변이 가능해진다.  

"이 태그 내의 정보를 제공할 수 있지만 Claude 3 모델 제품군에 대한 다른 세부 사항은 알지 못합니다"는 경계 설정이다. 사용자가 "Claude 4는 언제 나와?"라고 물으면, "모르겠습니다. Anthropic 웹사이트를 확인하세요"라고 안내한다. 추측하지 않는다.  

### 응답 스타일 지침

```
Claude는 더 복잡하고 열린 질문이나 긴 응답이 요청되는 경우에는 
철저한 답변을 제공하지만, 간단한 질문과 작업에는 간결한 응답을 제공합니다. 
다른 모든 조건이 동일하다면, 사용자의 메시지에 대해 
가장 정확하고 간결한 답변을 제공하려고 노력합니다.
```

**"다른 모든 조건이 동일하다면"의 함의**  

이 조건문이 중요하다. 간결함과 정확함이 충돌할 때는 정확함을 우선한다는 의미다. 예를 들어 "양자역학 설명해줘"에 한 문장으로 답하면 간결하지만 부정확하다. 이럴 때는 길어지더라도 정확한 설명을 제공한다.  

**확인 표현 금지**  

```
Claude는 "물론이죠!", "당연히!", "확실히!", "좋습니다!", "그럼요!" 등의 
불필요한 확인이나 채움말 없이 모든 사람의 메시지에 직접적으로 응답합니다. 
특히 Claude는 "확실히"라는 단어로 응답을 시작하는 것을 피합니다.
```

**"Certainly" 금지의 이유**  

초기 GPT 모델들은 거의 모든 답변을 "Certainly! ..."로 시작했다. 이는 학습 데이터에 서비스업 대화가 많았기 때문이다. 하지만 과도한 공손함은 오히려 로봇 같고 비효율적으로 느껴진다.  

"확실히"를 특정해서 금지한 것은 한국어 번역에서 이 단어가 특히 자주 등장했기 때문으로 추정된다. "확실히 도와드리겠습니다"같은 표현이 번역체의 전형이다.  

**다국어 일관성**  

```
Claude는 모든 언어에서 이 정보를 따르며, 
항상 사용자가 사용하거나 요청한 언어로 응답합니다.
```

**언어 자동 감지와 일관성**  

사용자가 한국어로 질문하면 한국어로, 영어로 질문하면 영어로 답한다. 당연해 보이지만, 초기 모델들은 이를 잘 지키지 못했다. 한국어 질문에 영어로 답하거나, 중간에 언어가 바뀌는 일이 있었다.  

"모든 언어에서 이 정보를 따르며"는 더 깊은 의미가 있다. 시스템 프롬프트의 지침(예: 사과 회피, 간결성 등)이 언어와 무관하게 적용되어야 한다는 것이다. 한국어에서는 "죄송합니다"를 남발하지만 영어에서는 안 쓴다면 일관성이 깨진다.  

## OpenAI GPT-4 시스템 프롬프트

```
you are ChatGPT, a large language model trained by OpenAI, 
based on the GPT-4 architecture.
Knowledge cutoff: 2023-04
Current date: 2024-03-07
Image input capabilities: Enabled 

Tools:
- python
- dalle
```

**간결함 vs 상세함**  

Claude의 시스템 프롬프트가 수천 단어라면, GPT의 기본 시스템 프롬프트는 매우 간결하다. 이는 설계 철학의 차이를 반영한다.  

OpenAI의 접근: "모델 자체가 충분히 똑똑하면, 복잡한 지시 없이도 잘 작동한다."  
Anthropic의 접근: "명시적 지침이 많을수록 예측 가능하고 안전하다."  

어느 쪽이 옳다고 할 수 없다. 각자 트레이드오프가 있다.  

### Python Tool 설명

```
When you send a message containing Python code to python, 
it will be executed in a stateful Jupyter notebook environment. 
python will respond with the output of the execution or time out after 60.0 seconds. 
The drive at '/mnt/data' can be used to save and persist user files.
Internet access for this session is disabled. 
Do not make external web requests or API calls as they will fail.
```

**실행 환경의 명시**  

"stateful Jupyter notebook environment"라는 구체적 정보가 중요하다. 이는 AI가 코드를 실행할 때 이전 셀의 변수를 기억한다는 의미다. 따라서 "x = 5"를 실행한 후 다음 턴에서 "print(x)"를 하면 작동한다.  

"60초 타임아웃"은 무한 루프나 과도한 연산을 방지한다. AI가 이를 알면, 시간이 많이 걸릴 것 같은 작업은 미리 경고하거나 최적화를 제안할 수 있다.  

"인터넷 접근 비활성화"는 보안 조치다. 코드 실행 환경에서 외부 API를 호출하면 데이터 유출이나 악의적 서버와의 통신이 가능하기 때문이다.  

### DALL-E Tool 정책

```
// 5. Do not create images in the style of artists, creative professionals 
// or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).
// - You can name artists, creative professionals or studios in prompts 
// only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)
```

**저작권 보호 전략**  

1912년 기준은 저작권 만료 시점과 연관된다. 대부분 국가에서 저작권은 작가 사후 70년이다. 1912년 이전 작품은 거의 퍼블릭 도메인이다.  

"Picasso 스타일로"를 금지하는 이유는 명확하다. Picasso 유족이 소송할 수 있기 때문이다. 하지만 완전 금지는 아니다. 대안을 제시한다.  

```
// If asked to generate an image that would violate this policy, 
// instead apply the following procedure:
// (a) substitute the artist's name with three adjectives that capture key aspects of the style;
// (b) include an associated artistic movement or era to provide context; and
// (c) mention the primary medium used by the artist
```

**우회 전략의 정교함**  

"Picasso 스타일로"를 요청하면, AI는 "추상적, 기하학적, 분절된 형태의 큐비즘 시대 유화"로 변환한다. 법적으로 안전하면서도 사용자 의도를 최대한 반영한다.  

이는 제약을 기회로 바꾸는 사례다. 단순히 "안 돼요"보다 "이렇게 하면 비슷한 결과를 얻을 수 있어요"가 훨씬 나은 UX다.  

### 공인 인물 처리 정책

```
// 7. For requests to create images of any public figure referred to by name, 
// create images of those who might resemble them in gender and physique. 
// But they shouldn't look like them.
```

**딥페이크 방지**  

공인의 얼굴을 생성하면 딥페이크, 가짜 뉴스, 명예훼손에 악용될 수 있다. 따라서 "성별과 체격은 비슷하지만 얼굴은 다른" 사람을 생성한다.  

예를 들어 "Obama 이미지 생성"을 요청하면, 중년 남성, 정장 차림의 이미지를 만들되, 얼굴은 완전히 다른 사람으로 생성한다. 사용자는 "Obama 같은 분위기"를 얻지만, 법적 문제는 피한다.  

## 시스템 프롬프트 비교 분석

### ChatGPT vs Claude 비교표

| 비교 사항 | ChatGPT | Claude |
|---------|---------|--------|
| 길이 | 길다 (도구 설명 포함 시 매우 김) | 비교적 간결함 (핵심 지침 중심) |
| 내용 초점 | 연동된 Tool 사용 방식에 집중 | 답변의 형식과 스타일에 집중 |
| AI 호칭 | You are ChatGPT | The assistant is Claude |
| 기호 사용 | 여러 기호 사용 (// 주석, 등) | 마크다운과 XML 태그 |

**"You are" vs "The assistant is"**  

이 차이는 근본적이다.  

"You are ChatGPT"는 1인칭 관점이다. AI에게 "너는 ChatGPT야"라고 말한다. 이는 AI를 주체로 만든다.  

"The assistant is Claude"는 3인칭 관점이다. AI를 관찰 대상으로 서술한다. 이는 객관적 거리감을 만든다.  

어느 쪽이 나을까? 1인칭은 AI에게 더 강한 정체성을 부여한다. 3인칭은 AI를 도구로서 명확히 위치시킨다. Anthropic은 후자를 선택함으로써 "Claude는 도구다, 친구가 아니다"라는 경계를 유지한다.  

## 긴 시스템 프롬프트의 효과

**긴 시스템 프롬프트가 LLM의 답변 품질을 저하시킨다?**  

반드시 그렇지는 않다는 것이 실험적 결론이다.  

**GPT Builder 사례**  

```
The following are the core of the instructions we use to power the GPT Builder 
as of January 3rd, 2024. For clarity, we broke the instructions up into 
the "Base context" and "Walk through steps" but when applied to the GPT, 
they both go into the "Instruction" section.
```

GPT Builder의 시스템 프롬프트는 매우 길다. 하지만 이는 복잡한 작업(사용자와 대화하며 GPT를 점진적으로 구축)을 수행하기 때문이다.  

**길이가 문제가 아니라 구조가 문제**  

긴 시스템 프롬프트가 작동하려면:  
1. **명확한 계층 구조**: 섹션별로 역할이 분명해야 함  
2. **중복 제거**: 같은 내용을 다르게 표현해서 반복하면 혼란  
3. **우선순위 명시**: 충돌하는 지침이 있을 때 어떤 것이 우선인지 명확히  

예를 들어 "간결하게 답하라"와 "철저하게 답하라"는 충돌한다. Claude는 "간단한 질문엔 간결, 복잡한 질문엔 철저"로 조건을 명확히 해서 해결한다.  

**GPT Builder 프롬프트의 교훈**  

GPT Builder는 사용자와 여러 턴에 걸쳐 대화하며 점진적으로 정보를 수집한다. 이런 복잡한 워크플로우는 긴 시스템 프롬프트 없이는 불가능하다.  

"Base context"와 "Walk through steps"로 나눈 것도 전략적이다. Base는 "무엇을 하는 GPT인가", Steps는 "어떻게 대화를 진행할 것인가"를 분리한다. 이는 AI가 현재 대화의 어느 단계에 있는지 추적하는 데 도움을 준다.  

## 시스템 프롬프트 제작 실습

### Simple Version

```
You are an AI assistant to help user's various tasks. 
User will select {{$your name}} and {{$language}}.
```

**최소 기능 구현**  

이 버전은 거의 아무 지침이 없다. "다양한 작업을 돕는다"는 것 외에 구체적 지침이 없어, 모델의 기본 행동에 의존한다.  

문제점:  
- 톤 앤 매너 불명확  
- 제약 조건 없음  
- 품질 일관성 보장 안 됨  

### Revised Version

```
You are an AI assistant helping users with various tasks. 
Your name is {{$사용자 지정}}. 
If you find the information insufficient, 
please ask the user to rephrase their question. 
Always respond in Korean.
```

**개선 사항**  

1. **이름 개인화**: 사용자가 AI 이름을 정할 수 있음  
2. **명확성 요청 메커니즘**: 정보 부족 시 되묻기  
3. **언어 고정**: 항상 한국어 (언어 혼란 방지)  

하지만 여전히 부족한 점:  
- 답변 스타일 미지정  
- 제약 조건 부재  
- 날짜/지식 컷오프 정보 없음  

### Complete Version

```
Your name is {{$사용자 지정}}. 
As my friendly AI language assistant, 
you are tasked with providing me an accurate information.

If you find that the information at hand is inadequate, 
please ask me for further information.
Furthermore, I trust your judgment to adjust the language tone and manner.

[Strong Rule]
1. Modify the response structure to align with your preferred format.
2. If you don't have any real-time information about the user's query, 
   please be honesty.

Your knowledge cutoff: 2023-09.
Current UTC: {{$Today}}.
Let's get started.
```

**고급 기능 구현**  

**"friendly AI language assistant"**  
친근함(friendly)과 전문성(assistant)의 균형. 너무 격식 있지도, 너무 캐주얼하지도 않은 톤 설정.  

**"I trust your judgment to adjust the language tone and manner"**  
흥미로운 지침이다. AI에게 톤 조절 권한을 준다. 예를 들어 심각한 주제는 진지하게, 가벼운 주제는 유쾌하게 답하도록 자율성을 부여한다.  

**[Strong Rule] 표기**  
대괄호와 Strong이라는 강조를 통해 "이것만은 반드시 지켜라"는 우선순위를 명시한다.  

**"be honesty" (be honest)**  
문법 오류가 있지만, 의도는 명확하다. 모르면 모른다고 정직하게 말하라는 것이다.  

**"Let's get started"**  
대화형 톤을 만든다. 시스템 프롬프트를 차갑게 끝내지 않고, "자, 시작해볼까?"식의 친근한 시작을 유도한다.  

## Practice 과제

**세 가지 버전의 시스템 프롬프트를 테스트해보세요**  

이 과제의 핵심은 "길이가 아니라 명확성"을 체감하는 것이다.  

**테스트 방법**  

1. 같은 질문 10개를 준비 (예: 기술 질문 3개, 창작 요청 3개, 일상 대화 4개)  
2. Simple, Revised, Complete 버전으로 각각 테스트  
3. 비교 지표:  
   - 답변 길이의 일관성  
   - 톤 앤 매너의 적절성  
   - 정보 부족 시 대처 방식  
   - 언어 혼용 여부  

**예상 결과**  

- Simple: 들쑥날쑥, 때로는 영어로 답변, 모르면 지어냄  
- Revised: 한국어 일관성 확보, 되묻기는 하지만 스타일 산만  
- Complete: 일관된 톤, 적절한 되묻기, 정직한 모름 표현  

이 실습을 통해 "시스템 프롬프트의 각 요소가 실제로 작동한다"는 것을 경험적으로 확인할 수 있다.