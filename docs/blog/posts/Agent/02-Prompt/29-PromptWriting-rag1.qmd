---
title: "RAG (Retrieval Augmented Generation): ì™¸ë¶€ ì§€ì‹ê³¼ ìƒì„±ì˜ ê²°í•©"
subtitle: ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•˜ì—¬ LLMì˜ ì§€ì‹ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” í•µì‹¬ ê¸°ë²•
description: |
  Retrieval Augmented Generation (RAG)ì˜ ì •ì˜ë¶€í„° ì‹¤ì „ êµ¬í˜„ê¹Œì§€ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•œë‹¤.
  Lewis et al. (2020) "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" ì—°êµ¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ
  ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰(Retrieval)ê³¼ ë‹µë³€ ìƒì„±(Generation)ì˜ ê²°í•© ì›ë¦¬, ë²¡í„° ì„ë² ë”©ê³¼ ìœ ì‚¬ë„ ê²€ìƒ‰ ë©”ì»¤ë‹ˆì¦˜,
  ë¬¸ì„œ ì²­í‚¹(Chunking) ì „ëµ(í† í° ê¸°ë°˜, ë¬¸ì¥ ê¸°ë°˜, ì˜ë¯¸ë¡ ì , êµ¬ì¡° ê¸°ë°˜)ì„ ë¶„ì„í•œë‹¤.
  Indexingê³¼ Query ë‹¨ê³„ì˜ 2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤, BM25/Dense/Hybrid ê²€ìƒ‰ ë°©ë²•, Re-ranking ìµœì í™”ë¥¼ ì œì‹œí•˜ê³ ,
  ê°„ë‹¨í•œ Q&A ì‹œìŠ¤í…œ, ê³ ê° ì§€ì› ì±—ë´‡, ë¬¸ì„œ ë¶„ì„ ë„êµ¬ ë“± ì‹¤ë¬´ ì˜ˆì‹œì™€
  Python êµ¬í˜„ ì½”ë“œ(ë²¡í„° DB, ì„ë² ë”© ëª¨ë¸ í™œìš©)ë¥¼ í†µí•´ ì‹¤ì „ êµ¬ì¶• ë°©ë²•ì„ ìƒì„¸íˆ ë‹¤ë£¬ë‹¤.
  ì²­í¬ í¬ê¸° ìµœì í™”, top_k ì„¤ì •, ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€(Precision, Recall, MRR),
  í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ì „ëµê³¼ ì¶œì²˜ ì¶”ì  íŒ¨í„´ì„ ì œì‹œí•œë‹¤.
categories:
  - Prompt Engineering
  - LLM
  - AI
  - Agent
author: Kwangmin Kim
date: 02/05/2025
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

## ë“¤ì–´ê°€ë©°

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì•„ë¬´ë¦¬ ë°©ëŒ€í•œ ì§€ì‹ì„ í•™ìŠµí–ˆë”ë¼ë„, ë‹¤ìŒê³¼ ê°™ì€ ê·¼ë³¸ì ì¸ í•œê³„ê°€ ìˆë‹¤:

1. **ì§€ì‹ì˜ ì‹œê°„ì  í•œê³„**: í•™ìŠµ ë°ì´í„°ì˜ ì»·ì˜¤í”„ ì´í›„ ì •ë³´ë¥¼ ëª¨ë¦„
2. **ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ ë¶€ì¡±**: ê¸°ì—… ë‚´ë¶€ ë¬¸ì„œ, ì „ë¬¸ ë¶„ì•¼ ì§€ì‹ ë¶€ì¬
3. **í• ë£¨ì‹œë„¤ì´ì…˜**: í™•ì‹  ìˆê²Œ í‹€ë¦° ì •ë³´ë¥¼ ìƒì„±
4. **ì¶œì²˜ ë¶ˆëª…í™•**: ì–´ë””ì„œ ì–»ì€ ì •ë³´ì¸ì§€ ì¶”ì  ë¶ˆê°€

ì˜ˆë¥¼ ë“¤ì–´, "ìš°ë¦¬ íšŒì‚¬ì˜ ìµœì‹  íœ´ê°€ ì •ì±…ì€?"ì´ë¼ëŠ” ì§ˆë¬¸ì— LLMì€ ë‹µí•  ìˆ˜ ì—†ë‹¤. ì´ ì •ë³´ëŠ” í•™ìŠµ ë°ì´í„°ì— ì—†ê¸° ë•Œë¬¸ì´ë‹¤.

**Retrieval Augmented Generation (RAG)**ì€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤. ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•œ í›„, ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ëŠ” LLMì—ê²Œ "ì°¸ê³  ìë£Œ"ë¥¼ ì œê³µí•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” RAGì˜ ì›ë¦¬ë¶€í„° ì‹¤ì „ êµ¬ì¶•ê¹Œì§€ ìƒì„¸íˆ ë‹¤ë£¬ë‹¤.

## RAGë€?

### í•µì‹¬ ê°œë…

**RAG (Retrieval Augmented Generation)**ëŠ” ì§ˆë¬¸ì— ë‹µí•˜ê¸° ì „ì—:
1. ë¨¼ì € ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ **ê²€ìƒ‰(Retrieve)**
2. ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ **ìƒì„±(Generate)**

```
Question â†’ [Retriever] â†’ Relevant Documents â†’ [LLM + Documents] â†’ Answer
              â†“                                        â†‘
           Knowledge Base â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì „í†µì  ì ‘ê·¼ë²•ê³¼ì˜ ì°¨ì´

**Before RAG (ëª¨ë¸ ì§€ì‹ë§Œ ì‚¬ìš©)**:
```python
question = "Anthropicì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€?"

# ëª¨ë¸ì´ í•™ìŠµí•œ ì§€ì‹ì—ë§Œ ì˜ì¡´
answer = llm.generate(question)
# â†’ "ì €ëŠ” í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤..." (ë˜ëŠ” í• ë£¨ì‹œë„¤ì´ì…˜)
```

**After RAG (ì™¸ë¶€ ì§€ì‹ í™œìš©)**:
```python
question = "Anthropicì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€?"

# Step 1: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
docs = retrieve_from_knowledge_base(question)
# â†’ ["Anthropic was founded by Dario Amodei...", ...]

# Step 2: ë¬¸ì„œì™€ í•¨ê»˜ ë‹µë³€ ìƒì„±
context = "\n".join(docs)
prompt = f"Context: {context}\n\nQuestion: {question}"
answer = llm.generate(prompt)
# â†’ "Dario Amodeiê°€ Anthropicì˜ CEOì…ë‹ˆë‹¤."
```

### RAGì˜ ì¥ì 

* **ìµœì‹  ì •ë³´ í™œìš©**: ì§€ì‹ë² ì´ìŠ¤ë§Œ ì—…ë°ì´íŠ¸í•˜ë©´ ìµœì‹  ì •ë³´ ì‚¬ìš© ê°€ëŠ¥  
* **ë„ë©”ì¸ íŠ¹í™”**: íšŒì‚¬ ë‚´ë¶€ ë¬¸ì„œ, ì „ë¬¸ ì§€ì‹ ì¶”ê°€ ê°€ëŠ¥  
* **í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì†Œ**: ì‹¤ì œ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ìœ¼ë¡œ ì‹ ë¢°ë„ í–¥ìƒ  
* **ì¶œì²˜ ì¶”ì **: ì–´ë–¤ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™”ëŠ”ì§€ ëª…í™•  
* **ë¹„ìš© íš¨ìœ¨ì **: ëª¨ë¸ ì¬í•™ìŠµ ì—†ì´ ì§€ì‹ í™•ì¥  
* **ê°œì¸ì •ë³´ ë³´í˜¸**: ë¯¼ê° ì •ë³´ë¥¼ ëª¨ë¸ì— ë„£ì§€ ì•Šê³  DBì—ë§Œ ì €ì¥

### RAGì˜ ì œì•½

* **ê²€ìƒ‰ í’ˆì§ˆ ì˜ì¡´**: ì˜ëª»ëœ ë¬¸ì„œ ê²€ìƒ‰ ì‹œ ì˜ëª»ëœ ë‹µë³€  
* **ì§€ì—° ì‹œê°„**: ê²€ìƒ‰ ë‹¨ê³„ë¡œ ì¸í•œ ì¶”ê°€ ë ˆì´í„´ì‹œ  
* **ì¸í”„ë¼ í•„ìš”**: Vector DB, ì„ë² ë”© ëª¨ë¸ ë“± ì¶”ê°€ ì‹œìŠ¤í…œ  
* **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ**: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì²˜ë¦¬ ë¶ˆê°€  
* **ë³µì¡ë„ ì¦ê°€**: ë‹¨ìˆœ LLM í˜¸ì¶œë³´ë‹¤ êµ¬í˜„ ë³µì¡

## RAGì˜ 2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤

### Indexing (ì˜¤í”„ë¼ì¸)

ì§€ì‹ë² ì´ìŠ¤ë¥¼ ì¤€ë¹„í•˜ëŠ” ë‹¨ê³„ë‹¤. ì‚¬ìš©ìê°€ ì§ˆë¬¸í•˜ê¸° ì „ì— ë¯¸ë¦¬ ìˆ˜í–‰ëœë‹¤.

```
Documents â†’ [Chunking] â†’ Chunks â†’ [Embedding] â†’ Vectors â†’ [Store] â†’ Vector DB
```

#### ë¬¸ì„œ ìˆ˜ì§‘

ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•œë‹¤:

```python
from typing import List, Dict

class DocumentLoader:
    """
    ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë¬¸ì„œ ë¡œë“œ
    """
    
    def load_from_files(self, file_paths: List[str]) -> List[Dict]:
        """íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ"""
        documents = []
        
        for path in file_paths:
            if path.endswith('.pdf'):
                text = self.extract_from_pdf(path)
            elif path.endswith('.docx'):
                text = self.extract_from_docx(path)
            elif path.endswith('.txt'):
                with open(path, 'r', encoding='utf-8') as f:
                    text = f.read()
            else:
                continue
            
            documents.append({
                'text': text,
                'metadata': {
                    'source': path,
                    'type': path.split('.')[-1]
                }
            })
        
        return documents
    
    def load_from_web(self, urls: List[str]) -> List[Dict]:
        """ì›¹í˜ì´ì§€ì—ì„œ ë¬¸ì„œ ë¡œë“œ"""
        import requests
        from bs4 import BeautifulSoup
        
        documents = []
        
        for url in urls:
            response = requests.get(url)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë³¸ë¬¸ ì¶”ì¶œ (ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            text = soup.get_text()
            
            documents.append({
                'text': text,
                'metadata': {
                    'source': url,
                    'type': 'web'
                }
            })
        
        return documents
    
    def load_from_database(self, query: str) -> List[Dict]:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¬¸ì„œ ë¡œë“œ"""
        # SQL ì¿¼ë¦¬ ì‹¤í–‰
        # results = db.execute(query)
        # return results
        pass
```

#### ë¬¸ì„œ ì²­í‚¹ (Chunking)

ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆˆë‹¤. 

**ì™œ ì²­í‚¹ì´ í•„ìš”í•œê°€?**
- LLM ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
- ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ (ì‘ì€ ë‹¨ìœ„ê°€ ë” ì •í™•)
- ë¬¸ë§¥ì˜ ë²”ìœ„ ë° ê´€ë ¨ì„± ë†’ì€ ë¶€ë¶„ë§Œ ì„ íƒ ê°€ëŠ¥

**ì²­í‚¹ ì „ëµ**:

```python
class TextChunker:
    """
    ë‹¤ì–‘í•œ ì²­í‚¹ ì „ëµ êµ¬í˜„
    """
    
    def chunk_by_tokens(
        self, 
        text: str, 
        chunk_size: int = 512,
        overlap: int = 50
    ) -> List[str]:
        """
        í† í° ê¸°ë°˜ ì²­í‚¹ (ê°€ì¥ ì¼ë°˜ì )
        
        Args:
            chunk_size: ì²­í¬ë‹¹ í† í° ìˆ˜
            overlap: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” í† í° ìˆ˜
        """
        # í† í°í™” (tiktoken ì‚¬ìš©)
        import tiktoken
        
        encoding = tiktoken.get_encoding("cl100k_base")
        tokens = encoding.encode(text)
        
        chunks = []
        start = 0
        
        while start < len(tokens):
            end = start + chunk_size
            chunk_tokens = tokens[start:end]
            chunk_text = encoding.decode(chunk_tokens)
            chunks.append(chunk_text)
            
            start += chunk_size - overlap  # ì˜¤ë²„ë© ì ìš©
        
        return chunks
    
    def chunk_by_sentences(
        self, 
        text: str, 
        sentences_per_chunk: int = 5,
        overlap_sentences: int = 1
    ) -> List[str]:
        """
        ë¬¸ì¥ ê¸°ë°˜ ì²­í‚¹ (ì˜ë¯¸ ë³´ì¡´)
        """
        import re
        
        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        chunks = []
        start = 0
        
        while start < len(sentences):
            end = start + sentences_per_chunk
            chunk = ' '.join(sentences[start:end])
            chunks.append(chunk)
            
            start += sentences_per_chunk - overlap_sentences
        
        return chunks
    
    def chunk_by_semantic(
        self, 
        text: str,
        similarity_threshold: float = 0.7
    ) -> List[str]:
        """
        ì˜ë¯¸ë¡ ì  ì²­í‚¹ (ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ê¸°ë°˜)
        
        ë¬¸ì¥ë“¤ì„ ì„ë² ë”©í•˜ê³ , ìœ ì‚¬ë„ê°€ ë‚®ì•„ì§€ëŠ” ì§€ì ì—ì„œ ë¶„ë¦¬
        """
        import re
        
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        if len(sentences) <= 1:
            return [text]
        
        # ê° ë¬¸ì¥ ì„ë² ë”©
        embeddings = [self.embed_text(s) for s in sentences]
        
        chunks = []
        current_chunk = [sentences[0]]
        
        for i in range(1, len(sentences)):
            # ì´ì „ ë¬¸ì¥ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = self.cosine_similarity(
                embeddings[i-1], 
                embeddings[i]
            )
            
            if similarity >= similarity_threshold:
                # ìœ ì‚¬ë„ ë†’ìŒ â†’ ê°™ì€ ì²­í¬
                current_chunk.append(sentences[i])
            else:
                # ìœ ì‚¬ë„ ë‚®ìŒ â†’ ìƒˆ ì²­í¬ ì‹œì‘
                chunks.append(' '.join(current_chunk))
                current_chunk = [sentences[i]]
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def chunk_by_structure(self, text: str, format_type: str) -> List[str]:
        """
        ë¬¸ì„œ êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ (ë§ˆí¬ë‹¤ìš´, HTML ë“±)
        """
        if format_type == 'markdown':
            # í—¤ë” ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
            import re
            chunks = re.split(r'\n#+\s', text)
            return [c.strip() for c in chunks if c.strip()]
        
        elif format_type == 'html':
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(text, 'html.parser')
            
            # ì„¹ì…˜ë³„ë¡œ ë¶„ë¦¬
            chunks = []
            for section in soup.find_all(['section', 'article', 'div']):
                chunk_text = section.get_text()
                if chunk_text.strip():
                    chunks.append(chunk_text.strip())
            
            return chunks
        
        else:
            # ê¸°ë³¸: ë¬¸ë‹¨ ê¸°ì¤€
            paragraphs = text.split('\n\n')
            return [p.strip() for p in paragraphs if p.strip()]
```

**ì²­í‚¹ íŒŒë¼ë¯¸í„° ì„ íƒ ê°€ì´ë“œ**:

| ì²­í¬ í¬ê¸° | ì¥ì  | ë‹¨ì  | ê¶Œì¥ ìš©ë„ |
|---------|------|------|----------|
| ì‘ìŒ (128-256) | ê²€ìƒ‰ ì •í™•ë„ ë†’ìŒ | ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡± | Q&A, íŒ©íŠ¸ ì²´í¬ |
| ì¤‘ê°„ (512-1024) | ê· í˜• ì¡í˜ | - | ì¼ë°˜ì  ì‚¬ìš© â­ |
| í¼ (2048+) | ì»¨í…ìŠ¤íŠ¸ í’ë¶€ | ë…¸ì´ì¦ˆ ì¦ê°€ | ìš”ì•½, ë¶„ì„ |

**Chunkingê°„ ì˜¤ë²„ë© ë²”ìœ„**:
- 0-50 í† í°: í‘œì¤€
- 50-100 í† í°: ë¬¸ë§¥ ì—°ê²° ì¤‘ìš” ì‹œ
- 100+ í† í°: ê³¼ë„, ë¹„íš¨ìœ¨ì 

#### ì„ë² ë”© ìƒì„±

ê° ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•œë‹¤.

```python
class EmbeddingGenerator:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
    """
    
    def __init__(self, model_name: str = "text-embedding-3-large"):
        """
        OpenAI Embeddings ì‚¬ìš© ì˜ˆì‹œ
        ë‹¤ë¥¸ ì˜µì…˜: sentence-transformers, Cohere, etc.
        """
        import openai
        self.client = openai.OpenAI()
        self.model = model_name
    
    def embed_text(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
        """
        response = self.client.embeddings.create(
            input=text,
            model=self.model
        )
        
        return response.data[0].embedding
    
    def embed_batch(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:
        """
        ë°°ì¹˜ ì„ë² ë”© (íš¨ìœ¨ì„±)
        """
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            
            response = self.client.embeddings.create(
                input=batch,
                model=self.model
            )
            
            batch_embeddings = [item.embedding for item in response.data]
            embeddings.extend(batch_embeddings)
        
        return embeddings
```

**ì„ë² ë”© ëª¨ë¸ ì„ íƒ**:

| ëª¨ë¸ | ì°¨ì› | ì„±ëŠ¥ | ë¹„ìš© | ì¶”ì²œ ìš©ë„ |
|-----|------|------|------|----------|
| text-embedding-3-small | 1536 | ì¤‘ | ë‚®ìŒ | ê°œë°œ/í…ŒìŠ¤íŠ¸ |
| text-embedding-3-large | 3072 | ë†’ìŒ | ì¤‘ê°„ | í”„ë¡œë•ì…˜ â­ |
| sentence-transformers | 384-768 | ì¤‘ | ë¬´ë£Œ | ë¡œì»¬/ì˜¤í”ˆì†ŒìŠ¤ |
| Cohere embed-v3 | 1024 | ë†’ìŒ | ì¤‘ê°„ | ë‹¤êµ­ì–´ ì§€ì› |

#### Vector DB ì €ì¥

ì„ë² ë”©ì„ ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì €ì¥í•œë‹¤.

```python
import chromadb
from typing import List, Dict

class VectorStore:
    """
    ChromaDBë¥¼ ì‚¬ìš©í•œ ë²¡í„° ì €ì¥ì†Œ
    """
    
    def __init__(self, collection_name: str = "documents"):
        self.client = chromadb.Client()
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        )
    
    def add_documents(
        self, 
        texts: List[str],
        embeddings: List[List[float]],
        metadatas: List[Dict] = None,
        ids: List[str] = None
    ):
        """
        ë¬¸ì„œ ì¶”ê°€
        """
        if ids is None:
            ids = [f"doc_{i}" for i in range(len(texts))]
        
        if metadatas is None:
            metadatas = [{} for _ in texts]
        
        self.collection.add(
            documents=texts,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )
    
    def query(
        self, 
        query_embedding: List[float],
        n_results: int = 5
    ) -> Dict:
        """
        ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        """
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        
        return results
```

**Vector DB ì„ íƒ ê°€ì´ë“œ**:

| DB | ì¥ì  | ë‹¨ì  | ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤ |
|----|------|------|-------------|
| **ChromaDB** | ê°„ë‹¨, ë¡œì»¬ ê°€ëŠ¥ | ìŠ¤ì¼€ì¼ ì œí•œ | ê°œë°œ, ì¤‘ì†Œê·œëª¨ |
| **Pinecone** | ê´€ë¦¬í˜•, ë¹ ë¦„ | ë¹„ìš©, ë²¤ë”ë½ì¸ | í”„ë¡œë•ì…˜, ê³ ì„±ëŠ¥ |
| **Weaviate** | ì˜¤í”ˆì†ŒìŠ¤, ê¸°ëŠ¥í’ë¶€ | ì„¤ì • ë³µì¡ | ì»¤ìŠ¤í„°ë§ˆì´ì§• í•„ìš” |
| **Qdrant** | ë¹ ë¦„, Rust ê¸°ë°˜ | ìƒëŒ€ì  ì‹ ìƒ | ê³ ì„±ëŠ¥ í•„ìš” |
| **FAISS** | ë§¤ìš° ë¹ ë¦„, Meta | DB ì•„ë‹˜(ë¼ì´ë¸ŒëŸ¬ë¦¬) | ì—°êµ¬, ë²¤ì¹˜ë§ˆí¬ |

### Query (ì˜¨ë¼ì¸ìƒ ì‚¬ìš©ì ì§ˆë¬¸)

ì‚¬ìš©ì ì§ˆë¬¸ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ë‹¨ê³„ë‹¤.

```
Question â†’ [Embed] â†’ Query Vector â†’ [Search] â†’ Top-K Docs â†’ [LLM + Context] â†’ Answer
              â†“                        â†“
         Embedding Model            Vector DB
```

#### ì§ˆë¬¸ ì„ë² ë”©

```python
def embed_query(query: str) -> List[float]:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
    ë¬¸ì„œì™€ ë™ì¼í•œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© í•„ìˆ˜!
    """
    embedding_generator = EmbeddingGenerator()
    return embedding_generator.embed_text(query)
```

#### ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰

```python
def retrieve_documents(
    query: str,
    vector_store: VectorStore,
    top_k: int = 5
) -> List[Dict]:
    """
    ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
    """
    # ì§ˆë¬¸ ì„ë² ë”©
    query_embedding = embed_query(query)
    
    # ê²€ìƒ‰
    results = vector_store.query(
        query_embedding=query_embedding,
        n_results=top_k
    )
    
    # ê²°ê³¼ í¬ë§·íŒ…
    documents = []
    for i in range(len(results['documents'][0])):
        documents.append({
            'text': results['documents'][0][i],
            'metadata': results['metadatas'][0][i],
            'distance': results['distances'][0][i]
        })
    
    return documents
```

**top_k ì„ íƒ ê°€ì´ë“œ**:

```python
# top_kê°€ ë„ˆë¬´ ì‘ìœ¼ë©´: í•„ìš”í•œ ì •ë³´ ëˆ„ë½
top_k = 1  # ë‹¨ìˆœ íŒ©íŠ¸ ì²´í¬ì—ë§Œ ì í•©

# top_kê°€ ì ë‹¹í•˜ë©´: ê· í˜• (ê¶Œì¥)
top_k = 3-5  # ëŒ€ë¶€ë¶„ì˜ ê²½ìš° â­

# top_kê°€ ë„ˆë¬´ í¬ë©´: ë…¸ì´ì¦ˆ ì¦ê°€, ì»¨í…ìŠ¤íŠ¸ ì˜¤ì—¼
top_k = 20+  # ë¹„íš¨ìœ¨ì , ì„±ëŠ¥ ì €í•˜
```

#### í”„ë¡¬í”„íŠ¸ êµ¬ì„±

ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•œë‹¤.

```python
def construct_rag_prompt(query: str, documents: List[Dict]) -> str:
    """
    RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    """
    # ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    context_parts = []
    for i, doc in enumerate(documents, 1):
        source = doc['metadata'].get('source', 'Unknown')
        text = doc['text']
        context_parts.append(f"[Document {i}] (Source: {source})\n{text}")
    
    context = "\n\n".join(context_parts)
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

    <documents>
    {context}
    </documents>

    <question>
    {query}
    </question>

    ë‹µë³€ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­:
    1. ë¬¸ì„œì˜ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
    2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
    3. ê°€ëŠ¥í•˜ë©´ ì–´ëŠ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™”ëŠ”ì§€ ëª…ì‹œí•˜ì„¸ìš”.
    4. ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ ì†”ì§íˆ "ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.

    ë‹µë³€:"""
    
    return prompt
```

**ì¢‹ì€ RAG í”„ë¡¬í”„íŠ¸ì˜ íŠ¹ì§•**:
- ë¬¸ì„œì™€ ì§ˆë¬¸ì„ ëª…í™•íˆ êµ¬ë¶„
- "ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€"ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì‹œ
- í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ì§€ì¹¨ í¬í•¨
- ì¶œì²˜ ì–¸ê¸‰ ìœ ë„

#### ë‹µë³€ ìƒì„±

```python
import anthropic

def generate_answer(prompt: str) -> str:
    """
    LLMìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
    """
    client = anthropic.Anthropic(api_key="your-api-key")
    
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1000,
        temperature=0,  # ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì´ë¯€ë¡œ ë‚®ì€ temperature
        messages=[{"role": "user", "content": prompt}]
    )
    
    return message.content[0].text
```

## ì™„ì „í•œ RAG ì‹œìŠ¤í…œ êµ¬í˜„

ì´ì œ ëª¨ë“  ë‹¨ê³„ë¥¼ ê²°í•©í•œ ì™„ì „í•œ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ë³´ì.

```python
import anthropic
import chromadb
import openai
from typing import List, Dict
import tiktoken

class SimpleRAGSystem:
    """
    ê°„ë‹¨í•˜ì§€ë§Œ ì™„ì „í•œ RAG ì‹œìŠ¤í…œ
    """
    
    def __init__(
        self,
        anthropic_api_key: str,
        openai_api_key: str,
        collection_name: str = "rag_docs"
    ):
        # LLM í´ë¼ì´ì–¸íŠ¸
        self.llm_client = anthropic.Anthropic(api_key=anthropic_api_key)
        
        # ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸
        self.embedding_client = openai.OpenAI(api_key=openai_api_key)
        
        # Vector DB
        self.chroma_client = chromadb.Client()
        self.collection = self.chroma_client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
        
        # í† í¬ë‚˜ì´ì €
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    def chunk_text(
        self, 
        text: str, 
        chunk_size: int = 512,
        overlap: int = 50
    ) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        """
        tokens = self.tokenizer.encode(text)
        chunks = []
        start = 0
        
        while start < len(tokens):
            end = start + chunk_size
            chunk_tokens = tokens[start:end]
            chunk_text = self.tokenizer.decode(chunk_tokens)
            chunks.append(chunk_text)
            start += chunk_size - overlap
        
        return chunks
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """
        í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”©
        """
        response = self.embedding_client.embeddings.create(
            input=texts,
            model="text-embedding-3-large"
        )
        
        return [item.embedding for item in response.data]
    
    def index_documents(
        self, 
        documents: List[Dict[str, str]],
        chunk_size: int = 512,
        overlap: int = 50
    ):
        """
        ë¬¸ì„œë“¤ì„ ì¸ë±ì‹±
        
        Args:
            documents: [{"text": "...", "metadata": {...}}, ...]
        """
        print(f"ğŸ“š {len(documents)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘...")
        
        all_chunks = []
        all_metadatas = []
        all_ids = []
        
        chunk_counter = 0
        
        for doc_idx, doc in enumerate(documents):
            text = doc['text']
            metadata = doc.get('metadata', {})
            
            # ì²­í‚¹
            chunks = self.chunk_text(text, chunk_size, overlap)
            
            print(f"  ë¬¸ì„œ {doc_idx + 1}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            for chunk_idx, chunk in enumerate(chunks):
                all_chunks.append(chunk)
                
                # ë©”íƒ€ë°ì´í„°ì— ì²­í¬ ì •ë³´ ì¶”ê°€
                chunk_metadata = metadata.copy()
                chunk_metadata.update({
                    'doc_index': doc_idx,
                    'chunk_index': chunk_idx,
                    'chunk_id': f"doc{doc_idx}_chunk{chunk_idx}"
                })
                all_metadatas.append(chunk_metadata)
                
                all_ids.append(f"chunk_{chunk_counter}")
                chunk_counter += 1
        
        print(f"\nğŸ”¢ ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")
        print(f"ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘...")
        
        # ì„ë² ë”© ìƒì„± (ë°°ì¹˜ë¡œ ì²˜ë¦¬)
        batch_size = 100
        all_embeddings = []
        
        for i in range(0, len(all_chunks), batch_size):
            batch = all_chunks[i:i+batch_size]
            embeddings = self.embed_texts(batch)
            all_embeddings.extend(embeddings)
            print(f"  {i + len(batch)}/{len(all_chunks)} ì™„ë£Œ")
        
        print(f"\nğŸ’¾ Vector DBì— ì €ì¥ ì¤‘...")
        
        # Vector DBì— ì €ì¥
        self.collection.add(
            documents=all_chunks,
            embeddings=all_embeddings,
            metadatas=all_metadatas,
            ids=all_ids
        )
        
        print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ!\n")
    
    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰
        """
        # ì§ˆë¬¸ ì„ë² ë”©
        query_embedding = self.embed_texts([query])[0]
        
        # ê²€ìƒ‰
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        documents = []
        for i in range(len(results['documents'][0])):
            documents.append({
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i]
            })
        
        return documents
    
    def generate_answer(self, query: str, documents: List[Dict]) -> Dict:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        """
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for i, doc in enumerate(documents, 1):
            source = doc['metadata'].get('source', 'Unknown')
            chunk_id = doc['metadata'].get('chunk_id', 'Unknown')
            text = doc['text']
            distance = doc['distance']
            
            context_parts.append(
                f"[Document {i}] (Source: {source}, ID: {chunk_id}, "
                f"Relevance: {1-distance:.3f})\n{text}"
            )
        
        context = "\n\n".join(context_parts)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

        <documents>
        {context}
        </documents>

        <question>
        {query}
        </question>

        ë‹µë³€ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­:
        1. ë¬¸ì„œì˜ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
        3. ê°€ëŠ¥í•˜ë©´ ì–´ëŠ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™”ëŠ”ì§€ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: "Document 1ì— ë”°ë¥´ë©´...").
        4. ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.

        ë‹µë³€:"""
        
        # LLM í˜¸ì¶œ
        message = self.llm_client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1000,
            temperature=0,
            messages=[{"role": "user", "content": prompt}]
        )
        
        answer = message.content[0].text
        
        return {
            'answer': answer,
            'sources': documents,
            'prompt': prompt
        }
    
    def query(self, question: str, top_k: int = 5, verbose: bool = True) -> Dict:
        """
        RAG ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        """
        if verbose:
            print(f"â“ ì§ˆë¬¸: {question}\n")
            print(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ (top_k={top_k})...")
        
        # Step 1: ê²€ìƒ‰
        documents = self.retrieve(question, top_k=top_k)
        
        if verbose:
            print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ\n")
            for i, doc in enumerate(documents, 1):
                relevance = 1 - doc['distance']
                chunk_id = doc['metadata'].get('chunk_id', 'Unknown')
                print(f"  [{i}] {chunk_id} (ê´€ë ¨ë„: {relevance:.3f})")
                print(f"      {doc['text'][:100]}...")
            print()
        
        # Step 2: ë‹µë³€ ìƒì„±
        if verbose:
            print(f"ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘...")
        
        result = self.generate_answer(question, documents)
        
        if verbose:
            print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ\n")
            print(f"{'='*80}")
            print(f"ë‹µë³€:")
            print(f"{'='*80}")
            print(result['answer'])
            print(f"{'='*80}\n")
        
        return result


# ì‚¬ìš© ì˜ˆì‹œ
def main():
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = SimpleRAGSystem(
        anthropic_api_key="your-anthropic-key",
        openai_api_key="your-openai-key",
        collection_name="company_docs"
    )
    
    # ë¬¸ì„œ ì¤€ë¹„
    documents = [
        {
            "text": """
Anthropic is an AI safety and research company based in San Francisco. 
The company was founded in 2021 by former members of OpenAI, including 
Dario Amodei (CEO) and Daniela Amodei (President).

Anthropic's mission is to build reliable, interpretable, and steerable 
AI systems. The company is best known for developing Claude, a family 
of large language models designed with a focus on safety and helpfulness.

Claude is built using Constitutional AI (CAI), a method developed by 
Anthropic that trains AI systems to be helpful, harmless, and honest.
            """,
            "metadata": {
                "source": "company_overview.txt",
                "date": "2024-01"
            }
        },
        {
            "text": """
In 2023, Anthropic launched Claude 2, which featured improved performance 
and a larger context window of 100,000 tokens. The company has also 
developed Claude Pro, a paid subscription service offering priority 
access and enhanced capabilities.

Anthropic has raised significant funding, including a $4 billion 
investment from Amazon in 2023. The company partners with various 
organizations to deploy Claude in different applications, including 
customer service, content generation, and research assistance.

In 2024, Anthropic released Claude 3 family (Opus, Sonnet, Haiku) and 
later Claude 3.5 Sonnet, which showed significant improvements in 
reasoning and coding capabilities.
            """,
            "metadata": {
                "source": "company_history.txt",
                "date": "2024-06"
            }
        },
        {
            "text": """
Claude's architecture is based on transformer models, similar to GPT, 
but with several key differences in training methodology. The Constitutional 
AI approach uses a set of principles (a "constitution") to guide the AI's 
behavior during training.

The training process involves two main phases: supervised learning and 
reinforcement learning from human feedback (RLHF). However, Anthropic's 
approach reduces reliance on human feedback by having the model critique 
and revise its own responses based on constitutional principles.

This approach aims to create AI systems that are more aligned with human 
values and less likely to produce harmful outputs.
            """,
            "metadata": {
                "source": "technical_details.txt",
                "date": "2024-03"
            }
        }
    ]
    
    # ë¬¸ì„œ ì¸ë±ì‹±
    rag.index_documents(documents, chunk_size=256, overlap=30)
    
    # ì§ˆë¬¸ë“¤
    questions = [
        "Anthropicì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€?",
        "ClaudeëŠ” ì–´ë–»ê²Œ í•™ìŠµë˜ì—ˆë‚˜?",
        "Anthropicì€ ì–¸ì œ ì„¤ë¦½ë˜ì—ˆë‚˜?",
        "Claude 3ì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€?",
        "Anthropicì˜ ì§ì› ìˆ˜ëŠ” ëª‡ ëª…ì¸ê°€?"  # ë¬¸ì„œì— ì—†ëŠ” ì •ë³´
    ]
    
    # ê° ì§ˆë¬¸ì— ëŒ€í•´ RAG ì‹¤í–‰
    for question in questions:
        print("\n" + "="*100)
        result = rag.query(question, top_k=3, verbose=True)
        print("="*100 + "\n")


if __name__ == "__main__":
    main()
```

### ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ

```
ğŸ“š 3ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘...
  ë¬¸ì„œ 1: 3ê°œ ì²­í¬ ìƒì„±
  ë¬¸ì„œ 2: 3ê°œ ì²­í¬ ìƒì„±
  ë¬¸ì„œ 3: 3ê°œ ì²­í¬ ìƒì„±

ğŸ”¢ ì´ 9ê°œ ì²­í¬ ìƒì„±
ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘...
  9/9 ì™„ë£Œ

ğŸ’¾ Vector DBì— ì €ì¥ ì¤‘...
âœ… ì¸ë±ì‹± ì™„ë£Œ!

====================================================================================================
â“ ì§ˆë¬¸: Anthropicì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€?

ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ (top_k=3)...
âœ… 3ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ

  [1] doc0_chunk0 (ê´€ë ¨ë„: 0.876)
      Anthropic is an AI safety and research company based in San Francisco. 
The company was founded...
  [2] doc1_chunk0 (ê´€ë ¨ë„: 0.745)
      In 2023, Anthropic launched Claude 2, which featured improved performance...
  [3] doc2_chunk0 (ê´€ë ¨ë„: 0.698)
      Claude's architecture is based on transformer models, similar to GPT...

ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘...
âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ

================================================================================
ë‹µë³€:
================================================================================
Document 1ì— ë”°ë¥´ë©´, Anthropicì˜ CEOëŠ” Dario Amodeiì…ë‹ˆë‹¤. ê·¸ëŠ” OpenAIì˜ 
ì „ ë©¤ë²„ë¡œì„œ 2021ë…„ì— Anthropicì„ ê³µë™ ì„¤ë¦½í–ˆìŠµë‹ˆë‹¤.
================================================================================

====================================================================================================
```

## RAG ì‹œìŠ¤í…œ ìµœì í™” ê¸°ë²•

ê¸°ë³¸ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í–ˆë‹¤ë©´, ì´ì œ ì„±ëŠ¥ì„ ê°œì„ í•  ì°¨ë¡€ë‹¤. ì‹¤ë¬´ì—ì„œëŠ” ë‹¨ìˆœí•œ Naive RAGë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•œ ê²½ìš°ê°€ ë§ë‹¤.

### ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜

**ì¦ìƒ**: ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ê±°ë‚˜, ê´€ë ¨ ìˆëŠ” ë¬¸ì„œë¥¼ ë†“ì¹¨

**ì›ì¸**:
- ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±
- í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ í•„ìš”
- ë¬¸ì„œì˜ ì¤‘ìš”ë„ ë¬´ì‹œ

**í•´ê²°ì±…**: Hybrid Search (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)

```python
from typing import List, Dict, Tuple
import numpy as np

class HybridSearchRAG:
    """
    ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ RAG
    """
    
    def __init__(self):
        # ë²¡í„° ê²€ìƒ‰ (Dense)
        self.vector_store = None  # ChromaDB ë“±
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ (Sparse) - BM25
        self.bm25_index = None
        self.documents = []
    
    def index_documents(self, documents: List[Dict]):
        """
        ë¬¸ì„œë¥¼ ë²¡í„° DBì™€ BM25 ì¸ë±ìŠ¤ì— ëª¨ë‘ ì €ì¥
        """
        # 1. Vector DB ì¸ë±ì‹± (ê¸°ì¡´ê³¼ ë™ì¼)
        self._index_to_vector_db(documents)
        
        # 2. BM25 ì¸ë±ì‹±
        self._index_to_bm25(documents)
    
    def _index_to_bm25(self, documents: List[Dict]):
        """
        BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
        """
        from rank_bm25 import BM25Okapi
        
        # ë¬¸ì„œ í† í°í™”
        tokenized_docs = []
        for doc in documents:
            tokens = doc['text'].lower().split()
            tokenized_docs.append(tokens)
        
        # BM25 ì¸ë±ìŠ¤ ìƒì„±
        self.bm25_index = BM25Okapi(tokenized_docs)
        self.documents = documents
    
    def hybrid_search(
        self, 
        query: str, 
        top_k: int = 10,
        alpha: float = 0.5
    ) -> List[Dict]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            alpha: ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (0~1)
                   1.0 = 100% ë²¡í„° ê²€ìƒ‰
                   0.0 = 100% í‚¤ì›Œë“œ ê²€ìƒ‰
                   0.5 = ê· í˜•
        """
        # 1. ë²¡í„° ê²€ìƒ‰ (ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„)
        vector_results = self._vector_search(query, top_k=top_k*2)
        
        # 2. BM25 ê²€ìƒ‰ (í‚¤ì›Œë“œ ë§¤ì¹­)
        bm25_results = self._bm25_search(query, top_k=top_k*2)
        
        # 3. ì ìˆ˜ ì •ê·œí™” ë° ê²°í•©
        combined_scores = self._combine_scores(
            vector_results, 
            bm25_results, 
            alpha
        )
        
        # 4. ìƒìœ„ kê°œ ë°˜í™˜
        sorted_results = sorted(
            combined_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        top_docs = []
        for doc_id, score in sorted_results[:top_k]:
            top_docs.append({
                'document': self.documents[doc_id],
                'score': score
            })
        
        return top_docs
    
    def _vector_search(self, query: str, top_k: int) -> Dict[int, float]:
        """
        ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        Returns: {doc_id: similarity_score}
        """
        # ì§ˆë¬¸ ì„ë² ë”©
        query_embedding = self.embed_query(query)
        
        # Vector DB ê²€ìƒ‰
        results = self.vector_store.query(
            query_embedding=query_embedding,
            n_results=top_k
        )
        
        # doc_id: score ë§¤í•‘
        scores = {}
        for i, doc_id in enumerate(results['ids'][0]):
            # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ì½”ì‚¬ì¸ ê±°ë¦¬ â†’ ìœ ì‚¬ë„)
            distance = results['distances'][0][i]
            similarity = 1 - distance
            scores[doc_id] = similarity
        
        return scores
    
    def _bm25_search(self, query: str, top_k: int) -> Dict[int, float]:
        """
        BM25 ê²€ìƒ‰ ìˆ˜í–‰
        Returns: {doc_id: bm25_score}
        """
        # ì¿¼ë¦¬ í† í°í™”
        query_tokens = query.lower().split()
        
        # BM25 ì ìˆ˜ ê³„ì‚°
        bm25_scores = self.bm25_index.get_scores(query_tokens)
        
        # ìƒìœ„ kê°œì˜ ë¬¸ì„œ IDì™€ ì ìˆ˜
        top_indices = np.argsort(bm25_scores)[-top_k:][::-1]
        
        scores = {}
        for idx in top_indices:
            scores[idx] = bm25_scores[idx]
        
        return scores
    
    def _combine_scores(
        self, 
        vector_scores: Dict[int, float],
        bm25_scores: Dict[int, float],
        alpha: float
    ) -> Dict[int, float]:
        """
        ë‘ ê²€ìƒ‰ ê²°ê³¼ì˜ ì ìˆ˜ë¥¼ ê²°í•©
        
        Reciprocal Rank Fusion (RRF) ë˜ëŠ” ê°€ì¤‘ í•©ê³„ ì‚¬ìš© ê°€ëŠ¥
        ì—¬ê¸°ì„œëŠ” ì •ê·œí™” í›„ ê°€ì¤‘ í•©ê³„ ì‚¬ìš©
        """
        # ì ìˆ˜ ì •ê·œí™” (0~1 ë²”ìœ„ë¡œ)
        def normalize_scores(scores: Dict[int, float]) -> Dict[int, float]:
            if not scores:
                return {}
            
            min_score = min(scores.values())
            max_score = max(scores.values())
            
            if max_score == min_score:
                return {k: 1.0 for k in scores}
            
            return {
                k: (v - min_score) / (max_score - min_score)
                for k, v in scores.items()
            }
        
        norm_vector = normalize_scores(vector_scores)
        norm_bm25 = normalize_scores(bm25_scores)
        
        # ëª¨ë“  ë¬¸ì„œ ID ìˆ˜ì§‘
        all_doc_ids = set(norm_vector.keys()) | set(norm_bm25.keys())
        
        # ê°€ì¤‘ í•©ê³„
        combined = {}
        for doc_id in all_doc_ids:
            vector_score = norm_vector.get(doc_id, 0.0)
            bm25_score = norm_bm25.get(doc_id, 0.0)
            
            combined[doc_id] = alpha * vector_score + (1 - alpha) * bm25_score
        
        return combined
```

**Hybrid Search ì‚¬ìš© ì˜ˆì‹œ**:

```python
# ì´ˆê¸°í™”
rag = HybridSearchRAG()

# ë¬¸ì„œ ì¸ë±ì‹±
documents = load_documents()
rag.index_documents(documents)

# ê²€ìƒ‰
query = "Anthropic CEO"

# ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ë§Œ (alpha=1.0)
semantic_results = rag.hybrid_search(query, top_k=5, alpha=1.0)

# í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ (alpha=0.0)
keyword_results = rag.hybrid_search(query, top_k=5, alpha=0.0)

# í•˜ì´ë¸Œë¦¬ë“œ (alpha=0.5)
hybrid_results = rag.hybrid_search(query, top_k=5, alpha=0.5)
```

**alpha íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ**:

| alpha | íŠ¹ì§• | ì¶”ì²œ ì‚¬ìš© ì‚¬ë¡€ |
|-------|------|--------------|
| 0.0-0.3 | í‚¤ì›Œë“œ ì¤‘ì‹¬ | ì •í™•í•œ ìš©ì–´ ë§¤ì¹­ í•„ìš” (ë²•ë¥ , ì˜ë£Œ) |
| 0.4-0.6 | ê· í˜• | ì¼ë°˜ì  Q&A â­ |
| 0.7-1.0 | ì˜ë¯¸ë¡  ì¤‘ì‹¬ | ê°œë…ì  ì§ˆë¬¸, ìœ ì—°í•œ í‘œí˜„ |

### ê²€ìƒ‰ ìˆœìœ„ê°€ ìµœì ì´ ì•„ë‹˜

**ì¦ìƒ**: ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ì§€ë§Œ, ìˆœì„œê°€ ì˜ëª»ë¨

**í•´ê²°ì±…**: Re-ranking (ì¬ìˆœìœ„í™”)

```python
class RerankerRAG:
    """
    ê²€ìƒ‰ í›„ ì¬ìˆœìœ„í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” RAG
    """
    
    def __init__(self):
        self.base_rag = SimpleRAGSystem()
        self.reranker = self._load_reranker()
    
    def _load_reranker(self):
        """
        ì¬ìˆœìœ„í™” ëª¨ë¸ ë¡œë“œ
        
        ì˜µì…˜:
        1. Cross-encoder ëª¨ë¸ (BERT ê¸°ë°˜)
        2. Cohere Rerank API
        3. ì»¤ìŠ¤í…€ ëª¨ë¸
        """
        from sentence_transformers import CrossEncoder
        
        # MS MARCO ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ cross-encoder
        model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        return model
    
    def retrieve_and_rerank(
        self, 
        query: str,
        initial_k: int = 20,
        final_k: int = 5
    ) -> List[Dict]:
        """
        ê²€ìƒ‰ í›„ ì¬ìˆœìœ„í™”
        
        Args:
            initial_k: ì´ˆê¸° ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (ë§ì´ ê°€ì ¸ì˜´)
            final_k: ìµœì¢… ë°˜í™˜ ë¬¸ì„œ ìˆ˜ (ì¬ìˆœìœ„ í›„ ìƒìœ„)
        """
        # Step 1: ì´ˆê¸° ê²€ìƒ‰ (ë§ì€ ë¬¸ì„œ)
        candidates = self.base_rag.retrieve(query, top_k=initial_k)
        
        print(f"ğŸ” ì´ˆê¸° ê²€ìƒ‰: {len(candidates)}ê°œ ë¬¸ì„œ")
        
        # Step 2: ì¬ìˆœìœ„í™”
        pairs = [[query, doc['text']] for doc in candidates]
        scores = self.reranker.predict(pairs)
        
        # Step 3: ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì¬ì •ë ¬
        for i, doc in enumerate(candidates):
            doc['rerank_score'] = scores[i]
        
        reranked = sorted(
            candidates, 
            key=lambda x: x['rerank_score'], 
            reverse=True
        )
        
        print(f"ğŸ”„ ì¬ìˆœìœ„í™” ì™„ë£Œ")
        print(f"ğŸ“Š ìƒìœ„ ë¬¸ì„œ ì ìˆ˜ ë³€í™”:")
        for i in range(min(3, len(reranked))):
            doc = reranked[i]
            print(f"  [{i+1}] ì›ë˜ ìˆœìœ„: {candidates.index(doc)+1} "
                  f"â†’ ì¬ìˆœìœ„ ì ìˆ˜: {doc['rerank_score']:.3f}")
        
        # Step 4: ìƒìœ„ kê°œë§Œ ë°˜í™˜
        return reranked[:final_k]
```

**ì¬ìˆœìœ„í™” ëª¨ë¸ ì„ íƒ**:

| ëª¨ë¸ | ì†ë„ | ì„±ëŠ¥ | ë¹„ìš© | ì¶”ì²œ |
|-----|------|------|------|------|
| Cross-encoder (local) | ëŠë¦¼ | ë†’ìŒ | ë¬´ë£Œ | ì •í™•ë„ ì¤‘ìš” ì‹œ |
| Cohere Rerank API | ë¹ ë¦„ | ë§¤ìš° ë†’ìŒ | ìœ ë£Œ | í”„ë¡œë•ì…˜ â­ |
| LLM-based (GPT/Claude) | ë§¤ìš° ëŠë¦¼ | ë†’ìŒ | ë¹„ìŒˆ | íŠ¹ìˆ˜ ë„ë©”ì¸ |

**Cohere Rerank ì‚¬ìš© ì˜ˆì‹œ**:

```python
import cohere

def rerank_with_cohere(query: str, documents: List[str], top_k: int = 5):
    """
    Cohere Rerank API ì‚¬ìš©
    """
    co = cohere.Client("your-api-key")
    
    results = co.rerank(
        query=query,
        documents=documents,
        top_n=top_k,
        model="rerank-english-v2.0"
    )
    
    reranked_docs = []
    for result in results:
        reranked_docs.append({
            'text': documents[result.index],
            'score': result.relevance_score
        })
    
    return reranked_docs
```

### ì¿¼ë¦¬ê°€ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶€ì ì ˆí•¨

**ì¦ìƒ**: ì‚¬ìš©ì ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜, ì• ë§¤í•˜ê±°ë‚˜, ê²€ìƒ‰ì— ì í•©í•˜ì§€ ì•ŠìŒ

**í•´ê²°ì±…**: Query Transformation (ì¿¼ë¦¬ ë³€í™˜)

#### ê¸°ë²• 1: Query Expansion (ì¿¼ë¦¬ í™•ì¥)

```python
def expand_query(query: str) -> List[str]:
    """
    í•˜ë‚˜ì˜ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ë³€í˜•ìœ¼ë¡œ í™•ì¥
    """
    prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ 3ê°€ì§€ ë³€í˜•í•˜ì—¬ í‘œí˜„í•˜ì„¸ìš”.
ì›ë˜ ì˜ë¯¸ëŠ” ìœ ì§€í•˜ë˜, ë‹¤ë¥¸ ë‹¨ì–´ë‚˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.

ì›ë˜ ì§ˆë¬¸: {query}

ë³€í˜• 1:
ë³€í˜• 2:
ë³€í˜• 3:"""

    client = anthropic.Anthropic(api_key="your-api-key")
    
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=300,
        temperature=0.7,
        messages=[{"role": "user", "content": prompt}]
    )
    
    response = message.content[0].text
    
    # ë³€í˜• íŒŒì‹±
    variations = [query]  # ì›ë˜ ì¿¼ë¦¬ í¬í•¨
    for line in response.split('\n'):
        line = line.strip()
        if line and not line.startswith('ë³€í˜•'):
            variations.append(line)
    
    return variations[:4]  # ì›ë³¸ + 3ê°œ ë³€í˜•

def search_with_expanded_query(
    query: str, 
    rag: SimpleRAGSystem,
    top_k_per_query: int = 3
) -> List[Dict]:
    """
    í™•ì¥ëœ ì¿¼ë¦¬ë“¤ë¡œ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ í†µí•©
    """
    # ì¿¼ë¦¬ í™•ì¥
    expanded_queries = expand_query(query)
    
    print(f"ğŸ”„ ì¿¼ë¦¬ í™•ì¥:")
    for i, q in enumerate(expanded_queries):
        print(f"  [{i+1}] {q}")
    
    # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
    all_results = []
    seen_ids = set()
    
    for q in expanded_queries:
        results = rag.retrieve(q, top_k=top_k_per_query)
        
        for doc in results:
            doc_id = doc['metadata'].get('chunk_id', doc['text'][:50])
            
            # ì¤‘ë³µ ì œê±°
            if doc_id not in seen_ids:
                all_results.append(doc)
                seen_ids.add(doc_id)
    
    # ì¬ìˆœìœ„í™” (ì„ íƒì )
    # ...
    
    return all_results
```

#### Query Decomposition (ì¿¼ë¦¬ ë¶„í•´)

ë³µì¡í•œ ì§ˆë¬¸ì„ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.

```python
def decompose_query(query: str) -> List[str]:
    """
    ë³µì¡í•œ ì§ˆë¬¸ì„ í•˜ìœ„ ì§ˆë¬¸ë“¤ë¡œ ë¶„í•´
    """
    prompt = f"""ë‹¤ìŒ ë³µì¡í•œ ì§ˆë¬¸ì„ ë‹µí•˜ê¸° ìœ„í•´ í•„ìš”í•œ í•˜ìœ„ ì§ˆë¬¸ë“¤ë¡œ ë¶„í•´í•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

í•˜ìœ„ ì§ˆë¬¸ë“¤ (ê° ì¤„ì— í•˜ë‚˜ì”©):
1."""

    client = anthropic.Anthropic(api_key="your-api-key")
    
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=300,
        temperature=0,
        messages=[{"role": "user", "content": prompt}]
    )
    
    response = message.content[0].text
    
    # í•˜ìœ„ ì§ˆë¬¸ íŒŒì‹±
    sub_queries = []
    for line in response.split('\n'):
        line = line.strip()
        # "1. " ê°™ì€ ë²ˆí˜¸ ì œê±°
        import re
        line = re.sub(r'^\d+\.\s*', '', line)
        
        if line:
            sub_queries.append(line)
    
    return sub_queries

def answer_with_decomposition(
    query: str, 
    rag: SimpleRAGSystem
) -> str:
    """
    ì¿¼ë¦¬ ë¶„í•´ë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±
    """
    print(f"â“ ì›ë˜ ì§ˆë¬¸: {query}\n")
    
    # Step 1: ì§ˆë¬¸ ë¶„í•´
    sub_queries = decompose_query(query)
    
    print(f"ğŸ” í•˜ìœ„ ì§ˆë¬¸ë“¤:")
    for i, sq in enumerate(sub_queries, 1):
        print(f"  {i}. {sq}")
    print()
    
    # Step 2: ê° í•˜ìœ„ ì§ˆë¬¸ì— ë‹µë³€
    sub_answers = []
    for i, sq in enumerate(sub_queries, 1):
        print(f"ğŸ“ í•˜ìœ„ ì§ˆë¬¸ {i} ë‹µë³€ ì¤‘...")
        result = rag.query(sq, top_k=3, verbose=False)
        sub_answers.append({
            'question': sq,
            'answer': result['answer']
        })
    
    # Step 3: í•˜ìœ„ ë‹µë³€ë“¤ì„ ì¢…í•©
    synthesis_prompt = f"""ë‹¤ìŒì€ ë³µì¡í•œ ì§ˆë¬¸ì— ëŒ€í•œ í•˜ìœ„ ì§ˆë¬¸ë“¤ê³¼ ê·¸ ë‹µë³€ë“¤ì…ë‹ˆë‹¤.
ì´ë¥¼ ì¢…í•©í•˜ì—¬ ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ ì™„ì „í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ì›ë˜ ì§ˆë¬¸: {query}

í•˜ìœ„ ì§ˆë¬¸ê³¼ ë‹µë³€:
"""
    
    for i, sa in enumerate(sub_answers, 1):
        synthesis_prompt += f"\nì§ˆë¬¸ {i}: {sa['question']}\në‹µë³€ {i}: {sa['answer']}\n"
    
    synthesis_prompt += "\nì¢…í•© ë‹µë³€:"
    
    client = anthropic.Anthropic(api_key="your-api-key")
    
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1000,
        temperature=0,
        messages=[{"role": "user", "content": synthesis_prompt}]
    )
    
    final_answer = message.content[0].text
    
    print(f"\nâœ… ìµœì¢… ë‹µë³€:")
    print(final_answer)
    
    return final_answer
```

**ì‚¬ìš© ì˜ˆì‹œ**:

```python
# ë³µì¡í•œ ì§ˆë¬¸
query = "Anthropicì€ ì–´ë–¤ íšŒì‚¬ì´ê³ , ì£¼ìš” ì œí’ˆì€ ë¬´ì—‡ì´ë©°, ìµœê·¼ íˆ¬ì í˜„í™©ì€ ì–´ë–¤ê°€?"

# ì¿¼ë¦¬ ë¶„í•´ ë°©ì‹ìœ¼ë¡œ ë‹µë³€
answer = answer_with_decomposition(query, rag)

# ì¶œë ¥:
# í•˜ìœ„ ì§ˆë¬¸ë“¤:
#   1. Anthropicì€ ì–´ë–¤ íšŒì‚¬ì¸ê°€?
#   2. Anthropicì˜ ì£¼ìš” ì œí’ˆì€ ë¬´ì—‡ì¸ê°€?
#   3. Anthropicì˜ ìµœê·¼ íˆ¬ì í˜„í™©ì€?
# 
# (ê° í•˜ìœ„ ì§ˆë¬¸ ë‹µë³€ í›„)
# 
# ìµœì¢… ë‹µë³€:
# Anthropicì€ 2021ë…„ì— ì„¤ë¦½ëœ AI ì•ˆì „ì„± ì—°êµ¬ íšŒì‚¬ì…ë‹ˆë‹¤. 
# ì£¼ìš” ì œí’ˆì€ Claudeë¼ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ì‹œë¦¬ì¦ˆì´ë©°...
# 2023ë…„ Amazonìœ¼ë¡œë¶€í„° 40ì–µ ë‹¬ëŸ¬ì˜ íˆ¬ìë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.
```

#### Hypothetical Document Embeddings (HyDE)

ì‹¤ì œ ë‹µë³€ì„ ê°€ìƒìœ¼ë¡œ ìƒì„±í•œ í›„, ê·¸ ë‹µë³€ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•œë‹¤.

```python
def hyde_search(query: str, rag: SimpleRAGSystem, top_k: int = 5) -> List[Dict]:
    """
    HyDE (Hypothetical Document Embeddings) ê²€ìƒ‰
    
    1. ì§ˆë¬¸ì— ëŒ€í•œ ê°€ìƒì˜ ë‹µë³€ ìƒì„±
    2. ë‹µë³€ì„ ì„ë² ë”©í•˜ì—¬ ê²€ìƒ‰
    3. ìœ ì‚¬í•œ ì‹¤ì œ ë¬¸ì„œ ë°˜í™˜
    """
    print(f"â“ ì§ˆë¬¸: {query}\n")
    
    # Step 1: ê°€ìƒ ë‹µë³€ ìƒì„±
    hyde_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
ì‹¤ì œ ì •ë³´ë¥¼ ëª¨ë¥´ë”ë¼ë„ í•©ë¦¬ì ì´ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

ë‹µë³€:"""
    
    client = anthropic.Anthropic(api_key="your-api-key")
    
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=300,
        temperature=0.7,
        messages=[{"role": "user", "content": hyde_prompt}]
    )
    
    hypothetical_answer = message.content[0].text
    
    print(f"ğŸ’­ ê°€ìƒ ë‹µë³€:")
    print(hypothetical_answer[:200] + "...")
    print()
    
    # Step 2: ê°€ìƒ ë‹µë³€ìœ¼ë¡œ ê²€ìƒ‰
    # ì›ë˜ ì§ˆë¬¸ ëŒ€ì‹  ê°€ìƒ ë‹µë³€ì„ ì„ë² ë”©í•˜ì—¬ ê²€ìƒ‰
    print(f"ğŸ” ê°€ìƒ ë‹µë³€ ê¸°ë°˜ ê²€ìƒ‰ ì¤‘...")
    
    # ê°€ìƒ ë‹µë³€ ì„ë² ë”©
    hypothetical_embedding = rag.embed_texts([hypothetical_answer])[0]
    
    # Vector DB ê²€ìƒ‰
    results = rag.collection.query(
        query_embeddings=[hypothetical_embedding],
        n_results=top_k
    )
    
    # ê²°ê³¼ í¬ë§·íŒ…
    documents = []
    for i in range(len(results['documents'][0])):
        documents.append({
            'text': results['documents'][0][i],
            'metadata': results['metadatas'][0][i],
            'distance': results['distances'][0][i]
        })
    
    print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ\n")
    
    return documents
```

**HyDEê°€ íš¨ê³¼ì ì¸ ê²½ìš°**:
- âœ… ì§ˆë¬¸ì´ ì§§ê³  ëª¨í˜¸í•  ë•Œ
- âœ… ë‹µë³€ í˜•ì‹ì´ ì˜ˆì¸¡ ê°€ëŠ¥í•  ë•Œ
- âœ… ë„ë©”ì¸ ì „ë¬¸ ìš©ì–´ê°€ ë§ì„ ë•Œ

**ì£¼ì˜ì‚¬í•­**:
- âŒ ì¶”ê°€ LLM í˜¸ì¶œë¡œ ì¸í•œ ë¹„ìš©/ì§€ì—°
- âŒ ê°€ìƒ ë‹µë³€ì´ ì˜ëª»ë˜ë©´ ê²€ìƒ‰ë„ ì‹¤íŒ¨

## ê¸´ ë¬¸ì„œ ì²˜ë¦¬ ì „ëµ

### ë¬¸ì œ: ë‹¨ì¼ ë¬¸ì„œê°€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì´ˆê³¼

**ì‹œë‚˜ë¦¬ì˜¤**: 500í˜ì´ì§€ PDF ë³´ê³ ì„œë¥¼ RAGì— ì¶”ê°€

#### Parent Document Retriever

ì‘ì€ ì²­í¬ë¡œ ê²€ìƒ‰í•˜ë˜, ì „ì²´ ë¬¸ì„œë¥¼ ë°˜í™˜í•œë‹¤.

```python
class ParentDocumentRetriever:
    """
    ì‘ì€ ì²­í¬ë¡œ ê²€ìƒ‰, ë¶€ëª¨ ë¬¸ì„œ ì „ì²´ ë°˜í™˜
    """
    
    def __init__(self):
        self.chunk_store = {}  # chunk_id â†’ chunk_text
        self.parent_store = {}  # parent_id â†’ full_document
        self.chunk_to_parent = {}  # chunk_id â†’ parent_id
        self.vector_store = None
    
    def add_document(
        self, 
        document: str,
        doc_id: str,
        chunk_size: int = 256
    ):
        """
        ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  ì¸ë±ì‹±
        """
        # ë¶€ëª¨ ë¬¸ì„œ ì €ì¥
        self.parent_store[doc_id] = document
        
        # ì²­í¬ ìƒì„± (ì‘ì€ í¬ê¸°)
        chunks = self.chunk_text(document, chunk_size)
        
        # ê° ì²­í¬ ì¸ë±ì‹±
        for i, chunk in enumerate(chunks):
            chunk_id = f"{doc_id}_chunk_{i}"
            
            # ì²­í¬ ì €ì¥
            self.chunk_store[chunk_id] = chunk
            
            # ì²­í¬ â†’ ë¶€ëª¨ ë§¤í•‘
            self.chunk_to_parent[chunk_id] = doc_id
            
            # ë²¡í„° DBì— ì²­í¬ ì¸ë±ì‹±
            self._add_to_vector_db(chunk, chunk_id)
    
    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        ê²€ìƒ‰: ì²­í¬ë¡œ ì°¾ì§€ë§Œ ë¶€ëª¨ ë¬¸ì„œ ë°˜í™˜
        """
        # Step 1: ì²­í¬ ë ˆë²¨ì—ì„œ ê²€ìƒ‰
        chunk_results = self._search_chunks(query, top_k=top_k)
        
        # Step 2: ë¶€ëª¨ ë¬¸ì„œ ID ì¶”ì¶œ
        parent_ids = set()
        for chunk_result in chunk_results:
            chunk_id = chunk_result['chunk_id']
            parent_id = self.chunk_to_parent.get(chunk_id)
            if parent_id:
                parent_ids.add(parent_id)
        
        # Step 3: ë¶€ëª¨ ë¬¸ì„œ ë°˜í™˜
        parent_docs = []
        for parent_id in parent_ids:
            parent_docs.append({
                'id': parent_id,
                'text': self.parent_store[parent_id]
            })
        
        return parent_docs
```

**ì¥ì **:
- âœ… ê²€ìƒ‰ ì •í™•ë„ ë†’ìŒ (ì‘ì€ ì²­í¬)
- âœ… ì»¨í…ìŠ¤íŠ¸ í’ë¶€ (ì „ì²´ ë¬¸ì„œ)

**ë‹¨ì **:
- âŒ ì „ì²´ ë¬¸ì„œê°€ ë„ˆë¬´ í¬ë©´ ì—¬ì „íˆ ë¬¸ì œ
- âŒ ê´€ë ¨ ì—†ëŠ” ë¶€ë¶„ë„ í¬í•¨ë  ìˆ˜ ìˆìŒ

#### Summary Index

ë¬¸ì„œë¥¼ ê³„ì¸µì ìœ¼ë¡œ ìš”ì•½í•˜ê³ , ìš”ì•½ë³¸ìœ¼ë¡œ ê²€ìƒ‰í•œë‹¤.

```python
class SummaryIndexRAG:
    """
    ë¬¸ì„œ ìš”ì•½ ê¸°ë°˜ RAG
    """
    
    def __init__(self):
        self.summaries = {}  # doc_id â†’ summary
        self.documents = {}  # doc_id â†’ full_document
        self.vector_store = None
    
    def add_document(self, document: str, doc_id: str):
        """
        ë¬¸ì„œì™€ ìš”ì•½ë³¸ ëª¨ë‘ ì¸ë±ì‹±
        """
        # ì›ë³¸ ì €ì¥
        self.documents[doc_id] = document
        
        # ìš”ì•½ ìƒì„±
        summary = self._generate_summary(document)
        self.summaries[doc_id] = summary
        
        # ìš”ì•½ë³¸ì„ ë²¡í„° DBì— ì¸ë±ì‹±
        self._index_summary(summary, doc_id)
    
    def _generate_summary(self, document: str, max_length: int = 500) -> str:
        """
        ë¬¸ì„œ ìš”ì•½ ìƒì„±
        """
        # ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ì²­í¬ ë‹¨ìœ„ë¡œ ìš”ì•½ í›„ ì¬ìš”ì•½
        if len(document) > 10000:
            return self._hierarchical_summary(document)
        
        # ë‹¨ì¼ ìš”ì•½
        prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ {max_length}ì ì´ë‚´ë¡œ ìš”ì•½í•˜ì„¸ìš”.
í•µì‹¬ ë‚´ìš©ê³¼ ì£¼ìš” ì£¼ì œë¥¼ í¬í•¨í•˜ì„¸ìš”.

ë¬¸ì„œ:
{document}

ìš”ì•½:"""
        
        client = anthropic.Anthropic(api_key="your-api-key")
        
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=max_length,
            temperature=0,
            messages=[{"role": "user", "content": prompt}]
        )
        
        return message.content[0].text
    
    def _hierarchical_summary(self, document: str) -> str:
        """
        ê³„ì¸µì  ìš”ì•½ (Map-Reduce íŒ¨í„´)
        
        1. ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ”
        2. ê° ì²­í¬ ìš”ì•½ (Map)
        3. ìš”ì•½ë“¤ì„ ê²°í•©í•˜ì—¬ ìµœì¢… ìš”ì•½ (Reduce)
        """
        # Step 1: ì²­í¬ ìƒì„±
        chunks = self.chunk_text(document, chunk_size=4000)
        
        # Step 2: ê° ì²­í¬ ìš”ì•½ (Map)
        chunk_summaries = []
        for chunk in chunks:
            summary = self._generate_summary(chunk, max_length=300)
            chunk_summaries.append(summary)
        
        # Step 3: ìš”ì•½ë“¤ ê²°í•© (Reduce)
        combined = "\n\n".join(chunk_summaries)
        final_summary = self._generate_summary(combined, max_length=500)
        
        return final_summary
    
    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        2ë‹¨ê³„ ê²€ìƒ‰:
        1. ìš”ì•½ë³¸ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        2. ì›ë³¸ ë¬¸ì„œ ë°˜í™˜
        """
        # Step 1: ìš”ì•½ë³¸ìœ¼ë¡œ ê²€ìƒ‰
        summary_results = self._search_summaries(query, top_k=top_k)
        
        # Step 2: ì›ë³¸ ë¬¸ì„œ ë°˜í™˜
        full_docs = []
        for result in summary_results:
            doc_id = result['doc_id']
            full_docs.append({
                'id': doc_id,
                'text': self.documents[doc_id],
                'summary': self.summaries[doc_id]
            })
        
        return full_docs
```

## ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì²˜ë¦¬

### ë¬¸ì œ: ì§€ì‹ë² ì´ìŠ¤ê°€ ìì£¼ ë³€ê²½ë¨

**ì‹œë‚˜ë¦¬ì˜¤**: ë‰´ìŠ¤ ê¸°ì‚¬, ì œí’ˆ ì •ë³´, ê°€ê²© ë“±ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸

#### í•´ê²°ì±…: Incremental Indexing

```python
class IncrementalRAG:
    """
    ì¦ë¶„ ì¸ë±ì‹±ì„ ì§€ì›í•˜ëŠ” RAG
    """
    
    def __init__(self):
        self.vector_store = None
        self.document_index = {}  # doc_id â†’ metadata
    
    def add_document(self, document: Dict):
        """
        ìƒˆ ë¬¸ì„œ ì¶”ê°€
        """
        doc_id = document['id']
        
        # ì¤‘ë³µ ì²´í¬
        if doc_id in self.document_index:
            print(f"âš ï¸  ë¬¸ì„œ {doc_id} ì´ë¯¸ ì¡´ì¬ - ì—…ë°ì´íŠ¸ í•„ìš”")
            return self.update_document(document)
        
        # ì¸ë±ì‹±
        self._index_document(document)
        self.document_index[doc_id] = {
            'timestamp': datetime.now(),
            'version': 1
        }
        
        print(f"âœ… ë¬¸ì„œ {doc_id} ì¶”ê°€ ì™„ë£Œ")
    
    def update_document(self, document: Dict):
        """
        ê¸°ì¡´ ë¬¸ì„œ ì—…ë°ì´íŠ¸
        """
        doc_id = document['id']
        
        # Step 1: ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
        self.delete_document(doc_id)
        
        # Step 2: ìƒˆ ë²„ì „ ì¶”ê°€
        self._index_document(document)
        self.document_index[doc_id] = {
            'timestamp': datetime.now(),
            'version': self.document_index[doc_id].get('version', 0) + 1
        }
        
        print(f"ğŸ”„ ë¬¸ì„œ {doc_id} ì—…ë°ì´íŠ¸ ì™„ë£Œ (v{self.document_index[doc_id]['version']})")
    
    def delete_document(self, doc_id: str):
        """
        ë¬¸ì„œ ì‚­ì œ
        """
        # Vector DBì—ì„œ ì œê±°
        self.vector_store.delete(ids=[doc_id])
        
        # ì¸ë±ìŠ¤ì—ì„œ ì œê±°
        if doc_id in self.document_index:
            del self.document_index[doc_id]
        
        print(f"ğŸ—‘ï¸  ë¬¸ì„œ {doc_id} ì‚­ì œ ì™„ë£Œ")
    
    def get_document_info(self, doc_id: str) -> Dict:
        """
        ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        """
        return self.document_index.get(doc_id)
```

**ì‚¬ìš© ì˜ˆì‹œ**:

```python
rag = IncrementalRAG()

# ì´ˆê¸° ë¬¸ì„œ ì¶”ê°€
rag.add_document({
    'id': 'product_123',
    'text': 'ì œí’ˆ ê°€ê²©: $99',
    'metadata': {'category': 'electronics'}
})

# ë‚˜ì¤‘ì— ê°€ê²© ë³€ê²½
rag.update_document({
    'id': 'product_123',
    'text': 'ì œí’ˆ ê°€ê²©: $79 (í• ì¸)',
    'metadata': {'category': 'electronics'}
})

# ë¬¸ì„œ ì •ë³´ í™•ì¸
info = rag.get_document_info('product_123')
print(f"ë²„ì „: {info['version']}, ì—…ë°ì´íŠ¸: {info['timestamp']}")
```
