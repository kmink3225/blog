---
title: "Few-Shot Prompting"
subtitle: 적은 예시로 큰 효과를 만드는 In-Context Learning의 원리와 실전
description: |
  Few-Shot Prompting의 정의부터 실전 활용까지 체계적으로 설명한다.
  GPT-3 논문(Brown et al. 2020)의 핵심 발견을 통해 모델 크기와 예시 개수의 상관관계,
  In-Context Learning의 메커니즘을 분석한다.
  "What Makes In-Context Learning Work?"(Min et al. 2022) 연구를 바탕으로
  예시의 품질(정답 라벨 vs 랜덤 라벨), 입력-라벨 형식의 중요성,
  4가지 핵심 요소(Format, Label Space, Input Distribution, Mapping)를 상세히 다룬다.
  감정 분류, 새로운 단어 학습, 일기 작성 등 실무 예시와 
  복잡한 추론 과제에서의 한계를 통해 Few-Shot의 적절한 활용 시나리오를 제시한다.
categories:
  - Prompt Engineering
  - LLM
  - AI
  - Agent
author: Kwangmin Kim
date: 01/27/2025
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

# Part 3: Few-Shot Prompting - 적은 예시로 큰 효과를 만드는 법

## 들어가며

이전 글에서 Zero-Shot Prompting의 놀라운 능력을 확인했습니다. 하지만 복잡한 문제를 풀 때는 한계가 있었죠. 예를 들어, 홀수의 합이 짝수인지 판단하는 문제에서 Zero-Shot은 실패했습니다.

```
Q: 이 그룹의 홀수를 더하면 짝수가 된다. 15, 32, 5, 13, 82, 7, 1.
A: 네, 이 그룹의 홀수들을 더하면 107이 되는데, 이는 짝수입니다.

❌ 틀렸습니다! (107은 홀수)
```

이럴 때 우리는 어떻게 해야 할까요? 바로 **예시를 보여주는 것**입니다. 이것이 Few-Shot Prompting입니다.

## Few-Shot Prompting이란?

### 정의

Few-Shot Prompting은 **언어 모델에게 소수의 예제나 시연(Demonstrations)을 제공하여 작업을 수행하도록 하는 방법**입니다.

```
┌─────────────────────────────────────┐
│  Zero-Shot Prompting                │
├─────────────────────────────────────┤
│  작업 설명만 → 모델이 수행          │
│  예시: 0개                          │
└─────────────────────────────────────┘

┌─────────────────────────────────────┐
│  Few-Shot Prompting                 │
├─────────────────────────────────────┤
│  작업 설명 + 예시 → 모델이 학습     │
│  예시: 2~10개 (보통 2-5개)         │
└─────────────────────────────────────┘
```

### 핵심 아이디어

"보여주는 것이 말하는 것보다 낫다"는 원리입니다.

```
설명만 하기 (Zero-Shot):
"한국어를 영어로 번역해주세요."

예시 보여주기 (Few-Shot):
"한국어를 영어로 번역해주세요.
한국어: 사과 → 영어: apple
한국어: 바나나 → 영어: banana
한국어: 포도 → 영어:"
```

## GPT-3 논문의 발견

### Brown et al. (2020): "Language Models are Few-Shot Learners"

OpenAI의 GPT-3 논문은 Few-Shot Learning의 효과를 체계적으로 입증했습니다.

![Few-shot 다이어그램](이미지 생략)

```
┌──────────────────────────────────────────────┐
│ Few-shot 예시 (번역 작업)                     │
├──────────────────────────────────────────────┤
│ Translate English to French:                  │
│                                               │
│ sea otter => loutre de mer        (예시 1)   │
│ peppermint => menthe poivrée      (예시 2)   │
│ plush giraffe => girafe peluche   (예시 3)   │
│                                               │
│ cheese =>                         (실제 문제) │
│                                               │
│ [모델 출력: fromage] ✅                       │
└──────────────────────────────────────────────┘
```

### 모델 크기에 따른 In-Context Learning 효과

연구진은 모델 크기가 클수록 Few-Shot의 효과가 극대화됨을 발견했습니다.

![모델 크기별 성능 그래프](이미지 생략)

```
예시 개수(K)에 따른 정확도 변화:

GPT-3 175B (가장 큰 모델):
K=0 (Zero-shot):     ~50%
K=1 (One-shot):      ~55%
K=10 (Few-shot):     ~60%
K=64 (Few-shot):     ~65%

GPT-3 13B (중간 모델):
K=0:  ~20%
K=64: ~25%

GPT-3 1.3B (작은 모델):
K=0:  ~5%
K=64: ~5%
```

**핵심 발견:**
- **모델이 클수록 Few-Shot 효과가 큽니다**
- 약 10B 파라미터 이상에서 효과가 나타나기 시작합니다
- 175B 모델에서는 예시가 많을수록 성능이 계속 향상됩니다
- 작은 모델(<1B)에서는 Few-Shot이 거의 효과가 없습니다

### 왜 모델이 커야 할까?

```
작은 모델 (1.3B):
- 패턴 인식 능력 제한적
- 예시로부터 학습 능력 부족
- 단순 암기에 가까움

큰 모델 (175B):
- 복잡한 패턴 인식
- 예시로부터 일반화 가능
- 맥락 이해 능력 탁월
```

## Few-Shot Prompting의 실제 적용

### 예시 1: 새로운 단어 사용법 학습

GPT-3 논문에서 사용한 유명한 예시입니다.

**프롬프트:**
```
A "whatpu" is a small, furry animal native to Tanzania. 
An example of a sentence that uses the word whatpu is:
We were traveling in Africa and we saw these very cute whatpus.

To do a "farduddle" means to jump up and down really fast. 
An example of a sentence that uses the word farduddle is:
```

**모델 출력:**
```
When we won the game, we all started to farduddle in celebration.
```

✅ **성공!** 모델은 새로운 단어의 의미를 이해하고 적절하게 사용했습니다.

**분석:**
- "whatpu"와 "farduddle"은 실제로 존재하지 않는 단어입니다
- 모델은 정의와 예시 패턴을 학습했습니다
- 비슷한 형식으로 새로운 문장을 생성했습니다

### 예시 2: 감정 분류

**프롬프트:**
```
다음 텍스트의 감정을 분류하세요.

텍스트: 이 영화 정말 재미있었어요!
감정: 긍정

텍스트: 너무 지루하고 실망스러웠어요.
감정: 부정

텍스트: 그냥 평범한 영화였어요.
감정: 중립

텍스트: 최고의 영화였습니다!
감정:
```

**모델 출력:**
```
긍정
```

**분석:**
- 3개의 예시를 통해 패턴을 학습했습니다
- 입력-출력 형식을 일관되게 유지했습니다
- 감정 분류 기준을 이해했습니다

### 예시 3: 복잡한 산술 문제 (아직 실패)

**프롬프트:**
```
이 그룹의 홀수들을 더하면 짝수가 됩니다: 4, 8, 9, 15, 12, 2, 1.
답: 거짓입니다.

이 그룹의 홀수들을 더하면 짝수가 됩니다: 17, 10, 19, 4, 8, 12, 24.
답: 참입니다.

이 그룹의 홀수들을 더하면 짝수가 됩니다: 16, 11, 14, 4, 8, 13, 24.
답: 참입니다.

이 그룹의 홀수들을 더하면 짝수가 됩니다: 17, 9, 10, 12, 13, 4, 2.
답: 거짓입니다.

이 그룹의 홀수들을 더하면 짝수가 됩니다: 15, 32, 5, 13, 82, 7, 1.
답:
```

**모델 출력:**
```
정답입니다.
```

❌ **실패!** (15+5+13+7+1=41, 홀수이므로 거짓이어야 함)

**문제점:**
- 단순히 답만 제시하는 예시로는 부족합니다
- 중간 추론 과정이 없어 모델이 어떻게 답을 도출해야 하는지 모릅니다
- 이 문제는 Chain-of-Thought (Part 4)에서 해결됩니다

## 예시의 품질이 중요하다

### 연구: "Rethinking the Role of Demonstrations" (2022)

"What Makes In-Context Learning Work?" 연구는 Few-Shot의 효과를 분석했습니다.

**실험 설계:**
```
1. Gold labels (정답 라벨):
   입력 → 올바른 출력

2. Random labels (랜덤 라벨):
   입력 → 무작위 출력

3. No input-label format (형식 없음):
   형식을 유지하지 않음
```

### 실험 결과 1: 분류 작업

![분류 작업 결과](이미지 생략)

```
분류 작업 정확도 비교:

모델: GPT-3, MetaICL, GPT-J

1. No demonstrations (예시 없음):
   - Direct GPT-2: ~30%
   - Channel GPT-2: ~35%

2. Gold labels (정답 라벨):
   - Direct GPT-2: ~52%
   - Channel GPT-2: ~55%

3. Random labels (랜덤 라벨):
   - Direct GPT-2: ~50%
   - Channel GPT-2: ~52%

4. No input-label format (형식 없음):
   - 성능 크게 저하 (~30%)
```

**핵심 발견:**

**1. 예시를 사용하면 성능이 향상됩니다**
```
No demonstrations: ~30-35%
Demonstrations:     ~50-55%
→ 약 20%p 향상!
```

**2. 정답 라벨이 가장 좋지만, 랜덤도 효과가 있습니다**
```
Gold labels:   ~55%
Random labels: ~52%
→ 차이가 크지 않음!
```

**3. 형식을 유지하는 것이 가장 중요합니다**
```
With format:    ~50-55%
Without format: ~30%
→ 형식이 핵심!
```

### 실험 결과 2: 멀티초이스 작업

![멀티초이스 작업 결과](이미지 생략)

```
형식 변형 실험:

Format variations (형식 변형):
1. Gold labels: 55%
2. Random labels: 50%
3. Only inputs (입력만): 45%
4. Only labels (라벨만): 35%
5. Random English words: 30%
6. No labels: 30%
7. No demonstrations: 28%
```

**결론:**
- **입력-라벨 형식을 유지하는 것이 가장 중요합니다**
- 라벨이 정확하지 않아도 형식만 맞으면 어느 정도 효과가 있습니다
- 하지만 최고 성능을 위해서는 정답 라벨이 필요합니다

## 실습: Few-Shot Prompting 적용하기

### 실습 1: 일기 작성

**프롬프트:**
```
"""오늘의 일기를 작성해줘.
아침, 점심, 저녁, 취침 전 시간에 해당하는 문장을 써줘.
아래 예시를 참고해서 네 줄만 써줘"""

예시:
1. 아침: "나는 오늘 날씨가 좋아서 산책을 했다. 그리고 친구를 만났다."
```

**OpenAI Playground 설정:**
```
Model: gpt-3.5-turbo
Temperature: 0.1
Maximum tokens: 400
```

**기대 출력:**
```
1. 아침: "잠을 깨자마자 창문을 열었다. 상쾌한 공기가 방 안으로 들어왔다."
2. 점심: "가족들과 집에서 만든 파스타를 먹었다. 맛있게 먹으며 대화를 나눴다."
3. 저녁: "책을 읽으며 하루를 돌아봤다. 평화로운 시간이었다."
4. 취침 전: "내일도 좋은 하루가 되길 바라며 잠자리에 들었다."
```

### 실습 2: 형식 일관성 테스트

**Random Format (형식 없음):**
```
부정 이건 정말 굉장해!
와 정말 나쁘다! 긍정
그 영화 진짜 대박이더라. 긍정
아우 정말 끔찍해~~
```

**모델 출력:** (불안정)

**Proper Format (올바른 형식):**
```
이건 정말 굉장해! // 부정
와 정말 나쁘다! // 긍정
그 영화 진짜 대박이더라. // 긍정
아우 정말 끔찍해 //
```

**모델 출력:**
```
부정
```

✅ **형식을 유지하면 더 안정적인 결과를 얻습니다!**

### 실습 3: 홀수 합 문제 (여전히 실패)

**프롬프트:**
```
이 그룹의 홀수들을 더하면 짝수가 됩니다: 4, 8, 9, 15, 12, 2, 1.
답: 거짓입니다.

이 그룹의 홀수들을 더하면 짝수가 됩니다: 17, 10, 19, 4, 8, 12, 24.
답: 참입니다.

이 그룹의 홀수들을 더하면 짝수가 됩니다: 16, 11, 14, 4, 8, 13, 24.
답: 참입니다.

이 그룹의 홀수들을 더하면 짝수가 됩니다: 17, 9, 10, 12, 13, 4, 2.
답: 거짓입니다.

이 그룹의 홀수들을 더하면 짝수가 됩니다: 15, 32, 5, 13, 82, 7, 1.
답:
```

**실제로 시도해보세요!**

**결과:**
```
정답입니다. (X)
```

여전히 실패합니다. 왜일까요?

**문제 분석:**
- 모델이 **어떻게** 답을 도출해야 하는지 모릅니다
- 중간 추론 단계가 없습니다
- 단순히 패턴만 매칭하려고 시도합니다

→ 이 문제는 **Chain-of-Thought** (다음 글)에서 해결됩니다!

## Few-Shot Prompting 작성 가이드

### 1. 적절한 예시 개수 선택

```
너무 적음 (1개):
- 패턴 학습 부족
- 일반화 어려움

적당함 (2-5개):
- 패턴 학습 충분
- 비용 효율적
- 대부분의 경우 최적

너무 많음 (10개 이상):
- 추가 효과 미미
- 토큰 비용 증가
- 컨텍스트 윈도우 낭비
```

**권장:** 대부분의 경우 **3-5개**가 최적입니다.

### 2. 다양한 예시 포함

```
❌ 나쁜 예시 (모두 비슷함):
영화가 좋았어요 → 긍정
영화가 훌륭했어요 → 긍정
영화가 최고였어요 → 긍정

✅ 좋은 예시 (다양한 케이스):
영화가 좋았어요 → 긍정
너무 지루했어요 → 부정
그냥 평범했어요 → 중립
```

### 3. 일관된 형식 유지

```
❌ 나쁜 예시 (형식 불일치):
텍스트: 좋아요 감정: 긍정
매우 나빴어요 / 부정
중립입니다 <- 괜찮네요

✅ 좋은 예시 (형식 일관):
텍스트: 좋아요 | 감정: 긍정
텍스트: 나빴어요 | 감정: 부정
텍스트: 괜찮아요 | 감정: 중립
```

### 4. 명확한 구분자 사용

```
좋은 구분자:
- "=>" (화살표)
- "|" (파이프)
- ":" (콜론)
- "→" (화살표)
- 빈 줄

피해야 할 것:
- 애매한 구두점
- 일관성 없는 구분
```

### 5. 예시 순서 고려

```python
# 무작위 순서보다 논리적 순서가 좋을 수 있음

# 무작위:
"긍정 예시, 부정 예시, 긍정 예시, 중립 예시"

# 논리적:
"긍정 예시, 중립 예시, 부정 예시"
```

일부 연구에서는 예시 순서가 결과에 영향을 줄 수 있다고 보고했습니다.

## Few-Shot의 장점과 한계

### 장점

**1. Zero-Shot보다 높은 성능**
```
Zero-Shot: 50-60%
Few-Shot:  70-80%
→ 약 15-20%p 향상
```

**2. 특정 형식이나 스타일 학습 가능**
- 원하는 출력 형식 지정
- 특정 어조나 스타일 유지
- 도메인 특화 작업

**3. Fine-tuning 불필요**
- 모델 재학습 없음
- 즉시 적용 가능
- 다양한 작업에 유연하게 사용

**4. 이해하기 쉬움**
- 직관적인 방법
- "예시로 보여주기"
- 디버깅이 용이

### 한계

**1. 복잡한 추론 문제에 약함**
```
❌ 다단계 계산
❌ 논리적 추론
❌ 인과관계 파악
→ Chain-of-Thought 필요
```

**2. 토큰 비용 증가**
```
Zero-Shot: 50 tokens
Few-Shot:  200 tokens (예시 3-4개)
→ 4배 비용 증가
```

**3. 예시 작성 부담**
- 고품질 예시 작성 시간 필요
- 도메인 전문가 필요할 수 있음
- 예시 관리 필요

**4. 컨텍스트 윈도우 소비**
```
GPT-3.5: 4,096 tokens
예시가 많으면 실제 작업에 사용할 공간 감소
```

**5. 모델 크기 의존성**
- 작은 모델(<10B)에서는 효과 미미
- 대형 모델(>100B)에서만 효과적

## 언제 Few-Shot을 사용해야 할까?

### Few-Shot이 적합한 경우

✅ **Zero-Shot으로 충분하지 않을 때**
```
시도 순서:
1. Zero-Shot 먼저 시도
2. 결과가 불만족스러우면
3. Few-Shot 적용
```

✅ **특정 형식이 필요할 때**
- JSON 구조
- 특정 템플릿
- 일관된 스타일

✅ **도메인 특화 작업**
- 전문 용어 사용
- 특수한 규칙이나 컨벤션
- 업계별 표준

✅ **중간 난이도 작업**
- Zero-Shot보다 복잡
- Chain-of-Thought만큼 복잡하지 않음

### Zero-Shot이 더 나은 경우

✅ **간단하고 명확한 작업**
- 일반적인 번역
- 기본적인 분류
- 간단한 질문 답변

✅ **토큰 비용이 중요할 때**
- 대량 처리
- 비용 최적화 필요

✅ **빠른 프로토타이핑**
- 초기 테스트
- 개념 검증

### Chain-of-Thought가 필요한 경우

✅ **복잡한 추론 필요**
- 다단계 계산
- 논리적 추론
- 문제 분해

→ 다음 글에서 다룹니다!

## 실전 팁: Few-Shot 최적화하기

### 1. 점진적 접근

```python
# 단계별 최적화

# Step 1: Zero-Shot으로 시작
prompt = "텍스트를 분류하세요: [입력]"

# Step 2: 1-2개 예시 추가
prompt = """
예시: 좋아요 → 긍정
텍스트: [입력]
"""

# Step 3: 필요시 예시 추가 (3-5개)
# Step 4: 형식 최적화
# Step 5: 예시 다양성 개선
```

### 2. A/B 테스트

```python
# 예시 구성이 다른 두 버전을 테스트

Version A: 3개 예시, 간단한 형식
Version B: 5개 예시, 상세한 형식

→ 성능과 비용을 비교하여 선택
```

### 3. 예시 라이브러리 구축

```python
# 자주 사용하는 작업별로 예시 저장

examples_library = {
    "sentiment": [
        "좋아요 → 긍정",
        "나빠요 → 부정",
        "괜찮아요 → 중립"
    ],
    "translation": [
        "사과 → apple",
        "바나나 → banana"
    ],
    # ... 더 많은 작업
}
```

### 4. 동적 예시 선택

```python
# 입력과 가장 유사한 예시를 자동으로 선택

def select_examples(input_text, example_pool, k=3):
    # 유사도 계산 (예: 임베딩 거리)
    similarities = compute_similarities(input_text, example_pool)
    # 상위 k개 선택
    return top_k_examples(similarities, k)
```

## 비교 요약

| 특성 | Zero-Shot | Few-Shot | Chain-of-Thought (예고) |
|------|-----------|----------|------------------------|
| 예시 필요 | ❌ 0개 | ✅ 2-10개 | ✅ 2-10개 (+ 추론 과정) |
| 설정 시간 | 빠름 | 중간 | 느림 |
| 토큰 비용 | 낮음 | 중간 | 높음 |
| 간단한 작업 | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ |
| 중간 작업 | ⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 복잡한 추론 | ⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ |
| 형식 일관성 | ⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |

## 마무리

Few-Shot Prompting은 Zero-Shot의 성능을 크게 향상시키는 강력한 기법입니다. 핵심은 **좋은 예시를 제공하고, 일관된 형식을 유지하는 것**입니다.

**핵심 요약:**
- Few-Shot은 소수의 예시로 모델 성능을 향상시킵니다
- 모델이 클수록 (>100B) Few-Shot 효과가 큽니다
- 정답 라벨이 가장 좋지만, 형식 유지가 더 중요합니다
- 보통 3-5개 예시가 최적입니다
- Zero-Shot보다 15-20%p 성능 향상을 기대할 수 있습니다

하지만 Few-Shot에도 한계가 있습니다. 우리가 시도했던 홀수 합 문제처럼, **복잡한 추론이 필요한 문제**에서는 여전히 실패합니다. 단순히 답만 제시하는 것으로는 부족합니다.

다음 글에서는 **Chain-of-Thought (CoT) Prompting**을 통해 이 문제를 해결하는 방법을 알아보겠습니다. 모델에게 **중간 추론 과정**을 명시적으로 보여주면 어떻게 될까요? 놀라운 성능 향상을 확인하게 될 것입니다!

---

**다음 글 예고:** Part 4 - Chain-of-Thought Prompting: 단계별 추론으로 복잡한 문제 해결하기

## 참고문헌

- Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. *Advances in Neural Information Processing Systems, 33*, 1877-1901.
- Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., & Zettlemoyer, L. (2022). Rethinking the role of demonstrations: What makes in-context learning work?. *arXiv preprint arXiv:2202.12837*.