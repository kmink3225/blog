---
title: "Zero-Shot Prompting 완벽 가이드"
subtitle: 예시 없이 작업을 수행하는 대규모 언어모델의 원리와 발전 과정
description: |
  Zero-Shot Prompting의 정의부터 실제 동작 원리까지 체계적으로 설명한다.
  GPT-3 논문(Brown et al. 2020)의 핵심 발견, 모델 크기와 성능의 상관관계,
  대규모 사전 학습을 통한 암묵적 학습 메커니즘을 분석한다.
  초기 Zero-Shot의 한계를 극복한 Instruction Tuning(FLAN 2022)과
  RLHF(ChatGPT)의 등장 배경과 효과를 실전 예시와 함께 설명한다.
  텍스트 분류, 번역, 질문 답변 등 실무 활용 사례를 통해 
  Zero-Shot Prompting의 강점과 적절한 사용 시나리오를 제시한다.
categories:
  - Prompt Engineering
  - LLM
  - AI
  - Agent  
author: Kwangmin Kim
date: 01/26/2025
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

# Part 2: Zero-Shot Prompting - 예시 없이 작업을 수행하는 모델의 비밀

## 들어가며

"GPT-3에게 예시를 주지 않고 번역을 시켰는데 잘 해냈어요!" 

2020년, 이것은 놀라운 발견이었습니다. 전통적인 머신러닝에서는 모델에게 작업을 시키려면 반드시 그 작업에 대한 학습 데이터가 필요했습니다. 하지만 대형 언어 모델은 달랐습니다. **예시 없이도** 번역, 요약, 분류 등 다양한 작업을 수행할 수 있었죠.

이것이 바로 **Zero-Shot Prompting**입니다. 이번 글에서는 이 놀라운 능력이 어떻게 가능해졌는지, 그리고 어떻게 활용할 수 있는지 알아보겠습니다.

## Zero-Shot Prompting이란?

### 정의

Zero-Shot Prompting은 **언어 모델에게 예제나 시연(Demonstrations) 없이 작업을 수행하도록 하는 방법**입니다.

```
┌─────────────────────────────────────┐
│  Traditional ML (전통적 머신러닝)     │
├─────────────────────────────────────┤
│  학습 데이터 → 모델 훈련 → 예측      │
│  (수백~수천 개의 예시 필요)          │
└─────────────────────────────────────┘

┌─────────────────────────────────────┐
│  Zero-Shot Prompting                │
├─────────────────────────────────────┤
│  작업 설명만 → 모델이 바로 수행      │
│  (예시 0개)                          │
└─────────────────────────────────────┘
```

### 핵심 특징

1. **예시를 제공하지 않음**
   - 작업에 대한 자연어 설명만 제공
   - "이것을 번역해줘", "이것을 분류해줘" 같은 명령어

2. **모델의 기존 지식 활용**
   - 사전 학습(Pre-training) 과정에서 학습한 패턴 사용
   - 대량의 데이터로 학습했기 때문에 가능

3. **즉시 사용 가능**
   - 추가 학습이나 fine-tuning 불필요
   - 프롬프트만 작성하면 바로 실행

## 실제 예시로 이해하기

### 예시 1: 텍스트 분류

**프롬프트:**
```
다음 텍스트에서 긍정, 부정, 중립 중 하나로 분류해.

텍스트: 나는 마라탕 맛이 그저 그랬어.
Sentiment:
```

**모델 출력:**
```
부정
```

**분석:**
- 예시를 전혀 주지 않았습니다
- 단지 "긍정, 부정, 중립 중 하나로 분류해"라는 지시만 했습니다
- 모델은 "그저 그랬어"라는 표현이 부정적임을 이해했습니다

### 예시 2: 번역

**프롬프트:**
```
다음 한국어 단어를 영어로 번역해줘.

단어: 인공 눈물
```

**모델 출력:**
```
Artificial tears
```

**분석:**
- 번역 예시를 제공하지 않았습니다
- 전문 용어(의학 용어)도 올바르게 번역했습니다
- 모델의 사전 학습 지식을 활용한 것입니다

### 예시 3: 질문 답변

**프롬프트:**
```
대한민국의 수도는 서울이야?
```

**모델 출력:**
```
네, 맞습니다. 대한민국의 수도는 서울입니다.
```

**분석:**
- 질문-답변 형식의 예시가 없었습니다
- 사실 확인 작업을 즉시 수행했습니다
- 간단한 사실 질문에 효과적입니다

## GPT-3의 등장과 Zero-Shot의 발견

### 연구 배경: Brown et al. (2020)

OpenAI의 획기적인 논문 "Language Models are Few-Shot Learners"(2020)에서 GPT-3를 소개하며 Zero-Shot 능력을 체계적으로 분석했습니다.

**주요 발견:**

![Zero-shot, One-shot, Few-shot 비교](이미지 생략)

```
┌──────────────────────────────────────────────┐
│ Zero-shot                                     │
├──────────────────────────────────────────────┤
│ Translate English to French:                  │
│ cheese =>                                     │
│                                               │
│ [모델 출력: fromage]                          │
└──────────────────────────────────────────────┘

┌──────────────────────────────────────────────┐
│ One-shot                                      │
├──────────────────────────────────────────────┤
│ Translate English to French:                  │
│ sea otter => loutre de mer                    │
│ cheese =>                                     │
│                                               │
│ [모델 출력: fromage]                          │
└──────────────────────────────────────────────┘

┌──────────────────────────────────────────────┐
│ Few-shot                                      │
├──────────────────────────────────────────────┤
│ Translate English to French:                  │
│ sea otter => loutre de mer                    │
│ peppermint => menthe poivrée                  │
│ plush girafe => girafe peluche                │
│ cheese =>                                     │
│                                               │
│ [모델 출력: fromage]                          │
└──────────────────────────────────────────────┘
```

### 모델 크기의 중요성

연구진은 모델 크기에 따른 Zero-shot 성능을 측정했습니다:

```
모델 크기별 Zero-shot 정확도:
- 125M (1억 2500만): ~10%
- 1.3B (13억): ~20%
- 6.7B (67억): ~35%
- 13B (130억): ~45%
- 175B (1750억): ~60%
```

**핵심 인사이트:**
- 모델이 클수록 Zero-shot 성능이 향상됩니다
- 약 10억 파라미터 이상에서 실용적인 성능을 보입니다
- GPT-3 175B는 많은 작업에서 Few-shot에 근접하는 Zero-shot 성능을 보였습니다

## Zero-Shot은 어떻게 가능한가?

대형 언어 모델이 예시 없이도 작업을 수행할 수 있는 이유는 무엇일까요?

### 1. 대규모 사전 학습 (Pre-training)

```
학습 데이터:
- 웹 페이지: 수천억 개의 단어
- 책: 수백만 권
- 위키피디아: 전체 내용
- 대화 데이터: 다양한 형식
- 코드: GitHub 등

→ 모델은 이 과정에서 암묵적으로 다양한 작업을 학습
```

**예시:**
- 번역 작업: 웹에서 영한 대역 텍스트를 자주 봤음
- 요약 작업: 뉴스 기사와 제목의 관계를 학습함
- 분류 작업: "이것은 긍정적이다", "부정적이다" 같은 문장을 봤음

### 2. 패턴 인식과 일반화

모델은 학습 과정에서 다음과 같은 패턴을 습득합니다:

```python
# 학습 데이터에서 본 패턴 예시

"Translate X to Y: [원문] => [번역문]"
"Classify the sentiment: [텍스트] → [감정]"
"Q: [질문] A: [답변]"
"Summarize: [긴 글] Summary: [요약]"
```

새로운 입력이 들어오면:
1. 입력 패턴을 인식합니다
2. 학습한 유사 패턴을 찾습니다
3. 적절한 출력을 생성합니다

### 3. 컨텍스트 학습 능력

Transformer 아키텍처의 attention 메커니즘 덕분에:
- 프롬프트 내의 문맥을 이해합니다
- 작업의 의도를 파악합니다
- 적절한 형식으로 응답합니다

## 하지만 초기 Zero-Shot에는 한계가 있었다

GPT-3의 Zero-shot은 인상적이었지만, 여전히 문제가 있었습니다:

### 문제점 1: 불안정한 성능

```
프롬프트: "이 문장의 감정을 분류해: 정말 좋았어요!"
→ 때로는 "긍정", 때로는 "이것은 긍정적인 문장입니다", 
   때로는 다른 형식으로 답변
```

### 문제점 2: 지시사항 무시

```
프롬프트: "다음을 요약해줘: [긴 텍스트]"
→ 요약 대신 계속해서 텍스트를 생성하거나
   관련 없는 내용을 출력
```

### 문제점 3: 형식 불일치

```
원하는 출력: "긍정" 또는 "부정"
실제 출력: "이 문장은 긍정적인 감정을 담고 있습니다. 
            왜냐하면..."
```

이러한 문제를 해결하기 위해 두 가지 혁신적인 기법이 등장했습니다.

## Instruction Tuning: 지시 따르기 학습

### 연구 배경: FLAN (2021)

Google Research의 Wei et al. (2021)은 "Finetuned Language Models are Zero-Shot Learners" 논문에서 **Instruction Tuning**을 제안했습니다.

### 핵심 아이디어

```
일반 사전 학습:
"The cat sat on the mat" → 다음 단어 예측

Instruction Tuning:
"Translate to French: The cat sat..." 
→ "Le chat..."

"Classify sentiment: I love this!" 
→ "Positive"

"Answer: What is 2+2?" 
→ "4"
```

**차이점:**
- 자연어 지시사항과 함께 학습합니다
- 60개 이상의 NLP 데이터셋을 활용합니다
- 다양한 작업 형식에 노출됩니다

### FLAN의 성능

Google은 137B 파라미터 모델에 Instruction Tuning을 적용한 **FLAN** 모델을 만들었습니다.

**성능 비교 (Zero-shot):**

```
작업별 정확도:

자연어 추론 (Natural Language Inference):
- GPT-3 175B: ~52%
- LaMDA-PT 137B: ~60%
- FLAN 137B: ~70% ⭐

독해 (Reading Comprehension):
- GPT-3 175B: ~65%
- LaMDA-PT 137B: ~72%
- FLAN 137B: ~78% ⭐

폐쇄형 QA (Closed-book QA):
- GPT-3 175B: ~58%
- LaMDA-PT 137B: ~62%
- FLAN 137B: ~71% ⭐

번역 (Translation):
- 대부분의 언어 쌍에서 FLAN이 우수한 성능
```

![FLAN 성능 그래프](이미지 생략)

**주요 발견:**
- Instruction Tuning은 Zero-shot 성능을 크게 향상시킵니다
- 학습하지 않은 작업(unseen tasks)에서도 성능이 향상됩니다
- 다양한 작업 클러스터에서 학습하면 일반화가 더 잘됩니다

### Instruction Tuning의 장점

**1. 사전 학습 데이터 없이도 성능 향상**
- 새로운 작업에 대해 추가 사전 학습 불필요
- 기존 NLP 데이터셋만으로 학습 가능

**2. 다양한 작업 수행 능력**
- 60개 이상의 다양한 작업을 동시에 잘 수행
- 작업 간 지식 전이(transfer)

**3. 모델 크기가 충분할 때 효과적**
- 약 10B 파라미터 이상에서 명확한 효과
- 모델이 클수록 효과가 커짐

## RLHF: 인간 피드백으로 정렬하기

### 연구 배경: Christiano et al. (2017)

OpenAI의 "Deep Reinforcement Learning from Human Preferences" 논문에서 인간 피드백을 활용한 강화학습 방법을 제안했습니다.

### 핵심 아이디어

전통적인 강화학습에서는 **보상 함수(reward function)**를 사람이 직접 설계해야 했습니다. 하지만 이것은 매우 어려운 일입니다.

```
문제: "좋은 답변"을 어떻게 정의할까?
- 정확해야 한다?
- 유용해야 한다?
- 안전해야 한다?
- 친절해야 한다?

→ 이 모든 것을 수식으로 표현하기는 거의 불가능!
```

**RLHF의 해결책:**
인간이 직접 보상 함수를 설계하는 대신, **인간의 선호도를 학습**합니다.

### RLHF 프로세스

```
┌─────────────────────────────────────────┐
│ 1단계: 두 개의 답변 생성                 │
├─────────────────────────────────────────┤
│ 질문: "파이썬으로 피보나치 수열을"      │
│       "구현하는 방법은?"                 │
│                                          │
│ 답변 A: [코드만 제시]                   │
│ 답변 B: [설명 + 코드 + 예시]           │
└─────────────────────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│ 2단계: 인간 평가자가 선호도 선택         │
├─────────────────────────────────────────┤
│ "답변 B가 더 좋습니다"                  │
│ → Reward Predictor 학습                 │
└─────────────────────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│ 3단계: 강화학습으로 모델 개선            │
├─────────────────────────────────────────┤
│ Reward Predictor가 높은 점수를          │
│ 주는 답변을 생성하도록 학습             │
└─────────────────────────────────────────┘
```

![RLHF 다이어그램](이미지 생략)

### ChatGPT의 비밀: InstructGPT

OpenAI는 GPT-3에 Instruction Tuning과 RLHF를 적용하여 **InstructGPT**를 만들었습니다. 이것이 나중에 ChatGPT의 기반이 되었습니다.

**과정:**
1. **Supervised Fine-Tuning**: 고품질 대화 데이터로 학습
2. **Reward Model Training**: 인간 평가자의 선호도 학습
3. **PPO Reinforcement Learning**: 보상 모델을 최대화하는 방향으로 학습

**결과:**
- 지시사항을 훨씬 잘 따름
- 더 도움이 되는 답변 생성
- 유해하거나 편향된 내용 감소
- 일관된 형식의 출력

### RLHF의 효과

```
사용자: "간단한 레시피를 추천해줘"

GPT-3 (RLHF 전):
"레시피라는 것은 요리를 만드는 방법을 설명한 것이다. 
역사적으로 레시피는 구전으로 전해져 왔으며..."
→ 질문에 직접 답하지 않음

InstructGPT (RLHF 후):
"간단한 토마토 파스타 레시피를 추천드립니다:
재료: 파스타 200g, 토마토 소스 1컵, 마늘 2쪽, 올리브유
1. 물을 끓여 파스타를 삶습니다 (8-10분)
2. 팬에 올리브유를 두르고 마늘을 볶습니다..."
→ 정확하고 유용한 답변
```

## 현대의 Zero-Shot: 두 기법의 결합

현대 언어 모델(GPT-4, Claude, Gemini 등)은 **Instruction Tuning + RLHF**를 모두 적용합니다.

### 결합 효과

```
┌──────────────────────┐
│  기본 사전 학습      │
│  (Pre-training)      │
└──────┬───────────────┘
       │
       ↓
┌──────────────────────┐
│  Instruction Tuning  │
│  (작업 이해 향상)    │
└──────┬───────────────┘
       │
       ↓
┌──────────────────────┐
│  RLHF               │
│  (인간 선호 정렬)    │
└──────┬───────────────┘
       │
       ↓
   강력한 Zero-shot!
```

**상승 효과:**
- Instruction Tuning: 다양한 작업 형식 이해
- RLHF: 인간이 선호하는 답변 스타일 학습
- 결과: 예시 없이도 고품질 출력

## 실습: Zero-Shot Prompting 활용하기

이제 실제로 Zero-Shot Prompting을 사용해봅시다.

### 실습 1: 텍스트 분류

**OpenAI Playground 설정:**
```
Model: gpt-3.5-turbo
Temperature: 0.2
Maximum tokens: 256
```

**프롬프트:**
```
다음 텍스트에서 긍정, 부정, 중립 중 하나로 분류해.

텍스트: 나는 마라탕 맛이 그저 그랬어.
Sentiment:
```

**실행해보세요!**

**예상 출력:**
```
부정
```

**프롬프트 변형 실험:**

**버전 1 (명확한 형식 지정):**
```
다음 텍스트의 감정을 분류하세요.
출력 형식: "긍정", "부정", 또는 "중립" 중 하나만 출력

텍스트: 이 영화는 기대 이하였어요.
분류:
```

**버전 2 (JSON 형식):**
```
다음 텍스트의 감정을 JSON 형식으로 분류하세요.

텍스트: 오늘 날씨가 정말 좋네요!

형식:
{
  "sentiment": "긍정/부정/중립",
  "confidence": "높음/중간/낮음"
}
```

### 실습 2: 번역

**프롬프트:**
```
다음 한국어를 영어로 번역해주세요.

한국어: 인공지능은 우리 삶을 변화시키고 있습니다.
영어:
```

**예상 출력:**
```
Artificial intelligence is changing our lives.
```

**도전 과제:**
전문 용어나 은어도 시도해보세요:
- "머신러닝 모델의 과적합 문제"
- "개발자들이 코딩 테스트를 준비한다"

### 실습 3: 질문 답변

**프롬프트:**
```
다음 질문에 간단히 답해주세요.

질문: 파이썬에서 리스트와 튜플의 주요 차이점은?
답변:
```

**예상 출력:**
```
리스트는 수정 가능(mutable)하고 대괄호 []를 사용하며,
튜플은 수정 불가능(immutable)하고 소괄호 ()를 사용합니다.
```

### 실습 4: 데이터 추출

**프롬프트:**
```
다음 텍스트에서 날짜, 장소, 인물을 추출해주세요.

텍스트: 2024년 3월 15일, 서울에서 김철수 교수가 
AI 윤리에 관한 강연을 진행했다.

날짜:
장소:
인물:
```

**예상 출력:**
```
날짜: 2024년 3월 15일
장소: 서울
인물: 김철수 교수
```

## Zero-Shot Prompting 작성 팁

### 1. 명확한 지시사항

```
❌ 나쁜 예:
"이것 좀 해줘: [텍스트]"

✅ 좋은 예:
"다음 텍스트를 영어로 번역해주세요: [텍스트]"
```

### 2. 원하는 형식 명시

```
❌ 나쁜 예:
"이 영화 리뷰의 감정을 알려줘"

✅ 좋은 예:
"이 영화 리뷰의 감정을 '긍정', '부정', '중립' 중 
하나의 단어로만 답해주세요."
```

### 3. 컨텍스트 제공

```
❌ 나쁜 예:
"이것을 요약해"

✅ 좋은 예:
"다음은 신제품 발표 기사입니다. 
핵심 내용을 3문장으로 요약해주세요."
```

### 4. 역할 지정 (선택사항)

```
"당신은 전문 번역가입니다. 
다음 법률 문서를 정확하게 번역해주세요."
```

### 5. 제약사항 명시

```
"다음 질문에 답하되, 200자 이내로 작성해주세요."
```

## Zero-Shot의 장점과 한계

### 장점

**1. 즉시 사용 가능**
- 예시 작성 불필요
- 빠른 프로토타이핑
- 시간과 비용 절약

**2. 유연성**
- 다양한 작업에 즉시 적용
- 새로운 도메인에도 바로 시도

**3. 간단함**
- 복잡한 설정 불필요
- 배우기 쉬움

### 한계

**1. 복잡한 추론 문제에 약함**

```
문제: 이 그룹의 홀수를 더하면 짝수가 된다.
      15, 32, 5, 13, 82, 7, 1

Zero-shot 시도:
Q: 이 그룹의 홀수를 더하면 짝수가 된다. 
   15, 32, 5, 13, 82, 7, 1.

A: 네, 이 그룹의 홀수들을 더하면 107이 되는데, 
   이는 짝수입니다.

❌ 오답! (107은 홀수)
```

**2. 일관성 문제**
- 출력 형식이 매번 다를 수 있음
- 특정 스타일 강제하기 어려움

**3. 도메인 특화 작업의 한계**
- 매우 전문적이거나 특수한 작업에서는 성능 저하
- 예시가 있으면 훨씬 더 잘함

**4. 미묘한 뉘앙스 처리 어려움**
```
"이 제품은 괜찮아요" 
→ 긍정일까, 중립일까? Zero-shot은 헷갈릴 수 있음
```

## 언제 Zero-Shot을 사용해야 할까?

### Zero-Shot이 적합한 경우

✅ **간단하고 명확한 작업**
- 번역, 요약, 분류
- 일반적인 질문 답변
- 데이터 추출

✅ **빠른 프로토타이핑**
- 아이디어 검증
- 개념 증명(PoC)
- 초기 테스트

✅ **예시 작성이 어려운 경우**
- 새로운 도메인
- 다양한 형식의 입력
- 예시 데이터가 없을 때

### Zero-Shot이 부적합한 경우

❌ **복잡한 추론이 필요한 경우**
- 다단계 계산
- 논리적 추론
- → Few-Shot 또는 Chain-of-Thought 사용

❌ **특정 형식이나 스타일 필요**
- 일관된 JSON 구조
- 특정 글쓰기 스타일
- → Few-Shot으로 예시 제공

❌ **도메인 특화 전문 지식**
- 의료, 법률 등 전문 분야
- 특수한 규칙이나 컨벤션
- → Fine-tuning 또는 Few-Shot

## 마무리

Zero-Shot Prompting은 현대 언어 모델의 가장 기본이 되는 강력한 능력입니다. Instruction Tuning과 RLHF의 발전 덕분에, 이제 우리는 예시 없이도 모델에게 다양한 작업을 시킬 수 있습니다.

**핵심 요약:**
- Zero-Shot은 예시 없이 작업을 수행하는 기법입니다
- 대규모 사전 학습으로 가능해졌습니다
- Instruction Tuning은 작업 이해를 향상시켰습니다
- RLHF는 인간 선호도에 맞춰 모델을 정렬했습니다
- 간단한 작업에 매우 효과적이지만, 복잡한 추론에는 한계가 있습니다

하지만 Zero-Shot에도 한계가 있습니다. 복잡한 작업이나 특정 형식이 필요한 경우에는 어떻게 해야 할까요?

다음 글에서는 **Few-Shot Prompting**을 통해 모델에게 예시를 제공하여 성능을 향상시키는 방법을 알아보겠습니다. 예시의 개수, 품질, 형식이 어떻게 모델 성능에 영향을 미치는지 연구 결과와 함께 살펴보겠습니다.

---

**다음 글 예고:** Part 3 - Few-Shot Prompting: 적은 예시로 큰 효과를 만드는 법

## 참고문헌

- Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. *Advances in Neural Information Processing Systems, 33*, 1877-1901.
- Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., ... & Le, Q. V. (2021). Finetuned language models are zero-shot learners. *arXiv preprint arXiv:2109.01652*.
- Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., & Amodei, D. (2017). Deep reinforcement learning from human preferences. *Advances in Neural Information Processing Systems, 30*.