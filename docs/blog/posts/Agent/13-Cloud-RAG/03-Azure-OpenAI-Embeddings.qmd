---
title: "Azure OpenAI Embeddings"
subtitle: ë¬¸ì„œ ì„ë² ë”© ìƒì„±
description: |
  Azure OpenAI Serviceë¥¼ í™œìš©í•œ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ë°©ë²•ì„ ë‹¤ë£¬ë‹¤.
categories:
  - AI
  - RAG
  - Azure
  - LangChain
author: Kwangmin Kim
date: 11/04/2025
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
execute:
    eval: false
---


## Azure OpenAI Embeddingsë€?  

Azure OpenAI EmbeddingsëŠ” í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì„œë¹„ìŠ¤ë‹¤. RAG ì‹œìŠ¤í…œì—ì„œ ë¬¸ì„œë¥¼ ìˆ«ì ë²¡í„°ë¡œ í‘œí˜„í•˜ì—¬ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.  

**RAG íŒŒì´í”„ë¼ì¸ì—ì„œì˜ ì—­í• :**  
- ë¬¸ì„œ ì²­í¬ â†’ ë²¡í„° ë³€í™˜  
- ì‚¬ìš©ì ì§ˆì˜ â†’ ë²¡í„° ë³€í™˜  
- ë²¡í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚° â†’ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰  

**ì™œ ì„ë² ë”©ì´ í•„ìš”í•œê°€?**  
- í‚¤ì›Œë“œ ë§¤ì¹­ì˜ í•œê³„ ê·¹ë³µ (ì˜ˆ: "ìë™ì°¨" vs "ìŠ¹ìš©ì°¨")  
- ì˜ë¯¸ì  ìœ ì‚¬ì„± ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)  
- ë‹¤êµ­ì–´ ì§€ì› (ë™ì¼ ì˜ë¯¸ ê³µê°„ì— ë§¤í•‘)  

## Azure OpenAI Embeddings ëª¨ë¸  

Azure OpenAIëŠ” 3ê°€ì§€ ì£¼ìš” ì„ë² ë”© ëª¨ë¸ì„ ì œê³µí•œë‹¤:  

### 1. text-embedding-ada-002 (ë ˆê±°ì‹œ)  
**ì¶œì‹œ**: 2022ë…„ 12ì›”  

**ì‚¬ì–‘:**  
- ì°¨ì›: 1536 (ê³ ì •)  
- ìµœëŒ€ í† í°: 8,191  
- ê°€ê²©: $0.10 / 1M í† í°  

**íŠ¹ì§•:**  
- ë²”ìš© ì„ë² ë”© ëª¨ë¸  
- ì•ˆì •ì ì´ê³  ê²€ì¦ëœ ì„±ëŠ¥  
- ì‹ ê·œ í”„ë¡œì íŠ¸ëŠ” text-embedding-3 ê¶Œì¥  

### 2. text-embedding-3-small (ê¶Œì¥)  
**ì¶œì‹œ**: 2024ë…„ 1ì›”  

**ì‚¬ì–‘:**  
- ì°¨ì›: 512 ë˜ëŠ” 1536 (ì„ íƒ ê°€ëŠ¥)  
- ìµœëŒ€ í† í°: 8,191  
- ê°€ê²©: $0.02 / 1M í† í° (ada-002 ëŒ€ë¹„ **5ë°° ì €ë ´**)  

**íŠ¹ì§•:**  
- ì„±ëŠ¥ì€ ada-002ì™€ ìœ ì‚¬  
- **ë¹„ìš© íš¨ìœ¨ì ** (ê°€ê²© 1/5)  
- ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„  

**RAG ì‹œìŠ¤í…œ ê¶Œì¥**: âœ… **ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ìµœì„ ì˜ ì„ íƒ**  

### 3. text-embedding-3-large (ê³ ì„±ëŠ¥)  
**ì¶œì‹œ**: 2024ë…„ 1ì›”  

**ì‚¬ì–‘:**  
- ì°¨ì›: 256 ~ 3072 (ì„ íƒ ê°€ëŠ¥)  
- ìµœëŒ€ í† í°: 8,191  
- ê°€ê²©: $0.13 / 1M í† í°  

**íŠ¹ì§•:**  
- **ìµœê³  ì„±ëŠ¥** (MTEB ë²¤ì¹˜ë§ˆí¬ 1ìœ„ê¶Œ)  
- ë‹¤êµ­ì–´ ì§€ì› ê°•í™”  
- ë³µì¡í•œ ë„ë©”ì¸ì— ì í•©  

**RAG ì‹œìŠ¤í…œ ê¶Œì¥**: ë†’ì€ ì •í™•ë„ê°€ ì¤‘ìš”í•œ ê²½ìš°  

### ëª¨ë¸ ë¹„êµ  

| í•­ëª© | ada-002 | text-embedding-3-small | text-embedding-3-large |  
|------|---------|------------------------|------------------------|  
| **ì°¨ì›** | 1536 | 512/1536 | 256~3072 |  
| **ê°€ê²©** | $0.10/1M | $0.02/1M | $0.13/1M |  
| **ì„±ëŠ¥** | ê¸°ì¤€ | ìœ ì‚¬ | **ìµœê³ ** |  
| **ì†ë„** | ë³´í†µ | **ë¹ ë¦„** | ë³´í†µ |  
| **ê¶Œì¥** | ë ˆê±°ì‹œ | âœ… ë²”ìš© | ê³ ì •ë°€ë„ í•„ìš” ì‹œ |  

## Azure OpenAI ë¦¬ì†ŒìŠ¤ ìƒì„±  

### Azure Portalì—ì„œ ìƒì„±  

**1. Azure OpenAI ë¦¬ì†ŒìŠ¤ ë§Œë“¤ê¸°:**  
- [portal.azure.com](https://portal.azure.com) â†’ "ë¦¬ì†ŒìŠ¤ ë§Œë“¤ê¸°"  
- "Azure OpenAI" ê²€ìƒ‰ ë° ì„ íƒ  
- **ì‹ ì²­ ì–‘ì‹ ì‘ì„± í•„ìš”** (ì²˜ìŒ ì‚¬ìš© ì‹œ)  

**2. ê¸°ë³¸ ì„¤ì •:**  
- **êµ¬ë…**: ì‚¬ìš©í•  êµ¬ë… ì„ íƒ  
- **ë¦¬ì†ŒìŠ¤ ê·¸ë£¹**: `rg-rag-prod`  
- **ì§€ì—­**: East US (GPT-4 ì§€ì›) ë˜ëŠ” Sweden Central  
  - âš ï¸ **Korea Centralì€ í˜„ì¬ ì¼ë¶€ ëª¨ë¸ ë¯¸ì§€ì›**  
- **ì´ë¦„**: `openai-rag-prod`  
- **ê°€ê²© ì±…ì • ê³„ì¸µ**: Standard S0  

**3. ë„¤íŠ¸ì›Œí‚¹:**  
- ëª¨ë“  ë„¤íŠ¸ì›Œí¬ (ê°œë°œ) ë˜ëŠ” ì„ íƒëœ ë„¤íŠ¸ì›Œí¬ (í”„ë¡œë•ì…˜)  

**4. ê²€í†  + ë§Œë“¤ê¸°** â†’ ìƒì„± ì™„ë£Œ  

### ëª¨ë¸ ë°°í¬  

Azure OpenAIì—ì„œëŠ” ë¦¬ì†ŒìŠ¤ ìƒì„± í›„ **ëª¨ë¸ì„ ë°°í¬**í•´ì•¼ í•œë‹¤.  

**1. Azure OpenAI Studio ì ‘ì†:**  
- ìƒì„±ëœ ë¦¬ì†ŒìŠ¤ â†’ "Azure OpenAI Studioë¡œ ì´ë™"  

**2. ë°°í¬ ë§Œë“¤ê¸°:**  
- "ë°°í¬" ë©”ë‰´ â†’ "+ ë°°í¬ ë§Œë“¤ê¸°"  
- **ëª¨ë¸ ì„ íƒ**: text-embedding-3-small  
- **ë°°í¬ ì´ë¦„**: `text-embedding-3-small` (ë˜ëŠ” ì›í•˜ëŠ” ì´ë¦„)  
- **ëª¨ë¸ ë²„ì „**: ìµœì‹  ë²„ì „  
- **TPM (ë¶„ë‹¹ í† í°)**: 120K (ê¸°ë³¸ê°’)  

**3. ì—”ë“œí¬ì¸íŠ¸ ë° í‚¤ í™•ì¸:**  
- ë¦¬ì†ŒìŠ¤ â†’ "í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸"  
- **KEY 1**, **ì—”ë“œí¬ì¸íŠ¸** ë³µì‚¬  

### Azure CLIë¡œ ìƒì„±  

```bash  
# Azure OpenAI ë¦¬ì†ŒìŠ¤ ìƒì„±  
az cognitiveservices account create \  
    --name openai-rag-prod \  
    --resource-group rg-rag-prod \  
    --kind OpenAI \  
    --sku S0 \  
    --location eastus \  
    --yes  

# í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ ì¡°íšŒ  
az cognitiveservices account keys list \  
    --name openai-rag-prod \  
    --resource-group rg-rag-prod  

az cognitiveservices account show \  
    --name openai-rag-prod \  
    --resource-group rg-rag-prod \  
    --query properties.endpoint  
```  

## í™˜ê²½ ì„¤ì •  

### Python SDK ì„¤ì¹˜  

```bash  
pip install openai  
pip install langchain-openai  
pip install python-dotenv  
pip install tiktoken  # í† í° ê³„ì‚°ìš©  
```  

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •  

`.env` íŒŒì¼:  
```  
AZURE_OPENAI_ENDPOINT=https://openai-rag-prod.openai.azure.com/  
AZURE_OPENAI_API_KEY=your-key-here  
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small  
AZURE_OPENAI_API_VERSION=2024-02-01  
```  

## ê¸°ë³¸ ì‚¬ìš©ë²•  

### ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©  

```{python}  
from openai import AzureOpenAI  
from dotenv import load_dotenv  
import os  

load_dotenv()  

# Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±  
client = AzureOpenAI(  
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),  
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")  
)  

# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±  
text = "Azure OpenAIëŠ” Microsoftì˜ ê´€ë¦¬í˜• OpenAI ì„œë¹„ìŠ¤ë‹¤."  
response = client.embeddings.create(  
    input=text,  
    model=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT")  
)  

# ì„ë² ë”© ë²¡í„° ì¶”ì¶œ  
embedding = response.data[0].embedding  

print(f"ì„ë² ë”© ì°¨ì›: {len(embedding)}")  
print(f"ì²« 10ê°œ ê°’: {embedding[:10]}")  
```  

### ë°°ì¹˜ ì„ë² ë”© ìƒì„±  

```{python}  
# ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ë™ì‹œ ì„ë² ë”©  
texts = [  
    "AzureëŠ” Microsoftì˜ í´ë¼ìš°ë“œ í”Œë«í¼ì´ë‹¤.",  
    "RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ì´ë‹¤.",  
    "ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•œë‹¤."  
]  

response = client.embeddings.create(  
    input=texts,  
    model=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT")  
)  

# ëª¨ë“  ì„ë² ë”© ì¶”ì¶œ  
embeddings = [data.embedding for data in response.data]  

print(f"ìƒì„±ëœ ì„ë² ë”© ìˆ˜: {len(embeddings)}")  
print(f"ê° ì„ë² ë”© ì°¨ì›: {len(embeddings[0])}")  
```  

## LangChain í†µí•©  

### AzureOpenAIEmbeddings ì‚¬ìš©  

```{python}  
from langchain_openai import AzureOpenAIEmbeddings  

# LangChain ì„ë² ë”© í´ë˜ìŠ¤ ìƒì„±  
embeddings = AzureOpenAIEmbeddings(  
    azure_deployment=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT"),  
    openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),  
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),  
    api_key=os.getenv("AZURE_OPENAI_API_KEY")  
)  

# ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©  
text = "LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ë‹¤."  
embedding = embeddings.embed_query(text)  

print(f"ì„ë² ë”© ì°¨ì›: {len(embedding)}")  
print(f"ì²« 5ê°œ ê°’: {embedding[:5]}")  
```  

### ë¬¸ì„œ ëª©ë¡ ì„ë² ë”©  

```{python}  
from langchain_core.documents import Document  

# ë¬¸ì„œ ëª©ë¡ ìƒì„±  
documents = [  
    Document(page_content="Azure AI SearchëŠ” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë‹¤.", metadata={"source": "doc1"}),  
    Document(page_content="Document IntelligenceëŠ” OCR ì„œë¹„ìŠ¤ë‹¤.", metadata={"source": "doc2"}),  
    Document(page_content="Blob StorageëŠ” íŒŒì¼ ì €ì¥ì†Œë‹¤.", metadata={"source": "doc3"})  
]  

# ë¬¸ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ì„ë² ë”©  
texts = [doc.page_content for doc in documents]  
doc_embeddings = embeddings.embed_documents(texts)  

print(f"ì„ë² ë”©ëœ ë¬¸ì„œ ìˆ˜: {len(doc_embeddings)}")  
print(f"ì²« ë²ˆì§¸ ë¬¸ì„œ ì„ë² ë”© ì°¨ì›: {len(doc_embeddings[0])}")  
```  

## ë¬¸ì„œ ì²­í¬ ì„ë² ë”©  

RAG ì‹œìŠ¤í…œì—ì„œëŠ” ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•œ í›„ ê° ì²­í¬ë¥¼ ì„ë² ë”©í•œë‹¤.  

### ì „ì²´ íŒŒì´í”„ë¼ì¸  

```{python}  
from langchain.text_splitter import RecursiveCharacterTextSplitter  
from langchain_community.document_loaders import TextLoader  

# 1. ë¬¸ì„œ ë¡œë”©  
loader = TextLoader("sample_document.txt", encoding="utf-8")  
documents = loader.load()  

# 2. í…ìŠ¤íŠ¸ ë¶„í•   
text_splitter = RecursiveCharacterTextSplitter(  
    chunk_size=1000,  
    chunk_overlap=200,  
    length_function=len,  
    separators=["\n\n", "\n", " ", ""]  
)  
chunks = text_splitter.split_documents(documents)  

print(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")  

# 3. ê° ì²­í¬ ì„ë² ë”©  
chunk_texts = [chunk.page_content for chunk in chunks]  
chunk_embeddings = embeddings.embed_documents(chunk_texts)  

print(f"ì„ë² ë”© ì™„ë£Œ: {len(chunk_embeddings)}ê°œ ì²­í¬")  
```  

### ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥  

```{python}  
# ì„ë² ë”©ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ì €ì¥  
embedded_chunks = []  
for chunk, embedding in zip(chunks, chunk_embeddings):  
    embedded_chunks.append({  
        "text": chunk.page_content,  
        "embedding": embedding,  
        "metadata": {  
            **chunk.metadata,  
            "chunk_size": len(chunk.page_content),  
            "embedding_model": "text-embedding-3-small"  
        }  
    })  

print(f"ì²« ë²ˆì§¸ ì²­í¬ ë©”íƒ€ë°ì´í„°: {embedded_chunks[0]['metadata']}")  
```  

## ìœ ì‚¬ë„ ê³„ì‚°  

### ì½”ì‚¬ì¸ ìœ ì‚¬ë„  

```{python}  
import numpy as np  

def cosine_similarity(vec1, vec2):  
    """ë‘ ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""  
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))  

# ì˜ˆì‹œ: ì§ˆì˜ì™€ ë¬¸ì„œ ìœ ì‚¬ë„ ê³„ì‚°  
query = "Azureì˜ AI ì„œë¹„ìŠ¤ëŠ” ë¬´ì—‡ì¸ê°€?"  
doc1 = "Azure AI SearchëŠ” ê²€ìƒ‰ ì„œë¹„ìŠ¤ë‹¤."  
doc2 = "Blob StorageëŠ” íŒŒì¼ ì €ì¥ì†Œë‹¤."  

# ì„ë² ë”© ìƒì„±  
query_embedding = embeddings.embed_query(query)  
doc1_embedding = embeddings.embed_query(doc1)  
doc2_embedding = embeddings.embed_query(doc2)  

# ìœ ì‚¬ë„ ê³„ì‚°  
similarity1 = cosine_similarity(query_embedding, doc1_embedding)  
similarity2 = cosine_similarity(query_embedding, doc2_embedding)  

print(f"ì§ˆì˜ vs ë¬¸ì„œ1 ìœ ì‚¬ë„: {similarity1:.4f}")  
print(f"ì§ˆì˜ vs ë¬¸ì„œ2 ìœ ì‚¬ë„: {similarity2:.4f}")  
```  

### ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°  

```{python}  
def find_most_similar(query_embedding, doc_embeddings, top_k=3):  
    """ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°"""  
    similarities = []  
    for idx, doc_emb in enumerate(doc_embeddings):  
        sim = cosine_similarity(query_embedding, doc_emb)  
        similarities.append((idx, sim))  
    
    # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬  
    similarities.sort(key=lambda x: x[1], reverse=True)  
    return similarities[:top_k]  

# ì‚¬ìš© ì˜ˆì‹œ  
query = "RAG ì‹œìŠ¤í…œ êµ¬ì¶• ë°©ë²•"  
query_emb = embeddings.embed_query(query)  

# ìƒìœ„ 3ê°œ ìœ ì‚¬ ë¬¸ì„œ ì°¾ê¸°  
top_docs = find_most_similar(query_emb, chunk_embeddings, top_k=3)  

print("ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ:")  
for idx, similarity in top_docs:  
    print(f"ì²­í¬ {idx}: {similarity:.4f}")  
    print(f"ë‚´ìš©: {chunks[idx].page_content[:100]}...\n")  
```  

## ì°¨ì› ì¶•ì†Œ (Matryoshka Embeddings)  

text-embedding-3 ëª¨ë¸ì€ ì°¨ì› ì¶•ì†Œë¥¼ ì§€ì›í•œë‹¤.  

### ì¶•ì†Œ ì°¨ì›ìœ¼ë¡œ ì„ë² ë”© ìƒì„±  

```{python}  
# 512 ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ (ê¸°ë³¸ 1536)  
response = client.embeddings.create(  
    input="Azure OpenAI ì„ë² ë”© í…ŒìŠ¤íŠ¸",  
    model="text-embedding-3-small",  
    dimensions=512  # 1536 â†’ 512  
)  

embedding_512 = response.data[0].embedding  
print(f"ì¶•ì†Œëœ ì°¨ì›: {len(embedding_512)}")  
```  

### ì°¨ì› ì¶•ì†Œ í›„ ìœ ì‚¬ë„ ë¹„êµ  

```{python}  
# ì „ì²´ ì°¨ì› (1536) vs ì¶•ì†Œ ì°¨ì› (512) ë¹„êµ  
text1 = "AzureëŠ” í´ë¼ìš°ë“œ í”Œë«í¼ì´ë‹¤."  
text2 = "Microsoftì˜ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ë‹¤."  
text3 = "Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë‹¤."  

# 1536 ì°¨ì›  
resp_full = client.embeddings.create(  
    input=[text1, text2, text3],  
    model="text-embedding-3-small"  
)  
emb_full = [d.embedding for d in resp_full.data]  

# 512 ì°¨ì›  
resp_reduced = client.embeddings.create(  
    input=[text1, text2, text3],  
    model="text-embedding-3-small",  
    dimensions=512  
)  
emb_reduced = [d.embedding for d in resp_reduced.data]  

# ìœ ì‚¬ë„ ë¹„êµ  
sim_full_12 = cosine_similarity(emb_full[0], emb_full[1])  
sim_reduced_12 = cosine_similarity(emb_reduced[0], emb_reduced[1])  

print(f"1536 ì°¨ì›: text1-text2 ìœ ì‚¬ë„ = {sim_full_12:.4f}")  
print(f"512 ì°¨ì›: text1-text2 ìœ ì‚¬ë„ = {sim_reduced_12:.4f}")  
print(f"ì°¨ì´: {abs(sim_full_12 - sim_reduced_12):.4f}")  
```  

**ì°¨ì› ì¶•ì†Œ ì¥ì :**  
- ì €ì¥ ê³µê°„ ì ˆì•½ (512 ì°¨ì›: 1/3 í¬ê¸°)  
- ë¹ ë¥¸ ê²€ìƒ‰ ì†ë„  
- ìœ ì‚¬ë„ ì •í™•ë„ ì•½ê°„ ê°ì†Œ (ë³´í†µ 5% ì´ë‚´)  

## ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”  

### ëŒ€ëŸ‰ ë¬¸ì„œ ì„ë² ë”©  

```{python}  
def embed_documents_batch(texts, batch_size=100):  
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëŒ€ëŸ‰ ë¬¸ì„œ ì„ë² ë”©"""  
    all_embeddings = []  
    
    for i in range(0, len(texts), batch_size):  
        batch = texts[i:i+batch_size]  
        print(f"ë°°ì¹˜ {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘...")  
        
        # ë°°ì¹˜ ì„ë² ë”©  
        batch_embeddings = embeddings.embed_documents(batch)  
        all_embeddings.extend(batch_embeddings)  
    
    return all_embeddings  

# ì‚¬ìš© ì˜ˆì‹œ (1000ê°œ ë¬¸ì„œ)  
# large_texts = [f"ë¬¸ì„œ {i} ë‚´ìš©..." for i in range(1000)]  
# embeddings_result = embed_documents_batch(large_texts, batch_size=100)  
```  

### ë³‘ë ¬ ì²˜ë¦¬  

```{python}  
from concurrent.futures import ThreadPoolExecutor  
import time  

def embed_chunk(texts_chunk):  
    """ì²­í¬ ë‹¨ìœ„ ì„ë² ë”© (ë³‘ë ¬ ì‹¤í–‰ìš©)"""  
    return embeddings.embed_documents(texts_chunk)  

def embed_parallel(texts, num_workers=4, chunk_size=25):  
    """ë³‘ë ¬ë¡œ ì„ë² ë”© ìƒì„±"""  
    # ì²­í¬ë¡œ ë¶„í•   
    chunks = [texts[i:i+chunk_size] for i in range(0, len(texts), chunk_size)]  
    
    # ë³‘ë ¬ ì‹¤í–‰  
    start_time = time.time()  
    with ThreadPoolExecutor(max_workers=num_workers) as executor:  
        results = list(executor.map(embed_chunk, chunks))  
    
    # ê²°ê³¼ ë³‘í•©  
    all_embeddings = [emb for chunk_result in results for emb in chunk_result]  
    
    duration = time.time() - start_time  
    print(f"ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {len(all_embeddings)}ê°œ ì„ë² ë”©, {duration:.2f}ì´ˆ")  
    
    return all_embeddings  

# ì‚¬ìš© ì˜ˆì‹œ  
# texts_sample = [f"ìƒ˜í”Œ í…ìŠ¤íŠ¸ {i}" for i in range(100)]  
# parallel_embeddings = embed_parallel(texts_sample, num_workers=4, chunk_size=25)  
```  

## ìºì‹± ì „ëµ  

ë™ì¼í•œ í…ìŠ¤íŠ¸ëŠ” ì¬ê³„ì‚°í•˜ì§€ ì•Šë„ë¡ ìºì‹±í•œë‹¤.  

### ê°„ë‹¨í•œ ìºì‹±  

```{python}  
import hashlib  
import json  

class EmbeddingCache:  
    def __init__(self, cache_file=".embedding_cache.json"):  
        self.cache_file = cache_file  
        self.cache = self._load_cache()  
    
    def _load_cache(self):  
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""  
        try:  
            with open(self.cache_file, "r") as f:  
                return json.load(f)  
        except FileNotFoundError:  
            return {}  
    
    def _save_cache(self):  
        """ìºì‹œ íŒŒì¼ ì €ì¥"""  
        with open(self.cache_file, "w") as f:  
            json.dump(self.cache, f)  
    
    def _get_hash(self, text):  
        """í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„±"""  
        return hashlib.md5(text.encode()).hexdigest()  
    
    def get_embedding(self, text, embeddings_func):  
        """ìºì‹œëœ ì„ë² ë”© ë°˜í™˜ ë˜ëŠ” ìƒì„±"""  
        text_hash = self._get_hash(text)  
        
        # ìºì‹œ í™•ì¸  
        if text_hash in self.cache:  
            print(f"ìºì‹œ íˆíŠ¸: {text[:50]}...")  
            return self.cache[text_hash]  
        
        # ì„ë² ë”© ìƒì„±  
        print(f"ìºì‹œ ë¯¸ìŠ¤: {text[:50]}...")  
        embedding = embeddings_func(text)  
        
        # ìºì‹œ ì €ì¥  
        self.cache[text_hash] = embedding  
        self._save_cache()  
        
        return embedding  

# ì‚¬ìš© ì˜ˆì‹œ  
cache = EmbeddingCache()  

text = "Azure OpenAI ì„œë¹„ìŠ¤"  
emb1 = cache.get_embedding(text, embeddings.embed_query)  # ìºì‹œ ë¯¸ìŠ¤  
emb2 = cache.get_embedding(text, embeddings.embed_query)  # ìºì‹œ íˆíŠ¸ (ì¦‰ì‹œ ë°˜í™˜)  
```  

### LangChain CacheBackedEmbeddings  

```{python}  
from langchain.embeddings import CacheBackedEmbeddings  
from langchain.storage import LocalFileStore  

# ë¡œì»¬ íŒŒì¼ ìºì‹œ ìŠ¤í† ì–´  
store = LocalFileStore("./.embedding_cache/")  

# ìºì‹œê°€ ì ìš©ëœ ì„ë² ë”©  
cached_embedder = CacheBackedEmbeddings.from_bytes_store(  
    underlying_embeddings=embeddings,  
    document_embedding_cache=store,  
    namespace="azure-openai-embeddings"  
)  

# ì‚¬ìš© (ìë™ ìºì‹±)  
texts = ["Azure AI", "OpenAI Service", "Azure AI"]  # "Azure AI" ì¤‘ë³µ  
embeddings_result = cached_embedder.embed_documents(texts)  
# ë‘ ë²ˆì§¸ "Azure AI"ëŠ” ìºì‹œì—ì„œ ì¦‰ì‹œ ë°˜í™˜  
```  

## í† í° ê³„ì‚° ë° ë¹„ìš© ì¶”ì •  

### í† í° ìˆ˜ ê³„ì‚°  

```{python}  
import tiktoken  

def count_tokens(text, model="cl100k_base"):  
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""  
    encoding = tiktoken.get_encoding(model)  
    return len(encoding.encode(text))  

# ì˜ˆì‹œ  
text = "Azure OpenAI EmbeddingsëŠ” í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì„œë¹„ìŠ¤ë‹¤."  
token_count = count_tokens(text)  

print(f"í…ìŠ¤íŠ¸: {text}")  
print(f"í† í° ìˆ˜: {token_count}")  
```  

### ë¹„ìš© ì¶”ì •  

```{python}  
def estimate_cost(token_count, model="text-embedding-3-small"):  
    """ì„ë² ë”© ë¹„ìš© ì¶”ì •"""  
    prices = {  
        "text-embedding-ada-002": 0.0001,  # $0.10 / 1M tokens  
        "text-embedding-3-small": 0.00002,  # $0.02 / 1M tokens  
        "text-embedding-3-large": 0.00013   # $0.13 / 1M tokens  
    }  
    
    price_per_token = prices.get(model, 0)  
    cost = token_count * price_per_token  
    
    return cost  

# ì˜ˆì‹œ: 10,000ê°œ ë¬¸ì„œ (ê° 500 í† í°)  
total_tokens = 10000 * 500  
cost_small = estimate_cost(total_tokens, "text-embedding-3-small")  
cost_large = estimate_cost(total_tokens, "text-embedding-3-large")  

print(f"ì´ í† í°: {total_tokens:,}")  
print(f"text-embedding-3-small ë¹„ìš©: ${cost_small:.2f}")  
print(f"text-embedding-3-large ë¹„ìš©: ${cost_large:.2f}")  
```  

## Azure AI Search ì—°ë™  

ì„ë² ë”© ìƒì„± í›„ Azure AI Searchì— ì €ì¥í•œë‹¤.  

```{python}  
from langchain_community.vectorstores.azuresearch import AzureSearch  

# Azure Search VectorStore ì´ˆê¸°í™”  
vector_store = AzureSearch(  
    azure_search_endpoint=os.getenv("AZURE_SEARCH_ENDPOINT"),  
    azure_search_key=os.getenv("AZURE_SEARCH_API_KEY"),  
    index_name="rag-embeddings",  
    embedding_function=embeddings.embed_query  
)  

# ë¬¸ì„œ ì¶”ê°€ (ìë™ ì„ë² ë”©)  
from langchain_core.documents import Document  

documents = [  
    Document(page_content="AzureëŠ” Microsoftì˜ í´ë¼ìš°ë“œ í”Œë«í¼ì´ë‹¤.", metadata={"source": "doc1"}),  
    Document(page_content="RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ì´ë‹¤.", metadata={"source": "doc2"}),  
    Document(page_content="ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•œë‹¤.", metadata={"source": "doc3"})  
]  

# ë¬¸ì„œ ì¶”ê°€ (ì„ë² ë”© ìë™ ìƒì„± ë° ì—…ë¡œë“œ)  
vector_store.add_documents(documents)  
print(f"{len(documents)}ê°œ ë¬¸ì„œê°€ Azure AI Searchì— ì¶”ê°€ë˜ì—ˆë‹¤.")  
```  

## ëª¨ë‹ˆí„°ë§  

### ì„ë² ë”© í’ˆì§ˆ í™•ì¸  

```{python}  
def check_embedding_quality(embeddings_list):  
    """ì„ë² ë”© í’ˆì§ˆ ë©”íŠ¸ë¦­"""  
    if not embeddings_list:  
        return None  
    
    # ë²¡í„° ê¸¸ì´ (L2 norm)  
    norms = [np.linalg.norm(emb) for emb in embeddings_list]  
    
    # ì°¨ì›ë³„ í†µê³„  
    embeddings_array = np.array(embeddings_list)  
    
    metrics = {  
        "count": len(embeddings_list),  
        "dimensions": len(embeddings_list[0]),  
        "norm_mean": np.mean(norms),  
        "norm_std": np.std(norms),  
        "value_mean": np.mean(embeddings_array),  
        "value_std": np.std(embeddings_array)  
    }  
    
    return metrics  

# ì‚¬ìš© ì˜ˆì‹œ  
sample_texts = ["Azure", "OpenAI", "RAG", "Embeddings"]  
sample_embeddings = embeddings.embed_documents(sample_texts)  

quality = check_embedding_quality(sample_embeddings)  
print("ì„ë² ë”© í’ˆì§ˆ ë©”íŠ¸ë¦­:")  
for key, value in quality.items():  
    print(f"- {key}: {value}")  
```  

## ë¬¸ì œ í•´ê²°  

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜  

**1. RateLimitError: ìš”ì²­ ì œí•œ ì´ˆê³¼**  
```python  
# í•´ê²°: ì¬ì‹œë„ ë¡œì§ êµ¬í˜„  
from tenacity import retry, stop_after_attempt, wait_exponential  

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))  
def embed_with_retry(text):  
    return embeddings.embed_query(text)  

# ì‚¬ìš©  
result = embed_with_retry("í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸")  
```  

**2. InvalidRequestError: í† í° ì´ˆê³¼**  
```python  
# í•´ê²°: í…ìŠ¤íŠ¸ ìë¥´ê¸° (ìµœëŒ€ 8,191 í† í°)  
def truncate_text(text, max_tokens=8000):  
    encoding = tiktoken.get_encoding("cl100k_base")  
    tokens = encoding.encode(text)  
    
    if len(tokens) > max_tokens:  
        tokens = tokens[:max_tokens]  
        text = encoding.decode(tokens)  
    
    return text  

long_text = "..." * 10000  
safe_text = truncate_text(long_text)  
embedding = embeddings.embed_query(safe_text)  
```  

## ì°¸ê³  ìë£Œ  

### ê³µì‹ ë¬¸ì„œ  
- [Azure OpenAI Embeddings](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/understand-embeddings)  
- [text-embedding-3 ëª¨ë¸](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models)  
- [Python SDK ì°¸ì¡°](https://learn.microsoft.com/en-us/python/api/overview/azure/openai-readme)  

### ë²¤ì¹˜ë§ˆí¬  
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) - ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ  

## ë‹¤ìŒ ë‹¨ê³„  

ì„ë² ë”© ìƒì„±ì´ ì™„ë£Œë˜ì—ˆë‹¤ë©´, ì´ì œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ì:  

ğŸ‘‰ [04-Azure-AI-Search-Integration.qmd](./04-Azure-AI-Search-Integration.qmd) - Azure AI Searchì™€ RAG ì‹œìŠ¤í…œ í†µí•©  