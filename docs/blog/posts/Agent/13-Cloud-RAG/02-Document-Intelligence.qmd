---
title: "Document Intelligence"
subtitle: OCR ë° ë¬¸ì„œ ë ˆì´ì•„ì›ƒ ë¶„ì„
description: |
  Azure Document Intelligenceë¥¼ í™œìš©í•œ ë¬¸ì„œ OCR ë° ë ˆì´ì•„ì›ƒ ë¶„ì„ ë°©ë²•ì„ ì„¤ëª…í•œë‹¤.
categories:
  - AI
  - RAG
  - Azure
author: Kwangmin Kim
date: 11/03/2025
format: 
  html:
    code-fold: true
    toc: true
    number-sections: true
draft: False
execute:
    eval: false
---

## Azure Document Intelligenceë€?

* Azure Document Intelligence(ì´ì „ Form Recognizer)ëŠ” AI ê¸°ë°˜ ë¬¸ì„œ ì´í•´ ì„œë¹„ìŠ¤ë¡œ, ìŠ¤ìº” ë¬¸ì„œë‚˜ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸, í‘œ, êµ¬ì¡°ë¥¼ ì¶”ì¶œí•œë‹¤. 
* Azure Document Intelligenceì˜ ì£¼ìš” ê¸°ëŠ¥
    * **ë¬¸ì„œ ë¶„ì„ ëª¨ë¸**
    * **ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸**
    * **ì‚¬ìš©ì ì§€ì • ëª¨ë¸** 
* RAG ì‹œìŠ¤í…œì—ì„œ PDF, ì´ë¯¸ì§€ ë“±ì˜ ë¹„ì •í˜• ë¬¸ì„œë¥¼ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•µì‹¬ ì—­í• ì„ í•œë‹¤.

**RAG íŒŒì´í”„ë¼ì¸ì—ì„œì˜ ì—­í• :**
- ìŠ¤ìº” PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)
- ì´ë¯¸ì§€ ì† í…ìŠ¤íŠ¸ ì¸ì‹
- í‘œ, ì œëª©, ë‹¨ë½ ë“± ë¬¸ì„œ êµ¬ì¡° íŒŒì•…
- í•œê¸€ ë° ë‹¤êµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬

**í•µì‹¬ ê¸°ëŠ¥:**
- **ë†’ì€ OCR ì •í™•ë„**: í•œêµ­ì–´ 95% ì´ìƒ
- **ë ˆì´ì•„ì›ƒ ë¶„ì„**: ì œëª©, ë‹¨ë½, í‘œ, ë¦¬ìŠ¤íŠ¸ êµ¬ë¶„
- **ë‹¤êµ­ì–´ ì§€ì›**: 100ê°œ ì´ìƒ ì–¸ì–´ (í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ë“±)
- **ì‚¬ì „ í›ˆë ¨ ëª¨ë¸**: ì˜ìˆ˜ì¦, ëª…í•¨, ì²­êµ¬ì„œ ë“± íŠ¹í™” ëª¨ë¸ ì œê³µ

## Document Intelligence ëª¨ë¸

Azure Document IntelligenceëŠ” ë¬¸ì„œ ìœ í˜•ì— ë”°ë¼ ë‹¤ì–‘í•œ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ì„ ì œê³µí•œë‹¤:

### Layout API (ë²”ìš© ë ˆì´ì•„ì›ƒ ë¶„ì„)
**ìš©ë„**: ëª¨ë“  ì¢…ë¥˜ì˜ ë¬¸ì„œ ë ˆì´ì•„ì›ƒ ë¶„ì„

**ì¶”ì¶œ ì •ë³´:**
- í…ìŠ¤íŠ¸ (OCR)
- í‘œ (í–‰, ì—´ êµ¬ì¡°)
- ì œëª© ë° ì„¹ì…˜
- ë‹¨ë½ ë° ì¤„ë°”ê¿ˆ
- ì„ íƒ ë§ˆí¬ (ì²´í¬ë°•ìŠ¤)

**RAG ì‹œìŠ¤í…œ ê¶Œì¥**: ê°€ì¥ ë§ì´ ì‚¬ìš©

**ê°€ê²©**: í˜ì´ì§€ë‹¹ $0.01

### Read API (í…ìŠ¤íŠ¸ ì¶”ì¶œ ì „ìš©)
**ìš©ë„**: ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ë¬´ì‹œ)

**ì¶”ì¶œ ì •ë³´:**
- í…ìŠ¤íŠ¸ë§Œ (êµ¬ì¡° ì •ë³´ ì—†ìŒ)
- ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„

**RAG ì‹œìŠ¤í…œ ê¶Œì¥**: ê°„ë‹¨í•œ ë¬¸ì„œ, ë¹„ìš© ì ˆê° í•„ìš” ì‹œ

**ê°€ê²©**: í˜ì´ì§€ë‹¹ $0.0015

### Prebuilt Models (íŠ¹í™” ëª¨ë¸)
| ëª¨ë¸ | ìš©ë„ | ì¶”ì¶œ ì •ë³´ |
|------|------|-----------|
| **Invoice** | ì²­êµ¬ì„œ, ì„¸ê¸ˆê³„ì‚°ì„œ | ë‚ ì§œ, ê¸ˆì•¡, í’ˆëª©, ê³µê¸‰ì |
| **Receipt** | ì˜ìˆ˜ì¦ | ìƒì ëª…, ë‚ ì§œ, ì´ì•¡, í•­ëª© |
| **ID Document** | ì‹ ë¶„ì¦, ì—¬ê¶Œ | ì´ë¦„, ìƒë…„ì›”ì¼, ì£¼ì†Œ |
| **Business Card** | ëª…í•¨ | ì´ë¦„, ì§í•¨, ì—°ë½ì²˜ |
| **W-2** | ë¯¸êµ­ ì„¸ê¸ˆ ì–‘ì‹ | ê¸‰ì—¬, ì„¸ê¸ˆ |

**RAG ì‹œìŠ¤í…œ ê¶Œì¥**: íŠ¹ì • ë¬¸ì„œ íƒ€ì…ë§Œ ì²˜ë¦¬ ì‹œ ìœ ìš©

## Document Intelligence ë¦¬ì†ŒìŠ¤ ìƒì„±

### Azure Portalì—ì„œ ìƒì„±

**ë¦¬ì†ŒìŠ¤ ë§Œë“¤ê¸°:**
- [portal.azure.com](https://portal.azure.com) â†’ "ë¦¬ì†ŒìŠ¤ ë§Œë“¤ê¸°"
- "Document Intelligence" ê²€ìƒ‰ ë° ì„ íƒ

**ê¸°ë³¸ ì„¤ì •:**
- **êµ¬ë…**: ì‚¬ìš©í•  êµ¬ë… ì„ íƒ
- **ë¦¬ì†ŒìŠ¤ ê·¸ë£¹**: `rg-rag-prod`
- **ì§€ì—­**: Korea Central (í•œêµ­ì–´ ì²˜ë¦¬ ìµœì í™”)
- **ì´ë¦„**: `doc-intel-rag-prod`
- **ê°€ê²© ì±…ì • ê³„ì¸µ**:
  - **Free F0**: 500í˜ì´ì§€/ì›” ë¬´ë£Œ (ê°œë°œ/í…ŒìŠ¤íŠ¸)
  - **Standard S0**: ì¢…ëŸ‰ì œ (í”„ë¡œë•ì…˜)

**ê²€í†  + ë§Œë“¤ê¸°** â†’ ìƒì„± ì™„ë£Œ

**í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ í™•ì¸:**
- ìƒì„±ëœ ë¦¬ì†ŒìŠ¤ â†’ "í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸"
- **KEY 1** ë° **ì—”ë“œí¬ì¸íŠ¸** ë³µì‚¬

### Azure CLIë¡œ ìƒì„±

```bash
# Document Intelligence ë¦¬ì†ŒìŠ¤ ìƒì„±
az cognitiveservices account create \
    --name doc-intel-rag-prod \
    --resource-group rg-rag-prod \
    --kind FormRecognizer \
    --sku S0 \
    --location koreacentral \
    --yes

# í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ ì¡°íšŒ
az cognitiveservices account keys list \
    --name doc-intel-rag-prod \
    --resource-group rg-rag-prod

az cognitiveservices account show \
    --name doc-intel-rag-prod \
    --resource-group rg-rag-prod \
    --query properties.endpoint
```

## í™˜ê²½ ì„¤ì •

### Python SDK ì„¤ì¹˜

```bash
pip install azure-ai-formrecognizer
pip install azure-storage-blob
pip install python-dotenv
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼:
```
AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://doc-intel-rag-prod.cognitiveservices.azure.com/
AZURE_DOCUMENT_INTELLIGENCE_KEY=your-key-here
```

## Layout API ì‚¬ìš©ë²•

### ë¡œì»¬ íŒŒì¼ ë¶„ì„

```{python}
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential
from dotenv import load_dotenv
import os

load_dotenv()

# í´ë¼ì´ì–¸íŠ¸ ìƒì„±
endpoint = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT")
key = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_KEY")

document_analysis_client = DocumentAnalysisClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(key)
)

# ë¡œì»¬ PDF íŒŒì¼ ë¶„ì„
with open("sample.pdf", "rb") as f:
    poller = document_analysis_client.begin_analyze_document(
        "prebuilt-layout", document=f
    )
    result = poller.result()

# ê²°ê³¼ ì¶œë ¥
print(f"ë¶„ì„ëœ í˜ì´ì§€ ìˆ˜: {len(result.pages)}")
print(f"ì¶”ì¶œëœ ë‹¨ë½ ìˆ˜: {len(result.paragraphs)}")
print(f"ì¶”ì¶œëœ í‘œ ìˆ˜: {len(result.tables)}")
```

### URLì—ì„œ ë¬¸ì„œ ë¶„ì„

```{python}
# Azure Blob Storage URLë¡œ ë¶„ì„
document_url = "https://stragdocs2025.blob.core.windows.net/rag-documents/sample.pdf?<sas-token>"

poller = document_analysis_client.begin_analyze_document_from_url(
    "prebuilt-layout", document_url=document_url
)
result = poller.result()

print("ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ")
```

## í…ìŠ¤íŠ¸ ì¶”ì¶œ

### ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ

```{python}
def extract_full_text(result):
    """ë¬¸ì„œì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    full_text = []
    
    # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    for page in result.pages:
        page_text = []
        for line in page.lines:
            page_text.append(line.content)
        
        full_text.append("\n".join(page_text))
    
    return "\n\n".join(full_text)

# ì‚¬ìš© ì˜ˆì‹œ
text = extract_full_text(result)
print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ì• 500ì):\n{text[:500]}")
```

### ë‹¨ë½ ë‹¨ìœ„ ì¶”ì¶œ

```{python}
def extract_paragraphs(result):
    """ë‹¨ë½ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ìœ ì§€)"""
    paragraphs = []
    
    for paragraph in result.paragraphs:
        paragraphs.append({
            "content": paragraph.content,
            "role": paragraph.role,  # title, sectionHeading, paragraph ë“±
            "page_number": paragraph.bounding_regions[0].page_number if paragraph.bounding_regions else None
        })
    
    return paragraphs

# ì‚¬ìš© ì˜ˆì‹œ
paragraphs = extract_paragraphs(result)
print(f"ì´ {len(paragraphs)}ê°œ ë‹¨ë½ ì¶”ì¶œ")

# ì œëª©ë§Œ ì¶”ì¶œ
titles = [p for p in paragraphs if p["role"] == "title"]
print(f"\në¬¸ì„œ ì œëª©ë“¤:")
for title in titles:
    print(f"- {title['content']} (í˜ì´ì§€ {title['page_number']})")
```

## í‘œ ì¶”ì¶œ

### í‘œ êµ¬ì¡° íŒŒì‹±

```{python}
def extract_tables(result):
    """ë¬¸ì„œì—ì„œ í‘œ ì¶”ì¶œ"""
    tables_data = []
    
    for table_idx, table in enumerate(result.tables):
        # í‘œ ë©”íƒ€ë°ì´í„°
        table_info = {
            "table_id": table_idx + 1,
            "row_count": table.row_count,
            "column_count": table.column_count,
            "page_number": table.bounding_regions[0].page_number,
            "cells": []
        }
        
        # ì…€ ë°ì´í„°
        for cell in table.cells:
            table_info["cells"].append({
                "row_index": cell.row_index,
                "column_index": cell.column_index,
                "content": cell.content,
                "kind": cell.kind  # columnHeader, rowHeader, content, stub
            })
        
        tables_data.append(table_info)
    
    return tables_data

# ì‚¬ìš© ì˜ˆì‹œ
tables = extract_tables(result)
print(f"ì¶”ì¶œëœ í‘œ ê°œìˆ˜: {len(tables)}")

for table in tables:
    print(f"\n[í‘œ {table['table_id']}] í˜ì´ì§€ {table['page_number']}")
    print(f"í¬ê¸°: {table['row_count']}í–‰ Ã— {table['column_count']}ì—´")
```

### í‘œë¥¼ Pandas DataFrameìœ¼ë¡œ ë³€í™˜

```{python}
import pandas as pd

def table_to_dataframe(table):
    """í‘œë¥¼ Pandas DataFrameìœ¼ë¡œ ë³€í™˜"""
    # í‘œ ì´ˆê¸°í™” (ë¹ˆ ì…€ í¬í•¨)
    data = [[None] * table["column_count"] for _ in range(table["row_count"])]
    
    # ì…€ ë°ì´í„° ì±„ìš°ê¸°
    for cell in table["cells"]:
        data[cell["row_index"]][cell["column_index"]] = cell["content"]
    
    # DataFrame ìƒì„± (ì²« í–‰ì„ í—¤ë”ë¡œ)
    df = pd.DataFrame(data[1:], columns=data[0])
    return df

# ì‚¬ìš© ì˜ˆì‹œ
if tables:
    df = table_to_dataframe(tables[0])
    print("\ní‘œ ë°ì´í„° (DataFrame):")
    print(df)
```

## ë ˆì´ì•„ì›ƒ ê¸°ë°˜ ë¬¸ì„œ ë¶„í• 

RAG ì‹œìŠ¤í…œì—ì„œëŠ” ë¬¸ì„œ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ì²­í¬ ë¶„í• ì´ ì¤‘ìš”í•˜ë‹¤.

```{python}
from langchain_core.documents import Document
from typing import List

def split_by_layout(result, max_chunk_size: int = 1000) -> List[Document]:
    """ë ˆì´ì•„ì›ƒ ê¸°ë°˜ ë¬¸ì„œ ë¶„í• """
    documents = []
    current_chunk = []
    current_size = 0
    current_page = 1
    
    for paragraph in result.paragraphs:
        # ë‹¨ë½ ì •ë³´
        content = paragraph.content
        role = paragraph.role or "paragraph"
        page_num = paragraph.bounding_regions[0].page_number if paragraph.bounding_regions else current_page
        
        # ì œëª©ì€ ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘
        if role in ["title", "sectionHeading"] and current_chunk:
            # ì´ì „ ì²­í¬ ì €ì¥
            doc = Document(
                page_content="\n\n".join(current_chunk),
                metadata={
                    "page": current_page,
                    "chunk_type": "section"
                }
            )
            documents.append(doc)
            current_chunk = []
            current_size = 0
        
        # í˜„ì¬ ë‹¨ë½ ì¶”ê°€
        current_chunk.append(content)
        current_size += len(content)
        current_page = page_num
        
        # ìµœëŒ€ í¬ê¸° ì´ˆê³¼ ì‹œ ì²­í¬ ë¶„í• 
        if current_size >= max_chunk_size:
            doc = Document(
                page_content="\n\n".join(current_chunk),
                metadata={
                    "page": current_page,
                    "chunk_type": "paragraph"
                }
            )
            documents.append(doc)
            current_chunk = []
            current_size = 0
    
    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk:
        doc = Document(
            page_content="\n\n".join(current_chunk),
            metadata={
                "page": current_page,
                "chunk_type": "paragraph"
            }
        )
        documents.append(doc)
    
    return documents

# ì‚¬ìš© ì˜ˆì‹œ
documents = split_by_layout(result, max_chunk_size=1000)
print(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(documents)}")
print(f"\nì²« ë²ˆì§¸ ì²­í¬ (ì• 300ì):\n{documents[0].page_content[:300]}")
```

## Azure Blob Storage ì—°ë™

### Blobì—ì„œ ì§ì ‘ ë¶„ì„

```{python}
from azure.storage.blob import BlobServiceClient

# Blob Storage í´ë¼ì´ì–¸íŠ¸
blob_service_client = BlobServiceClient.from_connection_string(
    os.getenv("AZURE_STORAGE_CONNECTION_STRING")
)

def analyze_blob_document(container_name: str, blob_name: str):
    """Blob Storageì˜ ë¬¸ì„œë¥¼ Document Intelligenceë¡œ ë¶„ì„"""
    
    # Blob SAS URL ìƒì„±
    from azure.storage.blob import generate_blob_sas, BlobSasPermissions
    from datetime import datetime, timedelta
    
    sas_token = generate_blob_sas(
        account_name="stragdocs2025",
        container_name=container_name,
        blob_name=blob_name,
        account_key=os.getenv("AZURE_STORAGE_KEY"),
        permission=BlobSasPermissions(read=True),
        expiry=datetime.utcnow() + timedelta(hours=1)
    )
    
    blob_url = f"https://stragdocs2025.blob.core.windows.net/{container_name}/{blob_name}?{sas_token}"
    
    # Document Intelligenceë¡œ ë¶„ì„
    poller = document_analysis_client.begin_analyze_document_from_url(
        "prebuilt-layout", document_url=blob_url
    )
    result = poller.result()
    
    return result

# ì‚¬ìš© ì˜ˆì‹œ
result = analyze_blob_document("rag-documents", "sample.pdf")
print("Blob ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ")
```

### ë°°ì¹˜ ì²˜ë¦¬

```{python}
def analyze_all_blobs(container_name: str):
    """ì»¨í…Œì´ë„ˆì˜ ëª¨ë“  ë¬¸ì„œ ë¶„ì„"""
    container_client = blob_service_client.get_container_client(container_name)
    blob_list = container_client.list_blobs()
    
    results = []
    for blob in blob_list:
        # PDF íŒŒì¼ë§Œ ì²˜ë¦¬
        if blob.name.endswith('.pdf'):
            print(f"ë¶„ì„ ì¤‘: {blob.name}")
            try:
                result = analyze_blob_document(container_name, blob.name)
                text = extract_full_text(result)
                
                results.append({
                    "blob_name": blob.name,
                    "page_count": len(result.pages),
                    "text": text,
                    "status": "success"
                })
            except Exception as e:
                print(f"ì˜¤ë¥˜: {blob.name} - {str(e)}")
                results.append({
                    "blob_name": blob.name,
                    "status": "error",
                    "error": str(e)
                })
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
# results = analyze_all_blobs("rag-documents")
# print(f"ì´ {len(results)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ")
```

## Read API ì‚¬ìš© (ê²½ëŸ‰ OCR)

ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œë§Œ í•„ìš”í•  ê²½ìš° Read APIê°€ ë” ë¹ ë¥´ê³  ì €ë ´í•˜ë‹¤.

```{python}
# Read APIë¡œ ë¬¸ì„œ ë¶„ì„
with open("simple_document.pdf", "rb") as f:
    poller = document_analysis_client.begin_analyze_document(
        "prebuilt-read", document=f
    )
    result = poller.result()

# í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ë¬´ì‹œ)
full_text = result.content
print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸:\n{full_text}")
```

**Layout vs Read ë¹„êµ:**
| í•­ëª© | Layout API | Read API |
|------|-----------|----------|
| **ê°€ê²©** | $0.01/í˜ì´ì§€ | $0.0015/í˜ì´ì§€ |
| **ì†ë„** | ëŠë¦¼ | ë¹ ë¦„ |
| **êµ¬ì¡° ì •ë³´** | âœ… (ì œëª©, í‘œ, ë‹¨ë½) | âŒ |
| **RAG ê¶Œì¥** | ë³µì¡í•œ ë¬¸ì„œ | ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¬¸ì„œ |

## í•œêµ­ì–´ ë¬¸ì„œ ìµœì í™”

### ì–¸ì–´ íŒíŠ¸ ì œê³µ

```{python}
# í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ (ì–¸ì–´ íŒíŠ¸)
with open("korean_document.pdf", "rb") as f:
    poller = document_analysis_client.begin_analyze_document(
        "prebuilt-layout",
        document=f,
        locale="ko-KR"  # í•œêµ­ì–´ íŒíŠ¸
    )
    result = poller.result()

print("í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ")
```

### í•œê¸€ OCR í›„ì²˜ë¦¬

```{python}
import re

def clean_korean_text(text: str) -> str:
    """í•œê¸€ OCR ê²°ê³¼ ì •ë¦¬"""
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    # ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r'\n\s*\n', '\n\n', text)
    
    # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
    text = text.replace('ã€ƒ', '"').replace('ã€ƒ', '"')
    
    return text.strip()

# ì‚¬ìš© ì˜ˆì‹œ
raw_text = extract_full_text(result)
cleaned_text = clean_korean_text(raw_text)
print(f"ì •ë¦¬ëœ í…ìŠ¤íŠ¸:\n{cleaned_text[:500]}")
```

## LangChain í†µí•©

### AzureAIDocumentIntelligenceLoader

```{python}
from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader

# Document Intelligence Loader ìƒì„±
loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint,
    api_key=key,
    file_path="sample.pdf",
    api_model="prebuilt-layout"
)

# ë¬¸ì„œ ë¡œë”©
documents = loader.load()

print(f"ë¡œë”©ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
print(f"ì²« ë²ˆì§¸ ì²­í¬:\n{documents[0].page_content[:300]}")
print(f"ë©”íƒ€ë°ì´í„°: {documents[0].metadata}")
```

### RAG íŒŒì´í”„ë¼ì¸ í†µí•©

```{python}
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Document Intelligenceë¡œ ë¬¸ì„œ ë¡œë”©
loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint,
    api_key=key,
    file_path="long_document.pdf",
    api_model="prebuilt-layout"
)
documents = loader.load()

# í…ìŠ¤íŠ¸ ë¶„í•  (ë ˆì´ì•„ì›ƒ ê³ ë ¤)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]
)
splits = text_splitter.split_documents(documents)

print(f"ì´ {len(splits)}ê°œ ì²­í¬ ìƒì„±")
```

## ë¹„ìš© ìµœì í™”

### ìºì‹± ì „ëµ

```{python}
import json
import hashlib

def get_cache_key(file_path: str) -> str:
    """íŒŒì¼ í•´ì‹œë¡œ ìºì‹œ í‚¤ ìƒì„±"""
    with open(file_path, "rb") as f:
        file_hash = hashlib.md5(f.read()).hexdigest()
    return f"doc_intel_{file_hash}"

def analyze_with_cache(file_path: str):
    """ìºì‹±ì„ ì‚¬ìš©í•œ ë¬¸ì„œ ë¶„ì„"""
    cache_key = get_cache_key(file_path)
    cache_file = f".cache/{cache_key}.json"
    
    # ìºì‹œ í™•ì¸
    try:
        with open(cache_file, "r", encoding="utf-8") as f:
            cached_result = json.load(f)
            print("ìºì‹œì—ì„œ ê²°ê³¼ ë¡œë”©")
            return cached_result
    except FileNotFoundError:
        pass
    
    # Document Intelligence ì‹¤í–‰
    with open(file_path, "rb") as f:
        poller = document_analysis_client.begin_analyze_document(
            "prebuilt-layout", document=f
        )
        result = poller.result()
    
    # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    result_dict = {
        "content": result.content,
        "pages": [{"page_number": p.page_number, "width": p.width, "height": p.height} for p in result.pages],
        "paragraphs": [{"content": p.content, "role": p.role} for p in result.paragraphs]
    }
    
    # ìºì‹œ ì €ì¥
    os.makedirs(".cache", exist_ok=True)
    with open(cache_file, "w", encoding="utf-8") as f:
        json.dump(result_dict, f, ensure_ascii=False, indent=2)
    
    print("Document Intelligence ì‹¤í–‰ ë° ìºì‹œ ì €ì¥")
    return result_dict

# ì‚¬ìš© ì˜ˆì‹œ
# cached_result = analyze_with_cache("sample.pdf")
```

### ë°°ì¹˜ í¬ê¸° ì¡°ì •

```{python}
def analyze_documents_batch(file_paths: List[str], batch_size: int = 5):
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¸ì„œ ë¶„ì„ (API ì œí•œ ê³ ë ¤)"""
    import time
    
    results = []
    for i in range(0, len(file_paths), batch_size):
        batch = file_paths[i:i+batch_size]
        
        print(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ íŒŒì¼)")
        for file_path in batch:
            with open(file_path, "rb") as f:
                poller = document_analysis_client.begin_analyze_document(
                    "prebuilt-layout", document=f
                )
                result = poller.result()
                results.append({
                    "file": file_path,
                    "result": result
                })
        
        # API ì œí•œ ë°©ì§€ (ì´ˆë‹¹ 15 ìš”ì²­)
        time.sleep(1)
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
# file_list = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
# results = analyze_documents_batch(file_list, batch_size=5)
```

## ëª¨ë‹ˆí„°ë§

### ë¶„ì„ í†µê³„ ì¶”ì 

```{python}
def analyze_with_metrics(file_path: str):
    """ë¶„ì„ ë©”íŠ¸ë¦­ ì¶”ì """
    import time
    
    start_time = time.time()
    
    with open(file_path, "rb") as f:
        file_size = os.path.getsize(file_path)
        
        poller = document_analysis_client.begin_analyze_document(
            "prebuilt-layout", document=f
        )
        result = poller.result()
    
    end_time = time.time()
    duration = end_time - start_time
    
    metrics = {
        "file_path": file_path,
        "file_size_mb": file_size / (1024 * 1024),
        "page_count": len(result.pages),
        "duration_seconds": duration,
        "pages_per_second": len(result.pages) / duration
    }
    
    print(f"ë¶„ì„ ì™„ë£Œ:")
    print(f"- íŒŒì¼ í¬ê¸°: {metrics['file_size_mb']:.2f} MB")
    print(f"- í˜ì´ì§€ ìˆ˜: {metrics['page_count']}")
    print(f"- ì²˜ë¦¬ ì‹œê°„: {metrics['duration_seconds']:.2f}ì´ˆ")
    print(f"- ì†ë„: {metrics['pages_per_second']:.2f} í˜ì´ì§€/ì´ˆ")
    
    return result, metrics

# ì‚¬ìš© ì˜ˆì‹œ
# result, metrics = analyze_with_metrics("large_document.pdf")
```

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜

**InvalidRequest: íŒŒì¼ í¬ê¸° ì´ˆê³¼**
```
ì˜¤ë¥˜: íŒŒì¼ì´ ë„ˆë¬´ í¼ (ìµœëŒ€ 500MB)
í•´ê²°: íŒŒì¼ì„ ë¶„í• í•˜ê±°ë‚˜ ì••ì¶•
```

**InvalidImage: ì´ë¯¸ì§€ í•´ìƒë„ ë¶€ì¡±**
```python
# í•´ê²°: ì´ë¯¸ì§€ í’ˆì§ˆ í™•ì¸ (ìµœì†Œ 150 DPI ê¶Œì¥)
from PIL import Image

img = Image.open("low_quality.png")
print(f"í•´ìƒë„: {img.size}")
print(f"DPI: {img.info.get('dpi', 'Unknown')}")
```

**Unauthorized: ì¸ì¦ ì‹¤íŒ¨**
```python
# í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
print(f"Endpoint: {endpoint}")
print(f"Key: {key[:10]}... (ê¸¸ì´: {len(key)})")
```

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Document Intelligence ê°œìš”](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/)
- [Python SDK ì°¸ì¡°](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-formrecognizer-readme)
- [ëª¨ë¸ ê°€ì´ë“œ](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-model-overview)

### ìƒ˜í”Œ ì½”ë“œ
- [Document Intelligence Samples](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/formrecognizer/azure-ai-formrecognizer/samples)
- [LangChain í†µí•©](https://python.langchain.com/docs/integrations/document_loaders/azure_document_intelligence)

## ë‹¤ìŒ ë‹¨ê³„

ë¬¸ì„œ OCR ë° ë ˆì´ì•„ì›ƒ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆë‹¤ë©´, ì´ì œ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì:

ğŸ‘‰ [03-Azure-OpenAI-Embeddings.qmd](./03-Azure-OpenAI-Embeddings.qmd) - Azure OpenAIë¡œ ë¬¸ì„œ ì„ë² ë”© ìƒì„±