---
title: "Document Intelligence for RAG"
subtitle: "RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ê³ ê¸‰ ë¬¸ì„œ ì²˜ë¦¬"
description: |
  Azure Document Intelligenceì˜ JSON ì¶œë ¥ êµ¬ì¡°ë¥¼ í™œìš©í•œ RAG ìµœì í™”, ì§€ëŠ¥í˜• ì²­í‚¹, ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° Agent ì‘ìš© ë°©ë²•ì„ ì„¤ëª…í•œë‹¤.
categories:
  - AI
  - RAG
  - Azure
  - Agent
author: Kwangmin Kim
date: 11/03/2025
format: 
  html:
    code-fold: true
    toc: true
    number-sections: true
draft: False
execute:
    eval: false
---

## RAGë¥¼ ìœ„í•œ Document Intelligence

> **ê¸°ëŠ¥ ì„¤ëª…, ê¸°ë³¸ ì„¤ì • ë° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±**ì€ [20-az-document-intelligence.qmd](../../Engineering/Infra/Cloud/Azure/20-az-document-intelligence.qmd)ë¥¼ ì°¸ê³ í•œë‹¤.

### RAG ì‹œìŠ¤í…œì—ì„œì˜ í•µì‹¬ ê°€ì¹˜

Document IntelligenceëŠ” ë‹¨ìˆœ OCRì„ ë„˜ì–´ **ë¬¸ì„œ êµ¬ì¡° ì´í•´**ë¥¼ ì œê³µí•œë‹¤. ì´ëŠ” RAG ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ ì •í™•ë„ì™€ ë‹µë³€ í’ˆì§ˆì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.

**ì¼ë°˜ OCR vs Document Intelligence (RAG ê´€ì )**

| ì¸¡ë©´ | ì¼ë°˜ OCR | Document Intelligence |
|------|---------|----------------------|
| **í…ìŠ¤íŠ¸ ì¶”ì¶œ** | âœ… ê°€ëŠ¥ | âœ… ê°€ëŠ¥ |
| **ë¬¸ì„œ êµ¬ì¡°** | âŒ ë¯¸ì§€ì› | âœ… ì œëª©, ë‹¨ë½, í‘œ êµ¬ë¶„ |
| **ì˜ë¯¸ì  ì²­í‚¹** | âŒ ë¶ˆê°€ëŠ¥ | âœ… ì„¹ì…˜ ë‹¨ìœ„ ë¶„í•  |
| **í‘œ ë°ì´í„°** | í…ìŠ¤íŠ¸ë¡œ ë­‰ê°œì§ | êµ¬ì¡°í™”ëœ JSON |
| **ë©”íƒ€ë°ì´í„°** | ì—†ìŒ | í˜ì´ì§€, ì¢Œí‘œ, ì—­í•  |
| **RAG ê²€ìƒ‰ í’ˆì§ˆ** | ë‚®ìŒ | ë†’ìŒ |

### RAG íŒŒì´í”„ë¼ì¸ì—ì„œì˜ ì—­í• 

```
ë¬¸ì„œ â†’ Document Intelligence â†’ êµ¬ì¡°í™”ëœ JSON â†’ ì§€ëŠ¥í˜• ì²­í‚¹ â†’ Vector DB â†’ RAG ê²€ìƒ‰
```

**í•µì‹¬ ì°¨ë³„ì :**
1. **ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• **: ë‹¨ë½/ì„¹ì…˜ ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ ìƒì„± â†’ ë¬¸ë§¥ ë³´ì¡´
2. **í‘œ ë°ì´í„° ë³´ì¡´**: DataFrameìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥ â†’ ìˆ«ì ë°ì´í„° ì •í™•ë„ í–¥ìƒ
3. **ê³„ì¸µì  ë©”íƒ€ë°ì´í„°**: ì œëª©-ë‹¨ë½ ê´€ê³„ íŒŒì•… â†’ ê´€ë ¨ ì„¹ì…˜ ê²€ìƒ‰
4. **ë‹¤ì¤‘ ëª¨ë“œ ê²€ìƒ‰**: í…ìŠ¤íŠ¸ + í‘œ + ì´ë¯¸ì§€ ìº¡ì…˜ í†µí•©

## Document Intelligence ëª¨ë¸

Azure Document IntelligenceëŠ” ë¬¸ì„œ ìœ í˜•ì— ë”°ë¼ ë‹¤ì–‘í•œ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ì„ ì œê³µí•œë‹¤:

### Layout API (ë²”ìš© ë ˆì´ì•„ì›ƒ ë¶„ì„)
**ìš©ë„**: ëª¨ë“  ì¢…ë¥˜ì˜ ë¬¸ì„œ ë ˆì´ì•„ì›ƒ ë¶„ì„

**ì¶”ì¶œ ì •ë³´:**
- í…ìŠ¤íŠ¸ (OCR)
- í‘œ (í–‰, ì—´ êµ¬ì¡°)
- ì œëª© ë° ì„¹ì…˜
- ë‹¨ë½ ë° ì¤„ë°”ê¿ˆ
- ì„ íƒ ë§ˆí¬ (ì²´í¬ë°•ìŠ¤)

**RAG ì‹œìŠ¤í…œ ê¶Œì¥**: ê°€ì¥ ë§ì´ ì‚¬ìš©

**ê°€ê²©**: í˜ì´ì§€ë‹¹ $0.01

### Read API (í…ìŠ¤íŠ¸ ì¶”ì¶œ ì „ìš©)
**ìš©ë„**: ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ë¬´ì‹œ)

**ì¶”ì¶œ ì •ë³´:**
- í…ìŠ¤íŠ¸ë§Œ (êµ¬ì¡° ì •ë³´ ì—†ìŒ)
- ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„

**RAG ì‹œìŠ¤í…œ ê¶Œì¥**: ê°„ë‹¨í•œ ë¬¸ì„œ, ë¹„ìš© ì ˆê° í•„ìš” ì‹œ

**ê°€ê²©**: í˜ì´ì§€ë‹¹ $0.0015

### Prebuilt Models (íŠ¹í™” ëª¨ë¸)
| ëª¨ë¸ | ìš©ë„ | ì¶”ì¶œ ì •ë³´ |
|------|------|-----------|
| **Invoice** | ì²­êµ¬ì„œ, ì„¸ê¸ˆê³„ì‚°ì„œ | ë‚ ì§œ, ê¸ˆì•¡, í’ˆëª©, ê³µê¸‰ì |
| **Receipt** | ì˜ìˆ˜ì¦ | ìƒì ëª…, ë‚ ì§œ, ì´ì•¡, í•­ëª© |
| **ID Document** | ì‹ ë¶„ì¦, ì—¬ê¶Œ | ì´ë¦„, ìƒë…„ì›”ì¼, ì£¼ì†Œ |
| **Business Card** | ëª…í•¨ | ì´ë¦„, ì§í•¨, ì—°ë½ì²˜ |
| **W-2** | ë¯¸êµ­ ì„¸ê¸ˆ ì–‘ì‹ | ê¸‰ì—¬, ì„¸ê¸ˆ |

**RAG ì‹œìŠ¤í…œ ê¶Œì¥**: íŠ¹ì • ë¬¸ì„œ íƒ€ì…ë§Œ ì²˜ë¦¬ ì‹œ ìœ ìš©

## ë¹ ë¥¸ ì‹œì‘ (ë¦¬ì†ŒìŠ¤ ìƒì„±)

> **ìƒì„¸ ì„¤ì • ê°€ì´ë“œ**: [20-az-document-intelligence.qmd](../../Engineering/Infra/Cloud/Azure/20-az-document-intelligence.qmd)

**RAG í”„ë¡œì íŠ¸ìš© ë¦¬ì†ŒìŠ¤ ìƒì„± (Azure CLI):**

```bash
# Document Intelligence ë¦¬ì†ŒìŠ¤ ìƒì„±
az cognitiveservices account create \
    --name doc-intel-rag-prod \
    --resource-group rg-rag-prod \
    --kind FormRecognizer \
    --sku S0 \
    --location koreacentral \
    --yes

# í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ ì¡°íšŒ
az cognitiveservices account keys list \
    --name doc-intel-rag-prod \
    --resource-group rg-rag-prod
```

**RAG í”„ë¡œì íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Standard S0 ê³„ì¸µ ì„ íƒ (F0ëŠ” Layout API ì œí•œì )
- [ ] Korea Central ë¦¬ì „ (í•œêµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬ ìµœì í™”)
- [ ] í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ì €ì¥ (`.env` íŒŒì¼)
- [ ] Blob Storageì™€ ë™ì¼ ë¦¬ì „ ë°°ì¹˜ (ë„¤íŠ¸ì›Œí¬ ë¹„ìš© ì ˆê°)

## í™˜ê²½ ì„¤ì •

### Python SDK ì„¤ì¹˜

```bash
# Document Intelligence + RAG íŒŒì´í”„ë¼ì¸ í•„ìˆ˜ íŒ¨í‚¤ì§€
pip install azure-ai-formrecognizer
pip install azure-storage-blob
pip install langchain langchain-community
pip install python-dotenv
pip install pandas  # í‘œ ë°ì´í„° ì²˜ë¦¬ìš©
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼:
```bash
# Document Intelligence
AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://doc-intel-rag-prod.cognitiveservices.azure.com/
AZURE_DOCUMENT_INTELLIGENCE_KEY=your-key-here

# Blob Storage (ë¬¸ì„œ ì €ì¥ì†Œ)
AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;...
AZURE_STORAGE_KEY=your-storage-key
```

## Layout API ì‚¬ìš©ë²•

### ë¡œì»¬ íŒŒì¼ ë¶„ì„

```{python}
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential
from dotenv import load_dotenv
import os

load_dotenv()

# í´ë¼ì´ì–¸íŠ¸ ìƒì„±
endpoint = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT")
key = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_KEY")

document_analysis_client = DocumentAnalysisClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(key)
)

# ë¡œì»¬ PDF íŒŒì¼ ë¶„ì„
with open("sample.pdf", "rb") as f:
    poller = document_analysis_client.begin_analyze_document(
        "prebuilt-layout", document=f
    )
    result = poller.result()

# ê²°ê³¼ ì¶œë ¥
print(f"ë¶„ì„ëœ í˜ì´ì§€ ìˆ˜: {len(result.pages)}")
print(f"ì¶”ì¶œëœ ë‹¨ë½ ìˆ˜: {len(result.paragraphs)}")
print(f"ì¶”ì¶œëœ í‘œ ìˆ˜: {len(result.tables)}")
```

### URLì—ì„œ ë¬¸ì„œ ë¶„ì„

```{python}
# Azure Blob Storage URLë¡œ ë¶„ì„
document_url = "https://stragdocs2025.blob.core.windows.net/rag-documents/sample.pdf?<sas-token>"

poller = document_analysis_client.begin_analyze_document_from_url(
    "prebuilt-layout", document_url=document_url
)
result = poller.result()

print("ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ")
```

## í…ìŠ¤íŠ¸ ì¶”ì¶œ

### ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ

```{python}
def extract_full_text(result):
    """ë¬¸ì„œì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    full_text = []
    
    # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    for page in result.pages:
        page_text = []
        for line in page.lines:
            page_text.append(line.content)
        
        full_text.append("\n".join(page_text))
    
    return "\n\n".join(full_text)

# ì‚¬ìš© ì˜ˆì‹œ
text = extract_full_text(result)
print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ì• 500ì):\n{text[:500]}")
```

### ë‹¨ë½ ë‹¨ìœ„ ì¶”ì¶œ

```{python}
def extract_paragraphs(result):
    """ë‹¨ë½ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ìœ ì§€)"""
    paragraphs = []
    
    for paragraph in result.paragraphs:
        paragraphs.append({
            "content": paragraph.content,
            "role": paragraph.role,  # title, sectionHeading, paragraph ë“±
            "page_number": paragraph.bounding_regions[0].page_number if paragraph.bounding_regions else None
        })
    
    return paragraphs

# ì‚¬ìš© ì˜ˆì‹œ
paragraphs = extract_paragraphs(result)
print(f"ì´ {len(paragraphs)}ê°œ ë‹¨ë½ ì¶”ì¶œ")

# ì œëª©ë§Œ ì¶”ì¶œ
titles = [p for p in paragraphs if p["role"] == "title"]
print(f"\në¬¸ì„œ ì œëª©ë“¤:")
for title in titles:
    print(f"- {title['content']} (í˜ì´ì§€ {title['page_number']})")
```

## í‘œ ì¶”ì¶œ

### í‘œ êµ¬ì¡° íŒŒì‹±

```{python}
def extract_tables(result):
    """ë¬¸ì„œì—ì„œ í‘œ ì¶”ì¶œ"""
    tables_data = []
    
    for table_idx, table in enumerate(result.tables):
        # í‘œ ë©”íƒ€ë°ì´í„°
        table_info = {
            "table_id": table_idx + 1,
            "row_count": table.row_count,
            "column_count": table.column_count,
            "page_number": table.bounding_regions[0].page_number,
            "cells": []
        }
        
        # ì…€ ë°ì´í„°
        for cell in table.cells:
            table_info["cells"].append({
                "row_index": cell.row_index,
                "column_index": cell.column_index,
                "content": cell.content,
                "kind": cell.kind  # columnHeader, rowHeader, content, stub
            })
        
        tables_data.append(table_info)
    
    return tables_data

# ì‚¬ìš© ì˜ˆì‹œ
tables = extract_tables(result)
print(f"ì¶”ì¶œëœ í‘œ ê°œìˆ˜: {len(tables)}")

for table in tables:
    print(f"\n[í‘œ {table['table_id']}] í˜ì´ì§€ {table['page_number']}")
    print(f"í¬ê¸°: {table['row_count']}í–‰ Ã— {table['column_count']}ì—´")
```

### í‘œë¥¼ Pandas DataFrameìœ¼ë¡œ ë³€í™˜

```{python}
import pandas as pd

def table_to_dataframe(table):
    """í‘œë¥¼ Pandas DataFrameìœ¼ë¡œ ë³€í™˜"""
    # í‘œ ì´ˆê¸°í™” (ë¹ˆ ì…€ í¬í•¨)
    data = [[None] * table["column_count"] for _ in range(table["row_count"])]
    
    # ì…€ ë°ì´í„° ì±„ìš°ê¸°
    for cell in table["cells"]:
        data[cell["row_index"]][cell["column_index"]] = cell["content"]
    
    # DataFrame ìƒì„± (ì²« í–‰ì„ í—¤ë”ë¡œ)
    df = pd.DataFrame(data[1:], columns=data[0])
    return df

# ì‚¬ìš© ì˜ˆì‹œ
if tables:
    df = table_to_dataframe(tables[0])
    print("\ní‘œ ë°ì´í„° (DataFrame):")
    print(df)
```

## ë ˆì´ì•„ì›ƒ ê¸°ë°˜ ë¬¸ì„œ ë¶„í• 

RAG ì‹œìŠ¤í…œì—ì„œëŠ” ë¬¸ì„œ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ì²­í¬ ë¶„í• ì´ ì¤‘ìš”í•˜ë‹¤.

```{python}
from langchain_core.documents import Document
from typing import List

def split_by_layout(result, max_chunk_size: int = 1000) -> List[Document]:
    """ë ˆì´ì•„ì›ƒ ê¸°ë°˜ ë¬¸ì„œ ë¶„í• """
    documents = []
    current_chunk = []
    current_size = 0
    current_page = 1
    
    for paragraph in result.paragraphs:
        # ë‹¨ë½ ì •ë³´
        content = paragraph.content
        role = paragraph.role or "paragraph"
        page_num = paragraph.bounding_regions[0].page_number if paragraph.bounding_regions else current_page
        
        # ì œëª©ì€ ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘
        if role in ["title", "sectionHeading"] and current_chunk:
            # ì´ì „ ì²­í¬ ì €ì¥
            doc = Document(
                page_content="\n\n".join(current_chunk),
                metadata={
                    "page": current_page,
                    "chunk_type": "section"
                }
            )
            documents.append(doc)
            current_chunk = []
            current_size = 0
        
        # í˜„ì¬ ë‹¨ë½ ì¶”ê°€
        current_chunk.append(content)
        current_size += len(content)
        current_page = page_num
        
        # ìµœëŒ€ í¬ê¸° ì´ˆê³¼ ì‹œ ì²­í¬ ë¶„í• 
        if current_size >= max_chunk_size:
            doc = Document(
                page_content="\n\n".join(current_chunk),
                metadata={
                    "page": current_page,
                    "chunk_type": "paragraph"
                }
            )
            documents.append(doc)
            current_chunk = []
            current_size = 0
    
    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk:
        doc = Document(
            page_content="\n\n".join(current_chunk),
            metadata={
                "page": current_page,
                "chunk_type": "paragraph"
            }
        )
        documents.append(doc)
    
    return documents

# ì‚¬ìš© ì˜ˆì‹œ
documents = split_by_layout(result, max_chunk_size=1000)
print(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(documents)}")
print(f"\nì²« ë²ˆì§¸ ì²­í¬ (ì• 300ì):\n{documents[0].page_content[:300]}")
```

## JSON ì¶œë ¥ êµ¬ì¡° ì™„ë²½ ë¶„ì„

Document Intelligenceì˜ JSON ì¶œë ¥ì€ RAG/Agent ì‹œìŠ¤í…œì—ì„œ í™œìš©í•  ìˆ˜ ìˆëŠ” í’ë¶€í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì œê³µí•œë‹¤.

### ì „ì²´ JSON êµ¬ì¡°

```{python}
import json

def analyze_json_structure(result):
    """Document Intelligence ê²°ê³¼ì˜ JSON êµ¬ì¡° ë¶„ì„"""
    
    # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    result_dict = {
        "api_version": result.api_version,
        "model_id": result.model_id,
        "content": result.content[:200] + "...",  # ì „ì²´ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
        
        # í˜ì´ì§€ ì •ë³´
        "pages_info": {
            "count": len(result.pages),
            "details": [
                {
                    "page_number": page.page_number,
                    "width": page.width,
                    "height": page.height,
                    "unit": page.unit,
                    "angle": page.angle,
                    "lines_count": len(page.lines),
                    "words_count": len(page.words) if hasattr(page, 'words') else 0
                }
                for page in result.pages
            ]
        },
        
        # ë‹¨ë½ ì •ë³´
        "paragraphs_info": {
            "count": len(result.paragraphs),
            "roles": {},  # ì—­í• ë³„ ì¹´ìš´íŠ¸
            "sample": []
        },
        
        # í‘œ ì •ë³´
        "tables_info": {
            "count": len(result.tables),
            "details": [
                {
                    "table_id": idx + 1,
                    "row_count": table.row_count,
                    "column_count": table.column_count,
                    "cell_count": len(table.cells),
                    "page": table.bounding_regions[0].page_number if table.bounding_regions else None
                }
                for idx, table in enumerate(result.tables)
            ]
        }
    }
    
    # ë‹¨ë½ ì—­í• ë³„ í†µê³„
    for para in result.paragraphs:
        role = para.role or "paragraph"
        result_dict["paragraphs_info"]["roles"][role] = \
            result_dict["paragraphs_info"]["roles"].get(role, 0) + 1
        
        # ìƒ˜í”Œ 3ê°œ
        if len(result_dict["paragraphs_info"]["sample"]) < 3:
            result_dict["paragraphs_info"]["sample"].append({
                "role": role,
                "content": para.content[:100] + "...",
                "page": para.bounding_regions[0].page_number if para.bounding_regions else None
            })
    
    return result_dict

# ì‚¬ìš© ì˜ˆì‹œ
# json_structure = analyze_json_structure(result)
# print(json.dumps(json_structure, indent=2, ensure_ascii=False))
```

**JSON ì¶œë ¥ ì˜ˆì‹œ:**
```json
{
  "api_version": "2023-07-31",
  "model_id": "prebuilt-layout",
  "content": "ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸...",
  "pages_info": {
    "count": 10,
    "details": [
      {"page_number": 1, "width": 8.5, "height": 11, "unit": "inch", "angle": 0, "lines_count": 45}
    ]
  },
  "paragraphs_info": {
    "count": 87,
    "roles": {"title": 5, "sectionHeading": 12, "paragraph": 70},
    "sample": [...]
  },
  "tables_info": {
    "count": 3,
    "details": [
      {"table_id": 1, "row_count": 10, "column_count": 4, "page": 3}
    ]
  }
}
```

### BoundingRegion (ì¢Œí‘œ ì •ë³´) í™œìš©

```{python}
def extract_bounding_info(paragraph):
    """ë‹¨ë½ì˜ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ"""
    if not paragraph.bounding_regions:
        return None
    
    region = paragraph.bounding_regions[0]
    
    return {
        "page_number": region.page_number,
        "polygon": region.polygon,  # [x1, y1, x2, y2, x3, y3, x4, y4]
        "bbox": {
            "x_min": min(region.polygon[::2]),
            "y_min": min(region.polygon[1::2]),
            "x_max": max(region.polygon[::2]),
            "y_max": max(region.polygon[1::2])
        }
    }

# RAG ë©”íƒ€ë°ì´í„°ë¡œ í™œìš©
for para in result.paragraphs:
    location = extract_bounding_info(para)
    if location:
        # Vector DBì— ì €ì¥ ì‹œ ë©”íƒ€ë°ì´í„°ë¡œ ì¶”ê°€
        metadata = {
            "page": location["page_number"],
            "bbox": location["bbox"],
            "role": para.role
        }
```

**ì¢Œí‘œ ì •ë³´ í™œìš© ì‚¬ë¡€:**
1. **ë¬¸ì„œ ì›ë³¸ í•˜ì´ë¼ì´íŒ…**: ê²€ìƒ‰ ê²°ê³¼ì˜ ì •í™•í•œ ìœ„ì¹˜ í‘œì‹œ
2. **ì´ë¯¸ì§€ ì˜ì—­ ì¶”ì¶œ**: OCR í…ìŠ¤íŠ¸ì™€ ì›ë³¸ ì´ë¯¸ì§€ ë§¤ì¹­
3. **ë ˆì´ì•„ì›ƒ ê¸°ë°˜ í•„í„°ë§**: ë¬¸ì„œ ìƒë‹¨/í•˜ë‹¨ ì œì™¸ (í—¤ë”/í‘¸í„° ì œê±°)
4. **ì»¬ëŸ¼ ë¶„ë¦¬**: 2ë‹¨ ë ˆì´ì•„ì›ƒ ë¬¸ì„œì˜ ì¢Œìš° ì»¬ëŸ¼ êµ¬ë¶„

### Style ì •ë³´ (í°íŠ¸, ìƒ‰ìƒ)

```{python}
def extract_styles(result):
    """í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ ì •ë³´ ì¶”ì¶œ (Formula Add-on í™œì„±í™” í•„ìš”)"""
    if not hasattr(result, 'styles') or not result.styles:
        return []
    
    style_info = []
    for style in result.styles:
        style_info.append({
            "is_handwritten": style.is_handwritten,
            "confidence": style.confidence,
            "spans": style.spans  # í…ìŠ¤íŠ¸ ë²”ìœ„
        })
    
    return style_info

# Agent ì‘ìš©: ì†ê¸€ì”¨ ì˜ì—­ë§Œ ë³„ë„ ì²˜ë¦¬
# handwritten_sections = [s for s in extract_styles(result) if s["is_handwritten"]]
```

## ê³ ê¸‰ RAG ì‘ìš© íŒ¨í„´

### 1. ê³„ì¸µì  ë¬¸ì„œ ì¸ë±ì‹±

ë¬¸ì„œ êµ¬ì¡°ë¥¼ í™œìš©í•œ ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì‹œìŠ¤í…œì´ë‹¤.

```{python}
from typing import Dict, List
from langchain_core.documents import Document

def create_hierarchical_index(result) -> Dict[str, List[Document]]:
    """ë¬¸ì„œë¥¼ ê³„ì¸µ êµ¬ì¡°ë¡œ ì¸ë±ì‹±"""
    
    hierarchy = {
        "titles": [],
        "sections": [],
        "paragraphs": [],
        "tables": []
    }
    
    current_title = None
    current_section = None
    
    for para in result.paragraphs:
        content = para.content
        role = para.role or "paragraph"
        page = para.bounding_regions[0].page_number if para.bounding_regions else 1
        
        if role == "title":
            current_title = content
            hierarchy["titles"].append(Document(
                page_content=content,
                metadata={"type": "title", "page": page}
            ))
        
        elif role == "sectionHeading":
            current_section = content
            hierarchy["sections"].append(Document(
                page_content=content,
                metadata={
                    "type": "section",
                    "parent_title": current_title,
                    "page": page
                }
            ))
        
        else:  # paragraph
            hierarchy["paragraphs"].append(Document(
                page_content=content,
                metadata={
                    "type": "paragraph",
                    "parent_title": current_title,
                    "parent_section": current_section,
                    "page": page
                }
            ))
    
    # í‘œ ë°ì´í„° ì¶”ê°€
    for table_idx, table in enumerate(result.tables):
        # í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        table_md = table_to_markdown(table)
        
        hierarchy["tables"].append(Document(
            page_content=table_md,
            metadata={
                "type": "table",
                "table_id": table_idx + 1,
                "rows": table.row_count,
                "columns": table.column_count,
                "page": table.bounding_regions[0].page_number if table.bounding_regions else 1
            }
        ))
    
    return hierarchy

def table_to_markdown(table) -> str:
    """í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    # ì…€ì„ 2D ë°°ì—´ë¡œ êµ¬ì„±
    grid = [[None] * table.column_count for _ in range(table.row_count)]
    
    for cell in table.cells:
        grid[cell.row_index][cell.column_index] = cell.content
    
    # ë§ˆí¬ë‹¤ìš´ ìƒì„±
    md_lines = []
    md_lines.append("| " + " | ".join(grid[0]) + " |")
    md_lines.append("|" + "|".join(["---"] * table.column_count) + "|")
    
    for row in grid[1:]:
        md_lines.append("| " + " | ".join(row) + " |")
    
    return "\n".join(md_lines)

# ì‚¬ìš© ì˜ˆì‹œ
# hierarchy = create_hierarchical_index(result)
# print(f"ì œëª©: {len(hierarchy['titles'])}ê°œ")
# print(f"ì„¹ì…˜: {len(hierarchy['sections'])}ê°œ")
# print(f"ë‹¨ë½: {len(hierarchy['paragraphs'])}ê°œ")
# print(f"í‘œ: {len(hierarchy['tables'])}ê°œ")
```

**RAG ê²€ìƒ‰ ì „ëµ:**
1. **1ë‹¨ê³„**: ì œëª©/ì„¹ì…˜ ê²€ìƒ‰ â†’ ê´€ë ¨ ì„¹ì…˜ ì‹ë³„
2. **2ë‹¨ê³„**: í•´ë‹¹ ì„¹ì…˜ ë‚´ ë‹¨ë½ ê²€ìƒ‰ â†’ ì •í™•í•œ ë‹µë³€ ìœ„ì¹˜
3. **3ë‹¨ê³„**: ê´€ë ¨ í‘œ ë°ì´í„° ì¡°íšŒ â†’ ìˆ«ì ì •ë³´ ë³´ê°•

### 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í…ìŠ¤íŠ¸ + í‘œ)

```{python}
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

def create_hybrid_vectorstore(hierarchy: Dict[str, List[Document]]):
    """í…ìŠ¤íŠ¸ì™€ í‘œë¥¼ ë³„ë„ ë²¡í„°ìŠ¤í† ì–´ë¡œ ìƒì„±"""
    
    embeddings = OpenAIEmbeddings()
    
    # í…ìŠ¤íŠ¸ ë²¡í„°ìŠ¤í† ì–´ (ë‹¨ë½ë§Œ)
    text_vectorstore = FAISS.from_documents(
        hierarchy["paragraphs"],
        embeddings
    )
    
    # í‘œ ë²¡í„°ìŠ¤í† ì–´ (í‘œë§Œ)
    table_vectorstore = FAISS.from_documents(
        hierarchy["tables"],
        embeddings
    ) if hierarchy["tables"] else None
    
    return {
        "text": text_vectorstore,
        "table": table_vectorstore
    }

def hybrid_search(query: str, vectorstores: Dict, k: int = 5):
    """í…ìŠ¤íŠ¸ì™€ í‘œë¥¼ ë™ì‹œ ê²€ìƒ‰"""
    
    results = {
        "text": vectorstores["text"].similarity_search(query, k=k),
        "table": vectorstores["table"].similarity_search(query, k=k//2) if vectorstores["table"] else []
    }
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
# vectorstores = create_hybrid_vectorstore(hierarchy)
# results = hybrid_search("2023ë…„ ë§¤ì¶œì€?", vectorstores)
# 
# print("í…ìŠ¤íŠ¸ ê²°ê³¼:", results["text"])
# print("í‘œ ê²°ê³¼:", results["table"])
```

### 3. Agentìš© ë„êµ¬ í•¨ìˆ˜

Document Intelligence ê²°ê³¼ë¥¼ Agentì˜ ë„êµ¬ë¡œ ì œê³µí•œë‹¤.

```{python}
from langchain.agents import tool

@tool
def search_document_by_section(section_name: str, result) -> str:
    """íŠ¹ì • ì„¹ì…˜ì˜ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬"""
    
    matching_paragraphs = []
    in_target_section = False
    
    for para in result.paragraphs:
        role = para.role or "paragraph"
        
        # ì„¹ì…˜ ì‹œì‘ ê°ì§€
        if role == "sectionHeading" and section_name.lower() in para.content.lower():
            in_target_section = True
            continue
        
        # ë‹¤ìŒ ì„¹ì…˜ ì‹œì‘ ì‹œ ì¢…ë£Œ
        if role == "sectionHeading" and in_target_section:
            break
        
        # ëŒ€ìƒ ì„¹ì…˜ì˜ ë‹¨ë½ ìˆ˜ì§‘
        if in_target_section:
            matching_paragraphs.append(para.content)
    
    return "\n\n".join(matching_paragraphs) if matching_paragraphs else "í•´ë‹¹ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

@tool
def extract_tables_from_page(page_number: int, result) -> str:
    """íŠ¹ì • í˜ì´ì§€ì˜ í‘œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ë„êµ¬"""
    
    page_tables = []
    for table in result.tables:
        if table.bounding_regions and table.bounding_regions[0].page_number == page_number:
            table_md = table_to_markdown(table)
            page_tables.append(table_md)
    
    return "\n\n".join(page_tables) if page_tables else f"í˜ì´ì§€ {page_number}ì— í‘œê°€ ì—†ìŠµë‹ˆë‹¤."

@tool
def get_document_outline(result) -> str:
    """ë¬¸ì„œ ëª©ì°¨ ìƒì„± ë„êµ¬"""
    
    outline = []
    for para in result.paragraphs:
        if para.role in ["title", "sectionHeading"]:
            page = para.bounding_regions[0].page_number if para.bounding_regions else "?"
            outline.append(f"[í˜ì´ì§€ {page}] {para.content}")
    
    return "\n".join(outline)

# Agentì— ë„êµ¬ ë“±ë¡
# tools = [search_document_by_section, extract_tables_from_page, get_document_outline]
```

**Agent ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ:**
```
ì‚¬ìš©ì: "2ì¥ì˜ ë§¤ì¶œ í‘œë¥¼ ìš”ì•½í•´ì¤˜"

Agent ì‹¤í–‰:
1. get_document_outline() â†’ "2ì¥"ì˜ í˜ì´ì§€ ë²ˆí˜¸ í™•ì¸ (ì˜ˆ: 5í˜ì´ì§€)
2. extract_tables_from_page(5) â†’ í•´ë‹¹ í˜ì´ì§€ í‘œ ì¶”ì¶œ
3. LLMìœ¼ë¡œ í‘œ ë‚´ìš© ìš”ì•½
```

### 4. ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§

```{python}
def create_filtered_chunks(result, filter_config: Dict) -> List[Document]:
    """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§"""
    
    documents = []
    
    for para in result.paragraphs:
        # í•„í„° ì¡°ê±´ ì²´í¬
        role = para.role or "paragraph"
        page = para.bounding_regions[0].page_number if para.bounding_regions else 1
        
        # ì—­í•  í•„í„°
        if "allowed_roles" in filter_config:
            if role not in filter_config["allowed_roles"]:
                continue
        
        # í˜ì´ì§€ í•„í„°
        if "page_range" in filter_config:
            start, end = filter_config["page_range"]
            if not (start <= page <= end):
                continue
        
        # ê¸¸ì´ í•„í„°
        if "min_length" in filter_config:
            if len(para.content) < filter_config["min_length"]:
                continue
        
        # ì¡°ê±´ í†µê³¼ ì‹œ ë¬¸ì„œ ìƒì„±
        doc = Document(
            page_content=para.content,
            metadata={
                "role": role,
                "page": page,
                "char_count": len(para.content)
            }
        )
        documents.append(doc)
    
    return documents

# ì‚¬ìš© ì˜ˆì‹œ
filter_config = {
    "allowed_roles": ["paragraph", "sectionHeading"],  # ì œëª© ì œì™¸
    "page_range": (1, 50),  # 1-50í˜ì´ì§€ë§Œ
    "min_length": 100  # 100ì ì´ìƒë§Œ
}

# filtered_docs = create_filtered_chunks(result, filter_config)
```

### 5. ì‹¤ì‹œê°„ ë¬¸ì„œ ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸

```{python}
import hashlib
from datetime import datetime

class DocumentIntelligenceCache:
    """ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, cache_dir: str = ".doc_intel_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_file_hash(self, file_path: str) -> str:
        """íŒŒì¼ í•´ì‹œ ìƒì„±"""
        with open(file_path, "rb") as f:
            return hashlib.sha256(f.read()).hexdigest()
    
    def is_cached(self, file_path: str) -> bool:
        """ìºì‹œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        file_hash = self.get_file_hash(file_path)
        cache_file = os.path.join(self.cache_dir, f"{file_hash}.json")
        return os.path.exists(cache_file)
    
    def get_cache(self, file_path: str):
        """ìºì‹œëœ ê²°ê³¼ ë¡œë“œ"""
        file_hash = self.get_file_hash(file_path)
        cache_file = os.path.join(self.cache_dir, f"{file_hash}.json")
        
        with open(cache_file, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def save_cache(self, file_path: str, result):
        """ë¶„ì„ ê²°ê³¼ ìºì‹±"""
        file_hash = self.get_file_hash(file_path)
        cache_file = os.path.join(self.cache_dir, f"{file_hash}.json")
        
        # ê²°ê³¼ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        result_dict = {
            "timestamp": datetime.now().isoformat(),
            "content": result.content,
            "paragraphs": [
                {"content": p.content, "role": p.role} 
                for p in result.paragraphs
            ],
            "tables": [
                {
                    "row_count": t.row_count,
                    "column_count": t.column_count,
                    "cells": [
                        {"row": c.row_index, "col": c.column_index, "content": c.content}
                        for c in t.cells
                    ]
                }
                for t in result.tables
            ]
        }
        
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)

# ì‚¬ìš© ì˜ˆì‹œ
# cache = DocumentIntelligenceCache()
# 
# if cache.is_cached("report.pdf"):
#     result = cache.get_cache("report.pdf")
#     print("ìºì‹œì—ì„œ ë¡œë”©")
# else:
#     # Document Intelligence ì‹¤í–‰
#     result = analyze_document("report.pdf")
#     cache.save_cache("report.pdf", result)
#     print("ìƒˆë¡œ ë¶„ì„ ë° ìºì‹±")
```

## ì„±ëŠ¥ ìµœì í™” ë° ë¹„ìš© ê´€ë¦¬

### ë°°ì¹˜ ì²˜ë¦¬ ì „ëµ

```{python}
import time
from typing import List

def batch_analyze_documents(
    file_paths: List[str],
    batch_size: int = 5,
    delay_seconds: float = 0.2
) -> List[Dict]:
    """ë°°ì¹˜ ë‹¨ìœ„ ë¬¸ì„œ ë¶„ì„ (API ì œí•œ ê³ ë ¤)"""
    
    results = []
    
    for i in range(0, len(file_paths), batch_size):
        batch = file_paths[i:i+batch_size]
        
        print(f"\në°°ì¹˜ {i//batch_size + 1}/{(len(file_paths)-1)//batch_size + 1}")
        
        for file_path in batch:
            start_time = time.time()
            
            try:
                with open(file_path, "rb") as f:
                    poller = document_analysis_client.begin_analyze_document(
                        "prebuilt-layout", document=f
                    )
                    result = poller.result()
                
                results.append({
                    "file": file_path,
                    "status": "success",
                    "page_count": len(result.pages),
                    "processing_time": time.time() - start_time,
                    "result": result
                })
                
                print(f"âœ“ {file_path} ({len(result.pages)} í˜ì´ì§€, {time.time()-start_time:.2f}ì´ˆ)")
                
            except Exception as e:
                results.append({
                    "file": file_path,
                    "status": "error",
                    "error": str(e)
                })
                print(f"âœ— {file_path}: {str(e)}")
            
            # API ì œí•œ ë°©ì§€
            time.sleep(delay_seconds)
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
# file_list = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
# results = batch_analyze_documents(file_list, batch_size=3, delay_seconds=0.5)
```

**API ì œí•œ (S0 ê³„ì¸µ):**
- ì´ˆë‹¹ ìš”ì²­: 15 TPS
- ë¶„ë‹¹ ìš”ì²­: ì œí•œ ì—†ìŒ
- ë™ì‹œ ìš”ì²­: 10ê°œ

### ë¹„ìš© ëª¨ë‹ˆí„°ë§

```{python}
class CostTracker:
    """Document Intelligence ë¹„ìš© ì¶”ì """
    
    # ê°€ê²© (2025ë…„ ê¸°ì¤€, USD)
    PRICES = {
        "prebuilt-read": 1.50 / 1000,     # $1.50 per 1K pages
        "prebuilt-layout": 10.0 / 1000,   # $10 per 1K pages
        "prebuilt-invoice": 10.0 / 1000,
    }
    
    def __init__(self):
        self.usage = {model: 0 for model in self.PRICES.keys()}
    
    def track(self, model: str, page_count: int):
        """ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
        self.usage[model] += page_count
    
    def get_cost(self) -> Dict:
        """ë¹„ìš© ê³„ì‚°"""
        costs = {}
        total = 0
        
        for model, pages in self.usage.items():
            cost = pages * self.PRICES[model]
            costs[model] = {
                "pages": pages,
                "cost_usd": round(cost, 4)
            }
            total += cost
        
        costs["total_usd"] = round(total, 4)
        return costs
    
    def reset(self):
        """ì‚¬ìš©ëŸ‰ ì´ˆê¸°í™”"""
        self.usage = {model: 0 for model in self.PRICES.keys()}

# ì‚¬ìš© ì˜ˆì‹œ
tracker = CostTracker()

# ë¬¸ì„œ ë¶„ì„ í›„
# tracker.track("prebuilt-layout", page_count=50)
# tracker.track("prebuilt-layout", page_count=120)
#
# print("ëˆ„ì  ë¹„ìš©:")
# print(json.dumps(tracker.get_cost(), indent=2))
```

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Document Intelligence ê°œìš”](https://learn.microsoft.com/azure/ai-services/document-intelligence/)
- [Layout API ê°€ì´ë“œ](https://learn.microsoft.com/azure/ai-services/document-intelligence/concept-layout)
- [JSON ì¶œë ¥ ìŠ¤í‚¤ë§ˆ](https://learn.microsoft.com/azure/ai-services/document-intelligence/concept-model-overview#output-schema)

### RAG í†µí•©
- [LangChain AzureAIDocumentIntelligenceLoader](https://python.langchain.com/docs/integrations/document_loaders/azure_document_intelligence)
- [Hybrid Search íŒ¨í„´](https://learn.microsoft.com/azure/search/search-get-started-vector#hybrid-search)

## ë‹¤ìŒ ë‹¨ê³„

Document Intelligenceë¡œ êµ¬ì¡°í™”ëœ ë¬¸ì„œë¥¼ ì–»ì—ˆë‹¤ë©´, ì´ì œ ë²¡í„° ì„ë² ë”©ì„ ìƒì„±í•˜ì:

ğŸ‘‰ [03-Azure-OpenAI-Embeddings.qmd](./03-Azure-OpenAI-Embeddings.qmd) - í…ìŠ¤íŠ¸ ì„ë² ë”© ë° ë²¡í„° ê²€ìƒ‰

### Blobì—ì„œ ì§ì ‘ ë¶„ì„

```{python}
from azure.storage.blob import BlobServiceClient

# Blob Storage í´ë¼ì´ì–¸íŠ¸
blob_service_client = BlobServiceClient.from_connection_string(
    os.getenv("AZURE_STORAGE_CONNECTION_STRING")
)

def analyze_blob_document(container_name: str, blob_name: str):
    """Blob Storageì˜ ë¬¸ì„œë¥¼ Document Intelligenceë¡œ ë¶„ì„"""
    
    # Blob SAS URL ìƒì„±
    from azure.storage.blob import generate_blob_sas, BlobSasPermissions
    from datetime import datetime, timedelta
    
    sas_token = generate_blob_sas(
        account_name="stragdocs2025",
        container_name=container_name,
        blob_name=blob_name,
        account_key=os.getenv("AZURE_STORAGE_KEY"),
        permission=BlobSasPermissions(read=True),
        expiry=datetime.utcnow() + timedelta(hours=1)
    )
    
    blob_url = f"https://stragdocs2025.blob.core.windows.net/{container_name}/{blob_name}?{sas_token}"
    
    # Document Intelligenceë¡œ ë¶„ì„
    poller = document_analysis_client.begin_analyze_document_from_url(
        "prebuilt-layout", document_url=blob_url
    )
    result = poller.result()
    
    return result

# ì‚¬ìš© ì˜ˆì‹œ
result = analyze_blob_document("rag-documents", "sample.pdf")
print("Blob ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ")
```

### ë°°ì¹˜ ì²˜ë¦¬

```{python}
def analyze_all_blobs(container_name: str):
    """ì»¨í…Œì´ë„ˆì˜ ëª¨ë“  ë¬¸ì„œ ë¶„ì„"""
    container_client = blob_service_client.get_container_client(container_name)
    blob_list = container_client.list_blobs()
    
    results = []
    for blob in blob_list:
        # PDF íŒŒì¼ë§Œ ì²˜ë¦¬
        if blob.name.endswith('.pdf'):
            print(f"ë¶„ì„ ì¤‘: {blob.name}")
            try:
                result = analyze_blob_document(container_name, blob.name)
                text = extract_full_text(result)
                
                results.append({
                    "blob_name": blob.name,
                    "page_count": len(result.pages),
                    "text": text,
                    "status": "success"
                })
            except Exception as e:
                print(f"ì˜¤ë¥˜: {blob.name} - {str(e)}")
                results.append({
                    "blob_name": blob.name,
                    "status": "error",
                    "error": str(e)
                })
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
# results = analyze_all_blobs("rag-documents")
# print(f"ì´ {len(results)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ")
```

## í•œêµ­ì–´ ë¬¸ì„œ ìµœì í™”

RAG ì‹œìŠ¤í…œì—ì„œ í•œêµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬ ì‹œ ì£¼ì˜ì‚¬í•­ì´ë‹¤.

### ì–¸ì–´ íŒíŠ¸ ì œê³µ

```{python}
# í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ (ì–¸ì–´ íŒíŠ¸)
with open("korean_document.pdf", "rb") as f:
    poller = document_analysis_client.begin_analyze_document(
        "prebuilt-layout",
        document=f,
        locale="ko-KR"  # í•œêµ­ì–´ íŒíŠ¸
    )
    result = poller.result()
```

### OCR í›„ì²˜ë¦¬

```{python}
import re

def clean_korean_ocr_text(text: str) -> str:
    """í•œê¸€ OCR ê²°ê³¼ ì •ë¦¬ (RAG ìµœì í™”)"""
    
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    # ì¤„ë°”ê¿ˆ ì •ë¦¬ (ë‹¨ë½ êµ¬ë¶„ ë³´ì¡´)
    text = re.sub(r'\n\s*\n', '\n\n', text)
    
    # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
    text = text.replace('ã€ƒ', '"').replace('ã€ƒ', '"')
    text = text.replace('Â·', 'â€¢')  # ê°€ìš´ë°ì  â†’ ë¶ˆë¦¿
    
    # ì¼ë°˜ì ì¸ OCR ì˜¤ë¥˜ ìˆ˜ì •
    text = text.replace('ã…ã…', 'ã„·')  # ã…ã… â†’ ã„·
    text = text.replace('0', 'O')  # ìˆ«ì 0ê³¼ ì˜ë¬¸ì O í˜¼ë™
    
    return text.strip()

# RAG íŒŒì´í”„ë¼ì¸ í†µí•©
raw_text = extract_full_text(result)
cleaned_text = clean_korean_ocr_text(raw_text)

# ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
from langchain_core.documents import Document
doc = Document(page_content=cleaned_text, metadata={"language": "ko"})
```

## LangChain RAG í†µí•©

### AzureAIDocumentIntelligenceLoader

LangChainì˜ ê³µì‹ Document Loaderë¡œ Document Intelligenceë¥¼ í†µí•©í•œë‹¤.

```{python}
from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader

# Document Intelligence Loader ìƒì„±
loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint,
    api_key=key,
    file_path="sample.pdf",
    api_model="prebuilt-layout"
)

# ë¬¸ì„œ ë¡œë”© (ìë™ìœ¼ë¡œ í˜ì´ì§€ ë¶„í• )
documents = loader.load()

print(f"ë¡œë”©ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
print(f"ì²« ë²ˆì§¸ ì²­í¬:\n{documents[0].page_content[:300]}")
print(f"ë©”íƒ€ë°ì´í„°: {documents[0].metadata}")
```

**ê¸°ë³¸ ë©”íƒ€ë°ì´í„°:**
```python
{
    'source': 'sample.pdf',
    'page': 1,
}
```

### RAG íŒŒì´í”„ë¼ì¸ ì™„ì „ êµ¬í˜„

```{python}
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS

# 1. Document Intelligenceë¡œ ë¬¸ì„œ ë¡œë”©
loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint,
    api_key=key,
    file_path="long_document.pdf",
    api_model="prebuilt-layout"
)
documents = loader.load()

# 2. ë ˆì´ì•„ì›ƒ ê³ ë ¤ í…ìŠ¤íŠ¸ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""],  # ë‹¨ë½ ìš°ì„  ë¶„í• 
    length_function=len
)
splits = text_splitter.split_documents(documents)

# 3. ë²¡í„° ì„ë² ë”© ìƒì„±
embeddings = OpenAIEmbeddings(
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

# 4. Vector Store ìƒì„±
vectorstore = FAISS.from_documents(splits, embeddings)

print(f"ì´ {len(splits)}ê°œ ì²­í¬ ìƒì„±")
print(f"Vector Store ì´ˆê¸°í™” ì™„ë£Œ")

# 5. RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
query = "2023ë…„ ë§¤ì¶œì€ ì–¼ë§ˆì¸ê°€?"
results = vectorstore.similarity_search(query, k=3)

for i, doc in enumerate(results, 1):
    print(f"\nê²°ê³¼ {i}:")
    print(f"ë‚´ìš©: {doc.page_content[:200]}...")
    print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}")
```

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Document Intelligence ê°œìš”](https://learn.microsoft.com/azure/ai-services/document-intelligence/)
- [Layout API ê°€ì´ë“œ](https://learn.microsoft.com/azure/ai-services/document-intelligence/concept-layout)
- [JSON ì¶œë ¥ ìŠ¤í‚¤ë§ˆ](https://learn.microsoft.com/azure/ai-services/document-intelligence/concept-model-overview#output-schema)
- [Python SDK ì°¸ì¡°](https://learn.microsoft.com/python/api/overview/azure/ai-formrecognizer-readme)

### RAG íŒ¨í„´
- [LangChain í†µí•©](https://python.langchain.com/docs/integrations/document_loaders/azure_document_intelligence)
- [Hierarchical Indexing](https://learn.microsoft.com/azure/search/search-get-started-vector#hybrid-search)
- [Document Chunking Strategies](https://www.pinecone.io/learn/chunking-strategies/)

### Agent ì‘ìš©
- [LangChain Agent Tools](https://python.langchain.com/docs/modules/agents/tools/)
- [Multi-Modal RAG](https://learn.microsoft.com/azure/ai-services/openai/how-to/gpt-with-vision)

## ë‹¤ìŒ ë‹¨ê³„

Document Intelligenceë¡œ êµ¬ì¡°í™”ëœ ë¬¸ì„œë¥¼ ì–»ì—ˆë‹¤ë©´, ì´ì œ ë²¡í„° ì„ë² ë”©ì„ ìƒì„±í•˜ì:

ğŸ‘‰ [03-Azure-OpenAI-Embeddings.qmd](./03-Azure-OpenAI-Embeddings.qmd) - í…ìŠ¤íŠ¸ ì„ë² ë”© ë° ë²¡í„° ê²€ìƒ‰


```{python}
import json
import hashlib

def get_cache_key(file_path: str) -> str:
    """íŒŒì¼ í•´ì‹œë¡œ ìºì‹œ í‚¤ ìƒì„±"""
    with open(file_path, "rb") as f:
        file_hash = hashlib.md5(f.read()).hexdigest()
    return f"doc_intel_{file_hash}"

def analyze_with_cache(file_path: str):
    """ìºì‹±ì„ ì‚¬ìš©í•œ ë¬¸ì„œ ë¶„ì„"""
    cache_key = get_cache_key(file_path)
    cache_file = f".cache/{cache_key}.json"
    
    # ìºì‹œ í™•ì¸
    try:
        with open(cache_file, "r", encoding="utf-8") as f:
            cached_result = json.load(f)
            print("ìºì‹œì—ì„œ ê²°ê³¼ ë¡œë”©")
            return cached_result
    except FileNotFoundError:
        pass
    
    # Document Intelligence ì‹¤í–‰
    with open(file_path, "rb") as f:
        poller = document_analysis_client.begin_analyze_document(
            "prebuilt-layout", document=f
        )
        result = poller.result()
    
    # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    result_dict = {
        "content": result.content,
        "pages": [{"page_number": p.page_number, "width": p.width, "height": p.height} for p in result.pages],
        "paragraphs": [{"content": p.content, "role": p.role} for p in result.paragraphs]
    }
    
    # ìºì‹œ ì €ì¥
    os.makedirs(".cache", exist_ok=True)
    with open(cache_file, "w", encoding="utf-8") as f:
        json.dump(result_dict, f, ensure_ascii=False, indent=2)
    
    print("Document Intelligence ì‹¤í–‰ ë° ìºì‹œ ì €ì¥")
    return result_dict

# ì‚¬ìš© ì˜ˆì‹œ
# cached_result = analyze_with_cache("sample.pdf")
```

### ë°°ì¹˜ í¬ê¸° ì¡°ì •

```{python}
def analyze_documents_batch(file_paths: List[str], batch_size: int = 5):
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¸ì„œ ë¶„ì„ (API ì œí•œ ê³ ë ¤)"""
    import time
    
    results = []
    for i in range(0, len(file_paths), batch_size):
        batch = file_paths[i:i+batch_size]
        
        print(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ íŒŒì¼)")
        for file_path in batch:
            with open(file_path, "rb") as f:
                poller = document_analysis_client.begin_analyze_document(
                    "prebuilt-layout", document=f
                )
                result = poller.result()
                results.append({
                    "file": file_path,
                    "result": result
                })
        
        # API ì œí•œ ë°©ì§€ (ì´ˆë‹¹ 15 ìš”ì²­)
        time.sleep(1)
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
# file_list = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
# results = analyze_documents_batch(file_list, batch_size=5)
```

## ëª¨ë‹ˆí„°ë§

### ë¶„ì„ í†µê³„ ì¶”ì 

```{python}
def analyze_with_metrics(file_path: str):
    """ë¶„ì„ ë©”íŠ¸ë¦­ ì¶”ì """
    import time
    
    start_time = time.time()
    
    with open(file_path, "rb") as f:
        file_size = os.path.getsize(file_path)
        
        poller = document_analysis_client.begin_analyze_document(
            "prebuilt-layout", document=f
        )
        result = poller.result()
    
    end_time = time.time()
    duration = end_time - start_time
    
    metrics = {
        "file_path": file_path,
        "file_size_mb": file_size / (1024 * 1024),
        "page_count": len(result.pages),
        "duration_seconds": duration,
        "pages_per_second": len(result.pages) / duration
    }
    
    print(f"ë¶„ì„ ì™„ë£Œ:")
    print(f"- íŒŒì¼ í¬ê¸°: {metrics['file_size_mb']:.2f} MB")
    print(f"- í˜ì´ì§€ ìˆ˜: {metrics['page_count']}")
    print(f"- ì²˜ë¦¬ ì‹œê°„: {metrics['duration_seconds']:.2f}ì´ˆ")
    print(f"- ì†ë„: {metrics['pages_per_second']:.2f} í˜ì´ì§€/ì´ˆ")
    
    return result, metrics

# ì‚¬ìš© ì˜ˆì‹œ
# result, metrics = analyze_with_metrics("large_document.pdf")
```

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜

**InvalidRequest: íŒŒì¼ í¬ê¸° ì´ˆê³¼**
```
ì˜¤ë¥˜: íŒŒì¼ì´ ë„ˆë¬´ í¼ (ìµœëŒ€ 500MB)
í•´ê²°: íŒŒì¼ì„ ë¶„í• í•˜ê±°ë‚˜ ì••ì¶•
```

**InvalidImage: ì´ë¯¸ì§€ í•´ìƒë„ ë¶€ì¡±**
```python
# í•´ê²°: ì´ë¯¸ì§€ í’ˆì§ˆ í™•ì¸ (ìµœì†Œ 150 DPI ê¶Œì¥)
from PIL import Image

img = Image.open("low_quality.png")
print(f"í•´ìƒë„: {img.size}")
print(f"DPI: {img.info.get('dpi', 'Unknown')}")
```

**Unauthorized: ì¸ì¦ ì‹¤íŒ¨**
```python
# í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
print(f"Endpoint: {endpoint}")
print(f"Key: {key[:10]}... (ê¸¸ì´: {len(key)})")
```

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Document Intelligence ê°œìš”](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/)
- [Python SDK ì°¸ì¡°](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-formrecognizer-readme)
- [ëª¨ë¸ ê°€ì´ë“œ](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-model-overview)

### ìƒ˜í”Œ ì½”ë“œ
- [Document Intelligence Samples](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/formrecognizer/azure-ai-formrecognizer/samples)
- [LangChain í†µí•©](https://python.langchain.com/docs/integrations/document_loaders/azure_document_intelligence)

## ë‹¤ìŒ ë‹¨ê³„

ë¬¸ì„œ OCR ë° ë ˆì´ì•„ì›ƒ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆë‹¤ë©´, ì´ì œ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì:

ğŸ‘‰ [03-Azure-OpenAI-Embeddings.qmd](./03-Azure-OpenAI-Embeddings.qmd) - Azure OpenAIë¡œ ë¬¸ì„œ ì„ë² ë”© ìƒì„±