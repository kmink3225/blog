{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b56dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import json\n",
    "\n",
    "# figure size/format\n",
    "fig_width = 7\n",
    "fig_height = 5\n",
    "fig_format = 'retina'\n",
    "fig_dpi = 96\n",
    "interactivity = ''\n",
    "is_shiny = False\n",
    "is_dashboard = False\n",
    "plotly_connected = True\n",
    "\n",
    "# matplotlib defaults / format\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "  plt.rcParams['figure.dpi'] = fig_dpi\n",
    "  plt.rcParams['savefig.dpi'] = fig_dpi\n",
    "  from IPython.display import set_matplotlib_formats\n",
    "  set_matplotlib_formats(fig_format)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# plotly use connected mode\n",
    "try:\n",
    "  import plotly.io as pio\n",
    "  if plotly_connected:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "  else:\n",
    "    pio.renderers.default = \"notebook\"\n",
    "  for template in pio.templates.keys():\n",
    "    pio.templates[template].layout.margin = dict(t=30,r=0,b=0,l=0)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# disable itables paging for dashboards\n",
    "if is_dashboard:\n",
    "  try:\n",
    "    from itables import options\n",
    "    options.dom = 'fiBrtlp'\n",
    "    options.maxBytes = 1024 * 1024\n",
    "    options.language = dict(info = \"Showing _TOTAL_ entries\")\n",
    "    options.classes = \"display nowrap compact\"\n",
    "    options.paging = False\n",
    "    options.searching = True\n",
    "    options.ordering = True\n",
    "    options.info = True\n",
    "    options.lengthChange = False\n",
    "    options.autoWidth = False\n",
    "    options.responsive = True\n",
    "    options.keys = True\n",
    "    options.buttons = []\n",
    "  except Exception:\n",
    "    pass\n",
    "  \n",
    "  try:\n",
    "    import altair as alt\n",
    "    # By default, dashboards will have container sized\n",
    "    # vega visualizations which allows them to flow reasonably\n",
    "    theme_sentinel = '_quarto-dashboard-internal'\n",
    "    def make_theme(name):\n",
    "        nonTheme = alt.themes._plugins[name]    \n",
    "        def patch_theme(*args, **kwargs):\n",
    "            existingTheme = nonTheme()\n",
    "            if 'height' not in existingTheme:\n",
    "              existingTheme['height'] = 'container'\n",
    "            if 'width' not in existingTheme:\n",
    "              existingTheme['width'] = 'container'\n",
    "\n",
    "            if 'config' not in existingTheme:\n",
    "              existingTheme['config'] = dict()\n",
    "            \n",
    "            # Configure the default font sizes\n",
    "            title_font_size = 15\n",
    "            header_font_size = 13\n",
    "            axis_font_size = 12\n",
    "            legend_font_size = 12\n",
    "            mark_font_size = 12\n",
    "            tooltip = False\n",
    "\n",
    "            config = existingTheme['config']\n",
    "\n",
    "            # The Axis\n",
    "            if 'axis' not in config:\n",
    "              config['axis'] = dict()\n",
    "            axis = config['axis']\n",
    "            if 'labelFontSize' not in axis:\n",
    "              axis['labelFontSize'] = axis_font_size\n",
    "            if 'titleFontSize' not in axis:\n",
    "              axis['titleFontSize'] = axis_font_size  \n",
    "\n",
    "            # The legend\n",
    "            if 'legend' not in config:\n",
    "              config['legend'] = dict()\n",
    "            legend = config['legend']\n",
    "            if 'labelFontSize' not in legend:\n",
    "              legend['labelFontSize'] = legend_font_size\n",
    "            if 'titleFontSize' not in legend:\n",
    "              legend['titleFontSize'] = legend_font_size  \n",
    "\n",
    "            # The header\n",
    "            if 'header' not in config:\n",
    "              config['header'] = dict()\n",
    "            header = config['header']\n",
    "            if 'labelFontSize' not in header:\n",
    "              header['labelFontSize'] = header_font_size\n",
    "            if 'titleFontSize' not in header:\n",
    "              header['titleFontSize'] = header_font_size    \n",
    "\n",
    "            # Title\n",
    "            if 'title' not in config:\n",
    "              config['title'] = dict()\n",
    "            title = config['title']\n",
    "            if 'fontSize' not in title:\n",
    "              title['fontSize'] = title_font_size\n",
    "\n",
    "            # Marks\n",
    "            if 'mark' not in config:\n",
    "              config['mark'] = dict()\n",
    "            mark = config['mark']\n",
    "            if 'fontSize' not in mark:\n",
    "              mark['fontSize'] = mark_font_size\n",
    "\n",
    "            # Mark tooltips\n",
    "            if tooltip and 'tooltip' not in mark:\n",
    "              mark['tooltip'] = dict(content=\"encoding\")\n",
    "\n",
    "            return existingTheme\n",
    "            \n",
    "        return patch_theme\n",
    "\n",
    "    # We can only do this once per session\n",
    "    if theme_sentinel not in alt.themes.names():\n",
    "      for name in alt.themes.names():\n",
    "        alt.themes.register(name, make_theme(name))\n",
    "      \n",
    "      # register a sentinel theme so we only do this once\n",
    "      alt.themes.register(theme_sentinel, make_theme('default'))\n",
    "      alt.themes.enable('default')\n",
    "\n",
    "  except Exception:\n",
    "    pass\n",
    "\n",
    "# enable pandas latex repr when targeting pdfs\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if fig_format == 'pdf':\n",
    "    pd.set_option('display.latex.repr', True)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# interactivity\n",
    "if interactivity:\n",
    "  from IPython.core.interactiveshell import InteractiveShell\n",
    "  InteractiveShell.ast_node_interactivity = interactivity\n",
    "\n",
    "# NOTE: the kernel_deps code is repeated in the cleanup.py file\n",
    "# (we can't easily share this code b/c of the way it is run).\n",
    "# If you edit this code also edit the same code in cleanup.py!\n",
    "\n",
    "# output kernel dependencies\n",
    "kernel_deps = dict()\n",
    "for module in list(sys.modules.values()):\n",
    "  # Some modules play games with sys.modules (e.g. email/__init__.py\n",
    "  # in the standard library), and occasionally this can cause strange\n",
    "  # failures in getattr.  Just ignore anything that's not an ordinary\n",
    "  # module.\n",
    "  if not isinstance(module, types.ModuleType):\n",
    "    continue\n",
    "  path = getattr(module, \"__file__\", None)\n",
    "  if not path:\n",
    "    continue\n",
    "  if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n",
    "    path = path[:-1]\n",
    "  if not os.path.exists(path):\n",
    "    continue\n",
    "  kernel_deps[path] = os.stat(path).st_mtime\n",
    "print(json.dumps(kernel_deps))\n",
    "\n",
    "# set run_path if requested\n",
    "if r'C:\\Users\\kmkim\\Desktop\\projects\\blog\\docs\\blog\\posts\\Agent\\13-Cloud-RAG':\n",
    "  os.chdir(r'C:\\Users\\kmkim\\Desktop\\projects\\blog\\docs\\blog\\posts\\Agent\\13-Cloud-RAG')\n",
    "\n",
    "# reset state\n",
    "%reset\n",
    "\n",
    "# shiny\n",
    "# Checking for shiny by using False directly because we're after the %reset. We don't want\n",
    "# to set a variable that stays in global scope.\n",
    "if False:\n",
    "  try:\n",
    "    import htmltools as _htmltools\n",
    "    import ast as _ast\n",
    "\n",
    "    _htmltools.html_dependency_render_mode = \"json\"\n",
    "\n",
    "    # This decorator will be added to all function definitions\n",
    "    def _display_if_has_repr_html(x):\n",
    "      try:\n",
    "        # IPython 7.14 preferred import\n",
    "        from IPython.display import display, HTML\n",
    "      except:\n",
    "        from IPython.core.display import display, HTML\n",
    "\n",
    "      if hasattr(x, '_repr_html_'):\n",
    "        display(HTML(x._repr_html_()))\n",
    "      return x\n",
    "\n",
    "    # ideally we would undo the call to ast_transformers.append\n",
    "    # at the end of this block whenver an error occurs, we do \n",
    "    # this for now as it will only be a problem if the user \n",
    "    # switches from shiny to not-shiny mode (and even then likely\n",
    "    # won't matter)\n",
    "    import builtins\n",
    "    builtins._display_if_has_repr_html = _display_if_has_repr_html\n",
    "\n",
    "    class _FunctionDefReprHtml(_ast.NodeTransformer):\n",
    "      def visit_FunctionDef(self, node):\n",
    "        node.decorator_list.insert(\n",
    "          0,\n",
    "          _ast.Name(id=\"_display_if_has_repr_html\", ctx=_ast.Load())\n",
    "        )\n",
    "        return node\n",
    "\n",
    "      def visit_AsyncFunctionDef(self, node):\n",
    "        node.decorator_list.insert(\n",
    "          0,\n",
    "          _ast.Name(id=\"_display_if_has_repr_html\", ctx=_ast.Load())\n",
    "        )\n",
    "        return node\n",
    "\n",
    "    ip = get_ipython()\n",
    "    ip.ast_transformers.append(_FunctionDefReprHtml())\n",
    "\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "def ojs_define(**kwargs):\n",
    "  import json\n",
    "  try:\n",
    "    # IPython 7.14 preferred import\n",
    "    from IPython.display import display, HTML\n",
    "  except:\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "  # do some minor magic for convenience when handling pandas\n",
    "  # dataframes\n",
    "  def convert(v):\n",
    "    try:\n",
    "      import pandas as pd\n",
    "    except ModuleNotFoundError: # don't do the magic when pandas is not available\n",
    "      return v\n",
    "    if type(v) == pd.Series:\n",
    "      v = pd.DataFrame(v)\n",
    "    if type(v) == pd.DataFrame:\n",
    "      j = json.loads(v.T.to_json(orient='split'))\n",
    "      return dict((k,v) for (k,v) in zip(j[\"index\"], j[\"data\"]))\n",
    "    else:\n",
    "      return v\n",
    "\n",
    "  v = dict(contents=list(dict(name=key, value=convert(value)) for (key, value) in kwargs.items()))\n",
    "  display(HTML('<script type=\"ojs-define\">' + json.dumps(v) + '</script>'), metadata=dict(ojs_define = True))\n",
    "globals()[\"ojs_define\"] = ojs_define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99be19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Embeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-3-small\",\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Vector Store\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    azure_search_key=os.getenv(\"AZURE_SEARCH_API_KEY\"),\n",
    "    index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n",
    "    embedding_function=embeddings.embed_query\n",
    ")\n",
    "\n",
    "# Retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"다음 컨텍스트를 참고하여 질문에 답변하세요.\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\"\"\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 실행\n",
    "answer = rag_chain.invoke(\"Azure AI Search란 무엇인가?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10c0078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class RAGState(TypedDict):\n",
    "    \"\"\"RAG 상태\"\"\"\n",
    "    question: str  # 사용자 질문\n",
    "    context: List[Document]  # 검색된 문서\n",
    "    answer: str  # 생성된 답변\n",
    "    retrieval_success: bool  # 검색 성공 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b2ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: RAGState) -> RAGState:\n",
    "    \"\"\"문서 검색 노드\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 검색 실행\n",
    "    docs = retriever.invoke(question)\n",
    "    \n",
    "    # 검색 성공 여부 확인\n",
    "    success = len(docs) > 0\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"context\": docs,\n",
    "        \"retrieval_success\": success\n",
    "    }\n",
    "\n",
    "def generate(state: RAGState) -> RAGState:\n",
    "    \"\"\"답변 생성 노드\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "    \n",
    "    # 컨텍스트 포맷팅\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in context])\n",
    "    \n",
    "    # 프롬프트 생성\n",
    "    messages = prompt.invoke({\n",
    "        \"context\": context_text,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    # 답변 생성\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"answer\": response.content\n",
    "    }\n",
    "\n",
    "def no_context_fallback(state: RAGState) -> RAGState:\n",
    "    \"\"\"컨텍스트 없을 때 기본 답변\"\"\"\n",
    "    return {\n",
    "        **state,\n",
    "        \"answer\": \"관련 정보를 찾을 수 없습니다. 질문을 다시 입력해주세요.\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310196c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# 그래프 생성\n",
    "workflow = StateGraph(RAGState)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"fallback\", no_context_fallback)\n",
    "\n",
    "# 시작점 설정\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "# 조건부 엣지 (검색 성공 여부)\n",
    "def should_generate(state: RAGState) -> str:\n",
    "    \"\"\"검색 성공 시 generate, 실패 시 fallback\"\"\"\n",
    "    if state[\"retrieval_success\"]:\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        return \"fallback\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    should_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"fallback\": \"fallback\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 종료 엣지\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"fallback\", END)\n",
    "\n",
    "# 컴파일\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5048d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행\n",
    "result = app.invoke({\n",
    "    \"question\": \"Azure AI Search란 무엇인가?\"\n",
    "})\n",
    "\n",
    "print(f\"질문: {result['question']}\")\n",
    "print(f\"검색 성공: {result['retrieval_success']}\")\n",
    "print(f\"답변: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12a02323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class ConversationState(TypedDict):\n",
    "    \"\"\"대화 상태\"\"\"\n",
    "    question: str\n",
    "    chat_history: List[BaseMessage]  # 대화 히스토리\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    retrieval_success: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c887a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "def retrieve_with_history(state: ConversationState) -> ConversationState:\n",
    "    \"\"\"대화 히스토리를 고려한 검색\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    \n",
    "    # 히스토리를 컨텍스트로 변환\n",
    "    history_text = \"\\n\".join([\n",
    "        f\"{'User' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}\"\n",
    "        for msg in chat_history[-3:]  # 최근 3개만\n",
    "    ])\n",
    "    \n",
    "    # 질문 재구성 (히스토리 포함)\n",
    "    if history_text:\n",
    "        enhanced_question = f\"대화 히스토리:\\n{history_text}\\n\\n현재 질문: {question}\"\n",
    "    else:\n",
    "        enhanced_question = question\n",
    "    \n",
    "    # 검색\n",
    "    docs = retriever.invoke(enhanced_question)\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"context\": docs,\n",
    "        \"retrieval_success\": len(docs) > 0\n",
    "    }\n",
    "\n",
    "def generate_with_history(state: ConversationState) -> ConversationState:\n",
    "    \"\"\"히스토리를 고려한 답변 생성\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    \n",
    "    # 컨텍스트 포맷팅\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in context])\n",
    "    \n",
    "    # 히스토리 포함 프롬프트\n",
    "    history_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"이전 대화:\n",
    "{history}\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\"\"\"\n",
    "    )\n",
    "    \n",
    "    history_text = \"\\n\".join([\n",
    "        f\"User: {msg.content}\" if isinstance(msg, HumanMessage) else f\"AI: {msg.content}\"\n",
    "        for msg in chat_history[-3:]\n",
    "    ])\n",
    "    \n",
    "    messages = history_prompt.invoke({\n",
    "        \"history\": history_text or \"없음\",\n",
    "        \"context\": context_text,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # 히스토리 업데이트\n",
    "    updated_history = chat_history + [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response.content)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"answer\": response.content,\n",
    "        \"chat_history\": updated_history\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "891a9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화형 워크플로우\n",
    "conversation_workflow = StateGraph(ConversationState)\n",
    "\n",
    "conversation_workflow.add_node(\"retrieve\", retrieve_with_history)\n",
    "conversation_workflow.add_node(\"generate\", generate_with_history)\n",
    "conversation_workflow.add_node(\"fallback\", no_context_fallback)\n",
    "\n",
    "conversation_workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "conversation_workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    should_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"fallback\": \"fallback\"\n",
    "    }\n",
    ")\n",
    "\n",
    "conversation_workflow.add_edge(\"generate\", END)\n",
    "conversation_workflow.add_edge(\"fallback\", END)\n",
    "\n",
    "conversation_app = conversation_workflow.compile()\n",
    "\n",
    "# 멀티턴 대화 실행\n",
    "state = {\n",
    "    \"question\": \"Azure AI Search란 무엇인가?\",\n",
    "    \"chat_history\": []\n",
    "}\n",
    "\n",
    "result1 = conversation_app.invoke(state)\n",
    "print(f\"답변 1: {result1['answer']}\\n\")\n",
    "\n",
    "# 후속 질문\n",
    "state2 = {\n",
    "    \"question\": \"그것의 주요 기능은?\",\n",
    "    \"chat_history\": result1[\"chat_history\"]\n",
    "}\n",
    "\n",
    "result2 = conversation_app.invoke(state2)\n",
    "print(f\"답변 2: {result2['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0457505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetryState(TypedDict):\n",
    "    \"\"\"재시도 가능한 RAG 상태\"\"\"\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    retrieval_success: bool\n",
    "    retry_count: int  # 재시도 횟수\n",
    "    max_retries: int  # 최대 재시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "256eb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_retry(state: RetryState) -> RetryState:\n",
    "    \"\"\"재시도 가능한 검색\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    # 재시도 시 쿼리 확장\n",
    "    if retry_count > 0:\n",
    "        expanded_question = f\"{question} (관련 정보, 설명, 개요 포함)\"\n",
    "    else:\n",
    "        expanded_question = question\n",
    "    \n",
    "    docs = retriever.invoke(expanded_question)\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"context\": docs,\n",
    "        \"retrieval_success\": len(docs) > 0,\n",
    "        \"retry_count\": retry_count + 1\n",
    "    }\n",
    "\n",
    "def should_retry(state: RetryState) -> str:\n",
    "    \"\"\"재시도 여부 결정\"\"\"\n",
    "    if state[\"retrieval_success\"]:\n",
    "        return \"generate\"\n",
    "    elif state[\"retry_count\"] < state.get(\"max_retries\", 2):\n",
    "        return \"retry\"\n",
    "    else:\n",
    "        return \"fallback\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ade4d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_workflow = StateGraph(RetryState)\n",
    "\n",
    "retry_workflow.add_node(\"retrieve\", retrieve_with_retry)\n",
    "retry_workflow.add_node(\"generate\", generate)\n",
    "retry_workflow.add_node(\"fallback\", no_context_fallback)\n",
    "\n",
    "retry_workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "retry_workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    should_retry,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"retry\": \"retrieve\",  # 루프백\n",
    "        \"fallback\": \"fallback\"\n",
    "    }\n",
    ")\n",
    "\n",
    "retry_workflow.add_edge(\"generate\", END)\n",
    "retry_workflow.add_edge(\"fallback\", END)\n",
    "\n",
    "retry_app = retry_workflow.compile()\n",
    "\n",
    "# 실행\n",
    "result = retry_app.invoke({\n",
    "    \"question\": \"희귀한 주제 검색\",\n",
    "    \"max_retries\": 2\n",
    "})\n",
    "\n",
    "print(f\"재시도 횟수: {result['retry_count']}\")\n",
    "print(f\"답변: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76487a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityState(TypedDict):\n",
    "    \"\"\"품질 검증 상태\"\"\"\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    retrieval_success: bool\n",
    "    answer_quality: str  # \"good\", \"bad\", \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ae721b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(state: QualityState) -> QualityState:\n",
    "    \"\"\"답변 품질 평가\"\"\"\n",
    "    answer = state[\"answer\"]\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 평가 프롬프트\n",
    "    eval_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"다음 질문과 답변의 품질을 평가하세요.\n",
    "\n",
    "질문: {question}\n",
    "답변: {answer}\n",
    "\n",
    "답변이 질문에 적절히 대답하고 있나요?\n",
    "- \"good\": 적절한 답변\n",
    "- \"bad\": 부적절하거나 관련 없는 답변\n",
    "- \"unknown\": 판단 불가\n",
    "\n",
    "평가 결과 (good/bad/unknown만 출력):\"\"\"\n",
    "    )\n",
    "    \n",
    "    messages = eval_prompt.invoke({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "    \n",
    "    evaluation = llm.invoke(messages).content.strip().lower()\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"answer_quality\": evaluation\n",
    "    }\n",
    "\n",
    "def regenerate_answer(state: QualityState) -> QualityState:\n",
    "    \"\"\"답변 재생성\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "    \n",
    "    # 더 상세한 프롬프트\n",
    "    detailed_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"다음 컨텍스트를 참고하여 질문에 상세히 답변하세요.\n",
    "반드시 컨텍스트의 정보를 활용하고, 구체적으로 설명하세요.\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "상세 답변:\"\"\"\n",
    "    )\n",
    "    \n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in context])\n",
    "    messages = detailed_prompt.invoke({\n",
    "        \"context\": context_text,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"answer\": response.content\n",
    "    }\n",
    "\n",
    "def route_by_quality(state: QualityState) -> str:\n",
    "    \"\"\"품질에 따라 라우팅\"\"\"\n",
    "    quality = state[\"answer_quality\"]\n",
    "    \n",
    "    if quality == \"good\":\n",
    "        return \"end\"\n",
    "    elif quality == \"bad\":\n",
    "        return \"regenerate\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "710e8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_workflow = StateGraph(QualityState)\n",
    "\n",
    "quality_workflow.add_node(\"retrieve\", retrieve)\n",
    "quality_workflow.add_node(\"generate\", generate)\n",
    "quality_workflow.add_node(\"evaluate\", evaluate_answer)\n",
    "quality_workflow.add_node(\"regenerate\", regenerate_answer)\n",
    "quality_workflow.add_node(\"fallback\", no_context_fallback)\n",
    "\n",
    "quality_workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "quality_workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    should_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"fallback\": \"fallback\"\n",
    "    }\n",
    ")\n",
    "\n",
    "quality_workflow.add_edge(\"generate\", \"evaluate\")\n",
    "\n",
    "quality_workflow.add_conditional_edges(\n",
    "    \"evaluate\",\n",
    "    route_by_quality,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"regenerate\": \"regenerate\"\n",
    "    }\n",
    ")\n",
    "\n",
    "quality_workflow.add_edge(\"regenerate\", END)\n",
    "quality_workflow.add_edge(\"fallback\", END)\n",
    "\n",
    "quality_app = quality_workflow.compile()\n",
    "\n",
    "# 실행\n",
    "result = quality_app.invoke({\n",
    "    \"question\": \"Azure AI Search의 장점은?\"\n",
    "})\n",
    "\n",
    "print(f\"답변 품질: {result['answer_quality']}\")\n",
    "print(f\"최종 답변: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd729b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# 그래프 시각화\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    print(app.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ab583f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍으로 중간 단계 확인\n",
    "for event in app.stream({\n",
    "    \"question\": \"Azure AI Search란?\"\n",
    "}):\n",
    "    print(f\"이벤트: {event}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}