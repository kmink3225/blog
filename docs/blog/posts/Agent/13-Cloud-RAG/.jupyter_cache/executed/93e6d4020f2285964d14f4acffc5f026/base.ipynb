{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6381c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import json\n",
    "\n",
    "# figure size/format\n",
    "fig_width = 7\n",
    "fig_height = 5\n",
    "fig_format = 'retina'\n",
    "fig_dpi = 96\n",
    "interactivity = ''\n",
    "is_shiny = False\n",
    "is_dashboard = False\n",
    "plotly_connected = True\n",
    "\n",
    "# matplotlib defaults / format\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "  plt.rcParams['figure.dpi'] = fig_dpi\n",
    "  plt.rcParams['savefig.dpi'] = fig_dpi\n",
    "  from IPython.display import set_matplotlib_formats\n",
    "  set_matplotlib_formats(fig_format)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# plotly use connected mode\n",
    "try:\n",
    "  import plotly.io as pio\n",
    "  if plotly_connected:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "  else:\n",
    "    pio.renderers.default = \"notebook\"\n",
    "  for template in pio.templates.keys():\n",
    "    pio.templates[template].layout.margin = dict(t=30,r=0,b=0,l=0)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# disable itables paging for dashboards\n",
    "if is_dashboard:\n",
    "  try:\n",
    "    from itables import options\n",
    "    options.dom = 'fiBrtlp'\n",
    "    options.maxBytes = 1024 * 1024\n",
    "    options.language = dict(info = \"Showing _TOTAL_ entries\")\n",
    "    options.classes = \"display nowrap compact\"\n",
    "    options.paging = False\n",
    "    options.searching = True\n",
    "    options.ordering = True\n",
    "    options.info = True\n",
    "    options.lengthChange = False\n",
    "    options.autoWidth = False\n",
    "    options.responsive = True\n",
    "    options.keys = True\n",
    "    options.buttons = []\n",
    "  except Exception:\n",
    "    pass\n",
    "  \n",
    "  try:\n",
    "    import altair as alt\n",
    "    # By default, dashboards will have container sized\n",
    "    # vega visualizations which allows them to flow reasonably\n",
    "    theme_sentinel = '_quarto-dashboard-internal'\n",
    "    def make_theme(name):\n",
    "        nonTheme = alt.themes._plugins[name]    \n",
    "        def patch_theme(*args, **kwargs):\n",
    "            existingTheme = nonTheme()\n",
    "            if 'height' not in existingTheme:\n",
    "              existingTheme['height'] = 'container'\n",
    "            if 'width' not in existingTheme:\n",
    "              existingTheme['width'] = 'container'\n",
    "\n",
    "            if 'config' not in existingTheme:\n",
    "              existingTheme['config'] = dict()\n",
    "            \n",
    "            # Configure the default font sizes\n",
    "            title_font_size = 15\n",
    "            header_font_size = 13\n",
    "            axis_font_size = 12\n",
    "            legend_font_size = 12\n",
    "            mark_font_size = 12\n",
    "            tooltip = False\n",
    "\n",
    "            config = existingTheme['config']\n",
    "\n",
    "            # The Axis\n",
    "            if 'axis' not in config:\n",
    "              config['axis'] = dict()\n",
    "            axis = config['axis']\n",
    "            if 'labelFontSize' not in axis:\n",
    "              axis['labelFontSize'] = axis_font_size\n",
    "            if 'titleFontSize' not in axis:\n",
    "              axis['titleFontSize'] = axis_font_size  \n",
    "\n",
    "            # The legend\n",
    "            if 'legend' not in config:\n",
    "              config['legend'] = dict()\n",
    "            legend = config['legend']\n",
    "            if 'labelFontSize' not in legend:\n",
    "              legend['labelFontSize'] = legend_font_size\n",
    "            if 'titleFontSize' not in legend:\n",
    "              legend['titleFontSize'] = legend_font_size  \n",
    "\n",
    "            # The header\n",
    "            if 'header' not in config:\n",
    "              config['header'] = dict()\n",
    "            header = config['header']\n",
    "            if 'labelFontSize' not in header:\n",
    "              header['labelFontSize'] = header_font_size\n",
    "            if 'titleFontSize' not in header:\n",
    "              header['titleFontSize'] = header_font_size    \n",
    "\n",
    "            # Title\n",
    "            if 'title' not in config:\n",
    "              config['title'] = dict()\n",
    "            title = config['title']\n",
    "            if 'fontSize' not in title:\n",
    "              title['fontSize'] = title_font_size\n",
    "\n",
    "            # Marks\n",
    "            if 'mark' not in config:\n",
    "              config['mark'] = dict()\n",
    "            mark = config['mark']\n",
    "            if 'fontSize' not in mark:\n",
    "              mark['fontSize'] = mark_font_size\n",
    "\n",
    "            # Mark tooltips\n",
    "            if tooltip and 'tooltip' not in mark:\n",
    "              mark['tooltip'] = dict(content=\"encoding\")\n",
    "\n",
    "            return existingTheme\n",
    "            \n",
    "        return patch_theme\n",
    "\n",
    "    # We can only do this once per session\n",
    "    if theme_sentinel not in alt.themes.names():\n",
    "      for name in alt.themes.names():\n",
    "        alt.themes.register(name, make_theme(name))\n",
    "      \n",
    "      # register a sentinel theme so we only do this once\n",
    "      alt.themes.register(theme_sentinel, make_theme('default'))\n",
    "      alt.themes.enable('default')\n",
    "\n",
    "  except Exception:\n",
    "    pass\n",
    "\n",
    "# enable pandas latex repr when targeting pdfs\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if fig_format == 'pdf':\n",
    "    pd.set_option('display.latex.repr', True)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# interactivity\n",
    "if interactivity:\n",
    "  from IPython.core.interactiveshell import InteractiveShell\n",
    "  InteractiveShell.ast_node_interactivity = interactivity\n",
    "\n",
    "# NOTE: the kernel_deps code is repeated in the cleanup.py file\n",
    "# (we can't easily share this code b/c of the way it is run).\n",
    "# If you edit this code also edit the same code in cleanup.py!\n",
    "\n",
    "# output kernel dependencies\n",
    "kernel_deps = dict()\n",
    "for module in list(sys.modules.values()):\n",
    "  # Some modules play games with sys.modules (e.g. email/__init__.py\n",
    "  # in the standard library), and occasionally this can cause strange\n",
    "  # failures in getattr.  Just ignore anything that's not an ordinary\n",
    "  # module.\n",
    "  if not isinstance(module, types.ModuleType):\n",
    "    continue\n",
    "  path = getattr(module, \"__file__\", None)\n",
    "  if not path:\n",
    "    continue\n",
    "  if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n",
    "    path = path[:-1]\n",
    "  if not os.path.exists(path):\n",
    "    continue\n",
    "  kernel_deps[path] = os.stat(path).st_mtime\n",
    "print(json.dumps(kernel_deps))\n",
    "\n",
    "# set run_path if requested\n",
    "if r'C:\\Users\\kmkim\\Desktop\\projects\\blog\\docs\\blog\\posts\\Agent\\13-Cloud-RAG':\n",
    "  os.chdir(r'C:\\Users\\kmkim\\Desktop\\projects\\blog\\docs\\blog\\posts\\Agent\\13-Cloud-RAG')\n",
    "\n",
    "# reset state\n",
    "%reset\n",
    "\n",
    "# shiny\n",
    "# Checking for shiny by using False directly because we're after the %reset. We don't want\n",
    "# to set a variable that stays in global scope.\n",
    "if False:\n",
    "  try:\n",
    "    import htmltools as _htmltools\n",
    "    import ast as _ast\n",
    "\n",
    "    _htmltools.html_dependency_render_mode = \"json\"\n",
    "\n",
    "    # This decorator will be added to all function definitions\n",
    "    def _display_if_has_repr_html(x):\n",
    "      try:\n",
    "        # IPython 7.14 preferred import\n",
    "        from IPython.display import display, HTML\n",
    "      except:\n",
    "        from IPython.core.display import display, HTML\n",
    "\n",
    "      if hasattr(x, '_repr_html_'):\n",
    "        display(HTML(x._repr_html_()))\n",
    "      return x\n",
    "\n",
    "    # ideally we would undo the call to ast_transformers.append\n",
    "    # at the end of this block whenver an error occurs, we do \n",
    "    # this for now as it will only be a problem if the user \n",
    "    # switches from shiny to not-shiny mode (and even then likely\n",
    "    # won't matter)\n",
    "    import builtins\n",
    "    builtins._display_if_has_repr_html = _display_if_has_repr_html\n",
    "\n",
    "    class _FunctionDefReprHtml(_ast.NodeTransformer):\n",
    "      def visit_FunctionDef(self, node):\n",
    "        node.decorator_list.insert(\n",
    "          0,\n",
    "          _ast.Name(id=\"_display_if_has_repr_html\", ctx=_ast.Load())\n",
    "        )\n",
    "        return node\n",
    "\n",
    "      def visit_AsyncFunctionDef(self, node):\n",
    "        node.decorator_list.insert(\n",
    "          0,\n",
    "          _ast.Name(id=\"_display_if_has_repr_html\", ctx=_ast.Load())\n",
    "        )\n",
    "        return node\n",
    "\n",
    "    ip = get_ipython()\n",
    "    ip.ast_transformers.append(_FunctionDefReprHtml())\n",
    "\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "def ojs_define(**kwargs):\n",
    "  import json\n",
    "  try:\n",
    "    # IPython 7.14 preferred import\n",
    "    from IPython.display import display, HTML\n",
    "  except:\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "  # do some minor magic for convenience when handling pandas\n",
    "  # dataframes\n",
    "  def convert(v):\n",
    "    try:\n",
    "      import pandas as pd\n",
    "    except ModuleNotFoundError: # don't do the magic when pandas is not available\n",
    "      return v\n",
    "    if type(v) == pd.Series:\n",
    "      v = pd.DataFrame(v)\n",
    "    if type(v) == pd.DataFrame:\n",
    "      j = json.loads(v.T.to_json(orient='split'))\n",
    "      return dict((k,v) for (k,v) in zip(j[\"index\"], j[\"data\"]))\n",
    "    else:\n",
    "      return v\n",
    "\n",
    "  v = dict(contents=list(dict(name=key, value=convert(value)) for (key, value) in kwargs.items()))\n",
    "  display(HTML('<script type=\"ojs-define\">' + json.dumps(v) + '</script>'), metadata=dict(ojs_define = True))\n",
    "globals()[\"ojs_define\"] = ojs_define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e856fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 클라이언트 생성\n",
    "endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(key)\n",
    ")\n",
    "\n",
    "# 로컬 PDF 파일 분석\n",
    "with open(\"sample.pdf\", \"rb\") as f:\n",
    "    poller = document_analysis_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\", document=f\n",
    "    )\n",
    "    result = poller.result()\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"분석된 페이지 수: {len(result.pages)}\")\n",
    "print(f\"추출된 단락 수: {len(result.paragraphs)}\")\n",
    "print(f\"추출된 표 수: {len(result.tables)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e256fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Blob Storage URL로 분석\n",
    "document_url = \"https://stragdocs2025.blob.core.windows.net/rag-documents/sample.pdf?<sas-token>\"\n",
    "\n",
    "poller = document_analysis_client.begin_analyze_document_from_url(\n",
    "    \"prebuilt-layout\", document_url=document_url\n",
    ")\n",
    "result = poller.result()\n",
    "\n",
    "print(\"문서 분석 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8eddc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_text(result):\n",
    "    \"\"\"문서에서 전체 텍스트 추출\"\"\"\n",
    "    full_text = []\n",
    "    \n",
    "    # 페이지별 텍스트 추출\n",
    "    for page in result.pages:\n",
    "        page_text = []\n",
    "        for line in page.lines:\n",
    "            page_text.append(line.content)\n",
    "        \n",
    "        full_text.append(\"\\n\".join(page_text))\n",
    "    \n",
    "    return \"\\n\\n\".join(full_text)\n",
    "\n",
    "# 사용 예시\n",
    "text = extract_full_text(result)\n",
    "print(f\"추출된 텍스트 (앞 500자):\\n{text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "085de46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs(result):\n",
    "    \"\"\"단락 단위로 텍스트 추출 (레이아웃 유지)\"\"\"\n",
    "    paragraphs = []\n",
    "    \n",
    "    for paragraph in result.paragraphs:\n",
    "        paragraphs.append({\n",
    "            \"content\": paragraph.content,\n",
    "            \"role\": paragraph.role,  # title, sectionHeading, paragraph 등\n",
    "            \"page_number\": paragraph.bounding_regions[0].page_number if paragraph.bounding_regions else None\n",
    "        })\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "# 사용 예시\n",
    "paragraphs = extract_paragraphs(result)\n",
    "print(f\"총 {len(paragraphs)}개 단락 추출\")\n",
    "\n",
    "# 제목만 추출\n",
    "titles = [p for p in paragraphs if p[\"role\"] == \"title\"]\n",
    "print(f\"\\n문서 제목들:\")\n",
    "for title in titles:\n",
    "    print(f\"- {title['content']} (페이지 {title['page_number']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e10546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables(result):\n",
    "    \"\"\"문서에서 표 추출\"\"\"\n",
    "    tables_data = []\n",
    "    \n",
    "    for table_idx, table in enumerate(result.tables):\n",
    "        # 표 메타데이터\n",
    "        table_info = {\n",
    "            \"table_id\": table_idx + 1,\n",
    "            \"row_count\": table.row_count,\n",
    "            \"column_count\": table.column_count,\n",
    "            \"page_number\": table.bounding_regions[0].page_number,\n",
    "            \"cells\": []\n",
    "        }\n",
    "        \n",
    "        # 셀 데이터\n",
    "        for cell in table.cells:\n",
    "            table_info[\"cells\"].append({\n",
    "                \"row_index\": cell.row_index,\n",
    "                \"column_index\": cell.column_index,\n",
    "                \"content\": cell.content,\n",
    "                \"kind\": cell.kind  # columnHeader, rowHeader, content, stub\n",
    "            })\n",
    "        \n",
    "        tables_data.append(table_info)\n",
    "    \n",
    "    return tables_data\n",
    "\n",
    "# 사용 예시\n",
    "tables = extract_tables(result)\n",
    "print(f\"추출된 표 개수: {len(tables)}\")\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"\\n[표 {table['table_id']}] 페이지 {table['page_number']}\")\n",
    "    print(f\"크기: {table['row_count']}행 × {table['column_count']}열\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3928d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def table_to_dataframe(table):\n",
    "    \"\"\"표를 Pandas DataFrame으로 변환\"\"\"\n",
    "    # 표 초기화 (빈 셀 포함)\n",
    "    data = [[None] * table[\"column_count\"] for _ in range(table[\"row_count\"])]\n",
    "    \n",
    "    # 셀 데이터 채우기\n",
    "    for cell in table[\"cells\"]:\n",
    "        data[cell[\"row_index\"]][cell[\"column_index\"]] = cell[\"content\"]\n",
    "    \n",
    "    # DataFrame 생성 (첫 행을 헤더로)\n",
    "    df = pd.DataFrame(data[1:], columns=data[0])\n",
    "    return df\n",
    "\n",
    "# 사용 예시\n",
    "if tables:\n",
    "    df = table_to_dataframe(tables[0])\n",
    "    print(\"\\n표 데이터 (DataFrame):\")\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16eeb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "def split_by_layout(result, max_chunk_size: int = 1000) -> List[Document]:\n",
    "    \"\"\"레이아웃 기반 문서 분할\"\"\"\n",
    "    documents = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    current_page = 1\n",
    "    \n",
    "    for paragraph in result.paragraphs:\n",
    "        # 단락 정보\n",
    "        content = paragraph.content\n",
    "        role = paragraph.role or \"paragraph\"\n",
    "        page_num = paragraph.bounding_regions[0].page_number if paragraph.bounding_regions else current_page\n",
    "        \n",
    "        # 제목은 새로운 청크 시작\n",
    "        if role in [\"title\", \"sectionHeading\"] and current_chunk:\n",
    "            # 이전 청크 저장\n",
    "            doc = Document(\n",
    "                page_content=\"\\n\\n\".join(current_chunk),\n",
    "                metadata={\n",
    "                    \"page\": current_page,\n",
    "                    \"chunk_type\": \"section\"\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "        \n",
    "        # 현재 단락 추가\n",
    "        current_chunk.append(content)\n",
    "        current_size += len(content)\n",
    "        current_page = page_num\n",
    "        \n",
    "        # 최대 크기 초과 시 청크 분할\n",
    "        if current_size >= max_chunk_size:\n",
    "            doc = Document(\n",
    "                page_content=\"\\n\\n\".join(current_chunk),\n",
    "                metadata={\n",
    "                    \"page\": current_page,\n",
    "                    \"chunk_type\": \"paragraph\"\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "    \n",
    "    # 마지막 청크\n",
    "    if current_chunk:\n",
    "        doc = Document(\n",
    "            page_content=\"\\n\\n\".join(current_chunk),\n",
    "            metadata={\n",
    "                \"page\": current_page,\n",
    "                \"chunk_type\": \"paragraph\"\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# 사용 예시\n",
    "documents = split_by_layout(result, max_chunk_size=1000)\n",
    "print(f\"생성된 청크 수: {len(documents)}\")\n",
    "print(f\"\\n첫 번째 청크 (앞 300자):\\n{documents[0].page_content[:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc5101f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Blob Storage 클라이언트\n",
    "blob_service_client = BlobServiceClient.from_connection_string(\n",
    "    os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    ")\n",
    "\n",
    "def analyze_blob_document(container_name: str, blob_name: str):\n",
    "    \"\"\"Blob Storage의 문서를 Document Intelligence로 분석\"\"\"\n",
    "    \n",
    "    # Blob SAS URL 생성\n",
    "    from azure.storage.blob import generate_blob_sas, BlobSasPermissions\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    sas_token = generate_blob_sas(\n",
    "        account_name=\"stragdocs2025\",\n",
    "        container_name=container_name,\n",
    "        blob_name=blob_name,\n",
    "        account_key=os.getenv(\"AZURE_STORAGE_KEY\"),\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=datetime.utcnow() + timedelta(hours=1)\n",
    "    )\n",
    "    \n",
    "    blob_url = f\"https://stragdocs2025.blob.core.windows.net/{container_name}/{blob_name}?{sas_token}\"\n",
    "    \n",
    "    # Document Intelligence로 분석\n",
    "    poller = document_analysis_client.begin_analyze_document_from_url(\n",
    "        \"prebuilt-layout\", document_url=blob_url\n",
    "    )\n",
    "    result = poller.result()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 사용 예시\n",
    "result = analyze_blob_document(\"rag-documents\", \"sample.pdf\")\n",
    "print(\"Blob 문서 분석 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40fafa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_blobs(container_name: str):\n",
    "    \"\"\"컨테이너의 모든 문서 분석\"\"\"\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_list = container_client.list_blobs()\n",
    "    \n",
    "    results = []\n",
    "    for blob in blob_list:\n",
    "        # PDF 파일만 처리\n",
    "        if blob.name.endswith('.pdf'):\n",
    "            print(f\"분석 중: {blob.name}\")\n",
    "            try:\n",
    "                result = analyze_blob_document(container_name, blob.name)\n",
    "                text = extract_full_text(result)\n",
    "                \n",
    "                results.append({\n",
    "                    \"blob_name\": blob.name,\n",
    "                    \"page_count\": len(result.pages),\n",
    "                    \"text\": text,\n",
    "                    \"status\": \"success\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"오류: {blob.name} - {str(e)}\")\n",
    "                results.append({\n",
    "                    \"blob_name\": blob.name,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 사용 예시\n",
    "# results = analyze_all_blobs(\"rag-documents\")\n",
    "# print(f\"총 {len(results)}개 문서 처리 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1db25855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read API로 문서 분석\n",
    "with open(\"simple_document.pdf\", \"rb\") as f:\n",
    "    poller = document_analysis_client.begin_analyze_document(\n",
    "        \"prebuilt-read\", document=f\n",
    "    )\n",
    "    result = poller.result()\n",
    "\n",
    "# 텍스트만 추출 (레이아웃 무시)\n",
    "full_text = result.content\n",
    "print(f\"추출된 텍스트:\\n{full_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f843862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 문서 분석 (언어 힌트)\n",
    "with open(\"korean_document.pdf\", \"rb\") as f:\n",
    "    poller = document_analysis_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\",\n",
    "        document=f,\n",
    "        locale=\"ko-KR\"  # 한국어 힌트\n",
    "    )\n",
    "    result = poller.result()\n",
    "\n",
    "print(\"한국어 문서 분석 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "274cfa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_korean_text(text: str) -> str:\n",
    "    \"\"\"한글 OCR 결과 정리\"\"\"\n",
    "    # 불필요한 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 줄바꿈 정리\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    \n",
    "    # 특수문자 정리\n",
    "    text = text.replace('〃', '\"').replace('〃', '\"')\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 사용 예시\n",
    "raw_text = extract_full_text(result)\n",
    "cleaned_text = clean_korean_text(raw_text)\n",
    "print(f\"정리된 텍스트:\\n{cleaned_text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc5980df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "\n",
    "# Document Intelligence Loader 생성\n",
    "loader = AzureAIDocumentIntelligenceLoader(\n",
    "    api_endpoint=endpoint,\n",
    "    api_key=key,\n",
    "    file_path=\"sample.pdf\",\n",
    "    api_model=\"prebuilt-layout\"\n",
    ")\n",
    "\n",
    "# 문서 로딩\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"로딩된 문서 수: {len(documents)}\")\n",
    "print(f\"첫 번째 청크:\\n{documents[0].page_content[:300]}\")\n",
    "print(f\"메타데이터: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26f898ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Document Intelligence로 문서 로딩\n",
    "loader = AzureAIDocumentIntelligenceLoader(\n",
    "    api_endpoint=endpoint,\n",
    "    api_key=key,\n",
    "    file_path=\"long_document.pdf\",\n",
    "    api_model=\"prebuilt-layout\"\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# 텍스트 분할 (레이아웃 고려)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"총 {len(splits)}개 청크 생성\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc139f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "\n",
    "def get_cache_key(file_path: str) -> str:\n",
    "    \"\"\"파일 해시로 캐시 키 생성\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "    return f\"doc_intel_{file_hash}\"\n",
    "\n",
    "def analyze_with_cache(file_path: str):\n",
    "    \"\"\"캐싱을 사용한 문서 분석\"\"\"\n",
    "    cache_key = get_cache_key(file_path)\n",
    "    cache_file = f\".cache/{cache_key}.json\"\n",
    "    \n",
    "    # 캐시 확인\n",
    "    try:\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            cached_result = json.load(f)\n",
    "            print(\"캐시에서 결과 로딩\")\n",
    "            return cached_result\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    # Document Intelligence 실행\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", document=f\n",
    "        )\n",
    "        result = poller.result()\n",
    "    \n",
    "    # 결과를 딕셔너리로 변환\n",
    "    result_dict = {\n",
    "        \"content\": result.content,\n",
    "        \"pages\": [{\"page_number\": p.page_number, \"width\": p.width, \"height\": p.height} for p in result.pages],\n",
    "        \"paragraphs\": [{\"content\": p.content, \"role\": p.role} for p in result.paragraphs]\n",
    "    }\n",
    "    \n",
    "    # 캐시 저장\n",
    "    os.makedirs(\".cache\", exist_ok=True)\n",
    "    with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result_dict, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"Document Intelligence 실행 및 캐시 저장\")\n",
    "    return result_dict\n",
    "\n",
    "# 사용 예시\n",
    "# cached_result = analyze_with_cache(\"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb7da4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_documents_batch(file_paths: List[str], batch_size: int = 5):\n",
    "    \"\"\"배치 단위로 문서 분석 (API 제한 고려)\"\"\"\n",
    "    import time\n",
    "    \n",
    "    results = []\n",
    "    for i in range(0, len(file_paths), batch_size):\n",
    "        batch = file_paths[i:i+batch_size]\n",
    "        \n",
    "        print(f\"배치 {i//batch_size + 1} 처리 중 ({len(batch)}개 파일)\")\n",
    "        for file_path in batch:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                poller = document_analysis_client.begin_analyze_document(\n",
    "                    \"prebuilt-layout\", document=f\n",
    "                )\n",
    "                result = poller.result()\n",
    "                results.append({\n",
    "                    \"file\": file_path,\n",
    "                    \"result\": result\n",
    "                })\n",
    "        \n",
    "        # API 제한 방지 (초당 15 요청)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 사용 예시\n",
    "# file_list = [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]\n",
    "# results = analyze_documents_batch(file_list, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cad9098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_metrics(file_path: str):\n",
    "    \"\"\"분석 메트릭 추적\"\"\"\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with open(file_path, \"rb\") as f:\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        \n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", document=f\n",
    "        )\n",
    "        result = poller.result()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    metrics = {\n",
    "        \"file_path\": file_path,\n",
    "        \"file_size_mb\": file_size / (1024 * 1024),\n",
    "        \"page_count\": len(result.pages),\n",
    "        \"duration_seconds\": duration,\n",
    "        \"pages_per_second\": len(result.pages) / duration\n",
    "    }\n",
    "    \n",
    "    print(f\"분석 완료:\")\n",
    "    print(f\"- 파일 크기: {metrics['file_size_mb']:.2f} MB\")\n",
    "    print(f\"- 페이지 수: {metrics['page_count']}\")\n",
    "    print(f\"- 처리 시간: {metrics['duration_seconds']:.2f}초\")\n",
    "    print(f\"- 속도: {metrics['pages_per_second']:.2f} 페이지/초\")\n",
    "    \n",
    "    return result, metrics\n",
    "\n",
    "# 사용 예시\n",
    "# result, metrics = analyze_with_metrics(\"large_document.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}