{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58c8f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import json\n",
    "\n",
    "# figure size/format\n",
    "fig_width = 7\n",
    "fig_height = 5\n",
    "fig_format = 'retina'\n",
    "fig_dpi = 96\n",
    "interactivity = ''\n",
    "is_shiny = False\n",
    "is_dashboard = False\n",
    "plotly_connected = True\n",
    "\n",
    "# matplotlib defaults / format\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "  plt.rcParams['figure.dpi'] = fig_dpi\n",
    "  plt.rcParams['savefig.dpi'] = fig_dpi\n",
    "  from IPython.display import set_matplotlib_formats\n",
    "  set_matplotlib_formats(fig_format)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# plotly use connected mode\n",
    "try:\n",
    "  import plotly.io as pio\n",
    "  if plotly_connected:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "  else:\n",
    "    pio.renderers.default = \"notebook\"\n",
    "  for template in pio.templates.keys():\n",
    "    pio.templates[template].layout.margin = dict(t=30,r=0,b=0,l=0)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# disable itables paging for dashboards\n",
    "if is_dashboard:\n",
    "  try:\n",
    "    from itables import options\n",
    "    options.dom = 'fiBrtlp'\n",
    "    options.maxBytes = 1024 * 1024\n",
    "    options.language = dict(info = \"Showing _TOTAL_ entries\")\n",
    "    options.classes = \"display nowrap compact\"\n",
    "    options.paging = False\n",
    "    options.searching = True\n",
    "    options.ordering = True\n",
    "    options.info = True\n",
    "    options.lengthChange = False\n",
    "    options.autoWidth = False\n",
    "    options.responsive = True\n",
    "    options.keys = True\n",
    "    options.buttons = []\n",
    "  except Exception:\n",
    "    pass\n",
    "  \n",
    "  try:\n",
    "    import altair as alt\n",
    "    # By default, dashboards will have container sized\n",
    "    # vega visualizations which allows them to flow reasonably\n",
    "    theme_sentinel = '_quarto-dashboard-internal'\n",
    "    def make_theme(name):\n",
    "        nonTheme = alt.themes._plugins[name]    \n",
    "        def patch_theme(*args, **kwargs):\n",
    "            existingTheme = nonTheme()\n",
    "            if 'height' not in existingTheme:\n",
    "              existingTheme['height'] = 'container'\n",
    "            if 'width' not in existingTheme:\n",
    "              existingTheme['width'] = 'container'\n",
    "\n",
    "            if 'config' not in existingTheme:\n",
    "              existingTheme['config'] = dict()\n",
    "            \n",
    "            # Configure the default font sizes\n",
    "            title_font_size = 15\n",
    "            header_font_size = 13\n",
    "            axis_font_size = 12\n",
    "            legend_font_size = 12\n",
    "            mark_font_size = 12\n",
    "            tooltip = False\n",
    "\n",
    "            config = existingTheme['config']\n",
    "\n",
    "            # The Axis\n",
    "            if 'axis' not in config:\n",
    "              config['axis'] = dict()\n",
    "            axis = config['axis']\n",
    "            if 'labelFontSize' not in axis:\n",
    "              axis['labelFontSize'] = axis_font_size\n",
    "            if 'titleFontSize' not in axis:\n",
    "              axis['titleFontSize'] = axis_font_size  \n",
    "\n",
    "            # The legend\n",
    "            if 'legend' not in config:\n",
    "              config['legend'] = dict()\n",
    "            legend = config['legend']\n",
    "            if 'labelFontSize' not in legend:\n",
    "              legend['labelFontSize'] = legend_font_size\n",
    "            if 'titleFontSize' not in legend:\n",
    "              legend['titleFontSize'] = legend_font_size  \n",
    "\n",
    "            # The header\n",
    "            if 'header' not in config:\n",
    "              config['header'] = dict()\n",
    "            header = config['header']\n",
    "            if 'labelFontSize' not in header:\n",
    "              header['labelFontSize'] = header_font_size\n",
    "            if 'titleFontSize' not in header:\n",
    "              header['titleFontSize'] = header_font_size    \n",
    "\n",
    "            # Title\n",
    "            if 'title' not in config:\n",
    "              config['title'] = dict()\n",
    "            title = config['title']\n",
    "            if 'fontSize' not in title:\n",
    "              title['fontSize'] = title_font_size\n",
    "\n",
    "            # Marks\n",
    "            if 'mark' not in config:\n",
    "              config['mark'] = dict()\n",
    "            mark = config['mark']\n",
    "            if 'fontSize' not in mark:\n",
    "              mark['fontSize'] = mark_font_size\n",
    "\n",
    "            # Mark tooltips\n",
    "            if tooltip and 'tooltip' not in mark:\n",
    "              mark['tooltip'] = dict(content=\"encoding\")\n",
    "\n",
    "            return existingTheme\n",
    "            \n",
    "        return patch_theme\n",
    "\n",
    "    # We can only do this once per session\n",
    "    if theme_sentinel not in alt.themes.names():\n",
    "      for name in alt.themes.names():\n",
    "        alt.themes.register(name, make_theme(name))\n",
    "      \n",
    "      # register a sentinel theme so we only do this once\n",
    "      alt.themes.register(theme_sentinel, make_theme('default'))\n",
    "      alt.themes.enable('default')\n",
    "\n",
    "  except Exception:\n",
    "    pass\n",
    "\n",
    "# enable pandas latex repr when targeting pdfs\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if fig_format == 'pdf':\n",
    "    pd.set_option('display.latex.repr', True)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# interactivity\n",
    "if interactivity:\n",
    "  from IPython.core.interactiveshell import InteractiveShell\n",
    "  InteractiveShell.ast_node_interactivity = interactivity\n",
    "\n",
    "# NOTE: the kernel_deps code is repeated in the cleanup.py file\n",
    "# (we can't easily share this code b/c of the way it is run).\n",
    "# If you edit this code also edit the same code in cleanup.py!\n",
    "\n",
    "# output kernel dependencies\n",
    "kernel_deps = dict()\n",
    "for module in list(sys.modules.values()):\n",
    "  # Some modules play games with sys.modules (e.g. email/__init__.py\n",
    "  # in the standard library), and occasionally this can cause strange\n",
    "  # failures in getattr.  Just ignore anything that's not an ordinary\n",
    "  # module.\n",
    "  if not isinstance(module, types.ModuleType):\n",
    "    continue\n",
    "  path = getattr(module, \"__file__\", None)\n",
    "  if not path:\n",
    "    continue\n",
    "  if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n",
    "    path = path[:-1]\n",
    "  if not os.path.exists(path):\n",
    "    continue\n",
    "  kernel_deps[path] = os.stat(path).st_mtime\n",
    "print(json.dumps(kernel_deps))\n",
    "\n",
    "# set run_path if requested\n",
    "if r'C:\\Users\\kmkim\\Desktop\\projects\\blog\\docs\\blog\\posts\\Agent\\13-Cloud-RAG':\n",
    "  os.chdir(r'C:\\Users\\kmkim\\Desktop\\projects\\blog\\docs\\blog\\posts\\Agent\\13-Cloud-RAG')\n",
    "\n",
    "# reset state\n",
    "%reset\n",
    "\n",
    "# shiny\n",
    "# Checking for shiny by using False directly because we're after the %reset. We don't want\n",
    "# to set a variable that stays in global scope.\n",
    "if False:\n",
    "  try:\n",
    "    import htmltools as _htmltools\n",
    "    import ast as _ast\n",
    "\n",
    "    _htmltools.html_dependency_render_mode = \"json\"\n",
    "\n",
    "    # This decorator will be added to all function definitions\n",
    "    def _display_if_has_repr_html(x):\n",
    "      try:\n",
    "        # IPython 7.14 preferred import\n",
    "        from IPython.display import display, HTML\n",
    "      except:\n",
    "        from IPython.core.display import display, HTML\n",
    "\n",
    "      if hasattr(x, '_repr_html_'):\n",
    "        display(HTML(x._repr_html_()))\n",
    "      return x\n",
    "\n",
    "    # ideally we would undo the call to ast_transformers.append\n",
    "    # at the end of this block whenver an error occurs, we do \n",
    "    # this for now as it will only be a problem if the user \n",
    "    # switches from shiny to not-shiny mode (and even then likely\n",
    "    # won't matter)\n",
    "    import builtins\n",
    "    builtins._display_if_has_repr_html = _display_if_has_repr_html\n",
    "\n",
    "    class _FunctionDefReprHtml(_ast.NodeTransformer):\n",
    "      def visit_FunctionDef(self, node):\n",
    "        node.decorator_list.insert(\n",
    "          0,\n",
    "          _ast.Name(id=\"_display_if_has_repr_html\", ctx=_ast.Load())\n",
    "        )\n",
    "        return node\n",
    "\n",
    "      def visit_AsyncFunctionDef(self, node):\n",
    "        node.decorator_list.insert(\n",
    "          0,\n",
    "          _ast.Name(id=\"_display_if_has_repr_html\", ctx=_ast.Load())\n",
    "        )\n",
    "        return node\n",
    "\n",
    "    ip = get_ipython()\n",
    "    ip.ast_transformers.append(_FunctionDefReprHtml())\n",
    "\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "def ojs_define(**kwargs):\n",
    "  import json\n",
    "  try:\n",
    "    # IPython 7.14 preferred import\n",
    "    from IPython.display import display, HTML\n",
    "  except:\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "  # do some minor magic for convenience when handling pandas\n",
    "  # dataframes\n",
    "  def convert(v):\n",
    "    try:\n",
    "      import pandas as pd\n",
    "    except ModuleNotFoundError: # don't do the magic when pandas is not available\n",
    "      return v\n",
    "    if type(v) == pd.Series:\n",
    "      v = pd.DataFrame(v)\n",
    "    if type(v) == pd.DataFrame:\n",
    "      j = json.loads(v.T.to_json(orient='split'))\n",
    "      return dict((k,v) for (k,v) in zip(j[\"index\"], j[\"data\"]))\n",
    "    else:\n",
    "      return v\n",
    "\n",
    "  v = dict(contents=list(dict(name=key, value=convert(value)) for (key, value) in kwargs.items()))\n",
    "  display(HTML('<script type=\"ojs-define\">' + json.dumps(v) + '</script>'), metadata=dict(ojs_define = True))\n",
    "globals()[\"ojs_define\"] = ojs_define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5690d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI 클라이언트 생성\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# 텍스트 임베딩 생성\n",
    "text = \"Azure OpenAI는 Microsoft의 관리형 OpenAI 서비스다.\"\n",
    "response = client.embeddings.create(\n",
    "    input=text,\n",
    "    model=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    ")\n",
    "\n",
    "# 임베딩 벡터 추출\n",
    "embedding = response.data[0].embedding\n",
    "\n",
    "print(f\"임베딩 차원: {len(embedding)}\")\n",
    "print(f\"첫 10개 값: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbae0f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 텍스트 동시 임베딩\n",
    "texts = [\n",
    "    \"Azure는 Microsoft의 클라우드 플랫폼이다.\",\n",
    "    \"RAG는 검색 증강 생성 기술이다.\",\n",
    "    \"임베딩은 텍스트를 벡터로 변환한다.\"\n",
    "]\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=texts,\n",
    "    model=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    ")\n",
    "\n",
    "# 모든 임베딩 추출\n",
    "embeddings = [data.embedding for data in response.data]\n",
    "\n",
    "print(f\"생성된 임베딩 수: {len(embeddings)}\")\n",
    "print(f\"각 임베딩 차원: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da7215f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# LangChain 임베딩 클래스 생성\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# 단일 텍스트 임베딩\n",
    "text = \"LangChain은 LLM 애플리케이션 프레임워크다.\"\n",
    "embedding = embeddings.embed_query(text)\n",
    "\n",
    "print(f\"임베딩 차원: {len(embedding)}\")\n",
    "print(f\"첫 5개 값: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f7a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# 문서 목록 생성\n",
    "documents = [\n",
    "    Document(page_content=\"Azure AI Search는 벡터 데이터베이스다.\", metadata={\"source\": \"doc1\"}),\n",
    "    Document(page_content=\"Document Intelligence는 OCR 서비스다.\", metadata={\"source\": \"doc2\"}),\n",
    "    Document(page_content=\"Blob Storage는 파일 저장소다.\", metadata={\"source\": \"doc3\"})\n",
    "]\n",
    "\n",
    "# 문서 텍스트만 추출하여 임베딩\n",
    "texts = [doc.page_content for doc in documents]\n",
    "doc_embeddings = embeddings.embed_documents(texts)\n",
    "\n",
    "print(f\"임베딩된 문서 수: {len(doc_embeddings)}\")\n",
    "print(f\"첫 번째 문서 임베딩 차원: {len(doc_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cced0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# 1. 문서 로딩\n",
    "loader = TextLoader(\"sample_document.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "\n",
    "# 3. 각 청크 임베딩\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "chunk_embeddings = embeddings.embed_documents(chunk_texts)\n",
    "\n",
    "print(f\"임베딩 완료: {len(chunk_embeddings)}개 청크\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "024bf83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩과 메타데이터를 함께 저장\n",
    "embedded_chunks = []\n",
    "for chunk, embedding in zip(chunks, chunk_embeddings):\n",
    "    embedded_chunks.append({\n",
    "        \"text\": chunk.page_content,\n",
    "        \"embedding\": embedding,\n",
    "        \"metadata\": {\n",
    "            **chunk.metadata,\n",
    "            \"chunk_size\": len(chunk.page_content),\n",
    "            \"embedding_model\": \"text-embedding-3-small\"\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(f\"첫 번째 청크 메타데이터: {embedded_chunks[0]['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9148974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"두 벡터 간 코사인 유사도 계산\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# 예시: 질의와 문서 유사도 계산\n",
    "query = \"Azure의 AI 서비스는 무엇인가?\"\n",
    "doc1 = \"Azure AI Search는 검색 서비스다.\"\n",
    "doc2 = \"Blob Storage는 파일 저장소다.\"\n",
    "\n",
    "# 임베딩 생성\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "doc1_embedding = embeddings.embed_query(doc1)\n",
    "doc2_embedding = embeddings.embed_query(doc2)\n",
    "\n",
    "# 유사도 계산\n",
    "similarity1 = cosine_similarity(query_embedding, doc1_embedding)\n",
    "similarity2 = cosine_similarity(query_embedding, doc2_embedding)\n",
    "\n",
    "print(f\"질의 vs 문서1 유사도: {similarity1:.4f}\")\n",
    "print(f\"질의 vs 문서2 유사도: {similarity2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501f6b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(query_embedding, doc_embeddings, top_k=3):\n",
    "    \"\"\"가장 유사한 문서 찾기\"\"\"\n",
    "    similarities = []\n",
    "    for idx, doc_emb in enumerate(doc_embeddings):\n",
    "        sim = cosine_similarity(query_embedding, doc_emb)\n",
    "        similarities.append((idx, sim))\n",
    "    \n",
    "    # 유사도 높은 순으로 정렬\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# 사용 예시\n",
    "query = \"RAG 시스템 구축 방법\"\n",
    "query_emb = embeddings.embed_query(query)\n",
    "\n",
    "# 상위 3개 유사 문서 찾기\n",
    "top_docs = find_most_similar(query_emb, chunk_embeddings, top_k=3)\n",
    "\n",
    "print(\"가장 유사한 문서:\")\n",
    "for idx, similarity in top_docs:\n",
    "    print(f\"청크 {idx}: {similarity:.4f}\")\n",
    "    print(f\"내용: {chunks[idx].page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9210a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 512 차원으로 축소 (기본 1536)\n",
    "response = client.embeddings.create(\n",
    "    input=\"Azure OpenAI 임베딩 테스트\",\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=512  # 1536 → 512\n",
    ")\n",
    "\n",
    "embedding_512 = response.data[0].embedding\n",
    "print(f\"축소된 차원: {len(embedding_512)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df0d81e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 차원 (1536) vs 축소 차원 (512) 비교\n",
    "text1 = \"Azure는 클라우드 플랫폼이다.\"\n",
    "text2 = \"Microsoft의 클라우드 서비스다.\"\n",
    "text3 = \"Python은 프로그래밍 언어다.\"\n",
    "\n",
    "# 1536 차원\n",
    "resp_full = client.embeddings.create(\n",
    "    input=[text1, text2, text3],\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "emb_full = [d.embedding for d in resp_full.data]\n",
    "\n",
    "# 512 차원\n",
    "resp_reduced = client.embeddings.create(\n",
    "    input=[text1, text2, text3],\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=512\n",
    ")\n",
    "emb_reduced = [d.embedding for d in resp_reduced.data]\n",
    "\n",
    "# 유사도 비교\n",
    "sim_full_12 = cosine_similarity(emb_full[0], emb_full[1])\n",
    "sim_reduced_12 = cosine_similarity(emb_reduced[0], emb_reduced[1])\n",
    "\n",
    "print(f\"1536 차원: text1-text2 유사도 = {sim_full_12:.4f}\")\n",
    "print(f\"512 차원: text1-text2 유사도 = {sim_reduced_12:.4f}\")\n",
    "print(f\"차이: {abs(sim_full_12 - sim_reduced_12):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d81adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents_batch(texts, batch_size=100):\n",
    "    \"\"\"배치 단위로 대량 문서 임베딩\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        print(f\"배치 {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1} 처리 중...\")\n",
    "        \n",
    "        # 배치 임베딩\n",
    "        batch_embeddings = embeddings.embed_documents(batch)\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# 사용 예시 (1000개 문서)\n",
    "# large_texts = [f\"문서 {i} 내용...\" for i in range(1000)]\n",
    "# embeddings_result = embed_documents_batch(large_texts, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79bb52ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def embed_chunk(texts_chunk):\n",
    "    \"\"\"청크 단위 임베딩 (병렬 실행용)\"\"\"\n",
    "    return embeddings.embed_documents(texts_chunk)\n",
    "\n",
    "def embed_parallel(texts, num_workers=4, chunk_size=25):\n",
    "    \"\"\"병렬로 임베딩 생성\"\"\"\n",
    "    # 청크로 분할\n",
    "    chunks = [texts[i:i+chunk_size] for i in range(0, len(texts), chunk_size)]\n",
    "    \n",
    "    # 병렬 실행\n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = list(executor.map(embed_chunk, chunks))\n",
    "    \n",
    "    # 결과 병합\n",
    "    all_embeddings = [emb for chunk_result in results for emb in chunk_result]\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"병렬 처리 완료: {len(all_embeddings)}개 임베딩, {duration:.2f}초\")\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# 사용 예시\n",
    "# texts_sample = [f\"샘플 텍스트 {i}\" for i in range(100)]\n",
    "# parallel_embeddings = embed_parallel(texts_sample, num_workers=4, chunk_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dcf9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "class EmbeddingCache:\n",
    "    def __init__(self, cache_file=\".embedding_cache.json\"):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = self._load_cache()\n",
    "    \n",
    "    def _load_cache(self):\n",
    "        \"\"\"캐시 파일 로드\"\"\"\n",
    "        try:\n",
    "            with open(self.cache_file, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        \"\"\"캐시 파일 저장\"\"\"\n",
    "        with open(self.cache_file, \"w\") as f:\n",
    "            json.dump(self.cache, f)\n",
    "    \n",
    "    def _get_hash(self, text):\n",
    "        \"\"\"텍스트 해시 생성\"\"\"\n",
    "        return hashlib.md5(text.encode()).hexdigest()\n",
    "    \n",
    "    def get_embedding(self, text, embeddings_func):\n",
    "        \"\"\"캐시된 임베딩 반환 또는 생성\"\"\"\n",
    "        text_hash = self._get_hash(text)\n",
    "        \n",
    "        # 캐시 확인\n",
    "        if text_hash in self.cache:\n",
    "            print(f\"캐시 히트: {text[:50]}...\")\n",
    "            return self.cache[text_hash]\n",
    "        \n",
    "        # 임베딩 생성\n",
    "        print(f\"캐시 미스: {text[:50]}...\")\n",
    "        embedding = embeddings_func(text)\n",
    "        \n",
    "        # 캐시 저장\n",
    "        self.cache[text_hash] = embedding\n",
    "        self._save_cache()\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "# 사용 예시\n",
    "cache = EmbeddingCache()\n",
    "\n",
    "text = \"Azure OpenAI 서비스\"\n",
    "emb1 = cache.get_embedding(text, embeddings.embed_query)  # 캐시 미스\n",
    "emb2 = cache.get_embedding(text, embeddings.embed_query)  # 캐시 히트 (즉시 반환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf4d1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "# 로컬 파일 캐시 스토어\n",
    "store = LocalFileStore(\"./.embedding_cache/\")\n",
    "\n",
    "# 캐시가 적용된 임베딩\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings=embeddings,\n",
    "    document_embedding_cache=store,\n",
    "    namespace=\"azure-openai-embeddings\"\n",
    ")\n",
    "\n",
    "# 사용 (자동 캐싱)\n",
    "texts = [\"Azure AI\", \"OpenAI Service\", \"Azure AI\"]  # \"Azure AI\" 중복\n",
    "embeddings_result = cached_embedder.embed_documents(texts)\n",
    "# 두 번째 \"Azure AI\"는 캐시에서 즉시 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "487fcb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text, model=\"cl100k_base\"):\n",
    "    \"\"\"텍스트의 토큰 수 계산\"\"\"\n",
    "    encoding = tiktoken.get_encoding(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# 예시\n",
    "text = \"Azure OpenAI Embeddings는 텍스트를 벡터로 변환하는 서비스다.\"\n",
    "token_count = count_tokens(text)\n",
    "\n",
    "print(f\"텍스트: {text}\")\n",
    "print(f\"토큰 수: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6106286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cost(token_count, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"임베딩 비용 추정\"\"\"\n",
    "    prices = {\n",
    "        \"text-embedding-ada-002\": 0.0001,  # $0.10 / 1M tokens\n",
    "        \"text-embedding-3-small\": 0.00002,  # $0.02 / 1M tokens\n",
    "        \"text-embedding-3-large\": 0.00013   # $0.13 / 1M tokens\n",
    "    }\n",
    "    \n",
    "    price_per_token = prices.get(model, 0)\n",
    "    cost = token_count * price_per_token\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# 예시: 10,000개 문서 (각 500 토큰)\n",
    "total_tokens = 10000 * 500\n",
    "cost_small = estimate_cost(total_tokens, \"text-embedding-3-small\")\n",
    "cost_large = estimate_cost(total_tokens, \"text-embedding-3-large\")\n",
    "\n",
    "print(f\"총 토큰: {total_tokens:,}\")\n",
    "print(f\"text-embedding-3-small 비용: ${cost_small:.2f}\")\n",
    "print(f\"text-embedding-3-large 비용: ${cost_large:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90280ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "# Azure Search VectorStore 초기화\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    azure_search_key=os.getenv(\"AZURE_SEARCH_API_KEY\"),\n",
    "    index_name=\"rag-embeddings\",\n",
    "    embedding_function=embeddings.embed_query\n",
    ")\n",
    "\n",
    "# 문서 추가 (자동 임베딩)\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"Azure는 Microsoft의 클라우드 플랫폼이다.\", metadata={\"source\": \"doc1\"}),\n",
    "    Document(page_content=\"RAG는 검색 증강 생성 기술이다.\", metadata={\"source\": \"doc2\"}),\n",
    "    Document(page_content=\"임베딩은 텍스트를 벡터로 변환한다.\", metadata={\"source\": \"doc3\"})\n",
    "]\n",
    "\n",
    "# 문서 추가 (임베딩 자동 생성 및 업로드)\n",
    "vector_store.add_documents(documents)\n",
    "print(f\"{len(documents)}개 문서가 Azure AI Search에 추가되었다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22852716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embedding_quality(embeddings_list):\n",
    "    \"\"\"임베딩 품질 메트릭\"\"\"\n",
    "    if not embeddings_list:\n",
    "        return None\n",
    "    \n",
    "    # 벡터 길이 (L2 norm)\n",
    "    norms = [np.linalg.norm(emb) for emb in embeddings_list]\n",
    "    \n",
    "    # 차원별 통계\n",
    "    embeddings_array = np.array(embeddings_list)\n",
    "    \n",
    "    metrics = {\n",
    "        \"count\": len(embeddings_list),\n",
    "        \"dimensions\": len(embeddings_list[0]),\n",
    "        \"norm_mean\": np.mean(norms),\n",
    "        \"norm_std\": np.std(norms),\n",
    "        \"value_mean\": np.mean(embeddings_array),\n",
    "        \"value_std\": np.std(embeddings_array)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# 사용 예시\n",
    "sample_texts = [\"Azure\", \"OpenAI\", \"RAG\", \"Embeddings\"]\n",
    "sample_embeddings = embeddings.embed_documents(sample_texts)\n",
    "\n",
    "quality = check_embedding_quality(sample_embeddings)\n",
    "print(\"임베딩 품질 메트릭:\")\n",
    "for key, value in quality.items():\n",
    "    print(f\"- {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}