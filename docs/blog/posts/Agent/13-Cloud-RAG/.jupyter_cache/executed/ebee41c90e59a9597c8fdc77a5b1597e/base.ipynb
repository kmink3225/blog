{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06552ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import json\n",
    "\n",
    "# figure size/format\n",
    "fig_width = 7\n",
    "fig_height = 5\n",
    "fig_format = 'retina'\n",
    "fig_dpi = 96\n",
    "interactivity = ''\n",
    "is_shiny = False\n",
    "is_dashboard = False\n",
    "plotly_connected = True\n",
    "\n",
    "# matplotlib defaults / format\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "  plt.rcParams['figure.dpi'] = fig_dpi\n",
    "  plt.rcParams['savefig.dpi'] = fig_dpi\n",
    "  from IPython.display import set_matplotlib_formats\n",
    "  set_matplotlib_formats(fig_format)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# plotly use connected mode\n",
    "try:\n",
    "  import plotly.io as pio\n",
    "  if plotly_connected:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "  else:\n",
    "    pio.renderers.default = \"notebook\"\n",
    "  for template in pio.templates.keys():\n",
    "    pio.templates[template].layout.margin = dict(t=30,r=0,b=0,l=0)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# disable itables paging for dashboards\n",
    "if is_dashboard:\n",
    "  try:\n",
    "    from itables import options\n",
    "    options.dom = 'fiBrtlp'\n",
    "    options.maxBytes = 1024 * 1024\n",
    "    options.language = dict(info = \"Showing _TOTAL_ entries\")\n",
    "    options.classes = \"display nowrap compact\"\n",
    "    options.paging = False\n",
    "    options.searching = True\n",
    "    options.ordering = True\n",
    "    options.info = True\n",
    "    options.lengthChange = False\n",
    "    options.autoWidth = False\n",
    "    options.responsive = True\n",
    "    options.keys = True\n",
    "    options.buttons = []\n",
    "  except Exception:\n",
    "    pass\n",
    "  \n",
    "  try:\n",
    "    import altair as alt\n",
    "    # By default, dashboards will have container sized\n",
    "    # vega visualizations which allows them to flow reasonably\n",
    "    theme_sentinel = '_quarto-dashboard-internal'\n",
    "    def make_theme(name):\n",
    "        nonTheme = alt.themes._plugins[name]    \n",
    "        def patch_theme(*args, **kwargs):\n",
    "            existingTheme = nonTheme()\n",
    "            if 'height' not in existingTheme:\n",
    "              existingTheme['height'] = 'container'\n",
    "            if 'width' not in existingTheme:\n",
    "              existingTheme['width'] = 'container'\n",
    "\n",
    "            if 'config' not in existingTheme:\n",
    "              existingTheme['config'] = dict()\n",
    "            \n",
    "            # Configure the default font sizes\n",
    "            title_font_size = 15\n",
    "            header_font_size = 13\n",
    "            axis_font_size = 12\n",
    "            legend_font_size = 12\n",
    "            mark_font_size = 12\n",
    "            tooltip = False\n",
    "\n",
    "            config = existingTheme['config']\n",
    "\n",
    "            # The Axis\n",
    "            if 'axis' not in config:\n",
    "              config['axis'] = dict()\n",
    "            axis = config['axis']\n",
    "            if 'labelFontSize' not in axis:\n",
    "              axis['labelFontSize'] = axis_font_size\n",
    "            if 'titleFontSize' not in axis:\n",
    "              axis['titleFontSize'] = axis_font_size  \n",
    "\n",
    "            # The legend\n",
    "            if 'legend' not in config:\n",
    "              config['legend'] = dict()\n",
    "            legend = config['legend']\n",
    "            if 'labelFontSize' not in legend:\n",
    "              legend['labelFontSize'] = legend_font_size\n",
    "            if 'titleFontSize' not in legend:\n",
    "              legend['titleFontSize'] = legend_font_size  \n",
    "\n",
    "            # The header\n",
    "            if 'header' not in config:\n",
    "              config['header'] = dict()\n",
    "            header = config['header']\n",
    "            if 'labelFontSize' not in header:\n",
    "              header['labelFontSize'] = header_font_size\n",
    "            if 'titleFontSize' not in header:\n",
    "              header['titleFontSize'] = header_font_size    \n",
    "\n",
    "            # Title\n",
    "            if 'title' not in config:\n",
    "              config['title'] = dict()\n",
    "            title = config['title']\n",
    "            if 'fontSize' not in title:\n",
    "              title['fontSize'] = title_font_size\n",
    "\n",
    "            # Marks\n",
    "            if 'mark' not in config:\n",
    "              config['mark'] = dict()\n",
    "            mark = config['mark']\n",
    "            if 'fontSize' not in mark:\n",
    "              mark['fontSize'] = mark_font_size\n",
    "\n",
    "            # Mark tooltips\n",
    "            if tooltip and 'tooltip' not in mark:\n",
    "              mark['tooltip'] = dict(content=\"encoding\")\n",
    "\n",
    "            return existingTheme\n",
    "            \n",
    "        return patch_theme\n",
    "\n",
    "    # We can only do this once per session\n",
    "    if theme_sentinel not in alt.themes.names():\n",
    "      for name in alt.themes.names():\n",
    "        alt.themes.register(name, make_theme(name))\n",
    "      \n",
    "      # register a sentinel theme so we only do this once\n",
    "      alt.themes.register(theme_sentinel, make_theme('default'))\n",
    "      alt.themes.enable('default')\n",
    "\n",
    "  except Exception:\n",
    "    pass\n",
    "\n",
    "# enable pandas latex repr when targeting pdfs\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if fig_format == 'pdf':\n",
    "    pd.set_option('display.latex.repr', True)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# interactivity\n",
    "if interactivity:\n",
    "  from IPython.core.interactiveshell import InteractiveShell\n",
    "  InteractiveShell.ast_node_interactivity = interactivity\n",
    "\n",
    "# NOTE: the kernel_deps code is repeated in the cleanup.py file\n",
    "# (we can't easily share this code b/c of the way it is run).\n",
    "# If you edit this code also edit the same code in cleanup.py!\n",
    "\n",
    "# output kernel dependencies\n",
    "kernel_deps = dict()\n",
    "for module in list(sys.modules.values()):\n",
    "  # Some modules play games with sys.modules (e.g. email/__init__.py\n",
    "  # in the standard library), and occasionally this can cause strange\n",
    "  # failures in getattr.  Just ignore anything that's not an ordinary\n",
    "  # module.\n",
    "  if not isinstance(module, types.ModuleType):\n",
    "    continue\n",
    "  path = getattr(module, \"__file__\", None)\n",
    "  if not path:\n",
    "    continue\n",
    "  if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n",
    "    path = path[:-1]\n",
    "  if not os.path.exists(path):\n",
    "    continue\n",
    "  kernel_deps[path] = os.stat(path).st_mtime\n",
    "print(json.dumps(kernel_deps))\n",
    "\n",
    "# set run_path if requested\n",
    "if r'C:\\Users\\kmkim\\Desktop\\projects\\blog\\docs\\blog\\posts\\RAG\\13-Cloud-RAG':\n",
    "  os.chdir(r'C:\\Users\\kmkim\\Desktop\\projects\\blog\\docs\\blog\\posts\\RAG\\13-Cloud-RAG')\n",
    "\n",
    "# reset state\n",
    "%reset\n",
    "\n",
    "# shiny\n",
    "# Checking for shiny by using False directly because we're after the %reset. We don't want\n",
    "# to set a variable that stays in global scope.\n",
    "if False:\n",
    "  try:\n",
    "    import htmltools as _htmltools\n",
    "    import ast as _ast\n",
    "\n",
    "    _htmltools.html_dependency_render_mode = \"json\"\n",
    "\n",
    "    # This decorator will be added to all function definitions\n",
    "    def _display_if_has_repr_html(x):\n",
    "      try:\n",
    "        # IPython 7.14 preferred import\n",
    "        from IPython.display import display, HTML\n",
    "      except:\n",
    "        from IPython.core.display import display, HTML\n",
    "\n",
    "      if hasattr(x, '_repr_html_'):\n",
    "        display(HTML(x._repr_html_()))\n",
    "      return x\n",
    "\n",
    "    # ideally we would undo the call to ast_transformers.append\n",
    "    # at the end of this block whenver an error occurs, we do \n",
    "    # this for now as it will only be a problem if the user \n",
    "    # switches from shiny to not-shiny mode (and even then likely\n",
    "    # won't matter)\n",
    "    import builtins\n",
    "    builtins._display_if_has_repr_html = _display_if_has_repr_html\n",
    "\n",
    "    class _FunctionDefReprHtml(_ast.NodeTransformer):\n",
    "      def visit_FunctionDef(self, node):\n",
    "        node.decorator_list.insert(\n",
    "          0,\n",
    "          _ast.Name(id=\"_display_if_has_repr_html\", ctx=_ast.Load())\n",
    "        )\n",
    "        return node\n",
    "\n",
    "      def visit_AsyncFunctionDef(self, node):\n",
    "        node.decorator_list.insert(\n",
    "          0,\n",
    "          _ast.Name(id=\"_display_if_has_repr_html\", ctx=_ast.Load())\n",
    "        )\n",
    "        return node\n",
    "\n",
    "    ip = get_ipython()\n",
    "    ip.ast_transformers.append(_FunctionDefReprHtml())\n",
    "\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "def ojs_define(**kwargs):\n",
    "  import json\n",
    "  try:\n",
    "    # IPython 7.14 preferred import\n",
    "    from IPython.display import display, HTML\n",
    "  except:\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "  # do some minor magic for convenience when handling pandas\n",
    "  # dataframes\n",
    "  def convert(v):\n",
    "    try:\n",
    "      import pandas as pd\n",
    "    except ModuleNotFoundError: # don't do the magic when pandas is not available\n",
    "      return v\n",
    "    if type(v) == pd.Series:\n",
    "      v = pd.DataFrame(v)\n",
    "    if type(v) == pd.DataFrame:\n",
    "      j = json.loads(v.T.to_json(orient='split'))\n",
    "      return dict((k,v) for (k,v) in zip(j[\"index\"], j[\"data\"]))\n",
    "    else:\n",
    "      return v\n",
    "\n",
    "  v = dict(contents=list(dict(name=key, value=convert(value)) for (key, value) in kwargs.items()))\n",
    "  display(HTML('<script type=\"ojs-define\">' + json.dumps(v) + '</script>'), metadata=dict(ojs_define = True))\n",
    "globals()[\"ojs_define\"] = ojs_define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9740977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    temperature=0,  # 결정적 출력\n",
    "    max_tokens=1000  # 최대 출력 길이\n",
    ")\n",
    "\n",
    "# 테스트\n",
    "response = llm.invoke(\"Azure OpenAI란 무엇인가?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcdbe944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"당신은 Azure 전문가입니다.\"),\n",
    "    HumanMessage(content=\"Azure AI Search를 설명해주세요.\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e27d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "basic_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"다음 컨텍스트를 참고하여 질문에 답변하세요.\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8762068",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"당신은 친절한 AI 어시스턴트입니다.\n",
    "주어진 컨텍스트를 바탕으로 사용자의 질문에 정확하고 상세하게 답변하세요.\n",
    "\n",
    "## 지침:\n",
    "1. 컨텍스트에 있는 정보만 사용하세요\n",
    "2. 확실하지 않으면 \"잘 모르겠습니다\"라고 답하세요\n",
    "3. 답변은 한국어로 작성하세요\n",
    "4. 전문 용어는 쉽게 설명하세요\n",
    "\n",
    "## 컨텍스트:\n",
    "{context}\n",
    "\n",
    "## 질문:\n",
    "{question}\n",
    "\n",
    "## 답변:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f146ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"다음 컨텍스트를 참고하여 질문에 답변하세요.\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변은 다음 형식으로 작성하세요:\n",
    "\n",
    "**요약:** (한 문장 요약)\n",
    "\n",
    "**상세 설명:**\n",
    "- 핵심 포인트 1\n",
    "- 핵심 포인트 2\n",
    "- 핵심 포인트 3\n",
    "\n",
    "**출처:** (컨텍스트에서 인용)\n",
    "\n",
    "답변:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8b6fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"다음 예시를 참고하여 질문에 답변하세요.\n",
    "\n",
    "예시 1:\n",
    "질문: Azure Blob Storage란?\n",
    "답변: Azure Blob Storage는 Microsoft의 클라우드 객체 스토리지 서비스입니다. \n",
    "대량의 비구조화 데이터를 저장할 수 있으며, Hot/Cool/Archive 티어를 제공합니다.\n",
    "\n",
    "예시 2:\n",
    "질문: Document Intelligence의 용도는?\n",
    "답변: Document Intelligence는 OCR 및 문서 분석 서비스입니다.\n",
    "PDF, 이미지에서 텍스트, 표, 레이아웃을 추출하여 RAG 시스템의 입력으로 사용합니다.\n",
    "\n",
    "이제 실제 질문에 답변하세요:\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65cb8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_context(docs, max_tokens=3000):\n",
    "    \"\"\"컨텍스트를 토큰 제한 내로 압축\"\"\"\n",
    "    import tiktoken\n",
    "    \n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    compressed = []\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for doc in docs:\n",
    "        tokens = encoding.encode(doc.page_content)\n",
    "        doc_tokens = len(tokens)\n",
    "        \n",
    "        if total_tokens + doc_tokens <= max_tokens:\n",
    "            compressed.append(doc.page_content)\n",
    "            total_tokens += doc_tokens\n",
    "        else:\n",
    "            # 남은 공간에 맞춰 자르기\n",
    "            remaining = max_tokens - total_tokens\n",
    "            truncated = encoding.decode(tokens[:remaining])\n",
    "            compressed.append(truncated + \"...\")\n",
    "            break\n",
    "    \n",
    "    return \"\\n\\n\".join(compressed)\n",
    "\n",
    "# 사용\n",
    "# context = compress_context(retrieved_docs, max_tokens=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd0935be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "def reorder_context(docs):\n",
    "    \"\"\"중요한 문서를 양 끝에 배치\"\"\"\n",
    "    reorderer = LongContextReorder()\n",
    "    reordered = reorderer.transform_documents(docs)\n",
    "    return reordered\n",
    "\n",
    "# 사용: 첫 번째와 마지막 문서가 가장 중요\n",
    "# reordered_docs = reorder_context(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10407c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"다음 컨텍스트를 참고하여 질문에 단계적으로 답변하세요.\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "단계별 추론:\n",
    "1. 먼저 질문의 핵심을 파악합니다\n",
    "2. 컨텍스트에서 관련 정보를 찾습니다\n",
    "3. 정보를 종합하여 답변을 구성합니다\n",
    "\n",
    "답변:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b5469b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_ask_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"질문에 답하기 위해 필요한 하위 질문을 먼저 생성하고 답변하세요.\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "하위 질문 1: [질문]\n",
    "답변 1: [답변]\n",
    "\n",
    "하위 질문 2: [질문]\n",
    "답변 2: [답변]\n",
    "\n",
    "최종 답변: [종합 답변]\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a60d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"컨텍스트를 참고하여 답변하고, 인용 출처를 표시하세요.\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변 형식:\n",
    "답변 내용 [출처: 문서명 또는 페이지]\n",
    "\n",
    "답변:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "151bd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 활성화\n",
    "streaming_llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    streaming=True,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 스트리밍 실행\n",
    "for chunk in streaming_llm.stream(\"Azure AI Search란?\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5a4186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 스트리밍 RAG 체인\n",
    "streaming_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | korean_prompt\n",
    "    | streaming_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 실행\n",
    "print(\"답변: \", end=\"\")\n",
    "for chunk in streaming_chain.stream(\"Azure AI Search의 장점은?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc72d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    temperature=0,\n",
    "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}}\n",
    ")\n",
    "\n",
    "json_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"다음 컨텍스트를 참고하여 질문에 JSON 형식으로 답변하세요.\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "JSON 형식:\n",
    "{{\n",
    "  \"answer\": \"답변 내용\",\n",
    "  \"confidence\": \"high/medium/low\",\n",
    "  \"sources\": [\"출처1\", \"출처2\"]\n",
    "}}\n",
    "\n",
    "JSON 응답:\"\"\"\n",
    ")\n",
    "\n",
    "# 사용\n",
    "response = json_llm.invoke(\n",
    "    json_prompt.invoke({\n",
    "        \"context\": \"Azure AI Search는 벡터 검색을 지원합니다.\",\n",
    "        \"question\": \"Azure AI Search의 주요 기능은?\"\n",
    "    })\n",
    ")\n",
    "\n",
    "import json\n",
    "result = json.loads(response.content)\n",
    "print(f\"답변: {result['answer']}\")\n",
    "print(f\"신뢰도: {result['confidence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3293f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_documents(query: str) -> str:\n",
    "    \"\"\"문서를 검색하는 도구\"\"\"\n",
    "    # 실제로는 retriever 사용\n",
    "    return f\"'{query}'에 대한 검색 결과입니다.\"\n",
    "\n",
    "@tool\n",
    "def get_metadata(doc_id: str) -> dict:\n",
    "    \"\"\"문서 메타데이터를 가져오는 도구\"\"\"\n",
    "    return {\"id\": doc_id, \"title\": \"샘플 문서\", \"date\": \"2025-01-01\"}\n",
    "\n",
    "tools = [search_documents, get_metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a138e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 바인딩\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# 실행\n",
    "response = llm_with_tools.invoke(\"Azure AI Search 문서를 검색해줘\")\n",
    "\n",
    "# 도구 호출 확인\n",
    "if response.tool_calls:\n",
    "    for tool_call in response.tool_calls:\n",
    "        print(f\"도구: {tool_call['name']}\")\n",
    "        print(f\"인자: {tool_call['args']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "904952c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4\"):\n",
    "    \"\"\"토큰 수 계산\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# 프롬프트 토큰 확인\n",
    "prompt_text = korean_prompt.invoke({\n",
    "    \"context\": \"샘플 컨텍스트\",\n",
    "    \"question\": \"샘플 질문\"\n",
    "}).to_string()\n",
    "\n",
    "tokens = count_tokens(prompt_text)\n",
    "print(f\"프롬프트 토큰: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f0b9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cost(input_tokens, output_tokens, model=\"gpt-4o\"):\n",
    "    \"\"\"비용 추정 (USD)\"\"\"\n",
    "    prices = {\n",
    "        \"gpt-4o\": {\"input\": 5.0, \"output\": 15.0},  # per 1M tokens\n",
    "        \"gpt-4-turbo\": {\"input\": 10.0, \"output\": 30.0},\n",
    "        \"gpt-3.5-turbo\": {\"input\": 0.5, \"output\": 1.5}\n",
    "    }\n",
    "    \n",
    "    price = prices.get(model, prices[\"gpt-4o\"])\n",
    "    \n",
    "    input_cost = (input_tokens / 1_000_000) * price[\"input\"]\n",
    "    output_cost = (output_tokens / 1_000_000) * price[\"output\"]\n",
    "    \n",
    "    return input_cost + output_cost\n",
    "\n",
    "# 예시: 1000개 질문 (각 3000 입력, 500 출력 토큰)\n",
    "total_cost = estimate_cost(3000 * 1000, 500 * 1000, \"gpt-4o\")\n",
    "print(f\"예상 비용: ${total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b535347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# 콜백 핸들러\n",
    "callback = StdOutCallbackHandler()\n",
    "\n",
    "# LLM 호출 시 콜백 전달\n",
    "response = llm.invoke(\n",
    "    \"Azure AI Search란?\",\n",
    "    config={\"callbacks\": [callback]}\n",
    ")\n",
    "\n",
    "# 토큰 사용량 확인\n",
    "print(f\"토큰 사용량: {response.response_metadata.get('token_usage')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aff85361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=10)\n",
    ")\n",
    "def invoke_llm_with_retry(messages):\n",
    "    \"\"\"재시도 가능한 LLM 호출\"\"\"\n",
    "    return llm.invoke(messages)\n",
    "\n",
    "# 사용\n",
    "try:\n",
    "    response = invoke_llm_with_retry(\"테스트 질문\")\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(f\"LLM 호출 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "980354f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def invoke_with_rate_limit(query, requests_per_minute=60):\n",
    "    \"\"\"Rate limit을 고려한 호출\"\"\"\n",
    "    sleep_time = 60 / requests_per_minute\n",
    "    \n",
    "    response = llm.invoke(query)\n",
    "    time.sleep(sleep_time)\n",
    "    \n",
    "    return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}