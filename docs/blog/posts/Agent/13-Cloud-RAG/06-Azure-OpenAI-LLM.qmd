---
title: "Azure OpenAI LLM"
subtitle: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ìƒì„± ë° í”„ë¡¬í”„íŠ¸ ìµœì í™”
description: |
  Azure OpenAI LLMì„ í™œìš©í•œ RAG ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ êµ¬ì¶• ë° í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë°©ë²•ì„ ë‹¤ë£¬ë‹¤.
categories:
  - AI
  - RAG
  - Azure
author: Kwangmin Kim
date: 11/07/2025
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
execute:
    eval: false
---

## Azure OpenAI LLMì´ë€?

Azure OpenAI ServiceëŠ” OpenAIì˜ ê°•ë ¥í•œ ì–¸ì–´ ëª¨ë¸ì„ Azure í´ë¼ìš°ë“œì—ì„œ ì œê³µí•˜ëŠ” ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì„œë¹„ìŠ¤ë‹¤.

**RAG ì‹œìŠ¤í…œì—ì„œì˜ ì—­í• :**
- ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
- ìì—°ì–´ ë‹µë³€ ìƒì„±
- ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ë¡ 
- í•œêµ­ì–´ ì§€ì›

**ì—”í„°í”„ë¼ì´ì¦ˆ ì¥ì :**
- 99.9% SLA ë³´ì¥
- ë°ì´í„° ì£¼ê¶Œ (í•œêµ­ ë¦¬ì „)
- Private Endpoint ì§€ì›
- RBAC ê¸°ë°˜ ì ‘ê·¼ ì œì–´

## Azure OpenAI ëª¨ë¸

### GPT-4 Turbo (ê¶Œì¥)

**ì¶œì‹œ**: 2024ë…„ 4ì›”

**ì‚¬ì–‘:**
- ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°: 128K í† í°
- ì¶œë ¥ í† í°: 4,096
- ê°€ê²©: $10 / 1M ì…ë ¥ í† í°, $30 / 1M ì¶œë ¥ í† í°

**íŠ¹ì§•:**
- **ìµœê³  ì„±ëŠ¥** (ë³µì¡í•œ ì¶”ë¡ )
- JSON ëª¨ë“œ ì§€ì›
- í•¨ìˆ˜ í˜¸ì¶œ
- ë¹„ì „ (ì´ë¯¸ì§€ ì´í•´)

**RAG ê¶Œì¥**: âœ… ë†’ì€ ì •í™•ë„ í•„ìš” ì‹œ

### GPT-4o (ìµœì‹ )

**ì¶œì‹œ**: 2024ë…„ 5ì›”

**ì‚¬ì–‘:**
- ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°: 128K í† í°
- ì¶œë ¥ í† í°: 4,096
- ê°€ê²©: $5 / 1M ì…ë ¥ í† í°, $15 / 1M ì¶œë ¥ í† í° (GPT-4 ëŒ€ë¹„ **50% ì €ë ´**)

**íŠ¹ì§•:**
- GPT-4 ìˆ˜ì¤€ ì„±ëŠ¥
- **2ë°° ë¹ ë¥¸ ì†ë„**
- ë©€í‹°ëª¨ë‹¬ (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤)
- í–¥ìƒëœ í•œêµ­ì–´ ì§€ì›

**RAG ê¶Œì¥**: âœ… **ìµœì„ ì˜ ì„ íƒ** (ì„±ëŠ¥ + ë¹„ìš©)

### GPT-3.5 Turbo

**ì‚¬ì–‘:**
- ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°: 16K í† í°
- ê°€ê²©: $0.50 / 1M ì…ë ¥ í† í°, $1.50 / 1M ì¶œë ¥ í† í°

**íŠ¹ì§•:**
- ë¹ ë¥¸ ì‘ë‹µ ì†ë„
- ë‚®ì€ ë¹„ìš©
- ê°„ë‹¨í•œ ì§ˆë¬¸ì— ì í•©

**RAG ê¶Œì¥**: ë‹¨ìˆœ FAQ ë˜ëŠ” ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½

### ëª¨ë¸ ë¹„êµ

| ëª¨ë¸ | ì»¨í…ìŠ¤íŠ¸ | ì…ë ¥ ê°€ê²© | ì¶œë ¥ ê°€ê²© | ì„±ëŠ¥ | ì†ë„ | RAG ê¶Œì¥ |
|------|---------|----------|----------|------|------|---------|
| **GPT-4o** | 128K | $5/1M | $15/1M | â­â­â­â­â­ | âš¡âš¡âš¡ | âœ… **ìµœê³ ** |
| GPT-4 Turbo | 128K | $10/1M | $30/1M | â­â­â­â­â­ | âš¡âš¡ | ê³ ì •ë°€ë„ |
| GPT-3.5 Turbo | 16K | $0.50/1M | $1.50/1M | â­â­â­ | âš¡âš¡âš¡ | ê°œë°œìš© |

## í™˜ê²½ ì„¤ì •

### Azure OpenAI ë¦¬ì†ŒìŠ¤ ìƒì„±

```bash
# Azure OpenAI ë¦¬ì†ŒìŠ¤ ìƒì„±
az cognitiveservices account create \
    --name openai-rag-prod \
    --resource-group rg-rag-prod \
    --kind OpenAI \
    --sku S0 \
    --location eastus

# í‚¤ ì¡°íšŒ
az cognitiveservices account keys list \
    --name openai-rag-prod \
    --resource-group rg-rag-prod
```

### ëª¨ë¸ ë°°í¬

Azure OpenAI Studioì—ì„œ:
1. "ë°°í¬" â†’ "+ ë°°í¬ ë§Œë“¤ê¸°"
2. ëª¨ë¸ ì„ íƒ: **gpt-4o**
3. ë°°í¬ ì´ë¦„: `gpt-4o`
4. TPM: 80K (ë¶„ë‹¹ í† í° ì œí•œ)

### í™˜ê²½ ë³€ìˆ˜

`.env` íŒŒì¼:
```
AZURE_OPENAI_ENDPOINT=https://openai-rag-prod.openai.azure.com/
AZURE_OPENAI_API_KEY=your-key-here
AZURE_OPENAI_DEPLOYMENT=gpt-4o
AZURE_OPENAI_API_VERSION=2024-02-01
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

### AzureChatOpenAI ì´ˆê¸°í™”

```{python}
from langchain_openai import AzureChatOpenAI
from dotenv import load_dotenv
import os

load_dotenv()

llm = AzureChatOpenAI(
    azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
    openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    temperature=0,  # ê²°ì •ì  ì¶œë ¥
    max_tokens=1000  # ìµœëŒ€ ì¶œë ¥ ê¸¸ì´
)

# í…ŒìŠ¤íŠ¸
response = llm.invoke("Azure OpenAIë€ ë¬´ì—‡ì¸ê°€?")
print(response.content)
```

### ë©”ì‹œì§€ í˜•ì‹

```{python}
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="ë‹¹ì‹ ì€ Azure ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
    HumanMessage(content="Azure AI Searchë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
]

response = llm.invoke(messages)
print(response.content)
```

## RAG í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

### ê¸°ë³¸ RAG í”„ë¡¬í”„íŠ¸

```{python}
from langchain_core.prompts import ChatPromptTemplate

basic_prompt = ChatPromptTemplate.from_template(
    """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
)
```

### í•œêµ­ì–´ ìµœì í™” í”„ë¡¬í”„íŠ¸

```{python}
korean_prompt = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

## ì§€ì¹¨:
1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”
4. ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”

## ì»¨í…ìŠ¤íŠ¸:
{context}

## ì§ˆë¬¸:
{question}

## ë‹µë³€:"""
)
```

### êµ¬ì¡°í™”ëœ ë‹µë³€ í”„ë¡¬í”„íŠ¸

```{python}
structured_prompt = ChatPromptTemplate.from_template(
    """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ì€ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:

**ìš”ì•½:** (í•œ ë¬¸ì¥ ìš”ì•½)

**ìƒì„¸ ì„¤ëª…:**
- í•µì‹¬ í¬ì¸íŠ¸ 1
- í•µì‹¬ í¬ì¸íŠ¸ 2
- í•µì‹¬ í¬ì¸íŠ¸ 3

**ì¶œì²˜:** (ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¸ìš©)

ë‹µë³€:"""
)
```

### Few-Shot í”„ë¡¬í”„íŠ¸

```{python}
few_shot_prompt = ChatPromptTemplate.from_template(
    """ë‹¤ìŒ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì˜ˆì‹œ 1:
ì§ˆë¬¸: Azure Blob Storageë€?
ë‹µë³€: Azure Blob StorageëŠ” Microsoftì˜ í´ë¼ìš°ë“œ ê°ì²´ ìŠ¤í† ë¦¬ì§€ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. 
ëŒ€ëŸ‰ì˜ ë¹„êµ¬ì¡°í™” ë°ì´í„°ë¥¼ ì €ì¥í•  ìˆ˜ ìˆìœ¼ë©°, Hot/Cool/Archive í‹°ì–´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì˜ˆì‹œ 2:
ì§ˆë¬¸: Document Intelligenceì˜ ìš©ë„ëŠ”?
ë‹µë³€: Document IntelligenceëŠ” OCR ë° ë¬¸ì„œ ë¶„ì„ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
PDF, ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸, í‘œ, ë ˆì´ì•„ì›ƒì„ ì¶”ì¶œí•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì´ì œ ì‹¤ì œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
)
```

## ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬

### ì»¨í…ìŠ¤íŠ¸ ì••ì¶•

```{python}
def compress_context(docs, max_tokens=3000):
    """ì»¨í…ìŠ¤íŠ¸ë¥¼ í† í° ì œí•œ ë‚´ë¡œ ì••ì¶•"""
    import tiktoken
    
    encoding = tiktoken.encoding_for_model("gpt-4")
    
    compressed = []
    total_tokens = 0
    
    for doc in docs:
        tokens = encoding.encode(doc.page_content)
        doc_tokens = len(tokens)
        
        if total_tokens + doc_tokens <= max_tokens:
            compressed.append(doc.page_content)
            total_tokens += doc_tokens
        else:
            # ë‚¨ì€ ê³µê°„ì— ë§ì¶° ìë¥´ê¸°
            remaining = max_tokens - total_tokens
            truncated = encoding.decode(tokens[:remaining])
            compressed.append(truncated + "...")
            break
    
    return "\n\n".join(compressed)

# ì‚¬ìš©
# context = compress_context(retrieved_docs, max_tokens=3000)
```

### ì»¨í…ìŠ¤íŠ¸ ì¬ì •ë ¬

```{python}
from langchain.document_transformers import LongContextReorder

def reorder_context(docs):
    """ì¤‘ìš”í•œ ë¬¸ì„œë¥¼ ì–‘ ëì— ë°°ì¹˜"""
    reorderer = LongContextReorder()
    reordered = reorderer.transform_documents(docs)
    return reordered

# ì‚¬ìš©: ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ë¬¸ì„œê°€ ê°€ì¥ ì¤‘ìš”
# reordered_docs = reorder_context(retrieved_docs)
```

## ê³ ê¸‰ RAG íŒ¨í„´

### Chain-of-Thought (CoT)

```{python}
cot_prompt = ChatPromptTemplate.from_template(
    """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹¨ê³„ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹¨ê³„ë³„ ì¶”ë¡ :
1. ë¨¼ì € ì§ˆë¬¸ì˜ í•µì‹¬ì„ íŒŒì•…í•©ë‹ˆë‹¤
2. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤
3. ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€ì„ êµ¬ì„±í•©ë‹ˆë‹¤

ë‹µë³€:"""
)
```

### Self-Ask

```{python}
self_ask_prompt = ChatPromptTemplate.from_template(
    """ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ í•„ìš”í•œ í•˜ìœ„ ì§ˆë¬¸ì„ ë¨¼ì € ìƒì„±í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

í•˜ìœ„ ì§ˆë¬¸ 1: [ì§ˆë¬¸]
ë‹µë³€ 1: [ë‹µë³€]

í•˜ìœ„ ì§ˆë¬¸ 2: [ì§ˆë¬¸]
ë‹µë³€ 2: [ë‹µë³€]

ìµœì¢… ë‹µë³€: [ì¢…í•© ë‹µë³€]"""
)
```

### ì¸ìš© í¬í•¨ ë‹µë³€

```{python}
citation_prompt = ChatPromptTemplate.from_template(
    """ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ê³ , ì¸ìš© ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ í˜•ì‹:
ë‹µë³€ ë‚´ìš© [ì¶œì²˜: ë¬¸ì„œëª… ë˜ëŠ” í˜ì´ì§€]

ë‹µë³€:"""
)
```

## ìŠ¤íŠ¸ë¦¬ë°

### ì‹¤ì‹œê°„ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°

```{python}
# ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
streaming_llm = AzureChatOpenAI(
    azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
    openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    streaming=True,
    temperature=0
)

# ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
for chunk in streaming_llm.stream("Azure AI Searchë€?"):
    print(chunk.content, end="", flush=True)
```

### RAG ì²´ì¸ ìŠ¤íŠ¸ë¦¬ë°

```{python}
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# ìŠ¤íŠ¸ë¦¬ë° RAG ì²´ì¸
streaming_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | korean_prompt
    | streaming_llm
    | StrOutputParser()
)

# ì‹¤í–‰
print("ë‹µë³€: ", end="")
for chunk in streaming_chain.stream("Azure AI Searchì˜ ì¥ì ì€?"):
    print(chunk, end="", flush=True)
print()
```

## JSON ëª¨ë“œ

### êµ¬ì¡°í™”ëœ ì¶œë ¥

```{python}
json_llm = AzureChatOpenAI(
    azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
    openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    temperature=0,
    model_kwargs={"response_format": {"type": "json_object"}}
)

json_prompt = ChatPromptTemplate.from_template(
    """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

JSON í˜•ì‹:
{{
  "answer": "ë‹µë³€ ë‚´ìš©",
  "confidence": "high/medium/low",
  "sources": ["ì¶œì²˜1", "ì¶œì²˜2"]
}}

JSON ì‘ë‹µ:"""
)

# ì‚¬ìš©
response = json_llm.invoke(
    json_prompt.invoke({
        "context": "Azure AI SearchëŠ” ë²¡í„° ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.",
        "question": "Azure AI Searchì˜ ì£¼ìš” ê¸°ëŠ¥ì€?"
    })
)

import json
result = json.loads(response.content)
print(f"ë‹µë³€: {result['answer']}")
print(f"ì‹ ë¢°ë„: {result['confidence']}")
```

## í•¨ìˆ˜ í˜¸ì¶œ

### ë„êµ¬ ì •ì˜

```{python}
from langchain_core.tools import tool

@tool
def search_documents(query: str) -> str:
    """ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬"""
    # ì‹¤ì œë¡œëŠ” retriever ì‚¬ìš©
    return f"'{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤."

@tool
def get_metadata(doc_id: str) -> dict:
    """ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬"""
    return {"id": doc_id, "title": "ìƒ˜í”Œ ë¬¸ì„œ", "date": "2025-01-01"}

tools = [search_documents, get_metadata]
```

### í•¨ìˆ˜ í˜¸ì¶œ LLM

```{python}
# ë„êµ¬ ë°”ì¸ë”©
llm_with_tools = llm.bind_tools(tools)

# ì‹¤í–‰
response = llm_with_tools.invoke("Azure AI Search ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ì¤˜")

# ë„êµ¬ í˜¸ì¶œ í™•ì¸
if response.tool_calls:
    for tool_call in response.tool_calls:
        print(f"ë„êµ¬: {tool_call['name']}")
        print(f"ì¸ì: {tool_call['args']}")
```

## í† í° ìµœì í™”

### í† í° ê³„ì‚°

```{python}
import tiktoken

def count_tokens(text, model="gpt-4"):
    """í† í° ìˆ˜ ê³„ì‚°"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# í”„ë¡¬í”„íŠ¸ í† í° í™•ì¸
prompt_text = korean_prompt.invoke({
    "context": "ìƒ˜í”Œ ì»¨í…ìŠ¤íŠ¸",
    "question": "ìƒ˜í”Œ ì§ˆë¬¸"
}).to_string()

tokens = count_tokens(prompt_text)
print(f"í”„ë¡¬í”„íŠ¸ í† í°: {tokens}")
```

### ë¹„ìš© ì¶”ì •

```{python}
def estimate_cost(input_tokens, output_tokens, model="gpt-4o"):
    """ë¹„ìš© ì¶”ì • (USD)"""
    prices = {
        "gpt-4o": {"input": 5.0, "output": 15.0},  # per 1M tokens
        "gpt-4-turbo": {"input": 10.0, "output": 30.0},
        "gpt-3.5-turbo": {"input": 0.5, "output": 1.5}
    }
    
    price = prices.get(model, prices["gpt-4o"])
    
    input_cost = (input_tokens / 1_000_000) * price["input"]
    output_cost = (output_tokens / 1_000_000) * price["output"]
    
    return input_cost + output_cost

# ì˜ˆì‹œ: 1000ê°œ ì§ˆë¬¸ (ê° 3000 ì…ë ¥, 500 ì¶œë ¥ í† í°)
total_cost = estimate_cost(3000 * 1000, 500 * 1000, "gpt-4o")
print(f"ì˜ˆìƒ ë¹„ìš©: ${total_cost:.2f}")
```

## ì½œë°±ì„ í†µí•œ ëª¨ë‹ˆí„°ë§

```{python}
from langchain.callbacks import StdOutCallbackHandler

# ì½œë°± í•¸ë“¤ëŸ¬
callback = StdOutCallbackHandler()

# LLM í˜¸ì¶œ ì‹œ ì½œë°± ì „ë‹¬
response = llm.invoke(
    "Azure AI Searchë€?",
    config={"callbacks": [callback]}
)

# í† í° ì‚¬ìš©ëŸ‰ í™•ì¸
print(f"í† í° ì‚¬ìš©ëŸ‰: {response.response_metadata.get('token_usage')}")
```

## ì˜¤ë¥˜ ì²˜ë¦¬

### ì¬ì‹œë„ ë¡œì§

```{python}
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def invoke_llm_with_retry(messages):
    """ì¬ì‹œë„ ê°€ëŠ¥í•œ LLM í˜¸ì¶œ"""
    return llm.invoke(messages)

# ì‚¬ìš©
try:
    response = invoke_llm_with_retry("í…ŒìŠ¤íŠ¸ ì§ˆë¬¸")
    print(response.content)
except Exception as e:
    print(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
```

### Rate Limit ì²˜ë¦¬

```{python}
import time

def invoke_with_rate_limit(query, requests_per_minute=60):
    """Rate limitì„ ê³ ë ¤í•œ í˜¸ì¶œ"""
    sleep_time = 60 / requests_per_minute
    
    response = llm.invoke(query)
    time.sleep(sleep_time)
    
    return response
```

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/)
- [GPT-4o ëª¨ë¸](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4o-and-gpt-4-turbo)
- [í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering)

### LangChain
- [AzureChatOpenAI](https://python.langchain.com/docs/integrations/chat/azure_chat_openai)

## ë‹¤ìŒ ë‹¨ê³„

Azure OpenAI LLM ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆë‹¤ë©´, ì´ì œ ì„œë²„ë¦¬ìŠ¤ ë°°í¬ë¥¼ ì¤€ë¹„í•˜ì:

ğŸ‘‰ [07-Azure-Functions-Apps.qmd](./07-Azure-Functions-Apps.qmd) - Azure Functionsë¥¼ í™œìš©í•œ ì„œë²„ë¦¬ìŠ¤ RAG ë°°í¬