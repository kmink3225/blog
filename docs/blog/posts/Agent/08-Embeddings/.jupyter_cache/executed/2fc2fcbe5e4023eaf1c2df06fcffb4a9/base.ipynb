{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46147884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import json\n",
    "\n",
    "# figure size/format\n",
    "fig_width = 7\n",
    "fig_height = 5\n",
    "fig_format = 'retina'\n",
    "fig_dpi = 96\n",
    "interactivity = ''\n",
    "is_shiny = False\n",
    "is_dashboard = False\n",
    "plotly_connected = True\n",
    "\n",
    "# matplotlib defaults / format\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "  plt.rcParams['figure.dpi'] = fig_dpi\n",
    "  plt.rcParams['savefig.dpi'] = fig_dpi\n",
    "  from IPython.display import set_matplotlib_formats\n",
    "  set_matplotlib_formats(fig_format)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# plotly use connected mode\n",
    "try:\n",
    "  import plotly.io as pio\n",
    "  if plotly_connected:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "  else:\n",
    "    pio.renderers.default = \"notebook\"\n",
    "  for template in pio.templates.keys():\n",
    "    pio.templates[template].layout.margin = dict(t=30,r=0,b=0,l=0)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# disable itables paging for dashboards\n",
    "if is_dashboard:\n",
    "  try:\n",
    "    from itables import options\n",
    "    options.dom = 'fiBrtlp'\n",
    "    options.maxBytes = 1024 * 1024\n",
    "    options.language = dict(info = \"Showing _TOTAL_ entries\")\n",
    "    options.classes = \"display nowrap compact\"\n",
    "    options.paging = False\n",
    "    options.searching = True\n",
    "    options.ordering = True\n",
    "    options.info = True\n",
    "    options.lengthChange = False\n",
    "    options.autoWidth = False\n",
    "    options.responsive = True\n",
    "    options.keys = True\n",
    "    options.buttons = []\n",
    "  except Exception:\n",
    "    pass\n",
    "  \n",
    "  try:\n",
    "    import altair as alt\n",
    "    # By default, dashboards will have container sized\n",
    "    # vega visualizations which allows them to flow reasonably\n",
    "    theme_sentinel = '_quarto-dashboard-internal'\n",
    "    def make_theme(name):\n",
    "        nonTheme = alt.themes._plugins[name]    \n",
    "        def patch_theme(*args, **kwargs):\n",
    "            existingTheme = nonTheme()\n",
    "            if 'height' not in existingTheme:\n",
    "              existingTheme['height'] = 'container'\n",
    "            if 'width' not in existingTheme:\n",
    "              existingTheme['width'] = 'container'\n",
    "\n",
    "            if 'config' not in existingTheme:\n",
    "              existingTheme['config'] = dict()\n",
    "            \n",
    "            # Configure the default font sizes\n",
    "            title_font_size = 15\n",
    "            header_font_size = 13\n",
    "            axis_font_size = 12\n",
    "            legend_font_size = 12\n",
    "            mark_font_size = 12\n",
    "            tooltip = False\n",
    "\n",
    "            config = existingTheme['config']\n",
    "\n",
    "            # The Axis\n",
    "            if 'axis' not in config:\n",
    "              config['axis'] = dict()\n",
    "            axis = config['axis']\n",
    "            if 'labelFontSize' not in axis:\n",
    "              axis['labelFontSize'] = axis_font_size\n",
    "            if 'titleFontSize' not in axis:\n",
    "              axis['titleFontSize'] = axis_font_size  \n",
    "\n",
    "            # The legend\n",
    "            if 'legend' not in config:\n",
    "              config['legend'] = dict()\n",
    "            legend = config['legend']\n",
    "            if 'labelFontSize' not in legend:\n",
    "              legend['labelFontSize'] = legend_font_size\n",
    "            if 'titleFontSize' not in legend:\n",
    "              legend['titleFontSize'] = legend_font_size  \n",
    "\n",
    "            # The header\n",
    "            if 'header' not in config:\n",
    "              config['header'] = dict()\n",
    "            header = config['header']\n",
    "            if 'labelFontSize' not in header:\n",
    "              header['labelFontSize'] = header_font_size\n",
    "            if 'titleFontSize' not in header:\n",
    "              header['titleFontSize'] = header_font_size    \n",
    "\n",
    "            # Title\n",
    "            if 'title' not in config:\n",
    "              config['title'] = dict()\n",
    "            title = config['title']\n",
    "            if 'fontSize' not in title:\n",
    "              title['fontSize'] = title_font_size\n",
    "\n",
    "            # Marks\n",
    "            if 'mark' not in config:\n",
    "              config['mark'] = dict()\n",
    "            mark = config['mark']\n",
    "            if 'fontSize' not in mark:\n",
    "              mark['fontSize'] = mark_font_size\n",
    "\n",
    "            # Mark tooltips\n",
    "            if tooltip and 'tooltip' not in mark:\n",
    "              mark['tooltip'] = dict(content=\"encoding\")\n",
    "\n",
    "            return existingTheme\n",
    "            \n",
    "        return patch_theme\n",
    "\n",
    "    # We can only do this once per session\n",
    "    if theme_sentinel not in alt.themes.names():\n",
    "      for name in alt.themes.names():\n",
    "        alt.themes.register(name, make_theme(name))\n",
    "      \n",
    "      # register a sentinel theme so we only do this once\n",
    "      alt.themes.register(theme_sentinel, make_theme('default'))\n",
    "      alt.themes.enable('default')\n",
    "\n",
    "  except Exception:\n",
    "    pass\n",
    "\n",
    "# enable pandas latex repr when targeting pdfs\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if fig_format == 'pdf':\n",
    "    pd.set_option('display.latex.repr', True)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# interactivity\n",
    "if interactivity:\n",
    "  from IPython.core.interactiveshell import InteractiveShell\n",
    "  InteractiveShell.ast_node_interactivity = interactivity\n",
    "\n",
    "# NOTE: the kernel_deps code is repeated in the cleanup.py file\n",
    "# (we can't easily share this code b/c of the way it is run).\n",
    "# If you edit this code also edit the same code in cleanup.py!\n",
    "\n",
    "# output kernel dependencies\n",
    "kernel_deps = dict()\n",
    "for module in list(sys.modules.values()):\n",
    "  # Some modules play games with sys.modules (e.g. email/__init__.py\n",
    "  # in the standard library), and occasionally this can cause strange\n",
    "  # failures in getattr.  Just ignore anything that's not an ordinary\n",
    "  # module.\n",
    "  if not isinstance(module, types.ModuleType):\n",
    "    continue\n",
    "  path = getattr(module, \"__file__\", None)\n",
    "  if not path:\n",
    "    continue\n",
    "  if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n",
    "    path = path[:-1]\n",
    "  if not os.path.exists(path):\n",
    "    continue\n",
    "  kernel_deps[path] = os.stat(path).st_mtime\n",
    "print(json.dumps(kernel_deps))\n",
    "\n",
    "# set run_path if requested\n",
    "if r'C:\\Users\\kmkim\\Desktop\\projects\\blog\\docs\\blog\\posts\\Agent\\08-Embeddings':\n",
    "  os.chdir(r'C:\\Users\\kmkim\\Desktop\\projects\\blog\\docs\\blog\\posts\\Agent\\08-Embeddings')\n",
    "\n",
    "# reset state\n",
    "%reset\n",
    "\n",
    "# shiny\n",
    "# Checking for shiny by using False directly because we're after the %reset. We don't want\n",
    "# to set a variable that stays in global scope.\n",
    "if False:\n",
    "  try:\n",
    "    import htmltools as _htmltools\n",
    "    import ast as _ast\n",
    "\n",
    "    _htmltools.html_dependency_render_mode = \"json\"\n",
    "\n",
    "    # This decorator will be added to all function definitions\n",
    "    def _display_if_has_repr_html(x):\n",
    "      try:\n",
    "        # IPython 7.14 preferred import\n",
    "        from IPython.display import display, HTML\n",
    "      except:\n",
    "        from IPython.core.display import display, HTML\n",
    "\n",
    "      if hasattr(x, '_repr_html_'):\n",
    "        display(HTML(x._repr_html_()))\n",
    "      return x\n",
    "\n",
    "    # ideally we would undo the call to ast_transformers.append\n",
    "    # at the end of this block whenver an error occurs, we do \n",
    "    # this for now as it will only be a problem if the user \n",
    "    # switches from shiny to not-shiny mode (and even then likely\n",
    "    # won't matter)\n",
    "    import builtins\n",
    "    builtins._display_if_has_repr_html = _display_if_has_repr_html\n",
    "\n",
    "    class _FunctionDefReprHtml(_ast.NodeTransformer):\n",
    "      def visit_FunctionDef(self, node):\n",
    "        node.decorator_list.insert(\n",
    "          0,\n",
    "          _ast.Name(id=\"_display_if_has_repr_html\", ctx=_ast.Load())\n",
    "        )\n",
    "        return node\n",
    "\n",
    "      def visit_AsyncFunctionDef(self, node):\n",
    "        node.decorator_list.insert(\n",
    "          0,\n",
    "          _ast.Name(id=\"_display_if_has_repr_html\", ctx=_ast.Load())\n",
    "        )\n",
    "        return node\n",
    "\n",
    "    ip = get_ipython()\n",
    "    ip.ast_transformers.append(_FunctionDefReprHtml())\n",
    "\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "def ojs_define(**kwargs):\n",
    "  import json\n",
    "  try:\n",
    "    # IPython 7.14 preferred import\n",
    "    from IPython.display import display, HTML\n",
    "  except:\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "  # do some minor magic for convenience when handling pandas\n",
    "  # dataframes\n",
    "  def convert(v):\n",
    "    try:\n",
    "      import pandas as pd\n",
    "    except ModuleNotFoundError: # don't do the magic when pandas is not available\n",
    "      return v\n",
    "    if type(v) == pd.Series:\n",
    "      v = pd.DataFrame(v)\n",
    "    if type(v) == pd.DataFrame:\n",
    "      j = json.loads(v.T.to_json(orient='split'))\n",
    "      return dict((k,v) for (k,v) in zip(j[\"index\"], j[\"data\"]))\n",
    "    else:\n",
    "      return v\n",
    "\n",
    "  v = dict(contents=list(dict(name=key, value=convert(value)) for (key, value) in kwargs.items()))\n",
    "  display(HTML('<script type=\"ojs-define\">' + json.dumps(v) + '</script>'), metadata=dict(ojs_define = True))\n",
    "globals()[\"ojs_define\"] = ojs_define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990ec1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06824c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH08-Embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ccd53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ./cache/ 경로에 다운로드 받도록 설정\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db1040e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"안녕, 만나서 반가워.\",\n",
    "    \"LangChain simplifies the process of building applications with large language models\",\n",
    "    \"랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. \",\n",
    "    \"LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3785ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=model_name,\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19feff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Document Embedding 수행\n",
    "embedded_documents = hf_embeddings.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f0e0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[HuggingFace Endpoint Embedding]\")\n",
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200d702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Embedding 수행\n",
    "embedded_query = hf_embeddings.embed_query(\"LangChain 에 대해서 알려주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c30abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ce3f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 질문(embedded_query): LangChain 에 대해서 알려주세요.\n",
    "np.array(embedded_query) @ np.array(embedded_documents).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36092ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = (np.array(embedded_query) @ np.array(embedded_documents).T).argsort()[::-1]\n",
    "sorted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caf483b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Query] LangChain 에 대해서 알려주세요.\\n====================================\")\n",
    "for i, idx in enumerate(sorted_idx):\n",
    "    print(f\"[{i}] {texts[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f120937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "# model_name = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"mps\"},  # cuda, cpu\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2f432d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Document\n",
    "embedded_documents1 = hf_embeddings.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fece5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "517fa7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"mps\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8548fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Document\n",
    "embedded_documents = hf_embeddings.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79ea1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0aca035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedded_query = hf_embeddings.embed_query(\"LangChain 에 대해서 알려주세요.\")\n",
    "embedded_documents = hf_embeddings.embed_documents(texts)\n",
    "\n",
    "# 질문(embedded_query): LangChain 에 대해서 알려주세요.\n",
    "np.array(embedded_query) @ np.array(embedded_documents).T\n",
    "\n",
    "sorted_idx = (np.array(embedded_query) @ np.array(embedded_documents).T).argsort()[::-1]\n",
    "\n",
    "print(\"[Query] LangChain 에 대해서 알려주세요.\\n====================================\")\n",
    "for i, idx in enumerate(sorted_idx):\n",
    "    print(f\"[{i}] {texts[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e5cc92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlagEmbedding 설치\n",
    "!pip install -qU FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91a5cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "bge_embeddings = BGEM3FlagModel(\n",
    "    model_name, use_fp16=True\n",
    ")  # use_fp16을 True로 설정하면 약간의 성능 저하와 함께 계산 속도가 빨라집니다.\n",
    "\n",
    "bge_embedded = bge_embeddings.encode(\n",
    "    texts,\n",
    "    batch_size=12,\n",
    "    max_length=8192,  # 이렇게 긴 길이가 필요하지 않은 경우 더 작은 값을 설정하여 인코딩 프로세스의 속도를 높일 수 있습니다.\n",
    ")[\"dense_vecs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36ccd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9210a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e457b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "bge_flagmodel = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\", use_fp16=True\n",
    ")  # use_fp16을 True로 설정하면 약간의 성능 저하와 함께 계산 속도가 빨라집니다.\n",
    "bge_encoded = bge_flagmodel.encode(texts, return_dense=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cc78c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 출력(행, 열)\n",
    "bge_encoded[\"dense_vecs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef3f8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_flagmodel = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\", use_fp16=True\n",
    ")  # use_fp16을 True로 설정하면 약간의 성능 저하와 함께 계산 속도가 빨라집니다.\n",
    "bge_encoded = bge_flagmodel.encode(texts, return_sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cae87c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_scores1 = bge_flagmodel.compute_lexical_matching_score(\n",
    "    bge_encoded[\"lexical_weights\"][0], bge_encoded[\"lexical_weights\"][0]\n",
    ")\n",
    "lexical_scores2 = bge_flagmodel.compute_lexical_matching_score(\n",
    "    bge_encoded[\"lexical_weights\"][0], bge_encoded[\"lexical_weights\"][1]\n",
    ")\n",
    "# 0 <-> 0\n",
    "print(lexical_scores1)\n",
    "# 0 <-> 1\n",
    "print(lexical_scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90a07f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_flagmodel = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\", use_fp16=True\n",
    ")  # use_fp16을 True로 설정하면 약간의 성능 저하와 함께 계산 속도가 빨라집니다.\n",
    "bge_encoded = bge_flagmodel.encode(texts, return_colbert_vecs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f6d9265",
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert_scores1 = bge_flagmodel.colbert_score(\n",
    "    bge_encoded[\"colbert_vecs\"][0], bge_encoded[\"colbert_vecs\"][0]\n",
    ")\n",
    "colbert_scores2 = bge_flagmodel.colbert_score(\n",
    "    bge_encoded[\"colbert_vecs\"][0], bge_encoded[\"colbert_vecs\"][1]\n",
    ")\n",
    "# 0 <-> 0\n",
    "print(colbert_scores1)\n",
    "# 0 <-> 1\n",
    "print(colbert_scores2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}