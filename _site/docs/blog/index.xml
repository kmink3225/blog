<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Kwangmin Kim</title>
<link>kk3225.netlify.app/docs/blog/</link>
<atom:link href="kk3225.netlify.app/docs/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>blog</description>
<generator>quarto-1.4.543</generator>
<lastBuildDate>Tue, 31 Dec 2999 15:00:00 GMT</lastBuildDate>
<item>
  <title>Blog Content List</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/content_list.html</link>
  <description><![CDATA[ 




<section id="contents" class="level1">

<ul>
<li><a href="../posts/Surveilance/guide_map/_index.qmd">Data Governance</a></li>
<li><a href="../posts/Engineering/guide_map/_index.qmd">Engineering</a></li>
<li><a href="../posts/Surveilance/guide_map/_index.qmd">Surveilance</a></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Scalars are denoted with a lower-case letter (ex a ) or a non-bolded lower-case Greek letter (ex <img src="https://latex.codecogs.com/png.latex?%5Calpha"> ).</li>
<li>Vectors are denoted using a bold-faced lower-case letter (ex <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20a">).</li>
<li>Matrices are denoted using a bold-faced upper-case letter (ex <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20%5Cphi">) or a bold-faced upper-case Greek letter (ex <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20%5CPhi">).</li>
<li>Tensors are denoted using a bold-faced upper-case letter with multiple subscripts or superscripts, indicating the number of indices and the dimensions of the tensor along each axis.
<ul>
<li>A second-order tensor (also known as a matrix) <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> with dimensions <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m"> can be represented as: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A_%7Bij%7D"> where <img src="https://latex.codecogs.com/png.latex?i%20=%201,%5Cdots,m"> and <img src="https://latex.codecogs.com/png.latex?j%20=%201,%5Cdots,n">, which are the indices that run over the rows and columns of the matrix, respectively.</li>
<li>A third-order tensor <img src="https://latex.codecogs.com/png.latex?T"> with dimensions <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m%20%5Ctimes%20p"> can be represented as: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A_%7Bijk%7D"> where <img src="https://latex.codecogs.com/png.latex?i%20=%201,%5Cdots,m">, <img src="https://latex.codecogs.com/png.latex?j%20=%201,%5Cdots,n">, which are <img src="https://latex.codecogs.com/png.latex?i">, and <img src="https://latex.codecogs.com/png.latex?k%20=%201,%5Cdots,p"> <img src="https://latex.codecogs.com/png.latex?j">, and <img src="https://latex.codecogs.com/png.latex?k">, which are the indices that run over the three dimensions of the tensor.</li>
</ul></li>
</ul>
</div>
</div>


</section>

 ]]></description>
  <guid>kk3225.netlify.app/docs/blog/posts/content_list.html</guid>
  <pubDate>Tue, 31 Dec 2999 15:00:00 GMT</pubDate>
</item>
<item>
  <title>RAG과 LangChain 소개</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Lang_Chain/1.intro_rag.html</link>
  <description><![CDATA[ 




<section id="lang-chain-소개" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Lang Chain 소개</h1>
<section id="langchain이란" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="langchain이란"><span class="header-section-number">1.1</span> LangChain이란?</h2>
<ul>
<li><strong>정의</strong>: LLM(Large Language Model) 기반 애플리케이션 개발을 위한 오픈소스 프레임워크</li>
<li><strong>주요 목적</strong>: 복잡한 LLM 애플리케이션을 쉽게 구축할 수 있도록 지원</li>
</ul>
</section>
<section id="langchain의-핵심-기능" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="langchain의-핵심-기능"><span class="header-section-number">1.2</span> LangChain의 핵심 기능</h2>
<ul>
<li><strong>체인(Chain)</strong>: 여러 컴포넌트를 연결하여 복잡한 워크플로우 구성</li>
<li><strong>프롬프트 템플릿</strong>: 동적으로 프롬프트 생성 및 관리</li>
<li><strong>메모리</strong>: 대화 히스토리 및 컨텍스트 유지</li>
<li><strong>에이전트</strong>: 도구 사용 및 의사결정 자동화</li>
<li><strong>문서 로더</strong>: 다양한 형식의 문서 처리</li>
<li><strong>벡터 스토어</strong>: 임베딩 기반 검색 시스템</li>
</ul>
</section>
<section id="langchain-사용-사례" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="langchain-사용-사례"><span class="header-section-number">1.3</span> LangChain 사용 사례</h2>
<ul>
<li><strong>챗봇</strong>: 문맥을 이해하는 대화형 AI 시스템</li>
<li><strong>문서 QA</strong>: 문서 기반 질의응답 시스템<br>
</li>
<li><strong>데이터 분석</strong>: 자연어로 데이터 분석 수행</li>
<li><strong>코드 생성</strong>: 자연어 명령으로 코드 자동 생성</li>
<li><strong>RAG 시스템</strong>: 검색 기반 답변 생성 시스템</li>
</ul>
</section>
</section>
<section id="rag-소개" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> RAG 소개</h1>
<section id="rag가-인기-있게-된-이유" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="rag가-인기-있게-된-이유"><span class="header-section-number">2.1</span> RAG가 인기 있게 된 이유</h2>
<ul>
<li><strong>LLM의 한계 극복</strong>: 학습 데이터 시점 이후 정보 부족 문제 해결</li>
<li><strong>실시간 정보 활용</strong>: 최신 정보를 동적으로 검색하여 활용</li>
<li><strong>도메인 특화</strong>: 특정 분야의 전문 지식 기반 답변 생성</li>
<li><strong>비용 효율성</strong>: 전체 모델 재학습 없이 지식 확장 가능</li>
<li><strong>신뢰성 향상</strong>: 검색된 문서 기반으로 답변의 근거 제공</li>
</ul>
</section>
<section id="chat-gpt-도래-및-성공" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="chat-gpt-도래-및-성공"><span class="header-section-number">2.2</span> Chat GPT 도래 및 성공</h2>
<ul>
<li><strong>2022년 11월 출시</strong>: OpenAI ChatGPT 공개 후 폭발적 인기</li>
<li><strong>사용자 급증</strong>: 출시 2개월 만에 1억 사용자 돌파</li>
<li><strong>산업 변화</strong>: AI 기반 대화형 인터페이스의 새로운 표준 제시</li>
<li><strong>기술 혁신</strong>: Transformer 기반 대화형 AI의 상용화 성공</li>
<li><strong>생성형 AI 붐</strong>: GPT 성공으로 생성형 AI 시장 급성장</li>
</ul>
</section>
<section id="chat-gpt의-문제점" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="chat-gpt의-문제점"><span class="header-section-number">2.3</span> Chat GPT의 문제점</h2>
<ul>
<li><strong>할루시네이션(Hallucination)</strong>: 그럴듯하지만 잘못된 정보 생성</li>
<li><strong>지식 컷오프</strong>: 학습 데이터 시점 이후 정보 부족</li>
<li><strong>일관성 부족</strong>: 같은 질문에 다른 답변 제공 가능</li>
<li><strong>도메인 특화 한계</strong>: 전문 분야 지식의 정확도 부족</li>
<li><strong>출처 불명</strong>: 답변 근거가 되는 정보 출처 제공 불가</li>
</ul>
</section>
<section id="rag-적용시-chat-gpt의-문제점-해결-방안들" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="rag-적용시-chat-gpt의-문제점-해결-방안들"><span class="header-section-number">2.4</span> RAG 적용시 Chat GPT의 문제점 해결 방안들</h2>
<ul>
<li><strong>실시간 정보 검색</strong>: 최신 문서에서 관련 정보 검색하여 제공</li>
<li><strong>근거 기반 답변</strong>: 검색된 문서를 바탕으로 답변 생성</li>
<li><strong>도메인 지식 확장</strong>: 특정 분야 문서 데이터베이스 구축</li>
<li><strong>일관성 개선</strong>: 동일한 문서 소스 기반으로 일관된 답변</li>
<li><strong>출처 추적</strong>: 답변에 사용된 문서 출처 명시</li>
</ul>
</section>
<section id="rag-기대-효과" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="rag-기대-효과"><span class="header-section-number">2.5</span> RAG 기대 효과</h2>
<ul>
<li><strong>정확도 향상</strong>: 검증된 문서 기반으로 답변 품질 개선</li>
<li><strong>신뢰성 확보</strong>: 출처가 명확한 근거 기반 답변 제공</li>
<li><strong>전문성 강화</strong>: 도메인 특화 지식 기반 전문 상담 가능</li>
<li><strong>비용 절감</strong>: 모델 재학습 없이 지식 업데이트</li>
<li><strong>확장성</strong>: 새로운 문서 추가만으로 지식 확장 가능</li>
</ul>
</section>
<section id="rag-적용-방법" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="rag-적용-방법"><span class="header-section-number">2.6</span> RAG 적용 방법</h2>
<p>Chat GPT의 할루시네이션을 줄이고 방대한 지식 기반으로 답변하는 도메인 특화 chatbot 구축 가능</p>
<p>즉, RAG란 chat gpt에게 잘 정제된 데이터를 제공하여 더 정확하고 신뢰할 수 있는 답변을 제공하는 방법이다.</p>
<p><strong>주요 과제들:</strong> - Chat GPT의 RAG 과정은 비공개되어 user가 통제할 수 없는 부분이기 때문에 사용자들은 문서를 chat gpt가 잘 검색할 수 있는 형태로 변경하는 것이 중요 - 어려운 점은 각 문서마다 파일 형식이 다르고 이를 gpt가 처리가능한 형태로 전처리하는 과정이 공수가 많이 들어감 - 고유의 RAG를 만들어줘야 함</p>
</section>
<section id="rag-process" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="rag-process"><span class="header-section-number">2.7</span> RAG Process</h2>
<p><strong>기본 워크플로우:</strong></p>
<pre><code>query -&gt; RAG(document -&gt; chunk -&gt; embedding -&gt; vector store (DB) -&gt; Retriever) -&gt; Prompt Engineering -&gt; LLM</code></pre>
<p><strong>세부 단계:</strong> 1. <strong>문서 수집</strong>: 도메인 특화 문서 데이터 수집 2. <strong>청킹(Chunking)</strong>: 문서를 검색 가능한 단위로 분할 3. <strong>임베딩</strong>: 텍스트를 벡터로 변환 4. <strong>벡터 스토어</strong>: 임베딩 벡터를 데이터베이스에 저장 5. <strong>검색(Retrieval)</strong>: 쿼리와 유사한 문서 청크 검색 6. <strong>프롬프트 엔지니어링</strong>: 검색 결과를 포함한 프롬프트 구성 7. <strong>답변 생성</strong>: LLM이 최종 답변 생성</p>
<p>이 고유의 RAG를 만들어주는 것이 중요하지만 매우 고되고 어려운 과정이다. 내가 구현하려고 하는 답변 기능이 안되는 이유는 정말 수백가지에 달하기 때문이다.</p>
<p>하지만 분명한건 RAG를 잘 적용하여 원하는 기능을 구현하는 사례들이 많이 나오고 있고 효과적인 방법론들이 존재한다:</p>
<p><strong>고급 RAG 기법들:</strong> - <strong>코사인 유사도 최적화</strong>: 벡터 검색 정확도 개선 - <strong>HyDE Retrieval</strong>: 가상 문서 생성을 통한 검색 성능 향상 - <strong>FT Embedding</strong>: 도메인 특화 임베딩 모델 파인튜닝 - <strong>Chunk Embedding 실험</strong>: 최적 청킹 전략 탐색 - <strong>Reranking</strong>: 검색 결과 재순위화 - <strong>Classification Step</strong>: 쿼리 유형 분류를 통한 검색 최적화 - <strong>Prompt Engineering</strong>: 효과적인 프롬프트 설계 - <strong>Tool Use</strong>: 외부 도구 활용 확장 - <strong>Query Expansion</strong>: 쿼리 확장 및 개선</p>
</section>
<section id="rag-구현-난이도" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="rag-구현-난이도"><span class="header-section-number">2.8</span> RAG 구현 난이도</h2>
<p>RAG 구현은 사실상 LLM을 Tuning하는 것과 같다.</p>
<p><strong>LLM Tuning 방법 난이도 비교:</strong> - <strong>Prompt Engineering</strong> (매우 쉬움): 프롬프트만 수정하여 성능 개선 - <strong>RAG</strong> (쉬움): 외부 지식 소스 연결하여 답변 품질 향상 - <strong>PEFT</strong> (어려움): Parameter-Efficient Fine-Tuning 적용 - <strong>Full Fine Tuning</strong> (매우 어려움): 전체 모델 파라미터 재학습</p>
<p><strong>RAG의 장점:</strong> - 상대적으로 구현 난이도가 낮음 - 기존 모델 파라미터 수정 불필요 - 지식 업데이트가 용이함 - 비용 효율적인 성능 개선 방법</p>
</section>
<section id="rag-구현-방법" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="rag-구현-방법"><span class="header-section-number">2.9</span> RAG 구현 방법</h2>
<section id="기본-rag-파이프라인-구축" class="level3" data-number="2.9.1">
<h3 data-number="2.9.1" class="anchored" data-anchor-id="기본-rag-파이프라인-구축"><span class="header-section-number">2.9.1</span> 기본 RAG 파이프라인 구축</h3>
<p><strong>문서 처리:</strong> - 다양한 형식(PDF, DOCX, TXT, HTML) 문서 로딩 - 텍스트 추출 및 전처리 - 의미 있는 단위로 청킹 분할</p>
<p><strong>벡터화 및 저장:</strong> - OpenAI Embeddings 또는 오픈소스 임베딩 모델 사용 - FAISS, Chroma, Pinecone 등 벡터 데이터베이스 구축 - 효율적인 유사도 검색 인덱스 생성</p>
</section>
<section id="검색-시스템-구현" class="level3" data-number="2.9.2">
<h3 data-number="2.9.2" class="anchored" data-anchor-id="검색-시스템-구현"><span class="header-section-number">2.9.2</span> 검색 시스템 구현</h3>
<p><strong>검색 전략:</strong> - 코사인 유사도 기반 벡터 검색 - 키워드 기반 하이브리드 검색 - 의미적 유사도와 키워드 매칭 결합</p>
<p><strong>검색 최적화:</strong> - Top-K 검색 결과 개수 조정 - 검색 임계값(threshold) 설정 - 문서 메타데이터 활용 필터링</p>
</section>
<section id="프롬프트-엔지니어링" class="level3" data-number="2.9.3">
<h3 data-number="2.9.3" class="anchored" data-anchor-id="프롬프트-엔지니어링"><span class="header-section-number">2.9.3</span> 프롬프트 엔지니어링</h3>
<p><strong>프롬프트 구조:</strong> - 시스템 메시지: 역할 및 지침 명시 - 컨텍스트: 검색된 문서 내용 포함 - 질문: 사용자 쿼리 - 답변 형식: 원하는 출력 형태 지정</p>
<p><strong>프롬프트 개선:</strong> - Few-shot 예시 추가 - 체인 오브 생각(Chain of Thought) 적용 - 답변 검증 및 출처 표기 요구</p>
</section>
<section id="평가-및-개선" class="level3" data-number="2.9.4">
<h3 data-number="2.9.4" class="anchored" data-anchor-id="평가-및-개선"><span class="header-section-number">2.9.4</span> 평가 및 개선</h3>
<p><strong>성능 평가:</strong> - 답변 정확도 측정 - 검색 정밀도(Precision) 및 재현율(Recall) - 사용자 만족도 조사</p>
<p><strong>지속적 개선:</strong> - A/B 테스트를 통한 파라미터 최적화 - 사용자 피드백 기반 모델 업데이트 - 새로운 문서 데이터 정기 추가</p>


</section>
</section>
</section>

 ]]></description>
  <category>RAG</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Lang_Chain/1.intro_rag.html</guid>
  <pubDate>Fri, 06 Jun 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>개발 환경의 숨은 암초, PATH 환경변수 오염</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Engineering/Conda/path_pollution.html</link>
  <description><![CDATA[ 




<section id="들어가며-개발자를-괴롭히는-path-문제" class="level2">
<h2 class="anchored" data-anchor-id="들어가며-개발자를-괴롭히는-path-문제">들어가며: 개발자를 괴롭히는 PATH 문제</h2>
<ul>
<li>나는 생화학, 수학, 통계 전공을 한터라 컴퓨터 공학을 전공하는 것이 아니었기 때문에 환경변수를 관리하지 않고 업무를 했고 그 결과 누적된 문제들이 얽혀 프로그램 오류를 찾아내는데 많은 시간을 소비하였다.</li>
<li>개발 환경을 설정하다 보면 예상치 못한 문제에 직면하게 된다. 특히 여러 도구와 언어를 함께 사용할 때, PATH 환경변수 오염은 마치 숨은 암초처럼 개발자를 괴롭힌다.</li>
<li>이 글에서는 복잡하게 꼬여버린 PATH 환경변수 문제의 진단부터 해결, 그리고 예방까지의 과정을 상세히 공유하고자 한다.</li>
<li><code>'conda.bat' is not recognized</code>, <code>reticulate 오류</code>, <code>Quarto 렌더링 실패</code> 등 겉보기엔 서로 다른 문제들이 사실은 하나의 거대한 PATH 오염에서 비롯된 것임을 밝혀내는 여정을 함께 따라가보자.</li>
</ul>
</section>
<section id="문제-상황-총체적-난국-오염된-path" class="level2">
<h2 class="anchored" data-anchor-id="문제-상황-총체적-난국-오염된-path">문제 상황: 총체적 난국, 오염된 PATH</h2>
<p>모든 문제의 시작은 처참하게 오염된 PATH 환경변수였다. 마치 뒤죽박죽 엉킨 실타래처럼, PATH는 다음과 같은 심각한 문제들을 안고 있었다.</p>
<section id="치명적인-경로-손상" class="level3">
<h3 class="anchored" data-anchor-id="치명적인-경로-손상">치명적인 경로 손상</h3>
<p>가장 심각한 문제는 Conda 환경 경로를 포함한 여러 경로가 완전히 깨져버린 것이다.</p>
<ul>
<li><code>miniconda3\\envs\\blog</code> 가 <code>miniconda3vlog</code> 로</li>
<li><code>mingw-w64\\bin</code> 가 <code>mingw-w6in</code> 로</li>
<li><code>usr\\bin</code> 가 <code>usin</code> 로</li>
<li><code>Library\\bin</code> 가 <code>Librarin</code> 로</li>
</ul>
<p>이러한 손상은 시스템이 정상적으로 실행 파일을 찾는 것을 불가능하게 만들었다.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예시: Git Bash에서 확인한 깨진 PATH의 일부</span></span>
<span id="cb1-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\\Users\\kmkim\\AppData\\Local\\miniconda3vlog</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">;</span>                           <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ❌ 깨진 경로 (envs\\blog → vlog)</span></span>
<span id="cb1-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\\Users\\kmkim\\AppData\\Local\\miniconda3vlog\\Library\\mingw-w6in</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">;</span>        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ❌ 심각하게 깨진 경로 (mingw-w64\\bin → mingw-w6in)</span></span>
<span id="cb1-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\\Users\\kmkim\\AppData\\Local\\miniconda3vlog\\Library\\usin</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">;</span>              <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ❌ 깨진 경로 (usr\\bin → usin)</span></span>
<span id="cb1-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\\Users\\kmkim\\AppData\\Local\\miniconda3vlog\\Librarin</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">;</span>                  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ❌ 깨진 경로 (Library\\bin → Librarin)</span></span>
<span id="cb1-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\\Users\\kmkim\\AppData\\Local\\miniconda3vlog\\Scripts</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">;</span>                   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ❌ 깨진 경로</span></span>
<span id="cb1-7"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\\Users\\kmkim\\AppData\\Local\\miniconda3vloin</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">;</span>                          <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ❌ 완전히 깨진 경로</span></span></code></pre></div>
</section>
<section id="끝없는-경로-중복" class="level3">
<h3 class="anchored" data-anchor-id="끝없는-경로-중복">끝없는 경로 중복</h3>
<p>마치 복사-붙여넣기를 반복한 듯, 동일한 경로가 5번에서 10번 이상 중복되어 PATH를 극도로 길고 비효율적으로 만들었다.</p>
<ul>
<li>PowerShell 관련 경로는 10번 이상 반복</li>
<li>ffmpeg, MySQL, R tools 등 거의 모든 경로가 중복</li>
</ul>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예시: 중복된 경로들</span></span>
<span id="cb2-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\\ffmpeg</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">;</span>                                                             <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ⚠️ 중복 (시스템/사용자 모두 존재)</span></span>
<span id="cb2-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\\ffmpeg\\</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">;</span>                                                            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ❌ 슬래시 중복</span></span>
<span id="cb2-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\\Program</span> Files<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\\</span>PowerShell<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\\</span>7<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">;</span>                                         <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ⚠️ 중복</span></span>
<span id="cb2-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\\Program</span> Files<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\\</span>PowerShell<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\\</span>7<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\\</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">;</span>                                        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ❌ 슬래시 중복</span></span>
<span id="cb2-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ... 기타 수많은 중복 경로 ...</span></span></code></pre></div>
</section>
<section id="일관성-없는-슬래시-사용" class="level3">
<h3 class="anchored" data-anchor-id="일관성-없는-슬래시-사용">일관성 없는 슬래시 사용</h3>
<p>경로 마지막에 슬래시(<code>/</code> 또는 <code>\\</code>)가 있거나 없는 경우가 혼재하여, 같은 경로가 다른 것으로 인식될 여지를 남겼다.</p>
<ul>
<li><code>C:\\path</code> 와 <code>C:\\path\\</code> 가 동시에 존재</li>
</ul>
</section>
<section id="사용자-path와-시스템-path의-혼란" class="level3">
<h3 class="anchored" data-anchor-id="사용자-path와-시스템-path의-혼란">사용자 PATH와 시스템 PATH의 혼란</h3>
<p>개인용 도구의 경로가 시스템 전체에 적용되는 시스템 PATH에 섞여 있었고, 그 반대의 경우도 존재하여 관리를 어렵게 만들었다.</p>
</section>
</section>
<section id="해결-과정-path-대청소" class="level2">
<h2 class="anchored" data-anchor-id="해결-과정-path-대청소">해결 과정: PATH 대청소</h2>
<p>엉망진창이 된 PATH를 정상으로 되돌리기 위한 대청소 작전은 다음과 같이 진행되었다.</p>
<section id="단계-path-현황-파악" class="level3">
<h3 class="anchored" data-anchor-id="단계-path-현황-파악">1단계: PATH 현황 파악</h3>
<p>가장 먼저 현재 PATH 상태를 정확히 파악하는 것이 중요했다.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">where</span> python  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Python 실행 파일 위치 확인</span></span>
<span id="cb3-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">where</span> conda   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Conda 실행 파일 위치 확인</span></span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">echo</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$PATH</span>    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (Git Bash 등에서) 전체 PATH 문자열 확인</span></span></code></pre></div>
<p>PowerShell에서는 다음과 같이 사용자 PATH와 시스템 PATH를 각각 확인할 수 있다.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode powershell code-with-copy"><code class="sourceCode powershell"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 사용자 PATH (개인 도구 및 설정)</span></span>
<span id="cb4-2"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>Environment<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]::</span>GetEnvironmentVariable<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Path"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"User"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span></span>
<span id="cb4-3"></span>
<span id="cb4-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 시스템 PATH (모든 사용자 및 시스템 전역 도구)</span></span>
<span id="cb4-5"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>Environment<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]::</span>GetEnvironmentVariable<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Path"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Machine"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span></span></code></pre></div>
</section>
<section id="단계-사용자-path와-시스템-path-분리-및-재정렬" class="level3">
<h3 class="anchored" data-anchor-id="단계-사용자-path와-시스템-path-분리-및-재정렬">2단계: 사용자 PATH와 시스템 PATH 분리 및 재정렬</h3>
<p>진단 결과를 바탕으로, 각 경로의 성격에 맞게 사용자 PATH와 시스템 PATH로 명확히 분리했다.</p>
<ul>
<li><strong>사용자 PATH</strong>: 개인적으로 설치한 프로그램(Conda, VS Code, Quarto 등) 및 사용자별 도구</li>
<li><strong>시스템 PATH</strong>: Windows 기본 구성 요소, 모든 사용자에게 필요한 프로그램(Git, PowerShell 등)</li>
</ul>
</section>
<section id="단계-오류-수정-및-중복-제거" class="level3">
<h3 class="anchored" data-anchor-id="단계-오류-수정-및-중복-제거">3단계: 오류 수정 및 중복 제거</h3>
<ul>
<li><strong>깨진 경로 복구</strong>: <code>miniconda3vlog</code> 와 같이 손상된 부분을 원래의 <code>miniconda3\\envs\\blog\\Scripts</code> 등으로 수정했다.</li>
<li><strong>중복 경로 통합</strong>: 반복되는 경로들을 하나만 남기고 모두 제거했다. 슬래시 유무로 인한 중복도 통일했다.</li>
<li><strong>불필요한 경로 삭제</strong>: 예를 들어 Java JDK의 <code>bin\\server</code> 같이 실제 실행에 필요 없는 하위 경로를 제거했다.</li>
</ul>
</section>
<section id="단계-conda-환경-우선순위-조정" class="level3">
<h3 class="anchored" data-anchor-id="단계-conda-환경-우선순위-조정">4단계: Conda 환경 우선순위 조정</h3>
<p>여러 Python/Conda 환경이 존재할 때, 특정 프로젝트(예: <code>blog</code>)의 환경이 기본 Conda 환경보다 우선적으로 인식되도록 PATH 순서를 조정했다.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode diff code-with-copy"><code class="sourceCode diff"><span id="cb5-1"># 수정 전 (일반 Conda가 우선될 수 있음)</span>
<span id="cb5-2">C:\Users\kmkim\AppData\Local\miniconda3\Scripts</span>
<span id="cb5-3">C:\Users\kmkim\AppData\Local\miniconda3\envs\blog\Scripts</span>
<span id="cb5-4"></span>
<span id="cb5-5"># 수정 후 (blog 환경 스크립트 최우선)</span>
<span id="cb5-6"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ C:\Users\kmkim\AppData\Local\miniconda3\envs\blog\Scripts</span></span>
<span id="cb5-7">  C:\Users\kmkim\AppData\Local\miniconda3\Scripts</span>
<span id="cb5-8">  C:\Users\kmkim\AppData\Local\miniconda3\condabin</span>
<span id="cb5-9">  C:\Users\kmkim\AppData\Local\miniconda3</span></code></pre></div>
</section>
</section>
<section id="개선-후-평화를-되찾은-path" class="level2">
<h2 class="anchored" data-anchor-id="개선-후-평화를-되찾은-path">개선 후: 평화를 되찾은 PATH</h2>
<p>대대적인 정리 작업 끝에, PATH는 다음과 같이 깔끔하게 정돈되었다.</p>
<section id="사용자-path-개인-도구" class="level3">
<h3 class="anchored" data-anchor-id="사용자-path-개인-도구">사용자 PATH (개인 도구)</h3>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Users\kmkim\AppData\Local\miniconda3\envs\blog\Scripts</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ 최우선 (특정 Conda 환경)</span></span>
<span id="cb6-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Users\kmkim\AppData\Local\miniconda3\Scripts</span>            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ 일반 Conda 스크립트</span></span>
<span id="cb6-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Users\kmkim\AppData\Local\miniconda3\condabin</span>          <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ Conda 실행 파일 경로</span></span>
<span id="cb6-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Users\kmkim\AppData\Local\miniconda3</span>                   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ Conda 기본 경로</span></span>
<span id="cb6-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Users\kmkim\AppData\Local\Programs\Quarto\bin</span>         <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ Quarto</span></span>
<span id="cb6-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Users\kmkim\AppData\Local\Programs\Microsoft</span> VS Code<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\b</span>in <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ VS Code</span></span>
<span id="cb6-7"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Users\kmkim\AppData\Local\Programs\cursor\resources\app\bin</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ Cursor</span></span>
<span id="cb6-8"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Users\kmkim\scoop\shims</span>                                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ Scoop 패키지</span></span>
<span id="cb6-9"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Users\kmkim\AppData\Local\Microsoft\WindowsApps</span>       <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ Windows Store 앱</span></span>
<span id="cb6-10"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\ffmpeg</span>                                                 <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ 사용자 ffmpeg</span></span>
<span id="cb6-11"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\R\rtools43\usr\bin</span>                                    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ R 도구</span></span></code></pre></div>
</section>
<section id="시스템-path-전역-도구" class="level3">
<h3 class="anchored" data-anchor-id="시스템-path-전역-도구">시스템 PATH (전역 도구)</h3>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Windows\system32</span>                                       <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ Windows 기본</span></span>
<span id="cb7-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Windows</span>                                               <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ Windows 기본</span></span>
<span id="cb7-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Windows\System32\Wbem</span>                                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ WMI</span></span>
<span id="cb7-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Windows\System32\WindowsPowerShell\v1.0</span>             <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ 구 PowerShell</span></span>
<span id="cb7-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Windows\System32\OpenSSH</span>                            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ SSH</span></span>
<span id="cb7-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Program</span> Files<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\P</span>owerShell<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\7</span>                          <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ 신 PowerShell</span></span>
<span id="cb7-7"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Program</span> Files<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\G</span>it<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\c</span>md                               <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ Git</span></span>
<span id="cb7-8"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\ProgramData\chocolatey\bin</span>                          <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ Chocolatey</span></span>
<span id="cb7-9"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Program</span> Files<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\J</span>ava<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\o</span>penjdk-23.0.1_windows-x64_bin<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\j</span>dk-23.0.1<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\b</span>in <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ Java</span></span>
<span id="cb7-10"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Program</span> Files<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\M</span>ATLAB<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\R</span>2022b<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\b</span>in                     <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ MATLAB</span></span>
<span id="cb7-11"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">C:\Program</span> Files <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">(</span><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">x86</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">)</span><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">\Microsoft</span> SDKs<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\A</span>zure<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\C</span>LI2<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\w</span>bin  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✅ Azure CLI</span></span></code></pre></div>
</section>
</section>
<section id="개선-효과-숫자로-보는-변화" class="level2">
<h2 class="anchored" data-anchor-id="개선-효과-숫자로-보는-변화">개선 효과: 숫자로 보는 변화</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>항목</th>
<th>개선 전</th>
<th>개선 후</th>
<th>개선율</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>총 PATH 항목 수</strong></td>
<td>~80개</td>
<td>~20개</td>
<td><strong>75% 감소</strong></td>
</tr>
<tr class="even">
<td><strong>중복 제거</strong></td>
<td>대량 중복</td>
<td>중복 없음</td>
<td><strong>100% 해결</strong></td>
</tr>
<tr class="odd">
<td><strong>깨진 경로</strong></td>
<td>5개 이상</td>
<td>0개</td>
<td><strong>100% 해결</strong></td>
</tr>
<tr class="even">
<td><strong>Python 인식</strong></td>
<td>실패</td>
<td>성공</td>
<td><strong>✅ 해결</strong></td>
</tr>
<tr class="odd">
<td><strong>Quarto 렌더링</strong></td>
<td>실패</td>
<td>성공</td>
<td><strong>✅ 해결</strong></td>
</tr>
</tbody>
</table>
</section>
<section id="최종-결과-평화로운-개발-환경" class="level2">
<h2 class="anchored" data-anchor-id="최종-결과-평화로운-개발-환경">최종 결과: 평화로운 개발 환경</h2>
<p>PATH 정리가 완료되자, 이전에 발생했던 모든 문제들이 거짓말처럼 사라졌다.</p>
<p><strong>이전</strong>: - <code>'conda.bat' is not recognized as an internal or external command...</code> - <code>Reticulate: Python N/A</code> - <code>Quarto failed to render (exit code: N)</code></p>
<p><strong>현재</strong>: - <code>conda activate blog</code> (정상 작동) - <code>python --version</code> (원하는 버전 출력) - <code>quarto preview</code> (성공적인 문서 미리보기)</p>
</section>
<section id="주요-오염-원인-분석" class="level2">
<h2 class="anchored" data-anchor-id="주요-오염-원인-분석">주요 오염 원인 분석</h2>
<p>이러한 대규모 PATH 오염은 왜 발생했을까? 몇 가지 가능한 원인들을 추정해볼 수 있다.</p>
<ol type="1">
<li><p><strong>설치 프로그램의 무분별한 PATH 수정</strong>: 일부 프로그램 설치 시, 기존 PATH를 정확히 파싱하지 못하거나, 잘못된 형식으로 경로를 추가/덮어쓰면서 문제가 누적될 수 있다. <code>bash     # 문제 패턴 예시     C:\Users\kmkim\AppData\Local\miniconda3vlog  # ← 'envs\blog'가 'vlog'로 깨짐     C:\R\rtools43\\usr\bin\                      # ← 백슬래시 중복</code></p></li>
<li><p><strong>빈번한 프로그램 설치/제거/업데이트</strong>: 다양한 버전의 Java, Python/Conda, PowerShell 등을 설치하고 제거하는 과정에서 PATH 항목이 정리되지 않고 계속 누적되어 중복과 꼬임을 유발한다.</p></li>
<li><p><strong>Conda 환경 관리 중 오류</strong>: Conda 환경을 생성하거나 삭제하는 도중 프로세스가 중단되거나, 특정 환경(예: 한글 경로)에서 인코딩 문제가 발생하면 PATH가 손상될 수 있다. <code>bash     # 정상적이라면: C:\...\miniconda3\envs\blog     # 실제 발생: C:\...\miniconda3vlog, C:\...\miniconda3\Library\usin 등</code></p></li>
<li><p><strong>Windows PATH 길이 제한 (구버전)</strong>: 과거 Windows 버전에서는 PATH 문자열의 최대 길이가 약 2048자로 제한되었다. 경로가 과도하게 길어지면 잘리면서 중간 부분이 손상될 수 있다.</p></li>
<li><p><strong>다양한 개발 도구의 독립적인 PATH 수정</strong>: MATLAB, R, Python, Java, Git, VS Code, Quarto 등 수많은 개발 도구들이 각자의 방식으로 PATH를 수정하면서 예기치 않은 충돌이나 꼬임이 발생할 수 있다.</p></li>
</ol>
</section>
<section id="예방-조치-건강한-path-유지를-위한-습관" class="level2">
<h2 class="anchored" data-anchor-id="예방-조치-건강한-path-유지를-위한-습관">예방 조치: 건강한 PATH 유지를 위한 습관</h2>
<p>향후 유사한 문제를 예방하기 위해 다음과 같은 습관을 들이는 것이 좋다.</p>
<ol type="1">
<li><p><strong>중요 작업 전 PATH 백업</strong>: 새로운 개발 도구를 설치하거나 환경 설정을 크게 변경하기 전에는 현재 PATH 상태를 백업한다. <code>powershell     $env:PATH | Out-File path_backup_YYYYMMDD.txt</code></p></li>
<li><p><strong>설치 프로그램 사용 시 신중함 유지</strong>:</p>
<ul>
<li>한 번에 하나의 프로그램만 설치하고, 설치 후 PATH 변경 사항 및 정상 작동 여부를 확인한다.</li>
<li>가능하다면 “Add to PATH” 옵션을 해제하고 수동으로 필요한 경로만 추가하는 것을 고려한다.</li>
<li>설치 후에는 시스템을 재부팅하여 변경사항이 완전히 적용되도록 한다.</li>
</ul></li>
<li><p><strong>정기적인 PATH 점검 및 정리</strong>: 주기적으로 PATH를 점검하여 불필요하거나 중복된 경로, 깨진 경로가 있는지 확인하고 정리한다. <code>powershell     # PowerShell에서 중복 없는 정렬된 PATH 목록 확인     $env:PATH -split ';' | Sort-Object | Get-Unique</code></p></li>
</ol>
</section>
<section id="교훈-모든-것은-path로-통한다" class="level2">
<h2 class="anchored" data-anchor-id="교훈-모든-것은-path로-통한다">교훈: 모든 것은 PATH로 통한다</h2>
<p>이번 PATH 오염 사태를 통해 얻은 교훈은 다음과 같다.</p>
<ul>
<li><strong>환경변수 오염은 소리 없는 암살자</strong>: 눈에 잘 보이지 않지만 시스템 전반의 안정성과 프로그램 작동에 치명적인 영향을 미칠 수 있다.</li>
<li><strong>PATH 순서의 중요성</strong>: 시스템은 PATH에 등록된 순서대로 실행 파일을 찾는다. 원하는 버전이나 환경의 도구가 먼저 실행되도록 순서를 올바르게 설정해야 한다.</li>
<li><strong>사용자 PATH와 시스템 PATH의 명확한 분리</strong>: 역할에 맞게 경로를 분리해야 충돌을 예방하고 관리가 용이해진다.</li>
<li><strong>복잡한 개발 스택일수록 PATH 관리는 필수</strong>: 특히 Quarto, R, Python처럼 여러 언어와 도구가 연동되는 환경에서는 깨끗하고 정확한 PATH 설정이 무엇보다 중요하다.</li>
</ul>
<p>결국, 겉으로 드러난 수많은 오류 메시지들은 “근본 원인”인 PATH 오염이 해결되자 모두 사라졌다. 복잡한 문제일수록 기본으로 돌아가 시스템의 가장 기초적인 설정부터 점검하는 자세가 필요함을 다시 한번 깨닫게 되었다.</p>


</section>

 ]]></description>
  <category>Engineering</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Engineering/Conda/path_pollution.html</guid>
  <pubDate>Wed, 30 Apr 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>데이터 플랫폼 리소스 계획 및 구현</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Engineering/data_engineering/2.IaaS.html</link>
  <description><![CDATA[ 




<section id="azure-sql을-사용하여-iaas-솔루션-배포" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="azure-sql을-사용하여-iaas-솔루션-배포"><span class="header-section-number">1</span> Azure SQL을 사용하여 IaaS 솔루션 배포</h2>
<section id="introduction" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1.1</span> <a href="https://learn.microsoft.com/en-us/training/modules/deploy-iaas-solutions-with-azure-sql/1-introduction">Introduction</a></h3>
<p>데이터베이스 시스템의 성능 최적화를 위한 주요 고려사항:</p>
<ul>
<li>데이터베이스 워크로드 성능 보장
<ul>
<li>가상 머신의 적절한 크기 선택이 필수적
<ul>
<li>CPU, 메모리, IOPS 등 리소스 요구사항 분석</li>
<li>워크로드 특성에 맞는 VM 시리즈 선택</li>
</ul></li>
<li>스토리지 구성의 최적화
<ul>
<li>Premium SSD, Ultra Disk 등 적절한 스토리지 타입 선택</li>
<li>데이터, 로그, 템프DB 파일의 효율적인 배치</li>
</ul></li>
<li>네트워크 설정 최적화
<ul>
<li>대역폭 요구사항에 맞는 네트워크 구성</li>
<li>보안 및 접근성 고려</li>
</ul></li>
</ul></li>
<li>고가용성 구성
<ul>
<li>비즈니스 요구사항에 맞는 가용성 수준 결정</li>
<li>Always On 가용성 그룹, 장애 조치 클러스터 등 적절한 솔루션 선택</li>
<li>백업 및 복구 전략 수립</li>
</ul></li>
<li>Azure VM에서의 SQL Server 배포 이점
<ul>
<li>기존 온프레미스 환경의 손쉬운 클라우드 마이그레이션</li>
<li>최소한의 아키텍처 변경으로 신속한 전환 가능</li>
<li>기존 라이선스 및 구성의 재사용 가능</li>
</ul></li>
<li>성공적인 마이그레이션을 위한 핵심 요소
<ul>
<li>상세한 마이그레이션 계획 수립</li>
<li>적절한 배포 옵션 선택</li>
<li>철저한 테스트 및 검증 절차</li>
</ul></li>
</ul>
<section id="자유도의-차이-iaas자유도가-높음-vs-paas자유도가-중간-vs-saas자유도가-낮음" class="level4">
<h4 class="anchored" data-anchor-id="자유도의-차이-iaas자유도가-높음-vs-paas자유도가-중간-vs-saas자유도가-낮음">자유도의 차이: IaaS(자유도가 높음) vs PaaS(자유도가 중간) vs SaaS(자유도가 낮음)</h4>
<ul>
<li>Software as a Service (SaaS)
<ul>
<li><strong>관리 수준</strong>: 완전 관리형으로 모든 것이 서비스 제공업체에 의해 관리됨</li>
<li><strong>유지보수</strong>: 서비스 제공업체가 모든 유지보수 담당</li>
<li><strong>확장성</strong>: 자동 확장, 사용자는 구독 수준만 선택</li>
<li><strong>비용</strong>: 구독 기반 과금, 사용량에 따른 요금제</li>
<li><strong>유연성</strong>: 매우 제한적, 제공되는 기능만 사용 가능</li>
<li><strong>적합 사례</strong>: 표준화된 소프트웨어 필요 시 (예: Office 365, Salesforce)</li>
</ul></li>
<li>Azure SQL Database (PaaS)
<ul>
<li><strong>관리 수준</strong>: 완전 관리형 서비스로 Microsoft가 OS, 하드웨어, 백업, 고가용성 등을 관리</li>
<li><strong>유지보수</strong>: 자동 패치 및 업그레이드</li>
<li><strong>확장성</strong>: 자동 확장 기능 지원</li>
<li><strong>비용</strong>: 사용한 리소스에 따라 비용 지불, 일반적으로 관리 오버헤드가 적음</li>
<li><strong>제한사항</strong>: 일부 고급 SQL Server 기능 사용 불가(CLR, SQL Agent 등)</li>
<li><strong>적합 사례</strong>: 새 애플리케이션 개발, 관리 오버헤드 최소화가 필요한 경우</li>
</ul></li>
<li>IaaS SQL Server (Azure VM)
<ul>
<li><strong>관리 수준</strong>: 셀프 관리형으로 사용자가 OS, 소프트웨어 업데이트, 백업 등 직접 관리</li>
<li><strong>유지보수</strong>: 수동 패치 및 업그레이드</li>
<li><strong>확장성</strong>: 수동 확장, VM 크기 변경 필요</li>
<li><strong>비용</strong>: VM 인프라에 대한 비용 지불, 관리 오버헤드가 더 많음</li>
<li><strong>유연성</strong>: 모든 SQL Server 기능 사용 가능(SSAS, SSIS, SSRS 등)</li>
<li><strong>적합 사례</strong>: 기존 온프레미스 SQL Server 마이그레이션, 특정 버전/기능 필요 시</li>
</ul></li>
<li>주요 차이점
<ul>
<li><strong>제어 수준</strong>: IaaS는 더 많은 제어를 제공하지만 더 많은 관리 책임이 따름</li>
<li><strong>호환성</strong>: IaaS는 온프레미스 SQL Server와 100% 호환, PaaS는 일부 제한</li>
<li><strong>운영 비용</strong>: PaaS는 일반적으로 운영 비용이 더 낮음</li>
</ul></li>
</ul>
</section>
</section>
<section id="azure에서-sql-server를-배포하기-위한-iaas-옵션-설명" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="azure에서-sql-server를-배포하기-위한-iaas-옵션-설명"><span class="header-section-number">1.2</span> <a href="https://learn.microsoft.com/en-us/training/modules/deploy-iaas-solutions-with-azure-sql/2-explain-iaas-options-deploy-azure">Azure에서 SQL Server를 배포하기 위한 IaaS 옵션 설명</a></h3>
<ul>
<li><p>Azure VM에서 SQL Server를 실행해야 하는 주요 시나리오들:</p>
<ol type="1">
<li>이전 버전의 SQL Server가 필요한 경우</li>
</ol>
<ul>
<li>일부 애플리케이션은 특정 이전 버전의 SQL Server에서만 작동</li>
<li>공급업체가 지원하는 특정 SQL Server 버전을 사용해야 하는 경우</li>
<li>이런 경우 VM에 원하는 버전을 직접 설치하여 실행하는 것이 가장 좋은 방법</li>
</ul>
<ol start="2" type="1">
<li>여러 SQL Server 서비스를 함께 사용해야 하는 경우</li>
</ol>
<ul>
<li>SQL Server 데이터베이스 엔진과 함께 Analysis Services(분석 서비스)나 Integration Services(통합 서비스), Reporting Services(보고 서비스)를 같이 사용해야 할 때</li>
<li>이들을 한 VM에서 함께 실행하면 라이선스 비용을 절약할 수 있음</li>
<li>Azure에서 일부 서비스는 PaaS로 제공되지만, 비용 효율성을 위해 VM에서 함께 실행하는 것이 유리할 수 있음</li>
</ul>
<ol start="3" type="1">
<li>애플리케이션 호환성 문제가 있는 경우</li>
</ol>
<ul>
<li>일부 애플리케이션은 여러 데이터베이스 간의 쿼리(교차 데이터베이스 쿼리)가 필요</li>
<li>Azure SQL Database는 이를 지원하지 않아 VM 사용이 필요</li>
<li>데이터베이스와 다른 서비스들이 특별한 방식으로 함께 동작해야 하는 경우</li>
</ul></li>
<li><p>IaaS(Infrastructure as a Service)의 장점과 특징:</p>
<ul>
<li>관리자의 세밀한 시스템 제어
<ul>
<li>Azure가 서버 하드웨어와 네트워크를 관리하지만, 관리자는 다음을 직접 제어 가능:
<ul>
<li>가상 스토리지 설정</li>
<li>가상 네트워크 구성</li>
<li>SQL Server 설치 및 설정</li>
<li>추가 소프트웨어 설치</li>
</ul></li>
</ul></li>
<li>인프라 구성의 높은 자유도
<ul>
<li>OS 레벨부터 완전한 제어 가능</li>
<li>커스텀 설정 및 튜닝의 자유로움</li>
</ul></li>
<li>세부적인 구성 계획의 중요성
<ul>
<li>성능 요구사항에 맞는 리소스 할당</li>
<li>확장성을 고려한 아키텍처 설계</li>
<li>비용 효율적인 리소스 사용 계획</li>
</ul></li>
</ul></li>
</ul>
<section id="azure-서비스-control-granularity제어-정밀도-비교" class="level4">
<h4 class="anchored" data-anchor-id="azure-서비스-control-granularity제어-정밀도-비교">Azure 서비스 Control Granularity(제어 정밀도) 비교</h4>
<ul>
<li>IaaS, PaaS, SaaS의 제어 수준 차이
<ul>
<li>IaaS: 가장 높은 제어 수준 제공</li>
<li>PaaS: 중간 수준의 제어 제공</li>
<li>SaaS: 가장 제한된 제어 수준</li>
</ul></li>
<li>각 서비스 별 관리 책임
<ul>
<li>SaaS: 사용자는 보안과 데이터 관리만 담당</li>
<li>PaaS: 클라우드 제공업체가 OS와 기본 소프트웨어 관리</li>
<li>IaaS: 사용자가 OS 패치, 네트워크, 스토리지 구성 등 대부분 관리</li>
</ul></li>
<li>Azure IaaS에서의 책임 분담
<ul>
<li>Microsoft 담당:
<ul>
<li>물리적 서버</li>
<li>스토리지</li>
<li>물리적 네트워킹</li>
</ul></li>
<li>사용자 담당:
<ul>
<li>OS 관리</li>
<li>SQL Server 인스턴스 구성</li>
<li>추가 소프트웨어 설치/관리</li>
</ul></li>
</ul></li>
<li>IaaS가 필요한 특수 상황
<ul>
<li>특정 SQL Server/Windows 버전 조합이 필요한 경우</li>
<li>SQL Server와 함께 추가 소프트웨어 설치가 필요한 경우</li>
<li>CLR, 복제 등 특수 기능 사용이 필요한 경우</li>
<li>기존 Active Directory 인증이 필요한 경우</li>
<li>OS 직접 접근이 필요한 애플리케이션 운영 시</li>
</ul></li>
<li>IaaS의 장점
<ul>
<li>높은 유연성과 제어 가능성</li>
<li>기존 온프레미스 환경과 유사한 구성 가능</li>
<li>클라우드의 이점과 기존 기능의 동시 활용</li>
<li>특수한 요구사항 수용 가능</li>
</ul></li>
</ul>
</section>
<section id="sql-server-iaas-agent-extensionsql-server-iaas-에이전트-확장" class="level4">
<h4 class="anchored" data-anchor-id="sql-server-iaas-agent-extensionsql-server-iaas-에이전트-확장">SQL Server IaaS Agent Extension(SQL Server IaaS 에이전트 확장)</h4>
<ul>
<li>Azure Marketplace에서 SQL Server VM을 배포할 때, 프로세스의 일부로 IaaS Agent Extension이 설치된다.</li>
<li>확장(Extension)은 VM 배포 후 실행되는 코드로, 일반적으로 배포 후 구성을 수행한다.</li>
<li>예를 들어 백신 소프트웨어 설치나 Windows 기능 활성화 등이 있다.</li>
<li>SQL Server IaaS Agent Extension은 관리 부담을 줄일 수 있는 다음과 같은 주요 기능을 제공한다:
<ul>
<li>자동 백업</li>
<li>자동 패치 적용</li>
<li>Azure Key Vault 통합</li>
<li>Microsoft Defender for Cloud 통합</li>
<li>포털에서 디스크 사용량 확인</li>
<li>유연한 라이선싱</li>
<li>유연한 버전/에디션 선택</li>
<li>SQL 모범 사례 평가</li>
</ul></li>
</ul>
<p>이러한 기능 외에도, 이 확장을 통해 SQL Server의 구성 및 스토리지 사용량 정보를 확인할 수 있다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="kk3225.netlify.app/images/azure/IaaS Agent Extension.PNG" class="img-fluid figure-img"></p>
<figcaption>IaaS Agent Extension</figcaption>
</figure>
</div>
</section>
<section id="sql-server-라이선싱-모델" class="level4">
<h4 class="anchored" data-anchor-id="sql-server-라이선싱-모델">SQL Server 라이선싱 모델</h4>
<ul>
<li>Azure VM에서 SQL Server 라이선스 사용 방법은 크게 두 가지</li>
</ul>
<ol type="1">
<li>종량제(Pay as you Go) 방식
<ul>
<li>Azure Marketplace에서 SQL Server가 설치된 VM 이미지 사용</li>
<li>사용한 시간만큼 VM 비용 + SQL Server 라이선스 비용 지불</li>
</ul></li>
<li>기존 라이선스 사용(BYOL) 방식
<ul>
<li>Software Assurance(SA) 프로그램 참여 고객만 가능</li>
<li>기존 보유한 SQL Server 라이선스를 Azure VM에 적용</li>
<li>VM 구현 후 10일 이내에 Microsoft에 라이선스 사용 보고 필요</li>
<li>SQL Server를 직접 설치하거나 커스텀 이미지 업로드 가능</li>
</ul></li>
</ol>
<p>비용 절감을 위한 추가 옵션: - Windows Server 라이선스도 기존 것 사용 가능 (Azure Hybrid Benefit) - VM을 1-3년 예약 구매 시 추가 할인 - 선불 결제 없이 월별 청구 가능 - 장기 사용 예정인 대형 VM에 특히 유리</p>
</section>
<section id="azure-vm-제품군-종류" class="level4">
<h4 class="anchored" data-anchor-id="azure-vm-제품군-종류">Azure VM 제품군 종류</h4>
<p>Azure VM은 다양한 제품군(시리즈)을 제공하며, 각각 특정 용도에 최적화되어 있다.</p>
<ol type="1">
<li>범용(General Purpose)
<ul>
<li>CPU와 메모리의 균형잡힌 구성</li>
<li>테스트/개발 환경, 소/중규모 DB 서버, 중소 트래픽 웹서버에 적합</li>
</ul></li>
<li>컴퓨팅 최적화(Compute Optimized)
<ul>
<li>높은 CPU 대 메모리 비율</li>
<li>중규모 웹서버, 네트워크 어플라이언스, 배치 처리에 적합</li>
<li>기본적인 머신러닝 워크로드 지원</li>
</ul></li>
<li>메모리 최적화(Memory Optimized)
<ul>
<li>높은 메모리 대 CPU 비율 (최대 4TB RAM)</li>
<li>대부분의 데이터베이스 워크로드에 적합</li>
</ul></li>
<li>스토리지 최적화(Storage Optimized)
<ul>
<li>고속 로컬 NVMe 임시 스토리지 제공</li>
<li>Cassandra 등 스케일아웃 데이터 워크로드에 적합</li>
<li>SQL Server 사용 시 Always On 가용성 그룹 등 데이터 보호 구성 필요</li>
</ul></li>
<li>GPU 최적화
<ul>
<li>비디오 렌더링/처리</li>
<li>GPU 기반 대규모 병렬 머신러닝 워크로드에 적합</li>
</ul></li>
<li>FPGA 가속
<ul>
<li>컴퓨팅 집약적 워크로드용</li>
<li>높은 스토리지 처리량과 네트워크 대역폭 제공</li>
</ul></li>
<li>고성능 컴퓨팅(HPC)
<ul>
<li>수천 개 CPU 코어로 수평 확장 가능</li>
<li>RDMA 네트워킹을 통한 낮은 지연시간 제공</li>
</ul></li>
</ol>
<ul>
<li>VM 크기 선택 방법
<ul>
<li>Azure 포털의 VM 생성 블레이드에서 ‘모든 크기 보기’ 선택</li>
<li>각 크기별 상세 정보 확인 가능:
<ul>
<li>vCPU 수</li>
<li>RAM 용량</li>
<li>데이터 디스크 수</li>
<li>최대 IOPS</li>
<li>임시 스토리지 용량</li>
<li>프리미엄 스토리지 지원 여부</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="azure-marketplace" class="level4">
<h4 class="anchored" data-anchor-id="azure-marketplace">Azure Marketplace</h4>
<p>Azure Marketplace는 Azure에서 제공하는 리소스 스토어로, 미리 만들어진 템플릿을 사용해 쉽게 리소스를 생성할 수 있다.</p>
<ul>
<li>주요 특징:
<ul>
<li>간단한 설정: 기본 정보만 입력하면 빠르게 리소스 생성 가능</li>
<li>빠른 구축: 몇 분 안에 리소스 사용 가능</li>
<li>다양한 옵션: SQL Server, Windows Server 등 다양한 제품 제공</li>
<li>사전 구성: OLTP, Data Warehouse 등 용도에 맞는 설정 제공</li>
</ul></li>
<li>장점
<ul>
<li>쉽고 빠른 시작 가능</li>
<li>직관적인 인터페이스</li>
<li>상세 설정 없이도 사용 가능</li>
</ul></li>
<li>단점
<ul>
<li>반복 작업이 어려움</li>
<li>자동화하기 불편함</li>
</ul></li>
</ul>
</section>
<section id="sql-server-configuration" class="level4">
<h4 class="anchored" data-anchor-id="sql-server-configuration">SQL Server configuration</h4>
<p>Azure VM에서 SQL Server를 설치할 때 다음과 같은 기본적인 설정들을 할 수 있다:</p>
<ul>
<li>보안 설정: 방화벽 규칙, 접근 권한 등을 설정</li>
<li>네트워크 설정: 가상 네트워크, 서브넷 등을 구성<br>
</li>
<li>SQL 인증: 사용자 계정과 비밀번호 설정</li>
<li>SQL 인스턴스: 데이터베이스 엔진 설정</li>
</ul>
</section>
</section>
<section id="understand-hybrid-scenarios" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="understand-hybrid-scenarios"><span class="header-section-number">1.3</span> <a href="https://learn.microsoft.com/en-us/training/modules/deploy-iaas-solutions-with-azure-sql/3-understand-hybrid-scenarios">Understand hybrid scenarios</a></h3>
<ul>
<li>하이브리드 인프라의 필요성
<ul>
<li>온프레미스/로컬 데이터센터의 SQL Server 인프라 투자 활용 가능</li>
<li>클라우드와 온프레미스의 장점을 모두 활용 가능</li>
<li>운영 복원력 향상과 비용 절감 효과</li>
</ul></li>
<li>하이브리드 인프라의 장점
<ul>
<li>클라우드 전환에 신중한 조직을 위한 좋은 시작점</li>
<li>물리적/가상화된 SQL Server 온프레미스 배포를 클라우드로 확장 가능</li>
<li>온프레미스와 클라우드 서비스의 상호 보완적 운영</li>
<li>IaaS 서비스(스토리지, SQL Server VM 등) 활용 가능</li>
</ul></li>
<li>하이브리드 구현 범위
<ul>
<li>온프레미스와 클라우드 간 하이브리드</li>
<li>여러 클라우드 서비스 간 하이브리드 구현 가능</li>
<li>다양한 SQL Server 하이브리드 시나리오 적용 가능</li>
</ul></li>
</ul>
<section id="sql-server를-위한-하이브리드-시나리오" class="level4">
<h4 class="anchored" data-anchor-id="sql-server를-위한-하이브리드-시나리오">SQL Server를 위한 하이브리드 시나리오</h4>
<p>SQL Server를 위한 하이브리드 솔루션을 배포할 때 고려할 수 있는 몇 가지 전략</p>
</section>
<section id="재해-복구" class="level4">
<h4 class="anchored" data-anchor-id="재해-복구">재해 복구</h4>
<p>재해 복구는 SQL Server를 하이브리드로 운영할 때 가장 많이 사용되는 방식</p>
<ul>
<li>주요 특징
<ul>
<li>재난 상황에서도 비즈니스 연속성 보장</li>
<li>여러 데이터 센터에 분산 배치 가능</li>
<li>Azure VM을 활용해 비용 효율적인 DR 구축</li>
</ul></li>
<li>운영 방식
<ul>
<li>일상적인 처리는 온프레미스 서버 사용</li>
<li>재해 발생 시 Azure로 전환(장애 조치)</li>
<li>여러 지역에 걸쳐 백업 가능</li>
</ul></li>
</ul>
</section>
<section id="sql-server-백업" class="level4">
<h4 class="anchored" data-anchor-id="sql-server-백업">SQL Server 백업</h4>
<p>SQL Server의 데이터를 안전하게 보관하기 위한 백업 전략</p>
<ul>
<li>백업 방식
<ul>
<li>Azure Storage에 직접 백업 (URL 사용)</li>
<li>Azure 파일 공유 활용 (SMB 프로토콜)</li>
</ul></li>
<li>주요 이점
<ul>
<li>현장 백업 실패 시에도 데이터 보호</li>
<li>Azure VM에서 백업 데이터 복원 테스트 가능</li>
<li>클라우드의 안정적인 스토리지 활용</li>
</ul></li>
</ul>
</section>
<section id="azure-arc-지원-sql-server" class="level4">
<h4 class="anchored" data-anchor-id="azure-arc-지원-sql-server">Azure Arc 지원 SQL Server</h4>
<p>여러 환경의 SQL Server를 Azure에서 통합 관리할 수 있게 해주는 서비스</p>
<ul>
<li>지원 환경
<ul>
<li>온프레미스</li>
<li>데이터 센터</li>
<li>엣지 환경</li>
<li>멀티클라우드</li>
</ul></li>
<li>주요 기능
<ul>
<li>모든 SQL Server 배포 현황 파악</li>
<li>구성/사용 패턴/보안 평가</li>
<li>실시간 보안 경고</li>
<li>취약점 보고</li>
</ul></li>
</ul>
</section>
<section id="보안-고려사항" class="level4">
<h4 class="anchored" data-anchor-id="보안-고려사항">보안 고려사항</h4>
<p>하이브리드 SQL 환경을 안전하게 운영하기 위한 보안 요소</p>
<ul>
<li>기본 요구사항
<ul>
<li>Active Directory와 DNS 설정 (온프레미스/Azure 모두)</li>
<li>안전한 양방향 통신 구축</li>
</ul></li>
<li>연결 방식
<ul>
<li>사이트 간(S2S) VPN</li>
<li>ExpressRoute</li>
</ul></li>
<li>ExpressRoute 특징
<ul>
<li>장점
<ul>
<li>최고 수준의 보안</li>
<li>최소 지연 시간</li>
<li>공용 인터넷과 분리된 전용 채널</li>
</ul></li>
<li>단점
<ul>
<li>높은 비용</li>
<li>멀티클라우드 환경에서 제한적 사용</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="explore-performance-and-security" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="explore-performance-and-security"><span class="header-section-number">1.4</span> <a href="https://learn.microsoft.com/en-us/training/modules/deploy-iaas-solutions-with-azure-sql/4-explore-performance-and-security">Explore performance and security</a></h3>
<ul>
<li>Azure 에코시스템은 Azure 가상 머신의 SQL Server 인스턴스에 대한 다양한 성능 및 보안 옵션을 제공한다</li>
<li>각 옵션은 워크로드의 용량 및 성능 요구사항을 충족하는 다양한 디스크 유형과 같은 기능을 제공한다.</li>
</ul>
<section id="스토리지-고려사항" class="level4">
<h4 class="anchored" data-anchor-id="스토리지-고려사항">스토리지 고려사항</h4>
<ul>
<li>SQL Server는 Azure VM이나 온프레미스 환경에서 모두 고성능을 위해 우수한 스토리지 성능이 필수적</li>
<li>Azure는 다양한 스토리지 솔루션을 제공하는데, SQL Server 워크로드는 주로 Azure 관리 디스크를 사용</li>
<li>Azure 관리 디스크는 VM에 제공되는 블록 수준 스토리지로, 높은 가용성과 확장성을 제공</li>
</ul>
<p>Azure 관리 디스크의 주요 특징:</p>
<ul>
<li>스토리지 유형
<ul>
<li>Blob, 파일, 큐, 테이블 등 다양한 유형 제공</li>
<li>SQL Server는 주로 관리 디스크 사용</li>
<li>장애 조치 클러스터는 파일 스토리지 사용 가능</li>
<li>백업은 blob 스토리지 활용</li>
</ul></li>
<li>관리 디스크의 장점
<ul>
<li>99.999%의 높은 가용성</li>
<li>지역당 구독당 최대 50,000개 VM 디스크 지원</li>
<li>가용성 집합/영역과의 통합으로 높은 복원력</li>
</ul></li>
<li>암호화 옵션
<ul>
<li>Azure 서버 측 암호화: 스토리지 서비스 레벨의 암호화</li>
<li>Azure 디스크 암호화: VM 내부의 OS/데이터 디스크 암호화</li>
<li>두 옵션 모두 Azure Key Vault 통합 지원</li>
</ul></li>
</ul>
<p>VM에 연결되는 디스크 유형:</p>
<ol type="1">
<li>운영 체제 디스크
<ul>
<li>부팅 볼륨 포함</li>
<li>Windows: C: 드라이브</li>
<li>Linux: /dev/sda1</li>
</ul></li>
<li>임시 디스크
<ul>
<li>임시 스토리지용</li>
<li>페이지/스왑 파일 등 비영구 데이터 저장</li>
<li>Windows: D:&nbsp;드라이브</li>
<li>Linux: /dev/sdb1</li>
<li>중요 데이터 저장 금지</li>
</ul></li>
<li>데이터 디스크
<ul>
<li>VM에 추가되는 관리 디스크</li>
<li>Windows: Storage Spaces로 풀링 가능</li>
<li>Linux: 논리 볼륨 관리로 풀링 가능</li>
<li>IOPS와 스토리지 용량 확장 가능</li>
</ul></li>
</ol>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 15%">
<col style="width: 20%">
<col style="width: 16%">
<col style="width: 17%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Ultra Disk</th>
<th>Premium SSD v2</th>
<th>Premium SSD</th>
<th>Standard SSD</th>
<th>Standard HDD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Disk type</td>
<td>SSD</td>
<td>SSD</td>
<td>SSD</td>
<td>SSD</td>
<td>HDD</td>
</tr>
<tr class="even">
<td>Best for</td>
<td>IO-intensive workloads</td>
<td>Performance-sensitive workloads</td>
<td>Performance-sensitive workloads</td>
<td>Lightweight workloads</td>
<td>Backups, non-critical workloads</td>
</tr>
<tr class="odd">
<td>Max disk size</td>
<td>65,536 GiB</td>
<td>64,000 GiB</td>
<td>32,767 GiB</td>
<td>32,767 GiB</td>
<td>32,767 GiB</td>
</tr>
<tr class="even">
<td>Max throughput</td>
<td>10,000 MB/s</td>
<td>1,200 MB/s</td>
<td>900 MB/s</td>
<td>750 MB/s</td>
<td>500 MB/s</td>
</tr>
<tr class="odd">
<td>Max IOPS</td>
<td>160,000</td>
<td>80,000</td>
<td>20,000</td>
<td>6,000</td>
<td>2,000</td>
</tr>
</tbody>
</table>
<p>Azure SQL Server의 스토리지 구성에 대한 모범 사례를 살펴보면, 성능 최적화를 위해 여러 디스크를 적절히 구성하는 것이 중요. 특히 IOPS와 스토리지 용량을 효과적으로 관리하기 위해서는 프리미엄 디스크를 풀링하여 사용하는 것이 권장됨</p>
<p>각 데이터 유형별 스토리지 구성 권장사항:</p>
<ul>
<li>데이터 파일
<ul>
<li>프리미엄 디스크의 자체 풀에 저장</li>
<li>읽기 캐싱 기능 활성화</li>
<li>높은 IOPS와 처리량 확보</li>
</ul></li>
<li>트랜잭션 로그 파일
<ul>
<li>별도의 디스크 풀에 저장</li>
<li>캐싱 기능 비활성화 (캐싱의 이점 없음)</li>
<li>안정적인 쓰기 성능 보장</li>
</ul></li>
<li>TempDB
<ul>
<li>두 가지 옵션 중 선택:
<ol type="1">
<li>자체 디스크 풀에 저장</li>
<li>VM의 임시 디스크 활용 (물리 서버와 직접 연결되어 낮은 지연시간 제공)</li>
</ol></li>
</ul></li>
</ul>
<p>성능 요구사항에 따른 디스크 선택: - 일반적인 워크로드: 프리미엄 SSD (밀리초 단위의 지연시간) - 미션 크리티컬 워크로드: Ultra SSD (더 낮은 지연시간 필요시)</p>
</section>
<section id="보안-고려사항-1" class="level4">
<h4 class="anchored" data-anchor-id="보안-고려사항-1">보안 고려사항</h4>
<p>Azure는 가상 머신에서 실행되는 SQL Server의 규정 준수 솔루션을 위해 다양한 보안 도구와 기능을 제공합니다.</p>
<p>주요 보안 도구:</p>
<ol type="1">
<li>SQL용 Microsoft Defender
<ul>
<li>취약성 평가 및 보안 경고 기능 제공</li>
<li>SQL Server 인스턴스와 데이터베이스의 잠재적 취약점 식별</li>
<li>보안 위험 감지 및 해결 방안 제시</li>
<li>보안 상태 모니터링 및 개선을 위한 실행 가능한 단계 제공</li>
</ul></li>
<li>Azure Security Center
<ul>
<li>통합 보안 관리 시스템</li>
<li>하이브리드 클라우드 워크로드 전반의 보안 상태 모니터링</li>
<li>공격 노출 감소 및 위협 대응 기능</li>
<li>보안 개선 기회 식별 및 제안</li>
</ul></li>
</ol>
</section>
<section id="성능-최적화-방안" class="level4">
<h4 class="anchored" data-anchor-id="성능-최적화-방안">성능 최적화 방안</h4>
<p>Azure VM의 SQL Server는 온프레미스 환경과 유사한 성능 최적화 기능을 제공합니다.</p>
<p>주요 성능 최적화 기능:</p>
<ol type="1">
<li>테이블 파티셔닝
<ul>
<li>대규모 테이블의 효율적 관리</li>
<li>쿼리 성능 향상</li>
<li>유지보수 작업 효율화</li>
</ul>
구현 단계:
<ul>
<li>파일 그룹 생성</li>
<li>파티션 함수 정의</li>
<li>파티션 스키마 생성</li>
<li>테이블 파티션 설정</li>
</ul></li>
<li>데이터 압축 압축 유형:
<ul>
<li>행 압축
<ul>
<li>기본적인 압축 방식</li>
<li>최소 저장 공간 사용</li>
<li>낮은 시스템 부하</li>
</ul></li>
<li>페이지 압축
<ul>
<li>행 압축 포함</li>
<li>접두사 압축과 사전 압축 기술 적용</li>
<li>높은 압축률 제공</li>
</ul></li>
<li>컬럼스토어 아카이브 압축
<ul>
<li>XPRESS 압축 알고리즘 사용</li>
<li>자주 접근하지 않는 보관 데이터에 적합</li>
<li>높은 CPU 사용률</li>
</ul></li>
</ul></li>
<li>추가 최적화 옵션
<ul>
<li>백업 압축 기능 활성화</li>
<li>즉각적인 파일 초기화 설정</li>
<li>데이터베이스 자동 증가 제한 설정</li>
<li>자동 축소/자동 닫기 기능 비활성화</li>
<li>시스템 데이터베이스 데이터 디스크 이전</li>
<li>로그 및 추적 파일 데이터 디스크 이전</li>
</ul></li>
</ol>
<pre><code>-- Partition function
CREATE PARTITION FUNCTION PartitionByMonth (datetime2)
    AS RANGE RIGHT
    -- The boundary values defined is the first day of each month, where the table will be partitioned into 13 partitions
    FOR VALUES ('20210101', '20210201', '20210301',
      '20210401', '20210501', '20210601', '20210701',
      '20210801', '20210901', '20211001', '20211101', 
      '20211201');

-- The partition scheme below will use the partition function created above, and assign each partition to a specific filegroup.
CREATE PARTITION SCHEME PartitionByMonthSch
    AS PARTITION PartitionByMonth
    TO (FILEGROUP1, FILEGROUP2, FILEGROUP3, FILEGROUP4,
        FILEGROUP5, FILEGROUP6, FILEGROUP7, FILEGROUP8,
        FILEGROUP9, FILEGROUP10, FILEGROUP11, FILEGROUP12);

-- Creates a partitioned table called Order that applies PartitionByMonthSch partition scheme to partition the OrderDate column  
CREATE TABLE Order ([Id] int PRIMARY KEY, OrderDate datetime2)  
    ON PartitionByMonthSch (OrderDate) ;  
GO
</code></pre>
</section>
<section id="데이터-압축" class="level4">
<h4 class="anchored" data-anchor-id="데이터-압축">데이터 압축</h4>
<p>SQL Server의 데이터 압축은 데이터베이스의 성능과 저장 공간을 최적화하는 중요한 기능입니다.</p>
<p>데이터 압축의 기본 구조: - SQL Server는 8KB 크기의 페이지 단위로 데이터를 저장 - 압축을 통해 한 페이지에 더 많은 데이터를 저장 가능</p>
<p>데이터 압축의 주요 효과: 1. 물리적 IO 감소 - 쿼리 실행 시 읽어야 할 페이지 수가 줄어듦 - 디스크 읽기/쓰기 작업이 감소</p>
<ol start="2" type="1">
<li>메모리 사용 효율성 향상
<ul>
<li>버퍼 풀의 메모리를 더 효율적으로 사용</li>
<li>동일한 메모리로 더 많은 데이터 처리 가능</li>
</ul></li>
</ol>
<p>데이터 압축의 주요 특징과 장단점은 다음과 같습니다:</p>
<ol type="1">
<li><p>주요 이점</p>
<ul>
<li>물리적 IO 감소</li>
<li>버퍼 풀의 효율적인 메모리 사용</li>
<li>저장 공간 절약</li>
<li>대부분의 경우 전반적인 성능 향상</li>
</ul></li>
<li><p>잠재적 단점</p>
<ul>
<li>CPU 사용량 증가</li>
<li>압축/해제 과정에서의 추가 처리 시간</li>
</ul></li>
<li><p>구현 특성</p>
<ul>
<li>개체 수준에서 구현 가능</li>
<li>개별 인덱스나 테이블 단위로 압축 가능</li>
<li>파티션 단위로도 압축 설정 가능</li>
<li>sp_estimate_data_compression_savings 프로시저로 압축 효과 예측 가능</li>
</ul></li>
<li><p>압축 유형별 특징</p>
<p>행 압축:</p>
<ul>
<li>기본적인 압축 방식으로 최소한의 시스템 부하</li>
<li>각 열의 값을 최소 필요 공간으로 저장</li>
<li>숫자 데이터는 가변 길이로 저장</li>
<li>고정 길이 문자열을 가변 길이로 변환</li>
</ul>
<p>페이지 압축:</p>
<ul>
<li>행 압축을 포함한 고급 압축 방식</li>
<li>접두사 압축으로 중복 데이터 제거</li>
<li>사전 압축으로 반복 값을 포인터로 대체</li>
<li>데이터 중복성이 높을수록 압축률 증가</li>
</ul>
<p>컬럼스토어 아카이브 압축:</p>
<ul>
<li>XPRESS 압축 알고리즘 사용</li>
<li>자주 접근하지 않는 보관용 데이터에 적합</li>
<li>높은 압축률 제공</li>
<li>CPU 사용량이 상대적으로 높음</li>
</ul></li>
</ol>
</section>
<section id="추가-옵션" class="level4">
<h4 class="anchored" data-anchor-id="추가-옵션">추가 옵션</h4>
<p>다음은 프로덕션 워크로드에 대해 고려해야 할 추가 SQL Server 기능 및 작업 목록:</p>
<ul>
<li>백업 압축 활성화</li>
<li>데이터 파일에 대한 즉각적인 파일 초기화 활성화</li>
<li>데이터베이스 자동 증가 제한</li>
<li>데이터베이스에 대한 자동 축소/자동 닫기 비활성화</li>
<li>시스템 데이터베이스를 포함한 모든 데이터베이스를 데이터 디스크로 이동</li>
<li>SQL Server 오류 로그 및 추적 파일 디렉터리를 데이터 디스크로 이동</li>
<li>최대 SQL Server 메모리 제한 설정</li>
<li>메모리의 페이지 잠금 활성화</li>
<li>OLTP 중심 환경을 위한 임시 워크로드 최적화 활성화</li>
<li>쿼리 스토어 활성화</li>
<li>DBCC CHECKDB, 인덱스 재구성, 인덱스 재구축 및 통계 업데이트 작업을 실행하도록 SQL Server Agent 작업 예약</li>
<li>트랜잭션 로그 파일의 상태 및 크기 모니터링 및 관리</li>
</ul>
</section>
</section>
</section>
<section id="explain-high-availability-and-disaster-recovery-options" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="explain-high-availability-and-disaster-recovery-options"><span class="header-section-number">2</span> <a href="https://learn.microsoft.com/en-us/training/modules/deploy-iaas-solutions-with-azure-sql/5-explain-high-availability-and-disaster-recovery-options">Explain high availability and disaster recovery options</a></h2>
<p>Azure 플랫폼의 고가용성 옵션:</p>
<ol type="1">
<li>기본 제공 고가용성
<ul>
<li>VM과 PaaS 워크로드에 기본적으로 제공</li>
</ul></li>
<li>추가 고가용성 옵션
<ul>
<li>가용성 영역(Availability Zones)</li>
<li>가용성 집합(Availability Sets)</li>
</ul></li>
<li>주요 보호 기능
<ul>
<li>계획된 유지보수 활동으로부터 보호</li>
<li>잠재적인 하드웨어 장애로부터 보호</li>
<li>더 높은 수준의 가용성 제공</li>
</ul></li>
</ol>
<section id="high-availability-options" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="high-availability-options"><span class="header-section-number">2.1</span> High availability options</h3>
<p>SQL Server 고가용성 솔루션의 Azure VM 지원:</p>
<ul>
<li>Azure 전용 솔루션
<ul>
<li>전체 HADR 시스템이 Azure에서 실행</li>
<li>완전한 클라우드 기반 운영 가능</li>
</ul></li>
<li>하이브리드 구성
<ul>
<li>일부는 Azure에서 실행</li>
<li>일부는 온프레미스에서 실행</li>
<li>유연한 구성 가능</li>
</ul></li>
<li>Azure 환경의 장점
<ul>
<li>예산에 맞춘 단계적 마이그레이션 가능</li>
<li>HADR 요구사항에 따른 유연한 구성</li>
<li>부분 또는 완전 마이그레이션 선택 가능</li>
</ul></li>
</ul>
<section id="availability-zones" class="level4">
<h4 class="anchored" data-anchor-id="availability-zones">Availability Zones</h4>
<section id="가용성-영역-개요" class="level5">
<h5 class="anchored" data-anchor-id="가용성-영역-개요">가용성 영역 개요</h5>
<ul>
<li>한 지역 내의 고유한 물리적 위치</li>
<li>각 영역은 독립적인 전원, 냉각, 네트워킹을 갖춘 데이터센터로 구성</li>
<li>지원되는 Azure 지역마다 3개의 가용성 영역 제공</li>
</ul>
</section>
<section id="가용성-영역-특징" class="level5">
<h5 class="anchored" data-anchor-id="가용성-영역-특징">가용성 영역 특징</h5>
<ul>
<li>VM 생성 시 배치할 영역 지정 가능</li>
<li>여러 VM을 다른 영역에 분산 배포하여 데이터센터 장애 대비</li>
<li>Microsoft는 한 번에 하나의 영역만 업데이트 (업데이트 도메인 사용)</li>
<li>VM 에코시스템을 3개 영역에 분산 가능</li>
</ul>
</section>
<section id="가용성-영역-장점" class="level5">
<h5 class="anchored" data-anchor-id="가용성-영역-장점">가용성 영역 장점</h5>
<ul>
<li>가동 시간 99.99% 보장 (연간 최대 52.60분 다운타임)</li>
<li>애플리케이션에 가장 높은 수준의 가용성 제공</li>
<li>docs.microsoft.com에서 지원 지역 확인 가능</li>
</ul>
</section>
<section id="가용성-영역-배포" class="level5">
<h5 class="anchored" data-anchor-id="가용성-영역-배포">가용성 영역 배포</h5>
<ul>
<li>Zone 1, 2, 3 중 선택하여 배포 가능</li>
<li>물리적 데이터센터의 논리적 표현</li>
<li>구독별로 Zone 번호가 다른 데이터센터를 나타낼 수 있음</li>
</ul>
</section>
<section id="사용-조건" class="level5">
<h5 class="anchored" data-anchor-id="사용-조건">사용 조건</h5>
<ul>
<li>해당 지역에서 가용성 영역 지원 필요</li>
<li>애플리케이션이 최소한의 영역 간 지연 시간 지원 필요</li>
</ul>
</section>
</section>
<section id="availability-sets" class="level4">
<h4 class="anchored" data-anchor-id="availability-sets">Availability Sets</h4>
<section id="가용성-집합-개요" class="level5">
<h5 class="anchored" data-anchor-id="가용성-집합-개요">가용성 집합 개요</h5>
<ul>
<li>가용성 영역과의 차이점
<ul>
<li>가용성 영역: 지역의 데이터 센터에 워크로드 분산</li>
<li>가용성 집합: 데이터 센터 내의 서버와 랙에 워크로드 분산</li>
</ul></li>
</ul>
</section>
<section id="주요-특징" class="level5">
<h5 class="anchored" data-anchor-id="주요-특징">주요 특징</h5>
<ul>
<li>VM 분산 배치 보장
<ul>
<li>Always On 가용성 그룹 멤버 VM들이 서로 다른 물리적 호스트에서 실행되도록 보장</li>
<li>Azure의 대부분 워크로드가 가상화되어 있어 효과적</li>
</ul></li>
</ul>
</section>
<section id="성능-및-사용-조건" class="level5">
<h5 class="anchored" data-anchor-id="성능-및-사용-조건">성능 및 사용 조건</h5>
<ul>
<li>가용성 보장
<ul>
<li>최대 99.95%의 가용성 제공</li>
</ul></li>
<li>사용 시나리오
<ul>
<li>가용성 영역을 사용할 수 없는 지역</li>
<li>애플리케이션이 영역 내 지연 시간을 허용할 수 없는 경우</li>
</ul></li>
</ul>
</section>
</section>
<section id="always-on-availability-groups-ag" class="level4">
<h4 class="anchored" data-anchor-id="always-on-availability-groups-ag">Always On availability groups (AG)</h4>
<ul>
<li>구현 범위
<ul>
<li>Azure 가상 머신에서 실행되는 두 개 이상(최대 9개)의 SQL Server 인스턴스 간</li>
<li>온프레미스 데이터 센터와 Azure 간 구현 가능</li>
</ul></li>
<li>작동 방식
<ul>
<li>데이터베이스 트랜잭션이 기본 복제본에 커밋</li>
<li>동기식 또는 비동기식으로 모든 보조 복제본으로 전송</li>
</ul></li>
<li>가용성 모드 선택 기준
<ul>
<li>서버 간 물리적 거리에 따라 결정</li>
<li>비동기식 가용성 모드 권장 상황:
<ul>
<li>워크로드가 낮은 지연 시간 요구</li>
<li>보조 복제본이 지리적으로 분산된 경우</li>
</ul></li>
<li>동기식 커밋 모드 권장 상황:
<ul>
<li>복제본이 동일한 Azure 지역 내 위치</li>
<li>애플리케이션이 일정 수준의 지연 시간 허용 가능</li>
</ul></li>
</ul></li>
<li>동기식 모드 특징
<ul>
<li>각 트랜잭션이 하나 이상의 보조 복제본에 커밋된 후 애플리케이션 진행</li>
</ul></li>
<li>장점
<ul>
<li>단일 가용성 그룹이 동기식과 비동기식 가용성 모드 모두 지원</li>
<li>고가용성과 재해 복구 동시 제공</li>
<li>장애 조치 단위는 데이터베이스 그룹(전체 인스턴스가 아님)</li>
</ul></li>
<li>재해 복구 기능
<ul>
<li>Azure 지역 전체에 걸쳐 최대 9개의 데이터베이스 복제본 구현 가능</li>
<li>분산 가용성 그룹을 통한 아키텍처 확장 가능</li>
<li>기본 지역 외 다른 위치에 데이터베이스 실행 가능한 복사본 보장</li>
<li>자연 재해와 인위적 재해로부터 데이터 생태계 보호</li>
</ul></li>
<li>구성 예시
<ul>
<li>Windows Server 장애 조치 클러스터에서 실행</li>
<li>하나의 기본 복제본과 4개의 보조 복제본 구성 가능</li>
<li>모든 복제본이 동기식이거나, 동기식과 비동기식 복제본 조합 가능</li>
<li>장애 조치 단위는 데이터베이스 그룹(인스턴스가 아님)</li>
<li>장애 조치 클러스터 인스턴스는 인스턴스 수준 HA 제공(재해 복구는 제공하지 않음)</li>
</ul></li>
</ul>
</section>
<section id="sql-server-failover-cluster-instances" class="level4">
<h4 class="anchored" data-anchor-id="sql-server-failover-cluster-instances">SQL Server Failover Cluster instances</h4>
<ul>
<li>SQL Server 장애 조치 클러스터 인스턴스(FCI) 개요
<ul>
<li>전체 인스턴스 보호를 위한 솔루션</li>
<li>단일 지역에서 전체 인스턴스에 대한 고가용성 제공</li>
<li>단독으로는 재해 복구 기능 제공하지 않음
<ul>
<li>가용성 그룹이나 로그 전달과 같은 기능과 결합 필요</li>
</ul></li>
<li>공유 스토리지 요구사항 존재
<ul>
<li>Azure에서 공유 파일 스토리지 사용 가능</li>
<li>Windows Server의 Storage Spaces Direct 활용 가능</li>
</ul></li>
</ul></li>
<li>Azure 환경에서의 FCI 고려사항
<ul>
<li>새로운 배포에서는 가용성 그룹이 선호됨
<ul>
<li>FCI의 공유 스토리지 요구사항이 배포 복잡성 증가</li>
</ul></li>
<li>온프레미스 솔루션 마이그레이션 시 FCI 필요 가능성
<ul>
<li>기존 애플리케이션 지원을 위한 요구사항 고려</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="disaster-recovery-options" class="level4">
<h4 class="anchored" data-anchor-id="disaster-recovery-options">Disaster Recovery options</h4>
<ul>
<li>Azure 플랫폼 기본 특성
<ul>
<li>기본적으로 99.9%의 가동 시간 제공</li>
<li>재해 발생 가능성 여전히 존재</li>
<li>애플리케이션 가동 시간에 영향을 미칠 수 있음</li>
</ul></li>
<li>재해 복구 계획의 중요성
<ul>
<li>마이그레이션 수행 시 적절한 계획 수립 필요</li>
<li>데이터 및 서비스 연속성 보장</li>
</ul></li>
<li>Azure의 SQL Server 보호 방법 (두 가지 구성 요소)
<ul>
<li>Azure 플랫폼 옵션
<ul>
<li>지역 복제 스토리지 (백업용)</li>
<li>Azure Site Recovery (포괄적인 재해 복구 솔루션)</li>
</ul></li>
<li>SQL Server 전용 기능
<ul>
<li>가용성 그룹(Availability Groups)</li>
<li>네이티브 백업 기능</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="native-sql-server-backups" class="level4">
<h4 class="anchored" data-anchor-id="native-sql-server-backups">Native SQL Server backups</h4>
<ul>
<li>백업의 중요성
<ul>
<li>모든 데이터베이스 관리자에게 있어 생명줄과 같은 역할</li>
<li>클라우드 솔루션에서도 동일하게 중요</li>
</ul></li>
<li>Azure VM의 SQL Server 백업 특징
<ul>
<li>백업 시기와 저장 위치에 대한 세밀한 제어 가능</li>
<li>SQL 에이전트 작업을 통한 Azure blob 스토리지 직접 백업 지원</li>
<li>URL을 통한 백업 연결 방식 제공</li>
</ul></li>
<li>Azure 스토리지 옵션
<ul>
<li>지역 중복 스토리지(GRS) 제공</li>
<li>읽기 액세스 지역 중복 스토리지(RA-GRS) 제공</li>
<li>지리적으로 분산된 환경에서 백업 파일 안전 보장</li>
</ul></li>
<li>자동화 옵션
<ul>
<li>Azure SQL VM 서비스 제공자를 통한 자동 백업 관리 가능</li>
<li>플랫폼 수준의 백업 자동화 지원</li>
</ul></li>
</ul>
</section>
<section id="azure-backup-for-sql-server" class="level4">
<h4 class="anchored" data-anchor-id="azure-backup-for-sql-server">Azure Backup for SQL Server</h4>
<ul>
<li>기본 구성 요소
<ul>
<li>가상 머신에 에이전트 설치 필요</li>
<li>SQL Server 데이터베이스의 자동 백업 관리</li>
<li>Azure 서비스와 에이전트 간 통신</li>
</ul></li>
<li>주요 기능
<ul>
<li>지정된 RPO/RTO 메트릭 충족 관리</li>
<li>백업 모니터링을 위한 중앙 집중식 인터페이스 제공</li>
<li>장기 데이터 보존 지원</li>
<li>자동화된 백업 관리 기능</li>
<li>추가적인 데이터 보호 기능</li>
</ul></li>
<li>비용 및 가치
<ul>
<li>직접 백업보다 높은 비용</li>
<li>SQL Server용 Azure 리소스 공급자보다 비용 증가</li>
<li>더 포괄적인 엔터프라이즈 백업 솔루션 제공</li>
<li>완벽한 백업 기능 세트 제공</li>
</ul></li>
</ul>
</section>
<section id="azure-site-recovery" class="level4">
<h4 class="anchored" data-anchor-id="azure-site-recovery">Azure Site Recovery</h4>
<ul>
<li>개요
<ul>
<li>Azure 가상 머신의 블록 수준 복제를 수행하는 저비용 솔루션</li>
<li>재해 복구 전략을 테스트하고 검증할 수 있는 다양한 기능 제공</li>
</ul></li>
<li>적합한 사용 환경
<ul>
<li>상태가 없는 환경(예: 웹 서버)에 가장 적합</li>
<li>트랜잭션 데이터베이스 가상 머신에는 덜 적합</li>
</ul></li>
<li>SQL Server와 함께 사용 시 고려사항
<ul>
<li>더 높은 복구 지점 설정 필요 (잠재적 데이터 손실 의미)</li>
<li>RTO(복구 시간 목표)가 본질적으로 RPO(복구 지점 목표)가 됨</li>
</ul></li>
<li>작동 프로세스
<ul>
<li>VM이 Azure Site Recovery에 등록</li>
<li>데이터가 캐시로 지속적으로 복제</li>
<li>캐시가 대상 스토리지 계정으로 복제</li>
<li>장애 조치(failover) 중에 가상 머신이 대상 환경에 추가</li>
</ul></li>
</ul>


</section>
</section>
</section>

 ]]></description>
  <category>Engineering</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Engineering/data_engineering/2.IaaS.html</guid>
  <pubDate>Wed, 02 Apr 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Azure 가상 머신의 SQL Server</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Engineering/data_engineering/1.azure_server_based_db_management.html</link>
  <description><![CDATA[ 




<section id="microsoft-intelligent-data-platform-역할-설명" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="microsoft-intelligent-data-platform-역할-설명"><span class="header-section-number">1</span> <a href="https://learn.microsoft.com/ko-kr/training/modules/prepare-to-maintain-sql-databases-azure/2-describe-azure-data-platform-roles">Microsoft Intelligent Data Platform 역할 설명</a></h2>
<p>Microsoft는 클라우드(Microsoft Intelligent Data Platform 서비스 활용) 데이터 관련 업무를 위한 5가지 주요 역할을 정의하고 있다:</p>
<ol type="1">
<li>Azure 데이터베이스 관리자
<ul>
<li>Azure 데이터 서비스와 SQL Server 기반 데이터 플랫폼 솔루션 관리 (데이터 관리, 모니터링, 보안 및 개인정보 보호 설계)</li>
<li>T-SQL을 활용한 일상적인 운영 및 관리 작업 수행</li>
</ul></li>
<li>Azure 데이터 분석가
<ul>
<li>Microsoft Power BI를 사용</li>
<li>확장 가능한 데이터 모델을 설계 및 구축하고, 데이터를 정리 및 변환하며, 이해하기 쉬운 데이터 시각화</li>
</ul></li>
<li>Azure 데이터 분석가
<ul>
<li>Power BI를 활용한 데이터 모델링 및 시각화</li>
<li>데이터 정제/변환을 통한 비즈니스 인사이트 도출</li>
</ul></li>
<li>Azure 데이터 과학자
<ul>
<li>Azure Machine Learning 기반 ML 워크로드 구현</li>
<li>데이터 과학/머신러닝 지식 활용</li>
</ul></li>
<li>Azure 인공지능 엔지니어
<ul>
<li>Cognitive Services, ML, Knowledge Mining 활용</li>
<li>자연어처리, 음성, 컴퓨터 비전, 봇 등 AI 솔루션 구현</li>
</ul></li>
</ol>
</section>
<section id="azure-vm의-sql-server" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="azure-vm의-sql-server"><span class="header-section-number">2</span> <a href="https://learn.microsoft.com/ko-kr/training/modules/prepare-to-maintain-sql-databases-azure/3-understand-sql-server-azure-virtual-machine">Azure VM의 SQL Server</a></h2>
<section id="azure-가상-머신의-sql-server-특징-백업-고가용성" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="azure-가상-머신의-sql-server-특징-백업-고가용성"><span class="header-section-number">2.1</span> Azure 가상 머신의 SQL Server: 특징, 백업, 고가용성</h3>
<ul>
<li>Azure 가상 머신에서 실행되는 SQL Server(IaaS)는 온프레미스 SQL Server와 동등</li>
<li>가상 머신에서 SQL Server를 선택하는 주요 이유:</li>
</ul>
<ol type="1">
<li><strong>애플리케이션 호환성</strong>: 특정 버전의 SQL Server가 필요하거나 PaaS와 호환되지 않는 설치 요구사항이 있는 경우</li>
<li><strong>다양한 SQL Server 서비스 활용</strong>: SQL Server Analysis Services(SSAS), Integration Services(SSIS), Reporting Services(SSRS)를 데이터베이스 엔진과 함께 실행할 수 있음</li>
</ol>
</section>
<section id="백업-솔루션" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="백업-솔루션"><span class="header-section-number">2.2</span> 백업 솔루션</h3>
<p>최근 SQL Server 릴리스에서는 다음 두 가지 주요 백업 기능을 제공: - <strong>URL로 백업</strong>: Azure Blob Storage 서비스에 데이터베이스를 백업 * Azure Blob Storage란? * Blob (Binary Large Object): 이미지, 오디오, 비디오, 문서 등과 같은 대용량 비정형 데이터를 저장하는 데 사용되는 데이터 형식 * Azure의 클라우드 기반 스토리지 서비스 * 비교적 저렴한 비용으로 높은 가용성, 내구성, 확장성을 제공 * 데이터 암호화, 보안, 백업 등 다양한 기능 제공 * <strong>Azure Backup</strong>: SQL Server VM을 위한 포괄적인 엔터프라이즈 백업 솔루션</p>
</section>
<section id="배포-옵션" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="배포-옵션"><span class="header-section-number">2.3</span> 배포 옵션</h3>
<ul>
<li>Azure의 모든 리소스는 Azure Resource Manager를 통해 관리되고 배포</li>
<li>최종적으로 JSON 문서인 Azure Resource Manager 템플릿으로 변환됨</li>
<li>대규모 배포에는 선언적 접근 방식이 권장됨</li>
</ul>
</section>
<section id="azure-스토리지-개요" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="azure-스토리지-개요"><span class="header-section-number">2.4</span> Azure 스토리지 개요</h3>
<p>SQL Server 운영 환경에서는 다음 4가지 스토리지 유형 제공:</p>
<ul>
<li><strong>Standard HDD</strong>: 비용 절감을 위해 사용, 데이터베이스 백업에는 표준 스토리지를 사용</li>
<li><strong>Standard SSD</strong>: 비교적 빠른 속도와 비교적 저렴한 비용</li>
<li><strong>Premium SSD</strong>: 5-10ms 지연 시간</li>
<li><strong>Ultra Disk</strong>: 1-2ms 지연 시간(최적화 시 1ms 미만 가능)</li>
</ul>
</section>
<section id="azure의-high-availability" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="azure의-high-availability"><span class="header-section-number">2.5</span> Azure의 High Availability</h3>
<ul>
<li>Azure 플랫폼은 fault tolerance을 갖추고 있으며 서비스 중단과 일시적 오류로부터 빠르게 복구됨</li>
<li>Premium SSD 또는 Ultra Disk를 사용하는 단일 인스턴스 Azure 가상 머신의 경우 최소 99.9%(1년의 0.01%인 약 9시간 정도의 가동 중단 리스크)의 가동 시간을 보장</li>
<li>가용성 집합, 가용성 영역, 부하 분산 기술을 통해 high availability을 제공</li>
</ul>
</section>
<section id="azure-arc-지원-sql-server" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="azure-arc-지원-sql-server"><span class="header-section-number">2.6</span> Azure Arc 지원 SQL Server</h3>
<ul>
<li>Azure Arc는 Azure 관리 기능을 온프레미스, 다른 클라우드 또는 엣지에서 실행되는 SQL Server 인스턴스로 확장</li>
<li>Azure Arc를 통해 기존 SQL Server를 Azure로 이전하지 않고도 일관된 정책 적용, 규정 준수, Azure Monitor 및 Security Center 활용이 가능</li>
<li>중앙 집중식 관리, 자동 업데이트, 백업 및 복원, 재해 복구와 같은 고급 기능을 활용할 수 있으며, Azure의 머신러닝과 AI 기능도 기존 데이터에 적용 가능</li>
</ul>
</section>
</section>
<section id="클라우드-기반-azure-sql-database-설계" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="클라우드-기반-azure-sql-database-설계"><span class="header-section-number">3</span> <a href="https://learn.microsoft.com/ko-kr/training/modules/prepare-to-maintain-sql-databases-azure/4-design-azure-sql-database-for-cloud-native-applications">클라우드 기반 Azure SQL Database 설계</a></h2>
<p>Azure SQL Database는 PaaS(Platform as a Service) 형태로 제공되는 고확장성 데이터베이스 서비스로, 최소한의 유지 관리로 특정 워크로드에 최적화되어 있다. 개발자에게 유연성과 세분화된 배포 옵션을 제공하여 새로운 애플리케이션 개발에 적합하다.</p>
<section id="구매-모델" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="구매-모델"><span class="header-section-number">3.1</span> 구매 모델</h3>
<section id="vcore-기반-모델" class="level4">
<h4 class="anchored" data-anchor-id="vcore-기반-모델">1. vCore 기반 모델</h4>
<ul>
<li>컴퓨팅과 스토리지 리소스를 독립적으로 확장 가능</li>
<li>서비스 Tier:
<ul>
<li><strong>범용(General Purpose)</strong>: 덜 집약적인 작업용, 프로비저닝 및 서버리스 컴퓨팅 계층 제공</li>
<li><strong>비즈니스 크리티컬(Business Critical)</strong>: In-Memory OLTP, 읽기 전용 복제본, 로컬 SSD 지원
<ul>
<li>OLTP: In-Memory OLTP(Online Transaction Processing)는 Microsoft SQL Server에서 제공하는 메모리 최적화 기술</li>
<li>이 기술은 데이터베이스 테이블과 저장 프로시저를 메인 메모리(RAM)에 저장하여 디스크 I/O 작업을 최소화함으로써 트랜잭션 처리 성능을 크게 향상</li>
<li>주요 특징
<ul>
<li>데이터를 디스크가 아닌 메모리에 저장하여 액세스 속도 향상</li>
<li>락(lock)이나 래치(latch) 없는 동시성 제어 메커니즘으로 경합 감소</li>
<li>높은 처리량과 낮은 지연 시간이 필요한 트랜잭션 중심 애플리케이션에 적합</li>
</ul></li>
</ul></li>
<li><strong>하이퍼스케일(Hyperscale)</strong>: 수평적 확장 기능, 대규모 데이터 처리에 적합</li>
</ul></li>
</ul>
</section>
<section id="dtu-기반-모델" class="level4">
<h4 class="anchored" data-anchor-id="dtu-기반-모델">2. DTU 기반 모델</h4>
<ul>
<li>컴퓨팅과 스토리지가 DTU 수준에 종속
<ul>
<li>DTU(Database Transaction Unit): Azure SQL Database에서 사용하는 성능 측정 단위</li>
<li>Microsoft가 개발한 이 단위는 CPU, 메모리, 데이터 I/O 및 트랜잭션 로그 I/O의 혼합된 측정값으로, 데이터베이스 성능을 단일 값으로 표현</li>
<li>DTU 기반 구매 모델에서는 데이터베이스에 특정 DTU 레벨을 할당하며, 이 레벨은 해당 데이터베이스가 사용할 수 있는 컴퓨팅 및 스토리지 리소스의 양을 결정 (DTU 계층에 따라 성능과 비용이 증가)</li>
<li>이 모델은 vCore 모델과 달리 컴퓨팅과 스토리지가 함께 묶여 있어, 스토리지만 늘리고 싶어도 DTU 수준을 전체적으로 올려야 하는 단점이 있다.</li>
</ul></li>
<li>Basic, Standard, and Premium 세 가지 서비스 tier 제공</li>
<li>스토리지 한도 도달 시 컴퓨팅 사용률과 무관하게 DTU 증가 필요</li>
</ul>
</section>
</section>
<section id="서버리스-컴퓨팅" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="서버리스-컴퓨팅"><span class="header-section-number">3.2</span> 서버리스 컴퓨팅</h3>
<ul>
<li>자동 확장 및 일시 중지 기능을 제공하여 개발/테스트 환경의 비용 절감에 효과적</li>
<li>워크로드에 따라 동적으로 확장되며 비활성 상태일 때는 자동으로 일시 중지되어 스토리지 비용만 발생</li>
</ul>
</section>
<section id="배포-모델" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="배포-모델"><span class="header-section-number">3.3</span> 배포 모델</h3>
<section id="단일-데이터베이스" class="level4">
<h4 class="anchored" data-anchor-id="단일-데이터베이스">1.단일 데이터베이스</h4>
<ul>
<li>각 데이터베이스를 개별적으로 관리</li>
<li>동일 서버에 배포되어도 각 데이터베이스는 전용 리소스 보유</li>
<li>Azure 포털을 통한 리소스 모니터링 가능</li>
</ul>
</section>
<section id="탄력적-풀" class="level4">
<h4 class="anchored" data-anchor-id="탄력적-풀">2.탄력적 풀</h4>
<ul>
<li>여러 데이터베이스가 리소스를 공유하는 모델</li>
<li>개별 데이터베이스 스케일링이 불필요해 관리 용이</li>
<li>SaaS 애플리케이션에 비용 효율적</li>
<li>사용률이 낮은 멀티테넌트 환경에 적합</li>
</ul>
</section>
</section>
<section id="주요-기능" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="주요-기능"><span class="header-section-number">3.4</span> 주요 기능</h3>
<section id="네트워크-옵션" class="level4">
<h4 class="anchored" data-anchor-id="네트워크-옵션">네트워크 옵션</h4>
<p>방화벽 규칙, 가상 네트워크 엔드포인트, Private Link 등으로 액세스 제어가 가능</p>
</section>
<section id="백업-및-복원" class="level4">
<h4 class="anchored" data-anchor-id="백업-및-복원">백업 및 복원</h4>
<p>Azure는 SQL Database 및 SQL Managed Instance에 대한 원활한 백업 및 복원 기능을 제공</p>
<ul>
<li><strong>지속적 백업(seamless backup)</strong>: 정기적인 백업과 지역 중복 스토리지, 전체 백업은 매주 수행되고, 차등 백업은 12~24시간마다 수행되며, 트랜잭션 로그 백업은 5~10분마다 수행됨</li>
<li><strong>지역 복원(geo-restore)</strong>: 다른 지리적 지역으로 복원 가능, 덜 엄격한 재해 복구 시나리오에 특히 유용</li>
<li><strong>시점 복원(PITR, point-in-time restore)</strong>: 특정 시점 복원 가능, 1-35일 보존 기간 설정 가능, 실제로 지정하지 않으면 기본 구성은 7일</li>
<li><strong>장기 보존(LTR, long-term retention)</strong>: 최대 10년까지 보존 정책 설정, 이 옵션은 기본적으로 사용하지 않도록 설정</li>
</ul>
</section>
<section id="자동-튜닝" class="level4">
<h4 class="anchored" data-anchor-id="자동-튜닝">자동 튜닝</h4>
<p>머신러닝 기반으로 쿼리 성능을 자동 최적화 - 비용 많은 쿼리 식별 - 실행 계획 최적화 - 인덱스 추가/제거 기능</p>
</section>
<section id="탄력적-쿼리-및-작업" class="level4">
<h4 class="anchored" data-anchor-id="탄력적-쿼리-및-작업">탄력적 쿼리 및 작업</h4>
<ul>
<li>여러 데이터베이스에 걸친 쿼리 실행과 유지 관리 작업 자동화를 지원</li>
<li>여러 데이터베이스를 연결하는 T-SQL 쿼리를 실행 가능하고 이 기능은 변경할 수 없는 세 부분 및 네 부분으로 된 이름을 사용하는 애플리케이션에 유용</li>
<li>마이그레이션을 허용하기 때문에 이식성이 향상</li>
<li>다음 분할 전략을 지원
<ul>
<li>수직 분할: 데이터베이스 간 쿼리라고도 합니다. 데이터는 여러 데이터베이스 간에 세로로 분할 (컬럼 단위 분할도 가능).</li>
<li>가로 분할: 데이터는 여러 확장된 데이터베이스에 행을 분산하기 위해 수평(행또는 record 단위)으로 분할</li>
</ul></li>
</ul>
</section>
<section id="microsoft-fabric-통합" class="level4">
<h4 class="anchored" data-anchor-id="microsoft-fabric-통합">Microsoft Fabric 통합</h4>
<ul>
<li>Microsoft 생태계와 완벽하게 통합되어 데이터 워크플로 간소화, 협업 강화, 고급 분석 지원 등의 이점을 제공</li>
<li>Microsoft Fabric은 모든 데이터 및 분석 요구 사항을 위한 통합 플랫폼
<ul>
<li>Microsoft의 SaaS(Software as a Service) 분석 제품군</li>
<li>Power BI, Azure Synapse Analytics, Azure Data Factory 등의 Microsoft 서비스를 하나의 통합된 환경으로 결합</li>
</ul></li>
<li>주요 특징:
<ul>
<li>데이터 수집, 처리, 저장, 분석, 시각화를 위한 end-to-end 솔루션</li>
<li>공통 데이터 저장소인 OneLake를 중심으로 구축</li>
<li>다양한 워크로드 지원: 데이터 엔지니어링, 데이터 과학, 실시간 분석, BI 등</li>
<li>통합된 거버넌스 및 보안 모델</li>
<li>Microsoft 365와 긴밀한 통합</li>
<li>Fabric은 SQL Database와 같은 여러 데이터 서비스를 통합하여 조직이 데이터 사일로를 제거하고 더 효율적으로 데이터를 활용하도록 지원</li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="azure-sql-managed-instance" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="azure-sql-managed-instance"><span class="header-section-number">4</span> <a href="https://learn.microsoft.com/ko-kr/training/modules/prepare-to-maintain-sql-databases-azure/5-explore-azure-sql-database-managed-instance">Azure SQL Managed Instance</a></h2>
<p>Azure SQL Managed Instance는 Azure SQL Database와 많은 공통 코드를 공유하는 완전 관리형 PaaS 솔루션으로 SQL Server 잘 작동하는 완전 관리형 데이터베이스 서비스</p>
<section id="주요-이점" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="주요-이점"><span class="header-section-number">4.1</span> 주요 이점</h3>
<ul>
<li>자동 백업 및 패치</li>
<li>기본 제공 고가용성(99.99% 가동시간 보장, 년간 52분 가동 중단 시간 허용 리스크)</li>
<li>보안 및 성능 도구</li>
<li>통합 감사 기능</li>
<li>SQL Server 설치 및 패치 불필요로 유지 관리 감소</li>
</ul>
</section>
<section id="sql-database와의-차별점" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="sql-database와의-차별점"><span class="header-section-number">4.2</span> SQL Database와의 차별점</h3>
<p>SQL Database가 단일 데이터베이스 중심인 반면, SQL Managed Instance는 다음을 지원: - 데이터베이스 간 쿼리 - CLR(Common Language Runtime, 공통 언어 런타임) - 다양한 .NET 언어(C#, Visual Basic .NET 등)로 작성된 코드를 SQL Server 내에서 실행 가능 - 스토어드 프로시저, 트리거, 사용자 정의 함수, 사용자 정의 형식, 사용자 정의 집계 등을 .NET 언어로 작성 가능 - T-SQL로 구현하기 어려운 복잡한 비즈니스 로직, 문자열 처리, 수학 계산 등을 효율적으로 처리 가능 - T-SQL(Transact-SQL): Microsoft SQL Server에서 사용되는, SQL(Structured Query Language)의 확장 버전 - 외부 리소스(파일 시스템, 네트워크 등)에 접근하는 코드 실행 가능 - 시스템 데이터베이스 액세스 - SQL 에이전트 기능 - Microsoft SQL Server의 작업 자동화 서비스 - 작업 예약, 작업 모니터링, 작업 로깅 등 다양한 기능 제공</p>
</section>
<section id="하이브리드-라이선스-옵션" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="하이브리드-라이선스-옵션"><span class="header-section-number">4.3</span> 하이브리드 라이선스 옵션</h3>
<ul>
<li>Active Software Assurance가 있는 기존 라이선스를 활용해 PaaS(SQL Database 및 SQL Managed Instance) 비용 절감(최대 40%)
<ul>
<li>Enterprise Edition: 코어당 비즈니스 크리티컬 vCore 1개 또는 범용 vCore 8개</li>
<li>Standard Edition: 코어당 범용 vCore 1개</li>
</ul></li>
</ul>
</section>
<section id="connectivity-architecture" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="connectivity-architecture"><span class="header-section-number">4.4</span> Connectivity Architecture</h3>
<ul>
<li>SQL Managed Instance에 대한 연결은 TDS 엔드포인트를 통한 연결
<ul>
<li>TDS(Tabular Data Stream)는 Microsoft SQL Server 및 Sybase에서 클라이언트와 데이터베이스 서버 간의 통신에 사용되는 네트워크 프로토콜</li>
</ul></li>
<li>고가용성 방식으로 배포된 게이트웨이 구성 요소</li>
<li>자동화된 백업(지역 중복 및 자동 복제)</li>
<li>자동 장애 조치 그룹 지원</li>
</ul>
</section>
<section id="migration-options" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="migration-options"><span class="header-section-number">4.5</span> Migration Options</h3>
<ol type="1">
<li><strong>Log 재생 서비스</strong>: 온라인 마이그레이션, 세부 제어 가능</li>
<li><strong>Azure Data Studio 확장</strong>: 준비 상태 평가, 리소스 추천, 중소규모 DB에 적합</li>
<li><strong>Managed Instance 링크</strong>: 분산 가용성 그룹 사용, 즉시 데이터 복제</li>
<li><strong>네이티브 백업 및 복원</strong>: 간단한 마이그레이션 방법</li>
<li><strong>트랜잭션 복제</strong>: 대규모 DB의 온/오프라인 마이그레이션에 적합</li>
</ol>
</section>
<section id="machine-learning-services" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="machine-learning-services"><span class="header-section-number">4.6</span> Machine Learning Services</h3>
<ul>
<li>Python 및 R 패키지 지원</li>
<li>데이터 이동 없이 기계 학습 모델 학습 및 배포</li>
<li>T-SQL 저장 프로시저를 통한 모델 배포</li>
<li>scikit-learn, PyTorch, TensorFlow 등 오픈소스 라이브러리 지원</li>
<li>T-SQL PREDICT 함수로 예측 가속화</li>
<li><code>sp_configure 'external scripts enabled', 1;</code> 명령으로 활성화</li>
</ul>


</section>
</section>

 ]]></description>
  <category>Engineering</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Engineering/data_engineering/1.azure_server_based_db_management.html</guid>
  <pubDate>Tue, 01 Apr 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Azure SQL Database 관리</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Engineering/data_engineering/0.azure_dba_intro.html</link>
  <description><![CDATA[ 




<section id="한눈에-보기" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 한눈에 보기</h1>
<ul>
<li>관련 서비스: Azure SQL Database, Azure SQL Managed Instance, Azure Virtual Machines의 SQL Server</li>
<li>역할: 데이터 분석가, 데이터 엔지니어, 데이터베이스 관리자</li>
<li>주제: 데이터베이스 관리</li>
</ul>
<section id="데이터베이스와-관리-시스템의-이해" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="데이터베이스와-관리-시스템의-이해"><span class="header-section-number">1.1</span> 데이터베이스와 관리 시스템의 이해</h2>
<section id="데이터베이스의-기본-개념" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="데이터베이스의-기본-개념"><span class="header-section-number">1.1.1</span> 데이터베이스의 기본 개념</h3>
<p>데이터베이스는 데이터를 체계적으로 저장하고 관리하는 시스템이다. 데이터의 저장, 검색, 갱신을 효율적으로 수행할 수 있는 기능을 제공한다.</p>
</section>
<section id="데이터베이스-관리-시스템dbms" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="데이터베이스-관리-시스템dbms"><span class="header-section-number">1.1.2</span> 데이터베이스 관리 시스템(DBMS)</h3>
<p>DBMS(Database Management System)는 데이터베이스를 운영하고 관리하는 소프트웨어 시스템으로, 다음과 같은 핵심 기능을 제공한다:</p>
<ul>
<li>데이터베이스 생성 및 스키마 관리</li>
<li>데이터 입력, 수정, 삭제 기능</li>
<li>데이터 무결성 유지</li>
<li>보안 및 접근 제어</li>
</ul>
</section>
<section id="데이터베이스-관리자dba의-역할" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3" class="anchored" data-anchor-id="데이터베이스-관리자dba의-역할"><span class="header-section-number">1.1.3</span> 데이터베이스 관리자(DBA)의 역할</h3>
<p>DBA(Database Administrator)는 데이터베이스의 전반적인 운영을 책임지는 전문가로서 다음과 같은 업무를 하는 사람이다:</p>
<ul>
<li>데이터베이스 설계 및 구현</li>
<li>성능 최적화 및 모니터링</li>
<li>보안 관리 및 백업/복구</li>
<li>사용자 권한 관리</li>
</ul>
</section>
</section>
<section id="azure-클라우드-환경에서의-데이터베이스-관리" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="azure-클라우드-환경에서의-데이터베이스-관리"><span class="header-section-number">1.2</span> Azure 클라우드 환경에서의 데이터베이스 관리</h2>
<section id="azure-데이터베이스-서비스의-중요성" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="azure-데이터베이스-서비스의-중요성"><span class="header-section-number">1.2.1</span> Azure 데이터베이스 서비스의 중요성</h3>
<ul>
<li>현대 비즈니스 운영에서 클라우드 플랫폼 활용은 필수적</li>
<li>Azure는 안정적이고 확장 가능한 데이터베이스 서비스 제공</li>
<li>Microsoft와의 협업을 통한 지속적인 서비스 개선</li>
</ul>
</section>
<section id="azure-서비스-모델" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="azure-서비스-모델"><span class="header-section-number">1.2.2</span> Azure 서비스 모델</h3>
<p>Azure는 세 가지 주요 서비스 모델을 제공한다:</p>
<ol type="1">
<li>Infrastructure as a Service (IaaS)
<ul>
<li>가상 머신, 스토리지, 네트워킹 제공</li>
<li>사용자가 직접 패치 및 소프트웨어 관리</li>
<li>높은 수준의 커스터마이징 가능</li>
</ul></li>
<li>Platform as a Service (PaaS)
<ul>
<li>클라우드 제공업체가 더 많은 관리 작업 담당</li>
<li>사용자는 애플리케이션과 데이터에 집중 가능</li>
<li>관리 부담 감소</li>
</ul></li>
<li>Software as a Service (SaaS)
<ul>
<li>완전히 관리되는 소프트웨어 애플리케이션 제공</li>
<li>사용자는 서비스만 이용</li>
<li>설치, 유지보수, 업그레이드가 모두 자동화</li>
</ul></li>
</ol>


</section>
</section>
</section>

 ]]></description>
  <category>Engineering</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Engineering/data_engineering/0.azure_dba_intro.html</guid>
  <pubDate>Mon, 31 Mar 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 벡터화: Attention 메커니즘의 이해</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-5.attention.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 기존 Seq2Seq 모델의 한계를 극복하기 위해 제안된 Attention 메커니즘의 기본 원리와 구조를 소개한다. Attention은 입력 시퀀스의 각 요소에 대한 중요도를 동적으로 계산하여 출력 시퀀스 생성 시 필요한 정보를 효과적으로 활용할 수 있게 해주는 혁신적인 접근 방식이다.</p>
<p>주요 내용은 다음과 같다.</p>
<ul>
<li><strong>Seq2Seq 모델의 한계와 Attention의 등장</strong>:
<ul>
<li>기존 Seq2Seq 모델은 입력 문장의 모든 정보를 고정된 크기의 컨텍스트 벡터에 압축하는 과정에서 정보 손실이 발생한다.</li>
<li>Attention은 이러한 한계를 극복하기 위해 입력 시퀀스의 각 요소에 대한 중요도를 동적으로 계산하여 필요한 정보를 선택적으로 활용한다.</li>
</ul></li>
<li><strong>Attention의 핵심 구성 요소 및 작동 원리</strong>:
<ul>
<li><strong>Query, Key, Value</strong>: 입력 시퀀스의 각 요소를 Query, Key, Value로 변환하여 유사도와 중요도를 계산한다.</li>
<li><strong>Attention Score</strong>: Query와 Key의 유사도를 계산하여 각 입력 요소의 중요도를 결정한다.</li>
<li><strong>Attention Weight</strong>: Attention Score를 정규화하여 각 입력 요소에 대한 가중치를 생성한다.</li>
<li><strong>Context Vector</strong>: Attention Weight와 Value를 결합하여 최종 컨텍스트 벡터를 생성한다.</li>
</ul></li>
<li><strong>Attention의 장점</strong>:
<ul>
<li>입력 시퀀스의 길이에 상관없이 모든 정보를 효과적으로 활용할 수 있다.</li>
<li>출력 시퀀스 생성 시 필요한 정보를 선택적으로 집중할 수 있다.</li>
<li>모델의 해석 가능성을 높이고, 장기 의존성 문제를 효과적으로 해결한다.</li>
</ul></li>
<li><strong>의의</strong>: Attention은 자연어 처리, 컴퓨터 비전 등 다양한 분야에서 혁신적인 성능을 보여주며, Transformer와 같은 최신 모델의 기반이 되었다.</li>
</ul>
<p>이 문서를 통해 독자는 Attention이 어떻게 시퀀스 데이터의 장기 의존성 문제를 해결하고, 입력과 출력 간의 관계를 효과적으로 모델링할 수 있는지에 대한 기본적인 이해를 얻을 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention
 
Transformer 이후 생성형 모델 발전 계열
├── Transformer 구조 (Vaswani et al., 2017)
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등</code></pre>
</section>
<section id="attention" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Attention</h1>
<section id="기존-sequence-to-sequence-seq2seq의-한계" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="기존-sequence-to-sequence-seq2seq의-한계"><span class="header-section-number">3.1</span> 기존 Sequence to Sequence, Seq2Seq의 한계</h2>
<ul>
<li>입력 문장의 길이와 상관없이 고정된 크기의 벡터에 정보를 모두 압축한다.</li>
<li>Bottleneck 문제: 입력 문장의 길이가 길어질수록 고정된 크기의 벡터에 정보가 다 압축되지 않아 정보 손실이 발생한다.</li>
<li>Encoder -&gt; Context Vector -&gt; Decoder
<ul>
<li>단어를 Embedding 후 Encoder에서 벡터화되어 Context Vector가 됨</li>
<li>즉, Context Vector는 Encoder의 LSTM의 마지막 hidden state의 출력값 (벡터 크기가 고정)</li>
<li>이 고정된 크기에 정보가 모두 압축되지 않는다면 정보 손실이 발생</li>
<li>벡터 크기가 고정되어 있기 때문에 입력 문장의 길이가 길어질수록 정보 손실이 발생한다.</li>
<li>입력 문장의 길이가 길어질수록 정보 손실이 발생한다.</li>
<li>정보 손실이 일어난 Context Vector가 Decoder의 Input 벡터가 됨</li>
</ul></li>
<li>RNN 자체 문제: RNN 계열의 고질적인 장기 의존성 문제로 초기 정보가 손실되며 전달된다.
<ul>
<li>LSTM과 GRU가 장기 손실을 줄이기 위해 고안된 모델이지만 여전히 장기 의존성 문제가 발생한다.</li>
<li>전체 한 문장도 잘 기억 못함 (even with LSTM, GRU)</li>
<li>Carnegie Mellon University의 연구 : BLEU Score 측정</li>
</ul></li>
</ul>
</section>
<section id="attention의-정의" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="attention의-정의"><span class="header-section-number">3.2</span> Attention의 정의</h2>
<ul>
<li>사전적 의미: 주의, 집중</li>
<li>NLP에서의 의미: 번역 문장을 만드는 과정에서 기존 문장에서 주용한 단어를 집중(Attention)</li>
<li>예: I am a good student를 한글로 번역할 때 각 문장에서 주요 단어에 집중하여 번역
<ol type="1">
<li><code>I</code> 에 집중 -&gt; 나는<br>
</li>
<li><code>good</code> 에 집중 -&gt; 나는 좋은</li>
<li><code>student</code> 에 집중 -&gt; 나는 좋은 학생</li>
<li><code>am</code> 에 집중 -&gt; 나는 좋은 학생이다.</li>
</ol></li>
<li>단어를 생성할 때 기존에 선택한 단어의 유사도와 문맥을 고려하여 다음 단어를 선택</li>
</ul>
</section>
<section id="원리" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="원리"><span class="header-section-number">3.3</span> 원리</h2>
<section id="key-value-형태로-학습" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="key-value-형태로-학습"><span class="header-section-number">3.3.1</span> Key Value 형태로 학습</h3>
<ul>
<li>Attention 메커니즘: Attention(Q, K, V) = Attention Value
<ul>
<li>Q: Query, K: Key, V: Value</li>
<li>Q,K,V를 입력받아 Attention Value를 출력</li>
</ul>
<ol type="1">
<li>어텐션 함수는 주어진 쿼리(Q)에 대해서 모든 키(K)와의 유사도를 각 각 계산한다.</li>
<li>구해낸 이 유사도를 키(K)와 맵핑되어있는 각각의 값(V)에 곱하여 반영해준다.</li>
<li>유사도가 반영된 값을 모두 더해서 리턴한다.</li>
<li>이렇게 출력된 값을 어테션 값 (attention value)이라고 한다.</li>
<li>이 어테션 값을 출력으로 사용한다.</li>
</ol></li>
<li>예를 들어 Q 1개, Key 3개, Value 3개가 있다면
<ul>
<li>Q와 K1, K2, K3의 유사도를 계산: <img src="https://latex.codecogs.com/png.latex?Q%20%5Ccdot%20K1">, <img src="https://latex.codecogs.com/png.latex?Q%20%5Ccdot%20K2">, <img src="https://latex.codecogs.com/png.latex?Q%20%5Ccdot%20K3"></li>
<li>유사도를 V1, V2, V3에 곱하여 반영: <img src="https://latex.codecogs.com/png.latex?V'1=%20K1%20%5Ccdot%20V1">, <img src="https://latex.codecogs.com/png.latex?V'2=%20K2%20%5Ccdot%20V2">, <img src="https://latex.codecogs.com/png.latex?V'3=%20K3%20%5Ccdot%20V3"></li>
<li>attention score = 유사도가 반영된 값 <img src="https://latex.codecogs.com/png.latex?V'1"> , <img src="https://latex.codecogs.com/png.latex?V'2"> , <img src="https://latex.codecogs.com/png.latex?V'3"></li>
<li>softmax([<img src="https://latex.codecogs.com/png.latex?V'1"> , <img src="https://latex.codecogs.com/png.latex?V'2"> , <img src="https://latex.codecogs.com/png.latex?V'3">])을 구함</li>
<li>attention value, a1 = 위의 3값과 hidden state의 값을 내적하여 모두 더함</li>
<li>즉, a1은 decoder의 예측 단어와 입력단어들의 유사도 정보가 있음. (유사도가 높으면 가중치가 높게 부여되어 반영됨)<br>
</li>
<li>a1과 decoder의 마지막 시점의 hidden state를 내적하여 tanh를 취하여 출력</li>
<li><img src="https://latex.codecogs.com/png.latex?y_t%20=%20%5Ctext%7Bsoftmax%7D(W_y%5Ctilde%7Bs_t%7D+b_y)"></li>
</ul></li>
<li>참고로 Seq2Seq의 hidden state가 Key이자 Value가 역할을 한다.</li>
</ul>
</section>
<section id="수식" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="수식"><span class="header-section-number">3.3.2</span> 수식</h3>
<ol type="1">
<li><p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bscore%7D_i%20=%20Q%20%5Ccdot%20K_i"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_i%20=%20%5Ctext%7Bsoftmax%7D(%5Ctext%7Bscore%7D_i)"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?a_t%20=%20%5Csum_i%20%5Calpha_i%20V_i"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bs_t%7D%20=%20%5Ctanh(W_c%20%5Ba_t;%20s_t%5D)"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?y_t%20=%20%5Ctext%7Bsoftmax%7D(W_y%20%5Ctilde%7Bs_t%7D%20+%20b_y)"></p></li>
<li><p><strong>유사도(Score) 계산: Query와 Key의 내적</strong></p></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bscore%7D_i%20=%20Q%20%5Ccdot%20K_i%20%5Cquad%20%5Ctext%7Bfor%20%7D%20i%20=%201,%202,%203%0A"></p>
<p>혹은 전체를 벡터화하면: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bscores%7D%20=%20Q%20K%5ET%20%5Cquad%20%5Ctext%7B(Q:%201%C3%97d,%20K:%203%C3%97d%20%E2%86%92%20scores:%201%C3%973)%7D%0A"></p>
<blockquote class="blockquote">
<p>Scaled Dot-Product Attention에서는 보통 다음과 같이 스케일 조정:</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bscores%7D%20=%20%5Cfrac%7BQ%20K%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%0A"></p>
<ol start="2" type="1">
<li><strong>Softmax로 유사도 정규화 (Attention Weights)</strong></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_i%20=%20%5Cfrac%7B%5Cexp(%5Ctext%7Bscore%7D_i)%7D%7B%5Csum_%7Bj=1%7D%5E%7B3%7D%20%5Cexp(%5Ctext%7Bscore%7D_j)%7D%0A%5Cquad%20%5Ctext%7B(i%20=%201,%202,%203)%7D%0A"></p>
<p>또는 벡터 전체:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboldsymbol%7B%5Calpha%7D%20=%20%5Ctext%7Bsoftmax%7D(Q%20K%5ET)%0A"></p>
<hr>
<ol start="3" type="1">
<li><strong>각 Value 벡터에 가중치를 곱해 합산 (Attention Output)</strong></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BAttention%7D(Q,%20K,%20V)%20=%20%5Csum_%7Bi=1%7D%5E%7B3%7D%20%5Calpha_i%20V_i%20=%20%5Cboldsymbol%7B%5Calpha%7D%20%5Ccdot%20V%0A"></p>
<p>즉, 전체 수식은:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BAttention%7D(Q,%20K,%20V)%20=%20%5Ctext%7Bsoftmax%7D%5Cleft(%20%5Cfrac%7BQ%20K%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%20%5Cright)%20V%0A"></p>
<ol start="4" type="1">
<li><strong>디코더 hidden state와 결합 후 출력 계산</strong></li>
</ol>
<ul>
<li>context vector (a₁)와 디코더의 hidden state <img src="https://latex.codecogs.com/png.latex?s_t"> 결합:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctilde%7Bs_t%7D%20=%20%5Ctanh(W_c%20%5Ba_t;%20s_t%5D)%0A"></p>
<ul>
<li>최종 출력 (예측 단어 확률 분포):</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay_t%20=%20%5Ctext%7Bsoftmax%7D(W_y%20%5Ctilde%7Bs_t%7D%20+%20b_y)%0A"></p>
</section>
</section>
<section id="강점" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="강점"><span class="header-section-number">3.4</span> 강점</h2>
<ul>
<li>RNN계열 Seq2Seq 구조에 도입되어 기계 번역의 성능을 상당 부분 개선</li>
<li>후에, attention으로 모든 state에 접근하여 더 나은 성능을 보임 = Attention만으로도 성능 월등</li>
<li>결국, RNN은 필요하지 않게 되었음 = 모든 정보를 벡터화하여 저장하는 것이 아니라 중요한 정보만 저장하고 있으면 됨</li>
<li>후에 이를 바탕으로 발전된 기술이 Transformer (Attention is all you need.)</li>
</ul>
</section>
<section id="결론" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="결론"><span class="header-section-number">3.5</span> 결론</h2>
<p>본 문서에서는 Attention 메커니즘의 기본 원리와 작동 방식을 살펴보았다. Attention은 시퀀스 데이터 처리에서 중요한 정보에 집중할 수 있게 해주는 핵심적인 기술로, 기계 번역을 비롯한 다양한 자연어 처리 태스크에서 혁신적인 성능 향상을 가져왔다.</p>
<ul>
<li><strong>Attention의 핵심 원리 요약</strong>:
<ul>
<li>Attention은 Query, Key, Value 세 가지 요소를 기반으로 작동하며, Query와 Key의 유사도를 계산하여 Value에 대한 가중치를 결정한다.</li>
<li>Softmax를 통해 정규화된 가중치를 사용하여 중요한 정보에 더 집중할 수 있게 해주며, 이를 통해 문맥에 따른 적절한 정보 선택이 가능해진다.</li>
</ul></li>
<li><strong>RNN 기반 모델과의 관계 및 장점</strong>:
<ul>
<li>기존 RNN 기반 Seq2Seq 모델의 한계를 극복하여, 긴 시퀀스에서도 중요한 정보를 효과적으로 포착할 수 있게 되었다.</li>
<li>모든 입력 정보에 직접 접근할 수 있어 장기 의존성 문제를 해결하고, 더 정확한 번역과 생성이 가능해졌다.</li>
</ul></li>
<li><strong>Transformer로의 발전</strong>:
<ul>
<li>Attention 메커니즘의 성공은 RNN을 완전히 대체하는 Transformer 아키텍처의 등장으로 이어졌다.</li>
<li>“Attention is all you need”라는 명제가 증명되었듯이, Attention만으로도 뛰어난 성능을 보일 수 있음을 보여주었다.</li>
</ul></li>
</ul>
<p>결론적으로, Attention 메커니즘은 자연어 처리 분야에서 혁신적인 변화를 가져온 핵심 기술이다. RNN의 한계를 극복하고 Transformer의 등장을 이끌어냄으로써, 현대 자연어 처리의 기반을 마련했으며, 이는 BERT, GPT와 같은 혁신적인 모델들의 등장으로 이어졌다.</p>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-5.attention.html</guid>
  <pubDate>Fri, 17 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 벡터화: 신경망 기반 방법론</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-6-1.transformer_BERT.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 단어의 의미가 문맥에 따라 변하는 현상을 효과적으로 다루기 위해 등장한 <strong>동적/문맥적 임베딩(Contextualized Embedding)</strong> 방법론을 탐구한다. 정적 임베딩의 한계를 지적하고, 이를 극복하기 위한 주요 모델들의 핵심 아이디어와 특징을 소개한다.</p>
<p>주요 내용은 다음과 같다.</p>
<ul>
<li><strong>정적 임베딩 vs.&nbsp;동적 임베딩</strong>:
<ul>
<li>정적 임베딩(예: Word2Vec, GloVe)은 단어마다 고정된 벡터를 할당하여 문맥에 따른 의미 변화(다의성)를 포착하지 못하는 한계가 있다.</li>
<li>동적 임베딩은 동일한 단어라도 문맥에 따라 다른 벡터 표현을 생성하여 이러한 문제를 해결한다.</li>
</ul></li>
<li><strong>주요 문맥 기반 임베딩 모델</strong>:
<ul>
<li><strong>ELMo (Embeddings from Language Models)</strong>: 양방향 LSTM(BiLSTM)의 각 계층에서 얻은 내부 상태들을 가중합하여 문맥 정보를 풍부하게 담은 임베딩을 생성한다. 문자 단위 표현부터 시작하여 다양한 수준의 정보를 결합한다.</li>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: 트랜스포머(Transformer)의 인코더 구조를 활용하여 문장 내 모든 단어의 양방향 문맥을 동시에 고려한다. ’Masked Language Model(MLM)’과 ’Next Sentence Prediction(NSP)’이라는 두 가지 혁신적인 사전 학습(pre-training) 목표를 통해 깊은 언어 이해 능력을 학습한다. 문서 전체의 표현으로는 <code>[CLS]</code> 토큰의 출력을 사용하거나 토큰 출력들의 풀링(pooling) 결과를 활용한다.</li>
<li><strong>SBERT (Sentence-BERT)</strong>: BERT의 출력을 문장 수준의 의미론적 벡터로 효율적으로 변환하기 위해 Siamese 또는 Triplet 네트워크 구조를 사용한다. 이를 통해 문장 간 유사도 계산 및 대규모 검색 작업의 효율성을 크게 향상시킨다.</li>
<li><strong>GPT (Generative Pre-trained Transformer)</strong>: 트랜스포머의 디코더 구조를 기반으로 하는 단방향(autoregressive) 언어 모델이다. 이전 단어들을 바탕으로 다음 단어를 예측하도록 학습하며, 이 과정에서 문맥을 이해하고 생성하는 능력을 키운다. 특히, 가중치 업데이트 없이 프롬프트에 몇 가지 예시(few-shot)를 제공하는 것만으로 새로운 작업을 수행하는 ‘In-context Learning’ 능력으로 주목받았다. 문서 표현으로는 첫 번째 토큰([BOS])의 출력을 활용하기도 한다.</li>
</ul></li>
<li><strong>실용적 응용 및 평가</strong>:
<ul>
<li>이러한 모델들은 문서 분류, 정보 검색, 질의응답, 기계 번역 등 다양한 NLP 태스크에서 혁신적인 성능 향상을 가져왔다.</li>
<li>모델 평가는 단어 유사도나 관계 유추 같은 내재적 평가(intrinsic evaluation)와 실제 다운스트림 태스크에서의 성능을 측정하는 외재적 평가(extrinsic evaluation)로 이루어진다.</li>
</ul></li>
</ul>
<p>이 문서를 통해 독자는 문맥을 이해하는 동적 임베딩 기술의 발전 과정과 핵심 원리를 파악하고, 다양한 NLP 문제 해결에 이를 어떻게 활용할 수 있는지에 대한 통찰을 얻을 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention
 
Transformer 이후 생성형 모델 발전 계열
├── Transformer 구조 (Vaswani et al., 2017)
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등</code></pre>
<section id="문맥을-고려한-벡터화-2018-현재-동적-임베딩" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="문맥을-고려한-벡터화-2018-현재-동적-임베딩"><span class="header-section-number">2.1</span> 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩</h2>
<section id="elmo-embedding-from-language-models-2018" class="level4" data-number="2.1.0.1">
<h4 data-number="2.1.0.1" class="anchored" data-anchor-id="elmo-embedding-from-language-models-2018"><span class="header-section-number">2.1.0.1</span> ELMo (Embedding from Language Models, 2018)</h4>
<ul>
<li>ELMo 수식: <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BELMo%7D_k%5E%7Btask%7D%20=%20%5Cgamma%5E%7Btask%7D%20%5Csum_%7Bj=0%7D%5EL%20s_j%5E%7Btask%7D%20%5Cmathbf%7Bh%7D_%7Bk,j%7D%5E%7BLM%7D">
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bh%7D_%7Bk,j%7D%5E%7BLM%7D">: 각 레이어의 hidden state</strong></li>
</ul>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예시: 3층 BiLSTM에서 "bank" 단어 (k번째 위치)</span></span>
<span id="cb2-2">h_{bank,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>} <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> character_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bank"</span>)     <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 레이어 0 (입력)</span></span>
<span id="cb2-3">h_{bank,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>} <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> first_LSTM_layer_output        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 레이어 1  </span></span>
<span id="cb2-4">h_{bank,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>} <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> second_LSTM_layer_output       <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 레이어 2</span></span>
<span id="cb2-5">h_{bank,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>} <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> third_LSTM_layer_output        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 레이어 3 (최상위)</span></span></code></pre></div>
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?s_j%5E%7Btask%7D">: 학습 가능한 가중치</strong>
<ul>
<li>각 레이어의 중요도를 태스크별로 학습</li>
<li>문법적 태스크 → 낮은 레이어 중시</li>
<li>의미적 태스크 → 높은 레이어 중시</li>
</ul></li>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Cgamma%5E%7Btask%7D">: 전체 스케일 조정</strong>
<ul>
<li>ELMo 벡터의 전체적인 크기 조정</li>
</ul></li>
</ul></li>
<li>계산 예시</li>
</ul>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "bank" 단어의 ELMo 벡터 (감정 분석 태스크)</span></span>
<span id="cb3-2">h_0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문자 레벨</span></span>
<span id="cb3-3">h_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 낮은 레벨 (문법적)  </span></span>
<span id="cb3-4">h_2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 높은 레벨 (의미적)</span></span>
<span id="cb3-5"></span>
<span id="cb3-6">s_0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문자 레벨 가중치 (낮음)</span></span>
<span id="cb3-7">s_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문법 레벨 가중치  </span></span>
<span id="cb3-8">s_2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 의미 레벨 가중치 (높음)</span></span>
<span id="cb3-9"></span>
<span id="cb3-10">ELMo_bank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> γ × (s_0×h_0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> s_1×h_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> s_2×h_2)</span>
<span id="cb3-11">          <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span> × (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>×[<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>×[<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>×[<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>])</span>
<span id="cb3-12">          <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span> × [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.55</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.65</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>]</span>
<span id="cb3-13">          <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>]</span></code></pre></div>
<ul>
<li>양방향 정보의 중요성
<ul>
<li><strong>Forward만 사용할 경우:</strong></li>
</ul>
<pre><code>"The bank was closed because of ___"
→ "bank"를 이해할 때 "The"만 참고</code></pre>
<ul>
<li><strong>Backward까지 사용할 경우:</strong></li>
</ul>
<pre><code>"The bank was closed because of ___"
→ "bank"를 이해할 때 "was closed" 정보도 참고
→ 금융 기관으로 해석 가능성 증가</code></pre></li>
</ul>
</section>
<section id="bert-bidirectional-encoder-representations-from-transformers-2018" class="level4" data-number="2.1.0.2">
<h4 data-number="2.1.0.2" class="anchored" data-anchor-id="bert-bidirectional-encoder-representations-from-transformers-2018"><span class="header-section-number">2.1.0.2</span> BERT (Bidirectional Encoder Representations from Transformers, 2018)</h4>
<ul>
<li>양방향 문맥 동시 고려
<ul>
<li>15% 단어를 마스킹하여 예측</li>
<li>문장 간 관계 학습</li>
</ul></li>
<li><strong>핵심 혁신:</strong>
<ul>
<li><strong>Transformer 기반</strong>: 양방향 문맥 동시 고려
<ul>
<li><p><strong>기존 RNN의 한계:</strong></p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># RNN은 순차적 처리 (병렬화 어려움)</span></span>
<span id="cb6-2">h_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RNN(x_1)</span>
<span id="cb6-3">h_2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RNN(x_2, h_1)      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># h_1이 완료되어야 시작 가능</span></span>
<span id="cb6-4">h_3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RNN(x_3, h_2)      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># h_2가 완료되어야 시작 가능</span></span></code></pre></div></li>
<li><p><strong>Transformer의 장점:</strong></p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모든 위치를 동시에 처리 (병렬화 가능)</span></span>
<span id="cb7-2">attention_weights <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_attention(all_words)</span>
<span id="cb7-3">all_representations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> apply_attention(all_words, attention_weights)</span></code></pre></div></li>
</ul></li>
<li><strong>Masked Language Model</strong>: 15% 단어를 마스킹하여 예측
<ul>
<li>BERT의 핵심 학습 방법
<ul>
<li><strong>기본 아이디어</strong>: 일부 단어를 숨기고 맞추게 하기</li>
</ul>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 원본 문장</span></span>
<span id="cb8-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"나는 [MASK]를 좋아한다"</span></span>
<span id="cb8-3"></span>
<span id="cb8-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모델이 학습하는 것</span></span>
<span id="cb8-5">P(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나는 [MASK]를 좋아한다"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span></span>
<span id="cb8-6">P(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"바나나"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나는 [MASK]를 좋아한다"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>  </span>
<span id="cb8-7">P(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"컴퓨터"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나는 [MASK]를 좋아한다"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span></code></pre></div>
<ul>
<li><strong>15% 마스킹 전략:</strong></li>
</ul>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">입력 문장의 <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> 단어에 대해:</span>
<span id="cb9-2"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>: [MASK] 토큰으로 교체</span>
<span id="cb9-3"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>: 랜덤한 다른 단어로 교체  </span>
<span id="cb9-4"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>: 원래 단어 그대로 유지</span></code></pre></div>
<ul>
<li><p><strong>왜 이렇게 하는가?</strong> ```python # 80% [MASK]: 메인 학습 목적 “나는 [MASK]를 좋아한다”</p>
<p># 10% 랜덤 교체: 노이즈에 강한 표현 학습 “나는 컴퓨터를 좋아한다” # 원래는 “사과”</p>
<p># 10% 원본 유지: 실제 사용 시와 동일한 조건 “나는 사과를 좋아한다” ```</p></li>
</ul></li>
</ul></li>
<li><strong>Next Sentence Prediction</strong>: 문장 간 관계 학습</li>
</ul>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 실제 연속된 문장 (Positive)</span></span>
<span id="cb10-2">문장A: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나는 아침에 일어났다"</span></span>
<span id="cb10-3">문장B: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"그리고 아침 식사를 했다"</span></span>
<span id="cb10-4">Label: IsNext <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb10-5"></span>
<span id="cb10-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 랜덤하게 조합된 문장 (Negative)  </span></span>
<span id="cb10-7">문장A: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나는 아침에 일어났다"</span></span>
<span id="cb10-8">문장B: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"축구는 재미있는 스포츠다"</span></span>
<span id="cb10-9">Label: IsNext <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span></code></pre></div></li>
<li><strong>BERT의 문서 벡터화 방법:</strong>
<ul>
<li><strong>[CLS] 토큰</strong>: 문장/문서 전체 표현</li>
</ul>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">입력: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"[CLS] 문장 내용 [SEP]"</span></span>
<span id="cb11-2">출력: [CLS]_벡터가 전체 문장의 의미를 담음</span>
<span id="cb11-3"></span>
<span id="cb11-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예시</span></span>
<span id="cb11-5">input_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"[CLS]"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나는"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과를"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"좋아한다"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"[SEP]"</span>]</span>
<span id="cb11-6">bert_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bert_model(input_tokens)</span>
<span id="cb11-7">sentence_vector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bert_output[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [CLS] 위치의 벡터</span></span></code></pre></div>
<ul>
<li><strong>Pooling 전략</strong>:
<ul>
<li>Mean pooling: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi=1%7D%5En%20%5Cmathbf%7Bh%7D_i"></li>
<li>Max pooling: <img src="https://latex.codecogs.com/png.latex?%5Cmax(%5Cmathbf%7Bh%7D_1,%20...,%20%5Cmathbf%7Bh%7D_n)"></li>
</ul>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모든 토큰의 BERT 출력</span></span>
<span id="cb12-2">token_representations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb12-3">   [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [CLS]</span></span>
<span id="cb12-4">   [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "나는"  </span></span>
<span id="cb12-5">   [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "사과를"</span></span>
<span id="cb12-6">   [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "좋아한다"</span></span>
<span id="cb12-7">   [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [SEP]</span></span>
<span id="cb12-8">]</span>
<span id="cb12-9"></span>
<span id="cb12-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Mean Pooling</span></span>
<span id="cb12-11">mean_vector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean(token_representations[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [CLS], [SEP] 제외</span></span>
<span id="cb12-12"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb12-13"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.43</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.53</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.63</span>]</span>
<span id="cb12-14"></span>
<span id="cb12-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Max Pooling  </span></span>
<span id="cb12-16">max_vector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(token_representations[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 각 차원별 최댓값</span></span>
<span id="cb12-17"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>]</span></code></pre></div></li>
</ul></li>
<li><strong>특징</strong>:
<ul>
<li>단어의 의미적, 문법적 정보를 벡터 공간에 학습.</li>
<li>벡터 간 연산을 통해 단어 간 유사도, 유추 등 관계 표현 가능 (예: “king” - “man” + “woman” ≈ “queen”).</li>
</ul></li>
<li><strong>중요성</strong>: 현대 NLP 딥러닝 모델의 핵심 구성 요소로, 성능 향상에 크게 기여.</li>
</ul>
</section>
<section id="sbert-sentence-bert" class="level4" data-number="2.1.0.3">
<h4 data-number="2.1.0.3" class="anchored" data-anchor-id="sbert-sentence-bert"><span class="header-section-number">2.1.0.3</span> SBERT (Sentence-BERT)</h4>
<ul>
<li><p>최근 가장 보편적인 문장 또는 문서 임베딩 방법으로 SBERT가 이용된다.</p></li>
<li><p>문서의 유사도를 구할 때는 SBERT 사용을 권장</p></li>
<li><p>문장 벡터화 전략</p>
<ul>
<li>문장 간 유사도 계산</li>
<li>문장 간 유사도 계산 시 문장 임베딩 사용</li>
</ul></li>
<li><p>기존 BERT의 한계: 문장 유사도 계산의 비효율성</p>
<div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1000개 문장의 유사도를 모두 구하려면</span></span>
<span id="cb13-2">sentences <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"문장1"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"문장2"</span>, ..., <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"문장1000"</span>]</span>
<span id="cb13-3"></span>
<span id="cb13-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 기존 BERT 방식 (비효율적)</span></span>
<span id="cb13-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>):</span>
<span id="cb13-6">   <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>):</span>
<span id="cb13-7">      combined <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"[CLS] </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sentences[i]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> [SEP] </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sentences[j]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> [SEP]"</span></span>
<span id="cb13-8">      similarity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bert_classifier(combined)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 매번 BERT 실행</span></span>
<span id="cb13-9"></span>
<span id="cb13-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 총 계산 횟수: 1000 × 999 / 2 = 499,500번!</span></span></code></pre></div>
<ul>
<li>BERT로 문장 유사도를 계산하려면:
<ul>
<li>두 문장을 [SEP]로 연결</li>
<li>BERT에 입력하여 분류</li>
<li><img src="https://latex.codecogs.com/png.latex?O(n%5E2)"> 시간 복잡도 (n개 문장 비교 시)</li>
</ul></li>
</ul></li>
<li><p>SBERT의 해결책: Siamese Network 구조</p></li>
</ul>
<div class="sourceCode" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># SBERT 방식 (효율적)</span></span>
<span id="cb14-2"></span>
<span id="cb14-3">문장 A → BERT → Pooling → Vector A</span>
<span id="cb14-4">문장 B → BERT → Pooling → Vector B</span>
<span id="cb14-5">유사도 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cosine_similarity(Vector A, Vector B)</span>
<span id="cb14-6"></span>
<span id="cb14-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1단계: 모든 문장을 미리 벡터화</span></span>
<span id="cb14-8">sentence_vectors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb14-9"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> sentences:</span>
<span id="cb14-10">    vector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sbert_model(sentence)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 각 문장마다 1번씩만 실행</span></span>
<span id="cb14-11">    sentence_vectors.append(vector)</span>
<span id="cb14-12"></span>
<span id="cb14-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2단계: 벡터 간 코사인 유사도로 빠른 계산</span></span>
<span id="cb14-14"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>):</span>
<span id="cb14-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>):</span>
<span id="cb14-16">        similarity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cosine_similarity(sentence_vectors[i], sentence_vectors[j])</span>
<span id="cb14-17">        </span>
<span id="cb14-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 총 SBERT 실행 횟수: 1000번 (대폭 감소!)</span></span></code></pre></div>
<ul>
<li><strong>학습 목적 함수:</strong>
<ul>
<li><strong>Classification</strong>: <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%20=%20-%5Csum_%7Bi%7D%20y_i%20%5Clog(%5Ctext%7Bsoftmax%7D(W%5B%5Cmathbf%7Bu%7D;%20%5Cmathbf%7Bv%7D;%20%7C%5Cmathbf%7Bu%7D-%5Cmathbf%7Bv%7D%7C%5D))"></li>
</ul>
<div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 두 문장의 SBERT 벡터</span></span>
<span id="cb15-2">u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sbert(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나는 사과를 좋아한다"</span>)      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [0.2, 0.4, 0.1, ...]</span></span>
<span id="cb15-3">v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sbert(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나는 바나나를 좋아한다"</span>)    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [0.3, 0.5, 0.2, ...]</span></span>
<span id="cb15-4"></span>
<span id="cb15-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 특성 벡터 구성</span></span>
<span id="cb15-6">concat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [u<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> v]                    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 연결: [0.2, 0.4, 0.1, 0.3, 0.5, 0.2, ...]</span></span>
<span id="cb15-7">abs_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> v<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 절댓값 차이: [0.1, 0.1, 0.1, ...]</span></span>
<span id="cb15-8">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [u<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> v<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> abs_diff]        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 최종 특성 벡터</span></span>
<span id="cb15-9"></span>
<span id="cb15-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 분류 (유사/비유사)</span></span>
<span id="cb15-11">logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b</span>
<span id="cb15-12">probability <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> softmax(logits)</span>
<span id="cb15-13">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_entropy(probability, true_label)</span></code></pre></div>
<ul>
<li><strong>Regression</strong>: <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%20=%20%5Ctext%7BMSE%7D(%5Ctext%7Bcosine%5C_sim%7D(%5Cmathbf%7Bu%7D,%20%5Cmathbf%7Bv%7D),%20%5Ctext%7Blabel%7D)"></li>
</ul>
<div class="sourceCode" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예측 유사도</span></span>
<span id="cb16-2">predicted_sim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cosine_similarity(u, v) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.85</span></span>
<span id="cb16-3"></span>
<span id="cb16-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 실제 라벨 (0~1 점수)</span></span>
<span id="cb16-5">true_sim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 사람이 평가한 유사도</span></span>
<span id="cb16-6"></span>
<span id="cb16-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 손실 계산</span></span>
<span id="cb16-8">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (predicted_sim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> true_sim)² <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.85</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>)² <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0025</span></span></code></pre></div></li>
<li><strong>성능 개선:</strong>
<ul>
<li>시간 복잡도: <img src="https://latex.codecogs.com/png.latex?O(n%5E2)%20%5Crightarrow%20O(n)"></li>
</ul>
<div class="sourceCode" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 시간 복잡도 비교</span></span>
<span id="cb17-2">기존_BERT_시간 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> O(n²) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">²</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">000</span></span>
<span id="cb17-3">SBERT_시간 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> O(n) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span></span>
<span id="cb17-4"></span>
<span id="cb17-5">속도_향상 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">000</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">배</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span></span></code></pre></div>
<ul>
<li>의미적 유사도 정확도 대폭 향상</li>
<li>대규모 문서 검색 시스템</li>
<li>실시간 문장 유사도 계산</li>
<li>추천 시스템에서의 텍스트 매칭</li>
</ul></li>
</ul>
</section>
<section id="gptgenerative-pre-trained-transformer" class="level4" data-number="2.1.0.4">
<h4 data-number="2.1.0.4" class="anchored" data-anchor-id="gptgenerative-pre-trained-transformer"><span class="header-section-number">2.1.0.4</span> GPT(Generative Pre-trained Transformer)</h4>
<ul>
<li><p>단방향 언어 모델의 핵심 개념</p></li>
<li><p>BERT vs GPT의 근본적 차이</p></li>
<li><p><strong>BERT (양방향)</strong>:</p>
<pre><code>입력: "나는 [MASK]를 좋아한다"
모델이 보는 정보: "나는" + "를 좋아한다" (양쪽 모두)
예측: [MASK] = "사과"</code></pre></li>
<li><p><strong>GPT (단방향)</strong>:</p>
<pre><code>입력: "나는 사과를"
모델이 보는 정보: "나는 사과를" (왼쪽만)
예측: 다음 단어 = "좋아한다"</code></pre></li>
<li><p>왜 단방향일까?</p>
<ul>
<li><p><strong>생성 태스크의 특성</strong>:</p>
<div class="sourceCode" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 실제 텍스트 생성 시</span></span>
<span id="cb20-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"안녕하세요, 오늘 날씨가"</span></span>
<span id="cb20-3">→ 모델: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"좋네요"</span> (미래 정보는 알 수 없음)</span>
<span id="cb20-4"></span>
<span id="cb20-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 만약 양방향이라면?</span></span>
<span id="cb20-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"안녕하세요, 오늘 날씨가 [미래정보] 입니다"</span></span>
<span id="cb20-7">→ 실제 생성 시에는 미래 정보가 없으므로 불일치</span></code></pre></div></li>
</ul></li>
<li><p><strong>GPT의 학습 방식: Autoregressive Language Modeling</strong></p>
<ul>
<li><p>이전 토큰들로 다음 토큰 예측</p></li>
<li><p>수학적 목적 함수: <img src="https://latex.codecogs.com/png.latex?P(%5Ctext%7B%EB%AC%B8%EC%9E%A5%7D)%20=%20%5Cprod_%7Bt=1%7D%5ET%20P(w_t%20%7C%20w_1,%20w_2,%20...,%20w_%7Bt-1%7D)"></p>
<ul>
<li>문장의 확률 = 각 단어가 이전 단어들 조건 하에 나타날 확률의 곱</li>
</ul></li>
<li><p>구체적 학습 예시</p>
<ul>
<li><strong>훈련 문장</strong>: “나는 사과를 좋아한다”</li>
</ul>
<div class="sourceCode" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 학습 데이터 구성</span></span>
<span id="cb21-2">입력 → 정답</span>
<span id="cb21-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"나는"</span> → <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과를"</span></span>
<span id="cb21-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"나는 사과를"</span> → <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"좋아한다"</span>  </span>
<span id="cb21-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"나는 사과를 좋아한다"</span> → <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;끝&gt;"</span></span>
<span id="cb21-6"></span>
<span id="cb21-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 손실 함수</span></span>
<span id="cb21-8">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>log P(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과를"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나는"</span>) </span>
<span id="cb21-9">      <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>log P(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"좋아한다"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나는 사과를"</span>)</span>
<span id="cb21-10">      <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>log P(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;끝&gt;"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나는 사과를 좋아한다"</span>)</span></code></pre></div></li>
<li><p>Causal Masking (인과 마스킹)</p>
<ul>
<li>Attention에서 미래 정보 차단</li>
</ul>
<div class="sourceCode" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Attention Matrix (4개 단어 예시)</span></span>
<span id="cb22-2">        나는  사과를  좋아한다  <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>끝<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span></span>
<span id="cb22-3">나는     ✓     ✗      ✗      ✗</span>
<span id="cb22-4">사과를    ✓     ✓      ✗      ✗  </span>
<span id="cb22-5">좋아한다  ✓     ✓      ✓      ✗</span>
<span id="cb22-6"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>끝<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span>     ✓     ✓      ✓      ✓</span>
<span id="cb22-7"></span>
<span id="cb22-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ✓: 참고 가능, ✗: 마스킹 (참고 불가)</span></span></code></pre></div></li>
<li><p><strong>코드 구현</strong>:</p>
<div class="sourceCode" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 마스킹 행렬</span></span>
<span id="cb23-2">mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tril(torch.ones(seq_len, seq_len))</span>
<span id="cb23-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 상삼각 부분을 -무한대로 설정</span></span>
<span id="cb23-4">attention_scores.masked_fill_(mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e9</span>)</span>
<span id="cb23-5">attention_weights <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> softmax(attention_scores)</span></code></pre></div></li>
</ul></li>
<li><p><strong>첫 번째 토큰을 문서 표현으로 활용</strong></p>
<ul>
<li>정보 흐름의 특성
<ul>
<li>입력: “[BOS] 문장 내용들…”</li>
<li>각 토큰이 보는 정보량</li>
<li>토큰1 ([BOS]): 자기 자신만</li>
<li>토큰2: [BOS] + 토큰2<br>
</li>
<li>토큰3: [BOS] + 토큰2 + 토큰3</li>
</ul></li>
</ul></li>
<li><p><strong>왜 첫 번째 토큰인가?</strong></p>
<ul>
<li>정보 흐름의 특성
<ul>
<li>입력: “[BOS] 문장 내용들…”</li>
<li>각 토큰이 보는 정보량</li>
<li>토큰1 ([BOS]): 자기 자신만</li>
<li>토큰2: [BOS] + 토큰2<br>
</li>
<li>토큰3: [BOS] + 토큰2 + 토큰3</li>
<li>마지막토큰: [BOS] + 전체 문장</li>
<li>역설적으로, [BOS]는 전체 문장을 “예측”해야 하므로</li>
<li>전체 문장 정보를 압축한 표현을 학습하게 됨</li>
</ul></li>
<li>구체적 메커니즘
<ul>
<li>학습 과정에서의 압축</li>
</ul>
<div class="sourceCode" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># GPT가 학습하는 것</span></span>
<span id="cb24-2">P(전체_문장 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> [BOS]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> P(w1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>[BOS]) × P(w2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>[BOS],w1) × ... × P(wn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>[BOS],w1,...,wn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb24-3"></span>
<span id="cb24-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [BOS] 토큰은 "이 문장이 어떤 내용일까?"를 예측해야 함</span></span>
<span id="cb24-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># → 문장의 주제, 감정, 스타일 등을 함축하는 표현을 학습</span></span></code></pre></div></li>
</ul></li>
<li><p><strong>실제 활용 예시</strong>:</p>
<div class="sourceCode" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문서 분류</span></span>
<span id="cb25-2">document <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"[BOS] 이 영화는 정말 재미있었다. 스토리도 좋고..."</span></span>
<span id="cb25-3">gpt_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gpt_model(document)</span>
<span id="cb25-4">document_vector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gpt_output[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [BOS] 위치의 벡터</span></span>
<span id="cb25-5">classification <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> classifier(document_vector)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 긍정/부정 분류</span></span></code></pre></div></li>
<li><p><strong>In-context Learning 심화 분석</strong></p>
<ul>
<li>기존 학습 방식과의 차이
<ul>
<li><p><strong>전통적 학습 (Fine-tuning)</strong>:</p>
<div class="sourceCode" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1단계: 새로운 태스크 데이터로 모델 가중치 업데이트</span></span>
<span id="cb26-2">model.train()</span>
<span id="cb26-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> batch <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> task_data:</span>
<span id="cb26-4">   loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_loss(model(batch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>), batch.target)</span>
<span id="cb26-5">   loss.backward()</span>
<span id="cb26-6">   optimizer.step()</span>
<span id="cb26-7"></span>
<span id="cb26-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2단계: 추론</span></span>
<span id="cb26-9">prediction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(new_input)</span></code></pre></div></li>
<li><p><strong>In-context Learning</strong>:</p>
<div class="sourceCode" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 가중치 업데이트 없이, 입력에 예시를 포함</span></span>
<span id="cb27-2">context <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb27-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">번역 예시:</span></span>
<span id="cb27-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">영어: Hello → 한국어: 안녕하세요</span></span>
<span id="cb27-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">영어: Thank you → 한국어: 감사합니다  </span></span>
<span id="cb27-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">영어: Good morning → 한국어: 좋은 아침</span></span>
<span id="cb27-7"></span>
<span id="cb27-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">영어: How are you? → 한국어:</span></span>
<span id="cb27-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb27-10"></span>
<span id="cb27-11">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gpt_model(context)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "어떻게 지내세요?" 출력</span></span></code></pre></div></li>
</ul></li>
</ul></li>
<li><p><strong>왜 In-context Learning이 가능한가?</strong></p>
<ul>
<li><p>패턴 인식 능력</p>
<ul>
<li>GPT가 학습 중 본 패턴들</li>
<li>“A는 B이다. C는 D이다. E는” → F 예측</li>
<li>“1+1=2, 2+2=4, 3+3=” → 6 예측</li>
<li>“cat→고양이, dog→개, bird→” → 새 예측</li>
</ul></li>
<li><p>메타 학습 (Learning to Learn)</p>
<div class="sourceCode" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 다양한 패턴을 학습하면서 "학습하는 방법"을 학습</span></span>
<span id="cb28-2">패턴<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: 번역 (A→B 형태)</span>
<span id="cb28-3">패턴<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: 수학 (계산 규칙)  </span>
<span id="cb28-4">패턴<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>: 분류 (라벨링 규칙)</span>
<span id="cb28-5"></span>
<span id="cb28-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 새로운 패턴이 주어져도 빠르게 적응</span></span></code></pre></div></li>
</ul></li>
<li><p><strong>실제 In-context Learning 예시</strong></p>
<ul>
<li><p><strong>감정 분석 태스크</strong>:</p>
<div class="sourceCode" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb29-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">다음은 리뷰와 감정을 분류한 예시입니다:</span></span>
<span id="cb29-3"></span>
<span id="cb29-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">리뷰: "이 영화 정말 재미있어요!" 감정: 긍정</span></span>
<span id="cb29-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">리뷰: "시간 낭비였습니다." 감정: 부정</span></span>
<span id="cb29-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">리뷰: "그냥 그래요." 감정: 중립</span></span>
<span id="cb29-7"></span>
<span id="cb29-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">리뷰: "배우들 연기가 훌륭했습니다!" 감정:</span></span>
<span id="cb29-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb29-10"></span>
<span id="cb29-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># GPT 출력: "긍정"</span></span></code></pre></div></li>
<li><p><strong>번역 태스크</strong>:</p>
<div class="sourceCode" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb30-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">English to Korean translation:</span></span>
<span id="cb30-3"></span>
<span id="cb30-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">English: I love programming</span></span>
<span id="cb30-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Korean: 나는 프로그래밍을 좋아합니다</span></span>
<span id="cb30-6"></span>
<span id="cb30-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">English: The weather is nice today  </span></span>
<span id="cb30-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Korean: 오늘 날씨가 좋네요</span></span>
<span id="cb30-9"></span>
<span id="cb30-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">English: What time is it now?</span></span>
<span id="cb30-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Korean:</span></span>
<span id="cb30-12"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb30-13"></span>
<span id="cb30-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># GPT 출력: "지금 몇 시인가요?"</span></span></code></pre></div></li>
</ul></li>
<li><p><strong>GPT 발전사와 특징</strong></p>
<ul>
<li><strong>GPT-1 (2018)</strong>
<ul>
<li>크기: 117M 파라미터</li>
<li>특징: Transformer 디코더만 사용</li>
<li>성능: 간단한 텍스트 생성</li>
</ul></li>
<li><strong>GPT-2 (2019)</strong>
<ul>
<li>크기: 1.5B 파라미터</li>
<li>특징: 스케일 확장의 효과 입증</li>
<li>성능: 일관성 있는 긴 텍스트 생성</li>
</ul></li>
<li><strong>GPT-3 (2020)</strong>
<ul>
<li>크기: 175B 파라미터<br>
</li>
<li>특징: In-context Learning의 강력한 능력</li>
<li>성능: Few-shot Learning으로 다양한 태스크 수행</li>
</ul></li>
<li><strong>GPT-4 (2023)</strong>
<ul>
<li>크기: 공개되지 않음 (추정 수조 개)</li>
<li>특징: 멀티모달 (텍스트 + 이미지)</li>
<li>성능: 인간 수준에 근접한 성능</li>
</ul></li>
</ul></li>
<li><p><strong>GPT vs BERT 비교 정리</strong></p></li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>측면</th>
<th>GPT</th>
<th>BERT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>방향성</strong></td>
<td>단방향 (왼쪽→오른쪽)</td>
<td>양방향</td>
</tr>
<tr class="even">
<td><strong>학습 목표</strong></td>
<td>다음 토큰 예측</td>
<td>마스킹된 토큰 예측</td>
</tr>
<tr class="odd">
<td><strong>주요 용도</strong></td>
<td>생성 태스크</td>
<td>이해 태스크</td>
</tr>
<tr class="even">
<td><strong>문서 벡터</strong></td>
<td>첫 번째 토큰</td>
<td>[CLS] 토큰</td>
</tr>
<tr class="odd">
<td><strong>특별 능력</strong></td>
<td>In-context Learning</td>
<td>Fine-tuning 효율성</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>결론</strong>: GPT는 “다음에 올 단어를 예측”하는 단순한 목표로 학습하지만, 이 과정에서 언어의 패턴, 의미, 추론 능력까지 학습하게 되어 강력한 생성 및 추론 모델이 되었다.</li>
</ul>
</section>
<section id="실용적-응용-및-평가" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="실용적-응용-및-평가"><span class="header-section-number">2.1.1</span> 실용적 응용 및 평가</h3>
<section id="평가-지표" class="level4" data-number="2.1.1.1">
<h4 data-number="2.1.1.1" class="anchored" data-anchor-id="평가-지표"><span class="header-section-number">2.1.1.1</span> 평가 지표</h4>
<p><strong>Intrinsic Evaluation (내재적 평가):</strong> - <strong>단어 유사도</strong>: WordSim-353, SimLex-999 - 사람이 평가한 단어 유사도와 모델 예측의 상관관계 측정 - <strong>단어 관계</strong>: “king - man + woman = queen” - 벡터 연산으로 의미 관계 포착 정도 평가</p>
<p><strong>Extrinsic Evaluation (외재적 평가):</strong> - <strong>문서 분류 정확도</strong>: 실제 분류 태스크에서의 성능 - <strong>정보 검색 성능</strong>: NDCG, MAP - 검색 결과의 관련성 및 순위 정확도 - <strong>의미적 텍스트 유사도</strong>: STS benchmark - 문장 간 의미적 유사성 예측 성능</p>
</section>
<section id="모델-선택-가이드" class="level4" data-number="2.1.1.2">
<h4 data-number="2.1.1.2" class="anchored" data-anchor-id="모델-선택-가이드"><span class="header-section-number">2.1.1.2</span> 모델 선택 가이드</h4>
<ul>
<li><strong>소규모 데이터</strong>: FastText (OOV 처리)</li>
<li><strong>대규모 문서 분류</strong>: BERT fine-tuning</li>
<li><strong>실시간 유사도 계산</strong>: SBERT</li>
<li><strong>창작/생성 태스크</strong>: GPT 계열</li>
</ul>
</section>
<section id="통계적-해석" class="level4" data-number="2.1.1.3">
<h4 data-number="2.1.1.3" class="anchored" data-anchor-id="통계적-해석"><span class="header-section-number">2.1.1.3</span> 통계적 해석</h4>
<p>임베딩 공간에서의 기하학적 관계: <img src="https://latex.codecogs.com/png.latex?%5Ccos(%5Cmathbf%7Bv%7D_%7B%5Ctext%7Bsimilar%20words%7D%7D)%20%3E%20%5Ccos(%5Cmathbf%7Bv%7D_%7B%5Ctext%7Bdissimilar%20words%7D%7D)"></p>
<p><strong>시각화 도구</strong>: t-SNE/UMAP을 통한 의미적 클러스터링 확인</p>
</section>
</section>
</section>
<section id="결론" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="결론"><span class="header-section-number">2.2</span> 결론</h2>
<p>본 문서에서는 단어의 고정된 의미 표현을 넘어, 문맥에 따라 유연하게 변화하는 의미를 포착하는 동적 임베딩 방법론들을 심층적으로 살펴보았다. ELMo에서 시작하여 BERT, GPT, SBERT에 이르기까지, 이러한 문맥 기반 임베딩 모델들은 자연어 처리(NLP) 분야에 혁명적인 발전을 가져왔다.</p>
<p>주요 내용을 다시 한번 정리하면 다음과 같다.</p>
<ul>
<li><p><strong>정적 임베딩의 한계 극복</strong>: 초기의 워드 임베딩(Word2Vec, GloVe 등)은 단어의 의미를 단일 벡터로 표현하여 문맥에 따른 다의성을 반영하지 못했다. 동적 임베딩은 이 한계를 극복하고, 동일한 단어라도 문맥에 따라 다른 벡터 표현을 생성함으로써 보다 정교한 의미 이해를 가능하게 했다.</p></li>
<li><p><strong>주요 모델들의 혁신과 기여</strong>:</p>
<ul>
<li><strong>ELMo</strong>: 양방향 LSTM을 통해 문맥 정보를 통합하고, 여러 계층의 표현을 활용하여 풍부한 임베딩을 제공했다.</li>
<li><strong>BERT</strong>: 트랜스포머 아키텍처와 Masked Language Model, Next Sentence Prediction과 같은 혁신적인 사전 학습 방식을 도입하여 양방향 문맥 이해의 새로운 지평을 열었다. 이는 다양한 NLP 다운스트림 태스크에서 SOTA(State-of-the-Art) 성능을 달성하는 데 크게 기여했다.</li>
<li><strong>GPT</strong>: 단방향 트랜스포머 디코더를 기반으로 강력한 텍스트 생성 능력을 보여주었으며, 특히 GPT-3 이후 모델들은 In-context Learning이라는 새로운 패러다임을 제시하며 모델 활용의 유연성을 크게 확장했다.</li>
<li><strong>SBERT</strong>: 기존 BERT 모델을 문장 임베딩 생성에 효율적으로 사용할 수 있도록 Siamese 및 Triplet 네트워크 구조를 활용하여, 의미적으로 유사한 문장 벡터를 효과적으로 생성하고 문장 간 유사도 비교 작업의 속도와 정확도를 크게 향상시켰다.</li>
</ul></li>
<li><p><strong>패러다임의 전환과 LLM의 토대</strong>: 이러한 문맥 기반 임베딩 모델들의 발전은 단순한 특징 추출기를 넘어, 언어 자체를 깊이 이해하고 생성할 수 있는 대규모 언어 모델(Large Language Models, LLMs) 시대로 나아가는 핵심적인 발판이 되었다. 사전 학습과 미세 조정(fine-tuning) 패러다임, 그리고 최근의 프롬프트 기반 학습은 모델의 활용 범위를 크게 넓혔다.</p></li>
<li><p><strong>적절한 전략 선택의 지속적 중요성</strong>: 해결하고자 하는 특정 문제의 요구사항, 가용 데이터의 특성, 계산 자원 등을 고려하여 가장 적합한 임베딩 전략과 모델을 선택하는 것은 여전히 중요하다. 실용적인 응용을 위해서는 모델의 성능뿐만 아니라 효율성, 해석 가능성 등도 함께 고려해야 한다.</p></li>
</ul>
<p>문맥을 이해하는 텍스트 벡터화 기술은 앞으로도 계속 발전하여, 기계가 인간의 언어를 더욱 정교하게 이해하고 상호작용하는 미래를 앞당길 것이다. 이러한 기술의 발전은 정보 검색, 질의응답, 창작, 교육 등 사회 여러 분야에 걸쳐 혁신적인 변화를 주도할 잠재력을 지니고 있다.</p>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-6-1.transformer_BERT.html</guid>
  <pubDate>Wed, 15 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>사전 학습 모델의 발전</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/5-0.ptm_overview.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 자연어 처리 분야에서 패러다임 전환을 이끈 <strong>사전 학습 모델(Pre-trained Model)</strong>들의 발전 과정과 핵심 원리를 탐구한다. 2015년 Google의 LSTM 사전 학습 실험부터 2023년 LLaMA까지, 각 모델이 가져온 혁신적 변화와 기술적 특징을 상세히 설명한다.</p>
<p>주요 내용은 다음과 같다:</p>
<ul>
<li><strong>사전 학습의 개념과 발전</strong>:
<ul>
<li>초기에는 Word2Vec, GloVe 같은 정적 임베딩에서 시작</li>
<li>Google의 LSTM 사전 학습 실험(2015)이 사전 학습의 효과를 입증</li>
<li>대규모 데이터로 미리 학습한 모델이 무작위 초기화보다 우수한 성능 확인</li>
</ul></li>
<li><strong>문맥 기반 임베딩의 등장</strong>:
<ul>
<li><strong>ELMo (2017)</strong>: BiLSTM 기반 양방향 문맥 임베딩의 선구자</li>
<li>동일한 단어라도 문맥에 따라 다른 벡터 표현 생성</li>
<li>“bank”가 금융기관과 강가에서 서로 다른 의미로 표현되는 혁신</li>
</ul></li>
<li><strong>Transformer 아키텍처의 혁명</strong>:
<ul>
<li><strong>Transformer (2017)</strong>: Self-Attention과 Position Encoding으로 순차 처리 방식 탈피</li>
<li>병렬 처리 가능, 장거리 의존성 포착 능력 향상</li>
<li>현대 모든 대규모 언어 모델의 기초 구조 제공</li>
</ul></li>
<li><strong>특화 모델들의 분화</strong>:
<ul>
<li><strong>GPT (2018)</strong>: Transformer 디코더 기반 생성 특화 모델</li>
<li><strong>BERT (2018)</strong>: Transformer 인코더 기반 이해 특화 모델</li>
<li><strong>BART (2019)</strong>: 인코더-디코더 결합으로 이해와 생성 모두 강화</li>
<li><strong>T5 (2020)</strong>: 모든 NLP 태스크를 텍스트-투-텍스트로 통합</li>
</ul></li>
<li><strong>최신 발전 동향</strong>:
<ul>
<li><strong>LLaMA (2023)</strong>: 효율적인 대규모 언어 모델의 새로운 표준</li>
<li><strong>UL2 (2023)</strong>: 다양한 사전 학습 방식의 융합</li>
<li><strong>FLAN (2022)</strong>: Instruction Tuning을 통한 명령어 이해 능력 강화</li>
</ul></li>
</ul>
<p>각 모델의 핵심 아이디어, 학습 방식, 활용 분야를 통해 현대 NLP 기술의 발전 궤적과 미래 방향성을 이해할 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>RNN Language Model
├── Seq2Seq
├── Beam Search
├── Subword Tokenization
├── Attention
├── Transformer Encoder (Vaswani et al., 2017)
|   ├── Positional Encoding
|   ├── Multi-Head Attention
|   └── Feed Forward Neural Network
|
├── Transformer Decoder (Vaswani et al., 2017)
|
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
|
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
|
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등</code></pre>
</section>
<section id="사전-학습-모델-pre-trained-model" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 사전 학습 모델 (Pre-trained Model)</h1>
<ul>
<li>원래는 언어모델보다는 word embedding에서 사용되던 개념이었음</li>
<li>사전 훈련된 임베딩 (Word2Vec, GloVe)은 대용량 텍스트의 단어들의 동시 등장 통계로부터 훈련시키는 방법</li>
<li>미리 학습시켜 놓은 모델을 가리고 새로운 문제를 풀었었음</li>
<li>Google의 LSTM 사전학습 실험 (Semi-Supervised Sequence Learning, 2015)
<ul>
<li>LSTM 언어 모델을 사전 학습한 후에 텍스트 분류에 적용해봄</li>
<li>LSTM을 사전 학습하지 않은 상태에서 텍스트 분류를 학습한 것과 성능 비교</li>
<li>사전 학습된 LSTM이 random 초기화된 LSTM보다 성능이 좋았음</li>
</ul></li>
</ul>
<section id="elmoembeddings-from-language-models.-2017" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="elmoembeddings-from-language-models.-2017"><span class="header-section-number">3.1</span> ELMo(Embeddings from Language Models. 2017)</h2>
<ul>
<li>사전 학습된 LSTM 언어 모델 2가지를 결합하여 좋은 임베딩 벡터값을 얻는 방법론</li>
<li>사전 학습된 언어 모델이 NLP에서 좋은 성능을 얻을 수 있다는 강한 인상을 줌</li>
<li>ELMo는 문장에서 단어의 의미를 상황에 맞게 다르게 표현해주는 단어 임베딩 기법</li>
<li>예를 들어,
<ul>
<li>“He went to the bank to deposit money.”</li>
<li>“She sat by the bank of the river.”</li>
<li>여기서 bank는 같은 철자지만 의미가 전혀 다르지만 기존 방식(Word2Vec, GloVe)은 이걸 같은 의미로 취급</li>
<li>그런데 ELMo는 문맥을 보고 각각 다른 벡터로 표현</li>
</ul></li>
<li>동작 방식
<ul>
<li>문장을 왼쪽에서 읽는 모델 + 오른쪽에서 읽는 모델 두 개를 활용하여 해당 단어가 문장 속에서 어떤 의미로 쓰였는지를 분석</li>
<li>문장을 양방향으로 읽어 단어의 문맥 정보를 파악함
<ul>
<li>앞에서부터 → 순방향 LSTM</li>
<li>뒤에서부터 → 역방향 LSTM</li>
<li>순방향에서의 bank에 대한 hidden state와 역방향에서의 bank에 대한 hidden state를 결합하여 임베딩을 생성</li>
</ul></li>
<li>모든 단어의 의미는 “문맥 기반”
<ul>
<li>“bank”가 앞뒤에 어떤 단어들과 쓰였는지 보면서 이게 ’돈 관련 은행’인지, ’강가’인지 판단함</li>
</ul></li>
<li>그리고, 임베딩을 뽑음
<ul>
<li>이렇게 판단한 의미를 벡터(숫자 집합)로 표현</li>
</ul></li>
</ul></li>
<li>ELMo는 등장하자마자 기존 NLP 모델들의 정확도를 확 뛰어넘었다</li>
<li>개체명 인식(NER), 질의응답(QA), 문장 분류 등에 광범위하게 쓰였다</li>
<li>지금은 BERT, GPT 같은 트랜스포머가 주도하고 있지만, ELMo는 문맥 기반 임베딩의 출발점이었음</li>
</ul>
</section>
<section id="transformer-2017" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="transformer-2017"><span class="header-section-number">3.2</span> Transformer (2017)</h2>
<ul>
<li>기존의 RNN, LSTM과 달리 <strong>순서를 따라 처리하지 않고</strong>, 문장 전체를 <strong>한꺼번에 보고</strong> 이해할 수 있도록 만든 모델이다.</li>
<li>Transformer는 두 가지 핵심 구조
<ul>
<li><ol type="1">
<li><strong>Self-Attention</strong></li>
</ol>
<ul>
<li>문장 안에서 <strong>각 단어가 다른 단어들과 얼마나 중요한 관계가 있는지를 계산</strong>한다.</li>
<li>예를 들어, “The cat sat on the mat”에서 “sat”와 “cat” 사이의 연결이 중요하다면, 모델은 그 둘의 관계를 더 강하게 본다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Position Encoding</strong></li>
</ol>
<ul>
<li>Transformer는 순서를 따라 읽지 않기 때문에, <strong>각 단어의 위치 정보</strong>를 따로 추가해줘야 한다.</li>
<li>위치 정보를 더해줘서 “첫 번째 단어”, “두 번째 단어” 등의 순서를 인식하게 한다.</li>
</ul></li>
</ul></li>
<li>Transformer의 구성요소
<ul>
<li><strong>인코더(Encoder)</strong>: 입력 문장을 이해하고 벡터로 변환함</li>
<li><strong>디코더(Decoder)</strong>: 그 벡터를 바탕으로 결과(예: 번역, 요약 등)를 생성함</li>
<li>예를 들어, 영어 문장을 프랑스어로 번역하는 경우,
<ul>
<li>인코더는 영어 문장을 이해하고</li>
<li>디코더는 그것을 프랑스어로 바꾸어 생성한다.</li>
</ul></li>
</ul></li>
<li>Transformer는 다음과 같은 이유로 혁신적이다:
<ul>
<li><strong>병렬처리 가능</strong>: RNN처럼 순서대로 처리하지 않기 때문에 연산 속도가 빠르다.</li>
<li><strong>문맥 파악 능력 향상</strong>: 문장의 멀리 떨어진 단어들 간의 관계도 잘 이해한다.</li>
<li><strong>기반 기술로 발전</strong>: BERT, GPT, T5, LLaMA 등 오늘날의 대부분의 대형 언어모델은 Transformer 기반이다.</li>
</ul></li>
<li>오늘날 대부분의 LLM은 이 구조를 바탕으로 하고 있다.</li>
<li>ELMo는 RNN 구조 기반이고,
<ul>
<li>Transformer는 그 한계를 뛰어넘기 위해 등장한 구조라는 점에서</li>
<li>이 둘의 <strong>패러다임 자체가 다르다</strong>는 것도 같이 기억해두면 좋겠다.</li>
</ul></li>
</ul>
</section>
<section id="gptgenerative-pre-trained-transformer.-2018" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="gptgenerative-pre-trained-transformer.-2018"><span class="header-section-number">3.3</span> GPT(Generative Pre-trained Transformer. 2018)</h2>
<ul>
<li>GPT는 문장을 생성하는 모델이다.</li>
<li>기존 모델들이 문장을 이해하는 데 집중했다면, GPT는 문장을 생성하는 데 집중한다.</li>
<li>OpenAI는 Google의 Transformer을 보고 STM이 사전 학습되어 사용되면 성능이 좋은 것을 확인.<br>
</li>
<li>Transformer의 디코더 구조를 분석하고 다음 단어를 예측하는 모듈에 해당하는 디코더로 사전 학습 언어 모델을 구현했다.</li>
<li>Transformer의 encoder를 버리고 decoder만 사용하여 문장을 생성하는 모델을 만들었다.</li>
<li>동작 방식
<ul>
<li>GPT는 <strong>Transformer의 디코더 구조만</strong> 사용한다.</li>
<li>즉, 문장을 생성하는 데 특화되어 있으며,</li>
<li>문장을 이해하는 인코더 구조는 없다.</li>
<li>핵심 동작 방식
<ul>
<li><strong>좌→우 방향의 언어 생성 학습</strong>
<ul>
<li>문장을 왼쪽에서 오른쪽으로 읽으면서,</li>
<li>다음에 올 단어를 예측하는 방식으로 학습한다.</li>
<li>예:
<ul>
<li>입력: “I want to eat”</li>
<li>목표: 다음 단어가 무엇일까? → “pizza”</li>
<li>이런 식으로 엄청나게 많은 문장 데이터를 통해 <strong>패턴을 학습</strong>한다.</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>사전학습 후 활용</strong>
<ul>
<li>대규모 텍스트로 먼저 학습(Pre-training)</li>
<li>이후 별도 작업(챗봇, 작문, 번역 등)에 맞게 사용(Fine-tuning or Prompting)</li>
</ul></li>
</ul></li>
<li>특징
<ul>
<li><strong>텍스트 생성 능력</strong>이 매우 뛰어남</li>
<li>다양한 태스크를 <strong>명시적 미세조정 없이 프롬프트만으로 해결 가능</strong>
<ul>
<li>이게 GPT 계열 모델의 큰 장점이다 (Few-shot, Zero-shot, etc.)</li>
</ul></li>
<li><strong>문장 완성, 요약, 번역, 창작, 대화 등</strong>
<ul>
<li>생성형 작업에 모두 강하다</li>
</ul></li>
</ul></li>
<li><strong>GPT는 문장을 생성하는 데 특화된 모델</strong>이다.</li>
</ul>
</section>
<section id="bertbidirectional-encoder-representations-from-transformers.-2018" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="bertbidirectional-encoder-representations-from-transformers.-2018"><span class="header-section-number">3.4</span> BERT(Bidirectional Encoder Representations from Transformers. 2018)</h2>
<ul>
<li>Google이 OpenAI의 GPT 모델을 보고 반대로 문장을 이해하는 데 집중하는 모델을 만들었다.</li>
<li>Transformer의 인코더 구조를 사용하여 문장을 이해하는 모델을 만들었다.</li>
<li>NLU가 특화된 부분이기 때문에 Text 분류에서 GPT보다 더 나은 성능을 보인다.</li>
<li>BERT는 문장을 <strong>양방향으로 이해하는</strong> 언어 모델이다.</li>
<li>기존 모델들이 <strong>왼쪽에서 오른쪽</strong> 혹은 <strong>오른쪽에서 왼쪽</strong>으로만 문장을 해석했다면,</li>
<li>BERT는 문장을 <strong>양쪽 방향에서 동시에</strong> 해석한다.</li>
<li>그래서 더 깊은 문맥 이해가 가능하다.</li>
<li>동작 방식
<ul>
<li>BERT는 <strong>Transformer의 인코더 구조만</strong> 사용한다.</li>
<li>즉, 문장을 이해하고 벡터로 바꾸는 데 특화되어 있으며,</li>
<li>문장을 새로 생성하는 디코더 구조는 없다.</li>
<li>핵심 동작 방식은 두 가지 학습 방식으로 이루어진다:
<ul>
<li><strong>Masked Language Modeling (MLM)</strong>
<ul>
<li>입력 문장 중 일부 단어를 가려놓고, 그 단어가 무엇인지 맞히도록 학습한다.</li>
<li>예: “The cat sat on the [MASK].” → 모델은 ’mat’이라고 예측해야 한다.</li>
<li>이를 통해 문장의 앞뒤 <strong>모든 문맥</strong>을 참고해서 단어를 이해하는 법을 배운다.</li>
</ul></li>
<li><strong>Next Sentence Prediction (NSP)</strong>
<ul>
<li>두 문장을 입력한 뒤, 두 번째 문장이 첫 번째 문장의 <strong>진짜 다음 문장인지</strong> 판단하게 학습한다.</li>
<li>예:
<ul>
<li>문장1: “She opened the door.”</li>
<li>문장2: “She picked up the package.” → 연결된 문장 (True)</li>
<li>문장2: “The sun is a star.” → 무관한 문장 (False)</li>
</ul></li>
<li>문장 간의 관계 이해 능력을 키우기 위한 학습이다.</li>
</ul></li>
</ul></li>
</ul></li>
<li>문맥 기반 단어 임베딩 제공
<ul>
<li>같은 단어라도 문맥에 따라 다른 벡터로 표현됨</li>
</ul></li>
<li>사전학습 + 미세조정 구조 (Pretraining + Fine-tuning)
<ul>
<li>대규모 텍스트로 미리 학습해두고,</li>
<li>이후 실제 태스크(NER, 분류, QA 등)에 맞게 추가 학습만 하면 됨.</li>
</ul></li>
<li>BERT는 다양한 NLP 태스크에서 <strong>기록적인 성능 향상</strong>을 이끌었다.</li>
<li>이후 등장한 RoBERTa, ALBERT, DistilBERT, DeBERTa 등은 BERT의 변형이다.</li>
<li>현재 GPT 계열이 생성에 특화되어 있다면,
<ul>
<li><strong>BERT는 이해(이해 기반 태스크)에 특화된 모델</strong>이라고 보면 된다.</li>
</ul></li>
<li>BERT는 <strong>Transformer 인코더 기반의 양방향 문맥 이해 모델</strong>이다.</li>
<li><strong>단어를 문맥 안에서 정확하게 해석</strong>하기 위해 만들어졌다.</li>
<li><strong>문장 분류, 질의응답, 개체명 인식 등 NLP의 다양한 작업에 폭넓게 활용</strong>된다.</li>
</ul>
</section>
<section id="bartbidirectional-and-auto-regressive-transformers.-2019" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="bartbidirectional-and-auto-regressive-transformers.-2019"><span class="header-section-number">3.5</span> BART(Bidirectional and Auto-Regressive Transformers. 2019)</h2>
<ul>
<li>Encoder-Decoder 가 결합된 구조를 사용하여 문장을 이해하고 생성하는 모델을 만들었다.</li>
<li>BART는 <strong>BERT + GPT의 장점</strong>을 결합한 모델로, <strong>이해와 생성 모두에 강한 모델</strong>이다.</li>
<li>BART는 <strong>문장을 이해하고 → 그것을 기반으로 문장을 생성하는 모델</strong>이다.</li>
<li>즉, <strong>BERT처럼 입력을 양방향으로 이해</strong>하고, <strong>GPT처럼 자연스럽게 문장을 생성</strong>한다.</li>
<li>구조적으로는 <strong>Transformer 인코더 + 디코더</strong>를 모두 사용한다.</li>
<li>쉽게 말해 <strong>BERT + GPT를 합친 하이브리드 모델</strong>이다.</li>
<li>동작 방식
<ul>
<li><strong>Denoising Autoencoder</strong>: BART의 학습은 <strong>“노이즈 추가 → 원문 복원”</strong> 방식으로 이루어진다.</li>
<li>입력 문장에 노이즈를 준다</li>
<li>예:
<ul>
<li>원래 문장: <code>The cat sat on the mat.</code></li>
<li>망가뜨린 문장: <code>The [MASK] on the mat.</code> 또는 <code>sat the mat on the cat.</code> (순서 뒤섞기)</li>
</ul></li>
<li>모델은 이 망가진 문장을 보고 <strong>원래 문장을 복원</strong>한다. 즉, 문장을 이해하고, 적절한 형태로 <strong>다시 생성</strong>할 수 있어야 한다.</li>
<li>이 과정을 통해 BART는 <strong>이해 능력</strong>과 <strong>생성 능력</strong>을 동시에 학습하게 된다.</li>
</ul></li>
<li>구성
<ul>
<li><strong>인코더</strong>는 BERT처럼 <strong>양방향 문맥 이해</strong></li>
<li><strong>디코더</strong>는 GPT처럼 <strong>왼쪽→오른쪽 순서대로 문장 생성</strong></li>
</ul></li>
<li>이 구조 덕분에 <strong>복잡한 입력을 해석하고</strong>, 그에 맞는 <strong>정확하고 자연스러운 출력</strong>을 만들어낼 수 있다.</li>
<li>활용 분야
<ul>
<li>BART는 다음과 같은 <strong>생성 기반 작업</strong>에 특히 강하다:
<ul>
<li>텍스트 요약 (Summarization)</li>
<li>문장 생성 (Text Generation)</li>
<li>기계 번역 (Machine Translation)</li>
<li>문법 오류 수정 (Grammatical Error Correction)</li>
<li>질문 생성 (Question Generation)</li>
</ul></li>
</ul></li>
<li>특히 <strong>요약 모델로서 매우 뛰어난 성능</strong>을 보여주었고, Facebook AI에서 개발한 이후 HuggingFace에서도 적극적으로 채택되었다.</li>
</ul>
</section>
<section id="t5-text-to-text-transfer-transformer.-2020" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="t5-text-to-text-transfer-transformer.-2020"><span class="header-section-number">3.6</span> T5 (Text-to-Text Transfer Transformer. 2020)</h2>
<ul>
<li>T5는 모든 NLP 문제를 텍스트 → 텍스트 문제로 바꾸자는 발상에서 출발했다.</li>
<li>즉, 입력도 텍스트, 출력도 텍스트로 통일된 프레임워크를 사용한다.</li>
<li>T5는 구글이 제안한 범용 NLP 모델이다.</li>
<li>자연어처리에서 벌어지는 거의 모든 작업을 <strong>텍스트 입력 → 텍스트 출력</strong>으로 통합하는 것이 핵심이다.</li>
<li>요약, 번역, 문장 분류, 질문 생성, 질의 응답 등 모두 동일한 구조에서 처리 가능하다.</li>
<li>동작 방식
<ul>
<li>T5는 BART처럼 <strong>Transformer 인코더 + 디코더 구조</strong>를 사용한다.</li>
<li>하지만 BART와 달리, <strong>모든 태스크를 통일된 방식으로 표현</strong>하는 철학이 핵심이다.</li>
</ul></li>
<li>예시:
<ul>
<li>문장 분류
<ul>
<li>입력: <code>"sst2 sentence: I love this movie."</code></li>
<li>출력: <code>"positive"</code></li>
</ul></li>
<li>문장 요약
<ul>
<li>입력: <code>"summarize: The cat sat on the mat. It was sleepy."</code></li>
<li>출력: <code>"The cat was sleepy."</code></li>
</ul></li>
<li>질의 응답
<ul>
<li>입력: <code>"question: Where is the Eiffel Tower? context: The Eiffel Tower is in Paris."</code></li>
<li>출력: <code>"Paris"</code></li>
</ul></li>
<li>이처럼 <strong>작업을 구분하는 태그 + 텍스트 입력</strong>을 넣으면 <strong>디코더가 원하는 정답 텍스트를 생성</strong>한다.</li>
</ul></li>
<li>사전학습 방식
<ul>
<li>T5도 BART처럼 <strong>Denoising Autoencoder</strong> 방식으로 학습된다.</li>
<li>하지만 T5는 자체적으로 만든 <strong>Span Corruption</strong>이라는 방식을 쓴다:</li>
<li>문장에서 일부 구간(span)을 가리고, 그 구간을 <code>&lt;extra_id_0&gt;</code>, <code>&lt;extra_id_1&gt;</code> 같은 토큰으로 대체</li>
<li>모델이 이 빈칸들을 복원하게 함</li>
<li>이 과정을 통해 <strong>문장 이해 + 생성 능력</strong>을 동시에 기른다.</li>
</ul></li>
<li>특징
<ul>
<li><strong>모든 NLP 태스크를 텍스트 → 텍스트 문제로 통일</strong></li>
<li>다양한 태스크를 하나의 모델로 처리 가능</li>
<li>프롬프트 기반으로 유연하게 태스크 전환</li>
<li>학습 구조는 BART와 유사하지만, <strong>설계 철학은 더 범용적</strong>임</li>
</ul></li>
<li><strong>T5는 BART와 구조는 유사하나, 태스크 프레임워크가 다르다.</strong></li>
<li><strong>모든 입력과 출력을 텍스트로 다루며, 태스크 이름을 붙여 명시함</strong></li>
<li>따라서 여러 NLP 작업을 하나의 파이프라인에서 처리할 수 있다.</li>
<li>대표적인 범용 자연어 처리 모델 중 하나로, 후속 버전으로 T5.1.1, UL2, FLAN-T5 등이 있다.</li>
</ul>
</section>
<section id="llamalarge-language-model-meta-ai.-2023" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="llamalarge-language-model-meta-ai.-2023"><span class="header-section-number">3.7</span> LLaMA(Large Language Model Meta AI. 2023)</h2>
<ul>
<li>LLaMA는 Meta(구 Facebook)에서 제안한 <strong>대규모 언어 모델 시리즈</strong>이다.</li>
<li>이름은 <strong>Large Language Model Meta AI</strong>의 약자이다.</li>
<li>LLaMA는 범용 텍스트 생성 능력을 목표로 하는 <strong>GPT 계열 디코더 기반 모델</strong>이다.</li>
<li>고성능 언어 모델을 <strong>비교적 작은 파라미터 수로도 구현 가능</strong>하다는 점을 증명하고자 설계되었다.</li>
<li>논문 및 모델은 공개되어 <strong>학계, 오픈소스 커뮤니티에서 널리 활용되고 있음</strong>.</li>
<li>LLaMA는 1~2단계 모델 학습을 통해 **사전학습된 기반 모델 (Base LM)**만 제공하며, <strong>대화, 요약, 추론 등에 맞게 파인튜닝은 사용자가 직접 수행</strong>하도록 설계되어 있다.</li>
<li>동작 방식
<ul>
<li>LLaMA는 GPT처럼 <strong>Transformer 디코더 구조</strong>만 사용한다.</li>
<li>입력을 <strong>왼쪽에서 오른쪽</strong>으로 읽으며 <strong>다음 토큰을 예측</strong>하는 방식으로 작동한다.</li>
<li>학습 시점에 <strong>자기회귀 언어모델 (Autoregressive Language Modeling)</strong> 방식 사용.</li>
</ul></li>
<li>특징
<ul>
<li>GPT처럼 텍스트 생성 중심의 모델이지만, <strong>효율성과 학습 품질 향상에 집중된 다양한 설계 전략</strong>을 포함한다.</li>
<li>예:
<ul>
<li><strong>Norm 위치 변경 (Pre-normalization)</strong></li>
<li><strong>GEGLU 활성화 함수</strong></li>
<li><strong>더 긴 시퀀스 학습 (최대 2,048 토큰)</strong></li>
<li><strong>높은 품질의 텍스트 코퍼스만 선별하여 학습</strong></li>
</ul></li>
<li>성능 대비 <strong>모델 크기 효율이 매우 우수</strong>하여, 작은 파라미터 수로도 <strong>GPT-3 수준의 성능</strong>을 낼 수 있음.</li>
</ul></li>
<li>버전별 주요 모델
<ul>
<li><strong>LLaMA 1 (2023 초)</strong>
<ul>
<li>파라미터 크기: 7B, 13B, 33B, 65B</li>
<li>공개 후 오픈소스 생태계에서 광범위한 활용이 시작됨</li>
</ul></li>
<li><strong>LLaMA 2 (2023 중반)</strong>
<ul>
<li>성능 개선 및 다양한 크기: 7B, 13B, 70B</li>
<li><strong>LLaMA 2-Chat</strong>: 대화용으로 미세조정된 모델</li>
</ul></li>
<li><strong>LLaMA 3 (2024 출시)</strong>
<ul>
<li>Meta가 직접 <strong>SOTA급 성능</strong>을 표방하며 출시</li>
<li>8B, 70B 모델 제공, 대화형 fine-tuning 포함</li>
<li>상용 사용 가능, HuggingFace 등에서 공개됨</li>
</ul></li>
</ul></li>
<li>활용 방식
<ul>
<li>LLaMA는 <strong>기반 모델만 제공</strong>하기 때문에, 대화형 모델로 쓰려면 <strong>Alpaca, Vicuna, OpenChat, Zephyr</strong> 등의 <strong>LoRA 파인튜닝</strong> 모델을 함께 사용하는 경우가 많다.</li>
<li>특히 LLaMA는 <strong>프롬프트 기반 제어</strong>, <strong>추론</strong>, <strong>코드 생성</strong> 등 다양한 태스크에서 활용 가능하며, <strong>고품질 오픈소스 AI 개발의 중심</strong>으로 자리 잡았다.</li>
</ul></li>
<li>LLaMA는 GPT 계열의 디코더 언어 모델이다.</li>
<li><strong>작은 모델 크기로도 높은 성능</strong>을 발휘할 수 있도록 최적화됨.</li>
<li>다양한 오픈소스 프로젝트에서 핵심 기반 모델로 활용됨.</li>
<li>후속 시리즈인 <strong>LLaMA 2, 3</strong>는 대화형 파인튜닝 모델과 함께 상용 및 학술용으로 널리 쓰이고 있음.</li>
</ul>
<p>좋다. 이번에는 <strong>UL2</strong>와 <strong>FLAN</strong>에 대해 각각 T5 형식에 맞춰 설명하겠다. 둘 다 <strong>구글이 T5 이후에 개발한 모델 또는 학습 기법</strong>이며, <strong>텍스트 생성과 이해를 모두 강화</strong>하기 위한 방향성을 가진다.</p>
</section>
<section id="flan-fine-tuned-language-net-2022" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="flan-fine-tuned-language-net-2022"><span class="header-section-number">3.8</span> FLAN (Fine-tuned LAnguage Net, 2022)</h2>
<ul>
<li><strong>FLAN은 “Instruction Tuning”의 대표적 구현</strong>으로, <strong>UL2와 같은 사전학습된 언어 모델에 다양한 명령어(Task Prompt)를 학습시키는 과정</strong>이다.</li>
<li>FLAN은 T5 또는 UL2-T5에 적용되어 등장했으며, FLAN-T5, FLAN-UL2 같은 이름으로 모델이 배포된다.</li>
<li>목표는 <strong>명령어(prompt)를 이해하고 정확히 수행하는 능력 강화</strong>이다.</li>
<li>동작 방식
<ul>
<li>먼저 T5 또는 UL2-T5 모델을 준비한다.</li>
<li>그런 다음 <strong>Instruction Tuning</strong>을 수행한다. → 다양한 NLP 작업(요약, 번역, 추론, QA 등)을 명시적인 지시문 형식으로 학습</li>
</ul></li>
<li>예시:
<ul>
<li>입력: <code>"Translate English to French: I am happy."</code></li>
<li>출력: <code>"Je suis heureux."</code></li>
<li>입력: <code>"Summarize: The sun is hot. It rises in the east."</code></li>
<li>출력: <code>"The sun is hot and rises in the east."</code></li>
</ul></li>
<li>특징
<ul>
<li>다양한 명령어와 태스크를 학습시켜, <strong>프롬프트 이해 능력을 대폭 향상</strong>시킴</li>
<li><strong>Zero-shot / Few-shot</strong> 능력이 크게 향상됨</li>
<li>단순한 텍스트 생성 모델이 아닌 <strong>명령어 기반의 범용 도우미로 진화</strong></li>
</ul></li>
<li>요약
<ul>
<li>FLAN은 모델이 아니라 <strong>Instruction tuning 과정 또는 결과 모델 이름</strong>이다.</li>
<li>일반 언어모델에 <strong>명령어 기반 태스크 학습을 추가한 것</strong></li>
<li>대표적인 모델: <strong>FLAN-T5</strong>, <strong>FLAN-UL2</strong></li>
<li>현재 Gemini, PaLM, ChatGPT 등의 <strong>Instruction Following 능력의 기초가 된 전략</strong></li>
</ul></li>
</ul>
</section>
<section id="ul2unifying-language-learning.-2023" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="ul2unifying-language-learning.-2023"><span class="header-section-number">3.9</span> UL2(Unifying Language Learning. 2023)</h2>
<ul>
<li><strong>UL2는 “Unifying Language Learning”의 약자</strong>로, 구글이 제안한 새로운 <strong>사전학습 방식</strong>이다.</li>
<li>목적은 기존 T5, BERT, GPT 계열 모델들의 <strong>단점을 보완하고 장점을 융합</strong>하는 것에 있다.</li>
<li><strong>이해 중심 태스크와 생성 중심 태스크 모두에 잘 작동</strong>하는 범용 사전학습 전략이다.</li>
<li>UL2는 기존 T5 구조를 사용하지만, <strong>학습 방식(프리트레이닝)이 완전히 다르다.</strong></li>
<li>동작 방식
<ul>
<li>UL2는 한 가지 방식이 아닌 <strong>세 가지 프리트레이닝 모드</strong>를 혼합하여 학습한다:
<ol type="1">
<li><strong>R-denoising (Random span masking)</strong>: T5처럼 일부 span을 가리고 복원</li>
<li><strong>X-denoising (Extreme masking)</strong>: 전체 문장을 거의 다 가리고 생성</li>
<li><strong>Causal LM</strong>: GPT처럼 왼쪽에서 오른쪽으로 생성 (Autoregressive) → 이 세 가지 모드를 비율에 따라 섞어 학습시킴 (Multi-task 사전학습)</li>
</ol></li>
</ul></li>
<li>예시:
<ul>
<li>입력: <code>fill in the blanks: The &lt;extra_id_0&gt; sat on the &lt;extra_id_1&gt;.</code></li>
<li>출력: <code>cat</code>, <code>mat</code></li>
<li>또는 GPT처럼 입력: <code>"The cat sat on"</code> → 출력: <code>" the mat."</code></li>
</ul></li>
<li>특징
<ul>
<li><strong>세 가지 방식의 장점을 융합</strong>하여 → 문장 이해 + 생성 모두에 강함</li>
<li>기존 T5는 디코더에서도 마스킹된 부분을 전부 본다 (비자연스러움) → UL2는 자연스러운 생성을 위해 <strong>Causal 방식도 병행</strong></li>
<li>다양한 태스크에 잘 작동하도록 설계됨</li>
</ul></li>
<li>요약
<ul>
<li>UL2는 모델이 아니라 <strong>사전학습 전략</strong>이다.</li>
<li>기존 Transformer 구조에 적용할 수 있음 (예: T5에 적용하면 UL2-T5)</li>
<li><strong>다양한 프리트레이닝 모드를 섞어 범용성과 자연스러움 극대화</strong></li>
<li>이후 FLAN 및 다양한 구글 LLM 개발의 기반이 됨</li>
</ul></li>
</ul>
</section>
<section id="결론" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="결론"><span class="header-section-number">3.10</span> 결론</h2>
<p>사전 학습 모델의 발전은 자연어 처리 분야에서 가장 중요한 패러다임 변화 중 하나다. 2015년 Google의 LSTM 실험부터 시작된 이 여정은 현재 우리가 사용하는 ChatGPT, Claude, Gemini 등 모든 대규모 언어 모델의 토대가 되었다.</p>
<ul>
<li><strong>기술적 진화의 핵심</strong>:
<ul>
<li><strong>정적 → 동적</strong>: Word2Vec에서 ELMo로의 전환으로 문맥 기반 표현 실현</li>
<li><strong>순차 → 병렬</strong>: Transformer의 등장으로 Self-Attention 기반 병렬 처리 가능</li>
<li><strong>단일 → 통합</strong>: T5의 텍스트-투-텍스트 프레임워크로 모든 NLP 태스크 통합</li>
<li><strong>일반 → 특화</strong>: GPT(생성), BERT(이해), BART(양방향) 등 목적별 특화</li>
</ul></li>
<li><strong>패러다임의 변화</strong>:
<ul>
<li><strong>Pre-training + Fine-tuning</strong>: 대규모 데이터로 사전 학습 후 특정 태스크 미세조정</li>
<li><strong>Transfer Learning</strong>: 학습된 지식을 다양한 하위 태스크로 전이</li>
<li><strong>Few-shot Learning</strong>: GPT 계열에서 보여준 예시 기반 학습 능력</li>
<li><strong>Instruction Following</strong>: FLAN으로 대표되는 명령어 이해 및 수행 능력</li>
</ul></li>
<li><strong>각 모델의 독특한 기여</strong>:
<ul>
<li><strong>ELMo</strong>: 문맥 기반 임베딩의 가능성 입증</li>
<li><strong>Transformer</strong>: 현대 AI의 기초 아키텍처 제공</li>
<li><strong>BERT</strong>: 양방향 문맥 이해의 혁신적 접근</li>
<li><strong>GPT</strong>: 생성형 AI와 In-context Learning의 선구자</li>
<li><strong>T5</strong>: 통합 프레임워크를 통한 범용성 확보</li>
<li><strong>LLaMA</strong>: 효율성과 성능의 균형점 제시</li>
</ul></li>
<li><strong>현재와 미래의 의미</strong>:
<ul>
<li>이들 모델은 단순한 기술적 발전을 넘어 AI와 인간의 상호작용 방식을 근본적으로 변화시켰다</li>
<li>ChatGPT의 성공으로 이어진 생성형 AI 붐의 기술적 토대 제공</li>
<li>언어 이해와 생성 능력의 비약적 향상으로 다양한 실용적 응용 가능</li>
</ul></li>
<li><strong>지속되는 혁신</strong>:
<ul>
<li>모델 크기와 성능의 지속적 확장</li>
<li>효율성과 접근성 개선을 위한 경량화 연구</li>
<li>다중 모달(텍스트, 이미지, 음성) 통합 모델로의 발전</li>
<li>더 나은 Instruction Following과 안전성 확보</li>
</ul></li>
</ul>
<p>사전 학습 모델의 발전은 여전히 진행 중이며, 각 모델이 제시한 핵심 아이디어들은 미래 AI 시스템의 기초가 되고 있다. 이러한 기술적 토대 위에서 더욱 강력하고 유용한 AI 시스템들이 계속 등장할 것으로 예상된다.</p>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/5-0.ptm_overview.html</guid>
  <pubDate>Wed, 15 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>RNN 기반 언어 모델과 Seq2Seq</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-4-1.RNN_SeqToSeq.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 RNN(순환 신경망)을 활용한 언어 모델의 기본 원리와 Seq2Seq 모델을 통한 기계 번역 시스템의 구현 방법을 설명한다. 언어 모델은 이전 단어들의 문맥을 바탕으로 다음에 올 단어를 예측하는 모델로, 자연어 생성과 기계 번역에서 핵심적인 역할을 한다.</p>
<p>주요 내용은 다음과 같다:</p>
<ul>
<li><strong>언어 모델의 기본 개념</strong>:
<ul>
<li>주어진 단어 시퀀스에서 다음 단어의 확률 분포를 예측하는 모델</li>
<li>수식: <img src="https://latex.codecogs.com/png.latex?P(w_1,%20w_2,%20%5Cldots,%20w_T)%20=%20%5Cprod_%7Bt=1%7D%5E%7BT%7D%20P(w_t%20%7C%20w_1,%20w_2,%20%5Cldots,%20w_%7Bt-1%7D)"></li>
<li>문맥 정보를 활용한 조건부 확률 모델링이 핵심</li>
</ul></li>
<li><strong>RNN 언어 모델 구조</strong>:
<ul>
<li>가변 길이 입력 처리 가능한 순환 구조</li>
<li>각 시점에서 hidden state가 문맥 정보를 누적하여 전달</li>
<li>Embedding → Hidden → Output 레이어로 구성된 표준 구조</li>
</ul></li>
<li><strong>Seq2Seq와 기계 번역</strong>:
<ul>
<li>Encoder-Decoder 구조를 통한 시퀀스 간 변환</li>
<li>Context Vector를 통한 소스 언어 정보의 압축 전달</li>
<li>1950년대 규칙 기반부터 2020년대 Transformer까지의 발전 과정</li>
</ul></li>
<li><strong>Teacher Forcing 학습 기법</strong>:
<ul>
<li>훈련 시 실제 정답 토큰을 디코더 입력으로 사용하는 방법</li>
<li>오류 누적 방지와 학습 안정성 확보</li>
<li>훈련과 테스트 단계의 차이점과 하이퍼파라미터로서의 활용</li>
</ul></li>
</ul>
<p>RNN 언어 모델과 Seq2Seq는 현대 자연어 처리의 기초가 되며, GPT와 같은 최신 언어 모델들의 토대를 제공한다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>RNN Language Model
├── Seq2Seq
├── Beam Search
├── Subword Tokenization
├── Attention
├── Transformer Encoder (Vaswani et al., 2017)
|   ├── Positional Encoding
|   ├── Multi-Head Attention
|   └── Feed Forward Neural Network
|
├── Transformer Decoder (Vaswani et al., 2017)
|
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
|
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
|
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등</code></pre>
</section>
<section id="rnn-기반-언어-모델" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> RNN 기반 언어 모델</h1>
<section id="nlp" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="nlp"><span class="header-section-number">3.1</span> NLP</h2>
<ul>
<li>NLP = NLU + NLG
<ul>
<li>NLU: 자연어 이해</li>
<li>NLG: 자연어 생성
<ul>
<li>Image Captioning</li>
<li>Text Summarization</li>
<li>Text Generation</li>
<li>Text Classification</li>
<li>Chatbot</li>
<li><strong>Neural Machine Translation</strong></li>
</ul></li>
</ul></li>
</ul>
<section id="machine-translation" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="machine-translation"><span class="header-section-number">3.1.1</span> Machine Translation</h3>
<ul>
<li>입력문장(source)을 번역한 출력 문장(target text)을 생성해내는 Task</li>
<li>Brief History
<ul>
<li>RBMT(Rule-based Machine Translation): 1950s, if-statement 기반 알고리즘, 성능 안좋음</li>
<li>EBMT(Example-based Machine Translation): 1980s, 예제 기반 알고리즘, 성능 안좋음</li>
<li>SMT(Statistical Machine Translation): 1990s, 통계 기반 알고리즘, 성능 보통</li>
<li>NMT(Neural Machine Translation): 2010s, 신경망 기반 알고리즘, 성능 비약적 상승</li>
<li>Transformer(Attention is all you need): 2017, 신경망 기반 알고리즘, 성능 매우 좋음</li>
<li>GPT-3(Generative Pre-trained Transformer 3): 2020, 성능 좋음</li>
<li>ChatGPT(Generative Pre-trained Transformer 3): 2022, 성능 매우 좋음</li>
<li>GPT-4(Generative Pre-trained Transformer 4): 2023, 성능 매우 좋음</li>
</ul></li>
<li>NMT(Neural Machine Translation) 성능 향상 이유
<ul>
<li>word embedding으로 인한 continuous representation 활용</li>
<li>기존 SMT가 여러 모듈이 결합된 결과였다면 이제는 end-to-end 모델의 시대
<ul>
<li>모듈 간 의존성 제거: upstream 모듈이 에러가 나면 전체 모델이 에러가 나는 문제 발생</li>
<li>end-to-end 모델 구조 도입: 하나의 모델이 수많은 파라미터 기반으로 오차함수에 의해 알고리즘이 일제히 업데이트 되는 구조</li>
</ul></li>
<li>Attention 으로 인해 길이가 긴 문장 또한 좋은 성능을 보이기 시작</li>
</ul></li>
</ul>
</section>
<section id="translation-모델-종류" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="translation-모델-종류"><span class="header-section-number">3.1.2</span> Translation 모델 종류</h3>
<ul>
<li>Seq2Seq: 2014, 입력 문장을 임베딩 벡터로 변환하여 출력 문장을 생성하는 모델</li>
<li>Transformer: 2017, 입력 문장을 임베딩 벡터로 변환하여 출력 문장을 생성하는 모델</li>
<li>BERT: 2018, 입력 문장을 임베딩 벡터로 변환하여 출력 문장을 생성하는 모델</li>
<li>GPT: 2018, 입력 문장을 임베딩 벡터로 변환하여 출력 문장을 생성하는 모델</li>
</ul>
</section>
</section>
<section id="seq2seq-sequence-to-sequence" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="seq2seq-sequence-to-sequence"><span class="header-section-number">3.2</span> Seq2Seq (Sequence to Sequence)</h2>
<ul>
<li>입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력한다.</li>
<li>구조: Encoder + Decoder
<ul>
<li>Encoder
<ul>
<li>NLU 모델: 입력 시퀀스를 임베딩 벡터로 변환</li>
<li>RNN으로 구성</li>
</ul></li>
<li>Decoder
<ul>
<li>NLG 모델: 출력 시퀀스를 출력 문장으로 변환</li>
<li>RNN으로 구성</li>
</ul></li>
</ul></li>
<li>동작 방식
<ul>
<li><p>Sequence to Sequence Learning with Neural Networks 논문 참조</p>
<ul>
<li>본격적인 신경망 기계 번역기를 제시</li>
<li>서로 다른 2개의 LSTM 아키텍쳐를 각각 인코더-디코더로 사용</li>
</ul></li>
<li><p>예문: 저는 학생입니다.</p>
<pre><code>Encoder-Decoder LSTM 번역 과정
│
├── Encoder Phase (인코딩 단계)
│   ├── Step 1: "저는"
│   │   ├── Input: "저는"
│   │   ├── Initial State: h0=0, c0=0
│   │   ├── Encoder LSTM1 연산
│   │   └── Output: h1, c1 → 다음 단계로 전달
│   │
│   ├── Step 2: "학생"  
│   │   ├── Input: "학생"
│   │   ├── Previous State: h1, c1 (from LSTM1)
│   │   ├── Encoder LSTM2 연산
│   │   └── Output: h2, c2 → 다음 단계로 전달
│   │
│   └── Step 3: "입니다"
│       ├── Input: "입니다"
│       ├── Previous State: h2, c2 (from LSTM2)
│       ├── Encoder LSTM3 연산
│       └── Output: Context Vector (h3, c3) ★ → Decoder로 전달
│
├── Context Transfer (문맥 전달)
│   └── Context Vector (h3, c3) → Decoder 초기 상태 (h_d0, c_d0)
│
└── Decoder Phase (디코딩 단계)
    ├── Step 1: 번역 시작
    │   ├── Input: EOS (번역 시작)
    │   ├── Initial State: h_d0=h3, c_d0=c3 (Context Vector)
    │   ├── Decoder LSTM1 연산
    │   ├── Output: "I"
    │   └── Hidden State: h_d1, c_d1 → 다음 단계로 전달
    │
    ├── Step 2: 첫 번째 단어 생성 후
    │   ├── Input: "I"
    │   ├── Previous State: h_d1, c_d1 (from Decoder LSTM1)
    │   ├── Decoder LSTM2 연산
    │   ├── Output: "am"
    │   └── Hidden State: h_d2, c_d2 → 다음 단계로 전달
    │
    ├── Step 3: 두 번째 단어 생성 후
    │   ├── Input: "am"
    │   ├── Previous State: h_d2, c_d2 (from Decoder LSTM2)
    │   ├── Decoder LSTM3 연산
    │   ├── Output: "a"
    │   └── Hidden State: h_d3, c_d3 → 다음 단계로 전달
    │
    ├── Step 4: 세 번째 단어 생성 후
    │   ├── Input: "a"
    │   ├── Previous State: h_d3, c_d3 (from Decoder LSTM3)
    │   ├── Decoder LSTM4 연산
    │   ├── Output: "student"
    │   └── Hidden State: h_d4, c_d4 → 다음 단계로 전달
    │
    ├── Step 5: 네 번째 단어 생성 후
    │   ├── Input: "student"
    │   ├── Previous State: h_d4, c_d4 (from Decoder LSTM4)
    │   ├── Decoder LSTM5 연산
    │   ├── Output: "."
    │   └── Hidden State: h_d5, c_d5 → 다음 단계로 전달
    │
    └── Step 6: 번역 종료
        ├── Input: "."
        ├── Previous State: h_d5, c_d5 (from Decoder LSTM5)
        ├── Decoder LSTM6 연산
        ├── Output: EOS (번역 종료) ★
        └── Final State: h_d6, c_d6 (번역 완료)</code></pre>
<ul>
<li>각 단어는 워드 임베딩이 된 벡터로 LSTM에 들어가지만 편의상 한글 단어로 표현</li>
<li>context vector
<ul>
<li>encoder 마지막 시점의 hidden state<br>
</li>
<li>인코다가 입력 문장의 모든 단어들을 수찬적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 context vector로 만든다.</li>
<li>즉, context vector는 decoder RNN 셀의 첫 번째 hidden state로 사용된다.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Teacher Forcing
<ul>
<li>Encoder RNN (학습방식) 과 Decoder RNN (Test 방식)의 동작 방식이 다르다.</li>
<li>파파고나 구글 번역기의 기초 동작 메커니즘</li>
<li>훈련 단계에서 Teacher Forcing 을 무조건 하는 것이 아니라 비율을 정해서 수행</li>
<li>이 비율이 hyper parameter 이다.</li>
<li>이 비율을 높게 설정할 수록 빠른 학습이 가능해지지만 overfit되어 테스트 단계에서 악영향을 줄 수 있음</li>
<li>test 단계에서는 teacher forcing을 사용하지 않으며 현 시점의 출력을 다음 시점의 입력으로 사용한다.</li>
</ul></li>
</ul>
</section>
<section id="결론" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="결론"><span class="header-section-number">3.3</span> 결론</h2>
<p>RNN 기반 언어 모델과 Seq2Seq 구조는 자연어 처리 분야에서 패러다임 전환을 이끈 핵심 기술이다. 이들은 기계 번역의 성능을 획기적으로 개선시켰으며, 현대 언어 모델들의 기초 개념을 제공했다.</p>
<ul>
<li><strong>기술적 의의</strong>:
<ul>
<li>가변 길이 시퀀스 처리 능력으로 자연어의 본질적 특성 반영</li>
<li>Encoder-Decoder 구조를 통한 서로 다른 도메인 간 매핑 실현</li>
<li>Context Vector 개념으로 긴 문장의 의미 압축 표현 가능</li>
</ul></li>
<li><strong>Teacher Forcing의 혁신</strong>:
<ul>
<li>훈련과 추론 단계의 차이를 체계적으로 관리하는 방법론 제시</li>
<li>오류 누적 문제 해결로 안정적인 시퀀스 생성 학습 실현</li>
<li>하이퍼파라미터 조정을 통한 성능 최적화 전략 제공</li>
</ul></li>
<li><strong>기계 번역의 발전</strong>:
<ul>
<li>규칙 기반(1950s) → 통계 기반(1990s) → 신경망 기반(2010s)의 발전 과정</li>
<li>End-to-end 학습으로 모듈 간 의존성 문제 해결</li>
<li>Word embedding과 연속 표현의 활용으로 성능 비약적 향상</li>
</ul></li>
<li><strong>현대적 의미</strong>:
<ul>
<li>LSTM, GRU 등 개선된 RNN 변형들의 토대 제공</li>
<li>Attention 메커니즘 도입의 동기와 배경 이해</li>
<li>Transformer, BERT, GPT 등 최신 모델들의 개념적 기초</li>
</ul></li>
</ul>
<p>비록 현재는 Transformer 기반 모델들이 주류를 이루고 있지만, RNN 언어 모델과 Seq2Seq의 핵심 아이디어들은 여전히 많은 NLP 시스템에서 활용되고 있다. 특히 실시간 처리가 중요한 애플리케이션이나 제한된 자원 환경에서는 RNN 기반 모델들이 여전히 유용한 선택지가 되고 있으며, 언어 모델의 기본 원리를 이해하는 데 필수적인 개념으로 남아있다.</p>
</section>
<section id="언어-모델의-수학적-정의" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="언어-모델의-수학적-정의"><span class="header-section-number">3.4</span> 언어 모델의 수학적 정의</h2>
<p>언어 모델은 주어진 단어 시퀀스의 확률을 계산하는 모델이다. <img src="https://latex.codecogs.com/png.latex?T">개의 단어로 구성된 문장 <img src="https://latex.codecogs.com/png.latex?w_1,%20w_2,%20%5Cldots,%20w_T">에 대해:</p>
<p><img src="https://latex.codecogs.com/png.latex?P(w_1,%20w_2,%20%5Cldots,%20w_T)%20=%20%5Cprod_%7Bt=1%7D%5E%7BT%7D%20P(w_t%20%7C%20w_1,%20w_2,%20%5Cldots,%20w_%7Bt-1%7D)"></p>
<p>여기서 <img src="https://latex.codecogs.com/png.latex?w_i"> 는 <img src="https://latex.codecogs.com/png.latex?i"> 번째 단어이고, <img src="https://latex.codecogs.com/png.latex?P(w_i%20%7C%20w_1,%20w_2,%20%5Cldots,%20w_%7Bi-1%7D)"> 는 이전 단어들이 주어졌을 때 현재 단어가 나올 조건부 확률이다.</p>
</section>
<section id="rnn을-이용한-구현" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="rnn을-이용한-구현"><span class="header-section-number">3.5</span> RNN을 이용한 구현</h2>
<p>RNN 언어 모델에서는 각 시점의 hidden state <img src="https://latex.codecogs.com/png.latex?h_t">가 지금까지의 모든 단어 정보를 압축하여 저장한다:</p>
<p><img src="https://latex.codecogs.com/png.latex?h_t%20=%20f(h_%7Bt-1%7D,%20x_t)"> <img src="https://latex.codecogs.com/png.latex?P(w_%7Bt+1%7D%20%7C%20w_1,%20%5Cldots,%20w_t)%20=%20%5Ctext%7Bsoftmax%7D(W_o%20h_t%20+%20b_o)"></p>
<p>각 시점 <img src="https://latex.codecogs.com/png.latex?t"> 에서의 계산 과정:</p>
<ol type="1">
<li><strong>입력 임베딩</strong>: <img src="https://latex.codecogs.com/png.latex?e_t%20=%20%5Ctext%7BEmbedding%7D(x_t)"></li>
<li><strong>Hidden State 업데이트</strong>: <img src="https://latex.codecogs.com/png.latex?h_t%20=%20%5Ctanh(W_%7Bhh%7D%20h_%7Bt-1%7D%20+%20W_%7Bxh%7D%20e_t%20+%20b_h)"></li>
<li><strong>출력 계산</strong>: <img src="https://latex.codecogs.com/png.latex?o_t%20=%20W_%7Bho%7D%20h_t%20+%20b_o"></li>
<li><strong>확률 분포</strong>: <img src="https://latex.codecogs.com/png.latex?P(w_%7Bt+1%7D)%20=%20%5Ctext%7Bsoftmax%7D(o_t)"></li>
</ol>
<section id="실제-동작-예시" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="실제-동작-예시"><span class="header-section-number">3.5.1</span> 실제 동작 예시</h3>
<p>문장 “what will the side effects be?”를 처리하는 과정:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?x_1%20=%20%5Ctext%7B%22what%22%7D"> → <img src="https://latex.codecogs.com/png.latex?h_1"> 계산 → <img src="https://latex.codecogs.com/png.latex?P(%5Ctext%7Bnext%20word%7D%20%7C%20%5Ctext%7B%22what%22%7D)"> → <img src="https://latex.codecogs.com/png.latex?y_1%20=%20%5Ctext%7B%22will%22%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?x_2%20=%20%5Ctext%7B%22will%22%7D"> → <img src="https://latex.codecogs.com/png.latex?h_2"> 계산 → <img src="https://latex.codecogs.com/png.latex?P(%5Ctext%7Bnext%20word%7D%20%7C%20%5Ctext%7B%22what%20will%22%7D)"> → <img src="https://latex.codecogs.com/png.latex?y_2%20=%20%5Ctext%7B%22the%22%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?x_3%20=%20%5Ctext%7B%22the%22%7D"> → <img src="https://latex.codecogs.com/png.latex?h_3"> 계산 → <img src="https://latex.codecogs.com/png.latex?P(%5Ctext%7Bnext%20word%7D%20%7C%20%5Ctext%7B%22what%20will%20the%22%7D)"> → <img src="https://latex.codecogs.com/png.latex?y_3%20=%20%5Ctext%7B%22side%22%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?x_4%20=%20%5Ctext%7B%22side%22%7D"> → <img src="https://latex.codecogs.com/png.latex?h_4"> 계산 → <img src="https://latex.codecogs.com/png.latex?P(%5Ctext%7Bnext%20word%7D%20%7C%20%5Ctext%7B%22what%20will%20the%20side%22%7D)"> → <img src="https://latex.codecogs.com/png.latex?y_4%20=%20%5Ctext%7B%22effects%22%7D"></li>
</ul>
<p>이 과정에서 <img src="https://latex.codecogs.com/png.latex?h_t"> 는 시점 <img src="https://latex.codecogs.com/png.latex?t">까지의 모든 이전 단어들의 문맥 정보를 압축적으로 담고 있다.</p>
</section>
</section>
<section id="teacher-forcing-교사-강요" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="teacher-forcing-교사-강요"><span class="header-section-number">3.6</span> Teacher Forcing (교사 강요)</h2>
<section id="개념과-필요성" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="개념과-필요성"><span class="header-section-number">3.6.1</span> 개념과 필요성</h3>
<p>Teacher Forcing은 sequence-to-sequence 모델의 훈련 과정에서 사용되는 기법이다:</p>
<ul>
<li><strong>훈련 시</strong>: 실제 정답(ground truth) 토큰을 디코더의 다음 입력으로 사용</li>
<li><strong>추론 시</strong>: 모델이 예측한 토큰을 다음 입력으로 사용</li>
</ul>
<p>이는 다음과 같은 문제를 해결한다: - <strong>오류 누적(Error Accumulation)</strong>: 잘못된 예측이 연쇄적으로 더 큰 오류를 만드는 문제 - <strong>학습 불안정성</strong>: 초기 학습 단계에서 무작위 예측으로 인한 학습 어려움 - <strong>수렴 속도</strong>: 올바른 패턴 학습을 위한 가이드 제공</p>
</section>
<section id="구현-시-고려사항" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="구현-시-고려사항"><span class="header-section-number">3.6.2</span> 구현 시 고려사항</h3>
<p><strong>Teacher Forcing 비율 조정</strong>: - 초기 학습: 높은 비율(0.8-1.0)로 안정적 학습 - 후기 학습: 점진적으로 비율 감소(0.3-0.5) - 실제 추론 상황과의 차이를 줄이기 위한 점진적 조정</p>
<p><strong>Scheduled Sampling</strong>: - 훈련 중에도 가끔 예측값을 사용하여 exposure bias 완화 - 커리큘럼 학습의 한 형태로 활용</p>
</section>
<section id="각-레이어의-상세-구조" class="level3" data-number="3.6.3">
<h3 data-number="3.6.3" class="anchored" data-anchor-id="각-레이어의-상세-구조"><span class="header-section-number">3.6.3</span> 각 레이어의 상세 구조</h3>
<section id="embedding-layer" class="level4" data-number="3.6.3.1">
<h4 data-number="3.6.3.1" class="anchored" data-anchor-id="embedding-layer"><span class="header-section-number">3.6.3.1</span> Embedding Layer</h4>
<ul>
<li><strong>역할</strong>: 단어 인덱스를 고정 크기의 벡터로 변환</li>
<li><strong>수식</strong>: <img src="https://latex.codecogs.com/png.latex?e_t%20=%20E%5Bw_t%5D">, 여기서 <img src="https://latex.codecogs.com/png.latex?E%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BV%20%5Ctimes%20d%7D"></li>
<li><strong>파라미터</strong>:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?V">: 어휘 크기 (일반적으로 10,000-50,000개)</li>
<li><img src="https://latex.codecogs.com/png.latex?d">: 임베딩 차원수 (일반적으로 100-300차원)</li>
</ul></li>
</ul>
</section>
<section id="rnn-hidden-layer" class="level4" data-number="3.6.3.2">
<h4 data-number="3.6.3.2" class="anchored" data-anchor-id="rnn-hidden-layer"><span class="header-section-number">3.6.3.2</span> RNN Hidden Layer</h4>
<ul>
<li><strong>역할</strong>: 시퀀스의 문맥 정보를 누적하여 표현</li>
<li><strong>수식</strong>: <img src="https://latex.codecogs.com/png.latex?h_t%20=%20%5Ctanh(W_%7Bhh%7D%20h_%7Bt-1%7D%20+%20W_%7Bxh%7D%20e_t%20+%20b_h)"></li>
<li><strong>파라미터</strong>:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?W_%7Bhh%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BH%20%5Ctimes%20H%7D">: hidden-to-hidden 가중치</li>
<li><img src="https://latex.codecogs.com/png.latex?W_%7Bxh%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BH%20%5Ctimes%20d%7D">: input-to-hidden 가중치</li>
<li><img src="https://latex.codecogs.com/png.latex?H">: 은닉 상태 차원수 (일반적으로 128-512차원)</li>
</ul></li>
</ul>
</section>
<section id="output-layer" class="level4" data-number="3.6.3.3">
<h4 data-number="3.6.3.3" class="anchored" data-anchor-id="output-layer"><span class="header-section-number">3.6.3.3</span> Output Layer</h4>
<ul>
<li><strong>역할</strong>: 은닉 상태를 어휘 크기의 로짓 벡터로 변환</li>
<li><strong>수식</strong>: <img src="https://latex.codecogs.com/png.latex?o_t%20=%20W_%7Bho%7D%20h_t%20+%20b_o"></li>
<li><strong>파라미터</strong>: <img src="https://latex.codecogs.com/png.latex?W_%7Bho%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BV%20%5Ctimes%20H%7D">, <img src="https://latex.codecogs.com/png.latex?b_o%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BV%7D"></li>
</ul>
</section>
<section id="softmax-loss" class="level4" data-number="3.6.3.4">
<h4 data-number="3.6.3.4" class="anchored" data-anchor-id="softmax-loss"><span class="header-section-number">3.6.3.4</span> Softmax &amp; Loss</h4>
<ul>
<li><strong>확률 분포</strong>: <img src="https://latex.codecogs.com/png.latex?P(w_t%20%7C%20w_%7B%3Ct%7D)%20=%20%5Ctext%7Bsoftmax%7D(o_t)%20=%20%5Cfrac%7Be%5E%7Bo_t%5E%7B(i)%7D%7D%7D%7B%5Csum_%7Bj=1%7D%5E%7BV%7D%20e%5E%7Bo_t%5E%7B(j)%7D%7D%7D"></li>
<li><strong>손실 함수</strong>: <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%20=%20-%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Clog%20P(w_t%5E*%20%7C%20w_%7B%3Ct%7D)"></li>
</ul>
</section>
</section>
<section id="모델-성능-최적화" class="level3" data-number="3.6.4">
<h3 data-number="3.6.4" class="anchored" data-anchor-id="모델-성능-최적화"><span class="header-section-number">3.6.4</span> 모델 성능 최적화</h3>
<p><strong>기울기 소실 문제 해결</strong>: - LSTM, GRU 같은 게이트 메커니즘 도입 - Residual Connection 활용 - 적절한 가중치 초기화 (Xavier, He 초기화)</p>
<p><strong>계산 효율성 개선</strong>: - 배치 처리를 통한 병렬화 - 적응적 softmax (hierarchical softmax) - 어휘 크기 축소 기법 (subword tokenization)</p>


</section>
</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-4-1.RNN_SeqToSeq.html</guid>
  <pubDate>Tue, 14 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 벡터화: Attention 메커니즘의 이해</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-4-2.RNN_BeamSearch.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 기존 Seq2Seq 모델의 한계를 극복하기 위해 제안된 Attention 메커니즘의 기본 원리와 구조를 소개한다. Attention은 입력 시퀀스의 각 요소에 대한 중요도를 동적으로 계산하여 출력 시퀀스 생성 시 필요한 정보를 효과적으로 활용할 수 있게 해주는 혁신적인 접근 방식이다.</p>
<p>주요 내용은 다음과 같다.</p>
<ul>
<li><strong>Seq2Seq 모델의 한계와 Attention의 등장</strong>:
<ul>
<li>기존 Seq2Seq 모델은 입력 문장의 모든 정보를 고정된 크기의 컨텍스트 벡터에 압축하는 과정에서 정보 손실이 발생한다.</li>
<li>Attention은 이러한 한계를 극복하기 위해 입력 시퀀스의 각 요소에 대한 중요도를 동적으로 계산하여 필요한 정보를 선택적으로 활용한다.</li>
</ul></li>
<li><strong>Attention의 핵심 구성 요소 및 작동 원리</strong>:
<ul>
<li><strong>Query, Key, Value</strong>: 입력 시퀀스의 각 요소를 Query, Key, Value로 변환하여 유사도와 중요도를 계산한다.</li>
<li><strong>Attention Score</strong>: Query와 Key의 유사도를 계산하여 각 입력 요소의 중요도를 결정한다.</li>
<li><strong>Attention Weight</strong>: Attention Score를 정규화하여 각 입력 요소에 대한 가중치를 생성한다.</li>
<li><strong>Context Vector</strong>: Attention Weight와 Value를 결합하여 최종 컨텍스트 벡터를 생성한다.</li>
</ul></li>
<li><strong>Attention의 장점</strong>:
<ul>
<li>입력 시퀀스의 길이에 상관없이 모든 정보를 효과적으로 활용할 수 있다.</li>
<li>출력 시퀀스 생성 시 필요한 정보를 선택적으로 집중할 수 있다.</li>
<li>모델의 해석 가능성을 높이고, 장기 의존성 문제를 효과적으로 해결한다.</li>
</ul></li>
<li><strong>의의</strong>: Attention은 자연어 처리, 컴퓨터 비전 등 다양한 분야에서 혁신적인 성능을 보여주며, Transformer와 같은 최신 모델의 기반이 되었다.</li>
</ul>
<p>이 문서를 통해 독자는 Attention이 어떻게 시퀀스 데이터의 장기 의존성 문제를 해결하고, 입력과 출력 간의 관계를 효과적으로 모델링할 수 있는지에 대한 기본적인 이해를 얻을 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention
 
Transformer 이후 생성형 모델 발전 계열
├── Transformer 구조 (Vaswani et al., 2017)
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등</code></pre>
</section>
<section id="attention" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Attention</h1>
<section id="기존-sequence-to-sequence-seq2seq의-한계" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="기존-sequence-to-sequence-seq2seq의-한계"><span class="header-section-number">3.1</span> 기존 Sequence to Sequence, Seq2Seq의 한계</h2>
<ul>
<li>입력 문장의 길이와 상관없이 고정된 크기의 벡터에 정보를 모두 압축한다.</li>
<li>Bottleneck 문제: 입력 문장의 길이가 길어질수록 고정된 크기의 벡터에 정보가 다 압축되지 않아 정보 손실이 발생한다.</li>
<li>Encoder -&gt; Context Vector -&gt; Decoder
<ul>
<li>단어를 Embedding 후 Encoder에서 벡터화되어 Context Vector가 됨</li>
<li>즉, Context Vector는 Encoder의 LSTM의 마지막 hidden state의 출력값 (벡터 크기가 고정)</li>
<li>이 고정된 크기에 정보가 모두 압축되지 않는다면 정보 손실이 발생</li>
<li>벡터 크기가 고정되어 있기 때문에 입력 문장의 길이가 길어질수록 정보 손실이 발생한다.</li>
<li>입력 문장의 길이가 길어질수록 정보 손실이 발생한다.</li>
<li>정보 손실이 일어난 Context Vector가 Decoder의 Input 벡터가 됨</li>
</ul></li>
<li>RNN 자체 문제: RNN 계열의 고질적인 장기 의존성 문제로 초기 정보가 손실되며 전달된다.
<ul>
<li>LSTM과 GRU가 장기 손실을 줄이기 위해 고안된 모델이지만 여전히 장기 의존성 문제가 발생한다.</li>
<li>전체 한 문장도 잘 기억 못함 (even with LSTM, GRU)</li>
<li>Carnegie Mellon University의 연구 : BLEU Score 측정</li>
</ul></li>
</ul>
</section>
<section id="attention의-정의" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="attention의-정의"><span class="header-section-number">3.2</span> Attention의 정의</h2>
<ul>
<li>사전적 의미: 주의, 집중</li>
<li>NLP에서의 의미: 번역 문장을 만드는 과정에서 기존 문장에서 주용한 단어를 집중(Attention)</li>
<li>예: I am a good student를 한글로 번역할 때 각 문장에서 주요 단어에 집중하여 번역
<ol type="1">
<li><code>I</code> 에 집중 -&gt; 나는<br>
</li>
<li><code>good</code> 에 집중 -&gt; 나는 좋은</li>
<li><code>student</code> 에 집중 -&gt; 나는 좋은 학생</li>
<li><code>am</code> 에 집중 -&gt; 나는 좋은 학생이다.</li>
</ol></li>
<li>단어를 생성할 때 기존에 선택한 단어의 유사도와 문맥을 고려하여 다음 단어를 선택</li>
</ul>
</section>
<section id="원리" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="원리"><span class="header-section-number">3.3</span> 원리</h2>
<section id="key-value-형태로-학습" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="key-value-형태로-학습"><span class="header-section-number">3.3.1</span> Key Value 형태로 학습</h3>
<ul>
<li>Attention 메커니즘: Attention(Q, K, V) = Attention Value
<ul>
<li>Q: Query, K: Key, V: Value</li>
<li>Q,K,V를 입력받아 Attention Value를 출력</li>
</ul>
<ol type="1">
<li>어텐션 함수는 주어진 쿼리(Q)에 대해서 모든 키(K)와의 유사도를 각 각 계산한다.</li>
<li>구해낸 이 유사도를 키(K)와 맵핑되어있는 각각의 값(V)에 곱하여 반영해준다.</li>
<li>유사도가 반영된 값을 모두 더해서 리턴한다.</li>
<li>이렇게 출력된 값을 어테션 값 (attention value)이라고 한다.</li>
<li>이 어테션 값을 출력으로 사용한다.</li>
</ol></li>
<li>예를 들어 Q 1개, Key 3개, Value 3개가 있다면
<ul>
<li>Q와 K1, K2, K3의 유사도를 계산: <img src="https://latex.codecogs.com/png.latex?Q%20%5Ccdot%20K1">, <img src="https://latex.codecogs.com/png.latex?Q%20%5Ccdot%20K2">, <img src="https://latex.codecogs.com/png.latex?Q%20%5Ccdot%20K3"></li>
<li>유사도를 V1, V2, V3에 곱하여 반영: <img src="https://latex.codecogs.com/png.latex?V'1=%20K1%20%5Ccdot%20V1">, <img src="https://latex.codecogs.com/png.latex?V'2=%20K2%20%5Ccdot%20V2">, <img src="https://latex.codecogs.com/png.latex?V'3=%20K3%20%5Ccdot%20V3"></li>
<li>attention score = 유사도가 반영된 값 <img src="https://latex.codecogs.com/png.latex?V'1"> , <img src="https://latex.codecogs.com/png.latex?V'2"> , <img src="https://latex.codecogs.com/png.latex?V'3"></li>
<li>softmax([<img src="https://latex.codecogs.com/png.latex?V'1"> , <img src="https://latex.codecogs.com/png.latex?V'2"> , <img src="https://latex.codecogs.com/png.latex?V'3">])을 구함</li>
<li>attention value, a1 = 위의 3값과 hidden state의 값을 내적하여 모두 더함</li>
<li>즉, a1은 decoder의 예측 단어와 입력단어들의 유사도 정보가 있음. (유사도가 높으면 가중치가 높게 부여되어 반영됨)<br>
</li>
<li>a1과 decoder의 마지막 시점의 hidden state를 내적하여 tanh를 취하여 출력</li>
<li><img src="https://latex.codecogs.com/png.latex?y_t%20=%20%5Ctext%7Bsoftmax%7D(W_y%5Ctilde%7Bs_t%7D+b_y)"></li>
</ul></li>
<li>참고로 Seq2Seq의 hidden state가 Key이자 Value가 역할을 한다.</li>
</ul>
</section>
<section id="수식" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="수식"><span class="header-section-number">3.3.2</span> 수식</h3>
<ol type="1">
<li><p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bscore%7D_i%20=%20Q%20%5Ccdot%20K_i"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_i%20=%20%5Ctext%7Bsoftmax%7D(%5Ctext%7Bscore%7D_i)"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?a_t%20=%20%5Csum_i%20%5Calpha_i%20V_i"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bs_t%7D%20=%20%5Ctanh(W_c%20%5Ba_t;%20s_t%5D)"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?y_t%20=%20%5Ctext%7Bsoftmax%7D(W_y%20%5Ctilde%7Bs_t%7D%20+%20b_y)"></p></li>
<li><p><strong>유사도(Score) 계산: Query와 Key의 내적</strong></p></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bscore%7D_i%20=%20Q%20%5Ccdot%20K_i%20%5Cquad%20%5Ctext%7Bfor%20%7D%20i%20=%201,%202,%203%0A"></p>
<p>혹은 전체를 벡터화하면: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bscores%7D%20=%20Q%20K%5ET%20%5Cquad%20%5Ctext%7B(Q:%201%C3%97d,%20K:%203%C3%97d%20%E2%86%92%20scores:%201%C3%973)%7D%0A"></p>
<blockquote class="blockquote">
<p>Scaled Dot-Product Attention에서는 보통 다음과 같이 스케일 조정:</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bscores%7D%20=%20%5Cfrac%7BQ%20K%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%0A"></p>
<ol start="2" type="1">
<li><strong>Softmax로 유사도 정규화 (Attention Weights)</strong></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_i%20=%20%5Cfrac%7B%5Cexp(%5Ctext%7Bscore%7D_i)%7D%7B%5Csum_%7Bj=1%7D%5E%7B3%7D%20%5Cexp(%5Ctext%7Bscore%7D_j)%7D%0A%5Cquad%20%5Ctext%7B(i%20=%201,%202,%203)%7D%0A"></p>
<p>또는 벡터 전체:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboldsymbol%7B%5Calpha%7D%20=%20%5Ctext%7Bsoftmax%7D(Q%20K%5ET)%0A"></p>
<hr>
<ol start="3" type="1">
<li><strong>각 Value 벡터에 가중치를 곱해 합산 (Attention Output)</strong></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BAttention%7D(Q,%20K,%20V)%20=%20%5Csum_%7Bi=1%7D%5E%7B3%7D%20%5Calpha_i%20V_i%20=%20%5Cboldsymbol%7B%5Calpha%7D%20%5Ccdot%20V%0A"></p>
<p>즉, 전체 수식은:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BAttention%7D(Q,%20K,%20V)%20=%20%5Ctext%7Bsoftmax%7D%5Cleft(%20%5Cfrac%7BQ%20K%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%20%5Cright)%20V%0A"></p>
<ol start="4" type="1">
<li><strong>디코더 hidden state와 결합 후 출력 계산</strong></li>
</ol>
<ul>
<li>context vector (a₁)와 디코더의 hidden state <img src="https://latex.codecogs.com/png.latex?s_t"> 결합:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctilde%7Bs_t%7D%20=%20%5Ctanh(W_c%20%5Ba_t;%20s_t%5D)%0A"></p>
<ul>
<li>최종 출력 (예측 단어 확률 분포):</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay_t%20=%20%5Ctext%7Bsoftmax%7D(W_y%20%5Ctilde%7Bs_t%7D%20+%20b_y)%0A"></p>
</section>
</section>
<section id="강점" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="강점"><span class="header-section-number">3.4</span> 강점</h2>
<ul>
<li>RNN계열 Seq2Seq 구조에 도입되어 기계 번역의 성능을 상당 부분 개선</li>
<li>후에, attention으로 모든 state에 접근하여 더 나은 성능을 보임 = Attention만으로도 성능 월등</li>
<li>결국, RNN은 필요하지 않게 되었음 = 모든 정보를 벡터화하여 저장하는 것이 아니라 중요한 정보만 저장하고 있으면 됨</li>
<li>후에 이를 바탕으로 발전된 기술이 Transformer (Attention is all you need.)</li>
</ul>
</section>
<section id="결론" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="결론"><span class="header-section-number">3.5</span> 결론</h2>
<p>본 문서에서는 Attention 메커니즘의 기본 원리와 작동 방식을 살펴보았다. Attention은 시퀀스 데이터 처리에서 중요한 정보에 집중할 수 있게 해주는 핵심적인 기술로, 기계 번역을 비롯한 다양한 자연어 처리 태스크에서 혁신적인 성능 향상을 가져왔다.</p>
<ul>
<li><strong>Attention의 핵심 원리 요약</strong>:
<ul>
<li>Attention은 Query, Key, Value 세 가지 요소를 기반으로 작동하며, Query와 Key의 유사도를 계산하여 Value에 대한 가중치를 결정한다.</li>
<li>Softmax를 통해 정규화된 가중치를 사용하여 중요한 정보에 더 집중할 수 있게 해주며, 이를 통해 문맥에 따른 적절한 정보 선택이 가능해진다.</li>
</ul></li>
<li><strong>RNN 기반 모델과의 관계 및 장점</strong>:
<ul>
<li>기존 RNN 기반 Seq2Seq 모델의 한계를 극복하여, 긴 시퀀스에서도 중요한 정보를 효과적으로 포착할 수 있게 되었다.</li>
<li>모든 입력 정보에 직접 접근할 수 있어 장기 의존성 문제를 해결하고, 더 정확한 번역과 생성이 가능해졌다.</li>
</ul></li>
<li><strong>Transformer로의 발전</strong>:
<ul>
<li>Attention 메커니즘의 성공은 RNN을 완전히 대체하는 Transformer 아키텍처의 등장으로 이어졌다.</li>
<li>“Attention is all you need”라는 명제가 증명되었듯이, Attention만으로도 뛰어난 성능을 보일 수 있음을 보여주었다.</li>
</ul></li>
</ul>
<p>결론적으로, Attention 메커니즘은 자연어 처리 분야에서 혁신적인 변화를 가져온 핵심 기술이다. RNN의 한계를 극복하고 Transformer의 등장을 이끌어냄으로써, 현대 자연어 처리의 기반을 마련했으며, 이는 BERT, GPT와 같은 혁신적인 모델들의 등장으로 이어졌다.</p>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-4-2.RNN_BeamSearch.html</guid>
  <pubDate>Tue, 14 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 벡터화: Attention 메커니즘의 이해</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-4-3.RNN_Subword.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 기존 Seq2Seq 모델의 한계를 극복하기 위해 제안된 Attention 메커니즘의 기본 원리와 구조를 소개한다. Attention은 입력 시퀀스의 각 요소에 대한 중요도를 동적으로 계산하여 출력 시퀀스 생성 시 필요한 정보를 효과적으로 활용할 수 있게 해주는 혁신적인 접근 방식이다.</p>
<p>주요 내용은 다음과 같다.</p>
<ul>
<li><strong>Seq2Seq 모델의 한계와 Attention의 등장</strong>:
<ul>
<li>기존 Seq2Seq 모델은 입력 문장의 모든 정보를 고정된 크기의 컨텍스트 벡터에 압축하는 과정에서 정보 손실이 발생한다.</li>
<li>Attention은 이러한 한계를 극복하기 위해 입력 시퀀스의 각 요소에 대한 중요도를 동적으로 계산하여 필요한 정보를 선택적으로 활용한다.</li>
</ul></li>
<li><strong>Attention의 핵심 구성 요소 및 작동 원리</strong>:
<ul>
<li><strong>Query, Key, Value</strong>: 입력 시퀀스의 각 요소를 Query, Key, Value로 변환하여 유사도와 중요도를 계산한다.</li>
<li><strong>Attention Score</strong>: Query와 Key의 유사도를 계산하여 각 입력 요소의 중요도를 결정한다.</li>
<li><strong>Attention Weight</strong>: Attention Score를 정규화하여 각 입력 요소에 대한 가중치를 생성한다.</li>
<li><strong>Context Vector</strong>: Attention Weight와 Value를 결합하여 최종 컨텍스트 벡터를 생성한다.</li>
</ul></li>
<li><strong>Attention의 장점</strong>:
<ul>
<li>입력 시퀀스의 길이에 상관없이 모든 정보를 효과적으로 활용할 수 있다.</li>
<li>출력 시퀀스 생성 시 필요한 정보를 선택적으로 집중할 수 있다.</li>
<li>모델의 해석 가능성을 높이고, 장기 의존성 문제를 효과적으로 해결한다.</li>
</ul></li>
<li><strong>의의</strong>: Attention은 자연어 처리, 컴퓨터 비전 등 다양한 분야에서 혁신적인 성능을 보여주며, Transformer와 같은 최신 모델의 기반이 되었다.</li>
</ul>
<p>이 문서를 통해 독자는 Attention이 어떻게 시퀀스 데이터의 장기 의존성 문제를 해결하고, 입력과 출력 간의 관계를 효과적으로 모델링할 수 있는지에 대한 기본적인 이해를 얻을 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention
 
Transformer 이후 생성형 모델 발전 계열
├── Transformer 구조 (Vaswani et al., 2017)
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등</code></pre>
</section>
<section id="attention" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Attention</h1>
<section id="기존-sequence-to-sequence-seq2seq의-한계" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="기존-sequence-to-sequence-seq2seq의-한계"><span class="header-section-number">3.1</span> 기존 Sequence to Sequence, Seq2Seq의 한계</h2>
<ul>
<li>입력 문장의 길이와 상관없이 고정된 크기의 벡터에 정보를 모두 압축한다.</li>
<li>Bottleneck 문제: 입력 문장의 길이가 길어질수록 고정된 크기의 벡터에 정보가 다 압축되지 않아 정보 손실이 발생한다.</li>
<li>Encoder -&gt; Context Vector -&gt; Decoder
<ul>
<li>단어를 Embedding 후 Encoder에서 벡터화되어 Context Vector가 됨</li>
<li>즉, Context Vector는 Encoder의 LSTM의 마지막 hidden state의 출력값 (벡터 크기가 고정)</li>
<li>이 고정된 크기에 정보가 모두 압축되지 않는다면 정보 손실이 발생</li>
<li>벡터 크기가 고정되어 있기 때문에 입력 문장의 길이가 길어질수록 정보 손실이 발생한다.</li>
<li>입력 문장의 길이가 길어질수록 정보 손실이 발생한다.</li>
<li>정보 손실이 일어난 Context Vector가 Decoder의 Input 벡터가 됨</li>
</ul></li>
<li>RNN 자체 문제: RNN 계열의 고질적인 장기 의존성 문제로 초기 정보가 손실되며 전달된다.
<ul>
<li>LSTM과 GRU가 장기 손실을 줄이기 위해 고안된 모델이지만 여전히 장기 의존성 문제가 발생한다.</li>
<li>전체 한 문장도 잘 기억 못함 (even with LSTM, GRU)</li>
<li>Carnegie Mellon University의 연구 : BLEU Score 측정</li>
</ul></li>
</ul>
</section>
<section id="attention의-정의" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="attention의-정의"><span class="header-section-number">3.2</span> Attention의 정의</h2>
<ul>
<li>사전적 의미: 주의, 집중</li>
<li>NLP에서의 의미: 번역 문장을 만드는 과정에서 기존 문장에서 주용한 단어를 집중(Attention)</li>
<li>예: I am a good student를 한글로 번역할 때 각 문장에서 주요 단어에 집중하여 번역
<ol type="1">
<li><code>I</code> 에 집중 -&gt; 나는<br>
</li>
<li><code>good</code> 에 집중 -&gt; 나는 좋은</li>
<li><code>student</code> 에 집중 -&gt; 나는 좋은 학생</li>
<li><code>am</code> 에 집중 -&gt; 나는 좋은 학생이다.</li>
</ol></li>
<li>단어를 생성할 때 기존에 선택한 단어의 유사도와 문맥을 고려하여 다음 단어를 선택</li>
</ul>
</section>
<section id="원리" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="원리"><span class="header-section-number">3.3</span> 원리</h2>
<section id="key-value-형태로-학습" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="key-value-형태로-학습"><span class="header-section-number">3.3.1</span> Key Value 형태로 학습</h3>
<ul>
<li>Attention 메커니즘: Attention(Q, K, V) = Attention Value
<ul>
<li>Q: Query, K: Key, V: Value</li>
<li>Q,K,V를 입력받아 Attention Value를 출력</li>
</ul>
<ol type="1">
<li>어텐션 함수는 주어진 쿼리(Q)에 대해서 모든 키(K)와의 유사도를 각 각 계산한다.</li>
<li>구해낸 이 유사도를 키(K)와 맵핑되어있는 각각의 값(V)에 곱하여 반영해준다.</li>
<li>유사도가 반영된 값을 모두 더해서 리턴한다.</li>
<li>이렇게 출력된 값을 어테션 값 (attention value)이라고 한다.</li>
<li>이 어테션 값을 출력으로 사용한다.</li>
</ol></li>
<li>예를 들어 Q 1개, Key 3개, Value 3개가 있다면
<ul>
<li>Q와 K1, K2, K3의 유사도를 계산: <img src="https://latex.codecogs.com/png.latex?Q%20%5Ccdot%20K1">, <img src="https://latex.codecogs.com/png.latex?Q%20%5Ccdot%20K2">, <img src="https://latex.codecogs.com/png.latex?Q%20%5Ccdot%20K3"></li>
<li>유사도를 V1, V2, V3에 곱하여 반영: <img src="https://latex.codecogs.com/png.latex?V'1=%20K1%20%5Ccdot%20V1">, <img src="https://latex.codecogs.com/png.latex?V'2=%20K2%20%5Ccdot%20V2">, <img src="https://latex.codecogs.com/png.latex?V'3=%20K3%20%5Ccdot%20V3"></li>
<li>attention score = 유사도가 반영된 값 <img src="https://latex.codecogs.com/png.latex?V'1"> , <img src="https://latex.codecogs.com/png.latex?V'2"> , <img src="https://latex.codecogs.com/png.latex?V'3"></li>
<li>softmax([<img src="https://latex.codecogs.com/png.latex?V'1"> , <img src="https://latex.codecogs.com/png.latex?V'2"> , <img src="https://latex.codecogs.com/png.latex?V'3">])을 구함</li>
<li>attention value, a1 = 위의 3값과 hidden state의 값을 내적하여 모두 더함</li>
<li>즉, a1은 decoder의 예측 단어와 입력단어들의 유사도 정보가 있음. (유사도가 높으면 가중치가 높게 부여되어 반영됨)<br>
</li>
<li>a1과 decoder의 마지막 시점의 hidden state를 내적하여 tanh를 취하여 출력</li>
<li><img src="https://latex.codecogs.com/png.latex?y_t%20=%20%5Ctext%7Bsoftmax%7D(W_y%5Ctilde%7Bs_t%7D+b_y)"></li>
</ul></li>
<li>참고로 Seq2Seq의 hidden state가 Key이자 Value가 역할을 한다.</li>
</ul>
</section>
<section id="수식" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="수식"><span class="header-section-number">3.3.2</span> 수식</h3>
<ol type="1">
<li><p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bscore%7D_i%20=%20Q%20%5Ccdot%20K_i"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_i%20=%20%5Ctext%7Bsoftmax%7D(%5Ctext%7Bscore%7D_i)"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?a_t%20=%20%5Csum_i%20%5Calpha_i%20V_i"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bs_t%7D%20=%20%5Ctanh(W_c%20%5Ba_t;%20s_t%5D)"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?y_t%20=%20%5Ctext%7Bsoftmax%7D(W_y%20%5Ctilde%7Bs_t%7D%20+%20b_y)"></p></li>
<li><p><strong>유사도(Score) 계산: Query와 Key의 내적</strong></p></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bscore%7D_i%20=%20Q%20%5Ccdot%20K_i%20%5Cquad%20%5Ctext%7Bfor%20%7D%20i%20=%201,%202,%203%0A"></p>
<p>혹은 전체를 벡터화하면: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bscores%7D%20=%20Q%20K%5ET%20%5Cquad%20%5Ctext%7B(Q:%201%C3%97d,%20K:%203%C3%97d%20%E2%86%92%20scores:%201%C3%973)%7D%0A"></p>
<blockquote class="blockquote">
<p>Scaled Dot-Product Attention에서는 보통 다음과 같이 스케일 조정:</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bscores%7D%20=%20%5Cfrac%7BQ%20K%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%0A"></p>
<ol start="2" type="1">
<li><strong>Softmax로 유사도 정규화 (Attention Weights)</strong></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_i%20=%20%5Cfrac%7B%5Cexp(%5Ctext%7Bscore%7D_i)%7D%7B%5Csum_%7Bj=1%7D%5E%7B3%7D%20%5Cexp(%5Ctext%7Bscore%7D_j)%7D%0A%5Cquad%20%5Ctext%7B(i%20=%201,%202,%203)%7D%0A"></p>
<p>또는 벡터 전체:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboldsymbol%7B%5Calpha%7D%20=%20%5Ctext%7Bsoftmax%7D(Q%20K%5ET)%0A"></p>
<hr>
<ol start="3" type="1">
<li><strong>각 Value 벡터에 가중치를 곱해 합산 (Attention Output)</strong></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BAttention%7D(Q,%20K,%20V)%20=%20%5Csum_%7Bi=1%7D%5E%7B3%7D%20%5Calpha_i%20V_i%20=%20%5Cboldsymbol%7B%5Calpha%7D%20%5Ccdot%20V%0A"></p>
<p>즉, 전체 수식은:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BAttention%7D(Q,%20K,%20V)%20=%20%5Ctext%7Bsoftmax%7D%5Cleft(%20%5Cfrac%7BQ%20K%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%20%5Cright)%20V%0A"></p>
<ol start="4" type="1">
<li><strong>디코더 hidden state와 결합 후 출력 계산</strong></li>
</ol>
<ul>
<li>context vector (a₁)와 디코더의 hidden state <img src="https://latex.codecogs.com/png.latex?s_t"> 결합:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctilde%7Bs_t%7D%20=%20%5Ctanh(W_c%20%5Ba_t;%20s_t%5D)%0A"></p>
<ul>
<li>최종 출력 (예측 단어 확률 분포):</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay_t%20=%20%5Ctext%7Bsoftmax%7D(W_y%20%5Ctilde%7Bs_t%7D%20+%20b_y)%0A"></p>
</section>
</section>
<section id="강점" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="강점"><span class="header-section-number">3.4</span> 강점</h2>
<ul>
<li>RNN계열 Seq2Seq 구조에 도입되어 기계 번역의 성능을 상당 부분 개선</li>
<li>후에, attention으로 모든 state에 접근하여 더 나은 성능을 보임 = Attention만으로도 성능 월등</li>
<li>결국, RNN은 필요하지 않게 되었음 = 모든 정보를 벡터화하여 저장하는 것이 아니라 중요한 정보만 저장하고 있으면 됨</li>
<li>후에 이를 바탕으로 발전된 기술이 Transformer (Attention is all you need.)</li>
</ul>
</section>
<section id="결론" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="결론"><span class="header-section-number">3.5</span> 결론</h2>
<p>본 문서에서는 Attention 메커니즘의 기본 원리와 작동 방식을 살펴보았다. Attention은 시퀀스 데이터 처리에서 중요한 정보에 집중할 수 있게 해주는 핵심적인 기술로, 기계 번역을 비롯한 다양한 자연어 처리 태스크에서 혁신적인 성능 향상을 가져왔다.</p>
<ul>
<li><strong>Attention의 핵심 원리 요약</strong>:
<ul>
<li>Attention은 Query, Key, Value 세 가지 요소를 기반으로 작동하며, Query와 Key의 유사도를 계산하여 Value에 대한 가중치를 결정한다.</li>
<li>Softmax를 통해 정규화된 가중치를 사용하여 중요한 정보에 더 집중할 수 있게 해주며, 이를 통해 문맥에 따른 적절한 정보 선택이 가능해진다.</li>
</ul></li>
<li><strong>RNN 기반 모델과의 관계 및 장점</strong>:
<ul>
<li>기존 RNN 기반 Seq2Seq 모델의 한계를 극복하여, 긴 시퀀스에서도 중요한 정보를 효과적으로 포착할 수 있게 되었다.</li>
<li>모든 입력 정보에 직접 접근할 수 있어 장기 의존성 문제를 해결하고, 더 정확한 번역과 생성이 가능해졌다.</li>
</ul></li>
<li><strong>Transformer로의 발전</strong>:
<ul>
<li>Attention 메커니즘의 성공은 RNN을 완전히 대체하는 Transformer 아키텍처의 등장으로 이어졌다.</li>
<li>“Attention is all you need”라는 명제가 증명되었듯이, Attention만으로도 뛰어난 성능을 보일 수 있음을 보여주었다.</li>
</ul></li>
</ul>
<p>결론적으로, Attention 메커니즘은 자연어 처리 분야에서 혁신적인 변화를 가져온 핵심 기술이다. RNN의 한계를 극복하고 Transformer의 등장을 이끌어냄으로써, 현대 자연어 처리의 기반을 마련했으며, 이는 BERT, GPT와 같은 혁신적인 모델들의 등장으로 이어졌다.</p>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-4-3.RNN_Subword.html</guid>
  <pubDate>Tue, 14 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 벡터화: GRU의 이해</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-6-0.transformer.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 순환 신경망(RNN)의 한계점인 장기 의존성 문제를 효과적으로 해결하기 위해 제안된 GRU(Gated Recurrent Unit)의 기본 원리와 구조를 소개한다. GRU는 LSTM(Long Short-Term Memory)과 유사한 성능을 보이면서도 내부 구조를 단순화하여 계산 효율성을 높인 모델이다.</p>
<p>주요 내용은 다음과 같다.</p>
<ul>
<li><strong>RNN의 장기 의존성 문제와 GRU의 등장</strong>:
<ul>
<li>기존 RNN은 시퀀스 길이가 길어질수록 과거의 중요 정보가 손실되는 장기 의존성 문제를 겪는다.</li>
<li>GRU는 이러한 문제를 해결하기 위해 LSTM과 마찬가지로 게이트 메커니즘을 사용하지만, 더 적은 수의 게이트로 구성된다.</li>
</ul></li>
<li><strong>GRU의 핵심 구성 요소 및 작동 원리</strong>:
<ul>
<li><strong>리셋 게이트 (Reset Gate, <img src="https://latex.codecogs.com/png.latex?r_t">)</strong>: 이전 시점의 은닉 상태(<img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D">)에서 어떤 정보를 무시하고 현재 입력(<img src="https://latex.codecogs.com/png.latex?x_t">)과 함께 새로운 후보 은닉 상태를 만들지 결정한다.</li>
<li><strong>업데이트 게이트 (Update Gate, <img src="https://latex.codecogs.com/png.latex?z_t">)</strong>: 이전 은닉 상태(<img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D">)의 정보를 얼마나 유지하고, 현재 계산된 후보 은닉 상태(<img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t">)의 정보를 얼마나 반영하여 새로운 은닉 상태(<img src="https://latex.codecogs.com/png.latex?h_t">)를 만들지 결정한다.</li>
<li>이 두 게이트는 시그모이드 함수를 통해 0과 1 사이의 값을 출력하며, 이를 통해 정보 흐름을 정교하게 제어한다. GRU는 LSTM과 달리 별도의 셀 상태(Cell State)를 사용하지 않고 은닉 상태(Hidden State)만으로 정보를 전달한다.</li>
</ul></li>
<li><strong>LSTM과의 비교</strong>:
<ul>
<li>GRU는 LSTM에 비해 게이트 수가 적고(2개 vs 3개), 파라미터 수도 적어 계산 비용이 낮고 학습 속도가 빠를 수 있다.</li>
<li>많은 경우 LSTM과 비슷한 성능을 보이며, 데이터셋의 크기가 작거나 특정 문제에서는 GRU가 더 나은 결과를 보이기도 한다.</li>
</ul></li>
<li><strong>의의</strong>: GRU는 장기 의존성 문제를 완화하여 긴 시퀀스에서도 효과적인 학습을 가능하게 하며, 자연어 처리, 음성 인식 등 다양한 분야에서 RNN 계열 모델의 중요한 선택지로 활용된다.</li>
</ul>
<p>이 문서를 통해 독자는 GRU가 어떻게 게이트 메커니즘을 통해 정보의 흐름을 제어하고 장기 기억을 가능하게 하는지에 대한 기본적인 이해를 얻을 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention
 
Transformer 이후 생성형 모델 발전 계열
├── Transformer 구조 (Vaswani et al., 2017)
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등</code></pre>
</section>
<section id="문맥을-고려한-벡터화-2018-현재-동적-임베딩" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩</h1>
<section id="gru-gated-recurrent-unit" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="gru-gated-recurrent-unit"><span class="header-section-number">3.1</span> GRU (Gated Recurrent Unit)</h2>
<p>GRU(Gated Recurrent Unit)는 2014년 조경현 교수 등이 제안한 순환 신경망의 한 종류로, LSTM(Long Short-Term Memory)과 마찬가지로 기존 RNN의 장기 의존성 문제를 해결하기 위해 설계되었다. GRU는 LSTM의 복잡한 구조를 단순화하면서도 유사한 성능을 내는 것을 목표로 하며, LSTM보다 적은 수의 게이트를 사용하여 계산 효율성을 높였다.</p>
<p>주요 특징은 다음과 같다:</p>
<ul>
<li>LSTM과 마찬가지로 장기 의존성 문제에 강인한 모습을 보인다.</li>
<li>LSTM에는 3개의 게이트(입력, 삭제, 출력 게이트)와 별도의 셀 상태(Cell State)가 있었던 반면, GRU는 <strong>리셋 게이트(Reset Gate)</strong>와 <strong>업데이트 게이트(Update Gate)</strong>라는 2개의 게이트만을 사용하며, 별도의 셀 상태 없이 은닉 상태(Hidden State)를 통해 정보를 전달한다.</li>
<li>이로 인해 LSTM보다 파라미터 수가 적어 일반적으로 학습 속도가 빠르고, 계산량이 적으며, 특히 데이터가 적은 경우 과적합(overfitting)에 대한 강점을 가질 수 있다.</li>
</ul>
<p>GRU의 핵심 아이디어는 각 시점에서 이전 정보를 얼마나 ’리셋’할지, 그리고 새로운 정보를 얼마나 ’업데이트’할지를 게이트를 통해 학습하여 조절하는 것이다.</p>
<section id="gru의-구조와-작동-원리" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="gru의-구조와-작동-원리"><span class="header-section-number">3.1.1</span> GRU의 구조와 작동 원리</h3>
<p>GRU는 현재 입력 <img src="https://latex.codecogs.com/png.latex?x_t"> 와 이전 시점의 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 을 받아 현재 시점의 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_t"> 를 출력한다. 이 과정은 다음의 두 가지 주요 게이트와 후보 은닉 상태 계산을 통해 이루어진다.</p>
<section id="리셋-게이트-reset-gate-r_t" class="level4" data-number="3.1.1.1">
<h4 data-number="3.1.1.1" class="anchored" data-anchor-id="리셋-게이트-reset-gate-r_t"><span class="header-section-number">3.1.1.1</span> 1. 리셋 게이트 (Reset Gate, <img src="https://latex.codecogs.com/png.latex?r_t"> )</h4>
<p>리셋 게이트는 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 의 정보를 얼마나 ‘잊을지’ 또는 ’무시할지’를 결정한다. 이 게이트는 현재 입력 <img src="https://latex.codecogs.com/png.latex?x_t">와 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 을 사용하여 계산된다.</p>
<p><img src="https://latex.codecogs.com/png.latex?r_t%20=%20%5Csigma(W_r%20x_t%20+%20U_r%20h_%7Bt-1%7D%20+%20b_r)"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?W_r,%20U_r"> : 리셋 게이트의 가중치 행렬</li>
<li><img src="https://latex.codecogs.com/png.latex?b_r"> : 리셋 게이트의 편향 벡터</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csigma"> : 시그모이드 함수 (출력값을 0과 1 사이로 제한하여 게이트의 열림/닫힘 정도를 결정)</li>
</ul>
<p>리셋 게이트의 출력 <img src="https://latex.codecogs.com/png.latex?r_t"> 는 0에 가까울수록 이전 은닉 상태의 정보를 많이 잊고(즉, 새로운 후보값 생성 시 이전 정보의 영향력을 줄임), 1에 가까울수록 많이 기억(활용)하게 된다. 이 <img src="https://latex.codecogs.com/png.latex?r_t"> 는 후보 은닉 상태 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 를 계산할 때 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 에 요소별 곱(element-wise product, <img src="https://latex.codecogs.com/png.latex?%5Codot"> )으로 적용되어, <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 의 어떤 부분을 새로운 후보 은닉 상태 계산에 사용할지 결정한다.</p>
</section>
<section id="업데이트-게이트-update-gate-z_t" class="level4" data-number="3.1.1.2">
<h4 data-number="3.1.1.2" class="anchored" data-anchor-id="업데이트-게이트-update-gate-z_t"><span class="header-section-number">3.1.1.2</span> 2. 업데이트 게이트 (Update Gate, <img src="https://latex.codecogs.com/png.latex?z_t"> )</h4>
<p>업데이트 게이트는 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 의 정보를 얼마나 현재 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_t">로 가져올지, 그리고 새로 계산된 후보 은닉 상태 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 의 정보를 얼마나 반영할지를 결정한다. LSTM의 삭제 게이트와 입력 게이트의 역할을 동시에 수행한다고 볼 수 있다.</p>
<p><img src="https://latex.codecogs.com/png.latex?z_t%20=%20%5Csigma(W_z%20x_t%20+%20U_z%20h_%7Bt-1%7D%20+%20b_z)"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?W_z,%20U_z"> : 업데이트 게이트의 가중치 행렬</li>
<li><img src="https://latex.codecogs.com/png.latex?b_z"> : 업데이트 게이트의 편향 벡터</li>
</ul>
<p>업데이트 게이트의 출력 <img src="https://latex.codecogs.com/png.latex?z_t"> 는 <img src="https://latex.codecogs.com/png.latex?h_t"> 를 계산할 때 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 에 곱해지는 가중치 역할을 하며, <img src="https://latex.codecogs.com/png.latex?(1-z_t)"> 는 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 에 곱해지는 가중치 역할을 한다. 즉, <img src="https://latex.codecogs.com/png.latex?z_t"> 가 1에 가까우면 후보 은닉 상태 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 의 정보를 많이 반영하고, <img src="https://latex.codecogs.com/png.latex?z_t"> 가 0에 가까우면 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 의 정보를 많이 유지한다.</p>
</section>
<section id="후보-은닉-상태-candidate-hidden-state-tildeh_t" class="level4" data-number="3.1.1.3">
<h4 data-number="3.1.1.3" class="anchored" data-anchor-id="후보-은닉-상태-candidate-hidden-state-tildeh_t"><span class="header-section-number">3.1.1.3</span> 3. 후보 은닉 상태 (Candidate Hidden State, <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> )</h4>
<p>후보 은닉 상태는 현재 시점의 정보를 담고 있는 새로운 은닉 값의 ’후보’이다. 이는 현재 입력 <img src="https://latex.codecogs.com/png.latex?x_t">와 리셋 게이트에 의해 조절된 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?(r_t%20%5Codot%20h_%7Bt-1%7D)"> 를 사용하여 계산된다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t%20=%20%5Ctanh(W_h%20x_t%20+%20U_h%20(r_t%20%5Codot%20h_%7Bt-1%7D)%20+%20b_h)"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?W_h,%20U_h"> : 후보 은닉 상태 계산을 위한 가중치 행렬</li>
<li><img src="https://latex.codecogs.com/png.latex?b_h"> : 후보 은닉 상태 계산을 위한 편향 벡터</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Ctanh"> : 하이퍼볼릭 탄젠트 함수 (출력값을 -1과 1 사이로 제한)</li>
</ul>
<p>리셋 게이트 <img src="https://latex.codecogs.com/png.latex?r_t"> 가 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 에 곱해짐으로써, <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 중 과거 정보 중 현재 필요한 부분만 선택적으로 활용하여 현재 입력 <img src="https://latex.codecogs.com/png.latex?x_t"> 와 함께 새로운 정보 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 를 구성하는 데 사용된다.</p>
</section>
<section id="최종-은닉-상태-final-hidden-state-h_t" class="level4" data-number="3.1.1.4">
<h4 data-number="3.1.1.4" class="anchored" data-anchor-id="최종-은닉-상태-final-hidden-state-h_t"><span class="header-section-number">3.1.1.4</span> 4. 최종 은닉 상태 (Final Hidden State, <img src="https://latex.codecogs.com/png.latex?h_t"> )</h4>
<p>최종적으로 현재 시점의 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_t"> 는 업데이트 게이트 <img src="https://latex.codecogs.com/png.latex?z_t"> 에 의해 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 과 후보 은닉 상태 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 가 조합되어 결정된다.</p>
<p><img src="https://latex.codecogs.com/png.latex?h_t%20=%20(1%20-%20z_t)%20%5Codot%20h_%7Bt-1%7D%20+%20z_t%20%5Codot%20%5Ctilde%7Bh%7D_t"></p>
<ul>
<li>이 식은 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 과 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 사이의 가중 평균과 유사한 형태로, <img src="https://latex.codecogs.com/png.latex?z_t"> 가 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 를 얼마나 반영할지, 그리고 <img src="https://latex.codecogs.com/png.latex?(1-z_t)"> 가 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 을 얼마나 유지할지를 결정한다.</li>
<li><img src="https://latex.codecogs.com/png.latex?z_t"> 가 1이면 <img src="https://latex.codecogs.com/png.latex?h_t%20=%20%5Ctilde%7Bh%7D_t"> (이전 상태 무시, 새 정보만 반영)</li>
<li><img src="https://latex.codecogs.com/png.latex?z_t"> 가 0이면 <img src="https://latex.codecogs.com/png.latex?h_t%20=%20h_%7Bt-1%7D"> (이전 상태 유지, 새 정보 무시)</li>
</ul>
<p>이러한 방식으로 GRU는 두 개의 게이트를 통해 정보의 흐름을 효과적으로 제어하며 장기 의존성 문제를 해결하려고 시도한다. LSTM에서 셀 상태와 은닉 상태를 분리했던 것과 달리, GRU는 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_t"> 하나로 이 두 가지 역할을 어느 정도 통합하여 수행한다.</p>
</section>
</section>
<section id="장기-의존성-문제-해결" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="장기-의존성-문제-해결"><span class="header-section-number">3.1.2</span> 장기 의존성 문제 해결</h3>
<p>GRU의 게이트 메커니즘은 RNN의 기울기 소실/폭주 문제를 완화하여 장기 의존성을 포착하는 데 도움을 준다.</p>
<ul>
<li><strong>업데이트 게이트 ( <img src="https://latex.codecogs.com/png.latex?z_t"> )</strong>: <img src="https://latex.codecogs.com/png.latex?z_t"> 값을 통해 이전 시점의 정보를 얼마나 유지할지 학습한다. 시퀀스에서 중요한 정보가 먼 과거에 있더라도, 네트워크는 <img src="https://latex.codecogs.com/png.latex?z_t"> 를 0에 가깝게 유지하여 해당 정보를 <img src="https://latex.codecogs.com/png.latex?h_t"> 로 계속 전달할 수 있다. 이는 장기적인 의존성을 유지하는 데 기여한다.</li>
<li><strong>리셋 게이트 ( <img src="https://latex.codecogs.com/png.latex?r_t"> )</strong>: <img src="https://latex.codecogs.com/png.latex?r_t"> 값을 통해 과거 정보 중 현재 예측에 불필요한 부분을 효과적으로 리셋(무시)할 수 있다. 이를 통해 현재 필요한 정보에 집중하고, 과거의 덜 중요한 정보로 인해 학습이 방해받는 것을 줄인다.</li>
</ul>
<p>이처럼 필요한 정보는 오래 유지하고, 불필요한 정보는 적절히 리셋함으로써 GRU는 긴 시퀀스에서도 안정적인 학습을 가능하게 한다.</p>
</section>
<section id="lstm과의-비교" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="lstm과의-비교"><span class="header-section-number">3.1.3</span> LSTM과의 비교</h3>
<p>GRU는 LSTM과 유사한 목적을 가지고 있지만 몇 가지 주요 차이점이 있다.</p>
<table class="table">
<colgroup>
<col style="width: 10%">
<col style="width: 44%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>특징</th>
<th>LSTM (Long Short-Term Memory)</th>
<th>GRU (Gated Recurrent Unit)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>게이트 수</strong></td>
<td>3개 (입력 게이트, 삭제 게이트, 출력 게이트)</td>
<td>2개 (리셋 게이트, 업데이트 게이트)</td>
</tr>
<tr class="even">
<td><strong>셀 상태</strong></td>
<td>별도의 셀 상태 ( <img src="https://latex.codecogs.com/png.latex?C_t"> )를 가짐 (장기 기억 담당)</td>
<td>별도의 셀 상태 없음 (은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_t"> 가 장기 기억과 현재 출력을 모두 담당)</td>
</tr>
<tr class="odd">
<td><strong>파라미터 수</strong></td>
<td>일반적으로 GRU보다 많음</td>
<td>일반적으로 LSTM보다 적음</td>
</tr>
<tr class="even">
<td><strong>계산 복잡도</strong></td>
<td>GRU보다 높음</td>
<td>LSTM보다 낮음 (학습 속도 빠를 수 있음)</td>
</tr>
<tr class="odd">
<td><strong>성능</strong></td>
<td>다양한 NLP 작업에서 강력한 성능 입증. 일반적으로 복잡한 데이터셋에서 유리</td>
<td>많은 경우 LSTM과 유사한 성능. 데이터가 적거나 특정 작업에서 더 나을 수 있음</td>
</tr>
<tr class="even">
<td><strong>정보 흐름</strong></td>
<td>셀 상태와 은닉 상태를 통해 정보 흐름 제어</td>
<td>은닉 상태와 두 게이트를 통해 정보 흐름 제어</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>구조적 차이</strong>: LSTM은 명시적인 메모리 셀( <img src="https://latex.codecogs.com/png.latex?C_t"> )을 사용하여 장기 정보를 저장하고, 세 개의 게이트로 이 셀과 은닉 상태( <img src="https://latex.codecogs.com/png.latex?h_t"> )를 제어한다. 반면 GRU는 셀 상태 없이 은닉 상태( <img src="https://latex.codecogs.com/png.latex?h_t"> )만으로 정보를 전달하며, 리셋 게이트와 업데이트 게이트 두 개로 정보 흐름을 제어한다. 업데이트 게이트가 LSTM의 삭제 게이트와 입력 게이트의 역할을 통합한 것으로 볼 수 있다.</li>
<li><strong>효율성</strong>: GRU는 파라미터 수가 적기 때문에 계산 효율성이 높고 학습 속도가 빠를 수 있으며, 특히 데이터셋의 크기가 작을 때 과적합을 피하는 데 유리할 수 있다.</li>
<li><strong>성능</strong>: 어떤 모델이 항상 우수하다고 단정하기는 어렵다. 문제의 성격, 데이터셋의 크기 및 복잡성, 하이퍼파라미터 튜닝 등에 따라 결과가 달라질 수 있다. 많은 연구에서 두 모델이 유사한 성능을 보인다고 보고되지만, 때로는 GRU가, 때로는 LSTM이 약간 더 나은 성능을 보이기도 한다.</li>
<li><strong>선택 기준</strong>: 일반적으로는 LSTM이 더 많은 표현력을 가질 수 있다고 여겨져 복잡한 문제에 우선 시도될 수 있지만, 계산 자원이 제한적이거나 빠른 학습이 필요할 경우, 또는 데이터가 충분하지 않을 경우에는 GRU가 좋은 대안이 될 수 있다. 실제 적용 시에는 두 모델을 모두 실험해보고 성능을 비교하는 것이 좋다.</li>
</ul>
</section>
<section id="양방향-gru-bidirectional-gru" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="양방향-gru-bidirectional-gru"><span class="header-section-number">3.1.4</span> 양방향 GRU (Bidirectional GRU)</h3>
<p>양방향 LSTM(BiLSTM)과 마찬가지로, 양방향 GRU(BiGRU)도 시퀀스 데이터 처리 시 과거와 미래의 문맥을 모두 고려하기 위해 사용된다. BiGRU는 순방향 GRU와 역방향 GRU 두 개를 병렬로 구성한다.</p>
<ul>
<li><strong>순방향 GRU</strong>: 입력 시퀀스를 처음부터 끝까지 순서대로 처리한다.</li>
<li><strong>역방향 GRU</strong>: 입력 시퀀스를 끝에서부터 처음까지 역순으로 처리한다.</li>
</ul>
<p>각 시점에서 두 GRU의 은닉 상태는 보통 연결(concatenation)되어 해당 시점의 최종적인 특징 표현으로 사용된다. 이를 통해 특정 시점의 단어나 요소에 대한 이해도를 높일 수 있다. 예를 들어, 문장에서 단어의 의미를 파악할 때 해당 단어의 앞뒤 단어들이 모두 중요한 역할을 하는 경우 BiGRU가 효과적이다.</p>
</section>
</section>
<section id="결론" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="결론"><span class="header-section-number">3.2</span> 결론</h2>
<p>본 문서에서는 RNN의 장기 의존성 문제를 해결하기 위해 설계된 GRU(Gated Recurrent Unit)의 핵심적인 구조와 작동 방식을 살펴보았다. GRU는 LSTM의 대안으로 제시되었으며, 더 단순한 구조와 적은 파라미터로도 유사하거나 때로는 더 나은 성능을 제공할 수 있음을 보여주었다.</p>
<ul>
<li><strong>GRU의 핵심 원리 요약</strong>:
<ul>
<li>GRU는 <strong>리셋 게이트</strong>와 <strong>업데이트 게이트</strong>라는 두 가지 게이트 메커니즘을 통해 정보의 흐름을 정교하게 제어한다. 리셋 게이트는 과거 정보 중 현재 예측에 덜 중요한 부분을 무시하도록 하고, 업데이트 게이트는 과거 정보와 현재 후보 정보 사이의 균형을 조절하여 다음 은닉 상태를 결정한다.</li>
<li>별도의 셀 상태 없이 은닉 상태만으로 장기 정보를 효과적으로 전달하며, 이는 LSTM에 비해 구조적 단순성과 계산 효율성을 가져다준다.</li>
</ul></li>
<li><strong>LSTM과의 관계 및 장점</strong>:
<ul>
<li>GRU는 LSTM보다 파라미터 수가 적어 학습 속도가 빠르고, 특히 데이터가 적은 환경에서 과적합의 위험을 줄일 수 있는 장점이 있다.</li>
<li>많은 자연어 처리 및 시퀀스 모델링 문제에서 LSTM과 대등한 성능을 보여주어, 모델 선택 시 중요한 고려 대상이 된다.</li>
</ul></li>
<li><strong>문맥 이해와 NLP에서의 중요성</strong>:
<ul>
<li>GRU는 장기 의존성을 효과적으로 처리할 수 있어 긴 시퀀스 데이터의 문맥적 의미를 파악하는 데 중요한 역할을 한다.</li>
<li>자연어 이해, 기계 번역, 음성 인식 등 다양한 NLP 태스크에서 순환 신경망의 핵심 구성 요소로 활용되며, 특히 계산 효율성이 중요하거나 빠른 프로토타이핑이 필요할 때 유용하다.</li>
</ul></li>
</ul>
<p>결론적으로, GRU는 LSTM과 함께 시퀀스 데이터, 특히 장기 의존성을 가진 데이터 처리에 있어 중요한 진보를 이룬 모델이다. 단순화된 게이트 메커니즘을 통해 정보의 선택적 기억과 업데이트를 가능하게 함으로써, 복잡한 패턴 학습 능력을 효과적으로 제공하며 다양한 응용 분야에서 그 가치를 입증하고 있다.</p>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-6-0.transformer.html</guid>
  <pubDate>Tue, 14 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>RNN 기반 언어 모델</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-4-0.RNN_language_model.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 RNN(순환 신경망)을 활용한 언어 모델의 기본 원리와 구현 방법을 설명한다. 언어 모델은 이전 단어들의 문맥을 바탕으로 다음에 올 단어를 예측하는 모델로, 자연어 생성과 이해에서 핵심적인 역할을 한다.</p>
<p>주요 내용은 다음과 같다:</p>
<ul>
<li><strong>RNN 언어 모델의 기본 구조</strong>:
<ul>
<li>이전 단어들로부터 다음 단어를 예측하는 순차적 구조</li>
<li>입력 길이가 가변적이며, 각 시점에서 현재 단어를 입력받아 다음 단어를 예측</li>
<li>수식: <img src="https://latex.codecogs.com/png.latex?x_1%20%5Crightarrow%20c_1%20%5Crightarrow%20y_1%20%5Crightarrow%20x_2%20%5Crightarrow%20c_2%20%5Crightarrow%20y_2%20%5Crightarrow%20%5Ccdots"></li>
</ul></li>
<li><strong>Teacher Forcing 학습 기법</strong>:
<ul>
<li>훈련 시에는 실제 정답 토큰을 디코더 입력으로 사용하는 방법</li>
<li>예측값을 반복 사용할 때 발생하는 오류 누적과 불안정성 문제를 해결</li>
<li>훈련 과정과 테스트 과정의 차이점: 훈련시엔 실제값 사용, 테스트시엔 이전 예측값 사용</li>
</ul></li>
<li><strong>모델 구조와 구현</strong>:
<ul>
<li>Embedding Layer, Hidden Layer, Output Layer로 구성</li>
<li>Output Layer에서 전체 어휘 크기만큼의 벡터로 다중 클래스 분류 수행</li>
<li>Softmax 함수를 통한 확률 분포 출력과 Cross Entropy Loss 사용</li>
</ul></li>
</ul>
<p>RNN 언어 모델은 자연스러운 텍스트 생성의 기초가 되며, 번역기나 챗봇 등 다양한 NLP 애플리케이션에서 활용된다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>RNN Language Model
├── Seq2Seq
├── Beam Search
├── Subword Tokenization
├── Attention
├── Transformer Encoder (Vaswani et al., 2017)
|   ├── Positional Encoding
|   ├── Multi-Head Attention
|   └── Feed Forward Neural Network
|
├── Transformer Decoder (Vaswani et al., 2017)
|
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
|
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
|
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등</code></pre>
</section>
<section id="rnn-기반-언어-모델" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> RNN 기반 언어 모델</h1>
<section id="언어-모델의-정의와-목적" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="언어-모델의-정의와-목적"><span class="header-section-number">3.1</span> 언어 모델의 정의와 목적</h2>
<p>언어 모델(Language Model)은 자연어의 확률적 성질을 모델링하는 것으로, 주어진 단어 시퀀스에 대해 다음 단어가 나올 확률을 예측한다. 수학적으로는 다음과 같이 표현할 수 있다:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(w_1,%20w_2,%20%5Cldots,%20w_n)%20=%20%5Cprod_%7Bi=1%7D%5E%7Bn%7D%20P(w_i%20%7C%20w_1,%20w_2,%20%5Cldots,%20w_%7Bi-1%7D)%0A"></p>
<p>여기서 <img src="https://latex.codecogs.com/png.latex?w_i"> 는 <img src="https://latex.codecogs.com/png.latex?i"> 번째 단어이고, <img src="https://latex.codecogs.com/png.latex?P(w_i%20%7C%20w_1,%20w_2,%20%5Cldots,%20w_%7Bi-1%7D)"> 는 이전 단어들이 주어졌을 때 현재 단어가 나올 조건부 확률이다.</p>
</section>
<section id="rnn을-이용한-구현" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="rnn을-이용한-구현"><span class="header-section-number">3.2</span> RNN을 이용한 구현</h2>
<p>RNN 언어 모델은 위의 조건부 확률 분포를 신경망으로 근사한다. 각 시점에서 이전 단어들의 정보를 은닉 상태(hidden state)에 누적하고, 이를 바탕으로 다음 단어를 예측한다.</p>
<section id="모델-구조와-정보-흐름" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="모델-구조와-정보-흐름"><span class="header-section-number">3.2.1</span> 모델 구조와 정보 흐름</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_1%20%5Crightarrow%20h_1%20%5Crightarrow%20y_1%20%5Crightarrow%20x_2%20%5Crightarrow%20h_2%20%5Crightarrow%20y_2%20%5Crightarrow%20%5Ccdots%20%5Crightarrow%20x_n%20%5Crightarrow%20h_n%20%5Crightarrow%20y_n%0A"></p>
<p>각 시점 <img src="https://latex.codecogs.com/png.latex?t"> 에서의 계산 과정:</p>
<ol type="1">
<li><strong>입력 임베딩</strong>: <img src="https://latex.codecogs.com/png.latex?e_t%20=%20%5Ctext%7BEmbedding%7D(x_t)"></li>
<li><strong>은닉 상태 갱신</strong>: <img src="https://latex.codecogs.com/png.latex?h_t%20=%20%5Ctanh(W_%7Bhh%7D%20h_%7Bt-1%7D%20+%20W_%7Bxh%7D%20e_t%20+%20b_h)"></li>
<li><strong>출력 계산</strong>: <img src="https://latex.codecogs.com/png.latex?o_t%20=%20W_%7Bho%7D%20h_t%20+%20b_o"></li>
<li><strong>확률 분포</strong>: <img src="https://latex.codecogs.com/png.latex?P(w_t%20%7C%20w_1,%20%5Cldots,%20w_%7Bt-1%7D)%20=%20%5Ctext%7Bsoftmax%7D(o_t)"></li>
</ol>
<p>여기서: - <img src="https://latex.codecogs.com/png.latex?W_%7Bhh%7D">: 은닉 상태 간 연결 가중치 - <img src="https://latex.codecogs.com/png.latex?W_%7Bxh%7D">: 입력-은닉 상태 연결 가중치<br>
- <img src="https://latex.codecogs.com/png.latex?W_%7Bho%7D">: 은닉 상태-출력 연결 가중치 - <img src="https://latex.codecogs.com/png.latex?b_h,%20b_o">: 편향 벡터</p>
</section>
<section id="구체적인-예시" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="구체적인-예시"><span class="header-section-number">3.2.2</span> 구체적인 예시</h3>
<p>예문: “what will the side effects of the drug be?”</p>
<p><strong>추론 과정 (테스트 시)</strong>: - <img src="https://latex.codecogs.com/png.latex?x_1%20=%20%5Ctext%7B%22what%22%7D"> → <img src="https://latex.codecogs.com/png.latex?h_1"> 계산 → <img src="https://latex.codecogs.com/png.latex?P(%5Ctext%7Bnext%20word%7D%20%7C%20%5Ctext%7B%22what%22%7D)"> → <img src="https://latex.codecogs.com/png.latex?y_1%20=%20%5Ctext%7B%22will%22%7D"> - <img src="https://latex.codecogs.com/png.latex?x_2%20=%20%5Ctext%7B%22will%22%7D"> → <img src="https://latex.codecogs.com/png.latex?h_2"> 계산 → <img src="https://latex.codecogs.com/png.latex?P(%5Ctext%7Bnext%20word%7D%20%7C%20%5Ctext%7B%22what%20will%22%7D)"> → <img src="https://latex.codecogs.com/png.latex?y_2%20=%20%5Ctext%7B%22the%22%7D"> - <img src="https://latex.codecogs.com/png.latex?x_3%20=%20%5Ctext%7B%22the%22%7D"> → <img src="https://latex.codecogs.com/png.latex?h_3"> 계산 → <img src="https://latex.codecogs.com/png.latex?P(%5Ctext%7Bnext%20word%7D%20%7C%20%5Ctext%7B%22what%20will%20the%22%7D)"> → <img src="https://latex.codecogs.com/png.latex?y_3%20=%20%5Ctext%7B%22side%22%7D"> - <img src="https://latex.codecogs.com/png.latex?x_4%20=%20%5Ctext%7B%22side%22%7D"> → <img src="https://latex.codecogs.com/png.latex?h_4"> 계산 → <img src="https://latex.codecogs.com/png.latex?P(%5Ctext%7Bnext%20word%7D%20%7C%20%5Ctext%7B%22what%20will%20the%20side%22%7D)"> → <img src="https://latex.codecogs.com/png.latex?y_4%20=%20%5Ctext%7B%22effects%22%7D"></p>
<p>이 과정에서 <img src="https://latex.codecogs.com/png.latex?h_t"> 는 시점 <img src="https://latex.codecogs.com/png.latex?t">까지의 모든 이전 단어들의 문맥 정보를 압축적으로 담고 있다.</p>
</section>
</section>
<section id="teacher-forcing-교사-강요" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="teacher-forcing-교사-강요"><span class="header-section-number">3.3</span> Teacher Forcing (교사 강요)</h2>
<section id="teacher-forcing의-개념과-필요성" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="teacher-forcing의-개념과-필요성"><span class="header-section-number">3.3.1</span> Teacher Forcing의 개념과 필요성</h3>
<p>Teacher Forcing은 시퀀스 생성 모델의 학습에서 디코더 입력으로 실제 정답 토큰을 사용하는 훈련 기법이다. 이 기법은 학습의 안정성과 효율성을 크게 향상시킨다.</p>
</section>
<section id="학습과-추론의-차이점" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="학습과-추론의-차이점"><span class="header-section-number">3.3.2</span> 학습과 추론의 차이점</h3>
<p><strong>학습 시 (Teacher Forcing 적용)</strong>: - 각 시점에서 실제 정답 단어를 입력으로 사용 - 모든 시점의 손실을 병렬적으로 계산 가능 - 안정적이고 빠른 학습</p>
<p><strong>추론 시 (자기회귀적 생성)</strong>: - 이전 시점의 예측 결과를 다음 시점의 입력으로 사용 - 순차적으로 단어를 생성 - 오류 누적 가능성 존재</p>
</section>
<section id="구체적인-학습-과정" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="구체적인-학습-과정"><span class="header-section-number">3.3.3</span> 구체적인 학습 과정</h3>
<p>예문: “what will the side effects of the drug be?”</p>
<p><strong>Teacher Forcing을 적용한 학습</strong>:</p>
<pre><code>입력 시퀀스: [&lt;SOS&gt;, what, will, the, side, effects, of, the, drug]
목표 시퀀스: [what, will, the, side, effects, of, the, drug, be]</code></pre>
<p>각 시점에서: - <img src="https://latex.codecogs.com/png.latex?t=1">: 입력 <code>&lt;SOS&gt;</code> → 예측 목표 <code>what</code> - <img src="https://latex.codecogs.com/png.latex?t=2">: 입력 <code>what</code> → 예측 목표 <code>will</code><br>
- <img src="https://latex.codecogs.com/png.latex?t=3">: 입력 <code>will</code> → 예측 목표 <code>the</code> - <img src="https://latex.codecogs.com/png.latex?t=4">: 입력 <code>the</code> → 예측 목표 <code>side</code> - <img src="https://latex.codecogs.com/png.latex?t=5">: 입력 <code>side</code> → 예측 목표 <code>effects</code></p>
</section>
<section id="teacher-forcing의-장단점" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4" class="anchored" data-anchor-id="teacher-forcing의-장단점"><span class="header-section-number">3.3.4</span> Teacher Forcing의 장단점</h3>
<p><strong>장점</strong>: - 학습 속도 향상: 모든 시점을 병렬 처리 가능 - 학습 안정성: 올바른 문맥 정보 제공으로 gradient 안정화 - 수렴 속도: 더 빠른 수렴과 안정적인 학습 곡선</p>
<p><strong>단점</strong>: - Exposure Bias: 학습 시와 추론 시의 입력 분포 차이 - 추론 시 오류 누적: 잘못된 예측이 후속 예측에 영향 - 일반화 문제: 실제 사용 환경과 학습 환경의 불일치</p>
</section>
<section id="실제-동작-예시" class="level3" data-number="3.3.5">
<h3 data-number="3.3.5" class="anchored" data-anchor-id="실제-동작-예시"><span class="header-section-number">3.3.5</span> 실제 동작 예시</h3>
<p><strong>Papago, ChatGPT 등의 실제 서비스</strong>에서는 다음과 같이 동작한다:</p>
<pre><code>사용자 입력: "What will the"
시스템 동작:
1. "What" → 다음 단어 예측 → "will" (확률: 0.8)
2. "What will" → 다음 단어 예측 → "the" (확률: 0.7)  
3. "What will the" → 다음 단어 예측 → "weather" (확률: 0.6)
4. 계속 진행...</code></pre>
</section>
</section>
<section id="모델-아키텍처와-구현" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="모델-아키텍처와-구현"><span class="header-section-number">3.4</span> 모델 아키텍처와 구현</h2>
<section id="전체-아키텍처" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="전체-아키텍처"><span class="header-section-number">3.4.1</span> 전체 아키텍처</h3>
<p>RNN 언어 모델은 다음 세 가지 주요 구성 요소로 이루어진다:</p>
<pre><code>Input → Embedding Layer → RNN Layer → Output Layer → Probability Distribution</code></pre>
</section>
<section id="각-레이어의-상세-구조" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="각-레이어의-상세-구조"><span class="header-section-number">3.4.2</span> 각 레이어의 상세 구조</h3>
<section id="embedding-layer" class="level4" data-number="3.4.2.1">
<h4 data-number="3.4.2.1" class="anchored" data-anchor-id="embedding-layer"><span class="header-section-number">3.4.2.1</span> Embedding Layer</h4>
<ul>
<li><strong>역할</strong>: 단어 인덱스를 고정 크기의 벡터로 변환</li>
<li><strong>수식</strong>: <img src="https://latex.codecogs.com/png.latex?e_t%20=%20E%5Bw_t%5D">, 여기서 <img src="https://latex.codecogs.com/png.latex?E%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BV%20%5Ctimes%20d%7D"></li>
<li><strong>파라미터</strong>:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?V">: 어휘 사전 크기 (Vocabulary size)</li>
<li><img src="https://latex.codecogs.com/png.latex?d">: 임베딩 차원수 (일반적으로 100-300차원)</li>
</ul></li>
</ul>
</section>
<section id="rnn-hidden-layer" class="level4" data-number="3.4.2.2">
<h4 data-number="3.4.2.2" class="anchored" data-anchor-id="rnn-hidden-layer"><span class="header-section-number">3.4.2.2</span> RNN Hidden Layer</h4>
<ul>
<li><strong>역할</strong>: 시퀀스의 문맥 정보를 누적하여 표현</li>
<li><strong>수식</strong>: <img src="https://latex.codecogs.com/png.latex?h_t%20=%20%5Ctanh(W_%7Bhh%7D%20h_%7Bt-1%7D%20+%20W_%7Bxh%7D%20e_t%20+%20b_h)"></li>
<li><strong>파라미터</strong>:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?W_%7Bhh%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BH%20%5Ctimes%20H%7D">: 은닉 상태 간 가중치</li>
<li><img src="https://latex.codecogs.com/png.latex?W_%7Bxh%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BH%20%5Ctimes%20d%7D">: 입력-은닉 가중치</li>
<li><img src="https://latex.codecogs.com/png.latex?H">: 은닉 상태 차원수 (일반적으로 128-512차원)</li>
</ul></li>
</ul>
</section>
<section id="output-layer" class="level4" data-number="3.4.2.3">
<h4 data-number="3.4.2.3" class="anchored" data-anchor-id="output-layer"><span class="header-section-number">3.4.2.3</span> Output Layer</h4>
<ul>
<li><strong>역할</strong>: 은닉 상태를 어휘 크기의 로짓 벡터로 변환</li>
<li><strong>수식</strong>: <img src="https://latex.codecogs.com/png.latex?o_t%20=%20W_%7Bho%7D%20h_t%20+%20b_o"></li>
<li><strong>파라미터</strong>: <img src="https://latex.codecogs.com/png.latex?W_%7Bho%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BV%20%5Ctimes%20H%7D">, <img src="https://latex.codecogs.com/png.latex?b_o%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BV%7D"></li>
</ul>
</section>
<section id="softmax-loss" class="level4" data-number="3.4.2.4">
<h4 data-number="3.4.2.4" class="anchored" data-anchor-id="softmax-loss"><span class="header-section-number">3.4.2.4</span> Softmax &amp; Loss</h4>
<ul>
<li><strong>확률 분포</strong>: <img src="https://latex.codecogs.com/png.latex?P(w_t%20%7C%20w_%7B%3Ct%7D)%20=%20%5Ctext%7Bsoftmax%7D(o_t)%20=%20%5Cfrac%7Be%5E%7Bo_t%5E%7B(i)%7D%7D%7D%7B%5Csum_%7Bj=1%7D%5E%7BV%7D%20e%5E%7Bo_t%5E%7B(j)%7D%7D%7D"></li>
<li><strong>손실 함수</strong>: <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%20=%20-%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Clog%20P(w_t%5E*%20%7C%20w_%7B%3Ct%7D)"></li>
</ul>
</section>
</section>
<section id="계산-복잡도와-메모리-요구사항" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="계산-복잡도와-메모리-요구사항"><span class="header-section-number">3.4.3</span> 계산 복잡도와 메모리 요구사항</h3>
<ul>
<li><strong>파라미터 수</strong>: <img src="https://latex.codecogs.com/png.latex?%7CE%7C%20+%20%7CW_%7Bhh%7D%7C%20+%20%7CW_%7Bxh%7D%7C%20+%20%7CW_%7Bho%7D%7C%20=%20V%20%5Ctimes%20d%20+%20H%5E2%20+%20H%20%5Ctimes%20d%20+%20V%20%5Ctimes%20H"></li>
<li><strong>시간 복잡도</strong>: <img src="https://latex.codecogs.com/png.latex?O(T%20%5Ctimes%20(H%5E2%20+%20H%20%5Ctimes%20V))"> (시퀀스 길이 <img src="https://latex.codecogs.com/png.latex?T">에 대해)</li>
<li><strong>공간 복잡도</strong>: <img src="https://latex.codecogs.com/png.latex?O(H%20+%20V)"> (배치 크기 1 기준)</li>
</ul>
</section>
<section id="실제-구현-고려사항" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="실제-구현-고려사항"><span class="header-section-number">3.4.4</span> 실제 구현 고려사항</h3>
<section id="perplexity-혼란도" class="level4" data-number="3.4.4.1">
<h4 data-number="3.4.4.1" class="anchored" data-anchor-id="perplexity-혼란도"><span class="header-section-number">3.4.4.1</span> Perplexity (혼란도)</h4>
<p>언어 모델의 성능은 주로 Perplexity로 측정된다:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BPPL%7D%20=%20%5Cexp%5Cleft(-%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Clog%20P(w_t%20%7C%20w_%7B%3Ct%7D)%5Cright)"></p>
<p>낮은 Perplexity 값이 더 좋은 성능을 의미한다.</p>
</section>
<section id="샘플링-기법" class="level4" data-number="3.4.4.2">
<h4 data-number="3.4.4.2" class="anchored" data-anchor-id="샘플링-기법"><span class="header-section-number">3.4.4.2</span> 샘플링 기법</h4>
<p>추론 시 다음 단어 선택 방법: - <strong>Greedy Decoding</strong>: <img src="https://latex.codecogs.com/png.latex?%5Carg%5Cmax_w%20P(w%20%7C%20w_%7B%3Ct%7D)"> - <strong>Temperature Sampling</strong>: <img src="https://latex.codecogs.com/png.latex?P(w)%20=%20%5Cfrac%7Be%5E%7Bo_w/%5Ctau%7D%7D%7B%5Csum_j%20e%5E%7Bo_j/%5Ctau%7D%7D"> (τ는 temperature) - <strong>Top-k Sampling</strong>: 상위 k개 후보 중에서 샘플링</p>
</section>
</section>
</section>
<section id="결론" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="결론"><span class="header-section-number">3.5</span> 결론</h2>
<p>RNN 기반 언어 모델은 자연어 처리에서 텍스트 생성과 이해의 기초가 되는 중요한 모델이다. 이전 단어들의 순차적 정보를 활용하여 다음 단어를 예측하는 간단하면서도 효과적인 구조를 가지고 있다.</p>
<ul>
<li><strong>핵심 특징 요약</strong>:
<ul>
<li>가변 길이 입력을 처리할 수 있는 순환 구조</li>
<li>각 시점에서 이전 문맥 정보를 누적하여 다음 단어 예측</li>
<li>Teacher Forcing을 통한 안정적인 학습 과정</li>
</ul></li>
<li><strong>Teacher Forcing의 중요성</strong>:
<ul>
<li>훈련 시 정답 토큰 사용으로 학습 안정성 확보</li>
<li>오류 누적 방지와 학습 효율성 향상</li>
<li>실제 서비스에서는 이전 예측값을 사용하는 자기회귀적 생성</li>
</ul></li>
<li><strong>실용적 의의</strong>:
<ul>
<li>기계 번역, 텍스트 요약, 대화 시스템 등에 광범위하게 활용</li>
<li>현대 언어 모델들의 기초 개념 제공</li>
<li>Embedding-Hidden-Output 구조의 표준 패턴 확립</li>
</ul></li>
</ul>
<p>RNN 언어 모델은 비교적 단순한 구조임에도 불구하고 자연어의 순차적 특성을 효과적으로 모델링할 수 있어, 이후 등장한 LSTM, GRU, Transformer 등 더 복잡한 모델들의 토대가 되었다. 현재도 많은 NLP 애플리케이션에서 기본 구성 요소로 활용되고 있으며, 언어 모델의 기본 원리를 이해하는 데 필수적인 개념이다.</p>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-4-0.RNN_language_model.html</guid>
  <pubDate>Mon, 13 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 벡터화: GRU의 이해</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-3-4.nn_contextual_ELMo.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 순환 신경망(RNN)의 한계점인 장기 의존성 문제를 효과적으로 해결하기 위해 제안된 GRU(Gated Recurrent Unit)의 기본 원리와 구조를 소개한다. GRU는 LSTM(Long Short-Term Memory)과 유사한 성능을 보이면서도 내부 구조를 단순화하여 계산 효율성을 높인 모델이다.</p>
<p>주요 내용은 다음과 같다.</p>
<ul>
<li><strong>RNN의 장기 의존성 문제와 GRU의 등장</strong>:
<ul>
<li>기존 RNN은 시퀀스 길이가 길어질수록 과거의 중요 정보가 손실되는 장기 의존성 문제를 겪는다.</li>
<li>GRU는 이러한 문제를 해결하기 위해 LSTM과 마찬가지로 게이트 메커니즘을 사용하지만, 더 적은 수의 게이트로 구성된다.</li>
</ul></li>
<li><strong>GRU의 핵심 구성 요소 및 작동 원리</strong>:
<ul>
<li><strong>리셋 게이트 (Reset Gate, <img src="https://latex.codecogs.com/png.latex?r_t">)</strong>: 이전 시점의 은닉 상태(<img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D">)에서 어떤 정보를 무시하고 현재 입력(<img src="https://latex.codecogs.com/png.latex?x_t">)과 함께 새로운 후보 은닉 상태를 만들지 결정한다.</li>
<li><strong>업데이트 게이트 (Update Gate, <img src="https://latex.codecogs.com/png.latex?z_t">)</strong>: 이전 은닉 상태(<img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D">)의 정보를 얼마나 유지하고, 현재 계산된 후보 은닉 상태(<img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t">)의 정보를 얼마나 반영하여 새로운 은닉 상태(<img src="https://latex.codecogs.com/png.latex?h_t">)를 만들지 결정한다.</li>
<li>이 두 게이트는 시그모이드 함수를 통해 0과 1 사이의 값을 출력하며, 이를 통해 정보 흐름을 정교하게 제어한다. GRU는 LSTM과 달리 별도의 셀 상태(Cell State)를 사용하지 않고 은닉 상태(Hidden State)만으로 정보를 전달한다.</li>
</ul></li>
<li><strong>LSTM과의 비교</strong>:
<ul>
<li>GRU는 LSTM에 비해 게이트 수가 적고(2개 vs 3개), 파라미터 수도 적어 계산 비용이 낮고 학습 속도가 빠를 수 있다.</li>
<li>많은 경우 LSTM과 비슷한 성능을 보이며, 데이터셋의 크기가 작거나 특정 문제에서는 GRU가 더 나은 결과를 보이기도 한다.</li>
</ul></li>
<li><strong>의의</strong>: GRU는 장기 의존성 문제를 완화하여 긴 시퀀스에서도 효과적인 학습을 가능하게 하며, 자연어 처리, 음성 인식 등 다양한 분야에서 RNN 계열 모델의 중요한 선택지로 활용된다.</li>
</ul>
<p>이 문서를 통해 독자는 GRU가 어떻게 게이트 메커니즘을 통해 정보의 흐름을 제어하고 장기 기억을 가능하게 하는지에 대한 기본적인 이해를 얻을 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention
</code></pre>
</section>
<section id="문맥을-고려한-벡터화-2018-현재-동적-임베딩" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩</h1>
<section id="elmo-embedding-from-language-models-2018" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="elmo-embedding-from-language-models-2018"><span class="header-section-number">3.1</span> ELMo (Embedding from Language Models, 2018)</h2>
<ul>
<li>ELMo 수식: <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BELMo%7D_k%5E%7Btask%7D%20=%20%5Cgamma%5E%7Btask%7D%20%5Csum_%7Bj=0%7D%5EL%20s_j%5E%7Btask%7D%20%5Cmathbf%7Bh%7D_%7Bk,j%7D%5E%7BLM%7D">
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bh%7D_%7Bk,j%7D%5E%7BLM%7D">: 각 레이어의 hidden state</strong></li>
</ul>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예시: 3층 BiLSTM에서 "bank" 단어 (k번째 위치)</span></span>
<span id="cb2-2">h_{bank,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>} <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> character_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bank"</span>)     <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 레이어 0 (입력)</span></span>
<span id="cb2-3">h_{bank,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>} <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> first_LSTM_layer_output        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 레이어 1  </span></span>
<span id="cb2-4">h_{bank,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>} <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> second_LSTM_layer_output       <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 레이어 2</span></span>
<span id="cb2-5">h_{bank,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>} <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> third_LSTM_layer_output        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 레이어 3 (최상위)</span></span></code></pre></div>
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?s_j%5E%7Btask%7D">: 학습 가능한 가중치</strong>
<ul>
<li>각 레이어의 중요도를 태스크별로 학습</li>
<li>문법적 태스크 → 낮은 레이어 중시</li>
<li>의미적 태스크 → 높은 레이어 중시</li>
</ul></li>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Cgamma%5E%7Btask%7D">: 전체 스케일 조정</strong>
<ul>
<li>ELMo 벡터의 전체적인 크기 조정</li>
</ul></li>
</ul></li>
<li>계산 예시</li>
</ul>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "bank" 단어의 ELMo 벡터 (감정 분석 태스크)</span></span>
<span id="cb3-2">h_0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문자 레벨</span></span>
<span id="cb3-3">h_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 낮은 레벨 (문법적)  </span></span>
<span id="cb3-4">h_2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 높은 레벨 (의미적)</span></span>
<span id="cb3-5"></span>
<span id="cb3-6">s_0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문자 레벨 가중치 (낮음)</span></span>
<span id="cb3-7">s_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문법 레벨 가중치  </span></span>
<span id="cb3-8">s_2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 의미 레벨 가중치 (높음)</span></span>
<span id="cb3-9"></span>
<span id="cb3-10">ELMo_bank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> γ × (s_0×h_0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> s_1×h_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> s_2×h_2)</span>
<span id="cb3-11">          <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span> × (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>×[<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>×[<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>×[<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>])</span>
<span id="cb3-12">          <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span> × [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.55</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.65</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>]</span>
<span id="cb3-13">          <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>]</span></code></pre></div>
<ul>
<li>양방향 정보의 중요성
<ul>
<li><strong>Forward만 사용할 경우:</strong></li>
</ul>
<pre><code>"The bank was closed because of ___"
→ "bank"를 이해할 때 "The"만 참고</code></pre>
<ul>
<li><strong>Backward까지 사용할 경우:</strong></li>
</ul>
<pre><code>"The bank was closed because of ___"
→ "bank"를 이해할 때 "was closed" 정보도 참고
→ 금융 기관으로 해석 가능성 증가</code></pre>
<h2 id="결론" data-number="3.2" class="anchored"><span class="header-section-number">3.2</span> 결론</h2></li>
</ul>
<p>본 문서에서는 RNN의 장기 의존성 문제를 해결하기 위해 설계된 GRU(Gated Recurrent Unit)의 핵심적인 구조와 작동 방식을 살펴보았다. GRU는 LSTM의 대안으로 제시되었으며, 더 단순한 구조와 적은 파라미터로도 유사하거나 때로는 더 나은 성능을 제공할 수 있음을 보여주었다.</p>
<ul>
<li><strong>GRU의 핵심 원리 요약</strong>:
<ul>
<li>GRU는 <strong>리셋 게이트</strong>와 <strong>업데이트 게이트</strong>라는 두 가지 게이트 메커니즘을 통해 정보의 흐름을 정교하게 제어한다. 리셋 게이트는 과거 정보 중 현재 예측에 덜 중요한 부분을 무시하도록 하고, 업데이트 게이트는 과거 정보와 현재 후보 정보 사이의 균형을 조절하여 다음 은닉 상태를 결정한다.</li>
<li>별도의 셀 상태 없이 은닉 상태만으로 장기 정보를 효과적으로 전달하며, 이는 LSTM에 비해 구조적 단순성과 계산 효율성을 가져다준다.</li>
</ul></li>
<li><strong>LSTM과의 관계 및 장점</strong>:
<ul>
<li>GRU는 LSTM보다 파라미터 수가 적어 학습 속도가 빠르고, 특히 데이터가 적은 환경에서 과적합의 위험을 줄일 수 있는 장점이 있다.</li>
<li>많은 자연어 처리 및 시퀀스 모델링 문제에서 LSTM과 대등한 성능을 보여주어, 모델 선택 시 중요한 고려 대상이 된다.</li>
</ul></li>
<li><strong>문맥 이해와 NLP에서의 중요성</strong>:
<ul>
<li>GRU는 장기 의존성을 효과적으로 처리할 수 있어 긴 시퀀스 데이터의 문맥적 의미를 파악하는 데 중요한 역할을 한다.</li>
<li>자연어 이해, 기계 번역, 음성 인식 등 다양한 NLP 태스크에서 순환 신경망의 핵심 구성 요소로 활용되며, 특히 계산 효율성이 중요하거나 빠른 프로토타이핑이 필요할 때 유용하다.</li>
</ul></li>
</ul>
<p>결론적으로, GRU는 LSTM과 함께 시퀀스 데이터, 특히 장기 의존성을 가진 데이터 처리에 있어 중요한 진보를 이룬 모델이다. 단순화된 게이트 메커니즘을 통해 정보의 선택적 기억과 업데이트를 가능하게 함으로써, 복잡한 패턴 학습 능력을 효과적으로 제공하며 다양한 응용 분야에서 그 가치를 입증하고 있다.</p>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-3-4.nn_contextual_ELMo.html</guid>
  <pubDate>Sun, 12 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 벡터화: GRU의 이해</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-3-3.nn_contextual_GRU.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 순환 신경망(RNN)의 한계점인 장기 의존성 문제를 효과적으로 해결하기 위해 제안된 GRU(Gated Recurrent Unit)의 기본 원리와 구조를 소개한다. GRU는 LSTM(Long Short-Term Memory)과 유사한 성능을 보이면서도 내부 구조를 단순화하여 계산 효율성을 높인 모델이다.</p>
<p>주요 내용은 다음과 같다.</p>
<ul>
<li><strong>RNN의 장기 의존성 문제와 GRU의 등장</strong>:
<ul>
<li>기존 RNN은 시퀀스 길이가 길어질수록 과거의 중요 정보가 손실되는 장기 의존성 문제를 겪는다.</li>
<li>GRU는 이러한 문제를 해결하기 위해 LSTM과 마찬가지로 게이트 메커니즘을 사용하지만, 더 적은 수의 게이트로 구성된다.</li>
</ul></li>
<li><strong>GRU의 핵심 구성 요소 및 작동 원리</strong>:
<ul>
<li><strong>리셋 게이트 (Reset Gate, <img src="https://latex.codecogs.com/png.latex?r_t">)</strong>: 이전 시점의 은닉 상태(<img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D">)에서 어떤 정보를 무시하고 현재 입력(<img src="https://latex.codecogs.com/png.latex?x_t">)과 함께 새로운 후보 은닉 상태를 만들지 결정한다.</li>
<li><strong>업데이트 게이트 (Update Gate, <img src="https://latex.codecogs.com/png.latex?z_t">)</strong>: 이전 은닉 상태(<img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D">)의 정보를 얼마나 유지하고, 현재 계산된 후보 은닉 상태(<img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t">)의 정보를 얼마나 반영하여 새로운 은닉 상태(<img src="https://latex.codecogs.com/png.latex?h_t">)를 만들지 결정한다.</li>
<li>이 두 게이트는 시그모이드 함수를 통해 0과 1 사이의 값을 출력하며, 이를 통해 정보 흐름을 정교하게 제어한다. GRU는 LSTM과 달리 별도의 셀 상태(Cell State)를 사용하지 않고 은닉 상태(Hidden State)만으로 정보를 전달한다.</li>
</ul></li>
<li><strong>LSTM과의 비교</strong>:
<ul>
<li>GRU는 LSTM에 비해 게이트 수가 적고(2개 vs 3개), 파라미터 수도 적어 계산 비용이 낮고 학습 속도가 빠를 수 있다.</li>
<li>많은 경우 LSTM과 비슷한 성능을 보이며, 데이터셋의 크기가 작거나 특정 문제에서는 GRU가 더 나은 결과를 보이기도 한다.</li>
</ul></li>
<li><strong>의의</strong>: GRU는 장기 의존성 문제를 완화하여 긴 시퀀스에서도 효과적인 학습을 가능하게 하며, 자연어 처리, 음성 인식 등 다양한 분야에서 RNN 계열 모델의 중요한 선택지로 활용된다.</li>
</ul>
<p>이 문서를 통해 독자는 GRU가 어떻게 게이트 메커니즘을 통해 정보의 흐름을 제어하고 장기 기억을 가능하게 하는지에 대한 기본적인 이해를 얻을 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention</code></pre>
</section>
<section id="문맥을-고려한-벡터화-2018-현재-동적-임베딩" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩</h1>
<section id="gru-gated-recurrent-unit" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="gru-gated-recurrent-unit"><span class="header-section-number">3.1</span> GRU (Gated Recurrent Unit)</h2>
<p>GRU(Gated Recurrent Unit)는 2014년 조경현 교수 등이 제안한 순환 신경망의 한 종류로, LSTM(Long Short-Term Memory)과 마찬가지로 기존 RNN의 장기 의존성 문제를 해결하기 위해 설계되었다. GRU는 LSTM의 복잡한 구조를 단순화하면서도 유사한 성능을 내는 것을 목표로 하며, LSTM보다 적은 수의 게이트를 사용하여 계산 효율성을 높였다.</p>
<p>주요 특징은 다음과 같다:</p>
<ul>
<li>LSTM과 마찬가지로 장기 의존성 문제에 강인한 모습을 보인다.</li>
<li>LSTM에는 3개의 게이트(입력, 삭제, 출력 게이트)와 별도의 셀 상태(Cell State)가 있었던 반면, GRU는 <strong>리셋 게이트(Reset Gate)</strong>와 <strong>업데이트 게이트(Update Gate)</strong>라는 2개의 게이트만을 사용하며, 별도의 셀 상태 없이 은닉 상태(Hidden State)를 통해 정보를 전달한다.</li>
<li>이로 인해 LSTM보다 파라미터 수가 적어 일반적으로 학습 속도가 빠르고, 계산량이 적으며, 특히 데이터가 적은 경우 과적합(overfitting)에 대한 강점을 가질 수 있다.</li>
</ul>
<p>GRU의 핵심 아이디어는 각 시점에서 이전 정보를 얼마나 ’리셋’할지, 그리고 새로운 정보를 얼마나 ’업데이트’할지를 게이트를 통해 학습하여 조절하는 것이다.</p>
<section id="gru의-구조와-작동-원리" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="gru의-구조와-작동-원리"><span class="header-section-number">3.1.1</span> GRU의 구조와 작동 원리</h3>
<p>GRU는 현재 입력 <img src="https://latex.codecogs.com/png.latex?x_t"> 와 이전 시점의 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 을 받아 현재 시점의 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_t"> 를 출력한다. 이 과정은 다음의 두 가지 주요 게이트와 후보 은닉 상태 계산을 통해 이루어진다.</p>
<section id="리셋-게이트-reset-gate-r_t" class="level4" data-number="3.1.1.1">
<h4 data-number="3.1.1.1" class="anchored" data-anchor-id="리셋-게이트-reset-gate-r_t"><span class="header-section-number">3.1.1.1</span> 1. 리셋 게이트 (Reset Gate, <img src="https://latex.codecogs.com/png.latex?r_t"> )</h4>
<p>리셋 게이트는 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 의 정보를 얼마나 ‘잊을지’ 또는 ’무시할지’를 결정한다. 이 게이트는 현재 입력 <img src="https://latex.codecogs.com/png.latex?x_t">와 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 을 사용하여 계산된다.</p>
<p><img src="https://latex.codecogs.com/png.latex?r_t%20=%20%5Csigma(W_r%20x_t%20+%20U_r%20h_%7Bt-1%7D%20+%20b_r)"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?W_r,%20U_r"> : 리셋 게이트의 가중치 행렬</li>
<li><img src="https://latex.codecogs.com/png.latex?b_r"> : 리셋 게이트의 편향 벡터</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csigma"> : 시그모이드 함수 (출력값을 0과 1 사이로 제한하여 게이트의 열림/닫힘 정도를 결정)</li>
</ul>
<p>리셋 게이트의 출력 <img src="https://latex.codecogs.com/png.latex?r_t"> 는 0에 가까울수록 이전 은닉 상태의 정보를 많이 잊고(즉, 새로운 후보값 생성 시 이전 정보의 영향력을 줄임), 1에 가까울수록 많이 기억(활용)하게 된다. 이 <img src="https://latex.codecogs.com/png.latex?r_t"> 는 후보 은닉 상태 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 를 계산할 때 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 에 요소별 곱(element-wise product, <img src="https://latex.codecogs.com/png.latex?%5Codot"> )으로 적용되어, <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 의 어떤 부분을 새로운 후보 은닉 상태 계산에 사용할지 결정한다.</p>
</section>
<section id="업데이트-게이트-update-gate-z_t" class="level4" data-number="3.1.1.2">
<h4 data-number="3.1.1.2" class="anchored" data-anchor-id="업데이트-게이트-update-gate-z_t"><span class="header-section-number">3.1.1.2</span> 2. 업데이트 게이트 (Update Gate, <img src="https://latex.codecogs.com/png.latex?z_t"> )</h4>
<p>업데이트 게이트는 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 의 정보를 얼마나 현재 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_t">로 가져올지, 그리고 새로 계산된 후보 은닉 상태 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 의 정보를 얼마나 반영할지를 결정한다. LSTM의 삭제 게이트와 입력 게이트의 역할을 동시에 수행한다고 볼 수 있다.</p>
<p><img src="https://latex.codecogs.com/png.latex?z_t%20=%20%5Csigma(W_z%20x_t%20+%20U_z%20h_%7Bt-1%7D%20+%20b_z)"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?W_z,%20U_z"> : 업데이트 게이트의 가중치 행렬</li>
<li><img src="https://latex.codecogs.com/png.latex?b_z"> : 업데이트 게이트의 편향 벡터</li>
</ul>
<p>업데이트 게이트의 출력 <img src="https://latex.codecogs.com/png.latex?z_t"> 는 <img src="https://latex.codecogs.com/png.latex?h_t"> 를 계산할 때 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 에 곱해지는 가중치 역할을 하며, <img src="https://latex.codecogs.com/png.latex?(1-z_t)"> 는 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 에 곱해지는 가중치 역할을 한다. 즉, <img src="https://latex.codecogs.com/png.latex?z_t"> 가 1에 가까우면 후보 은닉 상태 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 의 정보를 많이 반영하고, <img src="https://latex.codecogs.com/png.latex?z_t"> 가 0에 가까우면 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 의 정보를 많이 유지한다.</p>
</section>
<section id="후보-은닉-상태-candidate-hidden-state-tildeh_t" class="level4" data-number="3.1.1.3">
<h4 data-number="3.1.1.3" class="anchored" data-anchor-id="후보-은닉-상태-candidate-hidden-state-tildeh_t"><span class="header-section-number">3.1.1.3</span> 3. 후보 은닉 상태 (Candidate Hidden State, <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> )</h4>
<p>후보 은닉 상태는 현재 시점의 정보를 담고 있는 새로운 은닉 값의 ’후보’이다. 이는 현재 입력 <img src="https://latex.codecogs.com/png.latex?x_t">와 리셋 게이트에 의해 조절된 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?(r_t%20%5Codot%20h_%7Bt-1%7D)"> 를 사용하여 계산된다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t%20=%20%5Ctanh(W_h%20x_t%20+%20U_h%20(r_t%20%5Codot%20h_%7Bt-1%7D)%20+%20b_h)"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?W_h,%20U_h"> : 후보 은닉 상태 계산을 위한 가중치 행렬</li>
<li><img src="https://latex.codecogs.com/png.latex?b_h"> : 후보 은닉 상태 계산을 위한 편향 벡터</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Ctanh"> : 하이퍼볼릭 탄젠트 함수 (출력값을 -1과 1 사이로 제한)</li>
</ul>
<p>리셋 게이트 <img src="https://latex.codecogs.com/png.latex?r_t"> 가 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 에 곱해짐으로써, <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 중 과거 정보 중 현재 필요한 부분만 선택적으로 활용하여 현재 입력 <img src="https://latex.codecogs.com/png.latex?x_t"> 와 함께 새로운 정보 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 를 구성하는 데 사용된다.</p>
</section>
<section id="최종-은닉-상태-final-hidden-state-h_t" class="level4" data-number="3.1.1.4">
<h4 data-number="3.1.1.4" class="anchored" data-anchor-id="최종-은닉-상태-final-hidden-state-h_t"><span class="header-section-number">3.1.1.4</span> 4. 최종 은닉 상태 (Final Hidden State, <img src="https://latex.codecogs.com/png.latex?h_t"> )</h4>
<p>최종적으로 현재 시점의 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_t"> 는 업데이트 게이트 <img src="https://latex.codecogs.com/png.latex?z_t"> 에 의해 이전 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 과 후보 은닉 상태 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 가 조합되어 결정된다.</p>
<p><img src="https://latex.codecogs.com/png.latex?h_t%20=%20(1%20-%20z_t)%20%5Codot%20h_%7Bt-1%7D%20+%20z_t%20%5Codot%20%5Ctilde%7Bh%7D_t"></p>
<ul>
<li>이 식은 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 과 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 사이의 가중 평균과 유사한 형태로, <img src="https://latex.codecogs.com/png.latex?z_t"> 가 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bh%7D_t"> 를 얼마나 반영할지, 그리고 <img src="https://latex.codecogs.com/png.latex?(1-z_t)"> 가 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 을 얼마나 유지할지를 결정한다.</li>
<li><img src="https://latex.codecogs.com/png.latex?z_t"> 가 1이면 <img src="https://latex.codecogs.com/png.latex?h_t%20=%20%5Ctilde%7Bh%7D_t"> (이전 상태 무시, 새 정보만 반영)</li>
<li><img src="https://latex.codecogs.com/png.latex?z_t"> 가 0이면 <img src="https://latex.codecogs.com/png.latex?h_t%20=%20h_%7Bt-1%7D"> (이전 상태 유지, 새 정보 무시)</li>
</ul>
<p>이러한 방식으로 GRU는 두 개의 게이트를 통해 정보의 흐름을 효과적으로 제어하며 장기 의존성 문제를 해결하려고 시도한다. LSTM에서 셀 상태와 은닉 상태를 분리했던 것과 달리, GRU는 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_t"> 하나로 이 두 가지 역할을 어느 정도 통합하여 수행한다.</p>
</section>
</section>
<section id="장기-의존성-문제-해결" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="장기-의존성-문제-해결"><span class="header-section-number">3.1.2</span> 장기 의존성 문제 해결</h3>
<p>GRU의 게이트 메커니즘은 RNN의 기울기 소실/폭주 문제를 완화하여 장기 의존성을 포착하는 데 도움을 준다.</p>
<ul>
<li><strong>업데이트 게이트 ( <img src="https://latex.codecogs.com/png.latex?z_t"> )</strong>: <img src="https://latex.codecogs.com/png.latex?z_t"> 값을 통해 이전 시점의 정보를 얼마나 유지할지 학습한다. 시퀀스에서 중요한 정보가 먼 과거에 있더라도, 네트워크는 <img src="https://latex.codecogs.com/png.latex?z_t"> 를 0에 가깝게 유지하여 해당 정보를 <img src="https://latex.codecogs.com/png.latex?h_t"> 로 계속 전달할 수 있다. 이는 장기적인 의존성을 유지하는 데 기여한다.</li>
<li><strong>리셋 게이트 ( <img src="https://latex.codecogs.com/png.latex?r_t"> )</strong>: <img src="https://latex.codecogs.com/png.latex?r_t"> 값을 통해 과거 정보 중 현재 예측에 불필요한 부분을 효과적으로 리셋(무시)할 수 있다. 이를 통해 현재 필요한 정보에 집중하고, 과거의 덜 중요한 정보로 인해 학습이 방해받는 것을 줄인다.</li>
</ul>
<p>이처럼 필요한 정보는 오래 유지하고, 불필요한 정보는 적절히 리셋함으로써 GRU는 긴 시퀀스에서도 안정적인 학습을 가능하게 한다.</p>
</section>
<section id="lstm과의-비교" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="lstm과의-비교"><span class="header-section-number">3.1.3</span> LSTM과의 비교</h3>
<p>GRU는 LSTM과 유사한 목적을 가지고 있지만 몇 가지 주요 차이점이 있다.</p>
<table class="table">
<colgroup>
<col style="width: 10%">
<col style="width: 44%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>특징</th>
<th>LSTM (Long Short-Term Memory)</th>
<th>GRU (Gated Recurrent Unit)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>게이트 수</strong></td>
<td>3개 (입력 게이트, 삭제 게이트, 출력 게이트)</td>
<td>2개 (리셋 게이트, 업데이트 게이트)</td>
</tr>
<tr class="even">
<td><strong>셀 상태</strong></td>
<td>별도의 셀 상태 ( <img src="https://latex.codecogs.com/png.latex?C_t"> )를 가짐 (장기 기억 담당)</td>
<td>별도의 셀 상태 없음 (은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_t"> 가 장기 기억과 현재 출력을 모두 담당)</td>
</tr>
<tr class="odd">
<td><strong>파라미터 수</strong></td>
<td>일반적으로 GRU보다 많음</td>
<td>일반적으로 LSTM보다 적음</td>
</tr>
<tr class="even">
<td><strong>계산 복잡도</strong></td>
<td>GRU보다 높음</td>
<td>LSTM보다 낮음 (학습 속도 빠를 수 있음)</td>
</tr>
<tr class="odd">
<td><strong>성능</strong></td>
<td>다양한 NLP 작업에서 강력한 성능 입증. 일반적으로 복잡한 데이터셋에서 유리</td>
<td>많은 경우 LSTM과 유사한 성능. 데이터가 적거나 특정 작업에서 더 나을 수 있음</td>
</tr>
<tr class="even">
<td><strong>정보 흐름</strong></td>
<td>셀 상태와 은닉 상태를 통해 정보 흐름 제어</td>
<td>은닉 상태와 두 게이트를 통해 정보 흐름 제어</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>구조적 차이</strong>: LSTM은 명시적인 메모리 셀( <img src="https://latex.codecogs.com/png.latex?C_t"> )을 사용하여 장기 정보를 저장하고, 세 개의 게이트로 이 셀과 은닉 상태( <img src="https://latex.codecogs.com/png.latex?h_t"> )를 제어한다. 반면 GRU는 셀 상태 없이 은닉 상태( <img src="https://latex.codecogs.com/png.latex?h_t"> )만으로 정보를 전달하며, 리셋 게이트와 업데이트 게이트 두 개로 정보 흐름을 제어한다. 업데이트 게이트가 LSTM의 삭제 게이트와 입력 게이트의 역할을 통합한 것으로 볼 수 있다.</li>
<li><strong>효율성</strong>: GRU는 파라미터 수가 적기 때문에 계산 효율성이 높고 학습 속도가 빠를 수 있으며, 특히 데이터셋의 크기가 작을 때 과적합을 피하는 데 유리할 수 있다.</li>
<li><strong>성능</strong>: 어떤 모델이 항상 우수하다고 단정하기는 어렵다. 문제의 성격, 데이터셋의 크기 및 복잡성, 하이퍼파라미터 튜닝 등에 따라 결과가 달라질 수 있다. 많은 연구에서 두 모델이 유사한 성능을 보인다고 보고되지만, 때로는 GRU가, 때로는 LSTM이 약간 더 나은 성능을 보이기도 한다.</li>
<li><strong>선택 기준</strong>: 일반적으로는 LSTM이 더 많은 표현력을 가질 수 있다고 여겨져 복잡한 문제에 우선 시도될 수 있지만, 계산 자원이 제한적이거나 빠른 학습이 필요할 경우, 또는 데이터가 충분하지 않을 경우에는 GRU가 좋은 대안이 될 수 있다. 실제 적용 시에는 두 모델을 모두 실험해보고 성능을 비교하는 것이 좋다.</li>
</ul>
</section>
<section id="양방향-gru-bidirectional-gru" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="양방향-gru-bidirectional-gru"><span class="header-section-number">3.1.4</span> 양방향 GRU (Bidirectional GRU)</h3>
<p>양방향 LSTM(BiLSTM)과 마찬가지로, 양방향 GRU(BiGRU)도 시퀀스 데이터 처리 시 과거와 미래의 문맥을 모두 고려하기 위해 사용된다. BiGRU는 순방향 GRU와 역방향 GRU 두 개를 병렬로 구성한다.</p>
<ul>
<li><strong>순방향 GRU</strong>: 입력 시퀀스를 처음부터 끝까지 순서대로 처리한다.</li>
<li><strong>역방향 GRU</strong>: 입력 시퀀스를 끝에서부터 처음까지 역순으로 처리한다.</li>
</ul>
<p>각 시점에서 두 GRU의 은닉 상태는 보통 연결(concatenation)되어 해당 시점의 최종적인 특징 표현으로 사용된다. 이를 통해 특정 시점의 단어나 요소에 대한 이해도를 높일 수 있다. 예를 들어, 문장에서 단어의 의미를 파악할 때 해당 단어의 앞뒤 단어들이 모두 중요한 역할을 하는 경우 BiGRU가 효과적이다.</p>
</section>
</section>
<section id="결론" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="결론"><span class="header-section-number">3.2</span> 결론</h2>
<p>본 문서에서는 RNN의 장기 의존성 문제를 해결하기 위해 설계된 GRU(Gated Recurrent Unit)의 핵심적인 구조와 작동 방식을 살펴보았다. GRU는 LSTM의 대안으로 제시되었으며, 더 단순한 구조와 적은 파라미터로도 유사하거나 때로는 더 나은 성능을 제공할 수 있음을 보여주었다.</p>
<ul>
<li><strong>GRU의 핵심 원리 요약</strong>:
<ul>
<li>GRU는 <strong>리셋 게이트</strong>와 <strong>업데이트 게이트</strong>라는 두 가지 게이트 메커니즘을 통해 정보의 흐름을 정교하게 제어한다. 리셋 게이트는 과거 정보 중 현재 예측에 덜 중요한 부분을 무시하도록 하고, 업데이트 게이트는 과거 정보와 현재 후보 정보 사이의 균형을 조절하여 다음 은닉 상태를 결정한다.</li>
<li>별도의 셀 상태 없이 은닉 상태만으로 장기 정보를 효과적으로 전달하며, 이는 LSTM에 비해 구조적 단순성과 계산 효율성을 가져다준다.</li>
</ul></li>
<li><strong>LSTM과의 관계 및 장점</strong>:
<ul>
<li>GRU는 LSTM보다 파라미터 수가 적어 학습 속도가 빠르고, 특히 데이터가 적은 환경에서 과적합의 위험을 줄일 수 있는 장점이 있다.</li>
<li>많은 자연어 처리 및 시퀀스 모델링 문제에서 LSTM과 대등한 성능을 보여주어, 모델 선택 시 중요한 고려 대상이 된다.</li>
</ul></li>
<li><strong>문맥 이해와 NLP에서의 중요성</strong>:
<ul>
<li>GRU는 장기 의존성을 효과적으로 처리할 수 있어 긴 시퀀스 데이터의 문맥적 의미를 파악하는 데 중요한 역할을 한다.</li>
<li>자연어 이해, 기계 번역, 음성 인식 등 다양한 NLP 태스크에서 순환 신경망의 핵심 구성 요소로 활용되며, 특히 계산 효율성이 중요하거나 빠른 프로토타이핑이 필요할 때 유용하다.</li>
</ul></li>
</ul>
<p>결론적으로, GRU는 LSTM과 함께 시퀀스 데이터, 특히 장기 의존성을 가진 데이터 처리에 있어 중요한 진보를 이룬 모델이다. 단순화된 게이트 메커니즘을 통해 정보의 선택적 기억과 업데이트를 가능하게 함으로써, 복잡한 패턴 학습 능력을 효과적으로 제공하며 다양한 응용 분야에서 그 가치를 입증하고 있다.</p>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-3-3.nn_contextual_GRU.html</guid>
  <pubDate>Sat, 11 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 벡터화: LSTM의 이해</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-3-2.nn_contextual_LSTM.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 기존 RNN(Recurrent Neural Network)의 주요 한계점인 장기 의존성 문제(Long-Term Dependency Problem)를 해결하기 위해 제안된 LSTM(Long Short-Term Memory) 네트워크의 기본적인 작동 원리와 구조를 설명한다.</p>
<ul>
<li><strong>기존 RNN의 한계와 LSTM의 등장 배경</strong>:
<ul>
<li>바닐라 RNN은 시퀀스가 길어질수록 초기 정보가 소실되어 먼 과거의 정보를 현재까지 전달하기 어렵다는 장기 의존성 문제를 가진다. 이는 기울기 소실/폭주 문제와도 관련된다.</li>
<li>LSTM은 이러한 문제를 해결하기 위해 셀 내부에 ’게이트(gate)’라는 정교한 정보 제어 메커니즘을 도입했다.</li>
</ul></li>
<li><strong>LSTM의 핵심 구성 요소 및 작동 원리</strong>:
<ul>
<li><strong>셀 상태 (Cell State, <img src="https://latex.codecogs.com/png.latex?C_t">)</strong>: LSTM의 핵심으로, 컨베이어 벨트처럼 정보를 비교적 오래 기억하고 전달하는 역할을 한다. 게이트들을 통해 정보가 추가되거나 제거된다.</li>
<li><strong>게이트 (Gates)</strong>: 세 가지 주요 게이트가 셀 상태와 은닉 상태(hidden state, <img src="https://latex.codecogs.com/png.latex?h_t">)를 제어한다.
<ul>
<li><strong>삭제 게이트 (Forget Gate, <img src="https://latex.codecogs.com/png.latex?f_t">)</strong>: 이전 셀 상태(<img src="https://latex.codecogs.com/png.latex?C_%7Bt-1%7D">)에서 어떤 정보를 버릴지 결정한다.</li>
<li><strong>입력 게이트 (Input Gate, <img src="https://latex.codecogs.com/png.latex?i_t">)</strong>: 현재 입력(<img src="https://latex.codecogs.com/png.latex?x_t">)과 이전 은닉 상태(<img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D">)를 바탕으로 어떤 새로운 정보를 셀 상태에 저장할지 결정한다. 후보 값(<img src="https://latex.codecogs.com/png.latex?g_t">)과 함께 사용된다.</li>
<li><strong>출력 게이트 (Output Gate, <img src="https://latex.codecogs.com/png.latex?o_t">)</strong>: 현재 셀 상태를 바탕으로 어떤 정보를 현재 시점의 은닉 상태(<img src="https://latex.codecogs.com/png.latex?h_t">)로 출력할지 결정한다.</li>
</ul></li>
<li>이러한 게이트들은 시그모이드(sigmoid) 함수를 통해 0과 1 사이의 값을 출력하여 정보의 흐름을 조절하고, 요소별 곱셈(element-wise product)을 통해 정보를 선택적으로 통과시키거나 차단한다.</li>
</ul></li>
<li><strong>양방향 LSTM (Bidirectional LSTM, BiLSTM)</strong>:
<ul>
<li>단방향 LSTM의 정보 흐름(과거→미래) 한계를 극복하기 위해, 순방향 LSTM과 역방향 LSTM을 함께 사용하여 과거와 미래의 문맥을 모두 고려한다. 최종 출력은 두 LSTM의 은닉 상태를 결합하여 생성된다.</li>
</ul></li>
<li><strong>의의</strong>: LSTM은 장기 의존성 문제를 효과적으로 완화하여 더 긴 시퀀스에서도 의미 있는 정보를 학습할 수 있게 만들었다. 이는 자연어 처리, 음성 인식, 기계 번역 등 다양한 분야에서 RNN 계열 모델의 성능을 크게 향상시키는 데 기여했으며, ELMo와 같은 초기 문맥 기반 임베딩 모델의 중요 구성 요소로 활용되었다.</li>
</ul>
<p>이 문서를 통해 독자는 LSTM이 어떻게 게이트 메커니즘을 통해 정보의 흐름을 제어하고 장기 기억을 가능하게 하는지에 대한 기본적인 이해를 얻을 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    └── RNN 계열
        ├── LSTM
        ├── GRU
        └── ELMo</code></pre>
<section id="문맥을-고려한-벡터화-2018-현재-동적-임베딩" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="문맥을-고려한-벡터화-2018-현재-동적-임베딩"><span class="header-section-number">2.1</span> 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩</h2>
<section id="기존-rnn의-한계-장기-의존성-문제" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="기존-rnn의-한계-장기-의존성-문제"><span class="header-section-number">2.1.1</span> 기존 RNN의 한계: 장기 의존성 문제</h3>
<ul>
<li>장기 의존성 문제 (Long-Term Dependency Problem): 기존의 RNN은 시점이 깅러지면 앞에 있던 정보가 소실되는 장기 의존성 문제를 갖고 있다</li>
<li>즉, 너무 오래 전의 정보를 기억하지 못함. 이것은 RNN의 고질적인 문제입니다.</li>
<li>기울기 소실 문제 (Gradient Vanishing Problem): 너무 오래 전의 정보를 기억하지 못함</li>
<li>Vanilla RNN: 기존의 RNN, 아이스크림 맛중에서 vanilla가 가장 기본적인 맛이기 때문에 vanilla RNN이라고 부른다.</li>
</ul>
</section>
<section id="lstm-long-short-term-memory.-1997" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="lstm-long-short-term-memory.-1997"><span class="header-section-number">2.1.2</span> LSTM (Long Short-Term Memory. 1997)</h3>
<ul>
<li>LSTM은 기존의 RNN의 장기 의존성 문제를 해결하기 위해 은닉층에 추가적인 메커니즘을 도입한다.</li>
<li>추가 메커니즘은 기억력을 증가시키는 것이다.</li>
<li>industry에서 RNN을 썼다고 하면 대부분 LSTM이나 GRU를 쓰는 것을 의미한다.</li>
<li>기존의 RNN은 이전 시점의 hidden state의 정보를 현재 시점의 hidden state에 전달하면서 현재 시점의 출력값도 만들어내는 구조였다.</li>
<li>기존의 RNN은 cell state가 없었다.</li>
<li>LSTM은 이전의 hidden state와 cell state 둘 모두를 다음 시점의 hidden state와 cell state의 정보를 전달한다.</li>
<li>cell state에 gate라는 구조를 통해서 정보를 더하거나 빼는 등의 통제를 한다.</li>
</ul>
<section id="input-gate" class="level4" data-number="2.1.2.1">
<h4 data-number="2.1.2.1" class="anchored" data-anchor-id="input-gate"><span class="header-section-number">2.1.2.1</span> input gate</h4>
<ul>
<li>현재 정보를 기억하기 위한 gate이다.<br>
</li>
<li>cell state: <img src="https://latex.codecogs.com/png.latex?x_t"> 와 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 을 받아서 현재 시점의 선택된 기억할 값을 정한다.
<ul>
<li>sigmoid 함수를 통해 [0,1]을 반환</li>
<li>tanh 함수를 통해 [-1,1]을 반환</li>
<li>이 두 종류의 값을 갖고 cell state에서 이번에 선택된 기억할 값을 정한다. 즉,</li>
<li><img src="https://latex.codecogs.com/png.latex?i_t%20=%20%5Csigma(W_%7Bx_%7Bi%7D%7Dx_t%20+%20W_%7Bh_i%7Dh_%7Bt-1%7D%20+%20b_i)"></li>
<li><img src="https://latex.codecogs.com/png.latex?g_t%20=%20%5Ctanh(W_%7Bx_g%7D%20x_t%20+%20W_%7Bh_g%7D%20h_%7Bt-1%7D%20+%20b_g)"></li>
<li>이때, <img src="https://latex.codecogs.com/png.latex?i_t"> 는 0~1 사이의 값을 가지며, <img src="https://latex.codecogs.com/png.latex?g_t"> 는 -1~1 사이의 값을 가진다.</li>
<li>이 두 값을 곱해서 현재 시점의 cell state를 결정한다.</li>
</ul></li>
</ul>
</section>
<section id="forget-gate" class="level4" data-number="2.1.2.2">
<h4 data-number="2.1.2.2" class="anchored" data-anchor-id="forget-gate"><span class="header-section-number">2.1.2.2</span> forget gate</h4>
<ul>
<li>이전 정보를 잊기 위한 gate이다.<br>
</li>
<li>기억을 삭제하기 위한 gate이다.</li>
<li>sigmoid 함수를 지나 [0,1]을 반환된 값으로 0에 가까울수록 정보가 많이 삭제된 것이며, 1에 가까울수록 정보를 온전히 기억한 셈이다. 즉,</li>
<li><img src="https://latex.codecogs.com/png.latex?f_t=%5Csigma(W_%7Bx_f%7Dx_t+W_%7Bh_f%7Dh_%7Bt-1%7D+b_f)"></li>
<li>이렇게, 일부 기억을 소실하고 입력 게이트의 <img src="https://latex.codecogs.com/png.latex?i_t"> 와 <img src="https://latex.codecogs.com/png.latex?g_t"> 의 정보를 조합하여 elementwise product를 수행하여 더해서 이번에 기억할 값을 결정
<ul>
<li>elementwise product: 각 요소별 곱셈을 의미한다.</li>
<li>즉, 현재 시점의 cell state는 이전의 cell state와 현재 시점의 정보를 조합하여 결정된다.</li>
<li><img src="https://latex.codecogs.com/png.latex?C_t%20=%20f_t%20%5Codot%20C_%7Bt-1%7D%20+%20i_t%20%5Codot%20g_t"></li>
<li>이때, <img src="https://latex.codecogs.com/png.latex?%5Codot"> 는 요소별 곱셈을 의미한다.</li>
<li><img src="https://latex.codecogs.com/png.latex?i_t%20%5Codot%20g_t"> = input gate의 값, 현재 시점의 기억할 값</li>
<li><img src="https://latex.codecogs.com/png.latex?C_%7Bt-1%7D"> = 이전 시점의 cell state</li>
<li><img src="https://latex.codecogs.com/png.latex?f_t%20%5Codot%20C_%7Bt-1%7D"> = forget gate의 값으로 과거의 기억을 삭제한 값</li>
<li>즉, 현재 시점의 cell state는 이전의 cell state와 현재 시점의 정보를 조합하여 결정된다.</li>
</ul></li>
</ul>
</section>
<section id="input-gate와-forget-gate의-영향력" class="level4" data-number="2.1.2.3">
<h4 data-number="2.1.2.3" class="anchored" data-anchor-id="input-gate와-forget-gate의-영향력"><span class="header-section-number">2.1.2.3</span> input gate와 forget gate의 영향력</h4>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?f_t"> 가 0이 되면 이전 시점의 cell state가 완전히 삭제되고 오직 input gate만이 현재 시점의 cell state값을 결정한다.
<ul>
<li>이를 forget gate가 닫히고 input gate만 열린 상태라고 한다.</li>
</ul></li>
<li>반면, <img src="https://latex.codecogs.com/png.latex?i_t"> 가 0이 되면 이전 시점의 cell state를 완전히 삭제하고 오직 forget gate만이 현재 시점의 cell state값을 결정한다.
<ul>
<li>이를 input gate가 닫히고 forget gate만 열린 상태라고 한다.</li>
</ul></li>
<li>따라서, input gate는 현재 시점의 입력을 얼마나 반영할지 결정</li>
<li>반면, forget gate는 이전 시점의 기억을 얼마나 유지할지 결정</li>
</ul>
</section>
<section id="output-gate" class="level4" data-number="2.1.2.4">
<h4 data-number="2.1.2.4" class="anchored" data-anchor-id="output-gate"><span class="header-section-number">2.1.2.4</span> output gate</h4>
<ul>
<li>현재 시점의 출력을 결정하기 위한 gate이다.</li>
<li>Hidden state를 연산하는 일에 쓰이며 Cell state와 비교하여 단기 상태라고도 부른다.</li>
<li>이전 시점의 cell state는 현재 시점의 cell 내부에 기억력을 돕기위한 역할만 하고 현재 시점의 출력엔 영향을 주지 않는다.</li>
<li>현재 시점의 출력은 이전 시점의 hidden state에 영향을 받은 현재 시점의 hidden state에 의해서만 결정된다.</li>
<li>sigmoid 함수를 통해 [0,1]을 반환</li>
<li>이 값을 현재 시점의 cell state에 곱해서 현재 시점의 출력을 결정한다.즉,</li>
<li><img src="https://latex.codecogs.com/png.latex?o_t%20=%20%5Csigma(W_%7Bx_o%7Dx_t%20+%20W_%7Bh_o%7Dh_%7Bt-1%7D%20+%20b_o)"></li>
<li><img src="https://latex.codecogs.com/png.latex?h_t%20=%20o_t%20%5Codot%20%5Ctanh(C_t)"></li>
</ul>
</section>
</section>
<section id="양방향-lstm-bidirectional-lstm.-2005" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="양방향-lstm-bidirectional-lstm.-2005"><span class="header-section-number">2.1.3</span> 양방향 LSTM (Bidirectional LSTM. 2005)</h3>
<ul>
<li>기본 LSTM의 한계: 단방향 처리로 인한 정보 손실
<ul>
<li>기본 LSTM은 과거에서 미래로만 정보를 처리한다</li>
<li>현재 시점에서 미래의 정보를 활용할 수 없어 맥락 이해가 제한적이다</li>
<li>예: “그는 은행에 갔다”에서 “은행”이 금융기관인지 강둑인지 앞뒤 문맥을 모두 봐야 판단 가능</li>
</ul></li>
<li>양방향 LSTM은 순방향과 역방향 두 개의 LSTM을 동시에 사용한다</li>
<li>Forward LSTM과 Backward LSTM이 독립적으로 작동하여 전체 시퀀스의 맥락을 모두 활용한다</li>
<li>자연어 처리, 음성 인식 등에서 단방향 LSTM보다 우수한 성능을 보인다</li>
<li>Forward LSTM (순방향)
<ul>
<li>일반적인 LSTM과 동일하게 시점 1부터 시점 T까지 순차적으로 처리한다</li>
<li>각 시점에서 과거 정보를 현재로 전달한다</li>
<li>Forward hidden state: <img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7Bh_t%7D">, Forward cell state: <img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7BC_t%7D"></li>
<li>Gate 연산:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7Bi_t%7D%20=%20%5Csigma(W_%7Bx_i%7Dx_t%20+%20W_%7Bh_i%7D%5Coverrightarrow%7Bh_%7Bt-1%7D%7D%20+%20b_i)"> (input gate)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7Bf_t%7D%20=%20%5Csigma(W_%7Bx_f%7Dx_t%20+%20W_%7Bh_f%7D%5Coverrightarrow%7Bh_%7Bt-1%7D%7D%20+%20b_f)"> (forget gate)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7Bg_t%7D%20=%20%5Ctanh(W_%7Bx_g%7Dx_t%20+%20W_%7Bh_g%7D%5Coverrightarrow%7Bh_%7Bt-1%7D%7D%20+%20b_g)"> (candidate values)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7BC_t%7D%20=%20%5Coverrightarrow%7Bf_t%7D%20%5Codot%20%5Coverrightarrow%7BC_%7Bt-1%7D%7D%20+%20%5Coverrightarrow%7Bi_t%7D%20%5Codot%20%5Coverrightarrow%7Bg_t%7D"> (cell state)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7Bo_t%7D%20=%20%5Csigma(W_%7Bx_o%7Dx_t%20+%20W_%7Bh_o%7D%5Coverrightarrow%7Bh_%7Bt-1%7D%7D%20+%20b_o)"> (output gate)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7Bh_t%7D%20=%20%5Coverrightarrow%7Bo_t%7D%20%5Codot%20%5Ctanh(%5Coverrightarrow%7BC_t%7D)"> (hidden state)</li>
</ul></li>
</ul></li>
<li>Backward LSTM (역방향)
<ul>
<li>시점 T부터 시점 1까지 역순으로 처리한다</li>
<li>각 시점에서 미래 정보를 현재로 전달한다 (<img src="https://latex.codecogs.com/png.latex?t+1"> 시점에서 <img src="https://latex.codecogs.com/png.latex?t"> 시점으로)</li>
<li>Backward hidden state: <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7Bh_t%7D">, Backward cell state: <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BC_t%7D"></li>
<li>Gate 연산 (별도의 가중치 매개변수 <img src="https://latex.codecogs.com/png.latex?W'"> 사용):
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7Bi_t%7D%20=%20%5Csigma(W'_%7Bx_i%7Dx_t%20+%20W'_%7Bh_i%7D%5Coverleftarrow%7Bh_%7Bt+1%7D%7D%20+%20b'_i)"> (input gate)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7Bf_t%7D%20=%20%5Csigma(W'_%7Bx_f%7Dx_t%20+%20W'_%7Bh_f%7D%5Coverleftarrow%7Bh_%7Bt+1%7D%7D%20+%20b'_f)"> (forget gate)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7Bg_t%7D%20=%20%5Ctanh(W'_%7Bx_g%7Dx_t%20+%20W'_%7Bh_g%7D%5Coverleftarrow%7Bh_%7Bt+1%7D%7D%20+%20b'_g)"> (candidate values)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BC_t%7D%20=%20%5Coverleftarrow%7Bf_t%7D%20%5Codot%20%5Coverleftarrow%7BC_%7Bt+1%7D%7D%20+%20%5Coverleftarrow%7Bi_t%7D%20%5Codot%20%5Coverleftarrow%7Bg_t%7D"> (cell state)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7Bo_t%7D%20=%20%5Csigma(W'_%7Bx_o%7Dx_t%20+%20W'_%7Bh_o%7D%5Coverleftarrow%7Bh_%7Bt+1%7D%7D%20+%20b'_o)"> (output gate)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7Bh_t%7D%20=%20%5Coverleftarrow%7Bo_t%7D%20%5Codot%20%5Ctanh(%5Coverleftarrow%7BC_t%7D)"> (hidden state)</li>
</ul></li>
</ul></li>
<li>최종 출력 결합
<ul>
<li>각 시점에서 Forward와 Backward의 hidden state를 결합하여 최종 출력을 생성한다</li>
<li>가장 일반적인 방법은 연결(concatenation)이다: <img src="https://latex.codecogs.com/png.latex?h_t%20=%20%5B%5Coverrightarrow%7Bh_t%7D;%20%5Coverleftarrow%7Bh_t%7D%5D"></li>
<li>최종 hidden state의 차원은 단방향 LSTM의 2배가 된다</li>
<li>다른 결합 방식들:
<ul>
<li>합계: <img src="https://latex.codecogs.com/png.latex?h_t%20=%20%5Coverrightarrow%7Bh_t%7D%20+%20%5Coverleftarrow%7Bh_t%7D"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?h_t%20=%20%5Cfrac%7B%5Coverrightarrow%7Bh_t%7D%20+%20%5Coverleftarrow%7Bh_t%7D%7D%7B2%7D"></li>
<li>가중합: <img src="https://latex.codecogs.com/png.latex?h_t%20=%20%5Calpha%20%5Coverrightarrow%7Bh_t%7D%20+%20(1-%5Calpha)%20%5Coverleftarrow%7Bh_t%7D"></li>
</ul></li>
</ul></li>
<li>양방향 LSTM의 장점과 한계
<ul>
<li>장점: 전체 시퀀스의 맥락 정보를 모두 활용하여 더 정확한 표현 학습이 가능하다</li>
<li>단점: 전체 시퀀스가 필요하므로 실시간 처리가 불가능하다</li>
<li>계산 복잡도가 단방향 LSTM의 약 2배로 증가한다</li>
<li>파라미터 수와 메모리 사용량이 증가한다</li>
</ul></li>
</ul>
<section id="deep-bidirectional-rnn-20132015" class="level4" data-number="2.1.3.1">
<h4 data-number="2.1.3.1" class="anchored" data-anchor-id="deep-bidirectional-rnn-20132015"><span class="header-section-number">2.1.3.1</span> Deep Bidirectional RNN (2013~2015)</h4>
<ul>
<li>Deep RNN
<ul>
<li>RNN의 은닉층을 여러 개 쌓은 모델</li>
<li>각 은닉층은 이전 은닉층의 출력을 입력으로 받아 현재 은닉층의 출력을 생성</li>
<li>이렇게 하여 더 복잡한 패턴을 학습할 수 있다.</li>
</ul></li>
<li>Bidirectional RNN
<ul>
<li>기본적인 구조는 RNN과 유사하다.</li>
</ul></li>
<li>Deep Bidirectional RNN
<ul>
<li>기본적인 구조는 RNN과 유사하다.</li>
<li>하지만, 더 많은 레이어를 가지고 있어 더 많은 정보를 저장할 수 있다.</li>
</ul></li>
<li>또한, 더 많은 연산을 필요로 하여 더 많은 시간이 걸린다.</li>
<li>하지만, 더 나은 성능을 보인다.</li>
<li>일반적으로 GRU는 LSTM보다 더 빠르게 학습되지만, LSTM은 더 나은 성능을 보인다.</li>
<li>따라서, 일반적으로 LSTM이 더 많이 사용된다.</li>
</ul>
</section>
</section>
</section>
<section id="결론" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="결론"><span class="header-section-number">2.2</span> 결론</h2>
<p>본 문서에서는 RNN의 장기 의존성 문제를 해결하기 위해 설계된 LSTM(Long Short-Term Memory) 네트워크의 핵심적인 구조와 작동 방식을 살펴보았다. LSTM은 셀 상태와 세 가지 주요 게이트(입력, 삭제, 출력)를 통해 정보의 흐름을 정교하게 제어함으로써 장기 기억을 효과적으로 수행한다.</p>
<ul>
<li><strong>LSTM의 핵심 원리 요약</strong>:
<ul>
<li>LSTM의 중심에는 장기적인 정보를 저장하는 ’셀 상태(Cell State)’가 있으며, 이 셀 상태는 게이트들에 의해 선택적으로 정보가 추가되거나 제거되면서 업데이트된다.</li>
<li><strong>삭제 게이트</strong>는 과거 정보 중 불필요한 것을 잊도록 하고, <strong>입력 게이트</strong>는 현재 정보 중 중요한 것을 셀 상태에 추가하며, <strong>출력 게이트</strong>는 현재 셀 상태를 바탕으로 다음 은닉 상태(단기 기억) 및 출력을 결정한다.</li>
<li>이러한 게이트 메커니즘 덕분에 LSTM은 기울기 소실 문제를 완화하고, 시퀀스 내의 멀리 떨어진 정보 간의 의존성을 학습할 수 있다.</li>
</ul></li>
<li><strong>양방향 LSTM의 활용</strong>:
<ul>
<li>단일 방향 LSTM이 과거의 문맥만을 고려하는 한계를 보완하기 위해, 순방향과 역방향 LSTM을 결합한 양방향 LSTM(BiLSTM)이 널리 사용된다. BiLSTM은 특정 시점의 양쪽 문맥 정보를 모두 활용하여 보다 풍부한 표현을 학습할 수 있게 한다.</li>
</ul></li>
<li><strong>문맥 이해와 NLP에서의 중요성</strong>:
<ul>
<li>LSTM은 복잡한 시퀀스 데이터를 모델링하는 데 강력한 성능을 보여주었으며, 특히 자연어 처리 분야에서 문장이나 문서의 문맥적 의미를 파악하는 데 중요한 역할을 했다.</li>
<li>ELMo와 같은 초기 문맥 기반 워드 임베딩 모델의 기반 구조로 사용되었으며, 이후 등장한 트랜스포머 아키텍처 이전까지 다양한 NLP 태스크에서 핵심적인 구성 요소로 활용되었다.</li>
</ul></li>
</ul>
<p>결론적으로, LSTM은 RNN의 한계를 극복하고 시퀀스 데이터, 특히 장기 의존성을 가진 데이터 처리에 있어 중요한 진보를 이룬 모델이다. 정교한 내부 메커니즘을 통해 정보의 선택적 기억과 망각을 가능하게 함으로써, 복잡한 패턴 학습 능력을 크게 향상시켰다.</p>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-3-2.nn_contextual_LSTM.html</guid>
  <pubDate>Fri, 10 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 벡터화: RNN의 이해</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-3-1.nn_contextual_RNN.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 순차적인 데이터, 특히 텍스트와 같은 시퀀스 정보를 처리하기 위해 설계된 RNN(Recurrent Neural Network, 순환 신경망)의 기본적인 작동 원리와 구조를 설명한다.</p>
<ul>
<li><strong>RNN의 핵심 아이디어</strong>:
<ul>
<li>기존의 피드포워드 신경망(FFNN)과 달리, RNN은 내부에 순환하는 루프(loop)를 가지고 있어 이전 단계의 정보를 기억하고 현재 단계의 처리에 활용한다.</li>
<li>각 시점(time step)에서 입력값과 이전 시점의 은닉 상태(hidden state)를 함께 사용하여 현재 시점의 은닉 상태를 갱신한다. 이 은닉 상태가 문맥 정보를 담고 있다고 간주된다.</li>
</ul></li>
<li><strong>기본 구조 및 처리 과정</strong>:
<ul>
<li>RNN은 입력 시퀀스의 길이에 따라 네트워크가 펼쳐지며(unrolled), 각 시점마다 동일한 가중치를 공유한다.</li>
<li>주요 구성 요소로는 입력 벡터(<img src="https://latex.codecogs.com/png.latex?x_t">), 은닉 상태 벡터(<img src="https://latex.codecogs.com/png.latex?h_t">), 출력 벡터(<img src="https://latex.codecogs.com/png.latex?y_t">) 및 이들 간의 변환을 위한 가중치 행렬(<img src="https://latex.codecogs.com/png.latex?W_x,%20W_h,%20W_y">)과 편향(<img src="https://latex.codecogs.com/png.latex?b_h,%20b">)이 있다.</li>
<li>은닉 상태는 주로 하이퍼볼릭 탄젠트(tanh) 활성화 함수를 통해 계산된다.</li>
</ul></li>
<li><strong>다양한 RNN 구조</strong>:
<ul>
<li>입력과 출력의 관계에 따라 One-to-Many (예: 이미지 캡셔닝), Many-to-One (예: 텍스트 분류), Many-to-Many (예: 시퀀스 레이블링) 등 다양한 형태로 설계될 수 있다.</li>
</ul></li>
<li><strong>의의</strong>: RNN은 단어의 순서가 중요한 자연어 처리 분야에서 문맥을 이해하는 모델의 기초를 제공했다. 비록 장기 의존성 문제 등의 한계로 LSTM, GRU와 같은 개선된 모델이나 트랜스포머와 같은 새로운 아키텍처로 발전했지만, 순차 정보 처리의 기본적인 아이디어를 제시했다는 점에서 중요하다.</li>
</ul>
<p>이 문서를 통해 독자는 RNN이 어떻게 순차적 정보를 처리하고 문맥 정보를 기억하며 다음 예측에 활용하는지에 대한 기본적인 이해를 얻을 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
        ├── LSTM
        ├── GRU
        └── ELMo</code></pre>
<section id="문맥을-고려한-벡터화-2018-현재-동적-임베딩" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="문맥을-고려한-벡터화-2018-현재-동적-임베딩"><span class="header-section-number">2.1</span> 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩</h2>
<section id="rnn-recurrent-neural-network" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="rnn-recurrent-neural-network"><span class="header-section-number">2.1.1</span> RNN (Recurrent Neural Network)</h3>
<ul>
<li>FFNN(Feed Forward Neural Network): 행렬과 벡터 연산으로 이루어진다.</li>
<li>RNN: 행렬과 벡터 연산 + 자기 자신의 출력을 다시 입력으로 사용한다.
<ul>
<li>연속적인 시퀀스를 처리하기 위한 신경망</li>
<li>사람은 이전 단어들에 대한 이해를 바탕으로 다음 단어를 이해한다.</li>
<li>기존의 MLP에 비해서 RNN은 이러한 이슈를 다루며, 내부에 정보를 지속하는 루프로 구성된 신경망</li>
<li>단순한 행렬과 벡터 연산을 넘어, <strong>이전 시점의 은닉 상태(hidden state)를 현재 시점의 입력으로 다시 활용</strong>하는 순환 구조</li>
<li>이러한 “기억” 메커니즘 덕분에 RNN은 시간의 흐름에 따른 연속적인 데이터(시퀀스 데이터) 처리에 매우 효과적</li>
<li><strong>핵심 원리</strong>: 신경망 내부에 루프(loop)를 만들어 정보가 지속되도록 함으로써, 마치 사람이 이전 대화 내용을 기억하며 다음 문장을 이해하는 것과 유사한 방식으로 작동</li>
<li>RNN은 입력의 길이만큼 신경망이 펼쳐진다. (unrolled)</li>
<li>이때, 입력 받는 각 순간을 시점(time step)이라고 한다.</li>
<li>시점 <img src="https://latex.codecogs.com/png.latex?t"> 에서 입력 <img src="https://latex.codecogs.com/png.latex?x_t"> 와 이전 시점의 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 을 받아 현재 시점의 은닉 상태 <img src="https://latex.codecogs.com/png.latex?h_t"> 를 계산</li>
<li>매시점마다 새로운 입력값을 받고 은닉층에서 이전 시점의 정보를 다음 시점의 은닉층에 전달하는데 이것을 시간순대로 쭉 나열하여 도식화하면 그림이 너무 길어져 은닉층을 하나의 loop형태로 표현한다.</li>
<li>RNN은 FFNN (or MLP)에 시점을 도입한 개념과 같다.</li>
<li>RNN의 입력과 출력은 모두 기본적으로 벡터 단위를 가정한다. 따라서, 일반 RNN다이어 그램에선 입력층, 은닉층과 출력층이 소문자로 되어 있지만 모두 벡터라고 생각해야한다.</li>
<li>NLP에서 각 시점(time step)은 주로 단어 하나 (단어 벡터값) 또는 형태소 (한국어) 하나가 (형태소 벡터)가 된다.</li>
</ul></li>
</ul>
<section id="rnn의-설계" class="level4" data-number="2.1.1.1">
<h4 data-number="2.1.1.1" class="anchored" data-anchor-id="rnn의-설계"><span class="header-section-number">2.1.1.1</span> RNN의 설계</h4>
<ul>
<li>RNN의 구조는 설계하기 나름이지만 다음과 같은 유형을 갖는다.</li>
<li>One to Many
<ul>
<li>Image Captioning</li>
<li>이미지를 첫 시점에서 입력받아 각 시점에서 출력</li>
</ul></li>
<li>Many to One
<ul>
<li>단어를 각 시점에서 입력받아 맨 마지막 시점의 은닉 상태를 출력</li>
<li>Text Classification
<ul>
<li>단어들을 입력 받아 이것이 스펨메일인지 아닌지 맨 마지막 시점에서 분류</li>
</ul></li>
</ul></li>
<li>Many to Many
<ul>
<li>각 시점에서 입력받은 단어를 각 시점에서 출력</li>
<li>sequence labeling: 각 단어에 대해 특정 레이블을 할당하는 작업으로, 주로 품사 태깅이나 개체명 인식과 같은 태스크에 사용된다.
<ul>
<li>개체명 인식(Named Entity Recognition, NER): 문장에서 인물, 장소, 조직 등과 같은 고유명사를 식별하고 분류하는 작업</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="basic-architecture-of-rnn" class="level4" data-number="2.1.1.2">
<h4 data-number="2.1.1.2" class="anchored" data-anchor-id="basic-architecture-of-rnn"><span class="header-section-number">2.1.1.2</span> Basic Architecture of RNN</h4>
<ul>
<li>Cell: 은닉층에 있는 RNN의 처리 단위 도식상에서 부르는 명칭, 보통 cell 이나 hidden state 구분없이 부르기도 한다.</li>
<li>Hidden State: Cell의 출력, RNN에서 부르는 명칭</li>
<li>RNN은 시점(time step)마다 입력을 받는데 현재 시점의 hidden state인 <img src="https://latex.codecogs.com/png.latex?h_t"> 연산을 위해 직전 시점의 hidden state인 <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"> 을 입력받는다.</li>
<li>이게 RNN이 과거의 정보를 기억하는 원리이다.</li>
<li>이러한 구조 덕분에 RNN은 시퀀스 데이터를 처리하는 데 매우 효과적이다.</li>
<li>문장 내 각 단어는 시점(time step)이 되며, 각 단어는 벡터 형태로 입력된다.</li>
<li>각 시점에서 입력된 벡터와 이전 시점의 hidden state를 받아 현재 시점의 hidden state를 계산한다.</li>
<li>이렇게 계산된 hidden state는 다음 시점의 입력을 받을 때 사용된다.</li>
<li>이 과정을 모든 시점에 반복하여 수행하면 문장 전체에 대한 정보를 효과적으로 표현할 수 있다.</li>
<li>입력층: <img src="https://latex.codecogs.com/png.latex?x_t"></li>
<li>은닉층 (cell): <img src="https://latex.codecogs.com/png.latex?h_t%20=%20%5Ctanh(W_%7Bh%7Dh_%7Bt-1%7D%20+%20W_%7Bx%7Dx_t%20+%20b_h)">
<ul>
<li>ex) <img src="https://latex.codecogs.com/png.latex?h_t%20=%20%5Ctanh(W_%7Bh%7Dh_%7Bt-1%7D%20+%20W_%7Bx%7Dx_t%20+%20b_h)"></li>
</ul></li>
<li>출력층 (output): <img src="https://latex.codecogs.com/png.latex?y_t%20=%20activation(W_%7By%7Dh_t%20+%20b)">
<ul>
<li>ex) <img src="https://latex.codecogs.com/png.latex?y_t%20=%20%5Ctext%7Bsoftmax%7D(W_%7By%7Dh_t%20+%20b)"></li>
</ul></li>
<li>도식화</li>
</ul>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<svg width="672" height="480" viewbox="0.00 0.00 165.00 212.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 208)">
<title>RNN_Cell</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-208 161,-208 161,4 -4,4"></polygon>
<!-- y_t -->
<g id="node1" class="node">
<title>y_t</title>
<polygon fill="lightgray" stroke="black" points="54,-204 0,-204 0,-168 54,-168 54,-204"></polygon>
<text text-anchor="middle" x="27" y="-181.8" font-family="Times,serif" font-size="14.00">y_t</text>
</g>
<!-- h_prev -->
<g id="node2" class="node">
<title>h_prev</title>
<polygon fill="lightgreen" stroke="black" points="157,-120 103,-120 103,-84 157,-84 157,-120"></polygon>
</g>
<!-- cell -->
<g id="node3" class="node">
<title>cell</title>
<polygon fill="lightgreen" stroke="black" points="54,-120 0,-120 0,-84 54,-84 54,-120"></polygon>
<text text-anchor="middle" x="27" y="-97.8" font-family="Times,serif" font-size="14.00">Cell</text>
</g>
<!-- h_prev&#45;&gt;cell -->
<g id="edge1" class="edge">
<title>h_prev-&gt;cell</title>
<path fill="none" stroke="red" d="M102.95,-90.77C98.12,-89.21 93.08,-87.85 88.22,-87 79.71,-85.52 77.29,-85.52 68.78,-87 67.11,-87.29 65.42,-87.64 63.72,-88.04"></path>
<polygon fill="red" stroke="red" points="62.73,-84.69 54.05,-90.77 64.63,-91.42 62.73,-84.69"></polygon>
<text text-anchor="middle" x="78.5" y="-90" font-family="Times,serif" font-size="10.00" fill="red">W_h</text>
</g>
<!-- h_prev&#45;&gt;cell -->
<g id="edge5" class="edge">
<title>h_prev-&gt;cell</title>
<path fill="none" stroke="black" d="M102.72,-114.62C99.78,-115.59 96.79,-116.42 93.85,-117 82.77,-119.21 78.16,-119.59 64.27,-116.79"></path>
<polygon fill="black" stroke="black" points="64.79,-113.32 54.28,-114.62 63.31,-120.16 64.79,-113.32"></polygon>
</g>
<!-- cell&#45;&gt;y_t -->
<g id="edge3" class="edge">
<title>cell-&gt;y_t</title>
<path fill="none" stroke="red" d="M27,-120.08C27,-130.86 27,-145.01 27,-157.36"></path>
<polygon fill="red" stroke="red" points="23.5,-157.61 27,-167.61 30.5,-157.61 23.5,-157.61"></polygon>
<text text-anchor="middle" x="36.72" y="-141" font-family="Times,serif" font-size="10.00" fill="red">W_y</text>
</g>
<!-- cell&#45;&gt;h_prev -->
<g id="edge4" class="edge">
<title>cell-&gt;h_prev</title>
<path fill="none" stroke="red" d="M64.36,-102C77.06,-102 91,-102 102.76,-102"></path>
<polygon fill="red" stroke="red" points="64.06,-98.5 54.06,-102 64.06,-105.5 64.06,-98.5"></polygon>
<text text-anchor="middle" x="78.5" y="-108" font-family="Times,serif" font-size="10.00" fill="red">h_{t-1}</text>
</g>
<!-- x_t -->
<g id="node4" class="node">
<title>x_t</title>
<polygon fill="lightgray" stroke="black" points="54,-36 0,-36 0,0 54,0 54,-36"></polygon>
<text text-anchor="middle" x="27" y="-13.8" font-family="Times,serif" font-size="14.00">x_t</text>
</g>
<!-- x_t&#45;&gt;cell -->
<g id="edge2" class="edge">
<title>x_t-&gt;cell</title>
<path fill="none" stroke="red" d="M27,-36.08C27,-46.86 27,-61.01 27,-73.36"></path>
<polygon fill="red" stroke="red" points="23.5,-73.61 27,-83.61 30.5,-73.61 23.5,-73.61"></polygon>
<text text-anchor="middle" x="36.72" y="-57" font-family="Times,serif" font-size="10.00" fill="red">W_x</text>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="kk3225.netlify.app/images/rnn/rnn_matrix.PNG" class="img-fluid figure-img"></p>
<figcaption>RNN matrix</figcaption>
</figure>
</div>
<ul>
<li><p>d: t time step의 단어 벡터의 차원</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?D_h">: hidden state의 차원 (RNN의 주요 파라미터)</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?W_h">: hidden state에 대한 가중치, 역전파로 최적화되는 파라미터</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?W_x">: 입력에 대한 가중치, 역전파로 최적화되는 파라미터</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?b_h">: hidden state에 대한 편향, 역전파로 최적화되는 파라미터</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?W_y">: 출력에 대한 가중치, 역전파로 최적화되는 파라미터</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?b">: 출력에 대한 편향</p></li>
<li><p>tanh: hyperbolic tangent, RNN에서 주로 사용되는 활성화 함수</p>
<ul>
<li>sigmoid함수와 달리 -1~1 사이의 값을 가지며, 이는 모델의 출력이 sigmoid 함수(0~1)보다 더 넓은 범위의 값을 가지게 됨을 의미한다.</li>
<li>tanh 함수는 출력 범위가 -1에서 1로 넓어, 시그모이드 함수의 0에서 1 범위보다 기울기 소실 문제를 줄여준다. 이는 학습 시 더 안정적이고 빠른 수렴을 가능하게 하여 은닉층에서 연산적으로 유리하다.</li>
<li>따라서, tanh 함수는 시그모이드 함수보다 더 안정적이고 빠른 수렴을 가능하게 하여 은닉층에서 연산적으로 유리하다.</li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="결론" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="결론"><span class="header-section-number">2.2</span> 결론</h2>
<p>본 문서에서는 순차적인 데이터를 효과적으로 처리하기 위해 고안된 RNN(Recurrent Neural Network)의 기본적인 구조와 작동 원리를 살펴보았다. RNN은 이전 시점의 처리 결과를 현재 시점의 입력과 함께 활용하는 순환 구조를 통해 시간의 흐름에 따른 정보의 연속성을 모델링한다.</p>
<ul>
<li><strong>RNN의 핵심 원리 요약</strong>:
<ul>
<li>RNN은 각 시점(time step)에서 입력 벡터와 이전 시점의 은닉 상태(hidden state)를 입력으로 받아 현재 시점의 은닉 상태를 계산한다. 이 은닉 상태는 과거의 정보를 요약하고 있으며, 다음 시점으로 전달되어 문맥 정보를 누적한다.</li>
<li>이러한 순환적인 정보 전달 메커니즘은 단어의 순서가 중요한 텍스트 데이터나 시계열 데이터 분석에 적합하다.</li>
</ul></li>
<li><strong>구조적 특징과 다양성</strong>:
<ul>
<li>하나의 셀(cell)이 반복적으로 사용되며, 입력과 출력의 관계에 따라 다양한 형태(One-to-Many, Many-to-One, Many-to-Many)로 구성될 수 있어 여러 종류의 시퀀스 처리 문제에 적용될 수 있다.</li>
<li>활성화 함수로는 주로 하이퍼볼릭 탄젠트(tanh)가 사용되어 은닉 상태 값의 범위를 조절한다.</li>
</ul></li>
<li><strong>문맥 이해의 초기 단계와 한계점</strong>:
<ul>
<li>RNN은 문맥을 고려한 동적 임베딩의 초기 아이디어를 제공하며, 특히 단어의 순서 정보를 자연스럽게 처리할 수 있는 능력을 보여주었다.</li>
<li>그러나 기본적인 RNN 구조는 시퀀스가 길어질수록 앞부분의 정보가 소실되는 장기 의존성 문제(vanishing/exploding gradient)에 취약하다는 단점이 있다. 이러한 한계를 극복하기 위해 LSTM, GRU와 같은 개선된 RNN 셀 구조가 등장했으며, 더 나아가 어텐션 메커니즘과 트랜스포머 아키텍처로 발전하는 계기가 되었다.</li>
</ul></li>
</ul>
<p>결론적으로, RNN은 순차적 데이터 처리와 문맥 정보 활용의 기본적인 패러다임을 제시한 중요한 신경망 모델이다. 비록 자체적인 한계로 인해 현재는 더 발전된 모델들이 주로 사용되지만, 그 핵심 아이디어는 여전히 많은 시퀀스 모델링 기법의 근간을 이루고 있다.</p>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-3-1.nn_contextual_RNN.html</guid>
  <pubDate>Thu, 09 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 벡터화: 문맥 기반 임베딩의 이해</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-3-0.nn_contextual_enbedding.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 단어의 의미가 문맥에 따라 달라지는 점을 반영하는 <strong>동적/문맥적 임베딩(Contextualized Embedding)</strong> 방법론의 기본 개념을 소개한다. 정적 임베딩의 한계를 간략히 언급하고, 초기 RNN 기반의 시도부터 ELMo, 그리고 트랜스포머 기반의 BERT, GPT, SBERT 등 주요 모델들의 핵심 아이디어와 특징을 살펴본다.</p>
<p>주요 내용은 다음과 같다.</p>
<ul>
<li><strong>정적 임베딩 vs.&nbsp;동적 임베딩</strong>:
<ul>
<li>정적 임베딩(예: Word2Vec)은 단어마다 하나의 고정된 벡터를 할당하여, 같은 단어라도 문맥에 따라 의미가 달라지는 다의성을 처리하기 어렵다.</li>
<li>동적 임베딩은 문맥을 고려하여 동일한 단어라도 상황에 맞는 다른 벡터 표현을 생성한다. 이는 순차적 데이터 처리에 강점을 가진 RNN 계열 모델들을 통해 처음 시도되었고, 이후 ELMo나 트랜스포머 기반 모델들로 발전했다.</li>
</ul></li>
<li><strong>주요 문맥 기반 임베딩 모델 소개</strong>:
<ul>
<li><strong>ELMo (Embeddings from Language Models)</strong>: 순방향과 역방향 LSTM(Bidirectional LSTM, BiLSTM)을 독립적으로 학습시킨 후, 각 계층에서 얻은 내부 상태들을 문자 단위 표현부터 단어 수준의 의미까지 종합적으로 활용하여 문맥 정보를 풍부하게 담은 임베딩을 생성한다. 이는 깊은(deep) 언어 표현 학습의 가능성을 보여주었다.</li>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: 트랜스포머(Transformer)의 인코더 구조를 사용하여 문장 내 모든 단어의 양방향 문맥을 동시에, 그리고 깊게 고려한다. 특히 ’Masked Language Model(MLM)’을 통해 단어를 예측하고, ’Next Sentence Prediction(NSP)’으로 문장 간의 관계를 학습하는 혁신적인 사전 학습(pre-training) 목표를 통해 뛰어난 언어 이해 능력을 보여준다. 문장 전체의 표현으로는 특수 토큰 <code>[CLS]</code>의 최종 출력을 사용하거나, 모든 토큰 출력의 평균/최대 풀링(pooling) 등을 활용한다.</li>
<li><strong>SBERT (Sentence-BERT)</strong>: BERT와 같은 트랜스포머 모델의 출력을 문장 수준의 고정된 크기 의미 벡터로 효율적으로 변환하기 위해 Siamese 또는 Triplet 네트워크 구조를 사용한다. 이를 통해 문장 간 유사도 계산이나 의미 검색 작업의 효율성과 정확성을 크게 향상시킨다.</li>
<li><strong>GPT (Generative Pre-trained Transformer)</strong>: 트랜스포머의 디코더 구조를 기반으로 하는 단방향(autoregressive) 언어 모델이다. 이전 단어들을 바탕으로 다음 단어를 순차적으로 예측하도록 학습하며, 이 과정에서 문맥을 이해하고 자연스러운 텍스트를 생성하는 능력을 키운다. 특히, 별도의 미세 조정 없이 프롬프트에 몇 가지 예시(few-shot)를 제공하는 것만으로도 새로운 작업을 수행할 수 있는 ‘In-context Learning’ 능력으로 큰 주목을 받았다.</li>
</ul></li>
<li><strong>활용</strong>: 이러한 모델들은 문서 분류, 질의응답, 기계 번역 등 다양한 NLP 태스크에서 기존 방법론들의 성능을 크게 뛰어넘는 결과를 가져왔다.</li>
</ul>
<p>이 문서를 통해 독자는 문맥 기반 임베딩 기술의 발전 과정과 주요 모델들의 기본적인 아이디어 및 특징을 이해할 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<p>텍스트 벡터화</p>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    └── RNN 계열
        ├── LSTM
        ├── GRU
        └── ELMo</code></pre>
<section id="문맥을-고려한-벡터화-2018-현재-동적-임베딩" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="문맥을-고려한-벡터화-2018-현재-동적-임베딩"><span class="header-section-number">2.1</span> 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩</h2>
<p>정적 임베딩은 단어의 의미를 하나의 고정된 벡터로 표현하기 때문에, 문맥에 따라 달라지는 단어의 다양한 의미(다의성, 동음이의어)를 제대로 포착하지 못하는 근본적인 한계를 지닌다. 예를 들어, “사과”라는 단어가 과일인지, 아니면 누군가의 사과(apology)인지 문맥 없이는 알 수 없지만, 정적 임베딩에서는 항상 같은 벡터로 표현된다. 이러한 문제를 해결하기 위해 등장한 것이 바로 문맥 기반 동적 임베딩이다.</p>
<section id="초기-시도-rnn-기반-문맥-표현" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="초기-시도-rnn-기반-문맥-표현"><span class="header-section-number">2.1.1</span> 초기 시도: RNN 기반 문맥 표현</h3>
<p>동적 임베딩의 초기 아이디어는 순차적인 정보를 처리하는 데 강점을 가진 RNN(Recurrent Neural Network)과 그 변형인 LSTM(Long Short-Term Memory), GRU(Gated Recurrent Unit) 등을 활용하는 것에서 시작되었다. 이 모델들은 단어 시퀀스를 입력으로 받아 각 단어의 은닉 상태(hidden state)를 계산하는데, 이 은닉 상태가 해당 단어까지의 문맥 정보를 요약한다고 보았다.</p>
<ul>
<li><strong>작동 방식</strong>: 문장을 순차적으로 읽으면서 각 단어의 정보를 누적하여 현재 단어의 문맥적 의미를 표현하려 했다. 예를 들어, “나는 [사과]를 먹었다”에서 “사과”의 벡터를 만들 때, “나는”이라는 이전 단어의 정보가 영향을 미치는 식이다.</li>
<li><strong>한계</strong>: RNN 계열 모델은 문장 앞부분의 정보가 뒤로 갈수록 희석되는 장기 의존성 문제(long-term dependency problem)가 있었고, 문장 전체의 양방향 문맥을 동시에 고려하는 데에도 한계가 있었다. 또한, 주로 특정 다운스트림 태스크에 대해 처음부터 학습되어 일반적인 언어 표현 자체를 학습했다고 보기는 어려웠다.</li>
</ul>
<p>이러한 초기 시도들은 문맥의 중요성을 인식하게 했지만, 더 정교하고 일반화된 문맥 임베딩 방법론의 필요성을 야기했다.</p>
</section>
<section id="elmo-embeddings-from-language-models-2018" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="elmo-embeddings-from-language-models-2018"><span class="header-section-number">2.1.2</span> ELMo (Embeddings from Language Models, 2018)</h3>
<p>ELMo는 ’언어 모델로부터 얻는 임베딩’이라는 이름에서 알 수 있듯이, 깊은 양방향 언어 모델(deep BiLSTM)을 사전 학습하여 문맥에 따라 단어의 의미를 동적으로 결정하는 임베딩을 제안했다.</p>
<ul>
<li><strong>핵심 아이디어</strong>: 정방향 LSTM과 역방향 LSTM을 각각 독립적으로 학습한 후, 이 두 모델의 각 계층(layer)에서 나오는 은닉 상태들을 결합하여 사용한다. 특히, 문자 단위 합성곱 신경망(Character CNN)을 통해 단어의 철자 정보(형태론적 특징)까지 포착하여 OOV(Out-of-Vocabulary) 문제에도 강인함을 보였다.</li>
<li><strong>동적 표현</strong>: 특정 단어의 ELMo 임베딩은 해당 단어 주변의 전체 문맥을 고려하여 계산되며, 단순히 미리 정의된 벡터를 가져오는 것이 아니라 실제 입력 문장에 따라 동적으로 생성된다. 또한, 사전 학습된 BiLSTM의 여러 계층에서 나온 벡터들을 가중합(weighted sum)하여 사용함으로써, 구문론적 정보(낮은 계층)부터 의미론적 정보(높은 계층)까지 다양한 수준의 정보를 활용할 수 있게 했다.</li>
<li><strong>의의</strong>: ELMo는 사전 학습된 깊은 신경망을 통해 문맥 의존적인 단어 표현을 효과적으로 생성할 수 있음을 보여주었고, 이후 등장하는 트랜스포머 기반 모델들의 중요한 영감을 제공했다.</li>
</ul>
</section>
<section id="정적-vs-동적-임베딩" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="정적-vs-동적-임베딩"><span class="header-section-number">2.1.3</span> 정적 vs 동적 임베딩</h3>
<ul>
<li>문제 상황: 동음이의어와 다의어
<ul>
<li><strong>영어 예시: “bank”</strong></li>
</ul>
<pre><code>문장1: "I went to the bank to deposit money"  (은행)
문장2: "The river bank was muddy"            (강둑)</code></pre>
<ul>
<li><strong>한국어 예시: “배”</strong></li>
</ul>
<pre><code>문장1: "배가 고파서 밥을 먹었다"  (배 = 위장)
문장2: "배를 타고 바다에 나갔다"  (배 = 선박)  
문장3: "달콤한 배를 먹었다"      (배 = 과일)</code></pre></li>
<li>정적 임베딩의 한계
<ul>
<li><strong>Word2Vec/GloVe 방식:</strong>
<ul>
<li>서로 다른 의미임에도 같은 벡터 사용</li>
<li>문맥 정보를 활용하지 못함</li>
<li>의미 구분이 불가능</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 단어별로 고정된 하나의 벡터만 존재</span></span>
<span id="cb4-2">embedding_table <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb4-3">   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bank"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, ...],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 항상 같은 벡터</span></span>
<span id="cb4-4">   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"배"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, ...],    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 항상 같은 벡터</span></span>
<span id="cb4-5">}</span>
<span id="cb4-6"></span>
<span id="cb4-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문맥에 관계없이 항상 같은 벡터 반환</span></span>
<span id="cb4-8">vector_bank_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> embedding_table[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bank"</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 은행 문맥</span></span>
<span id="cb4-9">vector_bank_2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> embedding_table[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bank"</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 강둑 문맥</span></span>
<span id="cb4-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># vector_bank_1 == vector_bank_2 (문제!)</span></span></code></pre></div></li>
<li>동적 임베딩의 해결책
<ul>
<li><strong>BERT/ELMo 방식:</strong></li>
</ul>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 같은 단어라도 문맥에 따라 다른 벡터 생성</span></span>
<span id="cb5-2">sentence1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"I went to the bank to deposit money"</span></span>
<span id="cb5-3">sentence2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The river bank was muddy"</span></span>
<span id="cb5-4"></span>
<span id="cb5-5">vector_bank_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> contextual_embedding(sentence1, word_position<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)  </span>
<span id="cb5-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># → [0.8, 0.2, -0.1, ...]  (은행 의미)</span></span>
<span id="cb5-7"></span>
<span id="cb5-8">vector_bank_2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> contextual_embedding(sentence2, word_position<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)  </span>
<span id="cb5-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># → [-0.3, 0.9, 0.4, ...]  (강둑 의미)</span></span>
<span id="cb5-10"></span>
<span id="cb5-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># vector_bank_1 ≠ vector_bank_2 (해결!)</span></span></code></pre></div></li>
</ul>
</section>
<section id="bert-bidirectional-encoder-representations-from-transformers-2018" class="level3" data-number="2.1.4">
<h3 data-number="2.1.4" class="anchored" data-anchor-id="bert-bidirectional-encoder-representations-from-transformers-2018"><span class="header-section-number">2.1.4</span> BERT (Bidirectional Encoder Representations from Transformers, 2018)</h3>
<p>BERT는 트랜스포머(Transformer)의 인코더(Encoder) 구조만을 활용하여, 문장 내 모든 단어의 양방향 문맥을 동시에 그리고 깊게 학습하는 혁신적인 모델이다.</p>
<ul>
<li><strong>핵심 아이디어</strong>: 기존 언어 모델들이 주로 단방향(왼쪽에서 오른쪽 또는 오른쪽에서 왼쪽)으로 문맥을 학습했던 것과 달리, BERT는 ’Masked Language Model (MLM)’이라는 새로운 사전 학습 방식을 통해 진정한 의미의 양방향 학습을 가능하게 했다. MLM은 문장 내 일부 단어를 가리고 (마스킹하고), 주변 단어들만을 이용해 가려진 단어를 예측하도록 하는 방식이다. 또한, ’Next Sentence Prediction (NSP)’을 통해 두 문장이 이어지는 관계인지 예측하도록 학습하여 문장 간의 관계 이해 능력도 키웠다.</li>
<li><strong>문맥 표현</strong>: 특정 단어의 BERT 임베딩은 해당 단어뿐만 아니라 문장 전체의 모든 단어와의 관계를 동시에 고려하여 생성된다. 입력의 시작 부분에 추가되는 <code>[CLS]</code> 토큰의 최종 은닉 상태는 문장 전체를 대표하는 벡터로 활용되어 분류(classification) 문제 등에 사용될 수 있다.</li>
<li><strong>의의</strong>: BERT는 다양한 NLP 벤치마크에서 압도적인 성능 향상을 보여주며, 사전 학습-미세 조정(pre-training and fine-tuning) 패러다임을 NLP 분야의 표준으로 만들었다. 이후 RoBERTa, ALBERT, ELECTRA 등 수많은 변형 모델들이 등장하는 계기가 되었다.</li>
</ul>
</section>
<section id="gpt-generative-pre-trained-transformer-2018-이후" class="level3" data-number="2.1.5">
<h3 data-number="2.1.5" class="anchored" data-anchor-id="gpt-generative-pre-trained-transformer-2018-이후"><span class="header-section-number">2.1.5</span> GPT (Generative Pre-trained Transformer, 2018 이후)</h3>
<p>GPT 계열 모델들은 트랜스포머의 디코더(Decoder) 구조를 기반으로 하며, 주로 텍스트 생성(generation) 작업에 강력한 성능을 보인다.</p>
<ul>
<li><strong>핵심 아이디어</strong>: GPT는 이전 단어들(또는 토큰들)이 주어졌을 때 다음 단어를 예측하는 전통적인 단방향(autoregressive) 언어 모델링 방식으로 학습된다. 이 과정에서 문맥을 이해하고 일관성 있는 긴 텍스트를 생성하는 능력을 학습한다.</li>
<li><strong>In-context Learning</strong>: GPT-2부터 두드러지기 시작하여 GPT-3에서 크게 발전한 특징으로, 모델의 가중치를 직접 업데이트하는 미세 조정(fine-tuning) 없이, 프롬프트(prompt)에 작업 설명과 몇 가지 예시(few-shot)를 함께 제공하는 것만으로도 모델이 새로운 작업을 수행할 수 있는 능력을 의미한다. 이는 모델 활용의 유연성을 크게 높였다.</li>
<li><strong>의의</strong>: GPT는 대규모 데이터와 큰 모델 크기를 통해 놀라운 생성 능력과 일반화 성능을 보여주었으며, 챗봇, 요약, 번역, 코드 생성 등 매우 광범위한 응용 가능성을 열었다. 현재 대규모 언어 모델(LLM)의 대표적인 아키텍처 중 하나이다.</li>
</ul>
</section>
<section id="sbert-sentence-bert-2019" class="level3" data-number="2.1.6">
<h3 data-number="2.1.6" class="anchored" data-anchor-id="sbert-sentence-bert-2019"><span class="header-section-number">2.1.6</span> SBERT (Sentence-BERT, 2019)</h3>
<p>SBERT는 BERT와 같은 사전 학습된 트랜스포머 모델을 문장이나 짧은 단락 수준의 임베딩 생성에 효과적으로 사용하기 위해 제안된 방법론이다.</p>
<ul>
<li><strong>핵심 아이디어</strong>: BERT의 출력을 그대로 사용하여 문장 벡터를 만들 경우(예: <code>[CLS]</code> 토큰 출력 또는 토큰 임베딩 평균 풀링), 의미적으로 유사한 문장을 잘 찾아내지 못하는 경우가 있다. SBERT는 Siamese 또는 Triplet 네트워크 구조를 사용하여 BERT를 미세 조정함으로써, 문장 간 의미 유사도를 잘 반영하는 고정된 크기의 문장 임베딩을 생성한다.
<ul>
<li><strong>Siamese 네트워크</strong>: 두 개의 문장을 각각 동일한 BERT 모델에 통과시켜 나온 문장 임베딩 간의 유사도를 계산한다.</li>
<li><strong>Triplet 네트워크</strong>: 기준 문장(anchor), 긍정 문장(positive), 부정 문장(negative) 세 쌍을 이용하여, 기준 문장이 긍정 문장과는 가깝고 부정 문장과는 멀어지도록 학습한다.</li>
</ul></li>
<li><strong>효율성</strong>: SBERT를 통해 얻은 문장 임베딩은 코사인 유사도 등으로 매우 빠르게 비교할 수 있어, 대규모 문장 데이터셋에서의 의미 검색, 클러스터링, 정보 검색 등의 작업에 매우 효율적이다.</li>
<li><strong>의의</strong>: 복잡한 계산 없이도 고품질의 문장 임베딩을 생성하고 활용할 수 있는 실용적인 방법을 제시하여, 문장 수준의 의미 이해가 중요한 다양한 NLP 응용 분야에 널리 사용된다.</li>
</ul>
</section>
<section id="실용적-응용-및-평가" class="level3" data-number="2.1.7">
<h3 data-number="2.1.7" class="anchored" data-anchor-id="실용적-응용-및-평가"><span class="header-section-number">2.1.7</span> 실용적 응용 및 평가</h3>
<section id="평가-지표" class="level4" data-number="2.1.7.1">
<h4 data-number="2.1.7.1" class="anchored" data-anchor-id="평가-지표"><span class="header-section-number">2.1.7.1</span> 평가 지표</h4>
<p><strong>Intrinsic Evaluation (내재적 평가):</strong> - <strong>단어 유사도</strong>: WordSim-353, SimLex-999 - 사람이 평가한 단어 유사도와 모델 예측의 상관관계 측정 - <strong>단어 관계</strong>: “king - man + woman = queen” - 벡터 연산으로 의미 관계 포착 정도 평가</p>
<p><strong>Extrinsic Evaluation (외재적 평가):</strong> - <strong>문서 분류 정확도</strong>: 실제 분류 태스크에서의 성능 - <strong>정보 검색 성능</strong>: NDCG, MAP - 검색 결과의 관련성 및 순위 정확도 - <strong>의미적 텍스트 유사도</strong>: STS benchmark - 문장 간 의미적 유사성 예측 성능</p>
</section>
<section id="모델-선택-가이드" class="level4" data-number="2.1.7.2">
<h4 data-number="2.1.7.2" class="anchored" data-anchor-id="모델-선택-가이드"><span class="header-section-number">2.1.7.2</span> 모델 선택 가이드</h4>
<ul>
<li><strong>소규모 데이터</strong>: FastText (OOV 처리)</li>
<li><strong>대규모 문서 분류</strong>: BERT fine-tuning</li>
<li><strong>실시간 유사도 계산</strong>: SBERT</li>
<li><strong>창작/생성 태스크</strong>: GPT 계열</li>
</ul>
</section>
<section id="통계적-해석" class="level4" data-number="2.1.7.3">
<h4 data-number="2.1.7.3" class="anchored" data-anchor-id="통계적-해석"><span class="header-section-number">2.1.7.3</span> 통계적 해석</h4>
<p>임베딩 공간에서의 기하학적 관계: <img src="https://latex.codecogs.com/png.latex?%20%5Ccos(%5Cmathbf%7Bv%7D_%7B%5Ctext%7Bsimilar%20words%7D%7D)%20%3E%20%5Ccos(%5Cmathbf%7Bv%7D_%7B%5Ctext%7Bdissimilar%20words%7D%7D)%20"></p>
<p><strong>시각화 도구</strong>: t-SNE/UMAP을 통한 의미적 클러스터링 확인</p>
</section>
</section>
</section>
<section id="결론" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="결론"><span class="header-section-number">2.2</span> 결론</h2>
<p>본 문서에서는 단어의 고정된 벡터 표현을 넘어, 문맥에 따라 의미가 유동적으로 변하는 것을 포착하는 동적 임베딩 방법론의 기본 개념과 주요 모델들을 살펴보았다. ELMo부터 BERT, GPT, SBERT에 이르기까지, 이러한 문맥 기반 임베딩 모델들은 자연어 처리(NLP) 분야에 큰 발전을 가져왔다.</p>
<p>주요 내용을 다시 정리하면 다음과 같다.</p>
<ul>
<li><p><strong>정적 임베딩의 한계점 보완</strong>: 기존의 Word2Vec, GloVe 같은 정적 임베딩은 단어의 다의성을 반영하기 어려웠다. 동적 임베딩은 이 문제를 해결하여, 같은 단어라도 문맥에 따라 다른 벡터를 생성함으로써 보다 정확한 의미 표현을 가능하게 했다.</p></li>
<li><p><strong>주요 모델들의 아이디어와 기여</strong>:</p>
<ul>
<li><strong>ELMo</strong>: 양방향 LSTM을 통해 문맥 정보를 임베딩에 통합했다.</li>
<li><strong>BERT</strong>: 트랜스포머와 MLM, NSP 학습 방식을 통해 양방향 문맥 이해의 새로운 기준을 제시했고, 다양한 NLP 문제에서 높은 성능을 보였다.</li>
<li><strong>GPT</strong>: 단방향 트랜스포머 디코더를 통해 강력한 텍스트 생성 능력을 보여주었고, In-context Learning이라는 유연한 활용 가능성을 제시했다.</li>
<li><strong>SBERT</strong>: BERT를 문장 임베딩 생성에 효율적으로 적용하여, 문장 간 유사도 비교 작업의 성능을 높였다.</li>
</ul></li>
<li><p><strong>LLM 발전의 기반</strong>: 문맥 기반 임베딩 모델들의 발전은 언어를 깊이 이해하고 생성할 수 있는 대규모 언어 모델(Large Language Models, LLMs) 시대로 나아가는 중요한 발판이 되었다. 사전 학습과 미세 조정, 그리고 프롬프트 기반 학습 방식은 모델 활용의 폭을 넓혔다.</p></li>
<li><p><strong>적절한 모델 선택의 중요성</strong>: 문제의 특성, 데이터, 자원 등을 고려하여 적합한 임베딩 전략과 모델을 선택하는 것은 여전히 중요하다.</p></li>
</ul>
<p>문맥을 이해하는 텍스트 벡터화 기술은 기계가 인간의 언어를 더 잘 이해하고 상호작용하는 데 기여하며 지속적으로 발전하고 있다. 이러한 기술은 사회 여러 분야에 혁신을 가져올 잠재력을 가지고 있다.</p>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-3-0.nn_contextual_enbedding.html</guid>
  <pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 벡터화: FastText의 이해</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-2-3.nn_static_FastText.html</link>
  <description><![CDATA[ 




<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 기존 Word2Vec이나 GloVe와 달리 단어를 내부 단어(subword)들의 집합으로 간주하여 임베딩하는 FastText 모델의 기본 원리를 설명한다.</p>
<ul>
<li><strong>기존 방법의 한계와 FastText의 접근법</strong>:
<ul>
<li>Word2Vec, GloVe 등은 단어를 하나의 단위로 취급하여 OOV(Out-of-Vocabulary) 문제에 취약하고, 단어 내부의 형태론적 정보를 활용하기 어렵다는 한계가 있다.</li>
<li>FastText는 단어를 문자 n-gram(character n-grams)으로 분해하고, 이 n-gram 벡터들의 합으로 단어 벡터를 표현함으로써 이러한 문제를 해결하고자 한다.</li>
</ul></li>
<li><strong>FastText의 특징</strong>:
<ul>
<li><strong>Subword 정보 활용</strong>: 각 단어는 고유 벡터와 함께 해당 단어를 구성하는 모든 문자 n-gram 벡터들의 합으로 표현된다.</li>
<li><strong>OOV 문제 완화</strong>: 훈련 시 보지 못한 단어라도, 구성 n-gram이 훈련 데이터에 존재하면 해당 n-gram 벡터들을 조합하여 의미적으로 유사한 벡터를 생성할 수 있다.</li>
<li><strong>형태론적 정보 포착</strong>: 어근이나 접사와 같은 형태론적 특징을 공유하는 단어들은 유사한 n-gram을 공유하게 되어, 결과적으로 유사한 임베딩 벡터를 갖게 된다. 이는 특히 교착어에서 유용하다.</li>
</ul></li>
<li><strong>결론</strong>: FastText가 subword 정보를 활용하여 OOV 문제 및 희귀 단어 처리에 강점을 가지며, 형태론적 특징을 임베딩에 반영하는 방식을 소개하고 정적 임베딩 분야에서의 의미를 요약한다.</li>
</ul>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    └── RNN 계열
        ├── LSTM
        ├── GRU
        └── ELMo</code></pre>
</section>
<section id="신경망-사용-20082018-static-word-embedding" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 신경망 사용 (2008~2018): Static Word Embedding</h1>
<section id="fasttext-2017" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="fasttext-2017"><span class="header-section-number">3.1</span> FastText (2017)</h2>
<ul>
<li>Sub-word 정보 활용</li>
<li>단어를 character n-gram으로 분해</li>
<li>예: “apple” → {“ap”, “pp”, “pl”, “le”} + “apple”</li>
<li>장점:
<ul>
<li>OOV(Out-of-Vocabulary) 문제 해결</li>
<li>형태학적 정보 포착</li>
<li>한국어와 같은 교착어에 효과적</li>
</ul></li>
</ul>
<section id="기존-방법의-한계" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="기존-방법의-한계"><span class="header-section-number">3.1.1</span> 기존 방법의 한계</h3>
<ul>
<li><p>Word2Vec/GloVe 문제:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 훈련 데이터에 없는 단어 (OOV)</span></span>
<span id="cb2-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"사과"</span> → [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, ...]  ✓ (학습됨)</span>
<span id="cb2-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"사과들"</span> → ???  ✗ (학습 안됨)</span>
<span id="cb2-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"사과나무"</span> → ???  ✗ (학습 안됨)</span></code></pre></div></li>
<li><p>한국어의 특별한 어려움:</p>
<pre><code>"먹다" → "먹는다", "먹었다", "먹고", "먹어서", "먹지만", ...
수천 가지 변형이 가능하지만 모두 같은 어근 "먹"을 공유</code></pre></li>
</ul>
</section>
<section id="fasttext의-해결책" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="fasttext의-해결책"><span class="header-section-number">3.1.2</span> FastText의 해결책</h3>
<ul>
<li>Subword 분해</li>
<li>Character n-gram 분해
<ul>
<li>예시: “사과” (n=2,3으로 설정)</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"사과"</span> 분해:</span>
<span id="cb4-2"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>gram: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;사"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"과&gt;"</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># &lt;, &gt;는 단어 경계 표시</span></span>
<span id="cb4-3"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>gram: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;사과"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과&gt;"</span></span>
<span id="cb4-4"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> 전체 단어: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과"</span></span>
<span id="cb4-5"></span>
<span id="cb4-6">최종 n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>gram 집합: {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;사"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"과&gt;"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;사과"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과&gt;"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과"</span>}</span></code></pre></div>
<ul>
<li>벡터 표현
<ul>
<li><p>기존 Word2Vec:</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">vector(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lookup_table[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과"</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 하나의 벡터</span></span></code></pre></div></li>
<li><p>FastText:</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">vector(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vector(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;사"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> vector(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> vector(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"과&gt;"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> </span>
<span id="cb6-2">      vector(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;사과"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> vector(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과&gt;"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> vector(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과"</span>)</span>
<span id="cb6-3">      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># n-gram 벡터들의 합</span></span></code></pre></div></li>
</ul></li>
<li>OOV 문제 해결 과정
<ul>
<li>새로운 단어 처리</li>
</ul>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 훈련 시 보지 못한 단어: "사과나무"</span></span>
<span id="cb7-2"></span>
<span id="cb7-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1단계: n-gram 분해</span></span>
<span id="cb7-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"사과나무"</span> → {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;사"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"과나"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나무"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"무&gt;"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;사과"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과나"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"과나무"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나무&gt;"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과나무"</span>}</span>
<span id="cb7-5"></span>
<span id="cb7-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2단계: 학습된 n-gram 벡터 찾기</span></span>
<span id="cb7-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"&lt;사"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, ...]     ✓ (있음)</span>
<span id="cb7-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"사과"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, ...]    ✓ (있음)  </span>
<span id="cb7-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"과나"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, ...]    ✗ (없음 → <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">벡터</span>)</span>
<span id="cb7-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"나무"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, ...]    ✓ (있음)</span>
<span id="cb7-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"무&gt;"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, ...]     ✓ (있음)</span>
<span id="cb7-12"></span>
<span id="cb7-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3단계: 합계 계산</span></span>
<span id="cb7-14">vector(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"사과나무"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(존재하는_ngram_벡터들) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> 개수</span></code></pre></div>
<ul>
<li>결과: “사과나무”는 “사과”와 “나무”의 의미를 모두 반영한 벡터를 얻음!</li>
</ul></li>
<li>형태학적 정보 포착
<ul>
<li>한국어 예시</li>
</ul>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">훈련 데이터:</span>
<span id="cb8-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"먹는다"</span> → {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;먹"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"먹는"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"는다"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"다&gt;"</span>, ...}</span>
<span id="cb8-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"먹었다"</span> → {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;먹"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"먹었"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"었다"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"다&gt;"</span>, ...}  </span>
<span id="cb8-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"먹고"</span> → {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;먹"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"먹고"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"고&gt;"</span>, ...}</span>
<span id="cb8-5"></span>
<span id="cb8-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 공통 n-gram "&lt;먹", "먹"이 반복 학습됨</span></span>
<span id="cb8-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># → "먹" 관련 의미가 강화됨</span></span>
<span id="cb8-8"></span>
<span id="cb8-9">새로운 단어 <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"먹거나"</span>:</span>
<span id="cb8-10">{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;먹"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"먹거"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"거나"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"나&gt;"</span>, ...}</span>
<span id="cb8-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "&lt;먹" n-gram을 통해 "먹다"와 관련된 의미를 자동으로 얻음!</span></span></code></pre></div>
<ul>
<li>영어 예시</li>
</ul>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"running"</span> ↔ <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"run"</span></span>
<span id="cb9-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"running"</span> → {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ru"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"un"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nn"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ni"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"in"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ng"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"run"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"unn"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nni"</span>, ...}</span>
<span id="cb9-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"run"</span> → {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ru"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"un"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"run"</span>, ...}</span>
<span id="cb9-4"></span>
<span id="cb9-5">공통 부분: {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ru"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"un"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"run"</span>}을 통해 관계 학습</span></code></pre></div></li>
</ul>
</section>
<section id="실제-성능-비교" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="실제-성능-비교"><span class="header-section-number">3.1.3</span> 실제 성능 비교</h3>
<ul>
<li>한국어 형태소 분석 없이도 효과적</li>
<li><strong>Word2Vec</strong>:</li>
</ul>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"좋다"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, ...]</span>
<span id="cb10-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"좋은"</span>: ??? (없으면 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>UNK<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span>)</span>
<span id="cb10-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"좋아서"</span>: ??? (없으면 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>UNK<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span>)</span></code></pre></div>
<ul>
<li><strong>FastText</strong>:</li>
</ul>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"좋다"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, ...]</span>
<span id="cb11-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"좋은"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.18</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.38</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.12</span>, ...]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "좋" n-gram으로 유추</span></span>
<span id="cb11-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"좋아서"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.19</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.39</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.11</span>, ...]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "좋" n-gram으로 유추</span></span></code></pre></div>
<ul>
<li>결과: 형태소 분석기 없이도 어근의 의미를 공유하는 벡터들을 얻을 수 있음!</li>
<li>Trade-off:
<ul>
<li>장점: OOV 해결, 형태학적 정보 포착</li>
<li>단점: n-gram 개수만큼 파라미터 증가</li>
</ul></li>
<li>Word2Vec: 단어 수 × 벡터 차원</li>
<li>FastText: (단어 수 + 모든_ngram_수) × 벡터 차원</li>
</ul>
<p><strong>실용적 해결책</strong>: 빈도가 낮은 n-gram은 제외하여 크기 조절</p>
</section>
</section>
</section>
<section id="결론" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> 결론</h1>
<p>본 문서에서는 단어를 문자 n-gram들의 합으로 표현하여 임베딩하는 FastText 모델의 기본적인 아이디어와 작동 방식을 살펴보았다. FastText는 Word2Vec의 Skip-gram 모델을 기반으로 하면서 subword 정보를 추가적으로 활용하는 것이 핵심이다.</p>
<ul>
<li><strong>FastText의 주요 특징</strong>:
<ul>
<li><strong>Subword 단위 임베딩</strong>: 단어를 더 작은 단위인 문자 n-gram으로 분해하고, 각 n-gram에 대한 임베딩 벡터를 학습한다. 최종 단어 임베딩은 해당 단어를 구성하는 n-gram 벡터들의 합 (또는 평균)으로 구성된다. 이로 인해 단어의 내부 구조와 형태론적 특징을 포착할 수 있다.</li>
<li><strong>OOV 문제 대처</strong>: 훈련 말뭉치에 등장하지 않은 단어(OOV)에 대해서도, 해당 단어의 n-gram들이 훈련 과정에서 학습되었다면, 이를 조합하여 새로운 단어의 벡터를 추정할 수 있다. 이는 Word2Vec이나 GloVe가 OOV 단어에 대해 특별한 토큰(예: <code>&lt;UNK&gt;</code>)으로 처리하거나 무시하는 것과 대조적이다.</li>
<li><strong>희귀 단어 및 형태소 풍부 언어에 유리</strong>: 등장 빈도가 낮은 희귀 단어의 경우에도 구성 n-gram 정보를 통해 상대적으로 안정적인 임베딩을 얻을 수 있으며, 한국어와 같이 어미나 조사가 다양하게 변하는 교착어에서 형태론적 유사성을 잘 반영할 수 있다.</li>
</ul></li>
<li><strong>FastText의 의미</strong>:
<ul>
<li>FastText는 단어 자체뿐만 아니라 단어를 구성하는 내부 요소(subword)까지 고려함으로써 기존 정적 임베딩 방법론들의 한계를 일부 개선했다. 특히 OOV 문제에 대한 실용적인 해결책을 제시하고, 단어의 형태론적 정보를 임베딩에 자연스럽게 통합했다는 점에서 의미가 있다.</li>
<li>Word2Vec, GloVe와 함께 FastText는 다양한 NLP 문제에서 효과적인 단어 표현 방법으로 활용되며, 정적 임베딩 기법의 중요한 갈래를 형성한다.</li>
</ul></li>
</ul>
<p>결론적으로, FastText는 subword 정보를 활용하여 단어 벡터의 표현력을 높이고 OOV 문제에 강인함을 보이는 유용한 정적 임베딩 방법이다. 단순하면서도 효과적인 아이디어로 많은 자연어 처리 응용 분야에 기여했다.</p>


</section>

 ]]></description>
  <category>NLP</category>
  <category>Deep Learning</category>
  <guid>kk3225.netlify.app/docs/blog/posts/Deep_Learning/NLP/4-2-3.nn_static_FastText.html</guid>
  <pubDate>Tue, 07 Jan 2025 15:00:00 GMT</pubDate>
</item>
</channel>
</rss>
