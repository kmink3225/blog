<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="ko" xml:lang="ko"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>unsloth_llama3_íŒŒì¸íŠœë‹_alpaca â€“ Kwangmin Kim</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete-preset-algolia.umd.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "algolia": {
    "application-id": "DUOR1DRC9D",
    "search-only-api-key": "f264da5dea684ffb9e9b4a574af3ed61",
    "index-name": "prod_QUARTO",
    "analytics-events": true,
    "show-logo": true,
    "libDir": "site_libs"
  },
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": true,
  "language": {
    "search-no-results-text": "ì¼ì¹˜ ì—†ìŒ",
    "search-matching-documents-text": "ì¼ì¹˜ëœ ë¬¸ì„œ",
    "search-copy-link-title": "ê²€ìƒ‰ ë§í¬ ë³µì‚¬",
    "search-hide-matches-text": "ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼ ìˆ¨ê¸°ê¸°",
    "search-more-match-text": "ì¶”ê°€ ê²€ìƒ‰ê²°ê³¼",
    "search-more-matches-text": "ì¶”ê°€ ê²€ìƒ‰ê²°ê³¼",
    "search-clear-button-title": "ì œê±°",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "ì·¨ì†Œ",
    "search-submit-button-title": "ê²€ìƒ‰",
    "search-label": "ê²€ìƒ‰"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js"></script>


<script type="text/javascript">
var ALGOLIA_INSIGHTS_SRC = "https://cdn.jsdelivr.net/npm/search-insights/dist/search-insights.iife.min.js";
!function(e,a,t,n,s,i,c){e.AlgoliaAnalyticsObject=s,e[s]=e[s]||function(){
(e[s].queue=e[s].queue||[]).push(arguments)},i=a.createElement(t),c=a.getElementsByTagName(t)[0],
i.async=1,i.src=n,c.parentNode.insertBefore(i,c)
}(window,document,"script",ALGOLIA_INSIGHTS_SRC,"aa");
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/autocomplete-plugin-algolia-insights">

</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6W0EKFMWBN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6W0EKFMWBN', { 'anonymize_ip': true});
</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: black;
      }

      .quarto-title-block .quarto-title-banner {
        color: black;
background: #EDF3F9;
      }
</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>
<style>
.custom-footer { 
  text-align: center; 
  font-size: 0.8em; 
  color: #666; 
  margin-top: 2rem; 
}
</style>


<link rel="stylesheet" href="../../../../../styles.css">
<meta property="og:title" content="Kwangmin Kim">
<meta property="og:description" content="blog">
<meta property="og:site_name" content="Kwangmin Kim">
<meta name="twitter:title" content="Kwangmin Kim">
<meta name="twitter:description" content="blog">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="ê²€ìƒ‰"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="íƒìƒ‰ ì „í™˜" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html"> 
<span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"> <i class="bi bi-github" role="img" aria-label="Github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"> <i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="ë‹¤í¬ ëª¨ë“œ ì „í™˜"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">ëª©ì°¨</h2>
   
  <ul>
  <li><a href="#unsloth" id="toc-unsloth" class="nav-link active" data-scroll-target="#unsloth"><span class="header-section-number">1</span> Unsloth</a>
  <ul class="collapse">
  <li><a href="#ë°ì´í„°-ì¤€ë¹„" id="toc-ë°ì´í„°-ì¤€ë¹„" class="nav-link" data-scroll-target="#ë°ì´í„°-ì¤€ë¹„"><span class="header-section-number">1.1</span> ë°ì´í„° ì¤€ë¹„</a></li>
  <li><a href="#ëª¨ë¸-í›ˆë ¨í•˜ê¸°" id="toc-ëª¨ë¸-í›ˆë ¨í•˜ê¸°" class="nav-link" data-scroll-target="#ëª¨ë¸-í›ˆë ¨í•˜ê¸°"><span class="header-section-number">1.2</span> ëª¨ë¸ í›ˆë ¨í•˜ê¸°</a></li>
  <li><a href="#ì¶”ë¡ " id="toc-ì¶”ë¡ " class="nav-link" data-scroll-target="#ì¶”ë¡ "><span class="header-section-number">1.3</span> ì¶”ë¡ </a></li>
  <li><a href="#vllmì„-ìœ„í•œ-float16-ì €ì¥" id="toc-vllmì„-ìœ„í•œ-float16-ì €ì¥" class="nav-link" data-scroll-target="#vllmì„-ìœ„í•œ-float16-ì €ì¥"><span class="header-section-number">1.4</span> VLLMì„ ìœ„í•œ float16 ì €ì¥</a></li>
  <li><a href="#ì˜µì…˜-1-ë¡œì»¬ì—-ì €ì¥" id="toc-ì˜µì…˜-1-ë¡œì»¬ì—-ì €ì¥" class="nav-link" data-scroll-target="#ì˜µì…˜-1-ë¡œì»¬ì—-ì €ì¥"><span class="header-section-number">1.5</span> ì˜µì…˜ 1) ë¡œì»¬ì— ì €ì¥</a></li>
  <li><a href="#ì˜µì…˜-2-huggingface-ì—-ì—…ë¡œë“œ" id="toc-ì˜µì…˜-2-huggingface-ì—-ì—…ë¡œë“œ" class="nav-link" data-scroll-target="#ì˜µì…˜-2-huggingface-ì—-ì—…ë¡œë“œ"><span class="header-section-number">1.6</span> ì˜µì…˜ 2) HuggingFace ì— ì—…ë¡œë“œ</a></li>
  <li><a href="#gguf-ë³€í™˜" id="toc-gguf-ë³€í™˜" class="nav-link" data-scroll-target="#gguf-ë³€í™˜"><span class="header-section-number">1.7</span> GGUF ë³€í™˜</a></li>
  <li><a href="#ì˜µì…˜1-ë¡œì»¬-ì €ì¥" id="toc-ì˜µì…˜1-ë¡œì»¬-ì €ì¥" class="nav-link" data-scroll-target="#ì˜µì…˜1-ë¡œì»¬-ì €ì¥"><span class="header-section-number">1.8</span> ì˜µì…˜1) ë¡œì»¬ ì €ì¥</a></li>
  <li><a href="#ì˜µì…˜2-huggingface-í—ˆë¸Œì—-ì—…ë¡œë“œ" id="toc-ì˜µì…˜2-huggingface-í—ˆë¸Œì—-ì—…ë¡œë“œ" class="nav-link" data-scroll-target="#ì˜µì…˜2-huggingface-í—ˆë¸Œì—-ì—…ë¡œë“œ"><span class="header-section-number">1.9</span> ì˜µì…˜2) HuggingFace í—ˆë¸Œì— ì—…ë¡œë“œ</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><code>torch.cuda.get_device_capability()</code> í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ CUDA ì¥ì¹˜ì˜ major ë²„ì „ê³¼ minor ë²„ì „ì„ ì¡°íšŒí•©ë‹ˆë‹¤.</p>
<div id="5b47d8c1" class="cell" data-outputid="786ec057-c05d-4f44-c329-15c98a60a8e0" data-execution_count="1">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co"># CUDA ì¥ì¹˜ì˜ ì£¼ìš” ë²„ì „ê³¼ ë¶€ ë²„ì „ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>major_version, minor_version <span class="op">=</span> torch.cuda.get_device_capability()</span>
<span id="cb1-5"><a href="#cb1-5"></a>major_version, minor_version</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>(9, 0)</code></pre>
</div>
</div>
<p><code>unsloth</code> ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ê´€ë ¨ ë””íœë˜ì‹œë¥¼ ì„¤ì¹˜í•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.</p>
<ul>
<li>Colab í™˜ê²½ì—ì„œ <code>torch</code> ë²„ì „ 2.2.1ê³¼ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íŒ¨í‚¤ì§€ë¥¼ íšŒí”¼í•˜ê¸° ìœ„í•´ <code>unsloth</code> ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë³„ë„ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤.</li>
<li>GPUì˜ ì¢…ë¥˜(ì‹ í˜• ë˜ëŠ” êµ¬í˜•)ì— ë”°ë¼ ì¡°ê±´ë¶€ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.
<ul>
<li>ì‹ í˜• GPU(Ampere, Hopper ë“±)ì˜ ê²½ìš°, <code>packaging</code>, <code>ninja</code>, <code>einops</code>, <code>flash-attn</code>, <code>xformers</code>, <code>trl</code>, <code>peft</code>, <code>accelerate</code>, <code>bitsandbytes</code> íŒ¨í‚¤ì§€ë¥¼ ì˜ì¡´ì„± ì—†ì´ ì„¤ì¹˜í•©ë‹ˆë‹¤.</li>
<li>êµ¬í˜• GPU(V100, Tesla T4, RTX 20xx ë“±)ì˜ ê²½ìš°, <code>xformers</code>, <code>trl</code>, <code>peft</code>, <code>accelerate</code>, <code>bitsandbytes</code> íŒ¨í‚¤ì§€ë¥¼ ì˜ì¡´ì„± ì—†ì´ ì„¤ì¹˜í•©ë‹ˆë‹¤.</li>
</ul></li>
<li>ì„¤ì¹˜ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì¶œë ¥ì„ ìˆ¨ê¸°ê¸° ìœ„í•´ <code>%%capture</code> ë§¤ì§ ì»¤ë§¨ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</li>
</ul>
<div id="f2728224" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="op">%%</span>capture</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="co"># Colabì—ì„œ torch 2.2.1ì„ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë¯€ë¡œ, íŒ¨í‚¤ì§€ ì¶©ëŒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë³„ë„ë¡œ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.</span></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="op">!</span>pip install <span class="st">"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"</span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="cf">if</span> major_version <span class="op">&gt;=</span> <span class="dv">8</span>:</span>
<span id="cb3-5"><a href="#cb3-5"></a>    <span class="co"># ìƒˆë¡œìš´ GPU(ì˜ˆ: Ampere, Hopper GPUs - RTX 30xx, RTX 40xx, A100, H100, L40)ì— ì‚¬ìš©í•˜ì„¸ìš”.</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>    <span class="op">!</span>pip install <span class="op">--</span>no<span class="op">-</span>deps packaging ninja einops flash<span class="op">-</span>attn xformers trl peft accelerate bitsandbytes</span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="cf">else</span>:</span>
<span id="cb3-8"><a href="#cb3-8"></a>    <span class="co"># ì˜¤ë˜ëœ GPU(ì˜ˆ: V100, Tesla T4, RTX 20xx)ì— ì‚¬ìš©í•˜ì„¸ìš”.</span></span>
<span id="cb3-9"><a href="#cb3-9"></a>    <span class="op">!</span>pip install <span class="op">--</span>no<span class="op">-</span>deps xformers trl peft accelerate bitsandbytes</span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="cf">pass</span></span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="unsloth" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="unsloth"><span class="header-section-number">1</span> Unsloth</h2>
<ul>
<li><p><code>Unsloth</code>ëŠ” Llama, Mistral, CodeLlama, TinyLlama, Vicuna, Open Hermes ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  Yi, Qwen(<a href="https://huggingface.co/models?sort=trending&amp;search=qwen+llama">llamafied</a>), Deepseek, ëª¨ë“  Llama, Mistral íŒŒìƒ ì•„í‚¤í…ì²˜ë„ ì§€ì›í•©ë‹ˆë‹¤.</p></li>
<li><p><code>Unsloth</code>ëŠ” 16ë¹„íŠ¸ LoRA ë˜ëŠ” 4ë¹„íŠ¸ QLoRAë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ë‘˜ ë‹¤ 2ë°° ë¹ ë¦…ë‹ˆë‹¤.</p></li>
<li><p><code>max_seq_length</code>ëŠ” <a href="https://kaiokendev.github.io/til">kaiokendevì˜</a> ë°©ë²•ì„ í†µí•´ ìë™ìœ¼ë¡œ RoPE ìŠ¤ì¼€ì¼ë§ì„ í•˜ê¸° ë•Œë¬¸ì— ì–´ë–¤ ê°’ìœ¼ë¡œë„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p></li>
</ul>
<p><strong>ìƒˆë¡œìš´ ì†Œì‹</strong>!</p>
<ul>
<li><a href="https://github.com/huggingface/transformers/pull/26037">PR 26037</a>ì„ í†µí•´, ìš°ë¦¬ëŠ” 4ë¹„íŠ¸ ëª¨ë¸ì„ <strong>4ë°° ë¹ ë¥´ê²Œ</strong> ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤! <a href="https://huggingface.co/unsloth">Unsloth Repository</a>ì—ëŠ” Llama, Mistral 4ë¹„íŠ¸ ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤.</li>
</ul>
<p><code>FastLanguageModel.from_pretrained</code> í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.</p>
<ul>
<li>ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´(<code>max_seq_length</code>)ë¥¼ ì„¤ì •í•˜ì—¬ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì…ë ¥ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.</li>
<li>ë°ì´í„° íƒ€ì…(<code>dtype</code>)ì€ ìë™ ê°ì§€ë˜ê±°ë‚˜, íŠ¹ì • í•˜ë“œì›¨ì–´ì— ìµœì í™”ëœ í˜•ì‹(<code>Float16</code>, <code>Bfloat16</code>)ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li>4ë¹„íŠ¸ ì–‘ìí™”(<code>load_in_4bit</code>) ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì„ íƒì ì…ë‹ˆë‹¤.</li>
<li>ì‚¬ì „ ì •ì˜ëœ 4ë¹„íŠ¸ ì–‘ìí™” ëª¨ë¸ ëª©ë¡(<code>fourbit_models</code>)ì—ì„œ ì„ íƒí•˜ì—¬ ë‹¤ìš´ë¡œë“œ ì‹œê°„ì„ ë‹¨ì¶•í•˜ê³  ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li><code>FastLanguageModel.from_pretrained</code> í•¨ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ë©°, ì´ë•Œ ëª¨ë¸ ì´ë¦„(<code>model_name</code>), ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´, ë°ì´í„° íƒ€ì…, 4ë¹„íŠ¸ ë¡œë”© ì—¬ë¶€ë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.</li>
<li>ì„ íƒì ìœ¼ë¡œ, íŠ¹ì • ê²Œì´íŠ¸ ëª¨ë¸ì„ ì‚¬ìš©í•  ê²½ìš° í† í°(<code>token</code>)ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
</ul>
<div id="ebaf5ef0" class="cell" data-outputid="2407a1de-5bb4-41e6-e660-202b4d6645db">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">from</span> unsloth <span class="im">import</span> FastLanguageModel</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">import</span> torch</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a>max_seq_length <span class="op">=</span> <span class="dv">4096</span>  <span class="co"># ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ RoPE ìŠ¤ì¼€ì¼ë§ì„ ìë™ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤!</span></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="co"># ìë™ ê°ì§€ë¥¼ ìœ„í•´ Noneì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Tesla T4, V100ì€ Float16, Ampere+ëŠ” Bfloat16ì„ ì‚¬ìš©í•˜ì„¸ìš”.</span></span>
<span id="cb4-6"><a href="#cb4-6"></a>dtype <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="co"># ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ 4bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Falseì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>load_in_4bit <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-9"><a href="#cb4-9"></a></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="co"># 4ë°° ë¹ ë¥¸ ë‹¤ìš´ë¡œë“œì™€ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì§€ì›í•˜ëŠ” 4bit ì‚¬ì „ ì–‘ìí™” ëª¨ë¸ì…ë‹ˆë‹¤.</span></span>
<span id="cb4-11"><a href="#cb4-11"></a>fourbit_models <span class="op">=</span> [</span>
<span id="cb4-12"><a href="#cb4-12"></a>    <span class="st">"unsloth/mistral-7b-bnb-4bit"</span>,</span>
<span id="cb4-13"><a href="#cb4-13"></a>    <span class="st">"unsloth/mistral-7b-instruct-v0.2-bnb-4bit"</span>,</span>
<span id="cb4-14"><a href="#cb4-14"></a>    <span class="st">"unsloth/llama-2-7b-bnb-4bit"</span>,</span>
<span id="cb4-15"><a href="#cb4-15"></a>    <span class="st">"unsloth/gemma-7b-bnb-4bit"</span>,</span>
<span id="cb4-16"><a href="#cb4-16"></a>    <span class="st">"unsloth/gemma-7b-it-bnb-4bit"</span>,  <span class="co"># Gemma 7bì˜ Instruct ë²„ì „</span></span>
<span id="cb4-17"><a href="#cb4-17"></a>    <span class="st">"unsloth/gemma-2b-bnb-4bit"</span>,</span>
<span id="cb4-18"><a href="#cb4-18"></a>    <span class="st">"unsloth/gemma-2b-it-bnb-4bit"</span>,  <span class="co"># Gemma 2bì˜ Instruct ë²„ì „</span></span>
<span id="cb4-19"><a href="#cb4-19"></a>    <span class="st">"unsloth/llama-3-8b-bnb-4bit"</span>,  <span class="co"># Llama-3 8B</span></span>
<span id="cb4-20"><a href="#cb4-20"></a>]  <span class="co"># ë” ë§ì€ ëª¨ë¸ì€ https://huggingface.co/unsloth ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</span></span>
<span id="cb4-21"><a href="#cb4-21"></a></span>
<span id="cb4-22"><a href="#cb4-22"></a>model, tokenã„´izer <span class="op">=</span> FastLanguageModel.from_pretrained(</span>
<span id="cb4-23"><a href="#cb4-23"></a>    <span class="co"># model_name = "unsloth/llama-3-8b-bnb-4bit",</span></span>
<span id="cb4-24"><a href="#cb4-24"></a>    model_name<span class="op">=</span><span class="st">"beomi/Llama-3-Open-Ko-8B-Instruct-preview"</span>,  <span class="co"># ëª¨ë¸ ì´ë¦„ì„ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb4-25"><a href="#cb4-25"></a>    max_seq_length<span class="op">=</span>max_seq_length,  <span class="co"># ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb4-26"><a href="#cb4-26"></a>    dtype<span class="op">=</span>dtype,  <span class="co"># ë°ì´í„° íƒ€ì…ì„ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb4-27"><a href="#cb4-27"></a>    load_in_4bit<span class="op">=</span>load_in_4bit,  <span class="co"># 4bit ì–‘ìí™” ë¡œë“œ ì—¬ë¶€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb4-28"><a href="#cb4-28"></a>    <span class="co"># token = "hf_...", # ê²Œì´íŠ¸ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í† í°ì„ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆ: meta-llama/Llama-2-7b-hf</span></span>
<span id="cb4-29"><a href="#cb4-29"></a>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>ì´ì œ LoRA ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ëª¨ë“  íŒŒë¼ë¯¸í„° ì¤‘ ë‹¨ 1% ~ 10%ì˜ íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸í•˜ë©´ ë©ë‹ˆë‹¤!</p>
<p>FastLanguageModelì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ëª¨ë“ˆì— ëŒ€í•œ ì„±ëŠ¥ í–¥ìƒ ê¸°ë²•ì„ ì ìš©í•œ ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.</p>
<ul>
<li><code>FastLanguageModel.get_peft_model</code> í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³ , ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì—¬ëŸ¬ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.</li>
<li><code>r</code> íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ì„±ëŠ¥ í–¥ìƒ ê¸°ë²•ì˜ ê°•ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤. ê¶Œì¥ ê°’ìœ¼ë¡œëŠ” 8, 16, 32, 64, 128 ë“±ì´ ìˆìŠµë‹ˆë‹¤.</li>
<li><code>target_modules</code> ë¦¬ìŠ¤íŠ¸ì—ëŠ” ì„±ëŠ¥ í–¥ìƒì„ ì ìš©í•  ëª¨ë¸ì˜ ëª¨ë“ˆ ì´ë¦„ë“¤ì´ í¬í•¨ë©ë‹ˆë‹¤.</li>
<li><code>lora_alpha</code>ì™€ <code>lora_dropout</code>ì„ ì„¤ì •í•˜ì—¬ LoRA(Low-Rank Adaptation) ê¸°ë²•ì˜ ì„¸ë¶€ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.</li>
<li><code>bias</code> ì˜µì…˜ì„ í†µí•´ ëª¨ë¸ì˜ ë°”ì´ì–´ìŠ¤ ì‚¬ìš© ì—¬ë¶€ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìœ¼ë©°, ìµœì í™”ë¥¼ ìœ„í•´ â€œnoneâ€ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤.</li>
<li><code>use_gradient_checkpointing</code> ì˜µì…˜ì„ â€œunslothâ€ë¡œ ì„¤ì •í•˜ì—¬ VRAM ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³ , ë” í° ë°°ì¹˜ í¬ê¸°ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.</li>
<li><code>use_rslora</code> ì˜µì…˜ì„ í†µí•´ Rank Stabilized LoRAë¥¼ ì‚¬ìš©í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.</li>
</ul>
<div id="f990387e" class="cell" data-outputid="663b2111-d856-4e9c-999a-debdc08ca49e" data-execution_count="13">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>model <span class="op">=</span> FastLanguageModel.get_peft_model(</span>
<span id="cb5-2"><a href="#cb5-2"></a>    model,</span>
<span id="cb5-3"><a href="#cb5-3"></a>    r<span class="op">=</span><span class="dv">16</span>,  <span class="co"># 0ë³´ë‹¤ í° ì–´ë–¤ ìˆ«ìë„ ì„ íƒ ê°€ëŠ¥! 8, 16, 32, 64, 128ì´ ê¶Œì¥ë©ë‹ˆë‹¤.</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>    lora_alpha<span class="op">=</span><span class="dv">32</span>,  <span class="co"># LoRA ì•ŒíŒŒ ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb5-5"><a href="#cb5-5"></a>    lora_dropout<span class="op">=</span><span class="fl">0.05</span>,  <span class="co"># ë“œë¡­ì•„ì›ƒì„ ì§€ì›í•©ë‹ˆë‹¤.</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>    target_modules<span class="op">=</span>[</span>
<span id="cb5-7"><a href="#cb5-7"></a>        <span class="st">"q_proj"</span>,</span>
<span id="cb5-8"><a href="#cb5-8"></a>        <span class="st">"k_proj"</span>,</span>
<span id="cb5-9"><a href="#cb5-9"></a>        <span class="st">"v_proj"</span>,</span>
<span id="cb5-10"><a href="#cb5-10"></a>        <span class="st">"o_proj"</span>,</span>
<span id="cb5-11"><a href="#cb5-11"></a>        <span class="st">"gate_proj"</span>,</span>
<span id="cb5-12"><a href="#cb5-12"></a>        <span class="st">"up_proj"</span>,</span>
<span id="cb5-13"><a href="#cb5-13"></a>        <span class="st">"down_proj"</span>,</span>
<span id="cb5-14"><a href="#cb5-14"></a>    ],  <span class="co"># íƒ€ê²Ÿ ëª¨ë“ˆì„ ì§€ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>    bias<span class="op">=</span><span class="st">"none"</span>,  <span class="co"># ë°”ì´ì–´ìŠ¤ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.</span></span>
<span id="cb5-16"><a href="#cb5-16"></a>    <span class="co"># True ë˜ëŠ” "unsloth"ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¤ìš° ê¸´ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•´ VRAMì„ 30% ëœ ì‚¬ìš©í•˜ê³ , 2ë°° ë” í° ë°°ì¹˜ í¬ê¸°ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.</span></span>
<span id="cb5-17"><a href="#cb5-17"></a>    use_gradient_checkpointing<span class="op">=</span><span class="st">"unsloth"</span>,</span>
<span id="cb5-18"><a href="#cb5-18"></a>    random_state<span class="op">=</span><span class="dv">123</span>,  <span class="co"># ë‚œìˆ˜ ìƒíƒœë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb5-19"><a href="#cb5-19"></a>    use_rslora<span class="op">=</span><span class="va">False</span>,  <span class="co"># ìˆœìœ„ ì•ˆì •í™” LoRAë¥¼ ì§€ì›í•©ë‹ˆë‹¤.</span></span>
<span id="cb5-20"><a href="#cb5-20"></a>    loftq_config<span class="op">=</span><span class="va">None</span>,  <span class="co"># LoftQë¥¼ ì§€ì›í•©ë‹ˆë‹¤.</span></span>
<span id="cb5-21"><a href="#cb5-21"></a>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="ë°ì´í„°-ì¤€ë¹„" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="ë°ì´í„°-ì¤€ë¹„"><span class="header-section-number">1.1</span> ë°ì´í„° ì¤€ë¹„</h3>
<p><strong>[ì¤‘ìš”]</strong></p>
<ul>
<li>í† í°í™”ëœ ì¶œë ¥ì— <strong>EOS_TOKEN</strong>ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”! ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë¬´í•œ ìƒì„±ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
</ul>
<p><strong>[ì°¸ê³ ]</strong></p>
<ul>
<li>ì˜¤ì§ ì™„ì„±ëœ í…ìŠ¤íŠ¸ë§Œì„ í•™ìŠµí•˜ê³ ì í•œë‹¤ë©´, TRLì˜ ë¬¸ì„œë¥¼ <a href="https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only">ì—¬ê¸°</a>ì—ì„œ í™•ì¸í•˜ì„¸ìš”.</li>
</ul>
<p><code>load_dataset</code> í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³ , ì´ë¥¼ íŠ¹ì • í˜•ì‹ìœ¼ë¡œ í¬ë§¤íŒ…í•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.</p>
<ul>
<li><code>load_dataset</code> í•¨ìˆ˜ë¡œ â€œteddylee777/QA-Dataset-miniâ€ ë°ì´í„°ì…‹ì„ â€œtrainâ€ ë¶„í• ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.</li>
<li>ë°ì´í„°ì…‹ì˜ ê° ì˜ˆì œì— ëŒ€í•´ <code>formatting_prompts_func</code> í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ í¬ë§¤íŒ…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
<ul>
<li>ì´ í•¨ìˆ˜ëŠ” â€œinstructionâ€ê³¼ â€œoutputâ€ í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í¬ë§·ì— ë§ê²Œ í…ìŠ¤íŠ¸ë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.</li>
<li>ì¬êµ¬ì„±ëœ í…ìŠ¤íŠ¸ëŠ” <code>alpaca_prompt</code> í¬ë§·ì„ ë”°ë¥´ë©°, ê° í•­ëª©ì˜ ëì—ëŠ” <code>EOS_TOKEN</code>ì„ ì¶”ê°€í•˜ì—¬ ìƒì„±ì´ ì¢…ë£Œë˜ë„ë¡ í•©ë‹ˆë‹¤.</li>
</ul></li>
<li>ìµœì¢…ì ìœ¼ë¡œ, í¬ë§¤íŒ…ëœ í…ìŠ¤íŠ¸ëŠ” â€œtextâ€ í‚¤ë¥¼ ê°€ì§„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.</li>
<li>ì´ ê³¼ì •ì„ í†µí•´, AI ëª¨ë¸ì´ ì²˜ë¦¬í•˜ê¸° ì í•©í•œ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.</li>
</ul>
<div id="1eb5d1bf" class="cell" data-outputid="88da0b45-853b-4b14-e95f-6f1019e08390" data-execution_count="14">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb6-2"><a href="#cb6-2"></a></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="co"># EOS_TOKENì€ ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” í† í°ì…ë‹ˆë‹¤. ì´ í† í°ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.</span></span>
<span id="cb6-4"><a href="#cb6-4"></a>EOS_TOKEN <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb6-5"><a href="#cb6-5"></a></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="co"># AlpacaPromptë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì‹œì‚¬í•­ì„ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.</span></span>
<span id="cb6-7"><a href="#cb6-7"></a>alpaca_prompt <span class="op">=</span> <span class="st">"""Below is an instruction that describes a task. Write a response that appropriately completes the request.</span></span>
<span id="cb6-8"><a href="#cb6-8"></a></span>
<span id="cb6-9"><a href="#cb6-9"></a><span class="st">### Instruction:</span></span>
<span id="cb6-10"><a href="#cb6-10"></a><span class="sc">{}</span></span>
<span id="cb6-11"><a href="#cb6-11"></a></span>
<span id="cb6-12"><a href="#cb6-12"></a><span class="st">### Response:</span></span>
<span id="cb6-13"><a href="#cb6-13"></a><span class="sc">{}</span><span class="st">"""</span></span>
<span id="cb6-14"><a href="#cb6-14"></a></span>
<span id="cb6-15"><a href="#cb6-15"></a></span>
<span id="cb6-16"><a href="#cb6-16"></a><span class="co"># ì£¼ì–´ì§„ ì˜ˆì‹œë“¤ì„ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.</span></span>
<span id="cb6-17"><a href="#cb6-17"></a><span class="kw">def</span> formatting_prompts_func(examples):</span>
<span id="cb6-18"><a href="#cb6-18"></a>    instructions <span class="op">=</span> examples[<span class="st">"instruction"</span>]  <span class="co"># ì§€ì‹œì‚¬í•­ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.</span></span>
<span id="cb6-19"><a href="#cb6-19"></a>    outputs <span class="op">=</span> examples[<span class="st">"output"</span>]  <span class="co"># ì¶œë ¥ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.</span></span>
<span id="cb6-20"><a href="#cb6-20"></a>    texts <span class="op">=</span> []  <span class="co"># í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.</span></span>
<span id="cb6-21"><a href="#cb6-21"></a>    <span class="cf">for</span> instruction, output <span class="kw">in</span> <span class="bu">zip</span>(instructions, outputs):</span>
<span id="cb6-22"><a href="#cb6-22"></a>        <span class="co"># EOS_TOKENì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ìƒì„±ì´ ë¬´í•œíˆ ì§„í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</span></span>
<span id="cb6-23"><a href="#cb6-23"></a>        text <span class="op">=</span> alpaca_prompt.<span class="bu">format</span>(instruction, output) <span class="op">+</span> EOS_TOKEN</span>
<span id="cb6-24"><a href="#cb6-24"></a>        texts.append(text)</span>
<span id="cb6-25"><a href="#cb6-25"></a>    <span class="cf">return</span> {</span>
<span id="cb6-26"><a href="#cb6-26"></a>        <span class="st">"text"</span>: texts,  <span class="co"># í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.</span></span>
<span id="cb6-27"><a href="#cb6-27"></a>    }</span>
<span id="cb6-28"><a href="#cb6-28"></a></span>
<span id="cb6-29"><a href="#cb6-29"></a></span>
<span id="cb6-30"><a href="#cb6-30"></a><span class="co"># "teddylee777/QA-Dataset-mini" ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. í›ˆë ¨ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.</span></span>
<span id="cb6-31"><a href="#cb6-31"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"teddylee777/QA-Dataset-mini"</span>, split<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb6-32"><a href="#cb6-32"></a></span>
<span id="cb6-33"><a href="#cb6-33"></a><span class="co"># ë°ì´í„°ì…‹ì— formatting_prompts_func í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤. ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.</span></span>
<span id="cb6-34"><a href="#cb6-34"></a>dataset <span class="op">=</span> dataset.<span class="bu">map</span>(</span>
<span id="cb6-35"><a href="#cb6-35"></a>    formatting_prompts_func,</span>
<span id="cb6-36"><a href="#cb6-36"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-37"><a href="#cb6-37"></a>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="ëª¨ë¸-í›ˆë ¨í•˜ê¸°" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="ëª¨ë¸-í›ˆë ¨í•˜ê¸°"><span class="header-section-number">1.2</span> ëª¨ë¸ í›ˆë ¨í•˜ê¸°</h3>
<p>ì´ì œ Huggingface TRLì˜ <code>SFTTrainer</code>ë¥¼ ì‚¬ìš©í•´ ë´…ì‹œë‹¤!</p>
<ul>
<li>ì°¸ê³  ë¬¸ì„œ: <a href="https://huggingface.co/docs/trl/sft_trainer">TRL SFT ë¬¸ì„œ</a></li>
</ul>
<div id="b41b8fd9" class="cell" data-outputid="173f3598-b388-4f1b-98b6-55d7d9ff554d" data-execution_count="15">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="im">from</span> trl <span class="im">import</span> SFTTrainer</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="im">from</span> transformers <span class="im">import</span> TrainingArguments</span>
<span id="cb7-3"><a href="#cb7-3"></a></span>
<span id="cb7-4"><a href="#cb7-4"></a>tokenizer.padding_side <span class="op">=</span> <span class="st">"right"</span>  <span class="co"># í† í¬ë‚˜ì´ì €ì˜ íŒ¨ë”©ì„ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb7-5"><a href="#cb7-5"></a></span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="co"># SFTTrainerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµ ì„¤ì •</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>trainer <span class="op">=</span> SFTTrainer(</span>
<span id="cb7-8"><a href="#cb7-8"></a>    model<span class="op">=</span>model,  <span class="co"># í•™ìŠµí•  ëª¨ë¸</span></span>
<span id="cb7-9"><a href="#cb7-9"></a>    tokenizer<span class="op">=</span>tokenizer,  <span class="co"># í† í¬ë‚˜ì´ì €</span></span>
<span id="cb7-10"><a href="#cb7-10"></a>    train_dataset<span class="op">=</span>dataset,  <span class="co"># í•™ìŠµ ë°ì´í„°ì…‹</span></span>
<span id="cb7-11"><a href="#cb7-11"></a>    eval_dataset<span class="op">=</span>dataset,</span>
<span id="cb7-12"><a href="#cb7-12"></a>    dataset_text_field<span class="op">=</span><span class="st">"text"</span>,  <span class="co"># ë°ì´í„°ì…‹ì—ì„œ í…ìŠ¤íŠ¸ í•„ë“œì˜ ì´ë¦„</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>    max_seq_length<span class="op">=</span>max_seq_length,  <span class="co"># ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´</span></span>
<span id="cb7-14"><a href="#cb7-14"></a>    dataset_num_proc<span class="op">=</span><span class="dv">2</span>,  <span class="co"># ë°ì´í„° ì²˜ë¦¬ì— ì‚¬ìš©í•  í”„ë¡œì„¸ìŠ¤ ìˆ˜</span></span>
<span id="cb7-15"><a href="#cb7-15"></a>    packing<span class="op">=</span><span class="va">False</span>,  <span class="co"># ì§§ì€ ì‹œí€€ìŠ¤ì— ëŒ€í•œ í•™ìŠµ ì†ë„ë¥¼ 5ë°° ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆìŒ</span></span>
<span id="cb7-16"><a href="#cb7-16"></a>    args<span class="op">=</span>TrainingArguments(</span>
<span id="cb7-17"><a href="#cb7-17"></a>        per_device_train_batch_size<span class="op">=</span><span class="dv">2</span>,  <span class="co"># ê° ë””ë°”ì´ìŠ¤ë‹¹ í›ˆë ¨ ë°°ì¹˜ í¬ê¸°</span></span>
<span id="cb7-18"><a href="#cb7-18"></a>        gradient_accumulation_steps<span class="op">=</span><span class="dv">4</span>,  <span class="co"># ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„</span></span>
<span id="cb7-19"><a href="#cb7-19"></a>        warmup_steps<span class="op">=</span><span class="dv">5</span>,  <span class="co"># ì›œì—… ìŠ¤í… ìˆ˜</span></span>
<span id="cb7-20"><a href="#cb7-20"></a>        num_train_epochs<span class="op">=</span><span class="dv">3</span>,  <span class="co"># í›ˆë ¨ ì—í­ ìˆ˜</span></span>
<span id="cb7-21"><a href="#cb7-21"></a>        max_steps<span class="op">=</span><span class="dv">100</span>,  <span class="co"># ìµœëŒ€ ìŠ¤í… ìˆ˜</span></span>
<span id="cb7-22"><a href="#cb7-22"></a>        do_eval<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-23"><a href="#cb7-23"></a>        evaluation_strategy<span class="op">=</span><span class="st">"steps"</span>,</span>
<span id="cb7-24"><a href="#cb7-24"></a>        logging_steps<span class="op">=</span><span class="dv">1</span>,  <span class="co"># logging ìŠ¤í… ìˆ˜</span></span>
<span id="cb7-25"><a href="#cb7-25"></a>        learning_rate<span class="op">=</span><span class="fl">2e-4</span>,  <span class="co"># í•™ìŠµë¥ </span></span>
<span id="cb7-26"><a href="#cb7-26"></a>        fp16<span class="op">=</span><span class="kw">not</span> torch.cuda.is_bf16_supported(),  <span class="co"># fp16 ì‚¬ìš© ì—¬ë¶€, bf16ì´ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ì‚¬ìš©</span></span>
<span id="cb7-27"><a href="#cb7-27"></a>        bf16<span class="op">=</span>torch.cuda.is_bf16_supported(),  <span class="co"># bf16 ì‚¬ìš© ì—¬ë¶€, bf16ì´ ì§€ì›ë˜ëŠ” ê²½ìš°ì—ë§Œ ì‚¬ìš©</span></span>
<span id="cb7-28"><a href="#cb7-28"></a>        optim<span class="op">=</span><span class="st">"adamw_8bit"</span>,  <span class="co"># ìµœì í™” ì•Œê³ ë¦¬ì¦˜</span></span>
<span id="cb7-29"><a href="#cb7-29"></a>        weight_decay<span class="op">=</span><span class="fl">0.01</span>,  <span class="co"># ê°€ì¤‘ì¹˜ ê°ì†Œ</span></span>
<span id="cb7-30"><a href="#cb7-30"></a>        lr_scheduler_type<span class="op">=</span><span class="st">"cosine"</span>,  <span class="co"># í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìœ í˜•</span></span>
<span id="cb7-31"><a href="#cb7-31"></a>        seed<span class="op">=</span><span class="dv">123</span>,  <span class="co"># ëœë¤ ì‹œë“œ</span></span>
<span id="cb7-32"><a href="#cb7-32"></a>        output_dir<span class="op">=</span><span class="st">"outputs"</span>,  <span class="co"># ì¶œë ¥ ë””ë ‰í† ë¦¬</span></span>
<span id="cb7-33"><a href="#cb7-33"></a>    ),</span>
<span id="cb7-34"><a href="#cb7-34"></a>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/home/ubuntu/miniforge3/envs/ft_py310/lib/python3.10/site-packages/transformers/training_args.py:1483: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"6e0c5ee182cb4d1c94959fa8040cc1d2","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>max_steps is given, it will override any value given in num_train_epochs</code></pre>
</div>
</div>
<ul>
<li>GPUì˜ í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.</li>
<li><code>torch.cuda.get_device_properties(0)</code>ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ GPUì˜ ì†ì„±ì„ ì¡°íšŒí•©ë‹ˆë‹¤.</li>
<li><code>torch.cuda.max_memory_reserved()</code>ë¥¼ í†µí•´ í˜„ì¬ ì˜ˆì•½ëœ ìµœëŒ€ ë©”ëª¨ë¦¬ë¥¼ GB ë‹¨ìœ„ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.</li>
<li>GPUì˜ ì´ ë©”ëª¨ë¦¬ í¬ê¸°ë¥¼ GB ë‹¨ìœ„ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.</li>
<li>GPU ì´ë¦„ê³¼ ìµœëŒ€ ë©”ëª¨ë¦¬ í¬ê¸°, í˜„ì¬ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ í¬ê¸°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.</li>
</ul>
<div id="b91c701f" class="cell" data-outputid="2f6e197b-f8e0-451f-e498-b21d6b581c3d" data-execution_count="16">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>gpu_stats <span class="op">=</span> torch.cuda.get_device_properties(<span class="dv">0</span>)  <span class="co"># GPU ì†ì„± ê°€ì ¸ì˜¤ê¸°</span></span>
<span id="cb10-3"><a href="#cb10-3"></a>start_gpu_memory <span class="op">=</span> <span class="bu">round</span>(</span>
<span id="cb10-4"><a href="#cb10-4"></a>    torch.cuda.max_memory_reserved() <span class="op">/</span> <span class="dv">1024</span> <span class="op">/</span> <span class="dv">1024</span> <span class="op">/</span> <span class="dv">1024</span>, <span class="dv">3</span></span>
<span id="cb10-5"><a href="#cb10-5"></a>)  <span class="co"># ì‹œì‘ ì‹œ ì˜ˆì•½ëœ GPU ë©”ëª¨ë¦¬ ê³„ì‚°</span></span>
<span id="cb10-6"><a href="#cb10-6"></a>max_memory <span class="op">=</span> <span class="bu">round</span>(</span>
<span id="cb10-7"><a href="#cb10-7"></a>    gpu_stats.total_memory <span class="op">/</span> <span class="dv">1024</span> <span class="op">/</span> <span class="dv">1024</span> <span class="op">/</span> <span class="dv">1024</span>, <span class="dv">3</span></span>
<span id="cb10-8"><a href="#cb10-8"></a>)  <span class="co"># GPUì˜ ìµœëŒ€ ë©”ëª¨ë¦¬ ê³„ì‚°</span></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="bu">print</span>(</span>
<span id="cb10-10"><a href="#cb10-10"></a>    <span class="ss">f"GPU = </span><span class="sc">{</span>gpu_stats<span class="sc">.</span>name<span class="sc">}</span><span class="ss">. Max memory = </span><span class="sc">{</span>max_memory<span class="sc">}</span><span class="ss"> GB."</span></span>
<span id="cb10-11"><a href="#cb10-11"></a>)  <span class="co"># GPU ì´ë¦„ê³¼ ìµœëŒ€ ë©”ëª¨ë¦¬ ì¶œë ¥</span></span>
<span id="cb10-12"><a href="#cb10-12"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>start_gpu_memory<span class="sc">}</span><span class="ss"> GB of memory reserved."</span>)  <span class="co"># ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ ì–‘ ì¶œë ¥</span></span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>GPU = NVIDIA H100 80GB HBM3. Max memory = 79.109 GB.
11.514 GB of memory reserved.</code></pre>
</div>
</div>
<div id="91622afd" class="cell" data-outputid="f12e2b79-d75e-4b1b-a7c6-0ba3dc065818" data-execution_count="17">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>trainer_stats <span class="op">=</span> trainer.train()  <span class="co"># ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê³  í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.</span></span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="100" max="100" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [100/100 02:54, Epoch 50/50]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Step</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>3.110500</td>
<td>2.937336</td>
</tr>
<tr class="even">
<td>2</td>
<td>2.808400</td>
<td>2.871062</td>
</tr>
<tr class="odd">
<td>3</td>
<td>3.006300</td>
<td>2.561022</td>
</tr>
<tr class="even">
<td>4</td>
<td>2.482600</td>
<td>2.150073</td>
</tr>
<tr class="odd">
<td>5</td>
<td>2.239700</td>
<td>1.732249</td>
</tr>
<tr class="even">
<td>6</td>
<td>1.672000</td>
<td>1.332531</td>
</tr>
<tr class="odd">
<td>7</td>
<td>1.292300</td>
<td>1.121440</td>
</tr>
<tr class="even">
<td>8</td>
<td>1.205800</td>
<td>0.961627</td>
</tr>
<tr class="odd">
<td>9</td>
<td>0.919500</td>
<td>0.802066</td>
</tr>
<tr class="even">
<td>10</td>
<td>0.866600</td>
<td>0.640975</td>
</tr>
<tr class="odd">
<td>11</td>
<td>0.616000</td>
<td>0.499137</td>
</tr>
<tr class="even">
<td>12</td>
<td>0.558800</td>
<td>0.349512</td>
</tr>
<tr class="odd">
<td>13</td>
<td>0.298000</td>
<td>0.243137</td>
</tr>
<tr class="even">
<td>14</td>
<td>0.320100</td>
<td>0.148554</td>
</tr>
<tr class="odd">
<td>15</td>
<td>0.127700</td>
<td>0.110356</td>
</tr>
<tr class="even">
<td>16</td>
<td>0.151500</td>
<td>0.073943</td>
</tr>
<tr class="odd">
<td>17</td>
<td>0.068000</td>
<td>0.069657</td>
</tr>
<tr class="even">
<td>18</td>
<td>0.084000</td>
<td>0.058026</td>
</tr>
<tr class="odd">
<td>19</td>
<td>0.047100</td>
<td>0.057014</td>
</tr>
<tr class="even">
<td>20</td>
<td>0.071200</td>
<td>0.046704</td>
</tr>
<tr class="odd">
<td>21</td>
<td>0.048200</td>
<td>0.044196</td>
</tr>
<tr class="even">
<td>22</td>
<td>0.049000</td>
<td>0.052707</td>
</tr>
<tr class="odd">
<td>23</td>
<td>0.042200</td>
<td>0.060044</td>
</tr>
<tr class="even">
<td>24</td>
<td>0.076800</td>
<td>0.042833</td>
</tr>
<tr class="odd">
<td>25</td>
<td>0.051500</td>
<td>0.039529</td>
</tr>
<tr class="even">
<td>26</td>
<td>0.037800</td>
<td>0.041504</td>
</tr>
<tr class="odd">
<td>27</td>
<td>0.044800</td>
<td>0.040255</td>
</tr>
<tr class="even">
<td>28</td>
<td>0.041500</td>
<td>0.039923</td>
</tr>
<tr class="odd">
<td>29</td>
<td>0.040700</td>
<td>0.039858</td>
</tr>
<tr class="even">
<td>30</td>
<td>0.044600</td>
<td>0.036614</td>
</tr>
<tr class="odd">
<td>31</td>
<td>0.034900</td>
<td>0.035189</td>
</tr>
<tr class="even">
<td>32</td>
<td>0.040700</td>
<td>0.033273</td>
</tr>
<tr class="odd">
<td>33</td>
<td>0.034000</td>
<td>0.032872</td>
</tr>
<tr class="even">
<td>34</td>
<td>0.035200</td>
<td>0.032674</td>
</tr>
<tr class="odd">
<td>35</td>
<td>0.032000</td>
<td>0.033309</td>
</tr>
<tr class="even">
<td>36</td>
<td>0.038300</td>
<td>0.032616</td>
</tr>
<tr class="odd">
<td>37</td>
<td>0.034500</td>
<td>0.032572</td>
</tr>
<tr class="even">
<td>38</td>
<td>0.033900</td>
<td>0.032702</td>
</tr>
<tr class="odd">
<td>39</td>
<td>0.031800</td>
<td>0.032903</td>
</tr>
<tr class="even">
<td>40</td>
<td>0.036300</td>
<td>0.032324</td>
</tr>
<tr class="odd">
<td>41</td>
<td>0.034900</td>
<td>0.031295</td>
</tr>
<tr class="even">
<td>42</td>
<td>0.032100</td>
<td>0.031286</td>
</tr>
<tr class="odd">
<td>43</td>
<td>0.031800</td>
<td>0.032069</td>
</tr>
<tr class="even">
<td>44</td>
<td>0.034500</td>
<td>0.032129</td>
</tr>
<tr class="odd">
<td>45</td>
<td>0.034100</td>
<td>0.032002</td>
</tr>
<tr class="even">
<td>46</td>
<td>0.032800</td>
<td>0.031181</td>
</tr>
<tr class="odd">
<td>47</td>
<td>0.031600</td>
<td>0.030908</td>
</tr>
<tr class="even">
<td>48</td>
<td>0.033300</td>
<td>0.030805</td>
</tr>
<tr class="odd">
<td>49</td>
<td>0.033000</td>
<td>0.030693</td>
</tr>
<tr class="even">
<td>50</td>
<td>0.031000</td>
<td>0.030909</td>
</tr>
<tr class="odd">
<td>51</td>
<td>0.029900</td>
<td>0.030916</td>
</tr>
<tr class="even">
<td>52</td>
<td>0.033600</td>
<td>0.030903</td>
</tr>
<tr class="odd">
<td>53</td>
<td>0.028700</td>
<td>0.030974</td>
</tr>
<tr class="even">
<td>54</td>
<td>0.035600</td>
<td>0.030879</td>
</tr>
<tr class="odd">
<td>55</td>
<td>0.030600</td>
<td>0.030693</td>
</tr>
<tr class="even">
<td>56</td>
<td>0.033200</td>
<td>0.030456</td>
</tr>
<tr class="odd">
<td>57</td>
<td>0.030600</td>
<td>0.030581</td>
</tr>
<tr class="even">
<td>58</td>
<td>0.032100</td>
<td>0.030487</td>
</tr>
<tr class="odd">
<td>59</td>
<td>0.031300</td>
<td>0.030363</td>
</tr>
<tr class="even">
<td>60</td>
<td>0.030900</td>
<td>0.030410</td>
</tr>
<tr class="odd">
<td>61</td>
<td>0.030600</td>
<td>0.030501</td>
</tr>
<tr class="even">
<td>62</td>
<td>0.032500</td>
<td>0.030264</td>
</tr>
<tr class="odd">
<td>63</td>
<td>0.030400</td>
<td>0.030365</td>
</tr>
<tr class="even">
<td>64</td>
<td>0.032000</td>
<td>0.030379</td>
</tr>
<tr class="odd">
<td>65</td>
<td>0.030200</td>
<td>0.030307</td>
</tr>
<tr class="even">
<td>66</td>
<td>0.032300</td>
<td>0.030228</td>
</tr>
<tr class="odd">
<td>67</td>
<td>0.029600</td>
<td>0.030157</td>
</tr>
<tr class="even">
<td>68</td>
<td>0.031900</td>
<td>0.030151</td>
</tr>
<tr class="odd">
<td>69</td>
<td>0.031600</td>
<td>0.029995</td>
</tr>
<tr class="even">
<td>70</td>
<td>0.030200</td>
<td>0.029961</td>
</tr>
<tr class="odd">
<td>71</td>
<td>0.030400</td>
<td>0.030038</td>
</tr>
<tr class="even">
<td>72</td>
<td>0.031000</td>
<td>0.030159</td>
</tr>
<tr class="odd">
<td>73</td>
<td>0.030600</td>
<td>0.029986</td>
</tr>
<tr class="even">
<td>74</td>
<td>0.030800</td>
<td>0.030061</td>
</tr>
<tr class="odd">
<td>75</td>
<td>0.031300</td>
<td>0.030066</td>
</tr>
<tr class="even">
<td>76</td>
<td>0.030400</td>
<td>0.030008</td>
</tr>
<tr class="odd">
<td>77</td>
<td>0.030900</td>
<td>0.029997</td>
</tr>
<tr class="even">
<td>78</td>
<td>0.030000</td>
<td>0.030033</td>
</tr>
<tr class="odd">
<td>79</td>
<td>0.032100</td>
<td>0.029890</td>
</tr>
<tr class="even">
<td>80</td>
<td>0.029800</td>
<td>0.030026</td>
</tr>
<tr class="odd">
<td>81</td>
<td>0.028300</td>
<td>0.029941</td>
</tr>
<tr class="even">
<td>82</td>
<td>0.032200</td>
<td>0.029849</td>
</tr>
<tr class="odd">
<td>83</td>
<td>0.031900</td>
<td>0.029864</td>
</tr>
<tr class="even">
<td>84</td>
<td>0.029300</td>
<td>0.029983</td>
</tr>
<tr class="odd">
<td>85</td>
<td>0.028100</td>
<td>0.029869</td>
</tr>
<tr class="even">
<td>86</td>
<td>0.032600</td>
<td>0.029824</td>
</tr>
<tr class="odd">
<td>87</td>
<td>0.027900</td>
<td>0.029906</td>
</tr>
<tr class="even">
<td>88</td>
<td>0.033300</td>
<td>0.029897</td>
</tr>
<tr class="odd">
<td>89</td>
<td>0.027900</td>
<td>0.029946</td>
</tr>
<tr class="even">
<td>90</td>
<td>0.033400</td>
<td>0.029855</td>
</tr>
<tr class="odd">
<td>91</td>
<td>0.029700</td>
<td>0.029818</td>
</tr>
<tr class="even">
<td>92</td>
<td>0.030800</td>
<td>0.029852</td>
</tr>
<tr class="odd">
<td>93</td>
<td>0.028600</td>
<td>0.029863</td>
</tr>
<tr class="even">
<td>94</td>
<td>0.031700</td>
<td>0.029766</td>
</tr>
<tr class="odd">
<td>95</td>
<td>0.029700</td>
<td>0.029852</td>
</tr>
<tr class="even">
<td>96</td>
<td>0.031000</td>
<td>0.029774</td>
</tr>
<tr class="odd">
<td>97</td>
<td>0.030400</td>
<td>0.029805</td>
</tr>
<tr class="even">
<td>98</td>
<td>0.030700</td>
<td>0.029817</td>
</tr>
<tr class="odd">
<td>99</td>
<td>0.029400</td>
<td>0.029897</td>
</tr>
<tr class="even">
<td>100</td>
<td>0.031000</td>
<td>0.029763</td>
</tr>
</tbody>
</table>
<p>
</p></div>
</div>
<p>PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í›ˆë ¨ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ í›ˆë ¨ ì‹œê°„ì„ ê³„ì‚°í•˜ê³  ì¶œë ¥í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.</p>
<ul>
<li><code>torch.cuda.max_memory_reserved()</code>ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ì¤‘ì— ì˜ˆì•½ëœ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.</li>
<li>í›ˆë ¨ ì‹œì‘ ì‹œì ì˜ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ë¹„êµí•˜ì—¬ LoRA(Low-Rank Adaptation)ë¥¼ ìœ„í•´ ì‚¬ìš©ëœ ì¶”ê°€ ë©”ëª¨ë¦¬ ì–‘ì„ ê³„ì‚°í•©ë‹ˆë‹¤.</li>
<li>ì „ì²´ GPU ë©”ëª¨ë¦¬ ëŒ€ë¹„ ì‚¬ìš©ëœ ë©”ëª¨ë¦¬ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤.</li>
<li>í›ˆë ¨ì— ì†Œìš”ëœ ì´ ì‹œê°„ì„ ì´ˆì™€ ë¶„ ë‹¨ìœ„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.</li>
<li>ì˜ˆì•½ëœ ìµœëŒ€ ë©”ëª¨ë¦¬, LoRAë¥¼ ìœ„í•´ ì‚¬ìš©ëœ ë©”ëª¨ë¦¬, ê·¸ë¦¬ê³  ì´ë“¤ì´ ì „ì²´ GPU ë©”ëª¨ë¦¬ ëŒ€ë¹„ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ì„ ì¶œë ¥í•©ë‹ˆë‹¤.</li>
</ul>
<div id="0f17487f" class="cell" data-outputid="c3cc7749-a144-49b2-a5ab-0a0994193f8b" data-execution_count="18">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># ìµœì¢… ë©”ëª¨ë¦¬ ë° ì‹œê°„ í†µê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>used_memory <span class="op">=</span> <span class="bu">round</span>(</span>
<span id="cb13-3"><a href="#cb13-3"></a>    torch.cuda.max_memory_reserved() <span class="op">/</span> <span class="dv">1024</span> <span class="op">/</span> <span class="dv">1024</span> <span class="op">/</span> <span class="dv">1024</span>, <span class="dv">3</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>)  <span class="co"># ì‚¬ìš©ëœ ìµœëŒ€ ë©”ëª¨ë¦¬ë¥¼ GB ë‹¨ìœ„ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.</span></span>
<span id="cb13-5"><a href="#cb13-5"></a>used_memory_for_lora <span class="op">=</span> <span class="bu">round</span>(</span>
<span id="cb13-6"><a href="#cb13-6"></a>    used_memory <span class="op">-</span> start_gpu_memory, <span class="dv">3</span></span>
<span id="cb13-7"><a href="#cb13-7"></a>)  <span class="co"># LoRAë¥¼ ìœ„í•´ ì‚¬ìš©ëœ ë©”ëª¨ë¦¬ë¥¼ GB ë‹¨ìœ„ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.</span></span>
<span id="cb13-8"><a href="#cb13-8"></a>used_percentage <span class="op">=</span> <span class="bu">round</span>(</span>
<span id="cb13-9"><a href="#cb13-9"></a>    used_memory <span class="op">/</span> max_memory <span class="op">*</span> <span class="dv">100</span>, <span class="dv">3</span></span>
<span id="cb13-10"><a href="#cb13-10"></a>)  <span class="co"># ìµœëŒ€ ë©”ëª¨ë¦¬ ëŒ€ë¹„ ì‚¬ìš©ëœ ë©”ëª¨ë¦¬ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤.</span></span>
<span id="cb13-11"><a href="#cb13-11"></a>lora_percentage <span class="op">=</span> <span class="bu">round</span>(</span>
<span id="cb13-12"><a href="#cb13-12"></a>    used_memory_for_lora <span class="op">/</span> max_memory <span class="op">*</span> <span class="dv">100</span>, <span class="dv">3</span></span>
<span id="cb13-13"><a href="#cb13-13"></a>)  <span class="co"># ìµœëŒ€ ë©”ëª¨ë¦¬ ëŒ€ë¹„ LoRAë¥¼ ìœ„í•´ ì‚¬ìš©ëœ ë©”ëª¨ë¦¬ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤.</span></span>
<span id="cb13-14"><a href="#cb13-14"></a><span class="bu">print</span>(</span>
<span id="cb13-15"><a href="#cb13-15"></a>    <span class="ss">f"</span><span class="sc">{</span>trainer_stats<span class="sc">.</span>metrics[<span class="st">'train_runtime'</span>]<span class="sc">}</span><span class="ss"> seconds used for training."</span></span>
<span id="cb13-16"><a href="#cb13-16"></a>)  <span class="co"># í›ˆë ¨ì— ì‚¬ìš©ëœ ì‹œê°„ì„ ì´ˆ ë‹¨ìœ„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.</span></span>
<span id="cb13-17"><a href="#cb13-17"></a><span class="bu">print</span>(</span>
<span id="cb13-18"><a href="#cb13-18"></a>    <span class="co"># í›ˆë ¨ì— ì‚¬ìš©ëœ ì‹œê°„ì„ ë¶„ ë‹¨ìœ„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.</span></span>
<span id="cb13-19"><a href="#cb13-19"></a>    <span class="ss">f"</span><span class="sc">{</span><span class="bu">round</span>(trainer_stats.metrics[<span class="st">'train_runtime'</span>]<span class="op">/</span><span class="dv">60</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss"> minutes used for training."</span></span>
<span id="cb13-20"><a href="#cb13-20"></a>)</span>
<span id="cb13-21"><a href="#cb13-21"></a><span class="bu">print</span>(</span>
<span id="cb13-22"><a href="#cb13-22"></a>    <span class="ss">f"Peak reserved memory = </span><span class="sc">{</span>used_memory<span class="sc">}</span><span class="ss"> GB."</span></span>
<span id="cb13-23"><a href="#cb13-23"></a>)  <span class="co"># ì˜ˆì•½ëœ ìµœëŒ€ ë©”ëª¨ë¦¬ë¥¼ GB ë‹¨ìœ„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.</span></span>
<span id="cb13-24"><a href="#cb13-24"></a><span class="bu">print</span>(</span>
<span id="cb13-25"><a href="#cb13-25"></a>    <span class="ss">f"Peak reserved memory for training = </span><span class="sc">{</span>used_memory_for_lora<span class="sc">}</span><span class="ss"> GB."</span></span>
<span id="cb13-26"><a href="#cb13-26"></a>)  <span class="co"># í›ˆë ¨ì„ ìœ„í•´ ì˜ˆì•½ëœ ìµœëŒ€ ë©”ëª¨ë¦¬ë¥¼ GB ë‹¨ìœ„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.</span></span>
<span id="cb13-27"><a href="#cb13-27"></a><span class="bu">print</span>(</span>
<span id="cb13-28"><a href="#cb13-28"></a>    <span class="ss">f"Peak reserved memory % of max memory = </span><span class="sc">{</span>used_percentage<span class="sc">}</span><span class="ss"> %."</span></span>
<span id="cb13-29"><a href="#cb13-29"></a>)  <span class="co"># ìµœëŒ€ ë©”ëª¨ë¦¬ ëŒ€ë¹„ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ì˜ ë¹„ìœ¨ì„ ì¶œë ¥í•©ë‹ˆë‹¤.</span></span>
<span id="cb13-30"><a href="#cb13-30"></a><span class="bu">print</span>(</span>
<span id="cb13-31"><a href="#cb13-31"></a>    <span class="ss">f"Peak reserved memory for training % of max memory = </span><span class="sc">{</span>lora_percentage<span class="sc">}</span><span class="ss"> %."</span></span>
<span id="cb13-32"><a href="#cb13-32"></a>)  <span class="co"># ìµœëŒ€ ë©”ëª¨ë¦¬ ëŒ€ë¹„ í›ˆë ¨ì„ ìœ„í•´ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ì˜ ë¹„ìœ¨ì„ ì¶œë ¥í•©ë‹ˆë‹¤.</span></span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>176.7808 seconds used for training.
2.95 minutes used for training.
Peak reserved memory = 12.986 GB.
Peak reserved memory for training = 1.472 GB.
Peak reserved memory % of max memory = 16.415 %.
Peak reserved memory for training % of max memory = 1.861 %.</code></pre>
</div>
</div>
</section>
<section id="ì¶”ë¡ " class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="ì¶”ë¡ "><span class="header-section-number">1.3</span> ì¶”ë¡ </h3>
<p>ëª¨ë¸ì„ ì‹¤í–‰í•´ ë´…ì‹œë‹¤! ì§€ì‹œì‚¬í•­ê³¼ ì…ë ¥ê°’ì„ ë³€ê²½í•  ìˆ˜ ìˆìœ¼ë©°, ì¶œë ¥ê°’ì€ ë¹„ì›Œë‘ì„¸ìš”!</p>
<p><code>TextStreamer</code>ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ì†ì ì¸ ì¶”ë¡ ì„ ìˆ˜í–‰í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì „ì²´ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ëŒ€ì‹  í† í°ë³„ë¡œ ìƒì„± ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<ul>
<li><code>FastLanguageModel.for_inference(model)</code>ì„ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„ë¥¼ 2ë°° í–¥ìƒì‹œí‚µë‹ˆë‹¤.</li>
<li><code>tokenizer</code>ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • í¬ë§·ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ í† í°í™”í•˜ê³ , ì´ë¥¼ CUDA ê¸°ë°˜ì˜ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ê³„ì†í•˜ëŠ” ì§€ì‹œë¬¸, ì…ë ¥ê°’, ê·¸ë¦¬ê³  ì¶œë ¥ê°’ì„ ìœ„í•œ ë¹ˆ ê³µê°„ì„ í¬í•¨í•©ë‹ˆë‹¤.</li>
<li><code>TextStreamer</code> ê°ì²´ë¥¼ <code>tokenizer</code>ì™€ í•¨ê»˜ ì´ˆê¸°í™”í•˜ì—¬ í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ì„ í™œì„±í™”í•©ë‹ˆë‹¤.</li>
<li><code>model.generate</code> í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ìƒì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ë•Œ, ìµœëŒ€ 128ê°œì˜ ìƒˆë¡œìš´ í† í°ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•˜ê³ , <code>TextStreamer</code>ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.</li>
</ul>
<p><code>StoppingCriteria</code>ì™€ <code>StoppingCriteriaList</code>ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • í† í°ì—ì„œ ìƒì„±ì„ ì¤‘ë‹¨í•˜ëŠ” ë°©ë²•ì„ êµ¬í˜„í•©ë‹ˆë‹¤.</p>
<ul>
<li><code>StopOnToken</code> í´ë˜ìŠ¤ëŠ” <code>StoppingCriteria</code>ë¥¼ ìƒì†ë°›ì•„, ìƒì„± ì¤‘ íŠ¹ì • í† í°(<code>stop_token_id</code>)ì´ ë‚˜íƒ€ë‚˜ë©´ ìƒì„±ì„ ì¤‘ë‹¨í•˜ë„ë¡ í•©ë‹ˆë‹¤.</li>
<li><code>stop_token</code> ë³€ìˆ˜ì— ì¤‘ë‹¨í•  í† í°ì„ ë¬¸ìì—´ë¡œ ì§€ì •í•©ë‹ˆë‹¤.</li>
<li><code>tokenizer.encode</code> ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ <code>stop_token</code>ì„ í•´ë‹¹ ì–¸ì–´ ëª¨ë¸ì˜ í† í° IDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.</li>
<li><code>StoppingCriteriaList</code>ì— <code>StopOnToken</code> ì¸ìŠ¤í„´ìŠ¤ë¥¼ í¬í•¨ì‹œì¼œ, ìƒì„± ê³¼ì •ì—ì„œ ì´ë¥¼ ì¤‘ë‹¨ ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.</li>
</ul>
<div id="bbfd6d48" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="im">from</span> transformers <span class="im">import</span> StoppingCriteria, StoppingCriteriaList</span>
<span id="cb15-2"><a href="#cb15-2"></a></span>
<span id="cb15-3"><a href="#cb15-3"></a></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="kw">class</span> StopOnToken(StoppingCriteria):</span>
<span id="cb15-5"><a href="#cb15-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, stop_token_id):</span>
<span id="cb15-6"><a href="#cb15-6"></a>        <span class="va">self</span>.stop_token_id <span class="op">=</span> stop_token_id  <span class="co"># ì •ì§€ í† í° IDë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.</span></span>
<span id="cb15-7"><a href="#cb15-7"></a></span>
<span id="cb15-8"><a href="#cb15-8"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, input_ids, scores, <span class="op">**</span>kwargs):</span>
<span id="cb15-9"><a href="#cb15-9"></a>        <span class="cf">return</span> (</span>
<span id="cb15-10"><a href="#cb15-10"></a>            <span class="va">self</span>.stop_token_id <span class="kw">in</span> input_ids[<span class="dv">0</span>]</span>
<span id="cb15-11"><a href="#cb15-11"></a>        )  <span class="co"># ì…ë ¥ëœ ID ì¤‘ ì •ì§€ í† í° IDê°€ ìˆìœ¼ë©´ ì •ì§€í•©ë‹ˆë‹¤.</span></span>
<span id="cb15-12"><a href="#cb15-12"></a></span>
<span id="cb15-13"><a href="#cb15-13"></a></span>
<span id="cb15-14"><a href="#cb15-14"></a><span class="co"># end_tokenì„ ì„¤ì •</span></span>
<span id="cb15-15"><a href="#cb15-15"></a>stop_token <span class="op">=</span> <span class="st">"&lt;|end_of_text|&gt;"</span>  <span class="co"># end_tokenìœ¼ë¡œ ì‚¬ìš©í•  í† í°ì„ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb15-16"><a href="#cb15-16"></a>stop_token_id <span class="op">=</span> tokenizer.encode(stop_token, add_special_tokens<span class="op">=</span><span class="va">False</span>)[</span>
<span id="cb15-17"><a href="#cb15-17"></a>    <span class="dv">0</span></span>
<span id="cb15-18"><a href="#cb15-18"></a>]  <span class="co"># end_tokenì˜ IDë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.</span></span>
<span id="cb15-19"><a href="#cb15-19"></a></span>
<span id="cb15-20"><a href="#cb15-20"></a><span class="co"># Stopping criteria ì„¤ì •</span></span>
<span id="cb15-21"><a href="#cb15-21"></a>stopping_criteria <span class="op">=</span> StoppingCriteriaList(</span>
<span id="cb15-22"><a href="#cb15-22"></a>    [StopOnToken(stop_token_id)]</span>
<span id="cb15-23"><a href="#cb15-23"></a>)  <span class="co"># ì •ì§€ ì¡°ê±´ì„ ì„¤ì •í•©ë‹ˆë‹¤.</span></span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>(ì˜ˆì‹œ 1)</p>
<div id="8f8441b0" class="cell" data-outputid="380dc31c-4d36-4ee0-a32e-891ec36236aa" data-execution_count="25">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="im">from</span> transformers <span class="im">import</span> TextStreamer</span>
<span id="cb16-2"><a href="#cb16-2"></a></span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="co"># FastLanguageModelì„ ì´ìš©í•˜ì—¬ ì¶”ë¡  ì†ë„ë¥¼ 2ë°° ë¹ ë¥´ê²Œ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb16-4"><a href="#cb16-4"></a>FastLanguageModel.for_inference(model)</span>
<span id="cb16-5"><a href="#cb16-5"></a>inputs <span class="op">=</span> tokenizer(</span>
<span id="cb16-6"><a href="#cb16-6"></a>    [</span>
<span id="cb16-7"><a href="#cb16-7"></a>        alpaca_prompt.<span class="bu">format</span>(</span>
<span id="cb16-8"><a href="#cb16-8"></a>            <span class="st">"í…Œë””ë…¸íŠ¸ ìœ íŠœë¸Œ ì±„ë„ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”."</span>,  <span class="co"># ì§€ì‹œì‚¬í•­</span></span>
<span id="cb16-9"><a href="#cb16-9"></a>            <span class="st">""</span>,  <span class="co"># ì¶œë ¥ - ìƒì„±ì„ ìœ„í•´ ì´ ë¶€ë¶„ì„ ë¹„ì›Œë‘¡ë‹ˆë‹¤!</span></span>
<span id="cb16-10"><a href="#cb16-10"></a>        )</span>
<span id="cb16-11"><a href="#cb16-11"></a>    ],</span>
<span id="cb16-12"><a href="#cb16-12"></a>    return_tensors<span class="op">=</span><span class="st">"pt"</span>,</span>
<span id="cb16-13"><a href="#cb16-13"></a>).to(<span class="st">"cuda"</span>)</span>
<span id="cb16-14"><a href="#cb16-14"></a></span>
<span id="cb16-15"><a href="#cb16-15"></a></span>
<span id="cb16-16"><a href="#cb16-16"></a>text_streamer <span class="op">=</span> TextStreamer(tokenizer)</span>
<span id="cb16-17"><a href="#cb16-17"></a>_ <span class="op">=</span> model.generate(</span>
<span id="cb16-18"><a href="#cb16-18"></a>    <span class="op">**</span>inputs,</span>
<span id="cb16-19"><a href="#cb16-19"></a>    streamer<span class="op">=</span>text_streamer,</span>
<span id="cb16-20"><a href="#cb16-20"></a>    max_new_tokens<span class="op">=</span><span class="dv">4096</span>,  <span class="co"># ìµœëŒ€ ìƒì„± í† í° ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb16-21"><a href="#cb16-21"></a>    stopping_criteria<span class="op">=</span>stopping_criteria  <span class="co"># ìƒì„±ì„ ë©ˆì¶œ ê¸°ì¤€ì„ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb16-22"><a href="#cb16-22"></a>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;|begin_of_text|&gt;Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
í…Œë””ë…¸íŠ¸ ìœ íŠœë¸Œ ì±„ë„ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.

### Response:
í…Œë””ë…¸íŠ¸(TeddyNote)ëŠ” ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ ë“±ì˜ ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ” ìœ íŠœë¸Œ ì±„ë„ì…ë‹ˆë‹¤. ì´ ì±„ë„ì„ ìš´ì˜í•˜ëŠ” ì´ê²½ë¡ë‹˜ì€ ë°ì´í„° ë¶„ì„ê³¼ ì¸ê³µì§€ëŠ¥ì— ëŒ€í•œ ë‹¤ì–‘í•œ ê°•ì˜ë¥¼ ì œê³µí•˜ë©°, ì´ˆë³´ìë„ ì‰½ê²Œ ë”°ë¼í•  ìˆ˜ ìˆë„ë¡ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.&lt;|end_of_text|&gt;</code></pre>
</div>
</div>
<p>(ì˜ˆì‹œ2)</p>
<div id="6A62BryCpcST" class="cell" data-outputid="08b374ed-5a25-41cf-9251-1a005e1fe1a6" data-execution_count="28">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>inputs <span class="op">=</span> tokenizer(</span>
<span id="cb19-2"><a href="#cb19-2"></a>    [</span>
<span id="cb19-3"><a href="#cb19-3"></a>        alpaca_prompt.<span class="bu">format</span>(</span>
<span id="cb19-4"><a href="#cb19-4"></a>            <span class="st">"ë­ì²´ì¸ íŠœí† ë¦¬ì–¼ ê³µë¶€í• ë§Œí•œ ì‚¬ì´íŠ¸ëŠ”?"</span>,  <span class="co"># ì§€ì‹œì‚¬í•­</span></span>
<span id="cb19-5"><a href="#cb19-5"></a>            <span class="st">""</span>,  <span class="co"># ì¶œë ¥ - ìƒì„±ì„ ìœ„í•´ ì´ ë¶€ë¶„ì„ ë¹„ì›Œë‘¡ë‹ˆë‹¤!</span></span>
<span id="cb19-6"><a href="#cb19-6"></a>        )</span>
<span id="cb19-7"><a href="#cb19-7"></a>    ],</span>
<span id="cb19-8"><a href="#cb19-8"></a>    return_tensors<span class="op">=</span><span class="st">"pt"</span>,</span>
<span id="cb19-9"><a href="#cb19-9"></a>).to(<span class="st">"cuda"</span>)</span>
<span id="cb19-10"><a href="#cb19-10"></a></span>
<span id="cb19-11"><a href="#cb19-11"></a></span>
<span id="cb19-12"><a href="#cb19-12"></a>text_streamer <span class="op">=</span> TextStreamer(tokenizer)</span>
<span id="cb19-13"><a href="#cb19-13"></a>_ <span class="op">=</span> model.generate(</span>
<span id="cb19-14"><a href="#cb19-14"></a>    <span class="op">**</span>inputs,</span>
<span id="cb19-15"><a href="#cb19-15"></a>    streamer<span class="op">=</span>text_streamer,</span>
<span id="cb19-16"><a href="#cb19-16"></a>    max_new_tokens<span class="op">=</span><span class="dv">4096</span>,  <span class="co"># ìµœëŒ€ ìƒì„± í† í° ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb19-17"><a href="#cb19-17"></a>    stopping_criteria<span class="op">=</span>stopping_criteria  <span class="co"># ìƒì„±ì„ ë©ˆì¶œ ê¸°ì¤€ì„ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb19-18"><a href="#cb19-18"></a>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;|begin_of_text|&gt;Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
ë­ì²´ì¸ íŠœí† ë¦¬ì–¼ ê³µë¶€í• ë§Œí•œ ì‚¬ì´íŠ¸ëŠ”?

### Response:
í…Œë””ë…¸íŠ¸ì˜ LangChain íŠœí† ë¦¬ì–¼ì€ ì´ˆë³´ìë„ ì‰½ê²Œ ë”°ë¼í•  ìˆ˜ ìˆë„ë¡ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤. ë§í¬: https://notebook.ai/_learn/langchain&lt;|end_of_text|&gt;</code></pre>
</div>
</div>
<p>(ì˜ˆì‹œ 3)</p>
<div id="LnTb41D9p8AS" class="cell" data-outputid="51edcd24-b578-47e6-f1f2-790620998e83" data-execution_count="30">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>inputs <span class="op">=</span> tokenizer(</span>
<span id="cb22-2"><a href="#cb22-2"></a>    [</span>
<span id="cb22-3"><a href="#cb22-3"></a>        alpaca_prompt.<span class="bu">format</span>(</span>
<span id="cb22-4"><a href="#cb22-4"></a>            <span class="st">"í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ì´ì–´ê°€ì„¸ìš”.(ìµœëŒ€ 10ê°œ)"</span>,  <span class="co"># ì§€ì‹œì‚¬í•­</span></span>
<span id="cb22-5"><a href="#cb22-5"></a>            <span class="st">"1, 1, 2, 3, 5, 8"</span>,  <span class="co"># ì¶œë ¥ - ì•ë¶€ë¶„ì˜ íŒíŠ¸ ì œê³µ ì˜ˆì‹œ</span></span>
<span id="cb22-6"><a href="#cb22-6"></a>        )</span>
<span id="cb22-7"><a href="#cb22-7"></a>    ],</span>
<span id="cb22-8"><a href="#cb22-8"></a>    return_tensors<span class="op">=</span><span class="st">"pt"</span>,</span>
<span id="cb22-9"><a href="#cb22-9"></a>).to(<span class="st">"cuda"</span>)</span>
<span id="cb22-10"><a href="#cb22-10"></a></span>
<span id="cb22-11"><a href="#cb22-11"></a></span>
<span id="cb22-12"><a href="#cb22-12"></a>text_streamer <span class="op">=</span> TextStreamer(tokenizer)</span>
<span id="cb22-13"><a href="#cb22-13"></a>_ <span class="op">=</span> model.generate(</span>
<span id="cb22-14"><a href="#cb22-14"></a>    <span class="op">**</span>inputs,</span>
<span id="cb22-15"><a href="#cb22-15"></a>    streamer<span class="op">=</span>text_streamer,</span>
<span id="cb22-16"><a href="#cb22-16"></a>    max_new_tokens<span class="op">=</span><span class="dv">4096</span>,  <span class="co"># ìµœëŒ€ ìƒì„± í† í° ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb22-17"><a href="#cb22-17"></a>    stopping_criteria<span class="op">=</span>stopping_criteria  <span class="co"># ìƒì„±ì„ ë©ˆì¶œ ê¸°ì¤€ì„ ì„¤ì •í•©ë‹ˆë‹¤.</span></span>
<span id="cb22-18"><a href="#cb22-18"></a>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;|begin_of_text|&gt;Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ì´ì–´ê°€ì„¸ìš”.(ìµœëŒ€ 10ê°œ)

### Response:
1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144&lt;|end_of_text|&gt;</code></pre>
</div>
</div>
<div id="7425142b" class="cell" data-execution_count="31">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>model.save_pretrained(<span class="st">"Llama-3-Open-Ko-8B-teddynote"</span>)  <span class="co"># ëª¨ë¸ì„ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.</span></span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="co"># model.push_to_hub("your_name/lora_model", token = "...") # ëª¨ë¸ì„ ì˜¨ë¼ì¸ í—ˆë¸Œì— ì €ì¥í•©ë‹ˆë‹¤.</span></span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>ë§Œì•½ ìš°ë¦¬ê°€ ì €ì¥í•œ LoRA ì–´ëŒ‘í„°ë¥¼ ì¶”ë¡ ì„ ìœ„í•´ ë¶ˆëŸ¬ì˜¤ê³  ì‹¶ë‹¤ë©´, <code>False</code>ë¥¼ <code>True</code>ë¡œ ì„¤ì •í•˜ì„¸ìš”.</p>
</section>
<section id="vllmì„-ìœ„í•œ-float16-ì €ì¥" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="vllmì„-ìœ„í•œ-float16-ì €ì¥"><span class="header-section-number">1.4</span> VLLMì„ ìœ„í•œ float16 ì €ì¥</h3>
<p>ìš°ë¦¬ëŠ” <code>float16</code>ìœ¼ë¡œ ì§ì ‘ ì €ì¥í•˜ëŠ” ê²ƒì„ ì§€ì›í•©ë‹ˆë‹¤. <code>float16</code>ì„ ìœ„í•´ì„œëŠ” <code>merged_16bit</code>ë¥¼ ì„ íƒí•˜ê±°ë‚˜, <code>int4</code>ë¥¼ ìœ„í•´ì„œëŠ” <code>merged_4bit</code>ë¥¼ ì„ íƒí•˜ì„¸ìš”. ë˜í•œ, ëŒ€ì²´ ë°©ì•ˆìœ¼ë¡œ <code>lora</code> ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<p>ëª¨ë¸ì„ ì €ì¥í•˜ê³  Hugging Face Hubì— í‘¸ì‹œí•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œì…ë‹ˆë‹¤.</p>
<ul>
<li>16ë¹„íŠ¸ì™€ 4ë¹„íŠ¸ ë³‘í•© ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ì €ì¥í•˜ê³  í‘¸ì‹œí•˜ëŠ” ì¡°ê±´ë¬¸ì´ ìˆìœ¼ë‚˜, ì´ë“¤ì€ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.</li>
<li><code>model.save_pretrained_merged</code> í•¨ìˆ˜ì™€ <code>model.push_to_hub_merged</code> í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬, â€œbeomi/Llama-3-Open-Ko-8Bâ€ ëª¨ë¸ì„ â€œmerged_4bit_forcedâ€ ë°©ì‹ìœ¼ë¡œ ì €ì¥í•˜ê³ , â€œteddylee777/Llama-3-Open-Ko-8B-teddynoteâ€ë¡œ Hugging Face Hubì— í‘¸ì‹œí•©ë‹ˆë‹¤.</li>
<li>LoRA ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì €ì¥í•˜ê³  í‘¸ì‹œí•˜ëŠ” ì½”ë“œë„ í¬í•¨ë˜ì–´ ìˆìœ¼ë‚˜, ì´ ë˜í•œ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.</li>
<li>ëª¨ë“  ì €ì¥ ë° í‘¸ì‹œ ì‘ì—…ì—ëŠ” <code>tokenizer</code>ì™€ íŠ¹ì • <code>save_method</code>ê°€ í•„ìš”í•˜ë©°, í‘¸ì‹œ ì‘ì—…ì—ëŠ” ì¶”ê°€ì ìœ¼ë¡œ <code>token</code>ì´ í•„ìš”í•©ë‹ˆë‹¤.</li>
</ul>
<p>ì €ì¥</p>
<div id="j23USuYYsFLg" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>base_model <span class="op">=</span> <span class="st">"beomi/Llama-3-Open-Ko-8B"</span>  <span class="co"># ë³‘í•©ì„ ìˆ˜í–‰í•  ë² ì´ìŠ¤ ëª¨ë¸</span></span>
<span id="cb26-2"><a href="#cb26-2"></a>huggingface_token <span class="op">=</span> <span class="st">""</span>  <span class="co"># HuggingFace í† í°</span></span>
<span id="cb26-3"><a href="#cb26-3"></a>huggingface_repo <span class="op">=</span> <span class="st">"Llama-3-Open-Ko-8B-Instruct-teddynote"</span>  <span class="co"># ëª¨ë¸ì„ ì—…ë¡œë“œí•  repository</span></span>
<span id="cb26-4"><a href="#cb26-4"></a>save_method <span class="op">=</span> (</span>
<span id="cb26-5"><a href="#cb26-5"></a>    <span class="st">"merged_16bit"</span>  <span class="co"># "merged_4bit", "merged_4bit_forced", "merged_16bit", "lora"</span></span>
<span id="cb26-6"><a href="#cb26-6"></a>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="ì˜µì…˜-1-ë¡œì»¬ì—-ì €ì¥" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="ì˜µì…˜-1-ë¡œì»¬ì—-ì €ì¥"><span class="header-section-number">1.5</span> ì˜µì…˜ 1) ë¡œì»¬ì— ì €ì¥</h3>
<div id="ExDul25jxScw" class="cell" data-outputid="705172fe-03be-4102-ba47-fa451e9ef8b6" data-execution_count="16">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>model.save_pretrained_merged(</span>
<span id="cb27-2"><a href="#cb27-2"></a>    base_model,</span>
<span id="cb27-3"><a href="#cb27-3"></a>    tokenizer,</span>
<span id="cb27-4"><a href="#cb27-4"></a>    save_method<span class="op">=</span>save_method,  <span class="co"># ì €ì¥ ë°©ì‹ì„ 16ë¹„íŠ¸ ë³‘í•©ìœ¼ë¡œ ì„¤ì •</span></span>
<span id="cb27-5"><a href="#cb27-5"></a>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Unsloth: Merging 4bit and LoRA weights to 16bit...
Unsloth: Will use up to 156.76 out of 221.18 RAM for saving.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 112.18it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Unsloth: Saving tokenizer... Done.
Unsloth: Saving model... This might take 5 minutes for Llama-7b...
Done.</code></pre>
</div>
</div>
</section>
<section id="ì˜µì…˜-2-huggingface-ì—-ì—…ë¡œë“œ" class="level3" data-number="1.6">
<h3 data-number="1.6" class="anchored" data-anchor-id="ì˜µì…˜-2-huggingface-ì—-ì—…ë¡œë“œ"><span class="header-section-number">1.6</span> ì˜µì…˜ 2) HuggingFace ì— ì—…ë¡œë“œ</h3>
<div id="C-tqusAUsnmE" class="cell" data-outputid="c3aad4b3-b52e-4da8-b583-6ac3ae823fb8" data-execution_count="18">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="co"># Hub ì— ì—…ë¡œë“œ</span></span>
<span id="cb31-2"><a href="#cb31-2"></a>model.push_to_hub_merged(</span>
<span id="cb31-3"><a href="#cb31-3"></a>    huggingface_repo,</span>
<span id="cb31-4"><a href="#cb31-4"></a>    tokenizer,</span>
<span id="cb31-5"><a href="#cb31-5"></a>    save_method<span class="op">=</span>save_method,</span>
<span id="cb31-6"><a href="#cb31-6"></a>    token<span class="op">=</span>huggingface_token,</span>
<span id="cb31-7"><a href="#cb31-7"></a>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Unsloth: Merging 4bit and LoRA weights to 16bit...
Unsloth: Will use up to 156.73 out of 221.18 RAM for saving.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 131.65it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Unsloth: Saving to organization with address teddylee777/Llama-3-Open-Ko-8B-Instruct-teddynote
Unsloth: Saving tokenizer... Done.
Unsloth: Saving model... This might take 5 minutes for Llama-7b...
Unsloth: Saving to organization with address teddylee777/Llama-3-Open-Ko-8B-Instruct-teddynote
Unsloth: Uploading all files... Please wait...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"03c7b8628a7c4dc6a935adafedd47534","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ba0f19a5c299492db237706e5bb4b649","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"89208b5c1cba4ebfb794848995c5fe71","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e606e58974fa4380857682370522c535","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7f1a9f7fa9ff47d2a9c69cba9ce75451","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Done.
Saved merged model to https://huggingface.co/None/Llama-3-Open-Ko-8B-Instruct-teddynote</code></pre>
</div>
</div>
</section>
<section id="gguf-ë³€í™˜" class="level3" data-number="1.7">
<h3 data-number="1.7" class="anchored" data-anchor-id="gguf-ë³€í™˜"><span class="header-section-number">1.7</span> GGUF ë³€í™˜</h3>
<p>Unsloth ëŠ” <code>llama.cpp</code>ë¥¼ ë³µì œí•˜ê³  ê¸°ë³¸ì ìœ¼ë¡œ <code>q8_0</code>ì— ì €ì¥í•©ë‹ˆë‹¤. <code>q4_k_m</code>ê³¼ ê°™ì€ ëª¨ë“  ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¡œì»¬ ì €ì¥ì„ ìœ„í•´ì„œëŠ” <code>save_pretrained_gguf</code>ë¥¼ ì‚¬ìš©í•˜ê³ , HFì— ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ì„œëŠ” <code>push_to_hub_gguf</code>ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.</p>
<p><strong>[ì°¸ê³ ]</strong> - ê°œì¸ í† í°ì€ https://huggingface.co/settings/tokens ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
</section>
<section id="ì˜µì…˜1-ë¡œì»¬-ì €ì¥" class="level3" data-number="1.8">
<h3 data-number="1.8" class="anchored" data-anchor-id="ì˜µì…˜1-ë¡œì»¬-ì €ì¥"><span class="header-section-number">1.8</span> ì˜µì…˜1) ë¡œì»¬ ì €ì¥</h3>
<div id="PUrfPmwD0vK9" class="cell">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a><span class="co"># Quantization ë°©ì‹ ì„¤ì •</span></span>
<span id="cb36-2"><a href="#cb36-2"></a>quantization_method <span class="op">=</span> <span class="st">"q8_0"</span>  <span class="co"># "f16" "q8_0" "q4_k_m" "q5_k_m"</span></span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="OdeU9vxA-JY4" class="cell">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb37-2"><a href="#cb37-2"></a></span>
<span id="cb37-3"><a href="#cb37-3"></a>drive.mount(<span class="st">"/content/drive"</span>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="uGPwS_JTtJsq" class="cell" data-outputid="c3fd47ce-bae0-4877-c43f-a7f88ded0e5a" data-execution_count="75">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>model.save_pretrained_gguf(</span>
<span id="cb38-2"><a href="#cb38-2"></a>    <span class="st">"./content/drive/MyDrive/90_HuggingFace/Llama-3-Open-Ko-8B-Instruct-teddynote"</span>,</span>
<span id="cb38-3"><a href="#cb38-3"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb38-4"><a href="#cb38-4"></a>    quantization_method<span class="op">=</span>quantization_method,</span>
<span id="cb38-5"><a href="#cb38-5"></a>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Unsloth: Merging 4bit and LoRA weights to 16bit...
Unsloth: Will use up to 62.71 out of 83.48 RAM for saving.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 69.82it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Unsloth: Saving tokenizer... Done.
Unsloth: Saving model... This might take 5 minutes for Llama-7b...
Done.
==((====))==  Unsloth: Conversion from QLoRA to GGUF information
   \\   /|    [0] Installing llama.cpp will take 3 minutes.
O^O/ \_/ \    [1] Converting HF to GUUF 16bits will take 3 minutes.
\        /    [2] Converting GGUF 16bits to f16 will take 20 minutes.
 "-____-"     In total, you will have to wait around 26 minutes.

Unsloth: [0] Installing llama.cpp. This will take 3 minutes...
Unsloth: [1] Converting model at ./content/drive/MyDrive/90_HuggingFace/Llama-3-Open-Ko-8B-Instruct-teddynote into f16 GGUF format.
The output location will be ././content/drive/MyDrive/90_HuggingFace/Llama-3-Open-Ko-8B-Instruct-teddynote-unsloth.F16.gguf
This will take 3 minutes...
INFO:hf-to-gguf:Loading model: Llama-3-Open-Ko-8B-Instruct-teddynote
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 8192
INFO:hf-to-gguf:gguf: embedding length = 4096
INFO:hf-to-gguf:gguf: feed forward length = 14336
INFO:hf-to-gguf:gguf: head count = 32
INFO:hf-to-gguf:gguf: key-value head count = 8
INFO:hf-to-gguf:gguf: rope theta = 500000.0
INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
INFO:gguf.vocab:Adding 280147 merge(s).
INFO:gguf.vocab:Setting special token type bos to 128000
INFO:gguf.vocab:Setting special token type eos to 128001
INFO:gguf.vocab:Setting special token type pad to 128255
INFO:gguf.vocab:Setting chat_template to {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '&lt;|start_header_id|&gt;' + message['role'] + '&lt;|end_header_id|&gt;

'+ message['content'] | trim + '&lt;|eot_id|&gt;' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;

' }}{% endif %}
INFO:hf-to-gguf:Exporting model to 'content/drive/MyDrive/90_HuggingFace/Llama-3-Open-Ko-8B-Instruct-teddynote-unsloth.F16.gguf'
INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'
INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'
INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --&gt; F16, shape = {4096, 128256}
INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'
INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'
INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'
INFO:hf-to-gguf:output.weight,               torch.bfloat16 --&gt; F16, shape = {4096, 128256}
INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --&gt; F32, shape = {4096}
Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.1G/16.1G [01:09&lt;00:00, 230Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to 'content/drive/MyDrive/90_HuggingFace/Llama-3-Open-Ko-8B-Instruct-teddynote-unsloth.F16.gguf'
Unsloth: Conversion completed! Output location: ././content/drive/MyDrive/90_HuggingFace/Llama-3-Open-Ko-8B-Instruct-teddynote-unsloth.F16.gguf</code></pre>
</div>
</div>
</section>
<section id="ì˜µì…˜2-huggingface-í—ˆë¸Œì—-ì—…ë¡œë“œ" class="level3" data-number="1.9">
<h3 data-number="1.9" class="anchored" data-anchor-id="ì˜µì…˜2-huggingface-í—ˆë¸Œì—-ì—…ë¡œë“œ"><span class="header-section-number">1.9</span> ì˜µì…˜2) HuggingFace í—ˆë¸Œì— ì—…ë¡œë“œ</h3>
<p>ì§€ì›ë˜ëŠ” ëª‡ ê°€ì§€ ì–‘ìí™” ë°©ë²•ë“¤(ì „ì²´ ëª©ë¡ì€ ìš°ë¦¬ì˜ <a href="https://github.com/unslothai/unsloth/wiki#gguf-quantization-options">ìœ„í‚¤ í˜ì´ì§€</a>ì—ì„œ í™•ì¸ ê°€ëŠ¥):</p>
<ul>
<li><code>q8_0</code> - ë¹ ë¥¸ ë³€í™˜. ë†’ì€ ìì› ì‚¬ìš©ì´ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.</li>
<li><code>q4_k_m</code> - ì¶”ì²œë©ë‹ˆë‹¤. attention.wvì™€ feed_forward.w2 í…ì„œì˜ ì ˆë°˜ì— Q6_Kë¥¼ ì‚¬ìš©í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” Q4_Kë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</li>
<li><code>q5_k_m</code> - ì¶”ì²œë©ë‹ˆë‹¤. attention.wvì™€ feed_forward.w2 í…ì„œì˜ ì ˆë°˜ì— Q6_Kë¥¼ ì‚¬ìš©í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” Q5_Kë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</li>
</ul>
<div id="n7LtG2zatLN0" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a><span class="co"># Quantization ë°©ì‹ ì„¤ì •</span></span>
<span id="cb42-2"><a href="#cb42-2"></a>quantization_method <span class="op">=</span> <span class="st">"q8_0"</span>  <span class="co"># "f16" "q8_0" "q4_k_m" "q5_k_m"</span></span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="G3XEM6X5t83k" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>ì½”ë“œ</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a><span class="co"># Hub ì— GGUF ì—…ë¡œë“œ</span></span>
<span id="cb43-2"><a href="#cb43-2"></a>model.push_to_hub_gguf(</span>
<span id="cb43-3"><a href="#cb43-3"></a>    huggingface_repo <span class="op">+</span> <span class="st">"-gguf"</span>,</span>
<span id="cb43-4"><a href="#cb43-4"></a>    tokenizer,</span>
<span id="cb43-5"><a href="#cb43-5"></a>    quantization_method<span class="op">=</span>quantization_method,</span>
<span id="cb43-6"><a href="#cb43-6"></a>    token<span class="op">=</span>huggingface_token,</span>
<span id="cb43-7"><a href="#cb43-7"></a>)</span></code><button title="í´ë¦½ë³´ë“œ ë³µì‚¬" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Unsloth: Merging 4bit and LoRA weights to 16bit...
Unsloth: Will use up to 177.53 out of 221.18 RAM for saving.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 131.00it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Unsloth: Saving tokenizer... Done.
Unsloth: Saving model... This might take 5 minutes for Llama-7b...
Done.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Unsloth: Converting llama model. Can use fast conversion = False.
Unsloth: We must use f16 for non Llama and Mistral models.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>==((====))==  Unsloth: Conversion from QLoRA to GGUF information
   \\   /|    [0] Installing llama.cpp will take 3 minutes.
O^O/ \_/ \    [1] Converting HF to GUUF 16bits will take 3 minutes.
\        /    [2] Converting GGUF 16bits to q8_0 will take 20 minutes.
 "-____-"     In total, you will have to wait around 26 minutes.

Unsloth: [0] Installing llama.cpp. This will take 3 minutes...
Unsloth: [1] Converting model at Llama-3-Open-Ko-8B-Instruct-teddynote-gguf into f16 GGUF format.
The output location will be ./Llama-3-Open-Ko-8B-Instruct-teddynote-gguf-unsloth.F16.gguf
This will take 3 minutes...
INFO:hf-to-gguf:Loading model: Llama-3-Open-Ko-8B-Instruct-teddynote-gguf
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 8192
INFO:hf-to-gguf:gguf: embedding length = 4096
INFO:hf-to-gguf:gguf: feed forward length = 14336
INFO:hf-to-gguf:gguf: head count = 32
INFO:hf-to-gguf:gguf: key-value head count = 8
INFO:hf-to-gguf:gguf: rope theta = 500000.0
INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
INFO:gguf.vocab:Adding 280147 merge(s).
INFO:gguf.vocab:Setting special token type bos to 128000
INFO:gguf.vocab:Setting special token type eos to 128001
INFO:gguf.vocab:Setting special token type pad to 128255
INFO:gguf.vocab:Setting chat_template to {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '&lt;|start_header_id|&gt;' + message['role'] + '&lt;|end_header_id|&gt;

'+ message['content'] | trim + '&lt;|eot_id|&gt;' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;

' }}{% endif %}
INFO:hf-to-gguf:Exporting model to 'Llama-3-Open-Ko-8B-Instruct-teddynote-gguf-unsloth.F16.gguf'
INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'
INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'
INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --&gt; F16, shape = {4096, 128256}
INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'
INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'
INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 14336}
INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 4096}
INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'
INFO:hf-to-gguf:output.weight,               torch.bfloat16 --&gt; F16, shape = {4096, 128256}
INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = {14336, 4096}
INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --&gt; F32, shape = {4096}
Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.1G/16.1G [00:51&lt;00:00, 315Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to 'Llama-3-Open-Ko-8B-Instruct-teddynote-gguf-unsloth.F16.gguf'
Unsloth: Conversion completed! Output location: ./Llama-3-Open-Ko-8B-Instruct-teddynote-gguf-unsloth.F16.gguf
Unsloth: [2] Converting GGUF 16bit into q8_0. This will take 20 minutes...
main: build = 2939 (1ea2a003)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing './Llama-3-Open-Ko-8B-Instruct-teddynote-gguf-unsloth.F16.gguf' to './Llama-3-Open-Ko-8B-Instruct-teddynote-gguf-unsloth.Q8_0.gguf' as Q8_0 using 32 threads
llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ./Llama-3-Open-Ko-8B-Instruct-teddynote-gguf-unsloth.F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Llama-3-Open-Ko-8B-Instruct-teddynote...
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&amp;", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ä  Ä ", "Ä  Ä Ä Ä ", "Ä Ä  Ä Ä ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128255
llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  22:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q8_0 .. size =  1002.00 MiB -&gt;   532.31 MiB
[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  20/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  21/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  22/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  23/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  24/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  25/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  26/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  27/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  28/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  29/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  30/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  31/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  32/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  33/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  34/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  35/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  36/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  37/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  38/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  39/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  40/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  41/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  42/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  43/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  44/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  45/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  46/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  47/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  48/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  49/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  50/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  51/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  52/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  53/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  54/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  55/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  56/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  57/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  58/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  59/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  60/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  61/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  62/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  63/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  64/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  65/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  66/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  67/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  68/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  69/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  70/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  71/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  72/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  73/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  74/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  75/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  76/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  77/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  78/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  79/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  80/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  81/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  82/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  83/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  84/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  85/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  86/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  87/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  88/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  89/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  90/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  91/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  92/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  93/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  94/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  95/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[  96/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  97/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[  98/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[  99/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 100/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 101/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 102/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 103/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 104/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 105/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 106/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 107/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 108/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 109/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 110/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 111/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 112/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 113/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 114/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 115/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 116/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 117/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 118/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 119/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 120/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 121/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 122/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 123/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 124/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 125/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 126/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 127/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 128/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 129/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 130/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 131/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 132/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 133/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 134/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 135/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 136/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 137/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 138/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 139/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 140/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 141/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 142/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 143/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 144/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 145/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 146/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 147/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 148/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 149/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 150/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 151/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 152/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 153/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 154/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 155/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 156/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 157/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 158/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 159/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 160/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 161/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 162/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 163/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 164/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 165/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 166/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 167/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 168/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 169/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 170/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 171/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 172/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 173/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 174/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 175/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 176/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 177/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 178/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 179/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 180/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 181/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 182/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 183/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 184/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 185/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 186/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 187/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 188/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 189/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 200/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 201/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 202/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 203/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 204/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 205/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 206/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 207/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 208/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 209/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 210/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 211/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 212/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 213/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 214/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 215/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 216/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 217/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 218/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 219/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 220/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 221/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 222/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 223/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 224/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 225/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 226/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 227/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 228/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 229/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 230/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 231/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 232/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 233/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 234/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 235/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 236/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 237/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 238/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 239/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 240/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 241/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 242/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 243/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 244/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 245/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 246/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 247/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 248/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 249/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 250/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 251/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 252/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 253/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 254/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 255/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 256/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 257/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 258/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 259/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 260/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 261/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 262/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 263/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 264/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 265/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 266/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 267/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 268/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 269/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 270/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 271/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 272/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 273/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 274/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 275/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 276/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 277/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 278/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 279/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 280/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 281/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 282/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 285/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
[ 286/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
[ 287/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q8_0 .. size =  1002.00 MiB -&gt;   532.31 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB -&gt;    59.50 MiB
[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
llama_model_quantize_internal: model size  = 15317.02 MB
llama_model_quantize_internal: quant size  =  8137.64 MB

main: quantize time = 13752.39 ms
main:    total time = 13752.39 ms
Unsloth: Conversion completed! Output location: ./Llama-3-Open-Ko-8B-Instruct-teddynote-gguf-unsloth.Q8_0.gguf
Unsloth: Uploading GGUF to Huggingface Hub...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"eaa494ef56f843a4bfc1ffd7c6e67926","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Saved GGUF to https://huggingface.co/teddylee777/Llama-3-Open-Ko-8B-Instruct-teddynote-gguf</code></pre>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script type="text/javascript">

// replace cmd keyboard shortcut w/ control on non-Mac platforms
const kPlatformMac = typeof navigator !== 'undefined' ? /Mac/.test(navigator.platform) : false;
if (!kPlatformMac) {
   var kbds = document.querySelectorAll("kbd")
   kbds.forEach(function(kbd) {
      kbd.innerHTML = kbd.innerHTML.replace(/âŒ˜/g, 'âŒƒ');
   });
}

// tweak headings in pymd
document.querySelectorAll(".pymd span.co").forEach(el => {
   if (!el.innerText.startsWith("#|")) {
      el.style.fontWeight = 1000;
   }
});

</script>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup" style="padding-bottom: 1em; max-width: 400px;" class="ms-auto me-auto">
  <p style="font-weight: 600; margin-bottom: 0;">Subscribe</p>
  <span style="font-size: 0.9em;">Enjoy this blog? Get notified of new posts by email:</span>
<form action="https://quarto.us14.list-manage.com/subscribe/post?u=c79fb56a311ae347fbe916740&amp;id=ec05dfca03" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
    <div id="mc_embed_signup_scroll">
	
        <div class="input-group mt-1 mb-2">
        <input type="email" class="form-control" placeholder="Email Address" aria-label="Email Address" name="EMAIL" style="font-size: 0.8em; padding: .2em;">
        </div>              

	<div id="mce-responses" class="clear foot">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_c79fb56a311ae347fbe916740_ec05dfca03" tabindex="-1" value=""></div>
        <div class="optionalParent">
            <div class="clear foot" style="display: flex; align-items: center; justify-content: center;">
              
              
                <input type="submit" value="Subscribe" name="subscribe" style="min-width: 150px; font-size: 0.8em;" id="mc-embedded-subscribe" class="button btn btn-light btn-sm ms-auto me-auto">
            </div>
        </div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
<script type="application/vnd.jupyter.widget-state+json">
{"006aa3df063c4521ac5f832fe1307c85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0290d2187c07417d9302ab54743da2dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a266917c9fab4b21a9f4ce29da89f7ed","placeholder":"â€‹","style":"IPY_MODEL_847fe79e33994984be9595968751f326","value":"config.json:â€‡100%"}},"03f2d478a6f04a4ba73a1ab0bfbfcbe9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d67ce9b974ad4a3987782906ae9ae0fc","placeholder":"â€‹","style":"IPY_MODEL_84c11a93eec642ee84549e837e9564f5","value":"Generatingâ€‡trainâ€‡split:â€‡100%"}},"044656c5431746e2afa416c0ca91f3ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_059c48277af144f796a94f94e1a9c814","placeholder":"â€‹","style":"IPY_MODEL_8c654210b9674b959b9570f6d5addbf5","value":"â€‡16/16â€‡[00:00&lt;00:00,â€‡704.86â€‡examples/s]"}},"04576ed4663c4754bfc64ee8bd60898e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"059c48277af144f796a94f94e1a9c814":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"064a3daa88a54a96b1275c704b6625d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_535b6b46bdb54e9dabe5e6e23a40d346","max":1285579432,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7456cc527a9645068e91bb9e805bbd7e","value":1285579432}},"0683156f8cee42b38ccf22a3522461ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"087a0d727269459198204a6fdca7b445":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08c78aff0caa402da3740d4bc988267a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c35cb640bec45b3beb10e7c36ec41ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f38b82466bff4e9e9691717840b08f81","IPY_MODEL_fe145b3cf6cb4916aec92c855ac6f5cc","IPY_MODEL_8bc57e7f304f475ba344b4d44857c581"],"layout":"IPY_MODEL_e75e8e176da44cafbac169630481269e"}},"0d35ac03b2914b97b1c58d7a24afbca7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0d9335d8b2564a518df75d3d0ec8860b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e4b3f9b418c470c8e3a7e939e9c40c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e62afd9b85a4004878c63dfc26c641f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f9451dc85ba4044aa763f7349e69ad0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"100b019c5a214cb8b85b7f5ae2119d5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"104533e332514cfba5fb43fa4f7d0dbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_087a0d727269459198204a6fdca7b445","max":16,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a62b362e056c4bd2862517e804e22214","value":16}},"1051a18ef1a94646a330d1f42b33afc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c0f97331ac54e4096419842d32e4e7e","IPY_MODEL_d23634e1e21941c6a4631e1c24b9542e","IPY_MODEL_694947e5188942d1b2d112d82177d1b0"],"layout":"IPY_MODEL_2ae31dabcf524a3ca132fb6a6f6c52d0"}},"10ca0ddfebf745168c12f4dc6e5fef06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac2885b19eff4496bb085eaa9fcc8326","IPY_MODEL_a7baf821c0594e888b5dfbe1cfeec917","IPY_MODEL_5b6eaa201e654366bca96093b2959440"],"layout":"IPY_MODEL_5fcb8be821b8440d9a8a31bbbf1af48b"}},"1320c8b175874ceab90b3080c055578b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_03f2d478a6f04a4ba73a1ab0bfbfcbe9","IPY_MODEL_51a00fa35cf642fd8090298cf82fbe28","IPY_MODEL_9bff62df789f4675a2790a4708558e71"],"layout":"IPY_MODEL_c2c74e7cc99a40d69bc66a2ca0735dec"}},"142b69ecaa84447b9e3931c16df041f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d965b86b78ba46b5bafcfd853c5a1bed","IPY_MODEL_104533e332514cfba5fb43fa4f7d0dbf","IPY_MODEL_044656c5431746e2afa416c0ca91f3ad"],"layout":"IPY_MODEL_4e8c7b53ec5843a5ad18b7fdfdfcdca6"}},"14e2b8cd41764e2b9f0a2a75c5e7c22c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"156bdbb90bd9404c82751087ae709fae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16099d5666cb4765af04f65b9fe6842a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1658c20eb4244ce7b578bd4bd52a141f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1661a129f1e146828f9003b5b73a247c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17d5e64c2e0a49aeb88e22746b3ecbac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f38bc1f9d81458e9975e67e441da8cc","max":301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_22d658a6e600477cb704807c10c6408f","value":301}},"197a694dd64d41348475168098abb3ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b83a29252b14532867f02c9376cbdc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c1c2d5fbc314f70a120718ff1569241":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2982cc146cb747019135164113b63879","IPY_MODEL_064a3daa88a54a96b1275c704b6625d1","IPY_MODEL_33dc62ca03074c52ace954181c28a5f0"],"layout":"IPY_MODEL_5e57ccf6d4064e12be5faf377a51b565"}},"1d9cdf6079b648b4a1b8eeb0cea1fa43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"215c69a650fa447e90aa977a24fb4dc7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"217ab9f097954ba5975bff834f78a597":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22d658a6e600477cb704807c10c6408f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2324c9923fd046a0bc6580e901b92dad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c54491ff396146e0999a180e5be4aebf","placeholder":"â€‹","style":"IPY_MODEL_23f0ea83d72e4ab59d539a567eb20a36","value":"Loadingâ€‡checkpointâ€‡shards:â€‡100%"}},"23f0ea83d72e4ab59d539a567eb20a36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"241908b8940142dfbd2673ba6abb27a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"257aad534fcd4bfd9b2a795d54695645":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_241908b8940142dfbd2673ba6abb27a5","placeholder":"â€‹","style":"IPY_MODEL_1b83a29252b14532867f02c9376cbdc5","value":"special_tokens_map.json:â€‡100%"}},"25efba438e33473da11ac3a7e3a793e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26ed41293600404c926b0884aabadfd5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f862a90a580944b9b5c5ecb597f972b5","placeholder":"â€‹","style":"IPY_MODEL_c27a27d1c8834df6af1c309b0fcdabcb","value":"â€‡23.9k/23.9kâ€‡[00:00&lt;00:00,â€‡1.82MB/s]"}},"2982cc146cb747019135164113b63879":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6d17daa45624f0a9639409b47b94924","placeholder":"â€‹","style":"IPY_MODEL_5a7a205df38d4adb97875944c5dbdca3","value":"model-00006-of-00006.safetensors:â€‡100%"}},"2a41774120f94fc2a0a49d467dad9897":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aadcbfcf79840a6ad135b29cb4c9e57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_257aad534fcd4bfd9b2a795d54695645","IPY_MODEL_17d5e64c2e0a49aeb88e22746b3ecbac","IPY_MODEL_d9bca499d3674b57bbc50b89ff490d8c"],"layout":"IPY_MODEL_1661a129f1e146828f9003b5b73a247c"}},"2ae31dabcf524a3ca132fb6a6f6c52d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ebbc11da0d54aabacb4add5300b0d08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30ca7c425216497d88967d62d83bc9fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6829eb4b3e64d2caff44b932e63b4c4","placeholder":"â€‹","style":"IPY_MODEL_d6a2bd5bb29a40528c4d660efdb23e11","value":"â€‡132/132â€‡[00:00&lt;00:00,â€‡10.9kB/s]"}},"32526257e6774c15a88f908e4ce3a8f4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33dc62ca03074c52ace954181c28a5f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39bfc699ad3d4e73aabe1234a7ad1573","placeholder":"â€‹","style":"IPY_MODEL_dddc259c023b4fd88ee640e6d9730e38","value":"â€‡1.29G/1.29Gâ€‡[00:04&lt;00:00,â€‡296MB/s]"}},"343d5b87f7e846258d1c4242ae979a07":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37ab0e207b5245608ca7c58b566da4a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38224f13f2cb4bb9a315f7e1a6140cb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38849ecba0884da4a698fde06d91b13e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6690121b11a48b0a8c783df1cbfabbb","placeholder":"â€‹","style":"IPY_MODEL_d2c5b80c3b75464db9a34d01aa37950e","value":"model.safetensors.index.json:â€‡100%"}},"39bfc699ad3d4e73aabe1234a7ad1573":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c0f97331ac54e4096419842d32e4e7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2bf55927a7d433fada252635594dff5","placeholder":"â€‹","style":"IPY_MODEL_0683156f8cee42b38ccf22a3522461ec","value":"Downloadingâ€‡readme:â€‡100%"}},"3cb52fcabe9b4d21925dcf942816a827":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3db5f7d69a6e41e98d4bcc67b92f1ff5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44d18bd62fb145c1acab537f02b5a60b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1426915a6d74f1983445cbba14a8a12","placeholder":"â€‹","style":"IPY_MODEL_fed268fca78e41afbaf2be34971c2668","value":"â€‡6/6â€‡[01:13&lt;00:00,â€‡11.18s/it]"}},"453308addf72430c9c63a16d0f060e5b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"480a9a6f262d43eeb69cd218fb9ab5d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4be2ac8988a1470f87489ed571f5eaa9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56d6a5e55ad246398c49cc5565448799","placeholder":"â€‹","style":"IPY_MODEL_006aa3df063c4521ac5f832fe1307c85","value":"model-00001-of-00006.safetensors:â€‡100%"}},"4cda3126d0e3428b865dc24d7ae60f2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e8c7b53ec5843a5ad18b7fdfdfcdca6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ea84d4aeddb418987748906f8d89104":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f2654b42fc84943ba4ea5815a1baa3c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fe58c1d715748ce8c90e9567a9e69be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"508bb6ab2bf74e4a9ed57a0371180e17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"509f02d8d0f043a9a7eedc4e24fe29c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ee734f9997e24f64a1a6aec542662fbc","IPY_MODEL_7d810c23b32849e9b38f8b1ad944adbf","IPY_MODEL_f8bbb09e33a84fc4ae2777d08368224f"],"layout":"IPY_MODEL_e9911df32739492c9ae2b7202e5c5899"}},"51a00fa35cf642fd8090298cf82fbe28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f89378aef63148b290f9c33b36f0265f","max":16,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f818588783354dffa838660e33367672","value":16}},"52838461d16a4aa8b9806f379089cc78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"529c6085364d452fb974758ec6d3d22a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"535b6b46bdb54e9dabe5e6e23a40d346":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55941ea7b03f46c28d349fe28bfc8716":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5641155be7f947309112a36c48012799":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cb52fcabe9b4d21925dcf942816a827","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55941ea7b03f46c28d349fe28bfc8716","value":6}},"56d6a5e55ad246398c49cc5565448799":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58034845c96348bc89df363fcf344da6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9367da780a94ae2860863852c9515bc","placeholder":"â€‹","style":"IPY_MODEL_f686ebfdfa0e452081e44a191017aab6","value":"â€‡2.94G/2.94Gâ€‡[00:14&lt;00:00,â€‡67.7MB/s]"}},"59de36b98d12457182bb85fada4e53ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a7a205df38d4adb97875944c5dbdca3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b6eaa201e654366bca96093b2959440":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e0781040f3a436580dd6072ce5aa75e","placeholder":"â€‹","style":"IPY_MODEL_508bb6ab2bf74e4a9ed57a0371180e17","value":"â€‡2.94G/2.94Gâ€‡[00:10&lt;00:00,â€‡241MB/s]"}},"5c58b7a09fd14a0f994605b73e685e16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fec1d6b82c4440e81e4ff7748549392","IPY_MODEL_8dd7421380e64b86aada4e7b4a09b15b","IPY_MODEL_eaa2b1d6b5a64c2a8050a7888b3c4648"],"layout":"IPY_MODEL_fce99cbf870c41ffa3460cd8f6fd665e"}},"5cb65bde9e8242e38aa98862e05208a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb0549767ae94f649a9cbb15abe61bc9","IPY_MODEL_62e623c653c6413dba5cf0f0f24b4faf","IPY_MODEL_58034845c96348bc89df363fcf344da6"],"layout":"IPY_MODEL_ab2c635efd8644a985b6720f5a102c96"}},"5e0781040f3a436580dd6072ce5aa75e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e57ccf6d4064e12be5faf377a51b565":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f01c958d62f4ce58cdd818bb3b22973":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5fcb8be821b8440d9a8a31bbbf1af48b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fd4ca07fc894afcbcf55e29809c40d1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62e623c653c6413dba5cf0f0f24b4faf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_197a694dd64d41348475168098abb3ab","max":2936118096,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be8eba78342c44bf90bfb62b68a4bf09","value":2936118096}},"66e279d8f1ff4be9a3892cee6e700959":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f216321cede6408ba0ff90c0b52ece4f","IPY_MODEL_8813817abcf7487e8b83a7f5c6055f95","IPY_MODEL_f2d75fc6327243208c3740332d054637"],"layout":"IPY_MODEL_0e4b3f9b418c470c8e3a7e939e9c40c3"}},"6741a04cf7594497873d7cc5c1832a2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6939a02cb7ab40f08486b38f41490aae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"694947e5188942d1b2d112d82177d1b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7238acef7a784aec9ac2ad6942c34ff0","placeholder":"â€‹","style":"IPY_MODEL_6ba81f98392e47a1bc840362e1c56939","value":"â€‡339/339â€‡[00:00&lt;00:00,â€‡32.3kB/s]"}},"6ba81f98392e47a1bc840362e1c56939":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c5911e019e447ccba3405de971ae416":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f4f7d14c61b446a851cd93bd17ed003":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a41774120f94fc2a0a49d467dad9897","max":132,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4cda3126d0e3428b865dc24d7ae60f2e","value":132}},"6fec1d6b82c4440e81e4ff7748549392":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e62afd9b85a4004878c63dfc26c641f","placeholder":"â€‹","style":"IPY_MODEL_c1c0ab070f2448fe804693456aa6c679","value":"model-00003-of-00006.safetensors:â€‡100%"}},"7118e70b23aa426d9b5ad2fc922fd1f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7238acef7a784aec9ac2ad6942c34ff0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73b78381a5d54cf098a695503727c0cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73dabe4081f845e4ac4745c36acf1479":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16099d5666cb4765af04f65b9fe6842a","placeholder":"â€‹","style":"IPY_MODEL_9c0d336467d24b359efd024d204b8e8e","value":"â€‡16/16â€‡[00:00&lt;00:00,â€‡21.04â€‡examples/s]"}},"7456cc527a9645068e91bb9e805bbd7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"75ba0f0586e44a7ebb6d67a1dfa75995":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d810c23b32849e9b38f8b1ad944adbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_75ba0f0586e44a7ebb6d67a1dfa75995","max":9085698,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d35ac03b2914b97b1c58d7a24afbca7","value":9085698}},"7ec19800bab64e3db16c132439dc01fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80aa72d7797c44fa978ded5071d301f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52838461d16a4aa8b9806f379089cc78","max":2996900632,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f01c958d62f4ce58cdd818bb3b22973","value":2996900632}},"80cf3bbe986049a99fb59772545d12e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6877dd00cce4b469fde96f2394069bb","max":16,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e699c41d7d8547058ced67acbf97da7e","value":16}},"81fb844fe2c14e29897ea5b37e672adc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2324c9923fd046a0bc6580e901b92dad","IPY_MODEL_d4d8c992f3364bb5bbfa3c55d0dfb593","IPY_MODEL_83c0fd99eec54214b424512334a5e4a6"],"layout":"IPY_MODEL_900c7b7d5a3b4574bb54a47233bbdaea"}},"83c0fd99eec54214b424512334a5e4a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d9cdf6079b648b4a1b8eeb0cea1fa43","placeholder":"â€‹","style":"IPY_MODEL_2ebbc11da0d54aabacb4add5300b0d08","value":"â€‡6/6â€‡[00:06&lt;00:00,â€‡â€‡1.01it/s]"}},"8464366c6695491e8527fa649628def1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"847fe79e33994984be9595968751f326":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84c11a93eec642ee84549e837e9564f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85efd95e4f00493d94624bb706117a03":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86f7efb842744ee8a51753689dcbbbd6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"872f2ce0bc8745d89d2b0dfc76cebeda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8813817abcf7487e8b83a7f5c6055f95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aae8d86cb83c45d4a15e9ec768ce6de4","max":2936134712,"min":0,"orientation":"horizontal","style":"IPY_MODEL_04576ed4663c4754bfc64ee8bd60898e","value":2936134712}},"89530754ffb54aea80cc15d3fe05c461":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a52d1497a934d22a6ea073370425923":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b6feda602a040a482cba439f613e7a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d9f97aa8106466299752fecd1e7e2d6","IPY_MODEL_6f4f7d14c61b446a851cd93bd17ed003","IPY_MODEL_30ca7c425216497d88967d62d83bc9fd"],"layout":"IPY_MODEL_0d9335d8b2564a518df75d3d0ec8860b"}},"8bc57e7f304f475ba344b4d44857c581":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d49c61f792284b1f92b421b9f79d3ad1","placeholder":"â€‹","style":"IPY_MODEL_217ab9f097954ba5975bff834f78a597","value":"â€‡6.20k/6.20kâ€‡[00:00&lt;00:00,â€‡10.8kB/s]"}},"8c654210b9674b959b9570f6d5addbf5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8dd7421380e64b86aada4e7b4a09b15b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e3e3179c27e42698c442afaca46eb05","max":2969688800,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ede421717dd5461183db3a6af60d40f3","value":2969688800}},"8f7d53d84b1b4884b300e95ef65b251a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"900c7b7d5a3b4574bb54a47233bbdaea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90936a2c7fd44e1aace25772b6f4f217":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95cadd60850843049dd91fd64d7c95e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"997861b04cf647e9a6bd727b318be47a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6741a04cf7594497873d7cc5c1832a2f","max":23950,"min":0,"orientation":"horizontal","style":"IPY_MODEL_95cadd60850843049dd91fd64d7c95e1","value":23950}},"9bff62df789f4675a2790a4708558e71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ea84d4aeddb418987748906f8d89104","placeholder":"â€‹","style":"IPY_MODEL_e6c1e674d7794cd1ab8294042ac0dfb1","value":"â€‡16/16â€‡[00:00&lt;00:00,â€‡356.49â€‡examples/s]"}},"9c0d336467d24b359efd024d204b8e8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d9f97aa8106466299752fecd1e7e2d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9bb6750e657475d9373fbc0e7fc85d9","placeholder":"â€‹","style":"IPY_MODEL_4fe58c1d715748ce8c90e9567a9e69be","value":"generation_config.json:â€‡100%"}},"9db627ce367242e7a374a14d59419aac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f86f4fd1cc934cdbbfe44f84168646ab","IPY_MODEL_a93dc092082244049dd0589d90cc5d0b","IPY_MODEL_ae40412ec12249719f1a0bcb2650cff4"],"layout":"IPY_MODEL_32526257e6774c15a88f908e4ce3a8f4"}},"9e3e3179c27e42698c442afaca46eb05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e5a03ea3cf9446b8341978d942f7677":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e91d0517db54c74bfe980cebdbb4900":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ead35f0817c44e9bd32dc6114ea2eaf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8464366c6695491e8527fa649628def1","placeholder":"â€‹","style":"IPY_MODEL_529c6085364d452fb974758ec6d3d22a","value":"Mapâ€‡(num_proc=2):â€‡100%"}},"9f38bc1f9d81458e9975e67e441da8cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a266917c9fab4b21a9f4ce29da89f7ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3082881f419403b807007728c8a117e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd27c8c888ee4dbbb95021b537d52e63","placeholder":"â€‹","style":"IPY_MODEL_7118e70b23aa426d9b5ad2fc922fd1f0","value":"Downloadingâ€‡shards:â€‡100%"}},"a62b362e056c4bd2862517e804e22214":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6829eb4b3e64d2caff44b932e63b4c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7baf821c0594e888b5dfbe1cfeec917":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9591f8111984873abd2e3f6b5653f96","max":2936134664,"min":0,"orientation":"horizontal","style":"IPY_MODEL_acc5ca922af84ad095cb72552e0476ef","value":2936134664}},"a93dc092082244049dd0589d90cc5d0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c5911e019e447ccba3405de971ae416","max":50982,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e5a03ea3cf9446b8341978d942f7677","value":50982}},"a9bb6750e657475d9373fbc0e7fc85d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aae8d86cb83c45d4a15e9ec768ce6de4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab2c635efd8644a985b6720f5a102c96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab9e8aec9cc3469dab2ab1471c90bc8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac2885b19eff4496bb085eaa9fcc8326":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f9451dc85ba4044aa763f7349e69ad0","placeholder":"â€‹","style":"IPY_MODEL_ca3dac1f5f2b40fb8e4b30d172f3579a","value":"model-00002-of-00006.safetensors:â€‡100%"}},"acc5ca922af84ad095cb72552e0476ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae40412ec12249719f1a0bcb2650cff4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37ab0e207b5245608ca7c58b566da4a8","placeholder":"â€‹","style":"IPY_MODEL_cf8b9e484f29457aa723980d22d1f9db","value":"â€‡51.0k/51.0kâ€‡[00:00&lt;00:00,â€‡4.01MB/s]"}},"b6d5862c271c4118a72b067d53fb2343":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b95095d65b704679840eb5af96e5d6b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9591f8111984873abd2e3f6b5653f96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9892219d73f4696b5471e73f8b6dccb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_38849ecba0884da4a698fde06d91b13e","IPY_MODEL_997861b04cf647e9a6bd727b318be47a","IPY_MODEL_26ed41293600404c926b0884aabadfd5"],"layout":"IPY_MODEL_ef8ef08dd7284655a29ee567061c7a32"}},"ba00e694bbba41f5b8045352ad057f97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be8eba78342c44bf90bfb62b68a4bf09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bff46daf2e1b40bb941c991ab258cbeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4be2ac8988a1470f87489ed571f5eaa9","IPY_MODEL_80aa72d7797c44fa978ded5071d301f2","IPY_MODEL_ca314337bf7c4c698fd53f36a23cb710"],"layout":"IPY_MODEL_453308addf72430c9c63a16d0f060e5b"}},"c1c0ab070f2448fe804693456aa6c679":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c27a27d1c8834df6af1c309b0fcdabcb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2c74e7cc99a40d69bc66a2ca0735dec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c40e2bc874c34305bc5d8920310f7d94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4475b99355e4905a67fe11f6f47247b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c54491ff396146e0999a180e5be4aebf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6d17daa45624f0a9639409b47b94924":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c82b631ec4b34eccb3a88dd69cbeb2a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0290d2187c07417d9302ab54743da2dc","IPY_MODEL_e26b5300257343fa8de572476889a307","IPY_MODEL_fc50ec0bde054cd5b0630a81c48a0bea"],"layout":"IPY_MODEL_08c78aff0caa402da3740d4bc988267a"}},"ca314337bf7c4c698fd53f36a23cb710":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89530754ffb54aea80cc15d3fe05c461","placeholder":"â€‹","style":"IPY_MODEL_c4475b99355e4905a67fe11f6f47247b","value":"â€‡3.00G/3.00Gâ€‡[00:10&lt;00:00,â€‡321MB/s]"}},"ca3dac1f5f2b40fb8e4b30d172f3579a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf8b9e484f29457aa723980d22d1f9db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d23634e1e21941c6a4631e1c24b9542e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f7d53d84b1b4884b300e95ef65b251a","max":339,"min":0,"orientation":"horizontal","style":"IPY_MODEL_872f2ce0bc8745d89d2b0dfc76cebeda","value":339}},"d2c5b80c3b75464db9a34d01aa37950e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d49c61f792284b1f92b421b9f79d3ad1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4d8c992f3364bb5bbfa3c55d0dfb593":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73b78381a5d54cf098a695503727c0cb","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f856a6738c404995b68277678818fb7e","value":6}},"d67ce9b974ad4a3987782906ae9ae0fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6a2bd5bb29a40528c4d660efdb23e11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d965b86b78ba46b5bafcfd853c5a1bed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f2654b42fc84943ba4ea5815a1baa3c","placeholder":"â€‹","style":"IPY_MODEL_480a9a6f262d43eeb69cd218fb9ab5d6","value":"Map:â€‡100%"}},"d9bca499d3674b57bbc50b89ff490d8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_156bdbb90bd9404c82751087ae709fae","placeholder":"â€‹","style":"IPY_MODEL_38224f13f2cb4bb9a315f7e1a6140cb6","value":"â€‡301/301â€‡[00:00&lt;00:00,â€‡25.8kB/s]"}},"dd27c8c888ee4dbbb95021b537d52e63":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dddc259c023b4fd88ee640e6d9730e38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df0d196eecc74b34b5504f6830805fd2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3082881f419403b807007728c8a117e","IPY_MODEL_5641155be7f947309112a36c48012799","IPY_MODEL_44d18bd62fb145c1acab537f02b5a60b"],"layout":"IPY_MODEL_90936a2c7fd44e1aace25772b6f4f217"}},"e1426915a6d74f1983445cbba14a8a12":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e26b5300257343fa8de572476889a307":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a52d1497a934d22a6ea073370425923","max":698,"min":0,"orientation":"horizontal","style":"IPY_MODEL_85efd95e4f00493d94624bb706117a03","value":698}},"e2bf55927a7d433fada252635594dff5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6690121b11a48b0a8c783df1cbfabbb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6877dd00cce4b469fde96f2394069bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e699c41d7d8547058ced67acbf97da7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6c1e674d7794cd1ab8294042ac0dfb1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e75e8e176da44cafbac169630481269e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9911df32739492c9ae2b7202e5c5899":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eaa2b1d6b5a64c2a8050a7888b3c4648":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59de36b98d12457182bb85fada4e53ed","placeholder":"â€‹","style":"IPY_MODEL_fafdec94b3e147bba0d999342e98f550","value":"â€‡2.97G/2.97Gâ€‡[00:14&lt;00:00,â€‡198MB/s]"}},"eb0549767ae94f649a9cbb15abe61bc9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25efba438e33473da11ac3a7e3a793e6","placeholder":"â€‹","style":"IPY_MODEL_100b019c5a214cb8b85b7f5ae2119d5d","value":"model-00004-of-00006.safetensors:â€‡100%"}},"ebf0af95b9fc4bb88375dde9df666bb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ead35f0817c44e9bd32dc6114ea2eaf","IPY_MODEL_80cf3bbe986049a99fb59772545d12e7","IPY_MODEL_73dabe4081f845e4ac4745c36acf1479"],"layout":"IPY_MODEL_ab9e8aec9cc3469dab2ab1471c90bc8d"}},"ede421717dd5461183db3a6af60d40f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee734f9997e24f64a1a6aec542662fbc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4a3cb640ebc42d4a104329f878889a4","placeholder":"â€‹","style":"IPY_MODEL_1658c20eb4244ce7b578bd4bd52a141f","value":"tokenizer.json:â€‡100%"}},"ef8ef08dd7284655a29ee567061c7a32":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f216321cede6408ba0ff90c0b52ece4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba00e694bbba41f5b8045352ad057f97","placeholder":"â€‹","style":"IPY_MODEL_c40e2bc874c34305bc5d8920310f7d94","value":"model-00005-of-00006.safetensors:â€‡100%"}},"f2d75fc6327243208c3740332d054637":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_215c69a650fa447e90aa977a24fb4dc7","placeholder":"â€‹","style":"IPY_MODEL_9e91d0517db54c74bfe980cebdbb4900","value":"â€‡2.94G/2.94Gâ€‡[00:15&lt;00:00,â€‡155MB/s]"}},"f38b82466bff4e9e9691717840b08f81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_343d5b87f7e846258d1c4242ae979a07","placeholder":"â€‹","style":"IPY_MODEL_3db5f7d69a6e41e98d4bcc67b92f1ff5","value":"Downloadingâ€‡data:â€‡100%"}},"f3c41be2d6b54ac68d873432478f6b82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4a3cb640ebc42d4a104329f878889a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f686ebfdfa0e452081e44a191017aab6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f818588783354dffa838660e33367672":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f856a6738c404995b68277678818fb7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f862a90a580944b9b5c5ecb597f972b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f86f4fd1cc934cdbbfe44f84168646ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fd4ca07fc894afcbcf55e29809c40d1","placeholder":"â€‹","style":"IPY_MODEL_f3c41be2d6b54ac68d873432478f6b82","value":"tokenizer_config.json:â€‡100%"}},"f89378aef63148b290f9c33b36f0265f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8bbb09e33a84fc4ae2777d08368224f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6d5862c271c4118a72b067d53fb2343","placeholder":"â€‹","style":"IPY_MODEL_6939a02cb7ab40f08486b38f41490aae","value":"â€‡9.09M/9.09Mâ€‡[00:00&lt;00:00,â€‡300MB/s]"}},"f9367da780a94ae2860863852c9515bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fafdec94b3e147bba0d999342e98f550":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc50ec0bde054cd5b0630a81c48a0bea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86f7efb842744ee8a51753689dcbbbd6","placeholder":"â€‹","style":"IPY_MODEL_b95095d65b704679840eb5af96e5d6b0","value":"â€‡698/698â€‡[00:00&lt;00:00,â€‡60.5kB/s]"}},"fce99cbf870c41ffa3460cd8f6fd665e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe145b3cf6cb4916aec92c855ac6f5cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14e2b8cd41764e2b9f0a2a75c5e7c22c","max":6201,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ec19800bab64e3db16c132439dc01fe","value":6201}},"fed268fca78e41afbaf2be34971c2668":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "ë³µì‚¬ì™„ë£Œ!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "ë³µì‚¬ì™„ë£Œ!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("kk3225\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="kmink3225/blog" data-repo-id="R_kgDOLCZyDg" data-category="Blog" data-category-id="" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Â© Kwangmin Kim</p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html">
<p>About</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225/blog">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>