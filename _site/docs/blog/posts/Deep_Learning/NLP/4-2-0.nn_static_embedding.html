<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.543">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kwangmin Kim">
<meta name="dcterms.date" content="2025-01-05">
<meta name="description" content="자연어 처리(NLP)에서 텍스트의 의미와 문맥을 벡터로 표현하는 신경망 기반의 고급 벡터화 방법들을 심층적으로 탐구한다. 정적 워드 임베딩의 원리, 특징, 활용 방안을 다룬다.">

<title>Kwangmin Kim - 텍스트 벡터화: 신경망 기반 방법론</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete-preset-algolia.umd.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "algolia": {
    "application-id": "DUOR1DRC9D",
    "search-only-api-key": "f264da5dea684ffb9e9b4a574af3ed61",
    "index-name": "prod_QUARTO",
    "analytics-events": true,
    "show-logo": true,
    "libDir": "site_libs"
  },
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": true,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js"></script>


<script type="text/javascript">
var ALGOLIA_INSIGHTS_SRC = "https://cdn.jsdelivr.net/npm/search-insights/dist/search-insights.iife.min.js";
!function(e,a,t,n,s,i,c){e.AlgoliaAnalyticsObject=s,e[s]=e[s]||function(){
(e[s].queue=e[s].queue||[]).push(arguments)},i=a.createElement(t),c=a.getElementsByTagName(t)[0],
i.async=1,i.src=n,c.parentNode.insertBefore(i,c)
}(window,document,"script",ALGOLIA_INSIGHTS_SRC,"aa");
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/autocomplete-plugin-algolia-insights">

</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6W0EKFMWBN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6W0EKFMWBN', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../../styles.css">
<meta property="og:title" content="Kwangmin Kim - 텍스트 벡터화: 신경망 기반 방법론">
<meta property="og:description" content="자연어 처리(NLP)에서 텍스트의 의미와 문맥을 벡터로 표현하는 신경망 기반의 고급 벡터화 방법들을 심층적으로 탐구한다. 정적 워드 임베딩의 원리, 특징, 활용 방안을 다룬다.">
<meta property="og:site_name" content="Kwangmin Kim">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html"> 
<span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"> <i class="bi bi-github" role="img" aria-label="Github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"> <i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#요약" id="toc-요약" class="nav-link active" data-scroll-target="#요약"><span class="header-section-number">1</span> 요약</a></li>
  <li><a href="#텍스트-인코딩-및-벡터화" id="toc-텍스트-인코딩-및-벡터화" class="nav-link" data-scroll-target="#텍스트-인코딩-및-벡터화"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</a>
  <ul class="collapse">
  <li><a href="#신경망-사용-20082018" id="toc-신경망-사용-20082018" class="nav-link" data-scroll-target="#신경망-사용-20082018"><span class="header-section-number">2.0.1</span> 신경망 사용 (2008~2018)</a></li>
  <li><a href="#dtm-방식의-한계와-신경망-접근법의-등장" id="toc-dtm-방식의-한계와-신경망-접근법의-등장" class="nav-link" data-scroll-target="#dtm-방식의-한계와-신경망-접근법의-등장"><span class="header-section-number">2.0.2</span> DTM 방식의 한계와 신경망 접근법의 등장</a></li>
  <li><a href="#워드-임베딩-word-embedding" id="toc-워드-임베딩-word-embedding" class="nav-link" data-scroll-target="#워드-임베딩-word-embedding"><span class="header-section-number">2.0.3</span> 워드 임베딩 (Word Embedding)</a></li>
  </ul></li>
  <li><a href="#결론" id="toc-결론" class="nav-link" data-scroll-target="#결론"><span class="header-section-number">3</span> 결론</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">텍스트 벡터화: 신경망 기반 방법론</h1>
<p class="subtitle lead">워드 임베딩을 이용한 벡터 표현 소개</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>자연어 처리(NLP)에서 텍스트의 의미와 문맥을 벡터로 표현하는 신경망 기반의 고급 벡터화 방법들을 심층적으로 탐구한다. 정적 워드 임베딩의 원리, 특징, 활용 방안을 다룬다.</p>
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kwangmin Kim </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 5, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 통계 기반 벡터화의 한계를 넘어 텍스트 데이터로부터 풍부한 의미론적, 문맥적 정보를 추출하는 신경망 기반 벡터화 방법론을 소개한다.</p>
<ul>
<li><strong>DTM 방식의 한계와 신경망 접근법의 등장</strong>:
<ul>
<li>전통적인 DTM(문서-단어 행렬) 방식의 문제점(차원의 저주, 희소성, 의미 관계 표현 불가)을 지적하고, 이를 극복하기 위한 신경망 기반 밀집 벡터 표현(워드 임베딩)의 필요성을 설명한다.</li>
</ul></li>
<li><strong>워드 임베딩 (Word Embedding) - 정적 임베딩</strong>:
<ul>
<li><strong>핵심 원리</strong>: “같은 문맥에 나타나는 단어는 비슷한 의미를 가진다”는 분포 가설에 기반하여 단어를 저차원 밀집 벡터로 표현한다.</li>
<li><strong>Embedding Layer</strong>: 정수 인코딩된 단어를 밀집 벡터로 변환하는 신경망의 핵심 구성 요소로, 그 구조와 Look-up Table 방식을 설명한다.</li>
</ul></li>
<li><strong>실용적 응용 및 평가</strong>:
<ul>
<li>임베딩 모델의 성능을 평가하는 내재적 평가(단어 유사도, 관계 유추)와 외재적 평가(다운스트림 태스크 성능) 방법을 소개한다.</li>
</ul></li>
<li><strong>결론</strong>: 신경망 기반 벡터화 기법들의 발전 과정과 그 의의를 요약한다.</li>
</ul>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    └── RNN 계열
        ├── LSTM
        ├── GRU
        └── ELMo</code></pre>
<section id="신경망-사용-20082018" class="level3" data-number="2.0.1">
<h3 data-number="2.0.1" class="anchored" data-anchor-id="신경망-사용-20082018"><span class="header-section-number">2.0.1</span> 신경망 사용 (2008~2018)</h3>
</section>
<section id="dtm-방식의-한계와-신경망-접근법의-등장" class="level3" data-number="2.0.2">
<h3 data-number="2.0.2" class="anchored" data-anchor-id="dtm-방식의-한계와-신경망-접근법의-등장"><span class="header-section-number">2.0.2</span> DTM 방식의 한계와 신경망 접근법의 등장</h3>
<p><strong>DTM 방식의 문제점:</strong> - <strong>차원의 저주</strong>: 어휘집 크기 = 벡터 차원 (예: 50,000개 단어 → 50,000차원) - <strong>희소성</strong>: 대부분의 값이 0인 sparse vector - <strong>의미적 관계 부재</strong>: “왕”과 “여왕”의 관계를 벡터가 표현하지 못함 - <strong>문제 상황:</strong></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 기존 방식 (원-핫 인코딩)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">"사과"</span> <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, ...]  <span class="co"># 50,000차원 벡터</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">"바나나"</span> <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, ...]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">"과일"</span> <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, ...]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>모든 단어가 서로 똑같이 멀어 보임 (유클리드 거리 = √2)</li>
<li>“사과”와 “바나나”가 비슷한 과일이라는 정보가 없음</li>
<li>메모리 낭비 (대부분이 0)</li>
</ul>
<p><strong>신경망 접근법의 혁신:</strong> - <strong>밀집 벡터(Dense Vector)</strong>: 고정된 낮은 차원 (예: 300차원)에 0/1 값이 아닌 실수 값을 가짐. - <strong>의미적 유사도</strong>: 벡터 간 거리로 단어 유사도 측정 가능 - <strong>문맥 학습</strong>: 주변 단어들을 통해 의미 학습</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 신경망 모델: 워드 임베딩 (300차원)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">"사과"</span> <span class="op">=</span> [<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, ...]     <span class="co"># 300개 실수</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">"바나나"</span> <span class="op">=</span> [<span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.2</span>, ...]   <span class="co"># 비슷한 값들</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">"과일"</span> <span class="op">=</span> [<span class="fl">0.25</span>, <span class="op">-</span><span class="fl">0.45</span>, <span class="fl">0.65</span>, <span class="fl">0.15</span>, ...] <span class="co"># 과일 카테고리</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>문맥 고려 방법 (Neural / Context-dependent)</li>
<li>신경망을 통해 단어의 의미를 주변 문맥을 고려하여 학습하고, 이를 밀집 벡터(Dense Vector)로 표현.</li>
</ul>
</section>
<section id="워드-임베딩-word-embedding" class="level3" data-number="2.0.3">
<h3 data-number="2.0.3" class="anchored" data-anchor-id="워드-임베딩-word-embedding"><span class="header-section-number">2.0.3</span> 워드 임베딩 (Word Embedding)</h3>
<ul>
<li><p>문맥 속에서 각 단어가 어떻게 사용되는지까지 신경망을 통해 벡터값을 구해 벡터에 담아내려 시도.</p></li>
<li><p><a href="https://projector.tensorflow.org/">체험: 단어 유사도 측정 - https://projector.tensorflow.org/</a></p></li>
<li><p>학습 후에는 각 단어 벡터 간의 유사도(의미반영)를 계산할 수 있다.</p></li>
<li><p>즉, 신경망 기반의 벡터화라는 것은 벡터의 값이 학습에 의해 결정된다는 것을 의미.</p></li>
<li><p>워드 임베딩 모델의 예시</p>
<ul>
<li>Word2Vec, GloVe, FastText, 모델 내 <code>Embedding</code> Layer 사용.<br>
</li>
</ul></li>
<li><p>어떻게 단어를 벡터화?</p>
<ul>
<li>단어가 정수화 되면 차원이 정해진 임의의 가중치 테이블의 내적으로 벡터화된다.</li>
<li>이때, 이 가중치 테이블을 embedding table (= embedding layer) 이라고 하고 각 행이 단어를 의미한다.</li>
<li>따라서, embedding table의 행의 크기 = vocab_size가 되고 내적의 결과가 단어의 벡터가 되어 딥러닝 입력값으로 사용된다.</li>
<li>딥러닝의 학습을 통해 이 embedding table (가중치 행렬)의 값이 최적화되고 이 값이 단어의 벡터가 된다.</li>
<li>딥러닝 자연어 처리 시 거의 항상 하게 되는 작업</li>
<li>vocab_size (고차원, 20k~30k) x embedding_dim (저차원, 128,256,512 등) 크기의 가중치 행렬이 학습되어 임베딩 벡터가 된다.</li>
<li>단어 -&gt; 정수 인코딩 -&gt; Embedding Layer -&gt; 임베딩 벡터(=밀집 벡터)</li>
<li>자연어 처리에서 단어를 정수로 바꿔주는 이유가 Embedding Layer를 통해 밀집 벡터로 변환하기 위해서이다.</li>
<li>Lookup table: 정수 인코딩을 밀집 벡터로 변환하는 테이블</li>
</ul></li>
<li><p>워드 임베딩 2가지 유형</p>
<ul>
<li>랜덤 초기화 임베딩
<ul>
<li>NNLM(Neural Network Language Model)과 마찬가지로 초기에 랜덤값의 가중치를 가지고 오차를 구하는 과정에서 embedding table의 값이 학습</li>
<li>NNLM은 이전 단어가 주어졌을 때, 다음 단어를 구하는 학습과정에서 오차를 줄이면서 학습되었으나 텍스트 분류, 개체명 인식등 수많은 task에서도 오차를 줄이며 학습 가능</li>
<li>task에 맞도록 embedding vector값이 최적화된다.</li>
<li>pytorch, keras 등 딥러닝 프레임워크에서 랜덤 초기화된 embedding layer를 제공한다.</li>
<li>모델이 역전파하는 과정에서 embedding layer의 가중치가 학습되어 최적화된다.</li>
</ul></li>
<li>사전 훈련된 임베딩 (Pre-trained Word Embedding)
<ul>
<li>이미 만들어진 임베딩 테이블을 사용</li>
<li>정해진 특정 알고리즘에 방대한 데이터를 입력으로 학습시킨 후 여러 task의 입력으로 사용</li>
<li>대표적인 알고리즘으로 word2vec, glove, fasttext가 있음</li>
<li>이미 방대한 양의 텍스트 데이터로 훈련되어져 있는 임베딩 벡터값들을 갖고와서 딥러닝 모델의 입력값으로 사용</li>
<li>이때 이 임베딩 벡터들은 word2vec, glove, fasttext 등 특정 알고리즘으로 훈련되어져 있는 임베딩 벡터값들이다.</li>
<li>이미 학습된 임베딩 벡터를 사용하므로 모델 학습 시간이 줄어들고 더 좋은 성능을 보임</li>
</ul></li>
</ul></li>
<li><p>핵심 원리</p>
<ul>
<li><strong>분포 가설(Distributional Hypothesis):</strong>
<ul>
<li>“같은 문맥에서 나타나는 단어들은 유사한 의미를 가진다”</li>
<li>수학적으로 표현하면: <span class="math inline">\(\text{similarity}(w_i, w_j) \propto \text{context\_overlap}(w_i, w_j)\)</span></li>
</ul></li>
<li><strong>“같은 문맥에 나타나는 단어들은 비슷한 의미를 가진다”</strong></li>
<li><strong>예시:</strong></li>
</ul>
<pre><code>문장1: "나는 사과를 먹었다"
문장2: "나는 바나나를 먹었다"  
문장3: "나는 딸기를 먹었다"</code></pre>
<p>→ “사과”, “바나나”, “딸기”는 같은 위치(문맥)에 나타남 → 비슷한 벡터를 가져야 함</p></li>
<li><p>임베딩 벡터의 의미</p>
<ul>
<li><p>벡터 간 유사도</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>cosine_similarity(v_사과, v_바나나) <span class="op">=</span> <span class="fl">0.8</span>  <span class="co"># 높음 (비슷함)</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>cosine_similarity(v_사과, v_컴퓨터) <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># 낮음 (다름)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>수식: <span class="math inline">\(\text{similarity}(\mathbf{v}_1, \mathbf{v}_2) = \frac{\mathbf{v}_1 \cdot \mathbf{v}_2}{||\mathbf{v}_1|| \cdot ||\mathbf{v}_2||}\)</span></li>
<li>직관적 해석
<ul>
<li>1에 가까울수록 비슷한 의미</li>
<li>0에 가까울수록 관련 없음</li>
<li>-1에 가까울수록 반대 의미</li>
</ul></li>
</ul></li>
<li><p>벡터 연산의 마법</p>
<ul>
<li>유명한 예시:
<ul>
<li><span class="math inline">\(\vec{\text{king}} - \vec{\text{man}} + \vec{\text{woman}} \approx \vec{\text{queen}}\)</span></li>
<li><span class="math inline">\(\vec{\text{king}} - \vec{\text{man}}\)</span>: “남성성”을 제거 → “왕권” 개념만 남음</li>
<li><span class="math inline">\(+ \vec{\text{woman}}\)</span>: “여성성” 추가</li>
<li>결과: “여성 + 왕권” → “여왕”</li>
</ul></li>
<li>수학적 설명:
<ul>
<li>각 벡터를 의미 성분들의 조합으로 생각:</li>
</ul>
<pre><code>king = [왕권: 0.9, 남성: 0.8, 권력: 0.7, ...]
man = [남성: 0.9, 성인: 0.6, ...]  
woman = [여성: 0.9, 성인: 0.6, ...]</code></pre>
<ul>
<li>연산 후:</li>
</ul>
<pre><code>king - man + woman ≈ [왕권: 0.9, 여성: 0.9, 권력: 0.7, ...]</code></pre>
→ “queen”과 가장 유사!</li>
</ul></li>
</ul></li>
<li><p>실제 학습 예시</p>
<ul>
<li><p>초기 상태 (랜덤)</p>
<pre><code>"사과" = [0.1, -0.3, 0.7, ...]  (랜덤)
"바나나" = [-0.8, 0.2, -0.1, ...] (랜덤)</code></pre>
<ul>
<li>서로 전혀 관련 없어 보임</li>
</ul></li>
<li><p>학습 진행</p>
<pre><code>문장들을 계속 보면서:
"사과를 먹었다", "바나나를 먹었다", "딸기를 먹었다"
...

* 점차 비슷한 벡터로 수렴:
</code></pre>
<p>“사과” = [0.2, -0.4, 0.6, …] “바나나” = [0.3, -0.5, 0.7, …]<br>
“딸기” = [0.25, -0.45, 0.65, …] ```</p></li>
<li><p>학습 완료 후</p>
<ul>
<li>의미가 비슷한 단어들 → 벡터 공간에서 가까운 위치</li>
<li>반대 의미 단어들 → 먼 위치 또는 반대 방향</li>
<li>유추 관계 → 벡터 연산으로 표현 가능</li>
</ul></li>
</ul></li>
<li><p><strong>핵심</strong>: 신경망이 “문맥”이라는 단서를 통해 <strong>단어의 의미</strong>를 수치로 학습</p></li>
</ul>
<section id="embedding-layer-구조-분석" class="level4" data-number="2.0.3.1">
<h4 data-number="2.0.3.1" class="anchored" data-anchor-id="embedding-layer-구조-분석"><span class="header-section-number">2.0.3.1</span> Embedding Layer 구조 분석</h4>
<ul>
<li>Embedding Layer란?
<ul>
<li>정수 인코딩 → 밀집 벡터 변환기</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 이런 변환을 해주는 것</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="dv">15</span> → [<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.3</span>]  <span class="co"># 300차원 벡터</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="dv">23</span> → [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="fl">0.5</span>]   <span class="co"># 300차원 벡터  </span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span>  → [<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.4</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.9</span>]  <span class="co"># 300차원 벡터</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>왜 정수 인코딩이 필요한가?
<ul>
<li>문제: 컴퓨터는 “사과”라는 글자를 직접 처리할 수 없음</li>
<li>해결 과정:</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1단계: 단어 → 정수 (정수 인코딩)</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co">"사과"</span> → <span class="dv">15</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">"바나나"</span> → <span class="dv">23</span>  </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">"딸기"</span> → <span class="dv">7</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 2단계: 정수 → 벡터 (Embedding Layer)</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="dv">15</span> → [<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.7</span>, ...]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="dv">23</span> → [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, ...]</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span>  → [<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.4</span>, ...]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p>Look-up Table의 구체적 동작</p>
<ul>
<li>Embedding Matrix 구조: <span class="math inline">\(\mathbf{E} \in \mathbb{R}^{V \times d}\)</span></li>
<li>실제 예시:</li>
</ul>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># V = 5 (어휘 크기), d = 3 (임베딩 차원)</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>embedding_matrix <span class="op">=</span> [</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>],    <span class="co"># 단어 ID 0의 벡터</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>],    <span class="co"># 단어 ID 1의 벡터  </span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>],    <span class="co"># 단어 ID 2의 벡터</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.4</span>],   <span class="co"># 단어 ID 3의 벡터</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>],   <span class="co"># 단어 ID 4의 벡터</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>행렬의 의미:</p>
<ul>
<li>행(row): 각 단어의 임베딩 벡터</li>
<li>열(column): 임베딩 벡터의 각 차원</li>
<li>전체: 모든 단어의 벡터를 저장하는 “사전”</li>
</ul></li>
<li><p>Look-up 연산 과정</p>
<ul>
<li>입력: <code>input_ids = [2, 0, 4]</code></li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1단계: 각 ID에 해당하는 행을 추출</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>embedding_matrix[<span class="dv">2</span>] → [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>]    <span class="co"># ID 2의 벡터</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>embedding_matrix[<span class="dv">0</span>] → [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>]    <span class="co"># ID 0의 벡터  </span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>embedding_matrix[<span class="dv">4</span>] → [<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>]   <span class="co"># ID 4의 벡터</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 2단계: 결과 (3개 벡터)</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> [</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>],     <span class="co"># 첫 번째 단어</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>],     <span class="co"># 두 번째 단어</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>]     <span class="co"># 세 번째 단어  </span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>수학적 의미</p>
<ul>
<li><span class="math inline">\(\text{embedding}(i) = \mathbf{E}[i, :]\)</span>
<ul>
<li><span class="math inline">\(i\)</span>: 단어의 정수 ID</li>
<li><span class="math inline">\(\mathbf{E}[i, :]\)</span>: 행렬 E의 i번째 행 전체</li>
<li>결과: i번째 단어의 임베딩 벡터</li>
</ul></li>
<li>구체적 예시:</li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">2</span>  <span class="co"># "딸기"의 ID라고 가정</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>embedding(<span class="dv">2</span>) <span class="op">=</span> E[<span class="dv">2</span>, :] <span class="op">=</span> [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>]  <span class="co"># 2번째 행</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>실제 PyTorch 코드</p></li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Embedding Layer 생성</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">1000</span>      <span class="co"># 어휘 크기</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">300</span>    <span class="co"># 벡터 차원</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> nn.Embedding(vocab_size, embedding_dim)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 내부 구조 확인</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(embedding.weight.shape)  <span class="co"># torch.Size([1000, 300])</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># → 1000×300 크기의 look-up table</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 입력 데이터</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> torch.tensor([<span class="dv">15</span>, <span class="dv">23</span>, <span class="dv">7</span>])  <span class="co"># 3개 단어의 ID</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. 임베딩 변환</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> embedding(input_ids)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output.shape)  <span class="co"># torch.Size([3, 300])</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># → 3개 단어 × 300차원 벡터</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Look-up Table이 학습되는 과정
<ul>
<li>초기화 (랜덤)</li>
</ul>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 처음에는 랜덤 값들</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>embedding_matrix <span class="op">=</span> torch.randn(vocab_size, embedding_dim)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li>학습 과정</li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 예: "사과는 맛있다"라는 문장 학습</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> [<span class="dv">15</span>, <span class="dv">23</span>, <span class="dv">7</span>]  <span class="co"># [사과는, 맛있다, &lt;</span><span class="re">END</span><span class="co">&gt;]</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 현재 임베딩으로 예측</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> embedding_matrix[input_ids]  <span class="co"># Look-up</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> model(embeddings)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 손실 계산</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(prediction, target)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 역전파로 embedding_matrix 업데이트  </span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>loss.backward()  <span class="co"># embedding_matrix의 gradient 계산</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>optimizer.step()  <span class="co"># embedding_matrix 값들 업데이트</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p>핵심: 학습이 진행되면서 embedding_matrix의 각 행(단어 벡터)이 점점 더 의미 있는 값으로 변함!</p></li>
<li><p>왜 “Look-up Table”이라고 부르는가?</p>
<ul>
<li>전통적인 사전과 비교</li>
</ul>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 일반 사전</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>사전 <span class="op">=</span> {</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>   <span class="st">"사과"</span>: <span class="st">"빨간 과일"</span>,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>   <span class="st">"바나나"</span>: <span class="st">"노란 과일"</span>, </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>   <span class="st">"컴퓨터"</span>: <span class="st">"전자 기기"</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>의미 <span class="op">=</span> 사전[<span class="st">"사과"</span>]  <span class="co"># "빨간 과일"</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Embedding Table  </span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>임베딩_테이블 <span class="op">=</span> {</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>   <span class="dv">15</span>: [<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.7</span>, ...],   <span class="co"># "사과"</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>   <span class="dv">23</span>: [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, ...],   <span class="co"># "바나나"</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>   <span class="dv">78</span>: [<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.8</span>, <span class="fl">0.1</span>, ...]    <span class="co"># "컴퓨터"</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>벡터 <span class="op">=</span> 임베딩_테이블[<span class="dv">15</span>]  <span class="co"># [0.2, -0.4, 0.7, ...]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>차이점:
<ul>
<li>일반 사전: 단어 → 설명 (텍스트)</li>
<li>임베딩 테이블: 단어 ID → 숫자 벡터</li>
</ul></li>
</ul></li>
<li><p>전체 과정 정리</p></li>
</ul>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 전체 파이프라인</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co">"사과는 맛있다"</span> </span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>→ [<span class="st">"사과는"</span>, <span class="st">"맛있다"</span>]           <span class="co"># 토큰화</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>→ [<span class="dv">15</span>, <span class="dv">23</span>]                      <span class="co"># 정수 인코딩  </span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>→ [[<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.7</span>],           <span class="co"># Embedding Layer (Look-up)</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>]]</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>→ 신경망 처리                    <span class="co"># 후속 레이어들</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>핵심
<ul>
<li>Embedding Layer는 단순히 “정수 ID를 인덱스로 사용해서 미리 저장된 벡터를 가져오는” 매우 단순한 연산.</li>
<li>하지만 이 벡터들이 학습을 통해 의미 있는 값으로 변하기 때문에 강력한 도구가 되는 것</li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="결론" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 결론</h1>
<p>본 문서에서는 자연어 처리(NLP) 분야에서 텍스트 데이터의 의미를 효과적으로 포착하기 위해 통계 기반 방법의 한계를 넘어, 신경망은 단어와 문맥의 복잡한 관계를 학습하여 풍부한 정보를 담은 벡터 표현을 생성한다.</p>
<ul>
<li><strong>워드 임베딩의 발전</strong>:
<ul>
<li><strong>정적 임베딩 (Word2Vec, GloVe, FastText)</strong>: ’분포 가설’에 기반하여 단어를 저차원 밀집 벡터로 표현함으로써 단어 간 의미적 유사성과 관계(예: 유추)를 포착했다. <code>Embedding Layer</code>는 이러한 변환의 핵심이며, FastText는 하위 단어(subword) 정보를 활용하여 OOV 문제와 형태론적 특징 처리에 강점을 보였다.</li>
<li>이러한 초기 신경망 기반 방법들은 단어의 의미를 고정된 벡터로 표현하여 NLP 성능을 크게 향상시켰다.</li>
</ul></li>
<li><strong>벡터화 방법 선택의 중요성</strong>:
<ul>
<li>단순한 단어 유사도 측정부터 복잡한 문서 이해 및 생성에 이르기까지, 해결하고자 하는 문제의 특성, 데이터의 규모와 성격, 그리고 사용하려는 모델의 요구사항을 종합적으로 고려하여 적절한 벡터화 전략을 선택하는 것이 중요하다.</li>
<li>이러한 신경망 기반 벡터화 기법들은 현대 대규모 언어 모델(LLM) 발전의 핵심적인 토대가 되었으며, 자연어 이해 및 생성 능력의 비약적인 발전을 이끌고 있다.</li>
</ul></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>