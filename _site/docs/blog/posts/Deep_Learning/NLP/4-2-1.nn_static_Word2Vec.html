<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.543">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kwangmin Kim">
<meta name="dcterms.date" content="2025-01-06">
<meta name="description" content="자연어 처리(NLP)에서 단어를 벡터로 표현하는 핵심 방법론인 Word2Vec을 심층적으로 탐구한다. CBOW 및 Skip-gram 모델의 원리, 수학적 배경, 학습 과정, 그리고 Negative Sampling 기법을 상세히 다룬다.">

<title>Kwangmin Kim - 텍스트 벡터화: Word2Vec의 이해</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete-preset-algolia.umd.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "algolia": {
    "application-id": "DUOR1DRC9D",
    "search-only-api-key": "f264da5dea684ffb9e9b4a574af3ed61",
    "index-name": "prod_QUARTO",
    "analytics-events": true,
    "show-logo": true,
    "libDir": "site_libs"
  },
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": true,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js"></script>


<script type="text/javascript">
var ALGOLIA_INSIGHTS_SRC = "https://cdn.jsdelivr.net/npm/search-insights/dist/search-insights.iife.min.js";
!function(e,a,t,n,s,i,c){e.AlgoliaAnalyticsObject=s,e[s]=e[s]||function(){
(e[s].queue=e[s].queue||[]).push(arguments)},i=a.createElement(t),c=a.getElementsByTagName(t)[0],
i.async=1,i.src=n,c.parentNode.insertBefore(i,c)
}(window,document,"script",ALGOLIA_INSIGHTS_SRC,"aa");
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/autocomplete-plugin-algolia-insights">

</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6W0EKFMWBN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6W0EKFMWBN', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../../styles.css">
<meta property="og:title" content="Kwangmin Kim - 텍스트 벡터화: Word2Vec의 이해">
<meta property="og:description" content="자연어 처리(NLP)에서 단어를 벡터로 표현하는 핵심 방법론인 Word2Vec을 심층적으로 탐구한다. CBOW 및 Skip-gram 모델의 원리, 수학적 배경, 학습 과정, 그리고 Negative Sampling 기법을 상세히 다룬다.">
<meta property="og:site_name" content="Kwangmin Kim">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html"> 
<span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"> <i class="bi bi-github" role="img" aria-label="Github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"> <i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#요약" id="toc-요약" class="nav-link active" data-scroll-target="#요약"><span class="header-section-number">1</span> 요약</a></li>
  <li><a href="#텍스트-인코딩-및-벡터화" id="toc-텍스트-인코딩-및-벡터화" class="nav-link" data-scroll-target="#텍스트-인코딩-및-벡터화"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</a>
  <ul class="collapse">
  <li><a href="#신경망-사용-20082018-static-word-embedding" id="toc-신경망-사용-20082018-static-word-embedding" class="nav-link" data-scroll-target="#신경망-사용-20082018-static-word-embedding"><span class="header-section-number">2.1</span> 신경망 사용 (2008~2018): Static Word Embedding</a>
  <ul class="collapse">
  <li><a href="#word2vec-2013" id="toc-word2vec-2013" class="nav-link" data-scroll-target="#word2vec-2013"><span class="header-section-number">2.1.1</span> Word2Vec (2013)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#결론" id="toc-결론" class="nav-link" data-scroll-target="#결론"><span class="header-section-number">3</span> 결론</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">텍스트 벡터화: Word2Vec의 이해</h1>
<p class="subtitle lead">CBOW와 Skip-gram을 이용한 신경망 기반 단어 임베딩</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>자연어 처리(NLP)에서 단어를 벡터로 표현하는 핵심 방법론인 Word2Vec을 심층적으로 탐구한다. CBOW 및 Skip-gram 모델의 원리, 수학적 배경, 학습 과정, 그리고 Negative Sampling 기법을 상세히 다룬다.</p>
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kwangmin Kim </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 6, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 전통적인 DTM 방식의 한계를 넘어, 단어의 의미론적 정보를 벡터 공간에 표현하는 신경망 기반의 핵심 방법론인 Word2Vec을 심층적으로 소개한다.</p>
<ul>
<li><strong>DTM 방식의 한계와 신경망 접근법의 등장</strong>:
<ul>
<li>전통적인 DTM(문서-단어 행렬) 방식의 문제점(차원의 저주, 희소성, 의미 관계 표현 불가)을 지적하고, 이를 극복하기 위한 신경망 기반 밀집 벡터 표현(워드 임베딩)의 필요성을 설명한다.</li>
</ul></li>
<li><strong>Word2Vec (CBOW, Skip-gram)</strong>:
<ul>
<li><strong>핵심 원리</strong>: “같은 문맥에 나타나는 단어는 비슷한 의미를 가진다”는 분포 가설에 기반하여 단어를 저차원 밀집 벡터로 표현한다.</li>
<li><strong>CBOW (Continuous Bag-of-Words)</strong>: 주변 단어들을 이용하여 중심 단어를 예측하는 모델의 구조와 수학적 원리를 설명한다.</li>
<li><strong>Skip-gram</strong>: 중심 단어를 이용하여 주변 단어들을 예측하는 모델의 구조와 수학적 원리를 설명한다. 특히, 연산 효율성을 높이기 위한 Negative Sampling (SGNS) 기법을 상세히 다룬다.</li>
<li>두 모델의 학습 과정과 임베딩 벡터가 단어의 의미를 포착하는 방식을 예시와 함께 설명한다.</li>
</ul></li>
<li><strong>결론</strong>: Word2Vec이 신경망 기반 단어 임베딩의 기초를 어떻게 마련했는지, 그리고 그 의의를 요약한다.</li>
</ul>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
        ├── GRU
    │   └── ELMo
    ├── Attention 메커니즘
    │   ├── Basic Attention
    │   ├── Self-Attention
    │   └── Multi-Head Attention
    └── Transformer 계열
        ├── BERT, RoBERTa, ALBERT
        ├── GPT 시리즈
        ├── KoBERT, KoGPT 등 한국어 특화
        └── 기타 모델: T5, LaMDA, PaLM, XLNet, ELECTRA</code></pre>
<section id="신경망-사용-20082018-static-word-embedding" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="신경망-사용-20082018-static-word-embedding"><span class="header-section-number">2.1</span> 신경망 사용 (2008~2018): Static Word Embedding</h2>
<section id="word2vec-2013" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="word2vec-2013"><span class="header-section-number">2.1.1</span> Word2Vec (2013)</h3>
<section id="word2vec-2013의-두-가지-아키텍처" class="level4" data-number="2.1.1.1">
<h4 data-number="2.1.1.1" class="anchored" data-anchor-id="word2vec-2013의-두-가지-아키텍처"><span class="header-section-number">2.1.1.1</span> <strong>Word2Vec (2013)</strong>의 두 가지 아키텍처:</h4>
<ul>
<li>전제: 같은 문잭에서의 단어들은 유사한 단어들이 주로 출현하게 된다.
<ul>
<li>강아지 문맥: 귀여운, 개, 놀이터, 애견, 등</li>
<li>법률 문맥: 판사, 변호사, 검사, 소송, 법정 등</li>
</ul></li>
<li><a href="https://w.elnn.kr/search">한국어 word2vec 예시</a><br>
</li>
<li><strong>CBOW (Continuous Bag of Words)</strong>
<ul>
<li>주변 단어들로 중심 단어 예측</li>
<li>목적 함수: <span class="math inline">\(\max \sum_{w \in V} \log P(w|context(w))\)</span></li>
<li>기본 구조
<ul>
<li><strong>목표</strong>: 주변 단어들을 보고 가운데 단어를 맞추기</li>
</ul>
<pre><code>입력: [나는] [___] [먹었다]  
출력: [사과를]</code></pre></li>
</ul></li>
</ul>
</section>
<section id="수학적-모델링" class="level4" data-number="2.1.1.2">
<h4 data-number="2.1.1.2" class="anchored" data-anchor-id="수학적-모델링"><span class="header-section-number">2.1.1.2</span> 수학적 모델링</h4>
<ul>
<li><strong>1단계: 입력 표현</strong>
<ul>
<li>문맥 단어들: <span class="math inline">\(w_{-2}, w_{-1}, w_{+1}, w_{+2}\)</span> (윈도우 크기 2)</li>
<li>각 단어의 원-핫 벡터: <span class="math inline">\(\mathbf{x}_{w} \in \{0,1\}^V\)</span> (V = 어휘 크기)</li>
</ul></li>
<li><strong>2단계: 임베딩 변환</strong>
<ul>
<li><span class="math inline">\(\mathbf{v}_w = \mathbf{W}_{\text{in}} \mathbf{x}_w\)</span></li>
<li>여기서:
<ul>
<li><span class="math inline">\(\mathbf{W}_{\text{in}} \in \mathbb{R}^{d \times V}\)</span>: 입력 임베딩 행렬</li>
<li><span class="math inline">\(d\)</span>: 임베딩 차원 (예: 300)</li>
<li><span class="math inline">\(\mathbf{v}_w \in \mathbb{R}^d\)</span>: 단어의 임베딩 벡터</li>
</ul></li>
<li><strong>직관적 해석</strong>:
<ul>
<li><span class="math inline">\(\mathbf{W}_{\text{in}}\)</span> 는 “단어 ID → 의미 벡터” 변환표</li>
<li>원-핫 벡터와의 곱은 단순히 해당 행을 선택하는 것</li>
</ul></li>
</ul></li>
<li><strong>3단계: 문맥 벡터 계산</strong>
<ul>
<li><span class="math inline">\(\mathbf{h} = \frac{1}{C} \sum_{c \in \text{context}} \mathbf{v}_c\)</span></li>
<li><strong>직관적 해석</strong>:
<ul>
<li>주변 단어들의 평균 벡터</li>
<li>“이 위치에 올 수 있는 단어의 특징”을 나타냄</li>
</ul></li>
</ul></li>
<li><strong>4단계: 출력 확률 계산</strong>
<ul>
<li><span class="math inline">\(P(w_{\text{center}}|\text{context}) = \frac{\exp(\mathbf{u}_{w_{\text{center}}}^T \mathbf{h})}{\sum_{w'=1}^V \exp(\mathbf{u}_{w'}^T \mathbf{h})}\)</span></li>
<li><strong>직관적 해석</strong>:
<ul>
<li>분자: 정답 단어가 이 문맥에 얼마나 적합한지</li>
<li>분모: 모든 단어 중에서 정규화 (확률의 합 = 1)</li>
</ul></li>
<li>예시
<ul>
<li><strong>예시 문장</strong>: “나는 사과를 먹었다”</li>
</ul>
<pre><code>Step 1: 문맥 = ["나는", "먹었다"], 정답 = "사과를"
Step 2: 현재 모델이 "바나나를" 높은 확률로 예측
Step 3: 손실 계산 → "사과를"의 확률을 높이도록 가중치 업데이트
Step 4: 반복 학습 후 → "나는 ___ 먹었다" 문맥에서 과일 단어들이 높은 확률</code></pre>
<ul>
<li><strong>학습 결과</strong>:
<ul>
<li>비슷한 문맥에 나타나는 단어들 → 비슷한 벡터</li>
<li>“사과”, “바나나”, “딸기” → 가까운 위치의 벡터</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="cbow" class="level4" data-number="2.1.1.3">
<h4 data-number="2.1.1.3" class="anchored" data-anchor-id="cbow"><span class="header-section-number">2.1.1.3</span> <strong>CBOW:</strong></h4>
<ul>
<li>(생략 - 구버전이라 알필요없다고 판단)</li>
</ul>
</section>
<section id="skip-gram" class="level4" data-number="2.1.1.4">
<h4 data-number="2.1.1.4" class="anchored" data-anchor-id="skip-gram"><span class="header-section-number">2.1.1.4</span> <strong>Skip-gram:</strong></h4>
<ul>
<li>CBoW보다 Skip-gram이 더 좋은 성능을 보임</li>
<li>구버전(중심 단어로 주변 단어들 예측)과 신버전(주변 단어들로 중심 단어 예측)으로 나뉨<br>
</li>
<li>중심 단어로 주변 단어들 예측, window size (중심 단어를 중심으로 앞뒤 몇 개의 단어를 문맥으로 설정) 존재.</li>
<li>window가 슬라이딩하면서 모든 단어에 대해 중심 단어로 주변 단어들 예측</li>
<li>window size가 크면 (7~25) 문맥이 넓어지고 작으면(2~7) 문맥이 좁아짐</li>
<li>윈도우 사이즈가 작으면 상호 교환할 수 있을 정도의 높은 유사도를 가진다.</li>
<li>여기서 상호 교환이 가능하다는 것은 반의어도 포함될 수 있다.</li>
<li>반면, 윈도우 사이즈가 크면 관련 단어들을 군집하는 효과가 있다.</li>
<li>목적 함수: <span class="math inline">\(\max \sum_{w \in V} \sum_{c \in context(w)} \log P(c|w)\)</span></li>
<li>특징:
<ul>
<li>선형 관계 학습: <span class="math inline">\(\vec{king} - \vec{man} + \vec{woman} \approx \vec{queen}\)</span></li>
<li>속도가 빠름</li>
<li>단어별 고정된 하나의 벡터</li>
</ul></li>
<li>구버전 예시</li>
</ul>
<pre><code>문장: Thou shalt not make a machine in the likeness of a human mind.
Input word: [not] 일 때,
Target word: [thou], [shalt], [make], [a]
따라서, 4개의 다중 클래스 분류 문제로 딥러닝을 써서 학습

"중심 단어로 문맥 예측하기": 슬라이딩 하면서 중심단어를 바꿔가며 학습
Input words: [Thou], [shalt], [not], [make], [a], [machine], [in], [the], [likeness], [of], [a], [human], [mind]

예를 들어, 중심 단어가 [not]일 때, 주변 단어들은 [thou], [shalt], [make], [a]가 되도록 학습
softmax 와 cross entropy 손실 함수를 사용하여 학습
input layer, projection layer, output layer 3개의 층으로 구성된 신경망
소프트맥스 함수를 지난 예측값과 실제값으로부터 오차를 구한다.
하지만 이 구버전으로 사용하지 않는 방법이다.
그 이유는 속도가 너무 느리고 단어 집합의 크기에 대해서 softmax + cross entropy 손실 함수를 사용하면 연산이 너무 무겁다.
통상적으로, 단어 집합의 크기는 일반적으로 수 만개 이상이다.</code></pre>
<ul>
<li>신버전 예시</li>
</ul>
<pre><code>Skip-gram with Negative Sampling (SGNS)
다중 클래스 분류를 이진분류 문제로 바꾸어 연산량을 줄임
중심단어와 주변 단어를 입력값으로 주고 이 둘의 내적으로부터 0,1로 이진 분류: 1이면 이웃단어, 0이면 비이웃단어
중심 단어 데이터셋에 레이블을 1로 할당하고 Negative Sampling을 통해 비이웃 단어 샘플들도 추가
Negative Sampling: 비이웃 단어 샘플들을 전체 데이터셋에서 랜덤하게 추출한다.
2개의 Embedding Table을 사용
   * 하나는 임베딩 테이블 (size = 단어의 개수를 행으로 갖고 embedding dimension을 열로 갖는 행렬): 임베딩 디멘션은 사용자가 지정
   * 하나는 주변 단어 정보를 갖는 context 테이블 (임베딩 테이블의 동일한 크기를 갖는 테이블)

문장: Thou shalt not make a machine in the likeness of a human mind.
Input word: [not, not, not] = [0.2, 0.2, 0.2]
output words: [thou, taco, you] = [0.1, 0.7, -0.6]
target = [1, 0, 0]
input x output = [0.02 0.14 -0.12]
sigmoid(x) = [0.505 0.535 0.47]
error = target - sigmoid(x) = [1-0.505, 0-0.535, 0-0.47] = [0.495, -0.535, -0.47]
역전파를 통해 error를 최소화하는 input word와 output word의 임베딩 벡터를 학습

embedding vector의 차원을 정하는 것은 사용자의 몫
Negative Sampling의 비율 또한 성능에 영향을 주는 결정 요소
논문에서는 5-20을 최적의 숫자로 제안한다.
데이터가 방대하다면 2-5로도 충분하다.</code></pre>
<ul>
<li>수학적 모델</li>
<li>목적 함수: <span class="math inline">\(\max \sum_{w \in V} \sum_{c \in context(w)} \log P(c|w)\)</span></li>
<li>단계별 해석:
<ul>
<li><strong>1단계</strong>: <span class="math inline">\(w \in V\)</span> (모든 단어에 대해)
<ul>
<li>말뭉치의 모든 단어를 중심 단어로 한 번씩 사용</li>
</ul></li>
<li><strong>2단계</strong>: <span class="math inline">\(c \in context(w)\)</span> (각 중심 단어의 모든 문맥 단어에 대해)
<ul>
<li>윈도우 크기만큼 주변 단어들을 문맥으로 설정</li>
</ul></li>
<li><strong>3단계</strong>: <span class="math inline">\(\log P(c|w)\)</span> (확률의 로그값)
<ul>
<li>중심 단어 w가 주어졌을 때 문맥 단어 c가 나타날 확률</li>
</ul></li>
</ul></li>
<li>예시
<ul>
<li><strong>문장</strong>: “나는 사과를 정말 좋아한다” (윈도우 크기 = 2)</li>
</ul>
<pre><code>중심 단어: "사과를"
문맥 단어들: ["나는", "정말", "좋아한다"] (앞뒤 2개씩)

Skip-gram이 학습하는 것:
P("나는"|"사과를")     → 높아야 함
P("정말"|"사과를")     → 높아야 함  
P("좋아한다"|"사과를") → 높아야 함
P("컴퓨터"|"사과를")   → 낮아야 함 (문맥에 없음)</code></pre></li>
<li>확률 계산: <span class="math inline">\(P(c|w) = \frac{\exp(\mathbf{u}_c^T \mathbf{v}_w)}{\sum_{c'=1}^V \exp(\mathbf{u}_{c'}^T \mathbf{v}_w)}\)</span>
<ul>
<li><span class="math inline">\(\mathbf{v}_w\)</span> : 중심 단어 w의 입력 벡터 (우리가 원하는 임베딩)</li>
<li><span class="math inline">\(\mathbf{u}_c\)</span> : 문맥 단어 c의 출력 벡터</li>
<li><span class="math inline">\(\mathbf{u}_c^T \mathbf{v}_w\)</span> : 두 단어가 “같이 나타날 가능성” 점수</li>
<li>Softmax로 정규화하여 확률로 변환</li>
</ul></li>
<li>학습 과정 예시
<ul>
<li><p>초기 상태 (랜덤 벡터)</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>v_사과 <span class="op">=</span> [<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>]  <span class="co"># 중심 단어 벡터</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>u_나는 <span class="op">=</span> [<span class="fl">0.4</span>, <span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.1</span>]  <span class="co"># 문맥 단어 벡터</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>u_컴퓨터 <span class="op">=</span> [<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>점수 계산</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>score_나는 <span class="op">=</span> dot(u_나는, v_사과) <span class="op">=</span> <span class="fl">0.4</span><span class="op">*</span><span class="fl">0.1</span> <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>(<span class="op">-</span><span class="fl">0.2</span>) <span class="op">+</span> (<span class="op">-</span><span class="fl">0.1</span>)<span class="op">*</span><span class="fl">0.3</span> <span class="op">=</span> <span class="op">-</span><span class="fl">0.01</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>score_컴퓨터 <span class="op">=</span> dot(u_컴퓨터, v_사과) <span class="op">=</span> <span class="op">-</span><span class="fl">0.2</span><span class="op">*</span><span class="fl">0.1</span> <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span>(<span class="op">-</span><span class="fl">0.2</span>) <span class="op">+</span> <span class="fl">0.2</span><span class="op">*</span><span class="fl">0.3</span> <span class="op">=</span> <span class="op">-</span><span class="fl">0.08</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>확률 계산 (간단히)</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>P(나는<span class="op">|</span>사과) <span class="op">=</span> exp(<span class="op">-</span><span class="fl">0.01</span>) <span class="op">/</span> (exp(<span class="op">-</span><span class="fl">0.01</span>) <span class="op">+</span> exp(<span class="op">-</span><span class="fl">0.08</span>) <span class="op">+</span> ...) ≈ <span class="fl">0.3</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>P(컴퓨터<span class="op">|</span>사과) <span class="op">=</span> exp(<span class="op">-</span><span class="fl">0.08</span>) <span class="op">/</span> (exp(<span class="op">-</span><span class="fl">0.01</span>) <span class="op">+</span> exp(<span class="op">-</span><span class="fl">0.08</span>) <span class="op">+</span> ...) ≈ <span class="fl">0.2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>학습 업데이트</p>
<ul>
<li>목표: P(나는|사과)는 높이고, P(컴퓨터|사과)는 낮추기</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 실제로 "나는"이 문맥에 있었으므로</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># v_사과와 u_나는을 더 비슷하게 만들기</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>v_사과 <span class="op">+=</span> learning_rate <span class="op">*</span> u_나는  <span class="co"># 벡터를 가까워지게</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>u_나는 <span class="op">+=</span> learning_rate <span class="op">*</span> v_사과</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># "컴퓨터"는 문맥에 없었으므로  </span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># v_사과와 u_컴퓨터를 더 멀게 만들기</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>v_사과 <span class="op">-=</span> learning_rate <span class="op">*</span> u_컴퓨터  <span class="co"># 벡터를 멀어지게</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>u_컴퓨터 <span class="op">-=</span> learning_rate <span class="op">*</span> v_사과</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li>Word2Vec: 단어 수 × 벡터 차원</li>
</ul>
</section>
</section>
</section>
</section>
<section id="결론" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 결론</h1>
<p>본 문서에서는 자연어 처리(NLP) 분야에서 단어의 의미를 효과적으로 벡터 공간에 표현하기 위한 핵심적인 신경망 기반 방법론인 Word2Vec을 살펴보았다. Word2Vec은 ’분포 가설’에 착안하여, 단어가 사용되는 문맥을 학습함으로써 단어 간의 의미론적 유사성과 관계를 밀집 벡터 형태로 포착한다.</p>
<ul>
<li><strong>Word2Vec의 핵심</strong>:
<ul>
<li><strong>CBOW와 Skip-gram</strong>: Word2Vec은 주변 단어로부터 중심 단어를 예측하는 CBOW 방식과 중심 단어로부터 주변 단어를 예측하는 Skip-gram 방식을 통해 단어 임베딩을 학습한다. 각 방식은 서로 다른 관점에서 문맥 정보를 활용하며, 특히 Skip-gram은 Negative Sampling과 같은 최적화 기법을 통해 대규모 데이터셋에서도 효율적으로 학습할 수 있다.</li>
<li>학습된 임베딩 벡터는 단어의 의미적 유사성(예: ‘왕’ - ‘남자’ + ‘여자’ ≈ ‘여왕’)을 벡터 공간에서의 관계로 나타낼 수 있으며, 이는 정적 임베딩 방식의 중요한 성과이다.</li>
</ul></li>
<li><strong>Word2Vec의 의의</strong>:
<ul>
<li>Word2Vec은 단어의 의미를 고정된 벡터로 표현하는 정적 임베딩 방식의 대표적인 예로, 이후 등장하는 다양한 문맥 기반 동적 임베딩 방법론들의 중요한 기초가 되었다.</li>
<li>이러한 신경망 기반의 단어 표현 방식은 기존의 통계 기반 방법론들의 한계를 극복하고 NLP 분야의 성능을 크게 향상시키는 데 기여했으며, 현대 대규모 언어 모델(LLM) 발전의 핵심적인 토대를 마련했다.</li>
</ul></li>
</ul>
<p>선택하는 벡터화 전략은 해결하고자 하는 문제의 특성, 데이터의 규모와 성격, 그리고 사용하려는 모델의 요구사항을 종합적으로 고려해야 하지만, Word2Vec은 그 자체로도 여전히 유용하며, 더 복잡한 모델을 이해하는 데 있어 중요한 개념적 기반을 제공한다.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>