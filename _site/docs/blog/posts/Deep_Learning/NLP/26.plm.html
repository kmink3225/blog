<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.543">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kwangmin Kim">
<meta name="dcterms.date" content="2025-01-26">
<meta name="description" content="사전 학습 언어 모델(PLM)의 기술적 발전은 눈부시지만, 모든 프로젝트에 최신 모델을 적용할 수는 없다. 기업 규모, 프로젝트 특성, 비용, 성능 요구사항에 따라 적절한 모델을 선택하는 것이 성공의 핵심이다. LSTM부터 T5, ChatGPT까지의 발전 과정을 살펴보고, 실무에서 마주하는 현실적 제약들 속에서 최적의 모델을 선택하는 전략을 제시한다.">

<title>Kwangmin Kim - PLM: Pre-trained Language Model</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete-preset-algolia.umd.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "algolia": {
    "application-id": "DUOR1DRC9D",
    "search-only-api-key": "f264da5dea684ffb9e9b4a574af3ed61",
    "index-name": "prod_QUARTO",
    "analytics-events": true,
    "show-logo": true,
    "libDir": "site_libs"
  },
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": true,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js"></script>


<script type="text/javascript">
var ALGOLIA_INSIGHTS_SRC = "https://cdn.jsdelivr.net/npm/search-insights/dist/search-insights.iife.min.js";
!function(e,a,t,n,s,i,c){e.AlgoliaAnalyticsObject=s,e[s]=e[s]||function(){
(e[s].queue=e[s].queue||[]).push(arguments)},i=a.createElement(t),c=a.getElementsByTagName(t)[0],
i.async=1,i.src=n,c.parentNode.insertBefore(i,c)
}(window,document,"script",ALGOLIA_INSIGHTS_SRC,"aa");
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/autocomplete-plugin-algolia-insights">

</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6W0EKFMWBN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6W0EKFMWBN', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../../../../../styles.css">
<meta property="og:title" content="Kwangmin Kim - PLM: Pre-trained Language Model">
<meta property="og:description" content="사전 학습 언어 모델(PLM)의 기술적 발전은 눈부시지만, 모든 프로젝트에 최신 모델을 적용할 수는 없다. 기업 규모, 프로젝트 특성, 비용, 성능 요구사항에 따라 적절한 모델을 선택하는 것이 성공의 핵심이다. LSTM부터 T5, ChatGPT까지의 발전 과정을 살펴보고, 실무에서 마주하는 현실적 제약들 속에서 최적의 모델을 선택하는 전략을 제시한다.">
<meta property="og:site_name" content="Kwangmin Kim">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html"> 
<span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"> <i class="bi bi-github" role="img" aria-label="Github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"> <i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#요약" id="toc-요약" class="nav-link active" data-scroll-target="#요약"><span class="header-section-number">1</span> 요약</a>
  <ul class="collapse">
  <li><a href="#모델-발전의-딜레마" id="toc-모델-발전의-딜레마" class="nav-link" data-scroll-target="#모델-발전의-딜레마"><span class="header-section-number">1.1</span> 모델 발전의 딜레마</a></li>
  <li><a href="#현실적-선택-기준" id="toc-현실적-선택-기준" class="nav-link" data-scroll-target="#현실적-선택-기준"><span class="header-section-number">1.2</span> 현실적 선택 기준</a></li>
  <li><a href="#효과적인-전략들" id="toc-효과적인-전략들" class="nav-link" data-scroll-target="#효과적인-전략들"><span class="header-section-number">1.3</span> 효과적인 전략들</a></li>
  </ul></li>
  <li><a href="#nlp-모델-발전-과정" id="toc-nlp-모델-발전-과정" class="nav-link" data-scroll-target="#nlp-모델-발전-과정"><span class="header-section-number">2</span> NLP 모델 발전 과정</a></li>
  <li><a href="#실무에서의-모델-선택-전략" id="toc-실무에서의-모델-선택-전략" class="nav-link" data-scroll-target="#실무에서의-모델-선택-전략"><span class="header-section-number">3</span> 실무에서의 모델 선택 전략</a>
  <ul class="collapse">
  <li><a href="#성능-vs-현실-사이의-간극" id="toc-성능-vs-현실-사이의-간극" class="nav-link" data-scroll-target="#성능-vs-현실-사이의-간극"><span class="header-section-number">3.1</span> 성능 vs 현실 사이의 간극</a>
  <ul class="collapse">
  <li><a href="#기술-발전과-현실적-제약" id="toc-기술-발전과-현실적-제약" class="nav-link" data-scroll-target="#기술-발전과-현실적-제약"><span class="header-section-number">3.1.1</span> 기술 발전과 현실적 제약</a></li>
  <li><a href="#모델별-현실적-접근성" id="toc-모델별-현실적-접근성" class="nav-link" data-scroll-target="#모델별-현실적-접근성"><span class="header-section-number">3.1.2</span> 모델별 현실적 접근성</a></li>
  </ul></li>
  <li><a href="#프로젝트-특성별-모델-선택" id="toc-프로젝트-특성별-모델-선택" class="nav-link" data-scroll-target="#프로젝트-특성별-모델-선택"><span class="header-section-number">3.2</span> 프로젝트 특성별 모델 선택</a>
  <ul class="collapse">
  <li><a href="#문서-분류-프로젝트-사례" id="toc-문서-분류-프로젝트-사례" class="nav-link" data-scroll-target="#문서-분류-프로젝트-사례"><span class="header-section-number">3.2.1</span> 문서 분류 프로젝트 사례</a></li>
  <li><a href="#실제-선택-기준" id="toc-실제-선택-기준" class="nav-link" data-scroll-target="#실제-선택-기준"><span class="header-section-number">3.2.2</span> 실제 선택 기준</a></li>
  </ul></li>
  <li><a href="#현실적-하이브리드-전략" id="toc-현실적-하이브리드-전략" class="nav-link" data-scroll-target="#현실적-하이브리드-전략"><span class="header-section-number">3.3</span> 현실적 하이브리드 전략</a>
  <ul class="collapse">
  <li><a href="#계층적-모델-활용" id="toc-계층적-모델-활용" class="nav-link" data-scroll-target="#계층적-모델-활용"><span class="header-section-number">3.3.1</span> 계층적 모델 활용</a></li>
  <li><a href="#api를-활용한-데이터-생성-전략" id="toc-api를-활용한-데이터-생성-전략" class="nav-link" data-scroll-target="#api를-활용한-데이터-생성-전략"><span class="header-section-number">3.3.2</span> API를 활용한 데이터 생성 전략</a></li>
  <li><a href="#단계별-모델-도입-전략" id="toc-단계별-모델-도입-전략" class="nav-link" data-scroll-target="#단계별-모델-도입-전략"><span class="header-section-number">3.3.3</span> 단계별 모델 도입 전략</a></li>
  </ul></li>
  <li><a href="#의사결정-프레임워크" id="toc-의사결정-프레임워크" class="nav-link" data-scroll-target="#의사결정-프레임워크"><span class="header-section-number">3.4</span> 의사결정 프레임워크</a>
  <ul class="collapse">
  <li><a href="#모델-선택-체크리스트" id="toc-모델-선택-체크리스트" class="nav-link" data-scroll-target="#모델-선택-체크리스트"><span class="header-section-number">3.4.1</span> 모델 선택 체크리스트</a></li>
  <li><a href="#실무진을-위한-가이드라인" id="toc-실무진을-위한-가이드라인" class="nav-link" data-scroll-target="#실무진을-위한-가이드라인"><span class="header-section-number">3.4.2</span> 실무진을 위한 가이드라인</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#결론" id="toc-결론" class="nav-link" data-scroll-target="#결론"><span class="header-section-number">4</span> 결론</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">PLM: Pre-trained Language Model</h1>
<p class="subtitle lead">실무에서의 현실적 모델 선택과 적용 전략</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>사전 학습 언어 모델(PLM)의 기술적 발전은 눈부시지만, 모든 프로젝트에 최신 모델을 적용할 수는 없다. 기업 규모, 프로젝트 특성, 비용, 성능 요구사항에 따라 적절한 모델을 선택하는 것이 성공의 핵심이다. LSTM부터 T5, ChatGPT까지의 발전 과정을 살펴보고, 실무에서 마주하는 현실적 제약들 속에서 최적의 모델을 선택하는 전략을 제시한다.</p>
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kwangmin Kim </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 26, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>사전 학습 언어 모델(PLM)은 지속적으로 발전하고 있지만, <strong>무조건 최신 모델이 항상 정답은 아니다</strong>. 기업의 규모, 프로젝트의 특성, 예산 제약, 성능 요구사항에 따라 적절한 모델을 선택하는 것이 실무에서는 더욱 중요하다.</p>
<section id="모델-발전의-딜레마" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="모델-발전의-딜레마"><span class="header-section-number">1.1</span> 모델 발전의 딜레마</h2>
<p>기술적으로는 LSTM → BERT → T5 → ChatGPT로 발전할수록 성능이 향상되지만, 실무에서는 다음과 같은 현실적 제약이 있다:</p>
<ul>
<li><strong>비용과 리소스</strong>: 최신 모델일수록 막대한 컴퓨팅 비용과 인프라 필요</li>
<li><strong>개발 복잡성</strong>: 고성능 모델은 구현과 튜닝의 난이도가 높음</li>
<li><strong>운영 효율성</strong>: 실시간 서비스에서는 지연시간이 비즈니스 성패를 좌우</li>
<li><strong>도메인 적합성</strong>: 범용 모델보다 특화된 작은 모델이 더 효과적인 경우 존재</li>
</ul>
</section>
<section id="현실적-선택-기준" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="현실적-선택-기준"><span class="header-section-number">1.2</span> 현실적 선택 기준</h2>
<ol type="1">
<li><strong>스타트업/중소기업</strong>: 1.3B 이하 오픈소스 모델(BERT, KoGPT)이 현실적 마지노선</li>
<li><strong>중견기업</strong>: 하이브리드 접근법(API + 자체 모델)으로 비용 효율성 추구</li>
<li><strong>대기업</strong>: 자체 인프라와 전문 인력을 바탕으로 최신 모델 적용</li>
</ol>
</section>
<section id="효과적인-전략들" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="효과적인-전략들"><span class="header-section-number">1.3</span> 효과적인 전략들</h2>
<ul>
<li><strong>API 활용</strong>: ChatGPT API로 데이터 생성 후 작은 모델 학습</li>
<li><strong>모델 조합</strong>: 검색엔진 + ChatGPT + 프롬프트 엔지니어링</li>
<li><strong>단계적 접근</strong>: 작은 모델로 시작해서 필요에 따라 확장</li>
<li><strong>도메인 특화</strong>: 범용성보다는 특정 문제에 최적화</li>
</ul>
<p>결국, 문제의 복잡도와 요구사항에 맞는 적절한 모델을 선택하는 것이 성공의 핵심이다.</p>
</section>
</section>
<section id="nlp-모델-발전-과정" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> NLP 모델 발전 과정</h1>
<pre><code>RNN Language Model
├── Seq2Seq
├── Beam Search
├── Subword Tokenization
├── Attention
├── Transformer Encoder (Vaswani et al., 2017)
|   ├── Positional Encoding
|   ├── Multi-Head Attention
|   └── Feed Forward Neural Network
|
├── Transformer Decoder (Vaswani et al., 2017)
|
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
|
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa (Facebook, 2019)
|   ├── ALBERT (Google, 2019)
|   ├── DistilBERT (Hugging Face, 2019)
|   └── ELECTRA (Google, 2020)
|
└── 후속 발전 모델들
    ├── T5, XLNet, DeBERTa
    └── GPT-2/3/4, ChatGPT, PaLM 등</code></pre>
</section>
<section id="실무에서의-모델-선택-전략" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 실무에서의 모델 선택 전략</h1>
<section id="성능-vs-현실-사이의-간극" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="성능-vs-현실-사이의-간극"><span class="header-section-number">3.1</span> 성능 vs 현실 사이의 간극</h2>
<p>PLM 기술은 매년 눈부신 발전을 보이고 있다. 논문에서는 항상 더 큰 모델이 더 좋은 성능을 보여주고, 최신 모델들이 이전 모델들의 단점을 보완하며 SOTA를 경신한다. 하지만 실무에서는 이야기가 다르다.</p>
<section id="기술-발전과-현실적-제약" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="기술-발전과-현실적-제약"><span class="header-section-number">3.1.1</span> 기술 발전과 현실적 제약</h3>
<p><strong>기술적 우수성과 실용성은 별개의 문제다</strong>. T5가 BERT보다 우수하고, ChatGPT가 T5보다 뛰어나다고 해서 모든 프로젝트에 ChatGPT를 써야 하는 것은 아니다. 다음과 같은 현실적 제약들이 존재한다:</p>
<ul>
<li><strong>컴퓨팅 비용</strong>: GPT-4 API 호출 비용 vs 자체 BERT 모델 운영 비용</li>
<li><strong>지연시간</strong>: 실시간 챗봇에서 3초 응답 vs 100ms 응답의 차이</li>
<li><strong>데이터 보안</strong>: 민감한 데이터를 외부 API로 전송할 수 없는 경우</li>
<li><strong>커스터마이징</strong>: 도메인 특화 요구사항에 대한 대응 가능성</li>
<li><strong>안정성</strong>: 서비스 중단 없이 24/7 운영 가능한지</li>
</ul>
</section>
<section id="모델별-현실적-접근성" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="모델별-현실적-접근성"><span class="header-section-number">3.1.2</span> 모델별 현실적 접근성</h3>
<pre><code>[ 기업 규모별 현실적 모델 선택 ]

스타트업 (직원 ~50명)
├── BERT, KoGPT: 무료 오픈소스, 자체 서버 운영 가능
├── OpenAI API: 초기 프로토타입용, 월 예산 ~$1,000
└── 하이브리드: API로 데이터 생성 → 작은 모델 학습

중견기업 (직원 ~500명)  
├── DistilBERT, ALBERT: 효율성 최적화된 모델
├── T5-Small/Base: 적당한 성능과 비용의 균형
├── API + 자체모델: 복잡한 태스크는 API, 단순한 것은 자체 모델
└── 클라우드 GPU: AWS/GCP의 관리형 서비스 활용

대기업 (직원 1,000명+)
├── T5-Large, GPT-3.5 수준: 자체 인프라로 운영
├── 전용 하드웨어: A100 클러스터, TPU 등
├── 자체 모델 개발: 도메인 특화 대규모 모델
└── 하이브리드 전략: 용도별로 다양한 모델 조합</code></pre>
</section>
</section>
<section id="프로젝트-특성별-모델-선택" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="프로젝트-특성별-모델-선택"><span class="header-section-number">3.2</span> 프로젝트 특성별 모델 선택</h2>
<section id="문서-분류-프로젝트-사례" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="문서-분류-프로젝트-사례"><span class="header-section-number">3.2.1</span> 문서 분류 프로젝트 사례</h3>
<p><strong>상황</strong>: 고객 문의사항을 10개 카테고리로 자동 분류하는 시스템</p>
<p><strong>옵션별 비교</strong>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 옵션 1: BERT-Base (110M)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>장점: </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 분류 태스크에 최적화된 구조</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 빠른 추론 속도 (<span class="dv">10</span><span class="op">-</span><span class="dv">50</span><span class="er">ms</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 완전한 온프레미스 운영 가능</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 충분한 성능 (<span class="dv">95</span><span class="op">%+</span> 정확도)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>단점:</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 새로운 카테고리 추가 시 재학습 필요</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 복잡한 추론이나 설명 생성 불가</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>비용: 월 $<span class="dv">100</span> (AWS t3.medium 인스턴스)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 옵션 2: GPT-4 API</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>장점:</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Few<span class="op">-</span>shot learning으로 빠른 적응</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 분류 근거까지 자연어로 설명 가능</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 새 카테고리 추가가 프롬프트 수정만으로 가능</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 복잡한 경계 케이스도 잘 처리</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>단점:</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 높은 API 비용 (요청당 $<span class="fl">0.03</span><span class="op">-</span><span class="fl">0.06</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 외부 의존성으로 인한 서비스 리스크</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 응답 속도 변동성 (<span class="dv">200</span><span class="er">ms</span><span class="op">-</span><span class="dv">2</span><span class="er">s</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>비용: 월 $<span class="dv">3</span>,<span class="dv">000</span><span class="op">-</span><span class="dv">5</span>,<span class="dv">000</span> (일 <span class="dv">10</span>,<span class="dv">000</span><span class="er">건</span> 처리 시)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="실제-선택-기준" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="실제-선택-기준"><span class="header-section-number">3.2.2</span> 실제 선택 기준</h3>
<p><strong>언제 작은 모델을 선택해야 하는가?</strong></p>
<ol type="1">
<li><strong>명확한 태스크 정의</strong>: 감정 분석, 개체명 인식 등 잘 정의된 문제</li>
<li><strong>빠른 응답 필요</strong>: 실시간 추천, 챗봇 등</li>
<li><strong>대용량 배치 처리</strong>: 비용 효율성이 핵심인 경우 (단순 반복 작업)
<ul>
<li>예: 일일 수백만 건의 뉴스 기사 분류</li>
<li>예: 고객 리뷰의 감정 분석 (긍정/부정/중립)</li>
<li>예: 이메일 스팸 필터링</li>
<li>이유: API 비용($0.03/건 × 100만건 = $30,000/일) vs 자체 모델($100/월)</li>
</ul></li>
<li><strong>데이터 보안</strong>: 금융, 의료 등 민감한 정보</li>
<li><strong>예산 제약</strong>: 스타트업의 MVP(Minimum Viable Product) 단계</li>
</ol>
<p><strong>언제 큰 모델을 선택해야 하는가?</strong></p>
<ol type="1">
<li><strong>복잡한 추론</strong>: 다단계 논리적 사고가 필요한 경우</li>
<li><strong>유연성 중요</strong>: 요구사항이 자주 변하는 환경</li>
<li><strong>높은 품질 요구</strong>: 고객 대면 서비스, 브랜드 이미지가 중요한 경우</li>
<li><strong>다양한 태스크</strong>: 하나의 모델로 여러 기능을 처리해야 하는 경우</li>
<li><strong>소량 고품질 처리</strong>: 일일 수천~수만 건의 프리미엄 서비스 (고품질의 창의적 작업)
<ul>
<li>예: 법률 문서 분석 및 요약</li>
<li>예: 개인 맞춤형 투자 조언 생성</li>
<li>예: 고급 마케팅 콘텐츠 작성</li>
</ul></li>
<li><strong>충분한 예산</strong>: ROI가 명확하게 보장되는 경우</li>
</ol>
</section>
</section>
<section id="현실적-하이브리드-전략" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="현실적-하이브리드-전략"><span class="header-section-number">3.3</span> 현실적 하이브리드 전략</h2>
<section id="계층적-모델-활용" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="계층적-모델-활용"><span class="header-section-number">3.3.1</span> 계층적 모델 활용</h3>
<p>많은 기업들이 실제로 사용하는 전략은 <strong>단일 모델이 아닌 계층적 접근법</strong>이다. 태스크의 복잡도에 따라 적절한 모델을 라우팅하는 방식이다.</p>
<p><strong>핵심은 복잡도 분류 모델(라우터)</strong>이다. 이 작은 모델이 전체 시스템의 효율성을 결정한다:</p>
<p><strong>복잡도별 태스크 분류:</strong></p>
<ul>
<li><strong>Simple (단순)</strong>: 정형화된 정보 검색, 단순 분류
<ul>
<li>FAQ 검색, 카테고리 분류, 키워드 추출</li>
<li>BERT, DistilBERT 등 가벼운 모델로 충분</li>
</ul></li>
<li><strong>Medium (중간)</strong>: 텍스트 변환, 기본적인 생성
<ul>
<li>문서 요약, 번역, 제품 설명 생성, 이메일 자동 응답</li>
<li>템플릿 기반 보고서 작성, 간단한 QA</li>
<li>T5, BART 등 encoder-decoder 모델 적합</li>
</ul></li>
<li><strong>Complex (복잡)</strong>: 창의적 사고, 복합적 추론
<ul>
<li>마케팅 콘텐츠 창작, 코드 생성, 복잡한 분석</li>
<li>다단계 논리 추론, 개인화된 조언</li>
<li>GPT-4, Claude 등 대규모 모델 필요</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1단계: 질문 복잡도 분류 모델 (라우터)</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ComplexityClassifier:</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 가벼운 BERT 모델로 빠른 분류 (10-20ms)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> DistilBERT_for_classification()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_complexity(<span class="va">self</span>, query):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.extract_features(query)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model.predict(features)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract_features(<span class="va">self</span>, query):</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">'length'</span>: <span class="bu">len</span>(query.split()),</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">'question_words'</span>: count_wh_words(query),</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">'conjunctions'</span>: count_conjunctions(query),</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">'domain_keywords'</span>: check_domain_keywords(query),</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>            <span class="st">'sentiment_complexity'</span>: analyze_sentiment_depth(query)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> smart_routing(user_query):</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1단계: 빠른 분류기로 복잡도 판단</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    classifier <span class="op">=</span> ComplexityClassifier()</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    complexity <span class="op">=</span> classifier.predict_complexity(user_query)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> complexity <span class="op">==</span> <span class="st">"simple"</span>:</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 단순 반복의 간단한 질문은 BERT 기반 FAQ 검색</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 예: "운영시간이 언제인가요?", "배송비는 얼마인가요?"</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> faq_bert_model.search(user_query)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> complexity <span class="op">==</span> <span class="st">"medium"</span>:</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 중간 복잡도는 T5 모델 사용</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 예: 제품 설명 요약, 기본적인 문서 생성, 간단한 QA</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> t5_model.generate_response(user_query)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:  <span class="co"># complex</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 복잡한 질문만 GPT-4 API 호출</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 예: 창의적 글쓰기, 복잡한 분석, 다단계 추론</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> openai_api.complete(user_query)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>복잡도 분류 모델의 학습 데이터 예시:</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 학습 데이터셋 구축</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>training_data <span class="op">=</span> [</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simple (단순) - 정형화된 질문</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"운영시간이 언제인가요?"</span>, <span class="st">"simple"</span>),</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"배송비는 얼마인가요?"</span>, <span class="st">"simple"</span>), </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"환불 정책을 알려주세요"</span>, <span class="st">"simple"</span>),</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"계정을 삭제하고 싶어요"</span>, <span class="st">"simple"</span>),</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Medium (중간) - 변환/생성이 필요한 질문</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"이 제품의 장단점을 요약해주세요"</span>, <span class="st">"medium"</span>),</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"고객 불만사항을 정중한 답변으로 작성해주세요"</span>, <span class="st">"medium"</span>),</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"이 기사를 3문장으로 요약해주세요"</span>, <span class="st">"medium"</span>),</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"영어를 한국어로 번역해주세요"</span>, <span class="st">"medium"</span>),</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Complex (복잡) - 창의적/분석적 사고 필요</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"우리 회사에 맞는 마케팅 전략을 제안해주세요"</span>, <span class="st">"complex"</span>),</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"이 데이터를 분석해서 인사이트를 도출해주세요"</span>, <span class="st">"complex"</span>),</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"소설의 첫 장을 써주세요"</span>, <span class="st">"complex"</span>),</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"복잡한 법률 문제를 분석해주세요"</span>, <span class="st">"complex"</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 분류기 성능 최적화 포인트</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>accuracy_requirements <span class="op">=</span> {</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'simple_recall'</span>: <span class="fl">0.95</span>,  <span class="co"># 단순 질문을 놓치면 안됨 (비용 증가)</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'complex_precision'</span>: <span class="fl">0.90</span>,  <span class="co"># 복잡한 질문을 잘못 분류하면 품질 저하</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'medium_balance'</span>: <span class="fl">0.85</span>   <span class="co"># 중간 복잡도는 어느쪽으로 가도 큰 문제없음</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>라우터 모델의 실제 운영 고려사항:</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ProductionComplexityClassifier:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> DistilBERT()  <span class="co"># 20MB, 추론 10-15ms</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fallback_rules <span class="op">=</span> <span class="va">self</span>.load_rule_based_backup()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.confidence_threshold <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_with_confidence(<span class="va">self</span>, query):</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        prediction, confidence <span class="op">=</span> <span class="va">self</span>.model.predict_proba(query)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 확신이 낮으면 안전한 쪽(더 큰 모델)으로</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> confidence <span class="op">&lt;</span> <span class="va">self</span>.confidence_threshold:</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="st">"medium"</span>  <span class="co"># 애매하면 중간 모델로</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 키워드 기반 안전장치</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.check_safety_keywords(query):</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="st">"complex"</span>  <span class="co"># 민감한 키워드는 무조건 큰 모델</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> prediction</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> check_safety_keywords(<span class="va">self</span>, query):</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 법률, 의료, 금융 등 민감한 영역은 큰 모델로</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        sensitive_keywords <span class="op">=</span> [<span class="st">'법률'</span>, <span class="st">'의료'</span>, <span class="st">'투자'</span>, <span class="st">'세금'</span>, <span class="st">'계약'</span>]</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">return</span> <span class="bu">any</span>(keyword <span class="kw">in</span> query <span class="cf">for</span> keyword <span class="kw">in</span> sensitive_keywords)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>라우터 시스템의 비용 효율성 분석:</strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 월 100,000건 처리 시 비용 비교</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>routing_costs <span class="op">=</span> {</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 라우터 없이 모든 요청을 GPT-4로 처리</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'all_gpt4'</span>: {</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">'api_cost'</span>: <span class="dv">100000</span> <span class="op">*</span> <span class="fl">0.03</span>,  <span class="co"># $3,000</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">'router_cost'</span>: <span class="dv">0</span>,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">'total'</span>: <span class="dv">3000</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 라우터 사용 (70% simple, 20% medium, 10% complex)</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'with_router'</span>: {</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">'simple_cost'</span>: <span class="dv">70000</span> <span class="op">*</span> <span class="fl">0.001</span>,   <span class="co"># BERT 자체 호스팅: $70</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">'medium_cost'</span>: <span class="dv">20000</span> <span class="op">*</span> <span class="fl">0.01</span>,    <span class="co"># T5 자체 호스팅: $200  </span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">'complex_cost'</span>: <span class="dv">10000</span> <span class="op">*</span> <span class="fl">0.03</span>,   <span class="co"># GPT-4 API: $300</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">'router_cost'</span>: <span class="dv">100000</span> <span class="op">*</span> <span class="fl">0.0005</span>, <span class="co"># DistilBERT: $50</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">'total'</span>: <span class="dv">620</span>  <span class="co"># 80% 비용 절약!</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 성능 vs 비용 트레이드오프</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>performance_metrics <span class="op">=</span> {</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">'accuracy'</span>: {</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">'all_gpt4'</span>: <span class="fl">0.95</span>,        <span class="co"># 모든 요청 고품질</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">'with_router'</span>: <span class="fl">0.92</span>      <span class="co"># 라우팅 오류로 약간 감소</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'latency'</span>: {</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">'all_gpt4'</span>: <span class="st">'800ms'</span>,     <span class="co"># 모든 요청이 느림</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        <span class="st">'with_router'</span>: <span class="st">'200ms'</span>   <span class="co"># 70%는 빠른 응답</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">'cost_per_query'</span>: {</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>        <span class="st">'all_gpt4'</span>: <span class="st">'$0.03'</span>,</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>        <span class="st">'with_router'</span>: <span class="st">'$0.006'</span>  <span class="co"># 5배 저렴</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="api를-활용한-데이터-생성-전략" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="api를-활용한-데이터-생성-전략"><span class="header-section-number">3.3.2</span> API를 활용한 데이터 생성 전략</h3>
<p><strong>ChatGPT API로 학습 데이터 생성하기</strong>:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 고품질 데이터 생성 (일회성 비용)</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_training_data():</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    prompts <span class="op">=</span> [</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"고객 불만사항 100개를 생성해줘. 다양한 톤과 상황을 포함해서."</span>,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"제품 문의사항 50개를 생성해줘. 기술적 질문과 일반적 질문을 섞어서."</span>,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    training_data <span class="op">=</span> []</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> prompt <span class="kw">in</span> prompts:</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> openai.Completion.create(</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span><span class="st">"gpt-4"</span>,</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>            prompt<span class="op">=</span>prompt,</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>            max_tokens<span class="op">=</span><span class="dv">2000</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        training_data.append(response.choices[<span class="dv">0</span>].text)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> training_data</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 생성된 데이터로 작은 모델 학습</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_custom_model(training_data):</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># BERT나 DistilBERT를 fine-tuning</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AutoModel.from_pretrained(<span class="st">"bert-base-multilingual-cased"</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... 학습 코드</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>이 방식의 장점</strong>: - 초기에만 API 비용 발생 (월 $500-1000) - 이후 자체 모델로 무제한 사용 가능 - 도메인 특화된 고품질 데이터 확보 - 완전한 통제권과 커스터마이징 가능</p>
</section>
<section id="단계별-모델-도입-전략" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="단계별-모델-도입-전략"><span class="header-section-number">3.3.3</span> 단계별 모델 도입 전략</h3>
<p>실무에서는 처음부터 완벽한 시스템을 구축하려 하지 말고, 단계적으로 접근하는 것이 현명하다.</p>
<p><strong>Phase 1: MVP 단계 (Minimum Viable Produc, 최소 기능 제품)</strong></p>
<pre><code>목표: 빠른 검증과 프로토타입, 핵심 기능만 구현
모델: OpenAI API 또는 Hugging Face 사전 학습 모델
특징: 완벽함보다는 속도, 시장 반응 테스트가 우선
전략: 일단 돌아가는 것을 만든 후 사용자 피드백 수집
기간: 1-2개월
예산: $1,000-5,000

MVP 예시:
- 고객 문의 자동 분류: OpenAI API로 빠른 프로토타입
- 기본 챗봇: Hugging Face ChatBot 모델 + 간단한 인터페이스
- 문서 요약 도구: GPT-3.5 API + 웹 인터페이스</code></pre>
<p><strong>Phase 2: 확장 단계 (Product-Market Fit 달성 후)</strong></p>
<pre><code>목표: 비용 최적화와 성능 개선
모델: 자체 학습한 BERT/T5 + API 하이브리드
특징: 사용량 증가에 따른 비용 압박, 성능 요구사항 명확화
전략: 대부분은 자체 모델, 복잡한 것만 API 활용
기간: 3-6개월  
예산: $5,000-20,000

확장 단계 예시:
- 80% 질문은 자체 BERT 모델로 처리
- 20% 복잡한 질문만 GPT-4 API 사용
- 도메인 특화 데이터로 모델 fine-tuning</code></pre>
<p><strong>Phase 3: 성숙 단계 (스케일업)</strong></p>
<pre><code>목표: 완전한 커스터마이징과 독립성
모델: 도메인 특화 대규모 모델 + 자체 인프라
특징: 대규모 사용자, 차별화된 서비스 필요
전략: 핵심 기술의 내재화, 경쟁 우위 확보
기간: 6개월+
예산: $50,000+

성숙 단계 예시:
- 자체 개발한 도메인 특화 LLM
- 전용 GPU 클러스터 운영
- A/B 테스트 기반 지속적 모델 개선</code></pre>
</section>
</section>
<section id="의사결정-프레임워크" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="의사결정-프레임워크"><span class="header-section-number">3.4</span> 의사결정 프레임워크</h2>
<section id="모델-선택-체크리스트" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="모델-선택-체크리스트"><span class="header-section-number">3.4.1</span> 모델 선택 체크리스트</h3>
<p><strong>1. 기술적 요구사항</strong> - [ ] 정확도 임계값: 90%? 95%? 99%? - [ ] 응답 속도: 실시간(&lt;100ms)? 준실시간(&lt;1s)? 배치? - [ ] 동시 사용자 수: 10명? 1,000명? 10,000명? - [ ] 데이터 민감도: 공개 가능? 제한적? 극비?</p>
<p><strong>2. 비즈니스 제약사항</strong> - [ ] 월 예산: $100? $1,000? $10,000? - [ ] 개발 기간: 1주? 1개월? 6개월? - [ ] 팀 역량: 연구원 있음? 엔지니어만? 외주? - [ ] 인프라: 클라우드? 온프레미스? 하이브리드?</p>
<p><strong>3. 전략적 고려사항</strong> - [ ] 확장성: 현재만? 향후 10배 성장? - [ ] 차별화: 경쟁사 대비 핵심 요소? - [ ] 종속성: 외부 의존 허용? 독립성 필요? - [ ] 유지보수: 지속적 개선? 일회성 구축?</p>
</section>
<section id="실무진을-위한-가이드라인" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="실무진을-위한-가이드라인"><span class="header-section-number">3.4.2</span> 실무진을 위한 가이드라인</h3>
<p><strong>“이럴 때는 이 모델을”</strong></p>
<pre><code>📱 모바일 앱의 실시간 챗봇 (단순)
→ DistilBERT + 사전 정의된 답변 세트
이유: 빠른 응답, 낮은 배터리 소모, 오프라인 가능

🏪 이커머스 상품 검색 (단순)
→ BERT + Elasticsearch
이유: 정확한 의미 검색, 확장성, 비용 효율성

📋 고객 지원 티켓 분류 (단순→복잡)
→ BERT → 복잡한 경우만 GPT-4 API
이유: 대부분은 단순 분류, 어려운 케이스만 고급 모델

📰 뉴스 요약 서비스 (중간)
→ T5 또는 BART
이유: 정형화된 요약 패턴, 일관된 품질, 중간 수준의 이해력

📧 이메일 자동 응답 (중간)
→ T5 + 템플릿 시스템
이유: 기본 구조는 정해져 있고, 내용만 상황에 맞게 생성

🌐 다국어 번역 서비스 (중간)
→ T5 multilingual 또는 mBART
이유: 언어 쌍별 모델보다 효율적, 준수한 품질

✍️ 마케팅 콘텐츠 생성 (복잡)
→ GPT-4 API
이유: 창의성과 품질이 ROI에 직결

🏥 의료 텍스트 분석 (단순)
→ 도메인 특화 BERT (BioBERT)
이유: 전문성, 규제 준수, 설명 가능성

💰 금융 리스크 분석 (단순)
→ 자체 학습 모델 (데이터 보안)
이유: 규제, 보안, 실시간 대량 처리</code></pre>
<p>실무에서는 <strong>기술적 완벽함보다 비즈니스 목표 달성</strong>이 우선이다. 최고의 모델이 아니라 <strong>현재 상황에서 최적인 모델</strong>을 선택하는 것이 성공의 열쇠다.</p>
</section>
</section>
</section>
<section id="결론" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> 결론</h1>
<ul>
<li>PLM 기술의 발전은 놀랍지만, 실무에서는 <strong>기술적 우수성과 현실적 제약 사이의 균형</strong>을 맞추는 것이 핵심이다.</li>
<li>단순한 분류 문제에 수십억 파라미터의 모델을 사용하는 것은 비효율적이다. 반대로 복잡한 추론이 필요한 곳에 작은 모델만 고집하는 것도 기회 손실이다.</li>
<li>중요한 것은 <strong>문제의 본질을 이해하고, 제약 조건을 명확히 하며, 단계적으로 접근하는 것</strong>이다. 오늘은 BERT로 시작해서 내일은 T5로, 필요하다면 GPT-4로 발전시켜 나가는 것이 현실적인 전략이다.</li>
<li>결국 가장 좋은 모델은 <strong>가장 비싼 모델이 아니라, 주어진 상황에서 목표를 가장 효과적으로 달성하는 모델</strong>이다. 기술은 수단이지 목적이 아니라는 점을 항상 기억해</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>