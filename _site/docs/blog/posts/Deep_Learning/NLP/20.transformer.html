<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.543">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kwangmin Kim">
<meta name="dcterms.date" content="2025-01-20">
<meta name="description" content="정적 임베딩의 한계를 넘어, 단어의 문맥적 의미를 동적으로 포착하는 ELMo, BERT, GPT, SBERT와 같은 주요 문맥 기반 임베딩 모델들의 원리, 특징, 혁신적인 기여를 살펴본다.">

<title>Kwangmin Kim - 텍스트 벡터화: 신경망 기반 방법론</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete-preset-algolia.umd.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "algolia": {
    "application-id": "DUOR1DRC9D",
    "search-only-api-key": "f264da5dea684ffb9e9b4a574af3ed61",
    "index-name": "prod_QUARTO",
    "analytics-events": true,
    "show-logo": true,
    "libDir": "site_libs"
  },
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": true,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js"></script>


<script type="text/javascript">
var ALGOLIA_INSIGHTS_SRC = "https://cdn.jsdelivr.net/npm/search-insights/dist/search-insights.iife.min.js";
!function(e,a,t,n,s,i,c){e.AlgoliaAnalyticsObject=s,e[s]=e[s]||function(){
(e[s].queue=e[s].queue||[]).push(arguments)},i=a.createElement(t),c=a.getElementsByTagName(t)[0],
i.async=1,i.src=n,c.parentNode.insertBefore(i,c)
}(window,document,"script",ALGOLIA_INSIGHTS_SRC,"aa");
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/autocomplete-plugin-algolia-insights">

</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6W0EKFMWBN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6W0EKFMWBN', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../../styles.css">
<meta property="og:title" content="Kwangmin Kim - 텍스트 벡터화: 신경망 기반 방법론">
<meta property="og:description" content="정적 임베딩의 한계를 넘어, 단어의 문맥적 의미를 동적으로 포착하는 ELMo, BERT, GPT, SBERT와 같은 주요 문맥 기반 임베딩 모델들의 원리, 특징, 혁신적인 기여를 살펴본다.">
<meta property="og:site_name" content="Kwangmin Kim">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html"> 
<span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"> <i class="bi bi-github" role="img" aria-label="Github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"> <i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#요약" id="toc-요약" class="nav-link active" data-scroll-target="#요약"><span class="header-section-number">1</span> 요약</a></li>
  <li><a href="#텍스트-인코딩-및-벡터화" id="toc-텍스트-인코딩-및-벡터화" class="nav-link" data-scroll-target="#텍스트-인코딩-및-벡터화"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</a>
  <ul class="collapse">
  <li><a href="#문맥을-고려한-벡터화-2018-현재-동적-임베딩" id="toc-문맥을-고려한-벡터화-2018-현재-동적-임베딩" class="nav-link" data-scroll-target="#문맥을-고려한-벡터화-2018-현재-동적-임베딩"><span class="header-section-number">2.1</span> 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩</a>
  <ul class="collapse">
  <li><a href="#실용적-응용-및-평가" id="toc-실용적-응용-및-평가" class="nav-link" data-scroll-target="#실용적-응용-및-평가"><span class="header-section-number">2.1.1</span> 실용적 응용 및 평가</a></li>
  </ul></li>
  <li><a href="#결론" id="toc-결론" class="nav-link" data-scroll-target="#결론"><span class="header-section-number">2.2</span> 결론</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">텍스트 벡터화: 신경망 기반 방법론</h1>
<p class="subtitle lead">Word2Vec, GloVe, FastText부터 ELMo, BERT, SBERT까지 문맥을 이해하는 벡터 표현 소개</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>정적 임베딩의 한계를 넘어, 단어의 문맥적 의미를 동적으로 포착하는 ELMo, BERT, GPT, SBERT와 같은 주요 문맥 기반 임베딩 모델들의 원리, 특징, 혁신적인 기여를 살펴본다.</p>
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kwangmin Kim </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 20, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 단어의 의미가 문맥에 따라 변하는 현상을 효과적으로 다루기 위해 등장한 <strong>동적/문맥적 임베딩(Contextualized Embedding)</strong> 방법론을 탐구한다. 정적 임베딩의 한계를 지적하고, 이를 극복하기 위한 주요 모델들의 핵심 아이디어와 특징을 소개한다.</p>
<p>주요 내용은 다음과 같다.</p>
<ul>
<li><strong>정적 임베딩 vs.&nbsp;동적 임베딩</strong>:
<ul>
<li>정적 임베딩(예: Word2Vec, GloVe)은 단어마다 고정된 벡터를 할당하여 문맥에 따른 의미 변화(다의성)를 포착하지 못하는 한계가 있다.</li>
<li>동적 임베딩은 동일한 단어라도 문맥에 따라 다른 벡터 표현을 생성하여 이러한 문제를 해결한다.</li>
</ul></li>
<li><strong>주요 문맥 기반 임베딩 모델</strong>:
<ul>
<li><strong>ELMo (Embeddings from Language Models)</strong>: 양방향 LSTM(BiLSTM)의 각 계층에서 얻은 내부 상태들을 가중합하여 문맥 정보를 풍부하게 담은 임베딩을 생성한다. 문자 단위 표현부터 시작하여 다양한 수준의 정보를 결합한다.</li>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: 트랜스포머(Transformer)의 인코더 구조를 활용하여 문장 내 모든 단어의 양방향 문맥을 동시에 고려한다. ’Masked Language Model(MLM)’과 ’Next Sentence Prediction(NSP)’이라는 두 가지 혁신적인 사전 학습(pre-training) 목표를 통해 깊은 언어 이해 능력을 학습한다. 문서 전체의 표현으로는 <code>[CLS]</code> 토큰의 출력을 사용하거나 토큰 출력들의 풀링(pooling) 결과를 활용한다.</li>
<li><strong>SBERT (Sentence-BERT)</strong>: BERT의 출력을 문장 수준의 의미론적 벡터로 효율적으로 변환하기 위해 Siamese 또는 Triplet 네트워크 구조를 사용한다. 이를 통해 문장 간 유사도 계산 및 대규모 검색 작업의 효율성을 크게 향상시킨다.</li>
<li><strong>GPT (Generative Pre-trained Transformer)</strong>: 트랜스포머의 디코더 구조를 기반으로 하는 단방향(autoregressive) 언어 모델이다. 이전 단어들을 바탕으로 다음 단어를 예측하도록 학습하며, 이 과정에서 문맥을 이해하고 생성하는 능력을 키운다. 특히, 가중치 업데이트 없이 프롬프트에 몇 가지 예시(few-shot)를 제공하는 것만으로 새로운 작업을 수행하는 ‘In-context Learning’ 능력으로 주목받았다. 문서 표현으로는 첫 번째 토큰([BOS])의 출력을 활용하기도 한다.</li>
</ul></li>
<li><strong>실용적 응용 및 평가</strong>:
<ul>
<li>이러한 모델들은 문서 분류, 정보 검색, 질의응답, 기계 번역 등 다양한 NLP 태스크에서 혁신적인 성능 향상을 가져왔다.</li>
<li>모델 평가는 단어 유사도나 관계 유추 같은 내재적 평가(intrinsic evaluation)와 실제 다운스트림 태스크에서의 성능을 측정하는 외재적 평가(extrinsic evaluation)로 이루어진다.</li>
</ul></li>
</ul>
<p>이 문서를 통해 독자는 문맥을 이해하는 동적 임베딩 기술의 발전 과정과 핵심 원리를 파악하고, 다양한 NLP 문제 해결에 이를 어떻게 활용할 수 있는지에 대한 통찰을 얻을 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
    │   ├── GRU
    │   └── ELMo
    └── Attention 메커니즘
        ├── Basic Attention
        ├── Self-Attention
        └── Multi-Head Attention
 
Transformer 이후 생성형 모델 발전 계열
├── Transformer 구조 (Vaswani et al., 2017)
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등</code></pre>
<section id="문맥을-고려한-벡터화-2018-현재-동적-임베딩" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="문맥을-고려한-벡터화-2018-현재-동적-임베딩"><span class="header-section-number">2.1</span> 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩</h2>
<section id="elmo-embedding-from-language-models-2018" class="level4" data-number="2.1.0.1">
<h4 data-number="2.1.0.1" class="anchored" data-anchor-id="elmo-embedding-from-language-models-2018"><span class="header-section-number">2.1.0.1</span> ELMo (Embedding from Language Models, 2018)</h4>
<ul>
<li>ELMo 수식: <span class="math inline">\(\text{ELMo}_k^{task} = \gamma^{task} \sum_{j=0}^L s_j^{task} \mathbf{h}_{k,j}^{LM}\)</span>
<ul>
<li><strong><span class="math inline">\(\mathbf{h}_{k,j}^{LM}\)</span>: 각 레이어의 hidden state</strong></li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 예시: 3층 BiLSTM에서 "bank" 단어 (k번째 위치)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>h_{bank,<span class="dv">0</span>} <span class="op">=</span> character_embedding(<span class="st">"bank"</span>)     <span class="co"># 레이어 0 (입력)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>h_{bank,<span class="dv">1</span>} <span class="op">=</span> first_LSTM_layer_output        <span class="co"># 레이어 1  </span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>h_{bank,<span class="dv">2</span>} <span class="op">=</span> second_LSTM_layer_output       <span class="co"># 레이어 2</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>h_{bank,<span class="dv">3</span>} <span class="op">=</span> third_LSTM_layer_output        <span class="co"># 레이어 3 (최상위)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><span class="math inline">\(s_j^{task}\)</span>: 학습 가능한 가중치</strong>
<ul>
<li>각 레이어의 중요도를 태스크별로 학습</li>
<li>문법적 태스크 → 낮은 레이어 중시</li>
<li>의미적 태스크 → 높은 레이어 중시</li>
</ul></li>
<li><strong><span class="math inline">\(\gamma^{task}\)</span>: 전체 스케일 조정</strong>
<ul>
<li>ELMo 벡터의 전체적인 크기 조정</li>
</ul></li>
</ul></li>
<li>계산 예시</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># "bank" 단어의 ELMo 벡터 (감정 분석 태스크)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>h_0 <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>]  <span class="co"># 문자 레벨</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>h_1 <span class="op">=</span> [<span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>]  <span class="co"># 낮은 레벨 (문법적)  </span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>h_2 <span class="op">=</span> [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>]  <span class="co"># 높은 레벨 (의미적)</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>s_0 <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># 문자 레벨 가중치 (낮음)</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>s_1 <span class="op">=</span> <span class="fl">0.3</span>  <span class="co"># 문법 레벨 가중치  </span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>s_2 <span class="op">=</span> <span class="fl">0.6</span>  <span class="co"># 의미 레벨 가중치 (높음)</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>ELMo_bank <span class="op">=</span> γ × (s_0×h_0 <span class="op">+</span> s_1×h_1 <span class="op">+</span> s_2×h_2)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>          <span class="op">=</span> <span class="fl">2.0</span> × (<span class="fl">0.1</span>×[<span class="fl">0.1</span>,<span class="fl">0.2</span>,<span class="fl">0.3</span>] <span class="op">+</span> <span class="fl">0.3</span>×[<span class="fl">0.4</span>,<span class="fl">0.5</span>,<span class="fl">0.6</span>] <span class="op">+</span> <span class="fl">0.6</span>×[<span class="fl">0.7</span>,<span class="fl">0.8</span>,<span class="fl">0.9</span>])</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>          <span class="op">=</span> <span class="fl">2.0</span> × [<span class="fl">0.55</span>, <span class="fl">0.65</span>, <span class="fl">0.75</span>]</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>          <span class="op">=</span> [<span class="fl">1.1</span>, <span class="fl">1.3</span>, <span class="fl">1.5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>양방향 정보의 중요성
<ul>
<li><strong>Forward만 사용할 경우:</strong></li>
</ul>
<pre><code>"The bank was closed because of ___"
→ "bank"를 이해할 때 "The"만 참고</code></pre>
<ul>
<li><strong>Backward까지 사용할 경우:</strong></li>
</ul>
<pre><code>"The bank was closed because of ___"
→ "bank"를 이해할 때 "was closed" 정보도 참고
→ 금융 기관으로 해석 가능성 증가</code></pre></li>
</ul>
</section>
<section id="bert-bidirectional-encoder-representations-from-transformers-2018" class="level4" data-number="2.1.0.2">
<h4 data-number="2.1.0.2" class="anchored" data-anchor-id="bert-bidirectional-encoder-representations-from-transformers-2018"><span class="header-section-number">2.1.0.2</span> BERT (Bidirectional Encoder Representations from Transformers, 2018)</h4>
<ul>
<li>양방향 문맥 동시 고려
<ul>
<li>15% 단어를 마스킹하여 예측</li>
<li>문장 간 관계 학습</li>
</ul></li>
<li><strong>핵심 혁신:</strong>
<ul>
<li><strong>Transformer 기반</strong>: 양방향 문맥 동시 고려
<ul>
<li><p><strong>기존 RNN의 한계:</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># RNN은 순차적 처리 (병렬화 어려움)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>h_1 <span class="op">=</span> RNN(x_1)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>h_2 <span class="op">=</span> RNN(x_2, h_1)      <span class="co"># h_1이 완료되어야 시작 가능</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>h_3 <span class="op">=</span> RNN(x_3, h_2)      <span class="co"># h_2가 완료되어야 시작 가능</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Transformer의 장점:</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 모든 위치를 동시에 처리 (병렬화 가능)</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> compute_attention(all_words)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>all_representations <span class="op">=</span> apply_attention(all_words, attention_weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><strong>Masked Language Model</strong>: 15% 단어를 마스킹하여 예측
<ul>
<li>BERT의 핵심 학습 방법
<ul>
<li><strong>기본 아이디어</strong>: 일부 단어를 숨기고 맞추게 하기</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 원본 문장</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">"나는 [MASK]를 좋아한다"</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델이 학습하는 것</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>P(<span class="st">"사과"</span> <span class="op">|</span> <span class="st">"나는 [MASK]를 좋아한다"</span>) <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>P(<span class="st">"바나나"</span> <span class="op">|</span> <span class="st">"나는 [MASK]를 좋아한다"</span>) <span class="op">=</span> <span class="fl">0.2</span>  </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>P(<span class="st">"컴퓨터"</span> <span class="op">|</span> <span class="st">"나는 [MASK]를 좋아한다"</span>) <span class="op">=</span> <span class="fl">0.01</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>15% 마스킹 전략:</strong></li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>입력 문장의 <span class="dv">15</span><span class="op">%</span> 단어에 대해:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">80</span><span class="op">%</span>: [MASK] 토큰으로 교체</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">10</span><span class="op">%</span>: 랜덤한 다른 단어로 교체  </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">10</span><span class="op">%</span>: 원래 단어 그대로 유지</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p><strong>왜 이렇게 하는가?</strong> ```python # 80% [MASK]: 메인 학습 목적 “나는 [MASK]를 좋아한다”</p>
<p># 10% 랜덤 교체: 노이즈에 강한 표현 학습 “나는 컴퓨터를 좋아한다” # 원래는 “사과”</p>
<p># 10% 원본 유지: 실제 사용 시와 동일한 조건 “나는 사과를 좋아한다” ```</p></li>
</ul></li>
</ul></li>
<li><strong>Next Sentence Prediction</strong>: 문장 간 관계 학습</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 실제 연속된 문장 (Positive)</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>문장A: <span class="st">"나는 아침에 일어났다"</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>문장B: <span class="st">"그리고 아침 식사를 했다"</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>Label: IsNext <span class="op">=</span> <span class="va">True</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 랜덤하게 조합된 문장 (Negative)  </span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>문장A: <span class="st">"나는 아침에 일어났다"</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>문장B: <span class="st">"축구는 재미있는 스포츠다"</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>Label: IsNext <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><strong>BERT의 문서 벡터화 방법:</strong>
<ul>
<li><strong>[CLS] 토큰</strong>: 문장/문서 전체 표현</li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>입력: <span class="st">"[CLS] 문장 내용 [SEP]"</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>출력: [CLS]_벡터가 전체 문장의 의미를 담음</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 예시</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>input_tokens <span class="op">=</span> [<span class="st">"[CLS]"</span>, <span class="st">"나는"</span>, <span class="st">"사과를"</span>, <span class="st">"좋아한다"</span>, <span class="st">"[SEP]"</span>]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>bert_output <span class="op">=</span> bert_model(input_tokens)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>sentence_vector <span class="op">=</span> bert_output[<span class="dv">0</span>]  <span class="co"># [CLS] 위치의 벡터</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Pooling 전략</strong>:
<ul>
<li>Mean pooling: <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n \mathbf{h}_i\)</span></li>
<li>Max pooling: <span class="math inline">\(\max(\mathbf{h}_1, ..., \mathbf{h}_n)\)</span></li>
</ul>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 모든 토큰의 BERT 출력</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>token_representations <span class="op">=</span> [</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>],  <span class="co"># [CLS]</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>],  <span class="co"># "나는"  </span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>],  <span class="co"># "사과를"</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>],  <span class="co"># "좋아한다"</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>],  <span class="co"># [SEP]</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean Pooling</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>mean_vector <span class="op">=</span> mean(token_representations[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>])  <span class="co"># [CLS], [SEP] 제외</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="op">=</span> (<span class="fl">0.4</span><span class="op">+</span><span class="fl">0.7</span><span class="op">+</span><span class="fl">0.2</span>)<span class="op">/</span><span class="dv">3</span>, (<span class="fl">0.5</span><span class="op">+</span><span class="fl">0.8</span><span class="op">+</span><span class="fl">0.3</span>)<span class="op">/</span><span class="dv">3</span>, (<span class="fl">0.6</span><span class="op">+</span><span class="fl">0.9</span><span class="op">+</span><span class="fl">0.4</span>)<span class="op">/</span><span class="dv">3</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="op">=</span> [<span class="fl">0.43</span>, <span class="fl">0.53</span>, <span class="fl">0.63</span>]</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Max Pooling  </span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>max_vector <span class="op">=</span> <span class="bu">max</span>(token_representations[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>])  <span class="co"># 각 차원별 최댓값</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="op">=</span> [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><strong>특징</strong>:
<ul>
<li>단어의 의미적, 문법적 정보를 벡터 공간에 학습.</li>
<li>벡터 간 연산을 통해 단어 간 유사도, 유추 등 관계 표현 가능 (예: “king” - “man” + “woman” ≈ “queen”).</li>
</ul></li>
<li><strong>중요성</strong>: 현대 NLP 딥러닝 모델의 핵심 구성 요소로, 성능 향상에 크게 기여.</li>
</ul>
</section>
<section id="sbert-sentence-bert" class="level4" data-number="2.1.0.3">
<h4 data-number="2.1.0.3" class="anchored" data-anchor-id="sbert-sentence-bert"><span class="header-section-number">2.1.0.3</span> SBERT (Sentence-BERT)</h4>
<ul>
<li><p>최근 가장 보편적인 문장 또는 문서 임베딩 방법으로 SBERT가 이용된다.</p></li>
<li><p>문서의 유사도를 구할 때는 SBERT 사용을 권장</p></li>
<li><p>문장 벡터화 전략</p>
<ul>
<li>문장 간 유사도 계산</li>
<li>문장 간 유사도 계산 시 문장 임베딩 사용</li>
</ul></li>
<li><p>기존 BERT의 한계: 문장 유사도 계산의 비효율성</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1000개 문장의 유사도를 모두 구하려면</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"문장1"</span>, <span class="st">"문장2"</span>, ..., <span class="st">"문장1000"</span>]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 기존 BERT 방식 (비효율적)</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>   <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i<span class="op">+</span><span class="dv">1</span>, <span class="dv">1000</span>):</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>      combined <span class="op">=</span> <span class="ss">f"[CLS] </span><span class="sc">{</span>sentences[i]<span class="sc">}</span><span class="ss"> [SEP] </span><span class="sc">{</span>sentences[j]<span class="sc">}</span><span class="ss"> [SEP]"</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>      similarity <span class="op">=</span> bert_classifier(combined)  <span class="co"># 매번 BERT 실행</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 총 계산 횟수: 1000 × 999 / 2 = 499,500번!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>BERT로 문장 유사도를 계산하려면:
<ul>
<li>두 문장을 [SEP]로 연결</li>
<li>BERT에 입력하여 분류</li>
<li><span class="math inline">\(O(n^2)\)</span> 시간 복잡도 (n개 문장 비교 시)</li>
</ul></li>
</ul></li>
<li><p>SBERT의 해결책: Siamese Network 구조</p></li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># SBERT 방식 (효율적)</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>문장 A → BERT → Pooling → Vector A</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>문장 B → BERT → Pooling → Vector B</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>유사도 <span class="op">=</span> cosine_similarity(Vector A, Vector B)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 1단계: 모든 문장을 미리 벡터화</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>sentence_vectors <span class="op">=</span> []</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sentence <span class="kw">in</span> sentences:</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    vector <span class="op">=</span> sbert_model(sentence)  <span class="co"># 각 문장마다 1번씩만 실행</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    sentence_vectors.append(vector)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 2단계: 벡터 간 코사인 유사도로 빠른 계산</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i<span class="op">+</span><span class="dv">1</span>, <span class="dv">1000</span>):</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> cosine_similarity(sentence_vectors[i], sentence_vectors[j])</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 총 SBERT 실행 횟수: 1000번 (대폭 감소!)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>학습 목적 함수:</strong>
<ul>
<li><strong>Classification</strong>: <span class="math inline">\(\mathcal{L} = -\sum_{i} y_i \log(\text{softmax}(W[\mathbf{u}; \mathbf{v}; |\mathbf{u}-\mathbf{v}|]))\)</span></li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 두 문장의 SBERT 벡터</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> sbert(<span class="st">"나는 사과를 좋아한다"</span>)      <span class="co"># [0.2, 0.4, 0.1, ...]</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> sbert(<span class="st">"나는 바나나를 좋아한다"</span>)    <span class="co"># [0.3, 0.5, 0.2, ...]</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 특성 벡터 구성</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>concat <span class="op">=</span> [u<span class="op">;</span> v]                    <span class="co"># 연결: [0.2, 0.4, 0.1, 0.3, 0.5, 0.2, ...]</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>abs_diff <span class="op">=</span> <span class="op">|</span>u <span class="op">-</span> v<span class="op">|</span>                <span class="co"># 절댓값 차이: [0.1, 0.1, 0.1, ...]</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [u<span class="op">;</span> v<span class="op">;</span> abs_diff]        <span class="co"># 최종 특성 벡터</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 분류 (유사/비유사)</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> W <span class="op">@</span> features <span class="op">+</span> b</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>probability <span class="op">=</span> softmax(logits)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> cross_entropy(probability, true_label)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Regression</strong>: <span class="math inline">\(\mathcal{L} = \text{MSE}(\text{cosine\_sim}(\mathbf{u}, \mathbf{v}), \text{label})\)</span></li>
</ul>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 예측 유사도</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>predicted_sim <span class="op">=</span> cosine_similarity(u, v) <span class="op">=</span> <span class="fl">0.85</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 실제 라벨 (0~1 점수)</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>true_sim <span class="op">=</span> <span class="fl">0.9</span>  <span class="co"># 사람이 평가한 유사도</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 손실 계산</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> (predicted_sim <span class="op">-</span> true_sim)² <span class="op">=</span> (<span class="fl">0.85</span> <span class="op">-</span> <span class="fl">0.9</span>)² <span class="op">=</span> <span class="fl">0.0025</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><strong>성능 개선:</strong>
<ul>
<li>시간 복잡도: <span class="math inline">\(O(n^2) \rightarrow O(n)\)</span></li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 시간 복잡도 비교</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>기존_BERT_시간 <span class="op">=</span> O(n²) <span class="op">=</span> <span class="dv">1000</span><span class="er">²</span> <span class="op">=</span> <span class="dv">1</span>,<span class="dv">000</span>,<span class="dv">000</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>SBERT_시간 <span class="op">=</span> O(n) <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>속도_향상 <span class="op">=</span> <span class="dv">1</span>,<span class="dv">000</span>,<span class="dv">000</span> <span class="op">/</span> <span class="dv">1000</span> <span class="op">=</span> <span class="dv">1000</span><span class="er">배</span><span class="op">!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>의미적 유사도 정확도 대폭 향상</li>
<li>대규모 문서 검색 시스템</li>
<li>실시간 문장 유사도 계산</li>
<li>추천 시스템에서의 텍스트 매칭</li>
</ul></li>
</ul>
</section>
<section id="gptgenerative-pre-trained-transformer" class="level4" data-number="2.1.0.4">
<h4 data-number="2.1.0.4" class="anchored" data-anchor-id="gptgenerative-pre-trained-transformer"><span class="header-section-number">2.1.0.4</span> GPT(Generative Pre-trained Transformer)</h4>
<ul>
<li><p>단방향 언어 모델의 핵심 개념</p></li>
<li><p>BERT vs GPT의 근본적 차이</p></li>
<li><p><strong>BERT (양방향)</strong>:</p>
<pre><code>입력: "나는 [MASK]를 좋아한다"
모델이 보는 정보: "나는" + "를 좋아한다" (양쪽 모두)
예측: [MASK] = "사과"</code></pre></li>
<li><p><strong>GPT (단방향)</strong>:</p>
<pre><code>입력: "나는 사과를"
모델이 보는 정보: "나는 사과를" (왼쪽만)
예측: 다음 단어 = "좋아한다"</code></pre></li>
<li><p>왜 단방향일까?</p>
<ul>
<li><p><strong>생성 태스크의 특성</strong>:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 실제 텍스트 생성 시</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co">"안녕하세요, 오늘 날씨가"</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>→ 모델: <span class="st">"좋네요"</span> (미래 정보는 알 수 없음)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 만약 양방향이라면?</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">"안녕하세요, 오늘 날씨가 [미래정보] 입니다"</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>→ 실제 생성 시에는 미래 정보가 없으므로 불일치</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><p><strong>GPT의 학습 방식: Autoregressive Language Modeling</strong></p>
<ul>
<li><p>이전 토큰들로 다음 토큰 예측</p></li>
<li><p>수학적 목적 함수: <span class="math inline">\(P(\text{문장}) = \prod_{t=1}^T P(w_t | w_1, w_2, ..., w_{t-1})\)</span></p>
<ul>
<li>문장의 확률 = 각 단어가 이전 단어들 조건 하에 나타날 확률의 곱</li>
</ul></li>
<li><p>구체적 학습 예시</p>
<ul>
<li><strong>훈련 문장</strong>: “나는 사과를 좋아한다”</li>
</ul>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 학습 데이터 구성</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>입력 → 정답</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co">"나는"</span> → <span class="st">"사과를"</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co">"나는 사과를"</span> → <span class="st">"좋아한다"</span>  </span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co">"나는 사과를 좋아한다"</span> → <span class="st">"&lt;끝&gt;"</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 손실 함수</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="op">-</span>log P(<span class="st">"사과를"</span> <span class="op">|</span> <span class="st">"나는"</span>) </span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>      <span class="op">-</span>log P(<span class="st">"좋아한다"</span> <span class="op">|</span> <span class="st">"나는 사과를"</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>      <span class="op">-</span>log P(<span class="st">"&lt;끝&gt;"</span> <span class="op">|</span> <span class="st">"나는 사과를 좋아한다"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Causal Masking (인과 마스킹)</p>
<ul>
<li>Attention에서 미래 정보 차단</li>
</ul>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Attention Matrix (4개 단어 예시)</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>        나는  사과를  좋아한다  <span class="op">&lt;</span>끝<span class="op">&gt;</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>나는     ✓     ✗      ✗      ✗</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>사과를    ✓     ✓      ✗      ✗  </span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>좋아한다  ✓     ✓      ✓      ✗</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>끝<span class="op">&gt;</span>     ✓     ✓      ✓      ✓</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># ✓: 참고 가능, ✗: 마스킹 (참고 불가)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>코드 구현</strong>:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 마스킹 행렬</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.tril(torch.ones(seq_len, seq_len))</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 상삼각 부분을 -무한대로 설정</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>attention_scores.masked_fill_(mask <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span><span class="fl">1e9</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> softmax(attention_scores)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><p><strong>첫 번째 토큰을 문서 표현으로 활용</strong></p>
<ul>
<li>정보 흐름의 특성
<ul>
<li>입력: “[BOS] 문장 내용들…”</li>
<li>각 토큰이 보는 정보량</li>
<li>토큰1 ([BOS]): 자기 자신만</li>
<li>토큰2: [BOS] + 토큰2<br>
</li>
<li>토큰3: [BOS] + 토큰2 + 토큰3</li>
</ul></li>
</ul></li>
<li><p><strong>왜 첫 번째 토큰인가?</strong></p>
<ul>
<li>정보 흐름의 특성
<ul>
<li>입력: “[BOS] 문장 내용들…”</li>
<li>각 토큰이 보는 정보량</li>
<li>토큰1 ([BOS]): 자기 자신만</li>
<li>토큰2: [BOS] + 토큰2<br>
</li>
<li>토큰3: [BOS] + 토큰2 + 토큰3</li>
<li>마지막토큰: [BOS] + 전체 문장</li>
<li>역설적으로, [BOS]는 전체 문장을 “예측”해야 하므로</li>
<li>전체 문장 정보를 압축한 표현을 학습하게 됨</li>
</ul></li>
<li>구체적 메커니즘
<ul>
<li>학습 과정에서의 압축</li>
</ul>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># GPT가 학습하는 것</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>P(전체_문장 <span class="op">|</span> [BOS]) <span class="op">=</span> P(w1<span class="op">|</span>[BOS]) × P(w2<span class="op">|</span>[BOS],w1) × ... × P(wn<span class="op">|</span>[BOS],w1,...,wn<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># [BOS] 토큰은 "이 문장이 어떤 내용일까?"를 예측해야 함</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># → 문장의 주제, 감정, 스타일 등을 함축하는 표현을 학습</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><p><strong>실제 활용 예시</strong>:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 문서 분류</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>document <span class="op">=</span> <span class="st">"[BOS] 이 영화는 정말 재미있었다. 스토리도 좋고..."</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>gpt_output <span class="op">=</span> gpt_model(document)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>document_vector <span class="op">=</span> gpt_output[<span class="dv">0</span>]  <span class="co"># [BOS] 위치의 벡터</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>classification <span class="op">=</span> classifier(document_vector)  <span class="co"># 긍정/부정 분류</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>In-context Learning 심화 분석</strong></p>
<ul>
<li>기존 학습 방식과의 차이
<ul>
<li><p><strong>전통적 학습 (Fine-tuning)</strong>:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1단계: 새로운 태스크 데이터로 모델 가중치 업데이트</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> task_data:</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>   loss <span class="op">=</span> compute_loss(model(batch.<span class="bu">input</span>), batch.target)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>   loss.backward()</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>   optimizer.step()</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2단계: 추론</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> model(new_input)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>In-context Learning</strong>:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 가중치 업데이트 없이, 입력에 예시를 포함</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>context <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="st">번역 예시:</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="st">영어: Hello → 한국어: 안녕하세요</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="st">영어: Thank you → 한국어: 감사합니다  </span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="st">영어: Good morning → 한국어: 좋은 아침</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="st">영어: How are you? → 한국어:</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> gpt_model(context)  <span class="co"># "어떻게 지내세요?" 출력</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
</ul></li>
<li><p><strong>왜 In-context Learning이 가능한가?</strong></p>
<ul>
<li><p>패턴 인식 능력</p>
<ul>
<li>GPT가 학습 중 본 패턴들</li>
<li>“A는 B이다. C는 D이다. E는” → F 예측</li>
<li>“1+1=2, 2+2=4, 3+3=” → 6 예측</li>
<li>“cat→고양이, dog→개, bird→” → 새 예측</li>
</ul></li>
<li><p>메타 학습 (Learning to Learn)</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 다양한 패턴을 학습하면서 "학습하는 방법"을 학습</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>패턴<span class="dv">1</span>: 번역 (A→B 형태)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>패턴<span class="dv">2</span>: 수학 (계산 규칙)  </span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>패턴<span class="dv">3</span>: 분류 (라벨링 규칙)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 새로운 패턴이 주어져도 빠르게 적응</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><p><strong>실제 In-context Learning 예시</strong></p>
<ul>
<li><p><strong>감정 분석 태스크</strong>:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="st">다음은 리뷰와 감정을 분류한 예시입니다:</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="st">리뷰: "이 영화 정말 재미있어요!" 감정: 긍정</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="st">리뷰: "시간 낭비였습니다." 감정: 부정</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="st">리뷰: "그냥 그래요." 감정: 중립</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="st">리뷰: "배우들 연기가 훌륭했습니다!" 감정:</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="co"># GPT 출력: "긍정"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>번역 태스크</strong>:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="st">English to Korean translation:</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="st">English: I love programming</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="st">Korean: 나는 프로그래밍을 좋아합니다</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="st">English: The weather is nice today  </span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="st">Korean: 오늘 날씨가 좋네요</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="st">English: What time is it now?</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="st">Korean:</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="co"># GPT 출력: "지금 몇 시인가요?"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><p><strong>GPT 발전사와 특징</strong></p>
<ul>
<li><strong>GPT-1 (2018)</strong>
<ul>
<li>크기: 117M 파라미터</li>
<li>특징: Transformer 디코더만 사용</li>
<li>성능: 간단한 텍스트 생성</li>
</ul></li>
<li><strong>GPT-2 (2019)</strong>
<ul>
<li>크기: 1.5B 파라미터</li>
<li>특징: 스케일 확장의 효과 입증</li>
<li>성능: 일관성 있는 긴 텍스트 생성</li>
</ul></li>
<li><strong>GPT-3 (2020)</strong>
<ul>
<li>크기: 175B 파라미터<br>
</li>
<li>특징: In-context Learning의 강력한 능력</li>
<li>성능: Few-shot Learning으로 다양한 태스크 수행</li>
</ul></li>
<li><strong>GPT-4 (2023)</strong>
<ul>
<li>크기: 공개되지 않음 (추정 수조 개)</li>
<li>특징: 멀티모달 (텍스트 + 이미지)</li>
<li>성능: 인간 수준에 근접한 성능</li>
</ul></li>
</ul></li>
<li><p><strong>GPT vs BERT 비교 정리</strong></p></li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>측면</th>
<th>GPT</th>
<th>BERT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>방향성</strong></td>
<td>단방향 (왼쪽→오른쪽)</td>
<td>양방향</td>
</tr>
<tr class="even">
<td><strong>학습 목표</strong></td>
<td>다음 토큰 예측</td>
<td>마스킹된 토큰 예측</td>
</tr>
<tr class="odd">
<td><strong>주요 용도</strong></td>
<td>생성 태스크</td>
<td>이해 태스크</td>
</tr>
<tr class="even">
<td><strong>문서 벡터</strong></td>
<td>첫 번째 토큰</td>
<td>[CLS] 토큰</td>
</tr>
<tr class="odd">
<td><strong>특별 능력</strong></td>
<td>In-context Learning</td>
<td>Fine-tuning 효율성</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>결론</strong>: GPT는 “다음에 올 단어를 예측”하는 단순한 목표로 학습하지만, 이 과정에서 언어의 패턴, 의미, 추론 능력까지 학습하게 되어 강력한 생성 및 추론 모델이 되었다.</li>
</ul>
</section>
<section id="실용적-응용-및-평가" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="실용적-응용-및-평가"><span class="header-section-number">2.1.1</span> 실용적 응용 및 평가</h3>
<section id="평가-지표" class="level4" data-number="2.1.1.1">
<h4 data-number="2.1.1.1" class="anchored" data-anchor-id="평가-지표"><span class="header-section-number">2.1.1.1</span> 평가 지표</h4>
<p><strong>Intrinsic Evaluation (내재적 평가):</strong> - <strong>단어 유사도</strong>: WordSim-353, SimLex-999 - 사람이 평가한 단어 유사도와 모델 예측의 상관관계 측정 - <strong>단어 관계</strong>: “king - man + woman = queen” - 벡터 연산으로 의미 관계 포착 정도 평가</p>
<p><strong>Extrinsic Evaluation (외재적 평가):</strong> - <strong>문서 분류 정확도</strong>: 실제 분류 태스크에서의 성능 - <strong>정보 검색 성능</strong>: NDCG, MAP - 검색 결과의 관련성 및 순위 정확도 - <strong>의미적 텍스트 유사도</strong>: STS benchmark - 문장 간 의미적 유사성 예측 성능</p>
</section>
<section id="모델-선택-가이드" class="level4" data-number="2.1.1.2">
<h4 data-number="2.1.1.2" class="anchored" data-anchor-id="모델-선택-가이드"><span class="header-section-number">2.1.1.2</span> 모델 선택 가이드</h4>
<ul>
<li><strong>소규모 데이터</strong>: FastText (OOV 처리)</li>
<li><strong>대규모 문서 분류</strong>: BERT fine-tuning</li>
<li><strong>실시간 유사도 계산</strong>: SBERT</li>
<li><strong>창작/생성 태스크</strong>: GPT 계열</li>
</ul>
</section>
<section id="통계적-해석" class="level4" data-number="2.1.1.3">
<h4 data-number="2.1.1.3" class="anchored" data-anchor-id="통계적-해석"><span class="header-section-number">2.1.1.3</span> 통계적 해석</h4>
<p>임베딩 공간에서의 기하학적 관계: <span class="math display">\[\cos(\mathbf{v}_{\text{similar words}}) &gt; \cos(\mathbf{v}_{\text{dissimilar words}})\]</span></p>
<p><strong>시각화 도구</strong>: t-SNE/UMAP을 통한 의미적 클러스터링 확인</p>
</section>
</section>
</section>
<section id="결론" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="결론"><span class="header-section-number">2.2</span> 결론</h2>
<p>본 문서에서는 단어의 고정된 의미 표현을 넘어, 문맥에 따라 유연하게 변화하는 의미를 포착하는 동적 임베딩 방법론들을 심층적으로 살펴보았다. ELMo에서 시작하여 BERT, GPT, SBERT에 이르기까지, 이러한 문맥 기반 임베딩 모델들은 자연어 처리(NLP) 분야에 혁명적인 발전을 가져왔다.</p>
<p>주요 내용을 다시 한번 정리하면 다음과 같다.</p>
<ul>
<li><p><strong>정적 임베딩의 한계 극복</strong>: 초기의 워드 임베딩(Word2Vec, GloVe 등)은 단어의 의미를 단일 벡터로 표현하여 문맥에 따른 다의성을 반영하지 못했다. 동적 임베딩은 이 한계를 극복하고, 동일한 단어라도 문맥에 따라 다른 벡터 표현을 생성함으로써 보다 정교한 의미 이해를 가능하게 했다.</p></li>
<li><p><strong>주요 모델들의 혁신과 기여</strong>:</p>
<ul>
<li><strong>ELMo</strong>: 양방향 LSTM을 통해 문맥 정보를 통합하고, 여러 계층의 표현을 활용하여 풍부한 임베딩을 제공했다.</li>
<li><strong>BERT</strong>: 트랜스포머 아키텍처와 Masked Language Model, Next Sentence Prediction과 같은 혁신적인 사전 학습 방식을 도입하여 양방향 문맥 이해의 새로운 지평을 열었다. 이는 다양한 NLP 다운스트림 태스크에서 SOTA(State-of-the-Art) 성능을 달성하는 데 크게 기여했다.</li>
<li><strong>GPT</strong>: 단방향 트랜스포머 디코더를 기반으로 강력한 텍스트 생성 능력을 보여주었으며, 특히 GPT-3 이후 모델들은 In-context Learning이라는 새로운 패러다임을 제시하며 모델 활용의 유연성을 크게 확장했다.</li>
<li><strong>SBERT</strong>: 기존 BERT 모델을 문장 임베딩 생성에 효율적으로 사용할 수 있도록 Siamese 및 Triplet 네트워크 구조를 활용하여, 의미적으로 유사한 문장 벡터를 효과적으로 생성하고 문장 간 유사도 비교 작업의 속도와 정확도를 크게 향상시켰다.</li>
</ul></li>
<li><p><strong>패러다임의 전환과 LLM의 토대</strong>: 이러한 문맥 기반 임베딩 모델들의 발전은 단순한 특징 추출기를 넘어, 언어 자체를 깊이 이해하고 생성할 수 있는 대규모 언어 모델(Large Language Models, LLMs) 시대로 나아가는 핵심적인 발판이 되었다. 사전 학습과 미세 조정(fine-tuning) 패러다임, 그리고 최근의 프롬프트 기반 학습은 모델의 활용 범위를 크게 넓혔다.</p></li>
<li><p><strong>적절한 전략 선택의 지속적 중요성</strong>: 해결하고자 하는 특정 문제의 요구사항, 가용 데이터의 특성, 계산 자원 등을 고려하여 가장 적합한 임베딩 전략과 모델을 선택하는 것은 여전히 중요하다. 실용적인 응용을 위해서는 모델의 성능뿐만 아니라 효율성, 해석 가능성 등도 함께 고려해야 한다.</p></li>
</ul>
<p>문맥을 이해하는 텍스트 벡터화 기술은 앞으로도 계속 발전하여, 기계가 인간의 언어를 더욱 정교하게 이해하고 상호작용하는 미래를 앞당길 것이다. 이러한 기술의 발전은 정보 검색, 질의응답, 창작, 교육 등 사회 여러 분야에 걸쳐 혁신적인 변화를 주도할 잠재력을 지니고 있다.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>