<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.543">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kwangmin Kim">
<meta name="dcterms.date" content="2025-01-06">
<meta name="description" content="자연어 처리(NLP)에서 텍스트의 의미와 문맥을 벡터로 표현하는 신경망 기반의 고급 벡터화 방법들을 심층적으로 탐구한다. 정적 워드 임베딩(Word2Vec, GloVe, FastText)과 동적 문맥 임베딩(ELMo, BERT, SBERT)의 원리, 특징, 활용 방안을 다룬다.">

<title>Kwangmin Kim - 텍스트 벡터화: 신경망 기반 방법론</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete-preset-algolia.umd.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "algolia": {
    "application-id": "DUOR1DRC9D",
    "search-only-api-key": "f264da5dea684ffb9e9b4a574af3ed61",
    "index-name": "prod_QUARTO",
    "analytics-events": true,
    "show-logo": true,
    "libDir": "site_libs"
  },
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": true,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js"></script>


<script type="text/javascript">
var ALGOLIA_INSIGHTS_SRC = "https://cdn.jsdelivr.net/npm/search-insights/dist/search-insights.iife.min.js";
!function(e,a,t,n,s,i,c){e.AlgoliaAnalyticsObject=s,e[s]=e[s]||function(){
(e[s].queue=e[s].queue||[]).push(arguments)},i=a.createElement(t),c=a.getElementsByTagName(t)[0],
i.async=1,i.src=n,c.parentNode.insertBefore(i,c)
}(window,document,"script",ALGOLIA_INSIGHTS_SRC,"aa");
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/autocomplete-plugin-algolia-insights">

</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6W0EKFMWBN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6W0EKFMWBN', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../../styles.css">
<meta property="og:title" content="Kwangmin Kim - 텍스트 벡터화: 신경망 기반 방법론">
<meta property="og:description" content="자연어 처리(NLP)에서 텍스트의 의미와 문맥을 벡터로 표현하는 신경망 기반의 고급 벡터화 방법들을 심층적으로 탐구한다. 정적 워드 임베딩(Word2Vec, GloVe, FastText)과 동적 문맥 임베딩(ELMo, BERT, SBERT)의 원리, 특징, 활용 방안을 다룬다.">
<meta property="og:site_name" content="Kwangmin Kim">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html"> 
<span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"> <i class="bi bi-github" role="img" aria-label="Github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"> <i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#요약" id="toc-요약" class="nav-link active" data-scroll-target="#요약"><span class="header-section-number">1</span> 요약</a></li>
  <li><a href="#텍스트-인코딩-및-벡터화" id="toc-텍스트-인코딩-및-벡터화" class="nav-link" data-scroll-target="#텍스트-인코딩-및-벡터화"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</a>
  <ul class="collapse">
  <li><a href="#신경망-사용-20082018" id="toc-신경망-사용-20082018" class="nav-link" data-scroll-target="#신경망-사용-20082018"><span class="header-section-number">2.0.1</span> 신경망 사용 (2008~2018)</a></li>
  <li><a href="#dtm-방식의-한계와-신경망-접근법의-등장" id="toc-dtm-방식의-한계와-신경망-접근법의-등장" class="nav-link" data-scroll-target="#dtm-방식의-한계와-신경망-접근법의-등장"><span class="header-section-number">2.0.2</span> DTM 방식의 한계와 신경망 접근법의 등장</a></li>
  <li><a href="#워드-임베딩-word-embedding" id="toc-워드-임베딩-word-embedding" class="nav-link" data-scroll-target="#워드-임베딩-word-embedding"><span class="header-section-number">2.0.3</span> 워드 임베딩 (Word Embedding)</a></li>
  <li><a href="#결론" id="toc-결론" class="nav-link" data-scroll-target="#결론"><span class="header-section-number">2.1</span> 결론</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">텍스트 벡터화: 신경망 기반 방법론</h1>
<p class="subtitle lead">Word2Vec, GloVe, FastText을 이용한 벡터 표현 소개</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>자연어 처리(NLP)에서 텍스트의 의미와 문맥을 벡터로 표현하는 신경망 기반의 고급 벡터화 방법들을 심층적으로 탐구한다. 정적 워드 임베딩(Word2Vec, GloVe, FastText)과 동적 문맥 임베딩(ELMo, BERT, SBERT)의 원리, 특징, 활용 방안을 다룬다.</p>
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kwangmin Kim </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 6, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 통계 기반 벡터화의 한계를 넘어 텍스트 데이터로부터 풍부한 의미론적, 문맥적 정보를 추출하는 신경망 기반 벡터화 방법론을 소개한다.</p>
<ul>
<li><strong>DTM 방식의 한계와 신경망 접근법의 등장</strong>:
<ul>
<li>전통적인 DTM(문서-단어 행렬) 방식의 문제점(차원의 저주, 희소성, 의미 관계 표현 불가)을 지적하고, 이를 극복하기 위한 신경망 기반 밀집 벡터 표현(워드 임베딩)의 필요성을 설명한다.</li>
</ul></li>
<li><strong>워드 임베딩 (Word Embedding) - 정적 임베딩</strong>:
<ul>
<li><strong>핵심 원리</strong>: “같은 문맥에 나타나는 단어는 비슷한 의미를 가진다”는 분포 가설에 기반하여 단어를 저차원 밀집 벡터로 표현한다.</li>
<li><strong>Embedding Layer</strong>: 정수 인코딩된 단어를 밀집 벡터로 변환하는 신경망의 핵심 구성 요소로, 그 구조와 Look-up Table 방식을 설명한다.</li>
<li><strong>주요 모델</strong>:
<ul>
<li><strong>Word2Vec (CBOW, Skip-gram)</strong>: 주변 단어로 중심 단어를 예측(CBOW)하거나 중심 단어로 주변 단어를 예측(Skip-gram)하며 벡터를 학습한다.</li>
<li><strong>GloVe</strong>: 전체 단어 동시 등장 통계 정보를 직접 활용하여 벡터를 학습한다.</li>
<li><strong>FastText</strong>: 단어를 문자 n-gram으로 분해하여 벡터를 학습함으로써 OOV(Out-of-Vocabulary) 문제에 강직한 형태론적 특징을 포착한다.</li>
</ul></li>
</ul></li>
<li><strong>실용적 응용 및 평가</strong>:
<ul>
<li>임베딩 모델의 성능을 평가하는 내재적 평가(단어 유사도, 관계 유추)와 외재적 평가(다운스트림 태스크 성능) 방법을 소개한다.</li>
</ul></li>
<li><strong>결론</strong>: 신경망 기반 벡터화 기법들의 발전 과정과 그 의의를 요약한다.</li>
</ul>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 전통적 방법 (통계 기반)
│   ├── DTM (Document Term Matrix)
│   ├── BoW (Bag of Words)  
│   └── TF-IDF
└── 신경망 기반 방법
    ├── Embedding Layer (핵심 구성 요소)
    └── 구체적 모델들
        ├── Word2Vec (CBOW, Skip-gram)
        ├── GloVe
        ├── FastText
        └── 문맥 기반 모델 (BERT, GPT 등)</code></pre>
<section id="신경망-사용-20082018" class="level3" data-number="2.0.1">
<h3 data-number="2.0.1" class="anchored" data-anchor-id="신경망-사용-20082018"><span class="header-section-number">2.0.1</span> 신경망 사용 (2008~2018)</h3>
</section>
<section id="dtm-방식의-한계와-신경망-접근법의-등장" class="level3" data-number="2.0.2">
<h3 data-number="2.0.2" class="anchored" data-anchor-id="dtm-방식의-한계와-신경망-접근법의-등장"><span class="header-section-number">2.0.2</span> DTM 방식의 한계와 신경망 접근법의 등장</h3>
<p><strong>DTM 방식의 문제점:</strong> - <strong>차원의 저주</strong>: 어휘집 크기 = 벡터 차원 (예: 50,000개 단어 → 50,000차원) - <strong>희소성</strong>: 대부분의 값이 0인 sparse vector - <strong>의미적 관계 부재</strong>: “왕”과 “여왕”의 관계를 벡터가 표현하지 못함 - <strong>문제 상황:</strong></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 기존 방식 (원-핫 인코딩)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">"사과"</span> <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, ...]  <span class="co"># 50,000차원 벡터</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">"바나나"</span> <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, ...]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">"과일"</span> <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, ...]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>모든 단어가 서로 똑같이 멀어 보임 (유클리드 거리 = √2)</li>
<li>“사과”와 “바나나”가 비슷한 과일이라는 정보가 없음</li>
<li>메모리 낭비 (대부분이 0)</li>
</ul>
<p><strong>신경망 접근법의 혁신:</strong> - <strong>밀집 벡터(Dense Vector)</strong>: 고정된 낮은 차원 (예: 300차원)에 0/1 값이 아닌 실수 값을 가짐. - <strong>의미적 유사도</strong>: 벡터 간 거리로 단어 유사도 측정 가능 - <strong>문맥 학습</strong>: 주변 단어들을 통해 의미 학습</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 신경망 모델: 워드 임베딩 (300차원)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">"사과"</span> <span class="op">=</span> [<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, ...]     <span class="co"># 300개 실수</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">"바나나"</span> <span class="op">=</span> [<span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.2</span>, ...]   <span class="co"># 비슷한 값들</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">"과일"</span> <span class="op">=</span> [<span class="fl">0.25</span>, <span class="op">-</span><span class="fl">0.45</span>, <span class="fl">0.65</span>, <span class="fl">0.15</span>, ...] <span class="co"># 과일 카테고리</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>문맥 고려 방법 (Neural / Context-dependent)</li>
<li>신경망을 통해 단어의 의미를 주변 문맥을 고려하여 학습하고, 이를 밀집 벡터(Dense Vector)로 표현.</li>
</ul>
</section>
<section id="워드-임베딩-word-embedding" class="level3" data-number="2.0.3">
<h3 data-number="2.0.3" class="anchored" data-anchor-id="워드-임베딩-word-embedding"><span class="header-section-number">2.0.3</span> 워드 임베딩 (Word Embedding)</h3>
<ul>
<li><p>문맥 속에서 각 단어가 어떻게 사용되는지까지 신경망을 통해 벡터값을 구해 벡터에 담아내려 시도.</p></li>
<li><p>학습 후에는 각 단어 벡터 간의 유사도(의미반영)를 계산할 수 있다.</p></li>
<li><p>즉, 신경망 기반의 벡터화라는 것은 벡터의 값이 학습에 의해 결정된다는 것을 의미.</p></li>
<li><p>워드 임베딩 모델의 예시</p>
<ul>
<li>Word2Vec, GloVe, FastText, 모델 내 <code>Embedding</code> Layer 사용.<br>
</li>
</ul></li>
<li><p>Embedding Layer는 모델 내에 있는 레이어로, 입력 데이터를 밀집 벡터로 변환하는 역할을 한다.</p>
<ul>
<li>딥러닝 자연어 처리 시 거의 항상 하게 되는 작업</li>
<li>단어 -&gt; 정수 인코딩 -&gt; Embedding Layer -&gt; 임베딩 벡터(=밀집 벡터)</li>
<li>자연어 처리에서 단어를 정수로 바꿔주는 이유가 Embedding Layer를 통해 밀집 벡터로 변환하기 위해서이다.</li>
<li>Look up table: 정수 인코딩을 밀집 벡터로 변환하는 테이블</li>
</ul></li>
<li><p>핵심 원리</p>
<ul>
<li><strong>분포 가설(Distributional Hypothesis):</strong>
<ul>
<li>“같은 문맥에서 나타나는 단어들은 유사한 의미를 가진다”</li>
<li>수학적으로 표현하면: <span class="math inline">\(\text{similarity}(w_i, w_j) \propto \text{context\_overlap}(w_i, w_j)\)</span></li>
</ul></li>
<li><strong>“같은 문맥에 나타나는 단어들은 비슷한 의미를 가진다”</strong></li>
<li><strong>예시:</strong></li>
</ul>
<pre><code>문장1: "나는 사과를 먹었다"
문장2: "나는 바나나를 먹었다"  
문장3: "나는 딸기를 먹었다"</code></pre>
<p>→ “사과”, “바나나”, “딸기”는 같은 위치(문맥)에 나타남 → 비슷한 벡터를 가져야 함</p></li>
<li><p>임베딩 벡터의 의미</p>
<ul>
<li><p>벡터 간 유사도</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>cosine_similarity(v_사과, v_바나나) <span class="op">=</span> <span class="fl">0.8</span>  <span class="co"># 높음 (비슷함)</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>cosine_similarity(v_사과, v_컴퓨터) <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># 낮음 (다름)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>수식: <span class="math inline">\(\text{similarity}(\mathbf{v}_1, \mathbf{v}_2) = \frac{\mathbf{v}_1 \cdot \mathbf{v}_2}{||\mathbf{v}_1|| \cdot ||\mathbf{v}_2||}\)</span></li>
<li>직관적 해석
<ul>
<li>1에 가까울수록 비슷한 의미</li>
<li>0에 가까울수록 관련 없음</li>
<li>-1에 가까울수록 반대 의미</li>
</ul></li>
</ul></li>
<li><p>벡터 연산의 마법</p>
<ul>
<li>유명한 예시:
<ul>
<li><span class="math inline">\(\vec{\text{king}} - \vec{\text{man}} + \vec{\text{woman}} \approx \vec{\text{queen}}\)</span></li>
<li><span class="math inline">\(\vec{\text{king}} - \vec{\text{man}}\)</span>: “남성성”을 제거 → “왕권” 개념만 남음</li>
<li><span class="math inline">\(+ \vec{\text{woman}}\)</span>: “여성성” 추가</li>
<li>결과: “여성 + 왕권” → “여왕”</li>
</ul></li>
<li>수학적 설명:
<ul>
<li>각 벡터를 의미 성분들의 조합으로 생각:</li>
</ul>
<pre><code>king = [왕권: 0.9, 남성: 0.8, 권력: 0.7, ...]
man = [남성: 0.9, 성인: 0.6, ...]  
woman = [여성: 0.9, 성인: 0.6, ...]</code></pre>
<ul>
<li>연산 후:</li>
</ul>
<pre><code>king - man + woman ≈ [왕권: 0.9, 여성: 0.9, 권력: 0.7, ...]</code></pre>
→ “queen”과 가장 유사!</li>
</ul></li>
</ul></li>
<li><p>실제 학습 예시</p>
<ul>
<li><p>초기 상태 (랜덤)</p>
<pre><code>"사과" = [0.1, -0.3, 0.7, ...]  (랜덤)
"바나나" = [-0.8, 0.2, -0.1, ...] (랜덤)</code></pre>
<ul>
<li>서로 전혀 관련 없어 보임</li>
</ul></li>
<li><p>학습 진행</p>
<pre><code>문장들을 계속 보면서:
"사과를 먹었다", "바나나를 먹었다", "딸기를 먹었다"
...

* 점차 비슷한 벡터로 수렴:
</code></pre>
<p>“사과” = [0.2, -0.4, 0.6, …] “바나나” = [0.3, -0.5, 0.7, …]<br>
“딸기” = [0.25, -0.45, 0.65, …] ```</p></li>
<li><p>학습 완료 후</p>
<ul>
<li>의미가 비슷한 단어들 → 벡터 공간에서 가까운 위치</li>
<li>반대 의미 단어들 → 먼 위치 또는 반대 방향</li>
<li>유추 관계 → 벡터 연산으로 표현 가능</li>
</ul></li>
</ul></li>
<li><p><strong>핵심</strong>: 신경망이 “문맥”이라는 단서를 통해 <strong>단어의 의미</strong>를 수치로 학습</p></li>
</ul>
<section id="embedding-layer-구조-분석" class="level4" data-number="2.0.3.1">
<h4 data-number="2.0.3.1" class="anchored" data-anchor-id="embedding-layer-구조-분석"><span class="header-section-number">2.0.3.1</span> Embedding Layer 구조 분석</h4>
<ul>
<li>Embedding Layer란?
<ul>
<li>정수 인코딩 → 밀집 벡터 변환기</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 이런 변환을 해주는 것</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="dv">15</span> → [<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.3</span>]  <span class="co"># 300차원 벡터</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="dv">23</span> → [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="fl">0.5</span>]   <span class="co"># 300차원 벡터  </span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span>  → [<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.4</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.9</span>]  <span class="co"># 300차원 벡터</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>왜 정수 인코딩이 필요한가?
<ul>
<li>문제: 컴퓨터는 “사과”라는 글자를 직접 처리할 수 없음</li>
<li>해결 과정:</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1단계: 단어 → 정수 (정수 인코딩)</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co">"사과"</span> → <span class="dv">15</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">"바나나"</span> → <span class="dv">23</span>  </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">"딸기"</span> → <span class="dv">7</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 2단계: 정수 → 벡터 (Embedding Layer)</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="dv">15</span> → [<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.7</span>, ...]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="dv">23</span> → [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, ...]</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span>  → [<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.4</span>, ...]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p>Look-up Table의 구체적 동작</p>
<ul>
<li>Embedding Matrix 구조: <span class="math inline">\(\mathbf{E} \in \mathbb{R}^{V \times d}\)</span></li>
<li>실제 예시:</li>
</ul>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># V = 5 (어휘 크기), d = 3 (임베딩 차원)</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>embedding_matrix <span class="op">=</span> [</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>],    <span class="co"># 단어 ID 0의 벡터</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>],    <span class="co"># 단어 ID 1의 벡터  </span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>],    <span class="co"># 단어 ID 2의 벡터</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.4</span>],   <span class="co"># 단어 ID 3의 벡터</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>],   <span class="co"># 단어 ID 4의 벡터</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>행렬의 의미:</p>
<ul>
<li>행(row): 각 단어의 임베딩 벡터</li>
<li>열(column): 임베딩 벡터의 각 차원</li>
<li>전체: 모든 단어의 벡터를 저장하는 “사전”</li>
</ul></li>
<li><p>Look-up 연산 과정</p>
<ul>
<li>입력: <code>input_ids = [2, 0, 4]</code></li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1단계: 각 ID에 해당하는 행을 추출</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>embedding_matrix[<span class="dv">2</span>] → [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>]    <span class="co"># ID 2의 벡터</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>embedding_matrix[<span class="dv">0</span>] → [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>]    <span class="co"># ID 0의 벡터  </span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>embedding_matrix[<span class="dv">4</span>] → [<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>]   <span class="co"># ID 4의 벡터</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 2단계: 결과 (3개 벡터)</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> [</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>],     <span class="co"># 첫 번째 단어</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>],     <span class="co"># 두 번째 단어</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>]     <span class="co"># 세 번째 단어  </span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>수학적 의미</p>
<ul>
<li><span class="math inline">\(\text{embedding}(i) = \mathbf{E}[i, :]\)</span>
<ul>
<li><span class="math inline">\(i\)</span>: 단어의 정수 ID</li>
<li><span class="math inline">\(\mathbf{E}[i, :]\)</span>: 행렬 E의 i번째 행 전체</li>
<li>결과: i번째 단어의 임베딩 벡터</li>
</ul></li>
<li>구체적 예시:</li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">2</span>  <span class="co"># "딸기"의 ID라고 가정</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>embedding(<span class="dv">2</span>) <span class="op">=</span> E[<span class="dv">2</span>, :] <span class="op">=</span> [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>]  <span class="co"># 2번째 행</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>실제 PyTorch 코드</p></li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Embedding Layer 생성</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">1000</span>      <span class="co"># 어휘 크기</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">300</span>    <span class="co"># 벡터 차원</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> nn.Embedding(vocab_size, embedding_dim)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 내부 구조 확인</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(embedding.weight.shape)  <span class="co"># torch.Size([1000, 300])</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># → 1000×300 크기의 look-up table</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 입력 데이터</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> torch.tensor([<span class="dv">15</span>, <span class="dv">23</span>, <span class="dv">7</span>])  <span class="co"># 3개 단어의 ID</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. 임베딩 변환</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> embedding(input_ids)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output.shape)  <span class="co"># torch.Size([3, 300])</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># → 3개 단어 × 300차원 벡터</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Look-up Table이 학습되는 과정
<ul>
<li>초기화 (랜덤)</li>
</ul>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 처음에는 랜덤 값들</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>embedding_matrix <span class="op">=</span> torch.randn(vocab_size, embedding_dim)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li>학습 과정</li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 예: "사과는 맛있다"라는 문장 학습</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> [<span class="dv">15</span>, <span class="dv">23</span>, <span class="dv">7</span>]  <span class="co"># [사과는, 맛있다, &lt;</span><span class="re">END</span><span class="co">&gt;]</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 현재 임베딩으로 예측</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> embedding_matrix[input_ids]  <span class="co"># Look-up</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> model(embeddings)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 손실 계산</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(prediction, target)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 역전파로 embedding_matrix 업데이트  </span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>loss.backward()  <span class="co"># embedding_matrix의 gradient 계산</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>optimizer.step()  <span class="co"># embedding_matrix 값들 업데이트</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p>핵심: 학습이 진행되면서 embedding_matrix의 각 행(단어 벡터)이 점점 더 의미 있는 값으로 변함!</p></li>
<li><p>왜 “Look-up Table”이라고 부르는가?</p>
<ul>
<li>전통적인 사전과 비교</li>
</ul>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 일반 사전</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>사전 <span class="op">=</span> {</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>   <span class="st">"사과"</span>: <span class="st">"빨간 과일"</span>,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>   <span class="st">"바나나"</span>: <span class="st">"노란 과일"</span>, </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>   <span class="st">"컴퓨터"</span>: <span class="st">"전자 기기"</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>의미 <span class="op">=</span> 사전[<span class="st">"사과"</span>]  <span class="co"># "빨간 과일"</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Embedding Table  </span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>임베딩_테이블 <span class="op">=</span> {</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>   <span class="dv">15</span>: [<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.7</span>, ...],   <span class="co"># "사과"</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>   <span class="dv">23</span>: [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, ...],   <span class="co"># "바나나"</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>   <span class="dv">78</span>: [<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.8</span>, <span class="fl">0.1</span>, ...]    <span class="co"># "컴퓨터"</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>벡터 <span class="op">=</span> 임베딩_테이블[<span class="dv">15</span>]  <span class="co"># [0.2, -0.4, 0.7, ...]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>차이점:
<ul>
<li>일반 사전: 단어 → 설명 (텍스트)</li>
<li>임베딩 테이블: 단어 ID → 숫자 벡터</li>
</ul></li>
</ul></li>
<li><p>전체 과정 정리</p></li>
</ul>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 전체 파이프라인</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co">"사과는 맛있다"</span> </span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>→ [<span class="st">"사과는"</span>, <span class="st">"맛있다"</span>]           <span class="co"># 토큰화</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>→ [<span class="dv">15</span>, <span class="dv">23</span>]                      <span class="co"># 정수 인코딩  </span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>→ [[<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.7</span>],           <span class="co"># Embedding Layer (Look-up)</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>]]</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>→ 신경망 처리                    <span class="co"># 후속 레이어들</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>핵심
<ul>
<li>Embedding Layer는 단순히 “정수 ID를 인덱스로 사용해서 미리 저장된 벡터를 가져오는” 매우 단순한 연산.</li>
<li>하지만 이 벡터들이 학습을 통해 의미 있는 값으로 변하기 때문에 강력한 도구가 되는 것</li>
</ul></li>
</ul>
</section>
<section id="주요-모델별-특징" class="level4" data-number="2.0.3.2">
<h4 data-number="2.0.3.2" class="anchored" data-anchor-id="주요-모델별-특징"><span class="header-section-number">2.0.3.2</span> 주요 모델별 특징</h4>
<ul>
<li><p><strong>Word2Vec (2013)</strong>의 두 가지 아키텍처:</p>
<ul>
<li><strong>CBOW (Continuous Bag of Words)</strong>
<ul>
<li>주변 단어들로 중심 단어 예측</li>
<li>목적 함수: <span class="math inline">\(\max \sum_{w \in V} \log P(w|context(w))\)</span></li>
<li>기본 구조
<ul>
<li><strong>목표</strong>: 주변 단어들을 보고 가운데 단어를 맞추기</li>
</ul>
<pre><code>입력: [나는] [___] [먹었다]  
출력: [사과를]</code></pre></li>
<li>수학적 모델링
<ul>
<li><strong>1단계: 입력 표현</strong>
<ul>
<li>문맥 단어들: <span class="math inline">\(w_{-2}, w_{-1}, w_{+1}, w_{+2}\)</span> (윈도우 크기 2)</li>
<li>각 단어의 원-핫 벡터: <span class="math inline">\(\mathbf{x}_{w} \in \{0,1\}^V\)</span> (V = 어휘 크기)</li>
</ul></li>
<li><strong>2단계: 임베딩 변환</strong>
<ul>
<li><span class="math inline">\(\mathbf{v}_w = \mathbf{W}_{\text{in}} \mathbf{x}_w\)</span></li>
<li>여기서:
<ul>
<li><span class="math inline">\(\mathbf{W}_{\text{in}} \in \mathbb{R}^{d \times V}\)</span>: 입력 임베딩 행렬</li>
<li><span class="math inline">\(d\)</span>: 임베딩 차원 (예: 300)</li>
<li><span class="math inline">\(\mathbf{v}_w \in \mathbb{R}^d\)</span>: 단어의 임베딩 벡터</li>
</ul></li>
<li><strong>직관적 해석</strong>:
<ul>
<li><span class="math inline">\(\mathbf{W}_{\text{in}}\)</span> 는 “단어 ID → 의미 벡터” 변환표</li>
<li>원-핫 벡터와의 곱은 단순히 해당 행을 선택하는 것</li>
</ul></li>
</ul></li>
<li><strong>3단계: 문맥 벡터 계산</strong>
<ul>
<li><span class="math inline">\(\mathbf{h} = \frac{1}{C} \sum_{c \in \text{context}} \mathbf{v}_c\)</span></li>
<li><strong>직관적 해석</strong>:
<ul>
<li>주변 단어들의 평균 벡터</li>
<li>“이 위치에 올 수 있는 단어의 특징”을 나타냄</li>
</ul></li>
</ul></li>
<li><strong>4단계: 출력 확률 계산</strong>
<ul>
<li><span class="math inline">\(P(w_{\text{center}}|\text{context}) = \frac{\exp(\mathbf{u}_{w_{\text{center}}}^T \mathbf{h})}{\sum_{w'=1}^V \exp(\mathbf{u}_{w'}^T \mathbf{h})}\)</span></li>
<li><strong>직관적 해석</strong>:
<ul>
<li>분자: 정답 단어가 이 문맥에 얼마나 적합한지</li>
<li>분모: 모든 단어 중에서 정규화 (확률의 합 = 1)</li>
</ul></li>
<li>예시
<ul>
<li><strong>예시 문장</strong>: “나는 사과를 먹었다”</li>
</ul>
<pre><code>Step 1: 문맥 = ["나는", "먹었다"], 정답 = "사과를"
Step 2: 현재 모델이 "바나나를" 높은 확률로 예측
Step 3: 손실 계산 → "사과를"의 확률을 높이도록 가중치 업데이트
Step 4: 반복 학습 후 → "나는 ___ 먹었다" 문맥에서 과일 단어들이 높은 확률</code></pre>
<ul>
<li><strong>학습 결과</strong>:
<ul>
<li>비슷한 문맥에 나타나는 단어들 → 비슷한 벡터</li>
<li>“사과”, “바나나”, “딸기” → 가까운 위치의 벡터</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Skip-gram:</strong>
<ul>
<li><p>중심 단어로 주변 단어들 예측<br>
</p></li>
<li><p>목적 함수: <span class="math inline">\(\max \sum_{w \in V} \sum_{c \in context(w)} \log P(c|w)\)</span></p></li>
<li><p>특징:</p>
<ul>
<li>선형 관계 학습: <span class="math inline">\(\vec{king} - \vec{man} + \vec{woman} \approx \vec{queen}\)</span></li>
<li>속도가 빠름</li>
<li>단어별 고정된 하나의 벡터</li>
</ul></li>
<li><p>예시</p>
<pre><code>입력: [사과를]
출력: [나는], [먹었다] (주변 모든 단어들)
"중심 단어로 문맥 예측하기"</code></pre></li>
<li><p>수학적 모델</p>
<ul>
<li>목적 함수: <span class="math inline">\(\max \sum_{w \in V} \sum_{c \in context(w)} \log P(c|w)\)</span></li>
<li>단계별 해석:
<ul>
<li><strong>1단계</strong>: <span class="math inline">\(w \in V\)</span> (모든 단어에 대해)
<ul>
<li>말뭉치의 모든 단어를 중심 단어로 한 번씩 사용</li>
</ul></li>
<li><strong>2단계</strong>: <span class="math inline">\(c \in context(w)\)</span> (각 중심 단어의 모든 문맥 단어에 대해)
<ul>
<li>윈도우 크기만큼 주변 단어들을 문맥으로 설정</li>
</ul></li>
<li><strong>3단계</strong>: <span class="math inline">\(\log P(c|w)\)</span> (확률의 로그값)
<ul>
<li>중심 단어 w가 주어졌을 때 문맥 단어 c가 나타날 확률</li>
</ul></li>
</ul></li>
<li>예시
<ul>
<li><strong>문장</strong>: “나는 사과를 정말 좋아한다” (윈도우 크기 = 2)</li>
</ul>
<pre><code>중심 단어: "사과를"
문맥 단어들: ["나는", "정말", "좋아한다"] (앞뒤 2개씩)

Skip-gram이 학습하는 것:
P("나는"|"사과를")     → 높아야 함
P("정말"|"사과를")     → 높아야 함  
P("좋아한다"|"사과를") → 높아야 함
P("컴퓨터"|"사과를")   → 낮아야 함 (문맥에 없음)</code></pre></li>
<li>확률 계산: <span class="math inline">\(P(c|w) = \frac{\exp(\mathbf{u}_c^T \mathbf{v}_w)}{\sum_{c'=1}^V \exp(\mathbf{u}_{c'}^T \mathbf{v}_w)}\)</span>
<ul>
<li><span class="math inline">\(\mathbf{v}_w\)</span> : 중심 단어 w의 입력 벡터 (우리가 원하는 임베딩)</li>
<li><span class="math inline">\(\mathbf{u}_c\)</span> : 문맥 단어 c의 출력 벡터</li>
<li><span class="math inline">\(\mathbf{u}_c^T \mathbf{v}_w\)</span> : 두 단어가 “같이 나타날 가능성” 점수</li>
<li>Softmax로 정규화하여 확률로 변환</li>
</ul></li>
<li>학습 과정 예시
<ul>
<li><p>초기 상태 (랜덤 벡터)</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>v_사과 <span class="op">=</span> [<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>]  <span class="co"># 중심 단어 벡터</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>u_나는 <span class="op">=</span> [<span class="fl">0.4</span>, <span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.1</span>]  <span class="co"># 문맥 단어 벡터</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>u_컴퓨터 <span class="op">=</span> [<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>점수 계산</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>score_나는 <span class="op">=</span> dot(u_나는, v_사과) <span class="op">=</span> <span class="fl">0.4</span><span class="op">*</span><span class="fl">0.1</span> <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>(<span class="op">-</span><span class="fl">0.2</span>) <span class="op">+</span> (<span class="op">-</span><span class="fl">0.1</span>)<span class="op">*</span><span class="fl">0.3</span> <span class="op">=</span> <span class="op">-</span><span class="fl">0.01</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>score_컴퓨터 <span class="op">=</span> dot(u_컴퓨터, v_사과) <span class="op">=</span> <span class="op">-</span><span class="fl">0.2</span><span class="op">*</span><span class="fl">0.1</span> <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span>(<span class="op">-</span><span class="fl">0.2</span>) <span class="op">+</span> <span class="fl">0.2</span><span class="op">*</span><span class="fl">0.3</span> <span class="op">=</span> <span class="op">-</span><span class="fl">0.08</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>확률 계산 (간단히)</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>P(나는<span class="op">|</span>사과) <span class="op">=</span> exp(<span class="op">-</span><span class="fl">0.01</span>) <span class="op">/</span> (exp(<span class="op">-</span><span class="fl">0.01</span>) <span class="op">+</span> exp(<span class="op">-</span><span class="fl">0.08</span>) <span class="op">+</span> ...) ≈ <span class="fl">0.3</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>P(컴퓨터<span class="op">|</span>사과) <span class="op">=</span> exp(<span class="op">-</span><span class="fl">0.08</span>) <span class="op">/</span> (exp(<span class="op">-</span><span class="fl">0.01</span>) <span class="op">+</span> exp(<span class="op">-</span><span class="fl">0.08</span>) <span class="op">+</span> ...) ≈ <span class="fl">0.2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>학습 업데이트</p>
<ul>
<li>목표: P(나는|사과)는 높이고, P(컴퓨터|사과)는 낮추기</li>
</ul>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 실제로 "나는"이 문맥에 있었으므로</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co"># v_사과와 u_나는을 더 비슷하게 만들기</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>v_사과 <span class="op">+=</span> learning_rate <span class="op">*</span> u_나는  <span class="co"># 벡터를 가까워지게</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>u_나는 <span class="op">+=</span> learning_rate <span class="op">*</span> v_사과</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co"># "컴퓨터"는 문맥에 없었으므로  </span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># v_사과와 u_컴퓨터를 더 멀게 만들기</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>v_사과 <span class="op">-=</span> learning_rate <span class="op">*</span> u_컴퓨터  <span class="co"># 벡터를 멀어지게</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>u_컴퓨터 <span class="op">-=</span> learning_rate <span class="op">*</span> v_사과</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>GloVe (2014)</strong></p>
<ul>
<li>전역 통계 정보 활용</li>
<li>단어 동시 출현 행렬(Co-occurrence Matrix) 기반</li>
<li>목적 함수: <span class="math inline">\(\min \sum_{i,j=1}^V f(X_{ij})(\vec{w_i}^T \vec{w_j} + b_i + b_j - \log X_{ij})^2\)</span></li>
<li>여기서 <span class="math inline">\(X_{ij}\)</span> 는 단어 <span class="math inline">\(i\)</span> 와 <span class="math inline">\(j\)</span> 의 동시 출현 빈도</li>
<li>Word2Vec의 한계:
<ul>
<li>지역적 문맥 정보만 사용 (윈도우 크기 내)</li>
<li>전체 말뭉치의 통계 정보를 충분히 활용하지 못함</li>
</ul></li>
<li>GloVe의 해결책:
<ul>
<li>전체 말뭉치의 동시 출현 통계를 미리 계산</li>
<li>이 통계 정보를 직접 활용하여 벡터 학습</li>
</ul></li>
<li>동시 출현 행렬 (Co-occurrence Matrix)</li>
<li>구체적 예시
<ul>
<li><p>말뭉치:</p>
<pre><code>문장1: "나는 사과를 좋아한다"
문장2: "사과는 맛있는 과일이다"  
문장3: "바나나도 좋은 과일이다"</code></pre></li>
<li><p>윈도우 크기 2로 동시 출현 계산:</p>
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 8%">
<col style="width: 20%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>나는</th>
<th>사과를/사과는</th>
<th>좋아한다</th>
<th>맛있는</th>
<th>과일이다</th>
<th>바나나도</th>
<th>좋은</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>나는</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>사과를/사과는</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>좋아한다</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>맛있는</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>과일이다</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>바나나도</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>좋은</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table></li>
<li><p>해석: <span class="math inline">\(X_{ij}\)</span> = 단어 i와 j가 윈도우 내에서 함께 나타난 횟수</p></li>
</ul></li>
<li>GloVe 목적 함수 해부
<ul>
<li><span class="math inline">\(\min \sum_{i,j=1}^V f(X_{ij})(\mathbf{w}_i^T \mathbf{w}_j + b_i + b_j - \log X_{ij})^2\)</span></li>
<li>각 항목의 의미
<ul>
<li><span class="math inline">\(\mathbf{w}_i^T \mathbf{w}_j\)</span> : 단어 벡터들의 내적
<ul>
<li>두 단어의 유사도를 나타냄</li>
<li>자주 함께 나타나는 단어들은 높은 내적값을 가져야 함</li>
</ul></li>
<li><span class="math inline">\(b_i + b_j\)</span> : 편향(bias) 항
<ul>
<li>각 단어의 전반적인 빈도를 조정</li>
<li>자주 나타나는 단어는 높은 편향값</li>
</ul></li>
<li><span class="math inline">\(\log X_{ij}\)</span> : 실제 동시 출현 빈도의 로그
<ul>
<li>목표값 (우리가 맞추려는 값)</li>
<li>로그를 취하는 이유: 빈도의 분포가 매우 치우쳐 있어서</li>
</ul></li>
<li><span class="math inline">\(f(X_{ij})\)</span> : 가중치 함수
<ul>
<li>너무 희귀한 동시 출현은 신뢰도가 낮음 → 낮은 가중치</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><span class="math display">\[
f(x) = \begin{cases}
(\frac{x}{x_{max}})^{\alpha} &amp; \text{if } x &lt; x_{max} \\
1 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<pre><code>   * 가중치 함수의 역할:
      * 너무 희귀한 동시 출현은 신뢰도가 낮음 → 낮은 가중치
      * 너무 흔한 동시 출현도 정보량이 적음 → 가중치 제한
      * 적당한 빈도의 동시 출현에 높은 가중치</code></pre>
<ul>
<li>학습 과정 예시
<ul>
<li>예시: “사과”와 “과일” 관계 학습</li>
</ul>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 실제 동시 출현: X_사과_과일 = 10번</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 목표: w_사과^T * w_과일 + b_사과 + b_과일 ≈ log(10) = 2.3</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 초기 (랜덤)</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>w_사과 <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.2</span>]</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>w_과일 <span class="op">=</span> [<span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.1</span>]  </span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>내적 <span class="op">=</span> <span class="fl">0.1</span><span class="op">*</span><span class="fl">0.3</span> <span class="op">+</span> <span class="fl">0.2</span><span class="op">*</span>(<span class="op">-</span><span class="fl">0.1</span>) <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>편향합 <span class="op">=</span> <span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.3</span> <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>예측값 <span class="op">=</span> <span class="fl">0.01</span> <span class="op">+</span> <span class="fl">0.8</span> <span class="op">=</span> <span class="fl">0.81</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 손실: (0.81 - 2.3)^2 = 2.22 (크다!)</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 업데이트 후</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>w_사과 <span class="op">=</span> [<span class="fl">0.4</span>, <span class="fl">0.5</span>]  <span class="co"># 더 큰 값들로</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>w_과일 <span class="op">=</span> [<span class="fl">0.6</span>, <span class="fl">0.2</span>]</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>내적 <span class="op">=</span> <span class="fl">0.4</span><span class="op">*</span><span class="fl">0.6</span> <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span><span class="fl">0.2</span> <span class="op">=</span> <span class="fl">0.34</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>편향합 <span class="op">=</span> <span class="fl">1.2</span> <span class="op">+</span> <span class="fl">0.7</span> <span class="op">=</span> <span class="fl">1.9</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>예측값 <span class="op">=</span> <span class="fl">0.34</span> <span class="op">+</span> <span class="fl">1.9</span> <span class="op">=</span> <span class="fl">2.24</span>  <span class="co"># 목표 2.3에 가까워짐!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><p><strong>FastText (2017)</strong></p>
<ul>
<li>Sub-word 정보 활용</li>
<li>단어를 character n-gram으로 분해</li>
<li>예: “apple” → {“ap”, “pp”, “pl”, “le”} + “apple”</li>
<li>장점:
<ul>
<li>OOV(Out-of-Vocabulary) 문제 해결</li>
<li>형태학적 정보 포착</li>
<li>한국어와 같은 교착어에 효과적</li>
</ul></li>
<li>기존 방법의 한계
<ul>
<li><p>Word2Vec/GloVe 문제:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 훈련 데이터에 없는 단어 (OOV)</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="co">"사과"</span> → [<span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.1</span>, ...]  ✓ (학습됨)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co">"사과들"</span> → ???  ✗ (학습 안됨)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co">"사과나무"</span> → ???  ✗ (학습 안됨)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>한국어의 특별한 어려움:</p>
<pre><code>"먹다" → "먹는다", "먹었다", "먹고", "먹어서", "먹지만", ...
수천 가지 변형이 가능하지만 모두 같은 어근 "먹"을 공유</code></pre></li>
<li><p>FastText의 해결책: Subword 분해</p>
<ul>
<li>Character n-gram 분해
<ul>
<li>예시: “사과” (n=2,3으로 설정)</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co">"사과"</span> 분해:</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">2</span><span class="op">-</span>gram: <span class="st">"&lt;사"</span>, <span class="st">"사과"</span>, <span class="st">"과&gt;"</span>  <span class="co"># &lt;, &gt;는 단어 경계 표시</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">3</span><span class="op">-</span>gram: <span class="st">"&lt;사과"</span>, <span class="st">"사과&gt;"</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 전체 단어: <span class="st">"사과"</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>최종 n<span class="op">-</span>gram 집합: {<span class="st">"&lt;사"</span>, <span class="st">"사과"</span>, <span class="st">"과&gt;"</span>, <span class="st">"&lt;사과"</span>, <span class="st">"사과&gt;"</span>, <span class="st">"사과"</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>벡터 표현</p>
<ul>
<li><p>기존 Word2Vec:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>vector(<span class="st">"사과"</span>) <span class="op">=</span> lookup_table[<span class="st">"사과"</span>]  <span class="co"># 하나의 벡터</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>FastText:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>vector(<span class="st">"사과"</span>) <span class="op">=</span> vector(<span class="st">"&lt;사"</span>) <span class="op">+</span> vector(<span class="st">"사과"</span>) <span class="op">+</span> vector(<span class="st">"과&gt;"</span>) <span class="op">+</span> </span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    vector(<span class="st">"&lt;사과"</span>) <span class="op">+</span> vector(<span class="st">"사과&gt;"</span>) <span class="op">+</span> vector(<span class="st">"사과"</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># n-gram 벡터들의 합</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><p>OOV 문제 해결 과정</p>
<ul>
<li>새로운 단어 처리</li>
</ul>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 훈련 시 보지 못한 단어: "사과나무"</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1단계: n-gram 분해</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co">"사과나무"</span> → {<span class="st">"&lt;사"</span>, <span class="st">"사과"</span>, <span class="st">"과나"</span>, <span class="st">"나무"</span>, <span class="st">"무&gt;"</span>, <span class="st">"&lt;사과"</span>, <span class="st">"사과나"</span>, <span class="st">"과나무"</span>, <span class="st">"나무&gt;"</span>, <span class="st">"사과나무"</span>}</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 2단계: 학습된 n-gram 벡터 찾기</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co">"&lt;사"</span>: [<span class="fl">0.1</span>, <span class="fl">0.2</span>, ...]     ✓ (있음)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co">"사과"</span>: [<span class="fl">0.3</span>, <span class="fl">0.1</span>, ...]    ✓ (있음)  </span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="co">"과나"</span>: [<span class="fl">0.0</span>, <span class="fl">0.0</span>, ...]    ✗ (없음 → <span class="dv">0</span><span class="er">벡터</span>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="co">"나무"</span>: [<span class="fl">0.2</span>, <span class="fl">0.4</span>, ...]    ✓ (있음)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="co">"무&gt;"</span>: [<span class="fl">0.1</span>, <span class="fl">0.1</span>, ...]     ✓ (있음)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 3단계: 합계 계산</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>vector(<span class="st">"사과나무"</span>) <span class="op">=</span> <span class="bu">sum</span>(존재하는_ngram_벡터들) <span class="op">/</span> 개수</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>결과: “사과나무”는 “사과”와 “나무”의 의미를 모두 반영한 벡터를 얻음!</li>
</ul></li>
<li><p>형태학적 정보 포착</p>
<ul>
<li>한국어 예시</li>
</ul>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>훈련 데이터:</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="co">"먹는다"</span> → {<span class="st">"&lt;먹"</span>, <span class="st">"먹는"</span>, <span class="st">"는다"</span>, <span class="st">"다&gt;"</span>, ...}</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co">"먹었다"</span> → {<span class="st">"&lt;먹"</span>, <span class="st">"먹었"</span>, <span class="st">"었다"</span>, <span class="st">"다&gt;"</span>, ...}  </span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co">"먹고"</span> → {<span class="st">"&lt;먹"</span>, <span class="st">"먹고"</span>, <span class="st">"고&gt;"</span>, ...}</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 공통 n-gram "&lt;먹", "먹"이 반복 학습됨</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co"># → "먹" 관련 의미가 강화됨</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>새로운 단어 <span class="st">"먹거나"</span>:</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>{<span class="st">"&lt;먹"</span>, <span class="st">"먹거"</span>, <span class="st">"거나"</span>, <span class="st">"나&gt;"</span>, ...}</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="co"># "&lt;먹" n-gram을 통해 "먹다"와 관련된 의미를 자동으로 얻음!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>영어 예시</li>
</ul>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co">"running"</span> ↔ <span class="st">"run"</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co">"running"</span> → {<span class="st">"ru"</span>, <span class="st">"un"</span>, <span class="st">"nn"</span>, <span class="st">"ni"</span>, <span class="st">"in"</span>, <span class="st">"ng"</span>, <span class="st">"run"</span>, <span class="st">"unn"</span>, <span class="st">"nni"</span>, ...}</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co">"run"</span> → {<span class="st">"ru"</span>, <span class="st">"un"</span>, <span class="st">"run"</span>, ...}</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>공통 부분: {<span class="st">"ru"</span>, <span class="st">"un"</span>, <span class="st">"run"</span>}을 통해 관계 학습</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>실제 성능 비교</p>
<ul>
<li>한국어 형태소 분석 없이도 효과적</li>
<li><strong>Word2Vec</strong>:</li>
</ul>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co">"좋다"</span>: [<span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.1</span>, ...]</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="co">"좋은"</span>: ??? (없으면 <span class="op">&lt;</span>UNK<span class="op">&gt;</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="co">"좋아서"</span>: ??? (없으면 <span class="op">&lt;</span>UNK<span class="op">&gt;</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>FastText</strong>:</li>
</ul>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co">"좋다"</span>: [<span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.1</span>, ...]</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co">"좋은"</span>: [<span class="fl">0.18</span>, <span class="fl">0.38</span>, <span class="fl">0.12</span>, ...]  <span class="co"># "좋" n-gram으로 유추</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co">"좋아서"</span>: [<span class="fl">0.19</span>, <span class="fl">0.39</span>, <span class="fl">0.11</span>, ...]  <span class="co"># "좋" n-gram으로 유추</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>결과: 형태소 분석기 없이도 어근의 의미를 공유하는 벡터들을 얻을 수 있음!</li>
<li>Trade-off:
<ul>
<li>장점: OOV 해결, 형태학적 정보 포착</li>
<li>단점: n-gram 개수만큼 파라미터 증가</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Word2Vec: 단어 수 × 벡터 차원</p></li>
<li><p>FastText: (단어 수 + 모든_ngram_수) × 벡터 차원</p></li>
</ul>
<p><strong>실용적 해결책</strong>: 빈도가 낮은 n-gram은 제외하여 크기 조절</p>
</section>
</section>
<section id="결론" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="결론"><span class="header-section-number">2.1</span> 결론</h2>
<p>본 문서에서는 자연어 처리(NLP) 분야에서 텍스트 데이터의 의미를 효과적으로 포착하기 위해 통계 기반 방법의 한계를 넘어, 신경망은 단어와 문맥의 복잡한 관계를 학습하여 풍부한 정보를 담은 벡터 표현을 생성한다.</p>
<ul>
<li><strong>워드 임베딩의 발전</strong>:
<ul>
<li><strong>정적 임베딩 (Word2Vec, GloVe, FastText)</strong>: ’분포 가설’에 기반하여 단어를 저차원 밀집 벡터로 표현함으로써 단어 간 의미적 유사성과 관계(예: 유추)를 포착했다. <code>Embedding Layer</code>는 이러한 변환의 핵심이며, FastText는 하위 단어(subword) 정보를 활용하여 OOV 문제와 형태론적 특징 처리에 강점을 보였다.</li>
<li>이러한 초기 신경망 기반 방법들은 단어의 의미를 고정된 벡터로 표현하여 NLP 성능을 크게 향상시켰다.</li>
</ul></li>
<li><strong>벡터화 방법 선택의 중요성</strong>:
<ul>
<li>단순한 단어 유사도 측정부터 복잡한 문서 이해 및 생성에 이르기까지, 해결하고자 하는 문제의 특성, 데이터의 규모와 성격, 그리고 사용하려는 모델의 요구사항을 종합적으로 고려하여 적절한 벡터화 전략을 선택하는 것이 중요하다.</li>
<li>이러한 신경망 기반 벡터화 기법들은 현대 대규모 언어 모델(LLM) 발전의 핵심적인 토대가 되었으며, 자연어 이해 및 생성 능력의 비약적인 발전을 이끌고 있다.</li>
</ul></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>