<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.543">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kwangmin Kim">
<meta name="dcterms.date" content="2025-01-21">
<meta name="description" content="ELMo, BERT, GPT, T5, LLaMA 등 주요 사전 학습 모델들의 발전 과정과 핵심 원리를 다룬다. 문맥 기반 임베딩부터 대규모 언어 모델까지, 각 모델의 혁신적 기여와 특징을 상세히 설명한다.">

<title>Kwangmin Kim - 사전 학습 모델의 발전</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete-preset-algolia.umd.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "algolia": {
    "application-id": "DUOR1DRC9D",
    "search-only-api-key": "f264da5dea684ffb9e9b4a574af3ed61",
    "index-name": "prod_QUARTO",
    "analytics-events": true,
    "show-logo": true,
    "libDir": "site_libs"
  },
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": true,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js"></script>


<script type="text/javascript">
var ALGOLIA_INSIGHTS_SRC = "https://cdn.jsdelivr.net/npm/search-insights/dist/search-insights.iife.min.js";
!function(e,a,t,n,s,i,c){e.AlgoliaAnalyticsObject=s,e[s]=e[s]||function(){
(e[s].queue=e[s].queue||[]).push(arguments)},i=a.createElement(t),c=a.getElementsByTagName(t)[0],
i.async=1,i.src=n,c.parentNode.insertBefore(i,c)
}(window,document,"script",ALGOLIA_INSIGHTS_SRC,"aa");
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/autocomplete-plugin-algolia-insights">

</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6W0EKFMWBN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6W0EKFMWBN', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../../../../../styles.css">
<meta property="og:title" content="Kwangmin Kim - 사전 학습 모델의 발전">
<meta property="og:description" content="ELMo, BERT, GPT, T5, LLaMA 등 주요 사전 학습 모델들의 발전 과정과 핵심 원리를 다룬다. 문맥 기반 임베딩부터 대규모 언어 모델까지, 각 모델의 혁신적 기여와 특징을 상세히 설명한다.">
<meta property="og:site_name" content="Kwangmin Kim">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html"> 
<span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"> <i class="bi bi-github" role="img" aria-label="Github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"> <i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#요약" id="toc-요약" class="nav-link active" data-scroll-target="#요약"><span class="header-section-number">1</span> 요약</a></li>
  <li><a href="#텍스트-인코딩-및-벡터화" id="toc-텍스트-인코딩-및-벡터화" class="nav-link" data-scroll-target="#텍스트-인코딩-및-벡터화"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</a></li>
  <li><a href="#사전-학습-모델-pre-trained-model" id="toc-사전-학습-모델-pre-trained-model" class="nav-link" data-scroll-target="#사전-학습-모델-pre-trained-model"><span class="header-section-number">3</span> 사전 학습 모델 (Pre-trained Model)</a>
  <ul class="collapse">
  <li><a href="#elmoembeddings-from-language-models.-2017" id="toc-elmoembeddings-from-language-models.-2017" class="nav-link" data-scroll-target="#elmoembeddings-from-language-models.-2017"><span class="header-section-number">3.1</span> ELMo(Embeddings from Language Models. 2017)</a></li>
  <li><a href="#transformer-2017" id="toc-transformer-2017" class="nav-link" data-scroll-target="#transformer-2017"><span class="header-section-number">3.2</span> Transformer (2017)</a></li>
  <li><a href="#gptgenerative-pre-trained-transformer.-2018" id="toc-gptgenerative-pre-trained-transformer.-2018" class="nav-link" data-scroll-target="#gptgenerative-pre-trained-transformer.-2018"><span class="header-section-number">3.3</span> GPT(Generative Pre-trained Transformer. 2018)</a></li>
  <li><a href="#bertbidirectional-encoder-representations-from-transformers.-2018" id="toc-bertbidirectional-encoder-representations-from-transformers.-2018" class="nav-link" data-scroll-target="#bertbidirectional-encoder-representations-from-transformers.-2018"><span class="header-section-number">3.4</span> BERT(Bidirectional Encoder Representations from Transformers. 2018)</a></li>
  <li><a href="#bartbidirectional-and-auto-regressive-transformers.-2019" id="toc-bartbidirectional-and-auto-regressive-transformers.-2019" class="nav-link" data-scroll-target="#bartbidirectional-and-auto-regressive-transformers.-2019"><span class="header-section-number">3.5</span> BART(Bidirectional and Auto-Regressive Transformers. 2019)</a></li>
  <li><a href="#t5-text-to-text-transfer-transformer.-2020" id="toc-t5-text-to-text-transfer-transformer.-2020" class="nav-link" data-scroll-target="#t5-text-to-text-transfer-transformer.-2020"><span class="header-section-number">3.6</span> T5 (Text-to-Text Transfer Transformer. 2020)</a></li>
  <li><a href="#llamalarge-language-model-meta-ai.-2023" id="toc-llamalarge-language-model-meta-ai.-2023" class="nav-link" data-scroll-target="#llamalarge-language-model-meta-ai.-2023"><span class="header-section-number">3.7</span> LLaMA(Large Language Model Meta AI. 2023)</a></li>
  <li><a href="#flan-fine-tuned-language-net-2022" id="toc-flan-fine-tuned-language-net-2022" class="nav-link" data-scroll-target="#flan-fine-tuned-language-net-2022"><span class="header-section-number">3.8</span> FLAN (Fine-tuned LAnguage Net, 2022)</a></li>
  <li><a href="#ul2unifying-language-learning.-2023" id="toc-ul2unifying-language-learning.-2023" class="nav-link" data-scroll-target="#ul2unifying-language-learning.-2023"><span class="header-section-number">3.9</span> UL2(Unifying Language Learning. 2023)</a></li>
  <li><a href="#결론" id="toc-결론" class="nav-link" data-scroll-target="#결론"><span class="header-section-number">3.10</span> 결론</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">사전 학습 모델의 발전</h1>
<p class="subtitle lead">ELMo부터 LLaMA까지: 현대 NLP의 혁신적 변화</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>ELMo, BERT, GPT, T5, LLaMA 등 주요 사전 학습 모델들의 발전 과정과 핵심 원리를 다룬다. 문맥 기반 임베딩부터 대규모 언어 모델까지, 각 모델의 혁신적 기여와 특징을 상세히 설명한다.</p>
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kwangmin Kim </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 자연어 처리 분야에서 패러다임 전환을 이끈 <strong>사전 학습 모델(Pre-trained Model)</strong>들의 발전 과정과 핵심 원리를 탐구한다. 2015년 Google의 LSTM 사전 학습 실험부터 2023년 LLaMA까지, 각 모델이 가져온 혁신적 변화와 기술적 특징을 상세히 설명한다.</p>
<p>주요 내용은 다음과 같다:</p>
<ul>
<li><strong>사전 학습의 개념과 발전</strong>:
<ul>
<li>초기에는 Word2Vec, GloVe 같은 정적 임베딩에서 시작</li>
<li>Google의 LSTM 사전 학습 실험(2015)이 사전 학습의 효과를 입증</li>
<li>대규모 데이터로 미리 학습한 모델이 무작위 초기화보다 우수한 성능 확인</li>
</ul></li>
<li><strong>문맥 기반 임베딩의 등장</strong>:
<ul>
<li><strong>ELMo (2017)</strong>: BiLSTM 기반 양방향 문맥 임베딩의 선구자</li>
<li>동일한 단어라도 문맥에 따라 다른 벡터 표현 생성</li>
<li>“bank”가 금융기관과 강가에서 서로 다른 의미로 표현되는 혁신</li>
</ul></li>
<li><strong>Transformer 아키텍처의 혁명</strong>:
<ul>
<li><strong>Transformer (2017)</strong>: Self-Attention과 Position Encoding으로 순차 처리 방식 탈피</li>
<li>병렬 처리 가능, 장거리 의존성 포착 능력 향상</li>
<li>현대 모든 대규모 언어 모델의 기초 구조 제공</li>
</ul></li>
<li><strong>특화 모델들의 분화</strong>:
<ul>
<li><strong>GPT (2018)</strong>: Transformer 디코더 기반 생성 특화 모델</li>
<li><strong>BERT (2018)</strong>: Transformer 인코더 기반 이해 특화 모델</li>
<li><strong>BART (2019)</strong>: 인코더-디코더 결합으로 이해와 생성 모두 강화</li>
<li><strong>T5 (2020)</strong>: 모든 NLP 태스크를 텍스트-투-텍스트로 통합</li>
</ul></li>
<li><strong>최신 발전 동향</strong>:
<ul>
<li><strong>LLaMA (2023)</strong>: 효율적인 대규모 언어 모델의 새로운 표준</li>
<li><strong>UL2 (2023)</strong>: 다양한 사전 학습 방식의 융합</li>
<li><strong>FLAN (2022)</strong>: Instruction Tuning을 통한 명령어 이해 능력 강화</li>
</ul></li>
</ul>
<p>각 모델의 핵심 아이디어, 학습 방식, 활용 분야를 통해 현대 NLP 기술의 발전 궤적과 미래 방향성을 이해할 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>RNN Language Model
├── Seq2Seq
├── Beam Search
├── Subword Tokenization
├── Attention
├── Transformer Encoder (Vaswani et al., 2017)
|   ├── Positional Encoding
|   ├── Multi-Head Attention
|   └── Feed Forward Neural Network
|
├── Transformer Decoder (Vaswani et al., 2017)
|
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
|
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
|
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등</code></pre>
</section>
<section id="사전-학습-모델-pre-trained-model" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 사전 학습 모델 (Pre-trained Model)</h1>
<ul>
<li>원래는 언어모델보다는 word embedding에서 사용되던 개념이었음</li>
<li>사전 훈련된 임베딩 (Word2Vec, GloVe)은 대용량 텍스트의 단어들의 동시 등장 통계로부터 훈련시키는 방법</li>
<li>미리 학습시켜 놓은 모델을 가리고 새로운 문제를 풀었었음</li>
<li>Google의 LSTM 사전학습 실험 (Semi-Supervised Sequence Learning, 2015)
<ul>
<li>LSTM 언어 모델을 사전 학습한 후에 텍스트 분류에 적용해봄</li>
<li>LSTM을 사전 학습하지 않은 상태에서 텍스트 분류를 학습한 것과 성능 비교</li>
<li>사전 학습된 LSTM이 random 초기화된 LSTM보다 성능이 좋았음</li>
</ul></li>
</ul>
<section id="elmoembeddings-from-language-models.-2017" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="elmoembeddings-from-language-models.-2017"><span class="header-section-number">3.1</span> ELMo(Embeddings from Language Models. 2017)</h2>
<ul>
<li>사전 학습된 LSTM 언어 모델 2가지를 결합하여 좋은 임베딩 벡터값을 얻는 방법론</li>
<li>사전 학습된 언어 모델이 NLP에서 좋은 성능을 얻을 수 있다는 강한 인상을 줌</li>
<li>ELMo는 문장에서 단어의 의미를 상황에 맞게 다르게 표현해주는 단어 임베딩 기법</li>
<li>예를 들어,
<ul>
<li>“He went to the bank to deposit money.”</li>
<li>“She sat by the bank of the river.”</li>
<li>여기서 bank는 같은 철자지만 의미가 전혀 다르지만 기존 방식(Word2Vec, GloVe)은 이걸 같은 의미로 취급</li>
<li>그런데 ELMo는 문맥을 보고 각각 다른 벡터로 표현</li>
</ul></li>
<li>동작 방식
<ul>
<li>문장을 왼쪽에서 읽는 모델 + 오른쪽에서 읽는 모델 두 개를 활용하여 해당 단어가 문장 속에서 어떤 의미로 쓰였는지를 분석</li>
<li>문장을 양방향으로 읽어 단어의 문맥 정보를 파악함
<ul>
<li>앞에서부터 → 순방향 LSTM</li>
<li>뒤에서부터 → 역방향 LSTM</li>
<li>순방향에서의 bank에 대한 hidden state와 역방향에서의 bank에 대한 hidden state를 결합하여 임베딩을 생성</li>
</ul></li>
<li>모든 단어의 의미는 “문맥 기반”
<ul>
<li>“bank”가 앞뒤에 어떤 단어들과 쓰였는지 보면서 이게 ’돈 관련 은행’인지, ’강가’인지 판단함</li>
</ul></li>
<li>그리고, 임베딩을 뽑음
<ul>
<li>이렇게 판단한 의미를 벡터(숫자 집합)로 표현</li>
</ul></li>
</ul></li>
<li>ELMo는 등장하자마자 기존 NLP 모델들의 정확도를 확 뛰어넘었다</li>
<li>개체명 인식(NER), 질의응답(QA), 문장 분류 등에 광범위하게 쓰였다</li>
<li>지금은 BERT, GPT 같은 트랜스포머가 주도하고 있지만, ELMo는 문맥 기반 임베딩의 출발점이었음</li>
</ul>
</section>
<section id="transformer-2017" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="transformer-2017"><span class="header-section-number">3.2</span> Transformer (2017)</h2>
<ul>
<li>기존의 RNN, LSTM과 달리 <strong>순서를 따라 처리하지 않고</strong>, 문장 전체를 <strong>한꺼번에 보고</strong> 이해할 수 있도록 만든 모델이다.</li>
<li>Transformer는 두 가지 핵심 구조
<ul>
<li><ol type="1">
<li><strong>Self-Attention</strong></li>
</ol>
<ul>
<li>문장 안에서 <strong>각 단어가 다른 단어들과 얼마나 중요한 관계가 있는지를 계산</strong>한다.</li>
<li>예를 들어, “The cat sat on the mat”에서 “sat”와 “cat” 사이의 연결이 중요하다면, 모델은 그 둘의 관계를 더 강하게 본다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Position Encoding</strong></li>
</ol>
<ul>
<li>Transformer는 순서를 따라 읽지 않기 때문에, <strong>각 단어의 위치 정보</strong>를 따로 추가해줘야 한다.</li>
<li>위치 정보를 더해줘서 “첫 번째 단어”, “두 번째 단어” 등의 순서를 인식하게 한다.</li>
</ul></li>
</ul></li>
<li>Transformer의 구성요소
<ul>
<li><strong>인코더(Encoder)</strong>: 입력 문장을 이해하고 벡터로 변환함</li>
<li><strong>디코더(Decoder)</strong>: 그 벡터를 바탕으로 결과(예: 번역, 요약 등)를 생성함</li>
<li>예를 들어, 영어 문장을 프랑스어로 번역하는 경우,
<ul>
<li>인코더는 영어 문장을 이해하고</li>
<li>디코더는 그것을 프랑스어로 바꾸어 생성한다.</li>
</ul></li>
</ul></li>
<li>Transformer는 다음과 같은 이유로 혁신적이다:
<ul>
<li><strong>병렬처리 가능</strong>: RNN처럼 순서대로 처리하지 않기 때문에 연산 속도가 빠르다.</li>
<li><strong>문맥 파악 능력 향상</strong>: 문장의 멀리 떨어진 단어들 간의 관계도 잘 이해한다.</li>
<li><strong>기반 기술로 발전</strong>: BERT, GPT, T5, LLaMA 등 오늘날의 대부분의 대형 언어모델은 Transformer 기반이다.</li>
</ul></li>
<li>오늘날 대부분의 LLM은 이 구조를 바탕으로 하고 있다.</li>
<li>ELMo는 RNN 구조 기반이고,
<ul>
<li>Transformer는 그 한계를 뛰어넘기 위해 등장한 구조라는 점에서</li>
<li>이 둘의 <strong>패러다임 자체가 다르다</strong>는 것도 같이 기억해두면 좋겠다.</li>
</ul></li>
</ul>
</section>
<section id="gptgenerative-pre-trained-transformer.-2018" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="gptgenerative-pre-trained-transformer.-2018"><span class="header-section-number">3.3</span> GPT(Generative Pre-trained Transformer. 2018)</h2>
<ul>
<li>GPT는 문장을 생성하는 모델이다.</li>
<li>기존 모델들이 문장을 이해하는 데 집중했다면, GPT는 문장을 생성하는 데 집중한다.</li>
<li>OpenAI는 Google의 Transformer을 보고 STM이 사전 학습되어 사용되면 성능이 좋은 것을 확인.<br>
</li>
<li>Transformer의 디코더 구조를 분석하고 다음 단어를 예측하는 모듈에 해당하는 디코더로 사전 학습 언어 모델을 구현했다.</li>
<li>Transformer의 encoder를 버리고 decoder만 사용하여 문장을 생성하는 모델을 만들었다.</li>
<li>동작 방식
<ul>
<li>GPT는 <strong>Transformer의 디코더 구조만</strong> 사용한다.</li>
<li>즉, 문장을 생성하는 데 특화되어 있으며,</li>
<li>문장을 이해하는 인코더 구조는 없다.</li>
<li>핵심 동작 방식
<ul>
<li><strong>좌→우 방향의 언어 생성 학습</strong>
<ul>
<li>문장을 왼쪽에서 오른쪽으로 읽으면서,</li>
<li>다음에 올 단어를 예측하는 방식으로 학습한다.</li>
<li>예:
<ul>
<li>입력: “I want to eat”</li>
<li>목표: 다음 단어가 무엇일까? → “pizza”</li>
<li>이런 식으로 엄청나게 많은 문장 데이터를 통해 <strong>패턴을 학습</strong>한다.</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>사전학습 후 활용</strong>
<ul>
<li>대규모 텍스트로 먼저 학습(Pre-training)</li>
<li>이후 별도 작업(챗봇, 작문, 번역 등)에 맞게 사용(Fine-tuning or Prompting)</li>
</ul></li>
</ul></li>
<li>특징
<ul>
<li><strong>텍스트 생성 능력</strong>이 매우 뛰어남</li>
<li>다양한 태스크를 <strong>명시적 미세조정 없이 프롬프트만으로 해결 가능</strong>
<ul>
<li>이게 GPT 계열 모델의 큰 장점이다 (Few-shot, Zero-shot, etc.)</li>
</ul></li>
<li><strong>문장 완성, 요약, 번역, 창작, 대화 등</strong>
<ul>
<li>생성형 작업에 모두 강하다</li>
</ul></li>
</ul></li>
<li><strong>GPT는 문장을 생성하는 데 특화된 모델</strong>이다.</li>
</ul>
</section>
<section id="bertbidirectional-encoder-representations-from-transformers.-2018" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="bertbidirectional-encoder-representations-from-transformers.-2018"><span class="header-section-number">3.4</span> BERT(Bidirectional Encoder Representations from Transformers. 2018)</h2>
<ul>
<li>Google이 OpenAI의 GPT 모델을 보고 반대로 문장을 이해하는 데 집중하는 모델을 만들었다.</li>
<li>Transformer의 인코더 구조를 사용하여 문장을 이해하는 모델을 만들었다.</li>
<li>NLU가 특화된 부분이기 때문에 Text 분류에서 GPT보다 더 나은 성능을 보인다.</li>
<li>BERT는 문장을 <strong>양방향으로 이해하는</strong> 언어 모델이다.</li>
<li>기존 모델들이 <strong>왼쪽에서 오른쪽</strong> 혹은 <strong>오른쪽에서 왼쪽</strong>으로만 문장을 해석했다면,</li>
<li>BERT는 문장을 <strong>양쪽 방향에서 동시에</strong> 해석한다.</li>
<li>그래서 더 깊은 문맥 이해가 가능하다.</li>
<li>동작 방식
<ul>
<li>BERT는 <strong>Transformer의 인코더 구조만</strong> 사용한다.</li>
<li>즉, 문장을 이해하고 벡터로 바꾸는 데 특화되어 있으며,</li>
<li>문장을 새로 생성하는 디코더 구조는 없다.</li>
<li>핵심 동작 방식은 두 가지 학습 방식으로 이루어진다:
<ul>
<li><strong>Masked Language Modeling (MLM)</strong>
<ul>
<li>입력 문장 중 일부 단어를 가려놓고, 그 단어가 무엇인지 맞히도록 학습한다.</li>
<li>예: “The cat sat on the [MASK].” → 모델은 ’mat’이라고 예측해야 한다.</li>
<li>이를 통해 문장의 앞뒤 <strong>모든 문맥</strong>을 참고해서 단어를 이해하는 법을 배운다.</li>
</ul></li>
<li><strong>Next Sentence Prediction (NSP)</strong>
<ul>
<li>두 문장을 입력한 뒤, 두 번째 문장이 첫 번째 문장의 <strong>진짜 다음 문장인지</strong> 판단하게 학습한다.</li>
<li>예:
<ul>
<li>문장1: “She opened the door.”</li>
<li>문장2: “She picked up the package.” → 연결된 문장 (True)</li>
<li>문장2: “The sun is a star.” → 무관한 문장 (False)</li>
</ul></li>
<li>문장 간의 관계 이해 능력을 키우기 위한 학습이다.</li>
</ul></li>
</ul></li>
</ul></li>
<li>문맥 기반 단어 임베딩 제공
<ul>
<li>같은 단어라도 문맥에 따라 다른 벡터로 표현됨</li>
</ul></li>
<li>사전학습 + 미세조정 구조 (Pretraining + Fine-tuning)
<ul>
<li>대규모 텍스트로 미리 학습해두고,</li>
<li>이후 실제 태스크(NER, 분류, QA 등)에 맞게 추가 학습만 하면 됨.</li>
</ul></li>
<li>BERT는 다양한 NLP 태스크에서 <strong>기록적인 성능 향상</strong>을 이끌었다.</li>
<li>이후 등장한 RoBERTa, ALBERT, DistilBERT, DeBERTa 등은 BERT의 변형이다.</li>
<li>현재 GPT 계열이 생성에 특화되어 있다면,
<ul>
<li><strong>BERT는 이해(이해 기반 태스크)에 특화된 모델</strong>이라고 보면 된다.</li>
</ul></li>
<li>BERT는 <strong>Transformer 인코더 기반의 양방향 문맥 이해 모델</strong>이다.</li>
<li><strong>단어를 문맥 안에서 정확하게 해석</strong>하기 위해 만들어졌다.</li>
<li><strong>문장 분류, 질의응답, 개체명 인식 등 NLP의 다양한 작업에 폭넓게 활용</strong>된다.</li>
</ul>
</section>
<section id="bartbidirectional-and-auto-regressive-transformers.-2019" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="bartbidirectional-and-auto-regressive-transformers.-2019"><span class="header-section-number">3.5</span> BART(Bidirectional and Auto-Regressive Transformers. 2019)</h2>
<ul>
<li>Encoder-Decoder 가 결합된 구조를 사용하여 문장을 이해하고 생성하는 모델을 만들었다.</li>
<li>BART는 <strong>BERT + GPT의 장점</strong>을 결합한 모델로, <strong>이해와 생성 모두에 강한 모델</strong>이다.</li>
<li>BART는 <strong>문장을 이해하고 → 그것을 기반으로 문장을 생성하는 모델</strong>이다.</li>
<li>즉, <strong>BERT처럼 입력을 양방향으로 이해</strong>하고, <strong>GPT처럼 자연스럽게 문장을 생성</strong>한다.</li>
<li>구조적으로는 <strong>Transformer 인코더 + 디코더</strong>를 모두 사용한다.</li>
<li>쉽게 말해 <strong>BERT + GPT를 합친 하이브리드 모델</strong>이다.</li>
<li>동작 방식
<ul>
<li><strong>Denoising Autoencoder</strong>: BART의 학습은 <strong>“노이즈 추가 → 원문 복원”</strong> 방식으로 이루어진다.</li>
<li>입력 문장에 노이즈를 준다</li>
<li>예:
<ul>
<li>원래 문장: <code>The cat sat on the mat.</code></li>
<li>망가뜨린 문장: <code>The [MASK] on the mat.</code> 또는 <code>sat the mat on the cat.</code> (순서 뒤섞기)</li>
</ul></li>
<li>모델은 이 망가진 문장을 보고 <strong>원래 문장을 복원</strong>한다. 즉, 문장을 이해하고, 적절한 형태로 <strong>다시 생성</strong>할 수 있어야 한다.</li>
<li>이 과정을 통해 BART는 <strong>이해 능력</strong>과 <strong>생성 능력</strong>을 동시에 학습하게 된다.</li>
</ul></li>
<li>구성
<ul>
<li><strong>인코더</strong>는 BERT처럼 <strong>양방향 문맥 이해</strong></li>
<li><strong>디코더</strong>는 GPT처럼 <strong>왼쪽→오른쪽 순서대로 문장 생성</strong></li>
</ul></li>
<li>이 구조 덕분에 <strong>복잡한 입력을 해석하고</strong>, 그에 맞는 <strong>정확하고 자연스러운 출력</strong>을 만들어낼 수 있다.</li>
<li>활용 분야
<ul>
<li>BART는 다음과 같은 <strong>생성 기반 작업</strong>에 특히 강하다:
<ul>
<li>텍스트 요약 (Summarization)</li>
<li>문장 생성 (Text Generation)</li>
<li>기계 번역 (Machine Translation)</li>
<li>문법 오류 수정 (Grammatical Error Correction)</li>
<li>질문 생성 (Question Generation)</li>
</ul></li>
</ul></li>
<li>특히 <strong>요약 모델로서 매우 뛰어난 성능</strong>을 보여주었고, Facebook AI에서 개발한 이후 HuggingFace에서도 적극적으로 채택되었다.</li>
</ul>
</section>
<section id="t5-text-to-text-transfer-transformer.-2020" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="t5-text-to-text-transfer-transformer.-2020"><span class="header-section-number">3.6</span> T5 (Text-to-Text Transfer Transformer. 2020)</h2>
<ul>
<li>T5는 모든 NLP 문제를 텍스트 → 텍스트 문제로 바꾸자는 발상에서 출발했다.</li>
<li>즉, 입력도 텍스트, 출력도 텍스트로 통일된 프레임워크를 사용한다.</li>
<li>T5는 구글이 제안한 범용 NLP 모델이다.</li>
<li>자연어처리에서 벌어지는 거의 모든 작업을 <strong>텍스트 입력 → 텍스트 출력</strong>으로 통합하는 것이 핵심이다.</li>
<li>요약, 번역, 문장 분류, 질문 생성, 질의 응답 등 모두 동일한 구조에서 처리 가능하다.</li>
<li>동작 방식
<ul>
<li>T5는 BART처럼 <strong>Transformer 인코더 + 디코더 구조</strong>를 사용한다.</li>
<li>하지만 BART와 달리, <strong>모든 태스크를 통일된 방식으로 표현</strong>하는 철학이 핵심이다.</li>
</ul></li>
<li>예시:
<ul>
<li>문장 분류
<ul>
<li>입력: <code>"sst2 sentence: I love this movie."</code></li>
<li>출력: <code>"positive"</code></li>
</ul></li>
<li>문장 요약
<ul>
<li>입력: <code>"summarize: The cat sat on the mat. It was sleepy."</code></li>
<li>출력: <code>"The cat was sleepy."</code></li>
</ul></li>
<li>질의 응답
<ul>
<li>입력: <code>"question: Where is the Eiffel Tower? context: The Eiffel Tower is in Paris."</code></li>
<li>출력: <code>"Paris"</code></li>
</ul></li>
<li>이처럼 <strong>작업을 구분하는 태그 + 텍스트 입력</strong>을 넣으면 <strong>디코더가 원하는 정답 텍스트를 생성</strong>한다.</li>
</ul></li>
<li>사전학습 방식
<ul>
<li>T5도 BART처럼 <strong>Denoising Autoencoder</strong> 방식으로 학습된다.</li>
<li>하지만 T5는 자체적으로 만든 <strong>Span Corruption</strong>이라는 방식을 쓴다:</li>
<li>문장에서 일부 구간(span)을 가리고, 그 구간을 <code>&lt;extra_id_0&gt;</code>, <code>&lt;extra_id_1&gt;</code> 같은 토큰으로 대체</li>
<li>모델이 이 빈칸들을 복원하게 함</li>
<li>이 과정을 통해 <strong>문장 이해 + 생성 능력</strong>을 동시에 기른다.</li>
</ul></li>
<li>특징
<ul>
<li><strong>모든 NLP 태스크를 텍스트 → 텍스트 문제로 통일</strong></li>
<li>다양한 태스크를 하나의 모델로 처리 가능</li>
<li>프롬프트 기반으로 유연하게 태스크 전환</li>
<li>학습 구조는 BART와 유사하지만, <strong>설계 철학은 더 범용적</strong>임</li>
</ul></li>
<li><strong>T5는 BART와 구조는 유사하나, 태스크 프레임워크가 다르다.</strong></li>
<li><strong>모든 입력과 출력을 텍스트로 다루며, 태스크 이름을 붙여 명시함</strong></li>
<li>따라서 여러 NLP 작업을 하나의 파이프라인에서 처리할 수 있다.</li>
<li>대표적인 범용 자연어 처리 모델 중 하나로, 후속 버전으로 T5.1.1, UL2, FLAN-T5 등이 있다.</li>
</ul>
</section>
<section id="llamalarge-language-model-meta-ai.-2023" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="llamalarge-language-model-meta-ai.-2023"><span class="header-section-number">3.7</span> LLaMA(Large Language Model Meta AI. 2023)</h2>
<ul>
<li>LLaMA는 Meta(구 Facebook)에서 제안한 <strong>대규모 언어 모델 시리즈</strong>이다.</li>
<li>이름은 <strong>Large Language Model Meta AI</strong>의 약자이다.</li>
<li>LLaMA는 범용 텍스트 생성 능력을 목표로 하는 <strong>GPT 계열 디코더 기반 모델</strong>이다.</li>
<li>고성능 언어 모델을 <strong>비교적 작은 파라미터 수로도 구현 가능</strong>하다는 점을 증명하고자 설계되었다.</li>
<li>논문 및 모델은 공개되어 <strong>학계, 오픈소스 커뮤니티에서 널리 활용되고 있음</strong>.</li>
<li>LLaMA는 1~2단계 모델 학습을 통해 **사전학습된 기반 모델 (Base LM)**만 제공하며, <strong>대화, 요약, 추론 등에 맞게 파인튜닝은 사용자가 직접 수행</strong>하도록 설계되어 있다.</li>
<li>동작 방식
<ul>
<li>LLaMA는 GPT처럼 <strong>Transformer 디코더 구조</strong>만 사용한다.</li>
<li>입력을 <strong>왼쪽에서 오른쪽</strong>으로 읽으며 <strong>다음 토큰을 예측</strong>하는 방식으로 작동한다.</li>
<li>학습 시점에 <strong>자기회귀 언어모델 (Autoregressive Language Modeling)</strong> 방식 사용.</li>
</ul></li>
<li>특징
<ul>
<li>GPT처럼 텍스트 생성 중심의 모델이지만, <strong>효율성과 학습 품질 향상에 집중된 다양한 설계 전략</strong>을 포함한다.</li>
<li>예:
<ul>
<li><strong>Norm 위치 변경 (Pre-normalization)</strong></li>
<li><strong>GEGLU 활성화 함수</strong></li>
<li><strong>더 긴 시퀀스 학습 (최대 2,048 토큰)</strong></li>
<li><strong>높은 품질의 텍스트 코퍼스만 선별하여 학습</strong></li>
</ul></li>
<li>성능 대비 <strong>모델 크기 효율이 매우 우수</strong>하여, 작은 파라미터 수로도 <strong>GPT-3 수준의 성능</strong>을 낼 수 있음.</li>
</ul></li>
<li>버전별 주요 모델
<ul>
<li><strong>LLaMA 1 (2023 초)</strong>
<ul>
<li>파라미터 크기: 7B, 13B, 33B, 65B</li>
<li>공개 후 오픈소스 생태계에서 광범위한 활용이 시작됨</li>
</ul></li>
<li><strong>LLaMA 2 (2023 중반)</strong>
<ul>
<li>성능 개선 및 다양한 크기: 7B, 13B, 70B</li>
<li><strong>LLaMA 2-Chat</strong>: 대화용으로 미세조정된 모델</li>
</ul></li>
<li><strong>LLaMA 3 (2024 출시)</strong>
<ul>
<li>Meta가 직접 <strong>SOTA급 성능</strong>을 표방하며 출시</li>
<li>8B, 70B 모델 제공, 대화형 fine-tuning 포함</li>
<li>상용 사용 가능, HuggingFace 등에서 공개됨</li>
</ul></li>
</ul></li>
<li>활용 방식
<ul>
<li>LLaMA는 <strong>기반 모델만 제공</strong>하기 때문에, 대화형 모델로 쓰려면 <strong>Alpaca, Vicuna, OpenChat, Zephyr</strong> 등의 <strong>LoRA 파인튜닝</strong> 모델을 함께 사용하는 경우가 많다.</li>
<li>특히 LLaMA는 <strong>프롬프트 기반 제어</strong>, <strong>추론</strong>, <strong>코드 생성</strong> 등 다양한 태스크에서 활용 가능하며, <strong>고품질 오픈소스 AI 개발의 중심</strong>으로 자리 잡았다.</li>
</ul></li>
<li>LLaMA는 GPT 계열의 디코더 언어 모델이다.</li>
<li><strong>작은 모델 크기로도 높은 성능</strong>을 발휘할 수 있도록 최적화됨.</li>
<li>다양한 오픈소스 프로젝트에서 핵심 기반 모델로 활용됨.</li>
<li>후속 시리즈인 <strong>LLaMA 2, 3</strong>는 대화형 파인튜닝 모델과 함께 상용 및 학술용으로 널리 쓰이고 있음.</li>
</ul>
<p>좋다. 이번에는 <strong>UL2</strong>와 <strong>FLAN</strong>에 대해 각각 T5 형식에 맞춰 설명하겠다. 둘 다 <strong>구글이 T5 이후에 개발한 모델 또는 학습 기법</strong>이며, <strong>텍스트 생성과 이해를 모두 강화</strong>하기 위한 방향성을 가진다.</p>
</section>
<section id="flan-fine-tuned-language-net-2022" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="flan-fine-tuned-language-net-2022"><span class="header-section-number">3.8</span> FLAN (Fine-tuned LAnguage Net, 2022)</h2>
<ul>
<li><strong>FLAN은 “Instruction Tuning”의 대표적 구현</strong>으로, <strong>UL2와 같은 사전학습된 언어 모델에 다양한 명령어(Task Prompt)를 학습시키는 과정</strong>이다.</li>
<li>FLAN은 T5 또는 UL2-T5에 적용되어 등장했으며, FLAN-T5, FLAN-UL2 같은 이름으로 모델이 배포된다.</li>
<li>목표는 <strong>명령어(prompt)를 이해하고 정확히 수행하는 능력 강화</strong>이다.</li>
<li>동작 방식
<ul>
<li>먼저 T5 또는 UL2-T5 모델을 준비한다.</li>
<li>그런 다음 <strong>Instruction Tuning</strong>을 수행한다. → 다양한 NLP 작업(요약, 번역, 추론, QA 등)을 명시적인 지시문 형식으로 학습</li>
</ul></li>
<li>예시:
<ul>
<li>입력: <code>"Translate English to French: I am happy."</code></li>
<li>출력: <code>"Je suis heureux."</code></li>
<li>입력: <code>"Summarize: The sun is hot. It rises in the east."</code></li>
<li>출력: <code>"The sun is hot and rises in the east."</code></li>
</ul></li>
<li>특징
<ul>
<li>다양한 명령어와 태스크를 학습시켜, <strong>프롬프트 이해 능력을 대폭 향상</strong>시킴</li>
<li><strong>Zero-shot / Few-shot</strong> 능력이 크게 향상됨</li>
<li>단순한 텍스트 생성 모델이 아닌 <strong>명령어 기반의 범용 도우미로 진화</strong></li>
</ul></li>
<li>요약
<ul>
<li>FLAN은 모델이 아니라 <strong>Instruction tuning 과정 또는 결과 모델 이름</strong>이다.</li>
<li>일반 언어모델에 <strong>명령어 기반 태스크 학습을 추가한 것</strong></li>
<li>대표적인 모델: <strong>FLAN-T5</strong>, <strong>FLAN-UL2</strong></li>
<li>현재 Gemini, PaLM, ChatGPT 등의 <strong>Instruction Following 능력의 기초가 된 전략</strong></li>
</ul></li>
</ul>
</section>
<section id="ul2unifying-language-learning.-2023" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="ul2unifying-language-learning.-2023"><span class="header-section-number">3.9</span> UL2(Unifying Language Learning. 2023)</h2>
<ul>
<li><strong>UL2는 “Unifying Language Learning”의 약자</strong>로, 구글이 제안한 새로운 <strong>사전학습 방식</strong>이다.</li>
<li>목적은 기존 T5, BERT, GPT 계열 모델들의 <strong>단점을 보완하고 장점을 융합</strong>하는 것에 있다.</li>
<li><strong>이해 중심 태스크와 생성 중심 태스크 모두에 잘 작동</strong>하는 범용 사전학습 전략이다.</li>
<li>UL2는 기존 T5 구조를 사용하지만, <strong>학습 방식(프리트레이닝)이 완전히 다르다.</strong></li>
<li>동작 방식
<ul>
<li>UL2는 한 가지 방식이 아닌 <strong>세 가지 프리트레이닝 모드</strong>를 혼합하여 학습한다:
<ol type="1">
<li><strong>R-denoising (Random span masking)</strong>: T5처럼 일부 span을 가리고 복원</li>
<li><strong>X-denoising (Extreme masking)</strong>: 전체 문장을 거의 다 가리고 생성</li>
<li><strong>Causal LM</strong>: GPT처럼 왼쪽에서 오른쪽으로 생성 (Autoregressive) → 이 세 가지 모드를 비율에 따라 섞어 학습시킴 (Multi-task 사전학습)</li>
</ol></li>
</ul></li>
<li>예시:
<ul>
<li>입력: <code>fill in the blanks: The &lt;extra_id_0&gt; sat on the &lt;extra_id_1&gt;.</code></li>
<li>출력: <code>cat</code>, <code>mat</code></li>
<li>또는 GPT처럼 입력: <code>"The cat sat on"</code> → 출력: <code>" the mat."</code></li>
</ul></li>
<li>특징
<ul>
<li><strong>세 가지 방식의 장점을 융합</strong>하여 → 문장 이해 + 생성 모두에 강함</li>
<li>기존 T5는 디코더에서도 마스킹된 부분을 전부 본다 (비자연스러움) → UL2는 자연스러운 생성을 위해 <strong>Causal 방식도 병행</strong></li>
<li>다양한 태스크에 잘 작동하도록 설계됨</li>
</ul></li>
<li>요약
<ul>
<li>UL2는 모델이 아니라 <strong>사전학습 전략</strong>이다.</li>
<li>기존 Transformer 구조에 적용할 수 있음 (예: T5에 적용하면 UL2-T5)</li>
<li><strong>다양한 프리트레이닝 모드를 섞어 범용성과 자연스러움 극대화</strong></li>
<li>이후 FLAN 및 다양한 구글 LLM 개발의 기반이 됨</li>
</ul></li>
</ul>
</section>
<section id="결론" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="결론"><span class="header-section-number">3.10</span> 결론</h2>
<p>사전 학습 모델의 발전은 자연어 처리 분야에서 가장 중요한 패러다임 변화 중 하나다. 2015년 Google의 LSTM 실험부터 시작된 이 여정은 현재 우리가 사용하는 ChatGPT, Claude, Gemini 등 모든 대규모 언어 모델의 토대가 되었다.</p>
<ul>
<li><strong>기술적 진화의 핵심</strong>:
<ul>
<li><strong>정적 → 동적</strong>: Word2Vec에서 ELMo로의 전환으로 문맥 기반 표현 실현</li>
<li><strong>순차 → 병렬</strong>: Transformer의 등장으로 Self-Attention 기반 병렬 처리 가능</li>
<li><strong>단일 → 통합</strong>: T5의 텍스트-투-텍스트 프레임워크로 모든 NLP 태스크 통합</li>
<li><strong>일반 → 특화</strong>: GPT(생성), BERT(이해), BART(양방향) 등 목적별 특화</li>
</ul></li>
<li><strong>패러다임의 변화</strong>:
<ul>
<li><strong>Pre-training + Fine-tuning</strong>: 대규모 데이터로 사전 학습 후 특정 태스크 미세조정</li>
<li><strong>Transfer Learning</strong>: 학습된 지식을 다양한 하위 태스크로 전이</li>
<li><strong>Few-shot Learning</strong>: GPT 계열에서 보여준 예시 기반 학습 능력</li>
<li><strong>Instruction Following</strong>: FLAN으로 대표되는 명령어 이해 및 수행 능력</li>
</ul></li>
<li><strong>각 모델의 독특한 기여</strong>:
<ul>
<li><strong>ELMo</strong>: 문맥 기반 임베딩의 가능성 입증</li>
<li><strong>Transformer</strong>: 현대 AI의 기초 아키텍처 제공</li>
<li><strong>BERT</strong>: 양방향 문맥 이해의 혁신적 접근</li>
<li><strong>GPT</strong>: 생성형 AI와 In-context Learning의 선구자</li>
<li><strong>T5</strong>: 통합 프레임워크를 통한 범용성 확보</li>
<li><strong>LLaMA</strong>: 효율성과 성능의 균형점 제시</li>
</ul></li>
<li><strong>현재와 미래의 의미</strong>:
<ul>
<li>이들 모델은 단순한 기술적 발전을 넘어 AI와 인간의 상호작용 방식을 근본적으로 변화시켰다</li>
<li>ChatGPT의 성공으로 이어진 생성형 AI 붐의 기술적 토대 제공</li>
<li>언어 이해와 생성 능력의 비약적 향상으로 다양한 실용적 응용 가능</li>
</ul></li>
<li><strong>지속되는 혁신</strong>:
<ul>
<li>모델 크기와 성능의 지속적 확장</li>
<li>효율성과 접근성 개선을 위한 경량화 연구</li>
<li>다중 모달(텍스트, 이미지, 음성) 통합 모델로의 발전</li>
<li>더 나은 Instruction Following과 안전성 확보</li>
</ul></li>
</ul>
<p>사전 학습 모델의 발전은 여전히 진행 중이며, 각 모델이 제시한 핵심 아이디어들은 미래 AI 시스템의 기초가 되고 있다. 이러한 기술적 토대 위에서 더욱 강력하고 유용한 AI 시스템들이 계속 등장할 것으로 예상된다.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>