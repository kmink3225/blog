<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kwangmin Kim">
<meta name="dcterms.date" content="2025-01-15">
<meta name="description" content="RNN을 활용한 언어 모델의 구조와 작동 원리, 그리고 Seq2Seq 모델을 통한 기계 번역의 발전 과정을 다룬다. Teacher Forcing 학습 기법과 Encoder-Decoder 구조의 실제 동작 원리를 상세히 설명한다.">

<title>RNN 기반 언어 모델과 Seq2Seq – Kwangmin Kim</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete-preset-algolia.umd.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "algolia": {
    "application-id": "DUOR1DRC9D",
    "search-only-api-key": "f264da5dea684ffb9e9b4a574af3ed61",
    "index-name": "prod_QUARTO",
    "analytics-events": true,
    "show-logo": true,
    "libDir": "site_libs"
  },
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": true,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js"></script>


<script type="text/javascript">
var ALGOLIA_INSIGHTS_SRC = "https://cdn.jsdelivr.net/npm/search-insights/dist/search-insights.iife.min.js";
!function(e,a,t,n,s,i,c){e.AlgoliaAnalyticsObject=s,e[s]=e[s]||function(){
(e[s].queue=e[s].queue||[]).push(arguments)},i=a.createElement(t),c=a.getElementsByTagName(t)[0],
i.async=1,i.src=n,c.parentNode.insertBefore(i,c)
}(window,document,"script",ALGOLIA_INSIGHTS_SRC,"aa");
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/autocomplete-plugin-algolia-insights">

</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6W0EKFMWBN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6W0EKFMWBN', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../../styles.css">
<meta property="og:title" content="RNN 기반 언어 모델과 Seq2Seq – Kwangmin Kim">
<meta property="og:description" content="RNN을 활용한 언어 모델의 구조와 작동 원리, 그리고 Seq2Seq 모델을 통한 기계 번역의 발전 과정을 다룬다. Teacher Forcing 학습 기법과 Encoder-Decoder 구조의 실제 동작 원리를 상세히 설명한다.">
<meta property="og:site_name" content="Kwangmin Kim">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html"> 
<span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"> <i class="bi bi-github" role="img" aria-label="Github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"> <i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#요약" id="toc-요약" class="nav-link active" data-scroll-target="#요약"><span class="header-section-number">1</span> 요약</a></li>
  <li><a href="#텍스트-인코딩-및-벡터화" id="toc-텍스트-인코딩-및-벡터화" class="nav-link" data-scroll-target="#텍스트-인코딩-및-벡터화"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</a></li>
  <li><a href="#rnn-기반-언어-모델" id="toc-rnn-기반-언어-모델" class="nav-link" data-scroll-target="#rnn-기반-언어-모델"><span class="header-section-number">3</span> RNN 기반 언어 모델</a>
  <ul class="collapse">
  <li><a href="#nlp" id="toc-nlp" class="nav-link" data-scroll-target="#nlp"><span class="header-section-number">3.1</span> NLP</a>
  <ul class="collapse">
  <li><a href="#machine-translation" id="toc-machine-translation" class="nav-link" data-scroll-target="#machine-translation"><span class="header-section-number">3.1.1</span> Machine Translation</a></li>
  <li><a href="#translation-모델-종류" id="toc-translation-모델-종류" class="nav-link" data-scroll-target="#translation-모델-종류"><span class="header-section-number">3.1.2</span> Translation 모델 종류</a></li>
  </ul></li>
  <li><a href="#seq2seq-sequence-to-sequence" id="toc-seq2seq-sequence-to-sequence" class="nav-link" data-scroll-target="#seq2seq-sequence-to-sequence"><span class="header-section-number">3.2</span> Seq2Seq (Sequence to Sequence)</a></li>
  <li><a href="#결론" id="toc-결론" class="nav-link" data-scroll-target="#결론"><span class="header-section-number">3.3</span> 결론</a></li>
  <li><a href="#언어-모델의-수학적-정의" id="toc-언어-모델의-수학적-정의" class="nav-link" data-scroll-target="#언어-모델의-수학적-정의"><span class="header-section-number">3.4</span> 언어 모델의 수학적 정의</a></li>
  <li><a href="#rnn을-이용한-구현" id="toc-rnn을-이용한-구현" class="nav-link" data-scroll-target="#rnn을-이용한-구현"><span class="header-section-number">3.5</span> RNN을 이용한 구현</a>
  <ul class="collapse">
  <li><a href="#실제-동작-예시" id="toc-실제-동작-예시" class="nav-link" data-scroll-target="#실제-동작-예시"><span class="header-section-number">3.5.1</span> 실제 동작 예시</a></li>
  </ul></li>
  <li><a href="#teacher-forcing-교사-강요" id="toc-teacher-forcing-교사-강요" class="nav-link" data-scroll-target="#teacher-forcing-교사-강요"><span class="header-section-number">3.6</span> Teacher Forcing (교사 강요)</a>
  <ul class="collapse">
  <li><a href="#개념과-필요성" id="toc-개념과-필요성" class="nav-link" data-scroll-target="#개념과-필요성"><span class="header-section-number">3.6.1</span> 개념과 필요성</a></li>
  <li><a href="#구현-시-고려사항" id="toc-구현-시-고려사항" class="nav-link" data-scroll-target="#구현-시-고려사항"><span class="header-section-number">3.6.2</span> 구현 시 고려사항</a></li>
  <li><a href="#각-레이어의-상세-구조" id="toc-각-레이어의-상세-구조" class="nav-link" data-scroll-target="#각-레이어의-상세-구조"><span class="header-section-number">3.6.3</span> 각 레이어의 상세 구조</a></li>
  <li><a href="#모델-성능-최적화" id="toc-모델-성능-최적화" class="nav-link" data-scroll-target="#모델-성능-최적화"><span class="header-section-number">3.6.4</span> 모델 성능 최적화</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">RNN 기반 언어 모델과 Seq2Seq</h1>
<p class="subtitle lead">순환 신경망을 이용한 언어 모델링과 기계 번역</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>RNN을 활용한 언어 모델의 구조와 작동 원리, 그리고 Seq2Seq 모델을 통한 기계 번역의 발전 과정을 다룬다. Teacher Forcing 학습 기법과 Encoder-Decoder 구조의 실제 동작 원리를 상세히 설명한다.</p>
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kwangmin Kim </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 15, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>이 문서는 RNN(순환 신경망)을 활용한 언어 모델의 기본 원리와 Seq2Seq 모델을 통한 기계 번역 시스템의 구현 방법을 설명한다. 언어 모델은 이전 단어들의 문맥을 바탕으로 다음에 올 단어를 예측하는 모델로, 자연어 생성과 기계 번역에서 핵심적인 역할을 한다.</p>
<p>주요 내용은 다음과 같다:</p>
<ul>
<li><strong>언어 모델의 기본 개념</strong>:
<ul>
<li>주어진 단어 시퀀스에서 다음 단어의 확률 분포를 예측하는 모델</li>
<li>수식: <span class="math inline">\(P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^{T} P(w_t | w_1, w_2, \ldots, w_{t-1})\)</span></li>
<li>문맥 정보를 활용한 조건부 확률 모델링이 핵심</li>
</ul></li>
<li><strong>RNN 언어 모델 구조</strong>:
<ul>
<li>가변 길이 입력 처리 가능한 순환 구조</li>
<li>각 시점에서 hidden state가 문맥 정보를 누적하여 전달</li>
<li>Embedding → Hidden → Output 레이어로 구성된 표준 구조</li>
</ul></li>
<li><strong>Seq2Seq와 기계 번역</strong>:
<ul>
<li>Encoder-Decoder 구조를 통한 시퀀스 간 변환</li>
<li>Context Vector를 통한 소스 언어 정보의 압축 전달</li>
<li>1950년대 규칙 기반부터 2020년대 Transformer까지의 발전 과정</li>
</ul></li>
<li><strong>Teacher Forcing 학습 기법</strong>:
<ul>
<li>훈련 시 실제 정답 토큰을 디코더 입력으로 사용하는 방법</li>
<li>오류 누적 방지와 학습 안정성 확보</li>
<li>훈련과 테스트 단계의 차이점과 하이퍼파라미터로서의 활용</li>
</ul></li>
</ul>
<p>RNN 언어 모델과 Seq2Seq는 현대 자연어 처리의 기초가 되며, GPT와 같은 최신 언어 모델들의 토대를 제공한다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>RNN Language Model
├── Seq2Seq
├── Beam Search
├── Subword Tokenization
├── Attention
├── Transformer Encoder (Vaswani et al., 2017)
|   ├── Positional Encoding
|   ├── Multi-Head Attention
|   └── Feed Forward Neural Network
|
├── Transformer Decoder (Vaswani et al., 2017)
|
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
|
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
|
├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)
└── 기타 발전 모델
    ├── T5, XLNet, ELECTRA
    └── PaLM, LaMDA, Gemini, Claude 등</code></pre>
</section>
<section id="rnn-기반-언어-모델" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> RNN 기반 언어 모델</h1>
<section id="nlp" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="nlp"><span class="header-section-number">3.1</span> NLP</h2>
<ul>
<li>NLP = NLU + NLG
<ul>
<li>NLU: 자연어 이해</li>
<li>NLG: 자연어 생성
<ul>
<li>Image Captioning</li>
<li>Text Summarization</li>
<li>Text Generation</li>
<li>Text Classification</li>
<li>Chatbot</li>
<li><strong>Neural Machine Translation</strong></li>
</ul></li>
</ul></li>
</ul>
<section id="machine-translation" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="machine-translation"><span class="header-section-number">3.1.1</span> Machine Translation</h3>
<ul>
<li>입력문장(source)을 번역한 출력 문장(target text)을 생성해내는 Task</li>
<li>Brief History
<ul>
<li>RBMT(Rule-based Machine Translation): 1950s, if-statement 기반 알고리즘, 성능 안좋음</li>
<li>EBMT(Example-based Machine Translation): 1980s, 예제 기반 알고리즘, 성능 안좋음</li>
<li>SMT(Statistical Machine Translation): 1990s, 통계 기반 알고리즘, 성능 보통</li>
<li>NMT(Neural Machine Translation): 2010s, 신경망 기반 알고리즘, 성능 비약적 상승</li>
<li>Transformer(Attention is all you need): 2017, 신경망 기반 알고리즘, 성능 매우 좋음</li>
<li>GPT-3(Generative Pre-trained Transformer 3): 2020, 성능 좋음</li>
<li>ChatGPT(Generative Pre-trained Transformer 3): 2022, 성능 매우 좋음</li>
<li>GPT-4(Generative Pre-trained Transformer 4): 2023, 성능 매우 좋음</li>
</ul></li>
<li>NMT(Neural Machine Translation) 성능 향상 이유
<ul>
<li>word embedding으로 인한 continuous representation 활용</li>
<li>기존 SMT가 여러 모듈이 결합된 결과였다면 이제는 end-to-end 모델의 시대
<ul>
<li>모듈 간 의존성 제거: upstream 모듈이 에러가 나면 전체 모델이 에러가 나는 문제 발생</li>
<li>end-to-end 모델 구조 도입: 하나의 모델이 수많은 파라미터 기반으로 오차함수에 의해 알고리즘이 일제히 업데이트 되는 구조</li>
</ul></li>
<li>Attention 으로 인해 길이가 긴 문장 또한 좋은 성능을 보이기 시작</li>
</ul></li>
</ul>
</section>
<section id="translation-모델-종류" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="translation-모델-종류"><span class="header-section-number">3.1.2</span> Translation 모델 종류</h3>
<ul>
<li>Seq2Seq: 2014, 입력 문장을 임베딩 벡터로 변환하여 출력 문장을 생성하는 모델</li>
<li>Transformer: 2017, 입력 문장을 임베딩 벡터로 변환하여 출력 문장을 생성하는 모델</li>
<li>BERT: 2018, 입력 문장을 임베딩 벡터로 변환하여 출력 문장을 생성하는 모델</li>
<li>GPT: 2018, 입력 문장을 임베딩 벡터로 변환하여 출력 문장을 생성하는 모델</li>
</ul>
</section>
</section>
<section id="seq2seq-sequence-to-sequence" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="seq2seq-sequence-to-sequence"><span class="header-section-number">3.2</span> Seq2Seq (Sequence to Sequence)</h2>
<ul>
<li>입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력한다.</li>
<li>구조: Encoder + Decoder
<ul>
<li>Encoder
<ul>
<li>NLU 모델: 입력 시퀀스를 임베딩 벡터로 변환</li>
<li>RNN으로 구성</li>
</ul></li>
<li>Decoder
<ul>
<li>NLG 모델: 출력 시퀀스를 출력 문장으로 변환</li>
<li>RNN으로 구성</li>
</ul></li>
</ul></li>
<li>동작 방식
<ul>
<li><p>Sequence to Sequence Learning with Neural Networks 논문 참조</p>
<ul>
<li>본격적인 신경망 기계 번역기를 제시</li>
<li>서로 다른 2개의 LSTM 아키텍쳐를 각각 인코더-디코더로 사용</li>
</ul></li>
<li><p>예문: 저는 학생입니다.</p>
<pre><code>Encoder-Decoder LSTM 번역 과정
│
├── Encoder Phase (인코딩 단계)
│   ├── Step 1: "저는"
│   │   ├── Input: "저는"
│   │   ├── Initial State: h0=0, c0=0
│   │   ├── Encoder LSTM1 연산
│   │   └── Output: h1, c1 → 다음 단계로 전달
│   │
│   ├── Step 2: "학생"  
│   │   ├── Input: "학생"
│   │   ├── Previous State: h1, c1 (from LSTM1)
│   │   ├── Encoder LSTM2 연산
│   │   └── Output: h2, c2 → 다음 단계로 전달
│   │
│   └── Step 3: "입니다"
│       ├── Input: "입니다"
│       ├── Previous State: h2, c2 (from LSTM2)
│       ├── Encoder LSTM3 연산
│       └── Output: Context Vector (h3, c3) ★ → Decoder로 전달
│
├── Context Transfer (문맥 전달)
│   └── Context Vector (h3, c3) → Decoder 초기 상태 (h_d0, c_d0)
│
└── Decoder Phase (디코딩 단계)
    ├── Step 1: 번역 시작
    │   ├── Input: EOS (번역 시작)
    │   ├── Initial State: h_d0=h3, c_d0=c3 (Context Vector)
    │   ├── Decoder LSTM1 연산
    │   ├── Output: "I"
    │   └── Hidden State: h_d1, c_d1 → 다음 단계로 전달
    │
    ├── Step 2: 첫 번째 단어 생성 후
    │   ├── Input: "I"
    │   ├── Previous State: h_d1, c_d1 (from Decoder LSTM1)
    │   ├── Decoder LSTM2 연산
    │   ├── Output: "am"
    │   └── Hidden State: h_d2, c_d2 → 다음 단계로 전달
    │
    ├── Step 3: 두 번째 단어 생성 후
    │   ├── Input: "am"
    │   ├── Previous State: h_d2, c_d2 (from Decoder LSTM2)
    │   ├── Decoder LSTM3 연산
    │   ├── Output: "a"
    │   └── Hidden State: h_d3, c_d3 → 다음 단계로 전달
    │
    ├── Step 4: 세 번째 단어 생성 후
    │   ├── Input: "a"
    │   ├── Previous State: h_d3, c_d3 (from Decoder LSTM3)
    │   ├── Decoder LSTM4 연산
    │   ├── Output: "student"
    │   └── Hidden State: h_d4, c_d4 → 다음 단계로 전달
    │
    ├── Step 5: 네 번째 단어 생성 후
    │   ├── Input: "student"
    │   ├── Previous State: h_d4, c_d4 (from Decoder LSTM4)
    │   ├── Decoder LSTM5 연산
    │   ├── Output: "."
    │   └── Hidden State: h_d5, c_d5 → 다음 단계로 전달
    │
    └── Step 6: 번역 종료
        ├── Input: "."
        ├── Previous State: h_d5, c_d5 (from Decoder LSTM5)
        ├── Decoder LSTM6 연산
        ├── Output: EOS (번역 종료) ★
        └── Final State: h_d6, c_d6 (번역 완료)</code></pre>
<ul>
<li>각 단어는 워드 임베딩이 된 벡터로 LSTM에 들어가지만 편의상 한글 단어로 표현</li>
<li>context vector
<ul>
<li>encoder 마지막 시점의 hidden state<br>
</li>
<li>인코다가 입력 문장의 모든 단어들을 수찬적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 context vector로 만든다.</li>
<li>즉, context vector는 decoder RNN 셀의 첫 번째 hidden state로 사용된다.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Teacher Forcing
<ul>
<li>Encoder RNN (학습방식) 과 Decoder RNN (Test 방식)의 동작 방식이 다르다.</li>
<li>파파고나 구글 번역기의 기초 동작 메커니즘</li>
<li>훈련 단계에서 Teacher Forcing 을 무조건 하는 것이 아니라 비율을 정해서 수행</li>
<li>이 비율이 hyper parameter 이다.</li>
<li>이 비율을 높게 설정할 수록 빠른 학습이 가능해지지만 overfit되어 테스트 단계에서 악영향을 줄 수 있음</li>
<li>test 단계에서는 teacher forcing을 사용하지 않으며 현 시점의 출력을 다음 시점의 입력으로 사용한다.</li>
</ul></li>
</ul>
</section>
<section id="결론" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="결론"><span class="header-section-number">3.3</span> 결론</h2>
<p>RNN 기반 언어 모델과 Seq2Seq 구조는 자연어 처리 분야에서 패러다임 전환을 이끈 핵심 기술이다. 이들은 기계 번역의 성능을 획기적으로 개선시켰으며, 현대 언어 모델들의 기초 개념을 제공했다.</p>
<ul>
<li><strong>기술적 의의</strong>:
<ul>
<li>가변 길이 시퀀스 처리 능력으로 자연어의 본질적 특성 반영</li>
<li>Encoder-Decoder 구조를 통한 서로 다른 도메인 간 매핑 실현</li>
<li>Context Vector 개념으로 긴 문장의 의미 압축 표현 가능</li>
</ul></li>
<li><strong>Teacher Forcing의 혁신</strong>:
<ul>
<li>훈련과 추론 단계의 차이를 체계적으로 관리하는 방법론 제시</li>
<li>오류 누적 문제 해결로 안정적인 시퀀스 생성 학습 실현</li>
<li>하이퍼파라미터 조정을 통한 성능 최적화 전략 제공</li>
</ul></li>
<li><strong>기계 번역의 발전</strong>:
<ul>
<li>규칙 기반(1950s) → 통계 기반(1990s) → 신경망 기반(2010s)의 발전 과정</li>
<li>End-to-end 학습으로 모듈 간 의존성 문제 해결</li>
<li>Word embedding과 연속 표현의 활용으로 성능 비약적 향상</li>
</ul></li>
<li><strong>현대적 의미</strong>:
<ul>
<li>LSTM, GRU 등 개선된 RNN 변형들의 토대 제공</li>
<li>Attention 메커니즘 도입의 동기와 배경 이해</li>
<li>Transformer, BERT, GPT 등 최신 모델들의 개념적 기초</li>
</ul></li>
</ul>
<p>비록 현재는 Transformer 기반 모델들이 주류를 이루고 있지만, RNN 언어 모델과 Seq2Seq의 핵심 아이디어들은 여전히 많은 NLP 시스템에서 활용되고 있다. 특히 실시간 처리가 중요한 애플리케이션이나 제한된 자원 환경에서는 RNN 기반 모델들이 여전히 유용한 선택지가 되고 있으며, 언어 모델의 기본 원리를 이해하는 데 필수적인 개념으로 남아있다.</p>
</section>
<section id="언어-모델의-수학적-정의" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="언어-모델의-수학적-정의"><span class="header-section-number">3.4</span> 언어 모델의 수학적 정의</h2>
<p>언어 모델은 주어진 단어 시퀀스의 확률을 계산하는 모델이다. <span class="math inline">\(T\)</span>개의 단어로 구성된 문장 <span class="math inline">\(w_1, w_2, \ldots, w_T\)</span>에 대해:</p>
<p><span class="math display">\[P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^{T} P(w_t | w_1, w_2, \ldots, w_{t-1})\]</span></p>
<p>여기서 <span class="math inline">\(w_i\)</span> 는 <span class="math inline">\(i\)</span> 번째 단어이고, <span class="math inline">\(P(w_i | w_1, w_2, \ldots, w_{i-1})\)</span> 는 이전 단어들이 주어졌을 때 현재 단어가 나올 조건부 확률이다.</p>
</section>
<section id="rnn을-이용한-구현" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="rnn을-이용한-구현"><span class="header-section-number">3.5</span> RNN을 이용한 구현</h2>
<p>RNN 언어 모델에서는 각 시점의 hidden state <span class="math inline">\(h_t\)</span>가 지금까지의 모든 단어 정보를 압축하여 저장한다:</p>
<p><span class="math display">\[h_t = f(h_{t-1}, x_t)\]</span> <span class="math display">\[P(w_{t+1} | w_1, \ldots, w_t) = \text{softmax}(W_o h_t + b_o)\]</span></p>
<p>각 시점 <span class="math inline">\(t\)</span> 에서의 계산 과정:</p>
<ol type="1">
<li><strong>입력 임베딩</strong>: <span class="math inline">\(e_t = \text{Embedding}(x_t)\)</span></li>
<li><strong>Hidden State 업데이트</strong>: <span class="math inline">\(h_t = \tanh(W_{hh} h_{t-1} + W_{xh} e_t + b_h)\)</span></li>
<li><strong>출력 계산</strong>: <span class="math inline">\(o_t = W_{ho} h_t + b_o\)</span></li>
<li><strong>확률 분포</strong>: <span class="math inline">\(P(w_{t+1}) = \text{softmax}(o_t)\)</span></li>
</ol>
<section id="실제-동작-예시" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="실제-동작-예시"><span class="header-section-number">3.5.1</span> 실제 동작 예시</h3>
<p>문장 “what will the side effects be?”를 처리하는 과정:</p>
<ul>
<li><span class="math inline">\(x_1 = \text{"what"}\)</span> → <span class="math inline">\(h_1\)</span> 계산 → <span class="math inline">\(P(\text{next word} | \text{"what"})\)</span> → <span class="math inline">\(y_1 = \text{"will"}\)</span></li>
<li><span class="math inline">\(x_2 = \text{"will"}\)</span> → <span class="math inline">\(h_2\)</span> 계산 → <span class="math inline">\(P(\text{next word} | \text{"what will"})\)</span> → <span class="math inline">\(y_2 = \text{"the"}\)</span></li>
<li><span class="math inline">\(x_3 = \text{"the"}\)</span> → <span class="math inline">\(h_3\)</span> 계산 → <span class="math inline">\(P(\text{next word} | \text{"what will the"})\)</span> → <span class="math inline">\(y_3 = \text{"side"}\)</span></li>
<li><span class="math inline">\(x_4 = \text{"side"}\)</span> → <span class="math inline">\(h_4\)</span> 계산 → <span class="math inline">\(P(\text{next word} | \text{"what will the side"})\)</span> → <span class="math inline">\(y_4 = \text{"effects"}\)</span></li>
</ul>
<p>이 과정에서 <span class="math inline">\(h_t\)</span> 는 시점 <span class="math inline">\(t\)</span>까지의 모든 이전 단어들의 문맥 정보를 압축적으로 담고 있다.</p>
</section>
</section>
<section id="teacher-forcing-교사-강요" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="teacher-forcing-교사-강요"><span class="header-section-number">3.6</span> Teacher Forcing (교사 강요)</h2>
<section id="개념과-필요성" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="개념과-필요성"><span class="header-section-number">3.6.1</span> 개념과 필요성</h3>
<p>Teacher Forcing은 sequence-to-sequence 모델의 훈련 과정에서 사용되는 기법이다:</p>
<ul>
<li><strong>훈련 시</strong>: 실제 정답(ground truth) 토큰을 디코더의 다음 입력으로 사용</li>
<li><strong>추론 시</strong>: 모델이 예측한 토큰을 다음 입력으로 사용</li>
</ul>
<p>이는 다음과 같은 문제를 해결한다: - <strong>오류 누적(Error Accumulation)</strong>: 잘못된 예측이 연쇄적으로 더 큰 오류를 만드는 문제 - <strong>학습 불안정성</strong>: 초기 학습 단계에서 무작위 예측으로 인한 학습 어려움 - <strong>수렴 속도</strong>: 올바른 패턴 학습을 위한 가이드 제공</p>
</section>
<section id="구현-시-고려사항" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="구현-시-고려사항"><span class="header-section-number">3.6.2</span> 구현 시 고려사항</h3>
<p><strong>Teacher Forcing 비율 조정</strong>: - 초기 학습: 높은 비율(0.8-1.0)로 안정적 학습 - 후기 학습: 점진적으로 비율 감소(0.3-0.5) - 실제 추론 상황과의 차이를 줄이기 위한 점진적 조정</p>
<p><strong>Scheduled Sampling</strong>: - 훈련 중에도 가끔 예측값을 사용하여 exposure bias 완화 - 커리큘럼 학습의 한 형태로 활용</p>
</section>
<section id="각-레이어의-상세-구조" class="level3" data-number="3.6.3">
<h3 data-number="3.6.3" class="anchored" data-anchor-id="각-레이어의-상세-구조"><span class="header-section-number">3.6.3</span> 각 레이어의 상세 구조</h3>
<section id="embedding-layer" class="level4" data-number="3.6.3.1">
<h4 data-number="3.6.3.1" class="anchored" data-anchor-id="embedding-layer"><span class="header-section-number">3.6.3.1</span> Embedding Layer</h4>
<ul>
<li><strong>역할</strong>: 단어 인덱스를 고정 크기의 벡터로 변환</li>
<li><strong>수식</strong>: <span class="math inline">\(e_t = E[w_t]\)</span>, 여기서 <span class="math inline">\(E \in \mathbb{R}^{V \times d}\)</span></li>
<li><strong>파라미터</strong>:
<ul>
<li><span class="math inline">\(V\)</span>: 어휘 크기 (일반적으로 10,000-50,000개)</li>
<li><span class="math inline">\(d\)</span>: 임베딩 차원수 (일반적으로 100-300차원)</li>
</ul></li>
</ul>
</section>
<section id="rnn-hidden-layer" class="level4" data-number="3.6.3.2">
<h4 data-number="3.6.3.2" class="anchored" data-anchor-id="rnn-hidden-layer"><span class="header-section-number">3.6.3.2</span> RNN Hidden Layer</h4>
<ul>
<li><strong>역할</strong>: 시퀀스의 문맥 정보를 누적하여 표현</li>
<li><strong>수식</strong>: <span class="math inline">\(h_t = \tanh(W_{hh} h_{t-1} + W_{xh} e_t + b_h)\)</span></li>
<li><strong>파라미터</strong>:
<ul>
<li><span class="math inline">\(W_{hh} \in \mathbb{R}^{H \times H}\)</span>: hidden-to-hidden 가중치</li>
<li><span class="math inline">\(W_{xh} \in \mathbb{R}^{H \times d}\)</span>: input-to-hidden 가중치</li>
<li><span class="math inline">\(H\)</span>: 은닉 상태 차원수 (일반적으로 128-512차원)</li>
</ul></li>
</ul>
</section>
<section id="output-layer" class="level4" data-number="3.6.3.3">
<h4 data-number="3.6.3.3" class="anchored" data-anchor-id="output-layer"><span class="header-section-number">3.6.3.3</span> Output Layer</h4>
<ul>
<li><strong>역할</strong>: 은닉 상태를 어휘 크기의 로짓 벡터로 변환</li>
<li><strong>수식</strong>: <span class="math inline">\(o_t = W_{ho} h_t + b_o\)</span></li>
<li><strong>파라미터</strong>: <span class="math inline">\(W_{ho} \in \mathbb{R}^{V \times H}\)</span>, <span class="math inline">\(b_o \in \mathbb{R}^{V}\)</span></li>
</ul>
</section>
<section id="softmax-loss" class="level4" data-number="3.6.3.4">
<h4 data-number="3.6.3.4" class="anchored" data-anchor-id="softmax-loss"><span class="header-section-number">3.6.3.4</span> Softmax &amp; Loss</h4>
<ul>
<li><strong>확률 분포</strong>: <span class="math inline">\(P(w_t | w_{&lt;t}) = \text{softmax}(o_t) = \frac{e^{o_t^{(i)}}}{\sum_{j=1}^{V} e^{o_t^{(j)}}}\)</span></li>
<li><strong>손실 함수</strong>: <span class="math inline">\(\mathcal{L} = -\sum_{t=1}^{T} \log P(w_t^* | w_{&lt;t})\)</span></li>
</ul>
</section>
</section>
<section id="모델-성능-최적화" class="level3" data-number="3.6.4">
<h3 data-number="3.6.4" class="anchored" data-anchor-id="모델-성능-최적화"><span class="header-section-number">3.6.4</span> 모델 성능 최적화</h3>
<p><strong>기울기 소실 문제 해결</strong>: - LSTM, GRU 같은 게이트 메커니즘 도입 - Residual Connection 활용 - 적절한 가중치 초기화 (Xavier, He 초기화)</p>
<p><strong>계산 효율성 개선</strong>: - 배치 처리를 통한 병렬화 - 적응적 softmax (hierarchical softmax) - 어휘 크기 축소 기법 (subword tokenization)</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("kk3225\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>