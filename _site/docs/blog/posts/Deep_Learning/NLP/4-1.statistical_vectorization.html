<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.543">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kwangmin Kim">
<meta name="dcterms.date" content="2025-01-04">
<meta name="description" content="자연어 처리(NLP)에서 텍스트 데이터를 기계가 이해하고 처리할 수 있는 숫자 형태로 변환하는 인코딩 및 벡터화의 주요 개념과 방법들을 살펴본다.">

<title>Kwangmin Kim - 텍스트 인코딩 및 벡터화: NLP 숫자 변환의 모든 것</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete-preset-algolia.umd.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "algolia": {
    "application-id": "DUOR1DRC9D",
    "search-only-api-key": "f264da5dea684ffb9e9b4a574af3ed61",
    "index-name": "prod_QUARTO",
    "analytics-events": true,
    "show-logo": true,
    "libDir": "site_libs"
  },
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": true,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js"></script>


<script type="text/javascript">
var ALGOLIA_INSIGHTS_SRC = "https://cdn.jsdelivr.net/npm/search-insights/dist/search-insights.iife.min.js";
!function(e,a,t,n,s,i,c){e.AlgoliaAnalyticsObject=s,e[s]=e[s]||function(){
(e[s].queue=e[s].queue||[]).push(arguments)},i=a.createElement(t),c=a.getElementsByTagName(t)[0],
i.async=1,i.src=n,c.parentNode.insertBefore(i,c)
}(window,document,"script",ALGOLIA_INSIGHTS_SRC,"aa");
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/autocomplete-plugin-algolia-insights">

</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6W0EKFMWBN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6W0EKFMWBN', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../../styles.css">
<meta property="og:title" content="Kwangmin Kim - 텍스트 인코딩 및 벡터화: NLP 숫자 변환의 모든 것">
<meta property="og:description" content="자연어 처리(NLP)에서 텍스트 데이터를 기계가 이해하고 처리할 수 있는 숫자 형태로 변환하는 인코딩 및 벡터화의 주요 개념과 방법들을 살펴본다.">
<meta property="og:site_name" content="Kwangmin Kim">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html"> 
<span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"> <i class="bi bi-github" role="img" aria-label="Github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"> <i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#이-문서-한눈에-보기" id="toc-이-문서-한눈에-보기" class="nav-link active" data-scroll-target="#이-문서-한눈에-보기"><span class="header-section-number">1</span> 이 문서 한눈에 보기</a></li>
  <li><a href="#텍스트-인코딩-및-벡터화" id="toc-텍스트-인코딩-및-벡터화" class="nav-link" data-scroll-target="#텍스트-인코딩-및-벡터화"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</a>
  <ul class="collapse">
  <li><a href="#인코딩encoding" id="toc-인코딩encoding" class="nav-link" data-scroll-target="#인코딩encoding"><span class="header-section-number">2.1</span> 인코딩(Encoding)</a>
  <ul class="collapse">
  <li><a href="#정수-인코딩-integer-encoding" id="toc-정수-인코딩-integer-encoding" class="nav-link" data-scroll-target="#정수-인코딩-integer-encoding"><span class="header-section-number">2.1.1</span> 정수 인코딩 (Integer Encoding)</a></li>
  <li><a href="#oov-out-of-vocabulary-문제" id="toc-oov-out-of-vocabulary-문제" class="nav-link" data-scroll-target="#oov-out-of-vocabulary-문제"><span class="header-section-number">2.1.2</span> OOV (Out-of-Vocabulary) 문제</a></li>
  </ul></li>
  <li><a href="#패딩-padding" id="toc-패딩-padding" class="nav-link" data-scroll-target="#패딩-padding"><span class="header-section-number">2.2</span> 패딩 (Padding)</a></li>
  <li><a href="#벡터화-vectorization" id="toc-벡터화-vectorization" class="nav-link" data-scroll-target="#벡터화-vectorization"><span class="header-section-number">2.3</span> 벡터화 (Vectorization)</a>
  <ul class="collapse">
  <li><a href="#통계적-방법" id="toc-통계적-방법" class="nav-link" data-scroll-target="#통계적-방법"><span class="header-section-number">2.3.1</span> 통계적 방법</a></li>
  </ul></li>
  <li><a href="#결론" id="toc-결론" class="nav-link" data-scroll-target="#결론"><span class="header-section-number">2.4</span> 결론</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">텍스트 인코딩 및 벡터화: NLP 숫자 변환의 모든 것</h1>
<p class="subtitle lead">정수 인코딩, OOV 처리, 패딩부터 원-핫, BoW, TF-IDF까지 핵심 개념 총정리</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>자연어 처리(NLP)에서 텍스트 데이터를 기계가 이해하고 처리할 수 있는 숫자 형태로 변환하는 인코딩 및 벡터화의 주요 개념과 방법들을 살펴본다.</p>
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kwangmin Kim </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 4, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="이-문서-한눈에-보기" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 이 문서 한눈에 보기</h1>
<p>이 문서는 자연어 처리(NLP)를 위해 텍스트 데이터를 기계가 이해할 수 있는 숫자 형태로 변환하는 주요 과정인 <strong>인코딩(Encoding)</strong>과 <strong>벡터화(Vectorization)</strong>에 대해 심층적으로 다룬다.</p>
<p>주요 내용은 다음과 같다.</p>
<ul>
<li><strong>인코딩 (Encoding)</strong>:
<ul>
<li><strong>정의 및 필요성</strong>: 텍스트를 숫자(주로 정수 인덱스)로 변환하는 과정이다.</li>
<li><strong>정수 인코딩</strong>: 각 단어에 고유 정수를 부여하는 방법과 어휘 집합 크기 결정, 그리고 학습 데이터에 없는 단어(OOV: Out-of-Vocabulary)를 <code>UNK</code> 토큰으로 처리하는 방법을 설명한다.</li>
</ul></li>
<li><strong>패딩 (Padding)</strong>:
<ul>
<li><strong>정의 및 필요성</strong>: 길이가 다른 텍스트 시퀀스들을 모델 입력을 위해 동일한 길이로 맞춰주는 작업으로, <code>PAD</code> 토큰을 사용한다.</li>
</ul></li>
<li><strong>벡터화 (Vectorization)</strong>:
<ul>
<li><strong>정의</strong>: 정수 인코딩된 데이터를 숫자 벡터로 변환하여 텍스트의 의미나 통계적 정보를 표현한다.</li>
<li><strong>통계적 방법</strong>:
<ul>
<li><strong>원-핫 인코딩</strong>: 각 단어를 고유한 희소 벡터로 표현하며, 장단점(차원의 저주)을 다룬다.</li>
<li><strong>빈도 기반 방법 (DTM, BoW, TF-IDF)</strong>: 문서 내 단어 빈도를 기반으로 문서를 벡터화하는 DTM(Document Term Matrix), Bag-of-Words(BoW), TF-IDF 기법의 개념과 특징, 활용 방안을 설명한다.</li>
</ul></li>
<li><strong>벡터 표현의 단위</strong>: 단어는 벡터로, 문서는 벡터 또는 행렬로 표현될 수 있음을 설명한다.</li>
</ul></li>
<li><strong>결론</strong>: 효과적인 인코딩 및 벡터화 전략 선택의 중요성을 강조한다.</li>
</ul>
<p>이 문서를 통해 텍스트 데이터가 NLP 모델에서 어떻게 처리될 수 있도록 준비되는지에 대한 기본적인 이해를 얻을 수 있다.</p>
</section>
<section id="텍스트-인코딩-및-벡터화" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 텍스트 인코딩 및 벡터화</h1>
<pre><code>텍스트 벡터화
├── 1. 전통적 방법 (통계 기반)
│   ├── BoW
│   ├── DTM
│   └── TF-IDF
│
├── 2. 신경망 기반 (문맥 독립)
│   ├── 문맥 독립적 임베딩
│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)
│   ├── Word2Vec (CBOW, Skip-gram)
│   ├── FastText
│   ├── GloVe
│   └── 기타 모델: Swivel, LexVec 등
│
└── 3. 문맥 기반 임베딩 (Contextual Embedding)
    ├── RNN 계열
    │   ├── LSTM
        ├── GRU
    │   └── ELMo
    ├── Attention 메커니즘
    │   ├── Basic Attention
    │   ├── Self-Attention
    │   └── Multi-Head Attention
    └── Transformer 계열
        ├── BERT, RoBERTa, ALBERT
        ├── GPT 시리즈
        ├── KoBERT, KoGPT 등 한국어 특화
        └── 기타 모델: T5, LaMDA, PaLM, XLNet, ELECTRA</code></pre>
<section id="인코딩encoding" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="인코딩encoding"><span class="header-section-number">2.1</span> 인코딩(Encoding)</h2>
<ul>
<li><strong>인코딩 (Encoding)</strong>:
<ul>
<li><strong>정의</strong>: 자연어 처리(NLP)에서 <strong>텍스트 데이터를 기계가 이해하고 처리할 수 있는 숫자 형태로 변환하는 과정</strong> 전반을 의미. 컴퓨터는 텍스트를 직접 이해할 수 없으므로, 토큰화된 각 구성 요소(단어 등)를 숫자 표현(주로 정수 인덱스)으로 바꾸는 단계.</li>
<li><strong>핵심</strong>: 토큰화 후, 토큰들을 숫자로 매핑.</li>
</ul></li>
<li><strong>주요 기법</strong>: 정수 인코딩 결과를 바탕으로 한 원-핫 인코딩, 그리고 더 나아가 단어 임베딩 등.</li>
</ul>
<section id="정수-인코딩-integer-encoding" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="정수-인코딩-integer-encoding"><span class="header-section-number">2.1.1</span> 정수 인코딩 (Integer Encoding)</h3>
<ul>
<li><strong>개념</strong>: 어휘 집합(Vocabulary) 내 각 고유 토큰에 <strong>고유한 정수 인덱스를 부여</strong>.</li>
<li><strong>과정</strong>:
<ol type="1">
<li>어휘 집합 구축: 전체 텍스트에서 고유 토큰 추출</li>
</ol>
<ul>
<li>빈도, 최대 크기 고려하여 어휘 집합 크기 제한하는 것이 현실적인 전략</li>
</ul>
<ol start="2" type="1">
<li>각 토큰에 정수 할당</li>
</ol>
<ul>
<li>빈도 높은 단어에 낮은 숫자 할당</li>
</ul>
<ol start="3" type="1">
<li>텍스트의 토큰 시퀀스를 정수 시퀀스로 변환</li>
</ol></li>
<li><strong>예시</strong>:
<ul>
<li>어휘 집합 구축:
<ul>
<li>예시 문장: <code>"I love natural language processing"</code></li>
<li>토큰화 결과: <code>["i", "love", "natural", "language", "processing"]</code></li>
<li>초기 어휘 집합 (<code>word_to_index</code>)의 정수 인코딩 * <code>{"i": 1, "love": 2, "natural": 3, "language": 4, "processing": 5}</code></li>
<li>정수 시퀀스: <code>[1, 2, 3, 4, 5]</code></li>
<li>이때 어휘 집합의 크기는 5이다.</li>
</ul></li>
</ul></li>
<li><strong>한계</strong>
<ul>
<li>텍스트간 유사도 측정 불가: 숫자 값 자체가 단어 간 의미/관계 표현 못 함 (모델 오해 가능성).</li>
</ul></li>
</ul>
</section>
<section id="oov-out-of-vocabulary-문제" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="oov-out-of-vocabulary-문제"><span class="header-section-number">2.1.2</span> OOV (Out-of-Vocabulary) 문제</h3>
<ul>
<li><strong>정의</strong>: 학습 시 구축된 어휘 집합에 <strong>포함되지 않은 단어</strong>가 입력될 때 발생.</li>
<li><strong>원인</strong>: 신조어, 전문 용어, 오타, 제한된 어휘 크기 등.</li>
<li><strong>영향</strong>: 정보 손실, 잘못된 예측, 모델 성능 저하.</li>
<li><strong><code>UNK</code> (Unknown) 토큰 처리</strong>:
<ul>
<li>OOV 단어를 미리 정의된 <code>UNK</code> 토큰의 인덱스로 일괄 대체.</li>
<li>모든 OOV가 동일 토큰으로 매핑되어 원래 단어의 고유 정보는 손실.</li>
</ul></li>
<li><strong>어휘 집합 크기 및 신규 단어(OOV) 처리 예시 (영문)</strong>:
<ul>
<li>어휘 집합의 크기는 모델의 성능과 효율성에 영향을 미치며, 너무 작으면 OOV 문제가, 너무 크면 계산 비용 및 과적합 문제가 발생할 수 있다.</li>
<li><strong>신규 단어(OOV) 발생 시 <code>UNK</code> 토큰 처리</strong>:
<ul>
<li>기존 예시 문장: <code>"I love natural language processing"</code></li>
<li>새로운 문장: <code>"I also love deep learning"</code> 이 입력되었다고 가정</li>
<li>이 문장의 토큰: <code>["i", "also", "love", "deep", "learning"]</code></li>
<li>여기서 “also”, “deep”, “learning”은 기존 어휘 집합에 없는 새로운 단어(OOV)이다.</li>
<li>이런 OOV 단어를 처리하기 위해, 특별 토큰인 <code>"UNK"</code> (Unknown)를 어휘 집합에 추가하고, 이 <code>"UNK"</code> 토큰에 어휘 집합의 <strong>가장 마지막 다음 번호</strong>를 부여</li>
<li>업데이트된 어휘 집합 (<code>word_to_index</code>): <code>{"i": 1, "love": 2, "natural": 3, "language": 4, "processing": 5, "UNK": 6}</code></li>
<li>어휘 집합의 크기: 6 (<code>UNK</code> 토큰 포함).</li>
<li>새로운 문장 <code>"I also love deep learning"</code>의 정수 시퀀스</li>
<li>OOV 단어인 “also”, “deep”, “learning”은 <code>"UNK"</code> 토큰의 인덱스인 6으로 매핑되어 <code>[1, 6, 2, 6, 6]</code> 이 된다.</li>
<li>이렇게 <code>UNK</code> 토큰을 사용하면 모델이 학습하지 않은 단어에 대해서도 일관된 처리가 가능하지만, 모든 OOV 단어가 하나의 인덱스로 매핑되므로 원래 단어의 정보는 일부 손실된다.</li>
</ul></li>
</ul></li>
<li><strong>근본적 해결 시도</strong>: 서브워드 토큰화 (BPE, WordPiece 등)는 단어를 더 작은 단위로 나눠 OOV 발생 빈도를 크게 줄임.</li>
</ul>
</section>
</section>
<section id="패딩-padding" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="패딩-padding"><span class="header-section-number">2.2</span> 패딩 (Padding)</h2>
<ul>
<li><strong>개념</strong>: 입력되는 텍스트 길이가 다르기 때문에 토큰화 후 서로 다른 길이의 정수 시퀀스들의 <strong>길이를 동일하게 맞춰주는</strong> 작업.</li>
<li><strong>필요성</strong>: 딥러닝 모델의 고정된 입력 크기 요구 충족, 배치 단위 병렬 처리 효율 증대.</li>
<li><strong><code>PAD</code> 토큰 사용</strong>: 특별한 <code>PAD</code> 토큰 (주로 인덱스 0)을 사용.</li>
<li><strong>방법</strong>:
<ul>
<li><code>post-padding</code> (뒷부분 채움): 시퀀스 뒤에 <code>PAD</code> 인덱스 추가. (일반적)
<ul>
<li>예: <code>[[1,2,3,4], [5,6]]</code> -&gt; <code>maxlen=4</code> 가정 시 <code>[[1,2,3,4], [5,6,0,0]]</code>, 집합의 크기는 4</li>
</ul></li>
<li><code>pre-padding</code> (앞부분 채움): 시퀀스 앞에 <code>PAD</code> 인덱스 추가. (RNN 계열에서 마지막 정보 중요시할 때)
<ul>
<li>예: <code>[[1,2,3,4], [5,6]]</code> -&gt; <code>maxlen=4</code> 가정 시 <code>[[1,2,3,4], [0,0,5,6]]</code>, 집합의 크기는 4</li>
</ul></li>
</ul></li>
<li><strong>어휘 집합 크기</strong>: <code>PAD</code> 토큰 사용 시 어휘 집합 크기에 영향</li>
<li><strong>주의</strong>: 과도한 패딩은 실제 정보 비율 낮춰 학습에 부정적 영향 가능.</li>
</ul>
</section>
<section id="벡터화-vectorization" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="벡터화-vectorization"><span class="header-section-number">2.3</span> 벡터화 (Vectorization)</h2>
<ul>
<li><strong>정의</strong>: 정수 인코딩 등 숫자 표현을 바탕으로, 각 텍스트 단위(단어, 문장, 문서)를 <strong>숫자 벡터(Numeric Vector)로 변환</strong>하는 과정.</li>
<li><strong>목적</strong>: 기계 학습 모델 처리 가능 형태 변환, 텍스트의 의미/문맥 정보 표현, 데이터 효율적 처리.</li>
</ul>
<section id="통계적-방법" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="통계적-방법"><span class="header-section-number">2.3.1</span> 통계적 방법</h3>
<ul>
<li>신경망 미사용: 신경망을 사용하지 않는 전통적 통계 방식으로 벡터화</li>
<li>문맥 미고려 방법 (Non-neural / Context-independent)</li>
<li>각 단어를 주변 문맥과 독립적으로 고정된 벡터로 표현 (Sparse Vector).</li>
</ul>
<section id="단어-벡터-표현-방법-원-핫-인코딩-one-hot-encoding" class="level4" data-number="2.3.1.1">
<h4 data-number="2.3.1.1" class="anchored" data-anchor-id="단어-벡터-표현-방법-원-핫-인코딩-one-hot-encoding"><span class="header-section-number">2.3.1.1</span> 단어 벡터 표현 방법: 원-핫 인코딩 (One-Hot Encoding)</h4>
<ul>
<li>어휘 집합 크기의 벡터에서, 해당 단어의 정수 인덱스 위치만 1이고 나머지는 모두 0인 벡터로 표현.</li>
<li>예: 어휘집 <code>{"apple":0, "banana":1, "cherry":2, "PAD":3}</code> (4은 패딩용 가정), <code>vocab_size=4</code></li>
<li><code>apple</code> (인덱스 0) -&gt; <code>[1, 0, 0, 0]</code></li>
<li><code>banana</code> (인덱스 1) -&gt; <code>[0, 1, 0, 0]</code></li>
<li><code>cherry</code> (인덱스 2) -&gt; <code>[0, 0, 1, 0]</code></li>
<li><code>PAD</code> (인덱스 3) -&gt; <code>[0, 0, 0, 1]</code></li>
<li>장점: 단어 간 순서/크기에 의한 관계 없음 명확히 표현.</li>
<li>단점:</li>
<li>차원의 저주: 어휘 집합 크면 벡터 차원 매우 커짐 (희소 벡터) -&gt; 계산 비효율, 데이터 부족 문제.
<ul>
<li>차원의 저주란? 고차원 공간(많은 feature 또는 여기서는 매우 큰 어휘 집합으로 인한 고차원 벡터)으로 갈수록 데이터 포인트들이 해당 공간을 매우 드문드문(희소하게, sparsely) 채우게 되는 현상</li>
<li>희소성의 문제점:
<ul>
<li><strong>거리 계산의 무의미화</strong>: 고차원 공간에서는 대부분의 데이터 포인트들이 서로 멀리 떨어져 있게 되어, 유클리드 거리와 같은 전통적인 거리 척도가 의미를 잃어갑니다. 즉, 가장 가까운 이웃과 가장 먼 이웃 간의 거리 차이가 거의 없어지거나, 모든 점이 샘플링된 점들의 껍질(hull)에 가깝게 위치한다. 이는 최근접 이웃(Nearest Neighbor)과 같은 거리 기반 알고리즘의 성능을 저하시킵니다.</li>
<li><strong>데이터 부족 심화</strong>: 동일한 밀도로 데이터를 채우기 위해서는 차원이 증가할수록 기하급수적으로 더 많은 데이터가 필요합니다. 예를 들어, 1차원에서 10개의 구간을 커버하는데 10개의 데이터 포인트가 필요했다면, 10차원에서는 각 차원마다 10개의 구간을 커버하기 위해 (10^{10})개의 데이터 포인트가 필요하게 된다. 현실적으로 이만큼의 데이터를 확보하기는 매우 어렵다.</li>
<li><strong>과적합(Overfitting) 가능성 증가</strong>: 제한된 데이터로 고차원 모델을 학습시키면, 모델이 실제 데이터의 분포보다는 학습 데이터의 노이즈에 과도하게 적응하여 새로운 데이터에 대한 일반화 성능이 떨어질 수 있다.</li>
<li><strong>모델 학습의 어려움</strong>: 데이터가 희소해지면, 의미 있는 패턴을 찾거나 변수 간의 관계를 모델링하는 것이 더욱 어려워지고, 모델의 복잡도에 비해 학습할 수 있는 정보가 부족해집니다.</li>
<li><strong>단어 간 유사도 표현 불가</strong>: 모든 단어 벡터 직교.</li>
</ul></li>
</ul></li>
<li><strong>최근 동향</strong>: 단점들로 인해 NLP 딥러닝에서는 단어 임베딩으로 대체되는 추세.</li>
</ul>
</section>
<section id="문서-벡터-표현-방법-빈도-기반-방법-frequency-based-methods" class="level4" data-number="2.3.1.2">
<h4 data-number="2.3.1.2" class="anchored" data-anchor-id="문서-벡터-표현-방법-빈도-기반-방법-frequency-based-methods"><span class="header-section-number">2.3.1.2</span> 문서 벡터 표현 방법: 빈도 기반 방법 (Frequency-based Methods)</h4>
<ul>
<li>Document Term Matrix (DTM): 텍스트 마이닝과 자연어처리에서 문서 내 단어 빈도를 표현하는 <strong>문서-단어 행렬의 데이터 구조</strong>이다.
<ul>
<li>벡터가 단어 집합의 크기를 가지며 대부분의 원소가 0을 가진다.</li>
<li>DTM의 기본 구조
<ul>
<li><strong>행(rows)</strong>: 각 문서 (document), 즉, 문서가 행벡터가 된다.</li>
<li><strong>열(columns)</strong>: 어휘집(vocabulary)의 각 단어 (term)</li>
<li><strong>셀 값</strong>: 해당 문서에서 해당 단어의 빈도 또는 가중치</li>
</ul></li>
<li>수학적으로 표현하면 <span class="math inline">\(M \in \mathbb{R}^{d \times v}\)</span> 형태의 행렬이며, 여기서 <span class="math inline">\(d\)</span> 는 문서 수, <span class="math inline">\(v\)</span> 는 어휘집 크기</li>
</ul></li>
<li>DTM(행렬)의 셀 값을 채우는 방법
<ul>
<li><strong>Bag-of-Words (BoW)</strong>:
<ul>
<li>Raw Count (단순 빈도): 단어를 한 가방에 넣고 흔들면 단어의 순서는 무의미해지기 때문에 단어가 등장한 빈도수를 벡터화하는 이론</li>
<li>표현: 각 문서는 어휘 집합 크기의 벡터로, 각 차원은 해당 단어의 빈도수 (또는 존재 유무 0/1).</li>
<li>예: 문서1: “나는 바나나 사과 바나나”, 어휘집: <code>{"나는":0, "바나나":1, "사과":2}</code> -&gt; <code>[1, 2, 1]</code></li>
<li>장점: 단순하고 구현 용이.</li>
<li>단점: 어순 무시로 문맥 정보 손실, 단어 의미 모호성 해결 불가, 차원이 크고 희소(sparse)할 수 있음.</li>
<li>DTM[i,j] = count(word_j in document_i)</li>
</ul></li>
<li>Binary (이진)
<ul>
<li>DTM[i,j] = 1 if word_j appears in document_i, else 0</li>
<li>{“데이터”: 1, “과학”: 1, “분석”: 1}</li>
</ul></li>
<li><strong>TF (Term Frequency)</strong>
<ul>
<li>특정 문서 내 특정 단어의 등장 빈도를 정규화하여 BoW로 표현된 벡터에 가중치를 주는 방법</li>
<li>DTM[i,j] = TF(word_j, document_i)</li>
<li>{“데이터”: 2/4=0.5, “과학”: 1/4=0.25, “분석”: 1/4=0.25}</li>
</ul></li>
<li><strong>TF-IDF (Inverse Document Frequency)</strong>
<ul>
<li>여러 문서가 있을때 단어의 변별력을 측정하는 지표</li>
<li>문서의 유사도, 검색 시스템에서 검색 결과의 순위 등을 구하는데 사용</li>
<li>단어 빈도(TF)와 역문서 빈도(IDF)를 곱하여, 특정 문서 내 단어의 상대적 중요도를 가중치로 부여.</li>
<li>통계적 방법 (신경망 미사용)론 중 여전히 실무에서 괴장히 많이 쓰이는 벡터화 방법.<br>
</li>
<li>결과값이 벡터이므로 신경망의 입력값으로도 사용될 수 있다.</li>
<li>문서를 벡터화 한다면 문서 간 유사도 구할 수 있다.</li>
<li>문서 간 유사도 구하면 가능한 작업
<ul>
<li>문서 클러스터링, 유사한 문서 찾기, 문서 분류 문제</li>
</ul></li>
<li>IDF: 특정 문서에만 자주 나오는 단어일수록 높은 값을 가짐. 전체 문서 중 해당 단어가 등장한 문서 수의 역수</li>
<li>예: “the” 같이 여러 문서에 자주 나오는 단어는 낮은 TF-IDF, 특정 주제 문서에만 나오는 전문용어는 높은 TF-IDF.</li>
<li>장점: BoW보다 단어의 중요도를 더 잘 반영 (예: 불용어의 영향력 감소).</li>
<li>단점: 어순 무시, 의미적 유사도 표현에는 여전히 한계.</li>
<li><span class="math inline">\(\text{IDF}(w_j) = \log\left(\frac{N}{1+\text{df}(w_j)}\right)\)</span></li>
<li>여기서:
<ul>
<li><span class="math inline">\(N\)</span>: 전체 문서 수</li>
<li><span class="math inline">\(\text{df}(w_j)\)</span>: 단어 <span class="math inline">\(w_j\)</span>가 등장한 문서 수</li>
<li>특정 단어가 문서2,3에서 각 각 등장할 때 특정단어가 문서2에서 10000번 등장했더라도 df의 값은 2가 된다.</li>
<li>log 스케일: df값이 매우 크거나 작을 때 IDF값의 범위를 줄이기 위해 사용 (IDF값이 기하급수적으로 커질 수 있음)</li>
<li>불용어 등과 같이 자주 쓰이는 단어들은 비교적 자주 쓰이지 않는 단어들보다 최소 수십배 더 자주 등장한다.</li>
<li>비교적 자주 쓰이지 않는 단어들조차 희귀 단어들과 비교하면 최소 수백 배는 더 자주 등장하는 편이다. (is, a, I, the, and, …)</li>
<li>log를 씌우지 않으면 희귀 단어들에 엄청난 가중치가 부여될 위험이 있다.</li>
</ul></li>
<li>높은 IDF: 적은 문서에만 등장 → 희귀하고 특별한 단어</li>
<li>낮은 IDF: 많은 문서에 등장 → 일반적이고 흔한 단어</li>
</ul></li>
</ul></li>
<li>계산 예시</li>
</ul>
<pre><code>문서1: "머신러닝 알고리즘 연구"
문서2: "딥러닝 모델 개발" 
문서3: "데이터 과학 연구"
문서4: "인공지능 연구 동향"

"연구": df=4 → IDF = log(4/4) = 0  (낮음 - 흔한 단어)
"머신러닝": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)
"딥러닝": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)
"데이터": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)
"알고리즘": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)

TF만 사용할 경우 (문서1 기준):
"연구": TF = 1/3 = 0.333
"머신러닝": TF = 1/3 = 0.333
"알고리즘": TF = 1/3 = 0.333   

IDF (문서1 기준):
"연구": IDF = log(4/4) = 0
"머신러닝": IDF = log(4/2) = 0.693
"알고리즘": IDF = log(4/2) = 0.693         

TF-IDF 사용할 경우 (문서1 기준):
"연구": TF × IDF = 0.333 × 0 = 0  (낮음 - 일반적 단어)
"머신러닝": TF × IDF = 0.333 × 0.693 = 0.231  (높음 - 문서의 특징)
"알고리즘": TF × IDF = 0.333 × 0.693 = 0.231  (높음 - 문서의 특징)
→ 문서를 특징짓는 단어들이 강조됨</code></pre>
<ul>
<li>구체적 예시
<ul>
<li><strong>DTM with BoW:</strong>
<ul>
<li><p>문서 <span class="math inline">\(D_i\)</span> 에 대해 BoW 벡터 <span class="math inline">\(\mathbf{x}_i = [x_{i1}, x_{i2}, ..., x_{iv}]\)</span> 는 다음과 같이 정의: <span class="math inline">\(x_{ij} = \text{count}(w_j, D_i)\)</span></p></li>
<li><p>여기서 <span class="math inline">\(\text{count}(w_j, D_i)\)</span>는 문서 <span class="math inline">\(D_i\)</span>에서 단어 <span class="math inline">\(w_j\)</span>의 등장 횟수이다.</p></li>
<li><p><strong>문서 집합:</strong></p>
<ul>
<li>문서1: “데이터 과학은 흥미로운 분야다”</li>
<li>문서2: “머신러닝은 데이터 과학의 핵심이다”<br>
</li>
<li>문서3: “딥러닝도 머신러닝의 한 분야다”</li>
</ul></li>
<li><p><strong>어휘집 구성:</strong> {“데이터”, “과학은”, “흥미로운”, “분야다”, “머신러닝은”, “과학의”, “핵심이다”, “딥러닝도”, “머신러닝의”, “한”}</p></li>
<li><p><strong>DTM (BoW):</strong></p></li>
</ul>
<pre><code>         데이터 과학은 흥미로운 분야다 머신러닝은 과학의 핵심이다 딥러닝도 머신러닝의 한
문서1        1     1      1      1       0      0      0      0       0    0
문서2        1     0      0      0       1      1      1      0       0    0  
문서3        0     0      0      1       0      0      0      1       1    1</code></pre>
<ul>
<li><strong>통계적 특성</strong>
<ul>
<li><strong>희소성(Sparsity)</strong>: 대부분의 셀이 0인 희소 행렬</li>
<li><strong>차원의 저주</strong>: 어휘집 크기가 클수록 벡터 차원이 증가</li>
<li><strong>코사인 유사도</strong>: 문서 간 유사도 측정에 자주 사용 <span class="math display">\[\text{cosine\_similarity}(D_i, D_j) = \frac{\mathbf{x}_i \cdot \mathbf{x}_j}{||\mathbf{x}_i|| \cdot ||\mathbf{x}_j||}\]</span></li>
</ul></li>
</ul></li>
<li><strong>DTM with TF-IDF</strong>
<ul>
<li>TF-IDF는 두 구성요소의 곱으로 정의:
<ul>
<li><span class="math inline">\(\text{TF-IDF}(w_j, D_i) = \text{TF}(w_j, D_i) \times \text{IDF}(w_j)\)</span></li>
</ul></li>
<li>Term Frequency (TF)
<ul>
<li><span class="math inline">\(\text{TF}(w_j, D_i) = \frac{\text{count}(w_j, D_i)}{\sum_{k=1}^{|D_i|} \text{count}(w_k, D_i)}\)</span></li>
<li>또는 로그 정규화 (가장 보편적으로 사용):
<ul>
<li><span class="math inline">\(\text{TF}(w_j, D_i) = \log(1 + \text{count}(w_j, D_i))\)</span></li>
</ul></li>
</ul></li>
<li>Inverse Document Frequency (IDF)
<ul>
<li><span class="math inline">\(\text{IDF}(w_j) = \log\left(\frac{N}{\text{df}(w_j)}\right)\)</span></li>
<li>여기서:
<ul>
<li><span class="math inline">\(N\)</span> : 전체 문서 수</li>
<li><span class="math inline">\(\text{df}(w_j)\)</span> : 단어 <span class="math inline">\(w_j\)</span>가 등장한 문서 수</li>
</ul></li>
</ul></li>
<li>예시 (위의 동일한 문서 집합 사용):
<ul>
<li><p><strong>1단계: TF 계산 (로그 정규화 사용)</strong></p>
<ul>
<li><p>TF(“데이터”) = 1/4 = 0.25</p></li>
<li><p>TF(“과학은”) = 1/4 = 0.25<br>
</p></li>
<li><p>TF(“흥미로운”) = 1/4 = 0.25</p></li>
<li><p>TF(“분야다”) = 1/4 = 0.25</p></li>
<li><p>TF(“머신러닝은”) = 1/4 = 0.25</p></li>
<li><p>TF(“데이터”) = 1/4 = 0.25</p></li>
<li><p>TF(“과학의”) = 1/4 = 0.25<br>
</p></li>
<li><p>TF(“핵심이다”) = 1/4 = 0.25</p></li>
<li><p>TF(“딥러닝도”) = 1/4 = 0.25</p></li>
<li><p>TF(“머신러닝의”) = 1/4 = 0.25</p></li>
<li><p>TF(“한”) = 1/4 = 0.25</p></li>
<li><p>TF(“분야다”) = 1/4 = 0.25</p>
<pre><code>         데이터  과학은  흥미로운  분야다  머신러닝은  과학의  핵심이다  딥러닝도  머신러닝의  한
문서1      0.25    0.25    0.25    0.25     0       0       0       0       0       0
문서2      0.25    0       0       0      0.25     0.25    0.25     0        0      0
문서3       0       0       0      0.25     0        0       0     0.25    0.25  0.25</code></pre></li>
</ul></li>
<li><p><strong>2단계: IDF 계산</strong></p>
<ul>
<li>“데이터”: 문서1, 문서2 → df = 2</li>
<li>“분야다”: 문서1, 문서3 → df = 2<br>
</li>
<li>“과학은”: 문서1만 → df = 1</li>
<li>“흥미로운”: 문서1만 → df = 1</li>
<li>“머신러닝은”: 문서2만 → df = 1</li>
<li>“과학의”: 문서2만 → df = 1</li>
<li>“핵심이다”: 문서2만 → df = 1</li>
<li>“딥러닝도”: 문서3만 → df = 1</li>
<li>“머신러닝의”: 문서3만 → df = 1<br>
</li>
<li>“한”: 문서3만 → df = 1</li>
<li>IDF(“데이터”) = log(3/(1+2)) = log(1) = 0.000</li>
<li>IDF(“분야다”) = log(3/(1+2)) = log(1) = 0.000</li>
<li>IDF(“과학은”) = log(3/(1+1)) = log(1.5) = 0.405</li>
<li>IDF(“흥미로운”) = log(3/(1+1)) = log(1.5) = 0.405<br>
</li>
<li>IDF(“머신러닝은”) = log(3/(1+1)) = log(1.5) = 0.405</li>
<li>IDF(“과학의”) = log(3/(1+1)) = log(1.5) = 0.405</li>
<li>IDF(“핵심이다”) = log(3/(1+1)) = log(1.5) = 0.405</li>
<li>IDF(“딥러닝도”) = log(3/(1+1)) = log(1.5) = 0.405</li>
<li>IDF(“머신러닝의”) = log(3/(1+1)) = log(1.5) = 0.405</li>
<li>IDF(“한”) = log(3/(1+1)) = log(1.5) = 0.405</li>
</ul></li>
<li><p><strong>3단계: TF-IDF 최종 계산</strong></p>
<pre><code>         데이터   과학은   흥미로운  분야다   머신러닝은  과학의   핵심이다  딥러닝도  머신러닝의   한
문서1      0     0.101    0.101   0.101      0        0        0       0        0      0
문서2      0       0        0       0       0.081    0.081    0.081    0        0      0
문서3      0       0        0       0        0        0        0     0.081    0.081  0.081</code></pre></li>
</ul></li>
<li>TF-IDF의 통계적 해석
<ul>
<li><strong>정보 이론적 관점</strong>: IDF는 단어의 정보량(information content)을 측정
<ul>
<li><span class="math inline">\(\text{Information}(w_j) = -\log P(w_j) \approx -\log\left(\frac{\text{df}(w_j)}{N}\right) = \text{IDF}(w_j)\)</span></li>
</ul></li>
<li><strong>가중치 효과</strong>:
<ul>
<li>높은 TF: 해당 문서에서 <strong>중요한 단어</strong></li>
<li>높은 IDF: 전체 문서 집합에서 <strong>희귀한 단어</strong></li>
<li>높은 TF-IDF: 특정 문서의 <strong>특징을 잘 나타내는 중요한 단어</strong></li>
</ul></li>
<li><strong>정규화 효과</strong>: 문서 길이에 따른 편향을 줄임</li>
</ul></li>
</ul></li>
<li>실용적 고려사항
<ul>
<li><strong>불용어 처리</strong>: “은”, “는”, “이” 등은 보통 전처리 단계에서 제거</li>
<li><strong>최소 문서 빈도</strong>: 너무 희귀한 단어들을 필터링하여 차원 축소</li>
<li><strong>최대 문서 빈도</strong>: 너무 일반적인 단어들도 제외 가능</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="결론" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="결론"><span class="header-section-number">2.4</span> 결론</h2>
<p>본 문서에서는 자연어 처리(NLP)의 핵심 전처리 단계인 텍스트 인코딩과 벡터화 기법들을 살펴보았다. 주요 내용을 요약하면 다음과 같다.</p>
<ul>
<li><strong>텍스트 인코딩의 중요성</strong>: 기계가 텍스트를 이해하기 위한 첫걸음으로, 정수 인코딩과 같은 방법을 통해 토큰을 숫자 표현으로 변환합니다. 이때 어휘 집합에 없는 단어(OOV) 처리가 중요하며, <code>UNK</code> 토큰 사용이 일반적인 해결책이다.</li>
<li><strong>일관된 입력 처리를 위한 패딩</strong>: 모델의 고정된 입력 크기 요구를 충족시키기 위해, 다양한 길이의 시퀀스를 <code>PAD</code> 토큰을 사용하여 동일한 길이로 맞추는 패딩 작업이 필수적이다.</li>
<li><strong>다양한 벡터화 기법</strong>:
<ul>
<li><strong>통계 기반 방법</strong>: 원-핫 인코딩은 간단하지만 차원의 저주 문제가 있으며, 단어 빈도 기반의 DTM, BoW, TF-IDF 등은 문서 수준의 벡터 표현에 효과적이다. 특히 TF-IDF는 단어의 중요도를 반영하여 널리 사용된다.</li>
<li><strong>벡터 표현의 의미</strong>: 이러한 기법들은 단어를 벡터로, 문서를 벡터 또는 행렬로 변환하여 기계 학습 모델이 처리할 수 있도록 한다.</li>
</ul></li>
<li><strong>방법 선택의 기준</strong>: 최적의 인코딩 및 벡터화 전략은 당면한 문제의 특성, 데이터의 양과 질, 그리고 사용하려는 모델 등을 종합적으로 고려하여 선택해야 한다.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>