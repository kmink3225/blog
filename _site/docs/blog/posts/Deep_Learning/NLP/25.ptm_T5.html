<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.543">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kwangmin Kim">
<meta name="dcterms.date" content="2025-01-22">
<meta name="description" content="BERT는 Transformer 인코더 기반의 양방향 사전 학습 모델로 자연어 처리 분야에 혁신을 가져왔다. Masked Language Model과 Next Sentence Prediction을 통한 사전 학습 방식, 양방향 문맥 포착 능력, 그리고 다양한 NLP 태스크에서의 뛰어난 성능을 분석한다. BERT의 구조, 학습 방법, 활용 방식과 함께 후속 모델들에 미친 영향을 다룬다.">

<title>Kwangmin Kim - BERT: Bidirectional Encoder Representations from Transformers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete-preset-algolia.umd.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "algolia": {
    "application-id": "DUOR1DRC9D",
    "search-only-api-key": "f264da5dea684ffb9e9b4a574af3ed61",
    "index-name": "prod_QUARTO",
    "analytics-events": true,
    "show-logo": true,
    "libDir": "site_libs"
  },
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": true,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js"></script>


<script type="text/javascript">
var ALGOLIA_INSIGHTS_SRC = "https://cdn.jsdelivr.net/npm/search-insights/dist/search-insights.iife.min.js";
!function(e,a,t,n,s,i,c){e.AlgoliaAnalyticsObject=s,e[s]=e[s]||function(){
(e[s].queue=e[s].queue||[]).push(arguments)},i=a.createElement(t),c=a.getElementsByTagName(t)[0],
i.async=1,i.src=n,c.parentNode.insertBefore(i,c)
}(window,document,"script",ALGOLIA_INSIGHTS_SRC,"aa");
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/autocomplete-plugin-algolia-insights">

</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6W0EKFMWBN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6W0EKFMWBN', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../../../../../styles.css">
<meta property="og:title" content="Kwangmin Kim - BERT: Bidirectional Encoder Representations from Transformers">
<meta property="og:description" content="BERT는 Transformer 인코더 기반의 양방향 사전 학습 모델로 자연어 처리 분야에 혁신을 가져왔다. Masked Language Model과 Next Sentence Prediction을 통한 사전 학습 방식, 양방향 문맥 포착 능력, 그리고 다양한 NLP 태스크에서의 뛰어난 성능을 분석한다. BERT의 구조, 학습 방법, 활용 방식과 함께 후속 모델들에 미친 영향을 다룬다.">
<meta property="og:site_name" content="Kwangmin Kim">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html"> 
<span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"> <i class="bi bi-github" role="img" aria-label="Github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"> <i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#요약" id="toc-요약" class="nav-link active" data-scroll-target="#요약"><span class="header-section-number">1</span> 요약</a></li>
  <li><a href="#nlp-모델-발전-과정" id="toc-nlp-모델-발전-과정" class="nav-link" data-scroll-target="#nlp-모델-발전-과정"><span class="header-section-number">2</span> NLP 모델 발전 과정</a></li>
  <li><a href="#bert-이전-모델들의-한계점" id="toc-bert-이전-모델들의-한계점" class="nav-link" data-scroll-target="#bert-이전-모델들의-한계점"><span class="header-section-number">3</span> BERT 이전 모델들의 한계점</a>
  <ul class="collapse">
  <li><a href="#기존-언어-모델의-문제점" id="toc-기존-언어-모델의-문제점" class="nav-link" data-scroll-target="#기존-언어-모델의-문제점"><span class="header-section-number">3.1</span> 기존 언어 모델의 문제점</a>
  <ul class="collapse">
  <li><a href="#일방향성-문제" id="toc-일방향성-문제" class="nav-link" data-scroll-target="#일방향성-문제"><span class="header-section-number">3.1.1</span> 일방향성 문제</a></li>
  <li><a href="#elmo의-한계" id="toc-elmo의-한계" class="nav-link" data-scroll-target="#elmo의-한계"><span class="header-section-number">3.1.2</span> ELMo의 한계</a></li>
  <li><a href="#문맥-독립적-임베딩의-한계" id="toc-문맥-독립적-임베딩의-한계" class="nav-link" data-scroll-target="#문맥-독립적-임베딩의-한계"><span class="header-section-number">3.1.3</span> 문맥 독립적 임베딩의 한계</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bert의-핵심-아이디어와-혁신" id="toc-bert의-핵심-아이디어와-혁신" class="nav-link" data-scroll-target="#bert의-핵심-아이디어와-혁신"><span class="header-section-number">4</span> BERT의 핵심 아이디어와 혁신</a>
  <ul class="collapse">
  <li><a href="#양방향-문맥의-진정한-활용" id="toc-양방향-문맥의-진정한-활용" class="nav-link" data-scroll-target="#양방향-문맥의-진정한-활용"><span class="header-section-number">4.1</span> 양방향 문맥의 진정한 활용</a></li>
  <li><a href="#masked-language-model-mlm" id="toc-masked-language-model-mlm" class="nav-link" data-scroll-target="#masked-language-model-mlm"><span class="header-section-number">4.2</span> Masked Language Model (MLM)</a>
  <ul class="collapse">
  <li><a href="#mlm의-동작-원리" id="toc-mlm의-동작-원리" class="nav-link" data-scroll-target="#mlm의-동작-원리"><span class="header-section-number">4.2.1</span> MLM의 동작 원리</a></li>
  <li><a href="#마스킹-전략-15-토큰-중" id="toc-마스킹-전략-15-토큰-중" class="nav-link" data-scroll-target="#마스킹-전략-15-토큰-중"><span class="header-section-number">4.2.2</span> 마스킹 전략 (15% 토큰 중)</a></li>
  </ul></li>
  <li><a href="#next-sentence-prediction-nsp" id="toc-next-sentence-prediction-nsp" class="nav-link" data-scroll-target="#next-sentence-prediction-nsp"><span class="header-section-number">4.3</span> Next Sentence Prediction (NSP)</a>
  <ul class="collapse">
  <li><a href="#nsp의-목적과-방법" id="toc-nsp의-목적과-방법" class="nav-link" data-scroll-target="#nsp의-목적과-방법"><span class="header-section-number">4.3.1</span> NSP의 목적과 방법</a></li>
  <li><a href="#nsp의-한계와-후속-연구" id="toc-nsp의-한계와-후속-연구" class="nav-link" data-scroll-target="#nsp의-한계와-후속-연구"><span class="header-section-number">4.3.2</span> NSP의 한계와 후속 연구</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bert의-구조와-아키텍처" id="toc-bert의-구조와-아키텍처" class="nav-link" data-scroll-target="#bert의-구조와-아키텍처"><span class="header-section-number">5</span> BERT의 구조와 아키텍처</a>
  <ul class="collapse">
  <li><a href="#transformer-인코더-기반-설계" id="toc-transformer-인코더-기반-설계" class="nav-link" data-scroll-target="#transformer-인코더-기반-설계"><span class="header-section-number">5.1</span> Transformer 인코더 기반 설계</a>
  <ul class="collapse">
  <li><a href="#bert의-전체-구조" id="toc-bert의-전체-구조" class="nav-link" data-scroll-target="#bert의-전체-구조"><span class="header-section-number">5.1.1</span> BERT의 전체 구조</a></li>
  <li><a href="#모델-크기별-사양" id="toc-모델-크기별-사양" class="nav-link" data-scroll-target="#모델-크기별-사양"><span class="header-section-number">5.1.2</span> 모델 크기별 사양</a></li>
  <li><a href="#입력-표현-input-representation" id="toc-입력-표현-input-representation" class="nav-link" data-scroll-target="#입력-표현-input-representation"><span class="header-section-number">5.1.3</span> 입력 표현 (Input Representation)</a></li>
  </ul></li>
  <li><a href="#특수-토큰의-역할" id="toc-특수-토큰의-역할" class="nav-link" data-scroll-target="#특수-토큰의-역할"><span class="header-section-number">5.2</span> 특수 토큰의 역할</a>
  <ul class="collapse">
  <li><a href="#cls-토큰" id="toc-cls-토큰" class="nav-link" data-scroll-target="#cls-토큰"><span class="header-section-number">5.2.1</span> [CLS] 토큰</a></li>
  <li><a href="#sep-토큰" id="toc-sep-토큰" class="nav-link" data-scroll-target="#sep-토큰"><span class="header-section-number">5.2.2</span> [SEP] 토큰</a></li>
  <li><a href="#mask-토큰" id="toc-mask-토큰" class="nav-link" data-scroll-target="#mask-토큰"><span class="header-section-number">5.2.3</span> [MASK] 토큰</a></li>
  <li><a href="#pad-토큰" id="toc-pad-토큰" class="nav-link" data-scroll-target="#pad-토큰"><span class="header-section-number">5.2.4</span> [PAD] 토큰</a></li>
  </ul></li>
  <li><a href="#attention-mask-메커니즘" id="toc-attention-mask-메커니즘" class="nav-link" data-scroll-target="#attention-mask-메커니즘"><span class="header-section-number">5.3</span> Attention Mask 메커니즘</a></li>
  </ul></li>
  <li><a href="#bert의-학습-과정" id="toc-bert의-학습-과정" class="nav-link" data-scroll-target="#bert의-학습-과정"><span class="header-section-number">6</span> BERT의 학습 과정</a>
  <ul class="collapse">
  <li><a href="#사전-학습-pre-training" id="toc-사전-학습-pre-training" class="nav-link" data-scroll-target="#사전-학습-pre-training"><span class="header-section-number">6.1</span> 사전 학습 (Pre-training)</a>
  <ul class="collapse">
  <li><a href="#학습-데이터" id="toc-학습-데이터" class="nav-link" data-scroll-target="#학습-데이터"><span class="header-section-number">6.1.1</span> 학습 데이터</a></li>
  <li><a href="#학습-설정" id="toc-학습-설정" class="nav-link" data-scroll-target="#학습-설정"><span class="header-section-number">6.1.2</span> 학습 설정</a></li>
  <li><a href="#학습-시간과-비용" id="toc-학습-시간과-비용" class="nav-link" data-scroll-target="#학습-시간과-비용"><span class="header-section-number">6.1.3</span> 학습 시간과 비용</a></li>
  </ul></li>
  <li><a href="#파인튜닝-fine-tuning" id="toc-파인튜닝-fine-tuning" class="nav-link" data-scroll-target="#파인튜닝-fine-tuning"><span class="header-section-number">6.2</span> 파인튜닝 (Fine-tuning)</a>
  <ul class="collapse">
  <li><a href="#태스크별-아키텍처-변경" id="toc-태스크별-아키텍처-변경" class="nav-link" data-scroll-target="#태스크별-아키텍처-변경"><span class="header-section-number">6.2.1</span> 태스크별 아키텍처 변경</a></li>
  <li><a href="#파인튜닝-효율성" id="toc-파인튜닝-효율성" class="nav-link" data-scroll-target="#파인튜닝-효율성"><span class="header-section-number">6.2.2</span> 파인튜닝 효율성</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bert의-성능과-영향" id="toc-bert의-성능과-영향" class="nav-link" data-scroll-target="#bert의-성능과-영향"><span class="header-section-number">7</span> BERT의 성능과 영향</a>
  <ul class="collapse">
  <li><a href="#glue-벤치마크-성능" id="toc-glue-벤치마크-성능" class="nav-link" data-scroll-target="#glue-벤치마크-성능"><span class="header-section-number">7.1</span> GLUE 벤치마크 성능</a></li>
  <li><a href="#squad-질의응답-성능" id="toc-squad-질의응답-성능" class="nav-link" data-scroll-target="#squad-질의응답-성능"><span class="header-section-number">7.2</span> SQuAD 질의응답 성능</a></li>
  <li><a href="#한국어-bert-모델들" id="toc-한국어-bert-모델들" class="nav-link" data-scroll-target="#한국어-bert-모델들"><span class="header-section-number">7.3</span> 한국어 BERT 모델들</a>
  <ul class="collapse">
  <li><a href="#kobert-sktbrain-2019" id="toc-kobert-sktbrain-2019" class="nav-link" data-scroll-target="#kobert-sktbrain-2019"><span class="header-section-number">7.3.1</span> KoBERT (SKTBrain, 2019)</a></li>
  <li><a href="#koelectra-monologg-2020" id="toc-koelectra-monologg-2020" class="nav-link" data-scroll-target="#koelectra-monologg-2020"><span class="header-section-number">7.3.2</span> KoELECTRA (Monologg, 2020)</a></li>
  <li><a href="#klu-bert-시리즈" id="toc-klu-bert-시리즈" class="nav-link" data-scroll-target="#klu-bert-시리즈"><span class="header-section-number">7.3.3</span> KLU-BERT 시리즈</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bert의-한계점과-해결방안" id="toc-bert의-한계점과-해결방안" class="nav-link" data-scroll-target="#bert의-한계점과-해결방안"><span class="header-section-number">8</span> BERT의 한계점과 해결방안</a>
  <ul class="collapse">
  <li><a href="#bert-vs-gpt" id="toc-bert-vs-gpt" class="nav-link" data-scroll-target="#bert-vs-gpt"><span class="header-section-number">8.1</span> BERT vs GPT</a>
  <ul class="collapse">
  <li><a href="#bert와-gpt의-학습-방식-요약" id="toc-bert와-gpt의-학습-방식-요약" class="nav-link" data-scroll-target="#bert와-gpt의-학습-방식-요약"><span class="header-section-number">8.1.1</span> BERT와 GPT의 학습 방식 요약</a></li>
  <li><a href="#bert의-한계-masked-lm의-본질적-약점" id="toc-bert의-한계-masked-lm의-본질적-약점" class="nav-link" data-scroll-target="#bert의-한계-masked-lm의-본질적-약점"><span class="header-section-number">8.1.2</span> BERT의 한계: Masked LM의 본질적 약점</a></li>
  <li><a href="#gpt의-강점-자연스러운-생성과-학습-구조" id="toc-gpt의-강점-자연스러운-생성과-학습-구조" class="nav-link" data-scroll-target="#gpt의-강점-자연스러운-생성과-학습-구조"><span class="header-section-number">8.1.3</span> GPT의 강점: 자연스러운 생성과 학습 구조</a></li>
  <li><a href="#모델-크기와-학습-데이터-규모" id="toc-모델-크기와-학습-데이터-규모" class="nav-link" data-scroll-target="#모델-크기와-학습-데이터-규모"><span class="header-section-number">8.1.4</span> 모델 크기와 학습 데이터 규모</a></li>
  <li><a href="#응용-범위와-범용성" id="toc-응용-범위와-범용성" class="nav-link" data-scroll-target="#응용-범위와-범용성"><span class="header-section-number">8.1.5</span> 응용 범위와 범용성</a></li>
  </ul></li>
  <li><a href="#계산-복잡도-문제" id="toc-계산-복잡도-문제" class="nav-link" data-scroll-target="#계산-복잡도-문제"><span class="header-section-number">8.2</span> 계산 복잡도 문제</a>
  <ul class="collapse">
  <li><a href="#문제점" id="toc-문제점" class="nav-link" data-scroll-target="#문제점"><span class="header-section-number">8.2.1</span> 문제점</a></li>
  <li><a href="#해결방안들" id="toc-해결방안들" class="nav-link" data-scroll-target="#해결방안들"><span class="header-section-number">8.2.2</span> 해결방안들</a></li>
  </ul></li>
  <li><a href="#긴-시퀀스-처리-한계" id="toc-긴-시퀀스-처리-한계" class="nav-link" data-scroll-target="#긴-시퀀스-처리-한계"><span class="header-section-number">8.3</span> 긴 시퀀스 처리 한계</a>
  <ul class="collapse">
  <li><a href="#문제점-1" id="toc-문제점-1" class="nav-link" data-scroll-target="#문제점-1"><span class="header-section-number">8.3.1</span> 문제점</a></li>
  <li><a href="#해결방안들-1" id="toc-해결방안들-1" class="nav-link" data-scroll-target="#해결방안들-1"><span class="header-section-number">8.3.2</span> 해결방안들</a></li>
  </ul></li>
  <li><a href="#생성-태스크-한계" id="toc-생성-태스크-한계" class="nav-link" data-scroll-target="#생성-태스크-한계"><span class="header-section-number">8.4</span> 생성 태스크 한계</a>
  <ul class="collapse">
  <li><a href="#문제점-2" id="toc-문제점-2" class="nav-link" data-scroll-target="#문제점-2"><span class="header-section-number">8.4.1</span> 문제점</a></li>
  <li><a href="#해결방안들-2" id="toc-해결방안들-2" class="nav-link" data-scroll-target="#해결방안들-2"><span class="header-section-number">8.4.2</span> 해결방안들</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bert-변형-모델들" id="toc-bert-변형-모델들" class="nav-link" data-scroll-target="#bert-변형-모델들"><span class="header-section-number">9</span> BERT 변형 모델들</a>
  <ul class="collapse">
  <li><a href="#roberta-2019-facebook" id="toc-roberta-2019-facebook" class="nav-link" data-scroll-target="#roberta-2019-facebook"><span class="header-section-number">9.1</span> RoBERTa (2019, Facebook)</a>
  <ul class="collapse">
  <li><a href="#주요-개선사항" id="toc-주요-개선사항" class="nav-link" data-scroll-target="#주요-개선사항"><span class="header-section-number">9.1.1</span> 주요 개선사항</a></li>
  <li><a href="#성능-향상" id="toc-성능-향상" class="nav-link" data-scroll-target="#성능-향상"><span class="header-section-number">9.1.2</span> 성능 향상</a></li>
  </ul></li>
  <li><a href="#albert-2019-google" id="toc-albert-2019-google" class="nav-link" data-scroll-target="#albert-2019-google"><span class="header-section-number">9.2</span> ALBERT (2019, Google)</a>
  <ul class="collapse">
  <li><a href="#핵심-기술" id="toc-핵심-기술" class="nav-link" data-scroll-target="#핵심-기술"><span class="header-section-number">9.2.1</span> 핵심 기술</a></li>
  <li><a href="#효과" id="toc-효과" class="nav-link" data-scroll-target="#효과"><span class="header-section-number">9.2.2</span> 효과</a></li>
  </ul></li>
  <li><a href="#distilbert-2019-hugging-face" id="toc-distilbert-2019-hugging-face" class="nav-link" data-scroll-target="#distilbert-2019-hugging-face"><span class="header-section-number">9.3</span> DistilBERT (2019, Hugging Face)</a>
  <ul class="collapse">
  <li><a href="#지식-증류-knowledge-distillation" id="toc-지식-증류-knowledge-distillation" class="nav-link" data-scroll-target="#지식-증류-knowledge-distillation"><span class="header-section-number">9.3.1</span> 지식 증류 (Knowledge Distillation)</a></li>
  </ul></li>
  <li><a href="#electra-2020-google" id="toc-electra-2020-google" class="nav-link" data-scroll-target="#electra-2020-google"><span class="header-section-number">9.4</span> ELECTRA (2020, Google)</a>
  <ul class="collapse">
  <li><a href="#혁신적-학습-방법" id="toc-혁신적-학습-방법" class="nav-link" data-scroll-target="#혁신적-학습-방법"><span class="header-section-number">9.4.1</span> 혁신적 학습 방법</a></li>
  <li><a href="#성능" id="toc-성능" class="nav-link" data-scroll-target="#성능"><span class="header-section-number">9.4.2</span> 성능</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bert의-현재와-미래" id="toc-bert의-현재와-미래" class="nav-link" data-scroll-target="#bert의-현재와-미래"><span class="header-section-number">10</span> BERT의 현재와 미래</a>
  <ul class="collapse">
  <li><a href="#산업계-활용-현황" id="toc-산업계-활용-현황" class="nav-link" data-scroll-target="#산업계-활용-현황"><span class="header-section-number">10.1</span> 산업계 활용 현황</a>
  <ul class="collapse">
  <li><a href="#검색-엔진-개선" id="toc-검색-엔진-개선" class="nav-link" data-scroll-target="#검색-엔진-개선"><span class="header-section-number">10.1.1</span> 검색 엔진 개선</a></li>
  <li><a href="#실제-서비스-적용" id="toc-실제-서비스-적용" class="nav-link" data-scroll-target="#실제-서비스-적용"><span class="header-section-number">10.1.2</span> 실제 서비스 적용</a></li>
  </ul></li>
  <li><a href="#후속-모델들에-미친-영향" id="toc-후속-모델들에-미친-영향" class="nav-link" data-scroll-target="#후속-모델들에-미친-영향"><span class="header-section-number">10.2</span> 후속 모델들에 미친 영향</a>
  <ul class="collapse">
  <li><a href="#transformer-기반-모델들" id="toc-transformer-기반-모델들" class="nav-link" data-scroll-target="#transformer-기반-모델들"><span class="header-section-number">10.2.1</span> Transformer 기반 모델들</a></li>
  <li><a href="#설계-원칙의-확산" id="toc-설계-원칙의-확산" class="nav-link" data-scroll-target="#설계-원칙의-확산"><span class="header-section-number">10.2.2</span> 설계 원칙의 확산</a></li>
  </ul></li>
  <li><a href="#연구-동향과-발전-방향" id="toc-연구-동향과-발전-방향" class="nav-link" data-scroll-target="#연구-동향과-발전-방향"><span class="header-section-number">10.3</span> 연구 동향과 발전 방향</a>
  <ul class="collapse">
  <li><a href="#효율성-개선" id="toc-효율성-개선" class="nav-link" data-scroll-target="#효율성-개선"><span class="header-section-number">10.3.1</span> 효율성 개선</a></li>
  <li><a href="#성능-향상-1" id="toc-성능-향상-1" class="nav-link" data-scroll-target="#성능-향상-1"><span class="header-section-number">10.3.2</span> 성능 향상</a></li>
  <li><a href="#응용-분야-확장" id="toc-응용-분야-확장" class="nav-link" data-scroll-target="#응용-분야-확장"><span class="header-section-number">10.3.3</span> 응용 분야 확장</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#결론" id="toc-결론" class="nav-link" data-scroll-target="#결론"><span class="header-section-number">11</span> 결론</a>
  <ul class="collapse">
  <li><a href="#bert의-핵심-기여" id="toc-bert의-핵심-기여" class="nav-link" data-scroll-target="#bert의-핵심-기여"><span class="header-section-number">11.1</span> BERT의 핵심 기여</a></li>
  <li><a href="#후속-발전에-미친-영향" id="toc-후속-발전에-미친-영향" class="nav-link" data-scroll-target="#후속-발전에-미친-영향"><span class="header-section-number">11.2</span> 후속 발전에 미친 영향</a></li>
  <li><a href="#현재적-의미와-미래-전망" id="toc-현재적-의미와-미래-전망" class="nav-link" data-scroll-target="#현재적-의미와-미래-전망"><span class="header-section-number">11.3</span> 현재적 의미와 미래 전망</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">BERT: Bidirectional Encoder Representations from Transformers</h1>
<p class="subtitle lead">양방향 문맥 이해의 혁신과 NLP 패러다임 변화</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>BERT는 Transformer 인코더 기반의 양방향 사전 학습 모델로 자연어 처리 분야에 혁신을 가져왔다. Masked Language Model과 Next Sentence Prediction을 통한 사전 학습 방식, 양방향 문맥 포착 능력, 그리고 다양한 NLP 태스크에서의 뛰어난 성능을 분석한다. BERT의 구조, 학습 방법, 활용 방식과 함께 후속 모델들에 미친 영향을 다룬다.</p>
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kwangmin Kim </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 22, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="요약" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 요약</h1>
<p>BERT(Bidirectional Encoder Representations from Transformers)는 2018년 Google에서 발표한 혁신적인 사전 학습 언어 모델이다. 기존의 일방향 언어 모델들과 달리 양방향 문맥을 동시에 고려하여 깊은 언어 이해 능력을 획득했다.</p>
<p>주요 특징과 혁신 사항은 다음과 같다:</p>
<ul>
<li><strong>양방향 문맥 포착</strong>:
<ul>
<li>기존 GPT, ELMo와 달리 좌우 문맥을 동시에 고려</li>
<li>Transformer 인코더 구조를 사용하여 Self-Attention으로 모든 위치 간 관계 학습</li>
<li>단어의 의미를 문맥에 따라 동적으로 결정</li>
</ul></li>
<li><strong>혁신적인 사전 학습 방식</strong>:
<ul>
<li><strong>Masked Language Model (MLM)</strong>: 입력 토큰의 15%를 마스킹하고 원래 단어 예측</li>
<li><strong>Next Sentence Prediction (NSP)</strong>: 두 문장 간의 연속성 판단</li>
<li>대규모 무라벨 텍스트 데이터로 언어의 일반적 패턴 학습</li>
</ul></li>
<li><strong>Transfer Learning 패러다임 확립</strong>:
<ul>
<li>Pre-training + Fine-tuning 방식으로 다양한 NLP 태스크 해결</li>
<li>태스크별 최소한의 아키텍처 변경만으로 최고 성능 달성</li>
<li>텍스트 분류, 개체명 인식, 질의응답, 감정 분석 등 광범위한 적용</li>
</ul></li>
<li><strong>모델 구조와 성능</strong>:
<ul>
<li>BERT-Base: 12층 Transformer 인코더, 110M 파라미터</li>
<li>BERT-Large: 24층 Transformer 인코더, 340M 파라미터</li>
<li>11개 NLP 태스크에서 기존 최고 성능 대폭 개선</li>
</ul></li>
</ul>
<p>BERT의 등장은 자연어 처리 분야의 패러다임을 바꾸었으며, 이후 RoBERTa, ALBERT, DistilBERT 등 수많은 후속 모델들의 기반이 되었다.</p>
</section>
<section id="nlp-모델-발전-과정" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> NLP 모델 발전 과정</h1>
<pre><code>RNN Language Model
├── Seq2Seq
├── Beam Search
├── Subword Tokenization
├── Attention
├── Transformer Encoder (Vaswani et al., 2017)
|   ├── Positional Encoding
|   ├── Multi-Head Attention
|   └── Feed Forward Neural Network
|
├── Transformer Decoder (Vaswani et al., 2017)
|
├── GPT 시리즈 (OpenAI,2018~)
|   ├── GPT-1~4
|   └── ChatGPT (OpenAI,2022~)
|
├── BERT 시리즈 (Google,2018~)
|   ├── BERT
|   ├── RoBERTa
|   └── ALBERT
|
├── BERT 변형 모델들
|   ├── RoBERTa (Facebook, 2019)
|   ├── ALBERT (Google, 2019)
|   ├── DistilBERT (Hugging Face, 2019)
|   └── ELECTRA (Google, 2020)
|
└── 후속 발전 모델들
    ├── T5, XLNet, DeBERTa
    └── GPT-2/3/4, ChatGPT, PaLM 등</code></pre>
</section>
<section id="bert-이전-모델들의-한계점" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> BERT 이전 모델들의 한계점</h1>
<section id="기존-언어-모델의-문제점" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="기존-언어-모델의-문제점"><span class="header-section-number">3.1</span> 기존 언어 모델의 문제점</h2>
<section id="일방향성-문제" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="일방향성-문제"><span class="header-section-number">3.1.1</span> 일방향성 문제</h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># GPT-1의 일방향 예측 방식</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">"The man went to the [MASK]"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 오직 "The man went to the" 부분만 보고 다음 단어 예측</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 뒤의 문맥 정보 활용 불가</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="elmo의-한계" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="elmo의-한계"><span class="header-section-number">3.1.2</span> ELMo의 한계</h3>
<ul>
<li>BiLSTM을 사용하여 양방향 문맥 고려 시도</li>
<li>하지만 forward LSTM과 backward LSTM이 별도로 학습</li>
<li>진정한 의미의 양방향 문맥 통합 부족</li>
<li>계산 효율성 문제 (순차 처리 필요)</li>
</ul>
</section>
<section id="문맥-독립적-임베딩의-한계" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="문맥-독립적-임베딩의-한계"><span class="header-section-number">3.1.3</span> 문맥 독립적 임베딩의 한계</h3>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Word2Vec, GloVe의 문제점</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">"I went to the bank to deposit money"</span>  <span class="co"># 은행</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">"I sat by the river bank"</span>              <span class="co"># 강가</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 같은 "bank"이지만 다른 의미, 하지만 같은 벡터 표현</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="bert의-핵심-아이디어와-혁신" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> BERT의 핵심 아이디어와 혁신</h1>
<section id="양방향-문맥의-진정한-활용" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="양방향-문맥의-진정한-활용"><span class="header-section-number">4.1</span> 양방향 문맥의 진정한 활용</h2>
<ul>
<li>BERT는 Transformer 인코더의 Self-Attention 메커니즘을 활용하여 문장 내 모든 단어가 서로 상호작용할 수 있도록 설계되었다.</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BERT의 양방향 문맥 활용 예시</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">"The man went to the [MASK] to buy milk"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># [MASK] 예측 시 좌측 문맥: "The man went to the"</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 우측 문맥: "to buy milk"</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 양쪽 모든 정보를 동시에 고려하여 "store" 예측</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="masked-language-model-mlm" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="masked-language-model-mlm"><span class="header-section-number">4.2</span> Masked Language Model (MLM)</h2>
<section id="mlm의-동작-원리" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="mlm의-동작-원리"><span class="header-section-number">4.2.1</span> MLM의 동작 원리</h3>
<ul>
<li>입력 토큰의 15%를 랜덤하게 선택하여 마스킹</li>
<li>마스킹된 토큰의 원래 단어를 예측하도록 학습</li>
<li>양방향 문맥을 자연스럽게 활용하는 학습 목표</li>
</ul>
</section>
<section id="마스킹-전략-15-토큰-중" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="마스킹-전략-15-토큰-중"><span class="header-section-number">4.2.2</span> 마스킹 전략 (15% 토큰 중)</h3>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 마스킹 규칙 적용 예시</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>original_sentence <span class="op">=</span> <span class="st">"The quick brown fox jumps over the lazy dog"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 80%: [MASK] 토큰으로 대체</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">"The quick brown [MASK] jumps over the lazy dog"</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 10%: 랜덤 단어로 대체  </span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">"The quick brown cat jumps over the lazy dog"</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 10%: 원래 단어 유지</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">"The quick brown fox jumps over the lazy dog"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>이러한 전략을 사용하는 이유: * <strong>80% [MASK]</strong>: 주요 학습 목표 * <strong>10% 랜덤 대체</strong>: 실제 토큰에 대한 robustness 향상 * <strong>10% 원래 유지</strong>: [MASK] 토큰에만 의존하지 않도록 방지</p>
</section>
</section>
<section id="next-sentence-prediction-nsp" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="next-sentence-prediction-nsp"><span class="header-section-number">4.3</span> Next Sentence Prediction (NSP)</h2>
<section id="nsp의-목적과-방법" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="nsp의-목적과-방법"><span class="header-section-number">4.3.1</span> NSP의 목적과 방법</h3>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 연속된 문장 쌍 (IsNext = True)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>Sentence A: <span class="st">"The man went to the store."</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>Sentence B: <span class="st">"He bought milk and bread."</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>Label: IsNext</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 무관한 문장 쌍 (IsNext = False)  </span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>Sentence A: <span class="st">"The man went to the store."</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>Sentence B: <span class="st">"The weather is nice today."</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>Label: NotNext</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="nsp의-한계와-후속-연구" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="nsp의-한계와-후속-연구"><span class="header-section-number">4.3.2</span> NSP의 한계와 후속 연구</h3>
<ul>
<li>RoBERTa 연구에서 NSP가 성능 향상에 크게 기여하지 않음을 발견</li>
<li>너무 쉬운 태스크로 실제 문장 관계 이해에 제한적</li>
<li>후속 모델들에서는 NSP 대신 다른 학습 목표 사용</li>
</ul>
</section>
</section>
</section>
<section id="bert의-구조와-아키텍처" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> BERT의 구조와 아키텍처</h1>
<section id="transformer-인코더-기반-설계" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="transformer-인코더-기반-설계"><span class="header-section-number">5.1</span> Transformer 인코더 기반 설계</h2>
<section id="bert의-전체-구조" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="bert의-전체-구조"><span class="header-section-number">5.1.1</span> BERT의 전체 구조</h3>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BERT 아키텍처 개요</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>Input: [CLS] token_1 token_2 ... token_n [SEP] token_n<span class="op">+</span><span class="dv">1</span> ... [SEP] [PAD] ...</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>       <span class="op">|</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>       v</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>Embedding Layer (Token <span class="op">+</span> Position <span class="op">+</span> Segment)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>       <span class="op">|</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>       v</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>Transformer Encoder Layers × N</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>       <span class="op">|</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>       v</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>Output: contextualized representations <span class="cf">for</span> <span class="bu">all</span> tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="모델-크기별-사양" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="모델-크기별-사양"><span class="header-section-number">5.1.2</span> 모델 크기별 사양</h3>
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 16%">
<col style="width: 19%">
<col style="width: 25%">
<col style="width: 19%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th>모델</th>
<th>레이어 수</th>
<th>Hidden Size</th>
<th>Attention Heads</th>
<th>파라미터 수</th>
<th>용도</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BERT-Base</td>
<td>12</td>
<td>768</td>
<td>12</td>
<td>110M</td>
<td>일반적 사용, 연구</td>
</tr>
<tr class="even">
<td>BERT-Large</td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>340M</td>
<td>대규모 태스크, 최고 성능</td>
</tr>
</tbody>
</table>
</section>
<section id="입력-표현-input-representation" class="level3" data-number="5.1.3">
<h3 data-number="5.1.3" class="anchored" data-anchor-id="입력-표현-input-representation"><span class="header-section-number">5.1.3</span> 입력 표현 (Input Representation)</h3>
<p>BERT는 세 가지 임베딩을 합쳐서 입력 표현을 만든다:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 임베딩 구성 요소</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>Total_Embedding <span class="op">=</span> Token_Embedding <span class="op">+</span> Position_Embedding <span class="op">+</span> Segment_Embedding</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 예시: "Hello world [SEP] How are you?"</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>Token_Embedding:    [hello] [world] [SEP] [how] [are] [you] [?]</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>Position_Embedding: [<span class="dv">0</span>]     [<span class="dv">1</span>]     [<span class="dv">2</span>]   [<span class="dv">3</span>]   [<span class="dv">4</span>]   [<span class="dv">5</span>]   [<span class="dv">6</span>]</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>Segment_Embedding:  [A]     [A]     [A]   [B]   [B]   [B]   [B]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="token-embedding" class="level4" data-number="5.1.3.1">
<h4 data-number="5.1.3.1" class="anchored" data-anchor-id="token-embedding"><span class="header-section-number">5.1.3.1</span> Token Embedding</h4>
<ul>
<li>WordPiece 토크나이저 사용 (30,000개 vocab)</li>
<li>미등록어(OOV) 문제 해결을 위한 subword 분할</li>
<li>예: “playing” → “play” + “##ing”</li>
</ul>
</section>
<section id="position-embedding" class="level4" data-number="5.1.3.2">
<h4 data-number="5.1.3.2" class="anchored" data-anchor-id="position-embedding"><span class="header-section-number">5.1.3.2</span> Position Embedding</h4>
<ul>
<li>각 토큰의 위치 정보 인코딩</li>
<li>Transformer의 순서 정보 부족 문제 해결</li>
<li>학습 가능한 positional embedding 사용</li>
</ul>
</section>
<section id="segment-embedding" class="level4" data-number="5.1.3.3">
<h4 data-number="5.1.3.3" class="anchored" data-anchor-id="segment-embedding"><span class="header-section-number">5.1.3.3</span> Segment Embedding</h4>
<ul>
<li>두 문장을 구분하기 위한 임베딩</li>
<li>첫 번째 문장: Segment A, 두 번째 문장: Segment B</li>
<li>NSP 태스크를 위해 필수적</li>
</ul>
</section>
</section>
</section>
<section id="특수-토큰의-역할" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="특수-토큰의-역할"><span class="header-section-number">5.2</span> 특수 토큰의 역할</h2>
<section id="cls-토큰" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="cls-토큰"><span class="header-section-number">5.2.1</span> [CLS] 토큰</h3>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [CLS] 토큰 활용 예시</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>Input:  [CLS] This movie <span class="kw">is</span> great [SEP] I love it [SEP]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>Output: [CLS_repr] [token_reprs...] </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Classification에서 [CLS] 표현 사용</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>classification_output <span class="op">=</span> Linear([CLS_repr])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>분류 태스크의 핵심</strong>: 전체 시퀀스 정보를 압축한 표현</li>
<li><strong>문장 레벨 정보 집약</strong>: Self-Attention을 통해 모든 토큰 정보 통합</li>
</ul>
</section>
<section id="sep-토큰" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="sep-토큰"><span class="header-section-number">5.2.2</span> [SEP] 토큰</h3>
<ul>
<li>문장 경계 표시</li>
<li>NSP 태스크에서 문장 구분 역할</li>
<li>다중 문장 입력 시 필수</li>
</ul>
</section>
<section id="mask-토큰" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="mask-토큰"><span class="header-section-number">5.2.3</span> [MASK] 토큰</h3>
<ul>
<li>MLM 학습 시에만 사용</li>
<li>Fine-tuning이나 추론 시에는 사용하지 않음</li>
</ul>
</section>
<section id="pad-토큰" class="level3" data-number="5.2.4">
<h3 data-number="5.2.4" class="anchored" data-anchor-id="pad-토큰"><span class="header-section-number">5.2.4</span> [PAD] 토큰</h3>
<ul>
<li>배치 처리를 위한 길이 통일</li>
<li>Attention mask와 함께 사용하여 실제 계산에서 제외</li>
</ul>
</section>
</section>
<section id="attention-mask-메커니즘" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="attention-mask-메커니즘"><span class="header-section-number">5.3</span> Attention Mask 메커니즘</h2>
<ul>
<li>패딩 토큰을 제외하고 실제 토큰에 대해서만 attention을 계산하도록 하는 기법</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Attention Mask 예시</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>Input tokens:    [CLS] Hello world [SEP] [PAD] [PAD]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>Attention mask:  [<span class="dv">1</span>]   [<span class="dv">1</span>]   [<span class="dv">1</span>]   [<span class="dv">1</span>]   [<span class="dv">0</span>]   [<span class="dv">0</span>]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 1: 실제 토큰 (attention 계산에 포함)</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 0: 패딩 토큰 (attention 계산에서 제외)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Attention mask의 중요성: * <strong>메모리 효율성</strong>: 불필요한 패딩 토큰 계산 방지 * <strong>성능 향상</strong>: 의미 있는 토큰에만 집중 * <strong>배치 처리 가능</strong>: 다양한 길이의 문장을 효율적으로 처리</p>
</section>
</section>
<section id="bert의-학습-과정" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> BERT의 학습 과정</h1>
<section id="사전-학습-pre-training" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="사전-학습-pre-training"><span class="header-section-number">6.1</span> 사전 학습 (Pre-training)</h2>
<section id="학습-데이터" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="학습-데이터"><span class="header-section-number">6.1.1</span> 학습 데이터</h3>
<ul>
<li><strong>BookCorpus</strong>: 11,038권의 책 (800M words)</li>
<li><strong>English Wikipedia</strong>: 2,500M words (리스트와 테이블 제외)</li>
<li>총 3.3B words의 대규모 텍스트 데이터</li>
</ul>
</section>
<section id="학습-설정" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="학습-설정"><span class="header-section-number">6.1.2</span> 학습 설정</h3>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BERT-Base 학습 하이퍼파라미터</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: <span class="dv">256</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="fl">1e-4</span>,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"training_steps"</span>: <span class="dv">1_000_000</span>,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"warmup_steps"</span>: <span class="dv">10_000</span>,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"max_sequence_length"</span>: <span class="dv">512</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"masked_lm_prob"</span>: <span class="fl">0.15</span>,</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optimizer"</span>: <span class="st">"Adam"</span>,</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"hardware"</span>: <span class="st">"4×4 TPU Pod (16 TPUs)"</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="학습-시간과-비용" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="학습-시간과-비용"><span class="header-section-number">6.1.3</span> 학습 시간과 비용</h3>
<ul>
<li><strong>BERT-Base</strong>: 4일 (4×4 TPU Pod)</li>
<li><strong>BERT-Large</strong>: 4일 (16×4 TPU Pod)</li>
<li>당시 기준으로 수만 달러의 컴퓨팅 비용</li>
</ul>
</section>
</section>
<section id="파인튜닝-fine-tuning" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="파인튜닝-fine-tuning"><span class="header-section-number">6.2</span> 파인튜닝 (Fine-tuning)</h2>
<section id="태스크별-아키텍처-변경" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="태스크별-아키텍처-변경"><span class="header-section-number">6.2.1</span> 태스크별 아키텍처 변경</h3>
<section id="텍스트-분류-text-classification" class="level4" data-number="6.2.1.1">
<h4 data-number="6.2.1.1" class="anchored" data-anchor-id="텍스트-분류-text-classification"><span class="header-section-number">6.2.1.1</span> 텍스트 분류 (Text Classification)</h4>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 감정 분석, 스팸 분류 등</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>Input:  [CLS] This movie <span class="kw">is</span> amazing [SEP]</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        ↓</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>BERT → [CLS_representation] → Linear → Softmax → [Positive<span class="op">/</span>Negative]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="개체명-인식-named-entity-recognition" class="level4" data-number="6.2.1.2">
<h4 data-number="6.2.1.2" class="anchored" data-anchor-id="개체명-인식-named-entity-recognition"><span class="header-section-number">6.2.1.2</span> 개체명 인식 (Named Entity Recognition)</h4>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 토큰별 분류</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>Input:  [CLS] Barack Obama was born <span class="kw">in</span> Hawaii [SEP]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        ↓</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>BERT → [tok_reprs] → Linear → Softmax → [O B<span class="op">-</span>PER I<span class="op">-</span>PER O O O B<span class="op">-</span>LOC]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="질의응답-question-answering" class="level4" data-number="6.2.1.3">
<h4 data-number="6.2.1.3" class="anchored" data-anchor-id="질의응답-question-answering"><span class="header-section-number">6.2.1.3</span> 질의응답 (Question Answering)</h4>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># SQuAD 데이터셋 예시</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>Question: <span class="st">"Where was Barack Obama born?"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>Context:  <span class="st">"Barack Obama was born in Hawaii..."</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        ↓</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>BERT → start_logits, end_logits → Answer span: <span class="st">"Hawaii"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="문장-유사도-sentence-similarity" class="level4" data-number="6.2.1.4">
<h4 data-number="6.2.1.4" class="anchored" data-anchor-id="문장-유사도-sentence-similarity"><span class="header-section-number">6.2.1.4</span> 문장 유사도 (Sentence Similarity)</h4>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 두 문장 비교</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>Input:  [CLS] The cat sits on mat [SEP] A cat <span class="kw">is</span> on the mat [SEP]</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>        ↓</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>BERT → [CLS_representation] → Linear → Similarity_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="파인튜닝-효율성" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="파인튜닝-효율성"><span class="header-section-number">6.2.2</span> 파인튜닝 효율성</h3>
<ul>
<li><strong>빠른 수렴</strong>: 대부분 태스크에서 2-4 epoch로 충분</li>
<li><strong>높은 성능</strong>: 기존 태스크별 모델 대비 큰 성능 향상</li>
<li><strong>적은 데이터</strong>: Transfer learning으로 적은 labeled data로도 높은 성능</li>
</ul>
</section>
</section>
</section>
<section id="bert의-성능과-영향" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> BERT의 성능과 영향</h1>
<section id="glue-벤치마크-성능" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="glue-벤치마크-성능"><span class="header-section-number">7.1</span> GLUE 벤치마크 성능</h2>
<ul>
<li>GLUE: General Language Understanding Evaluation
<ul>
<li>11개 NLP 태스크를 평가하는 벤치마크</li>
<li>텍스트 분류, 개체명 인식, 질의응답, 문장 유사도 등 다양한 태스크 포함</li>
</ul></li>
<li>BERT는 발표 당시 11개 NLP 태스크에서 기존 최고 성능을 대폭 경신했다:</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>태스크</th>
<th>기존 최고</th>
<th>BERT-Base</th>
<th>BERT-Large</th>
<th>개선 폭</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MNLI</td>
<td>86.7</td>
<td>84.6</td>
<td>86.7</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>QQP</td>
<td>89.2</td>
<td>89.2</td>
<td>89.3</td>
<td>+0.1</td>
</tr>
<tr class="odd">
<td>QNLI</td>
<td>88.1</td>
<td>90.5</td>
<td>92.7</td>
<td>+4.6</td>
</tr>
<tr class="even">
<td>SST-2</td>
<td>95.8</td>
<td>93.5</td>
<td>94.9</td>
<td>-0.9</td>
</tr>
<tr class="odd">
<td>CoLA</td>
<td>60.5</td>
<td>52.1</td>
<td>60.5</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>STS-B</td>
<td>86.5</td>
<td>85.8</td>
<td>87.1</td>
<td>+0.6</td>
</tr>
<tr class="odd">
<td>MRPC</td>
<td>86.8</td>
<td>88.9</td>
<td>89.3</td>
<td>+2.5</td>
</tr>
<tr class="even">
<td>RTE</td>
<td>66.4</td>
<td>66.4</td>
<td>70.1</td>
<td>+3.7</td>
</tr>
</tbody>
</table>
</section>
<section id="squad-질의응답-성능" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="squad-질의응답-성능"><span class="header-section-number">7.2</span> SQuAD 질의응답 성능</h2>
<table class="table">
<thead>
<tr class="header">
<th>모델</th>
<th>EM</th>
<th>F1</th>
<th>특징</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BiDAF</td>
<td>67.7</td>
<td>77.3</td>
<td>기존 최고 모델</td>
</tr>
<tr class="even">
<td>BERT-Base</td>
<td>80.8</td>
<td>88.5</td>
<td>+13.1 EM 향상</td>
</tr>
<tr class="odd">
<td>BERT-Large</td>
<td>84.1</td>
<td>90.9</td>
<td>+16.4 EM 향상</td>
</tr>
<tr class="even">
<td>Human</td>
<td>82.3</td>
<td>91.2</td>
<td>인간 수준 근접</td>
</tr>
</tbody>
</table>
</section>
<section id="한국어-bert-모델들" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="한국어-bert-모델들"><span class="header-section-number">7.3</span> 한국어 BERT 모델들</h2>
<section id="kobert-sktbrain-2019" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="kobert-sktbrain-2019"><span class="header-section-number">7.3.1</span> KoBERT (SKTBrain, 2019)</h3>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># KoBERT 특징</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 한국어 Wikipedia <span class="op">+</span> 뉴스 데이터 학습</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> SentencePiece 기반 토크나이징</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">8</span>,<span class="dv">00</span><span class="er">2개</span> vocab size</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> BERT<span class="op">-</span>Base 구조 (<span class="dv">12</span><span class="er">층</span>, <span class="dv">768</span> hidden)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="koelectra-monologg-2020" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="koelectra-monologg-2020"><span class="header-section-number">7.3.2</span> KoELECTRA (Monologg, 2020)</h3>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 더 효율적인 한국어 사전학습 모델</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> ELECTRA 아키텍처 기반</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">34</span><span class="er">GB</span> 한국어 텍스트 데이터</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 다양한 크기: Small, Base, Large</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="klu-bert-시리즈" class="level3" data-number="7.3.3">
<h3 data-number="7.3.3" class="anchored" data-anchor-id="klu-bert-시리즈"><span class="header-section-number">7.3.3</span> KLU-BERT 시리즈</h3>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 카카오에서 개발한 한국어 특화 모델들</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> KLU<span class="op">-</span>RoBERTa</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> KLU<span class="op">-</span>ALBERT  </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> KLU<span class="op">-</span>ELECTRA</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="bert의-한계점과-해결방안" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> BERT의 한계점과 해결방안</h1>
<section id="bert-vs-gpt" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="bert-vs-gpt"><span class="header-section-number">8.1</span> BERT vs GPT</h2>
<ul>
<li>BERT의 양방향성은 분명히 강력한 특징이고, 얼핏 보면 더 “이해”에 유리해 보이지만, GPT가 실제 성능에서 더 우수한 이유는 단순한 방향성의 차이를 넘는 여러 구조적, 방법론적 요소들이 작용하고 있기 때문이다.</li>
</ul>
<section id="bert와-gpt의-학습-방식-요약" class="level3" data-number="8.1.1">
<h3 data-number="8.1.1" class="anchored" data-anchor-id="bert와-gpt의-학습-방식-요약"><span class="header-section-number">8.1.1</span> BERT와 GPT의 학습 방식 요약</h3>
<ul>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>
<ul>
<li>학습 방식: <strong>Masked Language Modeling (MLM)</strong></li>
<li>입력 문장에서 일부 단어를 가리고, 그 가려진 단어를 예측</li>
<li>양방향 문맥 정보를 활용 (왼쪽 + 오른쪽을 동시에 고려)</li>
<li>fine-tuning을 통해 다양한 NLP 태스크에 사용</li>
</ul></li>
<li><strong>GPT (Generative Pre-trained Transformer)</strong>
<ul>
<li>학습 방식: <strong>Auto-regressive Language Modeling</strong></li>
<li>입력 시 왼쪽에서 오른쪽으로만 예측 (순방향)</li>
<li>다음 단어를 예측하며 문장을 생성함</li>
<li>사전학습과 미세조정 없이도 다양한 태스크에서 성능을 보임 (in-context learning)</li>
</ul></li>
</ul>
</section>
<section id="bert의-한계-masked-lm의-본질적-약점" class="level3" data-number="8.1.2">
<h3 data-number="8.1.2" class="anchored" data-anchor-id="bert의-한계-masked-lm의-본질적-약점"><span class="header-section-number">8.1.2</span> BERT의 한계: Masked LM의 본질적 약점</h3>
<ul>
<li>Mask된 단어를 맞히는 과제는 <strong>문장 생성</strong>이나 <strong>상황 이해</strong>보다는 <strong>클로즈 테스트(cloze test)</strong>와 유사한 제한적 과제이다.</li>
<li>문장에서 단어 몇 개만 가리기 때문에, 실제로는 <strong>전체 문맥 생성 능력</strong>은 훈련되지 않음.</li>
<li>마스크된 입력은 실제 문장이 아니기 때문에, <strong>학습-추론 간 괴리(train-test discrepancy)</strong>가 발생함.</li>
<li>BERT는 <strong>단어 수준에서 잘 작동</strong>하지만, 문장 생성이나 응답 생성처럼 <strong>문맥을 흐름으로 이어가는 작업</strong>에는 약함.</li>
</ul>
</section>
<section id="gpt의-강점-자연스러운-생성과-학습-구조" class="level3" data-number="8.1.3">
<h3 data-number="8.1.3" class="anchored" data-anchor-id="gpt의-강점-자연스러운-생성과-학습-구조"><span class="header-section-number">8.1.3</span> GPT의 강점: 자연스러운 생성과 학습 구조</h3>
<ul>
<li>GPT는 학습 단계에서부터 실제 사용하는 방식과 거의 동일하게 훈련됨 → <strong>next-token prediction</strong></li>
<li>문장을 왼쪽부터 오른쪽으로 예측하면서 학습하기 때문에, <strong>문맥의 흐름에 맞는 문장 생성</strong> 능력이 매우 뛰어남.</li>
<li>특히 GPT-3부터는 <strong>few-shot, zero-shot, in-context learning</strong>이 가능해졌고, GPT-4에서는 그 능력이 폭발적으로 향상됨.
<ul>
<li>예: 예시 몇 개만 주면 태스크의 룰을 “이해하고 따라함”</li>
</ul></li>
<li>실시간 대화, 질의응답, 요약, 번역, 추론 등에서 자연스럽고 유연한 반응을 생성할 수 있음.</li>
</ul>
</section>
<section id="모델-크기와-학습-데이터-규모" class="level3" data-number="8.1.4">
<h3 data-number="8.1.4" class="anchored" data-anchor-id="모델-크기와-학습-데이터-규모"><span class="header-section-number">8.1.4</span> 모델 크기와 학습 데이터 규모</h3>
<ul>
<li>GPT는 BERT보다 훨씬 큰 모델이며, <strong>훨씬 더 많은 텍스트 데이터</strong>로 사전학습함.</li>
<li>특히 GPT-4 계열은 <strong>수조 개 단어 수준의 데이터</strong>로 사전학습되었고, 파라미터 수도 수천억 개 이상.</li>
<li>단순한 방향성보다 <strong>스케일(모델 크기와 데이터 양)</strong>이 언어 모델 성능에 미치는 영향이 훨씬 큼.</li>
</ul>
</section>
<section id="응용-범위와-범용성" class="level3" data-number="8.1.5">
<h3 data-number="8.1.5" class="anchored" data-anchor-id="응용-범위와-범용성"><span class="header-section-number">8.1.5</span> 응용 범위와 범용성</h3>
<ul>
<li>BERT는 특정 태스크(fine-tuning)를 거쳐야만 좋은 성능을 냄.</li>
<li>GPT는 prompt만 바꿔서 수많은 태스크에 바로 적용 가능 (범용성 높음).</li>
<li>따라서 실용성 면에서 GPT가 훨씬 유리함.</li>
</ul>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 35%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>요소</th>
<th>BERT</th>
<th>GPT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>문맥 방향성</td>
<td>양방향</td>
<td>일방향</td>
</tr>
<tr class="even">
<td>학습 방식</td>
<td>마스크 단어 예측 (MLM)</td>
<td>다음 단어 예측 (Auto-regressive)</td>
</tr>
<tr class="odd">
<td>문장 생성 능력</td>
<td>약함</td>
<td>강함</td>
</tr>
<tr class="even">
<td>학습-추론 일치도</td>
<td>낮음</td>
<td>높음</td>
</tr>
<tr class="odd">
<td>범용성</td>
<td>낮음 (fine-tuning 필요)</td>
<td>높음 (prompt만으로도 작동)</td>
</tr>
<tr class="even">
<td>스케일</td>
<td>비교적 작음</td>
<td>훨씬 큼 (GPT-3, 4는 초대형)</td>
</tr>
</tbody>
</table>
<ul>
<li>즉, “양방향성”이라는 요소 하나만으로 모델의 전반적인 성능을 판단하기는 어렵고, 실제로는 <strong>학습 방식, 생성 구조, 스케일, 추론 능력</strong> 같은 요소들이 종합적으로 작용한 결과 GPT가 더 뛰어난 성능을 보이고 있다.</li>
<li>원리적으론 BERT 방식이 더 “이해 중심”처럼 보일 수 있지만, 실전에서 요구되는 <strong>언어 생성, 응답의 자연스러움, 유연성</strong>은 GPT가 더 잘 처리하는 영역이다.</li>
</ul>
</section>
</section>
<section id="계산-복잡도-문제" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="계산-복잡도-문제"><span class="header-section-number">8.2</span> 계산 복잡도 문제</h2>
<section id="문제점" class="level3" data-number="8.2.1">
<h3 data-number="8.2.1" class="anchored" data-anchor-id="문제점"><span class="header-section-number">8.2.1</span> 문제점</h3>
<ul>
<li>Transformer의 Self-Attention: O(n²) 복잡도</li>
<li>긴 시퀀스 처리 시 메모리 사용량 급증</li>
<li>실시간 서비스에는 너무 무거움</li>
</ul>
</section>
<section id="해결방안들" class="level3" data-number="8.2.2">
<h3 data-number="8.2.2" class="anchored" data-anchor-id="해결방안들"><span class="header-section-number">8.2.2</span> 해결방안들</h3>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. DistilBERT: 지식 증류를 통한 경량화</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> BERT 성능의 <span class="dv">97</span><span class="op">%</span> 유지하면서 <span class="dv">60</span><span class="op">%</span> 크기 감소</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 추론 속도 <span class="dv">60</span><span class="op">%</span> 향상</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. ALBERT: 파라미터 공유</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Factorized Embedding: 임베딩 크기 분해</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Cross<span class="op">-</span>layer Parameter Sharing: 레이어 간 파라미터 공유</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">18</span><span class="er">배</span> 적은 파라미터로 더 좋은 성능</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. MobileBERT: 모바일 최적화</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Teacher<span class="op">-</span>Student 학습</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Bottleneck 구조로 레이어 압축</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="긴-시퀀스-처리-한계" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="긴-시퀀스-처리-한계"><span class="header-section-number">8.3</span> 긴 시퀀스 처리 한계</h2>
<section id="문제점-1" class="level3" data-number="8.3.1">
<h3 data-number="8.3.1" class="anchored" data-anchor-id="문제점-1"><span class="header-section-number">8.3.1</span> 문제점</h3>
<ul>
<li>최대 512 토큰 제한</li>
<li>긴 문서 처리 불가</li>
<li>문서 레벨 태스크에서 한계</li>
</ul>
</section>
<section id="해결방안들-1" class="level3" data-number="8.3.2">
<h3 data-number="8.3.2" class="anchored" data-anchor-id="해결방안들-1"><span class="header-section-number">8.3.2</span> 해결방안들</h3>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Longformer: Sparse Attention</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Local <span class="op">+</span> Global Attention 패턴</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">4</span>,<span class="dv">0</span><span class="er">96</span> 토큰까지 처리 가능</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. BigBird: Random + Window + Global Attention  </span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 이론적으로 증명된 sparse attention</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 더 긴 시퀀스 처리 가능</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. LED (Longformer-Encoder-Decoder)</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 긴 시퀀스 요약<span class="op">/</span>생성 태스크 특화</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="생성-태스크-한계" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="생성-태스크-한계"><span class="header-section-number">8.4</span> 생성 태스크 한계</h2>
<section id="문제점-2" class="level3" data-number="8.4.1">
<h3 data-number="8.4.1" class="anchored" data-anchor-id="문제점-2"><span class="header-section-number">8.4.1</span> 문제점</h3>
<ul>
<li>인코더 전용 구조로 생성 태스크 부적합</li>
<li>MLM은 생성보다 이해에 특화</li>
<li>자연스러운 텍스트 생성 어려움</li>
</ul>
</section>
<section id="해결방안들-2" class="level3" data-number="8.4.2">
<h3 data-number="8.4.2" class="anchored" data-anchor-id="해결방안들-2"><span class="header-section-number">8.4.2</span> 해결방안들</h3>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. BART: 인코더-디코더 구조</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Denoising Autoencoder 방식</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 생성과 이해 모두 강화</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. T5: Text-to-Text Transfer Transformer</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 모든 태스크를 생성 문제로 변환</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="st">"translate English to German: Hello"</span> → <span class="st">"Hallo"</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. UniLM: Unified Language Model</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 단일 모델로 이해<span class="op">/</span>생성 모두 수행</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="bert-변형-모델들" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> BERT 변형 모델들</h1>
<section id="roberta-2019-facebook" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="roberta-2019-facebook"><span class="header-section-number">9.1</span> RoBERTa (2019, Facebook)</h2>
<section id="주요-개선사항" class="level3" data-number="9.1.1">
<h3 data-number="9.1.1" class="anchored" data-anchor-id="주요-개선사항"><span class="header-section-number">9.1.1</span> 주요 개선사항</h3>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># RoBERTa 변경점</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> NSP 제거: Next Sentence Prediction 태스크 제거</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fl">2.</span> 동적 마스킹: 매 epoch마다 다른 마스킹 패턴</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="fl">3.</span> 더 큰 배치: <span class="dv">8</span><span class="er">K</span> 배치 크기 사용</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="fl">4.</span> 더 많은 데이터: <span class="dv">160</span><span class="er">GB</span> 텍스트 데이터</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="fl">5.</span> 더 긴 학습: <span class="dv">500</span><span class="er">K</span> steps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="성능-향상" class="level3" data-number="9.1.2">
<h3 data-number="9.1.2" class="anchored" data-anchor-id="성능-향상"><span class="header-section-number">9.1.2</span> 성능 향상</h3>
<ul>
<li>GLUE에서 BERT-Large 대비 평균 1-2% 성능 향상</li>
<li>SQuAD 2.0에서 큰 성능 개선</li>
<li>단순한 변경으로 큰 효과 입증</li>
</ul>
</section>
</section>
<section id="albert-2019-google" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="albert-2019-google"><span class="header-section-number">9.2</span> ALBERT (2019, Google)</h2>
<section id="핵심-기술" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="핵심-기술"><span class="header-section-number">9.2.1</span> 핵심 기술</h3>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Factorized Embedding Parameterization</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 기존: vocab_size × hidden_size</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ALBERT: vocab_size × embedding_size × hidden_size</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>E <span class="op">=</span> <span class="dv">128</span>  <span class="co"># embedding_size &lt;&lt; hidden_size (768)</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Cross-layer Parameter Sharing</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 모든 레이어가 같은 파라미터 공유</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 24층이어도 1층만큼의 파라미터만 사용</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. SOP (Sentence Order Prediction)</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co"># NSP 대신 문장 순서 예측 태스크 사용</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="효과" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="효과"><span class="header-section-number">9.2.2</span> 효과</h3>
<ul>
<li>BERT-Large 대비 18배 적은 파라미터</li>
<li>더 나은 성능 달성</li>
<li>훈련 시간 단축</li>
</ul>
</section>
</section>
<section id="distilbert-2019-hugging-face" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="distilbert-2019-hugging-face"><span class="header-section-number">9.3</span> DistilBERT (2019, Hugging Face)</h2>
<section id="지식-증류-knowledge-distillation" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="지식-증류-knowledge-distillation"><span class="header-section-number">9.3.1</span> 지식 증류 (Knowledge Distillation)</h3>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Teacher-Student 학습</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>Teacher: BERT<span class="op">-</span>Base (<span class="dv">110</span><span class="er">M</span> parameters)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>Student: DistilBERT (<span class="dv">66</span><span class="er">M</span> parameters)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 손실 함수</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>Loss <span class="op">=</span> α × distillation_loss <span class="op">+</span> β × student_loss <span class="op">+</span> γ × cosine_loss</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 결과</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">97</span><span class="op">%</span> 성능 유지</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">60</span><span class="op">%</span> 크기 감소  </span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">60</span><span class="op">%</span> 빠른 추론</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="electra-2020-google" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="electra-2020-google"><span class="header-section-number">9.4</span> ELECTRA (2020, Google)</h2>
<section id="혁신적-학습-방법" class="level3" data-number="9.4.1">
<h3 data-number="9.4.1" class="anchored" data-anchor-id="혁신적-학습-방법"><span class="header-section-number">9.4.1</span> 혁신적 학습 방법</h3>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace Token Detection (RTD)</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># MLM: 15% 토큰만 학습</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ELECTRA: 100% 토큰 모두 학습</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>Generator (작은 모델): 토큰 생성</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>Discriminator (ELECTRA): 각 토큰이 원본인지 생성된 것인지 판단</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 예시</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>Original: <span class="st">"The chef cooked the meal"</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>Generated: <span class="st">"The chef ate the meal"</span>  </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>ELECTRA: [Original, Original, Replaced, Original, Original]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="성능" class="level3" data-number="9.4.2">
<h3 data-number="9.4.2" class="anchored" data-anchor-id="성능"><span class="header-section-number">9.4.2</span> 성능</h3>
<ul>
<li>같은 컴퓨팅으로 BERT보다 높은 성능</li>
<li>특히 작은 모델에서 큰 효과</li>
</ul>
</section>
</section>
</section>
<section id="bert의-현재와-미래" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> BERT의 현재와 미래</h1>
<section id="산업계-활용-현황" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="산업계-활용-현황"><span class="header-section-number">10.1</span> 산업계 활용 현황</h2>
<section id="검색-엔진-개선" class="level3" data-number="10.1.1">
<h3 data-number="10.1.1" class="anchored" data-anchor-id="검색-엔진-개선"><span class="header-section-number">10.1.1</span> 검색 엔진 개선</h3>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Google Search (2019년부터)</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 검색 쿼리 이해 향상</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="dv">10</span><span class="op">%</span> 쿼리에 BERT 적용</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 특히 긴 꼬리(<span class="bu">long</span><span class="op">-</span>tail) 쿼리에서 큰 개선</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Microsoft Bing</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> BERT 기반 검색 개선</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 광고 관련성 향상</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="실제-서비스-적용" class="level3" data-number="10.1.2">
<h3 data-number="10.1.2" class="anchored" data-anchor-id="실제-서비스-적용"><span class="header-section-number">10.1.2</span> 실제 서비스 적용</h3>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 챗봇/가상 비서</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 의도(Intent) 분류</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 개체명 인식(NER)  </span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 감정 분석</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 콘텐츠 추천</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 텍스트 유사도 계산</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 사용자 관심사 파악</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 개인화 추천</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 문서 처리</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 자동 요약</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 문서 분류</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 정보 추출</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="후속-모델들에-미친-영향" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="후속-모델들에-미친-영향"><span class="header-section-number">10.2</span> 후속 모델들에 미친 영향</h2>
<section id="transformer-기반-모델들" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="transformer-기반-모델들"><span class="header-section-number">10.2.1</span> Transformer 기반 모델들</h3>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BERT의 영향을 받은 주요 모델들</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>├── 인코더 계열</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>│   ├── RoBERTa, ALBERT, ELECTRA</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>│   ├── DeBERTa, CANINE</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>│   └── 다국어: mBERT, XLM<span class="op">-</span>R</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>│</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>├── 인코더<span class="op">-</span>디코더 계열  </span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>│   ├── BART, T5</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>│   ├── PEGASUS, ProphetNet</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>│   └── mT5, ByT5</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>│</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>└── 디코더 계열</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    ├── GPT<span class="op">-</span><span class="dv">2</span><span class="op">/</span><span class="dv">3</span><span class="op">/</span><span class="dv">4</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    ├── PaLM, LaMDA</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    └── ChatGPT, Gemini</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="설계-원칙의-확산" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="설계-원칙의-확산"><span class="header-section-number">10.2.2</span> 설계 원칙의 확산</h3>
<ul>
<li><strong>Pre-training + Fine-tuning</strong>: 거의 모든 NLP 모델의 표준</li>
<li><strong>Large-scale Unsupervised Learning</strong>: 무라벨 데이터 활용</li>
<li><strong>Transfer Learning</strong>: 일반 지식을 특정 태스크로 전이</li>
<li><strong>Attention is All You Need</strong>: Transformer 아키텍처 표준화</li>
</ul>
</section>
</section>
<section id="연구-동향과-발전-방향" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="연구-동향과-발전-방향"><span class="header-section-number">10.3</span> 연구 동향과 발전 방향</h2>
<section id="효율성-개선" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="효율성-개선"><span class="header-section-number">10.3.1</span> 효율성 개선</h3>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 경량화 연구</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Pruning: 불필요한 가중치 제거</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Quantization: 낮은 정밀도 연산</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Knowledge Distillation: 지식 증류</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 아키텍처 개선</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Sparse Attention: 희소 어텐션 패턴</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Linear Attention: 선형 복잡도 어텐션</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Hardware<span class="op">-</span>aware Design: 하드웨어 최적화</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="성능-향상-1" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="성능-향상-1"><span class="header-section-number">10.3.2</span> 성능 향상</h3>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 더 나은 사전학습</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Better Objectives: MLM 개선</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Curriculum Learning: 점진적 학습</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Multi<span class="op">-</span>task Learning: 다중 태스크 학습</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 아키텍처 혁신</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Mixture of Experts: 전문가 혼합 모델</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Retrieval<span class="op">-</span>Augmented: 검색 증강 생성</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Multimodal: 다중 모달 통합</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="응용-분야-확장" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="응용-분야-확장"><span class="header-section-number">10.3.3</span> 응용 분야 확장</h3>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 새로운 도메인</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 과학 문헌: SciBERT, BioBERT</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 법률 문서: LegalBERT</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 금융 분야: FinBERT</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 의료 분야: ClinicalBERT</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 다국어 지원</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> mBERT: <span class="dv">104</span><span class="er">개</span> 언어</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> XLM<span class="op">-</span>R: <span class="dv">100</span><span class="er">개</span> 언어  </span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> 언어별 특화 모델들</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="결론" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> 결론</h1>
<p>BERT는 자연어 처리 분야에서 가장 중요한 혁신 중 하나로, 2018년 발표 이후 NLP 연구와 응용의 패러다임을 완전히 바꾸었다.</p>
<section id="bert의-핵심-기여" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="bert의-핵심-기여"><span class="header-section-number">11.1</span> BERT의 핵심 기여</h2>
<ul>
<li><strong>양방향 문맥 이해</strong>: Self-Attention을 통한 진정한 의미의 양방향 문맥 포착으로 기존 일방향 모델의 한계 극복</li>
<li><strong>혁신적 사전 학습</strong>: MLM과 NSP를 통해 대규모 무라벨 데이터에서 언어의 깊은 패턴을 학습하는 새로운 방법 제시</li>
<li><strong>Transfer Learning 확립</strong>: Pre-training + Fine-tuning 패러다임을 통해 하나의 모델로 다양한 NLP 태스크를 효과적으로 해결</li>
<li><strong>성능 혁신</strong>: 11개 주요 NLP 태스크에서 기존 최고 성능을 대폭 경신하며 인간 수준에 근접한 성능 달성</li>
</ul>
</section>
<section id="후속-발전에-미친-영향" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="후속-발전에-미친-영향"><span class="header-section-number">11.2</span> 후속 발전에 미친 영향</h2>
<p>BERT 등장 이후 NLP 분야는 완전히 새로운 국면에 접어들었다. RoBERTa, ALBERT, ELECTRA 등의 직접적 개선 모델뿐만 아니라, T5의 텍스트-투-텍스트 프레임워크, GPT 시리즈의 생성형 AI 혁신까지 모두 BERT가 확립한 기반 위에서 발전했다.</p>
<p>특히 ChatGPT로 대표되는 현재의 대화형 AI 시스템들도 BERT가 보여준 대규모 사전 학습의 효과성과 Transfer Learning 패러다임의 연장선상에 있다.</p>
</section>
<section id="현재적-의미와-미래-전망" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="현재적-의미와-미래-전망"><span class="header-section-number">11.3</span> 현재적 의미와 미래 전망</h2>
<p>BERT는 단순한 기술적 발전을 넘어 AI가 언어를 이해하는 방식을 근본적으로 변화시켰다. 검색, 번역, 질의응답, 문서 분류 등 실생활의 다양한 영역에서 BERT 기반 기술이 활용되고 있으며, 이는 인간과 기계의 상호작용을 더욱 자연스럽게 만들고 있다.</p>
<p>앞으로도 BERT의 핵심 아이디어들은 더욱 효율적이고 강력한 언어 모델의 기초가 될 것이며, 다중 모달 AI, 개인화된 AI 어시스턴트, 전문 도메인 특화 AI 등의 발전에 계속해서 중요한 역할을 할 것이다.</p>
<p>BERT의 등장은 AI가 인간의 언어를 진정으로 이해할 수 있다는 가능성을 보여준 역사적 전환점이었으며, 이후 모든 언어 AI 기술 발전의 출발점이 되었다.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>