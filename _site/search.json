[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\nBrief Introduction\n저는 R과 Python같은 오픈소스 도구를 사용하여 통계, 머신 러닝 및 딥러닝을 독학한 열정 가득한 데이터 과학자입니다. 약 7년 여 동안 데이터 모델링, 통계적 모델링, 머신 러닝 모델링 및 시각화를 통하여 데이터 관련 업무 경험을 쌓았습니다.\n\n\nExperience\n저는 한국에서 생화학 학사 학위를, 미국에서는 수학 학사 학위와 생물통계학 석사 학위를 취득했습니다. 저의 생화학 전공을 바탕으로 바이오와 의료분야에서 커리어를 시작했으며, 그 과정에서 많은 비전문가들과 협업하면서 통계 및 데이터 사이언스에 대해 그들과 소통하는 법을 익혔습니다.\n비전문가들과 SW 개발자들과의 협업을 통해서 효율적이고 효과적인 소통의 중요성을 깨달았습니다. 그 효과적인 소통은 수학, 통계, 데이터 엔지니어링 및 IT에 대한 탄탄한 지식에서 비롯된다고 생각하여, 기술에 대한 세부적이고 체계적인 지식축적을 지향합니다.\n\n\n\n\nBrief Introduction\nI am a passionate data scientist who has self-taught statistics, machine learning, and deep learning using open-source tools like R and Python. Over approximately 7 years, I have accumulated experience in data-related work through data modeling, statistical modeling, machine learning modeling, and visualization.\n\n\nExperience\nI earned my bachelor’s degree in Biochemistry in South Korea and in Mathematics in the USA, followed by a master’s degree in Biostatistics. Leveraging my background in Biochemistry, I began my career in the bio and medical fields. Throughout this journey, I collaborated with many non-experts in data science, learning how to communicate effectively about statistics and data science with them.\nThrough collaboration with non-experts and software developers, I realized the importance of efficient and effective communication. I believe that such effective communication stems from a solid understanding of mathematics, statistics, data engineering, and IT, which is why I aim to accumulate detailed and systematic knowledge in these technical areas."
  },
  {
    "objectID": "docs/blog/index.html",
    "href": "docs/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Blog Content List\n\n\nGuide Map of Blog Contents\n\n\nAs the number of blog topics has increased, organizing the content has become more challenging. To enhance content accessibility and facilitate an understanding of the blog’s relevance, we have provided links to a list of contents categorized by topic. \n\n\n\n\n\nJan 1, 3000\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Quality Management\n\n\n데이터 표준 관리: 데이터 표준 코드 등록 절차\n\n\n\nData Governance\n\n\n\n이 글에서는 애플리케이션 개발 시 발생하는 표준 코드의 신규 및 변경 요청과 승인 절차를 설명한다. 개발자가 신규 코드 요건을 도출하고 표준 코드 사전에서 검색한 후, 표준 담당자가 이를 검토 및 등록하는 과정을 단계별로 안내한다.\n\n\n\n\n\nAug 20, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - the contents agreement\n\n\n데이터 표준 관리: 데이터 표준 코드 등록 절차\n\n\n\nData Governance\n\n\n\n이 글에서는 애플리케이션 개발 시 발생하는 표준 코드의 신규 및 변경 요청과 승인 절차를 설명한다. 개발자가 신규 코드 요건을 도출하고 표준 코드 사전에서 검색한 후, 표준 담당자가 이를 검토 및 등록하는 과정을 단계별로 안내한다.\n\n\n\n\n\nAug 19, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Code Registration Process\n\n\n데이터 표준 관리: 데이터 표준 코드 등록 절차\n\n\n\nData Governance\n\n\n\n이 글에서는 애플리케이션 개발 시 발생하는 표준 코드의 신규 및 변경 요청과 승인 절차를 설명한다. 개발자가 신규 코드 요건을 도출하고 표준 코드 사전에서 검색한 후, 표준 담당자가 이를 검토 및 등록하는 과정을 단계별로 안내한다.\n\n\n\n\n\nAug 18, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Glossary Review Process\n\n\n데이터 표준 관리: 데이터 모델 표준 용어 점검 절차\n\n\n\nData Governance\n\n\n\n이 글에서는 데이터 모델 수정 시 표준 용어 점검 절차를 설명하고, 데이터 모델 담당자와 표준 담당자 간의 역할 구분, 표준 준수 여부 확인, 비표준 용어 처리 절차 등을 단계별로 다룬다. \n\n\n\n\n\nAug 17, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Registration Process\n\n\n데이터 표준 관리: 표준 단어, 용어 및 도메인 신청 절차\n\n\n\nData Governance\n\n\n\n이 블로그 포스트에서는 데이터 표준 용어 사전의 개념, 목적, 구성 요소 및 제작 과정을 설명한다. 표준 용어의 기본 원칙, 구성 원칙, 활용 원칙을 다룬다.\n\n\n\n\n\nAug 16, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Standard Code\n\n\n데이터 표준 관리: 표준 코드 사전\n\n\n\nData Governance\n\n\n\n이 블로그 포스트에서는 데이터 표준 코드의 개념, 목적, 특징, 그리고 관리 방법에 대해 상세히 설명한다. 단일코드, 계층코드, 목록코드, 복합코드 등 다양한 코드 유형과 그 사용 조건을 소개하고, 코드 표준화 대상과 관리 원칙을 제시한다.\n\n\n\n\n\nAug 15, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Standard Glossary\n\n\n데이터 표준 관리: 표준 용어 사전\n\n\n\nData Governance\n\n\n\n이 블로그 포스트에서는 데이터 표준 용어 사전의 개념, 목적, 구성 요소 및 제작 과정을 설명한다. 표준 용어의 기본 원칙, 구성 원칙, 활용 원칙을 다룬다.\n\n\n\n\n\nAug 14, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Domain Standardization\n\n\n데이터 표준 관리: 도메인 표준화\n\n\n\nData Governance\n\n\n\n이 블로그 포스트에서는 데이터 표준 도메인의 개념, 목적, 구성 요소를 상세히 설명한다. 도메인 그룹, 도메인, 인포타입, DBMS별 데이터 타입 등 주요 구성 요소를 소개하고, 날짜, 명칭, 내용, 수량, 율, 금액, 번호, 코드, 분류 등 다양한 도메인 그룹과 그 특성을 설명한다. 데이터 품질 향상과 일관성 있는 데이터 구조 설계를 위한 필수 지식을 담고 있어, 데이터 모델러, 데이터베이스 관리자, 그리고 데이터 거버넌스 담당자에게 유용한 정보를 제공한다.\n\n\n\n\n\nAug 13, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Standard Word Dictionary\n\n\n데이터 표준 관리: 표준 단어 사전\n\n\n\nData Governance\n\n\n\n이 블로그 포스트에서는 데이터 표준 단어 사전의 개념, 중요성, 그리고 구축 방법에 대해 상세히 설명한다. 표준 단어의 정의와 구성 요소, 사용 원칙, 그리고 제작 과정을 단계별로 소개하며, 한글, 영문, 복합어, 숫자 등 다양한 유형의 단어에 대한 사용 지침을 제공한다. 또한 동음이의어, 이음동의어, 금칙어 등의 처리 방법과 표준 단어 사전의 실제 예시를 통해 실무적인 적용 방안을 제시한다.\n\n\n\n\n\nAug 12, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Standard Governance\n\n\n데이터 표준 관리: 조직의 데이터 품질과 일관성을 위한 핵심 전략\n\n\n\nData Governance\n\n\n\n이 블로그 포스트에서는 데이터 표준 거버넌스의 중요성과 구성 요소를 이야기한다. 데이터 표준관리의 목적과 정의, 주요 정제 및 개선 사항을 소개하고, 데이터 표준화의 필요성을 다각도로 살펴본다. 또한 데이터 표준화 요소 간의 관계를 설명하며, 표준 데이터와 구조 데이터의 개념을 설명한다.\n\n\n\n\n\nAug 11, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Model (7)\n\n\n데이터 모델링 기초: 개념적 데이터 모델링의 ER Modeling\n\n\n\nData Governance\n\n\n\n이 블로그는 데이터베이스 설계의 초기 단계인 개념적 데이터 모델링에 대해 다룬다. ER(Entity-Relationship) 모델의 주요 구성 요소인 개체, 관계, 속성에 대해 설명하며, 각 요소의 특성과 표현 방법을 제시한다.\n\n\n\n\n\nAug 10, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Current Status Analysis\n\n\n데이터 현황 분석\n\n\n성공적인 데이터 거버넌스 구축을 위한 기반을 마련하는 중요한 과정이다\n\n\n\n\n\nAug 10, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Model (6)\n\n\n데이터 모델링 기초: 개념적 데이터 모델링 (Conceptual Data Modeling)의 ER Modeling\n\n\n\nData Governance\n\n\n\n데이터베이스 설계 초기 단계에서 비즈니스 요구사항을 추상화하는 과정, 주요 구성 요소인 엔티티, 속성, 관계의 개념, 그리고 ERD(Entity-Relationship Diagram)의 기본 원리를 설명한다. 데이터베이스 설계를 시작하는 초보자를 위한 정보를 제공한다.\n\n\n\n\n\nAug 9, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Model (5)\n\n\n데이터 모델링 기초: 업무기술서 작성법 (Requirements Collection & Analysis)\n\n\n\nData Governance\n\n\n\n이 블로그에서는 데이터베이스 설계 프로세스의 초기 단계를 다룬다. 그리고 업무기술서 작성 방법을 설명한다. 데이터 모델링과 데이터베이스 설계 프로젝트를 시작하는 실무자들을 위한 정보를 제공한다.\n\n\n\n\n\nAug 8, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Model (4)\n\n\n데이터 모델링 기초: Data Modeling과 DB 설계 기초\n\n\n\nData Governance\n\n\n\n이 블로그에서는 데이터 모델링과 데이터베이스 설계의 핵심 개념을 다룬다. DB 스키마, 데이터 모델링과 DB 설계의 차이, 설계 단계, ERD(Entity-Relationship Diagram)의 구성 요소 등을 상세히 설명한다.\n\n\n\n\n\nAug 7, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n데이터 저장소의 핵심 개념\n\n\nData Lake, Data Warehouse, Data Mart의 차이점과 추가적인 저장소 개념\n\n\n\nEngineering\n\n\n\n이 글에서는 Data Lake, Data Warehouse, Data Mart의 차이점과 각각의 특징을 설명하고, ODS, Data Lakehouse, NoSQL 및 Graph Databases와 같은 추가적인 데이터 저장소 개념도 함께 정리한다. \n\n\n\n\n\nAug 6, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Model (3)\n\n\n데이터 모델링 기초: DBMS를 다루는 언어, SQL의 개념\n\n\n\nData Governance\n\n\n\n이 블로그에서는 SQL(Structured Query Language)의 기본 개념과 구조를 소개한다. DDL, DML, DCL, TCL 등 SQL의 주요 구성 요소들을 설명하고, 각각의 예시 코드를 제공한다. 또한 SQL의 비절차적 특성과 그 장점을 다루며, 데이터베이스 설계와 SQL 사용에 관한 기초 정보를 제공한다.\n\n\n\n\n\nAug 6, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Model (2)\n\n\n데이터 모델링 기초: 데이터베이스의 기초: 개념부터 구조까지\n\n\n\nData Governance\n\n\n\n이 블로그에서는 데이터베이스를 처음 접하는 사람들을 위한 데이터베이스의 기본 개념, 중요성, 특성에 대해 설명한다. 데이터베이스 시스템의 구조, 테이블의 구성 요소, 그리고 SQL 쿼리 언어에 대한 소개를 포함하고 있다.\n\n\n\n\n\nAug 5, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Model (1)\n\n\n데이터 구조 관리를 위한 데이터 모델링\n\n\n\nData Governance\n\n\n\n데이터 거버넌스를 가장 크게 차지하는 부분이 데이터 구조 관리이다. 데이터 구조 관리를 위해 가장 먼저 선행되고 데이터 엔지니어 실무자들이 프로젝트 초기에 공수를 들이는 과정이 Data Modeling이다.\n\n\n\n\n\nAug 4, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Data Architecture Management (구조 관리)\n\n\n데이터 거버넌스를 위한 데이터 구조 관리\n\n\n\nData Governance\n\n\n\n데이터 구조 관리를 위해 Data Architecture와 Data Modeling이 확립되어야 한다.\n\n\n\n\n\nAug 3, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Task Process\n\n\n개념 및 업무 절차에 대한 리스트\n\n\n\nData Governance\n\n\n\n데이터 거버넌스에 대한 체계적인 지식 정리와 업무 절차 이해를 위한 항목\n\n\n\n\n\nAug 2, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Governance Study - Basic\n\n\n개념 및 업무 절차에 대한 리스트\n\n\n\nData Governance\n\n\n\n데이터 거버넌스에 대한 체계적인 지식 정리와 업무 절차 이해를 위한 항목\n\n\n\n\n\nAug 1, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n시계열 분석 기초 개념 - 정상성(stationarity)\n\n\nstationarity vs non-stationarity\n\n\n정상성(stationarity)은 시계열 데이터의 통계적 특성이 시간에 따라 일정하게 유지되는 상태를 말한다. \n\n\n\n\n\nJul 8, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series (시계열) Data Analysis\n\n\n시계열 데이터에 대한 이해, 처리 및 분석\n\n\n시계열 데이터 분석에 대한 학습 및 정리 \n\n\n\n\n\nJul 8, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nOCR\n\n\n\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMar 15, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nLinux_Error_Fix_rm_cannot_remove_files_busy\n\n\nWSL\n\n\n\nEngineering\n\n\n\nrm: cannot remove ‘files’: Device or resource busy” 오류 메시지는 유닉스나 리눅스 환경에서 시스템이나 어플리케이션이 사용 중인 디렉토리나 파일을 삭제하려고 할 때 흔히 발생한다. 그 대처법을 알아보자. \n\n\n\n\n\nJan 25, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nVisitor Tracking\n\n\nGoogle Analytics\n\n\n\nEngineering\n\n\n\nBasic HTTP Methods \n\n\n\n\n\nJan 25, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nWebsite Monetization\n\n\nGoogle AdSense\n\n\n\nEngineering\n\n\n\nBasic HTTP Methods \n\n\n\n\n\nJan 25, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPublic Data\n\n\nTemplate\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nJan 1, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPathlib Library\n\n\nFile System Path Management\n\n\n\nEngineering\n\n\n\npathlib은 Python의 표준 라이브러리 중 하나로, 파일 시스템 경로를 객체 지향적인 방식으로 쉽게 다룰 수 있게 해주는 모듈이다. 이전에는 파일 시스템 경로를 문자열로 처리했지만, pathlib을 사용하면 경로를 Path 객체로 표현하여 경로에 대한 다양한 작업을 보다 직관적이고 효율적으로 수행할 수 있다. \n\n\n\n\n\nJul 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPackage Management - 1\n\n\nrequirements.text 사용\n\n\n\nEngineering\n\n\n\nEngineering for Data Science \n\n\n\n\n\nJun 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nAirflow Introduction\n\n\nTemplate\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nEnvironment Setting for Airflow\n\n\nWSL, Docker Installation, Airflow Installation, Development Environment Setting, Python Interpreter Installation, VScode Installation, Git Evnvironment Setting, Airflow Library Installation\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nOperator Baisc (Bash Operator)\n\n\nBasic Operator(Bash Operator), Cron Scheduling, Task Dependencies(Connection), External Script File Operation, Email Operator\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPython Operator\n\n\nBasic Python Operator, Importing External Python Scripts, Usage of Decorator, Understanding of Python Parameters, op_args (airflow), op_kwargs (airflow)\n\n\n\nEngineering\n\n\n\nAirflow의 Python Operator에 op_args로 변수를 할당하는 방법 \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nJinja Template, Bash Operator with Jinja Template, Airflow Datetime Count, Python Operator with Jinja Template, Bash Operator with Macros, Python Operator with Macro\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Share\n\n\nPython Operator with Xcom, Bash Operator with Xcom, Xcom between Python Operator and Bash Operator, Xcom between Python Operator and Email Operator, Global Share Variable\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTask Handling Techniques\n\n\nBranchPython Operator (Branch Processing), @task.brancch (Branch Processing), BaseBranchOperator (Branch Processing), Trigger Rule Setting, Task Groups, Edge Labels\n\n\n\nEngineering\n\n\n\nAdvanced Techniques to handle tasks (Branch Processing) \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nMore Operators\n\n\nMore Operators Provided by Airflow, Trigger Dag Run Operators, Obtaining Seoul Public Data API Key, Retrieve Seoul Public Data API Using SimpleHttp Operator, Custom Operator Development\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nConnection & Hook\n\n\ndocker_compose.yaml 해석, Postgres Container 올리기, Connection, Hook, bulk_load(), Custom Hook, More Providers, Connection Type\n\n\n\nEngineering\n\n\n\nPostgreSQL DB Container 띄우기와 Connection & Hook 설정하기 \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nSensor\n\n\nSensor Concept, Bash Sensor, File Sensor, Python Sensor, External Task Sensor, Custom Sensor Creation\n\n\n\nEngineering\n\n\n\nSensor는 특정 조건이 만족하면 task를 실행하게하는 Operator. 실시간에 가까운 workflow를 가능하게 하는 기능. \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nAirflow Additional Function\n\n\nDag Triggering Using Dataset, defult_args Parameter of Dag, Sending an Email When a Task Fails, Task Operation Monitoring using sla and Emailing, Setting timeout, CLI Usage of dag trigger and backfill clear, Triggerer\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nConda Introduction\n\n\nConda Introduction, Conda Installation\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nDocker Introduction, Docker Installation\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nGit Introduction\n\n\nGit Introduction & Installation\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nLinux Commands\n\n\nWSL\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nVS code Introduction\n\n\nVS code Introduction and Installation\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nGET, POST, PUT, DELETE, HEAD, PATCH, OPTIONS\n\n\n\nEngineering\n\n\n\nBasic HTTP Methods \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nWSL Install\n\n\nWSL\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nInfrastructure Security\n\n\nWeek2\n\n\n\nEngineering\n\n\n\nAWS \n\n\n\n\n\nApr 5, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nStorage and Database\n\n\nWeek3\n\n\n\nEngineering\n\n\n\nAWS \n\n\n\n\n\nMar 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nStorage and Database\n\n\nWeek3\n\n\n\nEngineering\n\n\n\nAWS \n\n\n\n\n\nMar 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nComputing and Networking\n\n\nWeek2\n\n\n\nEngineering\n\n\n\nAWS \n\n\n\n\n\nMar 9, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (9) Priority Queue\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nFeb 3, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (8) Binary Search Tree\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (7) Deque\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 26, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (10) Graph\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 20, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (6) Queue\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 19, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (5) Stack\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 19, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (2) Array\n\n\nArray\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (3) Linked List\n\n\nLinked List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (1) Overview\n\n\nOverview\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 17, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (4) Python List\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 17, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nFDA Software Validation Guidance Presentation\n\n\nSource: General Principles of Software Validation\n\n\n\nSurveilance\n\n\n\nThe purpose of this article is to help understand the summary of the ‘General Principles of the ’Software Validation; Final Guidance for Industry and FDA Staff’ document issued on 2002-01-11. This article provides short sentences with many diagrams for intuitive understanding. \n\n\n\n\n\nDec 28, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nFDA Software Validation Guidance Summary\n\n\nDcoument: General Principles of Software Validation\n\n\n\nSurveilance\n\n\n\nThe purpose of this blog is to get a rough concept of the FDA approval process by making a summary of the ‘General Principles of the ’Software Validation; Final Guidance for Industry and FDA Staff’ document issued on 2002-01-11. So far, the document seems to be still valid taking into account that its guidance for the FDA approval are broad, general, and comprehensive, and that many recent FDA documents supplement it. \n\n\n\n\n\nDec 15, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/blog/posts/Language/mosaic.html",
    "href": "docs/blog/posts/Language/mosaic.html",
    "title": "Mosaic Package",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n수식 base로 함수를 그릴 수 있는 package\n\n\nCode\nlibrary(tidyverse)\nlibrary(mosaic)\n\nrm(list=ls())\n\n\n\n\nCode\nplotFun(x1^2~x1,\nx1.lim=range(-10,10),\nx2.lim=range(-10,10),\nsurface=TRUE,\n# surface=FALSE, # contour 를 보여줌\nxlab=expression(x[1]),\nylab=expression(f(x)))\n\nplotFun(5*x1^2~x1 & x2,\nx1.lim=range(-10,10),\nx2.lim=range(-10,10),\nsurface=TRUE,\nxlab=expression(x[1]),\nylab=expression(x[2]),\nzlab=expression(f(x[1],x[2])))\n\n\n그려진 그래프를 R studio에선 plot의 톱니바퀴를 누르면 축별로 회전이 가능하지만 vs code에서는 지원이 안되는 것 같음\n\n\n\n\n\n\n\n\n\n\nProject Content List\n\n\n\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/Language/mosaic.html#plotfun",
    "href": "docs/blog/posts/Language/mosaic.html#plotfun",
    "title": "Mosaic Package",
    "section": "",
    "text": "수식 base로 함수를 그릴 수 있는 package\n\n\nCode\nlibrary(tidyverse)\nlibrary(mosaic)\n\nrm(list=ls())\n\n\n\n\nCode\nplotFun(x1^2~x1,\nx1.lim=range(-10,10),\nx2.lim=range(-10,10),\nsurface=TRUE,\n# surface=FALSE, # contour 를 보여줌\nxlab=expression(x[1]),\nylab=expression(f(x)))\n\nplotFun(5*x1^2~x1 & x2,\nx1.lim=range(-10,10),\nx2.lim=range(-10,10),\nsurface=TRUE,\nxlab=expression(x[1]),\nylab=expression(x[2]),\nzlab=expression(f(x[1],x[2])))\n\n\n그려진 그래프를 R studio에선 plot의 톱니바퀴를 누르면 축별로 회전이 가능하지만 vs code에서는 지원이 안되는 것 같음"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html",
    "title": "ANCOVA",
    "section": "",
    "text": "(Draft, 바쁘니까 일단 대충이라도 적어놓음 ㅠ)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#description",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#description",
    "title": "ANCOVA",
    "section": "1 Description",
    "text": "1 Description\nANCOVA (Analysis of Covariance, ANCOVA)\n\nANOVA에 공변량 (covariate)을 추가하여 분석 수행\n공변량을 조정하여 독립변수의 순수한 영향을 검정\n공변량: 연속형 변수로 한정"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#example",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#example",
    "title": "ANCOVA",
    "section": "2 Example",
    "text": "2 Example\n\n2.1 Load Libraries and Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(faraway)\nlibrary(markdown)\nlibrary(effects)\nlibrary(HH)\nlibrary(psych)\n\n\n\n\n2.2 Data Description\n\n\nCode\nstr(sexab)\n\n\n'data.frame':   76 obs. of  3 variables:\n $ cpa : num  2.048 0.839 -0.241 -1.115 2.015 ...\n $ ptsd: num  9.71 6.17 15.16 11.31 9.95 ...\n $ csa : Factor w/ 2 levels \"Abused\",\"NotAbused\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nR console에 ?sexab를 입력하면 다음과 같은 설명이 나온다.\nPost traumatic stress disorder in abused adult females\nThe data for this example come from a study of the effects of childhood sexual abuse on adult females. 45 women being treated at a clinic, who reported childhood sexual abuse, were measured for post traumatic stress disorder and childhood physical abuse both on standardized scales. 31 women also being treated at the same clinic, who did not report childhood sexual abuse were also measured. The full study was more complex than reported here and so readers interested in the subject matter should refer to the original article.\n즉, 요약하면 아동기에 성폭력을 겸험한 성인들의 정신 건강을 측정한 데이터로서, 아동기의 성폭력 경험과 학대 경험이 성인기의 정신건강에 유의한 영향을 미치는지에 대한 실험을 한 것이다.\n이 data는 3개의 변수와 76개의 samples을 포함한다.\n\ncpa : Childhood physical abuse on standard scale, covariate\nptsd : post-traumatic stress disorder on standard scale, response variable\ncsa : Childhood sexual abuse - abused or not abused, independent variable\n\n친절하게 response variable, independent variable 및 covariate을 규명해놓았다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#eda",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#eda",
    "title": "ANCOVA",
    "section": "3 EDA",
    "text": "3 EDA\n\n3.1 Descriptive Statistics\n\n\nCode\ntemp&lt;-describeBy(ptsd~csa,data=sexab)\ntemp&lt;-rbind('abused'=temp$Abused,'notAbused'=temp$NotAbused)%&gt;%\nas.data.frame()\ntemp%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n\nabused\n1\n45\n11.941093\n3.440151\n11.31277\n11.883422\n3.857355\n5.98491\n18.99251\n13.00760\n0.1556159\n-0.9124483\n0.5128275\n\n\nnotAbused\n1\n31\n4.695874\n3.519743\n5.79447\n4.903441\n1.978841\n-3.34921\n10.91447\n14.26368\n-0.6589170\n-0.2008051\n0.6321645\n\n\n\n\n\n위의 요약된 기술 통계량들 중 표준 편차는 유사하지만 평균 ptsd가 약 7.245219의 차이를 보여준다. 아래의 histogram역시 성폭력을 경험한 그룹과 경험하지 않은 그룹간의 PTSD 수치가 다른것을 볼 수 있다.\n\n\nCode\nggplot(data=sexab,aes(x=ptsd,color=csa,fill=csa))+\ngeom_histogram(aes(y=..density..),position=\"identity\",fill='white')+\ngeom_density(alpha=0.5)+\nlabs(title=\"Histogram, PTSD Grouped by Childhood Sexual Abuse Experience\", x=\"PTSD\", y=\"Desnsity\")\n\n\n\n\n\n\n\n3.2 One-Way ANOVA\n성폭력 경험 유무에 따른 PTSD 평균 차이가 통계적으로 유의한지 확인하기 위해 ANOVA를 수행한다.\n\n\nCode\nsexab_aov&lt;-aov(ptsd~csa, data=sexab)\nsummary(sexab_aov)\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncsa          1  963.5   963.5    79.9 2.17e-13 ***\nResiduals   74  892.4    12.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n집단간 변수 csa p value가 &lt;0.05 인 것을 확인할 수 있다. csa는 5% 유의수준에서 유의하다.\n하지만 PTSD의 변동량은 아동 학대에 의해 설명될 수도 있기 때문에 ptsd의 평균은 csa뿐만 아니라 cpa에 또한 고려되어야한다.\n\n\nCode\nggplot(data=sexab,aes(x=cpa,y=ptsd))+geom_point()+geom_smooth(method=\"lm\")+\nlabs(title=\"Scatter Plot, PTSD vs CPA\", x=\"CPA\", y=\"PTSD\")\n\n\n\n\n\nCode\ncorrelation&lt;-cor.test(sexab$cpa,sexab$ptsd, method='pearson')\n\n\n그림과 같이 CPA가 증가하면서 PTSD또한 선형적으로 증가하는 패턴을 관찰할 수 있다. 두 변수간의 상관계수 = 0.49이고 p value= 6.2715909^{-6}으로 보아 두 변수 사이에 선형적인 상관관계가 있는 것으로 보인다.\n\n\nCode\nggplot(data=sexab,aes(x=cpa,y=ptsd))+geom_point()+geom_smooth(method=\"lm\")+\nfacet_wrap(.~csa)+\nlabs(title=\"Scatter Plot, PTSD vs CPA Grouped By CSA\", x=\"CPA\", y=\"PTSD\")\n\n\n\n\n\n아동기 성폭력 경험 유/무에도 PTSD와 CPA와 선형적인 관계가 있는 것으로 보이기 때문에 CSA의 PTSD로의 효과를 검정하기 위해선 CPA를 조정할 필요가 있는것으로 보인다.\n\n\nCode\n# ptsd로의 순수한 성폭력 경험의 영향도를 얻기 위해서는 아동기 신체적 학대(공변량)에 대해서 고려를 해줘야함\n\nsexab_aov&lt;-aov(ptsd~cpa+csa, data=sexab) \nsummary(sexab_aov)\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncpa          1  449.8   449.8   41.98 9.46e-09 ***\ncsa          1  624.0   624.0   58.25 6.91e-11 ***\nResiduals   73  782.1    10.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n아동기의 신체적 학대가 일정하다는 가정하에서 PTSD와 성폭력의 순수한 관계는 5% 유의수준에서 유의하고 공변량, CPA를 조정하기전과 그 유의성이 차이가 있음을 관찰할 수 있다.\n\n\nCode\n# CPA가 제거 된 후에 CSA의 순수한 효과를 알아보기\n\nancova(ptsd~cpa+csa, data=sexab) \n\n\nAnalysis of Variance Table\n\nResponse: ptsd\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ncpa        1 449.80  449.80  41.984 9.462e-09 ***\ncsa        1 624.03  624.03  58.247 6.907e-11 ***\nResiduals 73 782.08   10.71                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n두 csa집단에서 두 회귀선의 기울기 같고 절편이 다르게 나타나는 것을 관찰 할 수있다. 기울기가 같은 이유는 cpa가 ptsd에 영향을 미치는 정도가 두집단에서 일정하도록 공변량으로서 통제 했기 때문이다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#blog-guide-map-link",
    "title": "ANCOVA",
    "section": "4 Blog Guide Map Link",
    "text": "4 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "template.html",
    "href": "template.html",
    "title": "Template",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe radius of the circle is 10.\n\n\n\n1 Go to Project Content List\nProject Content List\n\n\n2 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html",
    "href": "docs/projects/qc_platform/index.html",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n진단 장비의 품질 관리는 의료 장비와 연관된 제품의 특성상 Global Market 진출시 각 나라의 정부에서 요구하는 규제사항 중 하나이다.\n\n시약의 안정성 검증 요구\n장비의 안정성 검증 요구\nSoftware의 안정성 검증 요구\nDiagnostic Algorithm의 안정성 검증 요구\n\n현재 Seegene이 사용하고 있는 진단 장비는 자사 제품이 아니기 때문에 진단 장비의 품질 관리 방식에 어려움이 있다.\nPCR 기기의 noise test는 의료 장비의 QC process 중 하나로서, Seegene의 시약 제품의 성능 안정성과 직접적으로 영향을 주는 품질 검증 시스템이다.\n회사의 규모가 커지고 잦은 조직 개편으로 수동 방식의 noise tests가 여러 부서로 이관됨에 따라 noise test 수행자의 이해도와 숙련도가 떨어져 noise test가 올바르게 수행되지 않고 있다.\n씨젠의 장기 목표인 전사 자동화를 위해 noise test를 자동화하는 프로젝트가 발탁됐다.\n\n\n\n\n\n2020.12.19~2021.03.06에 입고된 PCR기기 2201대를 2552번의 실험에서 만들어진 61,248개의 신호에서 얻은 data-driven parameters로 장비의 성능을 평가하여 합격/불합격 뿐만 아니라 장비에 등급을 차등 부여하여 시간에 따른 장비의 성능을 지속적으로 분석 가능하게 한다.\n다음의 주요 문제점을 개선한다.\n\n신호의 증폭 크기에 따라 noise test 결과에 영향을 크게 주어 잘못된 결과를 산출해주는 metric 개선\n단순한 휴먼 에러 신호에 무조건적으로 장비의 불합격처리가 결정 되는 문제 보완하여 robust한 평가체계로 현업부서의 부담을 덜어준다.\n장비 고유에서 발생하는 pattern을 찾아 장비 error 신호를 labeling 한다.\n\n20번의 test를 수동으로 계산하는 과정에 30분이 소요되는 것을 웹 기반의 자동화로 약 2~3분내로 단축시킬 수 있다.\n씨젠의 full automation을 위한 best practice example로 만들어 IT 부분과 제조 부문 및 BT부문과의 협력체계 구축 및 활성화 한다.\n시각화와 noise test result history를 제공하여 추적 및 VOC 대응 system을 구축한다.\n\n\n\n\n\n\n\n현업 부서와의 긴밀한 소통으로 QC process를 세분화하여 앞 단계 QC에서 발생하는 data를 활용하여 뒷 단계 QC인 noise test의 결과를 예측한다.\nnoise가 적다고 확실시 되는 기기에 한해서 noise test 생략\n\n\n\n\n\n전체 QC 프로세스를 자동화 또는 반 자동화\n기존의 noise 측정 metric 분석 및 새로운 metric 생성하여 검사 결과의 정확도를 향상\n시각화와 noise test result history를 제공하여 실무자의 이해도를 높이고 관리가능하게 한다.\n\n\n\n\n\n\n현업 업무 기술서 부재\n실무자의 백업 실수\n부서마다 산재된 데이터\n높은 난이도의 data cleansing\n\n실무자의 데이터로부터 분석 가능한 데이터 선별\n실무자의 데이터의 오입력\ndata 및 문서의 DRM 수동해제\n\nreverse engineering 필요\n잦은 조직 개편으로 관련 인원 및 부서 연락체계 부재\n\n\n\n\n\nBack-end Engineering for DevOps Pipeline Construction\nData Engineering for data cleansing and reverse engineering\nData Modeling for a RDB system construction\nStatistical Analysis for the noise test performance verification\nMachine Learning for pattern analysis\nFront-end Engineering for UI/UX construction\nBiologics, Biophysics, Physics Knowledge\n\n\n\n\n\n1 data scientist (me) - project owner\n5 mechanical engineers\n4 biologists\n2 patent attorneys\n2 data engineers\n3 full stack developers\n1 advisor (a professor of Computer Science at Seoul National University)\n\n\n\n\n\n\n\n\n\n\n\n\nNoise Test\nAs-Is\nTo-Be\n\n\n\n\nQC 알고리즘 개발에 사용되는 샘플 크기\nn=100\nSignals from 2552 experiments, n=61,248\n\n\nQC 알고리즘 성능 비교에 사용되는 샘플 크기\nn=61,248\nn=61,248\n\n\nEvaluation Metrics\n2 metrics\n10 metrics ( the exsting 2 metrics + new 8 metrics)\n\n\nInput Process\n특정 프로그램에서 추출한 엑셀 파일의 데이터를 수동으로 복사하여 붙여넣기\n웹 기반 자동화, 다수의 실험 파일 업로드\n\n\nOutput Process\nBatch Evaluation method,\n\n\n\n장비의 신호 중 하나라도 부적합 판정되면 장비 자체가 QC 부적합 판정 (맹점: 휴먼에러 신호가 1개라도 있으면 장비는 무조건 실격 처리)\nDifferential Evaluation method, 장비의 신호에 점수를 계산 후 평균값을 구하고 장비 등급을 A+, A, B, F로 지정. F인 경우 부적합. 오류 신호를 평가에서 제외하므로 오류 신호에 robust\n\n\n\nOutput 1\npass: 92.58%, fail: 7.42% (after excluding many human errors)\nA+ (pass): 7.01%, A (pass): 12.91%, B (pass): 75.72%, F (fail): 4.36%\n\n\nOutput 2\nNA\nVisualized Plots and Tables.\n\n\nOutput 3\nNA\nClassfication Results: Normal Signals, Human Errors, Device Errors, Manufacturing Errors\n\n\nTime Consumed\nAbout 30 minutes per 20 experiments\nAbout 25 minutes per 2552 experiments\n\n\nData Management\nNon-standard management method (작업자마다 다른 방식으로 Excel 파일로 다른 형태로 NAS 디렉토리에 저장)\nRDB uploaded by a scheduler (장비 고장 추적 분석이 가능)\n\n\n\n\n\n\n1% 유의수준에서 노이즈 테스트 결과가 전체 진단 과정에서 최종 결과의 평균 차이에 큰 영향을 미치지 않는다는 것을 통계적으로 증명했기 때문에 전체 QC 프로세스에서 노이즈 테스트를 폐지.\n데이터 사이언스 부서에서 실험과 장비에서 생성된 데이터에 대한 이해도를 높일 수 있는 기회가 됐음.\n전사 DB구축 및 플랫폼 아키텍처 구축의 기반이 됨.\n\n\n\n\n\n\n\n\nQuality Control of diagnostic equipment is one of the necessities for the regulations required by the government of each country when entering the global market due to the nature of products related to medical equipment.\n\nReagent stability verification & validation required\nEquipment stability verification & validation request\nSoftware stability verification & validation request\nStability Verification & validation Request of Diagnostic Algorithm\n\nSince the diagnostic equipment currently being used by Seegene is not its own product, it is difficult to manage the quality of the diagnostic equipment.\nThe noise test of PCR equipment is one of the QC processes of medical equipments, and it is a quality verification system that directly affects the performance stability of Seegene’s reagent products.\nAs the size of the company grows and frequent organizational reshuffles result in manual noise tests being transferred to various departments, the noise test performers’ understanding and skills are low, resulting in noise tests not being performed correctly.\nA project to automate the noise test was selected for Seegene’s long-term goal of enterprise automation.\n\n\n\n\n\nData-driven parameters obtained from 61,248 signals from 2552 experiments were used to evaluate 2201 PCR devices, which were stocked between 2020.12.19 and 2021.03.06. yield Enables continuous analysis of equipment performance over time.\nImprove the following major problems:\n\nImproved metrics that produces erroneous results by greatly affecting the noise test result depending on the size of the signal amplification.\nIt relieves the burden on the field department with a robust evaluation system by supplementing the problem of unconditionally determining equipment rejection in response to a simple human error signal.\nEquipment error signals are labeled by finding patterns that occur in equipment.\n\nThe time required for the manual calculation process can be reduced from 30 minutes per 20 tests to about 2 to 3 minutes with web-based automation.\nBy making this project the best practice example of Seegene’s full automation, establish and vitalize the cooperation system between the IT, manufacturing and BT sectors.\nVisualization and noise test result history are provided to build a tracking and VOC response system.\n\n\n\n\n\n\n\nThe QC process is subdivided through close communication with the field departments, and the results of the noise test, which is the next stage QC, are predicted by utilizing the data generated in the previous stage QC.\nNoise test omitted only for devices that are certain to have low noise.\n\n\n\n\n\nAutomate or semi-automate the entire QC process\nImprove the accuracy of inspection results by analyzing the existing noise measurement metric and creating a new metric\nVisualization and noise test result history are provided to increase the understanding of practitioners and enable management\n\n\n\n\n\n\nAbsence of job description\nBackup Mistakes by Practitioners\nData scattered across departments\nHigh level of data cleansing\n\nSelect data that can be analyzed from practitioner data\nIncorrect input of practitioner data\nManual release of DRM for data and documents\n\nreverse engineering required\nAbsence of contact system for related personnel and departments due to frequent organizational reshuffle\n\n\n\n\n\nBack-end Engineering for DevOps Pipeline Construction\nData Engineering for data cleansing and reverse engineering\nData Modeling for a RDB system construction\nStatistical Analysis for the noise test performance verification\nMachine Learning for pattern analysis\nFront-end Engineering for UI/UX construction\nBiologics, Biophysics, Physics Knowledge\n\n\n\n\n\n1 data scientist (me) - project owner\n5 mechanical engineers\n4 biologists\n2 patent attorneys\n2 data engineers\n3 full stack developers\n1 advisor (a professor of Computer Science at Seoul National University)\n\n\n\n\n\n\n\n\n\n\n\n\nNoise Test\nAs-Is\nTo-Be\n\n\n\n\nSample Size Used for QC Algorithm Development\nn=100\nSignals from 2552 experiments, n=61,248\n\n\nSample Size Used for QC Algorithm Performance Comparison\nn=61,248\nn=61,248\n\n\nEvaluation Metrics\n2 metrics\n10 metrics ( the exsting 2 metrics + new 8 metrics)\n\n\nInput Process\nmanually copy & paste data of excel files extracted from a certain program\nWeb-Based Automation, upload multiple experiment files\n\n\nOutput Process\nBatch Evaluation method, if even one of the signals from the equipment fails, the equipment fails. (Blind Spot: If there is one human error signal, the equipment is unconditionally disqualified.)\nDifferential Evaluation method, The signals from the equipment are scored, the average value is obtained, and the equipment is graded A+, A, B, and F. Failed if F. Robust on error signals as it excludes the error signal from evaluation.\n\n\nOutput 1\npass: 92.58%, fail: 7.42% (after excluding many human errors)\nA+ (pass): 7.01%, A (pass): 12.91%, B (pass): 75.72%, F (fail): 4.36%\n\n\nOutput 2\nNA\nVisualized Plots and Tables.\n\n\nOutput 3\nNA\nClassfication Results: Normal Signals, Human Errors, Device Errors, Manufacturing Errors\n\n\nTime Consumed\nAbout 30 minutes per 20 experiments\nAbout 25 minutes per 2552 experiments\n\n\nData Management\nNon-standard management method (stored in NAS directory in a different form as an Excel file in a different way for each worker)\nAutomatic loading in DB in standardized form by a scheduler (it is possible to conduct an equipment failure tracing analysis)\n\n\n\n\n\n\nThe noise test was abolished in the whole QC Process because I statistically proved that the noise test result does not have a significant impact on the difference of the mean of Ct, the final result in the whole diagnostic process at the 1% significance level.\nIt turned out to be an opportunity for the Data Science department to increase their understanding of experiments and data generated from equipment.\nIt served as the basis for building a company-wide DB and building a platform architecture."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#background",
    "href": "docs/projects/qc_platform/index.html#background",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "진단 장비의 품질 관리는 의료 장비와 연관된 제품의 특성상 Global Market 진출시 각 나라의 정부에서 요구하는 규제사항 중 하나이다.\n\n시약의 안정성 검증 요구\n장비의 안정성 검증 요구\nSoftware의 안정성 검증 요구\nDiagnostic Algorithm의 안정성 검증 요구\n\n현재 Seegene이 사용하고 있는 진단 장비는 자사 제품이 아니기 때문에 진단 장비의 품질 관리 방식에 어려움이 있다.\nPCR 기기의 noise test는 의료 장비의 QC process 중 하나로서, Seegene의 시약 제품의 성능 안정성과 직접적으로 영향을 주는 품질 검증 시스템이다.\n회사의 규모가 커지고 잦은 조직 개편으로 수동 방식의 noise tests가 여러 부서로 이관됨에 따라 noise test 수행자의 이해도와 숙련도가 떨어져 noise test가 올바르게 수행되지 않고 있다.\n씨젠의 장기 목표인 전사 자동화를 위해 noise test를 자동화하는 프로젝트가 발탁됐다."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#objective",
    "href": "docs/projects/qc_platform/index.html#objective",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "2020.12.19~2021.03.06에 입고된 PCR기기 2201대를 2552번의 실험에서 만들어진 61,248개의 신호에서 얻은 data-driven parameters로 장비의 성능을 평가하여 합격/불합격 뿐만 아니라 장비에 등급을 차등 부여하여 시간에 따른 장비의 성능을 지속적으로 분석 가능하게 한다.\n다음의 주요 문제점을 개선한다.\n\n신호의 증폭 크기에 따라 noise test 결과에 영향을 크게 주어 잘못된 결과를 산출해주는 metric 개선\n단순한 휴먼 에러 신호에 무조건적으로 장비의 불합격처리가 결정 되는 문제 보완하여 robust한 평가체계로 현업부서의 부담을 덜어준다.\n장비 고유에서 발생하는 pattern을 찾아 장비 error 신호를 labeling 한다.\n\n20번의 test를 수동으로 계산하는 과정에 30분이 소요되는 것을 웹 기반의 자동화로 약 2~3분내로 단축시킬 수 있다.\n씨젠의 full automation을 위한 best practice example로 만들어 IT 부분과 제조 부문 및 BT부문과의 협력체계 구축 및 활성화 한다.\n시각화와 noise test result history를 제공하여 추적 및 VOC 대응 system을 구축한다."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#strategies",
    "href": "docs/projects/qc_platform/index.html#strategies",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "현업 부서와의 긴밀한 소통으로 QC process를 세분화하여 앞 단계 QC에서 발생하는 data를 활용하여 뒷 단계 QC인 noise test의 결과를 예측한다.\nnoise가 적다고 확실시 되는 기기에 한해서 noise test 생략\n\n\n\n\n\n전체 QC 프로세스를 자동화 또는 반 자동화\n기존의 noise 측정 metric 분석 및 새로운 metric 생성하여 검사 결과의 정확도를 향상\n시각화와 noise test result history를 제공하여 실무자의 이해도를 높이고 관리가능하게 한다."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#issues",
    "href": "docs/projects/qc_platform/index.html#issues",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "현업 업무 기술서 부재\n실무자의 백업 실수\n부서마다 산재된 데이터\n높은 난이도의 data cleansing\n\n실무자의 데이터로부터 분석 가능한 데이터 선별\n실무자의 데이터의 오입력\ndata 및 문서의 DRM 수동해제\n\nreverse engineering 필요\n잦은 조직 개편으로 관련 인원 및 부서 연락체계 부재"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#required-skills",
    "href": "docs/projects/qc_platform/index.html#required-skills",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Back-end Engineering for DevOps Pipeline Construction\nData Engineering for data cleansing and reverse engineering\nData Modeling for a RDB system construction\nStatistical Analysis for the noise test performance verification\nMachine Learning for pattern analysis\nFront-end Engineering for UI/UX construction\nBiologics, Biophysics, Physics Knowledge"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#colaborators",
    "href": "docs/projects/qc_platform/index.html#colaborators",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "1 data scientist (me) - project owner\n5 mechanical engineers\n4 biologists\n2 patent attorneys\n2 data engineers\n3 full stack developers\n1 advisor (a professor of Computer Science at Seoul National University)"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#acheivements",
    "href": "docs/projects/qc_platform/index.html#acheivements",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Noise Test\nAs-Is\nTo-Be\n\n\n\n\nQC 알고리즘 개발에 사용되는 샘플 크기\nn=100\nSignals from 2552 experiments, n=61,248\n\n\nQC 알고리즘 성능 비교에 사용되는 샘플 크기\nn=61,248\nn=61,248\n\n\nEvaluation Metrics\n2 metrics\n10 metrics ( the exsting 2 metrics + new 8 metrics)\n\n\nInput Process\n특정 프로그램에서 추출한 엑셀 파일의 데이터를 수동으로 복사하여 붙여넣기\n웹 기반 자동화, 다수의 실험 파일 업로드\n\n\nOutput Process\nBatch Evaluation method,\n\n\n\n장비의 신호 중 하나라도 부적합 판정되면 장비 자체가 QC 부적합 판정 (맹점: 휴먼에러 신호가 1개라도 있으면 장비는 무조건 실격 처리)\nDifferential Evaluation method, 장비의 신호에 점수를 계산 후 평균값을 구하고 장비 등급을 A+, A, B, F로 지정. F인 경우 부적합. 오류 신호를 평가에서 제외하므로 오류 신호에 robust\n\n\n\nOutput 1\npass: 92.58%, fail: 7.42% (after excluding many human errors)\nA+ (pass): 7.01%, A (pass): 12.91%, B (pass): 75.72%, F (fail): 4.36%\n\n\nOutput 2\nNA\nVisualized Plots and Tables.\n\n\nOutput 3\nNA\nClassfication Results: Normal Signals, Human Errors, Device Errors, Manufacturing Errors\n\n\nTime Consumed\nAbout 30 minutes per 20 experiments\nAbout 25 minutes per 2552 experiments\n\n\nData Management\nNon-standard management method (작업자마다 다른 방식으로 Excel 파일로 다른 형태로 NAS 디렉토리에 저장)\nRDB uploaded by a scheduler (장비 고장 추적 분석이 가능)\n\n\n\n\n\n\n1% 유의수준에서 노이즈 테스트 결과가 전체 진단 과정에서 최종 결과의 평균 차이에 큰 영향을 미치지 않는다는 것을 통계적으로 증명했기 때문에 전체 QC 프로세스에서 노이즈 테스트를 폐지.\n데이터 사이언스 부서에서 실험과 장비에서 생성된 데이터에 대한 이해도를 높일 수 있는 기회가 됐음.\n전사 DB구축 및 플랫폼 아키텍처 구축의 기반이 됨."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#background-1",
    "href": "docs/projects/qc_platform/index.html#background-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Quality Control of diagnostic equipment is one of the necessities for the regulations required by the government of each country when entering the global market due to the nature of products related to medical equipment.\n\nReagent stability verification & validation required\nEquipment stability verification & validation request\nSoftware stability verification & validation request\nStability Verification & validation Request of Diagnostic Algorithm\n\nSince the diagnostic equipment currently being used by Seegene is not its own product, it is difficult to manage the quality of the diagnostic equipment.\nThe noise test of PCR equipment is one of the QC processes of medical equipments, and it is a quality verification system that directly affects the performance stability of Seegene’s reagent products.\nAs the size of the company grows and frequent organizational reshuffles result in manual noise tests being transferred to various departments, the noise test performers’ understanding and skills are low, resulting in noise tests not being performed correctly.\nA project to automate the noise test was selected for Seegene’s long-term goal of enterprise automation."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#objective-1",
    "href": "docs/projects/qc_platform/index.html#objective-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Data-driven parameters obtained from 61,248 signals from 2552 experiments were used to evaluate 2201 PCR devices, which were stocked between 2020.12.19 and 2021.03.06. yield Enables continuous analysis of equipment performance over time.\nImprove the following major problems:\n\nImproved metrics that produces erroneous results by greatly affecting the noise test result depending on the size of the signal amplification.\nIt relieves the burden on the field department with a robust evaluation system by supplementing the problem of unconditionally determining equipment rejection in response to a simple human error signal.\nEquipment error signals are labeled by finding patterns that occur in equipment.\n\nThe time required for the manual calculation process can be reduced from 30 minutes per 20 tests to about 2 to 3 minutes with web-based automation.\nBy making this project the best practice example of Seegene’s full automation, establish and vitalize the cooperation system between the IT, manufacturing and BT sectors.\nVisualization and noise test result history are provided to build a tracking and VOC response system."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#strategies-1",
    "href": "docs/projects/qc_platform/index.html#strategies-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "The QC process is subdivided through close communication with the field departments, and the results of the noise test, which is the next stage QC, are predicted by utilizing the data generated in the previous stage QC.\nNoise test omitted only for devices that are certain to have low noise.\n\n\n\n\n\nAutomate or semi-automate the entire QC process\nImprove the accuracy of inspection results by analyzing the existing noise measurement metric and creating a new metric\nVisualization and noise test result history are provided to increase the understanding of practitioners and enable management"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#issues-1",
    "href": "docs/projects/qc_platform/index.html#issues-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Absence of job description\nBackup Mistakes by Practitioners\nData scattered across departments\nHigh level of data cleansing\n\nSelect data that can be analyzed from practitioner data\nIncorrect input of practitioner data\nManual release of DRM for data and documents\n\nreverse engineering required\nAbsence of contact system for related personnel and departments due to frequent organizational reshuffle"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#required-skills-1",
    "href": "docs/projects/qc_platform/index.html#required-skills-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Back-end Engineering for DevOps Pipeline Construction\nData Engineering for data cleansing and reverse engineering\nData Modeling for a RDB system construction\nStatistical Analysis for the noise test performance verification\nMachine Learning for pattern analysis\nFront-end Engineering for UI/UX construction\nBiologics, Biophysics, Physics Knowledge"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#colaborators-1",
    "href": "docs/projects/qc_platform/index.html#colaborators-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "1 data scientist (me) - project owner\n5 mechanical engineers\n4 biologists\n2 patent attorneys\n2 data engineers\n3 full stack developers\n1 advisor (a professor of Computer Science at Seoul National University)"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#acheivements-1",
    "href": "docs/projects/qc_platform/index.html#acheivements-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Noise Test\nAs-Is\nTo-Be\n\n\n\n\nSample Size Used for QC Algorithm Development\nn=100\nSignals from 2552 experiments, n=61,248\n\n\nSample Size Used for QC Algorithm Performance Comparison\nn=61,248\nn=61,248\n\n\nEvaluation Metrics\n2 metrics\n10 metrics ( the exsting 2 metrics + new 8 metrics)\n\n\nInput Process\nmanually copy & paste data of excel files extracted from a certain program\nWeb-Based Automation, upload multiple experiment files\n\n\nOutput Process\nBatch Evaluation method, if even one of the signals from the equipment fails, the equipment fails. (Blind Spot: If there is one human error signal, the equipment is unconditionally disqualified.)\nDifferential Evaluation method, The signals from the equipment are scored, the average value is obtained, and the equipment is graded A+, A, B, and F. Failed if F. Robust on error signals as it excludes the error signal from evaluation.\n\n\nOutput 1\npass: 92.58%, fail: 7.42% (after excluding many human errors)\nA+ (pass): 7.01%, A (pass): 12.91%, B (pass): 75.72%, F (fail): 4.36%\n\n\nOutput 2\nNA\nVisualized Plots and Tables.\n\n\nOutput 3\nNA\nClassfication Results: Normal Signals, Human Errors, Device Errors, Manufacturing Errors\n\n\nTime Consumed\nAbout 30 minutes per 20 experiments\nAbout 25 minutes per 2552 experiments\n\n\nData Management\nNon-standard management method (stored in NAS directory in a different form as an Excel file in a different way for each worker)\nAutomatic loading in DB in standardized form by a scheduler (it is possible to conduct an equipment failure tracing analysis)\n\n\n\n\n\n\nThe noise test was abolished in the whole QC Process because I statistically proved that the noise test result does not have a significant impact on the difference of the mean of Ct, the final result in the whole diagnostic process at the 1% significance level.\nIt turned out to be an opportunity for the Data Science department to increase their understanding of experiments and data generated from equipment.\nIt served as the basis for building a company-wide DB and building a platform architecture."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html",
    "href": "docs/projects/phellinus_linteus/index.html",
    "title": "Project Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n현재까지 알려진 알레르기의 치료는, 증상이 유발된 경우 이에 대 한 즉시적인 약물치료가 전부임. 증상 치료를 위한 약물로서 가장 널리 사용되는 치료약으로는 항히스타민 약품 과 corticosteroid 계통의 약품이며, 지난 수년간 다수의 약물이 개발되었으나 일시적으로 증상을 완화하는 데 그칠 뿐 실제로 알레르기의 병태생리를 기본으로 한 치료법은 현재까지 없다. 따라서 새로운 개념의 알레르기 치료제 개발이 필요한 시점이며, 다음과 같은 몇몇 새로운 알레르기 치료법 개발이 구미와 일본을 중심으로 이루어지고 있다.\n\n\n\n\n\n확보된 알레르기 치료제 시제품이 있으며 이 시제품은 아토피환자를 대상으로 하는 소규모 임상에서 효능을 검증하였으나 이 시제품에 대한 특허 출원이 있지 않으므로 본 과제를 수행하여 특허출원을 이룸으로써 제품의 부가가치를 높일 수 있다.\n\n\n\n\n\n알레르기 동물 모델에서 효능 검증 및 림프구 혈관 생성 모델에서 효능 검증\n\nACA (Active Cutaneous Anaphlaxis) Assay\nPMA (Phorbol 12-myristate 13-acetate)-Induced Dermatitis\n\n림프구 조직에서 혈관생성 효과 검증 및 림프구 조직의 절편 및 신호 분자 분석\n\nLymphangiogenesis에 미치는 영향 분석 using western blot and immunohistochemistry\n\n\n\n\n\n\nAnyderm은 염증반응에 의해 유도되는 신혈관생성 과정을 억제할 수 있다 \nAnyderm은 TpCR 염증 동물 실험 모델에서 염증억제 효과 및 혈관생성과정의 표지단백질의 발현을 억제할 수 있다 \nAnyderm은 염증 동물 모델에서 증가하는 림프의 활성을 억제 하였으며, 이때 증가하는 LYVE-1 단백질의 발현을 저해 하였다 \n\n\n\n\n\n협력과제 책임자의 연구실에서는 지난 수년간 알레르기 반응에서의 신호전달, IgE 의존적, IgE 비의존적 알레르기 유발 동물모델을 이용한 항알레르기성 물질의 발굴등에 관한 많은 연구를 수행한 바 있다. 따라서 본 과제의 수행에 적합한 동물모델과 입증된 기술력을 갖고 있으므로 본 과제의 성공 가능성이 높다.\n천연물 발효추출물을 알레르기 치료제 시제품은 독성이 없으므로 chemical 기반의 치료제에 비해 유리한 면이 있다.\n천연물 발효추출물은 발효기법에 따라 다양한 발효추출물을 얻을 수 있다.\n\n\n\n\n\n\n\nThe treatment of allergies known so far is all about immediate drug treatment when the symptoms are triggered. Antihistamine drugs and corticosteroid drugs are the most widely used drugs for the treatment of symptoms. A number of drugs have been developed over the past few years, but only temporarily alleviate symptoms. Therefore, it is time to develop a new concept of allergy treatment, and several new allergy treatments are being developed in Europe and Japan as follows.\n\n\n\n\n\nThere is a secured allergy treatment prototype, and the efficacy of this prototype has been verified in small-scale clinical trials for atopic patients, but there is no patent application for this prototype.\n\n\n\n\n\nEfficacy validation in allergic animal models and efficacy validation in lymphocyte angiogenesis models\n\nACA (Active Cutaneous Anaphlaxis) Assay\nPMA (Phorbol 12-myristate 13-acetate)-Induced Dermatitis\n\nVerification of angiogenic effect in lymphoid tissue and analysis of slices and signal molecules in lymphoid tissue\n\nAnalysis of the effect on lymphangiogenesis using western blot and immunohistochemistry\n\n\n\n\n\n\nThis reagent can inhibit the angiogenesis process induced by the inflammatory response. \nThis reagent can suppress inflammation inhibitory effect and expression of marker protein of angiogenesis process in TpCR inflammatory animal model. \nThis reagent suppressed the increased lymphatic activity in an inflammatory animal model, and inhibited the increased LYVE-1 protein expression. \n\n\n\n\n\nOver the past few years, the laboratory of the person in charge of the cooperative project has conducted many studies on signal transduction in allergic reactions, discovery of anti-allergic substances using IgE-dependent and IgE-independent allergy-inducing animal models. Therefore, since we have an animal model suitable for the performance of this task and proven technology, the possibility of success of this task is high.\nAllergic treatment prototypes made from fermented extracts of natural substances are non-toxic, so they have an advantage over chemical-based treatments.\nVarious fermented extracts can be obtained from natural product fermentation extracts according to fermentation techniques."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#introduction",
    "href": "docs/projects/phellinus_linteus/index.html#introduction",
    "title": "Project Description",
    "section": "",
    "text": "현재까지 알려진 알레르기의 치료는, 증상이 유발된 경우 이에 대 한 즉시적인 약물치료가 전부임. 증상 치료를 위한 약물로서 가장 널리 사용되는 치료약으로는 항히스타민 약품 과 corticosteroid 계통의 약품이며, 지난 수년간 다수의 약물이 개발되었으나 일시적으로 증상을 완화하는 데 그칠 뿐 실제로 알레르기의 병태생리를 기본으로 한 치료법은 현재까지 없다. 따라서 새로운 개념의 알레르기 치료제 개발이 필요한 시점이며, 다음과 같은 몇몇 새로운 알레르기 치료법 개발이 구미와 일본을 중심으로 이루어지고 있다."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#objective",
    "href": "docs/projects/phellinus_linteus/index.html#objective",
    "title": "Project Description",
    "section": "",
    "text": "확보된 알레르기 치료제 시제품이 있으며 이 시제품은 아토피환자를 대상으로 하는 소규모 임상에서 효능을 검증하였으나 이 시제품에 대한 특허 출원이 있지 않으므로 본 과제를 수행하여 특허출원을 이룸으로써 제품의 부가가치를 높일 수 있다."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#methodology",
    "href": "docs/projects/phellinus_linteus/index.html#methodology",
    "title": "Project Description",
    "section": "",
    "text": "알레르기 동물 모델에서 효능 검증 및 림프구 혈관 생성 모델에서 효능 검증\n\nACA (Active Cutaneous Anaphlaxis) Assay\nPMA (Phorbol 12-myristate 13-acetate)-Induced Dermatitis\n\n림프구 조직에서 혈관생성 효과 검증 및 림프구 조직의 절편 및 신호 분자 분석\n\nLymphangiogenesis에 미치는 영향 분석 using western blot and immunohistochemistry"
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#result",
    "href": "docs/projects/phellinus_linteus/index.html#result",
    "title": "Project Description",
    "section": "",
    "text": "Anyderm은 염증반응에 의해 유도되는 신혈관생성 과정을 억제할 수 있다 \nAnyderm은 TpCR 염증 동물 실험 모델에서 염증억제 효과 및 혈관생성과정의 표지단백질의 발현을 억제할 수 있다 \nAnyderm은 염증 동물 모델에서 증가하는 림프의 활성을 억제 하였으며, 이때 증가하는 LYVE-1 단백질의 발현을 저해 하였다"
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#expected-effect",
    "href": "docs/projects/phellinus_linteus/index.html#expected-effect",
    "title": "Project Description",
    "section": "",
    "text": "협력과제 책임자의 연구실에서는 지난 수년간 알레르기 반응에서의 신호전달, IgE 의존적, IgE 비의존적 알레르기 유발 동물모델을 이용한 항알레르기성 물질의 발굴등에 관한 많은 연구를 수행한 바 있다. 따라서 본 과제의 수행에 적합한 동물모델과 입증된 기술력을 갖고 있으므로 본 과제의 성공 가능성이 높다.\n천연물 발효추출물을 알레르기 치료제 시제품은 독성이 없으므로 chemical 기반의 치료제에 비해 유리한 면이 있다.\n천연물 발효추출물은 발효기법에 따라 다양한 발효추출물을 얻을 수 있다."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#introduction-1",
    "href": "docs/projects/phellinus_linteus/index.html#introduction-1",
    "title": "Project Description",
    "section": "",
    "text": "The treatment of allergies known so far is all about immediate drug treatment when the symptoms are triggered. Antihistamine drugs and corticosteroid drugs are the most widely used drugs for the treatment of symptoms. A number of drugs have been developed over the past few years, but only temporarily alleviate symptoms. Therefore, it is time to develop a new concept of allergy treatment, and several new allergy treatments are being developed in Europe and Japan as follows."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#objective-1",
    "href": "docs/projects/phellinus_linteus/index.html#objective-1",
    "title": "Project Description",
    "section": "",
    "text": "There is a secured allergy treatment prototype, and the efficacy of this prototype has been verified in small-scale clinical trials for atopic patients, but there is no patent application for this prototype."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#methodology-1",
    "href": "docs/projects/phellinus_linteus/index.html#methodology-1",
    "title": "Project Description",
    "section": "",
    "text": "Efficacy validation in allergic animal models and efficacy validation in lymphocyte angiogenesis models\n\nACA (Active Cutaneous Anaphlaxis) Assay\nPMA (Phorbol 12-myristate 13-acetate)-Induced Dermatitis\n\nVerification of angiogenic effect in lymphoid tissue and analysis of slices and signal molecules in lymphoid tissue\n\nAnalysis of the effect on lymphangiogenesis using western blot and immunohistochemistry"
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#result-1",
    "href": "docs/projects/phellinus_linteus/index.html#result-1",
    "title": "Project Description",
    "section": "",
    "text": "This reagent can inhibit the angiogenesis process induced by the inflammatory response. \nThis reagent can suppress inflammation inhibitory effect and expression of marker protein of angiogenesis process in TpCR inflammatory animal model. \nThis reagent suppressed the increased lymphatic activity in an inflammatory animal model, and inhibited the increased LYVE-1 protein expression."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#expected-effect-1",
    "href": "docs/projects/phellinus_linteus/index.html#expected-effect-1",
    "title": "Project Description",
    "section": "",
    "text": "Over the past few years, the laboratory of the person in charge of the cooperative project has conducted many studies on signal transduction in allergic reactions, discovery of anti-allergic substances using IgE-dependent and IgE-independent allergy-inducing animal models. Therefore, since we have an animal model suitable for the performance of this task and proven technology, the possibility of success of this task is high.\nAllergic treatment prototypes made from fermented extracts of natural substances are non-toxic, so they have an advantage over chemical-based treatments.\nVarious fermented extracts can be obtained from natural product fermentation extracts according to fermentation techniques."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html",
    "href": "docs/projects/LLFS/self_description.html",
    "title": "Project Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n보안상의 이유로 프로젝트에서 사용됐던 실제 data를 사용하지 못하기 때문에 분석을 위해 사용됐던 방법론을 구체적으로 보여주기 어렵다. 이에 따라 대략적인 분석 방식을 고차원의 서로 상관 관계가 있는 data를 simulation을 통해 만들어 보여주려고 한다.\n\n\n\n이 시뮬레이션 연구의 목표는 AD와 None AD와 관련된 biomarkers를 구별할 수 있는 일련의 예측인자(또는 대사물질 또는 생화학물질)를 식별하는데 사용됐던 방법론을 소개하는 것이다.\n\n\n\n\n보안 문제로 인해 이 프로젝트에 사용된 실제 데이터와 전체 분석 파이프라인을 보여주기 어렵다.\n이 시뮬레이션 연구에서는 다변량 정규분포 하에서 대사 물질 데이터를 생성하여 대사 단계에서 가상의 데이터를 생성하고 분석 방법론을 기술하는 데에만 집중할 것이다.\n시뮬레이션 경험이 많지 않아 시뮬레이션이 수학적으로 통계적으로 틀린 부분이 있을 수 있다.\n시뮬레이션은 내가 수행했던 분석 방법론을 간단히 재현하는 용도로 사용하는 것이기 때문에 시뮬레이션 자체에 많은 시간을 할애하진 않았다.\n시뮬레이션 데이터는 실제 연구를 위해 표본으로 쓰인 sample 데이터의 분포를 전혀 반영하지 않았다.\n이 시뮬레이션 연구에서, 실제 데이터의 분포를 반영하지 않았고 범주형 변수 및 연속형 변수와 종속 변수를 통계적으로 잘 연관시키지 못했기 때문에 분석 결과가 생물학적인 사실과 많이 다를 수 있다.\n\n\n\n\n\nOperating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section\n\n\n\n\n\n\n\nIt is difficult to show the methodology used for analysis in detail because the actual data used in the project cannot be used for security reasons. Accordingly, I am going to show a rough analysis method through simulation of high-dimensional, mutually correlated data.\n\n\n\nThe aim of this simulation study is to identify a set of predictors (or metabolites or bio-chemicals) that will enable to differentiate bio-markers that are associated with AD vs. non-AD.\n\n\n\n\nIn this article, due to security concerns, it is difficult to display the real data and the entire analysis pipeline used in this project.\nIn this simulation study, I will focus only on generating fake data at the metabolomic stage by generating data under multivariate normal distributions.\nSince I don’t have much experience in simulation, there may be mathematically and statistically incorrect parts in the simulation.\nI did not put a lot of effort into the simulation itself because the simulation was used to simply reproduce the analysis methodology I had performed.\nThe simulated data does not reflect the distribution of the truely sampled data used in the LLFS at all.\nIn this simulation, since the categorical and continuous variables and the dependent variable could not be statistically associated properly, the analysis result for the discrete variables could be very different from the biological or medical fact.\n\n\n\n\n\nOperating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section"
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#introduction",
    "href": "docs/projects/LLFS/self_description.html#introduction",
    "title": "Project Description",
    "section": "",
    "text": "보안상의 이유로 프로젝트에서 사용됐던 실제 data를 사용하지 못하기 때문에 분석을 위해 사용됐던 방법론을 구체적으로 보여주기 어렵다. 이에 따라 대략적인 분석 방식을 고차원의 서로 상관 관계가 있는 data를 simulation을 통해 만들어 보여주려고 한다."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#goal",
    "href": "docs/projects/LLFS/self_description.html#goal",
    "title": "Project Description",
    "section": "",
    "text": "이 시뮬레이션 연구의 목표는 AD와 None AD와 관련된 biomarkers를 구별할 수 있는 일련의 예측인자(또는 대사물질 또는 생화학물질)를 식별하는데 사용됐던 방법론을 소개하는 것이다."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#feature",
    "href": "docs/projects/LLFS/self_description.html#feature",
    "title": "Project Description",
    "section": "",
    "text": "보안 문제로 인해 이 프로젝트에 사용된 실제 데이터와 전체 분석 파이프라인을 보여주기 어렵다.\n이 시뮬레이션 연구에서는 다변량 정규분포 하에서 대사 물질 데이터를 생성하여 대사 단계에서 가상의 데이터를 생성하고 분석 방법론을 기술하는 데에만 집중할 것이다.\n시뮬레이션 경험이 많지 않아 시뮬레이션이 수학적으로 통계적으로 틀린 부분이 있을 수 있다.\n시뮬레이션은 내가 수행했던 분석 방법론을 간단히 재현하는 용도로 사용하는 것이기 때문에 시뮬레이션 자체에 많은 시간을 할애하진 않았다.\n시뮬레이션 데이터는 실제 연구를 위해 표본으로 쓰인 sample 데이터의 분포를 전혀 반영하지 않았다.\n이 시뮬레이션 연구에서, 실제 데이터의 분포를 반영하지 않았고 범주형 변수 및 연속형 변수와 종속 변수를 통계적으로 잘 연관시키지 못했기 때문에 분석 결과가 생물학적인 사실과 많이 다를 수 있다."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#development-environment",
    "href": "docs/projects/LLFS/self_description.html#development-environment",
    "title": "Project Description",
    "section": "",
    "text": "Operating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section"
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#introduction-1",
    "href": "docs/projects/LLFS/self_description.html#introduction-1",
    "title": "Project Description",
    "section": "",
    "text": "It is difficult to show the methodology used for analysis in detail because the actual data used in the project cannot be used for security reasons. Accordingly, I am going to show a rough analysis method through simulation of high-dimensional, mutually correlated data."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#goal-1",
    "href": "docs/projects/LLFS/self_description.html#goal-1",
    "title": "Project Description",
    "section": "",
    "text": "The aim of this simulation study is to identify a set of predictors (or metabolites or bio-chemicals) that will enable to differentiate bio-markers that are associated with AD vs. non-AD."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#feature-1",
    "href": "docs/projects/LLFS/self_description.html#feature-1",
    "title": "Project Description",
    "section": "",
    "text": "In this article, due to security concerns, it is difficult to display the real data and the entire analysis pipeline used in this project.\nIn this simulation study, I will focus only on generating fake data at the metabolomic stage by generating data under multivariate normal distributions.\nSince I don’t have much experience in simulation, there may be mathematically and statistically incorrect parts in the simulation.\nI did not put a lot of effort into the simulation itself because the simulation was used to simply reproduce the analysis methodology I had performed.\nThe simulated data does not reflect the distribution of the truely sampled data used in the LLFS at all.\nIn this simulation, since the categorical and continuous variables and the dependent variable could not be statistically associated properly, the analysis result for the discrete variables could be very different from the biological or medical fact."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#development-environment-1",
    "href": "docs/projects/LLFS/self_description.html#development-environment-1",
    "title": "Project Description",
    "section": "",
    "text": "Operating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html",
    "href": "docs/projects/LLFS/project_description.html",
    "title": "Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n알츠하이머병(Alzheimer’s Disease, AD)은 수백만 명의 미국인에게 영향을 미치는 가장 흔한 형태의 치매이다. 알츠하이머병은 기억력, 사고력 및 행동에 영향을 주지만 증상이 나타나기까지 거의 20년에 걸쳐 진행이 된다. 따라서 전 임상 단계에서 생리학을 이해하는 것이 필수적이다. 유전적 요인이 AD에 거의 50% 기여하는 것으로 추정된다. 유전자가 세포 환경을 변경하여 알츠하이머병 위험에 어떻게 기여하는지 더 잘 이해하기 위해 AD와 연관이 있는 유전자인 APOE를 보유한 사람들의 대사체(Metabolome)를 조사했다. 대사체는 유전체(Genome)과 단백질체(Proteome)에서 생성된 산물을 의미한다. 이러한 생화학 부산물은 유전적 요인과 환경적 요인 모두의 영향을 받는다. 모집단은 장수마을에 사는 Caucasian (백인) 참여자들이다.\n\n\n\nLLFS(Long Life Family Study) 프로젝트의 목적은 유전체, 전사체, 단백질체 및 대사체 단계를 통해 유전체에서 대사체 단계에 이르는 여러 단계에서 통계 및 기계 학습을 사용하여 분석 파이프라인을 구축하고 알츠하이머병에 대한 중요한 바이오마커를 식별하는 것이다.\n\n\n\n읽기의 편의성을 위해 LLFS project에 대한 설명을 project와 self project 와 같이 2개의 section으로 나누었다. project는 내가 실제로 프로젝트를 수행했던 과정을 기술했고 self project는 그 방법론을 대략적으로 간소화된 형태로 기술했다.\n\nProject Description (Current)\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    subgraph Data_Collection\n        direction TB\n        Multi_Centerd_Blood_Sampling---\n        Mass_Spectrometry---\n        Data_Transfer\n    end\n    subgraph Quality_Control\n        direction TB\n        Identify_Anomaly_Data---\n        Identify_Missing_Values\n    end\n    subgraph Analytics\n        direction TB \n        EDA---\n        Data_Mining---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n    subgraph Reporting_and_Conclusion\n        direction TB \n        Share_with_Faculty\n    end\n\nData_Collection--&gt;Quality_Control--&gt;Analytics--&gt;Reporting_and_Conclusion\n\n\n\n\n\n\nData는 장수 마을에 거주하는 백인을 대상으로 New York, Bonston, Pittsburgh 및 Denmark에 있는 여러 medical centers에서 sampled blood를 MS Spectromtetry로 Digitalization을 했다. 여러 과정을 통해 data를 csv형태로 받아 data의 QC(Quality Control)를 진행한뒤 Data 분석 업무를 수행했다. EDA (Exploratory Data Analysis) 와 Data Mining을 통해 data에 대한 이해도를 높였고 이를 토대로 통계 분석과 machine learning을 이용하여 이 data에 적합한 모형을 찾았다. 모든 결과물은 The Taub Institute for Research on Alzheimer’s Disease and the Aging Brain 의 biostaticians, medical doctors, biologists, neurologists, bioinformaticians 및 epidemiologists와 공유를 했다.\n\n\n\n\n\n\n\n\nflowchart LR\n    subgraph Data_Quality_Control\n        direction TB\n        identify_anomaly_data---\n        identify_missing_values\n        subgraph Missing_Value_Analysis\n            direction LR\n            MCAR---\n            MAR---\n            MNAR\n        end\n        either_imputation_or_omission---\n        communication_with_labs---\n        set_data_inclusion/exclusion_criteria\n    end\n    identify_missing_values---Missing_Value_Analysis---either_imputation_or_omission\n    subgraph Data_Preprocessing\n        direction TB\n        data_transformation---\n        log_transformation---\n        standardization\n    end\n\nData_Quality_Control--&gt;Data_Preprocessing\n\n\n\n\n\n\n\nData의 품질 관리를 위해 data를 생성한 biochemists와 소통하여 실험실 기준에 따라 결측치와 이상치를 구분하여 labeling을 수행했고 missing value analysis를 통해 결과에 따라 medical doctors를 포함한 다른 faculty members와 상의하여 결측치 처리를 했다. data QC criteria는 rowwise 와 columwise sum의 합이 sample size에 대하여 missing values의 비율이 5%가 넘는 환자와 변수는 분석 대상에서 제외 됐다. 모든 metabolites data는 log transoformation 과 standardization을 통해 data의 단위를 표준화 했다.\n\n\n\n\n\n\n\n\nflowchart TB\n    subgraph Data_Analytics\n        direction TB\n        Exploratory_Data_Analysis---\n        Data_Minig---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n\n\n\n\n\n\nData 분석은 크게 EDA (Exploratory Data Analysis), Statistical Analysis 및 Machine Learning과 같이 3 단계로 수행했다. 각 단계에서 나온 결과가 각 각의 단계에서 일관되게 나오는 metabolites를 선별했다.\n\n\nstudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests 및 regression analysis이 수행됐고 visualization을 통해 검정 결과를 재확인하는 작업을 수행했다. 고차원 데이터를 시각화하여 data의 pattern을 관찰하기 위해 KNN, PCA, K means clustering 및 DB Scan을 이용했다.\n\n\n\nmultivariable linear regression, logistic regression 및 Cox PH(Proportional Hazards) regression anayses 가 수행됐고 질병과 유의한 metabolites를 선별했다. multiple testing으로 인한 1 종 오류를 범하는 것을 줄이기 위해 permuted p-values를 계산하여 유의성을 한번 더 확인했다.\n\n\n\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, SVM (support vector machine), partial least square 및 sparse partial least square가 사용됐다. 질병을 가장 잘 예측하는 classifier를 평가하여 최적의 classifier를 선택했다.\n\n\n\n\n146개의 관측치와 약 3,000여개의 변수로 구성된 data에서 약 60개 내외의 대사물질이 질병과 5% 유의수준으로 유의한 관계가 있는 것으로 관찰됐고 partial least suare 가 가장 성능이 좋은 것으로 관찰됐다.\n\n\n\n\n\n\nAlzheimer Disease (AD) is the most common form of dementia that affects millions of Americans. AD affects memory, thinking and behavior, but its progression is slow, spanning nearly two decades before the symptoms appear. Thus, it is imperative to understand the physiology at the pre-clinical stage. It is estimated that genetic factors contribute nearly 50% to AD. To better understand how genes contribute to the risk of AD by altering cellular milieu, I have examined the metabolome of individuals with the AD-related genotype, APOE. The metabolome represents the products that were generated from the genome and proteome. These biochemical products represent influences of both genetic and environmental factors. The population is Caucasian participants living in longevity village.\n\n\n\nThe objective of the Long Life Family Study (LLFS) project was to build an analysis pipeline of identifying significant biomarkers for AD using statistics and machine learning at the multi-stages from the genomic to the metabolomic stage through the transcriptomic and proteomic stage.\n\n\n\nFor the convenience of reading, the LLFS project is divided into the two sections: project and self-project. The project section roughly described the process of the project I actually carried out, and the self project one described the methodology in a roughly simplified form.\n\nProject Description (Current)\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nData Collection\n\n\ncluster1\n\nQuality Control\n\n\ncluster2\n\nAnalytics\n\n\ncluster3\n\nReporting and Conclusion\n\n\n\nMulti_Centered_Blood_Sampling\n\nMulti_Centered_Blood_Sampling\n\n\n\nMass_Spectrometry\n\nMass_Spectrometry\n\n\n\nData_Transfer\n\nData_Transfer\n\n\n\nIdentify_Anomaly_Data\n\nIdentify_Anomaly_Data\n\n\n\nData_Transfer-&gt;Identify_Anomaly_Data\n\n\n\n\n\nIdentify_Missing_Values\n\nIdentify_Missing_Values\n\n\n\nEDA\n\nEDA\n\n\n\nIdentify_Missing_Values-&gt;EDA\n\n\n\n\n\nData_Mining\n\nData_Mining\n\n\n\nStatistical_Analysis\n\nStatistical_Analysis\n\n\n\nMachine_Learning\n\nMachine_Learning\n\n\n\nShare_with_Faculty\n\nShare_with_Faculty\n\n\n\nMachine_Learning-&gt;Share_with_Faculty\n\n\n\n\n\n\n\n\n\n\nData were obtained by digitization through MS Spectromtetry of blood samples from multiple medical centers in New York, Bonston, Pittsburgh, and Denmark for Caucasians residing in longevity villages. After receiving the data in a csv format through various processes, QC (Quality Control) of the data and data analysis were performed. To better understand data, exploratory data analysis (EDA) and data mining were conducted. Based on the analysis findings on data, the machine learning model to explain the data most was selcted. All findings were shared with biostatisticians, medical doctors, biologists, neurologists and epidemiologists at the neurology department and the Taub Institute for Research on Alzheimer’s Disease and the Aging Brain in the Columbia University Irving Medical Center.\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nData Quality Control\n\n\ncluster1\n\nMissing Value Analysis\n\n\ncluster2\n\nData Preprocessing\n\n\n\nidentify_anomaly_data\n\nidentify_anomaly_data\n\n\n\nidentify_missing_values\n\nidentify_missing_values\n\n\n\nidentify_anomaly_data-&gt;identify_missing_values\n\n\n\n\n\nMissing_Completely_At_Random\n\nMissing_Completely_At_Random\n\n\n\nidentify_missing_values-&gt;Missing_Completely_At_Random\n\n\n\n\n\nMissing_At_Random\n\nMissing_At_Random\n\n\n\nMissing_Completely_At_Random-&gt;Missing_At_Random\n\n\n\n\n\nMissing_Not_at_Random\n\nMissing_Not_at_Random\n\n\n\nMissing_At_Random-&gt;Missing_Not_at_Random\n\n\n\n\n\neither_imputation_or_omission\n\neither_imputation_or_omission\n\n\n\nMissing_Not_at_Random-&gt;either_imputation_or_omission\n\n\n\n\n\ncommunication_with_labs\n\ncommunication_with_labs\n\n\n\neither_imputation_or_omission-&gt;communication_with_labs\n\n\n\n\n\nset_data_inclusion_exclusion_criteria\n\nset_data_inclusion_exclusion_criteria\n\n\n\ncommunication_with_labs-&gt;set_data_inclusion_exclusion_criteria\n\n\n\n\n\nData_Transformation\n\nData_Transformation\n\n\n\nset_data_inclusion_exclusion_criteria-&gt;Data_Transformation\n\n\n\n\n\nLog_Transformation\n\nLog_Transformation\n\n\n\nData_Transformation-&gt;Log_Transformation\n\n\n\n\n\nStandardization\n\nStandardization\n\n\n\nLog_Transformation-&gt;Standardization\n\n\n\n\n\n\n\n\n\n\nFor data quality control, I communicated with whom generated the data, classified missing values ​​and outliers according to laboratory standards, and labeled them. Based on the results through missing value analysis, I processed the missing values through consultation with the faculty members several times. For the data QC criteria, patients and variables whose ratio of missing values ​​for the sum of the rowwise and columnwise sums exceeded 5% for the sample size were excluded from the analysis. All metabolites data were standardized through log transformation and standardization.\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster2\n\nData Analytics\n\n\n\nExploratory_Data_Analysis\n\nExploratory_Data_Analysis\n\n\n\nData_Minig\n\nData_Minig\n\n\n\nExploratory_Data_Analysis-&gt;Data_Minig\n\n\n\n\n\nStatistical_Analysis\n\nStatistical_Analysis\n\n\n\nData_Minig-&gt;Statistical_Analysis\n\n\n\n\n\nMachine_Learning\n\nMachine_Learning\n\n\n\nStatistical_Analysis-&gt;Machine_Learning\n\n\n\n\n\n\n\n\n\n\nData analysis was performed in three stages: Exploratory Data Analysis (EDA), Statistical Analysis, and Machine Learning. In each stage, metabolites commonly associated with diseases were selected.\n\n\nStudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests, and regression testing were performed, and I visualizaed data to reconfirm the test results. To visualize high-dimensional data and observe data patterns, KNN, PCA, K means, Clustering, and DB Scan were used.\n\n\n\nMultivariable linear regression, logistic regression, and Cox PH (Proportional Hazards) regression analyses were conducted and the metabolites that are signficantly associated with the disease status were selected. In order to reduce the possibility of making a type 1 error due to multiple testing, the significance was checked once more by calculating permuted p-values.\n\n\n\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, support vector machine (SVM), partial least square, and sparse partial least square were used. The optimal classifier was selected by evaluating the classifier that best predicted the disease status.\n\n\n\n\nIn the data consisting of 146 observations and about 3,000 variables, about 60 metabolites were observed to have a significant relationship with the disease at the 5% significance level, and partial least suare was observed to perform the best."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#background",
    "href": "docs/projects/LLFS/project_description.html#background",
    "title": "Description",
    "section": "",
    "text": "알츠하이머병(Alzheimer’s Disease, AD)은 수백만 명의 미국인에게 영향을 미치는 가장 흔한 형태의 치매이다. 알츠하이머병은 기억력, 사고력 및 행동에 영향을 주지만 증상이 나타나기까지 거의 20년에 걸쳐 진행이 된다. 따라서 전 임상 단계에서 생리학을 이해하는 것이 필수적이다. 유전적 요인이 AD에 거의 50% 기여하는 것으로 추정된다. 유전자가 세포 환경을 변경하여 알츠하이머병 위험에 어떻게 기여하는지 더 잘 이해하기 위해 AD와 연관이 있는 유전자인 APOE를 보유한 사람들의 대사체(Metabolome)를 조사했다. 대사체는 유전체(Genome)과 단백질체(Proteome)에서 생성된 산물을 의미한다. 이러한 생화학 부산물은 유전적 요인과 환경적 요인 모두의 영향을 받는다. 모집단은 장수마을에 사는 Caucasian (백인) 참여자들이다."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#objective",
    "href": "docs/projects/LLFS/project_description.html#objective",
    "title": "Description",
    "section": "",
    "text": "LLFS(Long Life Family Study) 프로젝트의 목적은 유전체, 전사체, 단백질체 및 대사체 단계를 통해 유전체에서 대사체 단계에 이르는 여러 단계에서 통계 및 기계 학습을 사용하여 분석 파이프라인을 구축하고 알츠하이머병에 대한 중요한 바이오마커를 식별하는 것이다."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#how-to-read-the-contents",
    "href": "docs/projects/LLFS/project_description.html#how-to-read-the-contents",
    "title": "Description",
    "section": "",
    "text": "읽기의 편의성을 위해 LLFS project에 대한 설명을 project와 self project 와 같이 2개의 section으로 나누었다. project는 내가 실제로 프로젝트를 수행했던 과정을 기술했고 self project는 그 방법론을 대략적으로 간소화된 형태로 기술했다.\n\nProject Description (Current)"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#analysis-pipeline-architecture",
    "href": "docs/projects/LLFS/project_description.html#analysis-pipeline-architecture",
    "title": "Description",
    "section": "",
    "text": "flowchart LR\n    subgraph Data_Collection\n        direction TB\n        Multi_Centerd_Blood_Sampling---\n        Mass_Spectrometry---\n        Data_Transfer\n    end\n    subgraph Quality_Control\n        direction TB\n        Identify_Anomaly_Data---\n        Identify_Missing_Values\n    end\n    subgraph Analytics\n        direction TB \n        EDA---\n        Data_Mining---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n    subgraph Reporting_and_Conclusion\n        direction TB \n        Share_with_Faculty\n    end\n\nData_Collection--&gt;Quality_Control--&gt;Analytics--&gt;Reporting_and_Conclusion\n\n\n\n\n\n\nData는 장수 마을에 거주하는 백인을 대상으로 New York, Bonston, Pittsburgh 및 Denmark에 있는 여러 medical centers에서 sampled blood를 MS Spectromtetry로 Digitalization을 했다. 여러 과정을 통해 data를 csv형태로 받아 data의 QC(Quality Control)를 진행한뒤 Data 분석 업무를 수행했다. EDA (Exploratory Data Analysis) 와 Data Mining을 통해 data에 대한 이해도를 높였고 이를 토대로 통계 분석과 machine learning을 이용하여 이 data에 적합한 모형을 찾았다. 모든 결과물은 The Taub Institute for Research on Alzheimer’s Disease and the Aging Brain 의 biostaticians, medical doctors, biologists, neurologists, bioinformaticians 및 epidemiologists와 공유를 했다.\n\n\n\n\n\n\n\n\nflowchart LR\n    subgraph Data_Quality_Control\n        direction TB\n        identify_anomaly_data---\n        identify_missing_values\n        subgraph Missing_Value_Analysis\n            direction LR\n            MCAR---\n            MAR---\n            MNAR\n        end\n        either_imputation_or_omission---\n        communication_with_labs---\n        set_data_inclusion/exclusion_criteria\n    end\n    identify_missing_values---Missing_Value_Analysis---either_imputation_or_omission\n    subgraph Data_Preprocessing\n        direction TB\n        data_transformation---\n        log_transformation---\n        standardization\n    end\n\nData_Quality_Control--&gt;Data_Preprocessing\n\n\n\n\n\n\n\nData의 품질 관리를 위해 data를 생성한 biochemists와 소통하여 실험실 기준에 따라 결측치와 이상치를 구분하여 labeling을 수행했고 missing value analysis를 통해 결과에 따라 medical doctors를 포함한 다른 faculty members와 상의하여 결측치 처리를 했다. data QC criteria는 rowwise 와 columwise sum의 합이 sample size에 대하여 missing values의 비율이 5%가 넘는 환자와 변수는 분석 대상에서 제외 됐다. 모든 metabolites data는 log transoformation 과 standardization을 통해 data의 단위를 표준화 했다.\n\n\n\n\n\n\n\n\nflowchart TB\n    subgraph Data_Analytics\n        direction TB\n        Exploratory_Data_Analysis---\n        Data_Minig---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n\n\n\n\n\n\nData 분석은 크게 EDA (Exploratory Data Analysis), Statistical Analysis 및 Machine Learning과 같이 3 단계로 수행했다. 각 단계에서 나온 결과가 각 각의 단계에서 일관되게 나오는 metabolites를 선별했다.\n\n\nstudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests 및 regression analysis이 수행됐고 visualization을 통해 검정 결과를 재확인하는 작업을 수행했다. 고차원 데이터를 시각화하여 data의 pattern을 관찰하기 위해 KNN, PCA, K means clustering 및 DB Scan을 이용했다.\n\n\n\nmultivariable linear regression, logistic regression 및 Cox PH(Proportional Hazards) regression anayses 가 수행됐고 질병과 유의한 metabolites를 선별했다. multiple testing으로 인한 1 종 오류를 범하는 것을 줄이기 위해 permuted p-values를 계산하여 유의성을 한번 더 확인했다.\n\n\n\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, SVM (support vector machine), partial least square 및 sparse partial least square가 사용됐다. 질병을 가장 잘 예측하는 classifier를 평가하여 최적의 classifier를 선택했다.\n\n\n\n\n146개의 관측치와 약 3,000여개의 변수로 구성된 data에서 약 60개 내외의 대사물질이 질병과 5% 유의수준으로 유의한 관계가 있는 것으로 관찰됐고 partial least suare 가 가장 성능이 좋은 것으로 관찰됐다."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#background-1",
    "href": "docs/projects/LLFS/project_description.html#background-1",
    "title": "Description",
    "section": "",
    "text": "Alzheimer Disease (AD) is the most common form of dementia that affects millions of Americans. AD affects memory, thinking and behavior, but its progression is slow, spanning nearly two decades before the symptoms appear. Thus, it is imperative to understand the physiology at the pre-clinical stage. It is estimated that genetic factors contribute nearly 50% to AD. To better understand how genes contribute to the risk of AD by altering cellular milieu, I have examined the metabolome of individuals with the AD-related genotype, APOE. The metabolome represents the products that were generated from the genome and proteome. These biochemical products represent influences of both genetic and environmental factors. The population is Caucasian participants living in longevity village."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#objective-1",
    "href": "docs/projects/LLFS/project_description.html#objective-1",
    "title": "Description",
    "section": "",
    "text": "The objective of the Long Life Family Study (LLFS) project was to build an analysis pipeline of identifying significant biomarkers for AD using statistics and machine learning at the multi-stages from the genomic to the metabolomic stage through the transcriptomic and proteomic stage."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#how-to-read-the-contents-1",
    "href": "docs/projects/LLFS/project_description.html#how-to-read-the-contents-1",
    "title": "Description",
    "section": "",
    "text": "For the convenience of reading, the LLFS project is divided into the two sections: project and self-project. The project section roughly described the process of the project I actually carried out, and the self project one described the methodology in a roughly simplified form.\n\nProject Description (Current)"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#analysis-pipeline-architecture-1",
    "href": "docs/projects/LLFS/project_description.html#analysis-pipeline-architecture-1",
    "title": "Description",
    "section": "",
    "text": "G\n\n\ncluster0\n\nData Collection\n\n\ncluster1\n\nQuality Control\n\n\ncluster2\n\nAnalytics\n\n\ncluster3\n\nReporting and Conclusion\n\n\n\nMulti_Centered_Blood_Sampling\n\nMulti_Centered_Blood_Sampling\n\n\n\nMass_Spectrometry\n\nMass_Spectrometry\n\n\n\nData_Transfer\n\nData_Transfer\n\n\n\nIdentify_Anomaly_Data\n\nIdentify_Anomaly_Data\n\n\n\nData_Transfer-&gt;Identify_Anomaly_Data\n\n\n\n\n\nIdentify_Missing_Values\n\nIdentify_Missing_Values\n\n\n\nEDA\n\nEDA\n\n\n\nIdentify_Missing_Values-&gt;EDA\n\n\n\n\n\nData_Mining\n\nData_Mining\n\n\n\nStatistical_Analysis\n\nStatistical_Analysis\n\n\n\nMachine_Learning\n\nMachine_Learning\n\n\n\nShare_with_Faculty\n\nShare_with_Faculty\n\n\n\nMachine_Learning-&gt;Share_with_Faculty\n\n\n\n\n\n\n\n\n\n\nData were obtained by digitization through MS Spectromtetry of blood samples from multiple medical centers in New York, Bonston, Pittsburgh, and Denmark for Caucasians residing in longevity villages. After receiving the data in a csv format through various processes, QC (Quality Control) of the data and data analysis were performed. To better understand data, exploratory data analysis (EDA) and data mining were conducted. Based on the analysis findings on data, the machine learning model to explain the data most was selcted. All findings were shared with biostatisticians, medical doctors, biologists, neurologists and epidemiologists at the neurology department and the Taub Institute for Research on Alzheimer’s Disease and the Aging Brain in the Columbia University Irving Medical Center.\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nData Quality Control\n\n\ncluster1\n\nMissing Value Analysis\n\n\ncluster2\n\nData Preprocessing\n\n\n\nidentify_anomaly_data\n\nidentify_anomaly_data\n\n\n\nidentify_missing_values\n\nidentify_missing_values\n\n\n\nidentify_anomaly_data-&gt;identify_missing_values\n\n\n\n\n\nMissing_Completely_At_Random\n\nMissing_Completely_At_Random\n\n\n\nidentify_missing_values-&gt;Missing_Completely_At_Random\n\n\n\n\n\nMissing_At_Random\n\nMissing_At_Random\n\n\n\nMissing_Completely_At_Random-&gt;Missing_At_Random\n\n\n\n\n\nMissing_Not_at_Random\n\nMissing_Not_at_Random\n\n\n\nMissing_At_Random-&gt;Missing_Not_at_Random\n\n\n\n\n\neither_imputation_or_omission\n\neither_imputation_or_omission\n\n\n\nMissing_Not_at_Random-&gt;either_imputation_or_omission\n\n\n\n\n\ncommunication_with_labs\n\ncommunication_with_labs\n\n\n\neither_imputation_or_omission-&gt;communication_with_labs\n\n\n\n\n\nset_data_inclusion_exclusion_criteria\n\nset_data_inclusion_exclusion_criteria\n\n\n\ncommunication_with_labs-&gt;set_data_inclusion_exclusion_criteria\n\n\n\n\n\nData_Transformation\n\nData_Transformation\n\n\n\nset_data_inclusion_exclusion_criteria-&gt;Data_Transformation\n\n\n\n\n\nLog_Transformation\n\nLog_Transformation\n\n\n\nData_Transformation-&gt;Log_Transformation\n\n\n\n\n\nStandardization\n\nStandardization\n\n\n\nLog_Transformation-&gt;Standardization\n\n\n\n\n\n\n\n\n\n\nFor data quality control, I communicated with whom generated the data, classified missing values ​​and outliers according to laboratory standards, and labeled them. Based on the results through missing value analysis, I processed the missing values through consultation with the faculty members several times. For the data QC criteria, patients and variables whose ratio of missing values ​​for the sum of the rowwise and columnwise sums exceeded 5% for the sample size were excluded from the analysis. All metabolites data were standardized through log transformation and standardization.\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster2\n\nData Analytics\n\n\n\nExploratory_Data_Analysis\n\nExploratory_Data_Analysis\n\n\n\nData_Minig\n\nData_Minig\n\n\n\nExploratory_Data_Analysis-&gt;Data_Minig\n\n\n\n\n\nStatistical_Analysis\n\nStatistical_Analysis\n\n\n\nData_Minig-&gt;Statistical_Analysis\n\n\n\n\n\nMachine_Learning\n\nMachine_Learning\n\n\n\nStatistical_Analysis-&gt;Machine_Learning\n\n\n\n\n\n\n\n\n\n\nData analysis was performed in three stages: Exploratory Data Analysis (EDA), Statistical Analysis, and Machine Learning. In each stage, metabolites commonly associated with diseases were selected.\n\n\nStudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests, and regression testing were performed, and I visualizaed data to reconfirm the test results. To visualize high-dimensional data and observe data patterns, KNN, PCA, K means, Clustering, and DB Scan were used.\n\n\n\nMultivariable linear regression, logistic regression, and Cox PH (Proportional Hazards) regression analyses were conducted and the metabolites that are signficantly associated with the disease status were selected. In order to reduce the possibility of making a type 1 error due to multiple testing, the significance was checked once more by calculating permuted p-values.\n\n\n\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, support vector machine (SVM), partial least square, and sparse partial least square were used. The optimal classifier was selected by evaluating the classifier that best predicted the disease status.\n\n\n\n\nIn the data consisting of 146 observations and about 3,000 variables, about 60 metabolites were observed to have a significant relationship with the disease at the 5% significance level, and partial least suare was observed to perform the best."
  },
  {
    "objectID": "docs/projects/LLFS/mining.html",
    "href": "docs/projects/LLFS/mining.html",
    "title": "Data Mining",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\nCode\nmetabolite_data &lt;- all_data[, -c(1:5)]\noutcome_data &lt;- all_data[, 2]\n\n# normalize the metaoblites\nnormalized_metabolite_data &lt;-\n    as.data.frame(lapply(metabolite_data, function(x) scale_function(vector = x, method = \"min-max\")))\nnormalized_significant_metabolite_data &lt;-\n    normalized_metabolite_data %&gt;%\n    dplyr::select(all_of(significant_metabolites))\n\n# extract the latent variables (PCs: Principal Components)\npc_metabolites &lt;-\n    prcomp(normalized_metabolite_data)\npc_significant_metabolites &lt;-\n    prcomp(normalized_significant_metabolite_data)\n\n# calculate scores\nscores &lt;-\n    as.data.frame(pc_metabolites$x) %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(row_names = 1:n())\nsignificant_scores &lt;-\n    as.data.frame(pc_significant_metabolites$x) %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(row_names = 1:n())\n\ntemp &lt;-\n    as.data.frame(pc_metabolites$rotation) %&gt;%\n    janitor::clean_names()\nloadings &lt;- temp %&gt;%\n    mutate(\n        metabolites = rownames(.),\n        arrow_size_normalization = min(\n            (max(scores[, \"pc1\"]) - min(scores[, \"pc1\"]) /\n                (max(temp[, \"pc1\"]) - min(temp[, \"pc1\"]))),\n            (max(scores[, \"pc2\"]) - min(scores[, \"pc2\"]) /\n                (max(temp[, \"pc2\"]) - min(temp[, \"pc2\"]))),\n            (max(scores[, \"pc3\"]) - min(scores[, \"pc3\"]) /\n                (max(temp[, \"pc3\"]) - min(temp[, \"pc3\"])))\n        ),\n        arrow_pc1 = arrow_size_normalization * pc1,\n        arrow_pc2 = arrow_size_normalization * pc2,\n        arrow_pc3 = arrow_size_normalization * pc3\n    )\n\ntemp &lt;-\n    as.data.frame(pc_significant_metabolites$rotation) %&gt;%\n    janitor::clean_names()\nsignificant_loadings &lt;- temp %&gt;%\n    mutate(\n        metabolites = rownames(.),\n        arrow_size_normalization = min(\n            (max(scores[, \"pc1\"]) - min(scores[, \"pc1\"]) /\n                (max(temp[, \"pc1\"]) - min(temp[, \"pc1\"]))),\n            (max(scores[, \"pc2\"]) - min(scores[, \"pc2\"]) /\n                (max(temp[, \"pc2\"]) - min(temp[, \"pc2\"]))),\n            (max(scores[, \"pc3\"]) - min(scores[, \"pc3\"]) /\n                (max(temp[, \"pc3\"]) - min(temp[, \"pc3\"])))\n        ),\n        arrow_pc1 = arrow_size_normalization * pc1,\n        arrow_pc2 = arrow_size_normalization * pc2,\n        arrow_pc3 = arrow_size_normalization * pc3\n    )\n# arrow_size_normalization is a normalization factor that\n# ensures the variable loading arrows are scaled appropriately relative to the data points.\n# The min() function to find the smallest ratio between the range of the data points and\n# the range of the variable loadings along each principal component axis (pc1, pc2, and pc3).\n# The reason why I select the first 3 components is that\n# '3' is the maximum dimension that can visualize the PCA results in 3d.\n\noutcome_scores &lt;-\n    scores %&gt;%\n    mutate(\n        outcome = outcome_data,\n        row_names = 1:n()\n    )\noutcome_significant_scores &lt;-\n    significant_scores %&gt;%\n    mutate(\n        outcome = outcome_data,\n        row_names = 1:n()\n    )\n\n\n# total variance\ntotal_variance &lt;-\n    data.frame(\n        pc = 1:length(pc_metabolites$sdev),\n        pc_variance_proportion = summary(pc_metabolites)[[\"importance\"]][\"Proportion of Variance\", ],\n        cumulative_proportion = summary(pc_metabolites)[[\"importance\"]][\"Cumulative Proportion\", ] * 100\n    )\ntotal_variance_significance &lt;-\n    data.frame(\n        pc = 1:length(pc_significant_metabolites$sdev),\n        pc_variance_proportion = summary(pc_significant_metabolites)[[\"importance\"]][\"Proportion of Variance\", ],\n        cumulative_proportion = summary(pc_significant_metabolites)[[\"importance\"]][\"Cumulative Proportion\", ] * 100\n    )\n\nscree_plot &lt;- function(indata) {\n    scree_plot1 &lt;- ggplot(\n        data = indata,\n        aes(x = pc, y = pc_variance_proportion, group = 1)\n    ) +\n        geom_point() +\n        geom_line() +\n        labs(title = \"\", subtitle = paste0(\n            \"Scree Plot, Total Variance(\",\n            round(tail(indata, 1)[\"cumulative_proportion\"], 3),\n            \"%)Explained by \", nrow(indata), \" PCs\"\n        )) +\n        ylab(\"Total Variance Explained\") +\n        xlab(\"Principal Components\")\n    scree_plot2 &lt;- ggplot(\n        data = indata %&gt;% filter(pc &lt; 13),\n        aes(x = pc, y = pc_variance_proportion, group = 1)\n    ) +\n        geom_point() +\n        geom_line() +\n        labs(title = \"\", subtitle = paste0(\n            \"Scree Plot, Part of Variance(\",\n            round(tail(indata %&gt;% filter(pc &lt; 13), 1)[\"cumulative_proportion\"], 3),\n            \"%)Explained by \", indata %&gt;% filter(pc &lt; 13) %&gt;% nrow(), \" PCs\"\n        )) +\n        ylab(\"Total Variance Explained\") +\n        xlab(\"Principal Components\")\n    return(ggarrange(scree_plot1, scree_plot2, nrow = 1))\n}\n\nggarrange(scree_plot(total_variance), scree_plot(total_variance_significance),\n    labels = c(\n        paste0(\"All \", ncol(metabolite_data), \" Metabolites\"),\n        paste0(length(significant_metabolites), \" Significant Metabolites\")\n    ), nrow = 2\n)\n\n\n\n\n\n\n\n\n\nCode\n# 2D PCA Scatter Plots with PC1 and PC2\n\nscatter_plot &lt;- function(in_data) {\n    p &lt;- ggplot(\n        data = in_data,\n        aes(x = pc1, y = pc2, color = outcome)\n    ) +\n        geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n        stat_ellipse(type = \"norm\", level = .99) +\n        geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n        geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n        scale_color_manual(values = color_function(length(unique(in_data$outcome)))) +\n        labs(\n            title = \"2D Scatter Plot of the First 2 PCs Grouped by AD status\",\n            subtitle = paste0(ncol(in_data) - 2, \" Metabolites\")\n        )\n    return(p)\n}\n\nggarrange(scatter_plot(outcome_scores),\n    scatter_plot(outcome_significant_scores),\n    nrow = 2\n)\n\n\n\n\n\n\n\n\n\nCode\n# biplot\nbi_plot &lt;- function(in_data) {\n    p &lt;-\n        ggplot(data = in_data, aes(x = pc1, y = pc2, color = outcome)) +\n        geom_text(alpha = .75, size = 3, aes(label = row_names)) +\n        geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n        geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n        coord_equal() +\n        scale_color_manual(values = color_function(length(unique(in_data$outcome)))) +\n        stat_ellipse(type = \"norm\", level = .99) +\n        geom_text(\n            data = loadings, aes(x = arrow_pc1, y = arrow_pc2, label = metabolites),\n            alpha = 0.5, size = 5, vjust = 1, color = \"red\"\n        ) +\n        geom_segment(\n            data = loadings, aes(x = 0, y = 0, xend = arrow_pc1, yend = arrow_pc2),\n            arrow = arrow(length = unit(0.5, \"cm\")), alpha = 0.5, color = \"red\"\n        ) +\n        labs(\n            title = \"Biplot, the Effect of Metabolites on Samples with Disease Status\",\n            subtitle = paste0(ncol(in_data) - 2, \" Metabolites\")\n        ) +\n        ylab(\"PC2\") +\n        xlab(\"PC1\")\n    return(p)\n}\nggarrange(bi_plot(outcome_scores), bi_plot(outcome_significant_scores), nrow = 2)\n\n\n\n\n\n\n\n\n\nCode\nplot_ly(\n    data = outcome_scores,\n    x = ~pc1, y = ~pc2, z = ~pc3,\n    type = \"scatter3d\", mode = \"markers\", color = ~ outcome_scores$outcome,\n    colors = color_function(2),\n    size = 2\n) %&gt;%\n    layout(\n        title = \"Effect of 500 Metabolites on Samples with Disease Status in 3d\",\n        scene = list(\n            bgcolor = \"#e5ecf6\",\n            xaxis = list(title = \"PC1\"),\n            yaxis = list(title = \"PC2\"),\n            zaxis = list(title = \"PC3\")\n        ),\n        legend = list(title = list(text = \"Disease(AD) Status\"))\n    )\n\n\n\n\n\n\nCode\nplot_ly(\n    data = outcome_significant_scores,\n    x = ~pc1, y = ~pc2, z = ~pc3,\n    type = \"scatter3d\", mode = \"markers\", color = ~ outcome_scores$outcome,\n    colors = color_function(2),\n    size = 2\n) %&gt;%\n    layout(\n        title = \"Effect of 201 Metabolites on Samples with Disease Status in 3d\",\n        scene = list(\n            bgcolor = \"#e5ecf6\",\n            xaxis = list(title = \"PC1\"),\n            yaxis = list(title = \"PC2\"),\n            zaxis = list(title = \"PC3\")\n        ),\n        legend = list(title = list(text = \"Disease(AD) Status\"))\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nkmean_result_list &lt;- list(\n    \"mse_list\" = list(\n        \"all_metabolites\" = matrix(nrow = 20),\n        \"significant_metabolites\" = matrix(nrow = 20)\n    ),\n    \"cluster_sse_list\" = list(\n        \"all_metabolites\" = matrix(nrow = 20, ncol = 2),\n        \"significant_metabolites\" = matrix(nrow = 20, ncol = 2)\n    ),\n    \"Variance_Explained\" = list(\n        \"all_metabolites\" = matrix(nrow = 20),\n        \"significant_metabolites\" = matrix(nrow = 20)\n    )\n)\n\nfor (j in c(\"all_metabolites\", \"significant_metabolites\")) {\n    for (i in (1:20)) {\n        if (j == \"all_metabolites\") {\n            kmean_fit &lt;- kmeans(normalized_metabolite_data, centers = i, iter.max = 300)\n        } else {\n            kmean_fit &lt;- kmeans(normalized_significant_metabolite_data, centers = i, iter.max = 300)\n        }\n        kmean_result_list[[\"mse_list\"]][[j]][i] &lt;- mean(kmean_fit$withinss) %&gt;% round(3)\n        kmean_result_list[[\"cluster_sse_list\"]][[j]][i, 1] &lt;- i\n        kmean_result_list[[\"cluster_sse_list\"]][[j]][i, 2] &lt;- kmean_result_list[[\"mse_list\"]][[j]][i]\n        kmean_result_list[[\"Variance_Explained\"]][[j]][i] &lt;- kmean_fit$betweenss / kmean_fit$totss\n        cat(\n            \"For \", j, \", K: \", i,\n            \"within-cluster MSE: \", kmean_result_list[[\"mse_list\"]][[j]][i],\n            \"Variance_Explained: \", kmean_result_list[[\"Variance_Explained\"]][[j]][i], \"\\n\"\n        )\n    }\n}\n\n\nFor  all_metabolites , K:  1 within-cluster MSE:  13619.32 Variance_Explained:  4.941702e-15 \nFor  all_metabolites , K:  2 within-cluster MSE:  6487.104 Variance_Explained:  0.04736709 \nFor  all_metabolites , K:  3 within-cluster MSE:  4205.437 Variance_Explained:  0.0736459 \nFor  all_metabolites , K:  4 within-cluster MSE:  3107.38 Variance_Explained:  0.08736085 \nFor  all_metabolites , K:  5 within-cluster MSE:  2443.924 Variance_Explained:  0.1027728 \nFor  all_metabolites , K:  6 within-cluster MSE:  2010.74 Variance_Explained:  0.114167 \nFor  all_metabolites , K:  7 within-cluster MSE:  1705.235 Variance_Explained:  0.1235504 \nFor  all_metabolites , K:  8 within-cluster MSE:  1481.469 Variance_Explained:  0.1297835 \nFor  all_metabolites , K:  9 within-cluster MSE:  1306.354 Variance_Explained:  0.1367274 \nFor  all_metabolites , K:  10 within-cluster MSE:  1166.08 Variance_Explained:  0.1438044 \nFor  all_metabolites , K:  11 within-cluster MSE:  1051.345 Variance_Explained:  0.1508531 \nFor  all_metabolites , K:  12 within-cluster MSE:  957.817 Variance_Explained:  0.1560656 \nFor  all_metabolites , K:  13 within-cluster MSE:  880.831 Variance_Explained:  0.1592231 \nFor  all_metabolites , K:  14 within-cluster MSE:  812.71 Variance_Explained:  0.1645731 \nFor  all_metabolites , K:  15 within-cluster MSE:  755.508 Variance_Explained:  0.1679013 \nFor  all_metabolites , K:  16 within-cluster MSE:  702.891 Variance_Explained:  0.1742422 \nFor  all_metabolites , K:  17 within-cluster MSE:  657.791 Variance_Explained:  0.1789275 \nFor  all_metabolites , K:  18 within-cluster MSE:  617.96 Variance_Explained:  0.1832713 \nFor  all_metabolites , K:  19 within-cluster MSE:  582.219 Variance_Explained:  0.1877591 \nFor  all_metabolites , K:  20 within-cluster MSE:  550.074 Variance_Explained:  0.1922144 \nFor  significant_metabolites , K:  1 within-cluster MSE:  2660.227 Variance_Explained:  -8.034324e-15 \nFor  significant_metabolites , K:  2 within-cluster MSE:  1022.505 Variance_Explained:  0.2312645 \nFor  significant_metabolites , K:  3 within-cluster MSE:  632.194 Variance_Explained:  0.28706 \nFor  significant_metabolites , K:  4 within-cluster MSE:  452.85 Variance_Explained:  0.3190806 \nFor  significant_metabolites , K:  5 within-cluster MSE:  354.801 Variance_Explained:  0.3331372 \nFor  significant_metabolites , K:  6 within-cluster MSE:  290.621 Variance_Explained:  0.3445191 \nFor  significant_metabolites , K:  7 within-cluster MSE:  245.829 Variance_Explained:  0.3531377 \nFor  significant_metabolites , K:  8 within-cluster MSE:  212.9 Variance_Explained:  0.3597537 \nFor  significant_metabolites , K:  9 within-cluster MSE:  187.706 Variance_Explained:  0.3649574 \nFor  significant_metabolites , K:  10 within-cluster MSE:  167.108 Variance_Explained:  0.3718263 \nFor  significant_metabolites , K:  11 within-cluster MSE:  150.928 Variance_Explained:  0.3759169 \nFor  significant_metabolites , K:  12 within-cluster MSE:  137.487 Variance_Explained:  0.3798103 \nFor  significant_metabolites , K:  13 within-cluster MSE:  126.018 Variance_Explained:  0.3841728 \nFor  significant_metabolites , K:  14 within-cluster MSE:  116.658 Variance_Explained:  0.3860626 \nFor  significant_metabolites , K:  15 within-cluster MSE:  107.843 Variance_Explained:  0.3919131 \nFor  significant_metabolites , K:  16 within-cluster MSE:  100.885 Variance_Explained:  0.393227 \nFor  significant_metabolites , K:  17 within-cluster MSE:  94.421 Variance_Explained:  0.3966092 \nFor  significant_metabolites , K:  18 within-cluster MSE:  88.62 Variance_Explained:  0.4003684 \nFor  significant_metabolites , K:  19 within-cluster MSE:  83.8 Variance_Explained:  0.4014796 \nFor  significant_metabolites , K:  20 within-cluster MSE:  79.155 Variance_Explained:  0.4049023 \n\n\nCode\nkmean_mse_data &lt;-\n    rbind(\n        data.frame(\n            metabolites = \"all_metabolites\",\n            k = kmean_result_list[[\"cluster_sse_list\"]][[\"all_metabolites\"]][, 1],\n            mse = kmean_result_list[[\"cluster_sse_list\"]][[\"all_metabolites\"]][, 2],\n            variance_exaplained = kmean_result_list[[\"Variance_Explained\"]][[\"all_metabolites\"]]\n        ),\n        data.frame(\n            metabolites = \"significant_metabolites\",\n            k = kmean_result_list[[\"cluster_sse_list\"]][[\"significant_metabolites\"]][, 1],\n            mse = kmean_result_list[[\"cluster_sse_list\"]][[\"significant_metabolites\"]][, 2],\n            variance_exaplained = kmean_result_list[[\"Variance_Explained\"]][[\"significant_metabolites\"]]\n        )\n    )\n\nggarrange(\n    ggplot(\n        data = kmean_mse_data,\n        aes(x = k, y = mse, group = metabolites, color = metabolites)\n    ) +\n        geom_line() +\n        geom_point() +\n        scale_color_manual(values = color_function(2)) +\n        labs(title = \"K Mean Clustering Result: MSE for All Metabolites vs Significant Ones\") +\n        xlab(\"Number of Clusters\") +\n        ylab(\"Mean Squared Error\"),\n    ggplot(\n        data = kmean_mse_data,\n        aes(x = k, y = variance_exaplained, group = metabolites, color = metabolites)\n    ) +\n        geom_line() +\n        geom_point() +\n        scale_color_manual(values = color_function(2)) +\n        labs(title = \"K Mean Clustering Result: Variance Explained for All Metabolites vs Significant Ones\") +\n        xlab(\"Number of Clusters\") +\n        ylab(\"Variance Exaplained\"),\n    ncol = 1\n)\n\n\n\n\n\n\n\n\n\nCode\n# K means\n\n\nkm_clustering &lt;- kmeans(normalized_metabolite_data, centers = 2, iter.max = 300)\nkm_significant_clustering &lt;- kmeans(normalized_significant_metabolite_data, centers = 2, iter.max = 300)\n\nconfusionMatrix(table(all_data[, 2], ifelse(km_clustering$cluster == 1, \"negative\", \"positive\")))\n\n\nConfusion Matrix and Statistics\n\n          \n           negative positive\n  negative       96      214\n  positive      147       43\n                                          \n               Accuracy : 0.278           \n                 95% CI : (0.2391, 0.3195)\n    No Information Rate : 0.514           \n    P-Value [Acc &gt; NIR] : 1.0000000       \n                                          \n                  Kappa : -0.4344         \n                                          \n Mcnemar's Test P-Value : 0.0005134       \n                                          \n            Sensitivity : 0.3951          \n            Specificity : 0.1673          \n         Pos Pred Value : 0.3097          \n         Neg Pred Value : 0.2263          \n             Prevalence : 0.4860          \n         Detection Rate : 0.1920          \n   Detection Prevalence : 0.6200          \n      Balanced Accuracy : 0.2812          \n                                          \n       'Positive' Class : negative        \n                                          \n\n\nCode\nconfusionMatrix(table(all_data[, 2], ifelse(km_significant_clustering$cluster == 1, \"negative\", \"positive\")))\n\n\nConfusion Matrix and Statistics\n\n          \n           negative positive\n  negative      101      209\n  positive      143       47\n                                          \n               Accuracy : 0.296           \n                 95% CI : (0.2563, 0.3381)\n    No Information Rate : 0.512           \n    P-Value [Acc &gt; NIR] : 1.0000000       \n                                          \n                  Kappa : -0.3999         \n                                          \n Mcnemar's Test P-Value : 0.0005312       \n                                          \n            Sensitivity : 0.4139          \n            Specificity : 0.1836          \n         Pos Pred Value : 0.3258          \n         Neg Pred Value : 0.2474          \n             Prevalence : 0.4880          \n         Detection Rate : 0.2020          \n   Detection Prevalence : 0.6200          \n      Balanced Accuracy : 0.2988          \n                                          \n       'Positive' Class : negative        \n                                          \n\n\nCode\noutcome_pca_km &lt;- outcome_scores %&gt;%\n    mutate(\n        km_clusters = km_clustering$cluster,\n        km_clusters = factor(km_clusters, levels = c(1, 2)),\n        km_significant_clusters = km_significant_clustering$cluster,\n        km_significant_clusters = factor(km_significant_clusters, levels = c(1, 2))\n    )\n\n\nggplot(\n    data = outcome_pca_km,\n    aes(x = pc1, y = pc2, color = km_clusters)\n) +\n    geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n    stat_ellipse(type = \"norm\", level = .99) +\n    geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n    geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n    scale_color_manual(values = color_function(2)) +\n    labs(\n        title = \"2D Scatter Plot of the First 2 PCs Grouped by K Mean Clusters, AD Status\",\n        subtitle = paste0(ncol(outcome_scores) - 2, \" Metabolites\")\n    )\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    data = outcome_pca_km,\n    aes(x = pc1, y = pc2, color = km_significant_clusters)\n) +\n    geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n    stat_ellipse(type = \"norm\", level = .99) +\n    geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n    geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n    scale_color_manual(values = color_function(2)) +\n    labs(\n        title = \"2D Scatter Plot of the First 2 PCs Grouped by K Mean Clusters, AD Status\",\n        subtitle = paste0(ncol(outcome_significant_scores) - 2, \" Metabolites\")\n    )\n\n\n\n\n\n\n\n\n\n비지도 학습 방법인 PCA와 K-means clustering를 이용하여 차원 축소와 군집화를 시도하였으나, 이 방법을 사용하는 모든 대사체에 대해 AD 상태가 명확하게 분류되지 않는 것으로 보인다. PCA와 K-means는 EDA에서 선별된 대사산물로 군집화를 수행했을 때 전체 metabolites 보다 선별된 metabolites 에서 AD 상태에 대한 정보가 조금 더 많이 설명되는 것을 PCA를 통해 관찰할 수 있었다. K means clustering도 선별된 metabolites에 대해서 성능 향상을 보여준다. 그러나 전반적인 정확도가 매우 낮기 때문에 지도 학습을 통해 AD 상태를 잘 설명하는 대사체를 선택할 것이다.\nDimensionality reduction and clustering were attempted using PCA and K-means clustering, which are unsupervised learning methods, but AD status seems to not be clearly classified for all metabolites using the methods. When PCA and K means clustering were performed with the metabolites selected from EDA, it was observed through PCA that a little more information about AD status was explained with the selected metabolites than with the entire set of metaboliotes. K means clustering also showed an improvement in performance with the selected metabolites. However, the overall accuracy is very low, so we will select metabolites that explain AD status well through supervised learning."
  },
  {
    "objectID": "docs/projects/LLFS/mining.html#data-mining",
    "href": "docs/projects/LLFS/mining.html#data-mining",
    "title": "Data Mining",
    "section": "",
    "text": "Code\nmetabolite_data &lt;- all_data[, -c(1:5)]\noutcome_data &lt;- all_data[, 2]\n\n# normalize the metaoblites\nnormalized_metabolite_data &lt;-\n    as.data.frame(lapply(metabolite_data, function(x) scale_function(vector = x, method = \"min-max\")))\nnormalized_significant_metabolite_data &lt;-\n    normalized_metabolite_data %&gt;%\n    dplyr::select(all_of(significant_metabolites))\n\n# extract the latent variables (PCs: Principal Components)\npc_metabolites &lt;-\n    prcomp(normalized_metabolite_data)\npc_significant_metabolites &lt;-\n    prcomp(normalized_significant_metabolite_data)\n\n# calculate scores\nscores &lt;-\n    as.data.frame(pc_metabolites$x) %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(row_names = 1:n())\nsignificant_scores &lt;-\n    as.data.frame(pc_significant_metabolites$x) %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(row_names = 1:n())\n\ntemp &lt;-\n    as.data.frame(pc_metabolites$rotation) %&gt;%\n    janitor::clean_names()\nloadings &lt;- temp %&gt;%\n    mutate(\n        metabolites = rownames(.),\n        arrow_size_normalization = min(\n            (max(scores[, \"pc1\"]) - min(scores[, \"pc1\"]) /\n                (max(temp[, \"pc1\"]) - min(temp[, \"pc1\"]))),\n            (max(scores[, \"pc2\"]) - min(scores[, \"pc2\"]) /\n                (max(temp[, \"pc2\"]) - min(temp[, \"pc2\"]))),\n            (max(scores[, \"pc3\"]) - min(scores[, \"pc3\"]) /\n                (max(temp[, \"pc3\"]) - min(temp[, \"pc3\"])))\n        ),\n        arrow_pc1 = arrow_size_normalization * pc1,\n        arrow_pc2 = arrow_size_normalization * pc2,\n        arrow_pc3 = arrow_size_normalization * pc3\n    )\n\ntemp &lt;-\n    as.data.frame(pc_significant_metabolites$rotation) %&gt;%\n    janitor::clean_names()\nsignificant_loadings &lt;- temp %&gt;%\n    mutate(\n        metabolites = rownames(.),\n        arrow_size_normalization = min(\n            (max(scores[, \"pc1\"]) - min(scores[, \"pc1\"]) /\n                (max(temp[, \"pc1\"]) - min(temp[, \"pc1\"]))),\n            (max(scores[, \"pc2\"]) - min(scores[, \"pc2\"]) /\n                (max(temp[, \"pc2\"]) - min(temp[, \"pc2\"]))),\n            (max(scores[, \"pc3\"]) - min(scores[, \"pc3\"]) /\n                (max(temp[, \"pc3\"]) - min(temp[, \"pc3\"])))\n        ),\n        arrow_pc1 = arrow_size_normalization * pc1,\n        arrow_pc2 = arrow_size_normalization * pc2,\n        arrow_pc3 = arrow_size_normalization * pc3\n    )\n# arrow_size_normalization is a normalization factor that\n# ensures the variable loading arrows are scaled appropriately relative to the data points.\n# The min() function to find the smallest ratio between the range of the data points and\n# the range of the variable loadings along each principal component axis (pc1, pc2, and pc3).\n# The reason why I select the first 3 components is that\n# '3' is the maximum dimension that can visualize the PCA results in 3d.\n\noutcome_scores &lt;-\n    scores %&gt;%\n    mutate(\n        outcome = outcome_data,\n        row_names = 1:n()\n    )\noutcome_significant_scores &lt;-\n    significant_scores %&gt;%\n    mutate(\n        outcome = outcome_data,\n        row_names = 1:n()\n    )\n\n\n# total variance\ntotal_variance &lt;-\n    data.frame(\n        pc = 1:length(pc_metabolites$sdev),\n        pc_variance_proportion = summary(pc_metabolites)[[\"importance\"]][\"Proportion of Variance\", ],\n        cumulative_proportion = summary(pc_metabolites)[[\"importance\"]][\"Cumulative Proportion\", ] * 100\n    )\ntotal_variance_significance &lt;-\n    data.frame(\n        pc = 1:length(pc_significant_metabolites$sdev),\n        pc_variance_proportion = summary(pc_significant_metabolites)[[\"importance\"]][\"Proportion of Variance\", ],\n        cumulative_proportion = summary(pc_significant_metabolites)[[\"importance\"]][\"Cumulative Proportion\", ] * 100\n    )\n\nscree_plot &lt;- function(indata) {\n    scree_plot1 &lt;- ggplot(\n        data = indata,\n        aes(x = pc, y = pc_variance_proportion, group = 1)\n    ) +\n        geom_point() +\n        geom_line() +\n        labs(title = \"\", subtitle = paste0(\n            \"Scree Plot, Total Variance(\",\n            round(tail(indata, 1)[\"cumulative_proportion\"], 3),\n            \"%)Explained by \", nrow(indata), \" PCs\"\n        )) +\n        ylab(\"Total Variance Explained\") +\n        xlab(\"Principal Components\")\n    scree_plot2 &lt;- ggplot(\n        data = indata %&gt;% filter(pc &lt; 13),\n        aes(x = pc, y = pc_variance_proportion, group = 1)\n    ) +\n        geom_point() +\n        geom_line() +\n        labs(title = \"\", subtitle = paste0(\n            \"Scree Plot, Part of Variance(\",\n            round(tail(indata %&gt;% filter(pc &lt; 13), 1)[\"cumulative_proportion\"], 3),\n            \"%)Explained by \", indata %&gt;% filter(pc &lt; 13) %&gt;% nrow(), \" PCs\"\n        )) +\n        ylab(\"Total Variance Explained\") +\n        xlab(\"Principal Components\")\n    return(ggarrange(scree_plot1, scree_plot2, nrow = 1))\n}\n\nggarrange(scree_plot(total_variance), scree_plot(total_variance_significance),\n    labels = c(\n        paste0(\"All \", ncol(metabolite_data), \" Metabolites\"),\n        paste0(length(significant_metabolites), \" Significant Metabolites\")\n    ), nrow = 2\n)\n\n\n\n\n\n\n\n\n\nCode\n# 2D PCA Scatter Plots with PC1 and PC2\n\nscatter_plot &lt;- function(in_data) {\n    p &lt;- ggplot(\n        data = in_data,\n        aes(x = pc1, y = pc2, color = outcome)\n    ) +\n        geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n        stat_ellipse(type = \"norm\", level = .99) +\n        geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n        geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n        scale_color_manual(values = color_function(length(unique(in_data$outcome)))) +\n        labs(\n            title = \"2D Scatter Plot of the First 2 PCs Grouped by AD status\",\n            subtitle = paste0(ncol(in_data) - 2, \" Metabolites\")\n        )\n    return(p)\n}\n\nggarrange(scatter_plot(outcome_scores),\n    scatter_plot(outcome_significant_scores),\n    nrow = 2\n)\n\n\n\n\n\n\n\n\n\nCode\n# biplot\nbi_plot &lt;- function(in_data) {\n    p &lt;-\n        ggplot(data = in_data, aes(x = pc1, y = pc2, color = outcome)) +\n        geom_text(alpha = .75, size = 3, aes(label = row_names)) +\n        geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n        geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n        coord_equal() +\n        scale_color_manual(values = color_function(length(unique(in_data$outcome)))) +\n        stat_ellipse(type = \"norm\", level = .99) +\n        geom_text(\n            data = loadings, aes(x = arrow_pc1, y = arrow_pc2, label = metabolites),\n            alpha = 0.5, size = 5, vjust = 1, color = \"red\"\n        ) +\n        geom_segment(\n            data = loadings, aes(x = 0, y = 0, xend = arrow_pc1, yend = arrow_pc2),\n            arrow = arrow(length = unit(0.5, \"cm\")), alpha = 0.5, color = \"red\"\n        ) +\n        labs(\n            title = \"Biplot, the Effect of Metabolites on Samples with Disease Status\",\n            subtitle = paste0(ncol(in_data) - 2, \" Metabolites\")\n        ) +\n        ylab(\"PC2\") +\n        xlab(\"PC1\")\n    return(p)\n}\nggarrange(bi_plot(outcome_scores), bi_plot(outcome_significant_scores), nrow = 2)\n\n\n\n\n\n\n\n\n\nCode\nplot_ly(\n    data = outcome_scores,\n    x = ~pc1, y = ~pc2, z = ~pc3,\n    type = \"scatter3d\", mode = \"markers\", color = ~ outcome_scores$outcome,\n    colors = color_function(2),\n    size = 2\n) %&gt;%\n    layout(\n        title = \"Effect of 500 Metabolites on Samples with Disease Status in 3d\",\n        scene = list(\n            bgcolor = \"#e5ecf6\",\n            xaxis = list(title = \"PC1\"),\n            yaxis = list(title = \"PC2\"),\n            zaxis = list(title = \"PC3\")\n        ),\n        legend = list(title = list(text = \"Disease(AD) Status\"))\n    )\n\n\n\n\n\n\nCode\nplot_ly(\n    data = outcome_significant_scores,\n    x = ~pc1, y = ~pc2, z = ~pc3,\n    type = \"scatter3d\", mode = \"markers\", color = ~ outcome_scores$outcome,\n    colors = color_function(2),\n    size = 2\n) %&gt;%\n    layout(\n        title = \"Effect of 201 Metabolites on Samples with Disease Status in 3d\",\n        scene = list(\n            bgcolor = \"#e5ecf6\",\n            xaxis = list(title = \"PC1\"),\n            yaxis = list(title = \"PC2\"),\n            zaxis = list(title = \"PC3\")\n        ),\n        legend = list(title = list(text = \"Disease(AD) Status\"))\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nkmean_result_list &lt;- list(\n    \"mse_list\" = list(\n        \"all_metabolites\" = matrix(nrow = 20),\n        \"significant_metabolites\" = matrix(nrow = 20)\n    ),\n    \"cluster_sse_list\" = list(\n        \"all_metabolites\" = matrix(nrow = 20, ncol = 2),\n        \"significant_metabolites\" = matrix(nrow = 20, ncol = 2)\n    ),\n    \"Variance_Explained\" = list(\n        \"all_metabolites\" = matrix(nrow = 20),\n        \"significant_metabolites\" = matrix(nrow = 20)\n    )\n)\n\nfor (j in c(\"all_metabolites\", \"significant_metabolites\")) {\n    for (i in (1:20)) {\n        if (j == \"all_metabolites\") {\n            kmean_fit &lt;- kmeans(normalized_metabolite_data, centers = i, iter.max = 300)\n        } else {\n            kmean_fit &lt;- kmeans(normalized_significant_metabolite_data, centers = i, iter.max = 300)\n        }\n        kmean_result_list[[\"mse_list\"]][[j]][i] &lt;- mean(kmean_fit$withinss) %&gt;% round(3)\n        kmean_result_list[[\"cluster_sse_list\"]][[j]][i, 1] &lt;- i\n        kmean_result_list[[\"cluster_sse_list\"]][[j]][i, 2] &lt;- kmean_result_list[[\"mse_list\"]][[j]][i]\n        kmean_result_list[[\"Variance_Explained\"]][[j]][i] &lt;- kmean_fit$betweenss / kmean_fit$totss\n        cat(\n            \"For \", j, \", K: \", i,\n            \"within-cluster MSE: \", kmean_result_list[[\"mse_list\"]][[j]][i],\n            \"Variance_Explained: \", kmean_result_list[[\"Variance_Explained\"]][[j]][i], \"\\n\"\n        )\n    }\n}\n\n\nFor  all_metabolites , K:  1 within-cluster MSE:  13619.32 Variance_Explained:  4.941702e-15 \nFor  all_metabolites , K:  2 within-cluster MSE:  6487.104 Variance_Explained:  0.04736709 \nFor  all_metabolites , K:  3 within-cluster MSE:  4205.437 Variance_Explained:  0.0736459 \nFor  all_metabolites , K:  4 within-cluster MSE:  3107.38 Variance_Explained:  0.08736085 \nFor  all_metabolites , K:  5 within-cluster MSE:  2443.924 Variance_Explained:  0.1027728 \nFor  all_metabolites , K:  6 within-cluster MSE:  2010.74 Variance_Explained:  0.114167 \nFor  all_metabolites , K:  7 within-cluster MSE:  1705.235 Variance_Explained:  0.1235504 \nFor  all_metabolites , K:  8 within-cluster MSE:  1481.469 Variance_Explained:  0.1297835 \nFor  all_metabolites , K:  9 within-cluster MSE:  1306.354 Variance_Explained:  0.1367274 \nFor  all_metabolites , K:  10 within-cluster MSE:  1166.08 Variance_Explained:  0.1438044 \nFor  all_metabolites , K:  11 within-cluster MSE:  1051.345 Variance_Explained:  0.1508531 \nFor  all_metabolites , K:  12 within-cluster MSE:  957.817 Variance_Explained:  0.1560656 \nFor  all_metabolites , K:  13 within-cluster MSE:  880.831 Variance_Explained:  0.1592231 \nFor  all_metabolites , K:  14 within-cluster MSE:  812.71 Variance_Explained:  0.1645731 \nFor  all_metabolites , K:  15 within-cluster MSE:  755.508 Variance_Explained:  0.1679013 \nFor  all_metabolites , K:  16 within-cluster MSE:  702.891 Variance_Explained:  0.1742422 \nFor  all_metabolites , K:  17 within-cluster MSE:  657.791 Variance_Explained:  0.1789275 \nFor  all_metabolites , K:  18 within-cluster MSE:  617.96 Variance_Explained:  0.1832713 \nFor  all_metabolites , K:  19 within-cluster MSE:  582.219 Variance_Explained:  0.1877591 \nFor  all_metabolites , K:  20 within-cluster MSE:  550.074 Variance_Explained:  0.1922144 \nFor  significant_metabolites , K:  1 within-cluster MSE:  2660.227 Variance_Explained:  -8.034324e-15 \nFor  significant_metabolites , K:  2 within-cluster MSE:  1022.505 Variance_Explained:  0.2312645 \nFor  significant_metabolites , K:  3 within-cluster MSE:  632.194 Variance_Explained:  0.28706 \nFor  significant_metabolites , K:  4 within-cluster MSE:  452.85 Variance_Explained:  0.3190806 \nFor  significant_metabolites , K:  5 within-cluster MSE:  354.801 Variance_Explained:  0.3331372 \nFor  significant_metabolites , K:  6 within-cluster MSE:  290.621 Variance_Explained:  0.3445191 \nFor  significant_metabolites , K:  7 within-cluster MSE:  245.829 Variance_Explained:  0.3531377 \nFor  significant_metabolites , K:  8 within-cluster MSE:  212.9 Variance_Explained:  0.3597537 \nFor  significant_metabolites , K:  9 within-cluster MSE:  187.706 Variance_Explained:  0.3649574 \nFor  significant_metabolites , K:  10 within-cluster MSE:  167.108 Variance_Explained:  0.3718263 \nFor  significant_metabolites , K:  11 within-cluster MSE:  150.928 Variance_Explained:  0.3759169 \nFor  significant_metabolites , K:  12 within-cluster MSE:  137.487 Variance_Explained:  0.3798103 \nFor  significant_metabolites , K:  13 within-cluster MSE:  126.018 Variance_Explained:  0.3841728 \nFor  significant_metabolites , K:  14 within-cluster MSE:  116.658 Variance_Explained:  0.3860626 \nFor  significant_metabolites , K:  15 within-cluster MSE:  107.843 Variance_Explained:  0.3919131 \nFor  significant_metabolites , K:  16 within-cluster MSE:  100.885 Variance_Explained:  0.393227 \nFor  significant_metabolites , K:  17 within-cluster MSE:  94.421 Variance_Explained:  0.3966092 \nFor  significant_metabolites , K:  18 within-cluster MSE:  88.62 Variance_Explained:  0.4003684 \nFor  significant_metabolites , K:  19 within-cluster MSE:  83.8 Variance_Explained:  0.4014796 \nFor  significant_metabolites , K:  20 within-cluster MSE:  79.155 Variance_Explained:  0.4049023 \n\n\nCode\nkmean_mse_data &lt;-\n    rbind(\n        data.frame(\n            metabolites = \"all_metabolites\",\n            k = kmean_result_list[[\"cluster_sse_list\"]][[\"all_metabolites\"]][, 1],\n            mse = kmean_result_list[[\"cluster_sse_list\"]][[\"all_metabolites\"]][, 2],\n            variance_exaplained = kmean_result_list[[\"Variance_Explained\"]][[\"all_metabolites\"]]\n        ),\n        data.frame(\n            metabolites = \"significant_metabolites\",\n            k = kmean_result_list[[\"cluster_sse_list\"]][[\"significant_metabolites\"]][, 1],\n            mse = kmean_result_list[[\"cluster_sse_list\"]][[\"significant_metabolites\"]][, 2],\n            variance_exaplained = kmean_result_list[[\"Variance_Explained\"]][[\"significant_metabolites\"]]\n        )\n    )\n\nggarrange(\n    ggplot(\n        data = kmean_mse_data,\n        aes(x = k, y = mse, group = metabolites, color = metabolites)\n    ) +\n        geom_line() +\n        geom_point() +\n        scale_color_manual(values = color_function(2)) +\n        labs(title = \"K Mean Clustering Result: MSE for All Metabolites vs Significant Ones\") +\n        xlab(\"Number of Clusters\") +\n        ylab(\"Mean Squared Error\"),\n    ggplot(\n        data = kmean_mse_data,\n        aes(x = k, y = variance_exaplained, group = metabolites, color = metabolites)\n    ) +\n        geom_line() +\n        geom_point() +\n        scale_color_manual(values = color_function(2)) +\n        labs(title = \"K Mean Clustering Result: Variance Explained for All Metabolites vs Significant Ones\") +\n        xlab(\"Number of Clusters\") +\n        ylab(\"Variance Exaplained\"),\n    ncol = 1\n)\n\n\n\n\n\n\n\n\n\nCode\n# K means\n\n\nkm_clustering &lt;- kmeans(normalized_metabolite_data, centers = 2, iter.max = 300)\nkm_significant_clustering &lt;- kmeans(normalized_significant_metabolite_data, centers = 2, iter.max = 300)\n\nconfusionMatrix(table(all_data[, 2], ifelse(km_clustering$cluster == 1, \"negative\", \"positive\")))\n\n\nConfusion Matrix and Statistics\n\n          \n           negative positive\n  negative       96      214\n  positive      147       43\n                                          \n               Accuracy : 0.278           \n                 95% CI : (0.2391, 0.3195)\n    No Information Rate : 0.514           \n    P-Value [Acc &gt; NIR] : 1.0000000       \n                                          \n                  Kappa : -0.4344         \n                                          \n Mcnemar's Test P-Value : 0.0005134       \n                                          \n            Sensitivity : 0.3951          \n            Specificity : 0.1673          \n         Pos Pred Value : 0.3097          \n         Neg Pred Value : 0.2263          \n             Prevalence : 0.4860          \n         Detection Rate : 0.1920          \n   Detection Prevalence : 0.6200          \n      Balanced Accuracy : 0.2812          \n                                          \n       'Positive' Class : negative        \n                                          \n\n\nCode\nconfusionMatrix(table(all_data[, 2], ifelse(km_significant_clustering$cluster == 1, \"negative\", \"positive\")))\n\n\nConfusion Matrix and Statistics\n\n          \n           negative positive\n  negative      101      209\n  positive      143       47\n                                          \n               Accuracy : 0.296           \n                 95% CI : (0.2563, 0.3381)\n    No Information Rate : 0.512           \n    P-Value [Acc &gt; NIR] : 1.0000000       \n                                          \n                  Kappa : -0.3999         \n                                          \n Mcnemar's Test P-Value : 0.0005312       \n                                          \n            Sensitivity : 0.4139          \n            Specificity : 0.1836          \n         Pos Pred Value : 0.3258          \n         Neg Pred Value : 0.2474          \n             Prevalence : 0.4880          \n         Detection Rate : 0.2020          \n   Detection Prevalence : 0.6200          \n      Balanced Accuracy : 0.2988          \n                                          \n       'Positive' Class : negative        \n                                          \n\n\nCode\noutcome_pca_km &lt;- outcome_scores %&gt;%\n    mutate(\n        km_clusters = km_clustering$cluster,\n        km_clusters = factor(km_clusters, levels = c(1, 2)),\n        km_significant_clusters = km_significant_clustering$cluster,\n        km_significant_clusters = factor(km_significant_clusters, levels = c(1, 2))\n    )\n\n\nggplot(\n    data = outcome_pca_km,\n    aes(x = pc1, y = pc2, color = km_clusters)\n) +\n    geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n    stat_ellipse(type = \"norm\", level = .99) +\n    geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n    geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n    scale_color_manual(values = color_function(2)) +\n    labs(\n        title = \"2D Scatter Plot of the First 2 PCs Grouped by K Mean Clusters, AD Status\",\n        subtitle = paste0(ncol(outcome_scores) - 2, \" Metabolites\")\n    )\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    data = outcome_pca_km,\n    aes(x = pc1, y = pc2, color = km_significant_clusters)\n) +\n    geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n    stat_ellipse(type = \"norm\", level = .99) +\n    geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n    geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n    scale_color_manual(values = color_function(2)) +\n    labs(\n        title = \"2D Scatter Plot of the First 2 PCs Grouped by K Mean Clusters, AD Status\",\n        subtitle = paste0(ncol(outcome_significant_scores) - 2, \" Metabolites\")\n    )\n\n\n\n\n\n\n\n\n\n비지도 학습 방법인 PCA와 K-means clustering를 이용하여 차원 축소와 군집화를 시도하였으나, 이 방법을 사용하는 모든 대사체에 대해 AD 상태가 명확하게 분류되지 않는 것으로 보인다. PCA와 K-means는 EDA에서 선별된 대사산물로 군집화를 수행했을 때 전체 metabolites 보다 선별된 metabolites 에서 AD 상태에 대한 정보가 조금 더 많이 설명되는 것을 PCA를 통해 관찰할 수 있었다. K means clustering도 선별된 metabolites에 대해서 성능 향상을 보여준다. 그러나 전반적인 정확도가 매우 낮기 때문에 지도 학습을 통해 AD 상태를 잘 설명하는 대사체를 선택할 것이다.\nDimensionality reduction and clustering were attempted using PCA and K-means clustering, which are unsupervised learning methods, but AD status seems to not be clearly classified for all metabolites using the methods. When PCA and K means clustering were performed with the metabolites selected from EDA, it was observed through PCA that a little more information about AD status was explained with the selected metabolites than with the entire set of metaboliotes. K means clustering also showed an improvement in performance with the selected metabolites. However, the overall accuracy is very low, so we will select metabolites that explain AD status well through supervised learning."
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html",
    "href": "docs/projects/LLFS/data_preparation.html",
    "title": "Data Preparation",
    "section": "",
    "text": "flowchart TB\n    subgraph Simulation\n        direction TB\n        subgraph Assign_Setting_Values\n            direction LR\n            Assign_Sample_Size---\n            Assign_Dimension_Size---\n            Assign_Covariance_Correlation---\n            Assign_Several_Proportions---\n            Assign_Noise_Intensity\n        end\n        subgraph Generate_Metabolite_Variables\n            direction LR\n            Generate_Covariance_Matrix---\n            Apply_Noise_to_Covariance---\n            Generate_Weights_Matrix---\n            Use_MVN_Distribution---\n            Generate_Metabolite_Data\n        end\n        subgraph Generate_Outcome_Variable\n            direction LR\n            Calculate_Score_Matrix---\n            Use_Logit_Link---\n            Calculate_Outcome_Probabilities---\n            Use_Binomial_Distribution1---\n            Generate_Binary_Outcome_Data\n        end\n        subgraph Generate_Sex_Variable\n            direction LR\n            Use_Binomial_Distribution2---\n            Generate_Sex_Data\n        end\n        subgraph Generate_Age_Variable\n            direction LR\n            Search_the_Strongest_Metabolite---\n            Rescale_It_to_Age---\n            Generate_Age_Data\n        end\n        subgraph Generate_Genotype_Variable\n            direction LR       \n            Calculate_Marginal_Proportions---\n            Calcualte_Joint_Proportions---\n            Generate_Genotype_data\n        end\n        subgraph Merge_All_Data\n            direction LR\n            Outcome_Variable---\n            Sex_Variable---\n            Age_Variable---\n            Genotype_Variable---\n            Metabolite_Data\n        end\n        Assign_Setting_Values--&gt;Generate_Metabolite_Variables--&gt;Generate_Outcome_Variable--&gt;\n        Generate_Sex_Variable--&gt;Generate_Age_Variable--&gt;Generate_Genotype_Variable--&gt;\n        Merge_All_Data\n    end\n    subgraph Data_Analytics\n        direction LR\n        exploratory_data_analysis---\n        statistical_analysis---\n        machine_learning\n    end\n    subgraph Conclusion\n        direction LR\n    end\n    Simulation--&gt;Data_Analytics--&gt;Conclusion\n\n\n\n\n\n\n\nMVN: Multivariate Normal Distirubtion\n\n\n\n\nKorean\n\n\n\n\nEnglish\n\n\n\n\n대략적인 분석 방법론을 간단히 보여주기 위해 Simulation을 수행했다. 이해를 돕기위해 Simulation 순서도를 간략히 설명하자면 크게 7 단계로 Simulation을 수행했다.\n\n\nData Set Size Setting\nSimulation에 필요한 몇 가지 설정값들을 Global Variables로 설정하여 후차적으로 작성된 스크립트에서 호출이 자유롭도록 작성했다. 변수들은 아래의 Simulation section에 있는 Global Variables (see Section 2.2) 에서 확인 가능하다.\nCategorial Data Setting\n먼저, 고차원 데이터의 차원을 설정하기 위해 Sample Size와 변수의 수를 설정한 후 Categorical predictors를 만들기 위해 잘 알려진 분포, 내가 정한 분포, 혹은 임의로 발생하게 만든 분포를 설정하였다. (see Section 2.2, Section 2.3, and Section 2.4)\nSex Variable Setting\nSex 변수는 \\(X \\sim \\text{Bernoulli}(0.5)\\) 을 통해 data를 생성했다. (see Section 2.4)\nGenotype Variable Setting\nGenotype 변수의 data는 아직도 어떻게 통계적으로 생성해야하는지 감을 못잡은 상태이기 때문에 더 연구가 필요하다. 하지만, 질병에 대한 유전적 영향도는 반영해야하기 때문에 outcome variable과 이미 잘 알려진 genotype의 분포를 반영하려고 노력했다. 두 변수에 연관성을 갖게하기 위해 각 변수의 proportion을 marginal distirubtion으로 설정하여 두 변수의 joint proportion을 계산하여 Genotype data를 생성했다. (see Section 2.3 and Section 2.4)\nMetabolite Data Setting\n고차원의 metabolite data를 만드는 설정으로, 고차원이면서 그룹내 서로 상관 관계가 있는 변수들을 생성하기 위해 난수에 의해 발생되는 임의의 Covariance를 생성하여 MVN (Multivariate Normal Distribution)에 반영되게 했고 각 그룹의 반응 변수로의 영향(또는 가중치)도 또한 난수로 임의적으로 발생되게 설정했다. 이때, 난수에 의해 임의적으로 발생하는 수치는 내가 임의적으로 범위를 한정했다. 재현성을 위해 seed number를 고정했다. (see Section 2.4)\nOutcome Variable Setting\nMVN에 의해 만들어진 Data와 미리 만들어 놓은 가중치 Matrix의 곱을 통해 Score Matrix를 만들고 Logit Link를 이용하여 각 Sample의 확률값을 만들었다. 각 Sample의 확률값을 기반으로 \\(X \\sim \\text{Bernoulli}(p)\\), (여기서 \\(p\\)는 각 sample이 갖는 확률값을 뜻한다), 을 통해 disease status의 정보를 담은 binary outcome variable를 만들었다. (see Section 2.4)\nAge Variable Setting\nAge 변수는 생물학적, 의학적으로 치매와 연관성이 높은 요인으로 Outcome 변수로 가장 설명이 잘되는 metabolite를 탐색해 선별하여 Age 형태로 변환을 했다. 제일 어린 사람을 65세 그리고 제일 연장자를 105세로 설정하여 min max normalization을 적용했다. (see (see Section 2.3 and Section 2.4)\nMerge All Data\nSimulation을 통해 만들어진 각 변수들을 index를 만들어 병합시켜 data frame의 형태로 만들었다. (see Section 2.4)\nAnalytics & Conclusion\n분석 부분은 이 data preparation section에서는 자세히 기술하지 않고 EDA, Statistical Approach 및 ML Approach Section에서 자세히 다룰예정이다. 간략히 말하면, outcome 변수와 통계적으로 유의한 관계를 갖는 metabolite를 선별하고 그 결과가 machine learning을 이용하여 얻은 결과와 얼마나 같은지 비교 분석을 하여 Outcome variable에 가장 연관성이 있는 변수들을 규명하는 방법을 기술할 예정이다.\n\n\n\n\n\n\n\nShow the code\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nShow the code\n# the number of samples\nsample_size &lt;- 500 #1000\n# the number of predictors\npredictor_size &lt;- 1000 #5000\n# the number of groups\ngroup_size &lt;- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors &lt;- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors&lt;-floor(significant_predictors*0.4) \nnegatively_associated_predictors&lt;-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list&lt;-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%&gt;%round(3) \nnames(group_proportion_list)&lt;-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix &lt;- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data&lt;-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%&gt;%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%&gt;%round(0),\n            # the 1st index of predictors in each group\n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE),\n            # effect of each group on an outcome variable \n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]&lt;-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]&lt;-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata&lt;-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix&lt;-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients &lt;- rnorm(predictor_size,0,0.05)\n\nanswer_list&lt;-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=NULL,max=NULL,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data \n    # that are statistically associated with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)&lt;-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite&lt;-\n        temp_df%&gt;%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%&gt;%\n        filter(abs_mean_diff==max(abs_mean_diff))%&gt;%\n        dplyr::select(metabolite)%&gt;%pull\n    \n    ## generate age data with min max normalization\n    age_data&lt;-\n        data%&gt;%\n        dplyr::select(strong_metabolite)%&gt;%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%&gt;%\n        rename(age=1)%&gt;%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated with \n    # some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n    binary_proportion&lt;-as.numeric(table(in_response)/sample_size) #the simulated proportion for the disease vs non-disease cases\n    genotype_proportion&lt;-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion&lt;-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range &lt;- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] &lt;- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) &lt;- 1\n    data[, group_range] &lt;- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names&lt;-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]&lt;-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities &lt;- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%&gt;%\n    ifelse(.&gt;1,1,.)%&gt;%\n    ifelse(.&lt;0,0,.)\n\nresponse &lt;-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%&gt;%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data &lt;- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data &lt;- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data &lt;- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data&lt;-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%&gt;%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nShow the code\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\nThis data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers.\n\n\n\n\nSimulations were performed to outline the approximate analysis methodology. For your better understanding, I will briefly describe the simulation flow diagram. The simulation was conducted in 9 steps.\n\n\nData Set Size Setting\nSome of the setting values ​​required for simulation are set as global variables so that they can be freely called in the later scripts. (see Section 2.2)\nCategorial Data Setting\nI first set the dimensions of my high-dimensional data by setting the sample size and number of variables, then I created categorical data by choosing a well-known distribution, a distribution I determined, or a distribution that occurred randomly set. (see Section 2.2, Section 2.3, and Section 2.4)\nSex Variable Setting\nThe data of the sex variable were generated through \\(X \\sim \\text{Bernoulli}(0.5)\\). (Section 2.4)\nGenotype Variable Setting\nPersonally, I have not yet figured out how to generate data for genotype (categorical) variables statistically, so further research is needed. However, since the genetic influence on the disease should be reflected, I tried to reflect the distribution of outcome variables and well-known distribution of the genotypes, APOE (from Wiki). To make an association between the two variables, the proportions of each variable were set as marginal distirubtion and the joint distribution of the two variables was calculated to generate genotype data. (Section 2.3 and Section 2.4)\nMetabolite Data Setting\nAs a setting for generating high-dimensional metabolomic data, a covariance matrix generated by random numbers is generated to create high-dimensional and mutually correlated metabolites within a group, which is used as input in the MVN (Multivariate Normal Distribution) function, and for each group, the metabolites’ effect (or weight) toward the outcome variable is also set to be randomly generated with a random number. At this time, the range of numbers randomly generated by random numbers was arbitrarily limited by myself. A seed number was fixed for reproducibility. (Section 2.4)\nOutcome Variable Setting\nA score matrix was created through the matrix multiplication of the data created by MVN and a pre-made weight matrix with the probability values of samples that were created using the Logit Link. Based on the probability value of each sample, a binary outcome variable representing disease status information was created through \\(X \\sim \\text{Bernoulli}(p)\\), (where \\(p\\) means the probability value of each sample). (Section 2.4)\nAge Variable Setting\nSince the Age variable is a important factor related to dementia biologically and medically, the metabolite best explained as an Outcome variable was selected and converted into an age scale using min-max normalization by setting the youngest to 65 and the oldest to 105. (Section 2.3 and Section 2.4)\nMerge All Data\nEach variable created through simulation was merged into a data frame. (Section 2.4)\nAnalytics & Conclusion\nThe analysis part will not be discussed in detail in this data preparation section, but will be covered in detail in the EDA, Statistical Approaches and ML Approaches section. Briefly, I describe a method to identify the variables most associated with the outcome variable by selecting metabolites that have a statistically significant relationship with the outcome variable and comparing how similar the results are to those obtained through machine learning.\n\n\n\n\n\n\n\nShow the code\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nShow the code\n# the number of samples\nsample_size &lt;- 500 #1000\n# the number of predictors\npredictor_size &lt;- 1000 #5000\n# the number of groups\ngroup_size &lt;- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors &lt;- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors&lt;-floor(significant_predictors*0.4) \nnegatively_associated_predictors&lt;-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list&lt;-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%&gt;%round(3) \nnames(group_proportion_list)&lt;-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix &lt;- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data&lt;-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%&gt;%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%&gt;%round(0),\n            # the 1st index of predictors in each group \n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE), \n            # effect of each group on an outcome variable\n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]&lt;-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]&lt;-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata&lt;-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix&lt;-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients &lt;- rnorm(predictor_size,0,0.05)\n\nanswer_list&lt;-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=a,max=b,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))/(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data that are statistically associated \n    # with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)&lt;-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite&lt;-\n        temp_df%&gt;%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%&gt;%\n        filter(abs_mean_diff==max(abs_mean_diff))%&gt;%\n        dplyr::select(metabolite)%&gt;%pull\n    \n    ## generate age data with min max normalization\n    age_data&lt;-\n        data%&gt;%\n        dplyr::select(strong_metabolite)%&gt;%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%&gt;%\n        rename(age=1)%&gt;%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated \n    # with some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n\n    ##the simulated proportion for the disease vs non-disease cases\n    binary_proportion&lt;-as.numeric(table(in_response)/sample_size) \n    genotype_proportion&lt;-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion&lt;-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range &lt;- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] &lt;- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) &lt;- 1\n    data[, group_range] &lt;- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names&lt;-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]&lt;-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities &lt;- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%&gt;%\n    ifelse(.&gt;1,1,.)%&gt;%\n    ifelse(.&lt;0,0,.)\n\nresponse &lt;-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%&gt;%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data &lt;- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data &lt;- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data &lt;- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data&lt;-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%&gt;%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nShow the code\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\nThis data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#simulation-flowchart",
    "href": "docs/projects/LLFS/data_preparation.html#simulation-flowchart",
    "title": "Data Preparation",
    "section": "",
    "text": "flowchart TB\n    subgraph Simulation\n        direction TB\n        subgraph Assign_Setting_Values\n            direction LR\n            Assign_Sample_Size---\n            Assign_Dimension_Size---\n            Assign_Covariance_Correlation---\n            Assign_Several_Proportions---\n            Assign_Noise_Intensity\n        end\n        subgraph Generate_Metabolite_Variables\n            direction LR\n            Generate_Covariance_Matrix---\n            Apply_Noise_to_Covariance---\n            Generate_Weights_Matrix---\n            Use_MVN_Distribution---\n            Generate_Metabolite_Data\n        end\n        subgraph Generate_Outcome_Variable\n            direction LR\n            Calculate_Score_Matrix---\n            Use_Logit_Link---\n            Calculate_Outcome_Probabilities---\n            Use_Binomial_Distribution1---\n            Generate_Binary_Outcome_Data\n        end\n        subgraph Generate_Sex_Variable\n            direction LR\n            Use_Binomial_Distribution2---\n            Generate_Sex_Data\n        end\n        subgraph Generate_Age_Variable\n            direction LR\n            Search_the_Strongest_Metabolite---\n            Rescale_It_to_Age---\n            Generate_Age_Data\n        end\n        subgraph Generate_Genotype_Variable\n            direction LR       \n            Calculate_Marginal_Proportions---\n            Calcualte_Joint_Proportions---\n            Generate_Genotype_data\n        end\n        subgraph Merge_All_Data\n            direction LR\n            Outcome_Variable---\n            Sex_Variable---\n            Age_Variable---\n            Genotype_Variable---\n            Metabolite_Data\n        end\n        Assign_Setting_Values--&gt;Generate_Metabolite_Variables--&gt;Generate_Outcome_Variable--&gt;\n        Generate_Sex_Variable--&gt;Generate_Age_Variable--&gt;Generate_Genotype_Variable--&gt;\n        Merge_All_Data\n    end\n    subgraph Data_Analytics\n        direction LR\n        exploratory_data_analysis---\n        statistical_analysis---\n        machine_learning\n    end\n    subgraph Conclusion\n        direction LR\n    end\n    Simulation--&gt;Data_Analytics--&gt;Conclusion\n\n\n\n\n\n\n\nMVN: Multivariate Normal Distirubtion\n\n\n\n\nKorean\n\n\n\n\nEnglish\n\n\n\n\n대략적인 분석 방법론을 간단히 보여주기 위해 Simulation을 수행했다. 이해를 돕기위해 Simulation 순서도를 간략히 설명하자면 크게 7 단계로 Simulation을 수행했다.\n\n\nData Set Size Setting\nSimulation에 필요한 몇 가지 설정값들을 Global Variables로 설정하여 후차적으로 작성된 스크립트에서 호출이 자유롭도록 작성했다. 변수들은 아래의 Simulation section에 있는 Global Variables (see Section 2.2) 에서 확인 가능하다.\nCategorial Data Setting\n먼저, 고차원 데이터의 차원을 설정하기 위해 Sample Size와 변수의 수를 설정한 후 Categorical predictors를 만들기 위해 잘 알려진 분포, 내가 정한 분포, 혹은 임의로 발생하게 만든 분포를 설정하였다. (see Section 2.2, Section 2.3, and Section 2.4)\nSex Variable Setting\nSex 변수는 \\(X \\sim \\text{Bernoulli}(0.5)\\) 을 통해 data를 생성했다. (see Section 2.4)\nGenotype Variable Setting\nGenotype 변수의 data는 아직도 어떻게 통계적으로 생성해야하는지 감을 못잡은 상태이기 때문에 더 연구가 필요하다. 하지만, 질병에 대한 유전적 영향도는 반영해야하기 때문에 outcome variable과 이미 잘 알려진 genotype의 분포를 반영하려고 노력했다. 두 변수에 연관성을 갖게하기 위해 각 변수의 proportion을 marginal distirubtion으로 설정하여 두 변수의 joint proportion을 계산하여 Genotype data를 생성했다. (see Section 2.3 and Section 2.4)\nMetabolite Data Setting\n고차원의 metabolite data를 만드는 설정으로, 고차원이면서 그룹내 서로 상관 관계가 있는 변수들을 생성하기 위해 난수에 의해 발생되는 임의의 Covariance를 생성하여 MVN (Multivariate Normal Distribution)에 반영되게 했고 각 그룹의 반응 변수로의 영향(또는 가중치)도 또한 난수로 임의적으로 발생되게 설정했다. 이때, 난수에 의해 임의적으로 발생하는 수치는 내가 임의적으로 범위를 한정했다. 재현성을 위해 seed number를 고정했다. (see Section 2.4)\nOutcome Variable Setting\nMVN에 의해 만들어진 Data와 미리 만들어 놓은 가중치 Matrix의 곱을 통해 Score Matrix를 만들고 Logit Link를 이용하여 각 Sample의 확률값을 만들었다. 각 Sample의 확률값을 기반으로 \\(X \\sim \\text{Bernoulli}(p)\\), (여기서 \\(p\\)는 각 sample이 갖는 확률값을 뜻한다), 을 통해 disease status의 정보를 담은 binary outcome variable를 만들었다. (see Section 2.4)\nAge Variable Setting\nAge 변수는 생물학적, 의학적으로 치매와 연관성이 높은 요인으로 Outcome 변수로 가장 설명이 잘되는 metabolite를 탐색해 선별하여 Age 형태로 변환을 했다. 제일 어린 사람을 65세 그리고 제일 연장자를 105세로 설정하여 min max normalization을 적용했다. (see (see Section 2.3 and Section 2.4)\nMerge All Data\nSimulation을 통해 만들어진 각 변수들을 index를 만들어 병합시켜 data frame의 형태로 만들었다. (see Section 2.4)\nAnalytics & Conclusion\n분석 부분은 이 data preparation section에서는 자세히 기술하지 않고 EDA, Statistical Approach 및 ML Approach Section에서 자세히 다룰예정이다. 간략히 말하면, outcome 변수와 통계적으로 유의한 관계를 갖는 metabolite를 선별하고 그 결과가 machine learning을 이용하여 얻은 결과와 얼마나 같은지 비교 분석을 하여 Outcome variable에 가장 연관성이 있는 변수들을 규명하는 방법을 기술할 예정이다.\n\n\n\n\n\n\n\nShow the code\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nShow the code\n# the number of samples\nsample_size &lt;- 500 #1000\n# the number of predictors\npredictor_size &lt;- 1000 #5000\n# the number of groups\ngroup_size &lt;- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors &lt;- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors&lt;-floor(significant_predictors*0.4) \nnegatively_associated_predictors&lt;-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list&lt;-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%&gt;%round(3) \nnames(group_proportion_list)&lt;-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix &lt;- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data&lt;-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%&gt;%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%&gt;%round(0),\n            # the 1st index of predictors in each group\n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE),\n            # effect of each group on an outcome variable \n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]&lt;-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]&lt;-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata&lt;-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix&lt;-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients &lt;- rnorm(predictor_size,0,0.05)\n\nanswer_list&lt;-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=NULL,max=NULL,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data \n    # that are statistically associated with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)&lt;-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite&lt;-\n        temp_df%&gt;%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%&gt;%\n        filter(abs_mean_diff==max(abs_mean_diff))%&gt;%\n        dplyr::select(metabolite)%&gt;%pull\n    \n    ## generate age data with min max normalization\n    age_data&lt;-\n        data%&gt;%\n        dplyr::select(strong_metabolite)%&gt;%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%&gt;%\n        rename(age=1)%&gt;%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated with \n    # some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n    binary_proportion&lt;-as.numeric(table(in_response)/sample_size) #the simulated proportion for the disease vs non-disease cases\n    genotype_proportion&lt;-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion&lt;-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range &lt;- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] &lt;- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) &lt;- 1\n    data[, group_range] &lt;- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names&lt;-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]&lt;-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities &lt;- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%&gt;%\n    ifelse(.&gt;1,1,.)%&gt;%\n    ifelse(.&lt;0,0,.)\n\nresponse &lt;-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%&gt;%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data &lt;- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data &lt;- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data &lt;- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data&lt;-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%&gt;%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nShow the code\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\nThis data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers.\n\n\n\n\nSimulations were performed to outline the approximate analysis methodology. For your better understanding, I will briefly describe the simulation flow diagram. The simulation was conducted in 9 steps.\n\n\nData Set Size Setting\nSome of the setting values ​​required for simulation are set as global variables so that they can be freely called in the later scripts. (see Section 2.2)\nCategorial Data Setting\nI first set the dimensions of my high-dimensional data by setting the sample size and number of variables, then I created categorical data by choosing a well-known distribution, a distribution I determined, or a distribution that occurred randomly set. (see Section 2.2, Section 2.3, and Section 2.4)\nSex Variable Setting\nThe data of the sex variable were generated through \\(X \\sim \\text{Bernoulli}(0.5)\\). (Section 2.4)\nGenotype Variable Setting\nPersonally, I have not yet figured out how to generate data for genotype (categorical) variables statistically, so further research is needed. However, since the genetic influence on the disease should be reflected, I tried to reflect the distribution of outcome variables and well-known distribution of the genotypes, APOE (from Wiki). To make an association between the two variables, the proportions of each variable were set as marginal distirubtion and the joint distribution of the two variables was calculated to generate genotype data. (Section 2.3 and Section 2.4)\nMetabolite Data Setting\nAs a setting for generating high-dimensional metabolomic data, a covariance matrix generated by random numbers is generated to create high-dimensional and mutually correlated metabolites within a group, which is used as input in the MVN (Multivariate Normal Distribution) function, and for each group, the metabolites’ effect (or weight) toward the outcome variable is also set to be randomly generated with a random number. At this time, the range of numbers randomly generated by random numbers was arbitrarily limited by myself. A seed number was fixed for reproducibility. (Section 2.4)\nOutcome Variable Setting\nA score matrix was created through the matrix multiplication of the data created by MVN and a pre-made weight matrix with the probability values of samples that were created using the Logit Link. Based on the probability value of each sample, a binary outcome variable representing disease status information was created through \\(X \\sim \\text{Bernoulli}(p)\\), (where \\(p\\) means the probability value of each sample). (Section 2.4)\nAge Variable Setting\nSince the Age variable is a important factor related to dementia biologically and medically, the metabolite best explained as an Outcome variable was selected and converted into an age scale using min-max normalization by setting the youngest to 65 and the oldest to 105. (Section 2.3 and Section 2.4)\nMerge All Data\nEach variable created through simulation was merged into a data frame. (Section 2.4)\nAnalytics & Conclusion\nThe analysis part will not be discussed in detail in this data preparation section, but will be covered in detail in the EDA, Statistical Approaches and ML Approaches section. Briefly, I describe a method to identify the variables most associated with the outcome variable by selecting metabolites that have a statistically significant relationship with the outcome variable and comparing how similar the results are to those obtained through machine learning.\n\n\n\n\n\n\n\nShow the code\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nShow the code\n# the number of samples\nsample_size &lt;- 500 #1000\n# the number of predictors\npredictor_size &lt;- 1000 #5000\n# the number of groups\ngroup_size &lt;- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors &lt;- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors&lt;-floor(significant_predictors*0.4) \nnegatively_associated_predictors&lt;-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list&lt;-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%&gt;%round(3) \nnames(group_proportion_list)&lt;-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix &lt;- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data&lt;-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%&gt;%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%&gt;%round(0),\n            # the 1st index of predictors in each group \n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE), \n            # effect of each group on an outcome variable\n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]&lt;-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]&lt;-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata&lt;-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix&lt;-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients &lt;- rnorm(predictor_size,0,0.05)\n\nanswer_list&lt;-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=a,max=b,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))/(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data that are statistically associated \n    # with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)&lt;-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite&lt;-\n        temp_df%&gt;%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%&gt;%\n        filter(abs_mean_diff==max(abs_mean_diff))%&gt;%\n        dplyr::select(metabolite)%&gt;%pull\n    \n    ## generate age data with min max normalization\n    age_data&lt;-\n        data%&gt;%\n        dplyr::select(strong_metabolite)%&gt;%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%&gt;%\n        rename(age=1)%&gt;%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated \n    # with some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n\n    ##the simulated proportion for the disease vs non-disease cases\n    binary_proportion&lt;-as.numeric(table(in_response)/sample_size) \n    genotype_proportion&lt;-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion&lt;-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range &lt;- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] &lt;- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) &lt;- 1\n    data[, group_range] &lt;- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names&lt;-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]&lt;-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities &lt;- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%&gt;%\n    ifelse(.&gt;1,1,.)%&gt;%\n    ifelse(.&lt;0,0,.)\n\nresponse &lt;-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%&gt;%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data &lt;- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data &lt;- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data &lt;- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data&lt;-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%&gt;%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nShow the code\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\nThis data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#simulation",
    "href": "docs/projects/LLFS/data_preparation.html#simulation",
    "title": "Data Preparation",
    "section": "",
    "text": "Show the code\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nShow the code\n# the number of samples\nsample_size &lt;- 500 #1000\n# the number of predictors\npredictor_size &lt;- 1000 #5000\n# the number of groups\ngroup_size &lt;- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors &lt;- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors&lt;-floor(significant_predictors*0.4) \nnegatively_associated_predictors&lt;-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list&lt;-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%&gt;%round(3) \nnames(group_proportion_list)&lt;-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix &lt;- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data&lt;-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%&gt;%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%&gt;%round(0),\n            # the 1st index of predictors in each group\n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE),\n            # effect of each group on an outcome variable \n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]&lt;-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]&lt;-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata&lt;-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix&lt;-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients &lt;- rnorm(predictor_size,0,0.05)\n\nanswer_list&lt;-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=NULL,max=NULL,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data \n    # that are statistically associated with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)&lt;-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite&lt;-\n        temp_df%&gt;%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%&gt;%\n        filter(abs_mean_diff==max(abs_mean_diff))%&gt;%\n        dplyr::select(metabolite)%&gt;%pull\n    \n    ## generate age data with min max normalization\n    age_data&lt;-\n        data%&gt;%\n        dplyr::select(strong_metabolite)%&gt;%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%&gt;%\n        rename(age=1)%&gt;%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated with \n    # some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n    binary_proportion&lt;-as.numeric(table(in_response)/sample_size) #the simulated proportion for the disease vs non-disease cases\n    genotype_proportion&lt;-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion&lt;-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range &lt;- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] &lt;- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) &lt;- 1\n    data[, group_range] &lt;- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names&lt;-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]&lt;-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities &lt;- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%&gt;%\n    ifelse(.&gt;1,1,.)%&gt;%\n    ifelse(.&lt;0,0,.)\n\nresponse &lt;-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%&gt;%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data &lt;- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data &lt;- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data &lt;- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data&lt;-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%&gt;%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nShow the code\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)"
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#data-description",
    "href": "docs/projects/LLFS/data_preparation.html#data-description",
    "title": "Data Preparation",
    "section": "",
    "text": "This data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#simulation-1",
    "href": "docs/projects/LLFS/data_preparation.html#simulation-1",
    "title": "Data Preparation",
    "section": "",
    "text": "Show the code\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nShow the code\n# the number of samples\nsample_size &lt;- 500 #1000\n# the number of predictors\npredictor_size &lt;- 1000 #5000\n# the number of groups\ngroup_size &lt;- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors &lt;- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors&lt;-floor(significant_predictors*0.4) \nnegatively_associated_predictors&lt;-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list&lt;-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%&gt;%round(3) \nnames(group_proportion_list)&lt;-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix &lt;- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data&lt;-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%&gt;%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%&gt;%round(0),\n            # the 1st index of predictors in each group \n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE), \n            # effect of each group on an outcome variable\n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]&lt;-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]&lt;-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata&lt;-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix&lt;-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients &lt;- rnorm(predictor_size,0,0.05)\n\nanswer_list&lt;-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=a,max=b,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))/(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data that are statistically associated \n    # with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)&lt;-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite&lt;-\n        temp_df%&gt;%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%&gt;%\n        filter(abs_mean_diff==max(abs_mean_diff))%&gt;%\n        dplyr::select(metabolite)%&gt;%pull\n    \n    ## generate age data with min max normalization\n    age_data&lt;-\n        data%&gt;%\n        dplyr::select(strong_metabolite)%&gt;%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%&gt;%\n        rename(age=1)%&gt;%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated \n    # with some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n\n    ##the simulated proportion for the disease vs non-disease cases\n    binary_proportion&lt;-as.numeric(table(in_response)/sample_size) \n    genotype_proportion&lt;-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion&lt;-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range &lt;- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] &lt;- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) &lt;- 1\n    data[, group_range] &lt;- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names&lt;-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]&lt;-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities &lt;- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%&gt;%\n    ifelse(.&gt;1,1,.)%&gt;%\n    ifelse(.&lt;0,0,.)\n\nresponse &lt;-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%&gt;%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data &lt;- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data &lt;- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data &lt;- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data&lt;-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%&gt;%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nShow the code\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)"
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#data-description-1",
    "href": "docs/projects/LLFS/data_preparation.html#data-description-1",
    "title": "Data Preparation",
    "section": "",
    "text": "This data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html",
    "href": "docs/projects/heavy_metal/index.html",
    "title": "Project Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n중금속(예: 구리(Cu), 아연(Zn) 및 코발트(Co))은 silent 오염 물질이다. 물 속에서 그들의 존재는 쉽게 감지되지 않는다. 이러한 오염 물질은 다양한 방식으로 우리 몸에 들어갈 수 있는데 오염된 물을 마시거나 오염된 음식을 섭취함으로써 이러한 오염 물질은 인간과 다른 살아있는 유기체에 독성 영향을 미치게 된다. 사용한 찻잎은 오염된 폐수에서 발견되는 2가 Cu, Zn 및 Co 이온에 대한 흡착 기질로 사용할 수 있다.\n오염된 물에서 중금속을 제거하기 위해 찻잎을 사용할 수 있는 비율을 모델링하기 위해 미분 방정식이 사용되었다. 이 모델은 실험의 시계열 데이터와 비선형 최소 제곱 회귀를 사용하여 테스트되었다.\n\n\n\n\n찻잎에 대한 중금속의 흡착 과정은 미분 방정식을 사용하여 mechnistically 모델링할 수 있다.\nmechanistic model parameter는 시계열 데이터에 맞게 추정할 수 있다.\n\n\n\n\n\n\n\n\n\n\\(S(t)\\) is the number of heavy metal molecules adsorbed to the tea leaves at time \\(t\\).\n\\(W(t)\\) is the number of heavy metal molecules in the water (and not adsorbed) at time \\(t\\).\n\\(W_0=W(0)=S(t)+W(t)\\)\n\\(S_e\\) is the number of heavy metal molecules adsorbed at the equilibrium state = the “relevant” number of adsorption sites. \\(S_e\\) is system dependent: e.g., a different initial concentration will have a different \\(S_e\\)\n\n\\(S_e=W_0 - W_e\\)\n\\(q(t)\\) is the fraction of heavy metal molecules adsorbed out of the waste water at time \\(t\\)\n\\(q(t)=\\frac{S(t)}{W_0}\\)\n\n\n\n\n\\[\n\\begin{align*}\n\\frac{dS}{dt} &=\\frac{W_0-S(t)}{V}fb\\frac{S_e-S(t)}{S_e}p\\\\\n&=k_s(W_0-S(t))(S_e-S(t)) \\text{ where } k_s=\\frac{fbp}{VS_e}\n\\end{align*}\n\\]\nwhere\n\n\\(\\frac{W_0-S(t)}{V}fb\\) is a fraction of heavy metal molecules per \\(mL\\) of water,\n\\(f\\) is \\(mL\\)’s of water that contact leaves per minute,\n\\(b\\) is the probability that a leaf hitting molecule hits a binding site,\n\\(\\frac{S_e-S(t)}{S_e}\\) is a fraction of biniding sites that are occupied, and\n\\(p\\) is the probability that hitting an unoccupied binding site results in binding.\n\nSince \\(q(t)=\\frac{S(t)}{W_0}\\),\n\\[\n\\begin{align*}\n\\frac{dq}{dt} &=\\frac{S'(t)}{W_0}\\\\\n&=\\frac{k_S}{W_0}(W_0-S(t))(S_e-S(t))\\\\\n&=W_0k_S\\frac{(W_0-S(t))(S_e-S(t))}{W_0^2}\\\\\n&=k_S(1-q(t))(q_e-q(t)) \\text{ where } k_q=W_0k_S=\\frac{W_0fbp}{VS_e}=\\frac{fbp}{Vq_e} \\\\\n&=\\frac{dq}{dt} \\\\\n&=k_q(1-q)(q_e-q) \\\\\nq(0) &=0\n\\end{align*}\n\\]\nSolving the above initial value problem: \\[\n\\begin{align*}\n\\frac{dq}{dt}&=k_q(1-q)(q_e-q) \\\\\n\\frac{dq}{(1-q)(q_e-q)}&=k_qdt \\\\\n\\int \\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\frac{1}{(1-q)(q_e-q)}&=\\frac{A}{1-q}+\\frac{B}{q_e-q} \\\\\n1&=A(q_e-q)+B(1-q)\\\\\n1&=A(q_e-1) \\text{ by letting } q=1\\\\\n(q_e-1)^{-1}&=A\\\\\n1&=B(1-q_e) \\text{ by letting } q=q_e\\\\\n(1-q_e)^{-1}&=B\\\\\n\\int\\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\int \\left \\{\\frac{(q_e-1)^{-1}}{1-q}+\\frac{(1-q_e)^{-1}}{q_e-q}\\right\\}dq&=\\int k_qdt \\\\\nq(t)&= \\frac{q_e-q_ee^{-kt}}{1-q_ee^{-kt}}=q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}} \\text{ where } k=(1-q_e)k_q\\\\\n\\arg\\min_k&\\sum_{i=1}^{n}(q(t_i,k)-\\hat{q}(t_i))^2\n\\end{align*}\n\\] where\n\n\\((t_i,\\hat{q}(t_i))\\) is the observed values,\n\\(q(t,k) =q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}}\\)\n\n\n\n\n\n\n\nMathematics: differential equation\nStatistics: non-linear least square with the Levenberg-Marquardt algorithm\nBiology\n\n\n\n\n\n1 mathematics professor (advisor)\n1 biology professor (advisor)\n2 mathematicians\n1 biologist\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetal\n\\(q_e\\)\nparameter\nEstimate\nStd. Error\n\\(t\\) value\n\\(Pr(&gt;|t|)\\)\ndf\nn\n\n\n\n\ncopper\n.46\n\\(k\\)\n0.01281219\n0.0008855\n20.47\n\\(&lt;2e-16\\)\n34\n35\n\n\ncobalt\n.235\n\\(k\\)\n0.069941\n0.003279\n21.33\n\\(&lt;2e-16\\)\n34\n35\n\n\nzinc\n.24\n\\(k\\)\n0.150255\n0.009133\n16.45\n\\(&lt;2e-16\\)\n32\n33\n\n\n\n\n\n\n\n\n\nHeavy metals (such as Copper (Cu), Zinc (Zn), and Cobalt (Co)) are silent pollutants; their presence in water is not easily detected. These pollutants can enter our bodies in different ways, e.g. through drinking contaminated water or eating contaminated food. These pollutants have toxic effects on humans and other living organisms. Spent tea leaves can be used as adsorbent substrates for divalent Cu, Zn and Co ions found in polluted waste water.\nDifferential equations were used to model the rate at which tea leaves can be used to clear heavy metals from polluted water. The model was tested using time series data from experiments and non-linear least squares regression.\n\n\n\n\n\nthe adsorption process of heavy metals to tea leaves can be mechanistically modeld using a differential equation.\nthe parameter of the mechanistic model can be estimated to fit the time series data.\n\n\n\n\n\n\n\n\n\n\\(S(t)\\) is the number of heavy metal molecules adsorbed to the tea leaves at time \\(t\\).\n\\(W(t)\\) is the number of heavy metal molecules in the water (and not adsorbed) at time \\(t\\).\n\\(W_0=W(0)=S(t)+W(t)\\)\n\\(S_e\\) is the number of heavy metal molecules adsorbed at the equilibrium state = the “relevant” number of adsorption sites. \\(S_e\\) is system dependent: e.g., a different initial concentration will have a different \\(S_e\\)\n\n\\(S_e=W_0 - W_e\\)\n\\(q(t)\\) is the fraction of heavy metal molecules adsorbed out of the waste water at time \\(t\\)\n\\(q(t)=\\frac{S(t)}{W_0}\\)\n\n\n\n\n\\[\n\\begin{align*}\n\\frac{dS}{dt} &=\\frac{W_0-S(t)}{V}fb\\frac{S_e-S(t)}{S_e}p\\\\\n&=k_s(W_0-S(t))(S_e-S(t)) \\text{ where } k_s=\\frac{fbp}{VS_e}\n\\end{align*}\n\\]\nwhere\n\n\\(\\frac{W_0-S(t)}{V}fb\\) is a fraction of heavy metal molecules per \\(mL\\) of water,\n\\(f\\) is \\(mL\\)’s of water that contact leaves per minute,\n\\(b\\) is the probability that a leaf hitting molecule hits a binding site,\n\\(\\frac{S_e-S(t)}{S_e}\\) is a fraction of biniding sites that are occupied, and\n\\(p\\) is the probability that hitting an unoccupied binding site results in binding.\n\nSince \\(q(t)=\\frac{S(t)}{W_0}\\),\n\\[\n\\begin{align*}\n\\frac{dq}{dt} &=\\frac{S'(t)}{W_0}\\\\\n&=\\frac{k_S}{W_0}(W_0-S(t))(S_e-S(t))\\\\\n&=W_0k_S\\frac{(W_0-S(t))(S_e-S(t))}{W_0^2}\\\\\n&=k_S(1-q(t))(q_e-q(t)) \\text{ where } k_q=W_0k_S=\\frac{W_0fbp}{VS_e}=\\frac{fbp}{Vq_e} \\\\\n&=\\frac{dq}{dt} \\\\\n&=k_q(1-q)(q_e-q) \\\\\nq(0) &=0\n\\end{align*}\n\\]\nSolving the above initial value problem: \\[\n\\begin{align*}\n\\frac{dq}{dt}&=k_q(1-q)(q_e-q) \\\\\n\\frac{dq}{(1-q)(q_e-q)}&=k_qdt \\\\\n\\int \\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\frac{1}{(1-q)(q_e-q)}&=\\frac{A}{1-q}+\\frac{B}{q_e-q} \\\\\n1&=A(q_e-q)+B(1-q)\\\\\n1&=A(q_e-1) \\text{ by letting } q=1\\\\\n(q_e-1)^{-1}&=A\\\\\n1&=B(1-q_e) \\text{ by letting } q=q_e\\\\\n(1-q_e)^{-1}&=B\\\\\n\\int\\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\int \\left \\{\\frac{(q_e-1)^{-1}}{1-q}+\\frac{(1-q_e)^{-1}}{q_e-q}\\right\\}dq&=\\int k_qdt \\\\\nq(t)&= \\frac{q_e-q_ee^{-kt}}{1-q_ee^{-kt}}=q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}} \\text{ where } k=(1-q_e)k_q\\\\\n\\arg\\min_k&\\sum_{i=1}^{n}(q(t_i,k)-\\hat{q}(t_i))^2\n\\end{align*}\n\\] where\n\n\\((t_i,\\hat{q}(t_i))\\) is the observed values,\n\\(q(t,k) =q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}}\\)\n\n\n\n\n\n\n\nMathematics: differential equation\nStatistics: non-linear least square with the Levenberg-Marquardt algorithm\nBiology\n\n\n\n\n\n1 mathematics professor (advisor)\n1 biology professor (advisor)\n2 mathematicians\n1 biologist\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetal\n\\(q_e\\)\nparameter\nEstimate\nStd. Error\n\\(t\\) value\n\\(Pr(&gt;|t|)\\)\ndf\nn\n\n\n\n\ncopper\n.46\n\\(k\\)\n0.01281219\n0.0008855\n20.47\n\\(&lt;2e-16\\)\n34\n35\n\n\ncobalt\n.235\n\\(k\\)\n0.069941\n0.003279\n21.33\n\\(&lt;2e-16\\)\n34\n35\n\n\nzinc\n.24\n\\(k\\)\n0.150255\n0.009133\n16.45\n\\(&lt;2e-16\\)\n32\n33"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#objective",
    "href": "docs/projects/heavy_metal/index.html#objective",
    "title": "Project Description",
    "section": "",
    "text": "찻잎에 대한 중금속의 흡착 과정은 미분 방정식을 사용하여 mechnistically 모델링할 수 있다.\nmechanistic model parameter는 시계열 데이터에 맞게 추정할 수 있다."
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#methodology",
    "href": "docs/projects/heavy_metal/index.html#methodology",
    "title": "Project Description",
    "section": "",
    "text": "\\(S(t)\\) is the number of heavy metal molecules adsorbed to the tea leaves at time \\(t\\).\n\\(W(t)\\) is the number of heavy metal molecules in the water (and not adsorbed) at time \\(t\\).\n\\(W_0=W(0)=S(t)+W(t)\\)\n\\(S_e\\) is the number of heavy metal molecules adsorbed at the equilibrium state = the “relevant” number of adsorption sites. \\(S_e\\) is system dependent: e.g., a different initial concentration will have a different \\(S_e\\)\n\n\\(S_e=W_0 - W_e\\)\n\\(q(t)\\) is the fraction of heavy metal molecules adsorbed out of the waste water at time \\(t\\)\n\\(q(t)=\\frac{S(t)}{W_0}\\)\n\n\n\n\n\\[\n\\begin{align*}\n\\frac{dS}{dt} &=\\frac{W_0-S(t)}{V}fb\\frac{S_e-S(t)}{S_e}p\\\\\n&=k_s(W_0-S(t))(S_e-S(t)) \\text{ where } k_s=\\frac{fbp}{VS_e}\n\\end{align*}\n\\]\nwhere\n\n\\(\\frac{W_0-S(t)}{V}fb\\) is a fraction of heavy metal molecules per \\(mL\\) of water,\n\\(f\\) is \\(mL\\)’s of water that contact leaves per minute,\n\\(b\\) is the probability that a leaf hitting molecule hits a binding site,\n\\(\\frac{S_e-S(t)}{S_e}\\) is a fraction of biniding sites that are occupied, and\n\\(p\\) is the probability that hitting an unoccupied binding site results in binding.\n\nSince \\(q(t)=\\frac{S(t)}{W_0}\\),\n\\[\n\\begin{align*}\n\\frac{dq}{dt} &=\\frac{S'(t)}{W_0}\\\\\n&=\\frac{k_S}{W_0}(W_0-S(t))(S_e-S(t))\\\\\n&=W_0k_S\\frac{(W_0-S(t))(S_e-S(t))}{W_0^2}\\\\\n&=k_S(1-q(t))(q_e-q(t)) \\text{ where } k_q=W_0k_S=\\frac{W_0fbp}{VS_e}=\\frac{fbp}{Vq_e} \\\\\n&=\\frac{dq}{dt} \\\\\n&=k_q(1-q)(q_e-q) \\\\\nq(0) &=0\n\\end{align*}\n\\]\nSolving the above initial value problem: \\[\n\\begin{align*}\n\\frac{dq}{dt}&=k_q(1-q)(q_e-q) \\\\\n\\frac{dq}{(1-q)(q_e-q)}&=k_qdt \\\\\n\\int \\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\frac{1}{(1-q)(q_e-q)}&=\\frac{A}{1-q}+\\frac{B}{q_e-q} \\\\\n1&=A(q_e-q)+B(1-q)\\\\\n1&=A(q_e-1) \\text{ by letting } q=1\\\\\n(q_e-1)^{-1}&=A\\\\\n1&=B(1-q_e) \\text{ by letting } q=q_e\\\\\n(1-q_e)^{-1}&=B\\\\\n\\int\\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\int \\left \\{\\frac{(q_e-1)^{-1}}{1-q}+\\frac{(1-q_e)^{-1}}{q_e-q}\\right\\}dq&=\\int k_qdt \\\\\nq(t)&= \\frac{q_e-q_ee^{-kt}}{1-q_ee^{-kt}}=q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}} \\text{ where } k=(1-q_e)k_q\\\\\n\\arg\\min_k&\\sum_{i=1}^{n}(q(t_i,k)-\\hat{q}(t_i))^2\n\\end{align*}\n\\] where\n\n\\((t_i,\\hat{q}(t_i))\\) is the observed values,\n\\(q(t,k) =q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}}\\)"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#required-skills",
    "href": "docs/projects/heavy_metal/index.html#required-skills",
    "title": "Project Description",
    "section": "",
    "text": "Mathematics: differential equation\nStatistics: non-linear least square with the Levenberg-Marquardt algorithm\nBiology"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#colaborators",
    "href": "docs/projects/heavy_metal/index.html#colaborators",
    "title": "Project Description",
    "section": "",
    "text": "1 mathematics professor (advisor)\n1 biology professor (advisor)\n2 mathematicians\n1 biologist"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#result",
    "href": "docs/projects/heavy_metal/index.html#result",
    "title": "Project Description",
    "section": "",
    "text": "Metal\n\\(q_e\\)\nparameter\nEstimate\nStd. Error\n\\(t\\) value\n\\(Pr(&gt;|t|)\\)\ndf\nn\n\n\n\n\ncopper\n.46\n\\(k\\)\n0.01281219\n0.0008855\n20.47\n\\(&lt;2e-16\\)\n34\n35\n\n\ncobalt\n.235\n\\(k\\)\n0.069941\n0.003279\n21.33\n\\(&lt;2e-16\\)\n34\n35\n\n\nzinc\n.24\n\\(k\\)\n0.150255\n0.009133\n16.45\n\\(&lt;2e-16\\)\n32\n33"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#background",
    "href": "docs/projects/heavy_metal/index.html#background",
    "title": "Project Description",
    "section": "",
    "text": "Heavy metals (such as Copper (Cu), Zinc (Zn), and Cobalt (Co)) are silent pollutants; their presence in water is not easily detected. These pollutants can enter our bodies in different ways, e.g. through drinking contaminated water or eating contaminated food. These pollutants have toxic effects on humans and other living organisms. Spent tea leaves can be used as adsorbent substrates for divalent Cu, Zn and Co ions found in polluted waste water.\nDifferential equations were used to model the rate at which tea leaves can be used to clear heavy metals from polluted water. The model was tested using time series data from experiments and non-linear least squares regression."
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#objective-1",
    "href": "docs/projects/heavy_metal/index.html#objective-1",
    "title": "Project Description",
    "section": "",
    "text": "the adsorption process of heavy metals to tea leaves can be mechanistically modeld using a differential equation.\nthe parameter of the mechanistic model can be estimated to fit the time series data."
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#methodology-1",
    "href": "docs/projects/heavy_metal/index.html#methodology-1",
    "title": "Project Description",
    "section": "",
    "text": "\\(S(t)\\) is the number of heavy metal molecules adsorbed to the tea leaves at time \\(t\\).\n\\(W(t)\\) is the number of heavy metal molecules in the water (and not adsorbed) at time \\(t\\).\n\\(W_0=W(0)=S(t)+W(t)\\)\n\\(S_e\\) is the number of heavy metal molecules adsorbed at the equilibrium state = the “relevant” number of adsorption sites. \\(S_e\\) is system dependent: e.g., a different initial concentration will have a different \\(S_e\\)\n\n\\(S_e=W_0 - W_e\\)\n\\(q(t)\\) is the fraction of heavy metal molecules adsorbed out of the waste water at time \\(t\\)\n\\(q(t)=\\frac{S(t)}{W_0}\\)\n\n\n\n\n\\[\n\\begin{align*}\n\\frac{dS}{dt} &=\\frac{W_0-S(t)}{V}fb\\frac{S_e-S(t)}{S_e}p\\\\\n&=k_s(W_0-S(t))(S_e-S(t)) \\text{ where } k_s=\\frac{fbp}{VS_e}\n\\end{align*}\n\\]\nwhere\n\n\\(\\frac{W_0-S(t)}{V}fb\\) is a fraction of heavy metal molecules per \\(mL\\) of water,\n\\(f\\) is \\(mL\\)’s of water that contact leaves per minute,\n\\(b\\) is the probability that a leaf hitting molecule hits a binding site,\n\\(\\frac{S_e-S(t)}{S_e}\\) is a fraction of biniding sites that are occupied, and\n\\(p\\) is the probability that hitting an unoccupied binding site results in binding.\n\nSince \\(q(t)=\\frac{S(t)}{W_0}\\),\n\\[\n\\begin{align*}\n\\frac{dq}{dt} &=\\frac{S'(t)}{W_0}\\\\\n&=\\frac{k_S}{W_0}(W_0-S(t))(S_e-S(t))\\\\\n&=W_0k_S\\frac{(W_0-S(t))(S_e-S(t))}{W_0^2}\\\\\n&=k_S(1-q(t))(q_e-q(t)) \\text{ where } k_q=W_0k_S=\\frac{W_0fbp}{VS_e}=\\frac{fbp}{Vq_e} \\\\\n&=\\frac{dq}{dt} \\\\\n&=k_q(1-q)(q_e-q) \\\\\nq(0) &=0\n\\end{align*}\n\\]\nSolving the above initial value problem: \\[\n\\begin{align*}\n\\frac{dq}{dt}&=k_q(1-q)(q_e-q) \\\\\n\\frac{dq}{(1-q)(q_e-q)}&=k_qdt \\\\\n\\int \\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\frac{1}{(1-q)(q_e-q)}&=\\frac{A}{1-q}+\\frac{B}{q_e-q} \\\\\n1&=A(q_e-q)+B(1-q)\\\\\n1&=A(q_e-1) \\text{ by letting } q=1\\\\\n(q_e-1)^{-1}&=A\\\\\n1&=B(1-q_e) \\text{ by letting } q=q_e\\\\\n(1-q_e)^{-1}&=B\\\\\n\\int\\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\int \\left \\{\\frac{(q_e-1)^{-1}}{1-q}+\\frac{(1-q_e)^{-1}}{q_e-q}\\right\\}dq&=\\int k_qdt \\\\\nq(t)&= \\frac{q_e-q_ee^{-kt}}{1-q_ee^{-kt}}=q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}} \\text{ where } k=(1-q_e)k_q\\\\\n\\arg\\min_k&\\sum_{i=1}^{n}(q(t_i,k)-\\hat{q}(t_i))^2\n\\end{align*}\n\\] where\n\n\\((t_i,\\hat{q}(t_i))\\) is the observed values,\n\\(q(t,k) =q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}}\\)"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#required-skills-1",
    "href": "docs/projects/heavy_metal/index.html#required-skills-1",
    "title": "Project Description",
    "section": "",
    "text": "Mathematics: differential equation\nStatistics: non-linear least square with the Levenberg-Marquardt algorithm\nBiology"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#colaborators-1",
    "href": "docs/projects/heavy_metal/index.html#colaborators-1",
    "title": "Project Description",
    "section": "",
    "text": "1 mathematics professor (advisor)\n1 biology professor (advisor)\n2 mathematicians\n1 biologist"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#result-1",
    "href": "docs/projects/heavy_metal/index.html#result-1",
    "title": "Project Description",
    "section": "",
    "text": "Metal\n\\(q_e\\)\nparameter\nEstimate\nStd. Error\n\\(t\\) value\n\\(Pr(&gt;|t|)\\)\ndf\nn\n\n\n\n\ncopper\n.46\n\\(k\\)\n0.01281219\n0.0008855\n20.47\n\\(&lt;2e-16\\)\n34\n35\n\n\ncobalt\n.235\n\\(k\\)\n0.069941\n0.003279\n21.33\n\\(&lt;2e-16\\)\n34\n35\n\n\nzinc\n.24\n\\(k\\)\n0.150255\n0.009133\n16.45\n\\(&lt;2e-16\\)\n32\n33"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "",
    "text": "Notice\nLast Update\nIntroduction\n\nDefinition of SW Validation\nSome Terminology\nRationale\nObjective of SW Validation\nWhat to validate\nMain Institutions\n\nQuality System Regulation\nVerification\nValidation\nBenefits and Difficulty in SW V&V\nSW Development as Part of System Design\n\nOverview\nDesign Reveiw\n\n\n\n\nValidation Pinciples\n\nOverview\nConditions\nPlanning\nAfter SW Change\nSW Lifecycle\n\nSW Lifecycle Tasks\n\nOverview\nQuality Planning\nConfiguration Management\nTask Requirements\nDesign Overview\n\nDesign Consideration\nDesign Specification\nDesign Activity and Task\n\n\n\n\n\nTesting Tasks\n\nOverview\nConsideration Before Testing Tasks\nCode Based Testing\nSolution to White Box Testing\nDevelopment Testing\nUser Site Testing\n\nOverview\nTesting\n\n\nMaintenance and SW Changes\nValidation of Quality System SW\n\nOverview\nFactors in Validation"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#table-of-contents",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#table-of-contents",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "",
    "text": "Notice\nLast Update\nIntroduction\n\nDefinition of SW Validation\nSome Terminology\nRationale\nObjective of SW Validation\nWhat to validate\nMain Institutions\n\nQuality System Regulation\nVerification\nValidation\nBenefits and Difficulty in SW V&V\nSW Development as Part of System Design\n\nOverview\nDesign Reveiw\n\n\n\n\nValidation Pinciples\n\nOverview\nConditions\nPlanning\nAfter SW Change\nSW Lifecycle\n\nSW Lifecycle Tasks\n\nOverview\nQuality Planning\nConfiguration Management\nTask Requirements\nDesign Overview\n\nDesign Consideration\nDesign Specification\nDesign Activity and Task\n\n\n\n\n\nTesting Tasks\n\nOverview\nConsideration Before Testing Tasks\nCode Based Testing\nSolution to White Box Testing\nDevelopment Testing\nUser Site Testing\n\nOverview\nTesting\n\n\nMaintenance and SW Changes\nValidation of Quality System SW\n\nOverview\nFactors in Validation"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#notice",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#notice",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Notice",
    "text": "Notice\n\nI am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto. (it seems that Quarto system has some issues on mermaid diagrams.)\nThe FDA validation guidance document is a bit difficult to understand because its explanations provide abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\nLast Update\n\n2022-12-28, Summary of Document"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#introduction",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#introduction",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Introduction",
    "text": "Introduction\n\nDefinition of Software Validation\nSoftware Validation is a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997.\n(See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\n\n\nSome Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology: requirements, specification, verification, and validation.\n\n\n\nRationale\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\nObjective of SW validation is to ensure\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\nWhat to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\nMain Institutions\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#quality-system-regulation",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#quality-system-regulation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Quality System Regulation",
    "text": "Quality System Regulation\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n\n\n\nflowchart TB\n    subgraph Quality_System_Regulation\n        direction LR\n        subgraph Requirement\n            direction TB\n            user_requirements\n        end\n        subgraph Specification\n           direction TB\n           document_user_requirements \n        end \n        subgraph Verification\n           direction TB\n           verify_spacified_requirements\n        end\n        subgraph Validation\n           direction TB\n           Confirmation_by_Examinations\n           Provision_of_objective_3evidences\n        end\n        Requirement--&gt; Specification --&gt; Verification --&gt; Validation                    \n    end\n    subgraph First_Detail\n        direction TB\n        subgraph User_Requirement\n            direction TB\n            any_need_for_customer---\n            any_need_for_system---\n            any_need_for_software\n        end\n            subgraph Document_User_Requirement\n            direction TB\n            define_means_for_requirements---\n          define_criteria_for_requirements\n        end         \n        subgraph Verify_Spacified_Requirement\n            direction TB\n            Objective_Evidence---&gt;|needs|Software_Testing\n        end\n        subgraph SW_Validation\n            direction TB\n            subgraph Confirmation_by_Examination\n            direction TB\n                subgraph Examination_List_of_SW_LifeCycle\n                    direction TB\n                    comprehensiveness_of_software_testing---\n                    inspection_verification_test---\n                    analysis_verification_test---\n                    other_varification_tests    \n                end \n            end             \n            subgraph Provision_of_Objective_3evidences\n                direction TB\n                Software_specifications_conformity---\n                Consistent_SW_Implementation---\n                Correctness_Completeness_Traceability\n            end\n        end\n        Requirement---User_Requirement\n        Specification---Document_User_Requirement\n        Verification---Verify_Spacified_Requirement\n        Confirmation_by_Examinations---Confirmation_by_Examination\n        Provision_of_objective_3evidences---Provision_of_Objective_3evidences             \n    end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#verification",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#verification",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Verification",
    "text": "Verification\n\n\n\n\n\nflowchart LR\n    subgraph Objective_Evidence\n        direction LR\n        subgraph Design_Outputs_of_SW_life_cycle_for_Specified_Requirements\n            direction TB\n            Consistency---\n            Completeness---\n            Correctness---\n            Documentation\n        end       \n        subgraph Software_Testing\n            direction LR\n            subgraph Testing_Environments\n                direction TB\n                satisfaction_for_input_requirements\n                satisfaction_for_input_requirements---Simulated_Use_Environment\n                subgraph User_Site_Testing\n                    direction TB                            \n                    Installation_Qualification---\n                    Operational_Qualification---\n                    Performance_Qualification\n                end\n            end\n            satisfaction_for_input_requirements---User_Site_Testing\n            subgraph Testing_Activities\n                direction TB\n                static_analyses---\n                dynamic_analyses---\n                code_and_document_inspections---\n                walkthroughs\n            end \n        Testing_Environments--&gt;Testing_Activities\n        end\n    Design_Outputs_of_SW_life_cycle_for_Specified_Requirements--&gt;Software_Testing--&gt;Testing_Activities\nend    \n\n\n\n\n\n\n\n\nInstallation_Qualification (IQ): documentation of correct installations according to requirements, specifications, vendor’s recommendations, and the FDA’s guidance for all hardware, software, equipment and systems.\nOperational_Qualification (OQ): establishment of confidence that the software shows constant performances according to specified requirements.\nPerformance_Qualification (PQ): confirmation of the performance in the intended use according to the specified requirements for functionality and safety throughout the SW life cycle."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation",
    "text": "Validation\n\n\n\n\n\nflowchart LR\n    subgraph Validation\n    direction LR\n        subgraph Confirmation_by_Examination\n            direction TB\n            subgraph Examination_List_at_each_stage_of_SW_Life_Cycle\n                direction TB\n                comprehensiveness_of_software_testing---\n                inspection_verification_test---\n                analysis_verification_test---\n                other_varification_tests    \n            end \n        end\n        subgraph Provision_of_objective_3evidences\n            direction TB\n            subgraph Software_specifications_conform_to\n                direction TB\n                user_needs \n                intended_uses\n            end\n            subgraph Consistent_SW_Implementation\n                direction TB\n                particular_requirements\n            end\n            subgraph Correctness_Completeness_Traceability\n                direction TB\n                correct_complete_implementation_by_all_SW_requirements---\n                traceable_to_system_requirements\n            end\n            Software_specifications_conform_to---\n            Consistent_SW_Implementation---\n            Correctness_Completeness_Traceability\n        end\n        Confirmation_by_Examination--&gt;\n        Provision_of_objective_3evidences\n    end\n\n\n\n\n\n\n\n\n\n\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#benefits-and-difficulty-of-sw-vv",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#benefits-and-difficulty-of-sw-vv",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Benefits and Difficulty of SW V&V",
    "text": "Benefits and Difficulty of SW V&V\n\nBenefits of SW V&V\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nReduce long term costs by making V&V easier and less costly to reliably modify software and revalidate software changes.\n\n\n\nDifficulty in SW V&V\n\na developer cannot test forever, and\n\nit is difficult to know how much evidence is enough.\na matter of developing a level of confidence that the device meets all requirements\n\nConsiderations for an acceptable level of confidence\nmeasures and estimates such as defects found in specifications documents\ntesting coverage, and other techniques are all used before shipping the product.\na level of confidence varies depending upon the safety risk (hazard) of a SW or device"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-development-as-part-of-system-design",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-development-as-part-of-system-design",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Development as Part of System Design",
    "text": "SW Development as Part of System Design\n\nOverview\n\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        purpose_design_review---\n        design_review_types---\n        design_review_requirements---\n        design_review_outputs\n    end\n\n\n\n\n\n\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nuser’s needs\nintended uses from which the product is developed.\n\nA primary goal of SW validation is to demonstrate that all completed SW products comply with all documented requirements.\n\n\nDesign Review\n\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        subgraph Purpose_Design_Review\n            direction TB\n            documented_structured_comprehensive_systematic_examinations---\n            adequacy_of_design_requirements---\n            capability_of_design_for_requirements---\n            identification_of_problem   \n        end\n        subgraph Design_Reivew_Types\n            direction TB\n            subgraph Formal_Design_Review\n                direction TB\n                3rd_parties_outside_development_team\n            end\n            subgraph Informal_Design_Review\n                direction TB\n                within_development_team\n            end\n        Formal_Design_Review---Informal_Design_Review    \n        end\n        subgraph Design_Review_Requirements\n            direction TB\n               necessary_at_least_one_formal_design_review---\n               optinal_informal_design_review---\n               recommended_multiple_design_reviews\n        end\n        subgraph Formal_Design_Review_Outputs\n            direction TB\n            more_than_10_outputs\n        end\n        Purpose_Design_Review--&gt; Design_Reivew_Types--&gt; Design_Review_Requirements\n        Design_Review_Requirements--&gt;Formal_Design_Review_Outputs\n    end\n\n\n\n\n\n\n\n\nDesign review is a primary tool for managing and evaluating development projects.\nAt least one formal design review must be conducted during the device design process.\nIt is recommended that multiple design reviews be conducted.\nProblems found at this point can\n\nbe resolved more easily,\nsave time and money, and\nreduce the likelihood of missing a critical issue."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-principles",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-principles",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation Principles",
    "text": "Validation Principles\n\nOverview\n\n\n\n\n\nflowchart LR\n  subgraph Validation_Principles\n        direction LR\n        subgraph Validation_Starting_Point\n            direction TB\n            during_design_planning---\n            during_development_planning---\n            all_results_should_be_supported_by_evidence_collected_from_planning_SW_lifecylce\n        end\n        subgraph Validation_Conditions\n            direction TB\n            Requirements---Estabilishment_Confidence---SW_Lifecycle\n        end\n\n        subgraph Validation_Planning\n            direction TB\n            Specify_Areas\n            subgraph Validation_Coverage\n                direction TB\n            end\n            subgraph Validation_Process_Establishment\n                direction TB\n            end\n        Specify_Areas---Validation_Coverage---Validation_Process_Establishment\n        end\n\n        subgraph After_Self_Validation\n            direction TB\n            subgraph Validation_After_SW_Change\n        direction TB\n        end\n\n        subgraph Independence_of_Review\n        direction TB\n\n        end\n        Validation_After_SW_Change---Independence_of_Review\n        end\n            Validation_Starting_Point--&gt;Validation_Conditions--&gt;Validation_Planning--&gt;\nAfter_Self_Validation\n    end\n\n\n\n\n\n\n\nPreparation for software validation should begin as early as possible because the final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n\nConditions\n\n\n\n\n\nflowchart LR\n\nsubgraph Validation_Conditions\n    direction LR\n    subgraph SW_Requirments\n        direction TB\n        subgraph Documented_SW_Requirments_Specification\n            direction TB\n            Baseline_Provision_for_V&V---\n            establishment_of_software_requirements_specification\n        end\n    end\n    subgraph Estabilishment_Confidence\n        direction TB\n            mixture_of_methods_techinques---\n            preventing_SW_errors---\n            detecting_SW_errors                 \n    end\n    subgraph SW_Lifecycle\n        direction TB\n        validation_must_be_conducted_within_established_environment_across_lifecycle---\n        lifecycle_contains_SW_engineering_tasks_and_documentation---\n        V&V_tasks_must_reflect_intended_use\n    end\nend\nSW_Requirments---Estabilishment_Confidence---SW_Lifecycle\n\n\n\n\n\n\n\n\nPlanning\n\n\n\n\n\nflowchart LR\n    subgraph Validation_Planning\n        direction LR\n        define_what_to_accomplish\n        subgraph Specify_Areas\n            direction TB\n            scope---\n            approach---\n            resources---\n            schedules_activities---\n            types_activitieis---\n            extent_of_activities---\n            tasks---\n            work_items\n        end\n            define_what_to_accomplish--&gt;Specify_Areas\n        subgraph Validation_Coverage\n               direction TB\n            depending_on_SW_complexity_of_SW_design---\n            depending_on_safety_risk_for_specified_intended_use---\n            select_activities_tasks_work_items_for_complexity_safety_risk\n        end\n        subgraph Validation_Process_Establishment\n            direction TB\n            establish_how_to_conduct--&gt;\n            identify_sequence_of_specific_actions--&gt;\n            identify_specific_activitieis--&gt;\n            identify_specific_tasks--&gt;\n            identify_specific_work_items\n        end\n    Specify_Areas--&gt;Validation_Coverage--&gt;Validation_Process_Establishment\n    end\n\n\n\n\n\n\n\n\nAfter SW Change\n\n\n\n\n\nflowchart LR\n\nsubgraph After_Self_Validation\n    direction LR\n    subgraph Validation_After_SW_Change\n        direction TB\n        determine_extent_of_change_on_entire_SW_system--&gt;\n        determine_impact_of_change_on_entire_SW_system--&gt;\n        conduct_SW_regression_testing_on_unchanged_but_vulnerable_modules\n    end\n    subgraph Independence_of_Review\n        direction TB\n        follow_basic_quality_assurance_precept_of_independence_of_review---\n        avoid_self_validation---\n        should_conduct_contracted_3rd_party_independent_V&V---\n        or_conduct_blind_test_with_internal_staff\n    end\n    Validation_After_SW_Change---Independence_of_Review\nend\n    \n\n\n\n\n\n\n\n\nSW Lifecycle\n\n\n\n\n\nflowchart LR\nsubgraph SW_Lifecycle\n    direction TB\n    validation_must_be_conducted_within_the_established_environment_across_lifecycle---\n    lifecycle_contains_SW_engineering_tasks_and_documentation---\n    V&V_tasks_must_reflect_intended_use\nend\n\nsubgraph SW_Lifecycle_Activities\n    direction TB\n    subgraph should_establish_lifecycle_model\n        direction TB\n        subgraph SW_Lifecycle_Model_List_Defined_in_FDA\n            direction TB\n            waterfall---\n            spiral---\n            rapid_prototyping---\n            incremental_development---\n            etc\n        end     \n    end\n    subgraph should_cover_SW_birth_to_retirement\n        direction TB\n        subgraph Lifecycle_Activities\n            direction TB\n            Quality_Plan--&gt;\n            System_Requirements_Definition--&gt;\n            Detailed_Software_Requirements_Specification--&gt;\n            Software_Design_Specification--&gt;\n            Construction_or_Coding--&gt;\n            Testing--&gt;\n            Installation--&gt;\n            Operation_and_Support--&gt;\n            Maintenance--&gt;\n            Retirement\n        end\n    end\n    should_establish_lifecycle_model--&gt;should_cover_SW_birth_to_retirement\n    should_cover_SW_birth_to_retirement--&gt;Lifecycle_Activities\nend\nsubgraph SW_Lifecycle_Tasks\n    direction TB\n    should_define_and_document_risk_related_tasks---\n    should_define_and_document_which_tasks_are_appropriate_in_vice_versa---\n    Quality_Planning---\n    Quality_Planning_Tasks---\n    Inclusion_Task_List_for_Plan---\n    Identification_Task_List_for_Plan---\n    Configuration_Management---\n    Control---\n    Management---\n    Procedures---\n    ensure_proper_communications_and_documentation---\n    Task_Requirements\nend\nSW_Lifecycle--&gt;SW_Lifecycle_Activities--&gt;SW_Lifecycle_Tasks"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-lifecycle-tasks",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-lifecycle-tasks",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Lifecycle Tasks",
    "text": "SW Lifecycle Tasks\n\nOverview\n\n\n\n\n\n \nflowchart TB\n\nsubgraph SW_Lifecycle_Tasks\n    direction LR\n    subgraph Define_and_Document_List\n        direction TB\n        risk_related_tasks---\n        whether_or_not_tasks_are_appropriate\n    end\n    \n    subgraph Quality_Planning\n        direction TB\n        subgraph Quality_Planning_Tasks\n            direction TB\n        \n        end\n        subgraph Inclusion_List_for_Plan\n            direction TB\n            \n        end\n        subgraph Identification_List_for_Plan\n            direction TB\n            \n        end\n    Quality_Planning_Tasks--&gt;Inclusion_List_for_Plan--&gt;Identification_List_for_Plan\n    end\n    \n    subgraph Configuration_Management\n        direction TB\n        subgraph Control\n            direction TB\n            \n        end\n        subgraph Management\n            direction TB\n        end\n        subgraph Procedures\n            direction TB\n        end\n        ensure_proper_communications_and_documentation\n        Control--&gt;Management--&gt;Procedures--&gt;ensure_proper_communications_and_documentation \n    end\n    subgraph Task_Requirements\n        direction TB\n        identification---\n        analysis---\n        predetermined_documentation_about_device_its_intended_use---\n        Requirements_Specification_List---\n        Verfification_List_by_Evaluation---\n        Requirements_Tasks    \n    end\nDefine_and_Document_List--&gt;Quality_Planning--&gt;Configuration_Management--&gt;Task_Requirements\nend     \n\n\n\n\n\n\n\n\nQuality Planning\n\n\n\n\n\nflowchart TB\nsubgraph Quality_Planning\n    direction LR\n    subgraph Quality_Planning_Tasks\n        direction TB\n        Risk_Hazard_Management_Plan---\n        Configuration_Management_Plan---\n        Software_Quality_Assurance_Plan---\n        Software_Verification_and_Validation_Plan---\n        Verification_and_Validation_Tasks---\n        Acceptance_Criteria---\n        Schedule_and_Resource_Allocation_for_V&V_activities---\n        Reporting_Requirements---\n        Formal_Design_Review_Requirements---\n        Other_Technical_Review_Requirements---\n        Problem_Reporting_and_Resolution_Procedures---\n        Other_Support_Activities\n    end\n    subgraph Inclusion_List_for_Plan\n        direction TB\n        specific_tasks_for_each_life_cycle_activity---\n        Enumeration_of_important_quality_factors--- \n        like_reliability_maintainability_usability---\n        Methods_and_procedures_for_each_task---\n        Task_acceptance_criteria---\n        Criteria_for_defining_and_documenting_outputs_for_input_requirements---\n        Inputs_for_each_task---\n        Outputs_from_each_task---\n        Roles_resources_and_responsibilities_for_each_task---\n        Risks_and_assumptions---\n        Documentation_of_user_needs    \n    end\n    subgraph Identification_List_for_Plan\n        direction TB\n        personnel---\n        facility_and_equipment_resources_for_each_task---\n        role_that_risk_hazard_management        \n    end\nQuality_Planning_Tasks--&gt;Inclusion_List_for_Plan--&gt;Identification_List_for_Plan\nend\n\n\n\n\n\n\n\n\nConfiguration Management\n\n\n\n\n\nflowchart LR\nsubgraph Configuration_Management\n    direction LR\n    subgraph Control\n        direction TB\n        control_multiple_parallel_development_activities---\n        ensure_positive_and_correct_correspondence_of---\n        specifications_documents---\n        source_code---\n        object_code---\n        test_suites---\n        ensure_accurate_identification_of_approved_versions---\n        ensure_access_to_approved_versions---\n        create_procedures_for_reporting---\n        create_procedures_for_resolving_SW_anomalies                            \n    end\n    subgraph Management\n        direction TB\n        identify_reports---\n        specify_contents---\n        specify_format---\n        specify_responsible_organizational_elements_for_each_report\n    end\n    subgraph Procedures\n        direction TB\n        necessary_for_review_of_SW_development_results---\n        necessary_for_approval_of_SW_development_results\n    end\n    ensure_proper_communications_and_documentation\n    Control--&gt;Management--&gt;Procedures--&gt;ensure_proper_communications_and_documentation \nend\n\n\n\n\n\n\n\n\nTask Requirements\n\n\n\n\n\nflowchart TB\n    subgraph Task_Requirements\n        direction LR\n        subgraph group\n            direction TB\n            identification---\n            analysis---\n            predetermined_documentation_about_device_its_intended_use\n        end\n        \n        subgraph Requirements_Specification_List\n            direction TB\n            All_software_system_inputs---\n            All_software_system_outputs---\n            All_functions_that_software_system_will_perform---\n            All_performance_requirements_that_software_will_meet---\n            requirement_example_data_throughput_reliability_timing---\n            definition_of_all_external_and_user_interfaces---\n            any_internal_software_to_system_interfaces---\n            How_users_will_interact_with_system---\n            What_constitutes_error---\n            how_errors_should_be_handled---\n            Required_response_times---\n            Intended_operating_environment_for_software---\n            All_acceptable_ranges_limits_defaults_specific_values---\n            All_safety_related_requirements_that_will_be_implemented_in_SW---\n            All_safety_related_specifications_that_will_be_implemented_in_SW---\n            All_safety_related_features_that_will_be_implemented_in_SW---\n            All_safety_related_functions_that_will_be_implemented_in_SW---\n            clearly_identify_potential_hazards---\n            risk_evaluation_for_accuracy---\n            risk_evaluation_for_completeness---\n            risk_evaluation_for_consistency---\n            risk_evaluation_for_testability---\n            risk_evaluation_for_correctness---\n            risk_evaluation_for_clarity\n        end\n        subgraph Verfification_List_by_Evaluation\n            direction TB\n            no_internal_inconsistencies_among_requirements---\n            All_of_performance_requirements_for_system---\n            Complete_correct_Fault_tolerance_safety_security_requirements---\n            Accurate_Complete_Allocation_of_software_functions---\n            Appropriate_Software_requirements_for_system_hazards---\n            mesurable_requirements---\n            objectively_verifiable_requirements---\n            traceable_requirements\n        end\n        subgraph Requirements_Tasks\n            direction TB\n            Preliminary_Risk_Analysis---\n            Traceability_Analysis---\n            ex_Software_Requirements_to_System_Requirements_vice_versa---\n            ex_Software_Requirements_to_Risk_Analysis---\n            Description_of_User_Characteristics---\n            Listing_of_Characteristics_and_Limitations_of_Memory---\n            Software_Requirements_Evaluation---\n            Software_User_Interface_Requirements_Analysis---\n            System_Test_Plan_Generation---\n            Acceptance_Test_Plan_Generation---\n            Ambiguity_Review_or_Analysis\n        end\n    group--&gt;Requirements_Specification_List \n    Verfification_List_by_Evaluation--&gt;Requirements_Tasks\n    end\n\n\n\n\n\n\n\n\nDesign Overview\n\n\n\n\n\nflowchart TB\n    subgraph Deign_Task\n        direction LR\n    subgraph Design_Consideration_List\n        direction TB\n        subgraph Description\n                    direction TB\n                end\n        subgraph Human_Factors_Engineering\n          direction TB\n    \n        end\n        subgraph Safety_Usability_Issues_Conisderation\n            direction TB\n\n            end\n        Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n    subgraph Design_Specificiation\n        direction TB\n        subgraph Performing_List\n            direction TB\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n        end\n    Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design \n    end\n    subgraph Design_Activity_and_Task_List\n        direction TB\n        subgraph Final_Design_activity\n            direction TB\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n            end\n            subgraph Coding_Tasks\n                direction TB\n            end\n        traceability_analysis--&gt;Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end\n    Design_Consideration_List---Design_Specificiation---Design_Activity_and_Task_List\n\n    end\n\n\n\n\n\n\n\nDesign Consideration\n\n\n\n\n\nflowchart TB\nsubgraph Design_Consideration_List\n    direction LR\n        subgraph Requirement_Specification\n            direction TB\n            logical_representation---\n            physical_representation\n        end\n    subgraph Description\n            direction TB\n            what_to_do---\n            how_to_do                   \n        end\n    subgraph Human_Factors_Engineering\n      direction TB\n            entire_design_and_development_process---\n            device_design_requirements---\n            analyses---\n            tests\n    end\n    subgraph Safety_Usability_Issues_Conisderation\n        direction TB\n                flowcharts--- \n                state_diagrams--- \n                prototyping_tools---\n                test_plans\n        end\n        Requirement_Specification---Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n\n\n\n\n\n\n\n\nDesign Specification\n\n\n\n\n\nflowchart TB\nsubgraph Design_Specificiation\n        direction LR\n        subgraph Conceptual_Specification\n            direction TB\n            requirements_specification---\n            predetermined_criteria---\n            Software_risk_analysis---\n            Development_procedures---\n            coding_guidelines\n        end\n        subgraph Performing_List\n            direction TB\n            task---\n            function_analyses---\n            risk_analyses---\n            prototype_tests_and_reviews---\n            full_usability_tests\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n            SW_requirements_specification---\n            predetermined_criteria_for_SW_acceptance---\n            SW_risk_analysis---\n            Development_procedure_list---\n            coding_guidance---\n            Systems_documentation---\n            Hardware_to_be_used---\n            Parameters_to_be_measured---\n            Logical_structure---\n            Control_logic---\n            logical_processing_steps_aka_algorithms---\n            Data_structures_diagram---\n            data_flow_diagrams---\n            Definitions_of_variables---\n            description_of_where_they_are_used---\n            Error_alarm_and_warning_messages---\n            Supporting_software---\n            internal_modules_Communication_links---\n            supporting_sw_links---\n            link_with_hardware---\n            link_with_user---\n            physical_Security_measures---\n            logical_security_measures\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n            complete--- \n            correct---\n            consistent--- \n            unambiguous--- \n            feasible---\n            maintainable---\n            analyses_of_control_flow---\n            data_flow--- \n            complexity--- \n            timing--- \n            sizing--- \n            memory_allocation---\n            module_architecture---\n            traceability_analysis_of_modules--- \n            criticality_analysis\n        end\n    Conceptual_Specification---Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design  \n    end\n\n\n\n\n\n\n\n\nDesign Activity and Task\n\n\n\n\n\nflowchart TB\nsubgraph Design_Activity_and_Task_List\n        direction LR\n        subgraph Final_Design_activity\n            direction TB\n            Formal_Design_Review_Before_Design_Implementation---\n            correct_consistent_complete_accurate_testable\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n            Updated_Software_Risk_Analysis---\n            Traceability_Analysis---\n            Software_Design_Evaluation---\n            Design_Communication_Link_Analysis---\n            Module_Test_Plan_Generation---\n            Integration_Test_Plan_Generation---\n            module_Test_Design_Generation---\n            integration_Test_Design_Generation---\n            system_Test_Design_Generation---\n            acceptance_Test_Design_Generation   \n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n                each_element_implementation---\n                each_module_implementation_to_element_and_risk_analysis---\n                each_functions_implemented_to_element_and_risk_analysis---\n                Tests_for_modules_to_element_and_risk_analysis--- \n                Tests_for_functions_to_element_and_risk_analysis---\n                Tests_for_modules_to_source_code---\n                Tests_for_functions_to_source_code\n            end\n            subgraph Coding_Tasks\n                direction TB\n                Traceability_Analyses---\n                Source_Code_to_Design_Specification_and_vice_versa---\n                Test_Cases_to_Source_Code_and_to_Design_Specification---\n                Source_Code_and_Source_Code_Documentation_Evaluation---\n                Source_Code_Interface_Analysis---\n                Test_Procedure_and_Test_Case_Generation \n            end\n        traceability_analysis--&gt;Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#testing-task",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#testing-task",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Testing Task",
    "text": "Testing Task\n\nOverview\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction TB\n            subgraph Test_Plans\n                direction TB\n            end\n            subgraph Conditions\n                direction TB\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n        subgraph Code_Based_Testing\n            direction TB\n            subgraph white_box_testing\n                direction TB\n            end\n            subgraph Evaluation_of_level_of_white_box_testing\n                direction TB\n            end\n            subgraph Coverage_Metrics_of_White_Box_Testing\n                direction TB\n            end\n        white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n        end\n        subgraph Alternatives_to_White_Box_Testing\n            direction TB\n            subgraph Types_of_Functional_Software_Testing_Increasing_Cost\n                direction TB\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n            end\n            subgraph Change_in_SW\n                direction TB    \n            end\n        Types_of_Functional_Software_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW\n        end\n        \n\n        subgraph Development_Testing\n            direction TB\n            subgraph unit_level_testing\n                direction TB    \n            end\n            subgraph integration_level_testing\n                direction TB\n            end\n            subgraph system_level_testing\n                direction TB\n            end\n            subgraph Error_Detected\n                direction TB        \n            end\n        unit_level_testing--&gt;integration_level_testing--&gt;system_level_testing--&gt;Error_Detected\n        end\n\n        subgraph Testing_Tasks\n            direction TB\n        end\n        subgraph User_Site_Testing\n            direction TB\n            subgraph Quality_System_Rregulation\n                direction TB\n            end\n            subgraph Understand_Terminology\n                direction TB\n            end\n            subgraph Testing\n                direction TB\n            end\n            Quality_System_Rregulation---Understand_Terminology---Testing\n        end\nConsideration_Before_Testing_Tasks---Code_Based_Testing---Alternatives_to_White_Box_Testing\nDevelopment_Testing---Testing_Tasks---User_Site_Testing\n    end\n\n\n\n\n\n\n\n\nConsideration Before Testing Tasks\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction LR\n            subgraph Test_Plans\n                direction TB\n                should_identify_control_measures_like_traceability_analysis---\n                ensure_that_intended_coverage_is_achieved---\n                ensure_that_proper_documentation_is_prepared---\n                conduct_tests_not_by_SW_developers_but_in_other_sites\n            end\n            subgraph Conditions\n                direction TB\n                use_defined_inputs---\n                documented_outcomes---\n                gonnabe_time_consuming_activity---\n                gonnabe_difficult_activity---\n                gonnabe_imperfect_activity---\n                testing_all_program_functionality---\n                does_not_mean_100_prcnt_correction_perfection---\n                make_detailed_objective_evaluation---\n                requires_sophisticated_definition_specificiation---\n                all_test_procedures_data_results_are_documented---\n                all_test_procedures_data_results_are_suitable_for_review---\n                all_test_procedures_data_results_are_suitable_for_objective_decision_making---\n                all_test_procedures_data_results_are_suitable_for_subsequent_regression_testing\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n                make_test_plans---\n                make_test_cases---\n                plan_schedules---\n                plan_environments---\n                plan_resources_of_personnel_tools---\n                plan_methodologies---\n                plan_inputs_procedures_outputs_expected_results---\n                plan_documentation---\n                plan_reporting_criteria\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n                expected_test_outcome_is_predefined---\n                good_test_case_has_high_probability_of_exposing_errors---\n                successful_test_is_one_that_finds_errors---\n                There_is_independence_from_coding---\n                Both_application_for_user_and_SW_for_programming_expertise_are_employed---\n                Testers_use_different_tools_from_coders---\n                Examining_only_the_usual_case_is_insufficient---\n                Test_documentation_permits_its_reuse---\n                Test_documentation_permits_independent_confirmation_---\n                of_pass/fail_test_outcome_during_subsequent_review\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n\nend\n\n\n\n\n\n\n\n\nCode Based Testing\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n            subgraph Code_Based_Testing\n                direction LR\n                subgraph white_box_testing\n                    direction TB\n                    identify_dead_code_never_executed---\n                    conduct_unit_test---\n                    conduct_other_level_tests\n                end\n                subgraph Evaluation_of_level_of_white_box_testing\n                    direction TB\n                    use_coverage_metrics---\n                    metrics_of_completeness_of_test_selection_criteria---\n                    coverage_should_be_commensurate_with_level_of_SW_risk---\n                    coverage_means_100_prcnt_coverage\n                end\n                subgraph Coverage_Metrics_of_White_Box_Testing\n                    direction TB\n                    Statement_Coverage---\n                    Decision_or_Branch_Coverage---\n                    Condition_Coverage---\n                    Multi_Condition_Coverage\n                    Loop_Coverage---\n                    Path_Coverage---\n                    Data_Flow_Coverage\n                end\n            white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n            end\nend\n\n\n\n\n\n\n\n\nSolution to White Box Testing\n\n\n\n\n\n \nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Alternatives_to_White_Box_Testing\n            direction LR\n            subgraph Types_of_Testing_Increasing_Cost\n                direction TB\n                    Normal_Case---\n                    Output_Forcing---\n                    Robustness---\n                    Combinations_of_Inputs\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n                difficulty_in_linking_---\n                tests_completion_criteria_to_SW_reliability\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n                statistical_testing---\n                provide_further_assurance_of_reliability---\n                generate_randomly_test_data_from_defined_distributions---\n                distribution_defined_by_expected_use---\n                distribution_defined_by_hazardous_use---\n                distribution_defined_by_malicious_use---\n                large_test_data_cover_particular_areas_or_concerns---\n                statistical_testing_provides_high_structural_coverage---\n                statistical_testing_requires_stable_system---\n                structural_and_functional_testing_are_prerequisites_for_statistical_testing\n            end\n            subgraph Change_in_SW\n                direction TB\n                conduct_regression_analysis_and_testing---\n                should_demonstrate_correct_implementation---\n                should_demonstrate_no_adverse_impact_on_other_modules   \n            end\n            subgraph Testing_Tasks\n                direction TB\n                Test_Planning---\n                Structural_Test_Case_Identification---\n                Functional_Test_Case_Identification---\n                Traceability_Analysis_Testing---\n                Unit_Tests_to_Detailed_Design---\n                Integration_Tests_to_High_Level_Design---\n                System_Tests_to_Software_Requirements---\n                Unit_Test_Execution---\n                Integration_Test_Execution---\n                Functional_Test_Execution---\n                System_Test_Execution---\n                Acceptance_Test_Execution---\n                Test_Results_Evaluation---\n                Error_Evaluation_Resolution---\n                Final_Test_Report\n            end\n        Types_of_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW---Testing_Tasks\n        end\nend\n\n\n\n\n\n\n\n\nDevelopment Testing\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Development_Testing\n            direction LR\n            subgraph unit_level_testing\n                direction TB    \n                focus_on_early_examination_of_sub_program_functionality---\n                ensure_functionality_invisible_at_system_level_examined---\n                ensure_quality_software_units_furnished_for_integration\n            end\n            subgraph integration_level_testing\n                direction TB\n                focuses_on_transfer_of_data---\n                focuses_on_control_across_program's_internal_and_external_interfaces\n            end\n            subgraph system_level_testing\n                direction TB\n                demonstrate_all_specified_functionality_exists---\n                demonstrate_SW_is_trustworthy---\n                verifies_as_built_program's_functionality_and_performance_on_requirements---\n                addresses_functional_concerns_and_intended_uses---\n                like_Performance_issues---\n                like_Responses_to_stress_conditions---\n                like_Operation_of_internal_and_external_security_features---\n                like_Effectiveness_of_recovery_procedures---\n                like_disaster_recovery---\n                like_Usability---\n                like_Compatibility_with_other_SW---\n                like_Behavior_in_each_of_the_defined_hardware_configurations---\n                like_Accuracy_of_documentation\n            end\n            subgraph Error_Detected\n                direction TB        \n                should_be_logged---\n                should_be_classified---\n                should_be_reviewed---\n                should_be_resolved_before_SW_release\n            end\n        unit_level_testing--&gt;integration_level_testing--&gt;system_level_testing--&gt;Error_Detected\n        end\nend\n\n\n\n\n\n\n\n\nUser Site Testing\n\nOverview\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph User_Site_Testing\n            direction LR\n            subgraph Quality_System_Rregulation\n                direction TB\n                installation---\n                inspection_procedures---\n                testing_appropriateness---\n                documentation_of_inspection---\n                testing_to_demonstrate_proper_installation\n            end\n            subgraph Understand_Terminology\n                direction TB\n                beta_test---\n                site_validation---\n                user_acceptance_test---\n                installation_verification---\n                installation_testing\n            end\n            subgraph Testing\n                direction TB\n                subgraph Requirements\n                    direction TB\n                    either_actual_or_simulated_use---\n                    verification_of_intended_functionality---\n                    constant_contact_FDA_center\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n    \n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_System_Ability\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_User_Ability\n                        direction TB\n        \n                    end \n                    subgraph Evaluation_of_Operator_Ability\n                        direction TB\n        \n                    end\n                constant_contact_FDA_center--&gt;Follow_Predefiened_Plan--&gt;Documented_Evidence--&gt;\n            Evaluation_of_System_Ability--&gt;Evaluation_of_User_Ability--&gt;Evaluation_of_Operator_Ability    \n                end \n                        \n            \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements--&gt;User_Site_Testing_Task\n        end\n        Quality_System_Rregulation--&gt;    Understand_Terminology--&gt;Testing--&gt;User_Site_Testing_Task\n        end\nend\n\n\n\n\n\n\n\n\nTesting\n\n\n\n\n\nflowchart TB\n            subgraph Testing\n                direction LR\n                subgraph Requirements\n                    direction LR\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n                        either_actual_or_simulated_use---\n                        verification_of_intended_functionality---\n                        constant_contact_FDA_center---\n                        formal_summary_of_testing---\n                        record_of_formal_acceptance\n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n                        testing_plan_of_full_range_of_operating_conditions---\n                        testing_plan_to_detect_any_latent_faults---\n                        all_testing_procedures---\n                        test_input_data---\n                        test_results---\n                        hardware_installation_and_configuration---\n                        software_installation_and_configuration---\n                        exercising_measure_of_all_system_components---\n                        versions_of_all_system_components           \n                    end\n                    subgraph Evaluation\n                        direction TB\n                      subgraph Evaluation_of_System_Ability\n                            direction TB\n                            high_volume_of_data---\n                            heavy_loads_or_stresses---\n                            security\n                            subgraph fault_testing\n                                direction TB\n                                avoidance---\n                                detection---\n                                tolerance---\n                                recovery\n                            end\n                        security---fault_testing---\n                        error_message---\n                        implementation_of_safety_requirements\n                        end\n                      subgraph Evaluation_of_User_Ability\n                            direction TB\n                            ability_to_understand_system---\n                            ability_to_interface_with_system\n                        end \n                        subgraph Evaluation_of_Operator_Ability\n                            direction TB\n                            ability_to_perform_intended_functions---\n                            ability_to_respond_in_alarms---\n                            ability_to_respond_in_warnings---\n                            ability_to_respond_in_error_messages\n                        end\n\n                    end\n            Follow_Predefiened_Plan--&gt;Documented_Evidence--&gt;\n            Evaluation_of_System_Ability--&gt;Evaluation_of_User_Ability--&gt;Evaluation_of_Operator_Ability    \n            end     \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements--&gt;User_Site_Testing_Task\n        end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#maintenance-and-software-changes",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#maintenance-and-software-changes",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Maintenance and Software Changes",
    "text": "Maintenance and Software Changes\n\n\n\n\n\nflowchart LR\n    subgraph Hardware_VS_Software\n        direction LR\n        subgraph HW_maintenance_Inclusion\n            direction TB\n            preventive_hardware_maintenance_actions--- \n            component_replacement---\n            corrective_changes\n        end\n        subgraph SW_maintenance_Inclusion\n            direction TB\n            corrective---\n            perfective---\n            adaptive_maintenance---\n            not_include_preventive_maintenance_actions---\n            not_include_software_component_replacement\n        end\n    end\n    subgraph Maintenance_Type\n        direction TB\n        Corrective_maintenance---\n        Perfective_maintenance---\n        Adaptive_maintenance---\n        Sufficient_regression_analysis---\n        Sufficient_regression_testing\n    end\n    subgraph Factors_of_Validation_for_SW_change\n        direction TB\n        type_of_change---\n        development_products_affected---\n        impact_of_those_products_on_operation\n    end\n    subgraph Factors_of_Limitting_Validation_Effort\n        direction TB\n        documentation_of_design_structure---\n        documentation_of_interrelationships_of_modules---\n        documentation_of_interrelationships_of_interfaces---\n        test_documentation---\n    test_cases---\n        results_of_previous_verification_and_validation_testing\n    end\n    subgraph Maintenance_tasks\n        direction TB\n        Software_Validation_Plan_Revision---\n        Anomaly_Evaluation---\n        Problem_Identification_and_Resolution_Tracking---\n        Proposed_Change_Assessment---\n        Task_Iteration---\n        Documentation_Updating\n    end\nHardware_VS_Software--&gt;Maintenance_Type--&gt;Factors_of_Validation_for_SW_change--&gt;\nFactors_of_Limitting_Validation_Effort--&gt;Maintenance_tasks"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-of-quality-system-software",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-of-quality-system-software",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation of Quality System Software",
    "text": "Validation of Quality System Software\n\nOverview\n\n\n\n\n\nflowchart LR\n    subgraph Use_of_Computers_and_automated_equipment\n        direction TB\n        medical_device_design---\n        laboratory_testing_and_analysis---\n        product_inspection_and_acceptance---\n        production_and_process_control---\n        environmental_controls---\n        packaging---\n        labeling---\n        traceability---\n        document_control---\n        complaint_management---\n        programmable_logic_controllers---\n        digital_function_controllers---\n        statistical_process_control---\n        supervisory_control_and_data_acquisition---\n        robotics---\n        human_machine_interfaces---\n        input_output_devices---\n        computer_operating_systems\n    end\n    subgraph Factors_in_Validation\n        direction TB\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System--&gt;Validation_Supporting_Factors--&gt;Factors_of_Validation_Evidence_Level--&gt;\nFactors_of_Easing_Validation_Effort\n    end\n    subgraph Documented_User_Requirements\n        direction TB\n        intended_use_of_software_or_automated_equipment---\n      level_of_dependency_on_software_or_equipment\n    end\n    subgraph List_That_Must_Be_Defined_by_User\n        direction TB\n        \n    end\n    subgraph Documentation_List\n        direction TB\n        documented_protocol---\n        documented_validation_results\n        subgraph Documented_Test_Cases\n            direction TB\n        \n        end\n        documented_validation_results---Documented_Test_Cases\n    end\n\n    subgraph Manufaturer's_Responsbility\n        direction TB\n        \n    end\nUse_of_Computers_and_automated_equipment---Factors_in_Validation---Documented_User_Requirements---\nList_That_Must_Be_Defined_by_User---Documentation_List---Manufaturer's_Responsbility\n\n\n\n\n\n\n\n\nFactors in Validation\n\n\n\n\n\nflowchart LR\n    subgraph Factors_in_Validation\n        direction LR\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n                electronic_records_regulation---\n                electronic_signatures_regulation---\n                regulations_establishment---\n                security---\n                data_integrity---\n                validation_requirements \n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n            verifications_of_outputs_from_each_stage--- \n            verifications_of_outputs_throught_SW_life_cycle---\n            checking_for_proper_operation_in_intended_use_environment\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n            risk_posed_by_automated_operation---\n            complexity_of_process_software---\n            degree_of_dependence_on_automated_process\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System--&gt;Validation_Supporting_Factors--&gt;Factors_of_Validation_Evidence_Level--&gt;\nFactors_of_Easing_Validation_Effort\n    end"
  },
  {
    "objectID": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html",
    "href": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html",
    "title": "Categorical Data Analysis",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nVisualization Methods\n\nfor EDA: visualize patterns, trends, anomalies in data\nfor model diagnostic methods: visualize to assess violations of assumptions\nfor summary methods: visualize to provide an interpretable summary of data\n\napply theory to practice\n\nconert research questions into statistical hypotheses and models\nlook into the difference between non-parametric (ex. fisher exact test) vs parametric (ex. \\(\\chi^2 test for independence\\)) vs model-based methods (ex. logistic regression)\nfor summary methods: visualize to provide an interpretable summary of data\n\n\n\n\n\n\ncategorical (or frequency) data consist of a discrete set of categories, which may be ordered or unordered.\n\nunordered\n\ngener: {male, female, transgender}\nmarital status: {never married, married, separated, divorced, widowed}\nparty preference: {NDP, liberal, conservative, green}\ntreatment improvement: {none, some, marked}\n\nordered\n\nage group: {0s,10s,20s,30s, …}\nnumber of children: {0, 1 , 2 ,3, …} ## Structures\n\n\n\nCategorical data appears in various forms like:\n\ntables\n\none way\ntwo way\nthree way\n\nmatrices\narray\ndata frames\n\ncase form\nfrequency form"
  },
  {
    "objectID": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html#goal",
    "href": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html#goal",
    "title": "Categorical Data Analysis",
    "section": "",
    "text": "Visualization Methods\n\nfor EDA: visualize patterns, trends, anomalies in data\nfor model diagnostic methods: visualize to assess violations of assumptions\nfor summary methods: visualize to provide an interpretable summary of data\n\napply theory to practice\n\nconert research questions into statistical hypotheses and models\nlook into the difference between non-parametric (ex. fisher exact test) vs parametric (ex. \\(\\chi^2 test for independence\\)) vs model-based methods (ex. logistic regression)\nfor summary methods: visualize to provide an interpretable summary of data"
  },
  {
    "objectID": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html#definition-of-categorical-data",
    "href": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html#definition-of-categorical-data",
    "title": "Categorical Data Analysis",
    "section": "",
    "text": "categorical (or frequency) data consist of a discrete set of categories, which may be ordered or unordered.\n\nunordered\n\ngener: {male, female, transgender}\nmarital status: {never married, married, separated, divorced, widowed}\nparty preference: {NDP, liberal, conservative, green}\ntreatment improvement: {none, some, marked}\n\nordered\n\nage group: {0s,10s,20s,30s, …}\nnumber of children: {0, 1 , 2 ,3, …} ## Structures\n\n\n\nCategorical data appears in various forms like:\n\ntables\n\none way\ntwo way\nthree way\n\nmatrices\narray\ndata frames\n\ncase form\nfrequency form"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-03-21_mixed_model/index.html",
    "href": "docs/blog/posts/statistics/2023-03-21_mixed_model/index.html",
    "title": "template",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe radius of the circle is 10.\n\n\n\n1 Go to Project Content List\nProject Content List\n\n\n2 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html",
    "title": "Poisson Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\nDefinition 1 모수 (parameter)가 단위 시간 또는 공간 당 평균 발생 횟수 \\(\\lambda\\) 일 때 주어진 단위 시간 또는 공간 내에 발생하는 사건의 횟수를 확률 변수 \\(X\\) 로 하는 분포를 Poisson Distribution이라 한다. \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;\\lambda)&=\\frac{e^{-\\lambda}\\lambda^{x}}{x!} &(x=0,1,2, ..)\n\\end{aligned}\n\\] (\\(\\lambda\\) 는 단위 시간 또는 공간 당 평균 발생 횟수) 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=0}^{\\infty}xf(x)\\\\\n             &=\\sum_{x=0}^{\\infty}x\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n             &=\\sum_{x=1}^{\\infty}x\\frac{e^{-\\lambda}\\lambda^{x}}{x!} &\\because x=0 \\rightarrow \\text{equation}=0\\\\\n             &=\\sum_{x=1}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x}}{(x-1)!}\\\\\n             &=\\lambda\\sum_{x-1=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x-1}}{(x-1)!}\\\\\n             &=\\lambda\\sum_{y=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{y}}{(y)!} &\\because x-1=y \\text{  }(y=0,1,2, ...)\\\\\n             &=\\lambda\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n    \\text{E}(X(X-1))&=\\sum_{x=0}^{\\infty}x(x-1)f(x)\\\\\n             &=\\sum_{x=0}^{\\infty}x(x-1)\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n             &=\\lambda^2\\sum_{x=2}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x-2}}{(x-2)!}\\\\\n             &=\\lambda^2\\sum_{y=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{y}}{(y)!} &\\because x-2=y \\text{  }(y=0,1,2, ...)\\\\\n             &=\\lambda^2\\\\\n    \\text{E}(X^2)&=\\lambda^2+\\lambda\\\\\n    \\text{Var}(X)&=\\lambda^2+\\lambda-\\lambda^2=\\lambda\\\\\n\\end{aligned}\n\\]\n\n\n\n어느 의료 장비 제조 업체의 의료 장비 불량률이 2% 라고 가정했을 때 임의로 100대의 의료 장비를 구매하여 제조 업체의 Quality Control (QC) guide line을 따라 Quality Control (QC)를 진행 했을 때 불량품이 하나도 발생하지 않을 확률은 다음과 같다.\n\\[\n\\begin{aligned}\n  \\lambda &= 100*0.02=2\\\\\n  f(x)&=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}=\\frac{2^{x}e^{-2}}{x!}\\\\\n  f(0)&=\\frac{2^{0}e^{-2}}{0!}=e^{-2}\n\\end{aligned}\n\\]\n\n\nDefinition 2 \\(X\\sim B(n,p)\\) 일 때 \\(p\\) 가 충분히 작고 \\(n \\rightarrow \\infty\\) 고 \\(np=\\lambda\\) 한다면 \\(x=0,1,2, ...\\) 에 대하여\n\\[\n\\begin{aligned}\n  \\lim_{n \\to \\infty}\\binom{n}{x}p^{x}(1-p)^{n-x}=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}\n\\end{aligned}\n\\] (\\(\\lambda\\) 는 단위 시간 또는 공간 당 평균 발생 횟수) 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html#poisson-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html#poisson-distribution",
    "title": "Poisson Distribution",
    "section": "",
    "text": "Definition 1 모수 (parameter)가 단위 시간 또는 공간 당 평균 발생 횟수 \\(\\lambda\\) 일 때 주어진 단위 시간 또는 공간 내에 발생하는 사건의 횟수를 확률 변수 \\(X\\) 로 하는 분포를 Poisson Distribution이라 한다. \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;\\lambda)&=\\frac{e^{-\\lambda}\\lambda^{x}}{x!} &(x=0,1,2, ..)\n\\end{aligned}\n\\] (\\(\\lambda\\) 는 단위 시간 또는 공간 당 평균 발생 횟수) 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=0}^{\\infty}xf(x)\\\\\n             &=\\sum_{x=0}^{\\infty}x\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n             &=\\sum_{x=1}^{\\infty}x\\frac{e^{-\\lambda}\\lambda^{x}}{x!} &\\because x=0 \\rightarrow \\text{equation}=0\\\\\n             &=\\sum_{x=1}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x}}{(x-1)!}\\\\\n             &=\\lambda\\sum_{x-1=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x-1}}{(x-1)!}\\\\\n             &=\\lambda\\sum_{y=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{y}}{(y)!} &\\because x-1=y \\text{  }(y=0,1,2, ...)\\\\\n             &=\\lambda\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n    \\text{E}(X(X-1))&=\\sum_{x=0}^{\\infty}x(x-1)f(x)\\\\\n             &=\\sum_{x=0}^{\\infty}x(x-1)\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n             &=\\lambda^2\\sum_{x=2}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x-2}}{(x-2)!}\\\\\n             &=\\lambda^2\\sum_{y=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{y}}{(y)!} &\\because x-2=y \\text{  }(y=0,1,2, ...)\\\\\n             &=\\lambda^2\\\\\n    \\text{E}(X^2)&=\\lambda^2+\\lambda\\\\\n    \\text{Var}(X)&=\\lambda^2+\\lambda-\\lambda^2=\\lambda\\\\\n\\end{aligned}\n\\]\n\n\n\n어느 의료 장비 제조 업체의 의료 장비 불량률이 2% 라고 가정했을 때 임의로 100대의 의료 장비를 구매하여 제조 업체의 Quality Control (QC) guide line을 따라 Quality Control (QC)를 진행 했을 때 불량품이 하나도 발생하지 않을 확률은 다음과 같다.\n\\[\n\\begin{aligned}\n  \\lambda &= 100*0.02=2\\\\\n  f(x)&=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}=\\frac{2^{x}e^{-2}}{x!}\\\\\n  f(0)&=\\frac{2^{0}e^{-2}}{0!}=e^{-2}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html#blog-guide-map-link",
    "title": "Poisson Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html",
    "title": "Binomial Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 성공 확률이 p인 bernoulli distribution을 n 번 시행했을 때 성공횟수를 확률 변수 X로 갖는 probability distribution을 binomial distribution이라 한다. probability mass function은\n즉, \\[\n\\begin{aligned}\n  f_X(x;n,p)&=\\binom{n}{x}p^{x}(1-p)^{n-x} \\text{ } (y=0,1,2, ..., n)\\\\\n\\end{aligned}\n\\] 이고 Notation은 보통 \\(X \\sim Bin(n,p) \\text{ or } X \\sim B(n,p) \\text{ or } X \\sim Binomial(n,p)\\) 와 같이 쓰인다 (binomial distribution은 bernoulli distribution을 전제로 한다).\n\n\n\n\\[\n\\begin{aligned}\n    \\text{Let } &I_i \\text{ be } 1\\{x_i=1\\} \\\\\n    X&=\\sum_{i=1}^{n}I_i=I_1+I_2+ ... +I_n\\\\\n  \\text{E}(X)&=\\text{E}(\\sum_{i=1}^{n}I_i)\\\\\n             &=\\text{E}(I_1+I_2+ ... +I_n)\\\\\n             &=\\text{E}(I_1)+\\text{E}(I_2)+ ... +\\text{E}(I_n)\\\\\n             &=p+p+...+p\\\\\n             &=np\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{Var}(X)&=\\text{Var}(I_1+I_2+ ... +I_n)\\\\\n             &=\\text{Var}(I_1)+\\text{Var}(I_2)+ ... +\\text{Var}(I_n)\\\\\n             &=p(1-p)+p(1-p)+...+p(1-p)\\\\\n             &=np(1-p)\n\\end{aligned}\n\\]\n\n\n\n쌍란이 나올 확률이 0.05라고 가정했을 때 Super Market에서 1 pack of 12 eggs을 구매했을 때\n\n3개의 eggs에서 쌍란이 나올 확률은 \\[\n\\begin{aligned}\nX&\\sim Bin(12,0.05)\\\\\nf(X=3)&=\\binom{12}{3}0.05^3 0.95^9\n\\end{aligned}\n\\] 이다.\n적어도 3개의 eggs에서 쌍란이 나올 확률은 \\[\n\\begin{aligned}\nP(X\\ge3)&=1-F_X(3)\\\\\n         &=1-(f(X=3)+f(X=2)+f(X=1)+f(X=0))\\\\\n         &=1-(\\binom{12}{3}0.05^3 0.95^9+\\binom{12}{2}0.05^2 0.95^{10}+\\\\\n         &\\binom{12}{1}0.05^1 0.95^{11}+\\binom{12}{0}0.05^0 0.95^{12})\n\\end{aligned}\n\\] 이다.\n\n다른 예시로는, 분자 진단 시장에서 golden standard라고 평가받는 PCR (Polynomial Chain Reaction)에 사용되는 medical device가 2000 대 중 5대 꼴로 기계적 결함이 발견된다고 가정할 때, 1년에 평균 100대의 분잔 진단 장비를 공급받는 구매자 입장에서 장비의 결함이 발생할 연간 평균과 분산의 추정은 다음과 같다.\n\\[\n\\begin{aligned}\n    X&\\sim Bin(100,\\frac{5}{2000})\\\\\n    f(X=x)&=\\binom{100}{x}\\frac{5}{2000}^x (1-\\frac{5}{2000})^{100-x}\\\\\n    \\text{E}(X)&=100(\\frac{5}{2000})\\\\\n    \\text{Var}(X)&=100(\\frac{5}{2000})(1-\\frac{5}{2000})\n\\end{aligned}\n\\]\n\n\n\n\n\nDefinition 2 n번의 독립 시행에서 각 각 p_1, p_2, …, p_n 의 성공 확률로 E_1, E_2, …, E_n 중 어느 하나를 발생시킬 때 각 event E_i에 대응되는 발생 횟수를 확률 변수 X_1, X_2, …, X_n 로 갖는 joint probability mass function은\n\\[\n\\begin{aligned}\n  f_X(\\mathbf X = x_1,x_2, ...,x_n)&=\\binom{n}{x_1,x_2, ..., x_n}p_1^{x_1}(p_2)^{x_2}\\dots (p_n)^{x_n} \\\\\n\\end{aligned}\n\\] 이다. (단, \\(\\sum_{i=1}^{n}x_i=n, \\sum_{i=1}^{n}p_i=1\\))\n\n\n\n주사위를 5 번 던질 때 1 또는 6의 눈이 1번, 3, 4 또는 5의 눈이 2번 , 2의 눈이 2번 나올 확률은\n\\[\n\\begin{aligned}\n    &x_1= 1, x_2=2, x_3=2\\\\\n    f(X=(1,2,2))&=\\binom{5}{1,2,3}\\frac{1}{3}^1\\frac{1}{2}^2\\frac{1}{6}^2\n\\end{aligned}\n\\] 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html#binomial-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html#binomial-distribution",
    "title": "Binomial Distribution",
    "section": "",
    "text": "Definition 1 성공 확률이 p인 bernoulli distribution을 n 번 시행했을 때 성공횟수를 확률 변수 X로 갖는 probability distribution을 binomial distribution이라 한다. probability mass function은\n즉, \\[\n\\begin{aligned}\n  f_X(x;n,p)&=\\binom{n}{x}p^{x}(1-p)^{n-x} \\text{ } (y=0,1,2, ..., n)\\\\\n\\end{aligned}\n\\] 이고 Notation은 보통 \\(X \\sim Bin(n,p) \\text{ or } X \\sim B(n,p) \\text{ or } X \\sim Binomial(n,p)\\) 와 같이 쓰인다 (binomial distribution은 bernoulli distribution을 전제로 한다).\n\n\n\n\\[\n\\begin{aligned}\n    \\text{Let } &I_i \\text{ be } 1\\{x_i=1\\} \\\\\n    X&=\\sum_{i=1}^{n}I_i=I_1+I_2+ ... +I_n\\\\\n  \\text{E}(X)&=\\text{E}(\\sum_{i=1}^{n}I_i)\\\\\n             &=\\text{E}(I_1+I_2+ ... +I_n)\\\\\n             &=\\text{E}(I_1)+\\text{E}(I_2)+ ... +\\text{E}(I_n)\\\\\n             &=p+p+...+p\\\\\n             &=np\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{Var}(X)&=\\text{Var}(I_1+I_2+ ... +I_n)\\\\\n             &=\\text{Var}(I_1)+\\text{Var}(I_2)+ ... +\\text{Var}(I_n)\\\\\n             &=p(1-p)+p(1-p)+...+p(1-p)\\\\\n             &=np(1-p)\n\\end{aligned}\n\\]\n\n\n\n쌍란이 나올 확률이 0.05라고 가정했을 때 Super Market에서 1 pack of 12 eggs을 구매했을 때\n\n3개의 eggs에서 쌍란이 나올 확률은 \\[\n\\begin{aligned}\nX&\\sim Bin(12,0.05)\\\\\nf(X=3)&=\\binom{12}{3}0.05^3 0.95^9\n\\end{aligned}\n\\] 이다.\n적어도 3개의 eggs에서 쌍란이 나올 확률은 \\[\n\\begin{aligned}\nP(X\\ge3)&=1-F_X(3)\\\\\n         &=1-(f(X=3)+f(X=2)+f(X=1)+f(X=0))\\\\\n         &=1-(\\binom{12}{3}0.05^3 0.95^9+\\binom{12}{2}0.05^2 0.95^{10}+\\\\\n         &\\binom{12}{1}0.05^1 0.95^{11}+\\binom{12}{0}0.05^0 0.95^{12})\n\\end{aligned}\n\\] 이다.\n\n다른 예시로는, 분자 진단 시장에서 golden standard라고 평가받는 PCR (Polynomial Chain Reaction)에 사용되는 medical device가 2000 대 중 5대 꼴로 기계적 결함이 발견된다고 가정할 때, 1년에 평균 100대의 분잔 진단 장비를 공급받는 구매자 입장에서 장비의 결함이 발생할 연간 평균과 분산의 추정은 다음과 같다.\n\\[\n\\begin{aligned}\n    X&\\sim Bin(100,\\frac{5}{2000})\\\\\n    f(X=x)&=\\binom{100}{x}\\frac{5}{2000}^x (1-\\frac{5}{2000})^{100-x}\\\\\n    \\text{E}(X)&=100(\\frac{5}{2000})\\\\\n    \\text{Var}(X)&=100(\\frac{5}{2000})(1-\\frac{5}{2000})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html#multinomial-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html#multinomial-distribution",
    "title": "Binomial Distribution",
    "section": "",
    "text": "Definition 2 n번의 독립 시행에서 각 각 p_1, p_2, …, p_n 의 성공 확률로 E_1, E_2, …, E_n 중 어느 하나를 발생시킬 때 각 event E_i에 대응되는 발생 횟수를 확률 변수 X_1, X_2, …, X_n 로 갖는 joint probability mass function은\n\\[\n\\begin{aligned}\n  f_X(\\mathbf X = x_1,x_2, ...,x_n)&=\\binom{n}{x_1,x_2, ..., x_n}p_1^{x_1}(p_2)^{x_2}\\dots (p_n)^{x_n} \\\\\n\\end{aligned}\n\\] 이다. (단, \\(\\sum_{i=1}^{n}x_i=n, \\sum_{i=1}^{n}p_i=1\\))\n\n\n\n주사위를 5 번 던질 때 1 또는 6의 눈이 1번, 3, 4 또는 5의 눈이 2번 , 2의 눈이 2번 나올 확률은\n\\[\n\\begin{aligned}\n    &x_1= 1, x_2=2, x_3=2\\\\\n    f(X=(1,2,2))&=\\binom{5}{1,2,3}\\frac{1}{3}^1\\frac{1}{2}^2\\frac{1}{6}^2\n\\end{aligned}\n\\] 이다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html#blog-guide-map-link",
    "title": "Binomial Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html#bernoulli-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html#bernoulli-distribution",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Definition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html#blog-guide-map-link",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html#bernoulli-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html#bernoulli-distribution",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Definition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html#blog-guide-map-link",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html",
    "title": "Probability",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nexperiment(실험): 연구 수행 방식.\ntrial (시행): 연구 실험 시행.\nsample space(표본 공간, (\\(\\text{S or } \\Omega\\)): 실험의 가능한 모든 결과의 모음 또는 근원 사상의 집합.\nelement (근원 사상, \\(\\omega\\)): 표본 공간의 원소.\nevent (사건, \\(E\\)): 근원 사상의 집합 또는 표본 공간의 부분 집합.\n\n\n\n\n\n\\(\\omega \\in A\\) : \\(\\omega\\) is an element of a set A\n\\(\\omega \\not\\in A\\) : \\(\\omega\\) is not an element of a set A\n\\(B \\subset A\\) : B is a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\in A\\)\n\\(A = B\\) : \\(B \\subset A\\), \\(A \\subset B\\)\n\\(B \\not\\subset A\\) : B is not a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\not\\in A\\)\n\n\n\n\n확률은 우연 (또는 가능성)과 불확실성에 대한 연구이다. 그 이론은 집합 이론을 기반으로 한다. 고전 확률은 가능성 조합 게임, 이론 오류와 같은 도박과 같다. 확률은 통계학, 경제학, 연산연구, 심리학, 생물학, 역학, 의학 등에 사용된다. 확률을 이해하기 위해 미적분학과 집합론의 지식이 요구되며 관련 학문은 ​​해석학, 측도론, 확률과정 등이 있다. 확률을 정의하려면 사건 수집에 대한 규칙성이 필요하다.\n\n\n\n\n확률은 사건이 발생할 가능성을 나타낸다.\n고전적 정의\n\n\nDefinition 1 The probability of event A is the sum of the probabilities assigned to all sample points in event A. Therefore, \\(0 \\le P(A) \\le 1, P(\\emptyset)=0, P(\\Omega)=1\\). In addition, if \\(A_1, A_2, A_3, ...\\) are mutually exclusive,\n\\[\nP(A_1 \\cup A_2 \\cup A_3 \\dots)=P(A_1) + P(A_2) + P(A_3) + \\dots =\\sum_{i=1}^{\\infty}P(A_i)\n\\]\n\n위의 정의에서 the probabilities assigned to all sample points in event A 의 표현은 사건 A안에 있는 element의 가중치로 해석할 수 있다. 즉, 쉽게 말하면, 확률은 표본 공간, sample space \\(\\Omega\\) 의 원소 (element) \\(\\omega\\) 에 할당된 가중치를 더한 것이다.\n예를 들어, 주사위 모양을 어떤식으로든 조작해 홀수가 짝수보다 2배 더 많이 발생하게끔 만들어 1 번 던질 때 3 보다 작은 수가 나올 사건을 A라고 하면 \\(\\Omega=\\{1,2,3,4,5,6\\}\\) 이고 각 홀수 원소에 가중치가 2배씩 붙기 때문에 홀수 눈이 발생할 확률은 \\(\\frac{2x}{2x+x+2x+x+2x+x}=\\frac{2}{9}\\), 반면에, 짝수의 눈이 나올 확률은 \\(\\frac{x}{2x+x+2x+x+2x+x}=\\frac{1}{9}\\) 이다. 이 때 확률은 위의 정의를 따라야 하므로\n\n\\(0\\le P(evenNumber)=\\frac{2}{9}, P(oddNumber)=\\frac{1}{9}\\le 1\\) 이고\n\\(P(\\Omega=\\{1,2,3,4,5,6\\})=P(evenNumbers)+P(oddNumbers)=1\\)\n\n이므로 확률이라고 할 수있다.\n\n그러므로 \\(P(A={1,2})=P(1)+P(2)=\\frac{2}{9}+\\frac{1}{9}=\\frac{3}{9}\\) 이다.\n\n\nTheorem 1 Countable sample space \\(\\Omega\\) consists of \\(N\\) distinctive equally likely elements (i.e. \\(n(S)=N\\)), and an event \\(A\\) is a subset of the sample space. The event A consists of \\(n\\) distinctive equally likely elements (i.e. \\(n(A)=n\\)). Then \\[\nP(A)=\\frac{n}{N}\n\\]\n\nelement가 오직 동일한 확률로 발생할 때에만 (equally likely), 확률은 \\(\\frac{n(A)}{n(\\Omega \\space or \\space S)}\\) 의 비율(proportion)로 표현될 수 있다.\n예를 들어, 주사위의 눈이 3 보다 작은 수가 나올 사건을 A라고 하면 \\(P(A)= \\frac{n(A)}{n(S)}=\\frac{n(\\{1,2\\})}{n(\\{1,2,3,4,5,6\\})}=\\frac{2}{6}\\) 가 된다.\n\nTheorem 2 If in \\(N\\) identical and independent repeated experiments, an event \\(A\\) happens \\(n\\) times, the the probability of \\(A\\) is defined by \\[\nP(A)=\\lim_{N\\to\\infty}\\frac{n}{N}\n\\]\n\n\nTheorem 3 Basic probability theorem: the complement and additive rule. \\[\n\\begin{aligned}\nP(E^c)&=1-P(E) \\\\\nP(E_1 \\cup E_2)&= P(E_1) + P(E_2) - P(E_1 \\cap E_2) \\\\\n\\end{aligned}\n\\]\n\\(E_1\\) and \\(E_2\\) are mutually exclusive.\n\n\nTheorem 4  \nGeneralized additive rule $$\n\\[\\begin{aligned}\n\n\\end{aligned}\\]\n$$\n\n나머지는 확률 이론 서적을 살펴 보길 바란다.\n\n\n\n\n\n\n\n\nexperiment: the way carry out a study, study design.\ntrial: study experiment trial.\nsample space (\\(\\text{S or } \\Omega\\)): the set of all possible elements (i.e. the collection of all possible outcomes of an experiment).\nelement (\\(\\omega\\)): each outcome of sample space, it is also called ‘sample point’.\nevent (\\(E\\)): a set of sample points or outcomes or a subset of sample space.\n\n\n\n\n\n\\(\\omega \\in A\\) : \\(\\omega\\) is an element of a set A\n\\(\\omega \\not\\in A\\) : \\(\\omega\\) is not an element of a set A\n\\(B \\subset A\\) : B is a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\in A\\)\n\\(A = B\\) : \\(B \\subset A\\), \\(A \\subset B\\)\n\\(B \\not\\subset A\\) : B is not a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\not\\in A\\)\n\n\n\n\nProbability is on study of chance and uncertainty. Its theory builds on set theory. Classic probability is like a gambling of combinatorial games of chance, theory errors. Probability is used in statistics, economics, operation research, psychology, biology, epidemiology, medicine, etc. The prerequisite for probability is calculus and set theory, and the related study is real analysis, measure theory, and stochastic process. To define probability, some regularity on the collection of events is required.\n\n\n\n\nprobability shows the possibility of the occurrence of an event.\nclassic definition ::: {#def-classic}\n\nCountable sample space \\(\\Omgega\\) consists of \\(N\\) distinctive equally likely elements (i.e. \\(n(S)=N\\)), and an event \\(A\\) is a subset of the sample space. The event A consists of \\(n\\) distinctive equally likely elements (i.e. \\(n(A)=n\\)). Then \\[\nP(A)=\\frac{n}{N}\n\\]\n\n\n\nTheorem 5 If in \\(N\\) identical and independent repeated experiments, an event \\(A\\) happens \\(n\\) times, the the probability of \\(A\\) is defined by \\[\nP(A)=\\lim_{N\\to\\infty}\\frac{n}{N}\n\\]\n\n\nThe case of the sample space consisting of \\(N\\) distinctive not equally likely elements,\nThe case of the uncountable sample space\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#terms",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#terms",
    "title": "Probability",
    "section": "",
    "text": "experiment(실험): 연구 수행 방식.\ntrial (시행): 연구 실험 시행.\nsample space(표본 공간, (\\(\\text{S or } \\Omega\\)): 실험의 가능한 모든 결과의 모음 또는 근원 사상의 집합.\nelement (근원 사상, \\(\\omega\\)): 표본 공간의 원소.\nevent (사건, \\(E\\)): 근원 사상의 집합 또는 표본 공간의 부분 집합."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#notations",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#notations",
    "title": "Probability",
    "section": "",
    "text": "\\(\\omega \\in A\\) : \\(\\omega\\) is an element of a set A\n\\(\\omega \\not\\in A\\) : \\(\\omega\\) is not an element of a set A\n\\(B \\subset A\\) : B is a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\in A\\)\n\\(A = B\\) : \\(B \\subset A\\), \\(A \\subset B\\)\n\\(B \\not\\subset A\\) : B is not a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\not\\in A\\)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#overview",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#overview",
    "title": "Probability",
    "section": "",
    "text": "확률은 우연 (또는 가능성)과 불확실성에 대한 연구이다. 그 이론은 집합 이론을 기반으로 한다. 고전 확률은 가능성 조합 게임, 이론 오류와 같은 도박과 같다. 확률은 통계학, 경제학, 연산연구, 심리학, 생물학, 역학, 의학 등에 사용된다. 확률을 이해하기 위해 미적분학과 집합론의 지식이 요구되며 관련 학문은 ​​해석학, 측도론, 확률과정 등이 있다. 확률을 정의하려면 사건 수집에 대한 규칙성이 필요하다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#probability",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#probability",
    "title": "Probability",
    "section": "",
    "text": "확률은 사건이 발생할 가능성을 나타낸다.\n고전적 정의\n\n\nDefinition 1 The probability of event A is the sum of the probabilities assigned to all sample points in event A. Therefore, \\(0 \\le P(A) \\le 1, P(\\emptyset)=0, P(\\Omega)=1\\). In addition, if \\(A_1, A_2, A_3, ...\\) are mutually exclusive,\n\\[\nP(A_1 \\cup A_2 \\cup A_3 \\dots)=P(A_1) + P(A_2) + P(A_3) + \\dots =\\sum_{i=1}^{\\infty}P(A_i)\n\\]\n\n위의 정의에서 the probabilities assigned to all sample points in event A 의 표현은 사건 A안에 있는 element의 가중치로 해석할 수 있다. 즉, 쉽게 말하면, 확률은 표본 공간, sample space \\(\\Omega\\) 의 원소 (element) \\(\\omega\\) 에 할당된 가중치를 더한 것이다.\n예를 들어, 주사위 모양을 어떤식으로든 조작해 홀수가 짝수보다 2배 더 많이 발생하게끔 만들어 1 번 던질 때 3 보다 작은 수가 나올 사건을 A라고 하면 \\(\\Omega=\\{1,2,3,4,5,6\\}\\) 이고 각 홀수 원소에 가중치가 2배씩 붙기 때문에 홀수 눈이 발생할 확률은 \\(\\frac{2x}{2x+x+2x+x+2x+x}=\\frac{2}{9}\\), 반면에, 짝수의 눈이 나올 확률은 \\(\\frac{x}{2x+x+2x+x+2x+x}=\\frac{1}{9}\\) 이다. 이 때 확률은 위의 정의를 따라야 하므로\n\n\\(0\\le P(evenNumber)=\\frac{2}{9}, P(oddNumber)=\\frac{1}{9}\\le 1\\) 이고\n\\(P(\\Omega=\\{1,2,3,4,5,6\\})=P(evenNumbers)+P(oddNumbers)=1\\)\n\n이므로 확률이라고 할 수있다.\n\n그러므로 \\(P(A={1,2})=P(1)+P(2)=\\frac{2}{9}+\\frac{1}{9}=\\frac{3}{9}\\) 이다.\n\n\nTheorem 1 Countable sample space \\(\\Omega\\) consists of \\(N\\) distinctive equally likely elements (i.e. \\(n(S)=N\\)), and an event \\(A\\) is a subset of the sample space. The event A consists of \\(n\\) distinctive equally likely elements (i.e. \\(n(A)=n\\)). Then \\[\nP(A)=\\frac{n}{N}\n\\]\n\nelement가 오직 동일한 확률로 발생할 때에만 (equally likely), 확률은 \\(\\frac{n(A)}{n(\\Omega \\space or \\space S)}\\) 의 비율(proportion)로 표현될 수 있다.\n예를 들어, 주사위의 눈이 3 보다 작은 수가 나올 사건을 A라고 하면 \\(P(A)= \\frac{n(A)}{n(S)}=\\frac{n(\\{1,2\\})}{n(\\{1,2,3,4,5,6\\})}=\\frac{2}{6}\\) 가 된다.\n\nTheorem 2 If in \\(N\\) identical and independent repeated experiments, an event \\(A\\) happens \\(n\\) times, the the probability of \\(A\\) is defined by \\[\nP(A)=\\lim_{N\\to\\infty}\\frac{n}{N}\n\\]\n\n\nTheorem 3 Basic probability theorem: the complement and additive rule. \\[\n\\begin{aligned}\nP(E^c)&=1-P(E) \\\\\nP(E_1 \\cup E_2)&= P(E_1) + P(E_2) - P(E_1 \\cap E_2) \\\\\n\\end{aligned}\n\\]\n\\(E_1\\) and \\(E_2\\) are mutually exclusive.\n\n\nTheorem 4  \nGeneralized additive rule $$\n\\[\\begin{aligned}\n\n\\end{aligned}\\]\n$$\n\n나머지는 확률 이론 서적을 살펴 보길 바란다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#terms-1",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#terms-1",
    "title": "Probability",
    "section": "",
    "text": "experiment: the way carry out a study, study design.\ntrial: study experiment trial.\nsample space (\\(\\text{S or } \\Omega\\)): the set of all possible elements (i.e. the collection of all possible outcomes of an experiment).\nelement (\\(\\omega\\)): each outcome of sample space, it is also called ‘sample point’.\nevent (\\(E\\)): a set of sample points or outcomes or a subset of sample space."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#notations-1",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#notations-1",
    "title": "Probability",
    "section": "",
    "text": "\\(\\omega \\in A\\) : \\(\\omega\\) is an element of a set A\n\\(\\omega \\not\\in A\\) : \\(\\omega\\) is not an element of a set A\n\\(B \\subset A\\) : B is a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\in A\\)\n\\(A = B\\) : \\(B \\subset A\\), \\(A \\subset B\\)\n\\(B \\not\\subset A\\) : B is not a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\not\\in A\\)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#overview-1",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#overview-1",
    "title": "Probability",
    "section": "",
    "text": "Probability is on study of chance and uncertainty. Its theory builds on set theory. Classic probability is like a gambling of combinatorial games of chance, theory errors. Probability is used in statistics, economics, operation research, psychology, biology, epidemiology, medicine, etc. The prerequisite for probability is calculus and set theory, and the related study is real analysis, measure theory, and stochastic process. To define probability, some regularity on the collection of events is required."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#probability-1",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#probability-1",
    "title": "Probability",
    "section": "",
    "text": "probability shows the possibility of the occurrence of an event.\nclassic definition ::: {#def-classic}\n\nCountable sample space \\(\\Omgega\\) consists of \\(N\\) distinctive equally likely elements (i.e. \\(n(S)=N\\)), and an event \\(A\\) is a subset of the sample space. The event A consists of \\(n\\) distinctive equally likely elements (i.e. \\(n(A)=n\\)). Then \\[\nP(A)=\\frac{n}{N}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#blog-guide-map-link",
    "title": "Probability",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nBayes’ rule (베이즈 정리)는 prior probability(사전 확률)과 posterior probability(사후 확률)의 관계를 조건부 확률을 이용하여 확립한 것이다.\n\nprior probability(사전 확률)는 데이터를 얻기 전 연구자의 가설이 들어간 일종의 사건 발생의 신뢰도로 해석하기도 하고 prior probability density function (사전 확률 밀도 함수) 라고도 표현된다.\nposterior probability(사후 확률)는 데이터가 주어진 후 연구자의 가설이 들어간 사건 발생의 신뢰도로 해석하기도 하고 posterior probability desnsity function(사후 확률 밀도 함수) 라고도 표현된다.\n\n좀 더 구체적으로, 2개의 사건 A와 B로 한정시켜 생각해봤을 때, 조건부 확률 \\(P(A|B)\\) 는 각 사건의 확률 \\(P(A), P(B), P(B|A)\\) 를 사용하여 게산될 수 있다. 그래서 베이즈 정리는 \\(P(B|A)\\), \\(P(B|\\overline{A})\\), \\(P(A)\\) 의 정보를 알고있거나 계산 가능할 때 아래와 같은 \\(P(A|B)\\) 의 확률을 구할 수 있는 공식을 제공한다(Equation 1).\n\n\nBayes’ rule을 좀 더 직관적으로 이해하기 위해선 Bayes’ rule와 연관된 친숙한 개념들을 상기시킬 필요가 있다. 우리에게 친숙한 개념인 연역법과 귀납법에 대해서 간단이 살펴본다.\n\n\n\n\n연역법 (deduction method or deductive reasoning)는 하나 (=대전제) 또는 둘 이상의 명제(=대전제+소전제들)를 전제로 하여 명확한 논리에 근거해 새로운 명제(결론)를 도출하는 방법이다. 보통 일반적인 명제에서 구체적인 명제로 도출해내는 방식으로 연역법을 설명하기도 한다. 연역법은 전제와 결론의 타당성보다는 결론을 이끌어내는 논리 전개에 엄격함을 요구한다. 그래서 명쾌한 논리가 보장된다면 연역적 추론의 결론은 그 전제들이 결정적 근거가 되어 전제와 결론이 필연성을 갖게 된다. 따라서, 전제가 진리(=참)이면 결론도 항상 진리(=참)이고 전제가 거짓이면 결론도 거짓으로 도출된다. 하지만, 모든 연역적 추론에서 출발되는 최초의 명제가 결코 연역에 의해 도출될 수 없다는 약점을 갖고있다. 즉, 반드시 검증된 명제를 대전제로 하여 연역적 추리를 시작해야한다. Source: naver encyclopedia -deductive method (cf. 귀류법)\n예를 들어, 아리스토텔레스의 삼단논법의 논리 형식이 가장 많이 인용이 된다. 대전제와 소전제가 하나씩있는 둘 이상의 명제로부터 결론이 도출되는 예를 살펴보자.\n\n대전제: 모든 사람은 죽는다.\n소전제: 소크라테스는 사람이다.\n결론: 그러므로 소크라테스도 죽는다.\n\n\n\n\n귀납법 (Induction method or Inductive reasoning)은 전제와 결론을 뒷받침하는 논리에 의해 그 타당성이 평가된다. 귀납적 추론은 관찰과 실험에서 얻은 특수한 사례 (= data)를 근거로 전체에 적용시키는 귀납적 비약을 통해 이루어진다. 이와 같이 귀납에서 얻어진 결론은 일정한 개연성을 지닐 뿐이며, 특정 data에 따라 귀납적 추론의 타당성에 영향을 미친다. 그러므로, 검증된 data가 많을 수록 신뢰도와 타당성이 증가한다는 특징이 있다.하지만, 귀납적 추론의 결론이 진리인 것은 아니다. Source: naver encyclopedia - inductive method\n\n특수한 사례 (or data): 소크라테스는 죽었다, 플라톤도 죽었다, 아리스토텔레스도 죽었다.\n소전제: 소크라테스는 사람이다.\n결론: 그러므로 소크라테스도 죽는다.\n\n위와 같이 연역적 추론과 귀납적 추론은 서로 반대되는 개념으로 각 각 강점과 약점이 있으며 현실에서는 서로 상호 보완적으로 쓰이고 있다. 따라서, 전제로 삼는 대전제 역시 검증 과정이 필요하고 그 가설에서 몇 개의 명제를 연역해 실험과 관찰 등을 수행하는 가설연역법(hypothetical deductive method)이 널리 쓰이고 있다.\n\n\n\n\n통계학에선 모수(parameter)를 추정하는 여러 방법론들이 있는데 이번 블로그에서는 Frequentism와 Bayeseanism, 이 2가지 방법론에 초점을 둔다.\n\n\n통계학에서 가장 널리쓰이고 있는 방법론으로, 연역법에 근거한 결론 도출 방식을 이용한다. 간단히 말하면, 이미 알려진 분포에서 연구자의 관측치가 발생할 확률을 관찰하여 결론을 유도 하는 방법이다. p-value에 의한 결론 도출방식이 그 대표적인 예이다. 연구자의 데이터가 여러 수학자와 통계학자들이 증명해 놓은 분포하에서 발생한 사실이 입증이 됐을 때 연구자의 관측치가 그 named distribution(like normal distribution)에서 발생할 확률이 낮을 수록 p-value가 작아지고 일정 유의수준에 따라 연구자는 귀무가설을 기각하는 논리방식을 따른다.\n\n\n\n통계학에서 역시 많이 쓰이는 방법론으로, 귀납법에 근거한 결론 도출 방식을 이용한다. 간단히 말하면, 확률을 확률변수가 갖는 sample space에 대한 특정 사건이 발생한 사건의 비로 보는 것 (equally likely라고 가정)이 아니라 내가 설정한 가설에 대한 신뢰도로 바라보는 것이다. 따라서, 사전에 이미 알고있는 데이터가 있어 사전 확률 (prior probability)을 알고있고 이 사전 확률이 추가적인 data에 의해 조정되는 사후 확률 (posterior probability)이 계산된다. 이때 사전 확률자체보다는 추가적인 data와 사후 확률을 계산하는데 사용되는 likelihood의 타당성이 더 중요하다. 더 구체적인 내용은 Bayesean statistics에 기본이 되는 Bayes’ rule에서 살펴보기로 한다.\nsource: Frequentism vs Bayeseanism\n\n\n\n\n\n\nTheorem 1 \\[\n\\begin{aligned}\nP(A|B)&=\\frac{P(B|A)P(A)}{P(B)}\\\\\n      &=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\n\\end{aligned}\n\\tag{1}\\]\n\n\n\\(P(A|B)\\): posterior probability, B(data)가 주어졌을때 가설 A에 대한 신뢰도\n\\(P(A)\\): prior probability, 가설 A에대한 신뢰도\n\\(P(B|A)\\): likelihood, 가설 A가 주어졌을때 B(Data)가 발생할 신뢰도\n\\(P(B)\\): marginal probability, Data의 신뢰도\n\nEquation 1 의 두 분째 등식을 이해하기 위해선, Law of Total Probability (전 확률 법칙) 또는 Total Probability Rule (전 확률 정리)을 이해해야한다.\n\n\n\n\nTheorem 2 Let \\(A_1, A_2, ..., A_k\\) be a set of mutually exclusive and exhaustive events. Let \\(A\\) be a event and a partition of sample space \\(\\Omega\\), then\n\\[\nP(B)=\\sum_{i=1}^{n}P(B|A_i)P(A_i)\n\\]\n\n\n\n\n\n\nFigure 1: Law of Total Probability Example - Two Events\n\n\nSource: Law of ToTal Probability with Proof\n\n\n\n\nFigure 2: Law of Total Probability Example - Multiple Events\n\n\nSource: MIT RES.6-012 Introduction to Probability, Spring 2018 - Youtube\n\n\n\\[\n\\begin{aligned}\nP(B)&=P(B\\cap A) + P(B\\cap \\overline A)\\\\\n    &=P(B\\cap A) + P(B\\cap \\overline A)\\\\\n    &=P(B|A)P(A)+P(B|\\overline A)P(\\overline A)\\\\\nP(A\\cap B)&=P(B|A)P(A)=P(A|B)P(B)\\\\\n\\therefore\nP(A|B)&=\\frac{P(A \\cap B)}{P(B)}\\\\\n      &=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\n\\end{aligned}\n\\tag{2}\\]\nLaw of total probability 를 이용하여 Bayes’ rule이 Equation 2 와 같이 변형되었다. 최종식을 보면 좀 더 직관적인 해석이 가능해지는데 P(B) 가 A와의 교집합 확률의 총합이 되면서 분자가 그 일부가 되는 비율의 개념으로 해석될 수 있다. Figure 1 를 보면 \\(P(A|B)=\\frac{P(B \\cap A)}{P(B)}=\\frac{P(B \\cap A)}{P(B \\cap A)+P(B \\cap \\overline A)}\\) 로 표현되는 것을 볼 수 있다. 그 것을 조금 더 일반화 한 경우는 Figure 2 를 참고하여 유추할 수 있다.\n\n\n\n\\[\n\\begin{aligned}\nP(A|B)&=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\\\\\nP(\\theta|x)&=\\frac{P(x|\\theta)P(\\theta)}{P(x|\\theta)P(\\theta)+P(x|\\overline \\theta)P(\\overline \\theta)}\n\\end{aligned}\n\\]\n많은 참고 문헌에서 사건 A, B를 모수, \\(\\theta\\) 와 data, \\(x\\) 로 표현하기도 한다. 즉, data \\(x\\) 가 주어졌을 때 모수 \\(\\theta\\) 가 발생할 확률이 data에 의해서 update된다.\n\n\\(P(\\theta)\\)\n\nprior probability density function\n데이터없이 초기에 임시로 부여된 모델 또는 모수의 확률\n\n\\(P(x|\\theta)\\)\n\nlikelihood\n초기에 임시로 부여된 모델 또는 모수가 주어졌을 때 data x가 발생할 우도\n좀 더 파격적으로 해석하면, 초기에 임시로 부여된 모델 또는 모수가 data x에 들어맞을(or fittng) 확률\n\n\\(P(x)\\)\n\nmarginal proability\n데이터가 발생할 확률로 \\(\\theta\\) 와 상관없기 때문에 상수로 취급한다.\n\n\\(P(\\theta|x)\\)\n\nposterior probability density function\ndata가 주어졌을 때 모델 또는 모수의 확률\nBayes’ Rule에 의한 최적화에서 다음 최적화 iteration에서 Prior로 쓰인다.\n\n\n\\(P(x)\\) 는 상수이기 때문에 생략가능 하여 아래의 식과 같이 정리 할 수 있다. \\[\nP(\\theta|x)\\propto P(x|\\theta)P(\\theta)\n\\]\n\\(P(\\theta|x)\\) 는 \\(P(x|\\theta)P(\\theta)\\) 에만 영향을 받는 것을 볼 수 있다.\n\n\n\n펭수는 평소 관심이 있던 코니에게서 초콜릿을 선물받았다. 펭수는 초콜릿을 준 코니가 나를 좋아하는지가 궁금하기 때문에 이것을 통계적으로 계산해본다.\n펭수는 먼저 다음 두 상황을 가정한다.\n\n\\(P(like)=0.5\\)\n\n코니가 펭수를 좋아한다는 가설의 신뢰도는 반 반이다. 즉, 정보없는 상태에서의 펭수의 prior probability.\n0.5로 설정한 이유는 다음의 원리를 따랐다. The Principle of Insufficient Reason(이유불충분의 원리- 하나의 사건을 기대할만한 어떤 이유가 없는 경우에는 가능한 모든 사건에 동일한 확률을 할당해야 한다는 원칙).\n\n\\(P(choco)\\)\n\n초콜릿을 받았다라는 data가 발생할 신뢰도\n\n\n펭수는 코니에게 자신을 좋아하는지 알 길이 없으니 사람이 호감이 있을 때에 대한 초콜릿 선물 데이터를 조사하기 시작한다. 즉, 호감의 근거는 초콜릿으로 한정했고 초콜릿 선물 방식의 불확실성을 호감으로 설명하는 문헌을 찾기 시작했다. 그리고 펭수는 도서관에 있는 일반인 100명을 대상으로 초콜릿과 호감과의 관계를 연구한 초콜릿과 호감 논문을 통해 두 가지 정보를 알게된다.\n\n일반적으로, 어떤 사람이 상대방에게 호감이 있어서 초콜릿을 줄 확률은 \\(40%\\) 이다. 즉, \\(P(choco|like)=0.4\\)\n일반적으로, 어떤 사람이 상대방에게 호감이 없지만 예의상 초콜릿을 줄 확률은 \\(30%\\) 이다. 즉, \\(P(choco|\\overline{like})=0.3\\)\n위의 2가지 정보로 유추 가능한 정보\n\n\\(P(\\overline{choco}|like)=0.6\\)\n\\(P(\\overline{choco}|\\overline{like})=0.7\\)\n\n초콜릿에 관한 조사를 토대로 얻은 4가지 정보로 유추할 수 있는 정보\n\n\\(P(choco|like)=0.4\\): like를 받고 있는 50명 중 \\(40%\\) 인 20명은 초콜릿을 받는다.\n\\(P(\\overline{choco}|like)=0.6\\): like를 받고 있는 50명 중 \\(60%\\) 인 30명은 초콜릿을 받지 못한다.\n\\(P(choco|\\overline{like})=0.3\\): like를 받지 않는 50명 중 \\(30%\\) 인 15명은 예의상 준 초콜릿을 받는다.\n\\(P(\\overline{choco}|\\overline{like})=0.7\\): like를 받지 않는 50명 중 \\(70%\\) 인 35명은 초콜릿을 받지 못한다.\n\n\n펭수의 관점으로 정보를 재분류\n\n펭수가 궁금한 정보\n\n\\(P(like|choco)=?\\), posterior probability\n\n펭수가 가정한 정보\n\n\\(P(like)=0.5\\), prior probability by The Principle of Insufficient Reason\n\n펭수가 조사한 정보\n\n\\(P(choco|like)=0.4\\), likelihood\n\\(P(choco)\\): marginal probability\n\n\\(P(choco)=P(choco|like)+P(choco|\\overline{like})=\\frac{20+15}{100}=0.35\\)\n\n\n\n위의 정리한 정보를 Bayes’ rule에 대입하면,\n\\[\nP(like|choco)=\\frac{P(choco|like)\\times P(like)}{P(choco)}=\\frac{0.4\\times 0.5}{0.35}=0.57\n\\]\n펭수의 prior probability(\\(P(A)=0.5\\))가 posterior probability(\\(P(A|B)=0.57\\))로 업데이트 될 수 있다. 초콜릿과 호감 논문을 읽고 코니가 자신을 좋아할 확률이 높아진 것에 대해 기대감을 얻은 용기가 없는 펭수는 100명 보다 더 많은 독립적인 사람들로 실험한 논문을 찾아 다시 자신의 업데이트 된 사전 확률을 계속해서 업데이트할 생각이다. 그리고 자신의 사전 확률을 추가적인 데이터를 갖고 사후 확률로 계속해서 업데이트시켜 정확한 확률을 구한다.\n위의 예시는 영상 자료: 초콜릿을 준 코니의 마음을 시청하고 영감을 얻은 슬기로운 통계생활 tistory에 있는 Source: 베이즈 정리(Bayes’ rule) 완벽히 정리하기 슬기로운 통계생활 블로그를 요약 및 약간의 각색을 한 것이다.\n\n\n\n\n\nTheorem 3 Let \\(A_1, A_2, ..., A_k\\) be a set of mutually exclusive and exhaustive events. Let \\(A\\) be a event, then\n\\[\nP(A_i|B)=\\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^{k}P(B|A_i)P(A_i)}\n\\]\n\n\n\n\nMaximum a posterior estimation는 statistical estimation methods의 큰 기둥 중 하나인 maximum likelihood estimation과 더불어 parameter \\(\\theta\\) 를 추정하는데 많이 사용되는 방법론이다. 사후 확률 밀도 함수 \\(f(x|\\theta)\\) 또는 \\(P(x|\\theta)\\) 를 최대화하는 \\(\\theta\\) 의 추정치를 구하는 방법이며 아래와 같은 argument로 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\hat{\\theta}&=\\arg \\max_{\\theta}\\frac{f(x|\\theta)f(\\theta)}{\\int f(x|\\theta)f(\\theta)}\\\\\n            &\\propto\\arg \\max_{\\theta}f(x|\\theta)f(\\theta)\n\\end{aligned}\n\\]\n최대 우도 추정량과 달리 최대 사후 추정량에는 최대화하는 식에 사전 확률이 추가되어 있는 것을 볼 수 있다. 그러므로 분자 부분인 \\(f(x|\\theta)f(\\theta)\\) 만을 최대화 한다. 분모 부분인 \\(\\int f(x|\\theta)f(\\theta)\\) nomarlizing penalty 또는 constant로 간주한다. 여기서 \\(P(\\theta)\\) 초기 가정치 인데 아무렇게나 설정하기 보다는 good estimate로 설정해야 통계학자들로부터의 공격을 최소화시킬 수 있다. MAP는 나이브 베이즈의 알고리즘의 핵심이다.\n[참고] 최대 우도 추정량 \\[\n\\begin{aligned}\n\\hat{\\theta}=\\arg \\max_{\\theta}L(x|\\theta)=\\arg \\max_{\\theta}\\Pi_{i=1}^{n}f(x|\\theta)\n\\end{aligned}\n\\]\n\n\n\nNaive Bayes에 대한 구체적인 글은 다른 블로그에 소개한다. Naive Bayes는 Bayes’ Rule을 이용해 \\(\\theta\\) 를 최적화 시킨다. Naive Bayes의 Naive는 features 또는 explanotry variables이 서로 conditionally indepdent라고 가정한 것에서 이름 붙여졌다.\n\n\n\n\n\nBayes’ rule provides a formula how to calculate \\(P(A|B)\\) if \\(P(B|A)\\), \\(P(B|\\overline{A})\\), \\(P(A)\\) are available\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#bayes-rule",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#bayes-rule",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Bayes’ rule (베이즈 정리)는 prior probability(사전 확률)과 posterior probability(사후 확률)의 관계를 조건부 확률을 이용하여 확립한 것이다.\n\nprior probability(사전 확률)는 데이터를 얻기 전 연구자의 가설이 들어간 일종의 사건 발생의 신뢰도로 해석하기도 하고 prior probability density function (사전 확률 밀도 함수) 라고도 표현된다.\nposterior probability(사후 확률)는 데이터가 주어진 후 연구자의 가설이 들어간 사건 발생의 신뢰도로 해석하기도 하고 posterior probability desnsity function(사후 확률 밀도 함수) 라고도 표현된다.\n\n좀 더 구체적으로, 2개의 사건 A와 B로 한정시켜 생각해봤을 때, 조건부 확률 \\(P(A|B)\\) 는 각 사건의 확률 \\(P(A), P(B), P(B|A)\\) 를 사용하여 게산될 수 있다. 그래서 베이즈 정리는 \\(P(B|A)\\), \\(P(B|\\overline{A})\\), \\(P(A)\\) 의 정보를 알고있거나 계산 가능할 때 아래와 같은 \\(P(A|B)\\) 의 확률을 구할 수 있는 공식을 제공한다(Equation 1).\n\n\nBayes’ rule을 좀 더 직관적으로 이해하기 위해선 Bayes’ rule와 연관된 친숙한 개념들을 상기시킬 필요가 있다. 우리에게 친숙한 개념인 연역법과 귀납법에 대해서 간단이 살펴본다.\n\n\n\n\n연역법 (deduction method or deductive reasoning)는 하나 (=대전제) 또는 둘 이상의 명제(=대전제+소전제들)를 전제로 하여 명확한 논리에 근거해 새로운 명제(결론)를 도출하는 방법이다. 보통 일반적인 명제에서 구체적인 명제로 도출해내는 방식으로 연역법을 설명하기도 한다. 연역법은 전제와 결론의 타당성보다는 결론을 이끌어내는 논리 전개에 엄격함을 요구한다. 그래서 명쾌한 논리가 보장된다면 연역적 추론의 결론은 그 전제들이 결정적 근거가 되어 전제와 결론이 필연성을 갖게 된다. 따라서, 전제가 진리(=참)이면 결론도 항상 진리(=참)이고 전제가 거짓이면 결론도 거짓으로 도출된다. 하지만, 모든 연역적 추론에서 출발되는 최초의 명제가 결코 연역에 의해 도출될 수 없다는 약점을 갖고있다. 즉, 반드시 검증된 명제를 대전제로 하여 연역적 추리를 시작해야한다. Source: naver encyclopedia -deductive method (cf. 귀류법)\n예를 들어, 아리스토텔레스의 삼단논법의 논리 형식이 가장 많이 인용이 된다. 대전제와 소전제가 하나씩있는 둘 이상의 명제로부터 결론이 도출되는 예를 살펴보자.\n\n대전제: 모든 사람은 죽는다.\n소전제: 소크라테스는 사람이다.\n결론: 그러므로 소크라테스도 죽는다.\n\n\n\n\n귀납법 (Induction method or Inductive reasoning)은 전제와 결론을 뒷받침하는 논리에 의해 그 타당성이 평가된다. 귀납적 추론은 관찰과 실험에서 얻은 특수한 사례 (= data)를 근거로 전체에 적용시키는 귀납적 비약을 통해 이루어진다. 이와 같이 귀납에서 얻어진 결론은 일정한 개연성을 지닐 뿐이며, 특정 data에 따라 귀납적 추론의 타당성에 영향을 미친다. 그러므로, 검증된 data가 많을 수록 신뢰도와 타당성이 증가한다는 특징이 있다.하지만, 귀납적 추론의 결론이 진리인 것은 아니다. Source: naver encyclopedia - inductive method\n\n특수한 사례 (or data): 소크라테스는 죽었다, 플라톤도 죽었다, 아리스토텔레스도 죽었다.\n소전제: 소크라테스는 사람이다.\n결론: 그러므로 소크라테스도 죽는다.\n\n위와 같이 연역적 추론과 귀납적 추론은 서로 반대되는 개념으로 각 각 강점과 약점이 있으며 현실에서는 서로 상호 보완적으로 쓰이고 있다. 따라서, 전제로 삼는 대전제 역시 검증 과정이 필요하고 그 가설에서 몇 개의 명제를 연역해 실험과 관찰 등을 수행하는 가설연역법(hypothetical deductive method)이 널리 쓰이고 있다.\n\n\n\n\n통계학에선 모수(parameter)를 추정하는 여러 방법론들이 있는데 이번 블로그에서는 Frequentism와 Bayeseanism, 이 2가지 방법론에 초점을 둔다.\n\n\n통계학에서 가장 널리쓰이고 있는 방법론으로, 연역법에 근거한 결론 도출 방식을 이용한다. 간단히 말하면, 이미 알려진 분포에서 연구자의 관측치가 발생할 확률을 관찰하여 결론을 유도 하는 방법이다. p-value에 의한 결론 도출방식이 그 대표적인 예이다. 연구자의 데이터가 여러 수학자와 통계학자들이 증명해 놓은 분포하에서 발생한 사실이 입증이 됐을 때 연구자의 관측치가 그 named distribution(like normal distribution)에서 발생할 확률이 낮을 수록 p-value가 작아지고 일정 유의수준에 따라 연구자는 귀무가설을 기각하는 논리방식을 따른다.\n\n\n\n통계학에서 역시 많이 쓰이는 방법론으로, 귀납법에 근거한 결론 도출 방식을 이용한다. 간단히 말하면, 확률을 확률변수가 갖는 sample space에 대한 특정 사건이 발생한 사건의 비로 보는 것 (equally likely라고 가정)이 아니라 내가 설정한 가설에 대한 신뢰도로 바라보는 것이다. 따라서, 사전에 이미 알고있는 데이터가 있어 사전 확률 (prior probability)을 알고있고 이 사전 확률이 추가적인 data에 의해 조정되는 사후 확률 (posterior probability)이 계산된다. 이때 사전 확률자체보다는 추가적인 data와 사후 확률을 계산하는데 사용되는 likelihood의 타당성이 더 중요하다. 더 구체적인 내용은 Bayesean statistics에 기본이 되는 Bayes’ rule에서 살펴보기로 한다.\nsource: Frequentism vs Bayeseanism\n\n\n\n\n\n\nTheorem 1 \\[\n\\begin{aligned}\nP(A|B)&=\\frac{P(B|A)P(A)}{P(B)}\\\\\n      &=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\n\\end{aligned}\n\\tag{1}\\]\n\n\n\\(P(A|B)\\): posterior probability, B(data)가 주어졌을때 가설 A에 대한 신뢰도\n\\(P(A)\\): prior probability, 가설 A에대한 신뢰도\n\\(P(B|A)\\): likelihood, 가설 A가 주어졌을때 B(Data)가 발생할 신뢰도\n\\(P(B)\\): marginal probability, Data의 신뢰도\n\nEquation 1 의 두 분째 등식을 이해하기 위해선, Law of Total Probability (전 확률 법칙) 또는 Total Probability Rule (전 확률 정리)을 이해해야한다.\n\n\n\n\nTheorem 2 Let \\(A_1, A_2, ..., A_k\\) be a set of mutually exclusive and exhaustive events. Let \\(A\\) be a event and a partition of sample space \\(\\Omega\\), then\n\\[\nP(B)=\\sum_{i=1}^{n}P(B|A_i)P(A_i)\n\\]\n\n\n\n\n\n\nFigure 1: Law of Total Probability Example - Two Events\n\n\nSource: Law of ToTal Probability with Proof\n\n\n\n\nFigure 2: Law of Total Probability Example - Multiple Events\n\n\nSource: MIT RES.6-012 Introduction to Probability, Spring 2018 - Youtube\n\n\n\\[\n\\begin{aligned}\nP(B)&=P(B\\cap A) + P(B\\cap \\overline A)\\\\\n    &=P(B\\cap A) + P(B\\cap \\overline A)\\\\\n    &=P(B|A)P(A)+P(B|\\overline A)P(\\overline A)\\\\\nP(A\\cap B)&=P(B|A)P(A)=P(A|B)P(B)\\\\\n\\therefore\nP(A|B)&=\\frac{P(A \\cap B)}{P(B)}\\\\\n      &=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\n\\end{aligned}\n\\tag{2}\\]\nLaw of total probability 를 이용하여 Bayes’ rule이 Equation 2 와 같이 변형되었다. 최종식을 보면 좀 더 직관적인 해석이 가능해지는데 P(B) 가 A와의 교집합 확률의 총합이 되면서 분자가 그 일부가 되는 비율의 개념으로 해석될 수 있다. Figure 1 를 보면 \\(P(A|B)=\\frac{P(B \\cap A)}{P(B)}=\\frac{P(B \\cap A)}{P(B \\cap A)+P(B \\cap \\overline A)}\\) 로 표현되는 것을 볼 수 있다. 그 것을 조금 더 일반화 한 경우는 Figure 2 를 참고하여 유추할 수 있다.\n\n\n\n\\[\n\\begin{aligned}\nP(A|B)&=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\\\\\nP(\\theta|x)&=\\frac{P(x|\\theta)P(\\theta)}{P(x|\\theta)P(\\theta)+P(x|\\overline \\theta)P(\\overline \\theta)}\n\\end{aligned}\n\\]\n많은 참고 문헌에서 사건 A, B를 모수, \\(\\theta\\) 와 data, \\(x\\) 로 표현하기도 한다. 즉, data \\(x\\) 가 주어졌을 때 모수 \\(\\theta\\) 가 발생할 확률이 data에 의해서 update된다.\n\n\\(P(\\theta)\\)\n\nprior probability density function\n데이터없이 초기에 임시로 부여된 모델 또는 모수의 확률\n\n\\(P(x|\\theta)\\)\n\nlikelihood\n초기에 임시로 부여된 모델 또는 모수가 주어졌을 때 data x가 발생할 우도\n좀 더 파격적으로 해석하면, 초기에 임시로 부여된 모델 또는 모수가 data x에 들어맞을(or fittng) 확률\n\n\\(P(x)\\)\n\nmarginal proability\n데이터가 발생할 확률로 \\(\\theta\\) 와 상관없기 때문에 상수로 취급한다.\n\n\\(P(\\theta|x)\\)\n\nposterior probability density function\ndata가 주어졌을 때 모델 또는 모수의 확률\nBayes’ Rule에 의한 최적화에서 다음 최적화 iteration에서 Prior로 쓰인다.\n\n\n\\(P(x)\\) 는 상수이기 때문에 생략가능 하여 아래의 식과 같이 정리 할 수 있다. \\[\nP(\\theta|x)\\propto P(x|\\theta)P(\\theta)\n\\]\n\\(P(\\theta|x)\\) 는 \\(P(x|\\theta)P(\\theta)\\) 에만 영향을 받는 것을 볼 수 있다.\n\n\n\n펭수는 평소 관심이 있던 코니에게서 초콜릿을 선물받았다. 펭수는 초콜릿을 준 코니가 나를 좋아하는지가 궁금하기 때문에 이것을 통계적으로 계산해본다.\n펭수는 먼저 다음 두 상황을 가정한다.\n\n\\(P(like)=0.5\\)\n\n코니가 펭수를 좋아한다는 가설의 신뢰도는 반 반이다. 즉, 정보없는 상태에서의 펭수의 prior probability.\n0.5로 설정한 이유는 다음의 원리를 따랐다. The Principle of Insufficient Reason(이유불충분의 원리- 하나의 사건을 기대할만한 어떤 이유가 없는 경우에는 가능한 모든 사건에 동일한 확률을 할당해야 한다는 원칙).\n\n\\(P(choco)\\)\n\n초콜릿을 받았다라는 data가 발생할 신뢰도\n\n\n펭수는 코니에게 자신을 좋아하는지 알 길이 없으니 사람이 호감이 있을 때에 대한 초콜릿 선물 데이터를 조사하기 시작한다. 즉, 호감의 근거는 초콜릿으로 한정했고 초콜릿 선물 방식의 불확실성을 호감으로 설명하는 문헌을 찾기 시작했다. 그리고 펭수는 도서관에 있는 일반인 100명을 대상으로 초콜릿과 호감과의 관계를 연구한 초콜릿과 호감 논문을 통해 두 가지 정보를 알게된다.\n\n일반적으로, 어떤 사람이 상대방에게 호감이 있어서 초콜릿을 줄 확률은 \\(40%\\) 이다. 즉, \\(P(choco|like)=0.4\\)\n일반적으로, 어떤 사람이 상대방에게 호감이 없지만 예의상 초콜릿을 줄 확률은 \\(30%\\) 이다. 즉, \\(P(choco|\\overline{like})=0.3\\)\n위의 2가지 정보로 유추 가능한 정보\n\n\\(P(\\overline{choco}|like)=0.6\\)\n\\(P(\\overline{choco}|\\overline{like})=0.7\\)\n\n초콜릿에 관한 조사를 토대로 얻은 4가지 정보로 유추할 수 있는 정보\n\n\\(P(choco|like)=0.4\\): like를 받고 있는 50명 중 \\(40%\\) 인 20명은 초콜릿을 받는다.\n\\(P(\\overline{choco}|like)=0.6\\): like를 받고 있는 50명 중 \\(60%\\) 인 30명은 초콜릿을 받지 못한다.\n\\(P(choco|\\overline{like})=0.3\\): like를 받지 않는 50명 중 \\(30%\\) 인 15명은 예의상 준 초콜릿을 받는다.\n\\(P(\\overline{choco}|\\overline{like})=0.7\\): like를 받지 않는 50명 중 \\(70%\\) 인 35명은 초콜릿을 받지 못한다.\n\n\n펭수의 관점으로 정보를 재분류\n\n펭수가 궁금한 정보\n\n\\(P(like|choco)=?\\), posterior probability\n\n펭수가 가정한 정보\n\n\\(P(like)=0.5\\), prior probability by The Principle of Insufficient Reason\n\n펭수가 조사한 정보\n\n\\(P(choco|like)=0.4\\), likelihood\n\\(P(choco)\\): marginal probability\n\n\\(P(choco)=P(choco|like)+P(choco|\\overline{like})=\\frac{20+15}{100}=0.35\\)\n\n\n\n위의 정리한 정보를 Bayes’ rule에 대입하면,\n\\[\nP(like|choco)=\\frac{P(choco|like)\\times P(like)}{P(choco)}=\\frac{0.4\\times 0.5}{0.35}=0.57\n\\]\n펭수의 prior probability(\\(P(A)=0.5\\))가 posterior probability(\\(P(A|B)=0.57\\))로 업데이트 될 수 있다. 초콜릿과 호감 논문을 읽고 코니가 자신을 좋아할 확률이 높아진 것에 대해 기대감을 얻은 용기가 없는 펭수는 100명 보다 더 많은 독립적인 사람들로 실험한 논문을 찾아 다시 자신의 업데이트 된 사전 확률을 계속해서 업데이트할 생각이다. 그리고 자신의 사전 확률을 추가적인 데이터를 갖고 사후 확률로 계속해서 업데이트시켜 정확한 확률을 구한다.\n위의 예시는 영상 자료: 초콜릿을 준 코니의 마음을 시청하고 영감을 얻은 슬기로운 통계생활 tistory에 있는 Source: 베이즈 정리(Bayes’ rule) 완벽히 정리하기 슬기로운 통계생활 블로그를 요약 및 약간의 각색을 한 것이다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#generalized-bayes-rule",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#generalized-bayes-rule",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Theorem 3 Let \\(A_1, A_2, ..., A_k\\) be a set of mutually exclusive and exhaustive events. Let \\(A\\) be a event, then\n\\[\nP(A_i|B)=\\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^{k}P(B|A_i)P(A_i)}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#maximum-a-posterior-estimation-map",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#maximum-a-posterior-estimation-map",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Maximum a posterior estimation는 statistical estimation methods의 큰 기둥 중 하나인 maximum likelihood estimation과 더불어 parameter \\(\\theta\\) 를 추정하는데 많이 사용되는 방법론이다. 사후 확률 밀도 함수 \\(f(x|\\theta)\\) 또는 \\(P(x|\\theta)\\) 를 최대화하는 \\(\\theta\\) 의 추정치를 구하는 방법이며 아래와 같은 argument로 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\hat{\\theta}&=\\arg \\max_{\\theta}\\frac{f(x|\\theta)f(\\theta)}{\\int f(x|\\theta)f(\\theta)}\\\\\n            &\\propto\\arg \\max_{\\theta}f(x|\\theta)f(\\theta)\n\\end{aligned}\n\\]\n최대 우도 추정량과 달리 최대 사후 추정량에는 최대화하는 식에 사전 확률이 추가되어 있는 것을 볼 수 있다. 그러므로 분자 부분인 \\(f(x|\\theta)f(\\theta)\\) 만을 최대화 한다. 분모 부분인 \\(\\int f(x|\\theta)f(\\theta)\\) nomarlizing penalty 또는 constant로 간주한다. 여기서 \\(P(\\theta)\\) 초기 가정치 인데 아무렇게나 설정하기 보다는 good estimate로 설정해야 통계학자들로부터의 공격을 최소화시킬 수 있다. MAP는 나이브 베이즈의 알고리즘의 핵심이다.\n[참고] 최대 우도 추정량 \\[\n\\begin{aligned}\n\\hat{\\theta}=\\arg \\max_{\\theta}L(x|\\theta)=\\arg \\max_{\\theta}\\Pi_{i=1}^{n}f(x|\\theta)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#naive-bayes-classifier",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#naive-bayes-classifier",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Naive Bayes에 대한 구체적인 글은 다른 블로그에 소개한다. Naive Bayes는 Bayes’ Rule을 이용해 \\(\\theta\\) 를 최적화 시킨다. Naive Bayes의 Naive는 features 또는 explanotry variables이 서로 conditionally indepdent라고 가정한 것에서 이름 붙여졌다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#blog-guide-map-link",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html",
    "title": "MANOVA",
    "section": "",
    "text": "다변량 분산분석(Multivariate Analysis of Variance, MANOVA)\n\n2개 이상의 종속변수가 있을 경우 집단별 차이를 동시에 검정\n연구의 타당성 증가"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#description",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#description",
    "title": "MANOVA",
    "section": "",
    "text": "다변량 분산분석(Multivariate Analysis of Variance, MANOVA)\n\n2개 이상의 종속변수가 있을 경우 집단별 차이를 동시에 검정\n연구의 타당성 증가"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#example",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#example",
    "title": "MANOVA",
    "section": "2 Example",
    "text": "2 Example\n\n2.1 Load Libraries and Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(faraway)\nlibrary(markdown)\nlibrary(heplots)\nlibrary(HH)\nlibrary(psych)\n\n\n\n\n2.2 Data Description\n\n\nCode\nstr(Skulls)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ epoch: Ord.factor w/ 5 levels \"c4000BC\"&lt;\"c3300BC\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ mb   : num  131 125 131 119 136 138 139 125 131 134 ...\n $ bh   : num  138 131 132 132 143 137 130 136 134 134 ...\n $ bl   : num  89 92 99 96 100 89 108 93 102 99 ...\n $ nh   : num  49 48 50 44 54 56 48 48 51 51 ...\n\n\nR console에 ?Skulls를 입력하면 다음과 같은 설명이 나온다.\nMeasurements made on Egyptian skulls from five epochs.\n\nThe epochs correspond to the following periods of Egyptian history:\n\nthe early predynstic period (circa 4000 BC);\nthe late predynatic period (circa 3300 BC);\nthe 12th and 13t dynasties (circa 1850 BC);\nthe Ptolemiac peiod (circa 200 BC);\nthe Roman period(circa 150 AD).\n\n\nThe question is hether the measurements change over time. Non-constant measurements of the skulls over time would indicate interbreeding with immigrant populations. Note that using polynomial contrasts for epoch essentially treats the time points as equally spaced\n즉, skulls 고대 이집트 왕조 부터 로마시대까지 이집트 지역에서 발군된 두개골의 크기를 측정한 데이터 이집트 역사를 5개의 시대로 구분하고 각 시대별로 30개씩의 두개골을 4개의 지표로 측정\n이 data는 5개의 변수와 150개의 samples을 포함한다.\n\nepoch :\nmb :\nbh :\nbl :\nnh :"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#eda",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#eda",
    "title": "MANOVA",
    "section": "3 EDA",
    "text": "3 EDA\n\n3.1 Descriptive Statistics\n\n\nCode\nlibrary(heplots)\n#skulls 고대 이집트 왕조 부터 로마시대까지 이집트 지역에서 발군된 두개골의 크기를 측정한 데이터\n# 이집트 역사를 5개의 시대로 구분하고 각 시대별로 30개씩의 두개골을 4개의 지표로 측정\n# epoch: 이집트의 시대를 5개로 구분, 독립변수\n# mb : 두개골의 폭, 종속 변수\n# bh : 두개골의 높이, 종속 변수\n# bl : 두개골의 길이, 종속 변수\n# nh : 코의 높이, 종속 변수\n\nlibrary(dplyr)\nsample_n(Skulls,10)\n\n\n      epoch  mb  bh  bl nh\n19  c4000BC 139 136  96 50\n18  c4000BC 132 133  93 53\n111  c200BC 139 130  94 53\n36  c3300BC 135 136  98 52\n132  cAD150 141 136 101 54\n80  c1850BC 136 135  94 53\n138  cAD150 137 135  96 54\n133  cAD150 135 135  95 56\n22  c4000BC 135 135 103 47\n54  c3300BC 129 126  91 50\n\n\nCode\nattach(Skulls)# Skulls를 작업 경로에 포함시키기\nsearch() # 작업 경로 확인인\n\n\n [1] \".GlobalEnv\"           \"Skulls\"               \"package:psych\"       \n [4] \"package:HH\"           \"package:gridExtra\"    \"package:multcomp\"    \n [7] \"package:TH.data\"      \"package:MASS\"         \"package:survival\"    \n[10] \"package:mvtnorm\"      \"package:latticeExtra\" \"package:grid\"        \n[13] \"package:lattice\"      \"package:heplots\"      \"package:broom\"       \n[16] \"package:markdown\"     \"package:faraway\"      \"package:lubridate\"   \n[19] \"package:forcats\"      \"package:stringr\"      \"package:dplyr\"       \n[22] \"package:purrr\"        \"package:readr\"        \"package:tidyr\"       \n[25] \"package:tibble\"       \"package:ggplot2\"      \"package:tidyverse\"   \n[28] \"package:stats\"        \"package:graphics\"     \"package:grDevices\"   \n[31] \"package:utils\"        \"package:datasets\"     \"package:methods\"     \n[34] \"Autoloads\"            \"package:base\"        \n\n\nCode\n# 종속 변수를 결합시켜 하나의 행렬로 만들기\ny&lt;-cbind(mb,bh,bl,nh)\n# 시대별 두개골  길이의 평균 보기\naggregate(y,by=list(epoch),mean) # 언뜻 보기에 차이가 있는 것 처럼 보임\n\n\n  Group.1       mb       bh       bl       nh\n1 c4000BC 131.3667 133.6000 99.16667 50.53333\n2 c3300BC 132.3667 132.7000 99.06667 50.23333\n3 c1850BC 134.4667 133.8000 96.03333 50.56667\n4  c200BC 135.5000 132.3000 94.53333 51.96667\n5  cAD150 136.1667 130.3333 93.50000 51.36667\n\n\nCode\n# 모집단으로 일반화하기 위해 통계적 검정 시행\nskulls_manova&lt;-manova(y~epoch)\nsummary(skulls_manova)\n\n\n           Df  Pillai approx F num Df den Df    Pr(&gt;F)    \nepoch       4 0.35331    3.512     16    580 4.675e-06 ***\nResiduals 145                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# 시대별 두개골 측정값이 차이가 있는 것으로 보임\n\n# 구체적으로 어느 두개 골 측정값에서 차이가 나는지 확인\nsummary.aov(skulls_manova)\n\n\n Response mb :\n             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nepoch         4  502.83 125.707  5.9546 0.0001826 ***\nResiduals   145 3061.07  21.111                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response bh :\n             Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nepoch         4  229.9  57.477  2.4474 0.04897 *\nResiduals   145 3405.3  23.485                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response bl :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nepoch         4  803.3 200.823  8.3057 4.636e-06 ***\nResiduals   145 3506.0  24.179                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response nh :\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nepoch         4   61.2  15.300   1.507 0.2032\nResiduals   145 1472.1  10.153               \n\n\nCode\n# nh는 차이가 없는 것으로 보임\n\n## 시간에 따라 두개골 측정이 다르다는 것은 이민족 유입의 혼혈 가능성이 있음\n\ndetach(Skulls)# 작업경로에서 삭제제\n\n\n\n\n3.2 One-Way ANOVA"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#blog-guide-map-link",
    "title": "MANOVA",
    "section": "4 Blog Guide Map Link",
    "text": "4 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html",
    "href": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html",
    "title": "Conditional Probability",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n조건부 확률은 조건이 주어졌을 때 한 사건이 발생할 확률이며 하나의 사건이 다른 사건에 영향을 미쳐 그 확률값이 달라지는 것을 의미한다. 즉, 한 사건, B가 조건으로 주어지고 다른 사건A가 발생할 확률 \\(P(A|B)\\) 가 사건 A가 발생할 확률 \\(P(A)\\) 와 다르다는 것을 의미한다 (\\(P(A)\\not=P(A|B)\\)).\n예를 들어, 주사위를 던질 때 특정 주사위의 눈 (1~6)이 나올 확률은 \\(\\frac{1}{6}\\) 으로 같다 (eqaully likely)라고 가정할 때 주사위의 눈이 나올 수 있는 모든 집합 표본 공간 \\(S\\) 에 대한 특정 주사위의 눈이 나오는 사건 \\(A\\) 가 발생할 확률은 \\(\\frac{n(A)}{n(S)}\\) 와 같다. 다시 말해서, 조건부 확률은 2개 이상의 사건에 대해서 하나의 사건이 다른 사건이 발생할 확률에 영향을 미치는 개념을 말한다. 가장 간단한 2개의 사건 \\(A, B\\) 에 대해서 살펴볼 때 조건부 확률은 다음과 (Equation 1)과 같다.\n\nDefinition 1 If \\(A\\) and \\(B\\) are events in sample space \\(S\\), and \\(P(B)&gt;0\\), then the conditional probability of \\(A\\) given \\(B\\), written \\(P(A|B)\\), is \\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n\\tag{1}\\]\nwhere \\(0 &lt; P(B) \\le 1\\)\n\n위의 정의에서 볼 수 있듯이, sample space \\(S\\) 가 B로 update 되어 P(B|B)=1이 되고 사건 A의 outcome이 B에 관해서 조정된다.\n예를 들어, 사건 \\(A\\) 는 주사위의 눈이 1이 나오는 사건, 사건 \\(B\\) 는 주사위의 눈이 3 이하가 나오는 사건이라고 했을 때 사건 \\(A\\) 가 사건 \\(B\\) 의 부분 집합이므로 두 사건이 서로 독립이 아니다. 즉, $ AB $ 사건에서 주사위의 눈이 1 나오는 경우 밖에 없다. 이렇게 사건 \\(B\\) 가 주어졌을 때 혹은 \\(B\\) 가 먼저 일어났을 때 1이 나올 확률은 달라지게 된다. 즉, \\(A\\) 의 sample space = \\(\\{1,2,3,4,5,6\\}\\) 이고 \\(A|B\\) 의 sample space = \\(\\{1,2,3\\}\\) 이 되기 때문에 \\(P(A) \\not= P(A|B)\\) 가 된다.\n좀 더 구체적으로 계산을 하게 되면, \\(P(A)=\\frac{1}{6}, P(B)=\\frac{3}{6}, P(A\\cap B)=\\frac{1}{6}\\) 일 때,\n\\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}=\\frac{\\frac{1}{6}}{\\frac{3}{6}}=\\frac{1}{3}\n\\]\n인 것을 알 수 있다.\n\nDefinition 2 Independence \\[\n\\begin{aligned}\nP(A)&=P(A|B)\\\\\nP(B)&=P(B|A)\\\\\n\\end{aligned}\n\\tag{2}\\]\nA and B are independent.\n\n사건 A, B가 독립일 때 두 두사건이 동시에 발생할 확률은 \\(P(A \\cap B)=P(A)P(B)=P(B)P(A)\\) 이다. 반면에, 두 사건이 독립이 아니라면 Equation 1 을 이용하여 \\(P(A \\cap B)\\) 는 \\(P(B|A)P(A)\\) 또는 \\(P(A|B)P(B)\\) 로 표현될 수 있다.독립일 때 동시에 발생할 확률에서 \\(P(A)\\) 가 B를 조건으로 봤을 때 동시에 발생할 확률 \\(P(A|B)\\) 로 바뀐 것을 볼 수 있다.\n\nTheorem 1 Multiplicative Rule A,B dependent \\[\n\\begin{aligned}\nP(A\\cap B)&=P(A)P(B|A)\n\\end{aligned}\n\\]\nA,B independent \\[\n\\begin{aligned}\nP(A\\cap B)&=P(A)P(B)\n\\end{aligned}\n\\]\n\n\nTheorem 2 Generalized multiplicative rule $$\n$$\n\n\nTheorem 3 Total Probability Rule $$\n$$\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html#conditional-probability",
    "href": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html#conditional-probability",
    "title": "Conditional Probability",
    "section": "",
    "text": "조건부 확률은 조건이 주어졌을 때 한 사건이 발생할 확률이며 하나의 사건이 다른 사건에 영향을 미쳐 그 확률값이 달라지는 것을 의미한다. 즉, 한 사건, B가 조건으로 주어지고 다른 사건A가 발생할 확률 \\(P(A|B)\\) 가 사건 A가 발생할 확률 \\(P(A)\\) 와 다르다는 것을 의미한다 (\\(P(A)\\not=P(A|B)\\)).\n예를 들어, 주사위를 던질 때 특정 주사위의 눈 (1~6)이 나올 확률은 \\(\\frac{1}{6}\\) 으로 같다 (eqaully likely)라고 가정할 때 주사위의 눈이 나올 수 있는 모든 집합 표본 공간 \\(S\\) 에 대한 특정 주사위의 눈이 나오는 사건 \\(A\\) 가 발생할 확률은 \\(\\frac{n(A)}{n(S)}\\) 와 같다. 다시 말해서, 조건부 확률은 2개 이상의 사건에 대해서 하나의 사건이 다른 사건이 발생할 확률에 영향을 미치는 개념을 말한다. 가장 간단한 2개의 사건 \\(A, B\\) 에 대해서 살펴볼 때 조건부 확률은 다음과 (Equation 1)과 같다.\n\nDefinition 1 If \\(A\\) and \\(B\\) are events in sample space \\(S\\), and \\(P(B)&gt;0\\), then the conditional probability of \\(A\\) given \\(B\\), written \\(P(A|B)\\), is \\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n\\tag{1}\\]\nwhere \\(0 &lt; P(B) \\le 1\\)\n\n위의 정의에서 볼 수 있듯이, sample space \\(S\\) 가 B로 update 되어 P(B|B)=1이 되고 사건 A의 outcome이 B에 관해서 조정된다.\n예를 들어, 사건 \\(A\\) 는 주사위의 눈이 1이 나오는 사건, 사건 \\(B\\) 는 주사위의 눈이 3 이하가 나오는 사건이라고 했을 때 사건 \\(A\\) 가 사건 \\(B\\) 의 부분 집합이므로 두 사건이 서로 독립이 아니다. 즉, $ AB $ 사건에서 주사위의 눈이 1 나오는 경우 밖에 없다. 이렇게 사건 \\(B\\) 가 주어졌을 때 혹은 \\(B\\) 가 먼저 일어났을 때 1이 나올 확률은 달라지게 된다. 즉, \\(A\\) 의 sample space = \\(\\{1,2,3,4,5,6\\}\\) 이고 \\(A|B\\) 의 sample space = \\(\\{1,2,3\\}\\) 이 되기 때문에 \\(P(A) \\not= P(A|B)\\) 가 된다.\n좀 더 구체적으로 계산을 하게 되면, \\(P(A)=\\frac{1}{6}, P(B)=\\frac{3}{6}, P(A\\cap B)=\\frac{1}{6}\\) 일 때,\n\\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}=\\frac{\\frac{1}{6}}{\\frac{3}{6}}=\\frac{1}{3}\n\\]\n인 것을 알 수 있다.\n\nDefinition 2 Independence \\[\n\\begin{aligned}\nP(A)&=P(A|B)\\\\\nP(B)&=P(B|A)\\\\\n\\end{aligned}\n\\tag{2}\\]\nA and B are independent.\n\n사건 A, B가 독립일 때 두 두사건이 동시에 발생할 확률은 \\(P(A \\cap B)=P(A)P(B)=P(B)P(A)\\) 이다. 반면에, 두 사건이 독립이 아니라면 Equation 1 을 이용하여 \\(P(A \\cap B)\\) 는 \\(P(B|A)P(A)\\) 또는 \\(P(A|B)P(B)\\) 로 표현될 수 있다.독립일 때 동시에 발생할 확률에서 \\(P(A)\\) 가 B를 조건으로 봤을 때 동시에 발생할 확률 \\(P(A|B)\\) 로 바뀐 것을 볼 수 있다.\n\nTheorem 1 Multiplicative Rule A,B dependent \\[\n\\begin{aligned}\nP(A\\cap B)&=P(A)P(B|A)\n\\end{aligned}\n\\]\nA,B independent \\[\n\\begin{aligned}\nP(A\\cap B)&=P(A)P(B)\n\\end{aligned}\n\\]\n\n\nTheorem 2 Generalized multiplicative rule $$\n$$\n\n\nTheorem 3 Total Probability Rule $$\n$$"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html#blog-guide-map-link",
    "title": "Conditional Probability",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-21_transformation/index.html",
    "href": "docs/blog/posts/statistics/2023-02-21_transformation/index.html",
    "title": "Transformation of Random Variables",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n확률 변수 2개 이상에 대한 확률 분포를 joint probability distribution (결합확률분포)라고 하는데 두 확률 변수 \\(X\\) 와 \\(Y\\) 의 관계에 대해서 규명해야 할 때가 있다. 예를 들어, X 와 Y의 높은 상관계수라든지 또는 비선형적인 관계가 관찰될 때 그 관계가 수리적으로 모델링이 가능하고 한 확률 변수의 분포에 대한 정보를 알고있다면 미지의 다른 확률 변수의 분포가 추정가능해진다. 이 때 두 변수에 대한 관계 정도가 높으면 높을수록 추정이 쉬워진다.\n이번 블로그에서는 주어진 확률 변수 \\(X\\) 에 대해서 \\(X\\) 의 pmf (probability mass function) 또는 pdf (probability density function) \\(f_x(x)\\) 를 알고있을 때 확률 변수에 \\(X\\) 에 적절한 함수의 변환을 적용해 확률 변수 \\(Y\\) 를 \\(Y=u(X)\\) 라는 관계식이 정의 가능할 때 \\(Y\\) 의 pmf 또는 pdf를 구하는 방법에 집중한다. 후에 MGF (Momment Generating Function) 학습에 응용될 수 있는 개념으로 잘 정리할 필요가 있다.\n\n\n\n\nTheorem 1 Discrete random variable \\(X\\) 의 probability distribution이 \\(f_X(x)\\) 이고 \\(X\\) 와 \\(Y\\) 사이에는 \\(Y=u(X)\\) 라는 one-to-one relation이 성립될 때 \\(y=u(x)\\) 를 유일한 \\(x\\) 를 \\(y\\) 에 대한 함수인 \\(x=w(y)\\) 로 표현 가능하다면 \\(Y\\) 의 probability distribution는 \\[\nf_Y(y)=f_X(w(y))\n\\] 이다.\n\n\n\n\nIn the case of a One-to-One relation\n\n동전을 독립적으로 2번 던질 때, 확률 변수 \\(X\\) 가 앞면이 나오는 수라고 정의했을 때, 확률 분포 아래 표 (a)와 같다. \\(Y=2X+1\\) 라는 관계가 성립할 때 \\(Y\\) 의 분포는 아래 표 (b)와 같다.\n\n\nTable 1: Exmaple: Transformation of Discrete Random Variable (One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\\(X\\)\n0\n1\n2\n\n\n\n\n\\(P_X(X=x)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y=2X+1\\)\n\n\n\\(Y=2X+1\\)\n1\n3\n5\n\n\n\n\n\\(P_Y(Y=y)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n\nIn the case of not a One-to-One relation\n\n동전을 독립적으로 2번 던질 때, 확률 변수 \\(X\\) 가 앞면이 나오는 수의 합이라고 정의 했을때, \\(X\\) 의 확률 분포는 아래 표 (a) 와 같다. 이 때 \\(Y=mod(X,2)\\) 라는 관계가 성립할 때 \\(Y\\) 의 분포는 아래 표 (b) 같다.\n\n\nTable 2: Exmaple: Transformation of Discrete Random Variable (Not One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\\(X\\)\n0\n1\n2\n\n\n\n\n\\(P_X(X=x)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y=mod(x,2)\\)\n\n\n\\(Y=mod(x,2)\\)\n0\n1\n\n\n\n\n\\(P_Y(Y=y)\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{2}{4}\\)\n\n\n\n\n\n\n위의 예시와 같이 두 확률 변수가 one to one 관계일 때는 확률분포가 그대로 유지되어서 쉽게 변환된 확률 변수의 분포가 추정가능하지만 one to one 관계가 아닐 경우 확률 분포가 바뀌게 된다.\n\nanother example (In the case of not a One-to-One relation)\n\n기하분포 (Geometric Distribution)란 동일한 베르누이 (Bernoulli) 분포의 시행을 독립적으로 반복할 때 첫 성공까지의 시행 횟수를 확률변수 \\(X\\) 로 하는 분포이다. 즉, \\(x-1\\) 번째까지 베르누이 시행이 실패하고 \\(x\\) 번째 시행에서 성공할 확률 분포를 말한다.\nNotation은 \\(X \\sim Geometric(p)\\) 또는 \\(X \\sim Geo(p)\\) 라고 표현하고, \\(p\\) 는 독립 시행에서 성공할 확률이다. (참고: \\(text{E}(X)=\\frac{1}{p}\\), \\(\\text{Var}(X)=\\frac{1-p}{p^2}\\))\n\\(X \\sim Geometric(\\frac{4}{5})\\) 일 때 \\(X\\) 의 확률분포 \\(f(x)=\\frac{4}{5}(\\frac{1}{5})^{(x-1)}\\), \\(x=1,2,3 ...\\) 가 기하분포일 때 \\(Y=X^2\\) 의 확률분포는\n\\[\n\\begin{aligned}\n  y&=u(x)=x^2 \\\\\n  x&=w(y)=\\sqrt{y} \\text{ } (x&gt;0)\\\\\n  f_Y(y)&=f_X(w(y))=f_X(\\sqrt{y})=\\frac{4}{5}(\\frac{1}{5}^{(\\sqrt{y}-1)})\\\\\n  \\therefore f_Y(y)&=\n  \\begin{cases}\n    \\frac{4}{5}(\\frac{1}{5}^{(\\sqrt{y}-1)}) &\\text{if} \\text{  } y =1, 4, 9, ...\\\\\n     0 \\text{  } & \\text{otherwise}\n    \\end{cases}\n\\end{aligned}\n\\]\n이다.\n\n\n\n\n\nTheorem 2 Two discrete random variables \\(X_1\\) 과 \\(X_2\\) 의 joint probability distribution이 \\(f(x_1,x_2)\\) 이고 \\((x_1,x_2)\\) 와 \\((y_1,y_2)\\) 가 one-to-one relation이 성립되어 \\(y_1=u_1(x_1,x_2)\\) 와 \\(y_2=u_2(x_1,x_2)\\) 를 유일한 \\(x_1=w_1(y_1,y_2)\\) 와 \\(x_2=w_2(y_1,y_2)\\) 의 함수로 표현 가능하다면 새로운 확률변수 \\(\\mathbf{Y}\\), 즉, \\(Y_1\\) 과 \\(Y_2\\) 의 joint probability distribution는 \\[\nf_Y(y_1,y_2)=f_X(w_1(y_1,y_2),w_2(y_1,y_2))\n\\] 이다.\n\n\n\n\nexmaple 1\n\n다항 분포(multinomial distribution)란 2개 이상의 독립적인 확률 변수 \\(\\mathbf{X}=X_1, X_2, ...\\) 들에 대한 확률분포로, 여러 독립 시행에서 각 각의 값이 특정 횟수가 나타날 확률을 정의하고 독립 변수가 2개인 경우 다항 분포의 특별한 case로 이항 분포 (binomial distribution)가 된다.\n참고( Source: wiki) :\n\\[\n\\begin{aligned}\n  f_(x) & =\\frac{n!}{x_1!x_2!\\dots x_k!}p_1^{x_1}p_2^{x_2}\\dots p_k^{x_k}\\\\\n  \\text{E}(x)&=np_i\\\\\n  \\text{Var}(x)&=np_i(1-p_i)\n\\end{aligned}\n\\]\n\\(X_1\\) and \\(X_2\\) 가 다음과 같은 multinomial distribution을 따를 때 \\[\nf(x_1,x_2) = \\binom{2}{x_1,x_2,2-x_1-x_2}\\frac{1}{4}^{x_1}\\frac{1}{3}^{x_2}\\frac{5}{12}^{2-x_1-x_2} x_1=0,1,2, \\space x_2=0,1,2, \\space x_1+x_2\\le 2\n\\]\n\\(Y_1=X_1+X_2\\) 와 \\(Y_2=X_1-X_2\\) 의 결합 확률 분포는\n\\[\n\\begin{aligned}\n  y_1&=u_1(x_1,x_2)=x_1+x_2\\\\\n  y_2&=u_2(x_1,x_2)=x_1-x_2\\\\\n  x_1&=w_1(y_1,y_2)=\\frac{y_1+y_2}{2}\\\\\n  x_2&=w_2(y_1,y_2)=\\frac{y_1-y_2}{2}\\\\\n  f_Y(y_1,y_2)&=\\binom{2}{\\frac{y_1+y_2}{2},\\frac{y_1-y_2}{2},2-y_1 }\\frac{1}{4}^{\\frac{y_1+y_2}{2}}\\frac{5}{12}^{2-y_1} y_1=0,1,2, \\space y_2=-2,-1,0,1,2\n\\end{aligned}\n\\] 이다.\n\nexmaple 2\n\n\\(X_1\\) 과 \\(X_2\\) 의 결합확률분포가 다음과 같을 때 변환된 \\(Y_1=X_1+X_2\\) 와 \\(Y_2=X_1X_2\\) 의 결합확률 분포는?\n\n\nTable 3: Exmaple: Transformation of Joint Discrete Random Variable (Not One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\n\n\n\n\n\n\n\\((x_1,x_2)\\)\n(0, 0)\n(0, 1)\n(1, 0)\n(1, 1)\n\n\n\n\n\\(f_X(x_1,x_2)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y\\)\n\n\n\\((y_1,y_2)\\)\n(0, 0)\n(1, 0)\n(2, 1)\n\n\n\n\n\n\\(f_Y(y_1,y_2)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 3 continuous random variable \\(X\\) 의 probability distribution이 \\(f_X(x)\\) 이고 \\(X\\) 와 \\(Y\\) 사이에는 \\(Y=u(X)\\) 라는 one-to-one relation이 성립될 때 \\(y=u(x)\\) 를 유일한 \\(x\\) 를 \\(y\\) 에 대한 함수인 \\(x=w(y)\\) 로 표현 가능할 때 Y의 확률 분포는 \\[\nf_Y(y)=f_X(w(y))|J|\n\\] 이다. (\\(J=w'(y)\\), called ‘Jacobian’)\n\n\n\n\nIn the case of a One-to-One relation\n\n지수분포(exponential distribution)는 연속 확률 분포 중 하나로, 사건이 서로 독립적일 때, 일정 시간동안 발생하는 사건의 횟수가 푸아송 분포를 따른다면, 다음 사건이 발생할 때까지의 대기 시간을 확률 변수 \\(X \\in [0, \\infty)\\) 로 하는 분포이다. (Source: Wiki).\n참고: \\[\n\\begin{aligned}\n  X&\\sim Exp(\\lambda) \\\\\n  \\text E[X] &= \\frac{1}{\\lambda}\\\\\n  \\text{Var}[X] &= \\frac{1}{\\lambda^2}\\\\\n  f(x;\\lambda)&=\n  \\begin{cases}\n    \\lambda e^{-\\lambda x} & x \\ge 0\\\\\n    0 & x&lt;0\n  \\end{cases}\\\\\n\\end{aligned}\n\\] \\(\\lambda&gt;0\\) is the parameter of the distribution, called ‘rate parameter’.\nTransformation of Linear Function. When two random variables \\(X \\sim \\text{exp}(\\lambda)\\) and \\(Y\\) has the relation, \\(Y=aX+b\\) \\((a\\ne 0)\\). When \\(f_X(x)=\\lambda e^{-\\lambda x}\\), \\(f_Y(y)\\) is\nBy Theorem\n\\[\n\\begin{aligned}\n  Y&=u(X)=aX+b\\\\\n  y&=u(x)=ax+b\\\\\n  x&=w(y)=\\frac{y-b}{a}\\\\\n  f_Y(y)&=f_X(w(y))|J|=f_X(\\frac{y-b}{a})|J|\\\\\n        &=f_X(\\frac{y-b}{a})|w'(y)|\\\\\n        &=f_X(\\frac{y-b}{a})|\\frac{1}{a}|\\\\\n        &=\\frac{f_X(\\frac{y-b}{a})}{|a|}\\\\\n\\end{aligned}\n\\]\nBy Definition\n\n\n\n\\(a&gt;0\\) \\[\n\\begin{aligned}\nF_Y(Y)&=P_Y(Y\\le y)\\\\\n      &=P_Y(aX+b\\le y)\\\\\n      &=P_Y(X\\le \\frac{y-b}{a})\\\\\n      &=F_X(\\frac{y-b}{a})\\\\\nF'_Y(Y)&=\\frac{d}{dy}F_X(\\frac{y-b}{a})\\\\\n       &=f_X(\\frac{y-b}{a})\\frac{1}{a}\\\\\n\\end{aligned}\n\\]\n\n\n\n\\(a\\le 0\\) \\[\n\\begin{aligned}\nF_Y(Y)&=P_Y(Y\\le y)\\\\\n      &=P_Y(aX+b\\le y)\\\\\n      &=P_Y(X&gt; \\frac{y-b}{a})\\\\\n      &=1-F_X(\\frac{y-b}{a})\\\\\nF'_Y(Y)&=\\frac{d}{dy}(1-F_X(\\frac{y-b}{a}))\\\\\n       &=-f_X(\\frac{y-b}{a})\\frac{1}{a}\\\\  \n\\end{aligned}\n\\]\n\n\\[\n\\therefore f_Y(y)=\\frac{f_X(\\frac{y-b}{a})}{|a|}=\\frac{\\lambda exp(-\\lambda (\\frac{y-b}{a}))}{|a|}\n\\]\n\n위의 정리를 유도를 하진 않았지만 증명을 해보면 사실, \\(|J|\\) 는 역함수의 미분의 계수인 것을 할 수 있다. 위의 예시를 By definition 으로 푼 것을 보면 \\(y=u(x)\\) 와 \\(x=w(y)\\) 의 관계인 것을 알 수 있고 \\(|J|\\) 에 해당되는 \\(\\frac{1}{a}\\) 는 CDF \\(F(Y)\\) 의 derivative를 구하는 과정에서 발생하는 chain rule에 의해 생긴 것을 알 수 있다.\n\\[\n\\begin{aligned}\n  y&=u(x)\\\\\n  \\frac{dy}{dx}&=u'(x)\\\\\n  x&=w(y)=u^{-1}(y)\\\\\n  \\frac{dx}{dy}&=w'(y) \\\\\n  w'(y)&=\\frac{dx}{dy}=(\\frac{dy}{dx})^{-1}=\\frac{1}{u'(x)}\n\\end{aligned}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 20, 1000)\ny = 2*x+1\ny2 = x/2+1\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,x,color='black',label=r'$y=x$', linestyle='dashed')\nax.plot(x,2*x-2,color='black',label=r'$x=w(y) \\rightarrow y=2x-2 $')\nax.plot(x,y2,color='red',label=r'$y=u(x)=\\frac{1}{2}x+1$')\nax.set_aspect(1)\nax.set_title(r\"Example of Relation of X and Y, $y=u(x)$ vs $x=w(y)$\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nax.set_xlim(-20, 40)\n\nplt.legend(shadow=True, loc=(-0.5, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 20, 1000)\ny = 2*x+1\ny2 = x/2+1\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,y,color='black',label=r'$y=2x+1$')\nax.plot(x,y2,color='red',label=r'$y=\\frac{1}{2}x+1$')\nax.set_aspect(1)\nax.set_title(r\"Transformation of Linear Function, $f_Y(y)=\\frac{f_X(\\frac{y-b}{a})}{|a|}$\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y=u(X)\")\nax.set_xlim(-20, 40)\nax.text(10, 0,'|', color='black', horizontalalignment='center', verticalalignment='center')\nax.text(10, -5, r'$\\frac{y-b}{a}$', horizontalalignment='center',color='black')\nax.text(0, 5,'--', color='black', horizontalalignment='center', verticalalignment='center')\nax.text(-9, 5, r'$ax+b$', verticalalignment='center',color='black')\n\nplt.legend(shadow=True, loc=(-0.5, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\\(P(Y\\le y)=P(X\\le \\frac{y-b}{a})\\) 이기 때문에 \\(X\\) 의 특정 구간의 확률과 대응되는 \\(Y\\) 의 구간의 확률이 같다고 했을 때 \\(Y \\le y\\) 의 범위는 \\(X\\) 를 \\(b\\) 만큼 이동한 후 \\(a\\) 배한 \\(x\\) 좌표 이하 구간에 대응 된다 즉, \\(X\\le \\frac{y-b}{a}\\). 이 때, 두 구간에서의 확률 밀도가 같아야 하기 때문에 \\(f_x(x)\\) 가 \\(f_Y(y)\\) 의 \\(\\frac{1}{a}\\) 배 인 것을 알 수 있다.\n위의 그림에서 처럼, 두 확률 변수 \\(X\\) 와 \\(Y\\) 의 관계를 \\(y = 2x+1\\) \\((a&gt;1)\\) 와 \\(y = \\frac{1}{2}x+1\\) \\((0 \\le a\\le 1)\\) 로 두 가지의 예로 들면, 위의 그래프의 경우, \\(y = 2x+1\\) \\((a&gt;1)\\) 일 때 \\(0&lt;X&lt;10\\) 는 \\(1&lt;Y&lt;21\\) 의 구간이 2배가 되는 것을 관찰 할 수 있다. 반대로, \\(y = \\frac{1}{2}x+1\\) \\((0 \\le a\\le 1)\\) 일 때 \\(0&lt;X&lt;10\\) 는 \\(1&lt;Y&lt;6\\) 의 구간이 \\(\\frac{1}{2}\\) 배가 된다. 이 때 각 예시의 경우에 \\(X\\) 와 \\(Y\\) 의 확률 값이 같기 때문에 (\\(P(Y\\le y)=P(X\\le \\frac{y-1}{2})\\) 와 \\(P(Y\\le y)=P(X\\le \\frac{y-1}{\\frac{1}{2}})\\)). \\(f_x(x)\\) 가 \\(f_Y(y)\\) 의 각 각 \\(2\\) 배와 \\(\\frac{1}{2}\\) 배 인 것을 추정 할 수 있다.\n\nIn the case of not a One-to-One relation\n\n\\(Y=X^2\\) 일 때,\n\\[\n\\begin{aligned}\n  F_Y(Y)&=P_Y(Y\\le y)\\\\\n        &=P_Y(X^2\\le y)\\\\\n        &=P_Y(-\\sqrt{y}\\le X \\le \\sqrt y)\\\\\n        &=F_X(\\sqrt{y})-F_X(-\\sqrt{y})\\\\\n  F'_Y(Y)&=\\frac{d}{dy}(F_X(\\sqrt{y})-F_X(-\\sqrt{y}))\\\\\n        &=f_X(\\sqrt{y})\\frac{1}{2\\sqrt{y}}+f_X(-\\sqrt{y})\\frac{1}{2\\sqrt{y}}\\\\\n        &=\\frac{f_X(\\sqrt{y})}{2\\sqrt{y}}+\\frac{f_X(-\\sqrt{y})}{2\\sqrt{y}}\\\\\n\\end{aligned}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 1000)\ny = x*x\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,y,color='black',label=r'$y=x^2$')\n#ax.text(3.0,0.5,r'$\\sqrt{y}$')\n#ax.text(-4,0.5,r'$-\\sqrt{y}$')\nax.set_xlim([-10, 10])\nax.set_ylim([0, 10])\n\nax.set_aspect(1)\nax.set_title(r\"Transformation of Random Variables, $y=x^2$\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\n\nplt.legend(shadow=True, loc=(-0.2, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\\(X\\) 의 분포를 알고 \\(X\\) 와 \\(Y\\) 의 relation을 알고있을 때 \\(X\\) 의 분포로부터 \\(Y\\) 의 분포를 추정 가능한 것의 이점 중 하나는 \\(Y\\) 의 통계량을 \\(Y\\) 의 분포를 사용하지 않고 계산해낼 수 있다는 점이다.\n예를 들어, \\(X \\sim N(0,1)\\) 이고 \\(Y=u(X)\\) 라는 관계를 알고있을 때, \\(Y\\) 의 통계량 \\(E(Y)\\), \\(Var(Y)\\) 를 알고싶다면 굳이 힘들게 \\(Y\\) 의 분포를 계산할 필요가 없다. 이미 \\(X\\) 의 확률 분포 \\(f_X(x)\\) 를 알고있기 때문에 아래와 같이 \\(Y\\) 의 통계량을 계산해낼 수 있다.\n\\[\n\\begin{aligned}\n  \\text{E}(Y)&=\\int Y f_Y(y) dy\\\\\n            &=\\int u(X) f_X(x) dx\\\\\n  \\text{E}(Y^2)&=\\int Y^2 f_Y(y) dy\\\\\n            &=\\int u(X)^2 f_X(x) dx\\\\\n  \\text{Var}(Y)&=\\text{E}(Y^2)-\\text{E}(Y)^2\\\\\n  &=\\text{E}(g(X)^2)-\\text{E}(g(X))^2\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\nTheorem 4 Two continuous random variable \\(X_1\\) 과 \\(X_2\\) 의 joint probability distribution이 \\(f(x_1,x_2)\\) 이고 \\((x_1,x_2)\\) 와 \\((y_1,y_2)\\) 가 one-to-one relation이 성립될 때 \\(y_1=u_1(x_1,x_2)\\) 와 \\(y_2=u_2(x_1,x_2)\\) 를 유일한 \\(x_1=w_1(y_1,y_2)\\) 와 \\(x_2=w_2(y_1,y_2)\\) 의 함수로 표현 가능할 때 \\(Y_1=u_1(X_1,X_2)\\) 와 \\(Y_2=u_2(X_1,X_2)\\) 로 정의되는 새로운 확률변수 \\(Y_1\\) 과 \\(Y_2\\) 의 joint probability distribution는 \\[\ng(y_1.y_2)=f(w_1(y_1,y_2),w_2(y_1,y_2)) |J|\n\\] 이다.\n여기서, \\[\n\\begin{aligned}\n  |J|=|\n  \\begin{bmatrix}\n  \\frac{\\partial x_1}{\\partial y_1}&\\frac{\\partial x_1}{\\partial y_2}\\\\\n  \\frac{\\partial x_2}{\\partial y_1}&\\frac{\\partial x_2}{\\partial y_2}\\\\\n  \\end{bmatrix}| = | \\begin{bmatrix}\n  \\frac{\\partial w_1(y_1,y_2)}{\\partial y_1}&\\frac{\\partial w_1(y_1,y_2)}{\\partial y_2}\\\\\n  \\frac{\\partial w_2(y_1,y_2)}{\\partial y_1}&\\frac{\\partial w_2(y_1,y_2)}{\\partial y_2}\\\\\n  \\end{bmatrix} |\n\\end{aligned}\n\\] called ‘the determinant of jacobian matrix’\n\n\n\n\n예제\n\n연속확률변수 \\(X_1\\) 과 \\(X_2\\) 는 결합확률분포 \\[\nf(x_1,x_2)=\n  \\begin{cases}\n    4x_1x_2, & \\text{if  } 0&lt;x_1&lt;1,& 0&lt; x_2 &lt;1 \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\] 일 때, \\(Y_1=X_1^2\\) 과 \\(Y_2=X_1X_2\\) 의 결합확률 분포는\n\\[\n\\begin{aligned}\ny_1&=u_1(x_1,x_2)= x_1^2\\\\\ny_2&=u_2(x_1,x_2)= x_1x_2\\\\\nx_1&=w_1(y_1,y_2)= \\sqrt{y_1} & (y_1&gt;0)\\\\\nx_2&=w_2(y_1,y_2)= \\frac{y_2}{x_1} = \\frac{y_2}{\\sqrt{y_1}}  & (y_1&gt;0)\\\\\ng(y_1.y_2)&=f(w_1(y_1,y_2),w_2(y_1,y_2))|J|\\\\\ng(y_1.y_2)&=f(w_1(y_1,y_2),w_2(y_1,y_2))|J|\\\\\n          &=4\\sqrt{y_1}\\frac{y_2}{\\sqrt{y_1}}\\frac{1}{2y_1}\\\\\n          &=\\begin{cases}\n          \\frac{2y_2}{y_1} & \\text{if  } y_2^2&lt;y_1&lt;1, \\text{  } 0&lt;y_2&lt;1\\\\\n          0 & \\text{otherwise}\n          \\end{cases}\\\\\n          |J|&=|\n          \\begin{bmatrix}\n          \\frac{\\partial x_1}{\\partial y_1}&\\frac{\\partial x_1}{\\partial y_2}\\\\\n          \\frac{\\partial x_2}{\\partial y_1}&\\frac{\\partial x_2}{\\partial y_2}\\\\\n          \\end{bmatrix}|\\\\\n           &=|\n          \\begin{bmatrix}\n          \\frac{1}{2\\sqrt{y_1}}&0\\\\\n          y_2(-\\frac{1}{2}y_1^{\\frac{-3}{2}})&\\frac{1}{\\sqrt{y_1}}\\\\\n          \\end{bmatrix}|\\\\\n          &=\\frac{1}{2y_1}\n\\end{aligned}\n\\]\n이다.\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-21_transformation/index.html#trnasformation-of-random-variables",
    "href": "docs/blog/posts/statistics/2023-02-21_transformation/index.html#trnasformation-of-random-variables",
    "title": "Transformation of Random Variables",
    "section": "",
    "text": "확률 변수 2개 이상에 대한 확률 분포를 joint probability distribution (결합확률분포)라고 하는데 두 확률 변수 \\(X\\) 와 \\(Y\\) 의 관계에 대해서 규명해야 할 때가 있다. 예를 들어, X 와 Y의 높은 상관계수라든지 또는 비선형적인 관계가 관찰될 때 그 관계가 수리적으로 모델링이 가능하고 한 확률 변수의 분포에 대한 정보를 알고있다면 미지의 다른 확률 변수의 분포가 추정가능해진다. 이 때 두 변수에 대한 관계 정도가 높으면 높을수록 추정이 쉬워진다.\n이번 블로그에서는 주어진 확률 변수 \\(X\\) 에 대해서 \\(X\\) 의 pmf (probability mass function) 또는 pdf (probability density function) \\(f_x(x)\\) 를 알고있을 때 확률 변수에 \\(X\\) 에 적절한 함수의 변환을 적용해 확률 변수 \\(Y\\) 를 \\(Y=u(X)\\) 라는 관계식이 정의 가능할 때 \\(Y\\) 의 pmf 또는 pdf를 구하는 방법에 집중한다. 후에 MGF (Momment Generating Function) 학습에 응용될 수 있는 개념으로 잘 정리할 필요가 있다.\n\n\n\n\nTheorem 1 Discrete random variable \\(X\\) 의 probability distribution이 \\(f_X(x)\\) 이고 \\(X\\) 와 \\(Y\\) 사이에는 \\(Y=u(X)\\) 라는 one-to-one relation이 성립될 때 \\(y=u(x)\\) 를 유일한 \\(x\\) 를 \\(y\\) 에 대한 함수인 \\(x=w(y)\\) 로 표현 가능하다면 \\(Y\\) 의 probability distribution는 \\[\nf_Y(y)=f_X(w(y))\n\\] 이다.\n\n\n\n\nIn the case of a One-to-One relation\n\n동전을 독립적으로 2번 던질 때, 확률 변수 \\(X\\) 가 앞면이 나오는 수라고 정의했을 때, 확률 분포 아래 표 (a)와 같다. \\(Y=2X+1\\) 라는 관계가 성립할 때 \\(Y\\) 의 분포는 아래 표 (b)와 같다.\n\n\nTable 1: Exmaple: Transformation of Discrete Random Variable (One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\\(X\\)\n0\n1\n2\n\n\n\n\n\\(P_X(X=x)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y=2X+1\\)\n\n\n\\(Y=2X+1\\)\n1\n3\n5\n\n\n\n\n\\(P_Y(Y=y)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n\nIn the case of not a One-to-One relation\n\n동전을 독립적으로 2번 던질 때, 확률 변수 \\(X\\) 가 앞면이 나오는 수의 합이라고 정의 했을때, \\(X\\) 의 확률 분포는 아래 표 (a) 와 같다. 이 때 \\(Y=mod(X,2)\\) 라는 관계가 성립할 때 \\(Y\\) 의 분포는 아래 표 (b) 같다.\n\n\nTable 2: Exmaple: Transformation of Discrete Random Variable (Not One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\\(X\\)\n0\n1\n2\n\n\n\n\n\\(P_X(X=x)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y=mod(x,2)\\)\n\n\n\\(Y=mod(x,2)\\)\n0\n1\n\n\n\n\n\\(P_Y(Y=y)\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{2}{4}\\)\n\n\n\n\n\n\n위의 예시와 같이 두 확률 변수가 one to one 관계일 때는 확률분포가 그대로 유지되어서 쉽게 변환된 확률 변수의 분포가 추정가능하지만 one to one 관계가 아닐 경우 확률 분포가 바뀌게 된다.\n\nanother example (In the case of not a One-to-One relation)\n\n기하분포 (Geometric Distribution)란 동일한 베르누이 (Bernoulli) 분포의 시행을 독립적으로 반복할 때 첫 성공까지의 시행 횟수를 확률변수 \\(X\\) 로 하는 분포이다. 즉, \\(x-1\\) 번째까지 베르누이 시행이 실패하고 \\(x\\) 번째 시행에서 성공할 확률 분포를 말한다.\nNotation은 \\(X \\sim Geometric(p)\\) 또는 \\(X \\sim Geo(p)\\) 라고 표현하고, \\(p\\) 는 독립 시행에서 성공할 확률이다. (참고: \\(text{E}(X)=\\frac{1}{p}\\), \\(\\text{Var}(X)=\\frac{1-p}{p^2}\\))\n\\(X \\sim Geometric(\\frac{4}{5})\\) 일 때 \\(X\\) 의 확률분포 \\(f(x)=\\frac{4}{5}(\\frac{1}{5})^{(x-1)}\\), \\(x=1,2,3 ...\\) 가 기하분포일 때 \\(Y=X^2\\) 의 확률분포는\n\\[\n\\begin{aligned}\n  y&=u(x)=x^2 \\\\\n  x&=w(y)=\\sqrt{y} \\text{ } (x&gt;0)\\\\\n  f_Y(y)&=f_X(w(y))=f_X(\\sqrt{y})=\\frac{4}{5}(\\frac{1}{5}^{(\\sqrt{y}-1)})\\\\\n  \\therefore f_Y(y)&=\n  \\begin{cases}\n    \\frac{4}{5}(\\frac{1}{5}^{(\\sqrt{y}-1)}) &\\text{if} \\text{  } y =1, 4, 9, ...\\\\\n     0 \\text{  } & \\text{otherwise}\n    \\end{cases}\n\\end{aligned}\n\\]\n이다.\n\n\n\n\n\nTheorem 2 Two discrete random variables \\(X_1\\) 과 \\(X_2\\) 의 joint probability distribution이 \\(f(x_1,x_2)\\) 이고 \\((x_1,x_2)\\) 와 \\((y_1,y_2)\\) 가 one-to-one relation이 성립되어 \\(y_1=u_1(x_1,x_2)\\) 와 \\(y_2=u_2(x_1,x_2)\\) 를 유일한 \\(x_1=w_1(y_1,y_2)\\) 와 \\(x_2=w_2(y_1,y_2)\\) 의 함수로 표현 가능하다면 새로운 확률변수 \\(\\mathbf{Y}\\), 즉, \\(Y_1\\) 과 \\(Y_2\\) 의 joint probability distribution는 \\[\nf_Y(y_1,y_2)=f_X(w_1(y_1,y_2),w_2(y_1,y_2))\n\\] 이다.\n\n\n\n\nexmaple 1\n\n다항 분포(multinomial distribution)란 2개 이상의 독립적인 확률 변수 \\(\\mathbf{X}=X_1, X_2, ...\\) 들에 대한 확률분포로, 여러 독립 시행에서 각 각의 값이 특정 횟수가 나타날 확률을 정의하고 독립 변수가 2개인 경우 다항 분포의 특별한 case로 이항 분포 (binomial distribution)가 된다.\n참고( Source: wiki) :\n\\[\n\\begin{aligned}\n  f_(x) & =\\frac{n!}{x_1!x_2!\\dots x_k!}p_1^{x_1}p_2^{x_2}\\dots p_k^{x_k}\\\\\n  \\text{E}(x)&=np_i\\\\\n  \\text{Var}(x)&=np_i(1-p_i)\n\\end{aligned}\n\\]\n\\(X_1\\) and \\(X_2\\) 가 다음과 같은 multinomial distribution을 따를 때 \\[\nf(x_1,x_2) = \\binom{2}{x_1,x_2,2-x_1-x_2}\\frac{1}{4}^{x_1}\\frac{1}{3}^{x_2}\\frac{5}{12}^{2-x_1-x_2} x_1=0,1,2, \\space x_2=0,1,2, \\space x_1+x_2\\le 2\n\\]\n\\(Y_1=X_1+X_2\\) 와 \\(Y_2=X_1-X_2\\) 의 결합 확률 분포는\n\\[\n\\begin{aligned}\n  y_1&=u_1(x_1,x_2)=x_1+x_2\\\\\n  y_2&=u_2(x_1,x_2)=x_1-x_2\\\\\n  x_1&=w_1(y_1,y_2)=\\frac{y_1+y_2}{2}\\\\\n  x_2&=w_2(y_1,y_2)=\\frac{y_1-y_2}{2}\\\\\n  f_Y(y_1,y_2)&=\\binom{2}{\\frac{y_1+y_2}{2},\\frac{y_1-y_2}{2},2-y_1 }\\frac{1}{4}^{\\frac{y_1+y_2}{2}}\\frac{5}{12}^{2-y_1} y_1=0,1,2, \\space y_2=-2,-1,0,1,2\n\\end{aligned}\n\\] 이다.\n\nexmaple 2\n\n\\(X_1\\) 과 \\(X_2\\) 의 결합확률분포가 다음과 같을 때 변환된 \\(Y_1=X_1+X_2\\) 와 \\(Y_2=X_1X_2\\) 의 결합확률 분포는?\n\n\nTable 3: Exmaple: Transformation of Joint Discrete Random Variable (Not One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\n\n\n\n\n\n\n\\((x_1,x_2)\\)\n(0, 0)\n(0, 1)\n(1, 0)\n(1, 1)\n\n\n\n\n\\(f_X(x_1,x_2)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y\\)\n\n\n\\((y_1,y_2)\\)\n(0, 0)\n(1, 0)\n(2, 1)\n\n\n\n\n\n\\(f_Y(y_1,y_2)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 3 continuous random variable \\(X\\) 의 probability distribution이 \\(f_X(x)\\) 이고 \\(X\\) 와 \\(Y\\) 사이에는 \\(Y=u(X)\\) 라는 one-to-one relation이 성립될 때 \\(y=u(x)\\) 를 유일한 \\(x\\) 를 \\(y\\) 에 대한 함수인 \\(x=w(y)\\) 로 표현 가능할 때 Y의 확률 분포는 \\[\nf_Y(y)=f_X(w(y))|J|\n\\] 이다. (\\(J=w'(y)\\), called ‘Jacobian’)\n\n\n\n\nIn the case of a One-to-One relation\n\n지수분포(exponential distribution)는 연속 확률 분포 중 하나로, 사건이 서로 독립적일 때, 일정 시간동안 발생하는 사건의 횟수가 푸아송 분포를 따른다면, 다음 사건이 발생할 때까지의 대기 시간을 확률 변수 \\(X \\in [0, \\infty)\\) 로 하는 분포이다. (Source: Wiki).\n참고: \\[\n\\begin{aligned}\n  X&\\sim Exp(\\lambda) \\\\\n  \\text E[X] &= \\frac{1}{\\lambda}\\\\\n  \\text{Var}[X] &= \\frac{1}{\\lambda^2}\\\\\n  f(x;\\lambda)&=\n  \\begin{cases}\n    \\lambda e^{-\\lambda x} & x \\ge 0\\\\\n    0 & x&lt;0\n  \\end{cases}\\\\\n\\end{aligned}\n\\] \\(\\lambda&gt;0\\) is the parameter of the distribution, called ‘rate parameter’.\nTransformation of Linear Function. When two random variables \\(X \\sim \\text{exp}(\\lambda)\\) and \\(Y\\) has the relation, \\(Y=aX+b\\) \\((a\\ne 0)\\). When \\(f_X(x)=\\lambda e^{-\\lambda x}\\), \\(f_Y(y)\\) is\nBy Theorem\n\\[\n\\begin{aligned}\n  Y&=u(X)=aX+b\\\\\n  y&=u(x)=ax+b\\\\\n  x&=w(y)=\\frac{y-b}{a}\\\\\n  f_Y(y)&=f_X(w(y))|J|=f_X(\\frac{y-b}{a})|J|\\\\\n        &=f_X(\\frac{y-b}{a})|w'(y)|\\\\\n        &=f_X(\\frac{y-b}{a})|\\frac{1}{a}|\\\\\n        &=\\frac{f_X(\\frac{y-b}{a})}{|a|}\\\\\n\\end{aligned}\n\\]\nBy Definition\n\n\n\n\\(a&gt;0\\) \\[\n\\begin{aligned}\nF_Y(Y)&=P_Y(Y\\le y)\\\\\n      &=P_Y(aX+b\\le y)\\\\\n      &=P_Y(X\\le \\frac{y-b}{a})\\\\\n      &=F_X(\\frac{y-b}{a})\\\\\nF'_Y(Y)&=\\frac{d}{dy}F_X(\\frac{y-b}{a})\\\\\n       &=f_X(\\frac{y-b}{a})\\frac{1}{a}\\\\\n\\end{aligned}\n\\]\n\n\n\n\\(a\\le 0\\) \\[\n\\begin{aligned}\nF_Y(Y)&=P_Y(Y\\le y)\\\\\n      &=P_Y(aX+b\\le y)\\\\\n      &=P_Y(X&gt; \\frac{y-b}{a})\\\\\n      &=1-F_X(\\frac{y-b}{a})\\\\\nF'_Y(Y)&=\\frac{d}{dy}(1-F_X(\\frac{y-b}{a}))\\\\\n       &=-f_X(\\frac{y-b}{a})\\frac{1}{a}\\\\  \n\\end{aligned}\n\\]\n\n\\[\n\\therefore f_Y(y)=\\frac{f_X(\\frac{y-b}{a})}{|a|}=\\frac{\\lambda exp(-\\lambda (\\frac{y-b}{a}))}{|a|}\n\\]\n\n위의 정리를 유도를 하진 않았지만 증명을 해보면 사실, \\(|J|\\) 는 역함수의 미분의 계수인 것을 할 수 있다. 위의 예시를 By definition 으로 푼 것을 보면 \\(y=u(x)\\) 와 \\(x=w(y)\\) 의 관계인 것을 알 수 있고 \\(|J|\\) 에 해당되는 \\(\\frac{1}{a}\\) 는 CDF \\(F(Y)\\) 의 derivative를 구하는 과정에서 발생하는 chain rule에 의해 생긴 것을 알 수 있다.\n\\[\n\\begin{aligned}\n  y&=u(x)\\\\\n  \\frac{dy}{dx}&=u'(x)\\\\\n  x&=w(y)=u^{-1}(y)\\\\\n  \\frac{dx}{dy}&=w'(y) \\\\\n  w'(y)&=\\frac{dx}{dy}=(\\frac{dy}{dx})^{-1}=\\frac{1}{u'(x)}\n\\end{aligned}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 20, 1000)\ny = 2*x+1\ny2 = x/2+1\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,x,color='black',label=r'$y=x$', linestyle='dashed')\nax.plot(x,2*x-2,color='black',label=r'$x=w(y) \\rightarrow y=2x-2 $')\nax.plot(x,y2,color='red',label=r'$y=u(x)=\\frac{1}{2}x+1$')\nax.set_aspect(1)\nax.set_title(r\"Example of Relation of X and Y, $y=u(x)$ vs $x=w(y)$\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nax.set_xlim(-20, 40)\n\nplt.legend(shadow=True, loc=(-0.5, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 20, 1000)\ny = 2*x+1\ny2 = x/2+1\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,y,color='black',label=r'$y=2x+1$')\nax.plot(x,y2,color='red',label=r'$y=\\frac{1}{2}x+1$')\nax.set_aspect(1)\nax.set_title(r\"Transformation of Linear Function, $f_Y(y)=\\frac{f_X(\\frac{y-b}{a})}{|a|}$\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y=u(X)\")\nax.set_xlim(-20, 40)\nax.text(10, 0,'|', color='black', horizontalalignment='center', verticalalignment='center')\nax.text(10, -5, r'$\\frac{y-b}{a}$', horizontalalignment='center',color='black')\nax.text(0, 5,'--', color='black', horizontalalignment='center', verticalalignment='center')\nax.text(-9, 5, r'$ax+b$', verticalalignment='center',color='black')\n\nplt.legend(shadow=True, loc=(-0.5, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\\(P(Y\\le y)=P(X\\le \\frac{y-b}{a})\\) 이기 때문에 \\(X\\) 의 특정 구간의 확률과 대응되는 \\(Y\\) 의 구간의 확률이 같다고 했을 때 \\(Y \\le y\\) 의 범위는 \\(X\\) 를 \\(b\\) 만큼 이동한 후 \\(a\\) 배한 \\(x\\) 좌표 이하 구간에 대응 된다 즉, \\(X\\le \\frac{y-b}{a}\\). 이 때, 두 구간에서의 확률 밀도가 같아야 하기 때문에 \\(f_x(x)\\) 가 \\(f_Y(y)\\) 의 \\(\\frac{1}{a}\\) 배 인 것을 알 수 있다.\n위의 그림에서 처럼, 두 확률 변수 \\(X\\) 와 \\(Y\\) 의 관계를 \\(y = 2x+1\\) \\((a&gt;1)\\) 와 \\(y = \\frac{1}{2}x+1\\) \\((0 \\le a\\le 1)\\) 로 두 가지의 예로 들면, 위의 그래프의 경우, \\(y = 2x+1\\) \\((a&gt;1)\\) 일 때 \\(0&lt;X&lt;10\\) 는 \\(1&lt;Y&lt;21\\) 의 구간이 2배가 되는 것을 관찰 할 수 있다. 반대로, \\(y = \\frac{1}{2}x+1\\) \\((0 \\le a\\le 1)\\) 일 때 \\(0&lt;X&lt;10\\) 는 \\(1&lt;Y&lt;6\\) 의 구간이 \\(\\frac{1}{2}\\) 배가 된다. 이 때 각 예시의 경우에 \\(X\\) 와 \\(Y\\) 의 확률 값이 같기 때문에 (\\(P(Y\\le y)=P(X\\le \\frac{y-1}{2})\\) 와 \\(P(Y\\le y)=P(X\\le \\frac{y-1}{\\frac{1}{2}})\\)). \\(f_x(x)\\) 가 \\(f_Y(y)\\) 의 각 각 \\(2\\) 배와 \\(\\frac{1}{2}\\) 배 인 것을 추정 할 수 있다.\n\nIn the case of not a One-to-One relation\n\n\\(Y=X^2\\) 일 때,\n\\[\n\\begin{aligned}\n  F_Y(Y)&=P_Y(Y\\le y)\\\\\n        &=P_Y(X^2\\le y)\\\\\n        &=P_Y(-\\sqrt{y}\\le X \\le \\sqrt y)\\\\\n        &=F_X(\\sqrt{y})-F_X(-\\sqrt{y})\\\\\n  F'_Y(Y)&=\\frac{d}{dy}(F_X(\\sqrt{y})-F_X(-\\sqrt{y}))\\\\\n        &=f_X(\\sqrt{y})\\frac{1}{2\\sqrt{y}}+f_X(-\\sqrt{y})\\frac{1}{2\\sqrt{y}}\\\\\n        &=\\frac{f_X(\\sqrt{y})}{2\\sqrt{y}}+\\frac{f_X(-\\sqrt{y})}{2\\sqrt{y}}\\\\\n\\end{aligned}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 1000)\ny = x*x\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,y,color='black',label=r'$y=x^2$')\n#ax.text(3.0,0.5,r'$\\sqrt{y}$')\n#ax.text(-4,0.5,r'$-\\sqrt{y}$')\nax.set_xlim([-10, 10])\nax.set_ylim([0, 10])\n\nax.set_aspect(1)\nax.set_title(r\"Transformation of Random Variables, $y=x^2$\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\n\nplt.legend(shadow=True, loc=(-0.2, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\\(X\\) 의 분포를 알고 \\(X\\) 와 \\(Y\\) 의 relation을 알고있을 때 \\(X\\) 의 분포로부터 \\(Y\\) 의 분포를 추정 가능한 것의 이점 중 하나는 \\(Y\\) 의 통계량을 \\(Y\\) 의 분포를 사용하지 않고 계산해낼 수 있다는 점이다.\n예를 들어, \\(X \\sim N(0,1)\\) 이고 \\(Y=u(X)\\) 라는 관계를 알고있을 때, \\(Y\\) 의 통계량 \\(E(Y)\\), \\(Var(Y)\\) 를 알고싶다면 굳이 힘들게 \\(Y\\) 의 분포를 계산할 필요가 없다. 이미 \\(X\\) 의 확률 분포 \\(f_X(x)\\) 를 알고있기 때문에 아래와 같이 \\(Y\\) 의 통계량을 계산해낼 수 있다.\n\\[\n\\begin{aligned}\n  \\text{E}(Y)&=\\int Y f_Y(y) dy\\\\\n            &=\\int u(X) f_X(x) dx\\\\\n  \\text{E}(Y^2)&=\\int Y^2 f_Y(y) dy\\\\\n            &=\\int u(X)^2 f_X(x) dx\\\\\n  \\text{Var}(Y)&=\\text{E}(Y^2)-\\text{E}(Y)^2\\\\\n  &=\\text{E}(g(X)^2)-\\text{E}(g(X))^2\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\nTheorem 4 Two continuous random variable \\(X_1\\) 과 \\(X_2\\) 의 joint probability distribution이 \\(f(x_1,x_2)\\) 이고 \\((x_1,x_2)\\) 와 \\((y_1,y_2)\\) 가 one-to-one relation이 성립될 때 \\(y_1=u_1(x_1,x_2)\\) 와 \\(y_2=u_2(x_1,x_2)\\) 를 유일한 \\(x_1=w_1(y_1,y_2)\\) 와 \\(x_2=w_2(y_1,y_2)\\) 의 함수로 표현 가능할 때 \\(Y_1=u_1(X_1,X_2)\\) 와 \\(Y_2=u_2(X_1,X_2)\\) 로 정의되는 새로운 확률변수 \\(Y_1\\) 과 \\(Y_2\\) 의 joint probability distribution는 \\[\ng(y_1.y_2)=f(w_1(y_1,y_2),w_2(y_1,y_2)) |J|\n\\] 이다.\n여기서, \\[\n\\begin{aligned}\n  |J|=|\n  \\begin{bmatrix}\n  \\frac{\\partial x_1}{\\partial y_1}&\\frac{\\partial x_1}{\\partial y_2}\\\\\n  \\frac{\\partial x_2}{\\partial y_1}&\\frac{\\partial x_2}{\\partial y_2}\\\\\n  \\end{bmatrix}| = | \\begin{bmatrix}\n  \\frac{\\partial w_1(y_1,y_2)}{\\partial y_1}&\\frac{\\partial w_1(y_1,y_2)}{\\partial y_2}\\\\\n  \\frac{\\partial w_2(y_1,y_2)}{\\partial y_1}&\\frac{\\partial w_2(y_1,y_2)}{\\partial y_2}\\\\\n  \\end{bmatrix} |\n\\end{aligned}\n\\] called ‘the determinant of jacobian matrix’\n\n\n\n\n예제\n\n연속확률변수 \\(X_1\\) 과 \\(X_2\\) 는 결합확률분포 \\[\nf(x_1,x_2)=\n  \\begin{cases}\n    4x_1x_2, & \\text{if  } 0&lt;x_1&lt;1,& 0&lt; x_2 &lt;1 \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\] 일 때, \\(Y_1=X_1^2\\) 과 \\(Y_2=X_1X_2\\) 의 결합확률 분포는\n\\[\n\\begin{aligned}\ny_1&=u_1(x_1,x_2)= x_1^2\\\\\ny_2&=u_2(x_1,x_2)= x_1x_2\\\\\nx_1&=w_1(y_1,y_2)= \\sqrt{y_1} & (y_1&gt;0)\\\\\nx_2&=w_2(y_1,y_2)= \\frac{y_2}{x_1} = \\frac{y_2}{\\sqrt{y_1}}  & (y_1&gt;0)\\\\\ng(y_1.y_2)&=f(w_1(y_1,y_2),w_2(y_1,y_2))|J|\\\\\ng(y_1.y_2)&=f(w_1(y_1,y_2),w_2(y_1,y_2))|J|\\\\\n          &=4\\sqrt{y_1}\\frac{y_2}{\\sqrt{y_1}}\\frac{1}{2y_1}\\\\\n          &=\\begin{cases}\n          \\frac{2y_2}{y_1} & \\text{if  } y_2^2&lt;y_1&lt;1, \\text{  } 0&lt;y_2&lt;1\\\\\n          0 & \\text{otherwise}\n          \\end{cases}\\\\\n          |J|&=|\n          \\begin{bmatrix}\n          \\frac{\\partial x_1}{\\partial y_1}&\\frac{\\partial x_1}{\\partial y_2}\\\\\n          \\frac{\\partial x_2}{\\partial y_1}&\\frac{\\partial x_2}{\\partial y_2}\\\\\n          \\end{bmatrix}|\\\\\n           &=|\n          \\begin{bmatrix}\n          \\frac{1}{2\\sqrt{y_1}}&0\\\\\n          y_2(-\\frac{1}{2}y_1^{\\frac{-3}{2}})&\\frac{1}{\\sqrt{y_1}}\\\\\n          \\end{bmatrix}|\\\\\n          &=\\frac{1}{2y_1}\n\\end{aligned}\n\\]\n이다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-21_transformation/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-21_transformation/index.html#blog-guide-map-link",
    "title": "Transformation of Random Variables",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html#bernoulli-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html#bernoulli-distribution",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Definition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html#blog-guide-map-link",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html#bernoulli-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html#bernoulli-distribution",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Definition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html#blog-guide-map-link",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x;p)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}x^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html#bernoulli-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html#bernoulli-distribution",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Definition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x;p)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}x^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html#blog-guide-map-link",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html",
    "title": "Geometric Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 성공 확률이 \\(p\\) 인 independent Bernoulli trials을 시행할 때 첫 성공할때까지의 시행 횟수를 확률 변수 \\(X\\) 로 갖는 분포를 geometric distribution이라고 하고 \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;p)&=p(1-p)^{x-1}\n\\end{aligned}\n\\] 이다. (단, \\(x=1,2,...\\))\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=1}^{\\infty} x f(x)\\\\\n             &=\\sum_{x=1}^{\\infty} x pq^{x-1} &\\text{ }(q=1-p)\\\\\n             &=p\\sum_{x=1}^{\\infty} x q^{x-1} \\\\\n             &=p\\frac{d}{dq}\\sum_{x=1}^{\\infty} q^{x} \\\\\n             &=p\\frac{d}{dq}\\frac{q}{1-q} \\\\\n             &=p\\frac{(1-q)+q}{(1-q)^2} \\\\\n             &=p\\frac{(1-q)+q}{p^2} \\\\\n             &=p\\frac{1}{p^2}\\\\\n             &=\\frac{1}{p}\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X(X-1))&=\\sum_{x=1}^{\\infty}x(x-1)pq^{x-1}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}x(x-1)q^{x-2}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}\\frac{d^2}{dq^2}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}\\sum_{x=1}^{\\infty}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}(\\frac{q}{1-q})\\\\\n                  &=pq\\frac{d}{dq}(\\frac{1}{(1-q)^2})\\\\\n                  &=pq(\\frac{2(1-q)}{(1-q)^4})\\\\\n                  &=\\frac{2q}{p^2}\\\\\n  \\text{E}(X^2)&=\\frac{2q}{p^2}+\\text{E}(X)=\\frac{2q}{p^2}+\\frac{1}{p}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n               &=\\frac{2q}{p^2}+\\frac{1}{p}-\\frac{1}{p^2}\\\\\n               &=\\frac{1-p}{p^2}\\\\\n\\end{aligned}\n\\]\n\n\n\n한 장비 제조 업체에서 사람의 건강과 생명을 진단하는데 사용되는 medical device가 1000 개마다 2개 꼴로 불량이 발생한다고 가정할 때 그 장비를 공급받는 구매자가 medical device의 공급 업체의 불량 평가 기준으로 quality control을 1대 씩 진행할 때 불량 장비가 3번째 검사에서 발생할 확률은\n\\[\n\\begin{aligned}\n   \\text{f}(x=3;p=\\frac{2}{1000})&=(\\frac{998}{1000})^2\\frac{2}{1000}\\\\\n\\end{aligned}\n\\] 이다. 평균과 분산 각 각 \\((\\frac{2}{1000})^{-1}\\), \\(\\frac{1-\\frac{2}{1000}}{(\\frac{2}{1000})^2}\\) 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html#geometric-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html#geometric-distribution",
    "title": "Geometric Distribution",
    "section": "",
    "text": "Definition 1 성공 확률이 \\(p\\) 인 independent Bernoulli trials을 시행할 때 첫 성공할때까지의 시행 횟수를 확률 변수 \\(X\\) 로 갖는 분포를 geometric distribution이라고 하고 \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;p)&=p(1-p)^{x-1}\n\\end{aligned}\n\\] 이다. (단, \\(x=1,2,...\\))\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=1}^{\\infty} x f(x)\\\\\n             &=\\sum_{x=1}^{\\infty} x pq^{x-1} &\\text{ }(q=1-p)\\\\\n             &=p\\sum_{x=1}^{\\infty} x q^{x-1} \\\\\n             &=p\\frac{d}{dq}\\sum_{x=1}^{\\infty} q^{x} \\\\\n             &=p\\frac{d}{dq}\\frac{q}{1-q} \\\\\n             &=p\\frac{(1-q)+q}{(1-q)^2} \\\\\n             &=p\\frac{(1-q)+q}{p^2} \\\\\n             &=p\\frac{1}{p^2}\\\\\n             &=\\frac{1}{p}\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X(X-1))&=\\sum_{x=1}^{\\infty}x(x-1)pq^{x-1}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}x(x-1)q^{x-2}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}\\frac{d^2}{dq^2}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}\\sum_{x=1}^{\\infty}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}(\\frac{q}{1-q})\\\\\n                  &=pq\\frac{d}{dq}(\\frac{1}{(1-q)^2})\\\\\n                  &=pq(\\frac{2(1-q)}{(1-q)^4})\\\\\n                  &=\\frac{2q}{p^2}\\\\\n  \\text{E}(X^2)&=\\frac{2q}{p^2}+\\text{E}(X)=\\frac{2q}{p^2}+\\frac{1}{p}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n               &=\\frac{2q}{p^2}+\\frac{1}{p}-\\frac{1}{p^2}\\\\\n               &=\\frac{1-p}{p^2}\\\\\n\\end{aligned}\n\\]\n\n\n\n한 장비 제조 업체에서 사람의 건강과 생명을 진단하는데 사용되는 medical device가 1000 개마다 2개 꼴로 불량이 발생한다고 가정할 때 그 장비를 공급받는 구매자가 medical device의 공급 업체의 불량 평가 기준으로 quality control을 1대 씩 진행할 때 불량 장비가 3번째 검사에서 발생할 확률은\n\\[\n\\begin{aligned}\n   \\text{f}(x=3;p=\\frac{2}{1000})&=(\\frac{998}{1000})^2\\frac{2}{1000}\\\\\n\\end{aligned}\n\\] 이다. 평균과 분산 각 각 \\((\\frac{2}{1000})^{-1}\\), \\(\\frac{1-\\frac{2}{1000}}{(\\frac{2}{1000})^2}\\) 이다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html#blog-guide-map-link",
    "title": "Geometric Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-28_mgf/index.html",
    "href": "docs/blog/posts/statistics/2023-02-28_mgf/index.html",
    "title": "Momment Generating Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 확률 변수 \\(X^r\\) 의 expectation, \\(\\text{E}((X-\\mu)^r)\\) 를 확률 변수 \\(X\\) 의 평균 \\(\\mu\\) 에 대한 \\(r\\) 차 중심 적률(moment) 이라하고 그 notation은 \\(\\mu_r'=\\text{E}((X-\\mu)^r)\\) 로 한다. \\(\\mu_r'=\\text{E}(X^r)\\) 은 원점에 대한 \\(r\\) 차 중심 적률이라 한다.\n즉,\n\\[\n\\begin{aligned}\n  \\mu_{r}'&=\n    \\begin{cases}\n      \\text{E}((X-\\mu)^r)&=\n        \\begin{cases}\n          \\sum_{x}(x-\\mu)^rf(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}(x-\\mu)^rf(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases} & \\text{moment about }\\mu\\\\\n      \\text{E}(X^r)&=\n        \\begin{cases}\n          \\sum_{x}x^rf(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}x^rf(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases} & \\text{moment about origin}\\\\\\\\\n    \\end{cases}  \n\\end{aligned}\n\\]\n이다.\n\n분포의 특징을 묘사하는 parameters 중 많은 종류가 확률 변수의 적률을 이용해 계산될 수 있다. 그 대표적인 예가\n\n평균 (mean): 분포의 위치를 나타내는 척도, 1차 중심 적률로 계산\n분산 (variance): 분포가 평균으로부터 퍼진 정도를 나타내는 척도, 2차 중심 적률로 계산\n왜도 (skewedness): 분포가 기울어진 방향과 정도를 나타내는 척도, 3차 중심 적률로 계산\n첨도 (kurtosis): 분포가 위로 뾰족한 정도를 나타내는 척도, 4차 중심 적률로 계산\n\n\nDefinition 2 확률 변수 \\(X\\) 의 적률 생성 함수 (Moment Generating Function, mgf), \\(\\text{M}_X(t) =\\text{E}(e^{tX})\\) 로 정의한다.\n즉,\n\\[\n\\begin{aligned}\n  M_X(t)=\\text{E}(e^{tX})&=\n        \\begin{cases}\n          \\sum_{x}e^{tx}f(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}e^{tx}f(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases}\n\\end{aligned}\n\\]\n이다.\n\n\nTheorem 1 확률 변수 \\(X\\) 의 적률 생성 함수 (Moment Generating Function, mgf), \\(\\text{M}_X(t) =\\text{E}(e^{tX})\\) 로 r차 적률 계산은 다음과 같이 할 수 있다.\n\\[\n\\begin{aligned}\n  \\frac{d^r}{dt^r}M_X(t) \\bigg|_{t=0}=M_X^r(0)=\\text{E}(X^r)=\\mu_r'\n\\end{aligned}\n\\]\n이다. 즉, 적률 생성 함수 (mgf) \\(M_X(t)\\) 를 구하고 r 번 미번한 후에 \\(t=0\\) 대입하면 r차 중심 적률을 구할 수 있다.\n\nproof (for the only continuous case)\n\\[\n\\begin{aligned}\n  \\text{First Order Moment}\\\\\n  \\frac{d}{dt}M_X(t)&= \\frac{d}{dt}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d}{dt}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}\\frac{d}{dt}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}xe^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}xf(x)dx\\\\\n    &=  \\text{E}(X)\\\\\n    &=  \\mu_1'\\\\\n  \\text{Second Order Moment}\\\\\n  \\frac{d^2}{dt^2}M_X(t)&= \\frac{d^2}{dt^2}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d^2}{dt^2}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}\\frac{d}{dt}xe^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^2e^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^2f(x)dx\\\\\n    &=  \\text{E}(X^2)\\\\\n    &=  \\mu_2'\\\\\n  \\vdots\\\\\n  \\text{r th Order Moment}\\\\\n  \\frac{d^r}{dt^r}M_X(t)&= \\frac{d^r}{dt^r}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d^r}{dt^r}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^re^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^rf(x)dx\\\\\n    &=  \\text{E}(X^r)\\\\\n    &=  \\mu_r'\n\\end{aligned}\n\\]\n\n\n\n\n\nMGF of \\(X\\sim B(n,p)\\)\n\n확률 변수 \\(X \\sim B(n,p)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n\\[\n\\begin{aligned}\n  f(x)&=\\binom{n}{x}p^xq^{n-x}\\\\\n  \\sum_{x=0}^{n}f(x)&=\\sum_{x=0}^{n}\\binom{n}{x}p^xq^{n-x}=(p+q)^n\\\\\n  M_X(t)&=\\sum_{x=0}^{n}e^{tx}f(x)=\\sum_{x=0}^{n}e^{tx}\\binom{n}{x}p^xq^{n-x}\\\\\n        &=\\sum_{x=0}^{n}\\binom{n}{x}(pe^t)^xq^{n-x}=(pe^t+q)^n\\\\\n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}(pe^t+q)^n\\\\\n    &=n(pe^t+q)^{n-1}pe^t\\bigg|_{t=0}\\\\\n    &=n(p+q)^{n-1}p=np\\\\\n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}(n(pe^t+q)^{n-1}pe^t)\\bigg|_{t=0}\\\\\n    &=(n(n-1)(pe^t+q)^{n-2}pe^tpe^t+n(pe^t+q)^{n-1}pe^t)\\bigg|_{t=0}\\\\\n    &=n(n-1)(p+q)^{n-2}p^2+n(p+q)^{n-1}p\\\\\n    &=n(n-1)p^2+np\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=n(n-1)p^2+np-(np)^2\\\\\n    &=n^2p^2-np^2+np-n^2p^2\\\\\n    &=np(1-p)=npq\n\\end{aligned}\n\\]\n\nMGF of \\(X\\sim Poisson(\\lambda)\\)\n\n확률 변수 \\(X \\sim Poisson(\\lambda)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n  M_X(t)&=\\sum_{x=0}^{\\infty}e^{tx}f(x)=\\sum_{x=0}^{\\infty}e^{tx}\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n        &=e^{-\\lambda}\\sum_{x=0}^{\\infty}\\frac{{e^{t}\\lambda^{x}}^x}{x!}\\\\\n        &=e^{-\\lambda}e^{\\lambda e^t} \\because \\text{(Maclaurin's Series)}\\\\\n        &=e^{\\lambda(e^t-1)} \\\\\n        (&\\text{Maclaurin's Series: } \\sum_{n=0}^{\\infty}\\frac{x^n}{n!}=1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+ ... =e^x)\\\\     \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda e^{e^t}e^{\\lambda(e^t-1)}\\bigg|_{t=0}=\\lambda\\\\\n  \n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}\\lambda e^{e^t}e^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda e^te^{\\lambda (e^t-1)}+\\lambda e^t\\lambda e^te^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda + \\lambda^2\\\\\n\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=\\lambda + \\lambda^2 -\\lambda^2\\\\\n    &=\\lambda\n\\end{aligned}\\]\n$$\n\n\n\n\nMGF of \\(X\\sim N(0,1)\\)\n\n확률 변수 \\(X \\sim N(0,1)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}(-\\infty &lt; x &lt; \\infty)\\\\\n  M_X(t)&=\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx=\\int_{-\\infty}^{\\infty}e^{tx}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{x^2}{2}+tx}dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(x-t)^2+\\frac{1}{2}t^2}dx\\\\\n        &=e^{\\frac{t^2}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(x-t)^2}dx\\\\\n        &=e^{\\frac{t^2}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}u^2}du \\text{  }(\\because x-t=u \\rightarrow dx=du)\\\\\n        &=e^{\\frac{t^2}{2}} \\\\     \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=te^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=0\\\\\n  \n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}te^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=e^{\\frac{t^2}{2}}+t(te^{\\frac{t^2}{2}})\\bigg|_{t=0}\\\\\n    &=1\\\\\n\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=1\n    \n\\end{aligned}\\]\n$$\n\nMGF of \\(X\\sim N(\\mu,\\sigma^2)\\)\n\n확률 변수 \\(X\\sim N(\\mu,\\sigma^2)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}(-\\infty &lt; x &lt; \\infty)\\\\\n  M_X(t)&=\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{\\infty}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}+tx}dx\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}(\\frac{(x-(t\\sigma^2+\\mu))}{\\sigma})^2}dx\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{u^2}{2}}du\\\\\n        (&\\because \\frac{x-(t\\sigma^2+\\mu)}{\\sigma}=u \\rightarrow \\frac{dx}{\\sigma}=du) \\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^2}{2}}du\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\\\\n  \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\bigg|_{t=0}\\\\\n    &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}(\\sigma^2t+\\mu)\\bigg|_{t=0}\\\\\n    &=\\mu\\\\\n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}e^{\\frac{t^2\\sigma^2}{2}+t\\mu}(\\sigma^2t+\\mu)\\bigg|_{t=0}\\\\\n    &=\\sigma^2+\\mu^2 \\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=\\sigma^2\n\\end{aligned}\\]\n$$\n\n\n\n\n\nTheorem 2 확률 변수 \\(X\\) 와 \\(Y\\) 가 유한한 같은 적률 생성 함수를 가지면 두 확률 변수는 같은 확률 분포를 갖는다. (단, \\(t\\in[-c,c]\\) where \\(c\\) is a positive constant) \\[\n\\text{M}_X(t) =\\text{M}_Y(t) \\rightarrow F_X(a)=F_Y(a) \\text{ for } a \\in \\mathbb{R}\n\\] 다시 말해서, 확률 변수의 분포의 특징이 적률 생성 함수에 의하여 유일하게 결정된다.\n\nProof Reference-Washington University\nMGF reference\n\nTheorem 3 \\[\n\\text{M}_{X+a}(t) =e^{at}\\text{M}_X(t)\n\\]\n\nProof) \\(\\text{E}(e^{t(x+a)})=\\text{E}(e^{at}e^{tx})=e^{at}\\text{E}(e^{tx})=e^{at}\\text{M}_X(t)\\)\n\nTheorem 4 \\[\n\\text{M}_{aX}(t) =\\text{M}_X(at)\n\\]\n\nProof) \\(\\text{E}(e^{atx})=\\text{E}(e^{at(x)})=\\text{M}_X(at)\\)\n\nExample) when \\(X \\sim N(0,1)\\), the mgf of \\(Y=aX+b\\) is ?\n\n\\[\n\\begin{aligned}\n  \\text{M}_Y(t)&=\\text{M}_{aX+b}(t)\\\\\n               &=e^{bt}\\text{M}_{X}(at)\\\\\n               &=e^{bt}e^{\\frac{a^2t^2}{2}}\\\\\n               &=e^{bt+\\frac{a^2t^2}{2}}\n\\end{aligned}\n\\]\n\nTheorem 5 서로 독립인 확률 변수 \\(X_1,X_2, ..., X_n\\) 의 적률 생성 함수가 각 각 \\(\\text{M}_{X_1}(t), \\text{M}_{X_2}(t), ..., \\text{M}_{X_n}(t)\\) 일 때, 확률 변수 \\(Y=X_1+X_2+...+X_n\\) 의 적률 생성함수 \\(\\text{M}_Y(t)\\) 는 \\(\\text{M}_{X_1}(t)\\text{M}_{X_2}(t) \\dots \\text{M}_{X_n}(t)\\) 이다.\n\\[\n\\begin{aligned}\n  \\text{M}_Y(t) &= E(e^{Yt})\\\\\n                &= E(e^{t(X_1+X_2+ ...+ X_n)})\\\\\n                &= E(e^{tX_1}e^{tX_2}\\dots e^{tX_n})\\\\\n                &= E(e^{tX_1})E(e^{tX_2})\\dots E(e^{tX_n}) \\because X_i \\text{ are independent}\\\\\n                &= \\text{M}_{X_1}(t)\\text{M}_{X_2}(t) \\dots \\text{M}_{X_n}(t)\n\\end{aligned}\n\\]\n\n\nExample) \\(X_1, X_2, ..., X_n\\) 가 서로 독립이고 parameter 가 각 각 \\(\\lambda_1, \\lambda_2, ..., \\lambda_n\\) 인 poisson 분포를 따른다면 \\(Y=X_1+X_2+...+X_n\\) 의 mgf는\n\n$$\n\\[\\begin{aligned}\n  \\text{M}_X(t) &= e^{\\lambda(e^t-1)}\\\\\n  \\text{M}_Y(t) &= E(e^{Yt})\\\\\n                &= E(e^{t(X_1+X_2+ ...+ X_n)})\\\\\n                &= E(e^{tX_1}e^{tX_2}\\dots e^{tX_n})\\\\\n                &= E(e^{tX_1})E(e^{tX_2})\\dots E(e^{tX_n}) \\because X_i \\text{ are independent}\\\\\n                &= \\text{M}_{X_1}(t)\\text{M}_{X_2}(t) \\dots \\text{M}_{X_n}(t)\\\\\n                &= e^{\\lambda_1(e^t-1)}+e^{\\lambda_2(e^t-1)}+\\dots+e^{\\lambda_n(e^t-1)}\\\\\n                &= e^{(\\lambda_1+\\lambda_2+\\dots+\\lambda_n)(e^t-1)}\\\\\n                &= e^{\\sum_{i=1}^{n}\\lambda_i(e^t-1)}\\\\\n\\end{aligned}\\]\n$$\n이다. 즉, Y는 parameter가 \\(\\sum_{i=1}^{n}\\lambda_i\\) 인 poisson 분포를 따른다. 뿐만 아니라, 정규 분포, poisson 분포, 카이제곱 분포의 경우 독립 변수들의 합으로 만든 분포도 같은 종류의 분포를 따르게 된다.\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-28_mgf/index.html#momment-generating-function",
    "href": "docs/blog/posts/statistics/2023-02-28_mgf/index.html#momment-generating-function",
    "title": "Momment Generating Function",
    "section": "",
    "text": "Definition 1 확률 변수 \\(X^r\\) 의 expectation, \\(\\text{E}((X-\\mu)^r)\\) 를 확률 변수 \\(X\\) 의 평균 \\(\\mu\\) 에 대한 \\(r\\) 차 중심 적률(moment) 이라하고 그 notation은 \\(\\mu_r'=\\text{E}((X-\\mu)^r)\\) 로 한다. \\(\\mu_r'=\\text{E}(X^r)\\) 은 원점에 대한 \\(r\\) 차 중심 적률이라 한다.\n즉,\n\\[\n\\begin{aligned}\n  \\mu_{r}'&=\n    \\begin{cases}\n      \\text{E}((X-\\mu)^r)&=\n        \\begin{cases}\n          \\sum_{x}(x-\\mu)^rf(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}(x-\\mu)^rf(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases} & \\text{moment about }\\mu\\\\\n      \\text{E}(X^r)&=\n        \\begin{cases}\n          \\sum_{x}x^rf(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}x^rf(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases} & \\text{moment about origin}\\\\\\\\\n    \\end{cases}  \n\\end{aligned}\n\\]\n이다.\n\n분포의 특징을 묘사하는 parameters 중 많은 종류가 확률 변수의 적률을 이용해 계산될 수 있다. 그 대표적인 예가\n\n평균 (mean): 분포의 위치를 나타내는 척도, 1차 중심 적률로 계산\n분산 (variance): 분포가 평균으로부터 퍼진 정도를 나타내는 척도, 2차 중심 적률로 계산\n왜도 (skewedness): 분포가 기울어진 방향과 정도를 나타내는 척도, 3차 중심 적률로 계산\n첨도 (kurtosis): 분포가 위로 뾰족한 정도를 나타내는 척도, 4차 중심 적률로 계산\n\n\nDefinition 2 확률 변수 \\(X\\) 의 적률 생성 함수 (Moment Generating Function, mgf), \\(\\text{M}_X(t) =\\text{E}(e^{tX})\\) 로 정의한다.\n즉,\n\\[\n\\begin{aligned}\n  M_X(t)=\\text{E}(e^{tX})&=\n        \\begin{cases}\n          \\sum_{x}e^{tx}f(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}e^{tx}f(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases}\n\\end{aligned}\n\\]\n이다.\n\n\nTheorem 1 확률 변수 \\(X\\) 의 적률 생성 함수 (Moment Generating Function, mgf), \\(\\text{M}_X(t) =\\text{E}(e^{tX})\\) 로 r차 적률 계산은 다음과 같이 할 수 있다.\n\\[\n\\begin{aligned}\n  \\frac{d^r}{dt^r}M_X(t) \\bigg|_{t=0}=M_X^r(0)=\\text{E}(X^r)=\\mu_r'\n\\end{aligned}\n\\]\n이다. 즉, 적률 생성 함수 (mgf) \\(M_X(t)\\) 를 구하고 r 번 미번한 후에 \\(t=0\\) 대입하면 r차 중심 적률을 구할 수 있다.\n\nproof (for the only continuous case)\n\\[\n\\begin{aligned}\n  \\text{First Order Moment}\\\\\n  \\frac{d}{dt}M_X(t)&= \\frac{d}{dt}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d}{dt}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}\\frac{d}{dt}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}xe^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}xf(x)dx\\\\\n    &=  \\text{E}(X)\\\\\n    &=  \\mu_1'\\\\\n  \\text{Second Order Moment}\\\\\n  \\frac{d^2}{dt^2}M_X(t)&= \\frac{d^2}{dt^2}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d^2}{dt^2}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}\\frac{d}{dt}xe^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^2e^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^2f(x)dx\\\\\n    &=  \\text{E}(X^2)\\\\\n    &=  \\mu_2'\\\\\n  \\vdots\\\\\n  \\text{r th Order Moment}\\\\\n  \\frac{d^r}{dt^r}M_X(t)&= \\frac{d^r}{dt^r}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d^r}{dt^r}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^re^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^rf(x)dx\\\\\n    &=  \\text{E}(X^r)\\\\\n    &=  \\mu_r'\n\\end{aligned}\n\\]\n\n\n\n\n\nMGF of \\(X\\sim B(n,p)\\)\n\n확률 변수 \\(X \\sim B(n,p)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n\\[\n\\begin{aligned}\n  f(x)&=\\binom{n}{x}p^xq^{n-x}\\\\\n  \\sum_{x=0}^{n}f(x)&=\\sum_{x=0}^{n}\\binom{n}{x}p^xq^{n-x}=(p+q)^n\\\\\n  M_X(t)&=\\sum_{x=0}^{n}e^{tx}f(x)=\\sum_{x=0}^{n}e^{tx}\\binom{n}{x}p^xq^{n-x}\\\\\n        &=\\sum_{x=0}^{n}\\binom{n}{x}(pe^t)^xq^{n-x}=(pe^t+q)^n\\\\\n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}(pe^t+q)^n\\\\\n    &=n(pe^t+q)^{n-1}pe^t\\bigg|_{t=0}\\\\\n    &=n(p+q)^{n-1}p=np\\\\\n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}(n(pe^t+q)^{n-1}pe^t)\\bigg|_{t=0}\\\\\n    &=(n(n-1)(pe^t+q)^{n-2}pe^tpe^t+n(pe^t+q)^{n-1}pe^t)\\bigg|_{t=0}\\\\\n    &=n(n-1)(p+q)^{n-2}p^2+n(p+q)^{n-1}p\\\\\n    &=n(n-1)p^2+np\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=n(n-1)p^2+np-(np)^2\\\\\n    &=n^2p^2-np^2+np-n^2p^2\\\\\n    &=np(1-p)=npq\n\\end{aligned}\n\\]\n\nMGF of \\(X\\sim Poisson(\\lambda)\\)\n\n확률 변수 \\(X \\sim Poisson(\\lambda)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n  M_X(t)&=\\sum_{x=0}^{\\infty}e^{tx}f(x)=\\sum_{x=0}^{\\infty}e^{tx}\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n        &=e^{-\\lambda}\\sum_{x=0}^{\\infty}\\frac{{e^{t}\\lambda^{x}}^x}{x!}\\\\\n        &=e^{-\\lambda}e^{\\lambda e^t} \\because \\text{(Maclaurin's Series)}\\\\\n        &=e^{\\lambda(e^t-1)} \\\\\n        (&\\text{Maclaurin's Series: } \\sum_{n=0}^{\\infty}\\frac{x^n}{n!}=1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+ ... =e^x)\\\\     \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda e^{e^t}e^{\\lambda(e^t-1)}\\bigg|_{t=0}=\\lambda\\\\\n  \n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}\\lambda e^{e^t}e^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda e^te^{\\lambda (e^t-1)}+\\lambda e^t\\lambda e^te^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda + \\lambda^2\\\\\n\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=\\lambda + \\lambda^2 -\\lambda^2\\\\\n    &=\\lambda\n\\end{aligned}\\]\n$$\n\n\n\n\nMGF of \\(X\\sim N(0,1)\\)\n\n확률 변수 \\(X \\sim N(0,1)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}(-\\infty &lt; x &lt; \\infty)\\\\\n  M_X(t)&=\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx=\\int_{-\\infty}^{\\infty}e^{tx}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{x^2}{2}+tx}dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(x-t)^2+\\frac{1}{2}t^2}dx\\\\\n        &=e^{\\frac{t^2}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(x-t)^2}dx\\\\\n        &=e^{\\frac{t^2}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}u^2}du \\text{  }(\\because x-t=u \\rightarrow dx=du)\\\\\n        &=e^{\\frac{t^2}{2}} \\\\     \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=te^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=0\\\\\n  \n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}te^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=e^{\\frac{t^2}{2}}+t(te^{\\frac{t^2}{2}})\\bigg|_{t=0}\\\\\n    &=1\\\\\n\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=1\n    \n\\end{aligned}\\]\n$$\n\nMGF of \\(X\\sim N(\\mu,\\sigma^2)\\)\n\n확률 변수 \\(X\\sim N(\\mu,\\sigma^2)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}(-\\infty &lt; x &lt; \\infty)\\\\\n  M_X(t)&=\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{\\infty}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}+tx}dx\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}(\\frac{(x-(t\\sigma^2+\\mu))}{\\sigma})^2}dx\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{u^2}{2}}du\\\\\n        (&\\because \\frac{x-(t\\sigma^2+\\mu)}{\\sigma}=u \\rightarrow \\frac{dx}{\\sigma}=du) \\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^2}{2}}du\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\\\\n  \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\bigg|_{t=0}\\\\\n    &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}(\\sigma^2t+\\mu)\\bigg|_{t=0}\\\\\n    &=\\mu\\\\\n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}e^{\\frac{t^2\\sigma^2}{2}+t\\mu}(\\sigma^2t+\\mu)\\bigg|_{t=0}\\\\\n    &=\\sigma^2+\\mu^2 \\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=\\sigma^2\n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-28_mgf/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-28_mgf/index.html#blog-guide-map-link",
    "title": "Momment Generating Function",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-03-25_MLE/index.html",
    "href": "docs/blog/posts/statistics/2023-03-25_MLE/index.html",
    "title": "Maximum Likelihood Estimation, Statistical Bias, and Point Estimation",
    "section": "",
    "text": "0.1 Definition\nTo talk about MLE (Maximum Likelihood Estimation), we need to recap the concepts and definitions of probability and likelihood. They are related but distinct concepts.\n\nprobability is a measure of the chance that an event will occur, given some prior knowledge or assumptions.\nlikelihood is a measure of the plausibility or compatibility of a particular set of model parameters, given the observed data.\n\n\n\n\n\n\n\nChance vs Plausibility (personal opinion)\n\n\n\n\nchance is a general term but closer term to statistics, which is used in the situation of the probability or likelihood of an event occurring based on randomness or uncertainty.\n\nplausibility chance is a term more often used in everyday life, which refers to the degree to which something is believable based on the available evidence or information.\n\n\n\n\nDefinition 1 Let \\(X_1, X_2, ..., X_n\\) be a set of iid random variables with pdf or pmf \\(f(x_i | \\theta)\\), where \\(\\theta\\) is a vector of unknown parameters. Then, the likelihood function is defined as the joint pdf or pmf of the observed data, given the values of the parameters:\n\\[\nL(\\theta | x_1, x_2, ..., x_n) = L(\\theta | \\mathbf x) = \\prod_{i=1}^n f(x_i | \\theta)\n\\]\n\nThe likelihood function is a function of the parameters \\(\\theta\\). The function measures the probability of observing a set of data, given the values of the parameters of a statistical model in order to estimate the values of the parameters by finding the values that maximize the likelihood function. The likelihood function is often used in the maximum likelihood estimation (MLE) method, where the MLE estimator is the set of parameter values that maximize the likelihood function.\nThe likelihood function is related to the concept of conditional probability. Given a set of observed data \\(x_1, x_2, ..., x_n\\), the likelihood function measures the probability of observing these data, assuming a particular set of parameter values. The likelihood function is not a probability distribution, but it can be used to derive a probability distribution for the parameters, known as the posterior distribution, using Bayes’ theorem.\n\n\n\n\n\n\nProbability\n\n\n\nProbability is a measure of the likelihood or chance that an event will occur, which is used to quantify uncertainty and randomness.\nprobability is a function that maps a real number mapped from a random variable into \\([0,1]\\). The probability function, denoted by \\(P\\), satisfies the following axioms:\n\nNon-negativity: For any event \\(A \\in \\Omega\\), \\(P(A) \\geq 0\\).\nNormalization: The probability of the entire sample space is 1, i.e., \\(P(\\Omega) = 1\\).\nAdditivity: For any two disjoint events \\(A, B \\in \\Omega\\), or \\(A \\cap B = \\emptyset\\), the probability of their union is equal to the sum of their individual probabilities, i.e., \\(P(A \\cup B) = P(A) + P(B)\\).\n\n\n\n\n0.1.1 Probability vs Likelihood\nProbability and likelihood are related but distinct concepts in statistics.\n\nProbability refers to the measure of the likelihood that a particular event will occur, scaled on \\([0,1]\\). It is calculated based on a known probability distribution (= some prior knowledge or assumptions) before the data is observed.\nOn the other hand, likelihood refers to the probability of observing a set of data given a particular set of parameter values in a statistical model. It is calculated based on the unknown parameters after the data is observed.\n\nThe likelihood function is used to estimate the values of the parameters by finding the parameter values that maximize the likelihood function.\nFor instance of a coin flip, the probability of getting heads on a coin flip is 0.5, regardless of whether the coin has been flipped or not (i.e., without data). In contrast, the likelihood of observing heads after a coin has been flipped depends on the parameter of interest. We need to find the parameter given data, the results of multiple coin flips.\nIf we want to estimate the probability of heads, we can use the maximum likelihood estimation (MLE) approach, which involves finding the value of the coin bias that maximizes the likelihood of observing the observed sequence of heads and tails. The likelihood of the data is calculated using the binomial distribution, which gives the probability of observing a certain number of heads, given the number of tosses and the coin bias. In this case, the likelihood function is a function of the coin bias, and the probability of heads is the value of the coin bias that maximizes the likelihood function.\n\n\n0.1.2 Relation between Likelihood and PMF or PDF\nThe likelihood function is closely related to pmf or pdf of the data, which is a function that describes the probability of observing a particular value or range of values for the data, given the model parameters. The pmf or pdf is a function of the data, not the parameters, and is often written as \\(f(x|\\theta)\\). The likelihood function is proportional to the pdf because is a product of pdfs when \\(X_i\\) is independent, but with the data fixed and the parameter values treated as variables.\n\n\nCode\nlibrary(tidyverse)\n\n# Probability of getting heads on a fair coin flip\nprob &lt;- 0.5\n\n# likelihood\n## Simulate a coin flip with a biased coin\nset.seed(123) # Set random seed for reproducibility\nn &lt;- 100 # flipping numbers\np &lt;- 0.2 # Probability of getting heads\nx &lt;- rbinom(n, size = 1, prob = p) # Simulate n coin flips\nx%&gt;%head(10)\n\n\n [1] 0 0 0 1 1 0 0 1 0 0\n\n\nCode\n## Calculate the likelihood of observing the data given the parameter value p\nlikelihood &lt;- prod(dbinom(x, size = 1, prob = p))\nlikelihood%&gt;%round()\n\n\n[1] 0\n\n\nIn this example, prob is the assumed probability of getting heads on a fair coin flip. The probability of getting heads on a fair coin flip is 0.5, which is a fixed value that does not depend on any specific data. On the other hand, p is the probability of getting heads for the biased coin that we are simulating. The likelihood of observing a set of coin flips depends on the parameter value, which is unknown (actually, we know that it was \\(p=0.2\\)), and the observed data. We simulate a set of 100 coin flips with a biased coin that has a probability of 0.2 of getting heads. We then calculate the likelihood of observing this data given the parameter value of 0.2, which is the product of the probability mass function for each flip.\nHowever, in a real-world scenario, we would not know the true value of p and we would need to estimate it based on the observed data. By finding the Maximum Likelihood Estimation (MLE) of p, we are estimating the value of p that is most likely to have generated the observed data. To find MLE of p that maximizes the likelihood function, we can use numerical optimization methods.\nAs n increases, the product term in the likelihood function \\(\\prod_{i=1}^{n} f(x_i; \\theta)\\), where \\(f\\) is the pdf or pmf of the distribution being used, can become very small (since it is a product of values less than 1) and may result in numerical underflow (i.e., the product becomes so small that it rounds down to 0 in computer calculations). In practice, we typically take the logarithm of the likelihood function, called the log-likelihood, to avoid this issue:\n\\[\n\\log L(\\theta|x_1, x_2, ..., x_n) = \\sum_{i=1}^{n} \\log f(x_i; \\theta)\n\\]\nUsing the logarithm allows us to convert the product of small probabilities into a sum of log-probabilities, which are typically easier to work with numerically and mathematically. In this case, as n increases, the sum term in the log-likelihood can decrease (since it is a sum of negative values), but the decrease may not be as severe as in the product term of the likelihood function because of the log scale converting very small or large values into larger or smaller values .\n\nDefinition 2 The MLE estimator \\(\\hat{\\theta}\\) is the value of the parameter vector that maximizes the likelihood function:\n\\[\n\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} L(\\theta | x_1, x_2, ..., x_n)\n\\]\nor equivalently, maximizes the log-likelihood function:\n\\[\n\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\log L(\\theta | x_1, x_2, ..., x_n)\n\\]\n\nThe Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a statistical model by finding the values of the parameters that maximize the likelihood function. The likelihood function is the probability of observing the data, given the parameters of the model. The MLE estimator is the set of parameter values that maximize the likelihood function. In other words, the MLE is the set of parameter values that make the observed data most probable, given the assumed probability distribution. The likelihood function is typically the product or the sum (depending on whether the observations are assumed to be independent or not) of the probabilities or probability densities of the observations, evaluated at the values of the parameters.\nThe MLE estimator has desirable statistical properties, such as consistency, efficiency, and asymptotic normality, under certain regularity conditions on the likelihood function and the parameter space. However, it is important to note that the MLE is not always the best estimator for a given problem, and other estimation methods may be more appropriate depending on the specific characteristics of the data and the model.\nThe likelihood function is the joint probability density (or mass) function of the data, viewed as a function of the parameters, and we find the maximum of this function by differentiating it with respect to the parameters and setting the derivative to zero.\n\n\n\n0.2 MLE of OLS\nIn a linear regression, the maximum likelihood estimate of the ordinary least squares (OLS) coefficients is equivalent to the least squares estimate. To derive this, we assume that \\(\\epsilon_i \\sim N(0,\\sigma^2)\\) and that the observations are independent. Then, the likelihood function for the data \\(Y = (Y_1, Y_2, \\dots, Y_n)\\) is:\n\\[\nL(Y|\\theta) =L(Y|\\beta,\\sigma^2) = (2\\pi\\sigma^2)^{-\\frac{n}{2}} e^{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\beta)^2}\n\\]\nwhere \\(X_i\\) is the \\(i\\) th row of the design matrix \\(X\\) and \\(\\beta\\) is the vector of regression coefficients.\nTo find the maximum likelihood estimates of \\(\\beta\\) and \\(\\sigma^2\\), we maximize the likelihood function with respect to these parameters. Taking the log of the likelihood function and simplifying, we obtain:\n\\[\n\\log L(Y|\\theta) = \\log L(Y|\\beta,\\sigma^2) = -\\frac{n}{2} \\log (2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\beta)^2\n\\]\nTo maximize this function with respect to \\(\\beta\\), we differentiate with respect to \\(\\beta\\) and set the derivative to zero, \\(\\frac{\\partial}{\\partial \\theta} \\log L(Y|\\theta) = 0\\):\nSolving for \\(\\beta\\), we obtain:\n\\[\n\\frac{\\partial}{\\partial \\theta} \\log L(Y|\\beta,\\sigma^2) = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n 2X_i(Y_i - X_i\\beta) = 0\n\\]\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\] , which is the OLS estimate of \\(\\beta\\).\nSolving for \\(\\sigma^2\\), we differentiate with respect to \\(sigma^2\\) and set the derivative to zero:\n\\[\n\\frac{\\partial}{\\partial \\sigma^2} log L(Y|\\beta,\\sigma^2) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^n (Y_i - X_i\\beta)^2 = 0\n\\] \\[\n\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (Y_i - X_i\\beta)^2}{n}\n\\]\nwhich is the OLS estimate of \\(\\sigma^2\\).\nTherefore, we see that the maximum likelihood estimates of \\(\\beta\\) and \\(\\sigma^2\\) in linear regression with normally distributed errors are equivalent to the OLS estimates of these parameters.\n\n\n0.3 Statistical Bias\nStatistical bias refers to a systematic error or deviation in the results of a statistical analysis that is caused by factors other than chance. A biased estimator is one that consistently produces estimates that are systematically different from the true value of the parameter being estimated.\n\n\n\n\n\n\nThere are 5 Types of bias\n\n\n\nThe above article discusses five types of statistical bias that analysts, data scientists, and other business professionals should be aware of to minimize their effects on the final results.\n\nselection bias: data selection methods are not truly random, leading to unequal representation of the population.\nbias in assignment: pre-existing differences between groups in an experiment can affect the outcome, a.k.a allocation bias, treatment assignment bias, or exposure assignment bias.\nconfounders: additional variables not accounted for in the experimental design can impact the results.\nself-serving bias: individuals tend to downplay undesirable qualities and overemphasize desirable ones a.k.a cognitive bias. In other words, people tend to take credit for their successes and blame outside factors for their failures.\nexperimenter expectations: researchers can unconsciously influence the data through verbal or non-verbal cues.\n\nBeing aware of these biases can lead to better models and more reliable insights for data-backed business decisions.\nSource: Article Written by Jenny Gutbezahl\n\n\nIt is important to detect and correct for bias in statistical analyses, as biased estimates can lead to incorrect conclusions and decisions. One way to correct for bias is to use an unbiased estimator, which is one that has a zero bias, i.e., its expected value is equal to the true value of the parameter being estimated.\n\nDefinition 3 An estimator \\(\\hat{\\theta}\\) is said to be biased if\n\\[\n\\operatorname{E}(\\hat{\\theta})\\ne \\theta\n\\]\nwhere \\(\\operatorname{E}(\\hat{\\theta})\\) is the expected value of the estimator \\(\\hat{\\theta}\\), and \\(\\theta\\) is the true value of the parameter being estimated.\n\nAn estimator is said to be unbiased if its expected value is equal to the true value of the parameter being estimated. In other words, an estimator is unbiased if, on average, it gives an estimate that is equal to the true value of the parameter."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html",
    "title": "Variables",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n어떤 현상이나 사물의 의미를 추상적인 용어를 사용하여 관념적으로 구성한 것\nex) 성 (gender)\n\n\n\n\n\n한 연속선상에서 둘 이상의 값을 가지는 개념\nex) 성별(sex)\n\n\n\n\n\n결코 변하지 않는 단 하나의 값\nex) 남자, 여자\n\n\n\n\n\n\n\n\n독립변수 (independent variable) 는 조사하고자 하는 사건이나 상황을 독립적으로 발생시키는 원인이 되는 변수로서 원인변수, 설명변수, 예측변수 라고도 부름\nif s + v, s + v 에서 if s + v 에서 독립 변수의 정보를 추출해낼 수 있다.\n예를 들어, 사람이 운동을 하면 근육량이 늘어 난다.\n여기서 사람이 운동을 하면 부분에서 독립 변수를 뽑아낸다면 운동의 유무가 독립변수가 될 수 있다. 근육량의 변화는 운동의 유무에 따라 결정되는 원인 부분이기 때문이다.\n즉, 운동 (독립변수) \\(\\rightarrow\\) 근육량 (종속변수)\n\n\n\n\n\n종속변수 (dependent variable) 는 다른 변수에 영향을 받는 변수\n다른 변수에 영향을 미칠 수 없는 변수\n인과 관계에서 결과(effect)를 나타냄\n결과변수, 피설명변서, 피예측변수, 반응변수, 가설적 변수라고도 부른다.\nif s + v, s + v 에서 s + v 에서 독립 변수의 정보를 추출해낼 수 있다.\n예를 들어, 사람이 운동을 하면 근육량이 늘어난다.\n여기서 근육량이 늘어난다. 부분에서 종속 변수가 뽑힌다. 근육량이 종속변수가 될 수 있다. 근육량은 운동의 유무에 따라 그 효과가 영향을 받는 결과 부분이기 때문이다.\n즉, 운동 (독립변수) \\(\\rightarrow\\) 근육량 (종속변수)\n\n\n\n\n\n매개변수 (intervening or mediating variable)는 두 변수는 서로 직접적인 관계가 약하거나 없는데 두 변수가 간접적으로 관계를 갖는 것처럼 보이도록 하는 변수\n독립변수와 종속변수 사이에 개입하여 두 변수를 연결하는 변수\n매개변수는 독립변수의 결과변수인 동시에 종속변수의 원인이 되는 변수\n독립변수 \\(\\rightarrow\\) 매개변수 \\(\\rightarrow\\) 종속변수\n\n여기서 독립변수와 종속변수는 직접적인 관계가 없지만, 매개 변수에 의해 마치 관계를 갖는 것처럼 보임\n\n예를 들어, 매개 변수의 효과가 선행연구가 뒷받침 되거나 자명한 관계가 보여야 한다.\n\n여가 문화 요인의 매개효과를 고려한 장애인의 경제상태가 삶의 만족도에 미치는 영향\n독립변수: 경제 상태\n종속변수: 삶의 만족도\n매개변수: 여가 문화\n경제 상태는 여가 문화의 독립 변수로서 돈이 많으면 여가 문화를 즐길 수 있는 비용을 지불할 수 있는 원인이 된다.\n여가 문화는 삶의 만족도의 독립 변수로서 여가 문화를 즐김으로 인해 본인이 원하는 취미 활동을 향유함으로써 삶의 만족도가 증가하는 원인이 된다.\n\n예를 들어, 학업 집중도의 매개 효과를 고려한 수면의 질이 학업 성취도에 미치는 영향\n\n독립변수: 수면의 질\n종속변수: 학업 성취도\n매개변수: 학업 집중도\n수면의 질은 학업 집중도에 대한 독립 변수로서 수면의 질이 높으면 졸지 않고 집중도가 올라가는 경향의 원인이 된다.\n학업 집중도는 학업성취도의 독립 변수로서 학업 집중도가 올라가면 학업 성취도가 올라감으로써 학업 성취도 상승에 대한 원인된다.\n하지만, 수면의 질이 학업 성취도에 대한 연관성은 반드시 보장되진 않는다.\n\n데이터 분석에서 다음과 같은 증거가 있어야 한다.\n\n데이터 분석에서 경제 상태 (독립 변수)와 삶의 만족도 (종속 변수)가 강한 상관관계가 나타나야한다\n경제 상태 (독립 변수)와 여가 문화 (매개 변수)가 강한 상관관계가 나타나야한다.\n여가 문화 (매개 변수)와 삶의 만족도 (종속 변수)와 강한 상관관계가 나타나야한다.\n완전 매개의 경우: 회귀 모형에 경제 상태 (독립 변수)와 여가 문화(매개 변수)를 넣었을 때 사람의 만족도 (종속 변수)에 대한 경제 상태 (독립 변수)의 유의성이 낮아야 하고 여가 문화 (매개 변수)의 유의성이 높으면 여가 문화는 완전 매개 변수로 간주될 수 있다.\n부분 매개의 경우: 경제 상태 (매개 변수)의 유의성이 기준치 이상이지만 낮아졌을 경우 여가 문화는 부분 매개 변수로 간주될 수 있다.\n\n한계점\n\n매개 변수는 종종 성격, 지능, 태도, 문화와 같은 가상의 구성물일 수 있다.\n그래서, 정량화 시키기 힘들 수 있다.\n\n\n\n\n\n\n조절변수 (moderating variable) 는 독립변수가 종속변수에 원인이 되며 영향을 미치는 영향력의 강도, 세기 , 방향을 조절하는 변수\n독립변수 \\(\\xrightarrow{\\text{조절변수}}\\) 종속변수\n예를 들어, 따돌림 피해자 학생에 대한 교사의 지지도에 따라 집단따돌림이 자존감에 미치는 영향\n\n독립변수: 집단 따돌림\n종속변수: 학생의 자존감\n매개변수: 교사의 지지도\n집단 따돌림은 학생의 자존감에 악영향을 미친다는 선행 연구나 자명한 관계가 있다고 가정\n교사가 피해자 학생에게 무관심하다면 피해자 학생에 대한 학교폭력이 가중되면서 자존감이 더 하락할 것.\n교사가 피해자 학생에게 칭찬과 응원을 한다면 피해자 학생은 자존감이 올라갈 것.\n\n예를 들어, 자녀유무에 따라 부부간 의사소통이 이혼에 미치는 영향\n\n독립변수: 부부간 의사소통\n종속변수: 이혼률\n매개변수: 자녀 유무\n부부간 의사소통이 없다면 이혼률이 상승한다는 선행연구나 자명한 관계가 있다고 가정\n부부간 의사소통이 없다면 이혼가능성이 올라간다는 전제가 있고 자녀가 있다면 이혼 가능성이 낮아 질 수 있음\n\n데이터 분석에서 다음과 같은 증거가 있어야 한다.\n\n데이터 분석에서 독립 변수와 종속 변수가 강한 상관관계가 나타나야 한다\n회귀 모형에 독립 변수와 조절 변수의 interaction term을 넣었을 때 독립변수, 조절변수, interaction term의 유의성이 있어야한다.\n\n\n\n\n\n\n외생변수 (extraneous variable)는 독립변수 외에 종속변수에 영향을 주는 변수로서 원래 관계가 없는 변수를 관계가 있는 것처럼 만들어 종속 변수에 대한 가짜 독립변수가 만들어진다 (가식적 관계 또는 허위관계)\n외생변수를 통제하면 관계가 있는 것으로 나타났던 독립변수와 종속변수의 관계가 사라짐\n외새변수는 반드시 통제해줘야 함\n구조 방정식에서 독립 변수의 개념, 다른 변수에 영향을 주는 변수. 즉 기존에 원인인 것처럼 보이던 가짜 독립 변수가 아니라 실제 원인인 외생변수가 종속변수에 대한 진짜 독립변수.\n독립변수 \\(\\xleftarrow{\\text{+}}\\) 외생변수 \\(\\xrightarrow{\\text{+}}\\) 종속변수\n예를 들어, 무릎이 쑤시면 비가 온다.\n\n기압이 낮으면 무릎의 통증을 유발하고 동시에 비가 내리는 원인이 된다. 기압의 존재를 찾아내지 못하면 마치 무릎의 통증이 비를 내리게 하는 원인인 것 처럼 인식한다.\n대기압이 낮으면 비구름이 고기압에서 저기압으로 이동하면서 비를 내리게 하고 동시에 대기압이 낮을때 상대적으로 체내의 내부 기압이 높아져 팽창하여 관절이 부딪히게 된다.\n\n외생 변수를 간과할 때 발생하는 문제\n\nselection bias 발생\n\nsampling bias or ascertainment biase: 특정 samples이 포함되거나 포함되지 않음\nattrition bias: study에 중도하차한 samples이 study에 남은 samples과 systematic difference 발생\nsurvivorship bias: 결론 도출시 연구자 또는 분석가가 전체 cases를 고려하지 않고 성공 cases에만 집중하여 결론도출\nNonresponse bias: 설문에 응답하지 않은 samples이 설문에 응답한 samples과 다르게 선정됨\nundercoverage biase: 모집단의 일부가 sample에 대표성이 결여\n\n\n\n\n\n\n\n내생변수 (intraneous variable)는 구조 방정식에서 사용되는 용어로 다른 변수로부터 영향을 받는 변수로서 모형 안에서 그 값이 결정되는 변수\n구조 방정식에서 종속변수에 해당.\n구조 방정식은 수많은 변수들을 한번에 고려하는 모형으로 네트 워크 분석이 요구된다. 구조 방정식 안에서는\n\n외생변수 (원인) \\(\\rightarrow\\) 내생변수 (결과) 의 관계를 갖는다.\n이때, 외생 변수와 내생 변수에 많은 변수들이 관계를 맺는다.\n\n\n\n\n\n\n억압변수 (suppressor variable)은 원래 관계가 있는데 관계가 없는 것처럼 보이게 하는 변수\n독립변수와 종속변수 중 하나의 변수와는 양의 상관관계가 있고 다른 하나의 변수와는 음의 상관관계가 있어 독립변수와 종속변수 간에 마치 아무런 관계가 없는 것처럼 보이게 만드는 변수\n두 변수 간에 실제 존재하는 관계를 드러나지 못하게 억누른다는 의미에서 억밥 변수라고 부름\n억압 변수를 통제하면 독립변수와 종속 변수의 참된 관계가 드러남\n외생 변수와 마찬가지로 억압변수는 반드시 통제되어야한다.\n독립변수 (+) \\(\\leftarrow\\) 억압변수 \\(\\rightarrow\\) (-) 종속변수\n예를 들어, 교육 수준이 높으면 소득수준이 낮을 것이다.\n\n위의 명제는 마치 거짓인 것처럼 보이지만 고령자란 변수를 억압변수로 설정하면 교육 수준이 높은 고령자의 경우 젊었을 때 돈을 많이 벌어놨기 때문에 은퇴후 돈을 벌지 않아도 된다. 그러므로 소득 수준이 낮기 때문에 위의 명제가 참이 될 수 있다.\n\n\n\n\n\n\n통제변수 (control variable)는 독립변수와 종속변수의 관계에 영향을 미칠만한 제 3의 변수로서 외생변수, 매개변수, 조절변수, 억제변수들 중 연구자가 중점적으로 보고자 하는 변수들의 실제적 관계를 검증하기 위해 조사과정에 영향을 미치지 않도록 실제로 통제하는 변수\n즉, 통제변수란 독립변수와 종속변수의 인과관계에 영향을 주는 제 3의 변수 중 조사설계에서 조사자가 통제하려는 변수\n일반적으로 하나의 종속변수에 수많은 독립변수가 있지만 분석가가 관심있어하는 독립 변수의 종속변수에 대한 순수한 효과를 관찰하기 위해 제 3의 변수들 (통제 변수)을 통제하거나 일정하게 유지해줘야 관찰이 가능하다.\n\n\n\n\n\n\n\n\n이산변수는 명목척도 (nominal scale), 서열척도(ordinal scale)로 측정되는 변수\n이산이란 각 값의 사이가 떨어져 있어 그 사이의 값은 아무런 의미가 없다는 뜻\n성별, 종교 등\n예를 들어, 남성을 1, 여성을 2라고 값을 부여했을 때 1과 2사이의 값은 존재할 수 없음\n\n\n\n\n\n연속변수는 등간척도 (interval scale)와 비율척도 (ratio scale)로 측정된 변수\n키, 연령, 무게 등\n\n\n\n\n\n양이 아닌 해석 또는 의미가 중요한 변수. 성별 종교 등 (성별 1,2가 양을 의미하는 것이 아님)\n\n\n\n\n\n키 연령 무게 등 - 측정값이 양을 의미함\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#concept",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#concept",
    "title": "Variables",
    "section": "",
    "text": "어떤 현상이나 사물의 의미를 추상적인 용어를 사용하여 관념적으로 구성한 것\nex) 성 (gender)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#concept-of-variable",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#concept-of-variable",
    "title": "Variables",
    "section": "",
    "text": "한 연속선상에서 둘 이상의 값을 가지는 개념\nex) 성별(sex)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#constant",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#constant",
    "title": "Variables",
    "section": "",
    "text": "결코 변하지 않는 단 하나의 값\nex) 남자, 여자"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#독립변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#독립변수",
    "title": "Variables",
    "section": "",
    "text": "독립변수 (independent variable) 는 조사하고자 하는 사건이나 상황을 독립적으로 발생시키는 원인이 되는 변수로서 원인변수, 설명변수, 예측변수 라고도 부름\nif s + v, s + v 에서 if s + v 에서 독립 변수의 정보를 추출해낼 수 있다.\n예를 들어, 사람이 운동을 하면 근육량이 늘어 난다.\n여기서 사람이 운동을 하면 부분에서 독립 변수를 뽑아낸다면 운동의 유무가 독립변수가 될 수 있다. 근육량의 변화는 운동의 유무에 따라 결정되는 원인 부분이기 때문이다.\n즉, 운동 (독립변수) \\(\\rightarrow\\) 근육량 (종속변수)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#종속변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#종속변수",
    "title": "Variables",
    "section": "",
    "text": "종속변수 (dependent variable) 는 다른 변수에 영향을 받는 변수\n다른 변수에 영향을 미칠 수 없는 변수\n인과 관계에서 결과(effect)를 나타냄\n결과변수, 피설명변서, 피예측변수, 반응변수, 가설적 변수라고도 부른다.\nif s + v, s + v 에서 s + v 에서 독립 변수의 정보를 추출해낼 수 있다.\n예를 들어, 사람이 운동을 하면 근육량이 늘어난다.\n여기서 근육량이 늘어난다. 부분에서 종속 변수가 뽑힌다. 근육량이 종속변수가 될 수 있다. 근육량은 운동의 유무에 따라 그 효과가 영향을 받는 결과 부분이기 때문이다.\n즉, 운동 (독립변수) \\(\\rightarrow\\) 근육량 (종속변수)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#매개변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#매개변수",
    "title": "Variables",
    "section": "",
    "text": "매개변수 (intervening or mediating variable)는 두 변수는 서로 직접적인 관계가 약하거나 없는데 두 변수가 간접적으로 관계를 갖는 것처럼 보이도록 하는 변수\n독립변수와 종속변수 사이에 개입하여 두 변수를 연결하는 변수\n매개변수는 독립변수의 결과변수인 동시에 종속변수의 원인이 되는 변수\n독립변수 \\(\\rightarrow\\) 매개변수 \\(\\rightarrow\\) 종속변수\n\n여기서 독립변수와 종속변수는 직접적인 관계가 없지만, 매개 변수에 의해 마치 관계를 갖는 것처럼 보임\n\n예를 들어, 매개 변수의 효과가 선행연구가 뒷받침 되거나 자명한 관계가 보여야 한다.\n\n여가 문화 요인의 매개효과를 고려한 장애인의 경제상태가 삶의 만족도에 미치는 영향\n독립변수: 경제 상태\n종속변수: 삶의 만족도\n매개변수: 여가 문화\n경제 상태는 여가 문화의 독립 변수로서 돈이 많으면 여가 문화를 즐길 수 있는 비용을 지불할 수 있는 원인이 된다.\n여가 문화는 삶의 만족도의 독립 변수로서 여가 문화를 즐김으로 인해 본인이 원하는 취미 활동을 향유함으로써 삶의 만족도가 증가하는 원인이 된다.\n\n예를 들어, 학업 집중도의 매개 효과를 고려한 수면의 질이 학업 성취도에 미치는 영향\n\n독립변수: 수면의 질\n종속변수: 학업 성취도\n매개변수: 학업 집중도\n수면의 질은 학업 집중도에 대한 독립 변수로서 수면의 질이 높으면 졸지 않고 집중도가 올라가는 경향의 원인이 된다.\n학업 집중도는 학업성취도의 독립 변수로서 학업 집중도가 올라가면 학업 성취도가 올라감으로써 학업 성취도 상승에 대한 원인된다.\n하지만, 수면의 질이 학업 성취도에 대한 연관성은 반드시 보장되진 않는다.\n\n데이터 분석에서 다음과 같은 증거가 있어야 한다.\n\n데이터 분석에서 경제 상태 (독립 변수)와 삶의 만족도 (종속 변수)가 강한 상관관계가 나타나야한다\n경제 상태 (독립 변수)와 여가 문화 (매개 변수)가 강한 상관관계가 나타나야한다.\n여가 문화 (매개 변수)와 삶의 만족도 (종속 변수)와 강한 상관관계가 나타나야한다.\n완전 매개의 경우: 회귀 모형에 경제 상태 (독립 변수)와 여가 문화(매개 변수)를 넣었을 때 사람의 만족도 (종속 변수)에 대한 경제 상태 (독립 변수)의 유의성이 낮아야 하고 여가 문화 (매개 변수)의 유의성이 높으면 여가 문화는 완전 매개 변수로 간주될 수 있다.\n부분 매개의 경우: 경제 상태 (매개 변수)의 유의성이 기준치 이상이지만 낮아졌을 경우 여가 문화는 부분 매개 변수로 간주될 수 있다.\n\n한계점\n\n매개 변수는 종종 성격, 지능, 태도, 문화와 같은 가상의 구성물일 수 있다.\n그래서, 정량화 시키기 힘들 수 있다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#조절변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#조절변수",
    "title": "Variables",
    "section": "",
    "text": "조절변수 (moderating variable) 는 독립변수가 종속변수에 원인이 되며 영향을 미치는 영향력의 강도, 세기 , 방향을 조절하는 변수\n독립변수 \\(\\xrightarrow{\\text{조절변수}}\\) 종속변수\n예를 들어, 따돌림 피해자 학생에 대한 교사의 지지도에 따라 집단따돌림이 자존감에 미치는 영향\n\n독립변수: 집단 따돌림\n종속변수: 학생의 자존감\n매개변수: 교사의 지지도\n집단 따돌림은 학생의 자존감에 악영향을 미친다는 선행 연구나 자명한 관계가 있다고 가정\n교사가 피해자 학생에게 무관심하다면 피해자 학생에 대한 학교폭력이 가중되면서 자존감이 더 하락할 것.\n교사가 피해자 학생에게 칭찬과 응원을 한다면 피해자 학생은 자존감이 올라갈 것.\n\n예를 들어, 자녀유무에 따라 부부간 의사소통이 이혼에 미치는 영향\n\n독립변수: 부부간 의사소통\n종속변수: 이혼률\n매개변수: 자녀 유무\n부부간 의사소통이 없다면 이혼률이 상승한다는 선행연구나 자명한 관계가 있다고 가정\n부부간 의사소통이 없다면 이혼가능성이 올라간다는 전제가 있고 자녀가 있다면 이혼 가능성이 낮아 질 수 있음\n\n데이터 분석에서 다음과 같은 증거가 있어야 한다.\n\n데이터 분석에서 독립 변수와 종속 변수가 강한 상관관계가 나타나야 한다\n회귀 모형에 독립 변수와 조절 변수의 interaction term을 넣었을 때 독립변수, 조절변수, interaction term의 유의성이 있어야한다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#외생변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#외생변수",
    "title": "Variables",
    "section": "",
    "text": "외생변수 (extraneous variable)는 독립변수 외에 종속변수에 영향을 주는 변수로서 원래 관계가 없는 변수를 관계가 있는 것처럼 만들어 종속 변수에 대한 가짜 독립변수가 만들어진다 (가식적 관계 또는 허위관계)\n외생변수를 통제하면 관계가 있는 것으로 나타났던 독립변수와 종속변수의 관계가 사라짐\n외새변수는 반드시 통제해줘야 함\n구조 방정식에서 독립 변수의 개념, 다른 변수에 영향을 주는 변수. 즉 기존에 원인인 것처럼 보이던 가짜 독립 변수가 아니라 실제 원인인 외생변수가 종속변수에 대한 진짜 독립변수.\n독립변수 \\(\\xleftarrow{\\text{+}}\\) 외생변수 \\(\\xrightarrow{\\text{+}}\\) 종속변수\n예를 들어, 무릎이 쑤시면 비가 온다.\n\n기압이 낮으면 무릎의 통증을 유발하고 동시에 비가 내리는 원인이 된다. 기압의 존재를 찾아내지 못하면 마치 무릎의 통증이 비를 내리게 하는 원인인 것 처럼 인식한다.\n대기압이 낮으면 비구름이 고기압에서 저기압으로 이동하면서 비를 내리게 하고 동시에 대기압이 낮을때 상대적으로 체내의 내부 기압이 높아져 팽창하여 관절이 부딪히게 된다.\n\n외생 변수를 간과할 때 발생하는 문제\n\nselection bias 발생\n\nsampling bias or ascertainment biase: 특정 samples이 포함되거나 포함되지 않음\nattrition bias: study에 중도하차한 samples이 study에 남은 samples과 systematic difference 발생\nsurvivorship bias: 결론 도출시 연구자 또는 분석가가 전체 cases를 고려하지 않고 성공 cases에만 집중하여 결론도출\nNonresponse bias: 설문에 응답하지 않은 samples이 설문에 응답한 samples과 다르게 선정됨\nundercoverage biase: 모집단의 일부가 sample에 대표성이 결여"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#내생변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#내생변수",
    "title": "Variables",
    "section": "",
    "text": "내생변수 (intraneous variable)는 구조 방정식에서 사용되는 용어로 다른 변수로부터 영향을 받는 변수로서 모형 안에서 그 값이 결정되는 변수\n구조 방정식에서 종속변수에 해당.\n구조 방정식은 수많은 변수들을 한번에 고려하는 모형으로 네트 워크 분석이 요구된다. 구조 방정식 안에서는\n\n외생변수 (원인) \\(\\rightarrow\\) 내생변수 (결과) 의 관계를 갖는다.\n이때, 외생 변수와 내생 변수에 많은 변수들이 관계를 맺는다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#억압변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#억압변수",
    "title": "Variables",
    "section": "",
    "text": "억압변수 (suppressor variable)은 원래 관계가 있는데 관계가 없는 것처럼 보이게 하는 변수\n독립변수와 종속변수 중 하나의 변수와는 양의 상관관계가 있고 다른 하나의 변수와는 음의 상관관계가 있어 독립변수와 종속변수 간에 마치 아무런 관계가 없는 것처럼 보이게 만드는 변수\n두 변수 간에 실제 존재하는 관계를 드러나지 못하게 억누른다는 의미에서 억밥 변수라고 부름\n억압 변수를 통제하면 독립변수와 종속 변수의 참된 관계가 드러남\n외생 변수와 마찬가지로 억압변수는 반드시 통제되어야한다.\n독립변수 (+) \\(\\leftarrow\\) 억압변수 \\(\\rightarrow\\) (-) 종속변수\n예를 들어, 교육 수준이 높으면 소득수준이 낮을 것이다.\n\n위의 명제는 마치 거짓인 것처럼 보이지만 고령자란 변수를 억압변수로 설정하면 교육 수준이 높은 고령자의 경우 젊었을 때 돈을 많이 벌어놨기 때문에 은퇴후 돈을 벌지 않아도 된다. 그러므로 소득 수준이 낮기 때문에 위의 명제가 참이 될 수 있다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#통제변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#통제변수",
    "title": "Variables",
    "section": "",
    "text": "통제변수 (control variable)는 독립변수와 종속변수의 관계에 영향을 미칠만한 제 3의 변수로서 외생변수, 매개변수, 조절변수, 억제변수들 중 연구자가 중점적으로 보고자 하는 변수들의 실제적 관계를 검증하기 위해 조사과정에 영향을 미치지 않도록 실제로 통제하는 변수\n즉, 통제변수란 독립변수와 종속변수의 인과관계에 영향을 주는 제 3의 변수 중 조사설계에서 조사자가 통제하려는 변수\n일반적으로 하나의 종속변수에 수많은 독립변수가 있지만 분석가가 관심있어하는 독립 변수의 종속변수에 대한 순수한 효과를 관찰하기 위해 제 3의 변수들 (통제 변수)을 통제하거나 일정하게 유지해줘야 관찰이 가능하다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#이산변수-비연속변수-불연속변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#이산변수-비연속변수-불연속변수",
    "title": "Variables",
    "section": "",
    "text": "이산변수는 명목척도 (nominal scale), 서열척도(ordinal scale)로 측정되는 변수\n이산이란 각 값의 사이가 떨어져 있어 그 사이의 값은 아무런 의미가 없다는 뜻\n성별, 종교 등\n예를 들어, 남성을 1, 여성을 2라고 값을 부여했을 때 1과 2사이의 값은 존재할 수 없음"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#연속-변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#연속-변수",
    "title": "Variables",
    "section": "",
    "text": "연속변수는 등간척도 (interval scale)와 비율척도 (ratio scale)로 측정된 변수\n키, 연령, 무게 등"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#질적-변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#질적-변수",
    "title": "Variables",
    "section": "",
    "text": "양이 아닌 해석 또는 의미가 중요한 변수. 성별 종교 등 (성별 1,2가 양을 의미하는 것이 아님)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#양적변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#양적변수",
    "title": "Variables",
    "section": "",
    "text": "키 연령 무게 등 - 측정값이 양을 의미함"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#blog-guide-map-link",
    "title": "Variables",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html",
    "href": "docs/blog/posts/statistics/guide_map/index.html",
    "title": "Content List, Statistics",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#basic",
    "href": "docs/blog/posts/statistics/guide_map/index.html#basic",
    "title": "Content List, Statistics",
    "section": "Basic",
    "text": "Basic\n\nProbability Theory\n\n2023-02-05, Set Theory\n2023-02-05, [Basics of Probability Theory - Axiomatic Foundations]\n2023-02-05, [Basics of Probability Theory - Calculus of Probabilities]\n2023-02-05, Basics of Probability Theory - Probability\n2023-02-05, Conditional Probability\n2023-02-05, [Independence]\n2023-02-05, Bayes’ Rule\n2023-02-05, Random Variable\n1111-11-11, Probability Distribution\n\n\n\nTransformations and Expectations\n\n2023-02-21, Transformation of Random Variables\n1111-11-11, Expected Value vs Realizaed Value\n1111-11-11, Variance\n1111-11-11, Covariance and Correlation\n2023-02-28, Moment Generating Function, MGF\n\n\n\nExponential Family Distributions\n\nDiscrete Random Variable\n\n2023-02-27,Bernoulli Distribution\n2023-02-28,Binomial Distribution\n2023-03-01,Poisson Distribution\n2023-03-01,Geometric Distribution\n1111-11-11, Hypergeometric Distribution\n\nContinuous Random Variable\n\n1111-11-11, Normal Distribution\n1111-11-11, Exponential Distribution\n1111-11-11, Beta Distribution\n1111-11-11, Chi-squared Distribution\n\n1111-11-11,\n\n\n\nMultiple Random Variables\n\n1111-11-11, Joint Distribution and Marginal Distribution\n\n\n\nPoint Estimation\n\n1111-11-11, Estimation Methods - Method of Moments\n2023-03-29, Estimation Methods - Maximum Likelihood Estimation & Statistical Bias\n1111-11-11, Estimation Methods - Bayesian Estimation\n1111-11-11, Estimation Methods - The EM Algorithm\n1111-11-11, Evaluation Methods of Estimators - Mean Squared Error\n1111-11-11, Evaluation Methods of Estimators - Best Unbiased Estimators\n1111-11-11, Evaluation Methods of Estimators - Sufficiency and Unbiasedness\n1111-11-11, Evaluation Methods of Estimators - Loss Function Optimality"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#hypothesis-testing",
    "href": "docs/blog/posts/statistics/guide_map/index.html#hypothesis-testing",
    "title": "Content List, Statistics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n1111-11-11, Hypothesis Testing\n1111-11-11, Permutation Test\n\n\nMethods of Finding Tests\n\n1111-11-11, Likelihood Ratio Tests\n1111-11-11, Bayesian Tests\n1111-11-11, Union-Intersection and Intersection-Union Tets\n\n\n\nMethods of Evaluating Tests\n\n1111-11-11, Power\n1111-11-11, Error Proabilities and the Power Function\n1111-11-11, Most Powerful Tests\n2022-12-28, p-values\n1111-11-11, Loss Function Optimality\n1111-11-11, Multiple Testing\n1111-11-11, Sample Size Calculation\n1111-11-11, A/B Testing\n2023-01-07, ANOVA\n\n2023-01-27, ANCOVA\n2023-01-27, repeated measures ANOVA\n2023-01-28, MANOVA"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#categorical-data-analysis",
    "href": "docs/blog/posts/statistics/guide_map/index.html#categorical-data-analysis",
    "title": "Content List, Statistics",
    "section": "Categorical Data Analysis",
    "text": "Categorical Data Analysis\n\n1111-11-11, Introduction\n1111-11-11,\n1111-11-11,\n2022-12-28,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n2023-01-07,\n2023-01-27,\n2023-01-27,\n2023-01-28,"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#regression",
    "href": "docs/blog/posts/statistics/guide_map/index.html#regression",
    "title": "Content List, Statistics",
    "section": "Regression",
    "text": "Regression\n\n1111-11-11, Least Square and Simple Linear Regression\n1111-11-11, Multiple Linear Regression\n\n\nGeneralized Linear Models\n\n1111-11-11, Logistic Regression\n1111-11-11, Multinomial Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#longitudinal-data-analysis",
    "href": "docs/blog/posts/statistics/guide_map/index.html#longitudinal-data-analysis",
    "title": "Content List, Statistics",
    "section": "Longitudinal Data Analysis",
    "text": "Longitudinal Data Analysis\n\n2023-03-23, LDA (1) - Intro\n2023-03-23, LDA (2) - Concepts & Covariance Models\n2023-03-25, LDA (3) - WLS & REML\n2023-03-25, LDA (4) - Respiratory Infection Data Example\n2023-03-28, LDA (5) - Epileptic Seizures Data Example\n\n\nMixed Models\n\n1111-11-11, Linear Mixed Models"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#generalized-additive-models",
    "href": "docs/blog/posts/statistics/guide_map/index.html#generalized-additive-models",
    "title": "Content List, Statistics",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#survival-analysis",
    "href": "docs/blog/posts/statistics/guide_map/index.html#survival-analysis",
    "title": "Content List, Statistics",
    "section": "Survival Analysis",
    "text": "Survival Analysis\n\n1111-11-11, Cox-Hazard Model"
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/2_covariance_model.html",
    "href": "docs/blog/posts/statistics/LDA/2_covariance_model.html",
    "title": "LDA (2) - Concept & Covariance Models",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\\(y_{ij}\\) : the univariate response (i.e. scalar) for the \\(i\\) th subject at the \\(j\\) th occasion or measurement\n\nlater when I use the vector case, I will re-define this notation, but focus on the scalar case for now.\n\n\\(x_{ij}\\) : the predictor at time \\(t_{ij}\\), which is either a scalr or vector.\n\na scalar case: \\(x_{ij}\\) where \\(i\\) is the \\(i\\) th subject, and \\(j\\) is the \\(j\\) th measurement.\na vector case: \\(x_{ijk}\\) where \\(i\\) is the \\(i\\) th subject, \\(j\\) is the \\(j\\) th measurement, and \\(k \\in [1,p]\\) is the \\(k\\) th predictor.\nsometimes, covariate for different measurements could be the same. In this case, the notation could be written in \\(x_{i}\\)\n\nex) a gender does not change over time in the most cases.\n\n\n\\(i=1, \\dots, m\\) : i is the index for the \\(i\\) th subject\n\\(j=1, \\dots, n_i\\) : j is the index for the \\(j\\) th measurement of the \\(i\\) th subject\n\n\\({n_i}\\) is the number of measurements of the \\(i\\) th subject, each \\({n_i}\\) does not have to the same.\nbalanced desgin: \\({n_i}\\) is the same.\nunbalanced desgin: \\({n_i}\\) is different.\n\n\\(\\mathbf y_i\\) : a vector (not a matrix), \\((y_{i1},y_{i2},\\dots ,y_{in_i})\\) of the \\(i\\) th subject\n\\(\\mathbf Y\\) : the reponse matrix\n\\(\\mathbf X\\) : the predictor matrix\n\\(\\text{E}(y_{ij})\\) : \\(\\mu_{ij}\\)\n\\(\\text{E}(\\mathbf y_i)\\) : \\(\\mathbf \\mu_{i}\\)\n\\(\\text{Var}(\\mathbf y_i)\\) : \\(\\text{Var}(\\mathbf y_i)\\) is a variance-covariance matrix of the different measurement for the \\(i\\) th subject\n\nfor now, we do not care of the variance covariance of the different subjects because we assume that the measurements of different subjects are indpendent. \\[\n\\begin{bmatrix}\n\\text{Var}(y_{i1}) & \\text{Cov}( y_{i1}, y_{i2}) & \\dots & \\text{Cov}( y_{i1}, y_{in_i}) \\\\\n                           & \\text{Var}( y_{i2}) & \\dots & \\text{Cov}( y_{i2}, y_{in_i}) \\\\\n                             &                           & \\ddots & \\vdots \\\\\n                             &&                            \\dots & \\text{Var}( y_{in_i})\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nthe measurements for the same subject are not independent.\nthe measurements for the different subject are independent.\nsome correlation structures of the different measurements.\n\n\n\n\n\nMarginal Models\n\n\\(\\text{E}(y_{ij}) = \\mathbf x_{ij} \\mathbf \\beta\\)\n\\(\\text{Var}(\\mathbf y_i)= \\mathbf V_i\\)\nto build a marginal model, we just need info on the 3 things\n\nthe distribution : a multivariate normal distribution\nmean and variance-covariance\n\n\\(\\beta\\) is fixed. That’s why we call this marginal models ‘fixed effect’\n\n\n\n\n\n\n\n\nRecall\n\n\n\nWe find MLE for the linear regression with the 3 things: the normal distribution (iid), \\(\\mu\\) and \\(\\sigma^2\\)\n\n\n\nMixed Effects Models\n\n\\(\\text{E}(y_{ij}|\\mathbf \\beta_i) = \\mathbf x_{ij} \\mathbf \\beta_i\\)\n\\(\\mathbf \\beta_i = \\mathbf \\beta (\\text{fixed effect}) + \\mathbf u_i (\\text{subject-specific random effect})\\)\n\\(\\mathbf \\beta_i\\) is a random coefficient specific for the \\(i\\) th subject, That’s why we call this mixed effect models ‘random effect’\nsubject-specific random effect: differenct subjects have different \\(\\mathbf \\beta_i\\)\n\nTransition Models\n\n\\(\\text{E}(y_{ij}|y_{i,j-1},\\dots,y_{i,1},\\mathbf x_{ij})\\)\nMarkov Process: the response variable in the previous time point will affect the measurement in the current time point.\n\n\n\n\nConsider an example of a simple linear model (i.e., a univaiable linear model) \\[\ny_{ij}=\\beta_0+\\beta_1t_{ij} + \\epsilon_{ij}\n\\]\n\nmean part: \\(\\text{E}(y_{ij})\\)\nvariance part: \\(\\text{Var}(\\mathbf y_{i})=\\text{Var}(\\mathbf \\epsilon_{i})\\)\n\nmore often, a correlation matrix is used in LDA because correlation is more interpretable.\n\n\n\\[\n\\text{Corr}(\\mathbf y_i) =\n\\begin{bmatrix}\n1 & \\rho_{12}& \\dots & \\rho_{1n_i} \\\\\n\\rho_{21} & 1 & \\dots & \\rho_{2n_i} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\rho_{n_i1} & \\rho_{n_i2}& \\dots & 1\n\\end{bmatrix}\n\\]\n\nin this correlation matrix, there are \\(\\frac{n(n-1)}{2}\\) parameters to estimate\nin the mean part, there are 2 parameters, \\(\\mathbf \\beta\\) to estimate Likewise, the number of the estimators depends on the number of the measurements and the covriates.\n\nIn LDA, since the responses are multiple, we need to look into the correlation characteristics.\n\n\n\nIn empirical observations about the nature of the correlation among repeated measures,\n\ncorrelations among the repeated measures are usually positive\ncorrelations tend to decrease with increasing time separation\ncorrelations among repeated measures rarely approach zero\ncorrelations between any pair of repeated meausres regardless of distance in time is constrained by the reliability of the measurement process.\n\nif the measurement process is not very reliable or consistent, then even if two measurements are taken close together in time, their correlation will not be very strong. Similarly, if the measurement process is highly reliable or consistent, then two measurements taken far apart in time may still be highly correlated. Reliability refers to the degree to which a measurement process produces consistent and accurate results over time.\n\n\n\n\n\nThere are 2 types of covariance structure: unbalanced design and balanced design. For now, let’s focus on the balanced design.\n\n\n\nobservations for each subject are not made on the same grid\nthese observations can be made at different time points and different numbers of observations may be made for each subject.\nMissing observations falls into this category.\n\n\n\n\n\nobservations for each subject are made on the same grid and there is no missing data.\n\nnumber and timing of the repeated measurements are the same for all individuals.\n\nThen, \\(t_{ij}\\) can be denoted as \\(t_j\\) where \\(j \\in 1, \\dots, n\\) because the size of the measurements is the same (\\(n_i\\) is the same)\nThe covariance of the response variable \\(\\mathbf Y_{m\\times n}\\) :\n\n$$\n\\[\\begin{aligned}\n  \\text{Cov}(\\mathbf Y)\n  &=\\text{Cov}(\\mathbf y_1,\\dots,y_m) \\\\\n  &=\n  \\begin{bmatrix}\n    \\text{Var}(\\mathbf y_1) & 0 & \\dots & 0 \\\\\n    0 & \\text{Var}(\\mathbf y_2) & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\text{Var}(\\mathbf y_m)\n  \\end{bmatrix}\\\\\n  &=\n  \\begin{bmatrix}\n    \\begin{bmatrix}\n    \\text{Var}(y_{11}) & \\text{Cov}( y_{11}, y_{12}) & \\dots & \\text{Cov}( y_{11}, y_{1n_1}) \\\\\n                               & \\text{Var}( y_{12}) & \\dots & \\text{Cov}( y_{12}, y_{1n_1}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{1n_1})\n\\end{bmatrix} & 0 & \\dots & 0 \\\\\n    0 & \\begin{bmatrix}\n    \\text{Var}(y_{21}) & \\text{Cov}( y_{21}, y_{22}) & \\dots & \\text{Cov}( y_{21}, y_{in_2}) \\\\\n                               & \\text{Var}( y_{22}) & \\dots & \\text{Cov}( y_{22}, y_{in_2}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{2n_2})\n\\end{bmatrix} & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\begin{bmatrix}\n    \\text{Var}(y_{m1}) & \\text{Cov}( y_{m1}, y_{m2}) & \\dots & \\text{Cov}( y_{m1}, y_{mn_m}) \\\\\n                               & \\text{Var}( y_{m2}) & \\dots & \\text{Cov}( y_{m2}, y_{mn_m}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{mn_m})\n\\end{bmatrix}\n\n  \\end{bmatrix} \\\\\n  &=\n  \\begin{bmatrix}\n    \\Sigma_1 & 0 & \\dots & 0 \\\\\n    0 & \\Sigma_1 & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\Sigma_m\n  \\end{bmatrix}\n\\end{aligned}\\]\n$$\nIf we assume the covariance matrices for different subjects are the same, we can denote \\(\\text{Cov}(\\mathbf Y)=\\Sigma\\).\n\n\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho & \\rho & \\dots & \\rho \\\\\n    \\rho & 1 & \\rho & \\dots & \\rho \\\\\n    \\rho & \\rho & 1 & \\dots & \\rho \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho & \\rho & \\rho & \\dots & 1\n  \\end{bmatrix}\n\\]\n\ncompound symmetry is a.k.a Exchangeable\nAssume variance is constant across visits (say \\(\\sigma^2\\))\nAssume correlation between any two visits are constant (say \\(\\rho\\)).\nParsimonious: there are two parameters in the covariance, \\(\\sigma^2\\) and \\(\\rho\\) (computational benefit)\nWithout any contraint on \\(\\sigma^2\\), you will get closed form estimate.\nCovariance variance matrix is plugged into likelihood function to estimate 3 kinds of parameters \\(\\sigma^2\\), \\(\\rho\\), and \\(\\beta\\)\nThis structure is so parsimonuous that it could be unrealistic: not commonly used\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho_1 & \\rho_2 & \\dots & \\rho_{n-1} \\\\\n    \\rho_1 & 1 & \\rho_1 & \\dots & \\rho_{n-2} \\\\\n    \\rho_2 & \\rho_1 & 1 & \\dots & \\rho_{n-3} \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho_{n-1} & \\rho_{n-2} & \\rho_{n-3} & \\dots & 1\n  \\end{bmatrix}\n\\]\n\nToeplitz structure is more flexible than compound symmetry\nAssume variance is constant across visits and \\(\\text{Corr}(y_{ij}, y_{i,j+k}) = \\rho_k\\).\nAssume correlation among responses at adjacent measurements is constant.\nOnly suitable for measurements made at equal intervals of time between different measurement.\nWithout any contraint on \\(\\sigma^2\\), you will get closed form estimate.\nToeplitz covariance has free \\(n\\) parameters to estimate (\\(1\\) for variance and \\(n-1\\) correlation parameters)\nThe larger time differences, the smaller its correlations\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho^1 & \\rho^2 & \\dots & \\rho^{n-1} \\\\\n    \\rho^1 & 1 & \\rho^1 & \\dots & \\rho^{n-2} \\\\\n    \\rho^2 & \\rho^1 & 1 & \\dots & \\rho^{n-3} \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho^{n-1} & \\rho^{n-2} & \\rho^{n-3} & \\dots & 1\n  \\end{bmatrix}\n\\]\n\nA special case of toeplitz structure with \\(\\text{Corr}(y_{ij},y_{i,j+k})=\\rho^k\\)\nsimpler than toeplitz, only 2 parameters\nOnly suitable for measurements made at equal intervals of time between different measurement.\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho^1 & 0 & \\dots & 0 \\\\\n    \\rho^1 & 1 & \\rho^1 & \\dots & 0 \\\\\n    0 & \\rho^1 & 1 & \\dots & 0 \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & 0 & \\dots & 1\n  \\end{bmatrix}\n\\]\nLook at the more general case of the banded structure in Wiki.\n\nAssume correlation is 0 beyond some specified interval.\nCan be combined with the previous patterns.\nVery strong assumption about how quickly the correlation decays to 0 with increasing time separation.\n\n\n\n\n\nA generalization of autoregressive pattern\nThe most general and reasonable structure\nSuitable for unevenly spaced measurements, take actual time points (time difference), the larger time difference the smaller correlation\nAssumption that the variance of different measurements over time is the same, which can be easily generalized. You can put different variance on the diagonal.\nLet \\(\\{t_{i1},\\dots,t_{in_i}\\}\\) denote the observation times for the \\(i\\) th individual. Then, the correlation is \\(\\text{Corr}(Y_{ij} ,Y_{ik}) = \\rho^{|t_{ij}-t_{ik}|}\\)\nCorrelation decreases exponentially with the time separations between them.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/2_covariance_model.html#notations",
    "href": "docs/blog/posts/statistics/LDA/2_covariance_model.html#notations",
    "title": "LDA (2) - Concept & Covariance Models",
    "section": "",
    "text": "\\(y_{ij}\\) : the univariate response (i.e. scalar) for the \\(i\\) th subject at the \\(j\\) th occasion or measurement\n\nlater when I use the vector case, I will re-define this notation, but focus on the scalar case for now.\n\n\\(x_{ij}\\) : the predictor at time \\(t_{ij}\\), which is either a scalr or vector.\n\na scalar case: \\(x_{ij}\\) where \\(i\\) is the \\(i\\) th subject, and \\(j\\) is the \\(j\\) th measurement.\na vector case: \\(x_{ijk}\\) where \\(i\\) is the \\(i\\) th subject, \\(j\\) is the \\(j\\) th measurement, and \\(k \\in [1,p]\\) is the \\(k\\) th predictor.\nsometimes, covariate for different measurements could be the same. In this case, the notation could be written in \\(x_{i}\\)\n\nex) a gender does not change over time in the most cases.\n\n\n\\(i=1, \\dots, m\\) : i is the index for the \\(i\\) th subject\n\\(j=1, \\dots, n_i\\) : j is the index for the \\(j\\) th measurement of the \\(i\\) th subject\n\n\\({n_i}\\) is the number of measurements of the \\(i\\) th subject, each \\({n_i}\\) does not have to the same.\nbalanced desgin: \\({n_i}\\) is the same.\nunbalanced desgin: \\({n_i}\\) is different.\n\n\\(\\mathbf y_i\\) : a vector (not a matrix), \\((y_{i1},y_{i2},\\dots ,y_{in_i})\\) of the \\(i\\) th subject\n\\(\\mathbf Y\\) : the reponse matrix\n\\(\\mathbf X\\) : the predictor matrix\n\\(\\text{E}(y_{ij})\\) : \\(\\mu_{ij}\\)\n\\(\\text{E}(\\mathbf y_i)\\) : \\(\\mathbf \\mu_{i}\\)\n\\(\\text{Var}(\\mathbf y_i)\\) : \\(\\text{Var}(\\mathbf y_i)\\) is a variance-covariance matrix of the different measurement for the \\(i\\) th subject\n\nfor now, we do not care of the variance covariance of the different subjects because we assume that the measurements of different subjects are indpendent. \\[\n\\begin{bmatrix}\n\\text{Var}(y_{i1}) & \\text{Cov}( y_{i1}, y_{i2}) & \\dots & \\text{Cov}( y_{i1}, y_{in_i}) \\\\\n                           & \\text{Var}( y_{i2}) & \\dots & \\text{Cov}( y_{i2}, y_{in_i}) \\\\\n                             &                           & \\ddots & \\vdots \\\\\n                             &&                            \\dots & \\text{Var}( y_{in_i})\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/2_covariance_model.html#assumptions",
    "href": "docs/blog/posts/statistics/LDA/2_covariance_model.html#assumptions",
    "title": "LDA (2) - Concept & Covariance Models",
    "section": "",
    "text": "the measurements for the same subject are not independent.\nthe measurements for the different subject are independent.\nsome correlation structures of the different measurements."
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/2_covariance_model.html#for-continuous-responses",
    "href": "docs/blog/posts/statistics/LDA/2_covariance_model.html#for-continuous-responses",
    "title": "LDA (2) - Concept & Covariance Models",
    "section": "",
    "text": "Marginal Models\n\n\\(\\text{E}(y_{ij}) = \\mathbf x_{ij} \\mathbf \\beta\\)\n\\(\\text{Var}(\\mathbf y_i)= \\mathbf V_i\\)\nto build a marginal model, we just need info on the 3 things\n\nthe distribution : a multivariate normal distribution\nmean and variance-covariance\n\n\\(\\beta\\) is fixed. That’s why we call this marginal models ‘fixed effect’\n\n\n\n\n\n\n\n\nRecall\n\n\n\nWe find MLE for the linear regression with the 3 things: the normal distribution (iid), \\(\\mu\\) and \\(\\sigma^2\\)\n\n\n\nMixed Effects Models\n\n\\(\\text{E}(y_{ij}|\\mathbf \\beta_i) = \\mathbf x_{ij} \\mathbf \\beta_i\\)\n\\(\\mathbf \\beta_i = \\mathbf \\beta (\\text{fixed effect}) + \\mathbf u_i (\\text{subject-specific random effect})\\)\n\\(\\mathbf \\beta_i\\) is a random coefficient specific for the \\(i\\) th subject, That’s why we call this mixed effect models ‘random effect’\nsubject-specific random effect: differenct subjects have different \\(\\mathbf \\beta_i\\)\n\nTransition Models\n\n\\(\\text{E}(y_{ij}|y_{i,j-1},\\dots,y_{i,1},\\mathbf x_{ij})\\)\nMarkov Process: the response variable in the previous time point will affect the measurement in the current time point.\n\n\n\n\nConsider an example of a simple linear model (i.e., a univaiable linear model) \\[\ny_{ij}=\\beta_0+\\beta_1t_{ij} + \\epsilon_{ij}\n\\]\n\nmean part: \\(\\text{E}(y_{ij})\\)\nvariance part: \\(\\text{Var}(\\mathbf y_{i})=\\text{Var}(\\mathbf \\epsilon_{i})\\)\n\nmore often, a correlation matrix is used in LDA because correlation is more interpretable.\n\n\n\\[\n\\text{Corr}(\\mathbf y_i) =\n\\begin{bmatrix}\n1 & \\rho_{12}& \\dots & \\rho_{1n_i} \\\\\n\\rho_{21} & 1 & \\dots & \\rho_{2n_i} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\rho_{n_i1} & \\rho_{n_i2}& \\dots & 1\n\\end{bmatrix}\n\\]\n\nin this correlation matrix, there are \\(\\frac{n(n-1)}{2}\\) parameters to estimate\nin the mean part, there are 2 parameters, \\(\\mathbf \\beta\\) to estimate Likewise, the number of the estimators depends on the number of the measurements and the covriates.\n\nIn LDA, since the responses are multiple, we need to look into the correlation characteristics.\n\n\n\nIn empirical observations about the nature of the correlation among repeated measures,\n\ncorrelations among the repeated measures are usually positive\ncorrelations tend to decrease with increasing time separation\ncorrelations among repeated measures rarely approach zero\ncorrelations between any pair of repeated meausres regardless of distance in time is constrained by the reliability of the measurement process.\n\nif the measurement process is not very reliable or consistent, then even if two measurements are taken close together in time, their correlation will not be very strong. Similarly, if the measurement process is highly reliable or consistent, then two measurements taken far apart in time may still be highly correlated. Reliability refers to the degree to which a measurement process produces consistent and accurate results over time.\n\n\n\n\n\nThere are 2 types of covariance structure: unbalanced design and balanced design. For now, let’s focus on the balanced design.\n\n\n\nobservations for each subject are not made on the same grid\nthese observations can be made at different time points and different numbers of observations may be made for each subject.\nMissing observations falls into this category.\n\n\n\n\n\nobservations for each subject are made on the same grid and there is no missing data.\n\nnumber and timing of the repeated measurements are the same for all individuals.\n\nThen, \\(t_{ij}\\) can be denoted as \\(t_j\\) where \\(j \\in 1, \\dots, n\\) because the size of the measurements is the same (\\(n_i\\) is the same)\nThe covariance of the response variable \\(\\mathbf Y_{m\\times n}\\) :\n\n$$\n\\[\\begin{aligned}\n  \\text{Cov}(\\mathbf Y)\n  &=\\text{Cov}(\\mathbf y_1,\\dots,y_m) \\\\\n  &=\n  \\begin{bmatrix}\n    \\text{Var}(\\mathbf y_1) & 0 & \\dots & 0 \\\\\n    0 & \\text{Var}(\\mathbf y_2) & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\text{Var}(\\mathbf y_m)\n  \\end{bmatrix}\\\\\n  &=\n  \\begin{bmatrix}\n    \\begin{bmatrix}\n    \\text{Var}(y_{11}) & \\text{Cov}( y_{11}, y_{12}) & \\dots & \\text{Cov}( y_{11}, y_{1n_1}) \\\\\n                               & \\text{Var}( y_{12}) & \\dots & \\text{Cov}( y_{12}, y_{1n_1}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{1n_1})\n\\end{bmatrix} & 0 & \\dots & 0 \\\\\n    0 & \\begin{bmatrix}\n    \\text{Var}(y_{21}) & \\text{Cov}( y_{21}, y_{22}) & \\dots & \\text{Cov}( y_{21}, y_{in_2}) \\\\\n                               & \\text{Var}( y_{22}) & \\dots & \\text{Cov}( y_{22}, y_{in_2}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{2n_2})\n\\end{bmatrix} & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\begin{bmatrix}\n    \\text{Var}(y_{m1}) & \\text{Cov}( y_{m1}, y_{m2}) & \\dots & \\text{Cov}( y_{m1}, y_{mn_m}) \\\\\n                               & \\text{Var}( y_{m2}) & \\dots & \\text{Cov}( y_{m2}, y_{mn_m}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{mn_m})\n\\end{bmatrix}\n\n  \\end{bmatrix} \\\\\n  &=\n  \\begin{bmatrix}\n    \\Sigma_1 & 0 & \\dots & 0 \\\\\n    0 & \\Sigma_1 & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\Sigma_m\n  \\end{bmatrix}\n\\end{aligned}\\]\n$$\nIf we assume the covariance matrices for different subjects are the same, we can denote \\(\\text{Cov}(\\mathbf Y)=\\Sigma\\).\n\n\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho & \\rho & \\dots & \\rho \\\\\n    \\rho & 1 & \\rho & \\dots & \\rho \\\\\n    \\rho & \\rho & 1 & \\dots & \\rho \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho & \\rho & \\rho & \\dots & 1\n  \\end{bmatrix}\n\\]\n\ncompound symmetry is a.k.a Exchangeable\nAssume variance is constant across visits (say \\(\\sigma^2\\))\nAssume correlation between any two visits are constant (say \\(\\rho\\)).\nParsimonious: there are two parameters in the covariance, \\(\\sigma^2\\) and \\(\\rho\\) (computational benefit)\nWithout any contraint on \\(\\sigma^2\\), you will get closed form estimate.\nCovariance variance matrix is plugged into likelihood function to estimate 3 kinds of parameters \\(\\sigma^2\\), \\(\\rho\\), and \\(\\beta\\)\nThis structure is so parsimonuous that it could be unrealistic: not commonly used\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho_1 & \\rho_2 & \\dots & \\rho_{n-1} \\\\\n    \\rho_1 & 1 & \\rho_1 & \\dots & \\rho_{n-2} \\\\\n    \\rho_2 & \\rho_1 & 1 & \\dots & \\rho_{n-3} \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho_{n-1} & \\rho_{n-2} & \\rho_{n-3} & \\dots & 1\n  \\end{bmatrix}\n\\]\n\nToeplitz structure is more flexible than compound symmetry\nAssume variance is constant across visits and \\(\\text{Corr}(y_{ij}, y_{i,j+k}) = \\rho_k\\).\nAssume correlation among responses at adjacent measurements is constant.\nOnly suitable for measurements made at equal intervals of time between different measurement.\nWithout any contraint on \\(\\sigma^2\\), you will get closed form estimate.\nToeplitz covariance has free \\(n\\) parameters to estimate (\\(1\\) for variance and \\(n-1\\) correlation parameters)\nThe larger time differences, the smaller its correlations\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho^1 & \\rho^2 & \\dots & \\rho^{n-1} \\\\\n    \\rho^1 & 1 & \\rho^1 & \\dots & \\rho^{n-2} \\\\\n    \\rho^2 & \\rho^1 & 1 & \\dots & \\rho^{n-3} \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho^{n-1} & \\rho^{n-2} & \\rho^{n-3} & \\dots & 1\n  \\end{bmatrix}\n\\]\n\nA special case of toeplitz structure with \\(\\text{Corr}(y_{ij},y_{i,j+k})=\\rho^k\\)\nsimpler than toeplitz, only 2 parameters\nOnly suitable for measurements made at equal intervals of time between different measurement.\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho^1 & 0 & \\dots & 0 \\\\\n    \\rho^1 & 1 & \\rho^1 & \\dots & 0 \\\\\n    0 & \\rho^1 & 1 & \\dots & 0 \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & 0 & \\dots & 1\n  \\end{bmatrix}\n\\]\nLook at the more general case of the banded structure in Wiki.\n\nAssume correlation is 0 beyond some specified interval.\nCan be combined with the previous patterns.\nVery strong assumption about how quickly the correlation decays to 0 with increasing time separation.\n\n\n\n\n\nA generalization of autoregressive pattern\nThe most general and reasonable structure\nSuitable for unevenly spaced measurements, take actual time points (time difference), the larger time difference the smaller correlation\nAssumption that the variance of different measurements over time is the same, which can be easily generalized. You can put different variance on the diagonal.\nLet \\(\\{t_{i1},\\dots,t_{in_i}\\}\\) denote the observation times for the \\(i\\) th individual. Then, the correlation is \\(\\text{Corr}(Y_{ij} ,Y_{ik}) = \\rho^{|t_{ij}-t_{ik}|}\\)\nCorrelation decreases exponentially with the time separations between them."
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/2_covariance_model.html#go-to-blog-content-list",
    "href": "docs/blog/posts/statistics/LDA/2_covariance_model.html#go-to-blog-content-list",
    "title": "LDA (2) - Concept & Covariance Models",
    "section": "",
    "text": "Blog Content List"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html",
    "title": "FDA Software Validation Guidance Summary",
    "section": "",
    "text": "I am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto.\nThe FDA validation guidance document is a bit difficult to understand because its explanations provides abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\n\n\n2022-12-28, download this article as PDF\n2022-12-28, summary with diagrams\n\n\n\n\nFDA: General Principles of Software Validation\n\n\n\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\n\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#notice",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#notice",
    "title": "FDA Software Validation Guidance Summary",
    "section": "",
    "text": "I am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto.\nThe FDA validation guidance document is a bit difficult to understand because its explanations provides abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\n\n\n2022-12-28, download this article as PDF\n2022-12-28, summary with diagrams\n\n\n\n\nFDA: General Principles of Software Validation\n\n\n\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\n\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#purpose",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#purpose",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.1 Purpose",
    "text": "2.1 Purpose\nThe purpose is to make a sketch of general validation principle of the validation of medical device software or software used to design or develop."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#scope",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#scope",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.2 Scope",
    "text": "2.2 Scope\nThe scope of this guidance is broad. The important activities for the software validation include at least:\n\nplanning,\nverfication,\ntesting,\ntraceability, and\nconfiguration management.\n\nAll of the activities above should be\n\nintegrated\nbe able to describe software life cycle management and\nbe able to describe software risk management.\n\nThe software validation and verification activities should be focused into the entire software life cycle. (It does not necessarily mean that the activies must follow any technical models.)\nThe guidance is applicable to any software related to a regulated medical device and anyone who is employed in a bio or medical industry.\n\n2.2.1 The Least Burdensome Approach\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n2.2.2 Regulatory Requirements for Software Validation\n\nSoftware validation: a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997. (See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\nSpecific requirements for validation of device software are found in 21 CFR §820.30(g). Other design controls, such as planning, input, verification, and reviews, are required for medical device software. (See 21 CFR §820.30.)\ncomputer systems used to create, modify, and maintain electronic records and to manage electronic signatures are also subject to the validation requirements. (See 21 CFR §11.10(a).)\n\n\n2.2.2.1 Objective\nThe objective of software validation is to ensure:\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\n2.2.2.2 What to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\n\n2.2.3 Quality System Regulation vs Pre-market Submissions\nThis document does not address any specific requirements but general ones. Specific issues should be addressed to\n\nthe Office of Device Evaluation (ODE),\nCenter for Devices and Radiological Health (CDRH)\nthe Office of Blood Research and Review,\nCenter for Biologics Evaluation and Research (CBER). See the references in Appendix A for applicable FDA guidance documents for pre-market submissions."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#context-for-software-validation",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#context-for-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.3 Context for Software Validation",
    "text": "2.3 Context for Software Validation\n\nValidation elements that FDA expects to do for the Quality System regulation, using the principles and tasks are listed in Sections 4 and 5.\nAdditional specific information is available from many of the references listed in Appendix A\n\n\n2.3.1 Definition and Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology between the medical device Quality System regulation and the software industry:\n\nrequirements,\nspecification,\nverification, and\nvalidation.\n\n\n\n2.3.1.1 Requirements and Specifications\nThe Quality System regulation states\n\nthat design input requirements must be documented and\nthat specified requirements must be verified\n\nBut, the regulation does not further clarify the distinction between the terms “requirement” and “specification.”\n\nRequirement\n\ncan be any need or expectation for a system or for its software.\nreflects the stated or implied needs of the customer: requirements may be\n\nmarket-based,\ncontractual,\nstatutory, or\nan organization’s internal requirements.\n\nvarious examples of requirements\n\ndesign, functional, implementation, interface, performance, or physical requirements\n\nSoftware requirements derived from the system requirements for those aspects of system functionality\nSoftware requirements are typically stated in functional terms and are defined, refined, and updated as a development project progresses.\nSuccess in accurately and completely documenting software requirements is a crucial factor in successful validation of the resulting software.\n\nSpecification\n\ndefined as “a document that states requirements.” (See 21 CFR §820.3(y).)\nIt may refer to or include drawings, patterns, or other relevant documents\nIt usually indicates the means and the criteria whereby conformity with the requirement can be checked.\nVarious examples of written specifications\n\nsystem requirements specification,\nsoftware requirements specification,\nsoftware design specification,\nsoftware test specification,\nsoftware integration specification, etc.\n\nAll of these documents are design outputs for which various forms of verification are necessary.\n\n\n\n\n2.3.1.2 Verifiaction and Validation\nThe Quality System regulation is harmonized with ISO 8402:1994, which treats “verification” and “validation” as separate and distinct terms.\n\nSoftware verification\n\nIt provides objective evidence that the design outputs of a particular phase of the software development life cycle meet all of the specified requirements for that phase.\nIt looks for\n\nconsistency,\ncompleteness, and\ncorrectness of the software and its supporting documentation\n\nSoftware testing\n\nverification activities intended to confirm that software development output meets its input requirements.\n\nTypes of verification activities include\n\nvarious static and dynamic analyses,\ncode and document inspections,\nwalkthroughs, and other techniques.\n\n\nSoftware Validation\n\nConfirmation by examination and provision of the following objective evidence:\nEvidence 1: software specifications conform to user needs and intended uses, and\nEvidnece 2: the particular requirements implemented through software can be consistently fulfilled.\nEvidnece 3: all software requirements have been implemented correctly and completely and are traceable to system requirements.\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device.\n\nDifficulty in Software verification and validation\n\na developer cannot test forever, and\nit is difficult to know how much evidence is enough.\nIn large measure, software validation is a matter of developing a “level of confidence” that the device meets all requirements and user expectations for the software automated functions and features of the device.\nConsiderations for an acceptable level of confidence\n\nmeasures such as defects found in specifications documents,\nestimates of defects remaining,\ntesting coverage, and other techniques are all used to develop before shipping the product.\nHowever, a level of confidence varies depending upon the safety risk (hazard) posed by the automated functions of the device. (Info on safety risk is found in Section 4 and in the international standards ISO/IEC 14971-1 and IEC 60601-1-4 referenced in Appendix A).\n\n\n\n\n\n2.3.1.3 IQ/OQ/PQ\nIQ/OQ/PQ are the terminology related to user site software validation\n\nInstallation qualification (IQ)\nOperational qualification (OQ)\nPerformance qualification (PQ).\n\nDefinitions of these terms may be found in FDA’s Guideline on General Principles of Process Validation, dated May 11, 1987, and in FDA’s Glossary of Computerized System and Software Development Terminology, dated August 1995. Both FDA personnel and device manufacturers need to be aware of these differences in terminology as they ask for and provide information regarding software validation.\n\n\n\n2.3.2 Software Development as Part of System Design\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nthe user’s needs\nintended uses from which the product is developed.\n\nA primary goal of software validation is to then demonstrate that all completed software products comply with all documented software and system requirements.\n\n\n2.3.3 Software Is Different from Hardware\nSoftware engineering needs an even greater level of managerial scrutiny and control than does hardware engineering.\n\n\n2.3.4 Benefits of Software Validation\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nSoftware validation can also reduce long term costs by making it easier and less costly to reliably modify software and revalidate software changes.\n\n\n\n2.3.5 Design Review\nDesign reviews are documented, comprehensive, and systematic examinations of a design to evaluate\n\nthe adequacy of the design requirements,\nthe capability of the design to meet these requirements, and\nto identify problems.\n\nDesign review is a primary tool for managing and evaluating development projects.\n\nIt is strongly recommended that it should be formal design because it is more structured than the informal one.\nIt includes participation from others outside the development team.\nIt may review reference or include results from other formal and informal reviews.\nDesign reviews should include\n\nexamination of development plans,\nrequirements specifications,\ndesign specifications,\ntesting plans and procedures,\nall other documents and activities associated with the project,\nverification results from each stage of the defined life cycle, and\nvalidation results for the overall device.\n\nThe Quality System regulation requires that at least one formal design review be conducted during the device design process. However, it is recommended that multiple design reviews be conducted\n\n(e.g., at the end of each software life cycle activity, in preparation for proceeding to the next activity).\n\nFormal design reviews documented should include:\n\nthe appropriate tasks and expected results, outputs, or products been established for each software life cycle activity\ncorrectness, completeness, consistency, and accuracy\nsatisfaction for the standards, practices, and conventions of that activity\nestablishment of a proper basis for initiating tasks for the next software life cycle activity"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#principles-of-software-validation",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#principles-of-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.4 Principles of Software Validation",
    "text": "2.4 Principles of Software Validation\n\n2.4.1 Requirements\nA documented software requirements specification provides a baseline for both validation and verification. The software validation process must include an established software requirements specification (Ref: 21 CFR 820.3(z) and (aa) and 820.30(f) and (g)).\n\n\n2.4.2 Defect Prevention\nIn order to establish that confidence, software developers should use a mixture of methods and techniques to prevent software errors and to detect software errors that do occur.\n\n\n2.4.3 Time and Effort\nPreparation for software validation should begin early, i.e., during design and development planning and design input. The final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n2.4.4 Software Life Cycle\n\nSoftware validation takes place within the environment of an established software life cycle.\nThe software life cycle contains software engineering tasks and documentation necessary to support the software validation effort.\nspecific verification and validation tasks need to be appropriate for the intended use of the software\n\n\n\n2.4.5 Plans\n\nThe software validation process is defined and controlled through the use of a plan.\nThe software validation plan defines “what” is to be accomplished through the software validation effort.\nSoftware validation plans specify areas such as\n\nscope,\napproach,\nresources,\nschedules and the types and extent of activities,\ntasks, and\nwork items.\n\n\n\n\n2.4.6 Procedures\nThe software validation process is executed through the use of procedures. These procedures establish “how” to conduct the software validation effort. The procedures should identify the specific actions or sequence of actions that must be taken to complete individual validation activities, tasks, and work items.\n\n\n2.4.7 Software Validation After a Change\n\nDue to the complexity of software, a small local change may have a significant global system impact.\nIf a change exists in the software, the whole validation status of the software needs to be re-established.\nneed to determine the extent and impact of that change on the entire software system.\nthe software developer should then conduct an appropriate level of software regression testing to show that unchanged but vulnerable portions of the system have not been adversely affected.\n\n\n\n2.4.8 Validation Coverage\n\nValidation coverage should be based on the software’s complexity and safety risk.\nThe selection of validation activities, tasks, and work items should be commensurate with the complexity of the software design and the risk associated with the use of the software for the specified intended use.\n\n\n\n2.4.9 Independence of Review\n\nValidation activities should be based on the basic quality assurance precept of “independence of review.”\nSelf-validation is extremely difficult.\nWhen possible, an independent evaluation is always better (like a contracted third-party independent verification and validation)\nAnother approach is to assign internal staff members that are not involved in a particular design or its implementation, but who have sufficient knowledge to evaluate the project and conduct the verification and validation activities.\n\n\n\n2.4.10 Flexibility and Responsibility\nThe device manufacturer has flexibility in choosing how to apply these validation principles, but retains ultimate responsibility for demonstrating that the software has been validated. FDA regulated medical device applications include software that:\n\nIs a component, part, or accessory of a medical device;\n\ncomponents: e.g., application software, operating systems, compilers, debuggers, configuration management tools, and many more\n\nIs itself a medical device; or\nIs used in manufacturing, design and development, or other parts of the quality system.\nNo matter how complex and disperse the software is, the manufacturer is in charge of responsibility for software validation."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#activities-and-tasks",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#activities-and-tasks",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.5 Activities and Tasks",
    "text": "2.5 Activities and Tasks\nSoftware validation is accomplished through a series of activities and tasks that are planned and executed at various stages of the software development life cycle. These tasks may be\n\none time occurrences\niterated many times\n\n\n2.5.1 Software Life Cycle Activities\n\nSoftware developers should establish a software life cycle model that is appropriate for their product and organization.\nThe selected software life cycle model should cover the software from its birth to its retirement.\nActivities in a typical software life cycle model:\n\nQuality Planning\nSystem Requirements Definition\nDetailed Software Requirements Specification\nSoftware Design Specification\nConstruction or Coding\nTesting\nInstallation\nOperation and Support\nMaintenance\nRetirement\n\nVerification, testing, and other tasks that support software validation occur during each of these activities.\nSeveral software life cycle models defined in FDA’s Glossary of Computerized System and Software Development\n\nTerminology dated August 1995:\n\nwaterfall\nspiral\nrapid prototyping\nincremental development, etc.\n\n\n\n2.5.2 Typical Tasks Supporting Validation\nthe software developer should at least consider each of the risk-related tasks and should define and document which tasks are or are not appropriate for their specific application.\n\n2.5.2.1 Quality Planning\nDesign and development planning should culminate in a plan that identifies\n\nnecessary tasks,\nprocedures for anomaly reporting and resolution,\nnecessary resources, and\nmanagement review requirements including formal design reviews.\n\nThe plan should include:\n\nThe specific tasks for each life cycle activity;\nEnumeration of important quality factors (e.g., reliability, maintainability, and usability);\nMethods and procedures for each task;\nTask acceptance criteria;\nCriteria for defining and documenting outputs in terms that will allow evaluation of their conformance to input requirements;\nInputs for each task;\nOutputs from each task;\nRoles, resources, and responsibilities for each task;\nRisks and assumptions; and\nDocumentation of user needs.\n\nThe plan should identify\n\nthe personnel,\nthe facility and equipment resources for each task, and\nthe role that risk (hazard) management will play.\n\nA configuration management plan should be developed that will guide and control multiple parallel development activities and ensure proper communications and documentation.\nControls are necessary to ensure positive and correct correspondence among all approved versions of the specifications documents, source code, object code, and test suites that comprise a software system. The controls also should ensure accurate identification of, and access to, the currently approved versions.\nProcedures should be created for reporting and resolving software anomalies found through validation or other activities.\nManagement should identify the reports and specify the contents, format, and responsible organizational elements for each report. Procedures also are necessary for the review and approval of software development results, including the responsible organizational elements for such reviews and approvals.\nTypical Tasks – Quality Planning\n\nRisk (Hazard) Management Plan\nConfiguration Management Plan\nSoftware Quality Assurance Plan\n\nSoftware Verification and Validation Plan\n\nVerification and Validation Tasks, and Acceptance Criteria\nSchedule and Resource Allocation (for software verification and validation activities)\nReporting Requirements\n\nFormal Design Review Requirements\nOther Technical Review Requirements\n\nProblem Reporting and Resolution Procedures\nOther Support Activities\n\n\n\n2.5.2.2 Requirements\nRequirements development includes the\n\nidentification,\nanalysis, and\ndocumentation of information about the device and its intended use.\n\nAreas of special importance include allocation of system functions to\n\nhardware/software,\noperating conditions,\nuser characteristics,\npotential hazards, and\nanticipated tasks.\n\nIn addition, the requirements should state clearly the intended use of the software. It is not possible to validate software without predetermined and documented software requirements. Typical software requirements specify the following:\n\nAll software system inputs;\nAll software system outputs;\nAll functions that the software system will perform;\nAll performance requirements that the software will meet, (e.g., data throughput, reliability, and timing);\nThe definition of all external and user interfaces, as well as any internal software-to-system interfaces;\nHow users will interact with the system;\nWhat constitutes an error and how errors should be handled;\nRequired response times;\nThe intended operating environment for the software, if this is a design constraint (e.g., hardware platform, operating system);\nAll ranges, limits, defaults, and specific values that the software will accept; and\nAll safety related requirements, specifications, features, or functions that will be implemented in software.\n\nSoftware requirement specifications should identify clearly the potential hazards that can result from a software failure in the system as well as any safety requirements to be implemented in software.\nThe consequences of software failure should be evaluated, along with means of mitigating such failures (e.g., hardware mitigation, defensive programming, etc.).\nThe Quality System regulation requires a mechanism for addressing incomplete, ambiguous, or conflicting requirements. (See 21 CFR 820.30(c).) Each requirement (e.g., hardware, software, user, operator interface, and safety) identified in the software requirements specification should be evaluated for accuracy, completeness, consistency, testability, correctness, and clarity.\nFor example, software requirements should be evaluated to verify that:\n\nThere are no internal inconsistencies among requirements;\nAll of the performance requirements for the system have been spelled out;\nFault tolerance, safety, and security requirements are complete and correct;\nAllocation of software functions is accurate and complete;\nSoftware requirements are appropriate for the system hazards; and\nAll requirements are expressed in terms that are measurable or objectively verifiable.\n\nA software requirements traceability analysis should be conducted to trace software requirements to (and from) system requirements and to risk analysis results. In addition to any other analyses and documentation used to verify software requirements, a formal design review is recommended to confirm that requirements are fully specified and appropriate before extensive software design efforts begin. Requirements can be approved and released incrementally, but care should be taken that interactions and interfaces among software (and hardware) requirements are properly reviewed, analyzed, and controlled.\nTypical Tasks – Requirements\n\nPreliminary Risk Analysis\nTraceability Analysis\n\nSoftware Requirements to System Requirements (and vice versa)\nSoftware Requirements to Risk Analysis\n\nDescription of User Characteristics\nListing of Characteristics and Limitations of Primary and Secondary Memory\nSoftware Requirements Evaluation\nSoftware User Interface Requirements Analysis\nSystem Test Plan Generation\nAcceptance Test Plan Generation\nAmbiguity Review or Analysis\n\n\n\n2.5.2.3 Design\nIn the design process, the software requirements specification is translated into a logical and physical representation of the software to be implemented. The software design specification is a description of what the software should do and how it should do it. The design specification may contain both a high level summary of the design and detailed design information. Human factors engineering should be woven into\n\nthe entire design and development process,\nthe device design requirements,\nanalyses, and\ntests.\n\nDevice safety and usability issues should be considered when developing\n\nflowcharts,\nstate diagrams,\nprototyping tools, and\ntest plans.\n\nAlso, task and function analyses, risk analyses, prototype tests and reviews, and full usability tests should be performed. Participants from the user population should be included when applying these methodologies.\nThe software design specification should include:\n\nSoftware requirements specification, including predetermined criteria for acceptance of the software;\nSoftware risk analysis;\nDevelopment procedures and coding guidelines (or other programming procedures);\nSystems documentation (e.g., a narrative or a context diagram) that describes the systems context in which the program is intended to function, including the relationship of hardware, software, and the physical environment;\nHardware to be used;\nParameters to be measured or recorded;\nLogical structure (including control logic) and logical processing steps (e.g., algorithms);\nData structures and data flow diagrams;\nDefinitions of variables (control and data) and description of where they are used;\nError, alarm, and warning messages;\nSupporting software (e.g., operating systems, drivers, other application software);\nCommunication links (links among internal modules of the software, links with the supporting software, links with the hardware, and links with the user);\nSecurity measures (both physical and logical security); and\nAny additional constraints not identified in the above elements.\n\nThe first four of the elements noted above usually are separate pre-existing documents that are included by reference in the software design specification. Software requirements specification was discussed in the preceding section, as was software risk analysis.\nSoftware design evaluations criteria:\n\ncomplete,\ncorrect,\nconsistent,\nunambiguous,\nfeasible,\nmaintainable,\nanalyses of control flow,\ndata flow,\ncomplexity,\ntiming,\nsizing,\nmemory allocation,\ncriticality analysis, and many other aspects of the design\n\nAppropriate consideration of software architecture (e.g., modular structure) during design can reduce the magnitude of future validation efforts when software changes are needed.\nA traceability analysis should be conducted to verify that the software design implements all of the software requirements. As a technique for identifying where requirements are not sufficient, the traceability analysis should also verify that all aspects of the design are traceable to software requirements.\nAn analysis of communication links should be conducted to evaluate the proposed design with respect to hardware, user, and related software requirements. At the end of the software design activity, a Formal Design Review should be conducted to verify that the design is correct, consistent, complete, accurate, and testable, before moving to implement the design.\nSeveral versions of both the software requirement specification and the software design specification should be maintained. All approved versions should be archived and controlled in accordance with established configuration management procedures.\nTypical Tasks – Design\n\nUpdated Software Risk Analysis\nTraceability Analysis - Design Specification to Software Requirements (and vice versa)\nSoftware Design Evaluation\nDesign Communication Link Analysis\nModule Test Plan Generation\nIntegration Test Plan Generation\nTest Design Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.4 Construction or Coding\nSoftware may be constructed either by coding. Coding is the software activity where the detailed design specification is implemented as source code. It is the last stage in decomposition of the software requirements where module specifications are translated into a programming language.\nCoding usually involves the use of a high-level programming language, but may also entail the use of assembly language (or microcode) for time-critical operations.\nA source code traceability analysis is an important tool to verify that all code is linked to established specifications and established test procedures. A source code traceability analysis should be conducted and documented to verify that:\n\nEach element of the software design specification has been implemented in code;\nModules and functions implemented in code can be traced back to an element in the software design specification and to the risk analysis;\nTests for modules and functions can be traced back to an element in the software design specification and to the risk analysis; and\nTests for modules and functions can be traced to source code for the same modules and functions.\n\nTypical Tasks – Construction or Coding\n\nTraceability Analyses\n\nSource Code to Design Specification (and vice versa)\nTest Cases to Source Code and to Design Specification\n\nSource Code and Source Code Documentation Evaluation\nSource Code Interface Analysis\nTest Procedure and Test Case Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.5 Testing by the Software Developer\nSoftware testing entails running software products under known conditions with defined inputs and documented outcomes that can be compared to their predefined expectations. It is a time consuming, difficult, and imperfect activity.\nAs such, it requires early planning in order to be effective and efficient. Test plans and test cases should be created as early in the software development process as feasible.\nThey should identify\n\nthe schedules,\nenvironments,\nresources (personnel, tools, etc.),\nmethodologies,\ncases (inputs, procedures, outputs, expected results),\ndocumentation, and\nreporting criteria.\n\nDescriptions of categories of software and software testing effort appear in the literature\n\nNIST Special Publication 500-235, Structured Testing: A Testing Methodology Using the Cyclomatic Complexity Metric;\nNUREG/CR-6293, Verification and Validation Guidelines for High Integrity Systems; and\nIEEE Computer Society Press, Handbook of Software Reliability Engineering.\n\nTesting of all program functionality does not mean all of the program has been tested. Testing of all of a program’s code does not mean all necessary functionality is present in the program. Testing of all program functionality and all program code does not mean the program is 100% correct! Software testing that finds no errors should not be interpreted to mean that errors do not exist in the software product; it may mean the testing was superficial.\nAn essential element of a software test case is the expected result. It is the key detail that permits objective evaluation of the actual test result. This necessary testing information is obtained from the corresponding, predefined definition or specification.\nA software testing process should be based on principles that foster effective examinations of a software product. Applicable software testing tenets include:\n\nThe expected test outcome is predefined;\nA good test case has a high probability of exposing an error;\nA successful test is one that finds an error;\nThere is independence from coding;\nBoth application (user) and software (programming) expertise are employed;\nTesters use different tools from coders;\nExamining only the usual case is insufficient;\nTest documentation permits its reuse and an independent confirmation of the pass/fail status of a test outcome during subsequent review.\n\nCode-based testing is also known as structural testing or “white-box” testing. It identifies test cases based on knowledge obtained from the source code, detailed design specification, and other development documents. Structural testing can identify “dead” code that is never executed when the program is run. Structural testing is accomplished primarily with unit (module) level testing, but can be extended to other levels of software testing.\nThe level of structural testing can be evaluated using metrics that are designed to show what percentage of the software structure has been evaluated during structural testing. These metrics are typically referred to as “coverage” and are a measure of completeness with respect to test selection criteria. The amount of structural coverage should be commensurate with the level of risk posed by the software. Use of the term “coverage” usually means 100% coverage. Common structural coverage metrics include:\n\nStatement Coverage – This criteria requires sufficient test cases for each program statement to be executed at least once; however, its achievement is insufficient to provide confidence in a software product’s behavior.\nDecision (Branch) Coverage – This criteria requires sufficient test cases for each program decision or branch to be executed so that each possible outcome occurs at least once. It is considered to be a minimum level of coverage for most software products, but decision coverage alone is insufficient for high-integrity applications.\nCondition Coverage – This criteria requires sufficient test cases for each condition in a program decision to take on all possible outcomes at least once. It differs from branch coverage only when multiple conditions must be evaluated to reach a decision.\nMulti-Condition Coverage – This criteria requires sufficient test cases to exercise all possible combinations of conditions in a program decision.\nLoop Coverage – This criteria requires sufficient test cases for all program loops to be executed for zero, one, two, and many iterations covering initialization, typical running and termination (boundary) conditions.\nPath Coverage – This criteria requires sufficient test cases for each feasible path, basis path, etc., from start to exit of a defined program segment, to be executed at least once. Because of the very large number of possible paths through a software program, path coverage is generally not achievable. The amount of path coverage is normally established based on the risk or criticality of the software under test.\nData Flow Coverage – This criteria requires sufficient test cases for each feasible data flow to be executed at least once. A number of data flow testing strategies are available.\n\nThe following types of functional software testing involve generally increasing levels of effort:\n\nNormal Case – Testing with usual inputs is necessary. However, testing a software product only with expected, valid inputs does not thoroughly test that software product. By itself, normal case testing cannot provide sufficient confidence in the dependability of the software product.\nOutput Forcing – Choosing test inputs to ensure that selected (or all) software outputs are generated by testing.\nRobustness – Software testing should demonstrate that a software product behaves correctly when given unexpected, invalid inputs. Methods for identifying a sufficient set of such test cases include Equivalence Class Partitioning, Boundary Value Analysis, and Special Case Identification (Error Guessing). While important and necessary, these techniques do not ensure that all of the most appropriate challenges to a software product have been identified for testing.\nCombinations of Inputs – The functional testing methods identified above all emphasize individual or single test inputs. Most software products operate with multiple inputs under their conditions of use. Thorough software product testing should consider the combinations of inputs a software unit or system may encounter during operation. Error guessing can be extended to identify combinations of inputs, but it is an ad hoc technique. Cause-effect graphing is one functional software testing technique that systematically identifies combinations of inputs to a software product for inclusion in test cases.\n\nFunctional and structural software test case identification techniques provide specific inputs for testing, rather than random test inputs. One weakness of these techniques is the difficulty in linking structural and functional test completion criteria to a software product’s reliability.\nAdvanced software testing methods, such as statistical testing, can be employed to provide further assurance that a software product is dependable. Statistical testing uses randomly generated test data from defined distributions based on an operational profile (e.g., expected use, hazardous use, or malicious use of the software product). Large amounts of test data are generated and can be targeted to cover particular areas or concerns, providing an increased possibility of identifying individual and multiple rare operating conditions that were not anticipated by either the software product’s designers or its testers. Statistical testing also provides high structural coverage. It does require a stable software product. Thus, structural and functional testing are prerequisites for statistical testing of a software product.\nAnother aspect of software testing is the testing of software changes. Changes occur frequently during software development. These changes are the result of\n\ndebugging that finds an error and it is corrected,\nnew or changed requirements (“requirements creep”), and\nmodified designs as more effective or efficient implementations are found.\n\nOnce a software product has been baselined (approved), any change to that product should have its own “mini life cycle,” including testing. Testing of a changed software product requires additional effort. It should demonstrate\n\nthat the change was implemented correctly, and\nthat the change did not adversely impact other parts of the software product.\n\nRegression analysis is the determination of the impact of a change based on review of the relevant documentation in order to identify the necessary regression tests to be run. Regression testing is the rerunning of test cases that a program has previously executed correctly and comparing the current result to the previous result in order to detect unintended effects of a software change. Regression analysis and regression testing should also be employed when using integration methods to build a software product to ensure that newly integrated modules do not adversely impact the operation of previously integrated modules.\nIn order to provide a thorough and rigorous examination of a software product, development testing is typically organized into levels: unit, integration, and system levels of testing.\n\nUnit (module or component) level testing focuses on the early examination of sub-program functionality and ensures that functionality not visible at the system level is examined by testing. Unit testing ensures that quality software units are furnished for integration into the finished software product.\nIntegration level testing focuses on the transfer of data and control across a program’s internal and external interfaces. External interfaces are those with\n\nother software (including operating system software),\nsystem hardware, and\nthe users and can be described as communications links.\n\nSystem level testing demonstrates that all specified functionality exists and that the software product is trustworthy. This testing verifies the as-built program’s functionality and performance with respect to the requirements for the software product as exhibited on the specified operating platform(s). System level software testing addresses functional concerns and the following elements of a device’s software that are related to the intended use(s):\n\nPerformance issues (e.g., response times, reliability measurements);\nResponses to stress conditions, e.g., behavior under maximum load, continuous use;\nOperation of internal and external security features;\nEffectiveness of recovery procedures, including disaster recovery;\nUsability; (Usability vs Utility??)\nCompatibility with other software products;\nBehavior in each of the defined hardware configurations; and\nAccuracy of documentation.\n\n\nControl measures (e.g., a traceability analysis) should be used to ensure that the intended coverage is achieved.\nSystem level testing also exhibits the software product’s behavior in the intended operating environment. The location of such testing is dependent upon the software developer’s ability to produce the target operating environment(s). Depending upon the circumstances, simulation and/or testing at (potential) customer locations may be utilized.\nTest plans should identify the controls needed to ensure\n\nthat the intended coverage is achieved and\nthat proper documentation is prepared when planned system level testing is conducted at sites not directly controlled by the software developer.\n\nTest procedures, test data, and test results\n\nshould be documented in a manner permitting objective pass/fail decisions to be reached.\nshould also be suitable for review and objective decision making subsequent to running the test,\nshould be suitable for use in any subsequent regression testing.\n\nErrors detected during testing should be\n\nlogged,\nclassified,\nreviewed, and\nresolved prior to release of the software.\n\nSoftware error data that is collected and analyzed during a development life cycle may be used to determine the suitability of the software product for release for commercial distribution. Test reports should comply with the requirements of the corresponding test plans.\nSoftware testing tools are frequently used to ensure consistency, thoroughness, and efficiency in the testing of such software products and to fulfill the requirements of the planned testing activities.\nAppropriate documentation providing evidence of the validation of these software tools for their intended use should be maintained (see section 6 of this guidance).\nTypical Tasks – Testing by the Software Developer\n\nTest Planning\nStructural Test Case Identification\nFunctional Test Case Identification\nTraceability Analysis - Testing\nUnit (Module) Tests to Detailed Design\nIntegration Tests to High Level Design\nSystem Tests to Software Requirements\nUnit (Module) Test Execution\nIntegration Test Execution\nFunctional Test Execution\nSystem Test Execution\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.6 User Site Testing\nTesting at the user site is an essential part of software validation. The Quality System regulation requires\n\ninstallation and\ninspection procedures (including testing where appropriate) as well as\ndocumentation of inspection and\ntesting to demonstrate proper installation. (See 21 CFR §820.170.)\n\nLikewise, manufacturing equipment must meet specified requirements, and automated systems must be validated for their intended use. (See 21 CFR §820.70(g) and 21 CFR §820.70(i) respectively.)\nTerminology regarding user site testing can be confusing. Terms such as\n\nbeta test,\nsite validation,\nuser acceptance test,\ninstallation verification, and\ninstallation testing have all been used to describe user site testing.\n\nFor the purposes of this guidance, the term “user site testing” encompasses all of these and any other testing that takes place outside of the developer’s controlled environment.\nThis testing should take place at a user’s site with the actual hardware and software that will be part of the installed system configuration. The testing is accomplished through either actual or simulated use of the software being tested within the context in which it is intended to function.\nTest planners should check with the FDA Center(s) with the corresponding product jurisdiction to determine whether there are any additional regulatory requirements for user site testing.\nUser site testing should follow a pre-defined written plan with\n\na formal summary of testing and\na record of formal acceptance.\n\nThe following documented evidence should be retained:\n\nall testing procedures,\ntest input data, and\ntest results\n\nThere should be evidence that hardware and software are installed and configured as specified. Measures should ensure that all system components are exercised during the testing and that the versions of these components are those specified. The testing plan should specify testing throughout the full range of operating conditions and should specify continuation for a sufficient time to allow the system to encounter a wide spectrum of conditions and events in an effort to detect any latent faults that are not apparent during more normal activities.\nSome of the evaluations of the system’s ability that have been performed earlier by the software developer at the developer’s site should be repeated at the site of actual use. These may include tests for:\n\na high volume of data,\nheavy loads or stresses,\nsecurity,\nfault testing (avoidance, detection, tolerance, and recovery),\nerror messages, and\nimplementation of safety requirements.\n\nThere should be an evaluation of the ability of the users of the system to understand and correctly interface with it.\nOperators should be able to perform the intended functions and respond in an appropriate and timely manner to all alarms, warnings, and error messages.\nRecords should be maintained of both proper system performance and any system failures that are encountered.\nThe revision of the system to compensate for faults detected during this user site testing should follow the same procedures and controls as for any other software change.\nThe developers of the software may or may not be involved in the user site testing.\n\nIf the developers are involved, they may seamlessly carry over to the user’s site the last portions of design-level systems testing.\nIf the developers are not involved, it is all the more important that the user have persons who understand the importance of careful test planning, the definition of expected test results, and the recording of all test outputs.\n\nTypical Tasks – User Site Testing\n\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.7 Maintenance and Software Changes\n\n2.5.2.7.1 Hardware vs Software\nHardware maintenance typically includes\n\npreventive hardware maintenance actions,\ncomponent replacement, and\ncorrective changes.\n\nSoftware maintenance includes\n\ncorrective,\nperfective, and\nadaptive maintenance\nbut does not include preventive maintenance actions or software component replacement.\n\n\n\n2.5.2.7.2 Maintenance Types\n\nCorrective maintenance: Changes made to correct errors and faults in the software.\nPerfective maintenance: Changes made to the software to improve the performance, maintainability, or other attributes of the software system .\nAdaptive maintenance: Changes to make the software system usable in a changed environment.\n\nSufficient regression analysis and testing should be conducted to demonstrate that portions of the software not involved in the change were not adversely impacted. When changes are made to a software system,\n\neither during initial development or\nduring post release maintenance,\n\nThis is in addition to testing that evaluates the correctness of the implemented change(s). The specific validation effort necessary for each software change is determined by\n\nthe type of change,\nthe development products affected, and the\nimpact of those products on the operation of the software.\n\n\n\n2.5.2.7.3 Factors of Limitting Validation Effort Needed When a Change Is Made\n\ncareful and complete documentation of the design structure and\ncareful and complete documentation of interrelationships of various modules,\ninterfaces, etc.\nFor example,\n\ntest documentation,\ntest cases, and\nresults of previous verification and validation testing All of them need to be archived if they are to be available for performing subsequent regression testing.\n\n\nThe following additional maintenance tasks should be addressed:\n\nSoftware Validation Plan Revision - For software that was previously validated, the existing software validation plan should be revised to support the validation of the revised software. If no previous software validation plan exists, such a plan should be established to support the validation of the revised software.\nAnomaly Evaluation – Software organizations frequently maintain documentation, such as software problem reports that describe software anomalies discovered and the specific corrective action taken to fix each anomaly.\n\nToo often, however, mistakes are repeated because software developers do not take the next step to determine the root causes of problems and make the process and procedural changes needed to avoid recurrence of the problem.\nSoftware anomalies should be evaluated in terms of their severity and their effects on system operation and safety,\nbut they should also be treated as symptoms of process deficiencies in the quality system.\nA root cause analysis of anomalies can identify specific quality system deficiencies.\nWhere trends are identified (e.g., recurrence of similar software anomalies), appropriate corrective and preventive actions must be implemented and documented to avoid further recurrence of similar quality problems. (See 21 CFR 820.100.)\n\nProblem Identification and Resolution Tracking - All problems discovered during maintenance of the software should be documented. The resolution of each problem should be tracked to ensure it is fixed, for historical reference, and for trending.\nProposed Change Assessment - All proposed modifications, enhancements, or additions should be assessed to determine the effect each change would have on the system. This information should determine the extent to which verification and/or validation tasks need to be iterated.\nTask Iteration - For approved software changes, all necessary verification and validation tasks should be performed to ensure that planned changes are implemented correctly, all documentation is complete and up to date, and no unacceptable changes have occurred in software performance.\nDocumentation Updating – Documentation should be carefully reviewed to determine which documents have been impacted by a change. All approved documents (e.g., specifications, test procedures, user manuals, etc.) that have been affected should be updated in accordance with configuration management procedures. Specifications should be updated before any maintenance and software changes are made."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.6 Validation of Automated Process Equipment and Quality System Software",
    "text": "2.6 Validation of Automated Process Equipment and Quality System Software\nThe Quality System regulation requires that “when computers or automated data processing systems are used as part of production or the quality system, the [device] manufacturer shall validate computer software for its intended use according to an established protocol.” (See 21 CFR §820.70(i)). This has been a regulatory requirement of FDA’s medical device Good Manufacturing Practice (GMP) regulations since 1978.\nComputer systems that implement part of a device manufacturer’s production processes or quality system (or that are used to create and maintain records required by any other FDA regulation) are subject to the Electronic Records; Electronic Signatures regulation. (See 21 CFR Part 11.) This regulation establishes additional security, data integrity, and validation requirements when records are created or maintained electronically. These additional Part 11 requirements should be carefully considered and included in system requirements and software requirements for any automated record keeping systems. System validation and software validation should demonstrate that all Part 11 requirements have been met.\nComputers and automated equipment are used extensively throughout all aspects of\n\nmedical device design,\nlaboratory testing and analysis,\nproduct inspection and acceptance,\nproduction and process control,\nenvironmental controls,\npackaging,\nlabeling,\ntraceability,\ndocument control,\ncomplaint management, and many other aspects of the quality system.\n\nIncreasingly, automated plant floor operations can involve extensive use of embedded systems in:\n\nprogrammable logic controllers;\ndigital function controllers;\nstatistical process control;\nsupervisory control and data acquisition;\nrobotics;\nhuman-machine interfaces;\ninput/output devices; and\ncomputer operating systems.\n\nAll software tools used for software design are subject to the requirement for software validation, but the validation approach used for each application can vary widely.\nValidation is typically supported by:\n\nverifications of the outputs from each stage of that software development life cycle; and\nchecking for proper operation of the finished software in the device manufacturer’s intended use environment.\n\n\n2.6.1 How Much Validation Evidence Is Needed?\nThe level of validation effort should be commensurate with\n\nthe risk posed by the automated operation,\nthe complexity of the process software,\nthe degree to which the device manufacturer is dependent upon that automated process to produce a safe and effective device\n\nDocumented requirements and risk analysis of the automated process help to define the scope of the evidence needed to show that the software is validated for its intended use. Without a plan, extensive testing may be needed for:\n\na plant-wide electronic record and electronic signature system;\nan automated controller for a sterilization cycle; or\nautomated test equipment used for inspection and acceptance of finished circuit boards in a lifesustaining / life-supporting device.\n\nHigh risk applications should not be running in the same operating environment with non-validated software functions, even if those software functions are not used. Risk mitigation techniques such as memory partitioning or other approaches to resource protection may need to be considered when high risk applications and lower risk applications are to be used in the same operating environment.\nWhen software is upgraded or any changes are made to the software, the device manufacturer should consider how those changes may impact the “used portions” of the software and must reconfirm the validation of those portions of the software that are used. (See 21 CFR §820.70(i).)\n\n\n2.6.2 Defined User Equipment\nA very important key to software validation is a documented user requirements specification that defines:\n\nthe “intended use” of the software or automated equipment; and\nthe extent to which the device manufacturer is dependent upon that software or equipment for production of a quality medical device.\n\nThe device manufacturer (user) needs to define the expected operating environment including any required hardware and software configurations, software versions, utilities, etc. The user also needs to:\n\ndocument requirements for system performance, quality, error handling, startup, shutdown, security, etc.;\nidentify any safety related functions or features, such as sensors, alarms, interlocks, logical processing steps, or command sequences; and\ndefine objective criteria for determining acceptable performance.\n\nThe validation must be conducted in accordance with a documented protocol, and the validation results must also be documented. (See 21 CFR §820.70(i).) Test cases should be documented that will exercise the system to challenge its performance against the pre-determined criteria, especially for its most critical parameters.\nTest cases should address\n\nerror and alarm conditions,\nstartup, shutdown,\nall applicable user functions and operator controls,\npotential operator errors,\nmaximum and minimum ranges of allowed values, and\nstress conditions applicable to the intended use of the equipment.\n\nThe test cases should be executed and the results should be recorded and evaluated to determine whether the results support a conclusion that the software is validated for its intended use.\nA device manufacturer may conduct a validation using their own personnel or may depend on a third party such as the equipment/software vendor or a consultant. In any case, the device manufacturer retains the ultimate responsibility for ensuring that the production and quality system software:\n\nis validated according to a written procedure for the particular intended use; and\nwill perform as intended in the chosen application.\n\nThe device manufacturer should have documentation including:\n\ndefined user requirements;\nvalidation protocol used;\nacceptance criteria;\ntest cases and results; and\na validation summary that objectively confirms that the software is validated for its intended use.\n\n\n\n2.6.3 Validation of Off-The-Shelf Software and Automated Equipment\nMost of the automated equipment and systems used by device manufacturers are supplied by thirdparty vendors and are purchased off-the-shelf (OTS). The device manufacturer is responsible for ensuring that the product development methodologies used by the OTS software developer are appropriate and sufficient for the device manufacturer’s intended use of that OTS software.\nWhere possible and depending upon the device risk involved, the device manufacturer should consider auditing the vendor’s design and development methodologies used in the construction of the OTS software and should assess the development and validation documentation generated for the OTS software. Such audits can be conducted by the device manufacturer or by a qualified third party.\nThe audit should demonstrate that the vendor’s procedures for and results of the verification and validation activities performed the OTS software are appropriate and sufficient for the safety and effectiveness requirements of the medical device to be produced using that software."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html",
    "title": "Content List, Validation",
    "section": "",
    "text": "0000-00-00, EN62304"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#sgs",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#sgs",
    "title": "Content List, Validation",
    "section": "",
    "text": "0000-00-00, EN62304"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#fda",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#fda",
    "title": "Content List, Validation",
    "section": "2 FDA",
    "text": "2 FDA\n\n2023-01-27, General Principles of SW Validation\n2023-01-27, General Principles of SW Validation - Diagram Summary\n1111-11-11, Guidance for the Content of Premarket Submissions for Software Contained in Medical Devices"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#dhf",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#dhf",
    "title": "Content List, Validation",
    "section": "3 DHF",
    "text": "3 DHF"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#public-health",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#public-health",
    "title": "Content List, Validation",
    "section": "4 Public Health",
    "text": "4 Public Health"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#wet-lab",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#wet-lab",
    "title": "Content List, Validation",
    "section": "5 Wet Lab",
    "text": "5 Wet Lab\n\n0000-00-00, PCR (Polymerase Chain Reaction) Experiment"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html",
    "href": "docs/projects/dsp_validation/index.html",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n의료 장비와 연관된 시약 제품의 특성상 Global Market 진출시 각 국가의 정부에서 자국민의 건강 및 생명의 안전을 위해 요구하는 규제사항들이 있다.\n\n시약의 안정성 검증 요구\n장비의 안정성 검증 요구\nSoftware의 안정성 검증 요구\nDiagnostic Algorithm의 안정성 검증 요구\n\nCOVID19 특수 시기 해제 후 Global market으로의 진입 및 관리를 위해 각 국의 정부가 요구하는 제품의 안정성 검증 및 규제 사항들을 충족시켜야한다.\nEU(European Union)의 경우 IVDR (In Vitro Diagnostics Regulation) 을 요구한다.\n북미시장에 진출하기 위해 세계에서 가장 엄격한 기준을 요구하는 미국의 FDA와 캐나다의 Health Canada의 surveilance 기준으로 진단 알고리즘의 안정성 검증 문서를 기획하고 작성해야한다.\n시간이 지날 수록 각 국에서 software 및 algorithms에 대한 규제가 강화되고 있기때문에 기존의 Software Engineering에 의한 안전성 검증 방식보다 더 엄격한 Advanced Testing이 요구되고 있다.\nSeegene의 Diagnostic Signal Process (DSP) Algorithm의 안정성 검증 방식은 기업의 사업성과 직결되는 만큼 회사내에서 1급 보안사항으로 분류되어 구체적이고 자세한 기획 및 구현 내용을 공유할 수는 없다.\n\n\n\n\n\nalgorithm이 안전한 성능을 보여준다는 것을 통계적으로 증명하기 위한 system을 기획한다.\nalgorithm이 안전한 성능을 보여준다는 것을 Statistical Validation Sytem을 확립하여 통계적 분석으로 증명한다.\n\n여기서, 확립 (Establishment)은 정의(definition), 문서화(documentation) 및 구현 (Implement)으로 정의된다.\n\nalgorithm의 risk를 구체적으로 정의하고 risk가 algorithm에 미치는 영향도를 정량 분석한다.\nalgorithm이 risk 관리가 가능하다는 것을 statistical simulation을 통해 증명한다.\nalgorithm 구현 및 운영에 있어서 code의 변화가 있을 경우 새로운 validation report를 제출해야하므로 자동화 시스템을 구축한다.\n\n\n\n\n\n세계에서 가장 엄격한 검열 인증서를 발급 및 교육을 제공하는 기업인 SGS의 guidance를 참고한다.\nSGS는 FDA를 target으로 guidance를 제공한다.\nSoftware의 안전성 검증을 위해 FDA에서 제공하는 General Principles of Software Validation 문서를 정독 후 이 문서를 기반으로 validation system을 확립한다.\n\nCopy: General Principles of Software Validation\nSummary: General Principles of Software Validation\nDiagram: General Principles of Software Validation\n\nSoftware engineering은 General Principles of Software Validation 문서를 기반하여 수행한다.\nDiagnostic Algorithm의 안정성 검증은 Structural Testing와 Advanced Testing 모두 포함한다. Structural Testing은 code 기반의 Software Engineering Testing을 의미하고 Advanced Testing은 Statiscal Analysis에 기반한 Statistical Testing을 의미한다. Advanced Testing은 안정적인 Software Engineering System 구축이 그 전제가 된다.\nalgorithm 안전성에 대한 정의와 논리를 확립한다.\nalgorithm 안전성에 대한 지표를 확립한다.\nAdvanced Testing인 Statistical Testing은 data scientist의 창의성이 요구되는 작업으로 Testing Model을 기획하여 statistical analysis design을 구체화 및 문서화한다.\nBT (Biotechnology) 부문과 IT (Information technology) 부문의 협력이 전제가 되어야하며 BT 부서의 experiment design 및 limitation factors at a experimental level을 고려한 engineering design과 statistical design을 확립한다.\n기획된 Testing model에 적합한 statistical model을 찾고 minimum reuirement sample size를 계산한다.\n위의 전략대로, BT 부서에서의 실험과 IT 부서 (Data Science 팀)에서 분석을 수행한다.\nalgorithm 구현 및 운영에 있어서 code의 변화가 있을 경우와 새로운 제품에 대한 새로운 validation report를 제출해야하는 의무사항에 대비한 문서 자동화 시스템을 구축한다.\n\n\n\n\n\n\n\nBT에서 생성된 data를 입력할 수 있는 시스템 부재\nBT부서의 업무기술서 부재로 인한 소통의 어려움\n입력 데이터를 전처리하는 시스템 부재\nData Science 팀내 업무 기술서 부재로 인한 팀내 소통의 어려움\nValidation report에 대한 선례 및 template를 찾을 수 없을 정도로 매우 드물다.\n\n\n\n\n\nBT에서 생성된 data를 입력할 수 있는 시스템 구축\n\ndigitalization: experimental design file, raw data generated from medical device, data extracted from medical device\n\nBT부서와의 소통으로 업무 문서화를 진행하여 실험 결과의 기대 정답 기준 확립, 독립 변수 및 종속 변수 확립\n입력 데이터를 전처리하여 diagnostic algorithm의 결과물을 병합하는 engineering 시스템 구축\nData Quality Control Process 강화\n\n1단계 오타 교정\n2단계 결측치 처리\n3단계 anomaly data 처리\n4단계 algorithm data 정합성 1차 검정: FDA validtion을 위한 전처리된 algorithm vs Original algorithm\n5단계 algorithm data 정합성 2차 검정: Data Science팀의 FDA validtion을 위한 전처리된 algorithm vs BT 부서에 published algorithm\n\nData Science 팀내 업무 기술서 작성으로 코드 중앙화, 데이터 중앙화, 특이사항 문서화 실현\nSeegene 고유의 software testing & advanced testing model 기획 및 확립 후 statistical analysis 기획 및 수행\n\n\n\n\n\n\nFDA software validation knowledge\nStatistics\nDynamic documentation\nBiology\nClinical study design\n\n\n\n\n\n5 data scientists (I am a project owner.)\n3 data enineers\n27 biologists\n2 patent attorneys\n\n\n\n\n\n\n\n\n\n\n\nDSP Algorithm Output\nDescription\n\n\n\n\nFDA Validation 1st Draft\nFDA 제출용 verification & validation report 1차 초안\n\n\nData Input System\n임시적인 데이터 입력 시스템으로 대량의 data를 대량으로 연산하는 플랫폼 구축으로 발전\n\n\nDocumentation System\n기존에 부재했던 문서화 및 문서 자동화 시스템 구축 \\(\\rightarrow\\) 업무 소통과 Relational Database System 구축에 필요\n\n\nData Management System\ndata quality control system\n\n\nFDA Validation Model\nDSP algorithm을 위한 Validation Model 확립\n\n\nPatent Invention\nFDA Validation Model 발명\n\n\nIn-house first Performance evaluation of algorithms and reagent products\n사내에 기존에 존재하지 않았던 알고리즘 및 시약 제품의 종합 성능 평가\n\n\nStatistical analysis related to algorithmic risk management\n시약과 장비 고유의 random effect와 다른 교란자로 인해 발생할 수 있는 noise 및 anomaly data에 위험 관리 관련 통계 분석을 수행\n\n\n\n\n\n\n\nBT의 업무 기술서를 협업으로 공동작성하고 RDB system 구축\n시약, 장비, software 및 algorithm Validation용 DevOps Platform 구축\n\n\n\n\n\n\n\nDue to the nature of reagent products related to medical device, there are regulations required by each country’s government for the health and life safety of its citizens when entering the global market.\n\nReagent stability verification and validation required\nEquipment stability verification and validation request\nSoftware stability verification and validation request\nStability verification and validation Request of Diagnostic Algorithm\n\nIn order to enter and manage the global market after the COVID19 special period is lifted, product safety verification and regulatory requirements required by each country’s government must be met.\nIn the case of the EU (European Union), IVDR (In Vitro Diagnostics Regulation) is required\nIn order to enter the North American market, it is necessary to plan and write a document verifying the stability of the diagnostic algorithm based on the surveilance standards of the US FDA and Canada’s Health Canada, which require the world’s most stringent standards.\nAs time goes by, regulations on software and algorithms are being strengthened in each country, so advanced testing that is more stringent than the existing safety verification method by software engineering is required.\nTherefore, the stability verification and validation of the diagnostic algorithm includes software engineering testing and advanced testing. Here, advanced testing means statistical testing based on statistical analysis, and building a stable software engineering system is the prerequisite.\nSince the stability verification method of Seegene’s Diagnostic Signal Process (DSP) Algorithm is directly related to the business performance of the company, it is classified as a first-class security matter within the company, so specific and detailed planning and implementation details cannot be shared.\n\n\n\n\n\nDesign a system to statistically prove that the algorithm shows safe performance.\nEstablish a Statistical Validation System to prove that the algorithm shows safe performance through statistical analysis.\n\nHere, Establishment is defined as Definition, Documentation, and Implementation.\n\nDefine the risk of the algorithm in detail and quantitatively analyze the effect of the risk on the algorithm.\nIt is proved through statistical simulation that the algorithm is capable of risk management.\nIn the case of code changes according to algorithm implementations and operations, a new validation report must be submitted, so an automation system is built.\n\n\n\n\n\nRefer to the guidance of SGS, a company that issues and provides training for the world’s most stringent inspection certificates.\nSGS provides guidance to the FDA as a target.\nAfter thoroughly reading the General Principles of Software Validation document provided by the FDA for software safety verification, establish a validation system based on this document.\n\nCopy: General Principles of Software Validation\nSummary: General Principles of Software Validation\nDiagram: General Principles of Software Validation\n\nSoftware engineering is performed based on the General Principles of Software Validation document.\nThe stability verification of Diagnostic Algorithm includes both Structural Testing and Advanced Testing. Structural Testing means code-based Software Engineering Testing and Advanced Testing means Statistical Testing based on Statistical Analysis. Advanced Testing is based on the establishment of a stable Software Engineering System.\nEstablish a definition and logic for algorithm safety.\nEstablish metrics or indicators for algorithm safety.\nStatistical Testing, which is Advanced Testing, is a task that requires the creativity of a data scientist, and a testing model is planned to materialize and document statistical analysis design.\nCooperation between the BT (Biotechnology) sector and the IT (Information technology) sector must be a premise, and engineering design and statistical design should be established considering the BT department’s experimental design and limitation factors at a experimental level.\nFind a statistical model suitable for the planned testing model and calculate the minimum reuirement sample size.\nAs per the above strategy, the BT department conducts experiments and the IT department (Data Science team) conducts analysis.\nEstablish a document automation system in case of code changes in algorithm implementation and operation and the obligation to submit a new validation report for new products.\n\n\n\n\n\n\n\nAbsence of a system that can input data generated by BT departments\nDifficulties in communication due to lack of job description in BT departments.\nAbsence of a system that preprocesses input data.\nDifficulties in communication within the team due to lack of job description within the Data Science team.\nIt is so rare that no precedent or template for validation report can be found.\n\n\n\n\n\nBuilding a system that can input data generated by BT departments\n\ndigitalization: experimental design file, raw data generated from medical device, data extracted from medical device\n\nWork documented through communication with the BT department to establish the standard for the expected correct answer of the experiment results, and to establish independent and dependent variables\nBuilding an engineering system that preprocesses input data and merges the results of diagnostic algorithms\nStrengthen Data Quality Control Process\n\nStep 1 typo correction\nStep 2 missing value processing\nStep 3 anomaly data processing\nStep 4 algorithm data conformity 1st Test: Preprocessed algorithm for FDA validation vs Original algorithm\nStep 5 algorithm data conformity 2nd test: Data Science team’s preprocessed algorithm for FDA validation vs algorithm published by BT department\n\nRealization of code centralization, data centralization, and documentation of specific matters by writing job descriptions within the Data Science team\nPlan and conduct statistical analysis after planning and establishing Seegene’s own software testing & advanced testing model\n\n\n\n\n\n\nFDA software validation knowledge\nStatistics\nDynamic documentation\nBiology\nClinical study design\n\n\n\n\n\n5 data scientists (I am a project owner.)\n3 data enineers\n27 biologists\n2 patent attorneys\n\n\n\n\n\n\n\n\n\n\n\nDSP Algorithm Output\nDescription\n\n\n\n\nFDA Validation 1st Draft\nthe 1st draft of verification & validation report for FDA submission\n\n\nData Input System\nIt is a temporary data input system that develops into a platform that calculates a large amount of data in large quantities.\n\n\nDocumentation System\nEstablishment of previously absent documentation and document automation systems \\(\\rightarrow\\) Necessary for business communication and establishment of Relational Database System\n\n\nData Management System\ndata quality control system\n\n\nFDA Validation Model\nEstablishment of validation model for DSP algorithm\n\n\nPatent Invention\nInventing the FDA Validation Model\n\n\nIn-house first Performance evaluation of algorithms and reagent products\nComprehensive performance evaluation of algorithms and reagent products that did not previously exist in-house\n\n\nStatistical analysis related to algorithmic risk management\nRisk management-related statistical analysis is performed on noise and anomaly data that may occur due to reagent and equipment-specific random effects and other confounders.\n\n\n\n\n\n\n\nCollaboratively write BT’s job description and establish RDB system\nBuilding a DevOps Platform for reagents, equipment, software and algorithm validation"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#background",
    "href": "docs/projects/dsp_validation/index.html#background",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "의료 장비와 연관된 시약 제품의 특성상 Global Market 진출시 각 국가의 정부에서 자국민의 건강 및 생명의 안전을 위해 요구하는 규제사항들이 있다.\n\n시약의 안정성 검증 요구\n장비의 안정성 검증 요구\nSoftware의 안정성 검증 요구\nDiagnostic Algorithm의 안정성 검증 요구\n\nCOVID19 특수 시기 해제 후 Global market으로의 진입 및 관리를 위해 각 국의 정부가 요구하는 제품의 안정성 검증 및 규제 사항들을 충족시켜야한다.\nEU(European Union)의 경우 IVDR (In Vitro Diagnostics Regulation) 을 요구한다.\n북미시장에 진출하기 위해 세계에서 가장 엄격한 기준을 요구하는 미국의 FDA와 캐나다의 Health Canada의 surveilance 기준으로 진단 알고리즘의 안정성 검증 문서를 기획하고 작성해야한다.\n시간이 지날 수록 각 국에서 software 및 algorithms에 대한 규제가 강화되고 있기때문에 기존의 Software Engineering에 의한 안전성 검증 방식보다 더 엄격한 Advanced Testing이 요구되고 있다.\nSeegene의 Diagnostic Signal Process (DSP) Algorithm의 안정성 검증 방식은 기업의 사업성과 직결되는 만큼 회사내에서 1급 보안사항으로 분류되어 구체적이고 자세한 기획 및 구현 내용을 공유할 수는 없다."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#objective",
    "href": "docs/projects/dsp_validation/index.html#objective",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "algorithm이 안전한 성능을 보여준다는 것을 통계적으로 증명하기 위한 system을 기획한다.\nalgorithm이 안전한 성능을 보여준다는 것을 Statistical Validation Sytem을 확립하여 통계적 분석으로 증명한다.\n\n여기서, 확립 (Establishment)은 정의(definition), 문서화(documentation) 및 구현 (Implement)으로 정의된다.\n\nalgorithm의 risk를 구체적으로 정의하고 risk가 algorithm에 미치는 영향도를 정량 분석한다.\nalgorithm이 risk 관리가 가능하다는 것을 statistical simulation을 통해 증명한다.\nalgorithm 구현 및 운영에 있어서 code의 변화가 있을 경우 새로운 validation report를 제출해야하므로 자동화 시스템을 구축한다."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#methodology",
    "href": "docs/projects/dsp_validation/index.html#methodology",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "세계에서 가장 엄격한 검열 인증서를 발급 및 교육을 제공하는 기업인 SGS의 guidance를 참고한다.\nSGS는 FDA를 target으로 guidance를 제공한다.\nSoftware의 안전성 검증을 위해 FDA에서 제공하는 General Principles of Software Validation 문서를 정독 후 이 문서를 기반으로 validation system을 확립한다.\n\nCopy: General Principles of Software Validation\nSummary: General Principles of Software Validation\nDiagram: General Principles of Software Validation\n\nSoftware engineering은 General Principles of Software Validation 문서를 기반하여 수행한다.\nDiagnostic Algorithm의 안정성 검증은 Structural Testing와 Advanced Testing 모두 포함한다. Structural Testing은 code 기반의 Software Engineering Testing을 의미하고 Advanced Testing은 Statiscal Analysis에 기반한 Statistical Testing을 의미한다. Advanced Testing은 안정적인 Software Engineering System 구축이 그 전제가 된다.\nalgorithm 안전성에 대한 정의와 논리를 확립한다.\nalgorithm 안전성에 대한 지표를 확립한다.\nAdvanced Testing인 Statistical Testing은 data scientist의 창의성이 요구되는 작업으로 Testing Model을 기획하여 statistical analysis design을 구체화 및 문서화한다.\nBT (Biotechnology) 부문과 IT (Information technology) 부문의 협력이 전제가 되어야하며 BT 부서의 experiment design 및 limitation factors at a experimental level을 고려한 engineering design과 statistical design을 확립한다.\n기획된 Testing model에 적합한 statistical model을 찾고 minimum reuirement sample size를 계산한다.\n위의 전략대로, BT 부서에서의 실험과 IT 부서 (Data Science 팀)에서 분석을 수행한다.\nalgorithm 구현 및 운영에 있어서 code의 변화가 있을 경우와 새로운 제품에 대한 새로운 validation report를 제출해야하는 의무사항에 대비한 문서 자동화 시스템을 구축한다."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#issues-및-solutions",
    "href": "docs/projects/dsp_validation/index.html#issues-및-solutions",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "BT에서 생성된 data를 입력할 수 있는 시스템 부재\nBT부서의 업무기술서 부재로 인한 소통의 어려움\n입력 데이터를 전처리하는 시스템 부재\nData Science 팀내 업무 기술서 부재로 인한 팀내 소통의 어려움\nValidation report에 대한 선례 및 template를 찾을 수 없을 정도로 매우 드물다.\n\n\n\n\n\nBT에서 생성된 data를 입력할 수 있는 시스템 구축\n\ndigitalization: experimental design file, raw data generated from medical device, data extracted from medical device\n\nBT부서와의 소통으로 업무 문서화를 진행하여 실험 결과의 기대 정답 기준 확립, 독립 변수 및 종속 변수 확립\n입력 데이터를 전처리하여 diagnostic algorithm의 결과물을 병합하는 engineering 시스템 구축\nData Quality Control Process 강화\n\n1단계 오타 교정\n2단계 결측치 처리\n3단계 anomaly data 처리\n4단계 algorithm data 정합성 1차 검정: FDA validtion을 위한 전처리된 algorithm vs Original algorithm\n5단계 algorithm data 정합성 2차 검정: Data Science팀의 FDA validtion을 위한 전처리된 algorithm vs BT 부서에 published algorithm\n\nData Science 팀내 업무 기술서 작성으로 코드 중앙화, 데이터 중앙화, 특이사항 문서화 실현\nSeegene 고유의 software testing & advanced testing model 기획 및 확립 후 statistical analysis 기획 및 수행"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#required-skills",
    "href": "docs/projects/dsp_validation/index.html#required-skills",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "FDA software validation knowledge\nStatistics\nDynamic documentation\nBiology\nClinical study design"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#colaborators",
    "href": "docs/projects/dsp_validation/index.html#colaborators",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "5 data scientists (I am a project owner.)\n3 data enineers\n27 biologists\n2 patent attorneys"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#acheivements",
    "href": "docs/projects/dsp_validation/index.html#acheivements",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "DSP Algorithm Output\nDescription\n\n\n\n\nFDA Validation 1st Draft\nFDA 제출용 verification & validation report 1차 초안\n\n\nData Input System\n임시적인 데이터 입력 시스템으로 대량의 data를 대량으로 연산하는 플랫폼 구축으로 발전\n\n\nDocumentation System\n기존에 부재했던 문서화 및 문서 자동화 시스템 구축 \\(\\rightarrow\\) 업무 소통과 Relational Database System 구축에 필요\n\n\nData Management System\ndata quality control system\n\n\nFDA Validation Model\nDSP algorithm을 위한 Validation Model 확립\n\n\nPatent Invention\nFDA Validation Model 발명\n\n\nIn-house first Performance evaluation of algorithms and reagent products\n사내에 기존에 존재하지 않았던 알고리즘 및 시약 제품의 종합 성능 평가\n\n\nStatistical analysis related to algorithmic risk management\n시약과 장비 고유의 random effect와 다른 교란자로 인해 발생할 수 있는 noise 및 anomaly data에 위험 관리 관련 통계 분석을 수행"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#long-term-project",
    "href": "docs/projects/dsp_validation/index.html#long-term-project",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "BT의 업무 기술서를 협업으로 공동작성하고 RDB system 구축\n시약, 장비, software 및 algorithm Validation용 DevOps Platform 구축"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#background-1",
    "href": "docs/projects/dsp_validation/index.html#background-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "Due to the nature of reagent products related to medical device, there are regulations required by each country’s government for the health and life safety of its citizens when entering the global market.\n\nReagent stability verification and validation required\nEquipment stability verification and validation request\nSoftware stability verification and validation request\nStability verification and validation Request of Diagnostic Algorithm\n\nIn order to enter and manage the global market after the COVID19 special period is lifted, product safety verification and regulatory requirements required by each country’s government must be met.\nIn the case of the EU (European Union), IVDR (In Vitro Diagnostics Regulation) is required\nIn order to enter the North American market, it is necessary to plan and write a document verifying the stability of the diagnostic algorithm based on the surveilance standards of the US FDA and Canada’s Health Canada, which require the world’s most stringent standards.\nAs time goes by, regulations on software and algorithms are being strengthened in each country, so advanced testing that is more stringent than the existing safety verification method by software engineering is required.\nTherefore, the stability verification and validation of the diagnostic algorithm includes software engineering testing and advanced testing. Here, advanced testing means statistical testing based on statistical analysis, and building a stable software engineering system is the prerequisite.\nSince the stability verification method of Seegene’s Diagnostic Signal Process (DSP) Algorithm is directly related to the business performance of the company, it is classified as a first-class security matter within the company, so specific and detailed planning and implementation details cannot be shared."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#objective-1",
    "href": "docs/projects/dsp_validation/index.html#objective-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "Design a system to statistically prove that the algorithm shows safe performance.\nEstablish a Statistical Validation System to prove that the algorithm shows safe performance through statistical analysis.\n\nHere, Establishment is defined as Definition, Documentation, and Implementation.\n\nDefine the risk of the algorithm in detail and quantitatively analyze the effect of the risk on the algorithm.\nIt is proved through statistical simulation that the algorithm is capable of risk management.\nIn the case of code changes according to algorithm implementations and operations, a new validation report must be submitted, so an automation system is built."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#methodology-1",
    "href": "docs/projects/dsp_validation/index.html#methodology-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "Refer to the guidance of SGS, a company that issues and provides training for the world’s most stringent inspection certificates.\nSGS provides guidance to the FDA as a target.\nAfter thoroughly reading the General Principles of Software Validation document provided by the FDA for software safety verification, establish a validation system based on this document.\n\nCopy: General Principles of Software Validation\nSummary: General Principles of Software Validation\nDiagram: General Principles of Software Validation\n\nSoftware engineering is performed based on the General Principles of Software Validation document.\nThe stability verification of Diagnostic Algorithm includes both Structural Testing and Advanced Testing. Structural Testing means code-based Software Engineering Testing and Advanced Testing means Statistical Testing based on Statistical Analysis. Advanced Testing is based on the establishment of a stable Software Engineering System.\nEstablish a definition and logic for algorithm safety.\nEstablish metrics or indicators for algorithm safety.\nStatistical Testing, which is Advanced Testing, is a task that requires the creativity of a data scientist, and a testing model is planned to materialize and document statistical analysis design.\nCooperation between the BT (Biotechnology) sector and the IT (Information technology) sector must be a premise, and engineering design and statistical design should be established considering the BT department’s experimental design and limitation factors at a experimental level.\nFind a statistical model suitable for the planned testing model and calculate the minimum reuirement sample size.\nAs per the above strategy, the BT department conducts experiments and the IT department (Data Science team) conducts analysis.\nEstablish a document automation system in case of code changes in algorithm implementation and operation and the obligation to submit a new validation report for new products."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#issues-solutions",
    "href": "docs/projects/dsp_validation/index.html#issues-solutions",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "Absence of a system that can input data generated by BT departments\nDifficulties in communication due to lack of job description in BT departments.\nAbsence of a system that preprocesses input data.\nDifficulties in communication within the team due to lack of job description within the Data Science team.\nIt is so rare that no precedent or template for validation report can be found.\n\n\n\n\n\nBuilding a system that can input data generated by BT departments\n\ndigitalization: experimental design file, raw data generated from medical device, data extracted from medical device\n\nWork documented through communication with the BT department to establish the standard for the expected correct answer of the experiment results, and to establish independent and dependent variables\nBuilding an engineering system that preprocesses input data and merges the results of diagnostic algorithms\nStrengthen Data Quality Control Process\n\nStep 1 typo correction\nStep 2 missing value processing\nStep 3 anomaly data processing\nStep 4 algorithm data conformity 1st Test: Preprocessed algorithm for FDA validation vs Original algorithm\nStep 5 algorithm data conformity 2nd test: Data Science team’s preprocessed algorithm for FDA validation vs algorithm published by BT department\n\nRealization of code centralization, data centralization, and documentation of specific matters by writing job descriptions within the Data Science team\nPlan and conduct statistical analysis after planning and establishing Seegene’s own software testing & advanced testing model"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#required-skills-1",
    "href": "docs/projects/dsp_validation/index.html#required-skills-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "FDA software validation knowledge\nStatistics\nDynamic documentation\nBiology\nClinical study design"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#colaborators-1",
    "href": "docs/projects/dsp_validation/index.html#colaborators-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "5 data scientists (I am a project owner.)\n3 data enineers\n27 biologists\n2 patent attorneys"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#acheivements-1",
    "href": "docs/projects/dsp_validation/index.html#acheivements-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "DSP Algorithm Output\nDescription\n\n\n\n\nFDA Validation 1st Draft\nthe 1st draft of verification & validation report for FDA submission\n\n\nData Input System\nIt is a temporary data input system that develops into a platform that calculates a large amount of data in large quantities.\n\n\nDocumentation System\nEstablishment of previously absent documentation and document automation systems \\(\\rightarrow\\) Necessary for business communication and establishment of Relational Database System\n\n\nData Management System\ndata quality control system\n\n\nFDA Validation Model\nEstablishment of validation model for DSP algorithm\n\n\nPatent Invention\nInventing the FDA Validation Model\n\n\nIn-house first Performance evaluation of algorithms and reagent products\nComprehensive performance evaluation of algorithms and reagent products that did not previously exist in-house\n\n\nStatistical analysis related to algorithmic risk management\nRisk management-related statistical analysis is performed on noise and anomaly data that may occur due to reagent and equipment-specific random effects and other confounders."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#long-term-project-1",
    "href": "docs/projects/dsp_validation/index.html#long-term-project-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "Collaboratively write BT’s job description and establish RDB system\nBuilding a DevOps Platform for reagents, equipment, software and algorithm validation"
  },
  {
    "objectID": "docs/projects/index.html",
    "href": "docs/projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\nPLAN, MATH, SP, STAT, ML, and DL stands for 'Intellectual Property Planning', 'Mathematics', 'Signal Processing', 'Statistics', 'Machine Learning', and \"Deep Learning\", respectively.\n\n\n\n[STAT][Surveilance] Diagnostic Algorithm Validation for FDA (Current Project)\n[STAT][ML] LLFS (Long Life Family Study)\n[STAT][ML] Diagnostic Device QC Platform\n[PLAN] Platform IP Planning (To be written)\n[ML] Data-Driven Diagnostic Algorithm (To be written)\n[STAT][SP] Clinical Data Analysis for QC (To be written)\n[ML][MATH][Biology] Heavy Metal Removal Algorihtm using Tea Leaves\n[ML] Diffusion Model of Social Networks using Genetic Algorithm (To be written)\n[Biochemistry] Effects of Phellinus Linteus toward Formation of Lymphatic Vessel\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html",
    "href": "docs/projects/LLFS/eda.html",
    "title": "EDA",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n(한글 준비중)\nplease, read the English section first.\n\n\nCode\n## Function List\n\ncolor_function&lt;-function(category_number){\nreturn(\n    if(category_number==2){\n        c(\"darkblue\",\"darkred\")\n    }else if(category_number==3){\n        c(\"darkblue\",\"darkred\",\"yellow4\")\n    }else if(category_number==4){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\")\n    }else if(category_number==5){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\")\n    }else{\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\",\"darkgreen\")\n    }\n    )\n}\n\nscale_function=function(vector=x,min=NULL,max=NULL,method){\n    scaling_methods&lt;-c('min_max normalization','customized normalization','standardization')\n\n    if(method==\"min-max\"){\n        output=(vector-min(vector))/(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        output=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else if(method==\"standarized\"){\n        output=(vector-mean(vector))/sd(vector)\n    }else{\n        output=paste0(\"Error!, no such a scaling method in this module. \\n Please, put the first word of each method you want to use in the 'method' argument among the following tests: \", paste(scaling_methods,collapse=\", \"))\n    }\n  return(output)\n}\n\nmultiple_shapiro_test&lt;-function(in_data){\n        normality_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                            function(x)shapiro.test(x))\n        temp&lt;-data.frame(matrix(nrow=length(normality_test),ncol=4))\n        for (i in 1:length(normality_test)){\n            temp[i,]&lt;-c(\n                coloumn_name=names(normality_test)[i],\n                statistic=normality_test[[i]]$statistic,\n                p_value=normality_test[[i]]$p.value,\n                method=normality_test[[i]]$method)\n        }\n        names(temp)&lt;-c('column_name','statistic','p_value','method')\n        output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'not_normal','normal'))%&gt;%\n            dplyr::select('column_name','statistic','p_value','p_adjusted','type','method')\n        return(output)\n}    \n\nmultiple_levene_test&lt;-function(in_data,categorical_variable){\n        homoscedasticity_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                                    function(x)leveneTest(x~in_data[,categorical_variable]))\n        temp&lt;-data.frame(matrix(nrow=length(homoscedasticity_test),ncol=6))\n            for (i in 1:length(homoscedasticity_test)){\n                temp[i,]&lt;-c(\n                    coloumn_name=names(homoscedasticity_test)[i],\n                    group_df=homoscedasticity_test[[i]]$Df[1],\n                    residual_df=homoscedasticity_test[[i]]$Df[2],\n                    statistic=homoscedasticity_test[[i]]$`F value`[1],\n                    p_value=homoscedasticity_test[[i]]$`Pr(&gt;F)`[1],\n                    method=\"levene's test\")\n            }\n            names(temp)&lt;-c('column_name','group_df','residual_df','statistic','p_value','method')\n            output&lt;-temp%&gt;%\n                mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n                type=ifelse(p_adjusted&lt;0.05,'heteroscedasticity','homoscedasticity'))%&gt;%\n                dplyr::select('column_name','group_df','residual_df','statistic','p_value','p_adjusted','type','method')\n        return(output)} \n\nmultiple_unpaired_t_test&lt;-function(in_data,categorical_variable,homo_variables,hetero_variables){\n    homo_unpaired_t_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))][,homo_variables],2,\n                                    function(x)t.test(x~in_data[,categorical_variable],var.equal=TRUE))\n    hetero_unpaired_t_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))][,hetero_variables],2,\n                                    function(x)t.test(x~in_data[,categorical_variable],var.equal=FALSE)) \n    unpaired_t_test&lt;-c(homo_unpaired_t_test,hetero_unpaired_t_test)\n\n    temp&lt;-data.frame(matrix(nrow=length(unpaired_t_test),ncol=7))\n        for (i in 1:length(unpaired_t_test)){\n            temp[i,]&lt;-c(names(unpaired_t_test)[i], \n                        unpaired_t_test[[i]]$estimate,\n                        unpaired_t_test[[i]]$parameter,\n                        unpaired_t_test[[i]]$statistic,\n                        unpaired_t_test[[i]]$p.value,\n                        unpaired_t_test[[i]]$method)\n        }\n        names(temp)&lt;-c('column_name',names(unpaired_t_test[[1]]$estimate),'df','statistic','p_value','method')\n        output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'significant','insignificant'))%&gt;%\n            dplyr::select('column_name',names(unpaired_t_test[[1]]$estimate),'df','statistic','p_value','p_adjusted','type','method')\n    return(output)} \n\n\nmultiple_correlation_test&lt;-function(in_data,in_numeric_variable){\n    correlation_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                                    function(x)cor.test(x,in_data[,in_numeric_variable],method='pearson'))\n    temp&lt;-data.frame(matrix(nrow=length(correlation_test),ncol=6))\n        for (i in 1:length(correlation_test)){\n            temp[i,]&lt;-c(names(correlation_test)[i], \n                        correlation_test[[i]]$estimate,\n                        correlation_test[[i]]$parameter,\n                        correlation_test[[i]]$statistic,\n                        correlation_test[[i]]$p.value,\n                        correlation_test[[i]]$method)\n        }\n        names(temp)&lt;-c('column_name',names(correlation_test[[1]]$estimate),'df','statistic','p_value','method')\n        output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'significant','insignificant'))%&gt;%\n            dplyr::select('column_name',names(correlation_test[[1]]$estimate),'df','statistic','p_value','p_adjusted','type','method')\n    return(output)} \n\nmultiple_anova_test&lt;-function(in_data, in_categorical_variable){\n    aov_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                function(x)aov(x~get(in_categorical_variable),data=in_data)%&gt;%summary)\n\n    temp&lt;-data.frame(matrix(nrow=length(aov_test),ncol=10))\n    for (i in 1:length(aov_test)){\n        temp[i,]&lt;-c(names(aov_test)[i], \n                    aov_test[[i]][[1]]$`Df`[1],\n                    aov_test[[i]][[1]]$`Df`[2],\n                    aov_test[[i]][[1]]$`Sum Sq`[1],\n                    aov_test[[i]][[1]]$`Sum Sq`[2],\n                    aov_test[[i]][[1]]$`Mean Sq`[1],\n                    aov_test[[i]][[1]]$`Mean Sq`[2],\n                    aov_test[[i]][[1]]$`F value`[1],\n                    aov_test[[i]][[1]]$`Pr(&gt;F)`[1],\n                    'one_way_anova')\n    }\n    names(temp)&lt;-c('column_name','group_df','residual_df','group_ssq','residual_ssq',\n                    'group_msq','residual_msq','F_value','p_value','method')\n    output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'significant','insignificant'))%&gt;%\n            dplyr::select('column_name','group_df','residual_df','group_ssq','residual_ssq',\n                    'group_msq','residual_msq','F_value','p_value','method',\n                    'p_adjusted','type','method')\n    return(output)} \n\nmain_statistical_test&lt;-function(\n    in_data,method,categorical_variable,in_numeric_variable,\n    homo_variables=NULL,hetero_variables=NULL,\n    fun1=multiple_shapiro_test,\n    fun2=multiple_levene_test,\n    fun3=multiple_unpaired_t_test){\n    test_list&lt;-c(\"shapiro wilks test\",\"levene's test\",\"student t test\",\"anova\",\"correlation test\")#,\"ANCOVA\",\"MANOVA\",\"wilcoxon manwhitney\",\"kruskal wallis test\",\"fisher exact test\",\"anderson darling\")\n    error_massage&lt;-paste0(\"Error!, no such a test in this module. \\n Please, put the first word of each method you want to use in the 'method' argument among the following tests: \", paste(test_list,collapse=\", \"))\n    if(grepl('shapiro',method)){\n        output=multiple_shapiro_test(in_data)\n    }else if(grepl('levene',method)){\n        output=multiple_levene_test(in_data,categorical_variable)\n        # var.test()\n    }else if(grepl('student',method)){\n        # code unpaired vs paired t test in the future\n        output=multiple_unpaired_t_test(in_data,categorical_variable,homo_variables,hetero_variables)\n    }else if(grepl('kruskal',method)){\n        return(error_massage)\n    }else if(grepl('wilcoxon|manwhitney',method)){\n        return(error_massage)\n    }else if(grepl('anova|aov',method)){\n        output=multiple_anova_test(in_data,categorical_variable)\n    }else if(grepl('cor',method)){\n        output=multiple_correlation_test(in_data,in_numeric_variable)\n    }else{\n        return(error_massage)\n    }\n    return(output)\n}\n\n\ngetNumericSummaryTable=function(in_data,group_variable,summary_variable,set_color=color_function,...){\n    # table\n    temp&lt;-in_data %&gt;% \n    #group_by_at(vars(...)) %&gt;% \n    group_by_at(vars(group_variable)) %&gt;% \n    mutate(count=n())%&gt;%\n    summarise_at(vars(summary_variable,count),\n                 list(mean=mean,\n                 sd=sd,\n                 min=min,\n                 Q1=~quantile(., probs = 0.25),\n                 median=median, \n                 Q3=~quantile(., probs = 0.75),\n                 max=max))%&gt;%\n                 as.data.frame()%&gt;%\n                 rename(\n                 n=count_mean)%&gt;%\n                 dplyr::select(-contains('count'))%&gt;%\n                 as.data.frame()\n    names(temp)&lt;-c(\"group\",\n    sapply(names(temp)[-1],function(x)str_replace(x,paste0(summary_variable,\"_\"),\"\")))\n    output&lt;-temp%&gt;%\n    mutate(\n        variable=group_variable,\n        summary=summary_variable,\n        mean=mean%&gt;%round(2),\n        sd=sd%&gt;%round(2),\n        min=min%&gt;%round(2),\n        Q1=Q1%&gt;%round(2),\n        Q4=Q3%&gt;%round(2),\n        max=max%&gt;%round(2),\n        IQR_min=Q1-(Q3-Q1)*1.5%&gt;%round(2),\n    IQR_max=Q3+(Q3-Q1)*1.5%&gt;%round(2),\n    proportion=paste0(round(n/nrow(all_data)*100,2),\"%\"))%&gt;%\n    dplyr::select(variable,group,summary,n,proportion,mean,sd,min,IQR_min,Q1,median,Q3,IQR_max,max)\n    return(output)\n}\n\ngetNumericSummaryPlot=function(\n    in_data=all_data,group_variable,summary_variable,\n    set_color=color_function,\n    summary_function=getNumericSummaryTable,...){\n    # plot\n    temp=getNumericSummaryTable(in_data,group_variable,summary_variable)\n    temp2=temp\n    names(temp2)[2]=group_variable\n    plot&lt;-\n    in_data%&gt;%\n    dplyr::select(group_variable,summary_variable)%&gt;%\n    inner_join(.,temp2,by=group_variable)%&gt;%\n    ggplot(aes(x=age,fill=get(group_variable),color=get(group_variable)))+\n    geom_histogram(aes(y=..density..),binwidth=1,alpha=0.5, position=\"identity\")+\n    geom_vline(aes(xintercept=mean,color=get(group_variable)), linetype=\"dashed\", size=1.5) + \n    geom_density(aes(y=..density..),alpha=0.3) +\n    scale_color_manual(values=set_color(nrow(temp2)))+\n    scale_fill_manual(values=set_color(nrow(temp2)))+\n    theme_bw()+\n    theme(legend.position = c(.95, .95),\n    legend.justification = c(\"right\", \"top\"),\n    legend.margin = margin(6, 6, 6, 6),\n    legend.text = element_text(size = 10))+\n    guides(fill=guide_legend(title=group_variable),\n    color=FALSE)+\n    geom_text(aes(label=round(mean,1),y=0,x=mean),\n                vjust=-1,col='yellow',size=5)+\n    ggtitle(paste0(\"Histogram & Density, \", summary_variable, \" Grouped by \", group_variable))+\n        labs(x=summary_variable, y = \"Density\")\n\n    result&lt;-plot\n    return(result)\n}\n\n\n\n\nCode\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# raw data\nnormality_test_result&lt;-multiple_shapiro_test(all_data)%&gt;%\n    filter(column_name!='id')%&gt;%\n    group_by(type)%&gt;%\n    summarise(count=n())%&gt;%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%&gt;%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%&gt;%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n1001\n1001\n1\n\n\n\n\n\nOut of 1001 numeric variables, the variables following a normal distribution are 1001 (100%) and the ones that do not are NA (NA%).\n\n\n\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables&lt;-\n    multiple_shapiro_test(all_data)%&gt;%\n        filter(p_value&gt;0.05,column_name!='id')%&gt;%\n            dplyr::select(column_name)%&gt;%\n            pull%&gt;%sample(16)\n\nnormal_data&lt;-\n    all_data%&gt;%\n        dplyr::select(outcome,normal_variables)%&gt;%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%&gt;%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',linewidth=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution Grouped by Disease Status\")\n\n\n\n\n\n\n\n\n\n\n\n\nThere is no variable that do not follow a normal distribution.\n\n\n\n\n\n\nThrough the exploratory data analysis above, it was confirmed that all variables follow a normal distribution, and t tests were conducted to select metabolites that have significant relationships with the disease status, AD. To minimize type 1 error due to multiple testings, bonferroni correction was used in the EDA\n\nHomoscedasticity Test\n\nLeven’s test is performed to confirm that each variable has equality of variance, one of the assumptions of the t test and ANOVA.\n\n\nCode\nleven_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")%&gt;%\n    filter(column_name!='id')\n\nhomo_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&gt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\nhetero_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&lt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\n\n\n10 variables randomly selected out of 1001 variables with equal variance: meta408, meta959, meta796, meta957, meta740, meta212, meta171, meta769, meta986, meta178\n0 variables randomly selected with equal variance:\n\n\n\nCode\nhomo_variable_sample&lt;-homo_variable%&gt;%sample(10)\nhetero_variable_sample&lt;-hetero_variable\n\nstratified_levene_data&lt;-all_data%&gt;%\n    dplyr::select(outcome,homo_variable_sample,hetero_variable_sample)%&gt;%\n    gather(key=metabolite,value=value,c(homo_variable_sample,hetero_variable_sample))%&gt;%\n    mutate(levene_test=ifelse(metabolite%in%(homo_variable_sample),\"homoscedasticity\",\"heteroscedasticity\"))\n\nstratified_levene_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(stratified_levene_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    facet_wrap(.~levene_test)+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity vs Homoscedasticity\")\n\n\n\n\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\n\nCode\nt_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"student\",\n                        categorical_variable=\"outcome\",\n                        homo_variables=homo_variable,\n                        hetero_variables=hetero_variable)\n\nmetabolites_associated_AD&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nmetabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%&gt;%\n    group_by(outcome)%&gt;%\n    summarise(mean=mean(value),sd=sd(value))%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nmean\nsd\n\n\n\n\nnegative\n0.2184112\n0.9869302\n\n\npositive\n-0.4186542\n0.9830175\n\n\n\n\n\nCode\ntop_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%head(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\ntop_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,top_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\nbottom_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%tail(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nbottom_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,bottom_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,bottom_metabolites_associated_AD)        \n\na1&lt;-top_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(top_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Strongly Associated with AD\")\n\na2&lt;-bottom_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(bottom_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Weakly Associated with AD\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\n\nCode\nsignificant_metabolites&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\n\nAs a result of the t tests, there are 201 metabolites that are significantly associated with AD status at the 5% significance level. Metabolites with the highest significance were designated as strong metabolites and metabolites with the lowest significance among metabolites significantly related to AD status were designated as weak metabolites, and the expression level of metabolites between the disease status was confirmed through visualization. As a result, a greater difference was observed in strong metabolites than in weak metabolites, but both groups were not clearly separated in terms of AD status.\n\n\n\n\n\nCode\nad_sex_summary&lt;-all_data%&gt;%\n    group_by(outcome,sex)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, sex,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$sex)$p.value,\n    method=\"chisquare_test\")\n\nad_sex_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\nsex\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\nman\n150\n30%\n85.79333\n5.146572\n0.1049983\nchisquare_test\n\n\nnegative\nwoman\n160\n32%\n84.96875\n5.284868\n0.1049983\nchisquare_test\n\n\npositive\nman\n77\n15.4%\n79.85714\n5.167495\n0.1049983\nchisquare_test\n\n\npositive\nwoman\n113\n22.6%\n81.29204\n4.896576\n0.1049983\nchisquare_test\n\n\n\n\n\n\n\n\n\n\nCode\nad_genotype_summary&lt;-all_data%&gt;%\n    group_by(outcome,genotype)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, genotype,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\")\n\ngenotype_ad_summary&lt;-all_data%&gt;%\n    group_by(genotype,outcome)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(genotype,outcome,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\") \n\nad_genotype_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\ngenotype\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\ne3\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\nnegative\ne2\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\nnegative\ne4\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\npositive\ne3\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\npositive\ne2\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\npositive\ne4\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nCode\ngenotype_ad_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenotype\noutcome\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\ne3\nnegative\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\ne3\npositive\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\ne2\nnegative\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\ne2\npositive\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\ne4\nnegative\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\ne4\npositive\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nAs you see the tables above, the count of AD status and that of genotype status are proportionately the same. Thus, there is no relation between AD and genotype in this data at the significant level 5%.\n\n\n\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually. For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nad_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\n\n\nCode\nage_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n310\n62%\n85.37\n5.23\n72\n69.625\n81.25\n85\n89\n100.625\n105\n\n\noutcome\npositive\nage\n190\n38%\n80.71\n5.04\n65\n66.500\n77.00\n81\n84\n94.500\n93\n\n\nsex\nman\nage\n227\n45.4%\n83.78\n5.86\n65\n68.000\n80.00\n84\n88\n100.000\n105\n\n\nsex\nwoman\nage\n273\n54.6%\n83.45\n5.43\n67\n69.500\n80.00\n83\n87\n97.500\n100\n\n\ngenotype\ne3\nage\n387\n77.4%\n83.61\n5.64\n65\n69.500\n80.00\n84\n87\n97.500\n102\n\n\ngenotype\ne2\nage\n40\n8%\n85.83\n5.62\n78\n70.875\n81.75\n86\n89\n99.875\n105\n\n\ngenotype\ne4\nage\n73\n14.6%\n82.30\n5.25\n67\n68.500\n79.00\n82\n86\n96.500\n94\n\n\n\n\n\n\n\nCode\nplot&lt;- ggarrange(\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\"),\n    ncol=2, nrow=2,legend=\"bottom\")\nplot\n\n\n\n\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -4.66, but their standard deviations are 5.04 and 5.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status 4.66 years younger than those with a positive one.\n\n\nCode\na1&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=age,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(legend.position=\"top\")+\n    labs(title=\"Histogram, Age Distribution Grouped by Disease Status\")   \na2&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=outcome,y=age,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Age Distribution Grouped by Disease Status\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nage_correlation_data&lt;-\n    main_statistical_test(in_data=all_data,in_numeric_variable = 'age',method = \"cor\")%&gt;%\n    filter(p_adjusted&lt;0.05,column_name!='id')\n\n\n151 metabolites are significantly associated with age.\n\n\n\n\n\nCode\ngenotype_aov&lt;-main_statistical_test(in_data=all_data,categorical_variable=\"genotype\",method='aov')\ngenotype_aov%&gt;%\nfilter(column_name!='id',p_adjusted&lt;0.05)%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ngroup_df\nresidual_df\ngroup_ssq\nresidual_ssq\ngroup_msq\nresidual_msq\nF_value\np_value\nmethod\np_adjusted\ntype\n\n\n\n\n\n\n\n0 metabolites are significantly associated with the genotype variable.\n\n\n\n\n\n\n\n\nCode\nall_data%&gt;%\n    group_by(outcome,sex,genotype)%&gt;%\n    summarise(count=n(),\n    mean_age=mean(age),\n    sd_age=sd(age))%&gt;%ungroup%&gt;%\n    mutate(sum_count=sum(count),\n    proportion=paste0(round(count/sum_count*100,3),'%'))%&gt;%\n    dplyr::select(outcome,sex,genotype,count,proportion, mean_age,sd_age)%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\ngenotype\ncount\nproportion\nmean_age\nsd_age\n\n\n\n\nnegative\nman\ne3\n113\n22.6%\n85.56637\n5.091793\n\n\nnegative\nman\ne2\n14\n2.8%\n88.57143\n6.548215\n\n\nnegative\nman\ne4\n23\n4.6%\n85.21739\n4.067125\n\n\nnegative\nwoman\ne3\n129\n25.8%\n85.07752\n5.370074\n\n\nnegative\nwoman\ne2\n12\n2.4%\n85.75000\n5.446016\n\n\nnegative\nwoman\ne4\n19\n3.8%\n83.73684\n4.628920\n\n\npositive\nman\ne3\n66\n13.2%\n79.74242\n5.384624\n\n\npositive\nman\ne2\n5\n1%\n83.00000\n3.000000\n\n\npositive\nman\ne4\n6\n1.2%\n78.50000\n3.082207\n\n\npositive\nwoman\ne3\n79\n15.8%\n81.65823\n4.784821\n\n\npositive\nwoman\ne2\n9\n1.8%\n83.22222\n3.632416\n\n\npositive\nwoman\ne4\n25\n5%\n79.44000\n5.260545\n\n\n\n\n\nExcept for the disease status variable, it looks like there is no difference of age in average and count in the other categorical variables. Taking into account age varialble is significantly associated with the metabolites that are significantly associated with the disease status, outcome (or AD). There is no need to conduct futher exploratory data analysis in the multivariable analysis senction.\n\n\n\n\n\n\nplease, read the English section first.\n\n\nCode\n## Function List\n\ncolor_function&lt;-function(category_number){\nreturn(\n    if(category_number==2){\n        c(\"darkblue\",\"darkred\")\n    }else if(category_number==3){\n        c(\"darkblue\",\"darkred\",\"yellow4\")\n    }else if(category_number==4){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\")\n    }else if(category_number==5){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\")\n    }else{\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\",\"darkgreen\")\n    }\n    )\n}\n\nscale_function=function(vector=x,min=NULL,max=NULL,method){\n    scaling_methods&lt;-c('min_max normalization','customized normalization','standardization')\n\n    if(method==\"min-max\"){\n        output=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        output=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else if(method==\"standarized\"){\n        output=(vector-mean(vector))/sd(vector)\n    }else{\n        output=paste0(\"Error!, no such a scaling method in this module. \\n Please, put the first word of each method you want to use in the 'method' argument among the following tests: \", paste(scaling_methods,collapse=\", \"))\n    }\n  return(output)\n}\n\nmultiple_shapiro_test&lt;-function(in_data){\n        normality_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                            function(x)shapiro.test(x))\n        temp&lt;-data.frame(matrix(nrow=length(normality_test),ncol=4))\n        for (i in 1:length(normality_test)){\n            temp[i,]&lt;-c(\n                coloumn_name=names(normality_test)[i],\n                statistic=normality_test[[i]]$statistic,\n                p_value=normality_test[[i]]$p.value,\n                method=normality_test[[i]]$method)\n        }\n        names(temp)&lt;-c('column_name','statistic','p_value','method')\n        output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'not_normal','normal'))%&gt;%\n            dplyr::select('column_name','statistic','p_value','p_adjusted','type','method')\n        return(output)\n}    \n\nmultiple_levene_test&lt;-function(in_data,categorical_variable){\n        homoscedasticity_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                                    function(x)leveneTest(x~in_data[,categorical_variable]))\n        temp&lt;-data.frame(matrix(nrow=length(homoscedasticity_test),ncol=6))\n            for (i in 1:length(homoscedasticity_test)){\n                temp[i,]&lt;-c(\n                    coloumn_name=names(homoscedasticity_test)[i],\n                    group_df=homoscedasticity_test[[i]]$Df[1],\n                    residual_df=homoscedasticity_test[[i]]$Df[2],\n                    statistic=homoscedasticity_test[[i]]$`F value`[1],\n                    p_value=homoscedasticity_test[[i]]$`Pr(&gt;F)`[1],\n                    method=\"levene's test\")\n            }\n            names(temp)&lt;-c('column_name','group_df','residual_df','statistic','p_value','method')\n            output&lt;-temp%&gt;%\n                mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n                type=ifelse(p_adjusted&lt;0.05,'heteroscedasticity','homoscedasticity'))%&gt;%\n                dplyr::select('column_name','group_df','residual_df','statistic','p_value','p_adjusted','type','method')\n        return(output)} \n\nmultiple_unpaired_t_test&lt;-function(in_data,categorical_variable,homo_variables,hetero_variables){\n    homo_unpaired_t_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))][,homo_variables],2,\n                                    function(x)t.test(x~in_data[,categorical_variable],var.equal=TRUE))\n    hetero_unpaired_t_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))][,hetero_variables],2,\n                                    function(x)t.test(x~in_data[,categorical_variable],var.equal=FALSE)) \n    unpaired_t_test&lt;-c(homo_unpaired_t_test,hetero_unpaired_t_test)\n\n    temp&lt;-data.frame(matrix(nrow=length(unpaired_t_test),ncol=7))\n        for (i in 1:length(unpaired_t_test)){\n            temp[i,]&lt;-c(names(unpaired_t_test)[i], \n                        unpaired_t_test[[i]]$estimate,\n                        unpaired_t_test[[i]]$parameter,\n                        unpaired_t_test[[i]]$statistic,\n                        unpaired_t_test[[i]]$p.value,\n                        unpaired_t_test[[i]]$method)\n        }\n        names(temp)&lt;-c('column_name',names(unpaired_t_test[[1]]$estimate),'df','statistic','p_value','method')\n        output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'significant','insignificant'))%&gt;%\n            dplyr::select('column_name',names(unpaired_t_test[[1]]$estimate),'df','statistic','p_value','p_adjusted','type','method')\n    return(output)} \n\n\nmultiple_correlation_test&lt;-function(in_data,in_numeric_variable){\n    correlation_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                                    function(x)cor.test(x,in_data[,in_numeric_variable],method='pearson'))\n    temp&lt;-data.frame(matrix(nrow=length(correlation_test),ncol=6))\n        for (i in 1:length(correlation_test)){\n            temp[i,]&lt;-c(names(correlation_test)[i], \n                        correlation_test[[i]]$estimate,\n                        correlation_test[[i]]$parameter,\n                        correlation_test[[i]]$statistic,\n                        correlation_test[[i]]$p.value,\n                        correlation_test[[i]]$method)\n        }\n        names(temp)&lt;-c('column_name',names(correlation_test[[1]]$estimate),'df','statistic','p_value','method')\n        output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'significant','insignificant'))%&gt;%\n            dplyr::select('column_name',names(correlation_test[[1]]$estimate),'df','statistic','p_value','p_adjusted','type','method')\n    return(output)} \n\nmultiple_anova_test&lt;-function(in_data, in_categorical_variable){\n    aov_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                function(x)aov(x~get(in_categorical_variable),data=in_data)%&gt;%summary)\n\n    temp&lt;-data.frame(matrix(nrow=length(aov_test),ncol=10))\n    for (i in 1:length(aov_test)){\n        temp[i,]&lt;-c(names(aov_test)[i], \n                    aov_test[[i]][[1]]$`Df`[1],\n                    aov_test[[i]][[1]]$`Df`[2],\n                    aov_test[[i]][[1]]$`Sum Sq`[1],\n                    aov_test[[i]][[1]]$`Sum Sq`[2],\n                    aov_test[[i]][[1]]$`Mean Sq`[1],\n                    aov_test[[i]][[1]]$`Mean Sq`[2],\n                    aov_test[[i]][[1]]$`F value`[1],\n                    aov_test[[i]][[1]]$`Pr(&gt;F)`[1],\n                    'one_way_anova')\n    }\n    names(temp)&lt;-c('column_name','group_df','residual_df','group_ssq','residual_ssq',\n                    'group_msq','residual_msq','F_value','p_value','method')\n    output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'significant','insignificant'))%&gt;%\n            dplyr::select('column_name','group_df','residual_df','group_ssq','residual_ssq',\n                    'group_msq','residual_msq','F_value','p_value','method',\n                    'p_adjusted','type','method')\n    return(output)} \n\nmain_statistical_test&lt;-function(\n    in_data,method,categorical_variable,in_numeric_variable,\n    homo_variables=NULL,hetero_variables=NULL,\n    fun1=multiple_shapiro_test,\n    fun2=multiple_levene_test,\n    fun3=multiple_unpaired_t_test){\n    test_list&lt;-c(\"shapiro wilks test\",\"levene's test\",\"student t test\",\"anova\",\"correlation test\")#,\"ANCOVA\",\"MANOVA\",\"wilcoxon manwhitney\",\"kruskal wallis test\",\"fisher exact test\",\"anderson darling\")\n    error_massage&lt;-paste0(\"Error!, no such a test in this module. \\n Please, put the first word of each method you want to use in the 'method' argument among the following tests: \", paste(test_list,collapse=\", \"))\n    if(grepl('shapiro',method)){\n        output=multiple_shapiro_test(in_data)\n    }else if(grepl('levene',method)){\n        output=multiple_levene_test(in_data,categorical_variable)\n        # var.test()\n    }else if(grepl('student',method)){\n        # code unpaired vs paired t test in the future\n        output=multiple_unpaired_t_test(in_data,categorical_variable,homo_variables,hetero_variables)\n    }else if(grepl('kruskal',method)){\n        return(error_massage)\n    }else if(grepl('wilcoxon|manwhitney',method)){\n        return(error_massage)\n    }else if(grepl('anova|aov',method)){\n        output=multiple_anova_test(in_data,categorical_variable)\n    }else if(grepl('cor',method)){\n        output=multiple_correlation_test(in_data,in_numeric_variable)\n    }else{\n        return(error_massage)\n    }\n    return(output)\n}\n\n\ngetNumericSummaryTable=function(in_data,group_variable,summary_variable,set_color=color_function,...){\n    # table\n    temp&lt;-in_data %&gt;% \n    #group_by_at(vars(...)) %&gt;% \n    group_by_at(vars(group_variable)) %&gt;% \n    mutate(count=n())%&gt;%\n    summarise_at(vars(summary_variable,count),\n                 list(mean=mean,\n                 sd=sd,\n                 min=min,\n                 Q1=~quantile(., probs = 0.25),\n                 median=median, \n                 Q3=~quantile(., probs = 0.75),\n                 max=max))%&gt;%\n                 as.data.frame()%&gt;%\n                 rename(\n                 n=count_mean)%&gt;%\n                 dplyr::select(-contains('count'))%&gt;%\n                 as.data.frame()\n    names(temp)&lt;-c(\"group\",\n    sapply(names(temp)[-1],function(x)str_replace(x,paste0(summary_variable,\"_\"),\"\")))\n    output&lt;-temp%&gt;%\n    mutate(\n        variable=group_variable,\n        summary=summary_variable,\n        mean=mean%&gt;%round(2),\n        sd=sd%&gt;%round(2),\n        min=min%&gt;%round(2),\n        Q1=Q1%&gt;%round(2),\n        Q4=Q3%&gt;%round(2),\n        max=max%&gt;%round(2),\n        IQR_min=Q1-(Q3-Q1)*1.5%&gt;%round(2),\n    IQR_max=Q3+(Q3-Q1)*1.5%&gt;%round(2),\n    proportion=paste0(round(n/nrow(all_data)*100,2),\"%\"))%&gt;%\n    dplyr::select(variable,group,summary,n,proportion,mean,sd,min,IQR_min,Q1,median,Q3,IQR_max,max)\n    return(output)\n}\n\ngetNumericSummaryPlot=function(\n    in_data=all_data,group_variable,summary_variable,\n    set_color=color_function,\n    summary_function=getNumericSummaryTable,...){\n    # plot\n    temp=getNumericSummaryTable(in_data,group_variable,summary_variable)\n    temp2=temp\n    names(temp2)[2]=group_variable\n    plot&lt;-\n    in_data%&gt;%\n    dplyr::select(group_variable,summary_variable)%&gt;%\n    inner_join(.,temp2,by=group_variable)%&gt;%\n    ggplot(aes(x=age,fill=get(group_variable),color=get(group_variable)))+\n    geom_histogram(aes(y=..density..),binwidth=1,alpha=0.5, position=\"identity\")+\n    geom_vline(aes(xintercept=mean,color=get(group_variable)), linetype=\"dashed\", size=1.5) + \n    geom_density(aes(y=..density..),alpha=0.3) +\n    scale_color_manual(values=set_color(nrow(temp2)))+\n    scale_fill_manual(values=set_color(nrow(temp2)))+\n    theme_bw()+\n    theme(legend.position = c(.95, .95),\n    legend.justification = c(\"right\", \"top\"),\n    legend.margin = margin(6, 6, 6, 6),\n    legend.text = element_text(size = 10))+\n    guides(fill=guide_legend(title=group_variable),\n    color=FALSE)+\n    geom_text(aes(label=round(mean,1),y=0,x=mean),\n                vjust=-1,col='yellow',size=5)+\n    ggtitle(paste0(\"Histogram & Density, \", summary_variable, \" Grouped by \", group_variable))+\n        labs(x=summary_variable, y = \"Density\")\n\n    result&lt;-plot\n    return(result)\n}\n\n\n\n\nCode\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# raw data\nnormality_test_result&lt;-multiple_shapiro_test(all_data)%&gt;%\n    filter(column_name!='id')%&gt;%\n    group_by(type)%&gt;%\n    summarise(count=n())%&gt;%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%&gt;%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%&gt;%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n1001\n1001\n1\n\n\n\n\n\nOut of 1001 numeric variables, the variables following a normal distribution are 1001 (100%) and the ones that do not are NA (NA%).\n\n\n\n\n\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables&lt;-\n    multiple_shapiro_test(all_data)%&gt;%\n        filter(p_value&gt;0.05,column_name!='id')%&gt;%\n            dplyr::select(column_name)%&gt;%\n            pull%&gt;%sample(16)\n\nnormal_data&lt;-\n    all_data%&gt;%\n        dplyr::select(outcome,normal_variables)%&gt;%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%&gt;%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',linewidth=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution Grouped by Disease Status\")\n\n\n\n\n\n\n\n\n\n\n\n\nThere is no variable that do not follow a normal distribution.\n\n\n\n\n\n\nThrough the exploratory data analysis above, it was confirmed that all variables follow a normal distribution, and t tests were conducted to select metabolites that have significant relationships with the disease status, AD. To minimize type 1 error due to multiple testings, bonferroni correction was used in the EDA\n\nHomoscedasticity Test\n\nLeven’s test is performed to confirm that each variable has equality of variance, one of the assumptions of the t test and ANOVA.\n\n\nCode\nleven_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")%&gt;%\n    filter(column_name!='id')\n\nhomo_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&gt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\nhetero_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&lt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\n\n\n10 variables randomly selected out of 1001 variables with equal variance: meta408, meta959, meta796, meta957, meta740, meta212, meta171, meta769, meta986, meta178\n0 variables randomly selected with equal variance:\n\n\n\nCode\nhomo_variable_sample&lt;-homo_variable%&gt;%sample(10)\nhetero_variable_sample&lt;-hetero_variable\n\nstratified_levene_data&lt;-all_data%&gt;%\n    dplyr::select(outcome,homo_variable_sample,hetero_variable_sample)%&gt;%\n    gather(key=metabolite,value=value,c(homo_variable_sample,hetero_variable_sample))%&gt;%\n    mutate(levene_test=ifelse(metabolite%in%(homo_variable_sample),\"homoscedasticity\",\"heteroscedasticity\"))\n\nstratified_levene_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(stratified_levene_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    facet_wrap(.~levene_test)+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity vs Homoscedasticity\")\n\n\n\n\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\n\nCode\nt_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"student\",\n                        categorical_variable=\"outcome\",\n                        homo_variables=homo_variable,\n                        hetero_variables=hetero_variable)\n\nmetabolites_associated_AD&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nmetabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%&gt;%\n    group_by(outcome)%&gt;%\n    summarise(mean=mean(value),sd=sd(value))%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nmean\nsd\n\n\n\n\nnegative\n0.2184112\n0.9869302\n\n\npositive\n-0.4186542\n0.9830175\n\n\n\n\n\nCode\ntop_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%head(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\ntop_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,top_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\nbottom_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%tail(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nbottom_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,bottom_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,bottom_metabolites_associated_AD)        \n\na1&lt;-top_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(top_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Strongly Associated with AD\")\n\na2&lt;-bottom_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(bottom_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Weakly Associated with AD\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\n\nCode\nsignificant_metabolites&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n# significant_metabolites%&gt;%write_rds(.,file='./docs/data/llfs_fake_significant_metabolites.rds')\n\n\nAs a result of the t tests, there are 201 metabolites that are significantly associated with AD status. Metabolites with the highest significance were designated as strong metabolites and metabolites with the lowest significance among metabolites significantly related to AD status were designated as weak metabolites, and the expression level of metabolites between the disease status was confirmed through visualization. As a result, a greater difference was observed in strong metabolites than in weak metabolites, but both groups were not clearly separated in terms of AD status.\n\n\n\n\n\nCode\nad_sex_summary&lt;-all_data%&gt;%\n    group_by(outcome,sex)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, sex,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$sex)$p.value,\n    method=\"chisquare_test\")\n\nad_sex_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\nsex\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\nman\n150\n30%\n85.79333\n5.146572\n0.1049983\nchisquare_test\n\n\nnegative\nwoman\n160\n32%\n84.96875\n5.284868\n0.1049983\nchisquare_test\n\n\npositive\nman\n77\n15.4%\n79.85714\n5.167495\n0.1049983\nchisquare_test\n\n\npositive\nwoman\n113\n22.6%\n81.29204\n4.896576\n0.1049983\nchisquare_test\n\n\n\n\n\n\n\n\n\n\nCode\nad_genotype_summary&lt;-all_data%&gt;%\n    group_by(outcome,genotype)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, genotype,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\")\n\ngenotype_ad_summary&lt;-all_data%&gt;%\n    group_by(genotype,outcome)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(genotype,outcome,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\") \n\nad_genotype_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\ngenotype\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\ne3\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\nnegative\ne2\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\nnegative\ne4\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\npositive\ne3\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\npositive\ne2\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\npositive\ne4\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nCode\ngenotype_ad_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenotype\noutcome\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\ne3\nnegative\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\ne3\npositive\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\ne2\nnegative\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\ne2\npositive\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\ne4\nnegative\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\ne4\npositive\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nAs you see the tables above, the count of AD status and that of genotype status are proportionately the same. Thus, there is no relation between AD and genotype in this data at the significant level 5%.\n\n\n\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually. For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nad_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\n\n\nCode\nage_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n310\n62%\n85.37\n5.23\n72\n69.625\n81.25\n85\n89\n100.625\n105\n\n\noutcome\npositive\nage\n190\n38%\n80.71\n5.04\n65\n66.500\n77.00\n81\n84\n94.500\n93\n\n\nsex\nman\nage\n227\n45.4%\n83.78\n5.86\n65\n68.000\n80.00\n84\n88\n100.000\n105\n\n\nsex\nwoman\nage\n273\n54.6%\n83.45\n5.43\n67\n69.500\n80.00\n83\n87\n97.500\n100\n\n\ngenotype\ne3\nage\n387\n77.4%\n83.61\n5.64\n65\n69.500\n80.00\n84\n87\n97.500\n102\n\n\ngenotype\ne2\nage\n40\n8%\n85.83\n5.62\n78\n70.875\n81.75\n86\n89\n99.875\n105\n\n\ngenotype\ne4\nage\n73\n14.6%\n82.30\n5.25\n67\n68.500\n79.00\n82\n86\n96.500\n94\n\n\n\n\n\n\n\nCode\nplot&lt;- ggarrange(\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\"),\n    ncol=2, nrow=2,legend=\"bottom\")\nplot\n\n\n\n\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -4.66, but their standard deviations are 5.04 and 5.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status 4.66 years younger than those with a positive one.\n\n\nCode\na1&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=age,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(legend.position=\"top\")+\n    labs(title=\"Histogram, Age Distribution Grouped by Disease Status\")   \na2&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=outcome,y=age,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Age Distribution Grouped by Disease Status\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nage_correlation_data&lt;-\n    main_statistical_test(in_data=all_data,in_numeric_variable = 'age',method = \"cor\")%&gt;%\n    filter(p_adjusted&lt;0.05,column_name!='id')\n\n\n151 metabolites are significantly associated with age.\n\n\n\n\n\nCode\ngenotype_aov&lt;-main_statistical_test(in_data=all_data,categorical_variable=\"genotype\",method='aov')\ngenotype_aov%&gt;%\nfilter(column_name!='id',p_adjusted&lt;0.05)%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ngroup_df\nresidual_df\ngroup_ssq\nresidual_ssq\ngroup_msq\nresidual_msq\nF_value\np_value\nmethod\np_adjusted\ntype\n\n\n\n\n\n\n\n0 metabolites are significantly associated with the genotype variable.\n\n\n\n\n\n\n\n\nCode\nall_data%&gt;%\n    group_by(outcome,sex,genotype)%&gt;%\n    summarise(count=n(),\n    mean_age=mean(age),\n    sd_age=sd(age))%&gt;%ungroup%&gt;%\n    mutate(sum_count=sum(count),\n    proportion=paste0(round(count/sum_count*100,3),'%'))%&gt;%\n    dplyr::select(outcome,sex,genotype,count,proportion, mean_age,sd_age)%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\ngenotype\ncount\nproportion\nmean_age\nsd_age\n\n\n\n\nnegative\nman\ne3\n113\n22.6%\n85.56637\n5.091793\n\n\nnegative\nman\ne2\n14\n2.8%\n88.57143\n6.548215\n\n\nnegative\nman\ne4\n23\n4.6%\n85.21739\n4.067125\n\n\nnegative\nwoman\ne3\n129\n25.8%\n85.07752\n5.370074\n\n\nnegative\nwoman\ne2\n12\n2.4%\n85.75000\n5.446016\n\n\nnegative\nwoman\ne4\n19\n3.8%\n83.73684\n4.628920\n\n\npositive\nman\ne3\n66\n13.2%\n79.74242\n5.384624\n\n\npositive\nman\ne2\n5\n1%\n83.00000\n3.000000\n\n\npositive\nman\ne4\n6\n1.2%\n78.50000\n3.082207\n\n\npositive\nwoman\ne3\n79\n15.8%\n81.65823\n4.784821\n\n\npositive\nwoman\ne2\n9\n1.8%\n83.22222\n3.632416\n\n\npositive\nwoman\ne4\n25\n5%\n79.44000\n5.260545\n\n\n\n\n\nExcept for the disease status variable, it looks like there is no difference of age in average and count in the other categorical variables. Taking into account age varialble is significantly associated with the metabolites that are significantly associated with the disease status, outcome (or AD). There is no need to conduct futher exploratory data analysis in the multivariable analysis senction."
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#eda",
    "href": "docs/projects/LLFS/eda.html#eda",
    "title": "EDA",
    "section": "",
    "text": "Code\n# raw data\nnormality_test_result&lt;-multiple_shapiro_test(all_data)%&gt;%\n    filter(column_name!='id')%&gt;%\n    group_by(type)%&gt;%\n    summarise(count=n())%&gt;%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%&gt;%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%&gt;%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n1001\n1001\n1\n\n\n\n\n\nOut of 1001 numeric variables, the variables following a normal distribution are 1001 (100%) and the ones that do not are NA (NA%).\n\n\n\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables&lt;-\n    multiple_shapiro_test(all_data)%&gt;%\n        filter(p_value&gt;0.05,column_name!='id')%&gt;%\n            dplyr::select(column_name)%&gt;%\n            pull%&gt;%sample(16)\n\nnormal_data&lt;-\n    all_data%&gt;%\n        dplyr::select(outcome,normal_variables)%&gt;%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%&gt;%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',linewidth=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution Grouped by Disease Status\")\n\n\n\n\n\n\n\n\n\n\n\n\nThere is no variable that do not follow a normal distribution.\n\n\n\n\n\n\nThrough the exploratory data analysis above, it was confirmed that all variables follow a normal distribution, and t tests were conducted to select metabolites that have significant relationships with the disease status, AD. To minimize type 1 error due to multiple testings, bonferroni correction was used in the EDA\n\nHomoscedasticity Test\n\nLeven’s test is performed to confirm that each variable has equality of variance, one of the assumptions of the t test and ANOVA.\n\n\nCode\nleven_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")%&gt;%\n    filter(column_name!='id')\n\nhomo_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&gt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\nhetero_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&lt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\n\n\n10 variables randomly selected out of 1001 variables with equal variance: meta408, meta959, meta796, meta957, meta740, meta212, meta171, meta769, meta986, meta178\n0 variables randomly selected with equal variance:\n\n\n\nCode\nhomo_variable_sample&lt;-homo_variable%&gt;%sample(10)\nhetero_variable_sample&lt;-hetero_variable\n\nstratified_levene_data&lt;-all_data%&gt;%\n    dplyr::select(outcome,homo_variable_sample,hetero_variable_sample)%&gt;%\n    gather(key=metabolite,value=value,c(homo_variable_sample,hetero_variable_sample))%&gt;%\n    mutate(levene_test=ifelse(metabolite%in%(homo_variable_sample),\"homoscedasticity\",\"heteroscedasticity\"))\n\nstratified_levene_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(stratified_levene_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    facet_wrap(.~levene_test)+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity vs Homoscedasticity\")\n\n\n\n\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\n\nCode\nt_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"student\",\n                        categorical_variable=\"outcome\",\n                        homo_variables=homo_variable,\n                        hetero_variables=hetero_variable)\n\nmetabolites_associated_AD&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nmetabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%&gt;%\n    group_by(outcome)%&gt;%\n    summarise(mean=mean(value),sd=sd(value))%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nmean\nsd\n\n\n\n\nnegative\n0.2184112\n0.9869302\n\n\npositive\n-0.4186542\n0.9830175\n\n\n\n\n\nCode\ntop_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%head(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\ntop_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,top_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\nbottom_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%tail(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nbottom_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,bottom_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,bottom_metabolites_associated_AD)        \n\na1&lt;-top_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(top_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Strongly Associated with AD\")\n\na2&lt;-bottom_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(bottom_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Weakly Associated with AD\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\n\nCode\nsignificant_metabolites&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\n\nAs a result of the t tests, there are 201 metabolites that are significantly associated with AD status at the 5% significance level. Metabolites with the highest significance were designated as strong metabolites and metabolites with the lowest significance among metabolites significantly related to AD status were designated as weak metabolites, and the expression level of metabolites between the disease status was confirmed through visualization. As a result, a greater difference was observed in strong metabolites than in weak metabolites, but both groups were not clearly separated in terms of AD status.\n\n\n\n\n\nCode\nad_sex_summary&lt;-all_data%&gt;%\n    group_by(outcome,sex)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, sex,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$sex)$p.value,\n    method=\"chisquare_test\")\n\nad_sex_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\nsex\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\nman\n150\n30%\n85.79333\n5.146572\n0.1049983\nchisquare_test\n\n\nnegative\nwoman\n160\n32%\n84.96875\n5.284868\n0.1049983\nchisquare_test\n\n\npositive\nman\n77\n15.4%\n79.85714\n5.167495\n0.1049983\nchisquare_test\n\n\npositive\nwoman\n113\n22.6%\n81.29204\n4.896576\n0.1049983\nchisquare_test\n\n\n\n\n\n\n\n\n\n\nCode\nad_genotype_summary&lt;-all_data%&gt;%\n    group_by(outcome,genotype)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, genotype,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\")\n\ngenotype_ad_summary&lt;-all_data%&gt;%\n    group_by(genotype,outcome)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(genotype,outcome,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\") \n\nad_genotype_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\ngenotype\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\ne3\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\nnegative\ne2\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\nnegative\ne4\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\npositive\ne3\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\npositive\ne2\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\npositive\ne4\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nCode\ngenotype_ad_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenotype\noutcome\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\ne3\nnegative\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\ne3\npositive\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\ne2\nnegative\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\ne2\npositive\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\ne4\nnegative\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\ne4\npositive\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nAs you see the tables above, the count of AD status and that of genotype status are proportionately the same. Thus, there is no relation between AD and genotype in this data at the significant level 5%.\n\n\n\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually. For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nad_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\n\n\nCode\nage_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n310\n62%\n85.37\n5.23\n72\n69.625\n81.25\n85\n89\n100.625\n105\n\n\noutcome\npositive\nage\n190\n38%\n80.71\n5.04\n65\n66.500\n77.00\n81\n84\n94.500\n93\n\n\nsex\nman\nage\n227\n45.4%\n83.78\n5.86\n65\n68.000\n80.00\n84\n88\n100.000\n105\n\n\nsex\nwoman\nage\n273\n54.6%\n83.45\n5.43\n67\n69.500\n80.00\n83\n87\n97.500\n100\n\n\ngenotype\ne3\nage\n387\n77.4%\n83.61\n5.64\n65\n69.500\n80.00\n84\n87\n97.500\n102\n\n\ngenotype\ne2\nage\n40\n8%\n85.83\n5.62\n78\n70.875\n81.75\n86\n89\n99.875\n105\n\n\ngenotype\ne4\nage\n73\n14.6%\n82.30\n5.25\n67\n68.500\n79.00\n82\n86\n96.500\n94\n\n\n\n\n\n\n\nCode\nplot&lt;- ggarrange(\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\"),\n    ncol=2, nrow=2,legend=\"bottom\")\nplot\n\n\n\n\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -4.66, but their standard deviations are 5.04 and 5.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status 4.66 years younger than those with a positive one.\n\n\nCode\na1&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=age,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(legend.position=\"top\")+\n    labs(title=\"Histogram, Age Distribution Grouped by Disease Status\")   \na2&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=outcome,y=age,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Age Distribution Grouped by Disease Status\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nage_correlation_data&lt;-\n    main_statistical_test(in_data=all_data,in_numeric_variable = 'age',method = \"cor\")%&gt;%\n    filter(p_adjusted&lt;0.05,column_name!='id')\n\n\n151 metabolites are significantly associated with age.\n\n\n\n\n\nCode\ngenotype_aov&lt;-main_statistical_test(in_data=all_data,categorical_variable=\"genotype\",method='aov')\ngenotype_aov%&gt;%\nfilter(column_name!='id',p_adjusted&lt;0.05)%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ngroup_df\nresidual_df\ngroup_ssq\nresidual_ssq\ngroup_msq\nresidual_msq\nF_value\np_value\nmethod\np_adjusted\ntype\n\n\n\n\n\n\n\n0 metabolites are significantly associated with the genotype variable.\n\n\n\n\n\n\n\n\nCode\nall_data%&gt;%\n    group_by(outcome,sex,genotype)%&gt;%\n    summarise(count=n(),\n    mean_age=mean(age),\n    sd_age=sd(age))%&gt;%ungroup%&gt;%\n    mutate(sum_count=sum(count),\n    proportion=paste0(round(count/sum_count*100,3),'%'))%&gt;%\n    dplyr::select(outcome,sex,genotype,count,proportion, mean_age,sd_age)%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\ngenotype\ncount\nproportion\nmean_age\nsd_age\n\n\n\n\nnegative\nman\ne3\n113\n22.6%\n85.56637\n5.091793\n\n\nnegative\nman\ne2\n14\n2.8%\n88.57143\n6.548215\n\n\nnegative\nman\ne4\n23\n4.6%\n85.21739\n4.067125\n\n\nnegative\nwoman\ne3\n129\n25.8%\n85.07752\n5.370074\n\n\nnegative\nwoman\ne2\n12\n2.4%\n85.75000\n5.446016\n\n\nnegative\nwoman\ne4\n19\n3.8%\n83.73684\n4.628920\n\n\npositive\nman\ne3\n66\n13.2%\n79.74242\n5.384624\n\n\npositive\nman\ne2\n5\n1%\n83.00000\n3.000000\n\n\npositive\nman\ne4\n6\n1.2%\n78.50000\n3.082207\n\n\npositive\nwoman\ne3\n79\n15.8%\n81.65823\n4.784821\n\n\npositive\nwoman\ne2\n9\n1.8%\n83.22222\n3.632416\n\n\npositive\nwoman\ne4\n25\n5%\n79.44000\n5.260545\n\n\n\n\n\nExcept for the disease status variable, it looks like there is no difference of age in average and count in the other categorical variables. Taking into account age varialble is significantly associated with the metabolites that are significantly associated with the disease status, outcome (or AD). There is no need to conduct futher exploratory data analysis in the multivariable analysis senction."
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#eda-1",
    "href": "docs/projects/LLFS/eda.html#eda-1",
    "title": "EDA",
    "section": "",
    "text": "Code\n# raw data\nnormality_test_result&lt;-multiple_shapiro_test(all_data)%&gt;%\n    filter(column_name!='id')%&gt;%\n    group_by(type)%&gt;%\n    summarise(count=n())%&gt;%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%&gt;%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%&gt;%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n1001\n1001\n1\n\n\n\n\n\nOut of 1001 numeric variables, the variables following a normal distribution are 1001 (100%) and the ones that do not are NA (NA%).\n\n\n\n\n\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables&lt;-\n    multiple_shapiro_test(all_data)%&gt;%\n        filter(p_value&gt;0.05,column_name!='id')%&gt;%\n            dplyr::select(column_name)%&gt;%\n            pull%&gt;%sample(16)\n\nnormal_data&lt;-\n    all_data%&gt;%\n        dplyr::select(outcome,normal_variables)%&gt;%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%&gt;%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',linewidth=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution Grouped by Disease Status\")\n\n\n\n\n\n\n\n\n\n\n\n\nThere is no variable that do not follow a normal distribution.\n\n\n\n\n\n\nThrough the exploratory data analysis above, it was confirmed that all variables follow a normal distribution, and t tests were conducted to select metabolites that have significant relationships with the disease status, AD. To minimize type 1 error due to multiple testings, bonferroni correction was used in the EDA\n\nHomoscedasticity Test\n\nLeven’s test is performed to confirm that each variable has equality of variance, one of the assumptions of the t test and ANOVA.\n\n\nCode\nleven_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")%&gt;%\n    filter(column_name!='id')\n\nhomo_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&gt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\nhetero_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&lt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\n\n\n10 variables randomly selected out of 1001 variables with equal variance: meta408, meta959, meta796, meta957, meta740, meta212, meta171, meta769, meta986, meta178\n0 variables randomly selected with equal variance:\n\n\n\nCode\nhomo_variable_sample&lt;-homo_variable%&gt;%sample(10)\nhetero_variable_sample&lt;-hetero_variable\n\nstratified_levene_data&lt;-all_data%&gt;%\n    dplyr::select(outcome,homo_variable_sample,hetero_variable_sample)%&gt;%\n    gather(key=metabolite,value=value,c(homo_variable_sample,hetero_variable_sample))%&gt;%\n    mutate(levene_test=ifelse(metabolite%in%(homo_variable_sample),\"homoscedasticity\",\"heteroscedasticity\"))\n\nstratified_levene_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(stratified_levene_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    facet_wrap(.~levene_test)+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity vs Homoscedasticity\")\n\n\n\n\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\n\nCode\nt_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"student\",\n                        categorical_variable=\"outcome\",\n                        homo_variables=homo_variable,\n                        hetero_variables=hetero_variable)\n\nmetabolites_associated_AD&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nmetabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%&gt;%\n    group_by(outcome)%&gt;%\n    summarise(mean=mean(value),sd=sd(value))%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nmean\nsd\n\n\n\n\nnegative\n0.2184112\n0.9869302\n\n\npositive\n-0.4186542\n0.9830175\n\n\n\n\n\nCode\ntop_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%head(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\ntop_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,top_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\nbottom_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%tail(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nbottom_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,bottom_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,bottom_metabolites_associated_AD)        \n\na1&lt;-top_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(top_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Strongly Associated with AD\")\n\na2&lt;-bottom_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(bottom_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Weakly Associated with AD\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\n\nCode\nsignificant_metabolites&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n# significant_metabolites%&gt;%write_rds(.,file='./docs/data/llfs_fake_significant_metabolites.rds')\n\n\nAs a result of the t tests, there are 201 metabolites that are significantly associated with AD status. Metabolites with the highest significance were designated as strong metabolites and metabolites with the lowest significance among metabolites significantly related to AD status were designated as weak metabolites, and the expression level of metabolites between the disease status was confirmed through visualization. As a result, a greater difference was observed in strong metabolites than in weak metabolites, but both groups were not clearly separated in terms of AD status.\n\n\n\n\n\nCode\nad_sex_summary&lt;-all_data%&gt;%\n    group_by(outcome,sex)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, sex,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$sex)$p.value,\n    method=\"chisquare_test\")\n\nad_sex_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\nsex\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\nman\n150\n30%\n85.79333\n5.146572\n0.1049983\nchisquare_test\n\n\nnegative\nwoman\n160\n32%\n84.96875\n5.284868\n0.1049983\nchisquare_test\n\n\npositive\nman\n77\n15.4%\n79.85714\n5.167495\n0.1049983\nchisquare_test\n\n\npositive\nwoman\n113\n22.6%\n81.29204\n4.896576\n0.1049983\nchisquare_test\n\n\n\n\n\n\n\n\n\n\nCode\nad_genotype_summary&lt;-all_data%&gt;%\n    group_by(outcome,genotype)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, genotype,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\")\n\ngenotype_ad_summary&lt;-all_data%&gt;%\n    group_by(genotype,outcome)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(genotype,outcome,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\") \n\nad_genotype_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\ngenotype\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\ne3\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\nnegative\ne2\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\nnegative\ne4\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\npositive\ne3\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\npositive\ne2\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\npositive\ne4\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nCode\ngenotype_ad_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenotype\noutcome\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\ne3\nnegative\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\ne3\npositive\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\ne2\nnegative\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\ne2\npositive\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\ne4\nnegative\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\ne4\npositive\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nAs you see the tables above, the count of AD status and that of genotype status are proportionately the same. Thus, there is no relation between AD and genotype in this data at the significant level 5%.\n\n\n\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually. For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nad_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\n\n\nCode\nage_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n310\n62%\n85.37\n5.23\n72\n69.625\n81.25\n85\n89\n100.625\n105\n\n\noutcome\npositive\nage\n190\n38%\n80.71\n5.04\n65\n66.500\n77.00\n81\n84\n94.500\n93\n\n\nsex\nman\nage\n227\n45.4%\n83.78\n5.86\n65\n68.000\n80.00\n84\n88\n100.000\n105\n\n\nsex\nwoman\nage\n273\n54.6%\n83.45\n5.43\n67\n69.500\n80.00\n83\n87\n97.500\n100\n\n\ngenotype\ne3\nage\n387\n77.4%\n83.61\n5.64\n65\n69.500\n80.00\n84\n87\n97.500\n102\n\n\ngenotype\ne2\nage\n40\n8%\n85.83\n5.62\n78\n70.875\n81.75\n86\n89\n99.875\n105\n\n\ngenotype\ne4\nage\n73\n14.6%\n82.30\n5.25\n67\n68.500\n79.00\n82\n86\n96.500\n94\n\n\n\n\n\n\n\nCode\nplot&lt;- ggarrange(\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\"),\n    ncol=2, nrow=2,legend=\"bottom\")\nplot\n\n\n\n\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -4.66, but their standard deviations are 5.04 and 5.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status 4.66 years younger than those with a positive one.\n\n\nCode\na1&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=age,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(legend.position=\"top\")+\n    labs(title=\"Histogram, Age Distribution Grouped by Disease Status\")   \na2&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=outcome,y=age,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Age Distribution Grouped by Disease Status\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nage_correlation_data&lt;-\n    main_statistical_test(in_data=all_data,in_numeric_variable = 'age',method = \"cor\")%&gt;%\n    filter(p_adjusted&lt;0.05,column_name!='id')\n\n\n151 metabolites are significantly associated with age.\n\n\n\n\n\nCode\ngenotype_aov&lt;-main_statistical_test(in_data=all_data,categorical_variable=\"genotype\",method='aov')\ngenotype_aov%&gt;%\nfilter(column_name!='id',p_adjusted&lt;0.05)%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ngroup_df\nresidual_df\ngroup_ssq\nresidual_ssq\ngroup_msq\nresidual_msq\nF_value\np_value\nmethod\np_adjusted\ntype\n\n\n\n\n\n\n\n0 metabolites are significantly associated with the genotype variable.\n\n\n\n\n\n\n\n\nCode\nall_data%&gt;%\n    group_by(outcome,sex,genotype)%&gt;%\n    summarise(count=n(),\n    mean_age=mean(age),\n    sd_age=sd(age))%&gt;%ungroup%&gt;%\n    mutate(sum_count=sum(count),\n    proportion=paste0(round(count/sum_count*100,3),'%'))%&gt;%\n    dplyr::select(outcome,sex,genotype,count,proportion, mean_age,sd_age)%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\ngenotype\ncount\nproportion\nmean_age\nsd_age\n\n\n\n\nnegative\nman\ne3\n113\n22.6%\n85.56637\n5.091793\n\n\nnegative\nman\ne2\n14\n2.8%\n88.57143\n6.548215\n\n\nnegative\nman\ne4\n23\n4.6%\n85.21739\n4.067125\n\n\nnegative\nwoman\ne3\n129\n25.8%\n85.07752\n5.370074\n\n\nnegative\nwoman\ne2\n12\n2.4%\n85.75000\n5.446016\n\n\nnegative\nwoman\ne4\n19\n3.8%\n83.73684\n4.628920\n\n\npositive\nman\ne3\n66\n13.2%\n79.74242\n5.384624\n\n\npositive\nman\ne2\n5\n1%\n83.00000\n3.000000\n\n\npositive\nman\ne4\n6\n1.2%\n78.50000\n3.082207\n\n\npositive\nwoman\ne3\n79\n15.8%\n81.65823\n4.784821\n\n\npositive\nwoman\ne2\n9\n1.8%\n83.22222\n3.632416\n\n\npositive\nwoman\ne4\n25\n5%\n79.44000\n5.260545\n\n\n\n\n\nExcept for the disease status variable, it looks like there is no difference of age in average and count in the other categorical variables. Taking into account age varialble is significantly associated with the metabolites that are significantly associated with the disease status, outcome (or AD). There is no need to conduct futher exploratory data analysis in the multivariable analysis senction."
  },
  {
    "objectID": "docs/projects/LLFS/ml_approach.html",
    "href": "docs/projects/LLFS/ml_approach.html",
    "title": "ML Approach",
    "section": "",
    "text": "alpha      mse fit.name\n1    0.0 2217.331   alpha0\n2    0.1 2217.331 alpha0.1\n3    0.2 2217.331 alpha0.2\n4    0.3 2217.331 alpha0.3\n5    0.4 2217.331 alpha0.4\n6    0.5 2217.331 alpha0.5\n7    0.6 2217.331 alpha0.6\n8    0.7 2217.331 alpha0.7\n9    0.8 2217.331 alpha0.8\n10   0.9 2217.331 alpha0.9\n11   1.0 2217.331   alpha1"
  },
  {
    "objectID": "docs/projects/LLFS/statistical_approach.html",
    "href": "docs/projects/LLFS/statistical_approach.html",
    "title": "Statistical Approach",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 Go to Project Content List\nProject Content List\n\n\n2 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "README copy.html",
    "href": "README copy.html",
    "title": "website",
    "section": "",
    "text": "website"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\nFigure 1"
  },
  {
    "objectID": "about.html#enthusiastic-data-scientist",
    "href": "about.html#enthusiastic-data-scientist",
    "title": "Kwangmin Kim",
    "section": "Enthusiastic Data Scientist",
    "text": "Enthusiastic Data Scientist\n\nInterests\nData Modeling, Statistics, Machine Learning, Deep Learning, Optimization"
  },
  {
    "objectID": "docs/blog/posts/content_list.html",
    "href": "docs/blog/posts/content_list.html",
    "title": "Blog Content List",
    "section": "",
    "text": "Contents\n\nData Governance\nEngineering\nSurveilance\n\n\n\n\n\n\n\nNote\n\n\n\n\nScalars are denoted with a lower-case letter (ex a ) or a non-bolded lower-case Greek letter (ex \\(\\alpha\\) ).\nVectors are denoted using a bold-faced lower-case letter (ex \\(\\mathbf a\\)).\nMatrices are denoted using a bold-faced upper-case letter (ex \\(\\mathbf A\\), \\(\\mathbf \\phi\\)) or a bold-faced upper-case Greek letter (ex \\(\\mathbf \\Phi\\)).\nTensors are denoted using a bold-faced upper-case letter with multiple subscripts or superscripts, indicating the number of indices and the dimensions of the tensor along each axis.\n\nA second-order tensor (also known as a matrix) \\(\\mathbf A\\) with dimensions \\(n \\times m\\) can be represented as: \\(\\mathbf A_{ij}\\) where \\(i = 1,\\dots,m\\) and \\(j = 1,\\dots,n\\), which are the indices that run over the rows and columns of the matrix, respectively.\nA third-order tensor \\(T\\) with dimensions \\(n \\times m \\times p\\) can be represented as: \\(\\mathbf A_{ijk}\\) where \\(i = 1,\\dots,m\\), \\(j = 1,\\dots,n\\), which are \\(i\\), and \\(k = 1,\\dots,p\\) \\(j\\), and \\(k\\), which are the indices that run over the three dimensions of the tensor."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html",
    "title": "Content List, Mathematics",
    "section": "",
    "text": "2023-03-24, Variable types\n1111-11-11, Function\n\n2023-01-31, Function (1) - Univariable Scalar Function (One to One)\n2023-01-31, Function (2) - Multi-variable Scalar Function (Many to One)\n2023-01-31, Function (3) - Univariable Vector Function (One to Many)\n2023-01-31, Function (4) - Multi-variable Vector Function (Many to Many)\n2023-02-18, Function (5) - Composite Function\n\n2023-02-18, Transformations of Functions\n1111-11-11, Vector & Matrix\n2023-03-15, Limit, \\(\\epsilon-\\delta\\) Method\nDifferentiation\n\n2023-02-04, Derivative (1) - Univariable Scalar Funtion\n1111-11-11, Derivative (2) - Chain Rule & Partial Derivative\n1111-11-11, Derivative (3) - Higher Order Derivative\n1111-11-11, Derivative (4) - Mean Value Theorem\n1111-11-11, Derivative (5) - Gradient\n\n2023-03-15, Talyer’s Series\n1111-11-11, Gradient Direction\n1111-11-11, Random Variable\n1111-11-11, Probability Distribution\n1111-11-11, Information Theory - Entropy\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n\n\n\n\n\n2023-03-30, Basic Vector(1) - Vector Operations\n2023-03-30, Basic Vector(2) - Vector Norm and Dot Product”\n2023-03-30, Basic Vector(3) - Vector Equation\n2023-03-30, Basic Matrix(1) - Matrix Operation\n2023-03-30, Basic Matrix(2) - Matrix Multiplication\n2023-03-30, Basic Matrix(3) - System of Linear Equations\n2023-03-30, Basic Matrix(4) - Special Matrix\n2023-04-14, [Lineqr Equations]\n2023-04-14, [Vector Space and Subspaces]\n2023-04-21, [Orthogonality]\n1111-11-11, [Determinants]\n1111-11-11, [Eigen Value & Eigen Vector]\n1111-11-11, [Linear Transformations]\n1111-11-11, Basis, Dimension, & Rank\n1111-11-11,\n1111-11-11, Eigen Decomposition\n1111-11-11, Singular Value Decomposition (SVD)\n1111-11-11, Group\n1111-11-11, Rotation & Group\n2023-04-02, Matrix Transformation (5) - Quadratic Form\n2023-04-02, Matrix Calculus (1) - Quadratic Form\n1111-11-11,\n1111-11-11,\n1111-11-11,\n\n\n\n\n\n2023-03-23, Minimizer & Minimum\n1111-11-11, Convex Set\n1111-11-11, Convex Function\n1111-11-11, Unconstrained Optimization\n1111-11-11, Non-linear Least Square\n1111-11-11, Largrange Multiplier Method\n\n1111-11-11, Largrange Primal Function\n1111-11-11, Largrange Dual Function\n1111-11-11, KKT conditions\n\n1111-11-11, Gradient Descent Optimizers\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#basic",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#basic",
    "title": "Content List, Mathematics",
    "section": "",
    "text": "2023-03-24, Variable types\n1111-11-11, Function\n\n2023-01-31, Function (1) - Univariable Scalar Function (One to One)\n2023-01-31, Function (2) - Multi-variable Scalar Function (Many to One)\n2023-01-31, Function (3) - Univariable Vector Function (One to Many)\n2023-01-31, Function (4) - Multi-variable Vector Function (Many to Many)\n2023-02-18, Function (5) - Composite Function\n\n2023-02-18, Transformations of Functions\n1111-11-11, Vector & Matrix\n2023-03-15, Limit, \\(\\epsilon-\\delta\\) Method\nDifferentiation\n\n2023-02-04, Derivative (1) - Univariable Scalar Funtion\n1111-11-11, Derivative (2) - Chain Rule & Partial Derivative\n1111-11-11, Derivative (3) - Higher Order Derivative\n1111-11-11, Derivative (4) - Mean Value Theorem\n1111-11-11, Derivative (5) - Gradient\n\n2023-03-15, Talyer’s Series\n1111-11-11, Gradient Direction\n1111-11-11, Random Variable\n1111-11-11, Probability Distribution\n1111-11-11, Information Theory - Entropy\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#linear-algebra",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#linear-algebra",
    "title": "Content List, Mathematics",
    "section": "",
    "text": "2023-03-30, Basic Vector(1) - Vector Operations\n2023-03-30, Basic Vector(2) - Vector Norm and Dot Product”\n2023-03-30, Basic Vector(3) - Vector Equation\n2023-03-30, Basic Matrix(1) - Matrix Operation\n2023-03-30, Basic Matrix(2) - Matrix Multiplication\n2023-03-30, Basic Matrix(3) - System of Linear Equations\n2023-03-30, Basic Matrix(4) - Special Matrix\n2023-04-14, [Lineqr Equations]\n2023-04-14, [Vector Space and Subspaces]\n2023-04-21, [Orthogonality]\n1111-11-11, [Determinants]\n1111-11-11, [Eigen Value & Eigen Vector]\n1111-11-11, [Linear Transformations]\n1111-11-11, Basis, Dimension, & Rank\n1111-11-11,\n1111-11-11, Eigen Decomposition\n1111-11-11, Singular Value Decomposition (SVD)\n1111-11-11, Group\n1111-11-11, Rotation & Group\n2023-04-02, Matrix Transformation (5) - Quadratic Form\n2023-04-02, Matrix Calculus (1) - Quadratic Form\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#optimization",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#optimization",
    "title": "Content List, Mathematics",
    "section": "",
    "text": "2023-03-23, Minimizer & Minimum\n1111-11-11, Convex Set\n1111-11-11, Convex Function\n1111-11-11, Unconstrained Optimization\n1111-11-11, Non-linear Least Square\n1111-11-11, Largrange Multiplier Method\n\n1111-11-11, Largrange Primal Function\n1111-11-11, Largrange Dual Function\n1111-11-11, KKT conditions\n\n1111-11-11, Gradient Descent Optimizers\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html",
    "href": "docs/blog/posts/Engineering/guide_map/index.html",
    "title": "Content List, Engineering",
    "section": "",
    "text": "0000-00-00, Terminology"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#it-terminology",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#it-terminology",
    "title": "Content List, Engineering",
    "section": "",
    "text": "0000-00-00, Terminology"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#data-structure",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#data-structure",
    "title": "Content List, Engineering",
    "section": "2 Data Structure",
    "text": "2 Data Structure\n\n2023-01-17, Overview\n2023-01-18, Array\n2023-01-18, Linked List\n2023-01-18, Python List\n2023-01-19, Stack\n2023-01-19, Queue\n2023-01-26, Deque\n2023-01-26, Binary Search Tree\n2023-01-20, Priority Queue\n2023-01-20, Graph"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#development-environment-setting",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#development-environment-setting",
    "title": "Content List, Engineering",
    "section": "3 Development Environment Setting",
    "text": "3 Development Environment Setting\n\n3.1 WSL2\n\n2023-05-01, Introduction & Installation\n2023-05-01, Frequently Used Linux Command\n2024-01-25, Linux_Error_Fix_rm_cannot_remove_files_busy\n\n\n\n3.2 Docker\n\n2023-05-01, Introduction & Installation\n\n\n\n3.3 VS code\n\n2023-05-01, Introduction & Installation\n\n\n\n3.4 Conda\n\n2023-05-01, Introduction & Installation\n\n\n\n3.5 Git\n\n2023-05-01, Introduction & Installation"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#documentation",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#documentation",
    "title": "Content List, Engineering",
    "section": "4 Documentation",
    "text": "4 Documentation\n\n4.1 Dynamic Documentation\n\n2023-01-19, Quarto\n2023-01-19, xaringan[R]\n2023-01-19, Bookdown[R]\n2023-01-19, DISTL\n2023-01-26, Sphinx[Python]\n\n\n\n4.2 Diagrams\n\n2023-05-01, Quarto & Diagrams\n2023-05-01, Graphiz Gallery\n2023-05-01, Mermaid Gallery"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#aws-cloud",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#aws-cloud",
    "title": "Content List, Engineering",
    "section": "5 AWS Cloud",
    "text": "5 AWS Cloud\nCoursera Course: AWS Fundamentals\n\n2023-03-09, Computing and Networking\n2023-03-12, Storage and Database\n2023-03-26, Monitoring and SharedResponsibility\n2023-04-05, Infrastructure Security"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#azure-cloud",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#azure-cloud",
    "title": "Content List, Engineering",
    "section": "6 Azure Cloud",
    "text": "6 Azure Cloud"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#gcp",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#gcp",
    "title": "Content List, Engineering",
    "section": "7 GCP",
    "text": "7 GCP"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#apache-airflow",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#apache-airflow",
    "title": "Content List, Engineering",
    "section": "10 Apache Airflow",
    "text": "10 Apache Airflow\n\n2023-05-01, Introduction\n2023-05-01, Airflow Environment Setting\n2023-05-01, Operator Basics\n2023-05-01, Python Operators\n2023-05-01, Template Variable\n2023-05-01, Data Share\n2023-05-01, Task Handling - Advanced\n2023-05-01, More Operators\n2023-05-10, Connection & Hook\n2023-05-10, Sensor\n2023-05-10, More Airflow Functions\n2023-05-10, Operate WebApp Using Rshiny"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#apache-spark",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#apache-spark",
    "title": "Content List, Engineering",
    "section": "11 Apache Spark",
    "text": "11 Apache Spark"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#data-modeling",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#data-modeling",
    "title": "Content List, Engineering",
    "section": "12 Data Modeling",
    "text": "12 Data Modeling"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#front-end",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#front-end",
    "title": "Content List, Engineering",
    "section": "13 Front End",
    "text": "13 Front End"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#back-end",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#back-end",
    "title": "Content List, Engineering",
    "section": "14 Back End",
    "text": "14 Back End"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/01.intro.html",
    "href": "docs/blog/posts/Engineering/airflow/01.intro.html",
    "title": "Airflow Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n파이썬을 이용해 워크플로우를 만들고 관리할 수 있는 오픈소스 기반 워크플로우 관리 도구\n2014년 에어비앤비에서 만든 워크플로우 관리 솔루션으로 현재는 Apache Open Source 재단에서 관리되고 있는 프로젝트\nAirflow는 워크플로우를 DAG을 사용하여 정의하고, 관리하는 프로그램\n\n자유도가 크고, 확장성이 좋은 워크플로우 관리 프로그램\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nREST API를 사용한 데이터를 받아 전처리\n\nREST API를 사용한 데이터를 받아 전처리\n\n\n\n기존 DB의 데이터 삭제 (중복 제거)\n\n기존 DB의 데이터 삭제 (중복 제거)\n\n\n\nREST API를 사용한 데이터를 받아 전처리--기존 DB의 데이터 삭제 (중복 제거)\n\n\n\n\n전처리한 데이터를 DB에 삽입\n\n전처리한 데이터를 DB에 삽입\n\n\n\n기존 DB의 데이터 삭제 (중복 제거)--전처리한 데이터를 DB에 삽입\n\n\n\n\n\n\n\nFigure 1: Airflow Workflow Simple Example\n\n\n\n\n\n\n파이썬으로 제작된 도구이며 이용자가 워크플로우 생성시에도 파이썬으로 구현해야 함\n하나의 워크플로우는 DAG(Directed Acyclic Graph) 이라 부르며 DAG 안에는 1개 이상의 Task가 존재\n\n예를 들어, REST API로부터 데이터를 내려 받아 DB에 insert하려는 과제를 수행하기 위해 Figure 1 와 같은 단계들이 필요하다.\n각 각의 단계를 task라 하고 각 각 선/후행 단계가 있다. (1번 task \\(\\rightarrow\\) 2번 task \\(\\rightarrow\\) 3번 task)\n이 tasks의 집합을 DAG이라고 한다.\nTask간 선후행 연결이 가능하되 순환되지 않고 방향성을 가짐(=DAG)\n\nCron 기반의 스케줄링\n\nLinux에서 사용되는 스케쥴링으로 task들이 시작되어야 하는 시작 시간이나 주기를 설정\n\n모니터링 및 실패 작업에 대한 재실행 기능이 간편\n\n\n\n\n\n\n\nFigure 2: DAG Simple Example\n\n\n\n\nIn Figure 2, 초록색 테두리의 node는 성공한 task를 의미하고 분홍색 테두리의 node는 실패한 task를 의미한다. 위의 그림에는 없지만 회색 테두리는 queue (준비) 상태를 의미한다. Airflow에는 DAG이 일련의 task로 구성되어 있기 때문에 실행 상태도 성공, 실패 및 준비 상태같은 여러 종류가 있다. 나머지 상태는 뒷 부분에서 차차 다뤄보기로 한다.\n\n\n\n\nTask Status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Orchestrating Task Tools (a.k.a Data Workflows) Demand Comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Orchestrating Task Tools Computation Comparison\n\n\n\n\n\n\n\nFigure 3: Airflow Figure Reference: Airflow vs. Luigi vs. Argo vs. MLFlow vs. KubeFlow by Markus Schmitt\n\n\n\n\nMaturity: 성숙도로서 github에서 얼마나 많은 사람들이 관리하고 있는지 즉 community의 활성도를 나타내는 지표. 유료 서비스의 경우 user 가 제작사에게 패치를 요청할 수 있지만 open source에 경우 제작사가 없어 community의 활성도가 중요하다.\nPopularity: github starts의 개수\nSimplicity: workflow를 얼마나 쉽게 사용할 수 있는지에 대한 난이도로 airflow는 사용하기에 어려운 난이도를 보여준다.\nBreadth: 확장성. 즉 얼마나 customizing할 수 있는지 보여주는 척도\n\nFigure 3 을 보면, Figure 3 (a) 에서 볼수 있듯이 최근 들어 airflow의 인기가 급증하는 것을 볼 수 있다. Figure 3 (b) 에서 그 이유를 짐작할 수 있는데 airflow가 simplicity가 C 사용하기는 어렵지만 구현할 수 있는 폭 breadth가 높다 (Breadth: A). 즉, 다른 workflows에 비해 복잡한 코딩을 요구하는 만큼 그 자유도가 높다는 것을 짐작할 수 있다.\n\n\n\n\n\n파이썬에 익숙하다면 러닝 커브 빠르게 극복 가능\n대규모 워크플로우 환경에서 부하 증가시 수평적 확장 가능한 Kubenetes 등 아키텍처 지원\n파이썬에서 지원되는 라이브러리 활용하여 다양한 도구 컨트롤 가능 (GCP, AWS등 대다수 클라우드에서 제공하는 서비스)\n\nGCP: Google Cloud Platform\nAWS: Amazon Web Services\n\nAirflow에서 제공하는 파이썬 소스 기반으로 원하는 작업을 위한 커스터마이징이 가능 (오퍼레이터, Hook, 센서 등)\n\n\n\n\n\n실시간 워크플로우 관리에 적합치 않음 (최소 분 단위 실행)\n워크플로우(DAG) 개수가 많아질 경우 모니터링이 쉽지 않음\n워크플로우를 GUI환경에서 만들지 않기에 파이썬에 익숙하지 않다면 다루기 쉽지 않음 협업 환경에서 프로그래밍 표준이 없으면 유지관리가 쉽지 않음\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/01.intro.html#characteristics",
    "href": "docs/blog/posts/Engineering/airflow/01.intro.html#characteristics",
    "title": "Airflow Introduction",
    "section": "",
    "text": "G\n\n\n\nREST API를 사용한 데이터를 받아 전처리\n\nREST API를 사용한 데이터를 받아 전처리\n\n\n\n기존 DB의 데이터 삭제 (중복 제거)\n\n기존 DB의 데이터 삭제 (중복 제거)\n\n\n\nREST API를 사용한 데이터를 받아 전처리--기존 DB의 데이터 삭제 (중복 제거)\n\n\n\n\n전처리한 데이터를 DB에 삽입\n\n전처리한 데이터를 DB에 삽입\n\n\n\n기존 DB의 데이터 삭제 (중복 제거)--전처리한 데이터를 DB에 삽입\n\n\n\n\n\n\n\nFigure 1: Airflow Workflow Simple Example\n\n\n\n\n\n\n파이썬으로 제작된 도구이며 이용자가 워크플로우 생성시에도 파이썬으로 구현해야 함\n하나의 워크플로우는 DAG(Directed Acyclic Graph) 이라 부르며 DAG 안에는 1개 이상의 Task가 존재\n\n예를 들어, REST API로부터 데이터를 내려 받아 DB에 insert하려는 과제를 수행하기 위해 Figure 1 와 같은 단계들이 필요하다.\n각 각의 단계를 task라 하고 각 각 선/후행 단계가 있다. (1번 task \\(\\rightarrow\\) 2번 task \\(\\rightarrow\\) 3번 task)\n이 tasks의 집합을 DAG이라고 한다.\nTask간 선후행 연결이 가능하되 순환되지 않고 방향성을 가짐(=DAG)\n\nCron 기반의 스케줄링\n\nLinux에서 사용되는 스케쥴링으로 task들이 시작되어야 하는 시작 시간이나 주기를 설정\n\n모니터링 및 실패 작업에 대한 재실행 기능이 간편\n\n\n\n\n\n\n\nFigure 2: DAG Simple Example\n\n\n\n\nIn Figure 2, 초록색 테두리의 node는 성공한 task를 의미하고 분홍색 테두리의 node는 실패한 task를 의미한다. 위의 그림에는 없지만 회색 테두리는 queue (준비) 상태를 의미한다. Airflow에는 DAG이 일련의 task로 구성되어 있기 때문에 실행 상태도 성공, 실패 및 준비 상태같은 여러 종류가 있다. 나머지 상태는 뒷 부분에서 차차 다뤄보기로 한다.\n\n\n\n\nTask Status"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/01.intro.html#motivation",
    "href": "docs/blog/posts/Engineering/airflow/01.intro.html#motivation",
    "title": "Airflow Introduction",
    "section": "",
    "text": "(a) Orchestrating Task Tools (a.k.a Data Workflows) Demand Comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Orchestrating Task Tools Computation Comparison\n\n\n\n\n\n\n\nFigure 3: Airflow Figure Reference: Airflow vs. Luigi vs. Argo vs. MLFlow vs. KubeFlow by Markus Schmitt\n\n\n\n\nMaturity: 성숙도로서 github에서 얼마나 많은 사람들이 관리하고 있는지 즉 community의 활성도를 나타내는 지표. 유료 서비스의 경우 user 가 제작사에게 패치를 요청할 수 있지만 open source에 경우 제작사가 없어 community의 활성도가 중요하다.\nPopularity: github starts의 개수\nSimplicity: workflow를 얼마나 쉽게 사용할 수 있는지에 대한 난이도로 airflow는 사용하기에 어려운 난이도를 보여준다.\nBreadth: 확장성. 즉 얼마나 customizing할 수 있는지 보여주는 척도\n\nFigure 3 을 보면, Figure 3 (a) 에서 볼수 있듯이 최근 들어 airflow의 인기가 급증하는 것을 볼 수 있다. Figure 3 (b) 에서 그 이유를 짐작할 수 있는데 airflow가 simplicity가 C 사용하기는 어렵지만 구현할 수 있는 폭 breadth가 높다 (Breadth: A). 즉, 다른 workflows에 비해 복잡한 코딩을 요구하는 만큼 그 자유도가 높다는 것을 짐작할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/01.intro.html#strength",
    "href": "docs/blog/posts/Engineering/airflow/01.intro.html#strength",
    "title": "Airflow Introduction",
    "section": "",
    "text": "파이썬에 익숙하다면 러닝 커브 빠르게 극복 가능\n대규모 워크플로우 환경에서 부하 증가시 수평적 확장 가능한 Kubenetes 등 아키텍처 지원\n파이썬에서 지원되는 라이브러리 활용하여 다양한 도구 컨트롤 가능 (GCP, AWS등 대다수 클라우드에서 제공하는 서비스)\n\nGCP: Google Cloud Platform\nAWS: Amazon Web Services\n\nAirflow에서 제공하는 파이썬 소스 기반으로 원하는 작업을 위한 커스터마이징이 가능 (오퍼레이터, Hook, 센서 등)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/01.intro.html#weakness",
    "href": "docs/blog/posts/Engineering/airflow/01.intro.html#weakness",
    "title": "Airflow Introduction",
    "section": "",
    "text": "실시간 워크플로우 관리에 적합치 않음 (최소 분 단위 실행)\n워크플로우(DAG) 개수가 많아질 경우 모니터링이 쉽지 않음\n워크플로우를 GUI환경에서 만들지 않기에 파이썬에 익숙하지 않다면 다루기 쉽지 않음 협업 환경에서 프로그래밍 표준이 없으면 유지관리가 쉽지 않음"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Docker/01.docker_install.html",
    "href": "docs/blog/posts/Engineering/Docker/01.docker_install.html",
    "title": "Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n1 Docker\n\n리눅스내 가상화 관련 커널을 활용하여 어플리케이션을 독립적 환경에서 실행시키는 기술\n가상화 서버(VM) 대비 Guest OS가 없어 경량화된 가상화 서버로 볼 수 있음\n\nVM의 hypervisor요소가 다수의 VM의 Guest OS와 App을 독립적으로 운영될 수 있도록 관리해줌\n하지만 VM은 HOST OS의 자원을 할당을 해줘야하는 overhead문제가 있음\nDocker는 Hypervisor와 Guest OS가 필요없이 Apps을 독립적으로 구동 시킬 수 있음\nDocker에서는 하나의 App을 container라 부르고 container를 경량화된 가상화 서버라고 생각하면 됨\ncontainer는 VM만큼 완전히 독립적으로 운영할 수는 없지만 overhead를 최소화한 VM이라고 생각할 수 있음\n\n\n\n\n\nDocker Compoenent Architecture\n\n\n\n\n2 Docker Installation\n\nDocker 설치 링크\n\nUninstall old versions: for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg; done 실행\nInstall using the apt repository\n\nSet up the repository\n\nrepository update: sudo apt-get update\nDocker 설치에 필요한 사전 libraries: sudo apt-get install ca-certificates curl gnupg 실행\nAdd Docker’s official GPG key\n\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\nUse the following command to set up the repository:\n\necho \\ \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n\n\nInstall Docker Engine\n\nUpdate the apt package index: sudo apt-get update\nTo install the latest version of community edition (ce), run: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-compose-plugin은 docker-compose 기능을 사용할 수 있게함\n\nVerify that the Docker Engine Installation is successful by running the hello-world image.\n\nsudo docker run hello-world : hello world image 다운로드 받음\n\ndocker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?. 라고 뜨면 docker demon이 시작이 안되어 있기 때문에 발생하는 에러창. docker demon 띄우면 됨\n\ndocker demon 실행: sudo service docker start WSL2킬때마다 실행해야줘야함\n그래도 에러창 뜨면 sudo service docker status 실행 시켜 docker demon 켜져있는지 확인\n\n\n\n\n\n\n\n\n\n\n\n\n3 Go to Blog Content List\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n딥러닝은 다양한 알고리즘의 조합으로 수행되기 때문에 다양한 알고리즘을 정확하게 작성하기 위해서는 다수의 다양한 자료(data)를 담기 위해 사용되는 자료 구조를 이해할 필요가 있다. 즉, 자료구조는 정확한 알고리즘을 구현하기 위해 다수의 자료(data)를 담기 위한 구조이다.\n\n딥러닝 유저들간에도 자료구조를 이해하는 것에 대한 의견이 분분하지만\n올바른 자료구조를 사용하는 것은 프로그램을 조직적으로 만들 수 있는 능력을 키울 수 있다.\n데이터의 수가 많아질수록 효율적인 자료구조가 필요하다.\n예시) 학생 수가 1,000,000명 이상인 학생 관리 프로그램\n\n매일 자료 조회가 1억번 이상 발생한다면 더 빠르게 동작하는 자료 구조를 사용해야 프로그램의 효율성을 올릴 수 있다.\n\n\n\n\n\n자료구조의 필요성에 대해서 이해할 필요가 있다.\n성능 비교: 자료구조/알고리즘의 성능 측정 방법에 대해 이해할 필요가 있다.\n\nA: 적당한 속도의 삽입 & 적당한 속도의 추출 (삽입: \\(O (log N)\\) / 추출: \\(O(log N)\\))\nB: 느린 삽입 & 빠른 추출 (삽입: \\(O (N)\\) / 추출: \\(O (1)\\))\nA vs B? 상황에 따라 A를 만들지 B를 만들지 선택해야 한다. 삽입 연산이 많으면 A를, 추출 연산이 많으면 B를 택해야 한다. (속도 비교: \\(O (N) &lt; O (log N)&lt; O (1)\\))\n하지만, 실무적으로 많은 개발자들이 A를 택한다. 왜냐면 log 복잡도는 상수 복잡도와 속도가 비슷하기 때문\n\n\n\n\n\n\n\n이처럼 상황에 맞게 알고리즘의 연산 속도를 결정해야 하므로 데이터를 효과적으로 저장하고, 처리하는 방법에 대해 바르게 이해할 필요가 있다.\n자료구조를 제대로 이해해야 불필요하게 메모리와 계산을 낭비하지 않는다.\nC언어를 기준으로 정수(int) 형식의 데이터가 100만 개가량이 존재한다고 가정하자.\n해당 프로그램을 이용하면, 내부적으로 하루에 데이터 조회가 1억 번 이상 발생한다.\n이때 원하는 데이터를 가장 빠르게 찾도록 해주는 자료구조는 무엇일까?\n\n트리(tree)와 같은 자료구조를 활용할 수 있다.\n\n\n\n\n\n\n선형 자료 구조(linear data structure) 선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 하나 존재하는 자료구조이며 데이터가 일렬로 연속적으로(순차적으로) 연결되어 있다.\n\n배열(array)\n연결 리스트(linked list)\n스택(stack)\n큐(queue)\n\n비선형 자료 구조(non-linear data structure)\n비선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 여러 개 올 수 있는 자료구조이며 데이터가 일직선상으로 연결되어 있지 않아도 된다.\n\n트리(tree)\n그래프(graph)\n\n\n\n\n\n\n효율적인 자료구조 설계를 위해 알고리즘 지식이 필요하다.\n효율적인 알고리즘을 작성하기 위해서 문제 상황에 맞는 적절한 자료구조가 사용되어야 한다.\n프로그램을 작성할 때 자료구조와 알고리즘 모두 고려해야 한다.\n\n\n\n\n\n시간 복잡도(time complexity): 알고리즘에 사용되는 연산 횟수를 측정 (시간 측정)\n공간 복잡도(space complexity): 알고리즘에 사용되는 메모리의 양을 측정 (공간 측정)\n공간을 많이 사용하는 대신 시간을 단축하는 방법이 흔히 사용된다.\n프로그램의 성능 측정 방법: Big-O 표기법\n\n복잡도를 표현할 때는 Big-O 표기법을 사용한다.\n\n특정한 알고리즘이 얼마나 효율적인지 수치적으로 표현할 수 있다.\n가장 빠르게 증가하는 항만을 고려하는 표기법이다.\n\n아래의 알고리즘은 \\(O(n)\\) 의 시간 복잡도를 가진다. 왜냐면, n에 따라 summary += i의 연산 횟수가 정해지기 때문이다.\n\n\n\n\nCode\nn = 10\nsummary = 0\nfor i in range(n):\n    summary += i\nprint(summary)\n\n\n45\n\n\n\n다음 알고리즘은 \\(O (n^2)\\) 의 시간 복잡도를 가진다. 2 중 for loop은 i와 j가 n에 따라 각 각 n 번씩 연산되기때문에 \\(n \\times n\\) 회 만큼 연산된다.\n\n\n\nCode\nn = 3\nfor i in range(1, n + 1):\n    for j in range(1, n + 1):\n        print(f\"{i} X {j} = {i * j}\")\n\n\n1 X 1 = 1\n1 X 2 = 2\n1 X 3 = 3\n2 X 1 = 2\n2 X 2 = 4\n2 X 3 = 6\n3 X 1 = 3\n3 X 2 = 6\n3 X 3 = 9\n\n\n\n일반적으로 연산 횟수가 10억 (\\(1.0 \\times 10^9\\))을 넘어가면 1초 이상의 시간이 소요된다.\n[예시] n이 1,000일 때를 고려해 보자.\n\n\\(O(n)\\): 약 1,000번의 연산\n\\(O(nlogn )\\): 약 10,000번의 연산 (약 \\(log10=10\\))\n\\(O(n^2)\\): 약 1,000,000번의 연산\n\\(O(n^3)\\): 약 1,000,000,000번의 연산\n\n그러므로, 알고리즘 짤 때 코딩 레벨로 연산 횟수를 계산해서 연산 시간을 어림잡아 추정할 수 있다.\n시간 복잡도 속도 비교\n By Cmglee - Own work, CC BY-SA 4.0\nBig-O 표기법으로 시간 복잡도를 표기할 때는 가장 영향력이 큰 항만을 표시한다.\n\n\\(O(3n^2 + n) = O(n^2)\\)\n현실 세계에서는 동작 시간이 1초 이내인 알고리즘을 설계할 필요가 있다.\n실무적으로 프로그램 동작 시간이 1초 이상이면 매우 느린 것으로 간주.\n\n공간 복잡도를 나타낼 때는 MB 단위로 표기한다.\nint a[1000]: 4KB int a[1000000]: 4MB int a[2000][2000]: 16MB\n자료구조를 적절히 활용하기\n\n자료구조의 종류로는 스택, 큐, 트리 등이 있다.\n프로그램을 작성할 때는 자료구조를 적절히 활용하여 시간 복잡도를 최소화하여야 한다.\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#data-structure",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#data-structure",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "딥러닝은 다양한 알고리즘의 조합으로 수행되기 때문에 다양한 알고리즘을 정확하게 작성하기 위해서는 다수의 다양한 자료(data)를 담기 위해 사용되는 자료 구조를 이해할 필요가 있다. 즉, 자료구조는 정확한 알고리즘을 구현하기 위해 다수의 자료(data)를 담기 위한 구조이다.\n\n딥러닝 유저들간에도 자료구조를 이해하는 것에 대한 의견이 분분하지만\n올바른 자료구조를 사용하는 것은 프로그램을 조직적으로 만들 수 있는 능력을 키울 수 있다.\n데이터의 수가 많아질수록 효율적인 자료구조가 필요하다.\n예시) 학생 수가 1,000,000명 이상인 학생 관리 프로그램\n\n매일 자료 조회가 1억번 이상 발생한다면 더 빠르게 동작하는 자료 구조를 사용해야 프로그램의 효율성을 올릴 수 있다.\n\n\n\n\n\n자료구조의 필요성에 대해서 이해할 필요가 있다.\n성능 비교: 자료구조/알고리즘의 성능 측정 방법에 대해 이해할 필요가 있다.\n\nA: 적당한 속도의 삽입 & 적당한 속도의 추출 (삽입: \\(O (log N)\\) / 추출: \\(O(log N)\\))\nB: 느린 삽입 & 빠른 추출 (삽입: \\(O (N)\\) / 추출: \\(O (1)\\))\nA vs B? 상황에 따라 A를 만들지 B를 만들지 선택해야 한다. 삽입 연산이 많으면 A를, 추출 연산이 많으면 B를 택해야 한다. (속도 비교: \\(O (N) &lt; O (log N)&lt; O (1)\\))\n하지만, 실무적으로 많은 개발자들이 A를 택한다. 왜냐면 log 복잡도는 상수 복잡도와 속도가 비슷하기 때문"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료-구조의-필요성",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료-구조의-필요성",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "이처럼 상황에 맞게 알고리즘의 연산 속도를 결정해야 하므로 데이터를 효과적으로 저장하고, 처리하는 방법에 대해 바르게 이해할 필요가 있다.\n자료구조를 제대로 이해해야 불필요하게 메모리와 계산을 낭비하지 않는다.\nC언어를 기준으로 정수(int) 형식의 데이터가 100만 개가량이 존재한다고 가정하자.\n해당 프로그램을 이용하면, 내부적으로 하루에 데이터 조회가 1억 번 이상 발생한다.\n이때 원하는 데이터를 가장 빠르게 찾도록 해주는 자료구조는 무엇일까?\n\n트리(tree)와 같은 자료구조를 활용할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료-구조의-종류",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료-구조의-종류",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "선형 자료 구조(linear data structure) 선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 하나 존재하는 자료구조이며 데이터가 일렬로 연속적으로(순차적으로) 연결되어 있다.\n\n배열(array)\n연결 리스트(linked list)\n스택(stack)\n큐(queue)\n\n비선형 자료 구조(non-linear data structure)\n비선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 여러 개 올 수 있는 자료구조이며 데이터가 일직선상으로 연결되어 있지 않아도 된다.\n\n트리(tree)\n그래프(graph)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료구조와-알고리즘",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료구조와-알고리즘",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "효율적인 자료구조 설계를 위해 알고리즘 지식이 필요하다.\n효율적인 알고리즘을 작성하기 위해서 문제 상황에 맞는 적절한 자료구조가 사용되어야 한다.\n프로그램을 작성할 때 자료구조와 알고리즘 모두 고려해야 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#프로그램의-성능-측정-방법",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#프로그램의-성능-측정-방법",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "시간 복잡도(time complexity): 알고리즘에 사용되는 연산 횟수를 측정 (시간 측정)\n공간 복잡도(space complexity): 알고리즘에 사용되는 메모리의 양을 측정 (공간 측정)\n공간을 많이 사용하는 대신 시간을 단축하는 방법이 흔히 사용된다.\n프로그램의 성능 측정 방법: Big-O 표기법\n\n복잡도를 표현할 때는 Big-O 표기법을 사용한다.\n\n특정한 알고리즘이 얼마나 효율적인지 수치적으로 표현할 수 있다.\n가장 빠르게 증가하는 항만을 고려하는 표기법이다.\n\n아래의 알고리즘은 \\(O(n)\\) 의 시간 복잡도를 가진다. 왜냐면, n에 따라 summary += i의 연산 횟수가 정해지기 때문이다.\n\n\n\n\nCode\nn = 10\nsummary = 0\nfor i in range(n):\n    summary += i\nprint(summary)\n\n\n45\n\n\n\n다음 알고리즘은 \\(O (n^2)\\) 의 시간 복잡도를 가진다. 2 중 for loop은 i와 j가 n에 따라 각 각 n 번씩 연산되기때문에 \\(n \\times n\\) 회 만큼 연산된다.\n\n\n\nCode\nn = 3\nfor i in range(1, n + 1):\n    for j in range(1, n + 1):\n        print(f\"{i} X {j} = {i * j}\")\n\n\n1 X 1 = 1\n1 X 2 = 2\n1 X 3 = 3\n2 X 1 = 2\n2 X 2 = 4\n2 X 3 = 6\n3 X 1 = 3\n3 X 2 = 6\n3 X 3 = 9\n\n\n\n일반적으로 연산 횟수가 10억 (\\(1.0 \\times 10^9\\))을 넘어가면 1초 이상의 시간이 소요된다.\n[예시] n이 1,000일 때를 고려해 보자.\n\n\\(O(n)\\): 약 1,000번의 연산\n\\(O(nlogn )\\): 약 10,000번의 연산 (약 \\(log10=10\\))\n\\(O(n^2)\\): 약 1,000,000번의 연산\n\\(O(n^3)\\): 약 1,000,000,000번의 연산\n\n그러므로, 알고리즘 짤 때 코딩 레벨로 연산 횟수를 계산해서 연산 시간을 어림잡아 추정할 수 있다.\n시간 복잡도 속도 비교\n By Cmglee - Own work, CC BY-SA 4.0\nBig-O 표기법으로 시간 복잡도를 표기할 때는 가장 영향력이 큰 항만을 표시한다.\n\n\\(O(3n^2 + n) = O(n^2)\\)\n현실 세계에서는 동작 시간이 1초 이내인 알고리즘을 설계할 필요가 있다.\n실무적으로 프로그램 동작 시간이 1초 이상이면 매우 느린 것으로 간주.\n\n공간 복잡도를 나타낼 때는 MB 단위로 표기한다.\nint a[1000]: 4KB int a[1000000]: 4MB int a[2000][2000]: 16MB\n자료구조를 적절히 활용하기\n\n자료구조의 종류로는 스택, 큐, 트리 등이 있다.\n프로그램을 작성할 때는 자료구조를 적절히 활용하여 시간 복잡도를 최소화하여야 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/WSL/01.wsl-install.html",
    "href": "docs/blog/posts/Engineering/WSL/01.wsl-install.html",
    "title": "WSL Install",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nWindows Subsystem for Linux은 Windows에서 리눅스 실행환경을 지원하는 Windows의 확장 기능\n\nWindows에서 바로 리눅스 명령어를 실행할 수 있어서, Windows와 리눅스를 함께 사용하는 개발자들에게 편리\nWSL 이전에는 가상 머신 (Virtual Machine, VM)을 많이 사용했었음\n\nVM: 컴퓨터 안에 구축된 가상 컴퓨터 개념으로 CPU, Memory, Network Interface, and Storage를 갖춘 온전한 컴퓨터 시스템으로 작동하는 가상환경\n하지만 VM은 메모리 overhead가 심한 문제점이 있었음\n\noverhead: 컴퓨터가 어떤 연산 및 처리를 하기 위해 들어가는 간접적인 처리시간 메모리등을 말함\n예를 들어, VM을 쓰려면 컴퓨터가 디스크와 메모리의 일정 이상 부분을 할당해줘야 VM을 쓸 수 있었음\n그래서 VM이 많을 수록 overhead가 심해지는 현상이 발생했는데 WSL 개발 이후로 Linux를 더 가볍게 사용할 수 있게됐음\n\n\n\n\n\n\n\nAirflow는 Windows에 직접 설치 불가\nWindows에서 리눅스 작업환경을 만들기 위해서 WSL 설치가 필수\n여유가 된다면 가상화 VM 또는 Public Cloud (AWS, GCP, Azure)의 컴퓨팅 서비스에서 Linux 및 Airflow 설치 가능\n\n\n\n\n\n설치 전 체크사항 (시작버튼 → 시스템 정보에서 확인)\n\nWindows 10 버전 2004 이상\nWindows 11\n\nPowerShell 명령어로 설치\n\nwsl –install\n\n\n\n\nWSL Install 설명 공식 홈페이지\n\n\n\nOpen PowerShell or Windows Command Prompt in administrator mode by right-clicking and selecting “Run as administrator”\nwsl --install 실행\nEnter New UNIX username/password\nturn off PowerShell\nturn on PowerShell again\nwsl -l -v 실행 반드시 version 2가 설치되어 있어야함 (WSL2)\n\nWSL1이 설치되었으면 WSL2 업그래이드 해야함 (windows update 해야함)\nWSL1은 나중에 사용할 docker가 제대로 작동하지 않음\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/WSL/01.wsl-install.html#wsl-windows-subsystem-for-linux",
    "href": "docs/blog/posts/Engineering/WSL/01.wsl-install.html#wsl-windows-subsystem-for-linux",
    "title": "WSL Install",
    "section": "",
    "text": "Windows Subsystem for Linux은 Windows에서 리눅스 실행환경을 지원하는 Windows의 확장 기능\n\nWindows에서 바로 리눅스 명령어를 실행할 수 있어서, Windows와 리눅스를 함께 사용하는 개발자들에게 편리\nWSL 이전에는 가상 머신 (Virtual Machine, VM)을 많이 사용했었음\n\nVM: 컴퓨터 안에 구축된 가상 컴퓨터 개념으로 CPU, Memory, Network Interface, and Storage를 갖춘 온전한 컴퓨터 시스템으로 작동하는 가상환경\n하지만 VM은 메모리 overhead가 심한 문제점이 있었음\n\noverhead: 컴퓨터가 어떤 연산 및 처리를 하기 위해 들어가는 간접적인 처리시간 메모리등을 말함\n예를 들어, VM을 쓰려면 컴퓨터가 디스크와 메모리의 일정 이상 부분을 할당해줘야 VM을 쓸 수 있었음\n그래서 VM이 많을 수록 overhead가 심해지는 현상이 발생했는데 WSL 개발 이후로 Linux를 더 가볍게 사용할 수 있게됐음"
  },
  {
    "objectID": "docs/blog/posts/Engineering/WSL/01.wsl-install.html#why-to-install-wsl",
    "href": "docs/blog/posts/Engineering/WSL/01.wsl-install.html#why-to-install-wsl",
    "title": "WSL Install",
    "section": "",
    "text": "Airflow는 Windows에 직접 설치 불가\nWindows에서 리눅스 작업환경을 만들기 위해서 WSL 설치가 필수\n여유가 된다면 가상화 VM 또는 Public Cloud (AWS, GCP, Azure)의 컴퓨팅 서비스에서 Linux 및 Airflow 설치 가능"
  },
  {
    "objectID": "docs/blog/posts/Engineering/WSL/01.wsl-install.html#how-to-install-wsl",
    "href": "docs/blog/posts/Engineering/WSL/01.wsl-install.html#how-to-install-wsl",
    "title": "WSL Install",
    "section": "",
    "text": "설치 전 체크사항 (시작버튼 → 시스템 정보에서 확인)\n\nWindows 10 버전 2004 이상\nWindows 11\n\nPowerShell 명령어로 설치\n\nwsl –install\n\n\n\n\nWSL Install 설명 공식 홈페이지\n\n\n\nOpen PowerShell or Windows Command Prompt in administrator mode by right-clicking and selecting “Run as administrator”\nwsl --install 실행\nEnter New UNIX username/password\nturn off PowerShell\nturn on PowerShell again\nwsl -l -v 실행 반드시 version 2가 설치되어 있어야함 (WSL2)\n\nWSL1이 설치되었으면 WSL2 업그래이드 해야함 (windows update 해야함)\nWSL1은 나중에 사용할 docker가 제대로 작동하지 않음"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nWSL Installation\nFrequently Used Linux Commands\n\n\n\n\n\nDocker Installation\ndocker를 사용하기 전에 sudo service docker start 실행해야 함\ndocker 설치 확인\n\nsudo docker run hello-world 하면 다음과 같은 메세지 떠야함\n\n먼저 docker는 local에서 실행하고자 하는 image를 찾고, 없으면 가장 최신 version의 image를 download 받는다.\n하지만, 위의 그림에 있는 메세지와 같이 이미 실행하고자 하는 image가 있어 다운로드 받을 필요가 없으면 image를 찾고 다운로드 받았다는 메세지는 생략되게 된다.\n\n\n\n\n\n\nAirflow 설치 방법은 여러가지가 존재하며 그 중 하나가 도커 설치임\n도커 컴포즈 (docker compose)를 이용하여 한번에 쉽게 설치 가능\n\nDocker Compose를 이용하여 Airflow 설치 링크\ndocker compose: 여러 개의 도커 컨테이너 설정을 한방에 관리하기 위한 도커 확장 기술로 에어플로우를 설치하기 위한 도커 컨테이너 세팅 내용이 들어있음\n\nairflow 자체도 여러개의 docker containers로 구성됨\n\nDocker Compose를 이용하여 Airflow 설치 링크의 Fetching docker-compose.yaml 부터 시작\n\ncurl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.6.1/docker-compose.yaml'\nls로 docker-compose.yaml 다운로드됐는지 확인\nvi * 로 docker-compose.yaml 열어보면 주석처리와 airflow의 설정내용들을 확인할 수 있다.\n\nSetting the right Airflow user\n\ndirectories 만들기: mkdir -p ./dags ./logs ./plugins ./config\n.env 파일 만들기: echo -e \"AIRFLOW_UID=$(id -u)\" &gt; .env\nvi .env: AIRFLOW_UID=1000 인 이유는 OS 계정의 uid가 1000이라는 뜻\n\nInitialize the database\n\nsudo docker compose up airflow-init: sudo 반드시 앞에 붙여야함. exited with code 0가 떠야 정상적으로 설치 된 것임\n\n\n\n\n\n\n\nservice 띄우기\n\nsudo docker compose up 실행. sudo 반드시 앞에 붙여야함.\n\nhttp 상태가 계속해서 업데이트 되야 airflow가 돌아가고 있는 것임. 계속해서 update되는 http command 닫으면 airflow멈춤. 두번째 터미널 열어서 작업해야함\n두 번째 터미널 열고 sudo docker ps 실행하여 container list 상태 확인. 총 6개 올라와야 정상\n\nairflow-airflow-worker-1 : scheduler로 부터 부여된 tasks 수행\nairflow-airflow-triggerer-1\nairflow-airflow-webserver-1\nairflow-airflow-scheduler-1 : tasks와 dags을 monitoring\nairflow-postgres-1\nairflow-redis-1 : for queue service, scheduler에서 worker로 message 전달\n\n웹 브라우저 창에 localhost:8080 입력하여 airflow service창에 접속\n\ndefault ID/PW: airflow/airflow\n웹 브라우저에서 local로 airflow service 접속 원리\n\n웹 브라우저는 local PC에 있음\nairflow는 WSL안에 docker container로 실행되고 있음\n이렇게 2개의 다른 공간이 연결될 수 있는 이유는 WSL은 기본적으로 local PC의 local host IP와 연결이 되어 있음\n그래서 웹 브라우저에서 local로 localhost:8080 라고 입력하면 WSL에서 8080 port를 입력하는 것과 같은 효과가 있기 때문에 local 웹브라우저에서 WSL container로 들어갈 수 있는 것임.\nsample DAGs이 만들어져 있는 것을 확인 할 수 있음\n\nAirflow Webserver (or Web Browser) Port 변경\n\nAirflow의 default port 는 8080으로 설정되어 있는데 만약 port를 변경하고 싶으면 docker-compose.yaml을 수정해야한다. 다음은 docker-compose.yaml의 airflow webserver 부분을 복붙한 것이다. &lt;변경전 with comments&gt;\n\n  airflow-webserver:\n    &lt;&lt;: *airflow-common\n    command: webserver\n    ports:\n      - \"8080(local port 변경하고자 하는 부분):8080 (airflow의 healthcheck.test의 port)\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n      interval: 10s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n    networks:\n      network_custom:\n        ipv4_address: 172.28.0.6\n&lt;변경후&gt; markdown     airflow-webserver:       &lt;&lt;: *airflow-common       command: webserver       ports:         - \"8787:8080\"       healthcheck:         test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]         interval: 10s         timeout: 10s         retries: 5         start_period: 30s       restart: always       depends_on:         &lt;&lt;: *airflow-common-depends-on         airflow-init:           condition: service_completed_successfully       networks:         network_custom:           ipv4_address: 172.28.0.6 위 처럼 변경하면 되지만 나는 그냥 기본 port인 8080을 사용한다.\n\nexample_bash_operator DAG을 들어가 보면\n\nGrid: 수행 이력을 보여주는 tab\n\nDAG 이름 example_bash_operator 옆에 pause toggle를 활성화 시키면 unpaused 됨\nAuto-regresh toggle 활성화 시키면 DAG이 한번 돌아감\n\nGraph: DAG을 구성하고 있는 tasks를 보여주는 tab. 각 각의 task가 색 별로 상태를 보여주고 있음\nCalendar: 참고할 것\nTask Duration: 참고할 것\nTask Tries: 참고할 것\nLanding Times: 참고할 것\nGantt: 참고할 것\nDetails: 참고할 것\nCode: DAG을 구성하고 있는 python code를 볼 수 있음\nAudit Log: 참고할 것\n\n\n\n\n\n\n\n\nCPU: 4Core 이상\nMemory: 16GB (권장-문제없음) / 8GB (최소-약간 버벅 거림)\nWSL에서 다수의 컨테이너 실행시 메모리 점유율 상승할 수 있음\n\nairflow service창과 WSL 창 닫고 다시 키면 어느 정도 메로리 점유율 낮아짐\n\n\n\n\n\n\nuser가 만든 DAG이 airflow까지 전달되는 workflow가 아래와 같이 묘사되어 있다.\n\n\n\n\n개발 환경 workflow\n\n\n\n위의 그림에서 보면 6 containers가 있고 airflow setting 할때 dags, logs, plugins, config directories를 만들었는데 모두 airflow containers에 연결되어 있음\n\nmount 의미: directory안에 file을 넣으면 containers가 file을 인식할 수 있음\nuser가 만든 dag을 dags directory에 넣으면 airflow container가 dags안에 있는 dag을 인식하여 서비스에 띄어줌\n\n개발환경 세팅의 목표\n\n로컬 환경에서 만든 dag을 dags directory에 배포하여 containers가 user가 만든 dag을 인식하여 airflow서비스까지 띄우는 것이 목표\n다시 말해서, 그냥 로컬 환경에서 만든 dag을 dags directory에 배포하면 됨\n\nActions\n\n로컬 컴퓨터에 python interpreter 설치\n\n아무 python version을 설치하면 안되고 airflow containers가 쓰고있는 python version과 일치시켜야 함!\n\nIDE Tool(VScode) 개발환경 설정\nGithub 레파지토리 생성\n로컬 컴퓨터에 Python Airflow Libraries 설치\nWSL에 Git 설치 및 git pull이 가능한 환경구성\n\ngit repository에 DAG을 만들어 push하여 dags directory에 pull이 되어 dag이 들어가게 하면 됨.\n\n\n\n\n\n\n\nActions\n\n컨테이너에서 사용하는 파이썬 버전 확인\n\ncontainer안에 들어가기: sudo docker exec -it {container-name or container-id} 명령어 \\(\\rightarrow\\) sudo docker exec -it airflow-airflow-worker-1 bash: -it는 session이 안 끊어지도록 유지해주는 옵션\npython -V 실행하여 python version 확인 : 현재 나의 python version은 Python 3.7.16\nctrl D로 exit\n\n파이썬 인터프리터 다운로드\n\n보안상의 업데이트 말곤 기능이 같기 때문에 Python 3.7.16대신 Python 3.7.9 설치하면 됨\n\n로컬 컴퓨터에 파이썬 설치\n\nconda에 설치하고 싶으면 conda create -n airflow python=3.7.9 or\nglobal 환경에 설치하고 싶으면 Windows x86-64 executable installer 다운로드 및 설치\n\n\n\n\n\n\n\nVScode란?\n\nMicrosoft사에서 2015년에 제작, 다양한 언어 개발을 돕는 IDE tool\nVisual Studio 라는 IDE 툴과는 엄연히 다른 툴\n\nActions\n\nVScode 다운로드\n\n설치 마법사에서 추가 작업 선택란에 code로 열기 작업을 windows탐색기 파일의 상황에 맞는 메뉴에 추가 선택할 것. programming file을 열때 VScode가 디폴트가 되도록함\n\nVScode 설치, 파이썬 확장팩 설치\n프로젝트 생성, 파이썬 가상환경 설정\n\nVScode가 file이나 directory단위로 관리하는 IDE tool이라 프로젝트 생성 개념이 없음\nwindows에 프로젝트 directory하나 만들고 VScode에서 open folder로 열면 그 folder를 최상위 folder로 인식 (project 생성됨)\n\npython interpreter 설정\n\nVScode &gt; Terminal &gt; New Terminal &gt; python version 확인\n\n\n파이썬 가상환경\n\n라이브러리 버전 충돌 방지를 위해 설치/사용되는 파이썬 인터프리터 환경을 격리시키는 기술\n파이썬은 라이브러리 설치 시점에 따라서도 설치되는 버전이 상이한 경우가 많음\n\n\n\n\n가상 환경의 필요성\n\n\n\npython을 global 환경에 설치할 경우 위의 그림처럼 C,D프로젝트가 동시에 진행될 때 둘 중하나의 library version이 차이가 나면 old version의 library 로 진행되는 프로젝트는 에러가 발생함\n\n2개의 다른 프로젝트가 같은 python interpreter를 바라보고 library를 설치하기 때문에 종속성 문제가 생김 (library 충돌 발생)\n그래서 다른 가상환경 venv안에 다른 프로젝트를 할당해서 독립적으로 프로젝트를 진행하는게 일반적임\n\npython 가상환경 만들기\n\nconda로 만들 경우 conda 설치 후 만들면 됨. 설치 링크\npython에 있는 가상환경 생성 기능으로 만들 경우 python -m airflow ./venv 실행\n\n./venv directory에 python 설치하고 version 관리하겠다는 의미\n\n\nVScode가 python 가상환경 참조하도록 설정\n\nhelp&gt;show all commands or ctrl+shift+p 누른후 interpreter 입력하여 가상환경에 있는 python 클릭\n\nterminal 에서 가상환경 잘 잡혔는지 확인\n\n\n\n\n\n\nGit Installation & Environment Setting\n\n\n\n\n\nAirflow 라이브러리 설치 대상과 설치 이유\n\n설치 대상: 로컬 컴퓨터의 파이썬 가상환경(본인의 경우: airflow)\nWhy? Airflow DAG 개발을 위해 Airflow의 python class files 및 라이브러리들이 많기 때문에 필요\n\nAirflow 라이브러리 설치 가이드\n\nconda activate airflow 가상환경으로 들어감\npip install \"apache-airflow[celery]==2.6.1\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.6.1/constraints-3.7.txt\"\n\n리눅스에서 파이썬 Airflow 라이브러리 설치시 그 자체로 Airflow 서비스 사용 가능\n\n하지만 WSL에서 pip install 명령으로 Airflow를 설치하지 않는 이유?\npip install 로 Airflow 설치시 저사양의 아키텍처로 설치되며 여러 제약이 존재함 (Task를 한번에 1개씩만 실행 가능 등)\n그러므로 docker로 설치해야 제약이 없음\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html#개발-환경-권장-사양",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html#개발-환경-권장-사양",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "CPU: 4Core 이상\nMemory: 16GB (권장-문제없음) / 8GB (최소-약간 버벅 거림)\nWSL에서 다수의 컨테이너 실행시 메모리 점유율 상승할 수 있음\n\nairflow service창과 WSL 창 닫고 다시 키면 어느 정도 메로리 점유율 낮아짐"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html#개발-환경-workflow",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html#개발-환경-workflow",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "user가 만든 DAG이 airflow까지 전달되는 workflow가 아래와 같이 묘사되어 있다.\n\n\n\n\n개발 환경 workflow\n\n\n\n위의 그림에서 보면 6 containers가 있고 airflow setting 할때 dags, logs, plugins, config directories를 만들었는데 모두 airflow containers에 연결되어 있음\n\nmount 의미: directory안에 file을 넣으면 containers가 file을 인식할 수 있음\nuser가 만든 dag을 dags directory에 넣으면 airflow container가 dags안에 있는 dag을 인식하여 서비스에 띄어줌\n\n개발환경 세팅의 목표\n\n로컬 환경에서 만든 dag을 dags directory에 배포하여 containers가 user가 만든 dag을 인식하여 airflow서비스까지 띄우는 것이 목표\n다시 말해서, 그냥 로컬 환경에서 만든 dag을 dags directory에 배포하면 됨\n\nActions\n\n로컬 컴퓨터에 python interpreter 설치\n\n아무 python version을 설치하면 안되고 airflow containers가 쓰고있는 python version과 일치시켜야 함!\n\nIDE Tool(VScode) 개발환경 설정\nGithub 레파지토리 생성\n로컬 컴퓨터에 Python Airflow Libraries 설치\nWSL에 Git 설치 및 git pull이 가능한 환경구성\n\ngit repository에 DAG을 만들어 push하여 dags directory에 pull이 되어 dag이 들어가게 하면 됨."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html#python-interpreter-installation",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html#python-interpreter-installation",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "Actions\n\n컨테이너에서 사용하는 파이썬 버전 확인\n\ncontainer안에 들어가기: sudo docker exec -it {container-name or container-id} 명령어 \\(\\rightarrow\\) sudo docker exec -it airflow-airflow-worker-1 bash: -it는 session이 안 끊어지도록 유지해주는 옵션\npython -V 실행하여 python version 확인 : 현재 나의 python version은 Python 3.7.16\nctrl D로 exit\n\n파이썬 인터프리터 다운로드\n\n보안상의 업데이트 말곤 기능이 같기 때문에 Python 3.7.16대신 Python 3.7.9 설치하면 됨\n\n로컬 컴퓨터에 파이썬 설치\n\nconda에 설치하고 싶으면 conda create -n airflow python=3.7.9 or\nglobal 환경에 설치하고 싶으면 Windows x86-64 executable installer 다운로드 및 설치"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html#vscode-installation",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html#vscode-installation",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "VScode란?\n\nMicrosoft사에서 2015년에 제작, 다양한 언어 개발을 돕는 IDE tool\nVisual Studio 라는 IDE 툴과는 엄연히 다른 툴\n\nActions\n\nVScode 다운로드\n\n설치 마법사에서 추가 작업 선택란에 code로 열기 작업을 windows탐색기 파일의 상황에 맞는 메뉴에 추가 선택할 것. programming file을 열때 VScode가 디폴트가 되도록함\n\nVScode 설치, 파이썬 확장팩 설치\n프로젝트 생성, 파이썬 가상환경 설정\n\nVScode가 file이나 directory단위로 관리하는 IDE tool이라 프로젝트 생성 개념이 없음\nwindows에 프로젝트 directory하나 만들고 VScode에서 open folder로 열면 그 folder를 최상위 folder로 인식 (project 생성됨)\n\npython interpreter 설정\n\nVScode &gt; Terminal &gt; New Terminal &gt; python version 확인\n\n\n파이썬 가상환경\n\n라이브러리 버전 충돌 방지를 위해 설치/사용되는 파이썬 인터프리터 환경을 격리시키는 기술\n파이썬은 라이브러리 설치 시점에 따라서도 설치되는 버전이 상이한 경우가 많음\n\n\n\n\n가상 환경의 필요성\n\n\n\npython을 global 환경에 설치할 경우 위의 그림처럼 C,D프로젝트가 동시에 진행될 때 둘 중하나의 library version이 차이가 나면 old version의 library 로 진행되는 프로젝트는 에러가 발생함\n\n2개의 다른 프로젝트가 같은 python interpreter를 바라보고 library를 설치하기 때문에 종속성 문제가 생김 (library 충돌 발생)\n그래서 다른 가상환경 venv안에 다른 프로젝트를 할당해서 독립적으로 프로젝트를 진행하는게 일반적임\n\npython 가상환경 만들기\n\nconda로 만들 경우 conda 설치 후 만들면 됨. 설치 링크\npython에 있는 가상환경 생성 기능으로 만들 경우 python -m airflow ./venv 실행\n\n./venv directory에 python 설치하고 version 관리하겠다는 의미\n\n\nVScode가 python 가상환경 참조하도록 설정\n\nhelp&gt;show all commands or ctrl+shift+p 누른후 interpreter 입력하여 가상환경에 있는 python 클릭\n\nterminal 에서 가상환경 잘 잡혔는지 확인"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html#git-environment-setting",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html#git-environment-setting",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "Git Installation & Environment Setting"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html#airflow-library-installation",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html#airflow-library-installation",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "Airflow 라이브러리 설치 대상과 설치 이유\n\n설치 대상: 로컬 컴퓨터의 파이썬 가상환경(본인의 경우: airflow)\nWhy? Airflow DAG 개발을 위해 Airflow의 python class files 및 라이브러리들이 많기 때문에 필요\n\nAirflow 라이브러리 설치 가이드\n\nconda activate airflow 가상환경으로 들어감\npip install \"apache-airflow[celery]==2.6.1\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.6.1/constraints-3.7.txt\"\n\n리눅스에서 파이썬 Airflow 라이브러리 설치시 그 자체로 Airflow 서비스 사용 가능\n\n하지만 WSL에서 pip install 명령으로 Airflow를 설치하지 않는 이유?\npip install 로 Airflow 설치시 저사양의 아키텍처로 설치되며 여러 제약이 존재함 (Task를 한번에 1개씩만 실행 가능 등)\n그러므로 docker로 설치해야 제약이 없음"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#python",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#python",
    "title": "Content List, Engineering",
    "section": "8 python",
    "text": "8 python\n\n2023-06-01, Package Management: requirements.txt\n2023-07-01, Pathlib Library: File System Path Management"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Python/package_management.html",
    "href": "docs/blog/posts/Engineering/Python/package_management.html",
    "title": "Package Management - 1",
    "section": "",
    "text": "여러 projects를 오랫동안 관리하다 보면 다수의 packages를 설치해야하는데 이 와중에 PC 포맷, fork, cloning과 같은 code를 옮겨야하는 일이 생길 수 있다. 이렇게 새로운 환경에서 이전에 관리하던 projects를 재현해야하는데 수 많은 packages를 하나 하나씩 재설치해야하는 것은 여간 번거로운 작업이 아니다.\n\npip list를 이용한 pip로 설치된 패키지 조회\n\n아래와 같이 package list들이 출력이 된다 (일부 스크린샷함). 이 많은 packages를 하나 하나씩 conda install package_name 실행하여 설치할 순 없다. \n\npip freeze &gt; requirements.txt 명령어를 실행하여 requirements.txt 를 만들어 준다. 이 txt 파일 안에는 packages의 이름과 version 정보까지 기록되어 있는 것을 확인할 수 있다.\npip install -r requirements.txt 명령어를 실행하여 requirements.txt 에 있는 packages를 한꺼번에 설치할 수 있게 된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/Python/package_management.html#requirements.txt-를-이용한-packages-관리",
    "href": "docs/blog/posts/Engineering/Python/package_management.html#requirements.txt-를-이용한-packages-관리",
    "title": "Package Management - 1",
    "section": "",
    "text": "여러 projects를 오랫동안 관리하다 보면 다수의 packages를 설치해야하는데 이 와중에 PC 포맷, fork, cloning과 같은 code를 옮겨야하는 일이 생길 수 있다. 이렇게 새로운 환경에서 이전에 관리하던 projects를 재현해야하는데 수 많은 packages를 하나 하나씩 재설치해야하는 것은 여간 번거로운 작업이 아니다.\n\npip list를 이용한 pip로 설치된 패키지 조회\n\n아래와 같이 package list들이 출력이 된다 (일부 스크린샷함). 이 많은 packages를 하나 하나씩 conda install package_name 실행하여 설치할 순 없다. \n\npip freeze &gt; requirements.txt 명령어를 실행하여 requirements.txt 를 만들어 준다. 이 txt 파일 안에는 packages의 이름과 version 정보까지 기록되어 있는 것을 확인할 수 있다.\npip install -r requirements.txt 명령어를 실행하여 requirements.txt 에 있는 packages를 한꺼번에 설치할 수 있게 된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nDAG\n\n\n\nBash_Operator\n\nBash_Operator\n\n\n\nTask1\n\nTask1\n\n\n\nBash_Operator-&gt;Task1\n\n\n\n\n\nPython_Operator\n\nPython_Operator\n\n\n\nTask2\n\nTask2\n\n\n\nPython_Operator-&gt;Task2\n\n\n\n\n\nS3_Operator\n\nS3_Operator\n\n\n\nTask3\n\nTask3\n\n\n\nS3_Operator-&gt;Task3\n\n\n\n\n\nGCS_Operator\n\nGCS_Operator\n\n\n\nTask4\n\nTask4\n\n\n\nGCS_Operator-&gt;Task4\n\n\n\n\n\n\n\n\n\n\n\nworkflow = DAG\nOpeartor\n\n특정 행위를 할 수 있는 기능을 모아 놓은 클래스 또는 설계도\n\nTask\n\noperator가 객체화(instantiation)되어 DAG에서 실행 가능한 object\n방향성을 갖고 순환되지 않음 (DAG)\n\nBash Operator\n\nLinux에서 shell script 명령을 수행하는 operator\n\nPython Operator\n\npython 함수를 실행하는 operator\n\nS3 Operator\n\nAWS의 S3 solution (object storage)을 control할 수 있는 operator\n\nGCS Operator\n\nGCP의 GCS solution (object storage)을 control할 수 있는 operator\n\noperators을 사용하여 dags을 작성하여 git을 통해 배포한다.\ndag 작성 및 배포\n  from __future__ import annotations\n\n  import datetime # python에는 datatime이라는 data type이 있음\n  import pendulum # datetime data type을 처리하는 library\n\n  from airflow import DAG\n  from airflow.operators.bash import BashOperator\n  from airflow.operators.empty import EmptyOperator\n\n  with DAG(\n      dag_id=\"dags_bash_operator\", \n      # airflow service web 상에서 보여지는 이름, python file명과는 무관하지만 \n      # 실무에서는 일반적으로 python 파일명과 dag_id는 일치시키는 것이 다수의 dags 관리에 편리하다.\n      schedule=\"0 0 * * *\", # \"분 시 일 월 요일\", cron schedule로서 매일 0분 0시에 실행\n      start_date=pendulum.datetime(2023, 6, 9, tz=\"Asia/Seoul\"), #dags이 언제 실행될지 설정\n      # UTC: 세계 표준시로 한국 보다 9시간이 느림. Asia/Seoul로 변경해야 지정한 날짜에 0분 0시에 실행될 수 있다.\n      catchup=False, # start_date를 현재보다 과거로 설정하게 될 경우 \n      # catchup=True면 과거 부터 현재까지 소급해서 실행. \n      # 시간 순서대로 실행하는게 아니라 병렬로 한번에 실행하기 때문에 메모리를 많이 잡아먹을 수 있음. \n      # 그래서 보통 False로 처리. catchup=False면 현재부터만 실행\n      # dagrun_timeout=datetime.timedelta(minutes=60), # dag이 60분 이상 구동시 실패가 되도록 설정\n      # tags=[\"example\", \"example2\"], #airflow service web browser상 dag의 tag를 의미. 즉 dag id 바로 밑 파란색 박스를 의미. tag를 누르면 같은 tag를 가진 dags들만 filtering돼서 선택됨 \n      ## dags 이 수 백개가 될 때 tag로 filtering 하면 용이함 \n      # params={\"example_key\": \"example_value\"}, # as dag: 이하 tasks를 정의할 때, \n      ## tasks에 공통 passing parameters가 있을 때 씀\n  ) as dag:\n      # [START how to_operator_bash]\n      bash_task1 = BashOperator(\n          task_id=\"bash_task1\", # airflow web service의 dag graph에 표시될 task명\n          # task역시 task object name (bash_task1)과 task_id(bash_task1)를 일치시키는 것이 좋음\n          bash_command=\"echo this task works well!\",\n          # bash_command 이하는 shell script를 적어주면 됨\n      )\n      # [END how to_operator_bash]\n      bash_task2 = BashOperator(\n          task_id=\"bash_task2\",  \n          bash_command=\"echo $HOSTNAME\", # $HOSTNAME: HOSTNAME 환경변수 호출\n          # WSL terminal 이름이 출력된다.\n      )\n      bash_task1 &gt;&gt; bash_task2 # 수행될 tasks의 관계 설정\n배포된 dags을 airflow containers과 연결 시키기 위해 docker-compose.yaml 실행\n\nvi docker-compose.yaml 실행 후 docker-compose.yaml 안에서 Volumns 항목이 wsl의 directory와 container directory를 연결(mount)해주는 요소\n\nVolumes\n  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n\n위와 같이 Volumns 항목이 뜨는데 :을 기준으로 왼쪽이 WSL directories(volumns), 오른쪽이 Docker container directories(volumns)\n다른 WSL창을 열어 echo ${AIRFLOW_PROJ_DIR:-.} 실행하면 AIRFLOW_PROJ_DIR에 값이 없기 때문에 . 출력됨\n\nAIRFLOW_PROJ_DIR:-. : shell script문법으로 AIRFLOW_PROJ_DIR에 값이 있으면 출력하고 없으면 .을 출력하라는 의미\necho AIRFLOW_PROJ_DIR: 아무것도 출력 안됨\n\n${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags는 ./dags를 /opt/airflow/dags에 연결시키라는 의미\n\n./: docker-compose.yaml이 위치하고있는 현재 directory를 의미\n\n배포된 dags를 자동으로 docker container에 연동시키기 위해 Volumns을 다음과 같이 편집\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n\ndirectory hierarchy에 따라 위의 volumes path를 다르게 설정해야한다. docker service web browse(i.e. localhost:8080) 껏다 켜가면서 확인하면서 설정\n\n새로운 dags 배포할 때마다 airflow service 껐다가 켜야 한다.\n\nairflow service 껐다 켜서 잘 반영됐는지 확인\n\ndocker가 설치된 wsl directory이동 먼저 할 것\nairflow service 끄기: sudo docker compose down\nairflow service 켜기: sudo docker compose up\n\nairflow web service상에서 dags이 잘 mount 되었는지 확인\n\n기본적으로 dags은 airflow web service상에 올라올 때 unpaused 상태로 올라옴\n하지만 schedule이 걸려있는 dags은 unpaused상태에서 한번 돌고 올라옴\ndag을 클릭하면 긴 녹색 막대기를 누르면 수행된 schedule내용이 나오고\n각 각의 task에 대응되는 녹색 네모 박스를 누르면 결과들을 조회할 수 있다.\n\n네모 박스를 누르고 log (audit log 아님)를 누르면 결과가 자세히 조회된다.\nbash_task2 의 bash_command=\"echo $HOSTNAME\" 의 결과값으로 조회된 값은 docker worker container id 를 의미한다.\n\n하지만 본인의 경우, airflow web service에서 794f3b56824a가 출력된 것을 확인했고\nsudo docker ps로 container ID를 확인한 결과 airflow-airflow-worker-1 의 32092b201878 로 달랐다.\n\n실제 worker container로 들어가 echo $HOSTNAME 실행하면 worker container id 출력되어야 함\n\nworker container로 들어가기: sudo docker exec -it container-name bash \\(\\rightarrow\\) 본인의 경우: sudo docker exec -it airflow-airflow-worker-1 bash 이 과정이 dag을 돌린과정과 같은 mechanism임\necho $HOSTNAME 실행 : 32092b201878 출력됨 (어쨌든 airflow web service상의 794f3b56824a와 달랐음)\nsudo docker exec -it 794f3b56824a bash 결과 Error response from daemon: No such container: 794f3b56824a 라는 에러메세지 뜸\n\n즉, worker container가 실제 task를 처리하는 것을 볼 수 있었다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Process\n\n\n\nScheduler\n\nScheduler\n\n\n\nScheduler-&gt;Scheduler\n\n\n3.check start time\n\n\n\nDAG_file\n\nDAG_file\n\n\n\nScheduler-&gt;DAG_file\n\n\n1.parsing\n\n\n\nQueue\n\nQueue\n\n\n\nScheduler-&gt;Queue\n\n\n\n\n\nMeta_DB\n\nMeta_DB\n\n\n\nScheduler-&gt;Meta_DB\n\n\n2.save information\n\n\n\nWorker\n\nWorker\n\n\n\nDAG_file-&gt;Worker\n\n\n5.Processing after reading\n\n\n\nWorker-&gt;Meta_DB\n\n\n6.Results update\n\n\n\nQueue-&gt;Worker\n\n\n4.start instruction\n\n\n\n\n\n\n\n\n\nscheduler\n\nairflow에서 brain역할\n\nparsing: a user가 만든 dag 파일을 읽어들여 문법적 오류 여부와 tasks 간의 관계를 분석\nsave information: DAG Parsing 후 DB에 정보저장 (tasks, task relations, schedule, etc.)\ncheck start time: DAG 시작 실행 시간 확인\nstart instruction: DAG 시작 실행 시간마다 worker에 실행 지시\n\n\nscheduler와 workder 사이에 queue 상태가 있을 수 있음\n\n\nworker (Worker Container)\n\nairflow 처리 주체 (subject)\n\nProcessing after reading: scheduler가 시킨 DAG 파일을 찾아 읽고 처리\nResults update: 처리가 되기 전/후를 Meta DB에 update함\n\n\n\n\n\n\n\n\n\n\ntask가 실행되어야 하는 시간(주기)을 정하기 위한 다섯개의 필드로 구성된 문자열\nCron을 이용하면 왠만한 scheduling 모두 가능\n\n{minutes} {hour} {day} {month} {weekday}\n\n\n\n\n\n\n\n\nNumber\nSpecial Characters\nDescription\n\n\n\n\n1\n*\n모든 값\n\n\n2\n-\n범위 지정\n\n\n3\n,\n여러 값 지정\n\n\n4\n/\n증가값 지정. staring-value/ending-value\n\n\n5\nL\n마지막 값 (일, 요일에만 설정 가능)  * 일에 L 입력시 해당 월의 마지막 일 의미  ※ 요일에 L 입력시 토요일 의미\n\n\n6\n#\n몇 번째 요일인지 지정\n\n\n\n\n\n\n\n\n\n\n\nCron schedule\nDescription\nNote\n\n\n\n\n15 2 * * *\n매일 02시 15분에 도는 daily batch\n\n\n\n0 * * * *\n매시 정각에 도는 시간 단위 batch\n\n\n\n0 0 1 * *\n매월 1일 0시 0분 도는 monthly batch\n\n\n\n10 1 * * 1\n매주 월요일 1시 10분에 도는 weekly batch\n0: 일요일, 1: 월요일, 2: 화요일, 3:수요일, 4: 목요일, 5: 금요일, 6: 토요일\n\n\n0 9-18 * * *\n매일 9시부터 18시까지 정각마다 도는 daily batch\n보통 이렇게 scheduling하지는 않음. 하지만 구현할 수 있음\n\n\n0 1 1,2,3 * *\n매월 1일, 2일 3일만 1시에 도는 monthly batch\n보통 이렇게 scheduling하지는 않음. 하지만 구현할 수 있음\n\n\n*/30 * * *\n삼십분마다 (0분, 30분)\n\n\n\n10-59/30 * * * *\n10분부터 삼십분마다 (10분, 40분에 도는 작업)\n\n\n\n10 1 * * 1-5\n평일만 01시 10분\n\n\n\n0 */2 * * *\n2시간 마다 (0시, 02시, 04시 …)\n1-23/2: 1시부터 2시간 마다\n\n\n0 0 */2 * *\n짝수일 0시 0분\n\n\n\n10 1 L * *\n매월 마지막 일 01시 10분에 도는 montly batch\n빈번하게 사용되는 schedule\n\n\n10 1 * * 6#3\n매월 세 번째 토요일 01시 10분 도는 montly batch\n\n\n\n\n\n\n\n\n\n\n\nTask 연결 방법 종류\n\n&gt;&gt;, &lt;&lt; 사용하기 (Airflow 공식 추천방식)\n함수 사용하기\n\n복잡한 Task 는 어떻게 연결하는가?\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Connection\n\n\n\ntask1\n\ntask1\n\n\n\ntask2\n\ntask2\n\n\n\ntask1-&gt;task2\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask1-&gt;task3\n\n\n\n\n\ntask4\n\ntask4\n\n\n\ntask2-&gt;task4\n\n\n\n\n\ntask3-&gt;task4\n\n\n\n\n\ntask6\n\ntask6\n\n\n\ntask4-&gt;task6\n\n\n\n\n\ntask5\n\ntask5\n\n\n\ntask5-&gt;task4\n\n\n\n\n\ntask8\n\ntask8\n\n\n\ntask6-&gt;task8\n\n\n\n\n\ntask7\n\ntask7\n\n\n\ntask7-&gt;task6\n\n\n\n\n\n\n\n\n\n\n\n\n\n방법1 : 모든 경우의 수에 대해서 연결 가능하지만 가독성 떨어짐\n\ntask1 &gt;&gt; task2\ntask1 &gt;&gt; task3\ntask2 &gt;&gt; task4\ntask3 &gt;&gt; task4\ntask5 &gt;&gt; task4\ntask4 &gt;&gt; task6\ntask7 &gt;&gt; task6\ntask6 &gt;&gt; task8\n\n방법2: 같은 레벨의 tasks는 list로 묶어 준다. 가독성이 높지만 구현이 안되는 경우 있을 수 있음\n\ntask1 &gt;&gt; [task2, task3] &gt;&gt; task4\ntask5 &gt;&gt; task4\n[task4, task7] &gt;&gt; task6 &gt;&gt; task8\n\n방법3: 역방향은 &lt;&lt;를 이용 (권장 하지 않음)\n\ntask1 &gt;&gt; [task2, task3] &gt;&gt; task4 &lt;&lt; task5\ntask4 &gt;&gt; task 6 &lt;&lt; task7\ntask6 &gt;&gt; task8\n\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.empty import EmptyOperator\n#EmptyOperator는 어떤 연산도 하지 않는 class\n\nwith DAG(\n    dag_id=\"dags_task_connection\",\n    schedule=None,\n    start_date=pendulum.datetime(2023,3,1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # 8개의 instances: task1~task8\n    task1=EmptyOperator(\n        task_id='task1'\n    )\n    task2=EmptyOperator(\n        task_id='task2'\n    )\n    task3=EmptyOperator(\n        task_id='task3'\n    )\n    task4=EmptyOperator(\n        task_id='task4'\n    )\n    task5=EmptyOperator(\n        task_id='task5'\n    )\n    task6=EmptyOperator(\n        task_id='task6'\n    )\n    task7=EmptyOperator(\n        task_id='task7'\n    )\n    task8=EmptyOperator(\n        task_id='task8'\n    )\n  \n  task1 &gt;&gt; [task2, task3] &gt;&gt; task4\n  task5 &gt;&gt; task4\n  [task4, task7] &gt;&gt; task6 &gt;&gt; task8\n\n\n\n\n\nReference: Airflow Official Document\n\nContent/Core Concepts/DAGs 참고\nDAGs에 대한 숙련도가 올라가면 이 링크를 참고하면 매우 유용\n\nDAG을 어떤 상황에서 어떻게 짜야하는지에 대한 guidance가 자세히 적혀 있음\n예를 들어, dag을 생성하는 방법 (dag declaration)에는 with 문을 사용하는 방법과 standard constructor (표준 생성자)를 사용하는 방법이 있음\n\nwith statement\n\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\n\nwith DAG(\n    dag_id=\"my_dag_name\",\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@daily\",\n):\nEmptyOperator(task_id=\"task\")\n\nstandard constructor (class)\n\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\n\n#class 생성\nmy_dag = DAG( \n    dag_id=\"my_dag_name\",\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@daily\",\n)\nEmptyOperator(task_id=\"task\", dag=my_dag)\n\npython의 decorator기능 활용 (dag decorator to turn a function into a DAG generator)\n\nimport datetime\n\nfrom airflow.decorators import dag\nfrom airflow.operators.empty import EmptyOperator\n\n\n@dag(start_date=datetime.datetime(2021, 1, 1), schedule=\"@daily\")\ndef generate_dag():\n    EmptyOperator(task_id=\"task\")\n\n\ngenerate_dag()\n\n\ntask dependencies 설정을 위한 emplicit methods.\n\nset_upstream and set_downstream\n\nfirst_task.set_downstream(second_task, third_task)\nthird_task.set_upstream(fourth_task)\n\ncross_downstream\n\nfrom airflow.models.baseoperator import cross_downstream\n\n#Replaces\n#[op1, op2] &gt;&gt; op3\n#[op1, op2] &gt;&gt; op4\ncross_downstream([op1, op2], [op3, op4])\n\nchain\n\nfrom airflow.models.baseoperator import chain\n\n#Replaces op1 &gt;&gt; op2 &gt;&gt; op3 &gt;&gt; op4\nchain(op1, op2, op3, op4)\n\n#You can also do it dynamically\nchain(*[EmptyOperator(task_id='op' + i) for i in range(1, 6)])\n\n#or\n\nfrom airflow.models.baseoperator import chain\n\n#Replaces\n#op1 &gt;&gt; op2 &gt;&gt; op4 &gt;&gt; op6\n#op1 &gt;&gt; op3 &gt;&gt; op5 &gt;&gt; op6\nchain(op1, [op2, op3], [op4, op5], op6)\n\n\n\n\n\n\n\n외부 script file such as *.py and *.sh 은 docker가 인식할 수 있도록 docker의 plugins directory안에 넣어줘야 실행된다.\n\n\n\n\nUnix/Linux Shell 명령어로 적혀진 파일로 인터프리터에 의해 한 줄씩 처리된다.\n\ninterpreter: CPU가 programming 언어를 처리하는데 크게 compiling 방식과 interpreting 방식 2가지 방식이 있다.\n\ncompiling\n\nprogramming language를 목적 코드인 2진수로 처리한다음 읽음\ncompile 할 때 연산 시간은 다소 소요되지만 한 번 compile 된 script는 실행 속도가 매우 빠름\nC, Java\n\ninterpreting: compiling없이 한줄씩 읽는 방식\n\ncompiling방식에 비해 실행 속도가 느림\npython, shell\n\n\n\nbashOperator를 이용하여 shell script 처리\nEcho, mkdir, cd, cp, tar, touch 등의 기본적인 쉘 명령어를 입력하여 작성하며 변수를 입력받거나 For 문, if 문 그리고 함수도 사용 가능\n확장자가 없어도 동작하지만 주로 파일명에 .sh 확장자를 붙인다.\n\n\n\n\n\nbashOperator를 이용하다면 bashOperator안에 shell 명령어들을 써서 넣어도 동작은 하지만\n쉘 명령어를 이용하여 복잡한 로직을 처리하는 경우 shell script를 이용하는 것이 좋다\n\n예를들어, sftp (source sever)를 통해 csv나 json같은 파일을 받은 후 전처리하여 DB에 Insert & tar.gz으로 압축하고 싶을때, 이렇게 복잡한 tasks를 bashOperator에 모두 기입하기 보다는 script를 짜서 bashOperator에서 호출하는 방식이 가독성이나 유지보수 측면에서 더 효율적이다.\n\n쉘 명령어 재사용을 위한 경우\n\n위의 예시를 server 100대에 대하여 반복 수행할 때 logic이 같으면 shell script를 100번 호출하는 것이 더 간편\nsftp: 접속할 때 IP, Port, account, pw 가 필요한데 이런 것을 변수화 시키고 DB전처리 로직을 shell script에 짜 놓으면 됨.\n\n\n\n\n\n\n문제점\n\n컨테이너는 외부의 파일을 인식할 수 없다. shell script를 wsl directory 어딘가에 넣으면 container가 인식을 못함.\n컨테이너 안에 파일을 만들어주면 컨테이너 재시작시 파일이 사라진다. docker에서 이미지를 띄우는 것을 container를 만들었다라고 하는데 container 재 실행시 초기화 되어 실행된다. (docker의 특징). 그래서 컨테이너 안에 shell script 파일 넣어도 재시작시 삭제가 됨.\n\n해결방법\n\n\n빨간 네모박스의 plugins에 shell script를 저장한다. airflow document에서는 customized python and shell script를 plugins에 저장하는 것을 권장\n\nexample\n\ncd github-repository/plugins/shell\nvi select_fruit.sh #i 누르면 편집가능하고 편집 후 esc+wq! 입력하고 enter치면 저장하고 나감\nchmod +x select_fruit.sh #실행 권한을 부여\n./select_fruit.sh kmkim # ./test2.sh 는 test2.sh을 실행한다는 의미 출력물: kmkim 출력됨\ngit add -A\ngit commit -m \"shell script example\"\ngit push\n# echo $1 #첫 번째 인수 출력\n\nFRUIT=$1\nif [ $FRUIT == APPLE ]; then\n  echo \"You selected Apple!\"\nelif [ $FRUIT == ORANGE ]; then\n  echo \"You selected Orange!\"\nelif [ $FRUIT == Grape ]; then\n  echo \"You selected Grape!\"\nelse \n  echo \"You selected other Fruit!\"\nfi\n\ncontainer에서 github repository에 있는 plugins/shell에 있는 shell script 인식하게 하기\n\nvi docker-compose.yaml 에서 67line 수정\nvolumes:\n  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n\nselect_fruit.sh 실행 권한 부여\n\n아래와 같이 6번의 task 수행 실패가 발생했는데 처음엔 volumne의 path 설정이 잘못 됐는지 알고 계속 docker-compose.yaml을 살펴봤다. 하지만 이상이 없는 것을 확인하고 task의 log를 확인해 봤는데 다음과 같은 error가 뜬것을 확인할 수 있었다.\n\n\n\nexecution error\n\n\n/bin/bash: line 1: /opt/***/plugins/shell/select_fruit.sh: Permission denied\n이럴 땐 다음과 같이 실행권한을 부여하게 되면 해결된다.\n(airflow) kmkim@K100230201051:~/airflow/plugins/shell$ chmod +x select_fruit.sh\n\n\n\n\n\n이메일 전송해주는 오퍼레이터\nemail_t1 = EmailOperator(\n  task_id='email_t1',\n  to='hjkim_sun@naver.com',\n  subject='Airflow 처리결과',\n  html_content='정상 처리되었습니다.'\n)\n구글 메일 서버 사용\n\n\n\n\n\n\n이메일 전송을 위해 사전 셋팅 작업 필요(Google)\n\ngoogle mail server사용\ngmail &gt;&gt; settings(설정) &gt;&gt; See all settings (모든 설정 보기) &gt;&gt; Forwarding and POP/IMAP (전달 및 POP/IMAP) &gt;&gt; IMAP access (IMAP 접근): Enable IMAP (IMAP 사용)\nManage Your Google Acccount (구글 계정 관리) &gt;&gt; Security (보안) &gt;&gt; 2-Step Verification (2단계 인증) &gt;&gt; App Passwords: 앱비밀번호 setting &gt;&gt; select app: Mail , Select device: Windows Computer &gt;&gt; Generate app pasword message window popped up\n\n\n\n\n\n\n사전 설정 작업 (airflow)\n\ndocker-compose.yaml 편집 (environment 항목에 추가)\n\n# 띄어쓰기 주의\nAIRFLOW__SMTP__SMTP_HOST: 'smtp.gmail.com'  \nAIRFLOW__SMTP__SMTP_USER: '{gmail 계정}'\nAIRFLOW__SMTP__SMTP_PASSWORD: '{앱비밀번호}'\nAIRFLOW__SMTP__SMTP_PORT: 587\nAIRFLOW__SMTP__SMTP_MAIL_FROM: '{gmail 계정}' # 이메일을 누가 보내는 것으로 할건지 정함\n\n\n\n\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.email import EmailOperator\n\nwith DAG(\n    dag_id=\"dags_email_operator\",\n    schedule=\"0 8 1 * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    sending_email_task=EmailOperator(\n        task_id='sending_email_task',\n        to='sdf@naver.com',\n        cc=['sdf2@gmail.com', 'sdf3@gmail.com'],\n        subject='Airflow Test',\n        html_content= \"\"\"\n            this is a test for airflow.&lt;br/&gt;&lt;br/&gt;\n            \n            {{ ds }}&lt;br/&gt;\n        \"\"\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#airflow-dag-생성",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#airflow-dag-생성",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "G\n\n\ncluster0\n\nDAG\n\n\n\nBash_Operator\n\nBash_Operator\n\n\n\nTask1\n\nTask1\n\n\n\nBash_Operator-&gt;Task1\n\n\n\n\n\nPython_Operator\n\nPython_Operator\n\n\n\nTask2\n\nTask2\n\n\n\nPython_Operator-&gt;Task2\n\n\n\n\n\nS3_Operator\n\nS3_Operator\n\n\n\nTask3\n\nTask3\n\n\n\nS3_Operator-&gt;Task3\n\n\n\n\n\nGCS_Operator\n\nGCS_Operator\n\n\n\nTask4\n\nTask4\n\n\n\nGCS_Operator-&gt;Task4\n\n\n\n\n\n\n\n\n\n\n\nworkflow = DAG\nOpeartor\n\n특정 행위를 할 수 있는 기능을 모아 놓은 클래스 또는 설계도\n\nTask\n\noperator가 객체화(instantiation)되어 DAG에서 실행 가능한 object\n방향성을 갖고 순환되지 않음 (DAG)\n\nBash Operator\n\nLinux에서 shell script 명령을 수행하는 operator\n\nPython Operator\n\npython 함수를 실행하는 operator\n\nS3 Operator\n\nAWS의 S3 solution (object storage)을 control할 수 있는 operator\n\nGCS Operator\n\nGCP의 GCS solution (object storage)을 control할 수 있는 operator\n\noperators을 사용하여 dags을 작성하여 git을 통해 배포한다.\ndag 작성 및 배포\n  from __future__ import annotations\n\n  import datetime # python에는 datatime이라는 data type이 있음\n  import pendulum # datetime data type을 처리하는 library\n\n  from airflow import DAG\n  from airflow.operators.bash import BashOperator\n  from airflow.operators.empty import EmptyOperator\n\n  with DAG(\n      dag_id=\"dags_bash_operator\", \n      # airflow service web 상에서 보여지는 이름, python file명과는 무관하지만 \n      # 실무에서는 일반적으로 python 파일명과 dag_id는 일치시키는 것이 다수의 dags 관리에 편리하다.\n      schedule=\"0 0 * * *\", # \"분 시 일 월 요일\", cron schedule로서 매일 0분 0시에 실행\n      start_date=pendulum.datetime(2023, 6, 9, tz=\"Asia/Seoul\"), #dags이 언제 실행될지 설정\n      # UTC: 세계 표준시로 한국 보다 9시간이 느림. Asia/Seoul로 변경해야 지정한 날짜에 0분 0시에 실행될 수 있다.\n      catchup=False, # start_date를 현재보다 과거로 설정하게 될 경우 \n      # catchup=True면 과거 부터 현재까지 소급해서 실행. \n      # 시간 순서대로 실행하는게 아니라 병렬로 한번에 실행하기 때문에 메모리를 많이 잡아먹을 수 있음. \n      # 그래서 보통 False로 처리. catchup=False면 현재부터만 실행\n      # dagrun_timeout=datetime.timedelta(minutes=60), # dag이 60분 이상 구동시 실패가 되도록 설정\n      # tags=[\"example\", \"example2\"], #airflow service web browser상 dag의 tag를 의미. 즉 dag id 바로 밑 파란색 박스를 의미. tag를 누르면 같은 tag를 가진 dags들만 filtering돼서 선택됨 \n      ## dags 이 수 백개가 될 때 tag로 filtering 하면 용이함 \n      # params={\"example_key\": \"example_value\"}, # as dag: 이하 tasks를 정의할 때, \n      ## tasks에 공통 passing parameters가 있을 때 씀\n  ) as dag:\n      # [START how to_operator_bash]\n      bash_task1 = BashOperator(\n          task_id=\"bash_task1\", # airflow web service의 dag graph에 표시될 task명\n          # task역시 task object name (bash_task1)과 task_id(bash_task1)를 일치시키는 것이 좋음\n          bash_command=\"echo this task works well!\",\n          # bash_command 이하는 shell script를 적어주면 됨\n      )\n      # [END how to_operator_bash]\n      bash_task2 = BashOperator(\n          task_id=\"bash_task2\",  \n          bash_command=\"echo $HOSTNAME\", # $HOSTNAME: HOSTNAME 환경변수 호출\n          # WSL terminal 이름이 출력된다.\n      )\n      bash_task1 &gt;&gt; bash_task2 # 수행될 tasks의 관계 설정\n배포된 dags을 airflow containers과 연결 시키기 위해 docker-compose.yaml 실행\n\nvi docker-compose.yaml 실행 후 docker-compose.yaml 안에서 Volumns 항목이 wsl의 directory와 container directory를 연결(mount)해주는 요소\n\nVolumes\n  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n\n위와 같이 Volumns 항목이 뜨는데 :을 기준으로 왼쪽이 WSL directories(volumns), 오른쪽이 Docker container directories(volumns)\n다른 WSL창을 열어 echo ${AIRFLOW_PROJ_DIR:-.} 실행하면 AIRFLOW_PROJ_DIR에 값이 없기 때문에 . 출력됨\n\nAIRFLOW_PROJ_DIR:-. : shell script문법으로 AIRFLOW_PROJ_DIR에 값이 있으면 출력하고 없으면 .을 출력하라는 의미\necho AIRFLOW_PROJ_DIR: 아무것도 출력 안됨\n\n${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags는 ./dags를 /opt/airflow/dags에 연결시키라는 의미\n\n./: docker-compose.yaml이 위치하고있는 현재 directory를 의미\n\n배포된 dags를 자동으로 docker container에 연동시키기 위해 Volumns을 다음과 같이 편집\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n\ndirectory hierarchy에 따라 위의 volumes path를 다르게 설정해야한다. docker service web browse(i.e. localhost:8080) 껏다 켜가면서 확인하면서 설정\n\n새로운 dags 배포할 때마다 airflow service 껐다가 켜야 한다.\n\nairflow service 껐다 켜서 잘 반영됐는지 확인\n\ndocker가 설치된 wsl directory이동 먼저 할 것\nairflow service 끄기: sudo docker compose down\nairflow service 켜기: sudo docker compose up\n\nairflow web service상에서 dags이 잘 mount 되었는지 확인\n\n기본적으로 dags은 airflow web service상에 올라올 때 unpaused 상태로 올라옴\n하지만 schedule이 걸려있는 dags은 unpaused상태에서 한번 돌고 올라옴\ndag을 클릭하면 긴 녹색 막대기를 누르면 수행된 schedule내용이 나오고\n각 각의 task에 대응되는 녹색 네모 박스를 누르면 결과들을 조회할 수 있다.\n\n네모 박스를 누르고 log (audit log 아님)를 누르면 결과가 자세히 조회된다.\nbash_task2 의 bash_command=\"echo $HOSTNAME\" 의 결과값으로 조회된 값은 docker worker container id 를 의미한다.\n\n하지만 본인의 경우, airflow web service에서 794f3b56824a가 출력된 것을 확인했고\nsudo docker ps로 container ID를 확인한 결과 airflow-airflow-worker-1 의 32092b201878 로 달랐다.\n\n실제 worker container로 들어가 echo $HOSTNAME 실행하면 worker container id 출력되어야 함\n\nworker container로 들어가기: sudo docker exec -it container-name bash \\(\\rightarrow\\) 본인의 경우: sudo docker exec -it airflow-airflow-worker-1 bash 이 과정이 dag을 돌린과정과 같은 mechanism임\necho $HOSTNAME 실행 : 32092b201878 출력됨 (어쨌든 airflow web service상의 794f3b56824a와 달랐음)\nsudo docker exec -it 794f3b56824a bash 결과 Error response from daemon: No such container: 794f3b56824a 라는 에러메세지 뜸\n\n즉, worker container가 실제 task를 처리하는 것을 볼 수 있었다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#subject-of-task-performance",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#subject-of-task-performance",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "G\n\n\ncluster0\n\nTask Process\n\n\n\nScheduler\n\nScheduler\n\n\n\nScheduler-&gt;Scheduler\n\n\n3.check start time\n\n\n\nDAG_file\n\nDAG_file\n\n\n\nScheduler-&gt;DAG_file\n\n\n1.parsing\n\n\n\nQueue\n\nQueue\n\n\n\nScheduler-&gt;Queue\n\n\n\n\n\nMeta_DB\n\nMeta_DB\n\n\n\nScheduler-&gt;Meta_DB\n\n\n2.save information\n\n\n\nWorker\n\nWorker\n\n\n\nDAG_file-&gt;Worker\n\n\n5.Processing after reading\n\n\n\nWorker-&gt;Meta_DB\n\n\n6.Results update\n\n\n\nQueue-&gt;Worker\n\n\n4.start instruction\n\n\n\n\n\n\n\n\n\nscheduler\n\nairflow에서 brain역할\n\nparsing: a user가 만든 dag 파일을 읽어들여 문법적 오류 여부와 tasks 간의 관계를 분석\nsave information: DAG Parsing 후 DB에 정보저장 (tasks, task relations, schedule, etc.)\ncheck start time: DAG 시작 실행 시간 확인\nstart instruction: DAG 시작 실행 시간마다 worker에 실행 지시\n\n\nscheduler와 workder 사이에 queue 상태가 있을 수 있음\n\n\nworker (Worker Container)\n\nairflow 처리 주체 (subject)\n\nProcessing after reading: scheduler가 시킨 DAG 파일을 찾아 읽고 처리\nResults update: 처리가 되기 전/후를 Meta DB에 update함"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#cron-scheduling",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#cron-scheduling",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "task가 실행되어야 하는 시간(주기)을 정하기 위한 다섯개의 필드로 구성된 문자열\nCron을 이용하면 왠만한 scheduling 모두 가능\n\n{minutes} {hour} {day} {month} {weekday}\n\n\n\n\n\n\n\n\nNumber\nSpecial Characters\nDescription\n\n\n\n\n1\n*\n모든 값\n\n\n2\n-\n범위 지정\n\n\n3\n,\n여러 값 지정\n\n\n4\n/\n증가값 지정. staring-value/ending-value\n\n\n5\nL\n마지막 값 (일, 요일에만 설정 가능)  * 일에 L 입력시 해당 월의 마지막 일 의미  ※ 요일에 L 입력시 토요일 의미\n\n\n6\n#\n몇 번째 요일인지 지정\n\n\n\n\n\n\n\n\n\n\n\nCron schedule\nDescription\nNote\n\n\n\n\n15 2 * * *\n매일 02시 15분에 도는 daily batch\n\n\n\n0 * * * *\n매시 정각에 도는 시간 단위 batch\n\n\n\n0 0 1 * *\n매월 1일 0시 0분 도는 monthly batch\n\n\n\n10 1 * * 1\n매주 월요일 1시 10분에 도는 weekly batch\n0: 일요일, 1: 월요일, 2: 화요일, 3:수요일, 4: 목요일, 5: 금요일, 6: 토요일\n\n\n0 9-18 * * *\n매일 9시부터 18시까지 정각마다 도는 daily batch\n보통 이렇게 scheduling하지는 않음. 하지만 구현할 수 있음\n\n\n0 1 1,2,3 * *\n매월 1일, 2일 3일만 1시에 도는 monthly batch\n보통 이렇게 scheduling하지는 않음. 하지만 구현할 수 있음\n\n\n*/30 * * *\n삼십분마다 (0분, 30분)\n\n\n\n10-59/30 * * * *\n10분부터 삼십분마다 (10분, 40분에 도는 작업)\n\n\n\n10 1 * * 1-5\n평일만 01시 10분\n\n\n\n0 */2 * * *\n2시간 마다 (0시, 02시, 04시 …)\n1-23/2: 1시부터 2시간 마다\n\n\n0 0 */2 * *\n짝수일 0시 0분\n\n\n\n10 1 L * *\n매월 마지막 일 01시 10분에 도는 montly batch\n빈번하게 사용되는 schedule\n\n\n10 1 * * 6#3\n매월 세 번째 토요일 01시 10분 도는 montly batch"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#task-connection-methods",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#task-connection-methods",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "Task 연결 방법 종류\n\n&gt;&gt;, &lt;&lt; 사용하기 (Airflow 공식 추천방식)\n함수 사용하기\n\n복잡한 Task 는 어떻게 연결하는가?\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Connection\n\n\n\ntask1\n\ntask1\n\n\n\ntask2\n\ntask2\n\n\n\ntask1-&gt;task2\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask1-&gt;task3\n\n\n\n\n\ntask4\n\ntask4\n\n\n\ntask2-&gt;task4\n\n\n\n\n\ntask3-&gt;task4\n\n\n\n\n\ntask6\n\ntask6\n\n\n\ntask4-&gt;task6\n\n\n\n\n\ntask5\n\ntask5\n\n\n\ntask5-&gt;task4\n\n\n\n\n\ntask8\n\ntask8\n\n\n\ntask6-&gt;task8\n\n\n\n\n\ntask7\n\ntask7\n\n\n\ntask7-&gt;task6\n\n\n\n\n\n\n\n\n\n\n\n\n\n방법1 : 모든 경우의 수에 대해서 연결 가능하지만 가독성 떨어짐\n\ntask1 &gt;&gt; task2\ntask1 &gt;&gt; task3\ntask2 &gt;&gt; task4\ntask3 &gt;&gt; task4\ntask5 &gt;&gt; task4\ntask4 &gt;&gt; task6\ntask7 &gt;&gt; task6\ntask6 &gt;&gt; task8\n\n방법2: 같은 레벨의 tasks는 list로 묶어 준다. 가독성이 높지만 구현이 안되는 경우 있을 수 있음\n\ntask1 &gt;&gt; [task2, task3] &gt;&gt; task4\ntask5 &gt;&gt; task4\n[task4, task7] &gt;&gt; task6 &gt;&gt; task8\n\n방법3: 역방향은 &lt;&lt;를 이용 (권장 하지 않음)\n\ntask1 &gt;&gt; [task2, task3] &gt;&gt; task4 &lt;&lt; task5\ntask4 &gt;&gt; task 6 &lt;&lt; task7\ntask6 &gt;&gt; task8\n\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.empty import EmptyOperator\n#EmptyOperator는 어떤 연산도 하지 않는 class\n\nwith DAG(\n    dag_id=\"dags_task_connection\",\n    schedule=None,\n    start_date=pendulum.datetime(2023,3,1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # 8개의 instances: task1~task8\n    task1=EmptyOperator(\n        task_id='task1'\n    )\n    task2=EmptyOperator(\n        task_id='task2'\n    )\n    task3=EmptyOperator(\n        task_id='task3'\n    )\n    task4=EmptyOperator(\n        task_id='task4'\n    )\n    task5=EmptyOperator(\n        task_id='task5'\n    )\n    task6=EmptyOperator(\n        task_id='task6'\n    )\n    task7=EmptyOperator(\n        task_id='task7'\n    )\n    task8=EmptyOperator(\n        task_id='task8'\n    )\n  \n  task1 &gt;&gt; [task2, task3] &gt;&gt; task4\n  task5 &gt;&gt; task4\n  [task4, task7] &gt;&gt; task6 &gt;&gt; task8\n\n\n\n\n\nReference: Airflow Official Document\n\nContent/Core Concepts/DAGs 참고\nDAGs에 대한 숙련도가 올라가면 이 링크를 참고하면 매우 유용\n\nDAG을 어떤 상황에서 어떻게 짜야하는지에 대한 guidance가 자세히 적혀 있음\n예를 들어, dag을 생성하는 방법 (dag declaration)에는 with 문을 사용하는 방법과 standard constructor (표준 생성자)를 사용하는 방법이 있음\n\nwith statement\n\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\n\nwith DAG(\n    dag_id=\"my_dag_name\",\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@daily\",\n):\nEmptyOperator(task_id=\"task\")\n\nstandard constructor (class)\n\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\n\n#class 생성\nmy_dag = DAG( \n    dag_id=\"my_dag_name\",\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@daily\",\n)\nEmptyOperator(task_id=\"task\", dag=my_dag)\n\npython의 decorator기능 활용 (dag decorator to turn a function into a DAG generator)\n\nimport datetime\n\nfrom airflow.decorators import dag\nfrom airflow.operators.empty import EmptyOperator\n\n\n@dag(start_date=datetime.datetime(2021, 1, 1), schedule=\"@daily\")\ndef generate_dag():\n    EmptyOperator(task_id=\"task\")\n\n\ngenerate_dag()\n\n\ntask dependencies 설정을 위한 emplicit methods.\n\nset_upstream and set_downstream\n\nfirst_task.set_downstream(second_task, third_task)\nthird_task.set_upstream(fourth_task)\n\ncross_downstream\n\nfrom airflow.models.baseoperator import cross_downstream\n\n#Replaces\n#[op1, op2] &gt;&gt; op3\n#[op1, op2] &gt;&gt; op4\ncross_downstream([op1, op2], [op3, op4])\n\nchain\n\nfrom airflow.models.baseoperator import chain\n\n#Replaces op1 &gt;&gt; op2 &gt;&gt; op3 &gt;&gt; op4\nchain(op1, op2, op3, op4)\n\n#You can also do it dynamically\nchain(*[EmptyOperator(task_id='op' + i) for i in range(1, 6)])\n\n#or\n\nfrom airflow.models.baseoperator import chain\n\n#Replaces\n#op1 &gt;&gt; op2 &gt;&gt; op4 &gt;&gt; op6\n#op1 &gt;&gt; op3 &gt;&gt; op5 &gt;&gt; op6\nchain(op1, [op2, op3], [op4, op5], op6)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#what-is-shell-script",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#what-is-shell-script",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "Unix/Linux Shell 명령어로 적혀진 파일로 인터프리터에 의해 한 줄씩 처리된다.\n\ninterpreter: CPU가 programming 언어를 처리하는데 크게 compiling 방식과 interpreting 방식 2가지 방식이 있다.\n\ncompiling\n\nprogramming language를 목적 코드인 2진수로 처리한다음 읽음\ncompile 할 때 연산 시간은 다소 소요되지만 한 번 compile 된 script는 실행 속도가 매우 빠름\nC, Java\n\ninterpreting: compiling없이 한줄씩 읽는 방식\n\ncompiling방식에 비해 실행 속도가 느림\npython, shell\n\n\n\nbashOperator를 이용하여 shell script 처리\nEcho, mkdir, cd, cp, tar, touch 등의 기본적인 쉘 명령어를 입력하여 작성하며 변수를 입력받거나 For 문, if 문 그리고 함수도 사용 가능\n확장자가 없어도 동작하지만 주로 파일명에 .sh 확장자를 붙인다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#why-to-need-shell-script",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#why-to-need-shell-script",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "bashOperator를 이용하다면 bashOperator안에 shell 명령어들을 써서 넣어도 동작은 하지만\n쉘 명령어를 이용하여 복잡한 로직을 처리하는 경우 shell script를 이용하는 것이 좋다\n\n예를들어, sftp (source sever)를 통해 csv나 json같은 파일을 받은 후 전처리하여 DB에 Insert & tar.gz으로 압축하고 싶을때, 이렇게 복잡한 tasks를 bashOperator에 모두 기입하기 보다는 script를 짜서 bashOperator에서 호출하는 방식이 가독성이나 유지보수 측면에서 더 효율적이다.\n\n쉘 명령어 재사용을 위한 경우\n\n위의 예시를 server 100대에 대하여 반복 수행할 때 logic이 같으면 shell script를 100번 호출하는 것이 더 간편\nsftp: 접속할 때 IP, Port, account, pw 가 필요한데 이런 것을 변수화 시키고 DB전처리 로직을 shell script에 짜 놓으면 됨."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#worker-컨테이너가-쉘-스크립트를-수행하려면",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#worker-컨테이너가-쉘-스크립트를-수행하려면",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "문제점\n\n컨테이너는 외부의 파일을 인식할 수 없다. shell script를 wsl directory 어딘가에 넣으면 container가 인식을 못함.\n컨테이너 안에 파일을 만들어주면 컨테이너 재시작시 파일이 사라진다. docker에서 이미지를 띄우면 container라 하는데 container 재 실행시 초기화 되어 실행된다.\n\n해결방법\n\n\n빨간 네모박스의 plugins에 shell script를 저장한다. airflow document에서는 customized python and shell script를 plugins에 저장하는 것을 권장\n\nexample\n\ncd github-repository/plugins/shell\nvi select_fruit.sh #i 누르면 편집가능하고 편집 후 esc+wq! 입력하고 enter치면 저장하고 나감\nchmod +x select_fruit.sh #실행 권한을 부여\n./select_fruit.sh kmkim # ./test2.sh 는 test2.sh을 실행한다는 의미 출력물: kmkim 출력됨\ngit add -A\ngit commit -m \"shell script example\"\ngit push\necho $1 #첫 번째 인수 출력\n\ncontainer에서 github repository에 있는 plugins/shell에 있는 shell script 인식하게 하기\n\nvi docker-compose.yaml 에서 67line 수정\nvolumes:\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow//plugins:/opt/airflow/plugins\n\n\n이메일 전송해주는 오퍼레이터\nemail_t1 = EmailOperator(\n  task_id='email_t1',\n  to='hjkim_sun@naver.com',\n  subject='Airflow 처리결과',\n  html_content='정상 처리되었습니다.'\n)\n구글 메일 서버 사용"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#presetting",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#presetting",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "이메일 전송을 위해 사전 셋팅 작업 필요(Google)\n\ngoogle mail server사용\ngmail &gt;&gt; settings(설정) &gt;&gt; See all settings (모든 설정 보기) &gt;&gt; Forwarding and POP/IMAP (전달 및 POP/IMAP) &gt;&gt; IMAP access (IMAP 접근): Enable IMAP (IMAP 사용)\nManage Your Google Acccount (구글 계정 관리) &gt;&gt; Security (보안) &gt;&gt; 2-Step Verification (2단계 인증) &gt;&gt; App Passwords: 앱비밀번호 setting &gt;&gt; select app: Mail , Select device: Windows Computer &gt;&gt; Generate app pasword message window popped up\n\n\n\n\n\n\n사전 설정 작업 (airflow)\n\ndocker-compose.yaml 편집 (environment 항목에 추가)\n\n# 띄어쓰기 주의\nAIRFLOW__SMTP__SMTP_HOST: 'smtp.gmail.com'  \nAIRFLOW__SMTP__SMTP_USER: '{gmail 계정}'\nAIRFLOW__SMTP__SMTP_PASSWORD: '{앱비밀번호}'\nAIRFLOW__SMTP__SMTP_PORT: 587\nAIRFLOW__SMTP__SMTP_MAIL_FROM: '{gmail 계정}' # 이메일을 누가 보내는 것으로 할건지 정함"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#emailoperator-작성",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#emailoperator-작성",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "from airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.email import EmailOperator\n\nwith DAG(\n    dag_id=\"dags_email_operator\",\n    schedule=\"0 8 1 * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    sending_email_task=EmailOperator(\n        task_id='sending_email_task',\n        to='sdf@naver.com',\n        cc=['sdf2@gmail.com', 'sdf3@gmail.com'],\n        subject='Airflow Test',\n        html_content= \"\"\"\n            this is a test for airflow.&lt;br/&gt;&lt;br/&gt;\n            \n            {{ ds }}&lt;br/&gt;\n        \"\"\"\n    )"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#worker-컨테이너가-외부-스크립트shell를-수행하려면",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#worker-컨테이너가-외부-스크립트shell를-수행하려면",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "문제점\n\n컨테이너는 외부의 파일을 인식할 수 없다. shell script를 wsl directory 어딘가에 넣으면 container가 인식을 못함.\n컨테이너 안에 파일을 만들어주면 컨테이너 재시작시 파일이 사라진다. docker에서 이미지를 띄우는 것을 container를 만들었다라고 하는데 container 재 실행시 초기화 되어 실행된다. (docker의 특징). 그래서 컨테이너 안에 shell script 파일 넣어도 재시작시 삭제가 됨.\n\n해결방법\n\n\n빨간 네모박스의 plugins에 shell script를 저장한다. airflow document에서는 customized python and shell script를 plugins에 저장하는 것을 권장\n\nexample\n\ncd github-repository/plugins/shell\nvi select_fruit.sh #i 누르면 편집가능하고 편집 후 esc+wq! 입력하고 enter치면 저장하고 나감\nchmod +x select_fruit.sh #실행 권한을 부여\n./select_fruit.sh kmkim # ./test2.sh 는 test2.sh을 실행한다는 의미 출력물: kmkim 출력됨\ngit add -A\ngit commit -m \"shell script example\"\ngit push\n# echo $1 #첫 번째 인수 출력\n\nFRUIT=$1\nif [ $FRUIT == APPLE ]; then\n  echo \"You selected Apple!\"\nelif [ $FRUIT == ORANGE ]; then\n  echo \"You selected Orange!\"\nelif [ $FRUIT == Grape ]; then\n  echo \"You selected Grape!\"\nelse \n  echo \"You selected other Fruit!\"\nfi\n\ncontainer에서 github repository에 있는 plugins/shell에 있는 shell script 인식하게 하기\n\nvi docker-compose.yaml 에서 67line 수정\nvolumes:\n  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n\nselect_fruit.sh 실행 권한 부여\n\n아래와 같이 6번의 task 수행 실패가 발생했는데 처음엔 volumne의 path 설정이 잘못 됐는지 알고 계속 docker-compose.yaml을 살펴봤다. 하지만 이상이 없는 것을 확인하고 task의 log를 확인해 봤는데 다음과 같은 error가 뜬것을 확인할 수 있었다.\n\n\n\nexecution error\n\n\n/bin/bash: line 1: /opt/***/plugins/shell/select_fruit.sh: Permission denied\n이럴 땐 다음과 같이 실행권한을 부여하게 되면 해결된다.\n(airflow) kmkim@K100230201051:~/airflow/plugins/shell$ chmod +x select_fruit.sh"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#public-data",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#public-data",
    "title": "Content List, Engineering",
    "section": "15 Public data",
    "text": "15 Public data\n\n2024-01-01, Public Data Centers"
  },
  {
    "objectID": "docs/blog/posts/Engineering/public_data/index.html",
    "href": "docs/blog/posts/Engineering/public_data/index.html",
    "title": "Public Data",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\nGovernment and Health Data: datasets related to sociology, economic development, education, health care, and more. datasets from the Indian Government, European Union, UK Government, US Government, and US Bureau of Labor Statistics are typically well-maintained and comprehensive.\nSocioeconomic Data by World Bodies: Organizations like the United Nations, UNICEF, World Health Organization, World Bank, IMF, and the Asian Development Bank provide extensive datasets.\nFinancial Data: Datasets from the National Stock Exchange of India, Reserve Bank of India, NASDAQ, and the New York Stock Exchange offer historical trading data (time-series analysis, stock market trends, and algorithmic trading models).\nComputer Vision Datasets: datasets like ImageNet, COCO, Google’s Open Image Dataset, and LSUN.\nNatural Language Processing (NLP) Datasets: datasets for voice and speech recognition, language translation, and other linguistics-related tasks\n\n\n\n\n\nGoogle Cloud Datasets: datasets hosted on Google Cloud\nMicrosoft Azure Open Datasets: their curated datasets working within the Azure ecosystem for your machine learning projects.\nKaggle: data for its machine learning competitions\nUS Government’s Data (data.gov): the US government data.\nWorld Bank Data: economic and development datasets.\nQuandl: financial and economic dataset (time-series data)\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/public_data/index.html#topic",
    "href": "docs/blog/posts/Engineering/public_data/index.html#topic",
    "title": "Public Data",
    "section": "",
    "text": "Government and Health Data: datasets related to sociology, economic development, education, health care, and more. datasets from the Indian Government, European Union, UK Government, US Government, and US Bureau of Labor Statistics are typically well-maintained and comprehensive.\nSocioeconomic Data by World Bodies: Organizations like the United Nations, UNICEF, World Health Organization, World Bank, IMF, and the Asian Development Bank provide extensive datasets.\nFinancial Data: Datasets from the National Stock Exchange of India, Reserve Bank of India, NASDAQ, and the New York Stock Exchange offer historical trading data (time-series analysis, stock market trends, and algorithmic trading models).\nComputer Vision Datasets: datasets like ImageNet, COCO, Google’s Open Image Dataset, and LSUN.\nNatural Language Processing (NLP) Datasets: datasets for voice and speech recognition, language translation, and other linguistics-related tasks"
  },
  {
    "objectID": "docs/blog/posts/Engineering/public_data/index.html#api",
    "href": "docs/blog/posts/Engineering/public_data/index.html#api",
    "title": "Public Data",
    "section": "",
    "text": "Google Cloud Datasets: datasets hosted on Google Cloud\nMicrosoft Azure Open Datasets: their curated datasets working within the Azure ecosystem for your machine learning projects.\nKaggle: data for its machine learning competitions\nUS Government’s Data (data.gov): the US government data.\nWorld Bank Data: economic and development datasets.\nQuandl: financial and economic dataset (time-series data)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Conda/01.conda_install.html",
    "href": "docs/blog/posts/Engineering/Conda/01.conda_install.html",
    "title": "Conda Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n1 Conda\n(To be written)\n\n파이썬 가상환경\n\n라이브러리 버전 충돌 방지를 위해 설치/사용되는 파이썬 인터프리터 환경을 격리시키는 기술\n파이썬은 라이브러리 설치 시점에 따라서도 설치되는 버전이 상이한 경우가 많음\n\n\n\n\n가상 환경의 필요성\n\n\n\npython을 global 환경에 설치할 경우 위의 그림처럼 C,D프로젝트가 동시에 진행될 때 둘 중하나의 library version이 차이가 나면 old version의 library 로 진행되는 프로젝트는 에러가 발생함\n\n2개의 다른 프로젝트가 같은 python interpreter를 바라보고 library를 설치하기 때문에 종속성 문제가 생김 (library 충돌 발생)\n그래서 다른 가상환경 venv안에 다른 프로젝트를 할당해서 독립적으로 프로젝트를 진행하는게 일반적임\n\nminiconda install link\n\nLinux: wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nMac OS M1: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\nMac OS Intel: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\n\n설치 스크립트 실행\n\n./Miniconda3-latest-Linux-x86_64.sh\n만약, permission denied error message 뜨면 실행권한 줄것\nchmod +x Miniconda3-latest-Linux-x86_64.sh\n\nLicence 동의, Miniconda 설치 경로 지정, 초기화: 그냥 다 yes 하면 됨\n.bashrc 최신화\n\nsource ~/.bashrc\n\nprompt 재시작하면됨\npython 가상환경 만들기: 보통 가상환경 이름 = 프로젝트 이름\n\nconda create -n {project_name} python={python version}\npython에 있는 가상환경 생성 기능으로 만들 경우 python -m airflow ./venv 실행\n\n./venv directory에 python 설치하고 version 관리하겠다는 의미\n\n\nVScode가 python 가상환경 참조하도록 설정\n\nhelp&gt;show all commands or ctrl+shift+p 누른후 interpreter 입력하여 가상환경에 있는 python 클릭\n\nterminal 에서 가상환경 잘 잡혔는지 확인\n생성된 가상 환경 리스트 보기\n\nconda env list\n\n가상 환경 활성화\n\nconda activate {project_name or 가상환경_name}\n\n\n가상 환경 비활성화\n\nconda deactivate\n\n가상 환경을 바꿀 시 비활성화 해주고 다른 가상환경 활성화\n\n\n가상 환경 삭제\n\nconda remove --name {가상 환경_ 이름} -all\n\n\n\n\n\n\n\n\n\n\n\n\n2 Go to Blog Content List\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html",
    "title": "Python Operator",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nimport libraries\n\nfrom {package name} import {operator or class name}\nex) from airflow.operators.python import PythonOperator\n\n정의된 파이썬 함수를 실행시키는 오퍼레이터\n가장 많이 쓰이는 오퍼레이터\n\n\n\n\n\n\n\n\n\n\n\n\nPackage\nOperator\nImportance\nDescription\n\n\n\n\nairflow.operators.python\nPythonOperator\n***\n어떤 파이썬함수를실행시키기 위한오퍼레이터\n\n\n\nBranchPythonOperator\n*\n파이썬 함수 실행 결과에 따라 task를 선택적으로 실행시킬 때 사용되는 오퍼레이터. task1 &gt;&gt; [task2, task3] 와 같은 상황에서 BranchOperator에서 task1을 실행시키고 task2와 task3 중 하나를 택하여 실행시킬 수 있는 operator. 즉, 조건적으로 task를 실행시키는 operator\n\n\n\nShortCircuitOperator\n\n파이썬 함수 실행 결과에 따라 후행 Task를 실행하지 않고 dag자체를 종료시킬 수 있는 오퍼레이터\n\n\n\nPythonVirtualenvOperator\n\n파이썬 가상환경 생성후 Job 수행하고 마무리되면 가상환경을 삭제해주는 3개의 과정을 실행시켜 주는 오퍼레이터\n\n\n\nExternalPythonOperator\n\n기존에 존재하는 파이썬가상환경에서 Job 수행하게 하는 오퍼레이터\n\n\n\n\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nimport random # for random.randint(0,3)\n\nwith DAG(\n    dag_id=\"dags_python_operator\",\n    schedule=\"30 6 * * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    def select_fruit(): # 실행 시켜줄 함수 선언\n        fruit =['Apple','Kiwi','banana','avocado']\n        rand_int = random.randint(0,3) # 0,1,2,3 4개의 정수 중 하나 return\n        print(fruit[rand_int])\n    py_t1 = PythonOperator(\n        task_id ='py_t1',\n        python_callable=select_fruit\n    )\n    py_t1\n\n\n\n\n\n\n\n예) PyhonOperator의 Import원리\n\nfrom airflow.operators.python import PythonOperator\nAirflow 폴더 아래 operators 폴더 아래 python 파일 아래에서 PythonOperator 클래스를 호출하라는 의미\n\npython이 경로 찾는 방식을 알아놔야 함\n\ndag에서 우리가 만든 외부 함수를 import 해와야 하는데\nimport 경로를 python이 찾을 수 있도록 그 문법에 맞게 작성해야함\n\n파이썬은 sys.path 변수에서 모듈의 위치를 검색\n\npip install로 설치된 libraries\n\ncontainer 안에 들어간 후 설치된 libraries 조회\n\n\n\nconda 환경에 설치된 libraries\n  (airflow) PS C:\\Users\\kmkim\\Desktop\\projects\\airflow&gt; python\n  Python 3.8.18 | packaged by conda-forge | (default, Dec 23 2023, 17:17:17) [MSC v.1929 64 bit (AMD64)] on win32\n  Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n  &gt;&gt;&gt; import sys\n  &gt;&gt;&gt; from pprint import pprint \n  &gt;&gt;&gt; pprint(sys.path) \n  ['',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\python38.zip',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\DLLs',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\lib',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\lib\\\\site-packages']\n  &gt;&gt;&gt;\n\nsys.path 에 값을 추가하는 방법\n\n명시적으로 추가 (ex: sys.path.append(‘/home/kkm’) )\nOS 환경변수 PYTHONPATH 에 값을 추가\n\n\n\n\n\n\n\n\n다행히, 파이썬에서와 같이 귀찮은 방식보다는 Airflow에서는 자동적으로 dags 폴더와 plugins 폴더를 sys.path에 추가함\n\n컨테이너에서 airflow info 명령을 수행해보면 아래 그림과 같은 정보를 확인할 수 있다. \n\nplugins 폴더 이용하기 \n\nplugins까지는 airflow에서 기본적으로 인식하고 있기 때문에 from common.common_func import get_sftp에서와 같이 common부터 경로 써주면 됨.\n\n파이썬 스크립트를 이용하면 좋은 점\n\n공통함수 작성이 가능해지고\n재활용성이 증가하고\nDAG의 가독성이 올라가고\n디버깅에도 용이하다.\n\nPythonOperator 예시\n\ndags_python_import_func.py\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom common.common_func import get_sftp # 여기서 path 가 맞지 않아 local에서 error가 발생\n    # .env 파일을 만들어서 workspace 경로 설정을 해줘야 한다.\nimport random\n\nwith DAG(\n    dag_id=\"dags_python_import_func\",\n    schedule=\"30 6 * * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    get_sftp_task = PythonOperator(\n        task_id ='get_sftp_task',\n        python_callable=get_sftp\n    )\n    get_sftp_task\n\n.env: wsl airflow directory에는 배포될 필요없는 파일이기 때문에 .gitignore에 추가해도 됨\n\nWORKSPACE_FOLDER=C:\\Users\\kmkim\\Desktop\\projects\\airflow #'airflow' directory path입력\nPYTHONPATH=${WORKSPACE_FOLDER}/plugins #'plugins' directory추가\n\n\n\n\n\n\n\n\n\npython에 있는 기능\n데커레이터(Decorator): 장식하다, 꾸미다\nWhat to decorate?: 함수를 장식하다\n기능: python 함수를 좀 더 쉽게 사용할 수 있다. 원래의 함수를 감싸서 (Wrapping) 바깥에 추가 기능을 덧붙이는 방법\n\n파이썬은 함수 안에 함수를 선언하는 것이 가능하고\n함수의 인자로 함수를 전달하는 것이 가능하며\n함수 자체를 리턴하는 것이 가능하다.\n\nwarpping (함수 감싸기 )\n\n파이썬은 함수 안에 함수를 선언하는 것이 가능\n함수의 인자로 함수를 전달하는 것이 가능\n함수 자체를 리턴하는 것이 가능\n\n함수 감싸기 예시\n\n\nget_data() 함수 안에 log를 쌓아야하는 상황\n\ndef get_data()\n    print('function starts working')\n\nlog는 간단하게 여기서 print('target 함수 실행 전 입니다.') 와 print('target 함수 실행 후 입니다.') 라고 설정\nget_data() 함수 안에 log를 쌓는 python code를 작성하는 것이 아니라 get_data() 를 인수로 받는 warpping function, outer_func(target_func)을 만든다\n\ndef outer_func(target_func):\n    def inner_func():\n        print('target 함수 실행 전 입니다.')\n        print('function starts working')\n        print('target 함수 실행 후 입니다.')\n    return inner_func\na=outer_func(get_data) \na() # get_data()을 인수로 받은 inner_func()이 출력된다.\nwrapping 함수의 장점\n\nget_data()와 같은 target function이 수 백개가 되면 수 백개의 log정보를 수 백개의 target functions에 기입해야한다. wrapping 함수를 사용하면 코드의 재사용이 가능해진다.\n\nwrapping 함수의 단점\n\nget_data()와 같은 target function이 수 백개가 되면 a=outer_func(get_data1);a(), a=outer_func(get_data2);a(), \\(\\dots\\),a=outer_func(get_data100);a() 이런식으로 만들어야 함.\n이런 단점을 보완한 것이 decorator\n\n\ndecorator\n\n\n@outer_func 을 target function위에 작성\n\n@outer_func\ndef get_data()\n    print('function starts working')\n\n이렇게, decoration을 하게 되면 get_data1();get_data2();get_data3() 만 실행하면 자동으로 outer_func()이 실행됨\n\n\ntask decorator in airflow\n\nairflow에도 비슷한 기능이 있는데 파이썬 함수 정의만으로 쉽게 Task 생성\n\n\nairflow official document에서는 PythonOperator Import해서 DAG을 만드는 것 보다 task decorator를 사용하는 것을 더 권장\n\n실제로, airflow web service에서 example_python_operator DAG을 보면 example_python_deocartor라고 이름이 안지어졌는데도 decorator 예시가 바로 나옴.\n왜냐면, 나중에 task간에 data를 공유할 때 task decorator를 사용하면 PythonOperator를 사용하면 data 공유가 한결 더 쉬워진다.\ntask decorator를 사용하면 task connection 을 작성할 필요가 없어짐\ntask decorator dag 예시\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"dags_python_decorator\",\n    schedule='0 2 * * 1', #매주 월요일 2시 실행\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False,\n) as dag:\n\n    @task(task_id=\"python_task1\")\n    def print_context(some_input):\n        print(some_input)\n\n    python_task1 = print_context('task decorator execution!') #인수(argument) 추가\n\n\n\n\n\n\n\n일반적인 함수 인자 작성 방법\n\ndef regist(name, sex):\n    print(name)\n    print(sex)\n\nregist('kkm','man')\n\nBut, 호출하는 로직에서 몇 개의 파라미터를 넘길지 모를 때 또는 파라미터의 개수를 제한하지 않으려면?\n또는 선택적으로 변수를 받을 수도 있을 때는? (가령, 주소와 전화번호)\n파이썬 함수에서 인수 형태는 크게 3가지가 있음\n\n일반 변수 형태를 명시적으로 받는 것: name, sex\n*argument 방식: *args\n**keyword_arguement 방식: **kwargs\n\n\n\n\n\ndef regist(name, sex, *args): \n    #name, sex: 필수 정보로 설정\n    #*args: 부가 정보로 설정\n    print(type(args)) #tuple로 되어 있음, tuple에서 값을 꺼낼때는 index로 접근\n    country = args[0] if len(args)&gt;=1 else None #부가 정보가 없을때는 error가 발생하기 때문에 조건문을 달아줌. 즉, args의 길이가 1이상일때만 1번째 값을 꺼냄.\n    city = args[1] if len(args)&gt;=2 else None\n\nregist('gdhong','man') #필수 정보만 입력\nregist('gdhong','man','korea','seoul') #필수 정보 + 부가 정보\n\nregist('gdhong','man') 은 name='gdhong' 과 sex='man'\nregist('gdhong','man','korea','seoul') 은 name='gdhong' 과 sex='man'. 'korea','seoul' 는 *args가 catch한다. 이 경우에, name과 sex는 필수 정보 *args는 부가 정보의 개념\nargs로 들어온 값은 tuple 저장된다.\nargs에서 값을 꺼낼 때는 인덱스를 이용한다 (ex: args[0] , args[1])\nargs라는 이름 외 다른 이름으로 받아도 됨 (ex: some_func(*kk):)\n\n\n\n\ndef some_func(**kwargs):\n    print(type(kwargs)) # dictionary type\n    print(kwargs)\n    name = kwargs.get('name') or '' #dictionary 값을 얻을 때는 get('key value') 씀\n    country = kwargs.get('country') or '' # 'country' key 없으면 none이 반환됨\n    print(f'name:{name}, country:{country}')\n\nsome_func(name=’hjkim’, country=’kr’)\n\n\n\n\n\n\n방어 로직\n\n\n\n\ndict = {'name'='kkm'} 라고 dictionary 선언 후, kkm이라는 dictionary의 value을 꺼낼때, dict['name']로 값을 호출하면 name 이라는 키가 없을 때 에러가 발생.\n이 때 dict.get('name') 으로 시도하면 name 이라는 키가 없을 때 에러나지 않고 None이 반환되어 상대적으로 안전.\ndict.get('name') or '' 의 의미는 name 이라는 키가 있으면 value를 꺼내오고 키가 없으면 빈 문자 열('')을 받는다는 의미.\n\n\n\n\n\n\ndef regist(name, sex, *args, **kwargs):\n    print(name)\n    print(sex)\n    print(args)\n    print(kwargs)\n\nregist('kkm', 'man', 'korea', 'seoul', phone=010, email='kkm@naver.com')\n\n함수의 인수에 잘 작동원리\n\n*,** 가 없는 인수는 name과 sex로 고정 되어 있기 때문에 아무리 많은 인수값이 오더라도 첫 2개는 name과 sex에 무조건 할당 됨.\n나머지 인수값 중 'key':'value' 형태가 아닌 것은 모두 *arg에 할당됨. (즉, ‘korea’, ‘seoul’)\n'key':'value' arugments는 **kwargs에 할당된다. (즉, phone=010, email=‘kkm@naver.com’)\n\n\n\n\n\n\n\ndef regist(name, sex):\n    print(f'이름은 {name}이고 성별은 {sex}입니다')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=['hjkim','man'] #list로 작성되고 regist()의 인수값으로 사용된다. \n    #'hjkim'과 'man'은 각 각 regist()의 name과 sex에 할당된다.\n)\n\n\n\n\n\ndef regist(name, sex, *args):\n    print(name) # string 형태로\n    print(sex) # string 형태로\n    print(args) # tuple 형태로  출력 됨\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=[‘hjkim’,’man’,’kr’,’seoul’] # ‘hjkim’,’man’ 는 name과 sex에 ,’kr’,’seoul’는 *args에 할당된다.\n)\n\n\n\n\ndef regist(*args):\n    print(args) # tuple 형태로  출력 됨\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=[‘hjkim’,’man’,’kr’,’seoul’] # ‘hjkim’,’man’,’kr’,’seoul’는 *args에 할당된다.\n)\n\n\n\n\n\ndef regist(*args):\n    print(args) # tuple 인 ('hjkim','man','kr','seoul')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=['hjkim','man','kr','seoul'] # 모두 *args에 할당된다.\n)\n\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom common.common_func import regist\nwith DAG(\n    dag_id=\"dags_python_with_op_args\",\n    schedule=\"30 6 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    regist_t1 = PythonOperator(\n        task_id='regist_t1',\n        python_callable=regist,\n        op_args=['hjkim','man','kr','seoul']\n    )\n\n    regist_t1\n\n\n\n\n\n\n\ndef regist(name, sex):\n    print(f'이름은 {name}이고 성별은 {sex}입니다')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_kwargs={'name':'hjkim','sex':'man'} #dictonary 형태로 작성\n    # regist(name,sex)의 argument 가 name, sex이고 op_kwargs의 key 값에 regist(name,sex)의 argument를 똑같이 작성해준다.\n)\n\n\n\ndef regist(name, sex, **kwargs):\n    print(name)\n    print(sex)\n    print(kwargs) # dictionary 형태로 출력됨\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_kwargs={'name':'hjkim','sex':'man',\\\n    'country':'kr','city':'seoul'} # op_kwargs의 key값 중 name과 sex는 regist()의 인수명과 일치하므로 자동적으로 연결되어 regist()의 name과 sex에 할당되고 country와 city는 **kwargs에 할당된다.\n)\n\n\n\ndef regist(**kwargs):\n    name=kwargs['name'] or ''\n    sex=kwargs['sex'] or ''\n    country = kwargs['country'] or ''\n    city = kwargs['city'] or ''\n    print(f'name은 {name}이고, \\\n        성별은 {sex}이고, \\\n        국가는 {country} 이고, \\\n        도시는 {city} 입니다.')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_kwargs={'name':'hjkim','sex':'man',\\\n    'country':'kr',city:'seoul'}\n)\n\n\n\ndef regist(name, sex, *args, **kwargs):\n    print(name)\n    print(sex)\n    print(args)\n    print(kwargs)\n\npython_task_2 = PythonOperator(\n    task_id='python_task_2',\n    python_callable=regist,\n    op_args=['hjkim','man','kr','seoul'], #name='hjkim', sex='man',**args=('kr','seoul')\n    op_kwargs={'phone’:010,'email':'hjkim_sun@naver.com'} #**kwargs={'phone’:010,'email':'hjkim_sun@naver.com'}\n)\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom common.common_func import regist2\nwith DAG(\n    dag_id=\"dags_python_with_op_kwargs\",\n    schedule=\"30 6 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    regist2_t1 = PythonOperator(\n        task_id='regist2_t1',\n        python_callable=regist2,\n        op_args=['kkm','man','kr','seoul'],\n        op_kwargs={'email':'kkm@naver.com','phone':'010-9999-9999'}\n    )\n\n    regist2_t1\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-types",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-types",
    "title": "Python Operator",
    "section": "",
    "text": "Package\nOperator\nImportance\nDescription\n\n\n\n\nairflow.operators.python\nPythonOperator\n***\n어떤 파이썬함수를실행시키기 위한오퍼레이터\n\n\n\nBranchPythonOperator\n*\n파이썬 함수 실행 결과에 따라 task를 선택적으로 실행시킬 때 사용되는 오퍼레이터. task1 &gt;&gt; [task2, task3] 와 같은 상황에서 BranchOperator에서 task1을 실행시키고 task2와 task3 중 하나를 택하여 실행시킬 수 있는 operator. 즉, 조건적으로 task를 실행시키는 operator\n\n\n\nShortCircuitOperator\n\n파이썬 함수 실행 결과에 따라 후행 Task를 실행하지 않고 dag자체를 종료시킬 수 있는 오퍼레이터\n\n\n\nPythonVirtualenvOperator\n\n파이썬 가상환경 생성후 Job 수행하고 마무리되면 가상환경을 삭제해주는 3개의 과정을 실행시켜 주는 오퍼레이터\n\n\n\nExternalPythonOperator\n\n기존에 존재하는 파이썬가상환경에서 Job 수행하게 하는 오퍼레이터"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-example",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-example",
    "title": "Python Operator",
    "section": "",
    "text": "from airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nimport random # for random.randint(0,3)\n\nwith DAG(\n    dag_id=\"dags_python_operator\",\n    schedule=\"30 6 * * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    def select_fruit(): # 실행 시켜줄 함수 선언\n        fruit =['Apple','Kiwi','banana','avocado']\n        rand_int = random.randint(0,3) # 0,1,2,3 4개의 정수 중 하나 return\n        print(fruit[rand_int])\n    py_t1 = PythonOperator(\n        task_id ='py_t1',\n        python_callable=select_fruit\n    )\n    py_t1"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-module-path",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-module-path",
    "title": "Python Operator",
    "section": "",
    "text": "from airflow.operators.python import PythonOperator\n\nAirflow 폴더 아래 operators 폴더 아래 python 파일 아래에서 PythonOperator 클래스를 호출하라는 의미\n\npython이 경로 찾는 방식을 알아놔야 함\n\ndag에서 우리가 만든 외부 함수를 import 해와야 하는데\nimport 경로를 python이 찾을 수 있도록 그 문법에 맞게 작성해야함\n\n파이썬은 sys.path 변수에서 모듈의 위치를 검색\n\npip install로 설치된 libraries \nconda 환경에 설치된 libraries\n\n['c:\\\\Users\\\\kmkim\\\\Desktop\\\\projects\\\\website\\\\docs\\\\blog\\\\posts\\\\Engineering\\\\airflow', \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\python39.zip',             \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\DLLs', \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\lib', \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study', \n'', # 실행하는 파이썬 파일과 동일 디렉토리에 있는 파이썬 파일, 이것에 한해서 그냥 모듈명으로만 호출 가능\n    # 예를 들어, a.py 와 b.py와 동일한 디렉토리안에 있다면 a.py안에서 `import b` 라고 해도 호출 가능\n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\lib\\\\site-packages', \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\lib\\\\site-packages\\\\win32', \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\lib\\\\site-packages\\\\win32\\\\lib', \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\lib\\\\site-packages\\\\Pythonwin']\nsys.path 에 값을 추가하는 방법\n\n명시적으로 추가 (ex: sys.path.append(‘/home/hjkim’) )\nOS 환경변수 PYTHONPATH 에 값을 추가"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#plugins-directory-이용",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#plugins-directory-이용",
    "title": "Python Operator",
    "section": "",
    "text": "다행히, 파이썬에서와 같이 귀찮은 방식보다는 Airflow에서는 자동적으로 dags 폴더와 plugins 폴더를 sys.path에 추가함\n\n컨테이너에서 airflow info 명령을 수행해보면 아래 그림과 같은 정보를 확인할 수 있다. \n\nplugins 폴더 이용하기 \n\nplugins까지는 airflow에서 기본적으로 인식하고 있기 때문에 from common.common_func import get_sftp에서와 같이 common부터 경로 써주면 됨.\n\n파이썬 스크립트를 이용하면 좋은 점\n\n공통함수 작성이 가능해지고\n재활용성이 증가하고\nDAG의 가독성이 올라가고\n디버깅에도 용이하다.\n\nPythonOperator 예시\n\ndags_python_import_func.py\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom common.common_func import get_sftp # 여기서 path 가 맞지 않아 local에서 error가 발생\n    # .env 파일을 만들어서 workspace 경로 설정을 해줘야 한다.\nimport random\n\nwith DAG(\n    dag_id=\"dags_python_import_func\",\n    schedule=\"30 6 * * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    get_sftp_task = PythonOperator(\n        task_id ='get_sftp_task',\n        python_callable=get_sftp\n    )\n    get_sftp_task\n\n.env: wsl airflow directory에는 배포될 필요없는 파일이기 때문에 .gitignore에 추가해도 됨\n\nWORKSPACE_FOLDER=C:\\Users\\kmkim\\Desktop\\projects\\airflow #'airflow' directory path입력\nPYTHONPATH=${WORKSPACE_FOLDER}/plugins #'plugins' directory추가"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-function-parameter",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-function-parameter",
    "title": "Python Operator",
    "section": "",
    "text": "일반적인 함수 인자 작성 방법\n\ndef regist(name, sex):\n    print(name)\n    print(sex)\n\nregist('kkm','man')\n\nBut, 호출하는 로직에서 몇 개의 파라미터를 넘길지 모를 때 또는 파라미터의 개수를 제한하지 않으려면?\n또는 선택적으로 변수를 받을 수도 있을 때는? (가령, 주소와 전화번호)\n파이썬 함수에서 인수 형태는 크게 3가지가 있음\n\n일반 변수 형태를 명시적으로 받는 것: name, sex\n*argument 방식: *args\n**keyword_arguement 방식: **kwargs"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#arguement-arg",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#arguement-arg",
    "title": "Python Operator",
    "section": "",
    "text": "def regist(name, sex, *args): \n    #name, sex: 필수 정보로 설정\n    #*args: 부가 정보로 설정\n    print(type(args)) #tuple로 되어 있음, tuple에서 값을 꺼낼때는 index로 접근\n    country = args[0] if len(args)&gt;=1 else None #부가 정보가 없을때는 error가 발생하기 때문에 조건문을 달아줌. 즉, args의 길이가 1이상일때만 1번째 값을 꺼냄.\n    city = args[1] if len(args)&gt;=2 else None\n\nregist('gdhong','man') #필수 정보만 입력\nregist('gdhong','man','korea','seoul') #필수 정보 + 부가 정보\n\nregist('gdhong','man') 은 name='gdhong' 과 sex='man'\nregist('gdhong','man','korea','seoul') 은 name='gdhong' 과 sex='man'. 'korea','seoul' 는 *args가 catch한다. 이 경우에, name과 sex는 필수 정보 *args는 부가 정보의 개념\nargs로 들어온 값은 tuple 저장된다.\nargs에서 값을 꺼낼 때는 인덱스를 이용한다 (ex: args[0] , args[1])\nargs라는 이름 외 다른 이름으로 받아도 됨 (ex: some_func(*kk):)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#argument-kwargs",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#argument-kwargs",
    "title": "Python Operator",
    "section": "",
    "text": "def some_func(**kwargs):\n    print(type(kwargs)) # dictionary type\n    print(kwargs)\n    name = kwargs.get('name') or '' #dictionary 값을 얻을 때는 get('key value') 씀\n    country = kwargs.get('country') or '' # 'country' key 없으면 none이 반환됨\n    print(f'name:{name}, country:{country}')\n\nsome_func(name=’hjkim’, country=’kr’)\n\n\n\n\n\n\n방어 로직\n\n\n\n\ndict = {'name'='kkm'} 라고 dictionary 선언 후, kkm이라는 dictionary의 value을 꺼낼때, dict['name']로 값을 호출하면 name 이라는 키가 없을 때 에러가 발생.\n이 때 dict.get('name') 으로 시도하면 name 이라는 키가 없을 때 에러나지 않고 None이 반환되어 상대적으로 안전.\ndict.get('name') or '' 의 의미는 name 이라는 키가 있으면 value를 꺼내오고 키가 없으면 빈 문자 열('')을 받는다는 의미."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#mixed-usage-arg-kwargs",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#mixed-usage-arg-kwargs",
    "title": "Python Operator",
    "section": "",
    "text": "def regist(name, sex, *args, **kwargs):\n    print(name)\n    print(sex)\n    print(args)\n    print(kwargs)\n\nregist('kkm', 'man', 'korea', 'seoul', phone=010, email='kkm@naver.com')\n\n함수의 인수에 잘 작동원리\n\n*,** 가 없는 인수는 name과 sex로 고정 되어 있기 때문에 아무리 많은 인수값이 오더라도 첫 2개는 name과 sex에 무조건 할당 됨.\n나머지 인수값 중 'key':'value' 형태가 아닌 것은 모두 *arg에 할당됨. (즉, ‘korea’, ‘seoul’)\n'key':'value' arugments는 **kwargs에 할당된다. (즉, phone=010, email=‘kkm@naver.com’)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-op_args-usage",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-op_args-usage",
    "title": "Python Operator",
    "section": "",
    "text": "def regist(name, sex):\n    print(f'이름은 {name}이고 성별은 {sex}입니다')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=['hjkim','man'] #list로 작성되고 regist()의 인수값으로 사용된다. \n    #'hjkim'과 'man'은 각 각 regist()의 name과 sex에 할당된다.\n)\n\n\n\n\n\ndef regist(name, sex, *args):\n    print(name) # string 형태로\n    print(sex) # string 형태로\n    print(args) # tuple 형태로  출력 됨\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=[‘hjkim’,’man’,’kr’,’seoul’] # ‘hjkim’,’man’ 는 name과 sex에 ,’kr’,’seoul’는 *args에 할당된다.\n)\n\n\n\n\ndef regist(*args):\n    print(args) # tuple 형태로  출력 됨\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=[‘hjkim’,’man’,’kr’,’seoul’] # ‘hjkim’,’man’,’kr’,’seoul’는 *args에 할당된다.\n)\n\n\n\n\n\ndef regist(*args):\n    print(args) # tuple 인 ('hjkim','man','kr','seoul')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=['hjkim','man','kr','seoul'] # 모두 *args에 할당된다.\n)\n\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom common.common_func import regist\nwith DAG(\n    dag_id=\"dags_python_with_op_args\",\n    schedule=\"30 6 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    regist_t1 = PythonOperator(\n        task_id='regist_t1',\n        python_callable=regist,\n        op_args=['hjkim','man','kr','seoul']\n    )\n\n    regist_t1"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python의-module-path-탐색하는-방식의-이해",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python의-module-path-탐색하는-방식의-이해",
    "title": "Python Operator",
    "section": "",
    "text": "예) PyhonOperator의 Import원리\n\nfrom airflow.operators.python import PythonOperator\nAirflow 폴더 아래 operators 폴더 아래 python 파일 아래에서 PythonOperator 클래스를 호출하라는 의미\n\npython이 경로 찾는 방식을 알아놔야 함\n\ndag에서 우리가 만든 외부 함수를 import 해와야 하는데\nimport 경로를 python이 찾을 수 있도록 그 문법에 맞게 작성해야함\n\n파이썬은 sys.path 변수에서 모듈의 위치를 검색\n\npip install로 설치된 libraries\n\ncontainer 안에 들어간 후 설치된 libraries 조회\n\n\n\nconda 환경에 설치된 libraries\n  (airflow) PS C:\\Users\\kmkim\\Desktop\\projects\\airflow&gt; python\n  Python 3.8.18 | packaged by conda-forge | (default, Dec 23 2023, 17:17:17) [MSC v.1929 64 bit (AMD64)] on win32\n  Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n  &gt;&gt;&gt; import sys\n  &gt;&gt;&gt; from pprint import pprint \n  &gt;&gt;&gt; pprint(sys.path) \n  ['',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\python38.zip',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\DLLs',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\lib',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\lib\\\\site-packages']\n  &gt;&gt;&gt;\n\nsys.path 에 값을 추가하는 방법\n\n명시적으로 추가 (ex: sys.path.append(‘/home/kkm’) )\nOS 환경변수 PYTHONPATH 에 값을 추가"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-op_kwargs-usage",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-op_kwargs-usage",
    "title": "Python Operator",
    "section": "",
    "text": "def regist(name, sex):\n    print(f'이름은 {name}이고 성별은 {sex}입니다')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_kwargs={'name':'hjkim','sex':'man'} #dictonary 형태로 작성\n    # regist(name,sex)의 argument 가 name, sex이고 op_kwargs의 key 값에 regist(name,sex)의 argument를 똑같이 작성해준다.\n)\n\n\n\ndef regist(name, sex, **kwargs):\n    print(name)\n    print(sex)\n    print(kwargs) # dictionary 형태로 출력됨\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_kwargs={'name':'hjkim','sex':'man',\\\n    'country':'kr','city':'seoul'} # op_kwargs의 key값 중 name과 sex는 regist()의 인수명과 일치하므로 자동적으로 연결되어 regist()의 name과 sex에 할당되고 country와 city는 **kwargs에 할당된다.\n)\n\n\n\ndef regist(**kwargs):\n    name=kwargs['name'] or ''\n    sex=kwargs['sex'] or ''\n    country = kwargs['country'] or ''\n    city = kwargs['city'] or ''\n    print(f'name은 {name}이고, \\\n        성별은 {sex}이고, \\\n        국가는 {country} 이고, \\\n        도시는 {city} 입니다.')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_kwargs={'name':'hjkim','sex':'man',\\\n    'country':'kr',city:'seoul'}\n)\n\n\n\ndef regist(name, sex, *args, **kwargs):\n    print(name)\n    print(sex)\n    print(args)\n    print(kwargs)\n\npython_task_2 = PythonOperator(\n    task_id='python_task_2',\n    python_callable=regist,\n    op_args=['hjkim','man','kr','seoul'], #name='hjkim', sex='man',**args=('kr','seoul')\n    op_kwargs={'phone’:010,'email':'hjkim_sun@naver.com'} #**kwargs={'phone’:010,'email':'hjkim_sun@naver.com'}\n)\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom common.common_func import regist2\nwith DAG(\n    dag_id=\"dags_python_with_op_kwargs\",\n    schedule=\"30 6 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    regist2_t1 = PythonOperator(\n        task_id='regist2_t1',\n        python_callable=regist2,\n        op_args=['kkm','man','kr','seoul'],\n        op_kwargs={'email':'kkm@naver.com','phone':'010-9999-9999'}\n    )\n\n    regist2_t1"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html",
    "title": "Template Variabler",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nJinja는 Python에서 사용되는 인기 있는 템플릿 엔진으로 웹 프레임워크인 Flask와 종종 함께 사용되며, Django 템플릿 언어에 영향을 받았음.\nJinja를 사용하면 HTML 파일에 파이썬 코드를 삽입하여 동적인 웹 페이지를 쉽게 만들 수 있음\nJinja는 웹 개발을 더 효율적으로 만들어 주는 강력한 도구로 Python과 Flask 또는 Django와 같은 웹 프레임워크를 사용하는 개발자들에게 널리 사용되고 있음\ntemplate engine\n\n템플릿 엔진은 웹 개발에서 사용되는 소프트웨어 또는 라이브러리로, 프로그래머가 템플릿에 데이터를 삽입하여 동적인 웹 페이지를 생성할 수 있게 해줌\n이러한 엔진의 주요 기능은 템플릿이라고 불리는 특정한 형식의 문서에서 변수들을 실제 값으로 바꾸는 것\n쉽게 말해서 템플릿 엔진은 미리 정의된 문서 틀(템플릿)에 데이터를 채워 넣어 실제 사용자가 볼 수 있는 웹 페이지를 만드는 도구이다\n예시: HTML 템플릿 파일 (예: template.html) + Python Flask 라우트 (예: app.py)\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;{{ title }}&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;{{ heading }}&lt;/h1&gt;\n    &lt;p&gt;Welcome, {{ username }}!&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n이 HTML 파일은 Jinja 템플릿을 사용하여 동적인 데이터를 표시한다. 여기서 {{ title }}, {{ heading }}, {{ username }}은 템플릿에서 치환될 변수들이다.\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return render_template('template.html', title='Home Page', heading='Welcome to My Website', username='Alice')\n\nif __name__ == '__main__':\n    app.run(debug=True)\nFlask 앱에서 render_template 함수를 사용하여 template.html 파일을 렌더링하고, title, heading, username 변수에 값을 전달. 이 값들은 사용자가 최종적으로 보는 웹 페이지에서 해당 위치에 표시됨.\ntemplate engine은 여러 솔루션이 존재하며 그 중 Jinja 템플릿은 파이썬 언어에서 사용하는 엔진\nfrom jinja2 import Template # jinja2 library는 airflow 설치시 자동 설치됨\n\ntemplate = Template('my name is {{name}}') #Template은 class, {{name}}은 변수\nnew_template = template.render('name=kkm')\nprint(new_template)\nJinja 템플릿, 어디서 쓰이나?\n\n파이썬 기반 웹 프레임워크인 Flask, Django(장고)에서 주로 사용\n\n예를 들어, HTML 형태의 정적 template 문서를 만들어 놓고 back end server의 처리 결과에 따라 값을 바꾸어 보여줄 때 jinja template engine이 사용될 수 있다. (주로 HTML 템플릿 저장 후 화면에 보여질 때 실제 값으로 변환해서 출력)\n\nSQL작성시에도 활용 가능\n\n예를 들어, select * from tables where base_dt = {{}} 라는 SQL template을 만들어 넣고 jinja template engine을 이용해서 날짜 변수 {{}}에 runtime시 발생하는 실제 값을 할당할 수 있다. 이 예시는 tables에 있는 데이터가 매일 update될 때 base_dt라는 변수에 따라 데이터를 부분 선택할 수 있게 해준다.\n\n\n\n\n\n\n오퍼레이터 파라미터 입력시 중괄호 {} 2개({{}})를 이용하면 Airflow에서 기본적으로 제공하는 변수들(ex: 수행 날짜, DAG_ID)을 치환된 값으로 입력할 수 있음.\n\nairflow에서 제공하는 기본 variable list or google ‘airflow templates reference’\n{ data_interval_start }: schedule 구간의 시작점을 반환, pendulum.DateTime는 timestamped type (중요)\n{ data_interval_end }: schedule 구간의 끝점을 반환 (중요)\n{ ds }: { data_interval_start }의 value를 string 형태(‘YYYY-MM-DD’)로 반환 (중요)\n{ ds_nodash }: {ds}를 string 형태(‘YYYYMMDD’) 로 반환\n{ ts }: timestamped 의 약자로 { data_interval_start }를 string 형태(‘YYYY-MM-DD T00:00:00+00:00’) 로 반환\n{ ts_nodash_with_tz }: timestamped 의 약자로 { ts }를 string 형태(‘YYYYMMDDT000000+0000’) 로 반환\n{ ts_nodash  }: timestamped 의 약자로 {ts}를 string 형태(‘YYYYMMDDT000000’) 로 반환\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용 가능하지는 않음!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요 or google airflow operators\n\noperator 설명란에 parameters 란에 각 parameter의 설명 부분 맨 끝에 (templated) 라고 적혀 있는 parameter는 jinja template 사용 가능. template_fields에 요약되어 있음\n예를 들어, airflow.operators.bash 에서 Parameters를 보면\n\nbash_command (str) – The command, set of commands or reference to a bash script (must be ‘.sh’) to be executed. (templated) - jinja template 사용 가능\nenv (dict[str, str] | None) – If env is not None, it must be a dict that defines the environment variables for the new process; these are used instead of inheriting the current process environment, which is the default behavior. (templated) - jinja template 사용 가능\nappend_env (bool) – If False(default) uses the environment variables passed in env params and does not inherit the current process environment. If True, inherits the environment variables from current passes and then environment variable passed by the user will either update the existing inherited environment variables or the new variables gets appended to it - 사용 불가\noutput_encoding (str) – Output encoding of bash command - 사용 불가\nskip_on_exit_code (int | Container[int] | None) – If task exits with this exit code, leave the task in skipped state (default: 99). If set to None, any non-zero exit code will be treated as a failure. - 사용 불가\ncwd (str | None) – Working directory to execute the command in. If None (default), the command is run in a temporary directory. - 사용 불가\ntemplate_fields: Sequence[str]= (‘bash_command’, ‘env’)[source]\n\n하지만, parameter 설명란과 template_fields에 template 변수가 일치하지 않을 수 있음. 그럴 땐 template_fiedls를 기준으로 함\n예를 들어, airflow.operators.python에서 Parameters를 보면 아래와 같이 templates_dict만 사용 가능한 것 처럼 보이지만 template_fields를 보면 op_kwargs 와 op_args도 jinja template으로 사용 가능한 것을 알 수 있다.\n\npython_callable (Callable) – A reference to an object that is callable\nop_kwargs (Mapping[str, Any] | None) – a dictionary of keyword arguments that will get unpacked in your function\nop_args (Collection[Any] | None) – a list of positional arguments that will get unpacked when calling your callable\ntemplates_dict (dict[str, Any] | None) – a dictionary where the values are templates that will get templated by the Airflow engine sometime between __init__and execute takes place and are made available in your callable’s context after the template has been applied. (templated) - 사용 가능\ntemplates_exts (Sequence[str] | None) – a list of file extensions to resolve while processing templated fields, for examples [‘.sql’, ‘.hql’]\nshow_return_value_in_logs (bool) – a bool value whether to show return_value logs. Defaults to True, which allows return value log output. It can be set to False to prevent log output of return value when you return huge data such as transmission a large amount of XCom to TaskAPI.\ntemplate_fields: Sequence[str]= (‘templates_dict’, ‘op_args’, ‘op_kwargs’)[source]\n\n\n\n\n\n\n\n\n\n\n\n\nrecap) Bash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n\n\n파라미터\n\nbash_command (str) (templated)\nenv (dict[str, str] | None) (templated)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)\n\nDAG Full example ```markdown from airflow import DAG import pendulum import datetime from airflow.operators.bash import BashOperator\nwith DAG( dag_id=“dags_bash_with_template”, schedule=“10 0 * * *“, start_date=pendulum.datetime(2023, 3, 1, tz=”Asia/Seoul”), catchup=False ) as dag: bash_t1 = BashOperator( task_id=‘bash_t1’, bash_command=‘echo “data_interval_end: {{ data_interval_end }}”’ )\n  bash_t2 = BashOperator(\n      task_id='bash_t2',\n      env={\n          'START_DATE':'{{data_interval_start | ds }}', #| ds: time stamped type을 YYYY-MM-DD로 변환\n          'END_DATE':'{{data_interval_end | ds }}' #| ds: time stamped type을 YYYY-MM-DD로 변환\n      },\n      bash_command='echo $START_DATE && echo $END_DATE' #shell script syntax: statement1 && statement2\n      # statment1이 성공하면 statement2를 실행한다.\n  )\n\n  bash_t1 &gt;&gt; bash_t2\n```\n\nAirflow Web Service Result\n\n[2023-06-17, 01:00:00 UTC] {taskinstance.py:1327} INFO - Executing &lt;Task(BashOperator): bash_t1&gt; on 2023-06-15 15:10:00+00:00 에서 2023-06-15 15:10:00+00:00의 +뒤는 time zone을 의미. 00:00 이면 utc (세계 표준시로 한국 보다 9시간 느림)를 의미. 한국 시간으로 변환하려면 9시간을 더해야한다. 즉, 2023-06-16 00:10:00이 한국 서울 시간임\n\n\n[2023-06-17, 01:00:02 UTC] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'echo $START_DATE && echo $END_DATE']\n[2023-06-17, 01:00:02 UTC] {subprocess.py:86} INFO - Output:\n[2023-06-17, 01:00:02 UTC] {subprocess.py:93} INFO - 2023-06-15\n[2023-06-17, 01:00:02 UTC] {subprocess.py:93} INFO - 2023-06-16`\n\n\n\n\n\n\n\n\n\n상황\n\nDaily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행- 매일 00:00에 데이터 가져오기)\n전체 data는 너무 많기 때문에 증분된 데이터만 가져오기. 즉 오늘이 2023/02/25 라면 2023/02/24 와 2023/02/25 사이에 있는 data만 가져온다.\n\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n\n생각해볼 point: 각 관점에 따라 날짜가 다름\n\n데이터 관점의 시작일: 2023-02-24\n데이터 관점의 종료일: 2023-02-25\nDAG이 실행되는 시점: 2023-02-25\nairflow는 ETL을 위한 도구로 만들어졌기 때문에 data관점에서 전처리를 하는 사상이 담겨져 있다.\n\n\n\n\n\n\n예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start (airflow new version - from 2.5.2 version)\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (airflow old version - until 2.5.1 version)\n위와 같이 airflow의 대부분의 변수들이 논리적 기준일을 데이터 관점의 시작일을 기준으로 한다.\nexecution_date 라는 명명법이 너무 혼란스러웠음 실행 날짜란 의미는 대부분의 사람들이 dag이 실행되는 날로 인식을 하는데 data관점에서 날짜를 출력함. 그래서 data_interval_start로 변수명을 바꿈\n\nex. 2023-02-25 배치일 (DAG이 실행되는 날짜)\n\n= data_interval_end (airflow new version - from 2.5.2 version)\n=\n=\n=\n= next_execution_date (airflow old version - until 2.5.1 version)\nnext execution_date 라는 명명법은 대부분의 사람들이 dag이 실행되는 날로 인식을 하기 때문에 혼란스러워서 data_interval_end로 바꿈. 왜냐면 현재 dag 실행 날짜가 next execution_date로 표시되고 그 이전 실행 날짜를 execution_date로 표기해서 실제 실행날짜와 변수 이름이 맞지가 않음.\n그러므로, 배치가 돌고있는 현재 날짜를 출력하고 싶으면 data_interval_end에 접근해야하고 그 이전 배치의 날짜를 출력하고 싶으면 data_interval_start에 접근해야한다.\n\n\nFull Exmaple\n\nDAG\n\n# dags_bash_with_template.py\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"dags_python_show_templates\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 6, 10, tz=\"Asia/Seoul\"),\n    catchup=True #catchup 할때 task 순서를 유념해서 연결시키지 않으면 dags실행을 pause/unpuase시 task들이 꼬일 수 있다.\n) as dag:\n\n    @task(task_id='python_task')\n    def show_templates(**kwargs):\n        from pprint import pprint \n        pprint(kwargs) #pprint는 리스트나 딕셔너리를 줄넘김으로 이쁘게 출력해줌\n\n    show_templates()\n\nAirflow Web Service Result\n\n[2023-06-17, 01:40:17 UTC] {logging_mixin.py:149} INFO - {'conf': &lt;***.configuration.AirflowConfigParser object at 0x7f668aeec910&gt;,\n'conn': None,\n'dag': &lt;DAG: dags_python_show_templates&gt;,\n'dag_run': &lt;DagRun dags_python_show_templates @ 2023-06-09 00:30:00+00:00: scheduled__2023-06-09T00:30:00+00:00, state:running, queued_at: 2023-06-17 01:40:15.833772+00:00. externally triggered: False&gt;,\n**'data_interval_end': DateTime(2023, 6, 10, 0, 30, 0, tzinfo=Timezone('UTC')),**\n**'data_interval_start': DateTime(2023, 6, 9, 0, 30, 0, tzinfo=Timezone('UTC')),**\n**'ds': '2023-06-09',**\n**'ds_nodash': '20230609',**\n*'execution_date': &lt;Proxy at 0x7f665d530640 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'execution_date', DateTime(2023, 6, 9, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n'expanded_ti_count': None,\n'inlets': [],\n**'logical_date': DateTime(2023, 6, 9, 0, 30, 0, tzinfo=Timezone('UTC')),**\n'macros': &lt;module '***.macros' from '/home/***/.local/lib/python3.7/site-packages/***/macros/__init__.py'&gt;,\n*'next_ds': &lt;Proxy at 0x7f665d530690 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'next_ds', '2023-06-10')&gt;*,\n*'next_ds_nodash': &lt;Proxy at 0x7f665d5306e0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'next_ds_nodash', '20230610')&gt;*,\n*'next_execution_date': &lt;Proxy at 0x7f665d530780 with factory functools.partial*(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'next_execution_date', DateTime(2023, 6, 10, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n'outlets': [],\n'params': {},\n'prev_data_interval_end_success': DateTime(2023, 6, 6, 0, 30, 0, tzinfo=Timezone('UTC')),\n'prev_data_interval_start_success': DateTime(2023, 6, 5, 0, 30, 0, tzinfo=Timezone('UTC')),\n*'prev_ds': &lt;Proxy at 0x7f665d5307d0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_ds', '2023-06-08')&gt;*,\n*'prev_ds_nodash': &lt;Proxy at 0x7f665d530820 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_ds_nodash', '20230608')&gt;*,\n*'prev_execution_date': &lt;Proxy at 0x7f665d530870 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_execution_date', DateTime(2023, 6, 8, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n*'prev_execution_date_success': &lt;Proxy at 0x7f665d5308c0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_execution_date_success', DateTime(2023, 6, 5, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n'prev_start_date_success': DateTime(2023, 6, 17, 1, 40, 15, 103936, tzinfo=Timezone('UTC')),\n'run_id': 'scheduled__2023-06-09T00:30:00+00:00',\n'task': &lt;Task(_PythonDecoratedOperator): python_task&gt;,\n'task_instance': &lt;TaskInstance: dags_python_show_templates.python_task scheduled__2023-06-09T00:30:00+00:00 [running]&gt;,\n'task_instance_key_str': 'dags_python_show_templates__python_task__20230609',\n'templates_dict': None,\n'test_mode': False,\n'ti': &lt;TaskInstance: dags_python_show_templates.python_task scheduled__2023-06-09T00:30:00+00:00 [running]&gt;,\n*'tomorrow_ds': &lt;Proxy at 0x7f665d530910 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'tomorrow_ds', '2023-06-10')&gt;*,\n*'tomorrow_ds_nodash': &lt;Proxy at 0x7f665d530960 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'tomorrow_ds_nodash', '20230610')&gt;*,\n'triggering_dataset_events': {},\n**'ts': '2023-06-09T00:30:00+00:00',**\n**'ts_nodash': '20230609T003000',**\n**'ts_nodash_with_tz': '20230609T003000+0000',**\n'var': {'json': None, 'value': None},\n*'yesterday_ds': &lt;Proxy at 0x7f665d5309b0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'yesterday_ds', '2023-06-08')&gt;*,\n*'yesterday_ds_nodash': &lt;Proxy at 0x7f665d530a00 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'yesterday_ds_nodash', '20230608')&gt;}*\n\n위에서, 과거 혼란을 주는 변수들은 italic채로 표시를 했고 출력물을 보면 depreacted될 예정이라고 적혀져 있어 곧 안쓰일 예정이라고 적혀져 있다.\nbold채로 쓰여진 출력물이 개선된 명명법으로 이름 붙여진 변수들인데 대부분의 시간들이 data관점에서 logical date를 선정한 것을 알 수 있다. dag 배치 실행 날짜를 보기 위해선 data_interval_end를 보면 2023-06-10이 실행 날짜인 것을 알 수 있다. logical date의 2023-06-10 이전 배치 실행 날짜이다.\n실제 업무나 작업시 data_interval_end가 자주 쓰인다.\n\n\n\n\n\n\n\n\n\nPython 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs (templated)\nop_args (templated)\ntemplates_dict (templated)\ntemplates_exts\nshow_return_value_in_logs\n\nOperator Template\n\njinja template을 이용하여 runtime date를 얻을 때 2가지 방식이 있음\n\n함수를 만들어 op_kwargs에 jinja template 변수를 만들고 이 변수에 저장된 값을 꺼내 쓰는 법\n**kwargs로부터 얻음 - 2번째 방법이 더 편한것 같지만 개인 취향에 따름\n\n함수를 만들어 jinja template를 이용해 연산\n\n\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()\nFull Example\n\nDAGS\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"dags_python_template\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 3, 10, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    def python_function1(start_date, end_date, **kwargs):\n        print(start_date)\n        print(end_date)\n\n    python_t1 = PythonOperator(\n        task_id='python_t1',\n        python_callable=python_function1,\n        op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n    )\n\n    @task(task_id='python_t2')\n    def python_function2(**kwargs):\n        print(kwargs)\n        print('ds:' + kwargs['ds'])\n        print('ts:' + kwargs['ts'])\n        print('data_interval_start:' + str(kwargs['data_interval_start']))\n        print('data_interval_end:' + str(kwargs['data_interval_end']))\n        print('task_instance:' + str(kwargs['ti']))\n\n\n    python_t1 &gt;&gt; python_function2() #decorator사용시 함수를 실행주기만 해도 task가 생성되기 때문에 함수를 task로 연결할 수 있다.\n\nAirflow Web Service Result\n\n\n\n\n\n\n\n\njinja template 안에서 날짜 연산을 가능하게 해주는 기능\n\n파이썬의 datetime + dateutil library로 가능\n\nMacro 변수의 필요성\n\n가령, 어떤 DAG의 스케줄은 매일 말일에 도는 스케줄 (0 0 L * *)인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은 상황. 즉,\n\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\n날짜 구간을 hard coding 해놓는게 아니라 DAG이 도는 시점에 따라 알맞게 들어가야 함.\n예를 들어, 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되어야함 DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지 생각해봐야함 (반드시, data_interval_end 에서 하루 뺀 값이 나와야 하는데)\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN {{ data_interval_start }} AND {{ data_interval_start }} - 1day\n'''\n{ data_interval_start } - 1day 이 부분 연산을 하는데 macro 변수가 쓰임\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime, python의 datetime library 를 이용가능하게 하거나 datetime library를 template 변수내에서 날짜 연산 기능\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta, 날짜 연산 기능\n\n\nmacros.dateutil\nA reference to the dateutil package, python의 dateutil library를 이용가능하게 하거나 dateutil library를 template 변수내에서 이용가능하게 하여 날짜 연산 기능\n\n\nmacros.time\nThe standard lib’s time, 날짜 연산 기능\n\n\nmacros.uuid\nThe standard lib’s uuid, 고유 ID 부여\n\n\nmacros.random\nThe standard lib’s random, python rand() 사용가능하게 해줌\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리, 매우 빈번하게 쓰임\n예를 들어, macros.dateutil에서 relativedelta.relativedelta() 함수를 쓸수 있도록 해줌. macros.dateutil.relativedelta.relativedelta()\n\nMacro를 잘 쓰려면 python의 datetime 및 dateutil library에 익숙해져야 함.\n\n\n\n\n\n만약, jupyter notebook (대화형 입력창)이 없는 환경인데 jupyter notebook에서 python을 실행하고 싶으면 terminal에 다음 명령어를 실행해서 설치\n\n대화형 입력창: 일련의 명령어들을 한번에 실행시키는 script code 형식이 아니라 명령어 한줄마다 결과값을 볼 수 있는 창\n\n\npip install jupyter # 약 5분 소요\npython -m notebook\n\n\nCode\nfrom datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) #월을 1월로 변경하는 명령어, relativedelta library 사용\nprint(now.replace(month=1)) # 월을 1월로 변경하는 명령어, datetime library 사용, print(now+relativedelta.relativedelta(month=1)) 와 같은 명령어\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기: 먼저 month 값에서 1을 빼고 그 결과 값(month)의 가장 가까운 말일을 자동으로 선택해줌\n\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\n\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기. relativedelta library장점이 연산 연러개를 이어 붙일 수 있음\n\n\ncurrent time:2003-03-30 00:00:00\n-------------month operation-------------\n2003-01-30 00:00:00\n2003-01-30 00:00:00\n2003-02-28 00:00:00\n-------------day operation-------------\n2003-03-01 00:00:00\n2003-03-01 00:00:00\n2003-03-29 00:00:00\n-------------multiple operations-------------\n2003-02-27 00:00:00\n\n\n\n\n\n\n예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일 (6#2)에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''}, #env 변수에 template 변수를 작성\n)\n\n이 부분에 template + macro 활용\n\nDAG 예시1.\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_with_macro_eg1\",\n    schedule=\"10 0 L * *\", #매월 말일날 도는 DAG\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # START_DATE: 전월 말일, END_DATE: 1일 전\n    bash_task_1 = BashOperator(\n        task_id='bash_task_1',\n        env={'START_DATE':'{{ data_interval_start.in_timezone(\"Asia/Seoul\") | ds }}',\n                #template 변수에 꺼내쓰는 모든 날짜 변수는 default로 timezone이 UTC로 맞춰져있기 때문에 현지에 맞게 고쳐줘야한다. 한국 시간에 맞추려면 9시간을 더해야하는데, .in_timezone(\"Asia/Seoul\")로 해결 가능\n                #data_interval_start.in_timezone(\"Asia/Seoul\")는 timestamp형식으로 출력되기 때문에 yyyy-mm-dd로 출력하기위해 ds 연산 붙임\n             'END_DATE':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") - macros.dateutil.relativedelta.relativedelta(days=1)) | ds}}'\n             # 연산자가 -로 되어 있이기 때문에  days=-1로 할필요없음\n        },\n        bash_command='echo \"START_DATE: $START_DATE\" && echo \"END_DATE: $END_DATE\"'\n    )\n\n예시2. DAG full Exmaple\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_with_macro_eg2\",\n    schedule=\"10 0 * * 6#2\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # START_DATE: 2주전 월요일, END_DATE: 2주전 토요일\n    # 예를 들어, 2023-04-01 토요일은 첫째 주 토요일로 인식\n    # 2023-04-08 토요일은 둘째 주 토요일로 인식 (군대에서 순서를 세는 방식과 다름)\n    # 2023-04-08 토요일을 START_DATE(배치일)로 정하면 END_DATE는 배치일 기준으로부터 2 주를 뺀 토요일은 2023-03-25가 된다.\n    # 배치일 기준 (2023-04-08 토요일)으로 그 전 배치의 START_DATE를 구하려면 END_DATE로부터 5일을 뺀 날짜인 2023-03-20 (월요일)이 START_DATE가 된다.\n    # 이는 즉, 배치일 기준 (2023-04-08 토요일) 19일을 빼준 날짜와 같다.\n    bash_task_2 = BashOperator(\n        task_id='bash_task_2',\n        env={'START_DATE':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") - macros.dateutil.relativedelta.relativedelta(days=19)) | ds}}', #2주전 월요일\n             'END_DATE':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") - macros.dateutil.relativedelta.relativedelta(days=14)) | ds}}' #2주전 툐요일\n        },\n        bash_command='echo \"START_DATE: $START_DATE\" && echo \"END_DATE: $END_DATE\"'\n    )\n\n\n\n\n\nTemplate 변수를 지원하는 parameters\n패러미터\n\npython_callable (Callable | None)\nop_kwargs (Templated)\nop_args (Templated)\ntemplates_dict (Templated)\ntemplates_exts\nshow_return_value_in_logs\n\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\")\n    #templates 변수를 꺼내온 값들을 key:value 형태로 꺼내온 뒤\n    #get_datetime_macro(**kwargs)의 **kwargs에 전달된다. \n+ macros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}',\n# 배치일로 부터 한달을 빼고 일 1로 함. 즉, 전월 1일\n# 예를 들어, 배치일이 3월 15일이라면 2월 1일로 end_date를 설정한다.\n'end_date': '{{\n(data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\nmacros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n# end_date는 배치일이 3월 15일이라면 2월 28일로 된다.\n\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {} # kwargs.get('templates_dict')이 빈값이면 {}로 할당\n    if templates_dict:\n    start_date = templates_dict.get('start_date') or 'start_date없음'\n    end_date = templates_dict.get('end_date') or 'end_date없음'\n    print(start_date)\n    print(end_date)\n\nget_datetime_macro(kwargs)의 templates_dict에는 {‘start_date’:’{{ (data_interval_end.in_timezone(“Asia/Seoul”) #templates 변수를 꺼내온 값들을 key:value 형태로 꺼내온 뒤 #get_datetime_macro(kwargs)의 **kwargs에 전달된다.\nmacros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}‘, ’end_date’: ‘{{ (data_interval_end.in_timezone(“Asia/Seoul”).replace(day=1) + macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}’ } 전체가 들어감\n\n그러나 Python 오퍼레이터에서 굳이 macro를 사용할 필요가 있을까? 날짜 연산을 python 문법을 이용해서 DAG 안에서 직접 연산하면 macro 변수를 사용안해도 날짜를 계산할 수 있음.\n\nmacro 사용 : template 변수 내에서 macro를 이용해 날짜를 반환 후에 start_date에 할당\n\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") + macros.dateutil.relativedelta.relativedelta(months=-1,day=1)) | ds }}',\n    'end_date': '{{ (data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\n    macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n        start_date = templates_dict.get('start_date') or 'start_date없음'\n        end_date = templates_dict.get('end_date') or 'end_date없음'\n        print(start_date)\n        print(end_date)\n\npython 문법을 사용하여 직접 연산: 라이브러리를 이용해 날짜를 연산\n\n@task(task_id='task_direct_calc')\ndef get_datetime_calc(**kwargs):\n    from dateutil.relativedelta import relativedelta #relativedelta함수 직접 import\n    data_interval_end = kwargs['data_interval_end'] #data_interval_end는 datetime type\nprev_month_day_first = data_interval_end.in_timezone('Asia/Seoul') + relativedelta(months=-1, day=1) #data_interval_end는 datetime type에는 in_timezone() method가 있음\nprev_month_day_last = data_interval_end.in_timezone('Asia/Seoul').replace(day=1) + relativedelta(days=-1)\nprint(prev_month_day_first.strftime('%Y-%m-%d')) # | ds 구현\nprint(prev_month_day_last.strftime('%Y-%m-%d'))  # | ds 구현\n예시: Dags full example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.decorators import task\n\n\n\nwith DAG(\n    dag_id=\"dags_python_with_macro\",\n    schedule=\"10 0 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    # macro 이용\n    @task(task_id='task_using_macros',\n      templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") + macros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}',\n                      'end_date': '{{ (data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) + macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n     }\n    )\n    \n    def get_datetime_macro(**kwargs):\n        \n        templates_dict = kwargs.get('templates_dict') or {}\n        if templates_dict:\n            start_date = templates_dict.get('start_date') or 'start_date없음'\n            end_date = templates_dict.get('end_date') or 'end_date없음'\n            print(start_date)\n            print(end_date)\n\n\n    # python 이용\n    @task(task_id='task_direct_calc')\n    def get_datetime_calc(**kwargs):\n        from dateutil.relativedelta import relativedelta # 스케쥴러 부하 경감을 위해 task안에다가 library호출\n        # 다시 말해서, scheduler는 dag이 실행되지 않더라도 사용자가 작성한 dag을 주기적으로 문법적인 오류가 있는지를 검사하기 위해 parsing함\n        # DAG이 시작하기 이전 code (즉, `with DAG` 이전 부분) 와 task가 시작하기 이전 code (`as dag:` 이후 부분과 task 선언 이전 부분)를 parsing 및 검사\n        # 하지만 operator 안 과 task decorator안에 있는 부분은 parsing 및 검사하지 않음.\n        # 실제로 대형 프로젝트에서 겪는 scheduluer부하 문제를 해결하는 팁이 될 수 있음\n        data_interval_end = kwargs['data_interval_end']\n        prev_month_day_first = data_interval_end.in_timezone('Asia/Seoul') + relativedelta(months=-1, day=1)\n        prev_month_day_last = data_interval_end.in_timezone('Asia/Seoul').replace(day=1) +  relativedelta(days=-1)\n        print(prev_month_day_first.strftime('%Y-%m-%d'))\n        print(prev_month_day_last.strftime('%Y-%m-%d'))\n\n    get_datetime_macro() &gt;&gt; get_datetime_calc()\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#airflow에서-사용법",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#airflow에서-사용법",
    "title": "Template Variabler",
    "section": "",
    "text": "오퍼레이터 파라미터 입력시 중괄호 {} 2개({{}})를 이용하면 Airflow에서 기본적으로 제공하는 변수들(ex: 수행 날짜, DAG_ID)을 치환된 값으로 입력할 수 있음.\n\nairflow에서 제공하는 기본 variable list or google ‘airflow templates reference’\n{ data_interval_start }: schedule 구간의 시작점을 반환, pendulum.DateTime는 timestamped type (중요)\n{ data_interval_end }: schedule 구간의 끝점을 반환 (중요)\n{ ds }: { data_interval_start }의 value를 string 형태(‘YYYY-MM-DD’)로 반환 (중요)\n{ ds_nodash }: {ds}를 string 형태(‘YYYYMMDD’) 로 반환\n{ ts }: timestamped 의 약자로 { data_interval_start }를 string 형태(‘YYYY-MM-DD T00:00:00+00:00’) 로 반환\n{ ts_nodash_with_tz }: timestamped 의 약자로 { ts }를 string 형태(‘YYYYMMDDT000000+0000’) 로 반환\n{ ts_nodash  }: timestamped 의 약자로 {ts}를 string 형태(‘YYYYMMDDT000000’) 로 반환\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용 가능하지는 않음!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요 or google airflow operators\n\noperator 설명란에 parameters 란에 각 parameter의 설명 부분 맨 끝에 (templated) 라고 적혀 있는 parameter는 jinja template 사용 가능. template_fields에 요약되어 있음\n예를 들어, airflow.operators.bash 에서 Parameters를 보면\n\nbash_command (str) – The command, set of commands or reference to a bash script (must be ‘.sh’) to be executed. (templated) - jinja template 사용 가능\nenv (dict[str, str] | None) – If env is not None, it must be a dict that defines the environment variables for the new process; these are used instead of inheriting the current process environment, which is the default behavior. (templated) - jinja template 사용 가능\nappend_env (bool) – If False(default) uses the environment variables passed in env params and does not inherit the current process environment. If True, inherits the environment variables from current passes and then environment variable passed by the user will either update the existing inherited environment variables or the new variables gets appended to it - 사용 불가\noutput_encoding (str) – Output encoding of bash command - 사용 불가\nskip_on_exit_code (int | Container[int] | None) – If task exits with this exit code, leave the task in skipped state (default: 99). If set to None, any non-zero exit code will be treated as a failure. - 사용 불가\ncwd (str | None) – Working directory to execute the command in. If None (default), the command is run in a temporary directory. - 사용 불가\ntemplate_fields: Sequence[str]= (‘bash_command’, ‘env’)[source]\n\n하지만, parameter 설명란과 template_fields에 template 변수가 일치하지 않을 수 있음. 그럴 땐 template_fiedls를 기준으로 함\n예를 들어, airflow.operators.python에서 Parameters를 보면 아래와 같이 templates_dict만 사용 가능한 것 처럼 보이지만 template_fields를 보면 op_kwargs 와 op_args도 jinja template으로 사용 가능한 것을 알 수 있다.\n\npython_callable (Callable) – A reference to an object that is callable\nop_kwargs (Mapping[str, Any] | None) – a dictionary of keyword arguments that will get unpacked in your function\nop_args (Collection[Any] | None) – a list of positional arguments that will get unpacked when calling your callable\ntemplates_dict (dict[str, Any] | None) – a dictionary where the values are templates that will get templated by the Airflow engine sometime between __init__and execute takes place and are made available in your callable’s context after the template has been applied. (templated) - 사용 가능\ntemplates_exts (Sequence[str] | None) – a list of file extensions to resolve while processing templated fields, for examples [‘.sql’, ‘.hql’]\nshow_return_value_in_logs (bool) – a bool value whether to show return_value logs. Defaults to True, which allows return value log output. It can be set to False to prevent log output of return value when you return huge data such as transmission a large amount of XCom to TaskAPI.\ntemplate_fields: Sequence[str]= (‘templates_dict’, ‘op_args’, ‘op_kwargs’)[source]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#bashoperator",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#bashoperator",
    "title": "Template Variabler",
    "section": "",
    "text": "recap) Bash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n\n\n파라미터\n\nbash_command (str) (templated)\nenv (dict[str, str] | None) (templated)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)\n\nDAG Full example ```markdown from airflow import DAG import pendulum import datetime from airflow.operators.bash import BashOperator\nwith DAG( dag_id=“dags_bash_with_template”, schedule=“10 0 * * *“, start_date=pendulum.datetime(2023, 3, 1, tz=”Asia/Seoul”), catchup=False ) as dag: bash_t1 = BashOperator( task_id=‘bash_t1’, bash_command=‘echo “data_interval_end: {{ data_interval_end }}”’ )\n  bash_t2 = BashOperator(\n      task_id='bash_t2',\n      env={\n          'START_DATE':'{{data_interval_start | ds }}', #| ds: time stamped type을 YYYY-MM-DD로 변환\n          'END_DATE':'{{data_interval_end | ds }}' #| ds: time stamped type을 YYYY-MM-DD로 변환\n      },\n      bash_command='echo $START_DATE && echo $END_DATE' #shell script syntax: statement1 && statement2\n      # statment1이 성공하면 statement2를 실행한다.\n  )\n\n  bash_t1 &gt;&gt; bash_t2\n```\n\nAirflow Web Service Result\n\n[2023-06-17, 01:00:00 UTC] {taskinstance.py:1327} INFO - Executing &lt;Task(BashOperator): bash_t1&gt; on 2023-06-15 15:10:00+00:00 에서 2023-06-15 15:10:00+00:00의 +뒤는 time zone을 의미. 00:00 이면 utc (세계 표준시로 한국 보다 9시간 느림)를 의미. 한국 시간으로 변환하려면 9시간을 더해야한다. 즉, 2023-06-16 00:10:00이 한국 서울 시간임\n\n\n[2023-06-17, 01:00:02 UTC] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'echo $START_DATE && echo $END_DATE']\n[2023-06-17, 01:00:02 UTC] {subprocess.py:86} INFO - Output:\n[2023-06-17, 01:00:02 UTC] {subprocess.py:93} INFO - 2023-06-15\n[2023-06-17, 01:00:02 UTC] {subprocess.py:93} INFO - 2023-06-16`"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#데이터-추출-예시",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#데이터-추출-예시",
    "title": "Template Variabler",
    "section": "",
    "text": "상황\n\nDaily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행- 매일 00:00에 데이터 가져오기)\n전체 data는 너무 많기 때문에 증분된 데이터만 가져오기. 즉 오늘이 2023/02/25 라면 2023/02/24 와 2023/02/25 사이에 있는 data만 가져온다.\n\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n\n생각해볼 point: 각 관점에 따라 날짜가 다름\n\n데이터 관점의 시작일: 2023-02-24\n데이터 관점의 종료일: 2023-02-25\nDAG이 실행되는 시점: 2023-02-25\nairflow는 ETL을 위한 도구로 만들어졌기 때문에 data관점에서 전처리를 하는 사상이 담겨져 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#airflow-날짜-template-변수",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#airflow-날짜-template-변수",
    "title": "Template Variabler",
    "section": "",
    "text": "예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start (airflow new version - from 2.5.2 version)\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (airflow old version - until 2.5.1 version)\n위와 같이 airflow의 대부분의 변수들이 논리적 기준일을 데이터 관점의 시작일을 기준으로 한다.\nexecution_date 라는 명명법이 너무 혼란스러웠음 실행 날짜란 의미는 대부분의 사람들이 dag이 실행되는 날로 인식을 하는데 data관점에서 날짜를 출력함. 그래서 data_interval_start로 변수명을 바꿈\n\nex. 2023-02-25 배치일 (DAG이 실행되는 날짜)\n\n= data_interval_end (airflow new version - from 2.5.2 version)\n=\n=\n=\n= next_execution_date (airflow old version - until 2.5.1 version)\nnext execution_date 라는 명명법은 대부분의 사람들이 dag이 실행되는 날로 인식을 하기 때문에 혼란스러워서 data_interval_end로 바꿈. 왜냐면 현재 dag 실행 날짜가 next execution_date로 표시되고 그 이전 실행 날짜를 execution_date로 표기해서 실제 실행날짜와 변수 이름이 맞지가 않음.\n그러므로, 배치가 돌고있는 현재 날짜를 출력하고 싶으면 data_interval_end에 접근해야하고 그 이전 배치의 날짜를 출력하고 싶으면 data_interval_start에 접근해야한다.\n\n\nFull Exmaple\n\nDAG\n\n# dags_bash_with_template.py\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"dags_python_show_templates\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 6, 10, tz=\"Asia/Seoul\"),\n    catchup=True #catchup 할때 task 순서를 유념해서 연결시키지 않으면 dags실행을 pause/unpuase시 task들이 꼬일 수 있다.\n) as dag:\n\n    @task(task_id='python_task')\n    def show_templates(**kwargs):\n        from pprint import pprint \n        pprint(kwargs) #pprint는 리스트나 딕셔너리를 줄넘김으로 이쁘게 출력해줌\n\n    show_templates()\n\nAirflow Web Service Result\n\n[2023-06-17, 01:40:17 UTC] {logging_mixin.py:149} INFO - {'conf': &lt;***.configuration.AirflowConfigParser object at 0x7f668aeec910&gt;,\n'conn': None,\n'dag': &lt;DAG: dags_python_show_templates&gt;,\n'dag_run': &lt;DagRun dags_python_show_templates @ 2023-06-09 00:30:00+00:00: scheduled__2023-06-09T00:30:00+00:00, state:running, queued_at: 2023-06-17 01:40:15.833772+00:00. externally triggered: False&gt;,\n**'data_interval_end': DateTime(2023, 6, 10, 0, 30, 0, tzinfo=Timezone('UTC')),**\n**'data_interval_start': DateTime(2023, 6, 9, 0, 30, 0, tzinfo=Timezone('UTC')),**\n**'ds': '2023-06-09',**\n**'ds_nodash': '20230609',**\n*'execution_date': &lt;Proxy at 0x7f665d530640 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'execution_date', DateTime(2023, 6, 9, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n'expanded_ti_count': None,\n'inlets': [],\n**'logical_date': DateTime(2023, 6, 9, 0, 30, 0, tzinfo=Timezone('UTC')),**\n'macros': &lt;module '***.macros' from '/home/***/.local/lib/python3.7/site-packages/***/macros/__init__.py'&gt;,\n*'next_ds': &lt;Proxy at 0x7f665d530690 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'next_ds', '2023-06-10')&gt;*,\n*'next_ds_nodash': &lt;Proxy at 0x7f665d5306e0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'next_ds_nodash', '20230610')&gt;*,\n*'next_execution_date': &lt;Proxy at 0x7f665d530780 with factory functools.partial*(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'next_execution_date', DateTime(2023, 6, 10, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n'outlets': [],\n'params': {},\n'prev_data_interval_end_success': DateTime(2023, 6, 6, 0, 30, 0, tzinfo=Timezone('UTC')),\n'prev_data_interval_start_success': DateTime(2023, 6, 5, 0, 30, 0, tzinfo=Timezone('UTC')),\n*'prev_ds': &lt;Proxy at 0x7f665d5307d0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_ds', '2023-06-08')&gt;*,\n*'prev_ds_nodash': &lt;Proxy at 0x7f665d530820 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_ds_nodash', '20230608')&gt;*,\n*'prev_execution_date': &lt;Proxy at 0x7f665d530870 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_execution_date', DateTime(2023, 6, 8, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n*'prev_execution_date_success': &lt;Proxy at 0x7f665d5308c0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_execution_date_success', DateTime(2023, 6, 5, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n'prev_start_date_success': DateTime(2023, 6, 17, 1, 40, 15, 103936, tzinfo=Timezone('UTC')),\n'run_id': 'scheduled__2023-06-09T00:30:00+00:00',\n'task': &lt;Task(_PythonDecoratedOperator): python_task&gt;,\n'task_instance': &lt;TaskInstance: dags_python_show_templates.python_task scheduled__2023-06-09T00:30:00+00:00 [running]&gt;,\n'task_instance_key_str': 'dags_python_show_templates__python_task__20230609',\n'templates_dict': None,\n'test_mode': False,\n'ti': &lt;TaskInstance: dags_python_show_templates.python_task scheduled__2023-06-09T00:30:00+00:00 [running]&gt;,\n*'tomorrow_ds': &lt;Proxy at 0x7f665d530910 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'tomorrow_ds', '2023-06-10')&gt;*,\n*'tomorrow_ds_nodash': &lt;Proxy at 0x7f665d530960 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'tomorrow_ds_nodash', '20230610')&gt;*,\n'triggering_dataset_events': {},\n**'ts': '2023-06-09T00:30:00+00:00',**\n**'ts_nodash': '20230609T003000',**\n**'ts_nodash_with_tz': '20230609T003000+0000',**\n'var': {'json': None, 'value': None},\n*'yesterday_ds': &lt;Proxy at 0x7f665d5309b0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'yesterday_ds', '2023-06-08')&gt;*,\n*'yesterday_ds_nodash': &lt;Proxy at 0x7f665d530a00 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'yesterday_ds_nodash', '20230608')&gt;}*\n\n위에서, 과거 혼란을 주는 변수들은 italic채로 표시를 했고 출력물을 보면 depreacted될 예정이라고 적혀져 있어 곧 안쓰일 예정이라고 적혀져 있다.\nbold채로 쓰여진 출력물이 개선된 명명법으로 이름 붙여진 변수들인데 대부분의 시간들이 data관점에서 logical date를 선정한 것을 알 수 있다. dag 배치 실행 날짜를 보기 위해선 data_interval_end를 보면 2023-06-10이 실행 날짜인 것을 알 수 있다. logical date의 2023-06-10 이전 배치 실행 날짜이다.\n실제 업무나 작업시 data_interval_end가 자주 쓰인다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#python-오퍼레이터에서-template-변수-사용",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#python-오퍼레이터에서-template-변수-사용",
    "title": "Template Variabler",
    "section": "",
    "text": "Python 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs (templated)\nop_args (templated)\ntemplates_dict (templated)\ntemplates_exts\nshow_return_value_in_logs\n\nOperator Template\n\njinja template을 이용하여 runtime date를 얻을 때 2가지 방식이 있음\n\n함수를 만들어 op_kwargs에 jinja template 변수를 만들고 이 변수에 저장된 값을 꺼내 쓰는 법\n**kwargs로부터 얻음 - 2번째 방법이 더 편한것 같지만 개인 취향에 따름\n\n함수를 만들어 jinja template를 이용해 연산\n\n\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()\nFull Example\n\nDAGS\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"dags_python_template\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 3, 10, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    def python_function1(start_date, end_date, **kwargs):\n        print(start_date)\n        print(end_date)\n\n    python_t1 = PythonOperator(\n        task_id='python_t1',\n        python_callable=python_function1,\n        op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n    )\n\n    @task(task_id='python_t2')\n    def python_function2(**kwargs):\n        print(kwargs)\n        print('ds:' + kwargs['ds'])\n        print('ts:' + kwargs['ts'])\n        print('data_interval_start:' + str(kwargs['data_interval_start']))\n        print('data_interval_end:' + str(kwargs['data_interval_end']))\n        print('task_instance:' + str(kwargs['ti']))\n\n\n    python_t1 &gt;&gt; python_function2() #decorator사용시 함수를 실행주기만 해도 task가 생성되기 때문에 함수를 task로 연결할 수 있다.\n\nAirflow Web Service Result"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#macro-변수의-이해",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#macro-변수의-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "jinja template 안에서 날짜 연산을 가능하게 해주는 기능\n\n파이썬의 datetime + dateutil library로 가능\n\nMacro 변수의 필요성\n\n가령, 어떤 DAG의 스케줄은 매일 말일에 도는 스케줄 (0 0 L * *)인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은 상황. 즉,\n\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\n날짜 구간을 hard coding 해놓는게 아니라 DAG이 도는 시점에 따라 알맞게 들어가야 함.\n예를 들어, 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되어야함 DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지 생각해봐야함 (반드시, data_interval_end 에서 하루 뺀 값이 나와야 하는데)\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN {{ data_interval_start }} AND {{ data_interval_start }} - 1day\n'''\n{ data_interval_start } - 1day 이 부분 연산을 하는데 macro 변수가 쓰임\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime, python의 datetime library 를 이용가능하게 하거나 datetime library를 template 변수내에서 날짜 연산 기능\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta, 날짜 연산 기능\n\n\nmacros.dateutil\nA reference to the dateutil package, python의 dateutil library를 이용가능하게 하거나 dateutil library를 template 변수내에서 이용가능하게 하여 날짜 연산 기능\n\n\nmacros.time\nThe standard lib’s time, 날짜 연산 기능\n\n\nmacros.uuid\nThe standard lib’s uuid, 고유 ID 부여\n\n\nmacros.random\nThe standard lib’s random, python rand() 사용가능하게 해줌\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리, 매우 빈번하게 쓰임\n예를 들어, macros.dateutil에서 relativedelta.relativedelta() 함수를 쓸수 있도록 해줌. macros.dateutil.relativedelta.relativedelta()\n\nMacro를 잘 쓰려면 python의 datetime 및 dateutil library에 익숙해져야 함."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#파이썬-datetime-dateutil-라이브러리-이해",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#파이썬-datetime-dateutil-라이브러리-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "만약, jupyter notebook (대화형 입력창)이 없는 환경인데 jupyter notebook에서 python을 실행하고 싶으면 terminal에 다음 명령어를 실행해서 설치\n\n대화형 입력창: 일련의 명령어들을 한번에 실행시키는 script code 형식이 아니라 명령어 한줄마다 결과값을 볼 수 있는 창\n\n\npip install jupyter # 약 5분 소요\npython -m notebook\n\n\nCode\nfrom datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) #월을 1월로 변경하는 명령어, relativedelta library 사용\nprint(now.replace(month=1)) # 월을 1월로 변경하는 명령어, datetime library 사용, print(now+relativedelta.relativedelta(month=1)) 와 같은 명령어\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기: 먼저 month 값에서 1을 빼고 그 결과 값(month)의 가장 가까운 말일을 자동으로 선택해줌\n\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\n\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기. relativedelta library장점이 연산 연러개를 이어 붙일 수 있음\n\n\ncurrent time:2003-03-30 00:00:00\n-------------month operation-------------\n2003-01-30 00:00:00\n2003-01-30 00:00:00\n2003-02-28 00:00:00\n-------------day operation-------------\n2003-03-01 00:00:00\n2003-03-01 00:00:00\n2003-03-29 00:00:00\n-------------multiple operations-------------\n2003-02-27 00:00:00"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "title": "Template Variabler",
    "section": "",
    "text": "예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일 (6#2)에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''}, #env 변수에 template 변수를 작성\n)\n\n이 부분에 template + macro 활용\n\nDAG 예시1.\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_with_macro_eg1\",\n    schedule=\"10 0 L * *\", #매월 말일날 도는 DAG\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # START_DATE: 전월 말일, END_DATE: 1일 전\n    bash_task_1 = BashOperator(\n        task_id='bash_task_1',\n        env={'START_DATE':'{{ data_interval_start.in_timezone(\"Asia/Seoul\") | ds }}',\n                #template 변수에 꺼내쓰는 모든 날짜 변수는 default로 timezone이 UTC로 맞춰져있기 때문에 현지에 맞게 고쳐줘야한다. 한국 시간에 맞추려면 9시간을 더해야하는데, .in_timezone(\"Asia/Seoul\")로 해결 가능\n                #data_interval_start.in_timezone(\"Asia/Seoul\")는 timestamp형식으로 출력되기 때문에 yyyy-mm-dd로 출력하기위해 ds 연산 붙임\n             'END_DATE':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") - macros.dateutil.relativedelta.relativedelta(days=1)) | ds}}'\n             # 연산자가 -로 되어 있이기 때문에  days=-1로 할필요없음\n        },\n        bash_command='echo \"START_DATE: $START_DATE\" && echo \"END_DATE: $END_DATE\"'\n    )\n\n예시2. DAG full Exmaple\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_with_macro_eg2\",\n    schedule=\"10 0 * * 6#2\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # START_DATE: 2주전 월요일, END_DATE: 2주전 토요일\n    # 예를 들어, 2023-04-01 토요일은 첫째 주 토요일로 인식\n    # 2023-04-08 토요일은 둘째 주 토요일로 인식 (군대에서 순서를 세는 방식과 다름)\n    # 2023-04-08 토요일을 START_DATE(배치일)로 정하면 END_DATE는 배치일 기준으로부터 2 주를 뺀 토요일은 2023-03-25가 된다.\n    # 배치일 기준 (2023-04-08 토요일)으로 그 전 배치의 START_DATE를 구하려면 END_DATE로부터 5일을 뺀 날짜인 2023-03-20 (월요일)이 START_DATE가 된다.\n    # 이는 즉, 배치일 기준 (2023-04-08 토요일) 19일을 빼준 날짜와 같다.\n    bash_task_2 = BashOperator(\n        task_id='bash_task_2',\n        env={'START_DATE':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") - macros.dateutil.relativedelta.relativedelta(days=19)) | ds}}', #2주전 월요일\n             'END_DATE':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") - macros.dateutil.relativedelta.relativedelta(days=14)) | ds}}' #2주전 툐요일\n        },\n        bash_command='echo \"START_DATE: $START_DATE\" && echo \"END_DATE: $END_DATE\"'\n    )"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#python-operator에서-template-변수-사용",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#python-operator에서-template-변수-사용",
    "title": "Template Variabler",
    "section": "",
    "text": "Python 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs (templated)\nop_args (templated)\ntemplates_dict (templated)\ntemplates_exts\nshow_return_value_in_logs\n\nOperator Template\n\njinja template을 이용하여 runtime date를 얻을 때 2가지 방식이 있음\n\n함수를 만들어 op_kwargs에 jinja template 변수를 만들고 이 변수에 저장된 값을 꺼내 쓰는 법\n**kwargs로부터 얻음 - 2번째 방법이 더 편한것 같지만 개인 취향에 따름\n\n함수를 만들어 jinja template를 이용해 연산\n\n\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()\nFull Example\n\nDAGS\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"dags_python_template\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 3, 10, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    def python_function1(start_date, end_date, **kwargs):\n        print(start_date)\n        print(end_date)\n\n    python_t1 = PythonOperator(\n        task_id='python_t1',\n        python_callable=python_function1,\n        op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n    )\n\n    @task(task_id='python_t2')\n    def python_function2(**kwargs):\n        print(kwargs)\n        print('ds:' + kwargs['ds'])\n        print('ts:' + kwargs['ts'])\n        print('data_interval_start:' + str(kwargs['data_interval_start']))\n        print('data_interval_end:' + str(kwargs['data_interval_end']))\n        print('task_instance:' + str(kwargs['ti']))\n\n\n    python_t1 &gt;&gt; python_function2() #decorator사용시 함수를 실행주기만 해도 task가 생성되기 때문에 함수를 task로 연결할 수 있다.\n\nAirflow Web Service Result"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html",
    "title": "Data Share",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nXcom stands for Cross Communication.\nAirflow DAG 안 Task 간 작은 데이터 (or Message) 공유를 위해 사용되는 기술 (1개의 Dag 안에 있는 task끼리만 data 공유)\n\n예를 들어, Task1의 수행 중 내용이나 결과를 Task2에서 사용 또는 입력으로 주고 싶은 경우\ntask1 은 push, task2는 pull과 같은 tasks간 데이터 공유에 유용\n\n주로 작은 규모의 데이터 공유를 위해 사용\n\nXcom 내용은 meta DB의 Xcom 테이블에 값이 저장됨\n\n1GB 이상의 대용량 데이터 공유를 위해서는 외부 솔루션 사용 필요 (AWS의 S3, GCP의 GCS, HDFS (Hadoop File System) 등)\n\n\n\n\n\n\n\n\n크게 두 가지 방법으로 Xcom 사용 가능\n\n**kwargs에 존재하는 ti (task_instance) 객체 활용\n\n@task(task_id='python_xcom_push_task')\ndef xcom_push(**kwargs):\n    ti = kwargs['ti']\n    ti.xcom_push(key=\"result1\", value=\"value_1\") \n    ti.xcom_push(key=\"result2\", value=[1,2,3])\n    #xcom_push: xcom에다가 data를 올릴 수 있음\n    #data를 올릴 때는 key:value 형태로 올리기\n    #template 변수에서 task_instance 라는 객체를 얻을 수 있으며 task_instance 객체가 가진 xcom_push 메서드를 활용할 수 있음\n\n@task(task_id='python_xcom_pull_task')\ndef xcom_pull(**kwargs):\n    ti = kwargs['ti']\n    value_key1 = ti.xcom_pull(key=\"result1\") # value_1이 value_key1에 저장됨\n    value_key2 = ti.xcom_pull(key=\"result2\",\n    task_ids='python_xcom_push_task') # [1,2,3]이 value_key2에 저장됨\n    #xcom_pull: xcom으로부터 data를 내려 받을 수 있음\n    #data를 올릴 때는 key:value 형태로 올리기\n    print(value_key1)\n    print(value_key2)\n\nxcome_pull()을 할때 key값만 줘도 되고 key값과 task_ids값을 둘다 줘도 된다.\n\nkey값만 줘도 될때\n\nxcom_push를 한 task가 1개 밖에 없을 때 사용 가능\n혹은, key값이 중복될 때 xcom_push를 한 task가 여러 개 있을 때도 사용 가능한데 가장 마지막 (최신) task의 key값을 호출 한다.\n만약, key값이 중복이 되지 않는 다면 key값만으로도 data를 내려 받을 수 있다.\n\nkey값과 task_ids둘다 줘야할 때\n\nkey값이 중복되는 xcom_push를 한 task가 여러 개 있을 때 선택적으로 원하는 task의 data를 가지고 오고 싶으면 해당 task의 task_ids를 명시적으로 적어줘야한다.\n\n예를 들어,\n\n# 5개의 tasks 존재하는\n# task1: xcom_push(key='result1'...)\n# task2: xcom_push(key='result1'...)\n# task3: xcom_push(key='result2'...)\n# task4: xcom_pull(key='result1'...)\n# task5: xcom_pull(key='result1',task_ids=...)\n\ntask4가 수행이 될때 task1의 xcom을 가져우는게 아니라 가장 최신에 수행된 task2의 xcom을 가져오게 된다.\ntask1의 xcom을 가지고 오고 싶을땐 task5와 같이 task1의 task_id를 task5의 task_ids에 명시해주면 된다.\n가장 안전한 방법은 task의 key값과 task_ids를 명시적으로 적어주는 것이다. 아니면 tasks의 key값을 절대 중복이 되지않도록 적어주는 것이다.\n\n\n\n파이썬 함수의 return 값 활용\n\n(1안)\n\n@task(task_id='xcom_push_by_return')\ndef xcom_push_by_return(**kwargs):\n    transaction_value = 'status Good'\n    return transaction_value\n@task(task_id='xcom_pull_by_return')\ndef xcom_pull_by_return(status, **kwargs):\n    print(status)\nxcom_pull_by_return(xcom_push_by_return()) \n\nxcom을 이용한 task의 flow 정해주는 또 다른 방식\n암묵적인 task의 순서: xcom_push_by_return() &gt;&gt; xcom_pull_by_return()\n위의 스크립트에서 xcom_pull() 또는 xcom_push()가 명시적으로 쓰이지진 않았지만 airflow에서는 Task 데커레이터 사용시 함수 입력/출력 관계만으로 Task flow 정의가 된다. 즉, xcom_pull_by_return(xcom_push_by_return()) = xcom_push_by_return() &gt;&gt; xcom_pull_by_return()\nTask 데커레이터 사용시 custom 함수가 return을 하게 되면 자동으로 xcom에 data가 올라가게 된다.\n(2안)\n\n\n@task(task_id='xcom_push_by_return')\ndef xcom_push_return(**kwargs):\n    transaction_value = 'status Good'\n    return transaction_value\n    # return 한 값은 자동으로 xcom에 key='return_value', task_ids=task_id 로 저장됨\n\n@task(task_id='xcom_pull_by_return')\ndef xcom_pull_return_by_method(**kwargs):\n    ti = kwargs['ti']\n    pull_value = ti.xcom_pull(key='return_value', task_ids='xcom_push_by_return')\n    # ti.xcom_pull()을 이용하여 return 한 값을 꺼낼 때는 key를 명시하지 않아도 됨. (자동으로 key=return_value 를 찾음)\n    # task_ids='xcom_push_by_return' return한 Task가 여러개 있을 때는 task_ids 를 지정\n    print(pull_value)\n\nxcom_push_by_return() &gt;&gt; xcom_pull_by_return() # 2안에서는 task flow를 명시적으로 적어줘야함.\n\nDAG Full Example\n\n1안 DAG Full Exmaple ```markdown from airflow import DAG import pendulum import datetime from airflow.decorators import task\nwith DAG( dag_id=“dags_python_with_xcom_eg2”, schedule=“30 6 * * *“, start_date=pendulum.datetime(2023, 3, 1, tz=”Asia/Seoul”), catchup=False ) as dag:\n  @task(task_id='python_xcom_push_by_return')\n  def xcom_push_result(**kwargs):\n      return 'Success'\n\n\n  @task(task_id='python_xcom_pull_1')\n  def xcom_pull_1(**kwargs):\n      ti = kwargs['ti']\n      value1 = ti.xcom_pull(task_ids='python_xcom_push_by_return')\n      print('xcom_pull 메서드로 직접 찾은 리턴 값:' + value1)\n\n  @task(task_id='python_xcom_pull_2')\n  def xcom_pull_2(status, **kwargs):\n      print('함수 입력값으로 받은 값:' + status)\n\n\n  python_xcom_push_by_return = xcom_push_result() \n  # airflow의 task decorator가 쓰였기 때문에 python_xcom_push_by_return에 \n  # 단순한 'Sucess' 스트링이 할당되는게 아니라 decorator object가 할당된다.\n  xcom_pull_2(python_xcom_push_by_return)\n  python_xcom_push_by_return &gt;&gt; xcom_pull_1()\n\n  # 암묵적인 task flow는\n  # xcom_push_result &gt;&gt;[xcom_pull_2, xcom_pull_1] 형태임\n```\n2안 DAG Full Example ```markdown from airflow import DAG import pendulum import datetime from airflow.decorators import task\nwith DAG( dag_id=“dags_python_with_xcom_eg1”, schedule=“30 6 * * *“, start_date=pendulum.datetime(2023, 3, 1, tz=”Asia/Seoul”), catchup=False ) as dag:\n  @task(task_id='python_xcom_push_task1')\n  def xcom_push1(**kwargs):\n      ti = kwargs['ti']\n      ti.xcom_push(key=\"result1\", value=\"value_1\")\n      ti.xcom_push(key=\"result2\", value=[1,2,3])\n\n  @task(task_id='python_xcom_push_task2')\n  def xcom_push2(**kwargs):\n      ti = kwargs['ti']\n      ti.xcom_push(key=\"result1\", value=\"value_2\") \n      # python_xcom_push_task1의 key값은 같지만 value는 다름\n      ti.xcom_push(key=\"result2\", value=[1,2,3,4])\n\n  @task(task_id='python_xcom_pull_task')\n  def xcom_pull(**kwargs):\n      ti = kwargs['ti']\n      value1 = ti.xcom_pull(key=\"result1\")\n      value2 = ti.xcom_pull(key=\"result2\", task_ids='python_xcom_push_task1')\n      print(value1)\n      print(value2)\n\n\n  xcom_push1() &gt;&gt; xcom_push2() &gt;&gt; xcom_pull()\n  # xcom_pull()에서 key값이 result1으로만 명시되었기 때문에 value1에는 xcom_push2()의 'value_2'가 들어감    \n```\n\nairflow web service에서 log 대신 xcom을 사용해 결과값을 확인\n\n\n\n\n\nXcom push 방법\n\nti.xcom_push 명시적 사용\n함수 return\n\nXcom pull 방법\n\nti.xcom_pull 명시적 사용\nreturn 값을 input으로 사용\n\n\n\n\n\n\n\n\n\nBash 오퍼레이터에서 template 문법을 쓸수 있는 parameters: env, bash_command\ntemplate 이용하여 push/pull\n\nbash_push = BashOperator(\n    task_id='bash_push',\n    bash_command=\"echo START && \"\n                \"echo XCOM_PUSHED \"\n                \"{{ ti.xcom_push(key='bash_pushed',value='first_bash_message') }} && \"\n                \"echo COMPLETE\" \n                # bash 같은 경우엔 출력하는 값이 return값으로 간주됨. \n                # 위의 경우와 같이 여러 출력물(&&로 연결된 3개의 출력물)이 있을 경우 마지막 출력물(COMPLETE)이 자동으로 return_value 에 저장됨\n)\nbash_pull = BashOperator(\n    task_id='bash_pull',\n    env={'PUSHED_VALUE':\"{{ ti.xcom_pull(key='bash_pushed') }}\",\n        'RETURN_VALUE':\"{{ ti.xcom_pull(task_ids='bash_push') }}\"}, \n        # env 는 key: value 형태로 데이터를 받음\n        # task_ids 만 지정하면 key='return_value' 를 의미함\n        # RETURN_VALUE에 'complete'이 들어감\n    bash_command=\"echo $PUSHED_VALUE && echo $RETURN_VALUE \",\n    do_xcom_push=False \n    # bash_command에서 출력되는 \"echo $PUSHED_VALUE && echo $RETURN_VALUE \"의 \n    # 출력문을 자동으로 xcom에 올리지 말라는 의미\n)\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_with_xcom\",\n    schedule=\"10 0 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    bash_push = BashOperator(\n    task_id='bash_push',\n    bash_command=\"echo START && \"\n                 \"echo XCOM_PUSHED \"\n                 \"{{ ti.xcom_push(key='bash_pushed',value='first_bash_message') }} && \"\n                 \"echo COMPLETE\"\n    )\n\n    bash_pull = BashOperator(\n        task_id='bash_pull',\n        env={'PUSHED_VALUE':\"{{ ti.xcom_pull(key='bash_pushed') }}\",\n            'RETURN_VALUE':\"{{ ti.xcom_pull(task_ids='bash_push') }}\"},\n        bash_command=\"echo $PUSHED_VALUE && echo $RETURN_VALUE \",\n        do_xcom_push=False\n    )\n\n    bash_push &gt;&gt; bash_pull\n\n\n\n\nBash_command에 의해 출력된 값은 자동으로 return_value로 저장된다 (마지막 출력 문장만)\nreturn_value를 꺼낼 때는 xcom_pull에서 task_ids 값만 줘도 된다.\n키가 지정된 xcom 값을 꺼낼 때는 key 값만 줘도 된다 (단, 다른 task에서 동일 key로 push 하지 않았을 때만)\n\n\n\n\n\n\n\n@task task_id =='python push'\ndef python_push_xcom\n    result_dict = {'status':' Good','data':[1,2,3],'options_cnt': 100}\n    return result_dict\nbash_pull = BashOperator(\n    task_id='bash_pull',\n    env={\n        'STATUS': '{{ti.xcom_pull(task ids=\"python push\")[\"status\"]}}', #task_ids만 있으면 위의 파이썬 함수에서 리턴값을 자동으로 받음\n        'DATA': '{{ti.xcom_pull(task ids=\"python push\")[\"data\"]}}',\n        'OPTIONS_CNT': '{{ti.xcom_pull(task_ids=\"python_push\")[\"options_cnt\"]}}'\n    },\n    bash_command = 'echo $STATUS && echo $DATA && echo $OPTIONS_CNT'\n)\n \npython_push_xcom() &gt;&gt; bash_pull\n\n\n\nbash_push = BashOperator(\ntask_id ='bash_push',\nbash_command='echo PUSH_START'\n    '{{ti.xcom_push(key=\"bash_pushed\",value=200) }}&& 'echo PUSH_COMPLETE'\n)\n\n@task(task_id =='python_pull')\ndef python_pull_xcom(**kwargs): \n    ti = kwargs ['ti']\n    status_value= ti.xcom_pull(key ='bash_pushed')\n    return_value= ti.xcom_pull(task_ids ='bash_push')\n    print('status_value:'+ str (status_value))\n    print('return_value:'+ return_value)\n    bash_push&gt;&gt; python_pull_xcom()\n\nDAG Full Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_python_with_xcom\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    @task(task_id='python_push')\n    def python_push_xcom():\n        result_dict = {'status':'Good','data':[1,2,3],'options_cnt':100}\n        return result_dict\n\n    bash_pull = BashOperator(\n        task_id='bash_pull',\n        env={\n            'STATUS':'{{ti.xcom_pull(task_ids=\"python_push\")[\"status\"]}}',\n            'DATA':'{{ti.xcom_pull(task_ids=\"python_push\")[\"data\"]}}',\n            'OPTIONS_CNT':'{{ti.xcom_pull(task_ids=\"python_push\")[\"options_cnt\"]}}'\n\n        },\n        bash_command='echo $STATUS && echo $DATA && echo $OPTIONS_CNT'\n    )\n    python_push_xcom() &gt;&gt; bash_pull\n\n    bash_push = BashOperator(\n    task_id='bash_push',\n    bash_command='echo PUSH_START '\n                 '{{ti.xcom_push(key=\"bash_pushed\",value=200)}} && '\n                 'echo PUSH_COMPLETE'\n    )\n\n    @task(task_id='python_pull')\n    def python_pull_xcom(**kwargs):\n        ti = kwargs['ti']\n        status_value = ti.xcom_pull(key='bash_pushed')\n        return_value = ti.xcom_pull(task_ids='bash_push')\n        print('status_value:' + str(status_value))\n        print('return_value:' + return_value)\n\n    bash_push &gt;&gt; python_pull_xcom()\n\n\n\n\n\n\n\nEmail 오퍼레이터를 이용하여 Xcom을 받아와야함\nEmail 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nto\nsubject\nhtml_content\nfiles\ncc\nbcc\nnime_subtype\nmime_charset\ncustom_headers\n\n\n@task(task_id='something_task') # python operator를 task decorator로 만듦\ndef some_logic(**kwargs):\n    from random import choice \n    #choice 함수: list, tuple, string 중 아무 값이나 꺼낼 수 있게 해주는 함수\n    return choice(['Success','Fail']) # either Success or Fail is return됨\nsend_email = EmailOperator(\n    task_id='send_email',\n    to='hjkim_sun@naver.com',\n    subject='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} some_logic 처리결과',\n    html_content='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} 처리 결과는 &lt;br&gt; \\ {{ti.xcom_pull(task_ids=\"something_task\")}} 했습니다 &lt;br&gt;'\n)\n\nDAG Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.decorators import task\nfrom airflow.operators.email import EmailOperator\n\nwith DAG(\n    dag_id=\"dags_python_email_operator\",\n    schedule=\"0 8 1 * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    @task(task_id='something_task')\n    def some_logic(**kwargs):\n        from random import choice \n        return choice(['Success','Fail'])\n\n\n    send_email = EmailOperator(\n        task_id='send_email',\n        to='hjkim_sun@naver.com',\n        subject='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} some_logic 처리결과',\n        html_content='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} 처리 결과는 &lt;br&gt; \\\n                    {{ti.xcom_pull(task_ids=\"something_task\")}} 했습니다 &lt;br&gt;'\n    )\n\n    some_logic() &gt;&gt; send_email\n\n\n\n\n\n\nXcom: 특정 DAG, 특정 schedule 에 수행되는 Task 간에만 공유 (즉, 어제 수행한 task와 오늘 수행한 task간에는 xcom을 사용하여 데이터 공유가 안됨)\nvariable: 모든 DAG 이 공유할 수 있는 전역 변수 사용\nVariable 등록하기\n\nairflow web service에서 전역 변수 등록 가능\n\nairflow web service의 Admin &gt;&gt; Variables &gt;&gt; Plus Button &gt;&gt; Key, Val, Description 작성 &gt;&gt; save\n\n전역 변수 사용하기: 실제 Variable 의 Key, Value 값은 메타 DB 에 저장됨 (variable 테이블)\n\n방법1) Variable 라이브러리 이용 , 파이썬 문법을 이용해 미리 가져오기\n\nfrom airflow operators bash import BashOperator\nfrom airflow models import Variable \n\nvar_value = Variable.get('sample_key')\nbash_var_1= BashOperator(\n    task_id = \"bash_var_1\",\n    bash_command = f \"echo variable:{var_value}\"\" \n)\n\n스케줄러의 주기적 DAG 파싱시 Variable.get 개수만큼 DB 연결을 일으켜 불필요한 부하 발생 스케줄러 과부하 원인 중 하나 (권고하지 않음)\n\n주기적으로 아래 코드를 실행함\n\nfrom airflow models import Variable \nvar_value = Variable.get('sample_key')\n\n\n방법2) Jinja 템플릿 이용 , 오퍼레이터 내부에서 가져오기 (권고)\n\n스케쥴러는 Operator 안에 작성된 내용은 parsing 및 실행해보지 않음\n\nfrom airflow operators bash import BashOperator\nbash_var_2= BashOperator(\ntask_id=\"bash_var_2\",\nbash_command= f \"echo variable: {{var.value.sample_key}}\"\n)\n\n\n그런데 이 전역변수는 언제 , 어떻게 쓰면 좋을까\n\n협업 환경에서 표준화된 dag 을 만들기 위해 주로 사용. 개발자들마다 서로 다르게 사용하지 말아야할 주로 상수 (CONST) 로 지정해서 사용할 변수들 셋팅할 때 사용\n예) base_sh_dir = /opt/airflow/plugins/shell. shell file 의 위치를 고정\n예) base_file_dir = /opt/airflow/plugins/files\n예) email, Alert 메시지를 받을 담당자의 email 주소 정보\n\nDags Full Example\n\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.bash import BashOperator\nfrom airflow.models import Variable\n\nwith DAG(\n    dag_id=\"dags_bash_with_variable\",\n    schedule=\"10 9 * * *\",\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    #권고하지 않음\n    var_value = Variable.get(\"sample_key\")\n    bash_var_1 = BashOperator(\n    task_id=\"bash_var_1\",\n    bash_command=f\"echo variable:{var_value}\"\n    )\n\n    #권고함\n    bash_var_2 = BashOperator(\n    task_id=\"bash_var_2\",\n    bash_command=\"echo variable:{{var.value.sample_key}}\"\n    )\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#python-오퍼레이터에서-xcom-사용하기",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#python-오퍼레이터에서-xcom-사용하기",
    "title": "Data Share",
    "section": "",
    "text": "크게 두 가지 방법으로 Xcom 사용 가능\n\n**kwargs에 존재하는 ti (task_instance) 객체 활용\n\n@task(task_id='python_xcom_push_task')\ndef xcom_push(**kwargs):\n    ti = kwargs['ti']\n    ti.xcom_push(key=\"result1\", value=\"value_1\") \n    ti.xcom_push(key=\"result2\", value=[1,2,3])\n    #xcom_push: xcom에다가 data를 올릴 수 있음\n    #data를 올릴 때는 key:value 형태로 올리기\n    #template 변수에서 task_instance 라는 객체를 얻을 수 있으며 task_instance 객체가 가진 xcom_push 메서드를 활용할 수 있음\n\n@task(task_id='python_xcom_pull_task')\ndef xcom_pull(**kwargs):\n    ti = kwargs['ti']\n    value_key1 = ti.xcom_pull(key=\"result1\") # value_1이 value_key1에 저장됨\n    value_key2 = ti.xcom_pull(key=\"result2\",\n    task_ids='python_xcom_push_task') # [1,2,3]이 value_key2에 저장됨\n    #xcom_pull: xcom으로부터 data를 내려 받을 수 있음\n    #data를 올릴 때는 key:value 형태로 올리기\n    print(value_key1)\n    print(value_key2)\n\nxcome_pull()을 할때 key값만 줘도 되고 key값과 task_ids값을 둘다 줘도 된다.\n\nkey값만 줘도 될때\n\nxcom_push를 한 task가 1개 밖에 없을 때 사용 가능\n혹은, key값이 중복될 때 xcom_push를 한 task가 여러 개 있을 때도 사용 가능한데 가장 마지막 (최신) task의 key값을 호출 한다.\n만약, key값이 중복이 되지 않는 다면 key값만으로도 data를 내려 받을 수 있다.\n\nkey값과 task_ids둘다 줘야할 때\n\nkey값이 중복되는 xcom_push를 한 task가 여러 개 있을 때 선택적으로 원하는 task의 data를 가지고 오고 싶으면 해당 task의 task_ids를 명시적으로 적어줘야한다.\n\n예를 들어,\n\n# 5개의 tasks 존재하는\n# task1: xcom_push(key='result1'...)\n# task2: xcom_push(key='result1'...)\n# task3: xcom_push(key='result2'...)\n# task4: xcom_pull(key='result1'...)\n# task5: xcom_pull(key='result1',task_ids=...)\n\ntask4가 수행이 될때 task1의 xcom을 가져우는게 아니라 가장 최신에 수행된 task2의 xcom을 가져오게 된다.\ntask1의 xcom을 가지고 오고 싶을땐 task5와 같이 task1의 task_id를 task5의 task_ids에 명시해주면 된다.\n가장 안전한 방법은 task의 key값과 task_ids를 명시적으로 적어주는 것이다. 아니면 tasks의 key값을 절대 중복이 되지않도록 적어주는 것이다.\n\n\n\n파이썬 함수의 return 값 활용\n\n(1안)\n\n@task(task_id='xcom_push_by_return')\ndef xcom_push_by_return(**kwargs):\n    transaction_value = 'status Good'\n    return transaction_value\n@task(task_id='xcom_pull_by_return')\ndef xcom_pull_by_return(status, **kwargs):\n    print(status)\nxcom_pull_by_return(xcom_push_by_return()) \n\nxcom을 이용한 task의 flow 정해주는 또 다른 방식\n암묵적인 task의 순서: xcom_push_by_return() &gt;&gt; xcom_pull_by_return()\n위의 스크립트에서 xcom_pull() 또는 xcom_push()가 명시적으로 쓰이지진 않았지만 airflow에서는 Task 데커레이터 사용시 함수 입력/출력 관계만으로 Task flow 정의가 된다. 즉, xcom_pull_by_return(xcom_push_by_return()) = xcom_push_by_return() &gt;&gt; xcom_pull_by_return()\nTask 데커레이터 사용시 custom 함수가 return을 하게 되면 자동으로 xcom에 data가 올라가게 된다.\n(2안)\n\n\n@task(task_id='xcom_push_by_return')\ndef xcom_push_return(**kwargs):\n    transaction_value = 'status Good'\n    return transaction_value\n    # return 한 값은 자동으로 xcom에 key='return_value', task_ids=task_id 로 저장됨\n\n@task(task_id='xcom_pull_by_return')\ndef xcom_pull_return_by_method(**kwargs):\n    ti = kwargs['ti']\n    pull_value = ti.xcom_pull(key='return_value', task_ids='xcom_push_by_return')\n    # ti.xcom_pull()을 이용하여 return 한 값을 꺼낼 때는 key를 명시하지 않아도 됨. (자동으로 key=return_value 를 찾음)\n    # task_ids='xcom_push_by_return' return한 Task가 여러개 있을 때는 task_ids 를 지정\n    print(pull_value)\n\nxcom_push_by_return() &gt;&gt; xcom_pull_by_return() # 2안에서는 task flow를 명시적으로 적어줘야함.\n\nDAG Full Example\n\n1안 DAG Full Exmaple ```markdown from airflow import DAG import pendulum import datetime from airflow.decorators import task\nwith DAG( dag_id=“dags_python_with_xcom_eg2”, schedule=“30 6 * * *“, start_date=pendulum.datetime(2023, 3, 1, tz=”Asia/Seoul”), catchup=False ) as dag:\n  @task(task_id='python_xcom_push_by_return')\n  def xcom_push_result(**kwargs):\n      return 'Success'\n\n\n  @task(task_id='python_xcom_pull_1')\n  def xcom_pull_1(**kwargs):\n      ti = kwargs['ti']\n      value1 = ti.xcom_pull(task_ids='python_xcom_push_by_return')\n      print('xcom_pull 메서드로 직접 찾은 리턴 값:' + value1)\n\n  @task(task_id='python_xcom_pull_2')\n  def xcom_pull_2(status, **kwargs):\n      print('함수 입력값으로 받은 값:' + status)\n\n\n  python_xcom_push_by_return = xcom_push_result() \n  # airflow의 task decorator가 쓰였기 때문에 python_xcom_push_by_return에 \n  # 단순한 'Sucess' 스트링이 할당되는게 아니라 decorator object가 할당된다.\n  xcom_pull_2(python_xcom_push_by_return)\n  python_xcom_push_by_return &gt;&gt; xcom_pull_1()\n\n  # 암묵적인 task flow는\n  # xcom_push_result &gt;&gt;[xcom_pull_2, xcom_pull_1] 형태임\n```\n2안 DAG Full Example ```markdown from airflow import DAG import pendulum import datetime from airflow.decorators import task\nwith DAG( dag_id=“dags_python_with_xcom_eg1”, schedule=“30 6 * * *“, start_date=pendulum.datetime(2023, 3, 1, tz=”Asia/Seoul”), catchup=False ) as dag:\n  @task(task_id='python_xcom_push_task1')\n  def xcom_push1(**kwargs):\n      ti = kwargs['ti']\n      ti.xcom_push(key=\"result1\", value=\"value_1\")\n      ti.xcom_push(key=\"result2\", value=[1,2,3])\n\n  @task(task_id='python_xcom_push_task2')\n  def xcom_push2(**kwargs):\n      ti = kwargs['ti']\n      ti.xcom_push(key=\"result1\", value=\"value_2\") \n      # python_xcom_push_task1의 key값은 같지만 value는 다름\n      ti.xcom_push(key=\"result2\", value=[1,2,3,4])\n\n  @task(task_id='python_xcom_pull_task')\n  def xcom_pull(**kwargs):\n      ti = kwargs['ti']\n      value1 = ti.xcom_pull(key=\"result1\")\n      value2 = ti.xcom_pull(key=\"result2\", task_ids='python_xcom_push_task1')\n      print(value1)\n      print(value2)\n\n\n  xcom_push1() &gt;&gt; xcom_push2() &gt;&gt; xcom_pull()\n  # xcom_pull()에서 key값이 result1으로만 명시되었기 때문에 value1에는 xcom_push2()의 'value_2'가 들어감    \n```\n\nairflow web service에서 log 대신 xcom을 사용해 결과값을 확인"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#summary",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#summary",
    "title": "Data Share",
    "section": "",
    "text": "Xcom push 방법\n\nti.xcom_push 명시적 사용\n함수 return\n\nXcom pull 방법\n\nti.xcom_pull 명시적 사용\nreturn 값을 input으로 사용"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#bash-오퍼레이터에서-xcom-사용하기",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#bash-오퍼레이터에서-xcom-사용하기",
    "title": "Data Share",
    "section": "",
    "text": "Bash 오퍼레이터에서 template 문법을 쓸수 있는 parameters: env, bash_command\ntemplate 이용하여 push/pull\n\nbash_push = BashOperator(\n    task_id='bash_push',\n    bash_command=\"echo START && \"\n                \"echo XCOM_PUSHED \"\n                \"{{ ti.xcom_push(key='bash_pushed',value='first_bash_message') }} && \"\n                \"echo COMPLETE\" \n                # bash 같은 경우엔 출력하는 값이 return값으로 간주됨. \n                # 위의 경우와 같이 여러 출력물(&&로 연결된 3개의 출력물)이 있을 경우 마지막 출력물(COMPLETE)이 자동으로 return_value 에 저장됨\n)\nbash_pull = BashOperator(\n    task_id='bash_pull',\n    env={'PUSHED_VALUE':\"{{ ti.xcom_pull(key='bash_pushed') }}\",\n        'RETURN_VALUE':\"{{ ti.xcom_pull(task_ids='bash_push') }}\"}, \n        # env 는 key: value 형태로 데이터를 받음\n        # task_ids 만 지정하면 key='return_value' 를 의미함\n        # RETURN_VALUE에 'complete'이 들어감\n    bash_command=\"echo $PUSHED_VALUE && echo $RETURN_VALUE \",\n    do_xcom_push=False \n    # bash_command에서 출력되는 \"echo $PUSHED_VALUE && echo $RETURN_VALUE \"의 \n    # 출력문을 자동으로 xcom에 올리지 말라는 의미\n)\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_with_xcom\",\n    schedule=\"10 0 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    bash_push = BashOperator(\n    task_id='bash_push',\n    bash_command=\"echo START && \"\n                 \"echo XCOM_PUSHED \"\n                 \"{{ ti.xcom_push(key='bash_pushed',value='first_bash_message') }} && \"\n                 \"echo COMPLETE\"\n    )\n\n    bash_pull = BashOperator(\n        task_id='bash_pull',\n        env={'PUSHED_VALUE':\"{{ ti.xcom_pull(key='bash_pushed') }}\",\n            'RETURN_VALUE':\"{{ ti.xcom_pull(task_ids='bash_push') }}\"},\n        bash_command=\"echo $PUSHED_VALUE && echo $RETURN_VALUE \",\n        do_xcom_push=False\n    )\n\n    bash_push &gt;&gt; bash_pull"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#summary-1",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#summary-1",
    "title": "Data Share",
    "section": "",
    "text": "Bash_command에 의해 출력된 값은 자동으로 return_value로 저장된다 (마지막 출력 문장만)\nreturn_value를 꺼낼 때는 xcom_pull에서 task_ids 값만 줘도 된다.\n키가 지정된 xcom 값을 꺼낼 때는 key 값만 줘도 된다 (단, 다른 task에서 동일 key로 push 하지 않았을 때만)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#python-rightarrow-bash-오퍼레이터-xcom-전달",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#python-rightarrow-bash-오퍼레이터-xcom-전달",
    "title": "Data Share",
    "section": "",
    "text": "@task task_id =='python push'\ndef python_push_xcom\n    result_dict = {'status':' Good','data':[1,2,3],'options_cnt': 100}\n    return result_dict\nbash_pull = BashOperator(\n    task_id='bash_pull',\n    env={\n        'STATUS': '{{ti.xcom_pull(task ids=\"python push\")[\"status\"]}}', #task_ids만 있으면 위의 파이썬 함수에서 리턴값을 자동으로 받음\n        'DATA': '{{ti.xcom_pull(task ids=\"python push\")[\"data\"]}}',\n        'OPTIONS_CNT': '{{ti.xcom_pull(task_ids=\"python_push\")[\"options_cnt\"]}}'\n    },\n    bash_command = 'echo $STATUS && echo $DATA && echo $OPTIONS_CNT'\n)\n \npython_push_xcom() &gt;&gt; bash_pull"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#bash-rightarrow-python-오퍼레이터-xcom-전달",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#bash-rightarrow-python-오퍼레이터-xcom-전달",
    "title": "Data Share",
    "section": "",
    "text": "bash_push = BashOperator(\ntask_id ='bash_push',\nbash_command='echo PUSH_START'\n    '{{ti.xcom_push(key=\"bash_pushed\",value=200) }}&& 'echo PUSH_COMPLETE'\n)\n\n@task(task_id =='python_pull')\ndef python_pull_xcom(**kwargs): \n    ti = kwargs ['ti']\n    status_value= ti.xcom_pull(key ='bash_pushed')\n    return_value= ti.xcom_pull(task_ids ='bash_push')\n    print('status_value:'+ str (status_value))\n    print('return_value:'+ return_value)\n    bash_push&gt;&gt; python_pull_xcom()\n\nDAG Full Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_python_with_xcom\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    @task(task_id='python_push')\n    def python_push_xcom():\n        result_dict = {'status':'Good','data':[1,2,3],'options_cnt':100}\n        return result_dict\n\n    bash_pull = BashOperator(\n        task_id='bash_pull',\n        env={\n            'STATUS':'{{ti.xcom_pull(task_ids=\"python_push\")[\"status\"]}}',\n            'DATA':'{{ti.xcom_pull(task_ids=\"python_push\")[\"data\"]}}',\n            'OPTIONS_CNT':'{{ti.xcom_pull(task_ids=\"python_push\")[\"options_cnt\"]}}'\n\n        },\n        bash_command='echo $STATUS && echo $DATA && echo $OPTIONS_CNT'\n    )\n    python_push_xcom() &gt;&gt; bash_pull\n\n    bash_push = BashOperator(\n    task_id='bash_push',\n    bash_command='echo PUSH_START '\n                 '{{ti.xcom_push(key=\"bash_pushed\",value=200)}} && '\n                 'echo PUSH_COMPLETE'\n    )\n\n    @task(task_id='python_pull')\n    def python_pull_xcom(**kwargs):\n        ti = kwargs['ti']\n        status_value = ti.xcom_pull(key='bash_pushed')\n        return_value = ti.xcom_pull(task_ids='bash_push')\n        print('status_value:' + str(status_value))\n        print('return_value:' + return_value)\n\n    bash_push &gt;&gt; python_pull_xcom()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#python-email-오퍼레이터-xcom-전달",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#python-email-오퍼레이터-xcom-전달",
    "title": "Data Share",
    "section": "",
    "text": "Email 오퍼레이터를 이용하여 Xcom을 받아와야함\nEmail 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nto\nsubject\nhtml_content\nfiles\ncc\nbcc\nnime_subtype\nmime_charset\ncustom_headers\n\n\n@task(task_id='something_task') # python operator를 task decorator로 만듦\ndef some_logic(**kwargs):\n    from random import choice \n    #choice 함수: list, tuple, string 중 아무 값이나 꺼낼 수 있게 해주는 함수\n    return choice(['Success','Fail']) # either Success or Fail is return됨\nsend_email = EmailOperator(\n    task_id='send_email',\n    to='hjkim_sun@naver.com',\n    subject='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} some_logic 처리결과',\n    html_content='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} 처리 결과는 &lt;br&gt; \\ {{ti.xcom_pull(task_ids=\"something_task\")}} 했습니다 &lt;br&gt;'\n)\n\nDAG Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.decorators import task\nfrom airflow.operators.email import EmailOperator\n\nwith DAG(\n    dag_id=\"dags_python_email_operator\",\n    schedule=\"0 8 1 * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    @task(task_id='something_task')\n    def some_logic(**kwargs):\n        from random import choice \n        return choice(['Success','Fail'])\n\n\n    send_email = EmailOperator(\n        task_id='send_email',\n        to='hjkim_sun@naver.com',\n        subject='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} some_logic 처리결과',\n        html_content='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} 처리 결과는 &lt;br&gt; \\\n                    {{ti.xcom_pull(task_ids=\"something_task\")}} 했습니다 &lt;br&gt;'\n    )\n\n    some_logic() &gt;&gt; send_email"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\nTask 분기처리가 필요한 이유\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask2_1\n\ntask2_1\n\n\n\ntask1-&gt;task2_1\n\n\n\n\n\ntask2_2\n\ntask2_2\n\n\n\ntask1-&gt;task2_2\n\n\n\n\n\ntask2_3\n\ntask2_3\n\n\n\ntask1-&gt;task2_3\n\n\n\n\n\n\n\n\n\n\n\n위와 같이 task1이 실행된 후 여러 후차적인 task를 병렬로 실행되어야 할 때\n\ntask flow에서 task1의 결과에 따라 선택적으로 task2-x 중 하나만 수행되도록 구성해야 할 때가 있다.\neg) Task1 의 결과로 ‘Good’,’Bad’,’Pending’ 이라는 결과 3 개 중 하나가 나오고 그에 따라 ask2-1 ~ task2-3 중 하나가 실행되도록 해야 할 경우\n\n\n\n\n\nTask 분기처리 방법 3가지\n\nBranchPythonOperator\ntask.branch decorator 이용\nBaseBranchOperator 클래스를 상속하여 직접 개발\n\n\n\n\n\ndef select_random():\n    import random\n\n    item_lst= ['A','B','C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A';\n        return 'task_a' # task_id를 string 값으로 return해야함\n    elif selected_item in ['B','C] \n        return ['task_b','task_c'] # 여러 task를 동시에 수행시킬 땐 string 리스트로 반환\n\n# 일반 operator의 parameter도 있음\npython_branch_task = BranchPythonOperator(\n    task_id ='python_branch_task',\n    python_callable=select_random #select_random function 호출\n)\n\npython_branch_task &gt;&gt; [task_a , task_b , task_c]\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\npython_branch_task\n\npython_branch_task\n\n\n\ntask_a\n\ntask_a\n\n\n\npython_branch_task-&gt;task_a\n\n\n\n\n\ntask_b\n\ntask_b\n\n\n\npython_branch_task-&gt;task_b\n\n\n\n\n\ntask_c\n\ntask_c\n\n\n\npython_branch_task-&gt;task_c\n\n\n\n\n\n\n\n\n\n\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.python import BranchPythonOperator\n\nwith DAG(\n    dag_id='dags_branch_python_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'), \n    schedule='0 1 * * *',\n    catchup=False\n) as dag:\n    def select_random():\n        import random\n\n        item_lst = ['A','B','C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a' # task_id를 string 값으로 return해야함\n        elif selected_item in ['B','C']:\n            return ['task_b','task_c'] # 여러 task를 동시에 수행시킬 땐 리스트로 반환\n    \n    # 일반 operator의 parameter도 있음\n    python_branch_task = BranchPythonOperator(\n        task_id='python_branch_task',\n        python_callable=select_random\n    )\n    \n    # 후행 task 3개\n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    python_branch_task &gt;&gt; [task_a, task_b, task_c]\n\n나의 경우 airflow web service상에서 1회 실행 시켰을 때 selected_item의 값이 task_b, task_b가 선택됐음\n\ngraph 버튼을 눌러 보면 가장 최근에 돌았던 task들이 return 된다.\ntask_a가 분홍색 박스로 skipped 상태인 것을 확인 할 수 있다.\ngraph에서 python_branch_task를 누르고 xcom을 누르면 다음과 같은 table을 확인할 수 있다.\n\n\n\n\nKey\nValue\n\n\n\n\nskipmixin_key\n{‘followed’: [‘task_c’, ‘task_b’]}\n\n\nreturn_value\n[‘task_b’, ‘task_c’]\n\n\n\n\n여기서 skipmixin_key 의 value값의 key 값이 ‘followed’ 이고 [‘task_c’, ‘task_b’] 인 것을 볼 수 있다. 필요시 어떤 task들이 선택되었는지 확인하려면 xcom을 통해 확인 가능하다.\nlog 를 보면\n\n[2023-06-23, 23:20:01 UTC] {python.py:183} INFO - Done. Returned value was: ['task_b', 'task_c']\n[2023-06-23, 23:20:01 UTC] {python.py:216} INFO - Branch callable return ['task_b', 'task_c']\n[2023-06-23, 23:20:01 UTC] {skipmixin.py:161} INFO - Following branch ['task_b', 'task_c']\n[2023-06-23, 23:20:01 UTC] {skipmixin.py:221} INFO - Skipping tasks ['task_a']    \n\n\n\n\n\n\n\n\n\n\nfrom airflow.operators.python import BranchPythonOperator\ndef select_random(): \n    import random\n    item_lst = ['A','B','C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A':\n        return 'task_a'\n    elif selected_item in ['B','C']\n        return ['task_b','task_c']\n\npython_branch_task = BranchPythonOperator(\n    task_id= 'branching',\n    python_callable = select_random\n)\npython_branch_task &gt;&gt; [task_a , task_b , task_c]\n\nfrom airflow.operators.python import task\n\n@task.branch(task_id='python_branch_task')\ndef select_random(): \n    import random\n    item_lst = ['A','B','C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A':\n        return 'task_a'\n    elif selected_item in ['B','C']\n        return ['task_b','task_c']\n\nselect_random() &gt;&gt; [task_a , task_b , task_c]\n\n\n\nBranchPythonOperator와 비교하여 select_random()을 호출 또는 맵핑 하는 방식이 decorator에서는 @task.branch(task_id='python_branch_task')으로 표현 되었고 task flow를 표현하는 task connection 방식도 select_random() &gt;&gt; [task_a , task_b , task_c] 로 표현 됐다.\nBranchPythonOperator의 python_branch_task object와 task.branch (decorator)의 select_random()는 사실상 같은 객체이다.\n차이점은 BranchPythonOperator(...)를 실행시킨 것과 select_random(...) 함수를 실행한 것 외엔 그 역할과 기능은 같다 (같은 object 반환).\nDags Full Example\n\nfrom airflow import DAG\nfrom datetime import datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id='dags_python_with_branch_decorator',\n    start_date=datetime(2023,4,1),\n    schedule=None,\n    catchup=False\n) as dag:\n    @task.branch(task_id='python_branch_task')\n    def select_random():\n        import random\n        item_lst = ['A', 'B', 'C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a'\n        elif selected_item in ['B','C']:\n            return ['task_b','task_c']\n    \n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    select_random() &gt;&gt; [task_a, task_b, task_c]\n\nairflow web service의 결과물은 BranchPythonOperator나 decorator나 같았음\n\n\n\n\n\nBaseBranchOperator 클래스 상속해서 직접 함수를 개발해서 사용해야함.\n\n\nfrom airflow.operators.branch import BaseBranchOperator\nwith DAG(...\n) as dag:\n    class CustomBranchOperator(BaseBranchOperator): #클래스 이름은 임의로 지정해 줌\n    #Python의 class 상속 syntax: class MyclassName(상속할className):\n    #Python은 다중 상속가능\n        def choose_branch(self,context): \n        # 함수 재정의 : Overriding, 함수 이름 바꾸면 안됨!\n        # parameter도 바꾸면 안됨\n            import random\n            print(context) # context에 어떤 내용이 있는지 출력\n\n            item_lst = ['A', 'B','C]\n            selected_item = random.choice(item_lst)\n            if selected_item == 'A':\n                return 'task_a'\n            elif selected_item in ['B','C']:\n                return ['task_b','task_c']\n\ncustom_branch_operator = CustomBranchOperator(task_id ='python_branch_task') # 클래스 실행하여 custom_branch_operator object 생성\ncustom_branch_operator &gt;&gt; [task_a , task_b , task_c]\n\n클래스 상속하여 새로운 클래스 만들어야함: BaseBranchOperator 상속시 choose_branch 함수를 구현해 줘야 함\nCustomBranchOperator 클래스 이름은 임의로 지정해준 이름\nclass 선언시 class childClass(상속할parentClass): 상속할 부모클래스를 2개이상 지정하는 다중 상속이 가능하긴 하지만 권고하지 않음.\nchoose_branch() 함수를 만든 이유를 알기 위해선 BaseBranchOperator class에 대해서 알아야함\n\nairflow operators-airflow.operators.branch or google ‘airflow operators’ :::{.callout-note} ## Description\n\nBases: airflow.models.baseoperator.BaseOperator, airflow.models.skipmixin.SkipMixin A base class for creating operators with branching functionality, like to BranchPythonOperator. Users should create a subclass from this operator and implement the function choose_branch(self, context). This should run whatever business logic is needed to determine the branch, and return either the task_id for a single task (as a str) or a list of task_ids. The operator will continue with the returned task_id(s), and all other tasks directly downstream of this operator will be skipped. :::\n\n함수명과 인자(argument)명도 반드시 일치시켜야함\nchoose_branch(self,context)의 context는 pythonOperator 쓸때 **kwargs의 parameters들을 사용할 수 있게 해주는 parameter\n\ncontext 인자엔 op_kargs와 같이 data_interval_start, data_interval_end 등과 같은 정보를 제공해주는 인자\n\nprint(context) 결과\n\n[2023-06-24, 00:29:33 UTC] {logging_mixin.py:149} INFO - {'conf': &lt;***.configuration.AirflowConfigParser object at 0x7fc3d5dd2cd0&gt;, 'dag': &lt;DAG: dags_base_branch_operator&gt;, 'dag_run': &lt;DagRun dags_base_branch_operator @ 2023-06-24 00:29:31.444830+00:00: manual__2023-06-24T00:29:31.444830+00:00, state:running, queued_at: 2023-06-24 00:29:31.455604+00:00. externally triggered: True&gt;, 'data_interval_end': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'data_interval_start': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'ds': '2023-06-24', 'ds_nodash': '20230624', 'execution_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'expanded_ti_count': None, 'inlets': [], 'logical_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'macros': &lt;module '***.macros' from '/home/***/.local/lib/python3.7/site-packages/***/macros/__init__.py'&gt;, 'next_ds': '2023-06-24', 'next_ds_nodash': '20230624', 'next_execution_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'outlets': [], 'params': {}, 'prev_data_interval_start_success': None, 'prev_data_interval_end_success': None, 'prev_ds': '2023-06-24', 'prev_ds_nodash': '20230624', 'prev_execution_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'prev_execution_date_success': None, 'prev_start_date_success': None, 'run_id': 'manual__2023-06-24T00:29:31.444830+00:00', 'task': &lt;Task(CustomBranchOperator): python_branch_task&gt;, 'task_instance': &lt;TaskInstance: dags_base_branch_operator.python_branch_task manual__2023-06-24T00:29:31.444830+00:00 [running]&gt;, 'task_instance_key_str': 'dags_base_branch_operator__python_branch_task__20230624', 'test_mode': False, 'ti': &lt;TaskInstance: dags_base_branch_operator.python_branch_task manual__2023-06-24T00:29:31.444830+00:00 [running]&gt;, 'tomorrow_ds': '2023-06-25', 'tomorrow_ds_nodash': '20230625', 'triggering_dataset_events': &lt;Proxy at 0x7fc3ab28c8c0 with factory &lt;function TaskInstance.get_template_context.&lt;locals&gt;.get_triggering_events at 0x7fc3ab277c20&gt;&gt;, 'ts': '2023-06-24T00:29:31.444830+00:00', 'ts_nodash': '20230624T002931', 'ts_nodash_with_tz': '20230624T002931.444830+0000', 'var': {'json': None, 'value': None}, 'conn': None, 'yesterday_ds': '2023-06-23', 'yesterday_ds_nodash': '20230623'}\ncontext결과물은 위와 같은 시간 정보를 담고 있기 때문에 꺼내쓸 수 있다.\n분기 처리 결과는 다른 2 방식의 결과와 같음\nDAG Full Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.branch import BaseBranchOperator\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n    dag_id='dags_base_branch_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n    class CustomBranchOperator(BaseBranchOperator):\n        def choose_branch(self, context):\n            import random\n            print(context) # context에 어떤 내용이 있는지 출력\n            \n            item_lst = ['A', 'B', 'C']\n            selected_item = random.choice(item_lst)\n            if selected_item == 'A':\n                return 'task_a'\n            elif selected_item in ['B','C']:\n                return ['task_b','task_c']\n\n    \n    custom_branch_operator = CustomBranchOperator(task_id='python_branch_task')\n\n    \n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    custom_branch_operator &gt;&gt; [task_a, task_b, task_c]\n\n\n\n\nTask 분기처리 방법\n\nBranchPythonOperator (자주 사용)\ntask.branch 데커레이터 이용 (자주 사용)\nBaseBranchOperator 상속 , choose_branch 를 재정의해야 함 (덜 사용)\n\n공통적으로 리턴 값으로 후행 Task 의 id 를 str 또는 list 로 리턴해야 함\n3가지 분기처리 방법은 방법만 다를 뿐 결과는 동일함\n3 보다는 1 또는 2를 주로 사용함\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask4\n\ntask4\n\n\n\ntask1-&gt;task4\n\n\n\n\n\ntask2\n\ntask2\n\n\n\ntask2-&gt;task4\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask3-&gt;task4\n\n\n\n\n\n\n\n\n\n\n\nbranch와 반대되는 개념으로 여러 상위 tasks가 하나의 하위 task로 연결되는 flow로 만들때 사용\n즉, 여러 상위 Task 들의 상태에 따라 후행 task의 수행여부 결정할 때 쓰인다\n기본 값 : 여러 상위 Task들이 모두 성공시에만 수행\n상위 task의 수행 상태에 따라 조건적으로 후행 task의 수행 여부를 결정할 수 있다.\ntrigger option은 하위 task를 이용하여 줄 수 있다.\n모든 airflow operator는 trigger rule option을 줄 수 있다.\n11가지 trigger rules\n\n\n\n\n\n\n\n\nDefault\nLeft\n\n\n\n\nall_success (default)\n상위 tasks 가 모두 성공하면 실행\n\n\nall_failed\n상위 tasks 가 모두 실패하면 실행\n\n\nall_done\n상위 tasks 가 모두 수행되면 실행 (실패도 수행된것에 포함)\n\n\nall_skipped\n상위 tasks 가 모두 Skipped 상태면 실행\n\n\none_failed\n상위 tasks 중 하나 이상 실패하면 실행 (모든 상위 Tasks의 완료를 기다리지 않음)\n\n\none_success\n상위 tasks 중 하나 이상 성공하면 실행 (모든 상위 Tasks의 완료를 기다리지 않음)\n\n\none_done\n상위 tasks 중 하나 이상 성공 또는 실패 하면 실행\n\n\nnone_failed\n상위 task s중 실패가 없는 경우 실행 (성공 또는 Skipped 상태)\n\n\nnone_failed_min_one_success\n상위 tasks 중 실패가 없고 성공한 Task가 적어도 1개 이상이면 실행\n\n\nnone_skipped\nSkip된 상위 Task가 없으면 실행 (상위 Task가 성공, 실패하여도 무방)\n\n\nalways\n언제나 실행\n\n\n\n\n위의 표에서 모든 상위 task를 기다리지 않음은 각 각의 상위 task들의 처리 시간이 다를 때 가장 빠르게 처리되는 상위 task에 따라서 후행 task가 수행된다는 것을 의미한다. 예를 들어, one_failed의 경우\n\n상위 task1 (2분소요)\n상위 task2 (10분소요)\n상위 task3 (20분소요) 일때,\n상위 task 3개 중 task1의 결과가 먼저 fail이 나올 경우 task2,3 을 기다리지 않고 바로 triger가 발동되어 하위 task4가 수행된다.\n\n\n\n\n\n\n아래 예시에서 4개의 task가 정의됨\n\n\n\n\n# 상위 task1\nbash_upstream_1 = BashOperator(\n    task_id = 'bash_upstream_1',\n    bash_command = 'echo upstream1'\n)\n\n@task(task_id =='python_upstream_1') # 상위 task2\ndef python_upstream_1():\n    AirflowException('downstream_1 Exception!') # AirflowException() fail을 반환하여 무조건 task 실패처리가되도록 설정\n\n@task(task_id =='python_upstream_2') # 상위 task3\ndef python_upstream_2():\n    print('정상 처리')\n\n@task(task_id ='python_downstream_1', trigger_rule ='all_done') #하위 task4\ndef python_downstream_1():\n    print('정상 처리')\n\n[bash_upstream_1 , python_upstream_1(), python_upstream_2()] &gt;&gt; python_downstream_1()\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nbash_upstream_1\n\nbash_upstream_1\n\n\n\npython_downstream_1\n\npython_downstream_1\n\n\n\nbash_upstream_1-&gt;python_downstream_1\n\n\n\n\n\npython_upstream_1\n\npython_upstream_1\n\n\n\npython_upstream_1-&gt;python_downstream_1\n\n\n\n\n\npython_upstream_2\n\npython_upstream_2\n\n\n\npython_upstream_2-&gt;python_downstream_1\n\n\n\n\n\n\n\n\n\n\n\n\n\nbash_upstream_1(성공), python_upstream_1(실패), python_upstream_2(성공).\ntriger rule이 all done이기 때문에 python_upstream_1(실패)여도 python_downstream_1은 수행되어야 한다.\n다른 Operator such as BashOperator, pythonOperator의 경우도 trigger_rule =='all_done' parameter 똑같이 넣어주면 됨\n\n\n\n\n\n\n@task.branch(task_id ='branching') #상위 task1\ndef random_branch():\n    import random\n    item_lst = [' A', ' B', 'C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A':\n        return 'task_a'\n    elif selected_item == 'B':\n        return 'task_b'\n    elif selected_item == 'C':\n        return 'task_c'\n\n#상위 task2\ntask_a = BashOperator(\n    task_id ='task_a',\n    bash_command = 'echo upstream1'\n    )\n\n#상위 task3\n@task(task_id ='task_b')\ndef task_b():\n    print('정상 처리')\n\n#상위 task4\n@task(task_id =='task_c')\ndef task_c():\n    print('정상 처리')\n\n#하위 task5\n@task(task_id =='task_d', trigger_rule ='none_skipped')\ndef task_d():\n    print('정상 처리')\n\nrandom_branch() &gt;&gt; [task_a , task_b(), task_c()] &gt;&gt; task_d()\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nrandom_branch\n\nrandom_branch\n\n\n\ntask_a\n\ntask_a\n\n\n\nrandom_branch-&gt;task_a\n\n\n\n\n\ntask_b\n\ntask_b\n\n\n\nrandom_branch-&gt;task_b\n\n\n\n\n\ntask_c\n\ntask_c\n\n\n\nrandom_branch-&gt;task_c\n\n\n\n\n\ntask_d\n\ntask_d\n\n\n\ntask_a-&gt;task_d\n\n\n\n\n\ntask_b-&gt;task_d\n\n\n\n\n\ntask_c-&gt;task_d\n\n\n\n\n\n\n\n\n\n\n\n\n\nskip이 있기 때문에 실제로 task_d가 돌지 말아야한다.\nDags Full Example\n\n\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.exceptions import AirflowException\n\nimport pendulum\n\nwith DAG(\n    dag_id='dags_python_with_trigger_rule_eg1',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n    bash_upstream_1 = BashOperator(\n        task_id='bash_upstream_1',\n        bash_command='echo upstream1'\n    )\n\n    @task(task_id='python_upstream_1')\n    def python_upstream_1():\n        raise AirflowException('downstream_1 Exception!')\n\n\n    @task(task_id='python_upstream_2')\n    def python_upstream_2():\n        print('정상 처리')\n\n    @task(task_id='python_downstream_1', trigger_rule='all_done')\n    def python_downstream_1():\n        print('정상 처리')\n\n    [bash_upstream_1, python_upstream_1(), python_upstream_2()] &gt;&gt; python_downstream_1()\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.exceptions import AirflowException\n\nimport pendulum\n\nwith DAG(\n    dag_id='dags_python_with_trigger_rule_eg2',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n    @task.branch(task_id='branching')\n    def random_branch():\n        import random\n        item_lst = ['A', 'B', 'C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a'\n        elif selected_item == 'B':\n            return 'task_b'\n        elif selected_item == 'C':\n            return 'task_c'\n\n    task_a = BashOperator(\n        task_id='task_a',\n        bash_command='echo upstream1'\n    )\n\n    @task(task_id='task_b')\n    def task_b():\n        print('정상 처리')\n\n\n    @task(task_id='task_c')\n    def task_c():\n        print('정상 처리')\n\n    @task(task_id='task_d', trigger_rule='none_skipped')\n    def task_d():\n        print('정상 처리')\n\n    random_branch() &gt;&gt; [task_a, task_b(), task_c()] &gt;&gt; task_d()\n\n\n\n\n\n\n\n\n\ntasks를 모아 관리\nTask들의 모음: dags안에 task가 많을 경우 비슷한 기능의 tasks 그룹으로 모아서 관리\n\n예를 들어, dag안에 50개의 tasks 있다고 할 때, 5개 tasks가 서로 연관성이 높은 connection을 이루고 이런 group이 10개가 있을 수 있다.\n\nlink: UI Graph탭에서 Task 들을 Group 화하여 보여줌-TaskGroups or google ‘airflow dags’\n\ncontent &gt;&gt; Core Concepts &gt;&gt; DAGs &gt;&gt; DAG Visualization &gt;&gt; Task Groups\n\nTask Group 안에 Task Group 을 중첩하여 계층적으로 구성 가능\n위의 링크에서 section1 과 section2 로 grouping되어 있고 section2에는 inner_section_2 라는 또 다른 task group이 있다.\n꼭 써야하는 이유는 성능적인 면에서 딱히 없지만 task flow의 가독성이 높아짐\n\n\n\n\n\ntask_group 데커레이터 이용\n\nfrom airflow.decorators import task_group\nwith DAG(...\n) as dag:\n    @task_group(group_id ='first_group')\n    def group_1():\n    ''' task_group 데커레이터를 이용한 첫 번째 그룹''' # docstring: 함수를 설명하는 기법\n    # airflow UI에서는 tooltip이라고 표시됨\n\n    @task(task_id ='inner_function1')\n    def inner_func1(**kwargs):\n        print('첫 번째 TaskGroup 내 첫 번째 task 입니다')\n\n    inner_function2 = PythonOperator(\n        task_id ='inner_function2',\n        python_callable = inner_func,\n        op_kwargs={'msg':'첫 번째 TaskGroup 내 두 번쨰 task 입니다.'}\n    )\n    inner_func1() &gt;&gt; inner_function2\n\ntask_group 데커레이터 이용하지 않음 (클래스 이용)\n\nfrom airflow.utils.task_group import TaskGroup\n    with TaskGroup(group_id ='second_group', tooltip='두 번째 그룹') as group_2: # with MyClassName(arg1,age2,...) \n    # tooltipe은 decorator를 이용한 task_group 생성때의 docstring과 같은 역할을 함\n        @task(task_id ='inner_function1')\n        def inner_func1 (**kwargs):\n            print('두 번째 TaskGroup 내 첫 번째 task 입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id = 'inner_function2',\n            python_collable = inner_func,\n            op_kwargs = {'msg': '두 번째 TaskGroup 내 두 번째 task 입니다'}\n        )\ninner_func1() &gt;&gt; inner_function2\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\nfrom airflow.decorators import task_group\nfrom airflow.utils.task_group import TaskGroup\n\nwith DAG(\n    dag_id=\"dags_python_with_task_group\",\n    schedule=None,\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    def inner_func(**kwargs):\n        msg = kwargs.get('msg') or '' \n        print(msg)\n\n    @task_group(group_id='first_group')\n    def group_1():\n        ''' task_group 데커레이터를 이용한 첫 번째 그룹 '''\n\n        @task(task_id='inner_function1')\n        def inner_func1(**kwargs):\n            print('첫 번째 TaskGroup 내 첫 번째 task입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id='inner_function2',\n            python_callable=inner_func,\n            op_kwargs={'msg':'첫 번째 TaskGroup내 두 번쨰 task입니다.'}\n        )\n\n        inner_func1() &gt;&gt; inner_function2\n\n    with TaskGroup(group_id='second_group', tooltip='두 번째 그룹') as group_2:\n        ''' 클래스 안에 적은 docstring은 표시되지 않음'''\n        @task(task_id='inner_function1')\n        def inner_func1(**kwargs):\n            print('두 번째 TaskGroup 내 첫 번째 task입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id='inner_function2',\n            python_callable=inner_func,\n            op_kwargs={'msg': '두 번째 TaskGroup내 두 번째 task입니다.'}\n        )\n        inner_func1() &gt;&gt; inner_function2\n\n    group_1() &gt;&gt; group_2\n\n위에서 task_id와 group_id가 같지만 에러가 안나는 이유가 task group이 다르기 때문.\n위에서 볼 수 있듯이 task group 또한 flow 설정할 수 있음 group_1() &gt;&gt; group_2\n\n\n\n\n\nTask Group 작성 방법은 2 가지가 존재함 (데커레이터 & 클래스)\nTask Group 안에 Task Group 중첩하여 정의 가능\nTask Group 간에도 Flow 정의 가능\nGroup이 다르면 task_id 가 같아도 무방\nTooltip 파라미터를 이용해 UI 화면에서 Task group 에 대한 설명 제공 가능 (데커레이터 활용시 docstring 으로도 가능)\n\n\n\n\n\n\n\n\nTask 연결에 대한 설명 (즉 edge에 대한 Comment)\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ningest\n\ningest\n\n\n\nanalyze\n\nanalyze\n\n\n\ningest-&gt;analyze\n\n\n\n\n\ncheck_integrity\n\ncheck_integrity\n\n\n\nanalyze-&gt;check_integrity\n\n\n\n\n\ndescribe_integrity\n\ndescribe_integrity\n\n\n\ncheck_integrity-&gt;describe_integrity\n\n\nErrors Found\n\n\n\nsave\n\nsave\n\n\n\ncheck_integrity-&gt;save\n\n\nNo Errors\n\n\n\nemail_error\n\nemail_error\n\n\n\ndescribe_integrity-&gt;email_error\n\n\n\n\n\nreport\n\nreport\n\n\n\nemail_error-&gt;report\n\n\n\n\n\nsave-&gt;report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom airflow.utils.edgemodifier import Label\nempty_1 = EmptyOperator(\n    task_id ='empty_1'\n)\n\nempty_2 = EmptyOperator(\n    task_id='empty_2'\n)\nempty_1 &gt;&gt; Label ('1 과 2 사이') &gt;&gt; empty_2\n\n\n\n\nfrom airflow.utils.edgemodifier import Label\nempty_2 = EmptyOperator(\n    task_id = 'empty_2'\n)\n\nempty_3 = EmptyOperator(\n    task_id ='empty_3'\n)\n\nempty_4 = EmptyOperator(\n    task_id ='empty_4'\n)\n\nempty_5 = EmptyOperator(\n    task_id ='empty_5'\n)\n\nempty_6 = EmptyOperator(\n    task_id ='empty_6'\n)\n\nempty_2 &gt;&gt; Label('Start Branch') &gt;&gt; [empty_3, empty_4, empty_5 ] &gt;&gt; Label('End Branch') &gt;&gt; empty_6\n\n이렇게 분기가 펼쳐지고 모아지는 경우 모든 분기 edges에 label이 붙게 된다.\nFull DAG Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\n\n\nwith DAG(\n    dag_id=\"dags_empty_with_edge_label\",\n    schedule=None,\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    empty_1 = EmptyOperator(\n        task_id='empty_1'\n    )\n\n    empty_2 = EmptyOperator(\n        task_id='empty_2'\n    )\n\n    empty_1 &gt;&gt; Label('1과 2사이') &gt;&gt; empty_2\n\n    empty_3 = EmptyOperator(\n        task_id='empty_3'\n    )\n\n    empty_4 = EmptyOperator(\n        task_id='empty_4'\n    )\n\n    empty_5 = EmptyOperator(\n        task_id='empty_5'\n    )\n\n    empty_6 = EmptyOperator(\n        task_id='empty_6'\n    )\n\n    empty_2 &gt;&gt; Label('Start Branch') &gt;&gt; [empty_3,empty_4,empty_5] &gt;&gt; Label('End Branch') &gt;&gt; empty_6\n\nempty operator이기 때문에 실행은 airflow web servce에서 실행은 안해도 된다.\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-분기-처리-유형",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-분기-처리-유형",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "Task 분기처리가 필요한 이유\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask2_1\n\ntask2_1\n\n\n\ntask1-&gt;task2_1\n\n\n\n\n\ntask2_2\n\ntask2_2\n\n\n\ntask1-&gt;task2_2\n\n\n\n\n\ntask2_3\n\ntask2_3\n\n\n\ntask1-&gt;task2_3\n\n\n\n\n\n\n\n\n\n\n\n위와 같이 task1이 실행된 후 여러 후차적인 task를 병렬로 실행되어야 할 때\n\ntask flow에서 task1의 결과에 따라 선택적으로 task2-x 중 하나만 수행되도록 구성해야 할 때가 있다.\neg) Task1 의 결과로 ‘Good’,’Bad’,’Pending’ 이라는 결과 3 개 중 하나가 나오고 그에 따라 ask2-1 ~ task2-3 중 하나가 실행되도록 해야 할 경우"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#airflow에서-지원하는-task-분기처리-방법",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#airflow에서-지원하는-task-분기처리-방법",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "Task 분기처리 방법 3가지\n\nBranchPythonOperator\ntask.branch decorator 이용\nBaseBranchOperator 클래스를 상속하여 직접 개발\n\n\n\n\n\ndef select_random():\n    import random\n\n    item_lst= ['A','B','C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A';\n        return 'task_a' # task_id를 string 값으로 return해야함\n    elif selected_item in ['B','C] \n        return ['task_b','task_c'] # 여러 task를 동시에 수행시킬 땐 string 리스트로 반환\n\n# 일반 operator의 parameter도 있음\npython_branch_task = BranchPythonOperator(\n    task_id ='python_branch_task',\n    python_callable=select_random #select_random function 호출\n)\n\npython_branch_task &gt;&gt; [task_a , task_b , task_c]\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\npython_branch_task\n\npython_branch_task\n\n\n\ntask_a\n\ntask_a\n\n\n\npython_branch_task-&gt;task_a\n\n\n\n\n\ntask_b\n\ntask_b\n\n\n\npython_branch_task-&gt;task_b\n\n\n\n\n\ntask_c\n\ntask_c\n\n\n\npython_branch_task-&gt;task_c\n\n\n\n\n\n\n\n\n\n\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.python import BranchPythonOperator\n\nwith DAG(\n    dag_id='dags_branch_python_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'), \n    schedule='0 1 * * *',\n    catchup=False\n) as dag:\n    def select_random():\n        import random\n\n        item_lst = ['A','B','C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a' # task_id를 string 값으로 return해야함\n        elif selected_item in ['B','C']:\n            return ['task_b','task_c'] # 여러 task를 동시에 수행시킬 땐 리스트로 반환\n    \n    # 일반 operator의 parameter도 있음\n    python_branch_task = BranchPythonOperator(\n        task_id='python_branch_task',\n        python_callable=select_random\n    )\n    \n    # 후행 task 3개\n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    python_branch_task &gt;&gt; [task_a, task_b, task_c]\n\n나의 경우 airflow web service상에서 1회 실행 시켰을 때 selected_item의 값이 task_b, task_b가 선택됐음\n\ngraph 버튼을 눌러 보면 가장 최근에 돌았던 task들이 return 된다.\ntask_a가 분홍색 박스로 skipped 상태인 것을 확인 할 수 있다.\ngraph에서 python_branch_task를 누르고 xcom을 누르면 다음과 같은 table을 확인할 수 있다.\n\n\n\n\nKey\nValue\n\n\n\n\nskipmixin_key\n{‘followed’: [‘task_c’, ‘task_b’]}\n\n\nreturn_value\n[‘task_b’, ‘task_c’]\n\n\n\n\n여기서 skipmixin_key 의 value값의 key 값이 ‘followed’ 이고 [‘task_c’, ‘task_b’] 인 것을 볼 수 있다. 필요시 어떤 task들이 선택되었는지 확인하려면 xcom을 통해 확인 가능하다.\nlog 를 보면\n\n[2023-06-23, 23:20:01 UTC] {python.py:183} INFO - Done. Returned value was: ['task_b', 'task_c']\n[2023-06-23, 23:20:01 UTC] {python.py:216} INFO - Branch callable return ['task_b', 'task_c']\n[2023-06-23, 23:20:01 UTC] {skipmixin.py:161} INFO - Following branch ['task_b', 'task_c']\n[2023-06-23, 23:20:01 UTC] {skipmixin.py:221} INFO - Skipping tasks ['task_a']"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task.branch-이해-branchpythonoperator-vs-task.branch-decorator",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task.branch-이해-branchpythonoperator-vs-task.branch-decorator",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "from airflow.operators.python import BranchPythonOperator\ndef select_random(): \n    import random\n    item_lst = ['A','B','C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A':\n        return 'task_a'\n    elif selected_item in ['B','C']\n        return ['task_b','task_c']\n\npython_branch_task = BranchPythonOperator(\n    task_id= 'branching',\n    python_callable = select_random\n)\npython_branch_task &gt;&gt; [task_a , task_b , task_c]\n\nfrom airflow.operators.python import task\n\n@task.branch(task_id='python_branch_task')\ndef select_random(): \n    import random\n    item_lst = ['A','B','C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A':\n        return 'task_a'\n    elif selected_item in ['B','C']\n        return ['task_b','task_c']\n\nselect_random() &gt;&gt; [task_a , task_b , task_c]\n\n\n\nBranchPythonOperator와 비교하여 select_random()을 호출 또는 맵핑 하는 방식이 decorator에서는 @task.branch(task_id='python_branch_task')으로 표현 되었고 task flow를 표현하는 task connection 방식도 select_random() &gt;&gt; [task_a , task_b , task_c] 로 표현 됐다.\nBranchPythonOperator의 python_branch_task object와 task.branch (decorator)의 select_random()는 사실상 같은 객체이다.\n차이점은 BranchPythonOperator(...)를 실행시킨 것과 select_random(...) 함수를 실행한 것 외엔 그 역할과 기능은 같다 (같은 object 반환).\nDags Full Example\n\nfrom airflow import DAG\nfrom datetime import datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id='dags_python_with_branch_decorator',\n    start_date=datetime(2023,4,1),\n    schedule=None,\n    catchup=False\n) as dag:\n    @task.branch(task_id='python_branch_task')\n    def select_random():\n        import random\n        item_lst = ['A', 'B', 'C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a'\n        elif selected_item in ['B','C']:\n            return ['task_b','task_c']\n    \n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    select_random() &gt;&gt; [task_a, task_b, task_c]\n\nairflow web service의 결과물은 BranchPythonOperator나 decorator나 같았음"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#basebranchoperator-이해-요약",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#basebranchoperator-이해-요약",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "from airflow.operators.branch import BaseBranchOperator\nwith DAG(...\n) as dag:\n    class CustomBranchOperator(BaseBranchOperator): #클래스 이름은 임의로 지정해 줌\n    #Python의 class 상속 syntax: class MyclassName(상속할className):\n    #Python은 다중 상속가능\n        def choose_branch(self,context): \n        # 함수 재정의 : Overriding, 함수 이름 바꾸면 안됨!\n        # parameter도 바꾸면 안됨\n            import random\n            print(context) # context에 어떤 내용이 있는지 출력\n\n            item_lst = ['A', 'B','C]\n            selected_item = random.choice(item_lst)\n            if selected_item == 'A':\n                return 'task_a'\n            elif selected_item in ['B','C']:\n                return ['task_b','task_c']\n\ncustom_branch_operator = CustomBranchOperator(task_id ='python_branch_task') # 클래스 실행하여 custom_branch_operator object 생성\ncustom_branch_operator &gt;&gt; [task_a , task_b , task_c]\n\n클래스 상속하여 새로운 클래스 만들어야함: BaseBranchOperator 상속시 choose_branch 함수를 구현해 줘야 함\nCustomBranchOperator 클래스 이름은 임의로 지정해준 이름\nclass 선언시 class childClass(상속할parentClass): 상속할 부모클래스를 2개이상 지정하는 다중 상속이 가능하긴 하지만 권고하지 않음.\nchoose_branch() 함수를 만든 이유를 알기 위해선 BaseBranchOperator class에 대해서 알아야함\n\nairflow operators-airflow.operators.branch or google ‘airflow operators’ :::{.callout-note} ## Description\n\nBases: airflow.models.baseoperator.BaseOperator, airflow.models.skipmixin.SkipMixin A base class for creating operators with branching functionality, like to BranchPythonOperator. Users should create a subclass from this operator and implement the function choose_branch(self, context). This should run whatever business logic is needed to determine the branch, and return either the task_id for a single task (as a str) or a list of task_ids. The operator will continue with the returned task_id(s), and all other tasks directly downstream of this operator will be skipped. :::\n\n함수명과 인자(argument)명도 반드시 일치시켜야함\nchoose_branch(self,context)의 context는 pythonOperator 쓸때 **kwargs의 parameters들을 사용할 수 있게 해주는 parameter\n\ncontext 인자엔 op_kargs와 같이 data_interval_start, data_interval_end 등과 같은 정보를 제공해주는 인자\n\nprint(context) 결과\n\n[2023-06-24, 00:29:33 UTC] {logging_mixin.py:149} INFO - {'conf': &lt;***.configuration.AirflowConfigParser object at 0x7fc3d5dd2cd0&gt;, 'dag': &lt;DAG: dags_base_branch_operator&gt;, 'dag_run': &lt;DagRun dags_base_branch_operator @ 2023-06-24 00:29:31.444830+00:00: manual__2023-06-24T00:29:31.444830+00:00, state:running, queued_at: 2023-06-24 00:29:31.455604+00:00. externally triggered: True&gt;, 'data_interval_end': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'data_interval_start': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'ds': '2023-06-24', 'ds_nodash': '20230624', 'execution_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'expanded_ti_count': None, 'inlets': [], 'logical_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'macros': &lt;module '***.macros' from '/home/***/.local/lib/python3.7/site-packages/***/macros/__init__.py'&gt;, 'next_ds': '2023-06-24', 'next_ds_nodash': '20230624', 'next_execution_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'outlets': [], 'params': {}, 'prev_data_interval_start_success': None, 'prev_data_interval_end_success': None, 'prev_ds': '2023-06-24', 'prev_ds_nodash': '20230624', 'prev_execution_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'prev_execution_date_success': None, 'prev_start_date_success': None, 'run_id': 'manual__2023-06-24T00:29:31.444830+00:00', 'task': &lt;Task(CustomBranchOperator): python_branch_task&gt;, 'task_instance': &lt;TaskInstance: dags_base_branch_operator.python_branch_task manual__2023-06-24T00:29:31.444830+00:00 [running]&gt;, 'task_instance_key_str': 'dags_base_branch_operator__python_branch_task__20230624', 'test_mode': False, 'ti': &lt;TaskInstance: dags_base_branch_operator.python_branch_task manual__2023-06-24T00:29:31.444830+00:00 [running]&gt;, 'tomorrow_ds': '2023-06-25', 'tomorrow_ds_nodash': '20230625', 'triggering_dataset_events': &lt;Proxy at 0x7fc3ab28c8c0 with factory &lt;function TaskInstance.get_template_context.&lt;locals&gt;.get_triggering_events at 0x7fc3ab277c20&gt;&gt;, 'ts': '2023-06-24T00:29:31.444830+00:00', 'ts_nodash': '20230624T002931', 'ts_nodash_with_tz': '20230624T002931.444830+0000', 'var': {'json': None, 'value': None}, 'conn': None, 'yesterday_ds': '2023-06-23', 'yesterday_ds_nodash': '20230623'}\ncontext결과물은 위와 같은 시간 정보를 담고 있기 때문에 꺼내쓸 수 있다.\n분기 처리 결과는 다른 2 방식의 결과와 같음\nDAG Full Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.branch import BaseBranchOperator\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n    dag_id='dags_base_branch_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n    class CustomBranchOperator(BaseBranchOperator):\n        def choose_branch(self, context):\n            import random\n            print(context) # context에 어떤 내용이 있는지 출력\n            \n            item_lst = ['A', 'B', 'C']\n            selected_item = random.choice(item_lst)\n            if selected_item == 'A':\n                return 'task_a'\n            elif selected_item in ['B','C']:\n                return ['task_b','task_c']\n\n    \n    custom_branch_operator = CustomBranchOperator(task_id='python_branch_task')\n\n    \n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    custom_branch_operator &gt;&gt; [task_a, task_b, task_c]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#summary",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#summary",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "Task 분기처리 방법\n\nBranchPythonOperator (자주 사용)\ntask.branch 데커레이터 이용 (자주 사용)\nBaseBranchOperator 상속 , choose_branch 를 재정의해야 함 (덜 사용)\n\n공통적으로 리턴 값으로 후행 Task 의 id 를 str 또는 list 로 리턴해야 함\n3가지 분기처리 방법은 방법만 다를 뿐 결과는 동일함\n3 보다는 1 또는 2를 주로 사용함"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#trigger-rule-종류",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#trigger-rule-종류",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "G\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask4\n\ntask4\n\n\n\ntask1-&gt;task4\n\n\n\n\n\ntask2\n\ntask2\n\n\n\ntask2-&gt;task4\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask3-&gt;task4\n\n\n\n\n\n\n\n\n\n\n\nbranch와 반대되는 개념으로 여러 상위 tasks가 하나의 하위 task로 연결되는 flow로 만들때 사용\n즉, 여러 상위 Task 들의 상태에 따라 후행 task의 수행여부 결정할 때 쓰인다\n기본 값 : 여러 상위 Task들이 모두 성공시에만 수행\n상위 task의 수행 상태에 따라 조건적으로 후행 task의 수행 여부를 결정할 수 있다.\ntrigger option은 하위 task를 이용하여 줄 수 있다.\n모든 airflow operator는 trigger rule option을 줄 수 있다.\n11가지 trigger rules\n\n\n\n\n\n\n\n\nDefault\nLeft\n\n\n\n\nall_success (default)\n상위 tasks 가 모두 성공하면 실행\n\n\nall_failed\n상위 tasks 가 모두 실패하면 실행\n\n\nall_done\n상위 tasks 가 모두 수행되면 실행 (실패도 수행된것에 포함)\n\n\nall_skipped\n상위 tasks 가 모두 Skipped 상태면 실행\n\n\none_failed\n상위 tasks 중 하나 이상 실패하면 실행 (모든 상위 Tasks의 완료를 기다리지 않음)\n\n\none_success\n상위 tasks 중 하나 이상 성공하면 실행 (모든 상위 Tasks의 완료를 기다리지 않음)\n\n\none_done\n상위 tasks 중 하나 이상 성공 또는 실패 하면 실행\n\n\nnone_failed\n상위 task s중 실패가 없는 경우 실행 (성공 또는 Skipped 상태)\n\n\nnone_failed_min_one_success\n상위 tasks 중 실패가 없고 성공한 Task가 적어도 1개 이상이면 실행\n\n\nnone_skipped\nSkip된 상위 Task가 없으면 실행 (상위 Task가 성공, 실패하여도 무방)\n\n\nalways\n언제나 실행\n\n\n\n\n위의 표에서 모든 상위 task를 기다리지 않음은 각 각의 상위 task들의 처리 시간이 다를 때 가장 빠르게 처리되는 상위 task에 따라서 후행 task가 수행된다는 것을 의미한다. 예를 들어, one_failed의 경우\n\n상위 task1 (2분소요)\n상위 task2 (10분소요)\n상위 task3 (20분소요) 일때,\n상위 task 3개 중 task1의 결과가 먼저 fail이 나올 경우 task2,3 을 기다리지 않고 바로 triger가 발동되어 하위 task4가 수행된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#trigger-rule-실습-trigger_rule-all_done",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#trigger-rule-실습-trigger_rule-all_done",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "아래 예시에서 4개의 task가 정의됨\n\n\n\n\n# 상위 task1\nbash_upstream_1 = BashOperator(\n    task_id = 'bash_upstream_1',\n    bash_command = 'echo upstream1'\n)\n\n@task(task_id =='python_upstream_1') # 상위 task2\ndef python_upstream_1():\n    AirflowException('downstream_1 Exception!') # AirflowException() fail을 반환하여 무조건 task 실패처리가되도록 설정\n\n@task(task_id =='python_upstream_2') # 상위 task3\ndef python_upstream_2():\n    print('정상 처리')\n\n@task(task_id ='python_downstream_1', trigger_rule ='all_done') #하위 task4\ndef python_downstream_1():\n    print('정상 처리')\n\n[bash_upstream_1 , python_upstream_1(), python_upstream_2()] &gt;&gt; python_downstream_1()\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nbash_upstream_1\n\nbash_upstream_1\n\n\n\npython_downstream_1\n\npython_downstream_1\n\n\n\nbash_upstream_1-&gt;python_downstream_1\n\n\n\n\n\npython_upstream_1\n\npython_upstream_1\n\n\n\npython_upstream_1-&gt;python_downstream_1\n\n\n\n\n\npython_upstream_2\n\npython_upstream_2\n\n\n\npython_upstream_2-&gt;python_downstream_1\n\n\n\n\n\n\n\n\n\n\n\n\n\nbash_upstream_1(성공), python_upstream_1(실패), python_upstream_2(성공).\ntriger rule이 all done이기 때문에 python_upstream_1(실패)여도 python_downstream_1은 수행되어야 한다.\n다른 Operator such as BashOperator, pythonOperator의 경우도 trigger_rule =='all_done' parameter 똑같이 넣어주면 됨"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#trigger-rule-실습-triger_rule-none_skipped",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#trigger-rule-실습-triger_rule-none_skipped",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "@task.branch(task_id ='branching') #상위 task1\ndef random_branch():\n    import random\n    item_lst = [' A', ' B', 'C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A':\n        return 'task_a'\n    elif selected_item == 'B':\n        return 'task_b'\n    elif selected_item == 'C':\n        return 'task_c'\n\n#상위 task2\ntask_a = BashOperator(\n    task_id ='task_a',\n    bash_command = 'echo upstream1'\n    )\n\n#상위 task3\n@task(task_id ='task_b')\ndef task_b():\n    print('정상 처리')\n\n#상위 task4\n@task(task_id =='task_c')\ndef task_c():\n    print('정상 처리')\n\n#하위 task5\n@task(task_id =='task_d', trigger_rule ='none_skipped')\ndef task_d():\n    print('정상 처리')\n\nrandom_branch() &gt;&gt; [task_a , task_b(), task_c()] &gt;&gt; task_d()\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nrandom_branch\n\nrandom_branch\n\n\n\ntask_a\n\ntask_a\n\n\n\nrandom_branch-&gt;task_a\n\n\n\n\n\ntask_b\n\ntask_b\n\n\n\nrandom_branch-&gt;task_b\n\n\n\n\n\ntask_c\n\ntask_c\n\n\n\nrandom_branch-&gt;task_c\n\n\n\n\n\ntask_d\n\ntask_d\n\n\n\ntask_a-&gt;task_d\n\n\n\n\n\ntask_b-&gt;task_d\n\n\n\n\n\ntask_c-&gt;task_d\n\n\n\n\n\n\n\n\n\n\n\n\n\nskip이 있기 때문에 실제로 task_d가 돌지 말아야한다.\nDags Full Example\n\n\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.exceptions import AirflowException\n\nimport pendulum\n\nwith DAG(\n    dag_id='dags_python_with_trigger_rule_eg1',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n    bash_upstream_1 = BashOperator(\n        task_id='bash_upstream_1',\n        bash_command='echo upstream1'\n    )\n\n    @task(task_id='python_upstream_1')\n    def python_upstream_1():\n        raise AirflowException('downstream_1 Exception!')\n\n\n    @task(task_id='python_upstream_2')\n    def python_upstream_2():\n        print('정상 처리')\n\n    @task(task_id='python_downstream_1', trigger_rule='all_done')\n    def python_downstream_1():\n        print('정상 처리')\n\n    [bash_upstream_1, python_upstream_1(), python_upstream_2()] &gt;&gt; python_downstream_1()\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.exceptions import AirflowException\n\nimport pendulum\n\nwith DAG(\n    dag_id='dags_python_with_trigger_rule_eg2',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n    @task.branch(task_id='branching')\n    def random_branch():\n        import random\n        item_lst = ['A', 'B', 'C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a'\n        elif selected_item == 'B':\n            return 'task_b'\n        elif selected_item == 'C':\n            return 'task_c'\n\n    task_a = BashOperator(\n        task_id='task_a',\n        bash_command='echo upstream1'\n    )\n\n    @task(task_id='task_b')\n    def task_b():\n        print('정상 처리')\n\n\n    @task(task_id='task_c')\n    def task_c():\n        print('정상 처리')\n\n    @task(task_id='task_d', trigger_rule='none_skipped')\n    def task_d():\n        print('정상 처리')\n\n    random_branch() &gt;&gt; [task_a, task_b(), task_c()] &gt;&gt; task_d()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-개념",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-개념",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "tasks를 모아 관리\nTask들의 모음: dags안에 task가 많을 경우 비슷한 기능의 tasks 그룹으로 모아서 관리\n\n예를 들어, dag안에 50개의 tasks 있다고 할 때, 5개 tasks가 서로 연관성이 높은 connection을 이루고 이런 group이 10개가 있을 수 있다.\n\nlink: UI Graph탭에서 Task 들을 Group 화하여 보여줌-TaskGroups or google ‘airflow dags’\n\ncontent &gt;&gt; Core Concepts &gt;&gt; DAGs &gt;&gt; DAG Visualization &gt;&gt; Task Groups\n\nTask Group 안에 Task Group 을 중첩하여 계층적으로 구성 가능\n위의 링크에서 section1 과 section2 로 grouping되어 있고 section2에는 inner_section_2 라는 또 다른 task group이 있다.\n꼭 써야하는 이유는 성능적인 면에서 딱히 없지만 task flow의 가독성이 높아짐"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-실습-task_group-데커레이터-이용",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-실습-task_group-데커레이터-이용",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "from airflow.decorators import task_group\nwith DAG(...\n) as dag:\n    @task_group(group_id ='first_group')\n    def group_1():\n    ''' task_group 데커레이터를 이용한 첫 번째 그룹입니다. ''' # docstring: 함수를 설명하는 글\n    # airflow UI에서는 tooltip이라고 표시됨\n\n    @task(task_id ='inner_function1')\n    def inner_func1(**kwargs):\n        print('첫 번째 TaskGroup 내 첫 번째 task 입니다')\n\n    inner_function2 = PythonOperator(\n        task_id ='inner_function2',\n        python_callable = inner_func,\n        op_kwargs={'msg':'첫 번째 TaskGroup 내 두 번쨰 task 입니다.'}\n    )\n    inner_func1() &gt;&gt; inner_function2"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-실습-클래스-이용",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-실습-클래스-이용",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "from airflow.utils.task_group import TaskGroup\n    with TaskGroup(group_id ='second_group', tooltip='두 번째 그룹입니다.') as group_2:\n    #tooltipe은 docstring과 같은 역할을 함\n        @task(task_id ='inner_function1')\n        def inner_func1 (**kwargs):\n            print('두 번째 TaskGroup 내 첫 번째 task 입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id = 'inner_function2',\n            python_collable = inner_func,\n            op_kwargs = {'msg': '두 번째 TaskGroup 내 두 번째 task 입니다'}\n        )\ninner_func1() &gt;&gt; inner_function2\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\nfrom airflow.decorators import task_group\nfrom airflow.utils.task_group import TaskGroup\n\nwith DAG(\n    dag_id=\"dags_python_with_task_group\",\n    schedule=None,\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    def inner_func(**kwargs):\n        msg = kwargs.get('msg') or '' \n        print(msg)\n\n    @task_group(group_id='first_group')\n    def group_1():\n        ''' task_group 데커레이터를 이용한 첫 번째 그룹입니다. '''\n\n        @task(task_id='inner_function1')\n        def inner_func1(**kwargs):\n            print('첫 번째 TaskGroup 내 첫 번째 task입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id='inner_function2',\n            python_callable=inner_func,\n            op_kwargs={'msg':'첫 번째 TaskGroup내 두 번쨰 task입니다.'}\n        )\n\n        inner_func1() &gt;&gt; inner_function2\n\n    with TaskGroup(group_id='second_group', tooltip='두 번째 그룹입니다') as group_2:\n        ''' 여기에 적은 docstring은 표시되지 않습니다'''\n        @task(task_id='inner_function1')\n        def inner_func1(**kwargs):\n            print('두 번째 TaskGroup 내 첫 번째 task입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id='inner_function2',\n            python_callable=inner_func,\n            op_kwargs={'msg': '두 번째 TaskGroup내 두 번째 task입니다.'}\n        )\n        inner_func1() &gt;&gt; inner_function2\n\n    group_1() &gt;&gt; group_2\n\n위에서 task_id와 group_id가 같지만 에러가 안나는 이유가 task group이 다르기 때문.\n위에서 볼 수 있듯이 task group 또한 flow 설정할 수 있음 group_1() &gt;&gt; group_2"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#요약",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#요약",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "Task Group 작성 방법은 2 가지가 존재함 (데커레이터 & 클래스)\nTask Group 안에 Task Group 중첩하여 정의 가능\nTask Group 간에도 Flow 정의 가능\nGroup이 다르면 task_id 가 같아도 무방\nTooltip 파라미터를 이용해 UI 화면에서 Task group 에 대한 설명 제공 가능 (데커레이터 활용시 docstring 으로도 가능)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-개념",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-개념",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "Task 연결에 대한 설명 (즉 edge에 대한 Comment)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-만들기",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-만들기",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "G\n\n\ncluster0\n\nTask Flow\n\n\n\ningest\n\ningest\n\n\n\nanalyze\n\nanalyze\n\n\n\ningest-&gt;analyze\n\n\n\n\n\ncheck_integrity\n\ncheck_integrity\n\n\n\nanalyze-&gt;check_integrity\n\n\n\n\n\ndescribe_integrity\n\ndescribe_integrity\n\n\n\ncheck_integrity-&gt;describe_integrity\n\n\nErrors Found\n\n\n\nsave\n\nsave\n\n\n\ncheck_integrity-&gt;save\n\n\nNo Errors\n\n\n\nemail_error\n\nemail_error\n\n\n\ndescribe_integrity-&gt;email_error\n\n\n\n\n\nreport\n\nreport\n\n\n\nemail_error-&gt;report\n\n\n\n\n\nsave-&gt;report"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-실습-1",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-실습-1",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "from airflow.utils.edgemodifier import Label\nempty_1 = EmptyOperator(\n    task_id ='empty_1'\n)\n\nempty_2 = EmptyOperator(\n    task_id='empty_2'\n)\nempty_1 &gt;&gt; Label ('1 과 2 사이') &gt;&gt; empty_2"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-실습-2",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-실습-2",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "from airflow.utils.edgemodifier import Label\nempty_2 = EmptyOperator(\n    task_id = 'empty_2'\n)\n\nempty_3 = EmptyOperator(\n    task_id ='empty_3'\n)\n\nempty_4 = EmptyOperator(\n    task_id ='empty_4'\n)\n\nempty_5 = EmptyOperator(\n    task_id ='empty_5'\n)\n\nempty_6 = EmptyOperator(\n    task_id ='empty_6'\n)\n\nempty_2 &gt;&gt; Label('Start Branch') &gt;&gt; [empty_3, empty_4, empty_5 ] &gt;&gt; Label('End Branch') &gt;&gt; empty_6\n\n이렇게 분기가 펼쳐지고 모아지는 경우 모든 분기 edges에 label이 붙게 된다.\nFull DAG Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\n\n\nwith DAG(\n    dag_id=\"dags_empty_with_edge_label\",\n    schedule=None,\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    empty_1 = EmptyOperator(\n        task_id='empty_1'\n    )\n\n    empty_2 = EmptyOperator(\n        task_id='empty_2'\n    )\n\n    empty_1 &gt;&gt; Label('1과 2사이') &gt;&gt; empty_2\n\n    empty_3 = EmptyOperator(\n        task_id='empty_3'\n    )\n\n    empty_4 = EmptyOperator(\n        task_id='empty_4'\n    )\n\n    empty_5 = EmptyOperator(\n        task_id='empty_5'\n    )\n\n    empty_6 = EmptyOperator(\n        task_id='empty_6'\n    )\n\n    empty_2 &gt;&gt; Label('Start Branch') &gt;&gt; [empty_3,empty_4,empty_5] &gt;&gt; Label('End Branch') &gt;&gt; empty_6\n\nempty operator이기 때문에 실행은 airflow web servce에서 실행은 안해도 된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-실습",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-실습",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "task_group 데커레이터 이용\n\nfrom airflow.decorators import task_group\nwith DAG(...\n) as dag:\n    @task_group(group_id ='first_group')\n    def group_1():\n    ''' task_group 데커레이터를 이용한 첫 번째 그룹''' # docstring: 함수를 설명하는 기법\n    # airflow UI에서는 tooltip이라고 표시됨\n\n    @task(task_id ='inner_function1')\n    def inner_func1(**kwargs):\n        print('첫 번째 TaskGroup 내 첫 번째 task 입니다')\n\n    inner_function2 = PythonOperator(\n        task_id ='inner_function2',\n        python_callable = inner_func,\n        op_kwargs={'msg':'첫 번째 TaskGroup 내 두 번쨰 task 입니다.'}\n    )\n    inner_func1() &gt;&gt; inner_function2\n\ntask_group 데커레이터 이용하지 않음 (클래스 이용)\n\nfrom airflow.utils.task_group import TaskGroup\n    with TaskGroup(group_id ='second_group', tooltip='두 번째 그룹') as group_2: # with MyClassName(arg1,age2,...) \n    # tooltipe은 decorator를 이용한 task_group 생성때의 docstring과 같은 역할을 함\n        @task(task_id ='inner_function1')\n        def inner_func1 (**kwargs):\n            print('두 번째 TaskGroup 내 첫 번째 task 입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id = 'inner_function2',\n            python_collable = inner_func,\n            op_kwargs = {'msg': '두 번째 TaskGroup 내 두 번째 task 입니다'}\n        )\ninner_func1() &gt;&gt; inner_function2\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\nfrom airflow.decorators import task_group\nfrom airflow.utils.task_group import TaskGroup\n\nwith DAG(\n    dag_id=\"dags_python_with_task_group\",\n    schedule=None,\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    def inner_func(**kwargs):\n        msg = kwargs.get('msg') or '' \n        print(msg)\n\n    @task_group(group_id='first_group')\n    def group_1():\n        ''' task_group 데커레이터를 이용한 첫 번째 그룹 '''\n\n        @task(task_id='inner_function1')\n        def inner_func1(**kwargs):\n            print('첫 번째 TaskGroup 내 첫 번째 task입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id='inner_function2',\n            python_callable=inner_func,\n            op_kwargs={'msg':'첫 번째 TaskGroup내 두 번쨰 task입니다.'}\n        )\n\n        inner_func1() &gt;&gt; inner_function2\n\n    with TaskGroup(group_id='second_group', tooltip='두 번째 그룹') as group_2:\n        ''' 클래스 안에 적은 docstring은 표시되지 않음'''\n        @task(task_id='inner_function1')\n        def inner_func1(**kwargs):\n            print('두 번째 TaskGroup 내 첫 번째 task입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id='inner_function2',\n            python_callable=inner_func,\n            op_kwargs={'msg': '두 번째 TaskGroup내 두 번째 task입니다.'}\n        )\n        inner_func1() &gt;&gt; inner_function2\n\n    group_1() &gt;&gt; group_2\n\n위에서 task_id와 group_id가 같지만 에러가 안나는 이유가 task group이 다르기 때문.\n위에서 볼 수 있듯이 task group 또한 flow 설정할 수 있음 group_1() &gt;&gt; group_2"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html",
    "title": "More Operators",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile Path\nOperator (Class)\nImportance\nNote\n\n\n\n\nairflow.models.bashoperator\nBaseOperator\n***\n* base operator는 직접 쓰는게 아니라 user가 custom operator를 직접 개발하고싶은 경우 이 클래스를 상속하여 개발하기 위해 만든 operator (execute() 함수를 재정의(Override)하여 사용)  * 아래 오퍼레이터들은 모두 이 클래스를 상속하여 개발되어 있음  * Airflow를 잘 쓰려면 이 오퍼레이터 상속/개발하는 것을 자유자재로 할 줄 알아야 함.\n\n\nairflow.operators.bash\nBashOperator\n***\n* bash 쉘 스크립트를 실행  * 가장 많이 사용하는 오퍼레이터 중 하나임\n\n\nairflow.operators.branch\nBaseBranchOperator\n**\n* 직접 사용할 수는 없음.  * 이 클래스를 상속하여 choose_branch 함수를 구현해야 함 (그냥 사용시 에러 발생)  * 그러나 이 클래스 상속해서 사용하는것보다 @task.branch 데코레이터 사용을 권장\n\n\nairflow.operators.datetime\nBranchDateTimeOperator\n\n* 특정 Datetime 구간을 주어 Job 수행 날짜가 구간에 속하는지 여부에 따라 분기 결정\n\n\nairflow.operators.email\nEmailOperator\n\n이메일 전송하는 오퍼레이터 (Airflow 파라미터에 SMTP 서버 등 사전 셋팅 필요)\n\n\nairflow.operators.generic_transfer\nGenericTransfer\n\n데이터를 소스에서 타겟으로 전송 (Airflow 커넥션에 등록되어 있는 대상만 가능)\n\n\nairflow.operators.latest_only\nLatestOnlyOperator\n\ndag을 수행시킬 때 스케쥴이 아니라 backfill(과거 날짜로 dag 수행) 이 Task 뒤에 연결되어 있는 task들을 모두 가장 최근에 설정된 스케쥴에 의해서만 task가 실행되게끔 하는 오퍼레이터. 수작업으로 dag을 수행 시켰거나 과거날짜로 dag을 수행시켰을 때는 후행 task들은 돌아가지 않음. 가장 최근에 설정된 job에 의해서만 후행 task들이 돌아감.\n\n\nairflow.operators.subdag\nSubDagOperator\n\n* 일종의 task 그룹화, dag안에 또다른 dag을 불러올 수 있음. 해당 오퍼레이터 안에 다른 오퍼레이터를 둘 수 있음 (Task group과 유사)\n\n\nairflow.operators.trigger_dagrun\nTriggerDagRunOperator\n**\n* 다른 DAG을 수행시키기 위한 오퍼레이터. 예를 들어 task1에 의해 다른 dag이 수행되도록 설정할 수 있다.\n\n\nairflow.operators.weekday\nBranchDayOfWeekOperator\n\n* 특정 요일에 따라 분기처리할 수 있는 오퍼레이터\n\n\nairflow.operators.python\nPythonOperator\n***\n* 어떤 파이썬 함수를 실행시키기 위한 오퍼레이터\n\n\nairflow.operators.python\nBranchPythonOperator\n*\n* 파이썬 함수 실행 결과에 따라 task를 선택적으로 실행시킬 때 사용되는 오퍼레이터\n\n\nairflow.operators.python\nShortCircuitOperator\n\n* 파이썬 함수 return 값이 False면 후행 Tasks를 Skip처리하고 dag을 종료시키는 오퍼레이터\n\n\nairflow.operators.python\nPythonVirtualenvOperator\n\n* 파이썬 가상환경 생성후 Job 수행하고 마무리되면 가상환경을 삭제해주는 오퍼레이터\n\n\nairflow.operators.python\nExternalPythonOperator\n\n* 기존에 존재하는 파이썬 가상환경에서 Job 수행하게 하는 오퍼레이터\n\n\n\n\nmore oeprators-airflow operators\n\n\n\n\nairflow web service &gt;&gt; Admin &gt;&gt; Providers 에서 확인할 수 있음\nsolution 제공 업체에서 본인들의 solution을 잘 다루게 하기 위해 만든 airflow에 제공한 operator\n솔루션 제공 업체: AWS, GCP, MS Azure 등\n솔루션이란?\n\n제품 또는 서비스 패키지: 여기서 “solution”은 특정 사용 사례에 맞춰진, 사전 구성된 클라우드 서비스의 세트를 말한다. 이는 고객이 보다 쉽고 빠르게 해당 기술을 도입하고 활용할 수 있도록 도와주는 ‘패키지’ 형태의 제품이나 서비스를 의미한다.\n기술적 해결책 또는 구현: 이 경우, “solution”은 특정 비즈니스 요구사항이나 기술적 문제를 해결하기 위한 방법론, 소프트웨어, 서비스, 아키텍처의 조합을 의미한다. 예를 들어, 데이터 분석, 웹 호스팅, 머신 러닝 모델 훈련과 같은 특정 목적을 위한 AWS나 GCP의 기능 및 서비스의 집합을 “solution”이라고 할 수 있다.\n첫 번째 뜻으로 이해해도 많은 context 커버 가능\n\n예를 들어, airflow web service &gt;&gt; Admin &gt;&gt; Providers &gt;&gt; apache-airflow-providers-amazon &gt;&gt; Python API &gt;&gt; [airflow.providers.amazon.aws.hooks, airflow.providers.amazon.aws.operators]\n참고로 airflow는 GCP와 궁합이 잘맞음\n\ntransfer: data 이동\n\napache-airflow-providers-http\n\nSimpleHttpOperator를 이용하여 API 값을 얻어 올 수 있음.\n\n\n\n\n\n\n\n\n\n의존 관계 : dag간 선 후행 관계\nDAG 의존관계 설정 방법\n\nTriggerDagRun Operator\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask2\n\ntask2\n\n\n\ntask1-&gt;task2\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask1-&gt;task3\n\n\n\n\n\ntask4\n\ntask4\n\n\n\ntask1-&gt;task4\n\n\n\n\n\n\n\n\n\n\n\ntask를 만들 때 task내에 dag_id를 명시하는 parameter가 있음\ntask1: PythonOperator 등\ntask2,3,4: TriggerDagRun 오퍼레이터로 다른 DAG 을 실행시키는 오퍼레이터\n\n\nExternalTask Sensor\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nsensor1\n\nsensor1\n\n\n\ntask1\n\ntask1\n\n\n\nsensor1-&gt;task1\n\n\n\n\n\nsensor2\n\nsensor2\n\n\n\nsensor2-&gt;task1\n\n\n\n\n\nsensor3\n\nsensor3\n\n\n\nsensor3-&gt;task1\n\n\n\n\n\nsensor4\n\nsensor4\n\n\n\nsensor4-&gt;task1\n\n\n\n\n\n\n\n\n\n\n\nsensor를 통해 task를 만들기 때문에 여기서 sensor는 task를 의미함\nsensor를 만들 때도 감지해야할 dag_id를 명시해줘야함 (task_id도 명시 가능)\nExternalTask Sensor는 다른 여러 DAGs의 Tasks의 완료를 기다리는 센서\nDAG간 의존관계 설정\n\n방식\n\nTriggerDagRun Operator: 실행할 다른 DAG 의 ID 를 지정하여 수행\n\\(A\\quad Dag \\subset B\\quad Dag\\) 일 때, B Dag이 A Dag을 trigger한다.\n\nExternalTask 센서: 본 Task 가 수행되기 전 다른 DAG 의 완료를 기다린 후 수행\n\n권고 사용 시점\n\nTriggerDagRun Operator: Trigger 되는 DAG 의 선행 DAG 이 하나만 있을 경우\nExternalTask 센서: Trigger 되는 DAG 의 선행 DAG 이 2 개 이상인 경우\n\n\n\n\n\n\n\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\nwith DAG(...) as dag:\n\n    start_task = BashOperator(\n        task_id='start_task',\n        bash_command='echo \"start!\"',\n    )\n\n    trigger_dag_task = TriggerDagRunOperator(\n        task_id='trigger_dag_task', #필수값\n        trigger_dag_id='dags_python_operator', #필수값\n        trigger_run_id=None, # 중요: run_id 값 직접 지정\n        execution_date='{{data_interval_start}}',\n        reset_dag_run=True,\n        wait_for_completion=False,\n        poke_interval=60,\n        allowed_states=['success'],\n        failed_states=None\n        )\n\n    start_task &gt;&gt; trigger_dag_task\n\n\n\n\nrun_id: DAG의 수행 방식과 시간을 유일하게 식별해주는 키\n같은 시간이라 해도 수행 방식 (Schedule, manual, Backfill) 에 따라 키가 달라짐\n\n스케쥴: 스케줄에 의해 실행된 경우 scheduled__{{data_interval_start}} 값을 가짐\nmanual: airflow ui web에서 수작업 수행. manual__{{data_interval_start}} 값을 가짐\n\nmanual__{{data_interval_start}}은 수작업 수행 시간이 아니라 수작업으로 실행시킨 스케쥴의 구간값 중 data_interval_start값을 의미\n\nBackfill: 과거 날짜를 이용해 수행. backfill__{{data_interval_start}} 값을 가짐\n\nrun_id를 보는 방법\n\nairflow ui web service &gt;&gt; dag &gt;&gt; grid &gt;&gt; 초록색 긴막대기 &gt;&gt; status Run ID 있음\n\n\n\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\nwith DAG(...) as dag:\n\n    start_task = BashOperator(\n        task_id='start_task',\n        bash_command='echo \"start!\"',\n    )\n\n    trigger_dag_task = TriggerDagRunOperator(\n        task_id='trigger_dag_task', \n        trigger_dag_id='dags_python_operator',\n        trigger_run_id=None, # rund_id 값 직접 지정\n        execution_date='{{data_interval_start}}', # manual_{{execution_date}} 로 수행 (여기에 값을 주면 메뉴얼 방식으로 trigger로 된걸로 간주)\n        reset_dag_run=True, # 이미 run_id 값으로 수행된 이력이 있는 경우에도 dag을 재수행할 것 인지 결정. True면 재수행\n        wait_for_completion=False,\n        poke_interval=60,\n        allowed_states=['success'],\n        failed_states=None\n        )\n\n    start_task &gt;&gt; trigger_dag_task\n\nwait_for_completion:\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask2_by_trigger_dag_run\n\ntask2_by_trigger_dag_run\n\n\n\ntask1-&gt;task2_by_trigger_dag_run\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask2_by_trigger_dag_run-&gt;task3\n\n\n\n\n\ndag_c\n\ndag_c\n\n\n\ntask2_by_trigger_dag_run-&gt;dag_c\n\n\n\n\n\n\n\n\n\n\n\nwait_for_completion=True 의 의미: task2가 dag_c를 돌리고 dag_c가 성공한 상태 후 task2역시 성공 상태가 된 후에 task3을 돌리는 경우\nwait_for_completion=False 의 의미: dag_c의 성공여부와 상관없이 task2가 성공 상태로 빠져나옴\npoke_interval=60 : dag_c의 성공여부를 모니터링 하는 주기 60초\nallowed_states=[‘success’]: trigger dag의 task2가 성공상태로 끝나기 위해 dag_c가 어떤 상태로 수행이 끝나야하는지 명시. 만약 dag_c가 fail상태여도 task2가 성공 상태로 마킹이 되길 원한다면 allowed_states=[‘success’,‘fail’] 로 명시\nfailed_states=None: task2가 실패 상태로 마킹이 되기 위해 dag_c가 어떤 상태로 작업 수행이 완료 되어야 하는지 명시. failed_states=fail이면 dag_c가 실패가 되어야 task2도 실패로 마킹 된다.\nDag Full example\n\n# Package Import\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\nimport pendulum\n\nwith DAG(\n    dag_id='dags_trigger_dag_run_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='30 9 * * *', #9시 30분 daily schedule\n    catchup=False\n) as dag:\n\n    start_task = BashOperator(\n        task_id='start_task',\n        bash_command='echo \"start!\"',\n    )\n\n    trigger_dag_task = TriggerDagRunOperator(\n        task_id='trigger_dag_task',\n        trigger_dag_id='dags_python_operator',\n        trigger_run_id=None,\n        execution_date='{{data_interval_start}}', #9시 30분 daily schedule\n        reset_dag_run=True,\n        wait_for_completion=False,\n        poke_interval=60,\n        allowed_states=['success'],\n        failed_states=None\n        )\n\n    start_task &gt;&gt; trigger_dag_task\n\n\n\n\nSimpleHttp 오퍼레이터를 이용하여 공공데이터 키 발급받기\n\n\n\n“Public Data API Key”를 발급받는 과정은 대개 다음과 같은 단계를 포함한다. 이는 일반적인 절차로, 특정 공공 데이터 API 제공자에 따라 약간의 차이가 있을 수 있다.\n\n공공 데이터 포털 접속: 대부분의 국가에서는 공공 데이터를 제공하기 위한 중앙화된 포털을 운영한다.. 예를 들어, 한국에서는 ’공공데이터포털’이 있다.\n회원가입 및 로그인: 포털에 접속한 후, 사용자 계정을 생성하고 로그인한다.\nAPI 키 신청: 로그인 후, API 키를 신청할 수 있는 섹션을 찾는다. 이는 보통 ‘API 키 관리’, ‘내 어플리케이션’, ‘API 신청’ 등의 메뉴에서 찾을 수 있다.\n애플리케이션 등록: API 키를 신청하기 위해서는 대부분의 경우 애플리케이션을 등록해야 하는데 이 과정에서 애플리케이션 이름, 용도, URL 등의 정보를 입력해야 할 수도 있다.\nAPI 키 발급: 애플리케이션 등록이 완료되면, API 키가 발급된다. 이 키는 API를 호출할 때 필요하므로 안전하게 보관해야 한다.\nAPI 문서 확인: API를 효과적으로 사용하기 위해서는 해당 API의 문서를 확인하는 것이 중요한데 문서에서는 API의 엔드포인트, 요청 방식, 필요한 파라미터 등의 정보를 제공한다.\nAPI 호출 시험: API 키를 사용하여 간단한 API 호출하여 API가 정상적으로 작동하는지 확인한다.\n각 공공 데이터 포털의 구체적인 지침과 절차는 웹사이트를 참조해야 한다.\n\n\n\n\n\n\n서울시 열린데이터 광장 portal\n서울시가 보유한 데이터 다운로드 가능\n일회성 다운로드 : Csv, json, xml 등을 직접 다운로드\n스케쥴에 의한 주기성 다운로드 : openAPI(http method를 통해 data를 다운로드 할 수 있도록 개방해놓은 API) 통해 다운 가능\n데이터 검색\n\n먼저, 어떤 종류의 데이터를 다운로드 받을 수 있는지 확인하기 위해 서울 열린데이터 광장 검색창에 데이터 검색하거나 데이터 카테고리 선택하여 openAI Tag붙은 데이터를 선택해야 한다 (본인은 문화/관광 선택).\n\n\nopen API tag없으면 manual로 데이터 셋 다운로드 받아야함.\n\n\nSample URL: 서울시립미술관 전시 현황 http://openapi.seoul.go.kr:8088/(인증키)/xml/ListExhibitionOfSeoulMOAInfo/1/5/\nSample URL 작성은 요청 인자를 참고하여 적어 넣으면 된다. 예를 들어,\n\nhttp://openapi.seoul.go.kr:8088/(인증키)/xml/ListExhibitionOfSeoulMOAInfo/1/5/ 은 아래와 같은 요청 인자 양식에 의해 적혀져 있다.\nhttp://openapi.seoul.go.kr:8088/(API key)/(data type)/(service)/(data start_index)/(data end_index)/\n1 부터 1000행 까지는 한번에 가져올 수 있지만 1000행 넘어가면 에러 발생\n\n그래서, 1~1000행, 10001행~2000행, … 와 같이 끊어서 가져가야 함\n\n요청 인자 중: DP_SEQ, DP_NAME이 있는데 특정 값을 입력하면 filtering되어 조건에 만족하는 데이터만 query해서 가져올 수 있음.\n\n다른 Sample URL 예시: 서울시 자랑스러운 한국음식점 정보 (한국어)\n\nurl 양식: http://openapi.seoul.go.kr:8088/(key:인증키)/(type)/(servicec)/(type)/(start_index)/(end_index)/(main_key 혹은 날짜)\n샘플 URL: http://openapi.seoul.go.kr:8088/(인증키)/xml/SebcKoreanRestaurantsKor/1/5\n\n이렇게 Sample URL 양식은 http://openapi.seoul.go.kr:8088/(key:인증키)/(type)/(servicec)/(type)/(start_index)/(end_index) 까지는 공통됨.\n\nopenAPI 이용할 경우 api KEY 발급받아야 함 로그인 필요\n\n로그인 &gt;&gt; 이용 안내 &gt;&gt; open API 소개 &gt;&gt; API Key 신청 &gt;&gt; 애플리케이션 등록\nAPI Key 신청: 일반 인증키 신청 or 실시간 지하철 인증키 신청\n\nsheet는 최대 1,000건 (행) 까지 노출됨. 전체 데이터는 CSV파일을 내려받아 확인해야함.\n\n애플리케이션 등록\n\n서비스(사용) 환경: 웹사이트 개발\n사용 URL: localhost (or 이 데이터를 사용할 여러분들의 홈패이지 주소)\n관리용 대표 이메일: 홍길동@naver.com\n활용용도: 블로그 개발 (자유형식 - 적당히 내용 채워 넣음)\n내용: 문화/전시 관련 소식을 스케쥴을 이용해 전달 받음 (자유형식 - 적당히 내용 채워 넣음)\n\n\n\n\n\n\n\n\n\n\n\nHTTP 요청을 하고 결과로 text 를 리턴 받는 오퍼레이터 리턴 값은 Xcom 에 저장\nHTTP 를 이용하여 API 를 처리하는 RestAPI 호출시 사용 가능\n\nRestAPI: API 방법 중 하나로 http의 protocol을 이용해서 API data를 제공하거나, 다운로드, 변경할 수 있는 API를 제공하는 방식\n\n\n\n\n\n\n\nNote\n\n\n\n\nREST API는 Representational State Transfer의 약자로, 웹 기반의 서비스 간에 통신을 위한 일반적인 규칙(아키텍처 스타일)을 의미\nREST API는 인터넷에서 데이터를 교환하기 위한 표준 방법 중 하나로 널리 사용되는데 간단히 말해서, REST API는 웹 애플리케이션에서 다른 시스템과 정보를 주고받기 위한 방법이다.\nREST API의 주요 특징\n\n클라이언트-서버 구조: REST API는 클라이언트(예: 웹 브라우저)와 서버 간의 분리를 기반으로 함. 클라이언트는 사용자 인터페이스와 사용자 상호작용을 관리하고, 서버는 데이터 저장 및 백엔드 로직을 처리\n무상태(Stateless): 각 요청은 독립적. 즉, 이전 요청의 상태를 서버가 기억하지 않는다는 의미. 모든 필요한 정보는 각 요청과 함께 전송되어야 한다.\n캐시 가능: REST API 응답은 캐시될 수 있으므로, 성능을 향상시키고 서버의 부하를 줄일 수 있다.\n유니폼 인터페이스(Uniform Interface): REST API는 표준화된 방법을 사용하여 서버의 리소스에 접근. 이는 HTTP 메서드를 활용하는데, 예를 들어 GET(읽기), POST(생성), PUT(수정), DELETE(삭제) 등이 있다.\n리소스 기반: REST API에서 ’리소스’는 웹에서의 객체, 데이터 또는 서비스를 의미하며, 각 리소스는 고유한 URI(Uniform Resource Identifier)를 통해 식별됨.\n\n\n\n\n\nSimpleHttpOperator를 이용해서 RestAPI를 호출\nhttp://localhost:8080/provider/apache-airflow-providers-http 에서 오퍼레이터 명세 확인하기\npython API click\n\nhttp Operator click 의 자주 쓰는 parameters\n\nhttp_conn_id (str) – The http connection to run the operator against: full url의 ‘xxxx.com/(나머지)’ 의 xxxx.com 을 넣어줌\nendpoint (str | None) – The relative part of the full url. (templated): full url의 ‘xxxx.com/(나머지)’ 의 (나머지)를 넣어줌\nmethod (str) – The HTTP method to use, default = “POST”: http의 4개 methods\n\nGET: data 가져오기\nPOST: data insert\nPUT: data 변경/update\nDELETE: data 삭제하기\n\ndata (Any) – The data to pass. POST-data in POST/PUT and params in the URL for a GET request. (templated)\n\nPOST의 경우: insert할 data 값\nGET의 경우: HTTP Protocol을 GET으로 줬으면 GET요청의 parameter를 dictionary 형태로 입력해주면 됨\n\nheaders (dict[str, str] | None) – The HTTP headers to be added to the GET request\nresponse_check (Callable[…, bool] | None) – A check against the ‘requests’ response object. The callable takes the response object as the first positional argument and optionally any number of keyword arguments available in the context dictionary. It should return True for ‘pass’ and False otherwise.\n\ndata 요청시 응답이 제대로 왔는지 확인\ndata를 일회성으로 가져올 때 데이터 1000 행이 제대로 들어왔는지 간단한 조회로 알아볼 수 있지만\ndata를 open API를 이용하여 주기적으로 내려받는 자동화의 경우 일일히 확인하는게 아니라 데이터가 잘 내려 받았는지 확인하는 함수를 하나 만들어 이 parameter에 할당하면 됨.\ntrue로 resturn하면 API가 정상적으로 실행된 것으로 간주\n\nresponse_filter (Callable[…, Any] | None) – A function allowing you to manipulate the response text. e.g response_filter=lambda response: json.loads(response.text). The callable takes the response object as the first positional argument and optionally any number of keyword arguments available in the context dictionary.\n\nAPI의 요청 결과가 dictionary 형태의 string으로 출력됨. 나중에 dictionary type으로 바꿔주고 key: value 형태로 access하여 원하는 데이터를 가져 와야 한다.\n이런 전처리 과정을 수행하는 함수를 만들어 reponse_filter parameter에 넣어줘야함.\n\n\n\n\n\n\n\n\n\n\nhttp_conn_id에 들어갈 connection id 만들기\n\nairflow web service &gt;&gt; Admin &gt;&gt; Plus button\nConn_id: 다른 conn 이름과 중복되지 않게 string으로 작성\n\nConnection_type: HTTP\nHost: openapi.seoul.go.kr (Open Data 명세를 보고 적음. http://openapi.seoul.go.kr:8088/(key:인증키)/(type)/(servicec)/(type)/(start_index)/(end_index)/(main_key 혹은 날짜) 중에서 openapi.seoul.go.kr 만 적으면 됨 )\ntoken값에 의해 인증되는 방식이기 때문에 schema/login/password 필요없음\nPort: 8088\nTest 버튼 클릭시 “405:Method Not Allowed” 가 뜨지만 무방함\n\n\n\n\n\nfrom airflow.providers.http.operators.http import SimpleHttpOperator\nwith DAG(...) as dag:\n  tb_cycle_station_info = SimpleHttpOperator(\n    task_id ='tb_cycle_station_info',\n    http_conn_id = 'openapi.seoul.go.kr',\n    endpoint ='{{var.value.apikey_openpai_seoul_go_kr}}/json/ListExhibitionOfSeoulMOAInfo/1/1000/',\n    method ='GET',\n    headers ={'Content-Type: 'application/json',\n              charset': 'utf-8',\n              'Accept': '*/*'\n              }\n  )\n\n\n\nSimpleHttpOperator를 1000개의 DAGs 에 작성했는데 API 키가 바뀐다면 1000개의 스크립트를 다 바꿔줘야하나?\nDAG에다가 바로 인증키를 복붙하면 다른 사람들도 API키를 볼 수 있어 보안상의 문제가 될 수 있다.\n위의 2가지 문제를 해결하기 위해 global variable을 이용하여 적을 것.\n\nAPI key를 variable을 이용하여 등록: airflow web service &gt;&gt; admin &gt;&gt; Variables\n\nkey:value 형태로 등록 가능\n관리자가 DB에 들어가면 API Key값 볼 수 있음\n\n\nKey에 아래 이름이 있을 경우 val 을 자동 마스킹처리하여 보여줌\n\n‘access_token’,\n‘api_key’,\n‘apikey’,\n‘authorization’,\n‘passphrase’,\n‘passwd’,\n‘password’,\n‘private_key’,\n‘secret’,\n‘token’\n\nglobal variable 설정하면\n\n서울시 공공데이터 추출하는 DAG 이 여러 개 있어도 API 키를 하나로 관리 가능\nAPI 키를 코드에 노출시키지 않음\n\nDAG Full Example\n\n# Package Import\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.http.operators.http import SimpleHttpOperator\nfrom airflow.decorators import task\nimport pendulum\n\nwith DAG(\n    dag_id='dags_simple_http_operator',\n    start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule=None\n) as dag:\n\n    '''서울시립 전시 정보'''\n    seoul_exhibition_info = SimpleHttpOperator(\n        task_id='seoul_exhibition_info',\n        http_conn_id='openapi.seoul.go.kr',\n        endpoint='{{var.value.apikey_openapi_seoul_go_kr}}/json/ListExhibitionOfSeoulMOAInfo/1/1000/',\n        method='GET',\n        headers={'Content-Type': 'application/json',\n                        'charset': 'utf-8',\n                        'Accept': '*/*'\n                        }\n    )\n\n    # 이 task는 seoul_exhibition_info가 xcom 에 넣었던 data를 빼오는 것\n    @task(task_id='python_2')\n    def python_2(**kwargs):\n        ti = kwargs['ti']\n        rslt = ti.xcom_pull(task_ids='seoul_exhibition_info')\n        import json\n        from pprint import pprint\n\n        pprint(json.loads(rslt))\n        \n    seoul_exhibition_info &gt;&gt; python_2()\n\n\n\n\n\n\nsimpleHttpOperator는 기능상 불편한 점이 있을 수 있지만 .custom operator 개발을 하면 나에게 맞춤화된 operator를 만들 수 있다.\n\n\n\nAirflow는 오퍼레이터를 직접 만들어 사용할 수 있도록 클래스를 제공 (BaseOperator)\n\nBaseOperator를 상속 받아 나만의 custom operator를 만들 수 있다.\n\n확장성을 비약적으로 높여주는 기능으로 airflow가 인기가 많은 이유가 됨\nBaseOperator 상속한 자식 operator가 custom operator가 됨\nBaseOperator 상속시 두 가지 메서드를 재정의해야 함 (Overriding)\n\nOverriding: 객체 지향 언어에서 부모 클래스가 가지고있던 method를 자식 class에서 재정의\n\n\n생성자 (def __init__) 재정의\n\n\n클래스에서 객체 생성시 객체에 대한 초기값 지정하는 함수\n\n\ndef execute(self, context) 재정의\n\n\n자식 클래스에서 똑같은 이름으로 써야함: def execute(self, context) 자체를 이용해야함 변경하면 안됨\ninit 생성자로 객체를 얻은 후 execute 메서드를 실행시키도록 되어 있음\n비즈니스 로직은 execute 에 구현해야함\n\n예를 들어, 다음과 같은 custom operator만들고 싶을 땐 mardkown     custom_task=CustomOperator(       task_id='xxxxx',       A='aaaa',       B='bbbb'     )     custom_task &gt;&gt; python_task\n\n생성자 (def __init__)에 parameter A와 B에 대한 내용이 들어가 있어야함\ncustom_task &gt;&gt; python_task 실행이 되면 execute() 함수가 내부적으로 실행되는 구조.\n\nCustom Operator를 만들 때, 오퍼레이터의 기능 정의를 명확하게 해줘야함\n\n기존에 있던 operator들로 job을 수행하기에 제한적이었던 점을 보완할 기능을 정의해야함\nsimpleHttpOperator에서 불편했던 점 2가지\n\n첫 번째, 매번 endpoint에 시작행/끝행을 넣어서 호출 해줘야 했음 \\(\\rightarrow\\) custom operator에서는 이것을 1000행씩 불러오도록 하는 기능이 필요\n두 번째, xcom에서 data를 가지고 온후 data에 접근할 수 있는 형태로 전처리를 해줘야하는 부분이 있었음 \\(\\rightarrow\\) custom operator에서는 전처리없이 local에다가 바로 저장할 수 있도록 하는 기능이 필요\n\n\nCustom Operator 기능 정의\n\n서울시 공공데이터 API 를 호출하여 전체 데이터를 받은 후 .csv 파일로 저장하기\ndag과 operator의 위치\n\ndag 위치: /dags/dags_seoul_api.py 생성\nfrom operators.seoul_api_to_csv_operator # airflow container가 plugins까지는 인식하기 때문에 그 하위 directory인 operators부터 path를 적어주면 됨.\nimport SeoulApiToCsvOperator #클래스명\n\noperator의 위치: /plugins/operators/seoul_api_to_csv_operator.py 생성\n\nTemplate 문법을 사용할 수 있는 Template을 지정\n\nop_kwards, op_args, param 과 같은 사용 가능한 파라미터 지정하기\n\nCustom Operator 작성 예시\n\n아래의 HelloOperator는 __init__과 execute(self, context) 둘다 재정의 해줬기 때문에 코드상으론 심플하지만 이미 custom operator로서의 기능을 할 수 있다.\n\nclass HelloOperator(BaseOperator):\n  template_fields: Sequence[str] = (\"name\",) \n  # 어떤 parameter에 template 문법을 적용할지 지정해주면 됨\n  # 생성자 함수 __init__ 의 member들로 template 문법을 적용할 paratemer 지정\n  # 현재 name만 template 문법 적용\n\n  def __init__(self, name: str, world: str, **kwargs) -&gt; None:\n    super().__init__(**kwargs)\n    self.name = name\n    self.world = world\n\n  def execute(self, context):\n    message = f\"Hello {self.world} it's {self.name}!\"\n    print(message)\n    return message\n\nwith dag:\n  hello_task = HelloOperator(\n    task_id='task_id_1', # task_id는 생성자에서 명시 않았지만 기본적으로 생성자에 들어가도록 설계되어 있음\n    name '{{ task_instance.task_id }}', # 이 name에 template 문법을 적용하면 '{{ task_instance.task_id }}'의 값이 생성자 함수의 name 인자에 들어가게 된다.\n    # HelloOperator의 객체가 생성되게 될때 self.name은 실제값으로 치환됨\n    world='Earth' #world는  Sequence[str] = (\"name\",) 에 명시되어 있지 않기 때문에 치환안됨\n  )\nGoogle ‘Airflow Custom Operator’ : creating a custom operator\n\nfrom airflow.models.baseoperator import BaseOperator\n\n\nclass HelloOperator(BaseOperator):\n    def __init__(self, name: str, **kwargs) -&gt; None:\n        super().__init__(**kwargs) # 부모함수(BaseOperator)의 생성자를 호출\n        self.name = name\n\n    def execute(self, context):\n        message = f\"Hello {self.name}\"\n        print(message)\n        return message\n\n이렇게 아래와 같이 HelloOperator object가 생성이 되면 name의 ’xxxx’값이 def __init__(self, name: str, **kwargs)의 name: str에 할당된다.\n\nhello_task=HelloOperator(\n  task_id='xxx', \n  name='xxxx'\n)\n\n위의 task_id는 def __init__(self, name: str, **kwargs) 의 **kwargs에 할당됨. 이어서 그 값이 super().__init__(**kwargs) 에도 할당되어 부모 함수까지 전달됨\n이런 메카니즘으로, 생성자에 task_id를 명시해서 적어줄 필요 없음\n그리고, 가지고 온 name값을 self.name에 할당\n\n\n\n\nAirflow Custom Operator Example\n\n\n\n\n\n\n\n\n\ninit 생성자 재정의\n\nclass SeoulApiToCsvOperator(BaseOperator):\n  template_fields = (' endpoint', ' path','file_ name','base_dt')\n  \n  def __init__(self , dataset_nm , path , file_name , base_dt=None , **kwargs):\n    # 생성자의 인자s: dataset_nm , path , file_name , base_dt를 user로부터 받겠다는 것을 명시\n    super().__init__(**kwargs)\n    self.http_conn_id = 'openapi.seoul.go.kr' #hard coding: 무조건 이 값을 사용\n    self.path = path\n    self.file_name = file_name\n    self.endpoint = '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + datset_nm # template 문법 적용하여 variable 값을 호출\n    self.base_dt =base_dt\n    # template 문법이 적용될 수 있도록 self.path 을 path로, self.file_name을 file_name로, self.endpoint 을 '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + datset_nm로, self.base_dt을 base_dt로 지정\n\n위의 생성자에 들어갈 parameters 의 갯수는 operator 객체를 만들때 입력받아야 하는 인자들을 의미한다.\n예를 들어, 아래와 같이 클래스 object를 생성해서 아래의 4가지 인자만 입력받아도 task는 생성이 된다.\n\ntask_id는 __init__()의 **kwars에 할당\ndataset_nm은 __init__()의 dataset_nm에 할당\npath는 __init__()의 path 에 할당\nfile_name은 __init__()의 file_name에 할당\nbase_dt는 none으로 되어있기 때문에 값을 입력안해줘도 되고 값을 입력해주면 __init__()의 base_dt에 할당됨\n\nseoul_task = SeoulApiToCsvOperator(\n  task_id='xxxx',\n  dataset_nm='xxxx',\n  path='xxxx',\n  file_name='xxxx'\n)\nClass full example\n\n위치: airflow/plugins/operators/seoul_api_to_csv_operator.py\n\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.hooks.base import BaseHook\nimport pandas as pd \n\nclass SeoulApiToCsvOperator(BaseOperator):\n    template_fields = ('endpoint', 'path','file_name','base_dt')\n\n    def __init__(self, dataset_nm, path, file_name, base_dt=None, **kwargs):\n        super().__init__(**kwargs)\n        self.http_conn_id = 'openapi.seoul.go.kr'\n        self.path = path\n        self.file_name = file_name\n        self.endpoint = '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + dataset_nm\n        self.base_dt = base_dt\n\n    def execute(self, context):\n    '''\n    url:8080/endpoint\n    endpoint=apikey/type/dataset_nm/start/end\n    즉, url:8080/apikey/type/dataset_nm/start/end 로 줬어야 했다.\n    execute logic의 concept\n    - 모든 데이터를 다 가져와야 함\n    - seoul open api양식에 맞게 데이터의 시작행과 끝행 입력하는 번거러움을 없앰    \n    '''\n        import os\n\n        connection = BaseHook.get_connection(self.http_conn_id) #airflow webservice ui 화면에서 만들었던 connection 정보를 get_connection()로 가져올 수 있음\n        self.base_url = f'http://{connection.host}:{connection.port}/{self.endpoint}' #connection.host & connection.port: user가 입력했던 host와 port정보 호출\n\n        # 데이터의 start행과 end행 처리는 아래의 while loop으로 처리\n\n        total_row_df = pd.DataFrame()\n        start_row = 1\n        end_row = 1000\n        while True:\n            self.log.info(f'시작:{start_row}')\n            self.log.info(f'끝:{end_row}')\n            row_df = self._call_api(self.base_url, start_row, end_row)\n            total_row_df = pd.concat([total_row_df, row_df])\n            if len(row_df) &lt; 1000:\n                break\n            else:\n                start_row = end_row + 1 #1, 1001, 2001, ...\n                end_row += 1000 #1000, 2000, 3000, ...\n\n        # len(row_df)가 1000 미만이면 데이터 다 받은 것 아래의 조건문을 탐\n        if not os.path.exists(self.path): #directory 유/무검사\n            os.system(f'mkdir -p {self.path}')\n        total_row_df.to_csv(self.path + '/' + self.file_name, encoding='utf-8', index=False)\n\n    def _call_api(self, base_url, start_row, end_row):\n        import requests #http의 get 요청을 하는 library\n        import json \n\n        headers = {'Content-Type': 'application/json',\n                  'charset': 'utf-8',\n                  'Accept': '*/*'\n                  }\n\n        #요청할 url 주소 완성\n        request_url = f'{base_url}/{start_row}/{end_row}/' \n\n        if self.base_dt is not None:\n            request_url = f'{base_url}/{start_row}/{end_row}/{self.base_dt}'\n        response = requests.get(request_url, headers) # request library의 get함수 사용. response는 dictionary형태의 string으로 들어옴\n        contents = json.loads(response.text) # json.loads()이 string을 dictionary로 반환함. 즉 contents에는 dictionary type\n\n        key_nm = list(contents.keys())[0]\n        row_data = contents.get(key_nm).get('row')\n        row_df = pd.DataFrame(row_data)\n\n        return row_df\ndictionary data example: 1행\n\nkey: ‘ListExhibitionOfSeoulMOAInfo’\nvalue: {‘RESULT’: {‘CODE’: ‘INFO-000’,…}}\nkey_nm = list(contents.keys())[0]의 값은 'ListExhibitionOfSeoulMOAInfo'\nrow_data = contents.get(key_nm).get('row') 은 key_nm의 value 호출. 이것은 다시 dictionary type이기 때문에 다시 key값 row에 해당되는 값을 호출\n그 다음 데이터프래임으로 만듦\n\n{'ListExhibitionOfSeoulMOAInfo': {'RESULT': {'CODE': 'INFO-000',\n                                            'MESSAGE': '정상 처리되었습니다'},\n                                  'list_total_count': 738,\n                                  'row': [{'DP_ARTIST': '박미나, Sasa[44]',\n                                          'DP_ART_CNT': '180여 점',\n                                          'DP_ART_PART': '회화, 설치, 아카이브, 사운드, '\n                                                          '영상 등',\n                                          'DP_DATE': '2024-01-24',\n                                          'DP_END': '2024-03-31',\n                                          'DP_EVENT': '',\n                                          'DP_EX_NO': '1255383',\n                                          'DP_HOMEPAGE': 'https://semaaa.seoul.go.kr/front/main.do',\n                                          'DP_INFO': '&lt;p&gt;《이력서: 박미나와 '\n                                                      'Sasa[44]》는 사물과 정보를 '\n                                                      '조사-수집-분석하는 방법론을 발전시켜 온 '\n                                                      '박미나와 Sasa[44]의 2인전입니다. '\n                                                      '두 작가는 2002년 첫 협업 전시를 '\n                                                      '시작으로 생산과 소비, 원본과 복제의 전후 '\n                                                      '관계에 대한 문제의식을 공유했고, '\n                                                      '현재까지도 실험적 관계 설정을 통해 개인 '\n                                                      '작업과 공동 작업을 병행하고 있습니다. '\n                                                      '이번 전시는 박미나와 Sasa[44]가 '\n                                                      '지난 20여 년간 따로, 또 함께 선보인 '\n                                                      '전시와 그 기록들을 이력서의 형식을 빌려 '\n                                                      '하나의 전시로 '\n                                                      '재구성합니다.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;이력서는 '\n                                                      '한 사람이 거쳐 온 학업, 직업, 경험 등 '\n                                                      '개인의 활동을 기록하는 문서의 양식입니다. '\n                                                      '개인의 경험은 사회적 인식을 반영하는 '\n                                                      '항목에 맞춰 정보로 조직되고, 타인에게 '\n                                                      '나의 공적 서사를 전시하는 수단으로 '\n                                                      '쓰입니다. 이력서가 정보를 구조화하는 '\n                                                      '하나의 체계이듯, 박미나와 '\n                                                      'Sasa[44]는 자료 수집과 조사 연구를 '\n                                                      '기반으로 자신들의 작업 세계를 직조하는 '\n                                                      '체계적인 방법론을 설계해왔습니다. 박미나가 '\n                                                      '회화의 색채를 물감 유통 체계와 연결 짓고 '\n                                                      '회화의 동시대적 조건을 탐구한다면, '\n                                                      'Sasa[44]는 시대의 지표가 되는 각종 '\n                                                      '자료를 수집하고 피처링, 샘플링, 매시업 '\n                                                      '등 대중음악의 방법을 전유해 새로운 의미의 '\n                                                      '층위를 '\n                                                      '발생시킵니다.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;전시는 '\n                                                      '이력서의 양식에 따라 ‘전시 이력’과 '\n                                                      '‘참고문헌’으로 나뉩니다. ‘전시 '\n                                                      '이력’에서는 박미나와 Sasa[44]의 '\n                                                      '주요 전시를 가로지르며 초기작과 대표작, '\n                                                      '미발표작 140여 점을 살펴볼 수 '\n                                                      '있습니다. 각각의 작업들은 과거의 전시와 '\n                                                      '현재를 매개하는 장치이면서, 작업 간의 '\n                                                      '연계를 강조하는 분류와 배치에 의해 새로운 '\n                                                      '의미를 드러냅니다. ‘참고문헌’은 '\n                                                      '2001년부터 2022년까지 발행된 국내외 '\n                                                      '신문, 잡지 등의 연속간행물 중 박미나와 '\n                                                      'Sasa[44]가 언급된 1,259개의 '\n                                                      '기사를 수집하여 한 권의 책과 사운드 '\n                                                      '작업으로 재구성하였습니다. 이 전시는 '\n                                                      '박미나와 Sasa[44]의 작업 세계를 '\n                                                      '경유하여 수집과 아카이브, 기록의 의미를 '\n                                                      '탐구하고 새로운 방식으로 자료 수집과 '\n                                                      '연구의 과정을 포착해 보려는 '\n                                                      '시도입니다.&lt;/p&gt;',\n                                          'DP_LNK': 'https://sema.seoul.go.kr/kr/whatson/exhibition/detail?exNo=1255383',\n                                          'DP_MAIN_IMG': 'http://sema.seoul.go.kr/common/imageView?FILE_PATH=%2Fex%2FEXI01%2F2023%2F&FILE_NM=20231226080317_f421712f05eb4e75a9e63d0de2a61f8b_2b1155dcb3954a43b4cc090868e3f111',\n                                          'DP_NAME': '이력서: 박미나와 Sasa[44]',\n                                          'DP_PLACE': '서울시립 미술아카이브',\n                                          'DP_SEQ': '000738',\n                                          'DP_SPONSOR': '서울시립미술관',\n                                          'DP_START': '2023-12-21',\n                                          'DP_SUBNAME': '',\n                                          'DP_VIEWCHARGE': '',\n                                          'DP_VIEWPOINT': '',\n                                          'DP_VIEWTIME': ''}]}}\n\n\n\n\n\n\nCOVID19 data download\nseoul_api_to_csv_operator.py 에서 class 상속 받아 custom operator 생성\n\n\nfrom operators.seoul_api_to_csv_operator import SeoulApiToCsvOperator\nfrom airflow import DAG\nimport pendulum\n\nwith DAG(\n    dag_id='dags_seoul_api_corona',\n    schedule='0 7 * * *',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    catchup=False\n) as dag:\n    '''서울시 코로나19 확진자 발생동향'''\n    tb_corona19_count_status = SeoulApiToCsvOperator(\n        task_id='tb_corona19_count_status',\n        dataset_nm='TbCorona19CountStatus',\n        path='/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}', # worker container 위치, files directory를 worker container와 연결시켜줘야함\n        file_name='TbCorona19CountStatus.csv'\n    )\n    \n    '''서울시 코로나19 백신 예방접종 현황'''\n    tv_corona19_vaccine_stat_new = SeoulApiToCsvOperator(\n        task_id='tv_corona19_vaccine_stat_new',\n        dataset_nm='tvCorona19VaccinestatNew',\n        path='/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}',\n        file_name='tvCorona19VaccinestatNew.csv'\n    )\n\n    tb_corona19_count_status &gt;&gt; tv_corona19_vaccine_stat_new\n\ntb_corona19_count_status = SeoulApiToCsvOperator() 의 수행은 wokrer_container가 주체\nSeoulApiToCsvOperator()의 path=‘/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(“Asia/Seoul”) | ds_nodash }}’.\n\n여기서 worker container는 ‘/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(“Asia/Seoul”) | ds_nodash }}’ 연결이 안되어 있기 때문에\ncontainer가 내려갔다가 다시 올라오면 files의 내용은 다 사라짐\ndocker_compose.yaml에서 경로 지정을 해줘야 자동으로 인식하여 wsl/files에 csv가 자동으로 저장된다.\n\n  volumes:\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow/plugins:/opt/airflow/plugin\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow/files:/opt/airflow/files\n\n실행 날짜로 저장된 directory명안에 csv를 vi editor로 열고 se nu 명령어로 건수를 확인\n\n\n\n\n\n[2024-01-25, 10:57:24 UTC] {taskinstance.py:1824} ERROR - Task failed with exception\nTraceback (most recent call last):\n  File \"/opt/airflow/plugins/operators/seoul_api_to_csv_operator.py\", line 38, in execute\n    total_row_df.to_csv(self.path + '/' + self.file_name, encoding='utf-8', index=False)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py\", line 3482, in to_csv\n    storage_options=storage_options,\n  File \"/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/format.py\", line 1105, in to_csv\n    csv_formatter.save()\n  File \"/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/csvs.py\", line 243, in save\n    storage_options=self.storage_options,\n  File \"/home/airflow/.local/lib/python3.7/site-packages/pandas/io/common.py\", line 707, in get_handle\n    newline=\"\",\nFileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/files/TbCorona19CountStatus/20240125/TbCorona19CountStatus.csv'\n[2024-01-25, 10:57:24 UTC] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=dags_seoul_api_corona, task_id=tb_corona19_count_status, execution_date=20240125T105722, start_date=20240125T105724, end_date=20240125T105724\n[2024-01-25, 10:57:24 UTC] {standard_task_runner.py:109} ERROR - Failed to execute job 493 for task tb_corona19_count_status ([Errno 2] No such file or directory: '/opt/airflow/files/TbCorona19CountStatus/20240125/TbCorona19CountStatus.csv'; 33640)\n[2024-01-25, 10:57:24 UTC] {local_task_job_runner.py:225} INFO - Task exited with return code 1\n[2024-01-25, 10:57:24 UTC] {taskinstance.py:2651} INFO - 0 downstream tasks scheduled from follow-on schedule check\n\nFileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/files/TbCorona19CountStatus/20240125/TbCorona19CountStatus.csv' 발생시\ncsv를 저장하려는데 저장할 디렉토리가 없어서 발생하는 에러: seoul_api_to_csv_operator 파일의 36번째 라인에서 디렉토리를 미리 생성할 수 있도록 하고 있는데, 저 명령이 제대로 실행이 안됐을 가능성이 있음.\n저 명령문이 진짜 제대로 실행되지 않았는지 확인해보기 위해 dag 수행 후 실패가 발생했을때 worker 컨테이너 안에 들어가서 /opt/airflow/files/TbCorona19CountStatus/{연월일} 디렉토리가 잘 생성됐는지 확인해야함.\n저 디렉토리가 없다면 36번째 라인이 제대로 실행되지 않은 것\n워커 컨테이너로 들어가는 것은 wsl 터미널에서 sudo docker exec -it {worker컨테이어id} bash 치면 들어갈 수 있음\n실행 결과 TbCorona19CountStatus directory 없음 \n\ndefault@436d1afa88b7:/opt/airflow$ ls -al\ntotal 104\ndrwxrwxr-x  1 airflow root  4096 Jan 24 23:07 .\ndrwxr-xr-x  1 root    root  4096 May 16  2023 ..\n-rw-r--r--  1 default root     2 Jan 24 23:07 airflow-worker.pid\n-rw-------  1 default root 58175 Jan 24 23:07 airflow.cfg\ndrwxr-xr-x  2 root    root  4096 Jan 18 05:52 config\ndrwxr-xr-x  3 default root  4096 Jan 25 10:54 dags\ndrwxr-xr-x  0 root    root     0 Jan 18 05:52 files\ndrwxr-xr-x 27 default root  4096 Jan 25 10:57 logs\ndrwxr-xr-x  7 default root  4096 Jan 18 05:49 plugins\n-rw-rw-r--  1 default root  4771 Jan 24 23:07 webserver_config.py\n\ndrwxr-xr-x  0 root    root     0 Jan 18 05:52 files files에 root 권한이 있다. /opt/airflow 아래의 files 디렉토리의 owner가 root 권한으로 만들어져있다. dags나 plugins 같은 다른 디렉토리의 owner는 default 인데 files 만 root로 만들어진 이유는 아마도 WSL터미널에서 files 디렉토리를 생성할 때 sudo mkdir files 명령으로 만들었을 것. sudo 를 붙이면 root 권한을 빌려 만들면 files 디렉토리는 root owner 에 root group으로 생성된다. 따라서 도커 컨테이너도 files 디렉토리를 root owner로 연결된다. /opt/airflow/files 디렉토리가 root owner 이니 default uid(1000) 로는 권한이 없어서 디렉토리 생성이 불가능\nWSL 터미널에서 files 디렉토리를 지우고 sudo 없이 그냥 mkdir files 디렉토리를 만들면됨.\n만약 files directory가 사용중이서 안지워진다면 컨테이너 서비스를 다시 내렸다가 올려 볼것. sudo docker compose down으로 껐다가 sudo docker compose up으로 다시 킴\n안 그러면 어떤 프로그램이 지우고자 하는 directory를 사용하고 있는지 직접 찾아서 kill을 해야함\n\n\n\n\nCustom 오퍼레이터를 만들면 왜 좋을까 ?\n\n원하는대로 로직을 만들 수 있다.\n비효율성 제거\n\n만약 custom 오퍼레이터를 만들지 않았다면\n개발자마다 각자 서울 공공데이터 데이터셋 추출 저장하는 파이썬 파일을 만들어 PythonOperator 를 이용해 개발했을 것\n비슷한 동작을 하는 파이썬 파일이 관리되지 않은 채 수십 개 만들어지면 그 자체로 비효율 발생\n운영하는 사람 입장에서 비슷한 script가 여러 개 있으면 이해할 수가 없음\n\n재사용성 강화\n\n특정기능을 하는 모듈을 만들어 놓고 , 상세 조건은 파라미터로 받게끔하여 모듈을 재사용할 수 있도록 유도\nCustom 오퍼레이터 개발\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#provider-operator",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#provider-operator",
    "title": "More Operators",
    "section": "",
    "text": "airflow web service &gt;&gt; Admin &gt;&gt; Providers 에서 확인할 수 있음\nsolution 제공 업체에서 본인들의 solution을 잘 다루게 하기 위해 만든 airflow에 제공한 operator\n솔루션 제공 업체: AWS, GCP, MS Azure 등\n솔루션이란?\n\n제품 또는 서비스 패키지: 여기서 “solution”은 특정 사용 사례에 맞춰진, 사전 구성된 클라우드 서비스의 세트를 말한다. 이는 고객이 보다 쉽고 빠르게 해당 기술을 도입하고 활용할 수 있도록 도와주는 ‘패키지’ 형태의 제품이나 서비스를 의미한다.\n기술적 해결책 또는 구현: 이 경우, “solution”은 특정 비즈니스 요구사항이나 기술적 문제를 해결하기 위한 방법론, 소프트웨어, 서비스, 아키텍처의 조합을 의미한다. 예를 들어, 데이터 분석, 웹 호스팅, 머신 러닝 모델 훈련과 같은 특정 목적을 위한 AWS나 GCP의 기능 및 서비스의 집합을 “solution”이라고 할 수 있다.\n첫 번째 뜻으로 이해해도 많은 context 커버 가능\n\n예를 들어, airflow web service &gt;&gt; Admin &gt;&gt; Providers &gt;&gt; apache-airflow-providers-amazon &gt;&gt; Python API &gt;&gt; [airflow.providers.amazon.aws.hooks, airflow.providers.amazon.aws.operators]\n참고로 airflow는 GCP와 궁합이 잘맞음\n\ntransfer: data 이동\n\napache-airflow-providers-http\n\nSimpleHttpOperator를 이용하여 API 값을 얻어 올 수 있음."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#dag간-의존관계-설정",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#dag간-의존관계-설정",
    "title": "More Operators",
    "section": "",
    "text": "의존 관계 : dag간 선 후행 관계\nDAG 의존관계 설정 방법\n\nTriggerDagRun Operator\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask2\n\ntask2\n\n\n\ntask1-&gt;task2\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask1-&gt;task3\n\n\n\n\n\ntask4\n\ntask4\n\n\n\ntask1-&gt;task4\n\n\n\n\n\n\n\n\n\n\n\ntask를 만들 때 task내에 dag_id를 명시하는 parameter가 있음\ntask1: PythonOperator 등\ntask2,3,4: TriggerDagRun 오퍼레이터로 다른 DAG 을 실행시키는 오퍼레이터\n\n\nExternalTask Sensor\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nsensor1\n\nsensor1\n\n\n\ntask1\n\ntask1\n\n\n\nsensor1-&gt;task1\n\n\n\n\n\nsensor2\n\nsensor2\n\n\n\nsensor2-&gt;task1\n\n\n\n\n\nsensor3\n\nsensor3\n\n\n\nsensor3-&gt;task1\n\n\n\n\n\nsensor4\n\nsensor4\n\n\n\nsensor4-&gt;task1\n\n\n\n\n\n\n\n\n\n\n\nsensor를 통해 task를 만들기 때문에 여기서 sensor는 task를 의미함\nsensor를 만들 때도 감지해야할 dag_id를 명시해줘야함 (task_id도 명시 가능)\nExternalTask Sensor는 다른 여러 DAGs의 Tasks의 완료를 기다리는 센서\nDAG간 의존관계 설정\n\n방식\n\nTriggerDagRun Operator: 실행할 다른 DAG 의 ID 를 지정하여 수행\n\\(A\\quad Dag \\subset B\\quad Dag\\) 일 때, B Dag이 A Dag을 trigger한다.\n\nExternalTask 센서: 본 Task 가 수행되기 전 다른 DAG 의 완료를 기다린 후 수행\n\n권고 사용 시점\n\nTriggerDagRun Operator: Trigger 되는 DAG 의 선행 DAG 이 하나만 있을 경우\nExternalTask 센서: Trigger 되는 DAG 의 선행 DAG 이 2 개 이상인 경우"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#triggerdagrun-오퍼레이터",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#triggerdagrun-오퍼레이터",
    "title": "More Operators",
    "section": "",
    "text": "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\nwith DAG(...) as dag:\n\n    start_task = BashOperator(\n        task_id='start_task',\n        bash_command='echo \"start!\"',\n    )\n\n    trigger_dag_task = TriggerDagRunOperator(\n        task_id='trigger_dag_task', #필수값\n        trigger_dag_id='dags_python_operator', #필수값\n        trigger_run_id=None, # 중요: run_id 값 직접 지정\n        execution_date='{{data_interval_start}}',\n        reset_dag_run=True,\n        wait_for_completion=False,\n        poke_interval=60,\n        allowed_states=['success'],\n        failed_states=None\n        )\n\n    start_task &gt;&gt; trigger_dag_task"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#triggerdagrun-오퍼레이터-의-run_id",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#triggerdagrun-오퍼레이터-의-run_id",
    "title": "More Operators",
    "section": "",
    "text": "run_id: DAG의 수행 방식과 시간을 유일하게 식별해주는 키\n같은 시간이라 해도 수행 방식 (Schedule, manual, Backfill) 에 따라 키가 달라짐\n\n스케쥴: 스케줄에 의해 실행된 경우 scheduled__{{data_interval_start}} 값을 가짐\nmanual: airflow ui web에서 수작업 수행. manual__{{data_interval_start}} 값을 가짐\n\nmanual__{{data_interval_start}}은 수작업 수행 시간이 아니라 수작업으로 실행시킨 스케쥴의 구간값 중 data_interval_start값을 의미\n\nBackfill: 과거 날짜를 이용해 수행. backfill__{{data_interval_start}} 값을 가짐\n\nrun_id를 보는 방법\n\nairflow ui web service &gt;&gt; dag &gt;&gt; grid &gt;&gt; 초록색 긴막대기 &gt;&gt; status Run ID 있음\n\n\n\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\nwith DAG(...) as dag:\n\n    start_task = BashOperator(\n        task_id='start_task',\n        bash_command='echo \"start!\"',\n    )\n\n    trigger_dag_task = TriggerDagRunOperator(\n        task_id='trigger_dag_task', \n        trigger_dag_id='dags_python_operator',\n        trigger_run_id=None, # rund_id 값 직접 지정\n        execution_date='{{data_interval_start}}', # manual_{{execution_date}} 로 수행 (여기에 값을 주면 메뉴얼 방식으로 trigger로 된걸로 간주)\n        reset_dag_run=True, # 이미 run_id 값으로 수행된 이력이 있는 경우에도 dag을 재수행할 것 인지 결정. True면 재수행\n        wait_for_completion=False,\n        poke_interval=60,\n        allowed_states=['success'],\n        failed_states=None\n        )\n\n    start_task &gt;&gt; trigger_dag_task\n\nwait_for_completion:\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask2_by_trigger_dag_run\n\ntask2_by_trigger_dag_run\n\n\n\ntask1-&gt;task2_by_trigger_dag_run\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask2_by_trigger_dag_run-&gt;task3\n\n\n\n\n\ndag_c\n\ndag_c\n\n\n\ntask2_by_trigger_dag_run-&gt;dag_c\n\n\n\n\n\n\n\n\n\n\n\nwait_for_completion=True 의 의미: task2가 dag_c를 돌리고 dag_c가 성공한 상태 후 task2역시 성공 상태가 된 후에 task3을 돌리는 경우\nwait_for_completion=False 의 의미: dag_c의 성공여부와 상관없이 task2가 성공 상태로 빠져나옴\npoke_interval=60 : dag_c의 성공여부를 모니터링 하는 주기 60초\nallowed_states=[‘success’]: trigger dag의 task2가 성공상태로 끝나기 위해 dag_c가 어떤 상태로 수행이 끝나야하는지 명시. 만약 dag_c가 fail상태여도 task2가 성공 상태로 마킹이 되길 원한다면 allowed_states=[‘success’,‘fail’] 로 명시\nfailed_states=None: task2가 실패 상태로 마킹이 되기 위해 dag_c가 어떤 상태로 작업 수행이 완료 되어야 하는지 명시. failed_states=fail이면 dag_c가 실패가 되어야 task2도 실패로 마킹 된다.\nDag Full example\n\n# Package Import\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\nimport pendulum\n\nwith DAG(\n    dag_id='dags_trigger_dag_run_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='30 9 * * *', #9시 30분 daily schedule\n    catchup=False\n) as dag:\n\n    start_task = BashOperator(\n        task_id='start_task',\n        bash_command='echo \"start!\"',\n    )\n\n    trigger_dag_task = TriggerDagRunOperator(\n        task_id='trigger_dag_task',\n        trigger_dag_id='dags_python_operator',\n        trigger_run_id=None,\n        execution_date='{{data_interval_start}}', #9시 30분 daily schedule\n        reset_dag_run=True,\n        wait_for_completion=False,\n        poke_interval=60,\n        allowed_states=['success'],\n        failed_states=None\n        )\n\n    start_task &gt;&gt; trigger_dag_task"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#서울시-공공데이터-보기",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#서울시-공공데이터-보기",
    "title": "More Operators",
    "section": "",
    "text": "서울시 열린데이터 광장 portal\n서울시가 보유한 데이터 다운로드 가능\n일회성 다운로드 : Csv, json, xml 등을 직접 다운로드\n스케쥴에 의한 주기성 다운로드 : openAPI(http method를 통해 data를 다운로드 할 수 있도록 개방해놓은 API) 통해 다운 가능\n데이터 검색\n\n먼저, 어떤 종류의 데이터를 다운로드 받을 수 있는지 확인하기 위해 서울 열린데이터 광장 검색창에 데이터 검색하거나 데이터 카테고리 선택하여 openAI Tag붙은 데이터를 선택해야 한다 (본인은 문화/관광 선택).\n\n\nopen API tag없으면 manual로 데이터 셋 다운로드 받아야함.\n\n\nSample URL: 서울시립미술관 전시 현황 http://openapi.seoul.go.kr:8088/(인증키)/xml/ListExhibitionOfSeoulMOAInfo/1/5/\nSample URL 작성은 요청 인자를 참고하여 적어 넣으면 된다. 예를 들어,\n\nhttp://openapi.seoul.go.kr:8088/(인증키)/xml/ListExhibitionOfSeoulMOAInfo/1/5/ 은 아래와 같은 요청 인자 양식에 의해 적혀져 있다.\nhttp://openapi.seoul.go.kr:8088/(API key)/(data type)/(service)/(data start_index)/(data end_index)/\n1 부터 1000행 까지는 한번에 가져올 수 있지만 1000행 넘어가면 에러 발생\n\n그래서, 1~1000행, 10001행~2000행, … 와 같이 끊어서 가져가야 함\n\n요청 인자 중: DP_SEQ, DP_NAME이 있는데 특정 값을 입력하면 filtering되어 조건에 만족하는 데이터만 query해서 가져올 수 있음.\n\n다른 Sample URL 예시: 서울시 자랑스러운 한국음식점 정보 (한국어)\n\nurl 양식: http://openapi.seoul.go.kr:8088/(key:인증키)/(type)/(servicec)/(type)/(start_index)/(end_index)/(main_key 혹은 날짜)\n샘플 URL: http://openapi.seoul.go.kr:8088/(인증키)/xml/SebcKoreanRestaurantsKor/1/5\n\n이렇게 Sample URL 양식은 http://openapi.seoul.go.kr:8088/(key:인증키)/(type)/(servicec)/(type)/(start_index)/(end_index) 까지는 공통됨.\n\nopenAPI 이용할 경우 api KEY 발급받아야 함 로그인 필요\n\n로그인 &gt;&gt; 이용 안내 &gt;&gt; open API 소개 &gt;&gt; API Key 신청 &gt;&gt; 애플리케이션 등록\nAPI Key 신청: 일반 인증키 신청 or 실시간 지하철 인증키 신청\n\nsheet는 최대 1,000건 (행) 까지 노출됨. 전체 데이터는 CSV파일을 내려받아 확인해야함.\n\n애플리케이션 등록\n\n서비스(사용) 환경: 웹사이트 개발\n사용 URL: localhost (or 이 데이터를 사용할 여러분들의 홈패이지 주소)\n관리용 대표 이메일: 홍길동@naver.com\n활용용도: 블로그 개발 (자유형식 - 적당히 내용 채워 넣음)\n내용: 문화/전시 관련 소식을 스케쥴을 이용해 전달 받음 (자유형식 - 적당히 내용 채워 넣음)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#simplehttpoperator-란",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#simplehttpoperator-란",
    "title": "More Operators",
    "section": "",
    "text": "HTTP 요청을 하고 결과로 text 를 리턴 받는 오퍼레이터 리턴 값은 Xcom 에 저장\nHTTP 를 이용하여 API 를 처리하는 RestAPI 호출시 사용 가능\n\nRestAPI: API 방법 중 하나로 http의 protocol을 이용해서 API data를 제공하거나, 다운로드, 변경할 수 있는 API를 제공하는 방식\n\n\n\n\n\n\n\nNote\n\n\n\n\nREST API는 Representational State Transfer의 약자로, 웹 기반의 서비스 간에 통신을 위한 일반적인 규칙(아키텍처 스타일)을 의미\nREST API는 인터넷에서 데이터를 교환하기 위한 표준 방법 중 하나로 널리 사용되는데 간단히 말해서, REST API는 웹 애플리케이션에서 다른 시스템과 정보를 주고받기 위한 방법이다.\nREST API의 주요 특징\n\n클라이언트-서버 구조: REST API는 클라이언트(예: 웹 브라우저)와 서버 간의 분리를 기반으로 함. 클라이언트는 사용자 인터페이스와 사용자 상호작용을 관리하고, 서버는 데이터 저장 및 백엔드 로직을 처리\n무상태(Stateless): 각 요청은 독립적. 즉, 이전 요청의 상태를 서버가 기억하지 않는다는 의미. 모든 필요한 정보는 각 요청과 함께 전송되어야 한다.\n캐시 가능: REST API 응답은 캐시될 수 있으므로, 성능을 향상시키고 서버의 부하를 줄일 수 있다.\n유니폼 인터페이스(Uniform Interface): REST API는 표준화된 방법을 사용하여 서버의 리소스에 접근. 이는 HTTP 메서드를 활용하는데, 예를 들어 GET(읽기), POST(생성), PUT(수정), DELETE(삭제) 등이 있다.\n리소스 기반: REST API에서 ’리소스’는 웹에서의 객체, 데이터 또는 서비스를 의미하며, 각 리소스는 고유한 URI(Uniform Resource Identifier)를 통해 식별됨.\n\n\n\n\n\nSimpleHttpOperator를 이용해서 RestAPI를 호출\nhttp://localhost:8080/provider/apache-airflow-providers-http 에서 오퍼레이터 명세 확인하기\npython API click\n\nhttp Operator click 의 자주 쓰는 parameters\n\nhttp_conn_id (str) – The http connection to run the operator against: full url의 ‘xxxx.com/(나머지)’ 의 xxxx.com 을 넣어줌\nendpoint (str | None) – The relative part of the full url. (templated): full url의 ‘xxxx.com/(나머지)’ 의 (나머지)를 넣어줌\nmethod (str) – The HTTP method to use, default = “POST”: http의 4개 methods\n\nGET: data 가져오기\nPOST: data insert\nPUT: data 변경/update\nDELETE: data 삭제하기\n\ndata (Any) – The data to pass. POST-data in POST/PUT and params in the URL for a GET request. (templated)\n\nPOST의 경우: insert할 data 값\nGET의 경우: HTTP Protocol을 GET으로 줬으면 GET요청의 parameter를 dictionary 형태로 입력해주면 됨\n\nheaders (dict[str, str] | None) – The HTTP headers to be added to the GET request\nresponse_check (Callable[…, bool] | None) – A check against the ‘requests’ response object. The callable takes the response object as the first positional argument and optionally any number of keyword arguments available in the context dictionary. It should return True for ‘pass’ and False otherwise.\n\ndata 요청시 응답이 제대로 왔는지 확인\ndata를 일회성으로 가져올 때 데이터 1000 행이 제대로 들어왔는지 간단한 조회로 알아볼 수 있지만\ndata를 open API를 이용하여 주기적으로 내려받는 자동화의 경우 일일히 확인하는게 아니라 데이터가 잘 내려 받았는지 확인하는 함수를 하나 만들어 이 parameter에 할당하면 됨.\ntrue로 resturn하면 API가 정상적으로 실행된 것으로 간주\n\nresponse_filter (Callable[…, Any] | None) – A function allowing you to manipulate the response text. e.g response_filter=lambda response: json.loads(response.text). The callable takes the response object as the first positional argument and optionally any number of keyword arguments available in the context dictionary.\n\nAPI의 요청 결과가 dictionary 형태의 string으로 출력됨. 나중에 dictionary type으로 바꿔주고 key: value 형태로 access하여 원하는 데이터를 가져 와야 한다.\n이런 전처리 과정을 수행하는 함수를 만들어 reponse_filter parameter에 넣어줘야함."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#커넥션-등록",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#커넥션-등록",
    "title": "More Operators",
    "section": "",
    "text": "http_conn_id에 들어갈 connection id 만들기\n\nairflow web service &gt;&gt; Admin &gt;&gt; Plus button\nConn_id: 다른 conn 이름과 중복되지 않게 string으로 작성\n\nConnection_type: HTTP\nHost: openapi.seoul.go.kr (Open Data 명세를 보고 적음. http://openapi.seoul.go.kr:8088/(key:인증키)/(type)/(servicec)/(type)/(start_index)/(end_index)/(main_key 혹은 날짜) 중에서 openapi.seoul.go.kr 만 적으면 됨 )\ntoken값에 의해 인증되는 방식이기 때문에 schema/login/password 필요없음\nPort: 8088\nTest 버튼 클릭시 “405:Method Not Allowed” 가 뜨지만 무방함"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#simplehttpoperator-작성",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#simplehttpoperator-작성",
    "title": "More Operators",
    "section": "",
    "text": "from airflow.providers.http.operators.http import SimpleHttpOperator\nwith DAG(...) as dag:\n  tb_cycle_station_info = SimpleHttpOperator(\n    task_id ='tb_cycle_station_info',\n    http_conn_id = 'openapi.seoul.go.kr',\n    endpoint ='{{var.value.apikey_openpai_seoul_go_kr}}/json/ListExhibitionOfSeoulMOAInfo/1/1000/',\n    method ='GET',\n    headers ={'Content-Type: 'application/json',\n              charset': 'utf-8',\n              'Accept': '*/*'\n              }\n  )\n\n\n\nSimpleHttpOperator를 1000개의 DAGs 에 작성했는데 API 키가 바뀐다면 1000개의 스크립트를 다 바꿔줘야하나?\nDAG에다가 바로 인증키를 복붙하면 다른 사람들도 API키를 볼 수 있어 보안상의 문제가 될 수 있다.\n위의 2가지 문제를 해결하기 위해 global variable을 이용하여 적을 것.\n\nAPI key를 variable을 이용하여 등록: airflow web service &gt;&gt; admin &gt;&gt; Variables\n\nkey:value 형태로 등록 가능\n관리자가 DB에 들어가면 API Key값 볼 수 있음\n\n\nKey에 아래 이름이 있을 경우 val 을 자동 마스킹처리하여 보여줌\n\n‘access_token’,\n‘api_key’,\n‘apikey’,\n‘authorization’,\n‘passphrase’,\n‘passwd’,\n‘password’,\n‘private_key’,\n‘secret’,\n‘token’\n\nglobal variable 설정하면\n\n서울시 공공데이터 추출하는 DAG 이 여러 개 있어도 API 키를 하나로 관리 가능\nAPI 키를 코드에 노출시키지 않음\n\nDAG Full Example\n\n# Package Import\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.http.operators.http import SimpleHttpOperator\nfrom airflow.decorators import task\nimport pendulum\n\nwith DAG(\n    dag_id='dags_simple_http_operator',\n    start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule=None\n) as dag:\n\n    '''서울시립 전시 정보'''\n    seoul_exhibition_info = SimpleHttpOperator(\n        task_id='seoul_exhibition_info',\n        http_conn_id='openapi.seoul.go.kr',\n        endpoint='{{var.value.apikey_openapi_seoul_go_kr}}/json/ListExhibitionOfSeoulMOAInfo/1/1000/',\n        method='GET',\n        headers={'Content-Type': 'application/json',\n                        'charset': 'utf-8',\n                        'Accept': '*/*'\n                        }\n    )\n\n    # 이 task는 seoul_exhibition_info가 xcom 에 넣었던 data를 빼오는 것\n    @task(task_id='python_2')\n    def python_2(**kwargs):\n        ti = kwargs['ti']\n        rslt = ti.xcom_pull(task_ids='seoul_exhibition_info')\n        import json\n        from pprint import pprint\n\n        pprint(json.loads(rslt))\n        \n    seoul_exhibition_info &gt;&gt; python_2()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#airflow-의-꽃-custom-오퍼레이터",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#airflow-의-꽃-custom-오퍼레이터",
    "title": "More Operators",
    "section": "",
    "text": "Airflow는 오퍼레이터를 직접 만들어 사용할 수 있도록 클래스를 제공 (BaseOperator)\n확장성을 비약적으로 높여주는 기능으로 airflow가 인기가 많은 이유가 됨\nBaseOperator 상속한 자식 operator가 custom operator가 됨\nBaseOperator 상속시 두 가지 메서드를 재정의해야 함 (Overriding)\n\nOverriding: 객체 지향 언어에서 부모 클래스가 가지고있던 method를 자식 class에서 재정의\n\n\n생성자 재정의: def__init__\n\n\n클래스에서 객체 생성시 객체에 대한 초기값 지정하는 함수\n\n\ndef execute(self, context) (자식 클래스에서 똑같은 이름으로 써야함)\n\n\ninit 생성자로 객체를 얻은 후 execute 메서드를 실행시키도록 되어 있음\n비즈니스 로직은 execute 에 구현 필요\n\n오퍼레이터 기능 정의\n\n기존에 있던 operator들로 job을 수행하기에 제한적이었던 점을 보완할 기능을 정의해야함\n\nsimpleHttpOperator에서 불편했던 점은 매번 endpoint에 시작행/끝행을 넣어서 호출 해줘야 했는데 이것을 1000행씩 불러오도록 하는 기능이 필요\nxcom에서 data를 가지고 온후 data에 접근할 수 있는 형태로 전처리를 해줘야하는 부분이 있었는데 전처리없이 local에다가 바로 저장할 수 있도록 하는 기능이 필요\n\n서울시 공공데이터 API 를 호출하여 전체 데이터를 받은 후 .csv 파일로 저장하기\n\n오퍼레이터와 dag 의 위치\n\n/dags/dags_seoul_api.py 생성\nfrom operators.seoul_api_to_csv_operator\nimport SeoulApiToCsvOperator\n/plugins/operators/seoul_api_to_csv_operator.py 생성\n\nTemplate 문법을 사용할 수 있는 Template을 지정 (사용 가능한 파라미터 지정하기)\n아래의 HelloOperator는 __init__과 execute(self, context) 둘다 재정의 해줬기 때문에 코드상으론 심플하지만 이미 custom operator로서의 기능을 할 수 있다.\n\nclass HelloOperator(BaseOperator):\n  template_fields: Sequence[str] = (\"name\",) # 이 line 어떤 parameter에 template 문법을 적용할지 지정해주면 됨\n  # 생성자 함수 __init__ 의 member들로 template 문법을 적용할 paratemer 지정\n  def __init__(self, name: str, world: str, **kwargs) -&gt; None:\n    super().__init__(**kwargs)\n    self.name = name\n    self.world =world\n  def execute(self, context):\n    message = f\"Hello {self.world} it's {self.name}!\"\n    print(message)\n    return message\nwith dag:\n  hello_task = HelloOperator(\n    task_id='task_id_1',\n    name '{{ task_instance.task_id }}',\n    world='Earth'\n  )\n\nGoogle Airflow Custom Operator\n\n\n\n\nAirflow Custom Operator Example"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#custom-오퍼레이터-만들기",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#custom-오퍼레이터-만들기",
    "title": "More Operators",
    "section": "",
    "text": "init 생성자 재정의\n\nclass SeoulApiToCsvOperator(BaseOperator):\n  template_fields = (' endpoint', ' path','file_ name','base_dt')\n  \n  def __init__(self , dataset_nm , path , file_name , base_dt=None , **kwargs):\n    # 생성자의 인자s: dataset_nm , path , file_name , base_dt를 user로부터 받겠다는 것을 명시\n    super().__init__(**kwargs)\n    self.http_conn_id = 'openapi.seoul.go.kr' #hard coding: 무조건 이 값을 사용\n    self.path = path\n    self.file_name = file_name\n    self.endpoint = '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + datset_nm # template 문법 적용하여 variable 값을 호출\n    self.base_dt =base_dt\n    # template 문법이 적용될 수 있도록 self.path 을 path로, self.file_name을 file_name로, self.endpoint 을 '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + datset_nm로, self.base_dt을 base_dt로 지정\n\n위의 생성자에 들어갈 parameters 의 갯수는 operator 객체를 만들때 입력받아야 하는 인자들을 의미한다.\n예를 들어, 아래와 같이 클래스 object를 생성해서 아래의 4가지 인자만 입력받아도 task는 생성이 된다.\n\ntask_id는 __init__()의 **kwars에 할당\ndataset_nm은 __init__()의 dataset_nm에 할당\npath는 __init__()의 path 에 할당\nfile_name은 __init__()의 file_name에 할당\nbase_dt는 none으로 되어있기 때문에 값을 입력안해줘도 되고 값을 입력해주면 __init__()의 base_dt에 할당됨\n\nseoul_task = SeoulApiToCsvOperator(\n  task_id='xxxx',\n  dataset_nm='xxxx',\n  path='xxxx',\n  file_name='xxxx'\n)\nClass full example\n\n위치: airflow/plugins/operators/seoul_api_to_csv_operator.py\n\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.hooks.base import BaseHook\nimport pandas as pd \n\nclass SeoulApiToCsvOperator(BaseOperator):\n    template_fields = ('endpoint', 'path','file_name','base_dt')\n\n    def __init__(self, dataset_nm, path, file_name, base_dt=None, **kwargs):\n        super().__init__(**kwargs)\n        self.http_conn_id = 'openapi.seoul.go.kr'\n        self.path = path\n        self.file_name = file_name\n        self.endpoint = '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + dataset_nm\n        self.base_dt = base_dt\n\n    def execute(self, context):\n    '''\n    url:8080/endpoint\n    endpoint=apikey/type/dataset_nm/start/end\n    즉, url:8080/apikey/type/dataset_nm/start/end 로 줬어야 했다.\n    execute logic의 concept\n    - 모든 데이터를 다 가져와야 함\n    - seoul open api양식에 맞게 데이터의 시작행과 끝행 입력하는 번거러움을 없앰    \n    '''\n        import os\n\n        connection = BaseHook.get_connection(self.http_conn_id) #airflow webservice ui 화면에서 만들었던 connection 정보를 get_connection()로 가져올 수 있음\n        self.base_url = f'http://{connection.host}:{connection.port}/{self.endpoint}' #connection.host & connection.port: user가 입력했던 host와 port정보 호출\n\n        # 데이터의 start행과 end행 처리는 아래의 while loop으로 처리\n\n        total_row_df = pd.DataFrame()\n        start_row = 1\n        end_row = 1000\n        while True:\n            self.log.info(f'시작:{start_row}')\n            self.log.info(f'끝:{end_row}')\n            row_df = self._call_api(self.base_url, start_row, end_row)\n            total_row_df = pd.concat([total_row_df, row_df])\n            if len(row_df) &lt; 1000:\n                break\n            else:\n                start_row = end_row + 1 #1, 1001, 2001, ...\n                end_row += 1000 #1000, 2000, 3000, ...\n\n        # len(row_df)가 1000 미만이면 데이터 다 받은 것 아래의 조건문을 탐\n        if not os.path.exists(self.path): #directory 유/무검사\n            os.system(f'mkdir -p {self.path}')\n        total_row_df.to_csv(self.path + '/' + self.file_name, encoding='utf-8', index=False)\n\n    def _call_api(self, base_url, start_row, end_row):\n        import requests #http의 get 요청을 하는 library\n        import json \n\n        headers = {'Content-Type': 'application/json',\n                  'charset': 'utf-8',\n                  'Accept': '*/*'\n                  }\n\n        #요청할 url 주소 완성\n        request_url = f'{base_url}/{start_row}/{end_row}/' \n\n        if self.base_dt is not None:\n            request_url = f'{base_url}/{start_row}/{end_row}/{self.base_dt}'\n        response = requests.get(request_url, headers) # request library의 get함수 사용. response는 dictionary형태의 string으로 들어옴\n        contents = json.loads(response.text) # json.loads()이 string을 dictionary로 반환함. 즉 contents에는 dictionary type\n\n        key_nm = list(contents.keys())[0]\n        row_data = contents.get(key_nm).get('row')\n        row_df = pd.DataFrame(row_data)\n\n        return row_df\ndictionary data example: 1행\n\nkey: ‘ListExhibitionOfSeoulMOAInfo’\nvalue: {‘RESULT’: {‘CODE’: ‘INFO-000’,…}}\nkey_nm = list(contents.keys())[0]의 값은 'ListExhibitionOfSeoulMOAInfo'\nrow_data = contents.get(key_nm).get('row') 은 key_nm의 value 호출. 이것은 다시 dictionary type이기 때문에 다시 key값 row에 해당되는 값을 호출\n그 다음 데이터프래임으로 만듦\n\n{'ListExhibitionOfSeoulMOAInfo': {'RESULT': {'CODE': 'INFO-000',\n                                            'MESSAGE': '정상 처리되었습니다'},\n                                  'list_total_count': 738,\n                                  'row': [{'DP_ARTIST': '박미나, Sasa[44]',\n                                          'DP_ART_CNT': '180여 점',\n                                          'DP_ART_PART': '회화, 설치, 아카이브, 사운드, '\n                                                          '영상 등',\n                                          'DP_DATE': '2024-01-24',\n                                          'DP_END': '2024-03-31',\n                                          'DP_EVENT': '',\n                                          'DP_EX_NO': '1255383',\n                                          'DP_HOMEPAGE': 'https://semaaa.seoul.go.kr/front/main.do',\n                                          'DP_INFO': '&lt;p&gt;《이력서: 박미나와 '\n                                                      'Sasa[44]》는 사물과 정보를 '\n                                                      '조사-수집-분석하는 방법론을 발전시켜 온 '\n                                                      '박미나와 Sasa[44]의 2인전입니다. '\n                                                      '두 작가는 2002년 첫 협업 전시를 '\n                                                      '시작으로 생산과 소비, 원본과 복제의 전후 '\n                                                      '관계에 대한 문제의식을 공유했고, '\n                                                      '현재까지도 실험적 관계 설정을 통해 개인 '\n                                                      '작업과 공동 작업을 병행하고 있습니다. '\n                                                      '이번 전시는 박미나와 Sasa[44]가 '\n                                                      '지난 20여 년간 따로, 또 함께 선보인 '\n                                                      '전시와 그 기록들을 이력서의 형식을 빌려 '\n                                                      '하나의 전시로 '\n                                                      '재구성합니다.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;이력서는 '\n                                                      '한 사람이 거쳐 온 학업, 직업, 경험 등 '\n                                                      '개인의 활동을 기록하는 문서의 양식입니다. '\n                                                      '개인의 경험은 사회적 인식을 반영하는 '\n                                                      '항목에 맞춰 정보로 조직되고, 타인에게 '\n                                                      '나의 공적 서사를 전시하는 수단으로 '\n                                                      '쓰입니다. 이력서가 정보를 구조화하는 '\n                                                      '하나의 체계이듯, 박미나와 '\n                                                      'Sasa[44]는 자료 수집과 조사 연구를 '\n                                                      '기반으로 자신들의 작업 세계를 직조하는 '\n                                                      '체계적인 방법론을 설계해왔습니다. 박미나가 '\n                                                      '회화의 색채를 물감 유통 체계와 연결 짓고 '\n                                                      '회화의 동시대적 조건을 탐구한다면, '\n                                                      'Sasa[44]는 시대의 지표가 되는 각종 '\n                                                      '자료를 수집하고 피처링, 샘플링, 매시업 '\n                                                      '등 대중음악의 방법을 전유해 새로운 의미의 '\n                                                      '층위를 '\n                                                      '발생시킵니다.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;전시는 '\n                                                      '이력서의 양식에 따라 ‘전시 이력’과 '\n                                                      '‘참고문헌’으로 나뉩니다. ‘전시 '\n                                                      '이력’에서는 박미나와 Sasa[44]의 '\n                                                      '주요 전시를 가로지르며 초기작과 대표작, '\n                                                      '미발표작 140여 점을 살펴볼 수 '\n                                                      '있습니다. 각각의 작업들은 과거의 전시와 '\n                                                      '현재를 매개하는 장치이면서, 작업 간의 '\n                                                      '연계를 강조하는 분류와 배치에 의해 새로운 '\n                                                      '의미를 드러냅니다. ‘참고문헌’은 '\n                                                      '2001년부터 2022년까지 발행된 국내외 '\n                                                      '신문, 잡지 등의 연속간행물 중 박미나와 '\n                                                      'Sasa[44]가 언급된 1,259개의 '\n                                                      '기사를 수집하여 한 권의 책과 사운드 '\n                                                      '작업으로 재구성하였습니다. 이 전시는 '\n                                                      '박미나와 Sasa[44]의 작업 세계를 '\n                                                      '경유하여 수집과 아카이브, 기록의 의미를 '\n                                                      '탐구하고 새로운 방식으로 자료 수집과 '\n                                                      '연구의 과정을 포착해 보려는 '\n                                                      '시도입니다.&lt;/p&gt;',\n                                          'DP_LNK': 'https://sema.seoul.go.kr/kr/whatson/exhibition/detail?exNo=1255383',\n                                          'DP_MAIN_IMG': 'http://sema.seoul.go.kr/common/imageView?FILE_PATH=%2Fex%2FEXI01%2F2023%2F&FILE_NM=20231226080317_f421712f05eb4e75a9e63d0de2a61f8b_2b1155dcb3954a43b4cc090868e3f111',\n                                          'DP_NAME': '이력서: 박미나와 Sasa[44]',\n                                          'DP_PLACE': '서울시립 미술아카이브',\n                                          'DP_SEQ': '000738',\n                                          'DP_SPONSOR': '서울시립미술관',\n                                          'DP_START': '2023-12-21',\n                                          'DP_SUBNAME': '',\n                                          'DP_VIEWCHARGE': '',\n                                          'DP_VIEWPOINT': '',\n                                          'DP_VIEWTIME': ''}]}}"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#summary",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#summary",
    "title": "More Operators",
    "section": "",
    "text": "Custom 오퍼레이터를 만들면 왜 좋을까 ?\n\n원하는대로 로직을 만들 수 있다.\n비효율성 제거\n\n만약 custom 오퍼레이터를 만들지 않았다면\n개발자마다 각자 서울 공공데이터 데이터셋 추출 저장하는 파이썬 파일을 만들어 PythonOperator 를 이용해 개발했을 것\n비슷한 동작을 하는 파이썬 파일이 관리되지 않은 채 수십 개 만들어지면 그 자체로 비효율 발생\n운영하는 사람 입장에서 비슷한 script가 여러 개 있으면 이해할 수가 없음\n\n재사용성 강화\n\n특정기능을 하는 모듈을 만들어 놓고 , 상세 조건은 파라미터로 받게끔하여 모듈을 재사용할 수 있도록 유도\nCustom 오퍼레이터 개발"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#public-data-api-key-obtaining-steps",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#public-data-api-key-obtaining-steps",
    "title": "More Operators",
    "section": "",
    "text": "“Public Data API Key”를 발급받는 과정은 대개 다음과 같은 단계를 포함한다. 이는 일반적인 절차로, 특정 공공 데이터 API 제공자에 따라 약간의 차이가 있을 수 있다.\n\n공공 데이터 포털 접속: 대부분의 국가에서는 공공 데이터를 제공하기 위한 중앙화된 포털을 운영한다.. 예를 들어, 한국에서는 ’공공데이터포털’이 있다.\n회원가입 및 로그인: 포털에 접속한 후, 사용자 계정을 생성하고 로그인한다.\nAPI 키 신청: 로그인 후, API 키를 신청할 수 있는 섹션을 찾는다. 이는 보통 ‘API 키 관리’, ‘내 어플리케이션’, ‘API 신청’ 등의 메뉴에서 찾을 수 있다.\n애플리케이션 등록: API 키를 신청하기 위해서는 대부분의 경우 애플리케이션을 등록해야 하는데 이 과정에서 애플리케이션 이름, 용도, URL 등의 정보를 입력해야 할 수도 있다.\nAPI 키 발급: 애플리케이션 등록이 완료되면, API 키가 발급된다. 이 키는 API를 호출할 때 필요하므로 안전하게 보관해야 한다.\nAPI 문서 확인: API를 효과적으로 사용하기 위해서는 해당 API의 문서를 확인하는 것이 중요한데 문서에서는 API의 엔드포인트, 요청 방식, 필요한 파라미터 등의 정보를 제공한다.\nAPI 호출 시험: API 키를 사용하여 간단한 API 호출하여 API가 정상적으로 작동하는지 확인한다.\n각 공공 데이터 포털의 구체적인 지침과 절차는 웹사이트를 참조해야 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#web",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#web",
    "title": "Content List, Engineering",
    "section": "9 Web",
    "text": "9 Web\n\n2023-05-01, HTTP Methods"
  },
  {
    "objectID": "docs/blog/posts/Engineering/web/http_method.html",
    "href": "docs/blog/posts/Engineering/web/http_method.html",
    "title": "HTTP Method",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n1 Basic HTTP Methods\nHTTP 메서드(HTTP Methods)는 클라이언트가 웹 서버에 어떤 동작을 원하는지 서버에 알리기 위해 사용되는 명령들이다. HTTP 프로토콜의 일부로 정의되어 있으며, 가장 일반적인 메서드들은 다음과 같다:\n\nGET: 서버로부터 정보를 조회하기 위해 사용. 데이터를 가져오는 데 사용되며, 서버의 상태를 변경하지 않는다.\nPOST: 서버에 데이터를 전송하기 위해 사용. 주로 데이터베이스에 새로운 데이터를 추가하거나, 폼을 제출할 때 사용.\nPUT: 서버에 있는 자원을 업데이트하기 위해 사용. 주로 기존 데이터를 새 데이터로 대체할 때 사용.\nDELETE: 서버의 특정 자원을 삭제하기 위해 사용.\nHEAD: GET과 유사하지만, 응답 본문(body) 없이 HTTP 헤더 정보만을 요청할 때 사용. 주로 자원의 메타데이터를 확인할 때 사용.\nPATCH: PUT과 유사하지만, 전체 자원을 대체하는 것이 아니라 일부를 수정할 때 사용.\nOPTIONS: 웹 서버에서 지원하는 HTTP 메서드를 알아보기 위해 사용. 주로 CORS(Cross-Origin Resource Sharing)에서 사전 요청을 처리하는 데 사용.\n\n이러한 메서드들은 웹 서버와의 통신에서 특정한 종류의 요청을 나타내며, RESTful API 디자인에서 핵심적인 역할을 한다.\n\n\n\n\n\n\n\n\n\n2 Go to Blog Content List\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/web/visitor_trancking.html",
    "href": "docs/blog/posts/Engineering/web/visitor_trancking.html",
    "title": "Visitor Tracking",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nQuarto를 사용하여 만든 기술 블로그 웹사이트의 방문자 수를 추적하려면, 일반적으로 웹 분석 도구를 사용해야한다. 가장 널리 사용되는 도구 중 하나는 Google Analytics인데. Google Analytics를 사용하면 사이트 방문자 수, 페이지 뷰, 사용자 행동 등 다양한 통계를 확인할 수 있다.\n\n\n\n\nGoogle Analytics 계정 생성 및 설정\n\nGoogle Analytics 계정 생성 링크\n새로운 프로퍼티(Property)를 생성 링크\n\n웹사이트에 대한 정보를 입력하고 추적 ID를 수령\n\n추적 코드를 웹사이트에 추가\n\nGoogle Analytics에서 제공하는 추적 코드를 복사\nQuarto 웹사이트의 HTML 템플릿에 이 추적 코드를 추가\n이 코드는 보통\n\n태그 안에 넣음\n\nQuarto 프로젝트에 코드 추가\n\nQuarto 프로젝트의 _quarto.yml 파일이나 해당 HTML 템플릿 파일에 Google Analytics 스크립트를 추가\n\n웹사이트 업데이트 및 배포\n\nrendering을 하여 변경사항을 웹사이트에 적용하고 재배포\n\nGoogle Analytics에서 데이터 확인\n\nGoogle Analytics Dashboard에서 웹사이트의 트래픽과 관련 데이터를 확인\n\n\n참고: quarto official website &gt;&gt; guide &gt;&gt; websites &gt;&gt; webiste tools\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/web/visitor_trancking.html#google-analytics-활용",
    "href": "docs/blog/posts/Engineering/web/visitor_trancking.html#google-analytics-활용",
    "title": "Visitor Tracking",
    "section": "",
    "text": "Quarto를 사용하여 만든 기술 블로그 웹사이트의 방문자 수를 추적하려면, 일반적으로 웹 분석 도구를 사용해야한다. 가장 널리 사용되는 도구 중 하나는 Google Analytics인데. Google Analytics를 사용하면 사이트 방문자 수, 페이지 뷰, 사용자 행동 등 다양한 통계를 확인할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/web/visitor_trancking.html#integrating-google-analytics-into-a-quarto-website",
    "href": "docs/blog/posts/Engineering/web/visitor_trancking.html#integrating-google-analytics-into-a-quarto-website",
    "title": "Visitor Tracking",
    "section": "",
    "text": "Google Analytics 계정 생성 및 설정\n\nGoogle Analytics 계정 생성 링크\n새로운 프로퍼티(Property)를 생성 링크\n\n웹사이트에 대한 정보를 입력하고 추적 ID를 수령\n\n추적 코드를 웹사이트에 추가\n\nGoogle Analytics에서 제공하는 추적 코드를 복사\nQuarto 웹사이트의 HTML 템플릿에 이 추적 코드를 추가\n이 코드는 보통\n\n태그 안에 넣음\n\nQuarto 프로젝트에 코드 추가\n\nQuarto 프로젝트의 _quarto.yml 파일이나 해당 HTML 템플릿 파일에 Google Analytics 스크립트를 추가\n\n웹사이트 업데이트 및 배포\n\nrendering을 하여 변경사항을 웹사이트에 적용하고 재배포\n\nGoogle Analytics에서 데이터 확인\n\nGoogle Analytics Dashboard에서 웹사이트의 트래픽과 관련 데이터를 확인\n\n\n참고: quarto official website &gt;&gt; guide &gt;&gt; websites &gt;&gt; webiste tools"
  },
  {
    "objectID": "docs/blog/posts/Engineering/web/website_monetization.html",
    "href": "docs/blog/posts/Engineering/web/website_monetization.html",
    "title": "Website Monetization",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\nGoogle AdSense 계정 생성 및 승인 받기\n\nGoogle AdSense 웹사이트에 방문하여 계정을 만듭니다.\n계정이 승인되기를 기다린다 (이 과정은 몇 일이 걸릴 수 있음)\n\nAdSense 코드 생성\n\nGoogle AdSense 계정이 승인되면, 광고를 생성하고 광고 코드를 받는다.\n‘광고’ 섹션에서 새 광고 단위를 생성하고, 광고 스타일과 크기를 설정\n생성된 광고 코드를 복사\n\nQuarto 웹사이트에 코드 추가\n\nQuarto 프로젝트의 해당 HTML 파일을 연다.\n보통은 _output.yml 파일이나, 특정 레이아웃 파일일 수 있다.\nHTML의\n\n태그 안에 AdSense 코드를 붙여넣는다. 이는 전역적으로 광고를 관리하기 위함이다.\n또는, 특정 페이지나 섹션에 광고를 표시하고 싶다면 해당 위치에 광고 코드를 삽입\n\n웹사이트 업데이트 및 배포\n\n변경 사항을 저장하고 웹사이트를 다시 배포\n일정 시간이 지나면, 웹사이트에 광고가 표시되기 시작\n\n성능 모니터링\n\nGoogle AdSense 계정에서 광고 성능을 주기적으로 확인\n광고 위치나 스타일을 조정하여 최적화할 수 있다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/web/website_monetization.html#google-adsense-활용",
    "href": "docs/blog/posts/Engineering/web/website_monetization.html#google-adsense-활용",
    "title": "Website Monetization",
    "section": "",
    "text": "Google AdSense 계정 생성 및 승인 받기\n\nGoogle AdSense 웹사이트에 방문하여 계정을 만듭니다.\n계정이 승인되기를 기다린다 (이 과정은 몇 일이 걸릴 수 있음)\n\nAdSense 코드 생성\n\nGoogle AdSense 계정이 승인되면, 광고를 생성하고 광고 코드를 받는다.\n‘광고’ 섹션에서 새 광고 단위를 생성하고, 광고 스타일과 크기를 설정\n생성된 광고 코드를 복사\n\nQuarto 웹사이트에 코드 추가\n\nQuarto 프로젝트의 해당 HTML 파일을 연다.\n보통은 _output.yml 파일이나, 특정 레이아웃 파일일 수 있다.\nHTML의\n\n태그 안에 AdSense 코드를 붙여넣는다. 이는 전역적으로 광고를 관리하기 위함이다.\n또는, 특정 페이지나 섹션에 광고를 표시하고 싶다면 해당 위치에 광고 코드를 삽입\n\n웹사이트 업데이트 및 배포\n\n변경 사항을 저장하고 웹사이트를 다시 배포\n일정 시간이 지나면, 웹사이트에 광고가 표시되기 시작\n\n성능 모니터링\n\nGoogle AdSense 계정에서 광고 성능을 주기적으로 확인\n광고 위치나 스타일을 조정하여 최적화할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#airflow-의-꽃-custom-오퍼레이터-만드는-법",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#airflow-의-꽃-custom-오퍼레이터-만드는-법",
    "title": "More Operators",
    "section": "",
    "text": "Airflow는 오퍼레이터를 직접 만들어 사용할 수 있도록 클래스를 제공 (BaseOperator)\n\nBaseOperator를 상속 받아 나만의 custom operator를 만들 수 있다.\n\n확장성을 비약적으로 높여주는 기능으로 airflow가 인기가 많은 이유가 됨\nBaseOperator 상속한 자식 operator가 custom operator가 됨\nBaseOperator 상속시 두 가지 메서드를 재정의해야 함 (Overriding)\n\nOverriding: 객체 지향 언어에서 부모 클래스가 가지고있던 method를 자식 class에서 재정의\n\n\n생성자 (def __init__) 재정의\n\n\n클래스에서 객체 생성시 객체에 대한 초기값 지정하는 함수\n\n\ndef execute(self, context) 재정의\n\n\n자식 클래스에서 똑같은 이름으로 써야함: def execute(self, context) 자체를 이용해야함 변경하면 안됨\ninit 생성자로 객체를 얻은 후 execute 메서드를 실행시키도록 되어 있음\n비즈니스 로직은 execute 에 구현해야함\n\n예를 들어, 다음과 같은 custom operator만들고 싶을 땐 mardkown     custom_task=CustomOperator(       task_id='xxxxx',       A='aaaa',       B='bbbb'     )     custom_task &gt;&gt; python_task\n\n생성자 (def __init__)에 parameter A와 B에 대한 내용이 들어가 있어야함\ncustom_task &gt;&gt; python_task 실행이 되면 execute() 함수가 내부적으로 실행되는 구조.\n\nCustom Operator를 만들 때, 오퍼레이터의 기능 정의를 명확하게 해줘야함\n\n기존에 있던 operator들로 job을 수행하기에 제한적이었던 점을 보완할 기능을 정의해야함\nsimpleHttpOperator에서 불편했던 점 2가지\n\n첫 번째, 매번 endpoint에 시작행/끝행을 넣어서 호출 해줘야 했음 \\(\\rightarrow\\) custom operator에서는 이것을 1000행씩 불러오도록 하는 기능이 필요\n두 번째, xcom에서 data를 가지고 온후 data에 접근할 수 있는 형태로 전처리를 해줘야하는 부분이 있었음 \\(\\rightarrow\\) custom operator에서는 전처리없이 local에다가 바로 저장할 수 있도록 하는 기능이 필요\n\n\nCustom Operator 기능 정의\n\n서울시 공공데이터 API 를 호출하여 전체 데이터를 받은 후 .csv 파일로 저장하기\ndag과 operator의 위치\n\ndag 위치: /dags/dags_seoul_api.py 생성\nfrom operators.seoul_api_to_csv_operator # airflow container가 plugins까지는 인식하기 때문에 그 하위 directory인 operators부터 path를 적어주면 됨.\nimport SeoulApiToCsvOperator #클래스명\n\noperator의 위치: /plugins/operators/seoul_api_to_csv_operator.py 생성\n\nTemplate 문법을 사용할 수 있는 Template을 지정\n\nop_kwards, op_args, param 과 같은 사용 가능한 파라미터 지정하기\n\nCustom Operator 작성 예시\n\n아래의 HelloOperator는 __init__과 execute(self, context) 둘다 재정의 해줬기 때문에 코드상으론 심플하지만 이미 custom operator로서의 기능을 할 수 있다.\n\nclass HelloOperator(BaseOperator):\n  template_fields: Sequence[str] = (\"name\",) \n  # 어떤 parameter에 template 문법을 적용할지 지정해주면 됨\n  # 생성자 함수 __init__ 의 member들로 template 문법을 적용할 paratemer 지정\n  # 현재 name만 template 문법 적용\n\n  def __init__(self, name: str, world: str, **kwargs) -&gt; None:\n    super().__init__(**kwargs)\n    self.name = name\n    self.world = world\n\n  def execute(self, context):\n    message = f\"Hello {self.world} it's {self.name}!\"\n    print(message)\n    return message\n\nwith dag:\n  hello_task = HelloOperator(\n    task_id='task_id_1', # task_id는 생성자에서 명시 않았지만 기본적으로 생성자에 들어가도록 설계되어 있음\n    name '{{ task_instance.task_id }}', # 이 name에 template 문법을 적용하면 '{{ task_instance.task_id }}'의 값이 생성자 함수의 name 인자에 들어가게 된다.\n    # HelloOperator의 객체가 생성되게 될때 self.name은 실제값으로 치환됨\n    world='Earth' #world는  Sequence[str] = (\"name\",) 에 명시되어 있지 않기 때문에 치환안됨\n  )\nGoogle ‘Airflow Custom Operator’ : creating a custom operator\n\nfrom airflow.models.baseoperator import BaseOperator\n\n\nclass HelloOperator(BaseOperator):\n    def __init__(self, name: str, **kwargs) -&gt; None:\n        super().__init__(**kwargs) # 부모함수(BaseOperator)의 생성자를 호출\n        self.name = name\n\n    def execute(self, context):\n        message = f\"Hello {self.name}\"\n        print(message)\n        return message\n\n이렇게 아래와 같이 HelloOperator object가 생성이 되면 name의 ’xxxx’값이 def __init__(self, name: str, **kwargs)의 name: str에 할당된다.\n\nhello_task=HelloOperator(\n  task_id='xxx', \n  name='xxxx'\n)\n\n위의 task_id는 def __init__(self, name: str, **kwargs) 의 **kwargs에 할당됨. 이어서 그 값이 super().__init__(**kwargs) 에도 할당되어 부모 함수까지 전달됨\n이런 메카니즘으로, 생성자에 task_id를 명시해서 적어줄 필요 없음\n그리고, 가지고 온 name값을 self.name에 할당\n\n\n\n\nAirflow Custom Operator Example"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Linux/rm_cannot_remove_files.html",
    "href": "docs/blog/posts/Engineering/Linux/rm_cannot_remove_files.html",
    "title": "Linux_Error_Fix_rm_cannot_remove_files_busy",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n1 Error\nrm: cannot remove 'files': Device or resource busy\n\n\n2 How to Solve\n\n파일이나 디렉토리를 사용 중인 프로세스 식별: lsof 명령어를 사용하여 어떤 프로세스가 파일이나 디렉토리를 사용하고 있는지 찾을 수 있다. 명령어는 lsof | grep 'files' 실행. 이렇게 하면 삭제하려는 항목을 사용하고 있는 프로세스가 나열된다.\n프로세스 종료: 파일이나 디렉토리를 사용하는 프로세스를 찾았다면, 적절한 방법(예: 파일을 사용하는 프로그램 종료)으로 그 프로세스를 정상적으로 종료할 수 있다. 만약 그것이 불가능하거나 프로세스가 응답하지 않는 경우, 프로세스 ID 뒤에 kill 명령어를 사용하여 강제로 종료할 수 있다. 이 방법을 사용할 때는 데이터 손실이나 시스템 불안정을 초래할 수 있으므로 주의해야 함.\n파일 시스템 마운트 해제: 디렉토리가 마운트된 파일 시스템의 일부라면, 삭제하기 전에 마운트를 해제해야 할 수 있다. 마운트 지점이나 장치 이름 뒤에 umount 명령어를 사용. 마운트 해제하기 전에 어떤 프로세스도 파일 시스템을 사용하고 있지 않은지 확인.\n\n\n\n\n\n\n\n\n\n\n3 Go to Blog Content List\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#api-사용을-위한-키-발급-받기",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#api-사용을-위한-키-발급-받기",
    "title": "More Operators",
    "section": "",
    "text": "HTTP 요청을 하고 결과로 text 를 리턴 받는 오퍼레이터 리턴 값은 Xcom 에 저장\nHTTP 를 이용하여 API 를 처리하는 RestAPI 호출시 사용 가능\n\nRestAPI: API 방법 중 하나로 http의 protocol을 이용해서 API data를 제공하거나, 다운로드, 변경할 수 있는 API를 제공하는 방식\n\n\n\n\n\n\n\nNote\n\n\n\n\nREST API는 Representational State Transfer의 약자로, 웹 기반의 서비스 간에 통신을 위한 일반적인 규칙(아키텍처 스타일)을 의미\nREST API는 인터넷에서 데이터를 교환하기 위한 표준 방법 중 하나로 널리 사용되는데 간단히 말해서, REST API는 웹 애플리케이션에서 다른 시스템과 정보를 주고받기 위한 방법이다.\nREST API의 주요 특징\n\n클라이언트-서버 구조: REST API는 클라이언트(예: 웹 브라우저)와 서버 간의 분리를 기반으로 함. 클라이언트는 사용자 인터페이스와 사용자 상호작용을 관리하고, 서버는 데이터 저장 및 백엔드 로직을 처리\n무상태(Stateless): 각 요청은 독립적. 즉, 이전 요청의 상태를 서버가 기억하지 않는다는 의미. 모든 필요한 정보는 각 요청과 함께 전송되어야 한다.\n캐시 가능: REST API 응답은 캐시될 수 있으므로, 성능을 향상시키고 서버의 부하를 줄일 수 있다.\n유니폼 인터페이스(Uniform Interface): REST API는 표준화된 방법을 사용하여 서버의 리소스에 접근. 이는 HTTP 메서드를 활용하는데, 예를 들어 GET(읽기), POST(생성), PUT(수정), DELETE(삭제) 등이 있다.\n리소스 기반: REST API에서 ’리소스’는 웹에서의 객체, 데이터 또는 서비스를 의미하며, 각 리소스는 고유한 URI(Uniform Resource Identifier)를 통해 식별됨.\n\n\n\n\n\nSimpleHttpOperator를 이용해서 RestAPI를 호출\nhttp://localhost:8080/provider/apache-airflow-providers-http 에서 오퍼레이터 명세 확인하기\npython API click\n\nhttp Operator click 의 자주 쓰는 parameters\n\nhttp_conn_id (str) – The http connection to run the operator against: full url의 ‘xxxx.com/(나머지)’ 의 xxxx.com 을 넣어줌\nendpoint (str | None) – The relative part of the full url. (templated): full url의 ‘xxxx.com/(나머지)’ 의 (나머지)를 넣어줌\nmethod (str) – The HTTP method to use, default = “POST”: http의 4개 methods\n\nGET: data 가져오기\nPOST: data insert\nPUT: data 변경/update\nDELETE: data 삭제하기\n\ndata (Any) – The data to pass. POST-data in POST/PUT and params in the URL for a GET request. (templated)\n\nPOST의 경우: insert할 data 값\nGET의 경우: HTTP Protocol을 GET으로 줬으면 GET요청의 parameter를 dictionary 형태로 입력해주면 됨\n\nheaders (dict[str, str] | None) – The HTTP headers to be added to the GET request\nresponse_check (Callable[…, bool] | None) – A check against the ‘requests’ response object. The callable takes the response object as the first positional argument and optionally any number of keyword arguments available in the context dictionary. It should return True for ‘pass’ and False otherwise.\n\ndata 요청시 응답이 제대로 왔는지 확인\ndata를 일회성으로 가져올 때 데이터 1000 행이 제대로 들어왔는지 간단한 조회로 알아볼 수 있지만\ndata를 open API를 이용하여 주기적으로 내려받는 자동화의 경우 일일히 확인하는게 아니라 데이터가 잘 내려 받았는지 확인하는 함수를 하나 만들어 이 parameter에 할당하면 됨.\ntrue로 resturn하면 API가 정상적으로 실행된 것으로 간주\n\nresponse_filter (Callable[…, Any] | None) – A function allowing you to manipulate the response text. e.g response_filter=lambda response: json.loads(response.text). The callable takes the response object as the first positional argument and optionally any number of keyword arguments available in the context dictionary.\n\nAPI의 요청 결과가 dictionary 형태의 string으로 출력됨. 나중에 dictionary type으로 바꿔주고 key: value 형태로 access하여 원하는 데이터를 가져 와야 한다.\n이런 전처리 과정을 수행하는 함수를 만들어 reponse_filter parameter에 넣어줘야함.\n\n\n\n\n\n\n\n\n\n\nhttp_conn_id에 들어갈 connection id 만들기\n\nairflow web service &gt;&gt; Admin &gt;&gt; Plus button\nConn_id: 다른 conn 이름과 중복되지 않게 string으로 작성\n\nConnection_type: HTTP\nHost: openapi.seoul.go.kr (Open Data 명세를 보고 적음. http://openapi.seoul.go.kr:8088/(key:인증키)/(type)/(servicec)/(type)/(start_index)/(end_index)/(main_key 혹은 날짜) 중에서 openapi.seoul.go.kr 만 적으면 됨 )\ntoken값에 의해 인증되는 방식이기 때문에 schema/login/password 필요없음\nPort: 8088\nTest 버튼 클릭시 “405:Method Not Allowed” 가 뜨지만 무방함\n\n\n\n\n\nfrom airflow.providers.http.operators.http import SimpleHttpOperator\nwith DAG(...) as dag:\n  tb_cycle_station_info = SimpleHttpOperator(\n    task_id ='tb_cycle_station_info',\n    http_conn_id = 'openapi.seoul.go.kr',\n    endpoint ='{{var.value.apikey_openpai_seoul_go_kr}}/json/ListExhibitionOfSeoulMOAInfo/1/1000/',\n    method ='GET',\n    headers ={'Content-Type: 'application/json',\n              charset': 'utf-8',\n              'Accept': '*/*'\n              }\n  )\n\n\n\nSimpleHttpOperator를 1000개의 DAGs 에 작성했는데 API 키가 바뀐다면 1000개의 스크립트를 다 바꿔줘야하나?\nDAG에다가 바로 인증키를 복붙하면 다른 사람들도 API키를 볼 수 있어 보안상의 문제가 될 수 있다.\n위의 2가지 문제를 해결하기 위해 global variable을 이용하여 적을 것.\n\nAPI key를 variable을 이용하여 등록: airflow web service &gt;&gt; admin &gt;&gt; Variables\n\nkey:value 형태로 등록 가능\n관리자가 DB에 들어가면 API Key값 볼 수 있음\n\n\nKey에 아래 이름이 있을 경우 val 을 자동 마스킹처리하여 보여줌\n\n‘access_token’,\n‘api_key’,\n‘apikey’,\n‘authorization’,\n‘passphrase’,\n‘passwd’,\n‘password’,\n‘private_key’,\n‘secret’,\n‘token’\n\nglobal variable 설정하면\n\n서울시 공공데이터 추출하는 DAG 이 여러 개 있어도 API 키를 하나로 관리 가능\nAPI 키를 코드에 노출시키지 않음\n\nDAG Full Example\n\n# Package Import\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.http.operators.http import SimpleHttpOperator\nfrom airflow.decorators import task\nimport pendulum\n\nwith DAG(\n    dag_id='dags_simple_http_operator',\n    start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule=None\n) as dag:\n\n    '''서울시립 전시 정보'''\n    seoul_exhibition_info = SimpleHttpOperator(\n        task_id='seoul_exhibition_info',\n        http_conn_id='openapi.seoul.go.kr',\n        endpoint='{{var.value.apikey_openapi_seoul_go_kr}}/json/ListExhibitionOfSeoulMOAInfo/1/1000/',\n        method='GET',\n        headers={'Content-Type': 'application/json',\n                        'charset': 'utf-8',\n                        'Accept': '*/*'\n                        }\n    )\n\n    # 이 task는 seoul_exhibition_info가 xcom 에 넣었던 data를 빼오는 것\n    @task(task_id='python_2')\n    def python_2(**kwargs):\n        ti = kwargs['ti']\n        rslt = ti.xcom_pull(task_ids='seoul_exhibition_info')\n        import json\n        from pprint import pprint\n\n        pprint(json.loads(rslt))\n        \n    seoul_exhibition_info &gt;&gt; python_2()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html",
    "title": "Connection & Hook",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\nposgres DB를 container로 띄우기\nairflow의 connection & hook 설정\n\n\n\n\nDocker Compose를 사용하면 이러한 다중 컨테이너 환경을 간편하게 구성하고 관리할 수 있다.\n\n목적\n\n1 개 이상의 도커 컨테이너 생성시 스크립트 하나로 컨테이너들의 설정을 관리할 수 있도록 해주는 Docker 의 기능 중 하나\n각 각의 container 설정 관리 뿐만 아니라 containers간의 연관관계 및 dependency까지 설정할 수 있다.\n일부 containers를 같은 network에서 띄워지는지도 설정할 수 있는 network 설정도 할 수 있다.\n\n설정 파일\n\nDocker Compose는 docker-compose.yml이라는 YAML 파일을 사용하여 서비스, 네트워크, 볼륨 등을 정의. 이 파일에는 애플리케이션을 구성하는 모든 컨테이너와 그 설정이 포함됨.\n\n기능\n\n간편한 관리\n\ndocker-compose up 명령어 한 번으로 모든 서비스를 시작하고, docker-compose down 명령어로 모두 종료할 수 있다. 이는 복잡한 컨테이너 관리를 단순화한다.\n\n일관성 유지\n\nDocker Compose를 사용하면 개발, 테스팅, 프로덕션 환경에서 동일한 환경을 재현할 수 있어 일관성을 유지.\n\n다중 컨테이너 조정\n\n여러 컨테이너 간의 의존성과 순서를 관리할 수 있어, 올바른 순서로 서비스가 시작되고 종료된다.\n\n개발 효율성 증가\n\n개발 과정에서 빠른 반복과 변경이 가능하며, 변경 사항을 쉽게 적용하고 테스트할 수 있다.\n\n큰 규모의 프로덕션 환경에서는 Kubernetes와 같은 보다 복잡한 오케스트레이션 도구가 종종 사용된다.\n\n\n작성방법\n\n모든 설정은 docker_compose.yaml 파일에 컨테이너들의 설정 내용을 입력\n\ndocker compose service 시작\n\ndocker_compose.yaml 파일이 있는 위치에서 sudo docker compose up 명령 입력하면 docker_compose.yaml에 있는 모든 설정이 적용된다.\n기본적으로 Docker 서비스가 설치되어 있어야 함\n\ndocker compose.yaml 파일의 구성\n\nyaml 파일은 json 이나 xml 과 같이 key, value 가 중첩적으로 구성되며 계층적인 구조를 가진다. 파이썬처럼 들여쓰기 문법을 사용함. 들여쓰기 잘못하면 오류남.\n다시 말해서, json 이나 xml은 파이썬의 dictionary 같이 nested {key:value} structure로 작성할 수 있다.\n\ndocker_compose.yaml 파일의 1 Level 내용\n\nversion: '3.8' # yaml 파일의 버전 정보 옵션\nx-airflow-common: # 'x-': Extention Fields(각 서비스 항목에 또는 container에  공통 적용될 항목들 정의)\nservices: # 컨테이너로 실행할 서비스 정의로 가장 신경써서 적어야할 부분\nvolumes:  # 컨테이너에 할당할 volume 정의\nnetworks: # 컨테이너에 연결할 network 정의. 초기에는 level1에 networks 항목이 정의되어 있지 않아 정의해줘야함\n\n위의 내용에서 key값은 version, x-airflow-common, services, volumes, networks\nnetworks: containers에 IP나 network정보를 할당해주기 위해 작성하는 항목\nx-airflow-common: 공통 지정할 항목을 &를 붙여서 지정\nx-airflow-common\n    &airflow-common # 공통 지정 parameters 정의: image, environment, depends_on\n    image: ${AIRFLOW_IMAGE:-apache/airflow:2.5.2}\n    environment\n        &airflow-common-env\n        AIRFLOW__CORE__DEFAULT_TIMEZONE: 'Asia/Seoul'\n        AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    depends_on # containers 실행 순서를 결정\n        &airflow-common-depends-on # 공통 지정 parameters 정의: redis, postgres\n        redis:\n            condition: service_healthy\n        postgres:\n            condition: service_healthy\n\n&airflow-common는 image, environment, depends_on 인수를 미리 가지고 있음\n공통 지정 항목1: &airflow-common\n\n변수 또는 parameter: image, environment, depends_on\n\n공통 지정 항목2: &airflow-common-env\n\n변수 또는 parameter: AIRFLOW__CORE__DEFAULT_TIMEZONE, AIRFLOW__CORE__EXECUTOR, AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n\n공통 지정 항목3: &airflow-common-depends-on\n\n변수 또는 parameter: redis, postgres\n\n후차적인 스크립트에서 &airflow-common 를 호출 하면 아래의 내용이 모두 호출 됨\n\n&airflow-common # 공통 지정 parameters 정의: image, environment, depends_on\nimage: ${AIRFLOW_IMAGE:-apache/airflow:2.5.2}\nenvironment\n    &airflow-common-env\n    AIRFLOW__CORE__DEFAULT_TIMEZONE: 'Asia/Seoul'\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\ndepends_on # containers 실행 순서를 결정\n    &airflow-common-depends-on # 공통 지정 parameters 정의: redis, postgres\n    redis:\n        condition: service_healthy\n    postgres:\n        condition: service_healthy\nservices: 컨테이너로 올릴(실행할) 서비스 지정\n\nairflow-webserver key\n\n  airflow-webserver:\n    &lt;&lt;: *airflow-common #&lt;&lt; 붙여서 공통 지정 항목 (image, environment, depends_on) 호출 \n    command: webserver\n    ports:\n      - \"8080:8080\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n      interval: 10s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on: #위에서 &lt;&lt;: *airflow-common 때문에 depends on이 있지만 한번 더 설정하게 되면 앞에 있는 depends_on은 무시됨\n      &lt;&lt;: *airflow-common-depends-on # *airflow-common의 depends-on 호출 (redis, postgres)\n      airflow-init:\n        condition: service_completed_successfully\n    networks:\n      network_custom:\n        ipv4_address: 172.28.0.6\n\ndefault postgres 서비스 말고 새로 지정할 postgres 서비스. 초기의 docker_compose.yaml파일에서는 없음\n\nservices:\n   postgres:\n     image: postgres:13\n     environment:\n       POSTGRES_USER: airflow\n       POSTGRES_PASSWORD: airflow\n       POSTGRES_DB: airflow\n     volumes:\n       - postgres-db-volume:/var/lib/postgresql/data\n     healthcheck:\n       test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n       interval: 10s\n       retries: 5\n       start_period: 5s\n    ports\n        - 5432:5432\n     restart: always\n\nimage: postgres:13 은 image는 postgre:13 버전의 image를 쓴다는 것이고 이 image가 local에 있으면 그대로 쓰고 없으면 인터넷에서 download됨.\nenvironment: 는 postgres OS에 설정할 환경 변수들\nvolumes: container와 연결할 local file system 경로를 의미\n\npostgres-db-volume:/var/lib/postgresql/data :을 기준으로 왼쪽이 local file system의 경로 오른쪽이 연결할 container의 directory. 이 과정을 mount 시켰다라고 말함\npostgres-db-volume 문서 제일 하단에 미리 만들어져 있음\ncontainer가 실행되었다가 (띄어졌다가) 꺼지면 (내려지면) 안에 있는 data들이 모두 사라지기 때문에 mount시키는 것이 필요함. 특히, DB container는 mount가 잘 됐는지 확인해야함\npostgres-db-volume:/var/lib/postgresql/data는 postgresql의 data가 저장되는 directory를 local file system으로 연결시켜 놓은 것\n\n\nhealthcheck: container가 상태 꺼졌는지 켜졌는지 확인\nports: container에 접속하기 위해 공개할 port를 명시\n\n5432:5432 : 을 기준으로 왼쪽이 local에서 web에 접속할 port 번호고 오른쪽이 service가 갖고 있는 port번호. 다시 말해서, wsl 시스템안에 여러 컨테이너들이 있고 그 중 postgres 이미지가 깔려 있다면 postgres는 5432 port를 가지고 있는 상태이다. postgres 이미지에 접근하려면 wsl의 port를 통해서 접근해야하는데 그 wsl의 port가 5432로 지정된 것을 의미한다.\n원래 docker-compose.yaml 파일 최초 다운로드시 없는 항목이다. 추가해줘야 함.\n\nrestart: always container가 죽으면 언제 새로 띄워주겠냐는 것이고 always니까 항상 새로 띄워줌\nredis\n\n  redis:\n    image: redis:latest\n    expose:\n      - 6379\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 30s\n      retries: 50\n      start_period: 30s\n    restart: always\n\nexpose: 6379 이것 역시 port번호인데 외부와 연결될 때 사용되는 게 아니라 내부 다른 containers와 연결시 사용되는 port번호로 expose parameter로 공개 설정한다.\nairflow-webserver\n\n  airflow-webserver:\n    &lt;&lt;: *airflow-common # &airflow-common의 공통 지정 parameters 호출: image, environment, depends_on\n    command: webserver # container를 띄울 때 실행할 명령어\n    ports:\n      - \"8080:8080\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on: # 2번째 depends_on 선언이 되어 호출된 (&lt;&lt;: *airflow-common) &airflow-common의 depends_on 의 내용은 무시된다.\n      &lt;&lt;: *airflow-common-depends-on # 공통 지정 parameters 호출: redis, postgres\n      airflow-init:\n        condition: service_completed_successfully\n\nservices: 1-level (x-airflow-common 같은 level)\n\nairflow-webserver, airflow-scheduler, redis, postgres 등이 같은 level의 서비스 항목으로 열거 된다.\n\ndepends_on: containers를 띄우는 (실행하는) 순서를 설정하는 부분으로 위의 예시는\n\nredis, postgres, airflow-init을 띄우고 나서 airflow-webserver를 띄우겠다는 것.\n[redis, postgres, airflow-init]&gt;&gt;airflow-webserver\n\nairflow-scheduler: 1-level\n\n  airflow-scheduler: # 1-level\n       &lt;&lt;: *airflow-common\n       command: scheduler\n       healthcheck:\n         test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8974/health\"]\n         interval: 30s\n         timeout: 10s\n         retries: 5\n         start_period: 30s\n       restart: always\n       depends_on:\n         &lt;&lt;: *airflow-common-depends-on\n         airflow-init:\n           condition: service_completed_successfully\nvolumes: 컨테이너와 연결하기 위한 볼륨 (공간) 정보 ```markdown volumns: postgree-db-volume: #새로 만들 볼륨 이름\n```\n\n볼륨에 대한 정보 확인하기\n\n볼륨 리스트 보기 : sudo docker volume ls (현재 만들어진 volumes 리스트와 volumne_id가 보임)\n볼륨 상세 보기 : sudo docker volume inspect {volume_id}\n\n\nnetworks: 컨테이너의 network 정보 구성\nnetworks:\n    network_custom: # 새로 만들 네트워크 이름\n        driver: bridge\n        ipam:\n            driver: default\n            config:\n                - subnet : 172.18.0.0/16 # 네트워크 IP의 주소값이 2^16 개, host가 가질 IP의 주소값은 2^16-2 만큼을 할당할 수 있다.\n                  gateway: 172.18.0.1\n\nconfig:\n\nsubnet : 172.18.0.0/16\ngateway: 172.18.0.1\n\n:::{.callout-note} 여기서 언급된 “172.18.0.0/16”은 CIDR (Classless Inter-Domain Routing) 표기법을 사용한 네트워크 주소이다. 이 표기법은 IP 네트워크를 어떻게 분할하고 주소를 할당할 것인지 정의한다.\n“172.18.0.0/16”에서 “/16”은 네트워크 마스크의 길이를 나타낸다. 이는 전체 32비트 IPv4 주소 중 상위 16비트가 네트워크 주소를 위해 사용되고, 나머지 하위 16비트가 호스트 주소를 위해 사용됨을 의미한다.\n네트워크 주소의 수: “/16” 네트워크 마스크는 2^16, 즉 65,536개의 가능한 호스트 주소를 제공한다. 이는 네트워크의 첫 번째 주소 (172.18.0.0)부터 마지막 주소 (172.18.255.255)까지를 포함한다.\n호스트가 사용할 수 있는 주소 수: 실제 호스트가 사용할 수 있는 IP 주소는 이론상 65,536개에서 2개를 뺀 65,534개이다. 이는 네트워크 주소 (172.18.0.0)와 브로드캐스트 주소 (172.18.255.255)가 호스트 할당에 사용되지 않기 때문이다.\n네트워크 주소 (여기서는 172.18.0.0): 네트워크 부분은 호스트를 식별하는데 사용되는 부분이 아닌 네트워크 자체를 식별하는데 사용되는 부분. 브로드캐스트 주소 (여기서는 172.18.255.255): 네트워크 내의 모든 호스트에 데이터를 전송하는 데 사용됨.\n이러한 설정에서는 네트워크 IP의 주소값이 2^16 (또는 65536) 개로 할당된다.\n따라서, 이 설정에서 Docker는 172.18.0.1부터 172.18.255.254까지의 IP 주소 범위를 갖는 네트워크를 생성하며, 이 범위 내의 IP 주소를 컨테이너에 할당할 수 있는데, 그것이 2^16-2이다.\n참고로 네트워크 IP 주소는 클래스별로 사설 IP대역을 만들어놨음. A: 10.0.0.0 ~ 10.255.255.255 B: 172.16.0.0 ~ 172.31.255.255 C: 192.168.0.0 ~ 192.168.255.255\nB클래스의 네트워크 주소 bit는 16비트이다. 호스트 주소 bit도 16비트. :::\n네트워크에 대한 정보 확인하기\n\n네트워크 리스트 보기 : sudo docker network ls\n네트워크 상세 보기 : sudo docker network inspect {network_id}\n\n\n\n\n\n\n\n\n\npostgres_custom 이라는 이름의 컨테이너 서비스 추가하기\nnetworks를 만들어 컨테이너에 고정 IP 할당\nDBeaver로 postgres DB에 접속\n\nservices:\n  postgres_custom:\n    image: postgres:13\n    environment:\n      POSTGRES_USER: kmkim\n      POSTGRES_PASSWORD: kmkim\n      POSTGRES_DB: kmkim\n      TZ: Asia/Seoul\n    volumes:\n      - postgres-custom-db-volume:/var/lib/postgresql/data\n    ports:\n      - 5432:5432\n    networks:\n      network_custom: # 밑에서 정의한 network_custom 을 쓰겠다는 의미\n        ipv4_address: 172.28.0.3 # 할당된 IP\n\nnetworks:\n  network_custom: \n    driver: bridge\n    ipam:\n        driver: default\n        config:\n            - subnet: 172.28.0.0/16 # 네트 워크 ID 주소 밑에 16개/host id 주소 밑에 16개를 할당하겠다는 의미\n              gateway: 172.28.0.1 # default 네트워크 (172.18.0.0)가 쓰고 있지 않은 서브넷으로 구성\nvolumes:\n  postgres-db-volume:\n  postgres-custom-db-volume: #wsl의 어느 path에 mapping이 되어 있는지 확인할 것\n\n\n\n\n기본적으로 컨테이너들은 유동 IP를 지니며 (재기동시 IP 변경 가능)\npostgres DB에 접속하려면 고정 IP 필요\n고정 IP를 할당하려면 networks를 만들어서 할당해야 함.\nnetworks 를 지정하지 않은 컨테이너들(airflow를 설치하면서 기본적으로 설치되는 containers)은 default network에 묶이게 됨\n따라서 동일 네트워크에 두고 싶은 컨테이너들은 모두 동일 netwworks 할당 필요\n기존 containers와 custom container를 모두 custom networks를 바라보게 지정 필요\n\nnetworks:\n  network_custom: \n    driver: bridge\n    ipam:\n        driver: default\n        config:\n            - subnet: 172.28.0.0/16 # 네트 워크 ID 주소 밑에 16개/host id 주소 밑에 16개를 할당하겠다는 의미\n              gateway: 172.28.0.1 # default 네트워크 (172.18.0.0)가 쓰고 있지 않은 서브넷으로 구성\n\nPostgres_custom 컨테이너 뿐만 아니라 다른 컨테이너에도 network_custom 할당하고 IP 부여\npostgres_custom:  # 172.28.0.3\npostgres:  # 172.28.0.3 +  포트 노출 설정:(5431:5432)\nredis: # 172.28.0.5\nairflow-webserver: # 172.28.0.6\nairflow-scheduler: # 172.28.0.7\nairflow-worker: # 172.28.0.8\nairflow-triggerer: # 172.28.0.9\nairflow-init: # 172.28.0.10\n\npostgres: # airflow가 기본 메타DB로 쓰고 있는 postgress 컨테이너에 포트 번호 5431로 노출\n\nnetworks에 172.28.xxx.xxx 같이 172.28 대역을 준 이유\n\n아래와 같이 sudo docker ps 를 실행해 container list를 보고 container id를 확인 하여 sudo docker inspect {container_id} or sudo docker inspect b739a3494646 실행해보면 다음과 같은 정보를 볼 수 있다.\n\n \"NetworkSettings\": {\n          \"Bridge\": \"\",\n          \"SandboxID\": \"b1bda217ebe565bfcf64b3c52e7fbf47032821894db5d97ef9f8f85db5ee57d3\",            \"HairpinMode\": false,\n          \"LinkLocalIPv6Address\": \"\",\n          \"LinkLocalIPv6PrefixLen\": 0,\n          \"Ports\": {},\n          \"SandboxKey\": \"/var/run/docker/netns/b1bda217ebe5\",\n          \"SecondaryIPAddresses\": null,\n          \"SecondaryIPv6Addresses\": null,\n          \"EndpointID\": \"\",\n          \"Gateway\": \"\", # custom-network로 지정해주기전엔 여기 172.19.0.1 로 되어 있었음\n          \"GlobalIPv6Address\": \"\",\n          \"GlobalIPv6PrefixLen\": 0,\n          \"IPAddress\": \"\", # custom-network로 지정해주기전엔 여기 172.19.0.6 로 되어 있었음\n          \"IPPrefixLen\": 0,\n          \"IPv6Gateway\": \"\",\n          \"MacAddress\": \"\",\n          \"Networks\": {\n              \"airflow_network_custom\": {\n                  \"IPAMConfig\": {\n                      \"IPv4Address\": \"172.28.0.8\"\n                  },\n                  \"Links\": null,\n                  \"Aliases\": [\n                      \"airflow-airflow-worker-1\",\n                      \"airflow-worker\",\n                      \"b739a3494646\"\n                  ],\n                  \"NetworkID\": \"eb43aaa125bcf1aac0fd512057de947abafd9397dd0d51cf7f49582c8c7d5eb9\",\n                  \"EndpointID\": \"\",\n                  \"Gateway\": \"\",\n                  \"IPAddress\": \"\",\n                  \"IPPrefixLen\": 0,\n                  \"IPv6Gateway\": \"\",\n                  \"GlobalIPv6Address\": \"\",\n                  \"GlobalIPv6PrefixLen\": 0,\n                  \"MacAddress\": \"\",\n                  \"DriverOpts\": null\n              }\n          }\n      }\n\n현재 설치되어 있는 networks list 보기 : sudo docker network ls\n\nNETWORK ID     NAME                     DRIVER    SCOPE\n260163833c67   airflow_default          bridge    local\neb43aaa125bc   airflow_network_custom   bridge    local\n1543a7e87603   bridge                   bridge    local\n9d0e4a4e52ce   host                     host      local\n5c1f555d034f   none                     null      local\n\nworker container는 “NetworkID”: “eb43aaa125bcf1aac0fd512057de947abafd9397dd0d51cf7f49582c8c7d5eb9” 에서 앞 부분이 eb43aaa125bc 이기 때문에 airflow_network_custom을 사용하는 것을 볼 수 있다. (networks 지정 전에는 default에 있음)\n원래 default network는 IP를 172.19.xxx.xxx 대역을 쓰기 때문에 networkd대역을 만들때는 179.19대역은 피해야 한다. network custom에서는 안전하게 172.29.xxx.xxx IP 주소가 충돌이 되지 않도록 29로 설정한다.\n\nVolume 현황 보기 : sudo docker volume ls\nDRIVER    VOLUME NAME\nlocal     1e0ab35524a66be9d849f574436e148479f9af7dd76c763cd4dac2ac147aba3c # docker가 알아서 만든 volume\nlocal     2f2593b44a32b1474400a100216ccc0e9658b99b1cc0c83ad993bc3bb387d4ba # docker가 알아서 만든 volume \nlocal     4ae485474d6168c4b62c3bd6ba1b6cd130576d49e733314f3802e36ad34f47a2 # docker가 알아서 만든 volume\nlocal     4b80894138e5d2ed6b8d5a99723ac747e4325d215e04a81e7ec6254581758109 # docker가 알아서 만든 volume\nlocal     8bec2d1d658ea13617520f12f1d73981ee2cb7740bee4f3b10ce0aa1e565d0ee # docker가 알아서 만든 volume\nlocal     91a25038cbe4cec1757bb4ccb35b7a91d73000dfa501c45aa9ebebba351f4882 # docker가 알아서 만든 volume\nlocal     0315bc7a7513fc0480ae1b220b3f3022d1df134f21c31a3b05282960ea58b820 # docker가 알아서 만든 volume\nlocal     884de047d72dc84fcf02c7f2dba0c3c5ca6e4d2ef2eb7c2f471be32740ca6949 # docker가 알아서 만든 volume\nlocal     7300ab83ca5c136dbd95e2d969e5d7d8e09c285c169aaf7789d396d12a940b7a # docker가 알아서 만든 volume\nlocal     9619310c2f4f3e281b2bd49626cb7ffb157b97946e25bc2d84ea6a27b3842d7e # docker가 알아서 만든 volume\nlocal     94856658cd6a9ba830bdfd39a73fcf6737cdf82707454c3da2ea6f59c1599ce2 # docker가 알아서 만든 volume\nlocal     a9f0f6f5f1f4b83a4c1af47aef4ff8f0692cd21eed9db9cde77b08538ccd55a3 # docker가 알아서 만든 volume\nlocal     a97aa8ddd6bf1224517f37eaeee7425dbfcce1b144ec5c0f3a5bcab47cb80f69 # docker가 알아서 만든 volume\nlocal     ac91a112c4c03b110775e6ff6cdf8ae774f0376836501f96eb95130594038ec9 # docker가 알아서 만든 volume\nlocal     airflow_postgres-custom-db-volume # 내가 만든 volume\nlocal     airflow_postgres-db-volume # airflow가 postgres container를 실행하면서 만들었던 metaDB를 위한 volume\nlocal     b3b8ab88bbaa69adc798f5fbeebe75dd4d4e47843e9e2861922193695e614926 # docker가 알아서 만든 volume \nlocal     b41f5d39b0778ca5efdc714a54ae103503cb8a96778cd9bdd19ecf5857e92e85 # docker가 알아서 만든 volume\nlocal     c5c7439b17427b11aceee49239ed8f3f4805a9c531cf8a3673a635b2f17cc3ec # docker가 알아서 만든 volume\nlocal     d31b1c160d8127fab58d1585a44b62498c4f0ae5c42962d68cd725cffe9fdd2d # docker가 알아서 만든 volume\nlocal     f928be82c57a041a2e02e43b81e6e1280b00c71b39a12bb05ff9a1dd9d1ddb32 # docker가 알아서 만든 volume\n\nvolume detail 보기 : sudo docker volume inspect airflow_postgres-custom-db-volume\n\n[\n  {\n      \"CreatedAt\": \"2023-07-01T10:38:26+09:00\",\n      \"Driver\": \"local\",\n      \"Labels\": {\n          \"com.docker.compose.project\": \"airflow\",\n          \"com.docker.compose.version\": \"2.18.1\",\n          \"com.docker.compose.volume\": \"postgres-custom-db-volume\"\n      },\n      \"Mountpoint\": \"/var/lib/docker/volumes/airflow_postgres-custom-db-volume/_data\",\n      \"Name\": \"airflow_postgres-custom-db-volume\",\n      \"Options\": null,\n      \"Scope\": \"local\"\n  }\n]\n\nvolume의 위치: “/var/lib/docker/volumes/airflow_postgres-custom-db-volume/_data”\n\nsudo ls /var/lib/docker/volumes/airflow_postgres-custom-db-volume/_data 실행하면 postgres container가 쓰고있는 file list를 확인할 수 있다.\n\nPG_VERSION  global        pg_dynshmem  pg_ident.conf  pg_multixact  pg_replslot  pg_snapshots  pg_stat_tmp  pg_tblspc    pg_wal   postgresql.auto.conf  postmaster.opts   base  pg_commit_ts  pg_hba.conf  pg_logical     pg_notify     pg_serial    pg_stat       pg_subtrans  pg_twophase  pg_xact  postgresql.conf       postmaster.pid\n\n\nDbeaver 설치 https://dbeaver.io/\ncommunity versiono 설치하면 됨 &gt;&gt; windows installer download 받아 설치\n\n\n\n \n\n3번: localhost 자체가 local의 wsl을 의미하기 때문에 그대로 놔두면 됨\n4번: port는 docker_compose.yaml에 설정된 port번호 자동으로 입력되서 나옴\n5번: kmkim (docker_compose.yaml 설정대로 바꿔야함)\n6번: kmkim (docker_compose.yaml 설정대로 바꿔야함)\n\n\n\n\n\n\n\nAirflow MetaDB Connection\n\n\n\n1번: docker_compose.yaml 설정대로 5431이 나옴\n2번: database 이름은 airflow\n3번: username은 airflow\n4번: password는 airflow\n\n\n\n\n\n\n\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n    dag_id='dags_python_with_postgres',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n\n    \n    def insrt_postgres(ip, port, dbname, user, passwd, **kwargs):\n        import psycopg2 # postgres DB에 접속해서 sql query 를 날리고 결과를 가지고 올수있게 해주는 library\n        from contextlib import closing\n\n        with closing(psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))) as conn:\n            with closing(conn.cursor()) as cursor:\n                dag_id = kwargs.get('ti').dag_id\n                task_id = kwargs.get('ti').task_id\n                run_id = kwargs.get('ti').run_id\n                msg = 'insrt 수행'\n                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'\n                cursor.execute(sql,(dag_id,task_id,run_id,msg))\n                conn.commit()\n\n    insrt_postgres = PythonOperator(\n        task_id='insrt_postgres',\n        python_callable=insrt_postgres,\n        op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']\n    )\n        \n    insrt_postgres\n\npsycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port)) : DB server와의 연결 (Session)\n\nsession: TCP/IP 기반의 connection\n\nclosing() 은 psycopg2.connect() 객체를 닫아주는 역할\n\n예를 들어, with() as ~ statement없이 DB server에 연결하는 명령어는 다음과 같다\n\nconn = psycopg2.connect()\n~~~\nconn.close()\n여기서 conn.close()의 기능을 하는 명령어가 closing() 이다\nwith closing(psycopg2.connect(...)) as conn:\nconn session (con object)에서 sql을 이용한 구체적인 query 내용은 두 번째 with문에서 기술\n with closing(conn.cursor()) as cursor:\n              dag_id = kwargs.get('ti').dag_id # task instance object 에서 dag_id (property) 호출\n              task_id = kwargs.get('ti').task_id # task instance object 에서 task_id (property) 호출\n              run_id = kwargs.get('ti').run_id # task instance object 에서 run_id (property) 호출\n              msg = 'insrt 수행'\n              sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);' \n              cursor.execute(sql,(dag_id,task_id,run_id,msg)) # 실제 sql 실행하는 부분\n              conn.commit()\n\ncursor: client \\(\\xleftrightarrow{\\text{cursor}}\\) DB server. client(python로직을 호출하는 worker container)와 DB서버(container) 사이의 session(client와 DB 서버와의 connection 역할)안에서 client에서 query를 날리고 DB서버로부터 결과를 가져와주는 object. 이 cursor에서 sql 수행. 그러므로 cursor (=conn.cursor())가 없으면 query 수행을 못함\ncursor.execute(sql,(dag_id,task_id,run_id,msg)): 실제 sql 실행하는 부분\npy_opr_drct_insrt : 테이블 이름\nvalues (%s,%s,%s,%s) 4개의 값 binding: dag_id, task_id, run_id, msg. 즉, cursor.execute(sql,(dag_id,task_id,run_id,msg))의 dag_id,task_id,run_id,msg 과 연결\n\nconn이 끝나면 첫 번째 with문의 closing()이 session을 닫아줌\n위의 코드가 아래의 코드와 같은 내용임\nconn=psycopg2.connect(...)\n~~~\nconn.close()\ntask 수행\n\n  insrt_postgres = PythonOperator(\n        task_id='insrt_postgres',\n        python_callable=insrt_postgres,\n        op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']\n    )\n        \n    insrt_postgres\n\ndef insrt_postgres(ip, port, dbname, user, passwd, **kwargs) 에서 보듯이\n\nip = ‘172.28.0.3’\nport = ‘5422’\ndbname = ‘hjkim’\npasswd = ‘hjkim’\n**kwargs = NULL\n\npostgres DB에 table 만들기\n\nDBeaver Open &gt;&gt; kmkim databse 우클릭&gt;&gt; SQL editor &gt;&gt; New SQL Script &gt;&gt; py_opr_drct_insrt table 생성\n\ncreate table py_opr_drct_insrt(\n  dag_id varchar(100),\n  task_id varchar(100),\n  run_id varchar(100),\n  msg text # 가변길이 type\n)\n\ntable 확인: kmkim &gt;&gt; Databases &gt;&gt; Schemas &gt;&gt; public\n\nDAG full example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n    dag_id='dags_python_with_postgres',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n\n    \n    def insrt_postgres(ip, port, dbname, user, passwd, **kwargs):\n        import psycopg2\n        from contextlib import closing\n\n        with closing(psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))) as conn:\n            with closing(conn.cursor()) as cursor:\n                dag_id = kwargs.get('ti').dag_id\n                task_id = kwargs.get('ti').task_id\n                run_id = kwargs.get('ti').run_id\n                msg = 'insrt 수행'\n                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'\n                cursor.execute(sql,(dag_id,task_id,run_id,msg))\n                conn.commit()\n\n    insrt_postgres = PythonOperator(\n        task_id='insrt_postgres',\n        python_callable=insrt_postgres,\n        op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']\n    )\n        \n    insrt_postgres\n\n\n\n\n문제점\n\n접속정보 노출: postgres DB에 대한 User, Password 등\n\nDAG을 열어볼 수 있는 사람이나 github에 접속할 수 있는 사람은 모두 볼 수 있음\n\n접속정보 변경시 대응 어려움\n\n만약 직접 접속하는 DAG이 수백개라면?\n\n\n해결 방법\n\nVariable 이용 (User, Password 등을 Variable에 등록하고 꺼내오기) - 번거롭기 때문에 권장하는 방식은 아님\nHook 이용 (Variable 등록 필요없음)\n\n\n\n\n\n\nConnection\n\nAirflow UI 화면에서 등록한 커넥션 정보\n\nHook의 개념\n\nAirflow에서 외부 솔루션의 기능을 사용할 수 있도록 미리 구현된 메서드를 가진 클래스\n\nHook의 특징\n\nConnection 정보를 통해 생성되는 객체로 Hook을 사용하기 위해선 먼저 connection을 등록해야한다.\n\n접속정보를 Connection을 통해 받아오므로 접속정보가 코드상 노출되지 않음\n\n특정 솔루션을 다룰 수 있는 메서드가 구현되어 있음.\noperator나 sensor와는 달리 Hook은 task를 만들어내지 못하므로 Custom operator 안에서나 Python operator 내 함수에서 사용됨\n\n\n\n\n\n\nairflow web ui &gt;&gt; admin &gt;&gt; connections &gt;&gt; plus button &gt;&gt;\n\n\n\n\nConnection_id\nconn-db-postgres-custom\n\n\n\n\nConnection_type\npostgres\n\n\nHost\n172.28.0.3\n\n\nSchema\nkmkim\n\n\nLogin\nkmkim\n\n\nPassword\nkmkim\n\n\nPort\n5432\n\n\n\n\n\n\n\nairflow web service &gt;&gt; Providers &gt;&gt; apache.airflow.providers.postgres &gt;&gt; Python API &gt;&gt; airflow.providers.postgres.hooks.postgres &gt;&gt; get_conn() &gt;&gt; [source] &gt;&gt; def get_conn(self)\n제공하는 함수의 source code를 잘 관찰하고 custom object(custom operator, custom sensor, custom hook 등)를 만드는 것에 익숙해져야 airflow를 잘 활용할 수 있다.\n\ndef get_conn(self) -&gt; connection:\n        \"\"\"Establishes a connection to a postgres database.\"\"\"\n        conn_id = getattr(self, self.conn_name_attr)\n        conn = deepcopy(self.connection or self.get_connection(conn_id))\n\n        # check for authentication via AWS IAM\n        if conn.extra_dejson.get(\"iam\", False):\n            conn.login, conn.password, conn.port = self.get_iam_token(conn)\n\n        conn_args = dict(\n            host=conn.host,\n            user=conn.login,\n            password=conn.password,\n            dbname=self.database or conn.schema,\n            port=conn.port,\n        )\n        raw_cursor = conn.extra_dejson.get(\"cursor\", False)\n        if raw_cursor:\n            conn_args[\"cursor_factory\"] = self._get_cursor(raw_cursor)\n\n        for arg_name, arg_val in conn.extra_dejson.items():\n            if arg_name not in [\n                \"iam\",\n                \"redshift\",\n                \"cursor\",\n                \"cluster-identifier\",\n                \"aws_conn_id\",\n            ]:\n                conn_args[arg_name] = arg_val\n\n        self.conn = psycopg2.connect(**conn_args)\n        return self.conn\n\nconn = deepcopy(self.connection or self.get_connection(conn_id)) 를 보면 hook 클래스의 get_connection() method를 이용해 airflow web ui에서 입력했던 connection 입력값을 찾아내서 연결시켜줌\n\n\n\n\n\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n    dag_id='dags_python_with_postgres',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n\n    \n    def insrt_postgres(ip, port, dbname, user, passwd, **kwargs):\n        import psycopg2\n        from contextlib import closing\n\n        with closing(psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))) as conn:\n            with closing(conn.cursor()) as cursor:\n                dag_id = kwargs.get('ti').dag_id\n                task_id = kwargs.get('ti').task_id\n                run_id = kwargs.get('ti').run_id\n                msg = 'insrt 수행'\n                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'\n                cursor.execute(sql,(dag_id,task_id,run_id,msg))\n                conn.commit()\n\n    insrt_postgres = PythonOperator(\n        task_id='insrt_postgres',\n        python_callable=insrt_postgres,\n        op_args=['172.28.0.3', '5432', 'kmkim', 'kmkim', 'kmkim']\n    )\n        \n    insrt_postgres\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n        dag_id='dags_python_with_postgres_hook',\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        schedule=None,\n        catchup=False\n) as dag:\n    def insrt_postgres(postgres_conn_id, **kwargs):\n        from airflow.providers.postgres.hooks.postgres import PostgresHook\n        from contextlib import closing\n        \n        postgres_hook = PostgresHook(postgres_conn_id)\n        with closing(postgres_hook.get_conn()) as conn:\n            with closing(conn.cursor()) as cursor:\n                dag_id = kwargs.get('ti').dag_id\n                task_id = kwargs.get('ti').task_id\n                run_id = kwargs.get('ti').run_id\n                msg = 'hook insrt 수행'\n                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'\n                cursor.execute(sql, (dag_id, task_id, run_id, msg))\n                conn.commit()\n\n    insrt_postgres_with_hook = PythonOperator(\n        task_id='insrt_postgres_with_hook',\n        python_callable=insrt_postgres,\n        op_kwargs={'postgres_conn_id':'conn-db-postgres-custom'}\n    )\n    insrt_postgres_with_hook\n\n\n\n왼쪽과 오른쪽 conn 객체와 cursor는 사실상 같음. 단지 만들어지는 과정만 달라짐.\nop_args=['172.28.0.3', '5432', 'kmkim', 'kmkim', 'kmkim'] 와 같은 보안 사항이 오른 쪽 코드에서는 op_kwargs={'postgres_conn_id':'conn-db-postgres-custom'} 가려지게 된다.\n\n\n\n\n\n\nbulk_load: Hook은 특정 solution을 제어할 수 있도록 method\n\n\n\n\nairflow web service &gt;&gt; Providers &gt;&gt; apache.airflow.providers.postgres &gt;&gt; Python API &gt;&gt; airflow.providers.postgres.hooks.postgres &gt;&gt; def bulk_load(self, temp_file)[source]\nbulk_load(): Loads a tab-delimited file into a database table. 설명이 불충분하여 다음과 같은 사항을 확인할 수 없다.\n\n꼭 delimiter가 tab이어야 하는지?\ntemp_file에 header가 있으면 header가 있는 상태로 data를 올려도 되는지?\nDB table이 없으면 만들어지면서 올라가는지? 아니면 사전에 만들어 놔야하는지?\ntable에 기존 data가 있다면 truncate되면서 올라가는지? append되면서 올라가는지?\nparameter는 구체적으로 어떻게 입력해야하는지?\n\nbulk_load() 의 명세서를 확인해야 한다. (source code)\n\n[docs]    def bulk_load(self, table: str, tmp_file: str) -&gt; None:\n        \"\"\"Loads a tab-delimited file into a database table\"\"\"\n        self.copy_expert(f\"COPY {table} FROM STDIN\", tmp_file)\n\ncopy_expert(): postgres hook class가 갖고 있는 method\nself.copy_expert(f”COPY {table} FROM STDIN”, tmp_file)\n\n[docs]    def copy_expert(self, sql: str, filename: str) -&gt; None:\n        \"\"\"\n        Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.\n        \"\"\"\n        self.log.info(\"Running copy expert: %s, filename: %s\", sql, filename)\n        if not os.path.isfile(filename):\n            with open(filename, \"w\"):\n                pass\n\n        with open(filename, \"r+\") as file:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, file)\n                    file.truncate(file.tell())\n                    conn.commit()\n\nclosing(self.get_conn()) as conn: &gt;&gt; with closing(conn.cursor()) as cur:\n\nget.conn() &gt;&gt; cursor() 결국 postgres의 cursor로 postgres가 원래 갖고있는 copy_export() method를 이용\n그럼 postgres의 copy_export()의 source code를 확인해 봐야함. google psycopg2 cursor.copy_expert\n\ncopy_expert(sql, file, size=8192)\n  Submit a user-composed COPY statement. The method is useful to handle all the parameters that PostgreSQL makes available (see COPY command documentation).\n\n  Parameters:\n  sql – the COPY statement to execute.\n\n  file – a file-like object to read or write (according to sql).\n\n  size – size of the read buffer to be used in COPY FROM.\n\n  The sql statement should be in the form COPY table TO STDOUT to export table to the file object passed as argument or COPY table FROM STDIN to import the content of the file object into table. If you need to compose a COPY statement dynamically (because table, fields, or query parameters are in Python variables) you may use the objects provided by the psycopg2.sql module.\n\n  file must be a readable file-like object (as required by copy_from()) for sql statement COPY ... FROM STDIN or a writable one (as required by copy_to()) for COPY ... TO STDOUT.\n\n  Example:\n\n  &gt;&gt;&gt; cur.copy_expert(\"COPY test TO STDOUT WITH CSV HEADER\", sys.stdout)\n  id,num,data\n  1,100,abc'def\n  2,,dada\n  ...\n  New in version 2.0.6.\n\n  Changed in version 2.4: files implementing the io.TextIOBase interface are dealt with using Unicode data instead of bytes.\n\n이것이 가장 세부적인 정보로 나머지 정보는 troubleshooting으로 파악해야한다.\n\n꼭 delimiter가 tab이어야 하는지? troubleshooting으로 확인해야함\ntemp_file에 header가 있으면 header가 있는 상태로 data를 올려도 되는지? troubleshooting으로 확인해야함\nDB table이 없으면 만들어지면서 올라가는지? 아니면 사전에 만들어 놔야하는지? troubleshooting으로 확인해야함\ntable에 기존 data가 있다면 truncate되면서 올라가는지? troubleshooting으로 확인해야함\n\n\ntrouble shooting 할 DAG\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\n\nwith DAG(\n        dag_id='dags_python_with_postgres_hook_bulk_load',\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        schedule='0 7 * * *', # 서울시 공공데이터를 API를 통해 불러들이(dags_seoul_api_corona.py)는 시간이 7시\n        catchup=False\n) as dag:\n    def insrt_postgres(postgres_conn_id, tbl_nm, file_nm, **kwargs):\n        postgres_hook = PostgresHook(postgres_conn_id)\n        postgres_hook.bulk_load(tbl_nm, file_nm)\n\n    insrt_postgres = PythonOperator(\n        task_id='insrt_postgres',\n        python_callable=insrt_postgres,\n        op_kwargs={'postgres_conn_id': 'conn-db-postgres-custom',\n                   'tbl_nm':'TbCorona19CountStatus_bulk1',\n                   'file_nm':'/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash}}/TbCorona19CountStatus.csv'}\n    )\n\nop_kwargs={‘postgres_conn_id’: ‘conn-db-postgres-custom’, ‘tbl_nm’:‘TbCorona19CountStatus_bulk1’, ‘file_nm’:‘/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(“Asia/Seoul”) | ds_nodash}}/TbCorona19CountStatus.csv’} 는 dags_seoul_api_corona.py 의 tb_corona19_count_status task의 path와 file_name 인수로 부터 가져온다.\ntb_corona19_count_status = SeoulApiToCsvOperator( task_id=‘tb_corona19_count_status’, dataset_nm=‘TbCorona19CountStatus’, path=‘/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(“Asia/Seoul”) | ds_nodash }}’, file_name=‘TbCorona19CountStatus.csv’ )\ntroubleshooting\n\ntable이 사전에 만들어지지 않아도 되는지? 현재 DB에는 TbCorona19CountStatus_bulk1 table이 없음 \n실행 결과 log 에서 table이 없다는 에러 메세지 뜸. table 만들어 주면 됨\ntable만들어서 한번 더 task 실행하면 comma delimiter 인식 오류가 나기 떄문에 tab delimiter로 바꿔줘야한다.\n\ndags_seoul_api_corona.py 로 부터 받은 TbCorona19CountStatus.csv 파일을 열어 ,를 tab으로 바꿔준다\n\nvi TbCorona19CountStatus.csv\nvi editor\n\n: %s/,/\\t/g ,를 \\t으로 바꿔줌. 여기서 g는 global하게 적용하겠다는 의미\n\n51번째 line에 에러 발생: \\M 이라는 특수 문자가 있음\n\n\\M 윈도우와 리눅스 간의 줄넘김 차이 때문에 발생\n\n윈도우: enter key = CR (Carriage Return-한줄에서 왼쪽 끝으로 밀어주는것이 CR) + LF (Line Feed-다음 줄에 입력을 하도록 종이를 한줄 밀어주는 것 LF)\nDOS/Windows 계열에서는 엔터를 CR+LF(\\r\\n) 으로 처리하고\nUnix/Linux 계열에서는 엔터를 LF(\\n)으로 처리하고\nMAC 계열에서는 엔터를 CR(\\r)로 처리한다고 한다\n윈도우 환경에서 입력된 값이 리눅스로 넘어오게 될 때 CR+LF와 같은 불일치 값이 있으면 ^M 또는 \\M 로 표시됨\n\n51번째 record 지워서 해결\n\nairflow 상 errors는 더이상 발생하지 않지만 DB를 확인했을 때 첫번째 row에 column명이 들어간것을 확인되었다. 그래서 CSV상에 column값을 지워줘야한다는 것을 알 수 있다.\n\n\n\n\n\n\n문제점\n\nLoad 가능한 Delimiter는 Tab으로 고정되어 있음\nHeader까지 포함해서 업로드됨\n특수문자로 인해 파싱이 안될 경우 에러 발생\n\n개선방안\n\nCustom Hook 을 만들어서 Delimiter 유형을 입력받게 하고\nHeader 포함 여부를 선택하게끔 하며\n특수문자를 제거하는 로직을 추가 후\nsqlalchemy(python에서 DB 작업을 편리하게 해주는 library)를 이용하여 Load 한다면? 그리고 테이블을 생성하면서 업로드할 수 있도록 한다.\n\n\n\n\n\n\n\n\n\nCustom Hook은 BaseHook을 상속해서 작성\nAirflow Docs에서 Basehook Source Code\nBaseHook class의 methods\n\n[docs] def get_connection(cls, conn_id: str) -&gt; Connection:\n\nairflow 상에서 만들었던 connection_id : user name, password, IP, Port 의 정보를 담고있는 object return\n\n[docs] def get_connections(cls, conn_id: str) -&gt; list[Connection]:\n\nwill be deprecated. Use get_connection()\n\n[docs] def get_conn(self) -&gt; Any: # 이 함수를 쓰려면 상속받아서 구현할 때 get_conn() 함수를 구현해야함\n[docs] def get_hook(cls, conn_id: str) -&gt; BaseHook:\n[docs] def get_connection_form_widgets(cls) -&gt; dict[str, Any]: (안중요)\n[docs] def get_ui_field_behaviour(cls) -&gt; dict[str, Any]: (안중요)\n\n위의 method 중 get_conn()을 제외하곤 모든 method에 데코레이터 @classmethod 있음\n\npython있는 method 종류로 class method 라 하고 class method 는 class를 객체화 시키지 않고도 바로 호출할 수 있음\n예를 들어, 다음과 같은 방식으로 class method 호출 안해도 됨\nimport BaseHook\na=BaseHook()\na.get_connection(conn_id)\n바로 호출 해도됨\nimport BaseHook\nBaseHook.get_connection(conn_id)\n\n\n\n\n\n\n해야할 일\n\nget_conn 메서드 구현하기\n\nDB 와의 연결 세션 객체인 conn 을 리턴하도록 구현\n주의: get_connection() vs get_conn()\n\nget_connection(): Airflow 에서 등록한 Connection 정보를 담은 conn을 return\nget_conn(): postgres와의 연결하는 session 객체를 return\nBaseHook 의 추상 메서드 , 자식 클래스에서 구현 필요\n\n\nbulk_load 메서드 구현하기\n\n입맛대로 만들기: custom_postgres_hook.py\n\nfrom airflow.hooks.base import BaseHook\nimport psycopg2\nimport pandas as pd\n\nclass CustomPostgresHook(BaseHook):\n\n    # 생성자\n    def __init__(self, postgres_conn_id, **kwargs): # 입력은 하나만: postgres_conn_id\n        self.postgres_conn_id = postgres_conn_id\n\n    def get_conn(self):\n        airflow_conn = BaseHook.get_connection(self.postgres_conn_id) #class method라 바로 호출\n        # 아래의 보안 정보들이 hook을 통해서 노출되지 않고 접근 가능\n        self.host = airflow_conn.host\n        self.user = airflow_conn.login\n        self.password = airflow_conn.password\n        self.dbname = airflow_conn.schema\n        self.port = airflow_conn.port\n\n        # postgres DB 연결: session object를 return\n        self.postgres_conn = psycopg2.connect(host=self.host, user=self.user, password=self.password, dbname=self.dbname, port=self.port)\n        return self.postgres_conn\n\n    def bulk_load(self, table_name, file_name, delimiter: str, is_header: bool, is_replace: bool):\n\n        from sqlalchemy import create_engine\n\n        self.log.info('적재 대상파일:' + file_name)\n        self.log.info('테이블 :' + table_name)\n        self.get_conn()\n        header = 0 if is_header else None                       # is_header = True면 0, False면 None\n        if_exists = 'replace' if is_replace else 'append'       # is_replace = True면 replace, False면 append\n        file_df = pd.read_csv(file_name, header=header, delimiter=delimiter)\n\n        for col in file_df.columns:                             \n            try:\n                # string 문자열이 아닐 경우 continue\n                file_df[col] = file_df[col].str.replace('\\r\\n','')      # 줄넘김 및 ^M 제거\n                self.log.info(f'{table_name}.{col}: 개행문자 제거')\n            except:\n                continue \n\n        self.log.info('적재 건수:' + str(len(file_df)))\n        uri = f'postgresql://{self.user}:{self.password}@{self.host}/{self.dbname}'\n        engine = create_engine(uri)\n        file_df.to_sql(name=table_name,\n                            con=engine,\n                            schema='public',\n                            if_exists=if_exists,\n                            index=False\n                        )\n\n여기서 airflow_conn = BaseHook.get_connection(self.postgres_conn_id) #class method라 바로 호출 와 self.postgres_conn = psycopg2.connect(host=self.host, user=self.user, password=self.password, dbname=self.dbname, port=self.port) 다른 종류의 conn 객체를 return한다.\n\n\n\n\n\n\n\n현재 설치되어 있는 Providers 패키지 확인\n\n웹의 Admin Providers 에서 확인 가능\n\n설치 가능한 Providers 더 보기 &gt;&gt; Apache Airflow 2 is built in modular way. The “Core” of Apache Airflow provides core scheduler functionality which allow you to write some basic tasks, but the capabilities of Apache Airflow can be extended by installing additional packages, called providers.\n\nproviders package를 설치하면 다른 솔루션을 연동할 수 있도록 확장성을 제공\nThe full list of community managed providers is available at Providers Index.\n\nProviders packages:\n\nApache Hive\n\n\n\n\n\n\n\n\nAirflow Connection type 목록에 있는 대상은 이미 패키지 설치가 된 Providers 이며 Admin&gt;&gt;Providers 목록에서 설치된 대상 확인 가능\n만약 Hive 에 대한 커넥션을 추가하고 싶은데 Airflow Connection type 목록에 Hive가 없다면 관련된 package를 설치하여 본인이 직접 추가해야함\n\nProvider 검색, py 라이브러리 설치 목록 확인하여 pip install [pkg name] 실행\n\n주의사항: 윈도우와 wsl2에 package를 설치하는게 아니라 airflow containers에 설치해줘야 함\n\nscheduler\nworker\nwebserver\ntriggerer\n\n하지만, 각 각의 container에다가 pkg를 설치해주면 container가 꺼지게 되면 지워지게 됨\n그래서, custom한 docker image를 만들어야 함\n\nAirflow 이미지 Extend 방법으로 custom image 만들기\n\nbase image 에다가 custom image (pip install 및 다른 여러가지 layers) 추가\n즉, custom image = base image + layer1 (library 호출) +layer2 (pip install pkgs)+ \\(\\ldots\\)\n이런 방식의 custom image는 layer가 많아질 수록 무거워져 overhead 가 커지는 약점이 있다.\n\n애초에 base image 자체를 custom image 로 만드는 법\n\nextend의 약점인 overhead를 어느 정도 줄일 수 있지만 개발하는데 시간이 걸림\n\n\n\n\nAirflow 이미지 Extend 방법 & custom image 만드는 법 확인\n\n이미지 Extend vs Custom 이미지 생성\n\n\n\n\n\n\n\n\n\nComparison\nimage extend\nCustom Image Creation\n\n\n\n\n간단히 생성 가능\nO\nX (많은 source codes 필요)\n\n\n빌드 시간\n짧음 (5분 이내)\n상대적으로 긺\n\n\n크기 최적화된 이미지\nX\nO (약 20% 정도 사이즈 감소)\n\n\n폐쇄망에서 구성 가능\nX (인터넷이 되어야함)\nO\n\n\n\nAirflow web에 connection type 추가하는 steps\n\n이미지 extend를 위한 Dockerfile 만들기\n:::: {.columns}\n\n\n#&gt; cd; cd airflow\n#&gt; mkdir -p custom_image/airflow\n#&gt; cd custom_image/airflow\n#&gt; vi Dockerfile\n\n\nFROM apache/airflow:2.5.1 #base image 지정\nUSER root #root user\nRUN apt get update \\\n  && apt-get install -y --no-install-recommends \\\n    gcc \\ #library 1 for installing hdfs\n    heimdal-dev \\ #library 2 for installing hdfs\n    g++ \\ #library 3\n    libsasl2-dev \\ #library 4\n  && apt-get autoremove -yqq -purge \\\n  && apt-get clean \\\n  && rm -rf /var/lib/apt/lists/*\nUSER airflow\nRUN pip install \\\n  apache airflow providers apache hdfs \\\n  apache airflow providers apache hive\nRUN pip uninstall -y argparse\n\n::::\n\nDocker 이미지 생성하기\n\n#&gt; pwd \n#&gt; cd /home/hjkim/airflow/custom_image/airflow #Dockerfile 만든 경로\n#&gt; sudo docker build -t {image_name} . #.: 현재 디렉토리\n예) #&gt; sudo docker build -t airflow_custom .\nerror message\nkmkim@K100230201051:~/airflow/custom_image/airflow$ sudo docker build -t airflow_custom .\n\nfailed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-buildx: no such file or directory\n\nDEPRECATED: The legacy builder is deprecated and will be removed in a future release.\n            Install the buildx component to build images with BuildKit:\n            https://docs.docker.com/go/buildx/\n\nSending build context to Docker daemon  2.048kB\nStep 1/6 : FROM apache/airflow:2.6.1\n ---&gt; 52c34708e903\nStep 2/6 : USER root #root user\n ---&gt; Running in c3fe7d498d62\nRemoving intermediate container c3fe7d498d62\n ---&gt; d216e5376f4e\nStep 3/6 : RUN apt get update   && apt-get install -y --no-install-recommends     gcc     heimdal-dev     g++     libsasl2-dev     heimdal-dev   && apt-get autoremove -yqq -purge   && apt-get clean   && rm -rf /var/lib/apt/lists/*\n ---&gt; Running in 8238d95b680a\nunable to find user root #root user: no matching entries in passwd file\n\nDocker 이미지 확인\n\n#&gt; sudo docker image ls\n\ndocker-compose.yaml 수정하기\n\n#&gt; cd\n#&gt; vi docker-compose.yaml\n\nversion: '3.8'\nx-airflow-common:\n  &airflow-common\n# In order to add …\n  image: {image_name} #새로 만든 docker image 이름 넣을 것\n\ndocker compose (재) 기동\nConnection Type 에 추가 확인\n\nHDFS, Hive Client Wrapper, Hive Metastore Thrift, Hive Server 2 Thrift\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#postgress-컨테이너-추가하기",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#postgress-컨테이너-추가하기",
    "title": "Connection & Hook",
    "section": "",
    "text": "postgres_custom 이라는 이름의 컨테이너 서비스 추가하기\nnetworks를 만들어 컨테이너에 고정 IP 할당\nDBeaver로 postgres DB에 접속\n\nservices:\n  postgres_custom:\n    image: postgres:13\n    environment:\n      POSTGRES_USER: kmkim\n      POSTGRES_PASSWORD: kmkim\n      POSTGRES_DB: kmkim\n      TZ: Asia/Seoul\n    volumes:\n      - postgres-custom-db-volume:/var/lib/postgresql/data\n    ports:\n      - 5432:5432\n    networks:\n      network_custom: # 밑에서 정의한 network_custom 을 쓰겠다는 의미\n        ipv4_address: 172.28.0.3 # 할당된 IP\n\nnetworks:\n  network_custom: \n    driver: bridge\n    ipam:\n        driver: default\n        config:\n            - subnet: 172.28.0.0/16 # 네트 워크 ID 주소 밑에 16개/host id 주소 밑에 16개를 할당하겠다는 의미\n              gateway: 172.28.0.1 # default 네트워크 (172.18.0.0)가 쓰고 있지 않은 서브넷으로 구성\nvolumes:\n  postgres-db-volume:\n  postgres-custom-db-volume: #wsl의 어느 path에 mapping이 되어 있는지 확인할 것"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#컨테이너-고정-ip-할당하기",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#컨테이너-고정-ip-할당하기",
    "title": "Connection & Hook",
    "section": "",
    "text": "기본적으로 컨테이너들은 유동 IP를 지니며 (재기동시 IP 변경 가능)\npostgres DB에 접속하려면 고정 IP 필요\n고정 IP를 할당하려면 networks를 만들어서 할당해야 함.\nnetworks 를 지정하지 않은 컨테이너들(airflow를 설치하면서 기본적으로 설치되는 containers)은 default network에 묶이게 됨\n따라서 동일 네트워크에 두고 싶은 컨테이너들은 모두 동일 netwworks 할당 필요\n기존 containers와 custom container를 모두 custom networks를 바라보게 지정 필요\n\nnetworks:\n  network_custom: \n    driver: bridge\n    ipam:\n        driver: default\n        config:\n            - subnet: 172.28.0.0/16 # 네트 워크 ID 주소 밑에 16개/host id 주소 밑에 16개를 할당하겠다는 의미\n              gateway: 172.28.0.1 # default 네트워크 (172.18.0.0)가 쓰고 있지 않은 서브넷으로 구성\n\nPostgres_custom 컨테이너 뿐만 아니라 다른 컨테이너에도 network_custom 할당하고 IP 부여\npostgres_custom:  # 172.28.0.3\npostgres:  # 172.28.0.3 +  포트 노출 설정:(5431:5432)\nredis: # 172.28.0.5\nairflow-webserver: # 172.28.0.6\nairflow-scheduler: # 172.28.0.7\nairflow-worker: # 172.28.0.8\nairflow-triggerer: # 172.28.0.9\nairflow-init: # 172.28.0.10\n\npostgres: # airflow가 기본 메타DB로 쓰고 있는 postgress 컨테이너에 포트 번호 5431로 노출\n\nnetworks에 172.28.xxx.xxx 같이 172.28 대역을 준 이유\n\n아래와 같이 sudo docker ps 를 실행해 container list를 보고 container id를 확인 하여 sudo docker inspect {container_id} or sudo docker inspect b739a3494646 실행해보면 다음과 같은 정보를 볼 수 있다.\n\n \"NetworkSettings\": {\n          \"Bridge\": \"\",\n          \"SandboxID\": \"b1bda217ebe565bfcf64b3c52e7fbf47032821894db5d97ef9f8f85db5ee57d3\",            \"HairpinMode\": false,\n          \"LinkLocalIPv6Address\": \"\",\n          \"LinkLocalIPv6PrefixLen\": 0,\n          \"Ports\": {},\n          \"SandboxKey\": \"/var/run/docker/netns/b1bda217ebe5\",\n          \"SecondaryIPAddresses\": null,\n          \"SecondaryIPv6Addresses\": null,\n          \"EndpointID\": \"\",\n          \"Gateway\": \"\", # custom-network로 지정해주기전엔 여기 172.19.0.1 로 되어 있었음\n          \"GlobalIPv6Address\": \"\",\n          \"GlobalIPv6PrefixLen\": 0,\n          \"IPAddress\": \"\", # custom-network로 지정해주기전엔 여기 172.19.0.6 로 되어 있었음\n          \"IPPrefixLen\": 0,\n          \"IPv6Gateway\": \"\",\n          \"MacAddress\": \"\",\n          \"Networks\": {\n              \"airflow_network_custom\": {\n                  \"IPAMConfig\": {\n                      \"IPv4Address\": \"172.28.0.8\"\n                  },\n                  \"Links\": null,\n                  \"Aliases\": [\n                      \"airflow-airflow-worker-1\",\n                      \"airflow-worker\",\n                      \"b739a3494646\"\n                  ],\n                  \"NetworkID\": \"eb43aaa125bcf1aac0fd512057de947abafd9397dd0d51cf7f49582c8c7d5eb9\",\n                  \"EndpointID\": \"\",\n                  \"Gateway\": \"\",\n                  \"IPAddress\": \"\",\n                  \"IPPrefixLen\": 0,\n                  \"IPv6Gateway\": \"\",\n                  \"GlobalIPv6Address\": \"\",\n                  \"GlobalIPv6PrefixLen\": 0,\n                  \"MacAddress\": \"\",\n                  \"DriverOpts\": null\n              }\n          }\n      }\n\n현재 설치되어 있는 networks list 보기 : sudo docker network ls\n\nNETWORK ID     NAME                     DRIVER    SCOPE\n260163833c67   airflow_default          bridge    local\neb43aaa125bc   airflow_network_custom   bridge    local\n1543a7e87603   bridge                   bridge    local\n9d0e4a4e52ce   host                     host      local\n5c1f555d034f   none                     null      local\n\nworker container는 “NetworkID”: “eb43aaa125bcf1aac0fd512057de947abafd9397dd0d51cf7f49582c8c7d5eb9” 에서 앞 부분이 eb43aaa125bc 이기 때문에 airflow_network_custom을 사용하는 것을 볼 수 있다. (networks 지정 전에는 default에 있음)\n원래 default network는 IP를 172.19.xxx.xxx 대역을 쓰기 때문에 networkd대역을 만들때는 179.19대역은 피해야 한다. network custom에서는 안전하게 172.29.xxx.xxx IP 주소가 충돌이 되지 않도록 29로 설정한다.\n\nVolume 현황 보기 : sudo docker volume ls\nDRIVER    VOLUME NAME\nlocal     1e0ab35524a66be9d849f574436e148479f9af7dd76c763cd4dac2ac147aba3c # docker가 알아서 만든 volume\nlocal     2f2593b44a32b1474400a100216ccc0e9658b99b1cc0c83ad993bc3bb387d4ba # docker가 알아서 만든 volume \nlocal     4ae485474d6168c4b62c3bd6ba1b6cd130576d49e733314f3802e36ad34f47a2 # docker가 알아서 만든 volume\nlocal     4b80894138e5d2ed6b8d5a99723ac747e4325d215e04a81e7ec6254581758109 # docker가 알아서 만든 volume\nlocal     8bec2d1d658ea13617520f12f1d73981ee2cb7740bee4f3b10ce0aa1e565d0ee # docker가 알아서 만든 volume\nlocal     91a25038cbe4cec1757bb4ccb35b7a91d73000dfa501c45aa9ebebba351f4882 # docker가 알아서 만든 volume\nlocal     0315bc7a7513fc0480ae1b220b3f3022d1df134f21c31a3b05282960ea58b820 # docker가 알아서 만든 volume\nlocal     884de047d72dc84fcf02c7f2dba0c3c5ca6e4d2ef2eb7c2f471be32740ca6949 # docker가 알아서 만든 volume\nlocal     7300ab83ca5c136dbd95e2d969e5d7d8e09c285c169aaf7789d396d12a940b7a # docker가 알아서 만든 volume\nlocal     9619310c2f4f3e281b2bd49626cb7ffb157b97946e25bc2d84ea6a27b3842d7e # docker가 알아서 만든 volume\nlocal     94856658cd6a9ba830bdfd39a73fcf6737cdf82707454c3da2ea6f59c1599ce2 # docker가 알아서 만든 volume\nlocal     a9f0f6f5f1f4b83a4c1af47aef4ff8f0692cd21eed9db9cde77b08538ccd55a3 # docker가 알아서 만든 volume\nlocal     a97aa8ddd6bf1224517f37eaeee7425dbfcce1b144ec5c0f3a5bcab47cb80f69 # docker가 알아서 만든 volume\nlocal     ac91a112c4c03b110775e6ff6cdf8ae774f0376836501f96eb95130594038ec9 # docker가 알아서 만든 volume\nlocal     airflow_postgres-custom-db-volume # 내가 만든 volume\nlocal     airflow_postgres-db-volume # airflow가 postgres container를 실행하면서 만들었던 metaDB를 위한 volume\nlocal     b3b8ab88bbaa69adc798f5fbeebe75dd4d4e47843e9e2861922193695e614926 # docker가 알아서 만든 volume \nlocal     b41f5d39b0778ca5efdc714a54ae103503cb8a96778cd9bdd19ecf5857e92e85 # docker가 알아서 만든 volume\nlocal     c5c7439b17427b11aceee49239ed8f3f4805a9c531cf8a3673a635b2f17cc3ec # docker가 알아서 만든 volume\nlocal     d31b1c160d8127fab58d1585a44b62498c4f0ae5c42962d68cd725cffe9fdd2d # docker가 알아서 만든 volume\nlocal     f928be82c57a041a2e02e43b81e6e1280b00c71b39a12bb05ff9a1dd9d1ddb32 # docker가 알아서 만든 volume\n\nvolume detail 보기 : sudo docker volume inspect airflow_postgres-custom-db-volume\n\n[\n  {\n      \"CreatedAt\": \"2023-07-01T10:38:26+09:00\",\n      \"Driver\": \"local\",\n      \"Labels\": {\n          \"com.docker.compose.project\": \"airflow\",\n          \"com.docker.compose.version\": \"2.18.1\",\n          \"com.docker.compose.volume\": \"postgres-custom-db-volume\"\n      },\n      \"Mountpoint\": \"/var/lib/docker/volumes/airflow_postgres-custom-db-volume/_data\",\n      \"Name\": \"airflow_postgres-custom-db-volume\",\n      \"Options\": null,\n      \"Scope\": \"local\"\n  }\n]\n\nvolume의 위치: “/var/lib/docker/volumes/airflow_postgres-custom-db-volume/_data”\n\nsudo ls /var/lib/docker/volumes/airflow_postgres-custom-db-volume/_data 실행하면 postgres container가 쓰고있는 file list를 확인할 수 있다.\n\nPG_VERSION  global        pg_dynshmem  pg_ident.conf  pg_multixact  pg_replslot  pg_snapshots  pg_stat_tmp  pg_tblspc    pg_wal   postgresql.auto.conf  postmaster.opts   base  pg_commit_ts  pg_hba.conf  pg_logical     pg_notify     pg_serial    pg_stat       pg_subtrans  pg_twophase  pg_xact  postgresql.conf       postmaster.pid\n\n\nDbeaver 설치 https://dbeaver.io/\ncommunity versiono 설치하면 됨 &gt;&gt; windows installer download 받아 설치\n\n\n\n \n\n3번: localhost 자체가 local의 wsl을 의미하기 때문에 그대로 놔두면 됨\n4번: port는 docker_compose.yaml에 설정된 port번호 자동으로 입력되서 나옴\n5번: kmkim (docker_compose.yaml 설정대로 바꿔야함)\n6번: kmkim (docker_compose.yaml 설정대로 바꿔야함)\n\n\n\n\n\n\n\nAirflow MetaDB Connection\n\n\n\n1번: docker_compose.yaml 설정대로 5431이 나옴\n2번: database 이름은 airflow\n3번: username은 airflow\n4번: password는 airflow"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#postgres에-데이터-insert",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#postgres에-데이터-insert",
    "title": "Connection & Hook",
    "section": "",
    "text": "from airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n    dag_id='dags_python_with_postgres',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n\n    \n    def insrt_postgres(ip, port, dbname, user, passwd, **kwargs):\n        import psycopg2 # postgres DB에 접속해서 sql query 를 날리고 결과를 가지고 올수있게 해주는 library\n        from contextlib import closing\n\n        with closing(psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))) as conn:\n            with closing(conn.cursor()) as cursor:\n                dag_id = kwargs.get('ti').dag_id\n                task_id = kwargs.get('ti').task_id\n                run_id = kwargs.get('ti').run_id\n                msg = 'insrt 수행'\n                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'\n                cursor.execute(sql,(dag_id,task_id,run_id,msg))\n                conn.commit()\n\n    insrt_postgres = PythonOperator(\n        task_id='insrt_postgres',\n        python_callable=insrt_postgres,\n        op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']\n    )\n        \n    insrt_postgres\n\npsycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port)) : DB server와의 연결 (Session)\n\nsession: TCP/IP 기반의 connection\n\nclosing() 은 psycopg2.connect() 객체를 닫아주는 역할\n\n예를 들어, with() as ~ statement없이 DB server에 연결하는 명령어는 다음과 같다\n\nconn = psycopg2.connect()\n~~~\nconn.close()\n여기서 conn.close()의 기능을 하는 명령어가 closing() 이다\nwith closing(psycopg2.connect(...)) as conn:\nconn session (con object)에서 sql을 이용한 구체적인 query 내용은 두 번째 with문에서 기술\n with closing(conn.cursor()) as cursor:\n              dag_id = kwargs.get('ti').dag_id # task instance object 에서 dag_id (property) 호출\n              task_id = kwargs.get('ti').task_id # task instance object 에서 task_id (property) 호출\n              run_id = kwargs.get('ti').run_id # task instance object 에서 run_id (property) 호출\n              msg = 'insrt 수행'\n              sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);' \n              cursor.execute(sql,(dag_id,task_id,run_id,msg)) # 실제 sql 실행하는 부분\n              conn.commit()\n\ncursor: client \\(\\xleftrightarrow{\\text{cursor}}\\) DB server. client(python로직을 호출하는 worker container)와 DB서버(container) 사이의 session(client와 DB 서버와의 connection 역할)안에서 client에서 query를 날리고 DB서버로부터 결과를 가져와주는 object. 이 cursor에서 sql 수행. 그러므로 cursor (=conn.cursor())가 없으면 query 수행을 못함\ncursor.execute(sql,(dag_id,task_id,run_id,msg)): 실제 sql 실행하는 부분\npy_opr_drct_insrt : 테이블 이름\nvalues (%s,%s,%s,%s) 4개의 값 binding: dag_id, task_id, run_id, msg. 즉, cursor.execute(sql,(dag_id,task_id,run_id,msg))의 dag_id,task_id,run_id,msg 과 연결\n\nconn이 끝나면 첫 번째 with문의 closing()이 session을 닫아줌\n위의 코드가 아래의 코드와 같은 내용임\nconn=psycopg2.connect(...)\n~~~\nconn.close()\ntask 수행\n\n  insrt_postgres = PythonOperator(\n        task_id='insrt_postgres',\n        python_callable=insrt_postgres,\n        op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']\n    )\n        \n    insrt_postgres\n\ndef insrt_postgres(ip, port, dbname, user, passwd, **kwargs) 에서 보듯이\n\nip = ‘172.28.0.3’\nport = ‘5422’\ndbname = ‘hjkim’\npasswd = ‘hjkim’\n**kwargs = NULL\n\npostgres DB에 table 만들기\n\nDBeaver Open &gt;&gt; kmkim databse 우클릭&gt;&gt; SQL editor &gt;&gt; New SQL Script &gt;&gt; py_opr_drct_insrt table 생성\n\ncreate table py_opr_drct_insrt(\n  dag_id varchar(100),\n  task_id varchar(100),\n  run_id varchar(100),\n  msg text # 가변길이 type\n)\n\ntable 확인: kmkim &gt;&gt; Databases &gt;&gt; Schemas &gt;&gt; public\n\nDAG full example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n    dag_id='dags_python_with_postgres',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n\n    \n    def insrt_postgres(ip, port, dbname, user, passwd, **kwargs):\n        import psycopg2\n        from contextlib import closing\n\n        with closing(psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))) as conn:\n            with closing(conn.cursor()) as cursor:\n                dag_id = kwargs.get('ti').dag_id\n                task_id = kwargs.get('ti').task_id\n                run_id = kwargs.get('ti').run_id\n                msg = 'insrt 수행'\n                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'\n                cursor.execute(sql,(dag_id,task_id,run_id,msg))\n                conn.commit()\n\n    insrt_postgres = PythonOperator(\n        task_id='insrt_postgres',\n        python_callable=insrt_postgres,\n        op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']\n    )\n        \n    insrt_postgres"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#문제점-및-해결방법",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#문제점-및-해결방법",
    "title": "Connection & Hook",
    "section": "",
    "text": "문제점\n\n접속정보 노출: postgres DB에 대한 User, Password 등\n\nDAG을 열어볼 수 있는 사람이나 github에 접속할 수 있는 사람은 모두 볼 수 있음\n\n접속정보 변경시 대응 어려움\n\n만약 직접 접속하는 DAG이 수백개라면?\n\n\n해결 방법\n\nVariable 이용 (User, Password 등을 Variable에 등록하고 꺼내오기) - 번거롭기 때문에 권장하는 방식은 아님\nHook 이용 (Variable 등록 필요없음)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#connection과-hook의-개념",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#connection과-hook의-개념",
    "title": "Connection & Hook",
    "section": "",
    "text": "Connection\n\nAirflow UI 화면에서 등록한 커넥션 정보\n\nHook의 개념\n\nAirflow에서 외부 솔루션의 기능을 사용할 수 있도록 미리 구현된 메서드를 가진 클래스\n\nHook의 특징\n\nConnection 정보를 통해 생성되는 객체로 Hook을 사용하기 위해선 먼저 connection을 등록해야한다.\n\n접속정보를 Connection을 통해 받아오므로 접속정보가 코드상 노출되지 않음\n\n특정 솔루션을 다룰 수 있는 메서드가 구현되어 있음.\noperator나 sensor와는 달리 Hook은 task를 만들어내지 못하므로 Custom operator 안에서나 Python operator 내 함수에서 사용됨"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#connection-등록",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#connection-등록",
    "title": "Connection & Hook",
    "section": "",
    "text": "airflow web ui &gt;&gt; admin &gt;&gt; connections &gt;&gt; plus button &gt;&gt;\n\n\n\n\nConnection_id\nconn-db-postgres-custom\n\n\n\n\nConnection_type\npostgres\n\n\nHost\n172.28.0.3\n\n\nSchema\nkmkim\n\n\nLogin\nkmkim\n\n\nPassword\nkmkim\n\n\nPort\n5432"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#postgres-hook-명세-보기",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#postgres-hook-명세-보기",
    "title": "Connection & Hook",
    "section": "",
    "text": "airflow web service &gt;&gt; Providers &gt;&gt; apache.airflow.providers.postgres &gt;&gt; Python API &gt;&gt; airflow.providers.postgres.hooks.postgres &gt;&gt; get_conn() &gt;&gt; [source] &gt;&gt; def get_conn(self)\n제공하는 함수의 source code를 잘 관찰하고 custom object(custom operator, custom sensor, custom hook 등)를 만드는 것에 익숙해져야 airflow를 잘 활용할 수 있다.\n\ndef get_conn(self) -&gt; connection:\n        \"\"\"Establishes a connection to a postgres database.\"\"\"\n        conn_id = getattr(self, self.conn_name_attr)\n        conn = deepcopy(self.connection or self.get_connection(conn_id))\n\n        # check for authentication via AWS IAM\n        if conn.extra_dejson.get(\"iam\", False):\n            conn.login, conn.password, conn.port = self.get_iam_token(conn)\n\n        conn_args = dict(\n            host=conn.host,\n            user=conn.login,\n            password=conn.password,\n            dbname=self.database or conn.schema,\n            port=conn.port,\n        )\n        raw_cursor = conn.extra_dejson.get(\"cursor\", False)\n        if raw_cursor:\n            conn_args[\"cursor_factory\"] = self._get_cursor(raw_cursor)\n\n        for arg_name, arg_val in conn.extra_dejson.items():\n            if arg_name not in [\n                \"iam\",\n                \"redshift\",\n                \"cursor\",\n                \"cluster-identifier\",\n                \"aws_conn_id\",\n            ]:\n                conn_args[arg_name] = arg_val\n\n        self.conn = psycopg2.connect(**conn_args)\n        return self.conn\n\nconn = deepcopy(self.connection or self.get_connection(conn_id)) 를 보면 hook 클래스의 get_connection() method를 이용해 airflow web ui에서 입력했던 connection 입력값을 찾아내서 연결시켜줌"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#hook-이용하여-postgres-insert",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#hook-이용하여-postgres-insert",
    "title": "Connection & Hook",
    "section": "",
    "text": "from airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n    dag_id='dags_python_with_postgres',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n\n    \n    def insrt_postgres(ip, port, dbname, user, passwd, **kwargs):\n        import psycopg2\n        from contextlib import closing\n\n        with closing(psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))) as conn:\n            with closing(conn.cursor()) as cursor:\n                dag_id = kwargs.get('ti').dag_id\n                task_id = kwargs.get('ti').task_id\n                run_id = kwargs.get('ti').run_id\n                msg = 'insrt 수행'\n                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'\n                cursor.execute(sql,(dag_id,task_id,run_id,msg))\n                conn.commit()\n\n    insrt_postgres = PythonOperator(\n        task_id='insrt_postgres',\n        python_callable=insrt_postgres,\n        op_args=['172.28.0.3', '5432', 'kmkim', 'kmkim', 'kmkim']\n    )\n        \n    insrt_postgres\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n        dag_id='dags_python_with_postgres_hook',\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        schedule=None,\n        catchup=False\n) as dag:\n    def insrt_postgres(postgres_conn_id, **kwargs):\n        from airflow.providers.postgres.hooks.postgres import PostgresHook\n        from contextlib import closing\n        \n        postgres_hook = PostgresHook(postgres_conn_id)\n        with closing(postgres_hook.get_conn()) as conn:\n            with closing(conn.cursor()) as cursor:\n                dag_id = kwargs.get('ti').dag_id\n                task_id = kwargs.get('ti').task_id\n                run_id = kwargs.get('ti').run_id\n                msg = 'hook insrt 수행'\n                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'\n                cursor.execute(sql, (dag_id, task_id, run_id, msg))\n                conn.commit()\n\n    insrt_postgres_with_hook = PythonOperator(\n        task_id='insrt_postgres_with_hook',\n        python_callable=insrt_postgres,\n        op_kwargs={'postgres_conn_id':'conn-db-postgres-custom'}\n    )\n    insrt_postgres_with_hook\n\n\n\n왼쪽과 오른쪽 conn 객체와 cursor는 사실상 같음. 단지 만들어지는 과정만 달라짐.\nop_args=['172.28.0.3', '5432', 'kmkim', 'kmkim', 'kmkim'] 와 같은 보안 사항이 오른 쪽 코드에서는 op_kwargs={'postgres_conn_id':'conn-db-postgres-custom'} 가려지게 된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#postgres-hook-명세-보기-1",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#postgres-hook-명세-보기-1",
    "title": "Connection & Hook",
    "section": "",
    "text": "airflow web service &gt;&gt; Providers &gt;&gt; apache.airflow.providers.postgres &gt;&gt; Python API &gt;&gt; airflow.providers.postgres.hooks.postgres &gt;&gt; def bulk_load(self, temp_file)[source]\nbulk_load(): Loads a tab-delimited file into a database table. 설명이 불충분하여 다음과 같은 사항을 확인할 수 없다.\n\n꼭 delimiter가 tab이어야 하는지?\ntemp_file에 header가 있으면 header가 있는 상태로 data를 올려도 되는지?\nDB table이 없으면 만들어지면서 올라가는지? 아니면 사전에 만들어 놔야하는지?\ntable에 기존 data가 있다면 truncate되면서 올라가는지? append되면서 올라가는지?\nparameter는 구체적으로 어떻게 입력해야하는지?\n\nbulk_load() 의 명세서를 확인해야 한다. (source code)\n\n[docs]    def bulk_load(self, table: str, tmp_file: str) -&gt; None:\n        \"\"\"Loads a tab-delimited file into a database table\"\"\"\n        self.copy_expert(f\"COPY {table} FROM STDIN\", tmp_file)\n\ncopy_expert(): postgres hook class가 갖고 있는 method\nself.copy_expert(f”COPY {table} FROM STDIN”, tmp_file)\n\n[docs]    def copy_expert(self, sql: str, filename: str) -&gt; None:\n        \"\"\"\n        Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.\n        \"\"\"\n        self.log.info(\"Running copy expert: %s, filename: %s\", sql, filename)\n        if not os.path.isfile(filename):\n            with open(filename, \"w\"):\n                pass\n\n        with open(filename, \"r+\") as file:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, file)\n                    file.truncate(file.tell())\n                    conn.commit()\n\nclosing(self.get_conn()) as conn: &gt;&gt; with closing(conn.cursor()) as cur:\n\nget.conn() &gt;&gt; cursor() 결국 postgres의 cursor로 postgres가 원래 갖고있는 copy_export() method를 이용\n그럼 postgres의 copy_export()의 source code를 확인해 봐야함. google psycopg2 cursor.copy_expert\n\ncopy_expert(sql, file, size=8192)\n  Submit a user-composed COPY statement. The method is useful to handle all the parameters that PostgreSQL makes available (see COPY command documentation).\n\n  Parameters:\n  sql – the COPY statement to execute.\n\n  file – a file-like object to read or write (according to sql).\n\n  size – size of the read buffer to be used in COPY FROM.\n\n  The sql statement should be in the form COPY table TO STDOUT to export table to the file object passed as argument or COPY table FROM STDIN to import the content of the file object into table. If you need to compose a COPY statement dynamically (because table, fields, or query parameters are in Python variables) you may use the objects provided by the psycopg2.sql module.\n\n  file must be a readable file-like object (as required by copy_from()) for sql statement COPY ... FROM STDIN or a writable one (as required by copy_to()) for COPY ... TO STDOUT.\n\n  Example:\n\n  &gt;&gt;&gt; cur.copy_expert(\"COPY test TO STDOUT WITH CSV HEADER\", sys.stdout)\n  id,num,data\n  1,100,abc'def\n  2,,dada\n  ...\n  New in version 2.0.6.\n\n  Changed in version 2.4: files implementing the io.TextIOBase interface are dealt with using Unicode data instead of bytes.\n\n이것이 가장 세부적인 정보로 나머지 정보는 troubleshooting으로 파악해야한다.\n\n꼭 delimiter가 tab이어야 하는지? troubleshooting으로 확인해야함\ntemp_file에 header가 있으면 header가 있는 상태로 data를 올려도 되는지? troubleshooting으로 확인해야함\nDB table이 없으면 만들어지면서 올라가는지? 아니면 사전에 만들어 놔야하는지? troubleshooting으로 확인해야함\ntable에 기존 data가 있다면 truncate되면서 올라가는지? troubleshooting으로 확인해야함\n\n\ntrouble shooting 할 DAG\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\n\nwith DAG(\n        dag_id='dags_python_with_postgres_hook_bulk_load',\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        schedule='0 7 * * *', # 서울시 공공데이터를 API를 통해 불러들이(dags_seoul_api_corona.py)는 시간이 7시\n        catchup=False\n) as dag:\n    def insrt_postgres(postgres_conn_id, tbl_nm, file_nm, **kwargs):\n        postgres_hook = PostgresHook(postgres_conn_id)\n        postgres_hook.bulk_load(tbl_nm, file_nm)\n\n    insrt_postgres = PythonOperator(\n        task_id='insrt_postgres',\n        python_callable=insrt_postgres,\n        op_kwargs={'postgres_conn_id': 'conn-db-postgres-custom',\n                   'tbl_nm':'TbCorona19CountStatus_bulk1',\n                   'file_nm':'/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash}}/TbCorona19CountStatus.csv'}\n    )\n\nop_kwargs={‘postgres_conn_id’: ‘conn-db-postgres-custom’, ‘tbl_nm’:‘TbCorona19CountStatus_bulk1’, ‘file_nm’:‘/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(“Asia/Seoul”) | ds_nodash}}/TbCorona19CountStatus.csv’} 는 dags_seoul_api_corona.py 의 tb_corona19_count_status task의 path와 file_name 인수로 부터 가져온다.\ntb_corona19_count_status = SeoulApiToCsvOperator( task_id=‘tb_corona19_count_status’, dataset_nm=‘TbCorona19CountStatus’, path=‘/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(“Asia/Seoul”) | ds_nodash }}’, file_name=‘TbCorona19CountStatus.csv’ )\ntroubleshooting\n\ntable이 사전에 만들어지지 않아도 되는지? 현재 DB에는 TbCorona19CountStatus_bulk1 table이 없음 \n실행 결과 log 에서 table이 없다는 에러 메세지 뜸. table 만들어 주면 됨\ntable만들어서 한번 더 task 실행하면 comma delimiter 인식 오류가 나기 떄문에 tab delimiter로 바꿔줘야한다.\n\ndags_seoul_api_corona.py 로 부터 받은 TbCorona19CountStatus.csv 파일을 열어 ,를 tab으로 바꿔준다\n\nvi TbCorona19CountStatus.csv\nvi editor\n\n: %s/,/\\t/g ,를 \\t으로 바꿔줌. 여기서 g는 global하게 적용하겠다는 의미\n\n51번째 line에 에러 발생: \\M 이라는 특수 문자가 있음\n\n\\M 윈도우와 리눅스 간의 줄넘김 차이 때문에 발생\n\n윈도우: enter key = CR (Carriage Return-한줄에서 왼쪽 끝으로 밀어주는것이 CR) + LF (Line Feed-다음 줄에 입력을 하도록 종이를 한줄 밀어주는 것 LF)\nDOS/Windows 계열에서는 엔터를 CR+LF(\\r\\n) 으로 처리하고\nUnix/Linux 계열에서는 엔터를 LF(\\n)으로 처리하고\nMAC 계열에서는 엔터를 CR(\\r)로 처리한다고 한다\n윈도우 환경에서 입력된 값이 리눅스로 넘어오게 될 때 CR+LF와 같은 불일치 값이 있으면 ^M 또는 \\M 로 표시됨\n\n51번째 record 지워서 해결\n\nairflow 상 errors는 더이상 발생하지 않지만 DB를 확인했을 때 첫번째 row에 column명이 들어간것을 확인되었다. 그래서 CSV상에 column값을 지워줘야한다는 것을 알 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#bulk_load-문제점-개선방안",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#bulk_load-문제점-개선방안",
    "title": "Connection & Hook",
    "section": "",
    "text": "문제점\n\nLoad 가능한 Delimiter는 Tab으로 고정되어 있음\nHeader까지 포함해서 업로드됨\n특수문자로 인해 파싱이 안될 경우 에러 발생\n\n개선방안\n\nCustom Hook 을 만들어서 Delimiter 유형을 입력받게 하고\nHeader 포함 여부를 선택하게끔 하며\n특수문자를 제거하는 로직을 추가 후\nsqlalchemy(python에서 DB 작업을 편리하게 해주는 library)를 이용하여 Load 한다면? 그리고 테이블을 생성하면서 업로드할 수 있도록 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#basehook-명세-보기",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#basehook-명세-보기",
    "title": "Connection & Hook",
    "section": "",
    "text": "Custom Hook은 BaseHook을 상속해서 작성\nAirflow Docs에서 Basehook Source Code\nBaseHook class의 methods\n\n[docs] def get_connection(cls, conn_id: str) -&gt; Connection:\n\nairflow 상에서 만들었던 connection_id : user name, password, IP, Port 의 정보를 담고있는 object return\n\n[docs] def get_connections(cls, conn_id: str) -&gt; list[Connection]:\n\nwill be deprecated. Use get_connection()\n\n[docs] def get_conn(self) -&gt; Any: # 이 함수를 쓰려면 상속받아서 구현할 때 get_conn() 함수를 구현해야함\n[docs] def get_hook(cls, conn_id: str) -&gt; BaseHook:\n[docs] def get_connection_form_widgets(cls) -&gt; dict[str, Any]: (안중요)\n[docs] def get_ui_field_behaviour(cls) -&gt; dict[str, Any]: (안중요)\n\n위의 method 중 get_conn()을 제외하곤 모든 method에 데코레이터 @classmethod 있음\n\npython있는 method 종류로 class method 라 하고 class method 는 class를 객체화 시키지 않고도 바로 호출할 수 있음\n예를 들어, 다음과 같은 방식으로 class method 호출 안해도 됨\nimport BaseHook\na=BaseHook()\na.get_connection(conn_id)\n바로 호출 해도됨\nimport BaseHook\nBaseHook.get_connection(conn_id)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#custom-hook-개발",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#custom-hook-개발",
    "title": "Connection & Hook",
    "section": "",
    "text": "해야할 일\n\nget_conn 메서드 구현하기\n\nDB 와의 연결 세션 객체인 conn 을 리턴하도록 구현\n주의: get_connection() vs get_conn()\n\nget_connection(): Airflow 에서 등록한 Connection 정보를 담은 conn을 return\nget_conn(): postgres와의 연결하는 session 객체를 return\nBaseHook 의 추상 메서드 , 자식 클래스에서 구현 필요\n\n\nbulk_load 메서드 구현하기\n\n입맛대로 만들기: custom_postgres_hook.py\n\nfrom airflow.hooks.base import BaseHook\nimport psycopg2\nimport pandas as pd\n\nclass CustomPostgresHook(BaseHook):\n\n    # 생성자\n    def __init__(self, postgres_conn_id, **kwargs): # 입력은 하나만: postgres_conn_id\n        self.postgres_conn_id = postgres_conn_id\n\n    def get_conn(self):\n        airflow_conn = BaseHook.get_connection(self.postgres_conn_id) #class method라 바로 호출\n        # 아래의 보안 정보들이 hook을 통해서 노출되지 않고 접근 가능\n        self.host = airflow_conn.host\n        self.user = airflow_conn.login\n        self.password = airflow_conn.password\n        self.dbname = airflow_conn.schema\n        self.port = airflow_conn.port\n\n        # postgres DB 연결: session object를 return\n        self.postgres_conn = psycopg2.connect(host=self.host, user=self.user, password=self.password, dbname=self.dbname, port=self.port)\n        return self.postgres_conn\n\n    def bulk_load(self, table_name, file_name, delimiter: str, is_header: bool, is_replace: bool):\n\n        from sqlalchemy import create_engine\n\n        self.log.info('적재 대상파일:' + file_name)\n        self.log.info('테이블 :' + table_name)\n        self.get_conn()\n        header = 0 if is_header else None                       # is_header = True면 0, False면 None\n        if_exists = 'replace' if is_replace else 'append'       # is_replace = True면 replace, False면 append\n        file_df = pd.read_csv(file_name, header=header, delimiter=delimiter)\n\n        for col in file_df.columns:                             \n            try:\n                # string 문자열이 아닐 경우 continue\n                file_df[col] = file_df[col].str.replace('\\r\\n','')      # 줄넘김 및 ^M 제거\n                self.log.info(f'{table_name}.{col}: 개행문자 제거')\n            except:\n                continue \n\n        self.log.info('적재 건수:' + str(len(file_df)))\n        uri = f'postgresql://{self.user}:{self.password}@{self.host}/{self.dbname}'\n        engine = create_engine(uri)\n        file_df.to_sql(name=table_name,\n                            con=engine,\n                            schema='public',\n                            if_exists=if_exists,\n                            index=False\n                        )\n\n여기서 airflow_conn = BaseHook.get_connection(self.postgres_conn_id) #class method라 바로 호출 와 self.postgres_conn = psycopg2.connect(host=self.host, user=self.user, password=self.password, dbname=self.dbname, port=self.port) 다른 종류의 conn 객체를 return한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#providers-더-보기",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#providers-더-보기",
    "title": "Connection & Hook",
    "section": "",
    "text": "현재 설치되어 있는 Providers 패키지 확인\n\n웹의 Admin Providers 에서 확인 가능\n\n설치 가능한 Providers 더 보기 &gt;&gt; Apache Airflow 2 is built in modular way. The “Core” of Apache Airflow provides core scheduler functionality which allow you to write some basic tasks, but the capabilities of Apache Airflow can be extended by installing additional packages, called providers.\n\nproviders package를 설치하면 다른 솔루션을 연동할 수 있도록 확장성을 제공\nThe full list of community managed providers is available at Providers Index.\n\nProviders packages:\n\nApache Hive"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#connection-type-추가하기",
    "href": "docs/blog/posts/Engineering/airflow/09.connection_hook.html#connection-type-추가하기",
    "title": "Connection & Hook",
    "section": "",
    "text": "Airflow Connection type 목록에 있는 대상은 이미 패키지 설치가 된 Providers 이며 Admin&gt;&gt;Providers 목록에서 설치된 대상 확인 가능\n만약 Hive 에 대한 커넥션을 추가하고 싶은데 Airflow Connection type 목록에 Hive가 없다면 관련된 package를 설치하여 본인이 직접 추가해야함\n\nProvider 검색, py 라이브러리 설치 목록 확인하여 pip install [pkg name] 실행\n\n주의사항: 윈도우와 wsl2에 package를 설치하는게 아니라 airflow containers에 설치해줘야 함\n\nscheduler\nworker\nwebserver\ntriggerer\n\n하지만, 각 각의 container에다가 pkg를 설치해주면 container가 꺼지게 되면 지워지게 됨\n그래서, custom한 docker image를 만들어야 함\n\nAirflow 이미지 Extend 방법으로 custom image 만들기\n\nbase image 에다가 custom image (pip install 및 다른 여러가지 layers) 추가\n즉, custom image = base image + layer1 (library 호출) +layer2 (pip install pkgs)+ \\(\\ldots\\)\n이런 방식의 custom image는 layer가 많아질 수록 무거워져 overhead 가 커지는 약점이 있다.\n\n애초에 base image 자체를 custom image 로 만드는 법\n\nextend의 약점인 overhead를 어느 정도 줄일 수 있지만 개발하는데 시간이 걸림\n\n\n\n\nAirflow 이미지 Extend 방법 & custom image 만드는 법 확인\n\n이미지 Extend vs Custom 이미지 생성\n\n\n\n\n\n\n\n\n\nComparison\nimage extend\nCustom Image Creation\n\n\n\n\n간단히 생성 가능\nO\nX (많은 source codes 필요)\n\n\n빌드 시간\n짧음 (5분 이내)\n상대적으로 긺\n\n\n크기 최적화된 이미지\nX\nO (약 20% 정도 사이즈 감소)\n\n\n폐쇄망에서 구성 가능\nX (인터넷이 되어야함)\nO\n\n\n\nAirflow web에 connection type 추가하는 steps\n\n이미지 extend를 위한 Dockerfile 만들기\n:::: {.columns}\n\n\n#&gt; cd; cd airflow\n#&gt; mkdir -p custom_image/airflow\n#&gt; cd custom_image/airflow\n#&gt; vi Dockerfile\n\n\nFROM apache/airflow:2.5.1 #base image 지정\nUSER root #root user\nRUN apt get update \\\n  && apt-get install -y --no-install-recommends \\\n    gcc \\ #library 1 for installing hdfs\n    heimdal-dev \\ #library 2 for installing hdfs\n    g++ \\ #library 3\n    libsasl2-dev \\ #library 4\n  && apt-get autoremove -yqq -purge \\\n  && apt-get clean \\\n  && rm -rf /var/lib/apt/lists/*\nUSER airflow\nRUN pip install \\\n  apache airflow providers apache hdfs \\\n  apache airflow providers apache hive\nRUN pip uninstall -y argparse\n\n::::\n\nDocker 이미지 생성하기\n\n#&gt; pwd \n#&gt; cd /home/hjkim/airflow/custom_image/airflow #Dockerfile 만든 경로\n#&gt; sudo docker build -t {image_name} . #.: 현재 디렉토리\n예) #&gt; sudo docker build -t airflow_custom .\nerror message\nkmkim@K100230201051:~/airflow/custom_image/airflow$ sudo docker build -t airflow_custom .\n\nfailed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-buildx: no such file or directory\n\nDEPRECATED: The legacy builder is deprecated and will be removed in a future release.\n            Install the buildx component to build images with BuildKit:\n            https://docs.docker.com/go/buildx/\n\nSending build context to Docker daemon  2.048kB\nStep 1/6 : FROM apache/airflow:2.6.1\n ---&gt; 52c34708e903\nStep 2/6 : USER root #root user\n ---&gt; Running in c3fe7d498d62\nRemoving intermediate container c3fe7d498d62\n ---&gt; d216e5376f4e\nStep 3/6 : RUN apt get update   && apt-get install -y --no-install-recommends     gcc     heimdal-dev     g++     libsasl2-dev     heimdal-dev   && apt-get autoremove -yqq -purge   && apt-get clean   && rm -rf /var/lib/apt/lists/*\n ---&gt; Running in 8238d95b680a\nunable to find user root #root user: no matching entries in passwd file\n\nDocker 이미지 확인\n\n#&gt; sudo docker image ls\n\ndocker-compose.yaml 수정하기\n\n#&gt; cd\n#&gt; vi docker-compose.yaml\n\nversion: '3.8'\nx-airflow-common:\n  &airflow-common\n# In order to add …\n  image: {image_name} #새로 만든 docker image 이름 넣을 것\n\ndocker compose (재) 기동\nConnection Type 에 추가 확인\n\nHDFS, Hive Client Wrapper, Hive Metastore Thrift, Hive Server 2 Thrift"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/10.sensor.html",
    "href": "docs/blog/posts/Engineering/airflow/10.sensor.html",
    "title": "Sensor",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n일종의 특화된 오퍼레이터\n특정 조건이 만족되기를 주기적으로 확인 및 기다리고 만족되면 True를 반환하는 Task\n모든 센서는 BaseSensorOperator를 상속하여 구현되며 (BaseSensorOperator는 BaseOperator를 상속함) 상속시에는 init() 함수와 poke(context) 함수 재정의 해야한다\n센싱하는 로직은 poke 함수에 정의: 특정 조건이 만족하는지 체크하고 true를 return 하도록 정의\n\n\n\n\nairflow BaseSensorOperator\n\nBases: airflow.models.baseoperator.BaseOperator, airflow.models.skipmixin.SkipMixin Sensor operators are derived from this class and inherit these attributes. Sensor operators keep executing at a time interval and succeed when a criteria is met and fail if and when they time out. base operator를 상속했음.\nparameter\n\npoke_interval (float) – Time in seconds that the job should wait in between each try: sensor task의 특정 조건이 만족하는지 체크하는 주기로 초단위로 입력하면 된다. 60 = 1mins 은 1분 단위로 체크\ntimeout (float) – Time, in seconds before the task times out and fails. task가 계속 false 가 나올때 task failure 로 규정할 maximum 시간. 초단위로 입력. 보통 daily dag을 많이 만드므로 timeout도 보통 24시간 으로 입력한다. ex) 606024 (=24 시간)\nsoft_fail (bool) – Set to true to mark the task as SKIPPED on failure. timeout을 만났을 때 sensor task fail로 marking하지 말고 skip으로 marking하도록 설정\nmode (str) (중요) – How the sensor operates. ‘poke’ 아니면 ‘reschedule’ 로만 값을 넣을 수 있음. 중요하기 때문에 아래에서 따로 설명. Options are: { poke | reschedule }, default is poke. When set to poke the sensor is taking up a worker slot for its whole execution time and sleeps between pokes. Use this mode if the expected runtime of the sensor is short or if a short poke interval is required. Note that the sensor will hold onto a worker slot and a pool slot for the duration of the sensor’s runtime in this mode. When set to reschedule the sensor task frees the worker slot when the criteria is not yet met and it’s rescheduled at a later time. Use this mode if the time before the criteria is met is expected to be quite long. The poke interval should be more than one minute to prevent too much load on the scheduler.\nexponential_backoff (bool) – allow progressive longer waits between pokes by using exponential backoff algorithm. sensor task를 체크하는 주기가 \\(2^n\\) 으로 늘어지기 된다. 즉, 2초, 4초, 8초, \\(\\ldots\\)\nmax_wait (datetime.timedelta | float | None)\n\nmaximum wait interval between pokes, can be timedelta or float seconds.\nexponential_backoff가 true 일 때만 홠성화 되며 exponential_backoff 의 상한선을 의미\n\nsilent_fail (bool) – If true, and poke method raises an exception different from AirflowSensorTimeout, AirflowTaskTimeout, AirflowSkipException and AirflowFailException, the sensor will log the error and continue its execution. Otherwise, the sensor task fails, and it can be retried based on the provided retries parameter.\n\npoke(context)[source]: Function defined by the sensors while deriving this class should override.\n\npoke method를 재정의 하지 않으면 error 발생하게 되어 있음\n[docs]    def poke(self, context: Context) -&gt; bool | PokeReturnValue:\n        \"\"\"Function defined by the sensors while deriving this class should override.\"\"\"\n        raise AirflowException(\"Override me.\")\n\nexecute(self, context: Context): 재정의할 필요없음. 이미 정의가 되어 있음. overiding된 poke함수의 값이 있어야 밑의 while loop를 탈출하게 되어 있음. 다시 말해서, poke_return = self.poke(context)이 false를 반환하게 되면 infinite loop. 결국 poke() 가 중요\n[docs]    def execute(self, context: Context) -&gt; Any:\n      started_at: datetime.datetime | float \n\n      (...)\n\n      while True:\n          try:\n              poke_return = self.poke(context)\n          except (\n              AirflowSensorTimeout,\n              AirflowTaskTimeout,\n              AirflowSkipException,\n              AirflowFailException,\n          ) as e:\n              raise e\n          except Exception as e:\n              if self.silent_fail:\n                  logging.error(\"Sensor poke failed: \\n %s\", traceback.format_exc())\n                  poke_return = False\n              else:\n                  raise e\n\n          if poke_return: # poke_return = true이면 while loop 탈출\n              if isinstance(poke_return, PokeReturnValue):\n                  xcom_value = poke_return.xcom_value\n              break\n        (...)       \nBaseSensor 오퍼레이터 Mode 유형\n\nmode 유형\n\n\n\n\n\n\n\n\n\nComparison\nPoke Mode\nReschedule Mode\n\n\n\n\n원리\nDAG이 수행되는 내내 Running Slot(task가 수행될 때 차지하는 공간) 을 차지. sensor가 특정 조건을 체킹할때나 안할때나 항상 slot 차지. 다만 Slot 안에서 Sleep, active 를 반복\n센서가 조건을 체킹하는 동작 시기에만 Slot을 차지. 그 외에는 Slot을 점유하지 않음. slot을 들어갔다 나왔다를 반복\n\n\nWait에서의 Task 상태\nrunning (airflow web ui 에서 task bar가 연두색)\nup_for_reschedule (task bar가 민트색)\n\n\n유리한 적용 시점\n짧은 센싱 간격 (interval, 초 단위)\n긴 센싱 간격, 주로 분 단위 Reschedule될 때 (5분, 10분) 스케줄러의 부하 발생\n\n\n\nSlot의 이해\n\nPool\n\n모든 operator로 만들어진 Task는 특정 Pool에서 수행되며 Pool은 Slot이라는 것을 가지고 있음.\n기본적으로 Task 1개당 Slot 1개를 점유하며 Pool을 지정하지 않으면 default_pool에서 수행 \nairflow web service ui &gt;&gt; admin &gt;&gt; pools\n\npool: pool name\nslots: 128개의 공간\nRunning Slots, Queued Slots, Schedulued Slots.\n\n\n\n사용자 입장에서는 operator의 mode의 이해는 그렇게 중요하진 않지만 airflow를 운영하는 사람 입장에서는 중요한 변수가 될 수 있다.\n\n\n\n\n\n\n\n\nairflow.sensors.bash\n\nParameters\n\nbash_command – 조건문을 여기에다가 적음\n\nReturn True if and only if the return code is 0.\nshell 스크립트에서 return True를 주는 방법\n\n파이썬에서의 return True와 같은 의미로 shell script에서는 exit 0 를 사용\n\n모든 shell은 수행을 마친 후 EXIT_STATUS를 가지고 있으며 0~255 사이의 값을 가짐.\n\nEXIT 0 만 정상이며 나머지는 모두 비정상의 의미를 가짐\n마지막 명령 수행의 EXIT_STATUS를 확인하려면 echo $? 로 확인\n\nkmkim@K100230201051:~/airflow$ ls\nairflow  custom_image  docker-compose.20230708  files  plugins\nconfig   dags          docker-compose.yaml      logs\nkmkim@K100230201051:~/airflow$ echo $?\n0 # ls란 명령이 정상적으로 실행됐기 때문에 0을 반환\nkmkim@K100230201051:~/airflow$ ls sdf\nls: cannot access 'sdf': No such file or directory\nkmkim@K100230201051:~/airflow$ echo $?\n2\nkmkim@K100230201051:~/airflow$ sdfsd\nsdfsd: command not found\nkmkim@K100230201051:~/airflow$ echo $?\n127\nexit status 변경하기\n\nvi test.sh #exit status 변경할 shell script\n# vi editor: test.sh\nls\nexit 1\nchmod +x test.sh #실행권한 부여\n./test.sh #실행\necho $? #1 출력됨\nenv – If env is not None, it must be a mapping that defines the environment variables for the new process; these are used instead of inheriting the current process environment, which is the default behavior. (templated)\noutput_encoding – output encoding of bash command.\nretry_exit_code (int | None) – If task exits with this code, treat the sensor as not-yet-complete and retry the check later according to the usual retry/timeout settings. Any other non-zero return code will be treated as an error, and cause the sensor to fail. If set to None (the default), any non-zero exit code will cause a retry and the task will never raise an error except on time-out.\n\n\nDag Example\n\ncsv file 있는지 없는지 확인\n\n\nfrom airflow.sensors.bash import BashSensor\nfrom airflow.operators.bash import BashOperator\nfrom airflow import DAG\nimport pendulum\n\nwith DAG(\n    dag_id='dags_bash_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='0 6 * * *',\n    catchup=False\n) as dag:\n\n    sensor_task_by_poke = BashSensor(\n        task_id='sensor_task_by_poke',\n        # 오늘 날짜로 tvCorona19VaccinestatNew.csv 가 있는지 없는지 확인 \n        env={'FILE':'/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv'},\n        bash_command=f'''echo $FILE && \n                        if [ -f $FILE ]; then \n                              exit 0\n                        else \n                              exit 1\n                        fi''',\n        poke_interval=30,      #30초\n        timeout=60*2,          #2분\n        mode='poke',\n        soft_fail=False\n    )\n\n    sensor_task_by_reschedule = BashSensor(\n        task_id='sensor_task_by_reschedule',\n        env={'FILE':'/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv'},\n        bash_command=f'''echo $FILE && \n                        if [ -f $FILE ]; then \n                              exit 0\n                        else \n                              exit 1\n                        fi''',\n        poke_interval=60*3,    # 3분\n        timeout=60*9,          #9분\n        mode='reschedule',\n        soft_fail=True\n    )\n\n    bash_task = BashOperator(\n        task_id='bash_task',\n        env={'FILE': '/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv'},\n        bash_command='echo \"건수: `cat $FILE | wc -l`\"',\n    )\n\n    [sensor_task_by_poke,sensor_task_by_reschedule] &gt;&gt; bash_task\n\n\n\n\n\n\n\nairflow.sensors.filesystem\n\nWaits for a file or folder to land in a filesystem. (file이나 folder가 file system안에 들어왔는지 체크)\nParameters\n\nfs_conn_id – reference to the File (path) connection id (file connection id을 airflow web에 미리 등록해놔야함)\nfilepath – File or folder name (relative to the base path set within the connection), can be a glob. 상대 경로를 입력해야함\nrecursive – when set to True, enables recursive directory matching behavior of ** in glob filepath parameter. Defaults to False. (glob(**))\n\n구체적인 로직을 이해하기 위해 source를 봐야함\n\n모든 sensor를 만들때는 poke를 재정의해야만 함.\n즉, poke만 잘 이해하면 거의 모든 sensor를 잘 이해할 수 있다.\npoke()의 FSHook() 은 imported from airflow.hooks.filesystem\n\nhook = FSHook(self.fs_conn_id): hook을 만들 때 fs_conn_id를 넘겨받고 있음\n\nglob()을 이해하는 것이 핵심\n\nfile path안에 있는 files 또는 directories를 list로 반환\n\nglob('/home/kmkim') # 이 path 안에 있는 모든 files, directories이 리스트로 반환됨\nglob('/home/kmkim/docker-compose.yaml') # docker-compose.yaml 자체가 리스트로 반환됨\nglob('/home/kmkim/**', recursive=True) # /home/kmkim/ 하위에 있는 모든 files 또는 directories이 리스트로 반환됨\n\n주로 file을 찾기 위해 사용되는 함수이기 때문에 glob(‘/home/kmkim/docker-compose.yaml’) 이 형태가 가장 많이 사용된다.\n\n\n\nfrom __future__ import annotations\n\nimport datetime\nimport os\nfrom glob import glob\nfrom typing import Sequence\n\nfrom airflow.hooks.filesystem import FSHook\nfrom airflow.sensors.base import BaseSensorOperator\nfrom airflow.utils.context import Context\n\n\n[docs]class FileSensor(BaseSensorOperator):\n    \"\"\"\n    Waits for a file or folder to land in a filesystem.\n\n    If the path given is a directory then this sensor will only return true if\n    any files exist inside it (either directly, or within a subdirectory)\n\n    :param fs_conn_id: reference to the File (path)\n        connection id\n    :param filepath: File or folder name (relative to\n        the base path set within the connection), can be a glob.\n    :param recursive: when set to ``True``, enables recursive directory matching behavior of\n        ``**`` in glob filepath parameter. Defaults to ``False``.\n\n    .. seealso::\n        For more information on how to use this sensor, take a look at the guide:\n        :ref:`howto/operator:FileSensor`\n\n\n    \"\"\"\n\n[docs]    template_fields: Sequence[str] = (\"filepath\",)\n[docs]    ui_color = \"#91818a\"\n\n    def __init__(self, *, filepath, fs_conn_id=\"fs_default\", recursive=False, **kwargs):\n        super().__init__(**kwargs)\n        self.filepath = filepath\n        self.fs_conn_id = fs_conn_id\n        self.recursive = recursive\n\n[docs]    def poke(self, context: Context):\n        hook = FSHook(self.fs_conn_id)\n        basepath = hook.get_path() # hook이 가지고 있는 method\n        full_path = os.path.join(basepath, self.filepath)\n        self.log.info(\"Poking for file %s\", full_path)\n\n        for path in glob(full_path, recursive=self.recursive):\n            if os.path.isfile(path): # glob에 의한 결과물이 file이면 for loop 탈출\n                mod_time = datetime.datetime.fromtimestamp(os.path.getmtime(path)).strftime(\"%Y%m%d%H%M%S\")\n                self.log.info(\"Found File %s last modified: %s\", str(path), mod_time)\n                return True\n\n            for _, _, files in os.walk(path):\n                if len(files) &gt; 0:\n                    return True\n        return False\n\nglob: file path안에 있는 file이나 directory를 찾아서 list로 반환\n\nglob('/home/kmkim') : 모든 files and directories list 반환\nglob('/home/kmkim/docker-compose.yaml') : docker-compose.yaml만 리스트로 반환\nglob('/home/kmkim/**',recursive=True) : 하위 디렉토리 안에 있는 파일과 디렉토리들의 리스트로 반환\n\nFSHook() : 명세서\n[docs]class FSHook(BaseHook):\n\"\"\"\nAllows for interaction with an file server.\n\nConnection should have a name and a path specified under extra:\n\nexample:\nConnection Id: fs_test\nConnection Type: File (path)\nHost, Schema, Login, Password, Port: empty\nExtra: {\"path\": \"/tmp\"} # dictionary 형태\n\"\"\"\n\ndef __init__(self, conn_id: str = \"fs_default\"):\n    super().__init__()\n    conn = self.get_connection(conn_id)\n    self.basepath = conn.extra_dejson.get(\"path\", \"\")\n    self.conn = conn\n\n[docs]    def get_conn(self) -&gt; None:\n        pass\n\n\n[docs]    def get_path(self) -&gt; str:\n        \"\"\"\n        Get the path to the filesystem location.\n\n        :return: the path.\n        \"\"\"\n        return self.basepath\n\nget_path(): self.baspath return - 별 내용 없음\n생성자만 잘 이해하면 됨\n\nConnection should have a name and a path specified under extra (예시 잘 볼 것): \n\n\n\nConnection 작성\n\n\n\nConnection_id\nconn_file_opt_airflow_files\n\n\n\n\nConnection_type\nFile (path)\n\n\nHost\n\n\n\nSchema\n\n\n\nLogin\n\n\n\nPassword\n\n\n\nPort\n\n\n\nExtra\n{“path”:“/opt/airflow/files”}\n\n\n\nDag 작성\n\nfrom airflow import DAG\nfrom airflow.sensors.filesystem import FileSensor\nimport pendulum\n\nwith DAG(\n    dag_id='dags_file_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='0 7 * * *',\n    catchup=False\n) as dag:\n    tvCorona19VaccinestatNew_sensor = FileSensor(\n        task_id='tvCorona19VaccinestatNew_sensor',\n        fs_conn_id='conn_file_opt_airflow_files',\n        filepath='tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv',\n        recursive=False,\n        poke_interval=60,\n        timeout=60*60*24, # 1일\n        mode='reschedule'\n    )\n\n\n\n\n\npython operator를 이용하여 특정 날짜에 데이터가 업데이트 되었는지 확인하는 sensor를 만들기\nairflow.sensors.python 명세서 보기\npython operator와 매우 유사: 다른점은 python collabe function의 return값이 반드시 true로 되어야함\n\n\nclassairflow.sensors.python.PythonSensor(*, python_callable, op_args=None, op_kwargs=None, templates_dict=None, **kwargs)[source]\nBases: airflow.sensors.base.BaseSensorOperator\n\nWaits for a Python callable to return True.\n\nUser could put input argument in templates_dict e.g templates_dict = {'start_ds': 1970} and access the argument by calling kwargs['templates_dict']['start_ds'] in the callable\nParameters * python_callable (Callable) – A reference to an object that is callable. python callable에 넘겨줄 인수들은 아래의 op_kwars, op_args * op_kwargs (Mapping[str, Any] | None) – a dictionary of keyword arguments that will get unpacked in your function * op_args (list | None) – a list of positional arguments that will get unpacked when calling your callable * templates_dict (dict | None) – a dictionary where the values are templates that will get templated by the Airflow engine sometime between init and execute takes place and are made available in your callable’s context after the template has been applied. templates_dict도 python collable에 넘겨주는 인수로 {‘start_ds’:‘{{data_interval_start}}’} 와 같이 사용하면 된다. * 예를 들어, 다음과 같이 사용된다. 하지만 실무에서는 tamplates_dict보단 op_kwargs가 더 많이 사용된다.\ndef somefunc(x1,x2,**kwargs):\n  v=kwargs['templates_dict']['start_ds'] #{'start_ds':'{{data_interval_start}}'}의 '{{data_interval_start}}'값이 호출된다.\n\n\n\n무엇을 센싱할 것인가\n\n서울시 공공데이터에서 당일 날짜로 데이터가 생성되었는지 센싱하기(날짜 컬럼이 있는 경우)\n당일 날짜가 몇시에 업로드가 되는지 모름 (12시? ,00시, 1시?)\n\n서울 열린 데이터 광장\n\n검색창 &gt;&gt; ‘코로나’ 검색 &gt;&gt; 서울시 코로나19 확진자 발생동향 (2023.05.31.이전) &gt;&gt; 미리보기\n목표: 센서를 공용적으로 만들어 다른 dataset에도 적용할 수 있도록 작성\n\npython collable function(check_api_update())을 일반화 시켜서 작성 ```markdown from airflow import DAG from airflow.sensors.python import PythonSensor import pendulum from airflow.hooks.base import BaseHook\n\n\n\nwith DAG( dag_id=‘dags_python_sensor’, start_date=pendulum.datetime(2023,4,1, tz=‘Asia/Seoul’), schedule=’10 1 * * *’, catchup=False ) as dag: def check_api_update(http_conn_id, endpoint, base_dt_col, **kwargs): import requests import json from dateutil import relativedelta connection = BaseHook.get_connection(http_conn_id) url = f’http://{connection.host}:{connection.port}/{endpoint}/1/100’ #1부터 100행 까지만 가지고옴 response = requests.get(url)\n    contents = json.loads(response.text)\n    key_nm = list(contents.keys())[0]\n    row_data = contents.get(key_nm).get('row') \n      # row_data에 list형태로 데이터가 들어감\n      # [{1행},\n         {2행},\n         {3행},\n         {4행},...]\n    last_dt = row_data[0].get(base_dt_col) #row_data의 첫번째 행, base_dt_col의 key에 대한 value를 추출\n    last_date = last_dt[:10] \n      # 열번째 글자 까지만 slicing\n      # 왜냐면, 서울시 기준일(S_DT)가 date 형식이 아니라 string 형식으로 입력되어 있음\n      # 연/월/일 정보만 필요하기 때문에 시간은 제외\n    last_date = last_date.replace('.', '-').replace('/', '-')\n\n    # 밑에 try구문은 last_date 의 date 형식을 검증\n    try:\n        pendulum.from_format(last_date,'YYYY-MM-DD') # last_date가 'YYYY-MM-DD' 형식으로 parsing이 가능하면 코드 진행이 계속되고 안되면 except문으로 들어가게됨\n    except:\n        from airflow.exceptions import AirflowException\n        AirflowException(f'{base_dt_col} 컬럼은 YYYY.MM.DD 또는 YYYY/MM/DD 형태가 아닙니다.')\n        # 서울시 코로나19 확진자 발생동향 (2023.05.31.이전) 의 데이텉 명세에 따르면\n        # S_DT: 서울시 기준일 (데이터 기준일) 을 의미\n\n    today_ymd = kwargs.get('data_interval_end').in_timezone('Asia/Seoul').strftime('%Y-%m-%d') #2023-05-01 형태로 할당됨\n      # time stamp를 string형태로 바꿈\n      # today_ymd는 batch(DAG)가 도는 날짜\n    if last_date &gt;= today_ymd: # string 형태지만 크기는 비교 가능\n        print(f'생성 확인(배치 날짜: {today_ymd} / API Last 날짜: {last_date})')\n        return True\n    else:\n        print(f'Update 미완료 (배치 날짜: {today_ymd} / API Last 날짜:{last_date})')\n        return False\n\nsensor_task = PythonSensor(\n    task_id='sensor_task',\n    python_callable=check_api_update,\n    op_kwargs={'http_conn_id':'openapi.seoul.go.kr',\n               'endpoint':'{{var.value.apikey_openapi_seoul_go_kr}}/json/TbCorona19CountStatus',\n               'base_dt_col':'S_DT'},\n    poke_interval=600,   #10분\n    mode='reschedule'\n\n)\n위의 dag의 `check_api_update()` 모든 서울시 공공데이터에 적용할 수 있도록 적고 코로나 데이터에 대한 수행은 `sensor_task`로 이루어지도록 했다. 위와 같이, 코드의 재사용성이 있는 형태로 스크립트를 짜야함\n\n# ExternalTask 센서\n\n## DAG 간 의존관계 설정\n\n* DAG 의존관계 설정 방법\n  * TriggerDagRun 오퍼레이터    \n\n\n\n\n\n::::::{.cell layout-align=\"default\"}\n\n:::::{.cell-output-display}\n\n::::{}\n`&lt;figure class=''&gt;`{=html}\n\n:::{}\n```{=html}\n&lt;svg width=\"672\" height=\"480\" viewbox=\"0.00 0.00 189.55 209.00\" xmlns=\"http://www.w3.org/2000/svg\" xlink=\"http://www.w3.org/1999/xlink\" style=\"; max-width: none; max-height: none\"&gt;\n&lt;g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 205)\"&gt;\n&lt;title&gt;G&lt;/title&gt;\n&lt;polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-205 185.55,-205 185.55,4 -4,4\"&gt;&lt;/polygon&gt;\n&lt;g id=\"clust1\" class=\"cluster\"&gt;\n&lt;title&gt;cluster0&lt;/title&gt;\n&lt;polygon fill=\"none\" stroke=\"black\" points=\"8,-8 8,-193 173.55,-193 173.55,-8 8,-8\"&gt;&lt;/polygon&gt;\n&lt;text text-anchor=\"middle\" x=\"90.78\" y=\"-176.4\" font-family=\"Times,serif\" font-size=\"14.00\"&gt;Task Flow&lt;/text&gt;\n&lt;/g&gt;\n&lt;!-- task1 --&gt;\n&lt;g id=\"node1\" class=\"node\"&gt;\n&lt;title&gt;task1&lt;/title&gt;\n&lt;polygon fill=\"none\" stroke=\"black\" points=\"70,-106 16,-106 16,-70 70,-70 70,-106\"&gt;&lt;/polygon&gt;\n&lt;text text-anchor=\"middle\" x=\"43\" y=\"-83.8\" font-family=\"Times,serif\" font-size=\"14.00\"&gt;task1&lt;/text&gt;\n&lt;/g&gt;\n&lt;!-- task2_1 --&gt;\n&lt;g id=\"node2\" class=\"node\"&gt;\n&lt;title&gt;task2_1&lt;/title&gt;\n&lt;polygon fill=\"none\" stroke=\"black\" points=\"165.33,-52 106.22,-52 106.22,-16 165.33,-16 165.33,-52\"&gt;&lt;/polygon&gt;\n&lt;text text-anchor=\"middle\" x=\"135.78\" y=\"-29.8\" font-family=\"Times,serif\" font-size=\"14.00\"&gt;task2_1&lt;/text&gt;\n&lt;/g&gt;\n&lt;!-- task1&#45;&gt;task2_1 --&gt;\n&lt;g id=\"edge1\" class=\"edge\"&gt;\n&lt;title&gt;task1-&gt;task2_1&lt;/title&gt;\n&lt;path fill=\"none\" stroke=\"black\" d=\"M70.26,-72.38C78.66,-67.38 88.14,-61.74 97.18,-56.36\"&gt;&lt;/path&gt;\n&lt;polygon fill=\"black\" stroke=\"black\" points=\"99.23,-59.22 106.04,-51.1 95.65,-53.2 99.23,-59.22\"&gt;&lt;/polygon&gt;\n&lt;/g&gt;\n&lt;!-- task2_2 --&gt;\n&lt;g id=\"node3\" class=\"node\"&gt;\n&lt;title&gt;task2_2&lt;/title&gt;\n&lt;polygon fill=\"none\" stroke=\"black\" points=\"165.33,-106 106.22,-106 106.22,-70 165.33,-70 165.33,-106\"&gt;&lt;/polygon&gt;\n&lt;text text-anchor=\"middle\" x=\"135.78\" y=\"-83.8\" font-family=\"Times,serif\" font-size=\"14.00\"&gt;task2_2&lt;/text&gt;\n&lt;/g&gt;\n&lt;!-- task1&#45;&gt;task2_2 --&gt;\n&lt;g id=\"edge2\" class=\"edge\"&gt;\n&lt;title&gt;task1-&gt;task2_2&lt;/title&gt;\n&lt;path fill=\"none\" stroke=\"black\" d=\"M70.26,-88C78.22,-88 87.15,-88 95.77,-88\"&gt;&lt;/path&gt;\n&lt;polygon fill=\"black\" stroke=\"black\" points=\"96.04,-91.5 106.04,-88 96.04,-84.5 96.04,-91.5\"&gt;&lt;/polygon&gt;\n&lt;/g&gt;\n&lt;!-- task2_3 --&gt;\n&lt;g id=\"node4\" class=\"node\"&gt;\n&lt;title&gt;task2_3&lt;/title&gt;\n&lt;polygon fill=\"none\" stroke=\"black\" points=\"165.33,-160 106.22,-160 106.22,-124 165.33,-124 165.33,-160\"&gt;&lt;/polygon&gt;\n&lt;text text-anchor=\"middle\" x=\"135.78\" y=\"-137.8\" font-family=\"Times,serif\" font-size=\"14.00\"&gt;task2_3&lt;/text&gt;\n&lt;/g&gt;\n&lt;!-- task1&#45;&gt;task2_3 --&gt;\n&lt;g id=\"edge3\" class=\"edge\"&gt;\n&lt;title&gt;task1-&gt;task2_3&lt;/title&gt;\n&lt;path fill=\"none\" stroke=\"black\" d=\"M70.26,-103.62C78.66,-108.62 88.14,-114.26 97.18,-119.64\"&gt;&lt;/path&gt;\n&lt;polygon fill=\"black\" stroke=\"black\" points=\"95.65,-122.8 106.04,-124.9 99.23,-116.78 95.65,-122.8\"&gt;&lt;/polygon&gt;\n&lt;/g&gt;\n&lt;/g&gt;\n&lt;/svg&gt;\n:::  :::: ::::: ::::::\n* task1: PythonOperator\n* TriggerDagRun Operator: task1이 끝난 후 다른 dag을 돌리고 싶을 경우에 TriggerDagRun Operator사용. task2,3,4는 task1 후에 돌아가는 task로 triggerDagRun Operator에 의해 task2,3,4에 대응되는 각 각 다른 dag을 돌아가도록 수행된다.\n\nExternalTask 센서\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nsensor1\n\nsensor1\n\n\n\ntask2\n\ntask2\n\n\n\nsensor1-&gt;task2\n\n\n\n\n\nsensor2\n\nsensor2\n\n\n\nsensor2-&gt;task2\n\n\n\n\n\nsensor3\n\nsensor3\n\n\n\nsensor3-&gt;task2\n\n\n\n\n\nsensor4\n\nsensor4\n\n\n\nsensor4-&gt;task2\n\n\n\n\n\n\n\n\n\n\n* task2를 돌리기 위해 다른 dag의 task가 완료된 후에 돌아가야 하는 조건이 붙었을 때 사용\n* 예를 들어, 위의 그림 처럼, sensor1는 Dag A, sensor2는 Dag B, sensor3은 Dag C, sensor3은 Dag D에 속해있다고 할 때, 4개의 dag이 완료가 되어야 task2가 돌아갈 수 있다.\n\ndataset\n\nairflow 2.4 version에 나온 기능으로 dag의 dependency를 설정하는 새로운 방법\n\n\n\n\n\n\nairflow.sensors.external_task 명세\n\nBases: airflow.sensors.base.BaseSensorOperator\nWaits for a different DAG, task group, or task to complete for a specific logical date.\n\nParameter\n\n\n\n\n\n\n\n\n\nParameter\n필수여부\n설명\n\n\n\n\nexternal_dag_id\nO\n센싱할 dag 명\n\n\nexternal_task_id\nX (셋 중 하나만 입력 가능 없으면 안써도 되고 없을 경우 dag만 센싱)\n센싱할 task_id 명 (string)\n\n\nexternal_task_ids\nX (셋 중 하나만 입력 가능 없으면 안써도 되고 없을 경우 dag만 센싱)\n센싱할 1 개 이상의 task_id 명 (list)\n\n\nexternal_task_group_id\nX (셋 중 하나만 입력 가능 없으면 dag만 센싱)\n센싱할 task_group_id명\n\n\nallowed_states\nX (같은 상태가 입력되면 안됨)\n센서가 Success 되기 위한 센싱 대상의 상태 (기본적으로 센싱할 task가 success로 끝나야함)\n\n\nskipped_states\nX (같은 상태가 입력되면 안됨)\n센서가 Skipped 되기 위한 센싱 대상의 상태 (기본적으로 none, 아무런 정의가 안되어있음)\n\n\nfailed_states\nX (같은 상태가 입력되면 안됨)\n센서가 Fail 되기 위한 센싱 대상의 상태 (기본적으로 none, 아무런 정의가 안되어있음)\n\n\nexecution_delta\nLogin\n현재 dag과 센싱할 dag의 data_interval_start의 차이 (스케쥴 차이)를 입력\n\n\nexecution_date_fn\nPassword\n스케쥴 차이를 상수로 구할 수 없는 경우, 센싱할 dag의 data_interval_start를 구하기 위한 함수를 넣어주면 됨\n\n\ncheck_existence\nPassword\nsensidng dag의 dag_id 또는 task_id 가 있는지 확인 true or false로 입력해야한다. default=false\n\n\n\n\n위의 allowed_states, skipped_states, failed_states의 states 적을 수 있는 항목은 airflow.utils.state import State (State class) 에 있는 member들만 적을 수 있다.\n\nState.SKIPPED, State.SUCCESS, State.FAILED, State.QUEUED, State.SCHEDULED, State.UP_FOR_RESCHEDULE 등 (from airflow.utils.state import State 필요)\n\n위의 execution_delta 에는 timedelta() 를 이용하여 입력.\n\nExternal Task가 있는 dag보다 센싱할 dag이 얼마나 과거에 있는지 양수로 입력\n예를 들어, 센싱할 dag이 6시간 앞선다면 timedelta(hours=6)으로 입력. 주의) -6으로 쓰면 안됨\n\nexecution_date_fn의 경우 함수가 들어가게 되는데 주로 주 dag의 주기가 다를 때 사용한다.\n\n예를 들어, externalTask의 dag이 시간단위 스케쥴이고 센싱할 dag의 스케쥴이 월단위로 적혀 있을때, data_interval_schedule값이 점점 벌어지게 된다.\n센싱할 dag의 스케쥴: dag(0 1 1 * *)\nexternal task의 dag의 스케쥴 : dag(0 7 * * *)\ndata_interval_schedule = 6h (1일차), 1d+6h(2일차), 2d+6h(3일차), \\(\\ldots\\)\n\n가장 빈번하게 쓰이고 중요한 parameter: external_dag_id와 execution_delta\n\n\n\n\n\nMonitoring or sensing 할 dag: dags_branch_python_operator.py\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.python import BranchPythonOperator\n\nwith DAG(\n    dag_id='dags_branch_python_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'), \n    schedule='0 1 * * *',\n    catchup=False\n) as dag:\n    def select_random():\n        import random\n\n        item_lst = ['A','B','C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a' # task_id를 string 값으로 return해야함\n        elif selected_item in ['B','C']:\n            return ['task_b','task_c'] # 여러 task를 동시에 수행시킬 땐 리스트로 반환\n    \n    # 일반 operator의 parameter도 있음\n    python_branch_task = BranchPythonOperator(\n        task_id='python_branch_task',\n        python_callable=select_random\n    )\n    \n    # 후행 task 3개\n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    python_branch_task &gt;&gt; [task_a, task_b, task_c]\n\nExternalTask dag: dags_external_task_sensor.py\n\nfrom airflow import DAG\nfrom airflow.sensors.external_task import ExternalTaskSensor\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.utils.state import State \n\nwith DAG(\n    dag_id='dags_external_task_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='0 7 * * *',\n    catchup=False\n) as dag:\n    external_task_sensor_a = ExternalTaskSensor(\n        task_id='external_task_sensor_a',\n        external_dag_id='dags_branch_python_operator',\n        external_task_id='task_a',\n        allowed_states=[State.SKIPPED],\n        execution_delta=timedelta(hours=6),\n        poke_interval=10        #10초\n    )\n\n    external_task_sensor_b = ExternalTaskSensor(\n        task_id='external_task_sensor_b',\n        external_dag_id='dags_branch_python_operator',\n        external_task_id='task_b',\n        failed_states=[State.SKIPPED],\n        execution_delta=timedelta(hours=6),\n        poke_interval=10        #10초\n    )\n\n    external_task_sensor_c = ExternalTaskSensor(\n        task_id='external_task_sensor_c',\n        external_dag_id='dags_branch_python_operator',\n        external_task_id='task_c',\n        allowed_states=[State.SUCCESS],\n        execution_delta=timedelta(hours=6),\n        poke_interval=10        #10초\n    )\n\n\n\n\n\n\n\n어떤 센서를 만들 것인가?\n\nPython 센서에서 만들었던 로직을 Custom Sensor화 하기 (서울시 공공데이터에서 날짜 컬럼이 있는 경우 날짜 기준 update되었는지 센싱)\n\n재활용성이 높아 다른 DAG에서 활용될 가능성이 높다면 가급적이면 Custom 오퍼레이터화 해놓는 것이 좋다. 그 이유는\n\n협업 환경에서 코드 중복 구현의 방지\n로직의 일원화 등\n\n위치: plugins/sensors/seoul_api_date_sensors.py\n\n\n\n\nfrom airflow.sensors.base import BaseSensorOperator\nfrom airflow.hooks.base import BaseHook\n\n'''\n서울시 공공데이터 API 추출시 특정 날짜 컬럼을 조사하여 \n배치 날짜 기준 전날 데이터가 존재하는지 체크하는 센서 \n1. 데이터셋에 날짜 컬럼이 존재하고 \n2. API 사용시 그 날짜 컬럼으로 ORDER BY DESC 되어 가져온다는 가정하에 사용 가능\n'''\n\nclass SeoulApiDateSensor(BaseSensorOperator):\n    template_fields = ('endpoint',)\n    def __init__(self, dataset_nm, base_dt_col, day_off=0, **kwargs):\n        '''\n        dataset_nm: 서울시 공공데이터 포털에서 센싱하고자 하는 데이터셋 명\n        base_dt_col: 센싱 기준 컬럼 (yyyy.mm.dd... or yyyy/mm/dd... 형태만 가능)\n        day_off: 배치일 기준 생성여부를 확인하고자 하는 날짜 차이를 입력 (기본값: 0). 즉, 1일 전, 2일 전 데이터가 업데이트 되었는지 확인하는 것.\n        '''\n        super().__init__(**kwargs)\n        self.http_conn_id = 'openapi.seoul.go.kr'\n        self.endpoint = '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + dataset_nm + '/1/100'   # 100건만 추출\n        self.base_dt_col = base_dt_col\n        self.day_off = day_off\n\n        \n    def poke(self, context): # context에 templates 변수들을 호출할 수 있음\n        import requests\n        import json\n        from dateutil.relativedelta import relativedelta\n        connection = BaseHook.get_connection(self.http_conn_id)\n        url = f'http://{connection.host}:{connection.port}/{self.endpoint}'\n        self.log.info(f'request url:{url}')\n        response = requests.get(url)\n\n        contents = json.loads(response.text)\n        key_nm = list(contents.keys())[0]\n        row_data = contents.get(key_nm).get('row')\n        last_dt = row_data[0].get(self.base_dt_col)\n        last_date = last_dt[:10]\n        last_date = last_date.replace('.', '-').replace('/', '-')\n        search_ymd = (context.get('data_interval_end').in_timezone('Asia/Seoul') + relativedelta(days=self.day_off)).strftime('%Y-%m-%d')\n        try:\n            import pendulum\n            pendulum.from_format(last_date, 'YYYY-MM-DD')\n        except:\n            from airflow.exceptions import AirflowException\n            AirflowException(f'{self.base_dt_col} 컬럼은 YYYY.MM.DD 또는 YYYY/MM/DD 형태가 아닙니다.')\n\n        \n        if last_date &gt;= search_ymd: # last_date: API의 날짜, search_ymd: 조회할 날짜\n            self.log.info(f'생성 확인(기준 날짜: {search_ymd} / API Last 날짜: {last_date})')\n            return True\n        else:\n            self.log.info(f'Update 미완료 (기준 날짜: {search_ymd} / API Last 날짜:{last_date})')\n            return False\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/10.sensor.html#센서의-개념",
    "href": "docs/blog/posts/Engineering/airflow/10.sensor.html#센서의-개념",
    "title": "Sensor",
    "section": "",
    "text": "일종의 특화된 오퍼레이터\n특정 조건이 만족되기를 주기적으로 확인 및 기다리고 만족되면 True를 반환하는 Task\n모든 센서는 BaseSensorOperator를 상속하여 구현되며 (BaseSensorOperator는 BaseOperator를 상속함) 상속시에는 init() 함수와 poke(context) 함수 재정의 해야한다\n센싱하는 로직은 poke 함수에 정의: 특정 조건이 만족하는지 체크하고 true를 return 하도록 정의"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/10.sensor.html#basesensor-오퍼레이터-명세-확인",
    "href": "docs/blog/posts/Engineering/airflow/10.sensor.html#basesensor-오퍼레이터-명세-확인",
    "title": "Sensor",
    "section": "",
    "text": "airflow BaseSensorOperator\n\nBases: airflow.models.baseoperator.BaseOperator, airflow.models.skipmixin.SkipMixin Sensor operators are derived from this class and inherit these attributes. Sensor operators keep executing at a time interval and succeed when a criteria is met and fail if and when they time out. base operator를 상속했음.\nparameter\n\npoke_interval (float) – Time in seconds that the job should wait in between each try: sensor task의 특정 조건이 만족하는지 체크하는 주기로 초단위로 입력하면 된다. 60 = 1mins 은 1분 단위로 체크\ntimeout (float) – Time, in seconds before the task times out and fails. task가 계속 false 가 나올때 task failure 로 규정할 maximum 시간. 초단위로 입력. 보통 daily dag을 많이 만드므로 timeout도 보통 24시간 으로 입력한다. ex) 606024 (=24 시간)\nsoft_fail (bool) – Set to true to mark the task as SKIPPED on failure. timeout을 만났을 때 sensor task fail로 marking하지 말고 skip으로 marking하도록 설정\nmode (str) (중요) – How the sensor operates. ‘poke’ 아니면 ‘reschedule’ 로만 값을 넣을 수 있음. 중요하기 때문에 아래에서 따로 설명. Options are: { poke | reschedule }, default is poke. When set to poke the sensor is taking up a worker slot for its whole execution time and sleeps between pokes. Use this mode if the expected runtime of the sensor is short or if a short poke interval is required. Note that the sensor will hold onto a worker slot and a pool slot for the duration of the sensor’s runtime in this mode. When set to reschedule the sensor task frees the worker slot when the criteria is not yet met and it’s rescheduled at a later time. Use this mode if the time before the criteria is met is expected to be quite long. The poke interval should be more than one minute to prevent too much load on the scheduler.\nexponential_backoff (bool) – allow progressive longer waits between pokes by using exponential backoff algorithm. sensor task를 체크하는 주기가 \\(2^n\\) 으로 늘어지기 된다. 즉, 2초, 4초, 8초, \\(\\ldots\\)\nmax_wait (datetime.timedelta | float | None)\n\nmaximum wait interval between pokes, can be timedelta or float seconds.\nexponential_backoff가 true 일 때만 홠성화 되며 exponential_backoff 의 상한선을 의미\n\nsilent_fail (bool) – If true, and poke method raises an exception different from AirflowSensorTimeout, AirflowTaskTimeout, AirflowSkipException and AirflowFailException, the sensor will log the error and continue its execution. Otherwise, the sensor task fails, and it can be retried based on the provided retries parameter.\n\npoke(context)[source]: Function defined by the sensors while deriving this class should override.\n\npoke method를 재정의 하지 않으면 error 발생하게 되어 있음\n[docs]    def poke(self, context: Context) -&gt; bool | PokeReturnValue:\n        \"\"\"Function defined by the sensors while deriving this class should override.\"\"\"\n        raise AirflowException(\"Override me.\")\n\nexecute(self, context: Context): 재정의할 필요없음. 이미 정의가 되어 있음. overiding된 poke함수의 값이 있어야 밑의 while loop를 탈출하게 되어 있음. 다시 말해서, poke_return = self.poke(context)이 false를 반환하게 되면 infinite loop. 결국 poke() 가 중요\n[docs]    def execute(self, context: Context) -&gt; Any:\n      started_at: datetime.datetime | float \n\n      (...)\n\n      while True:\n          try:\n              poke_return = self.poke(context)\n          except (\n              AirflowSensorTimeout,\n              AirflowTaskTimeout,\n              AirflowSkipException,\n              AirflowFailException,\n          ) as e:\n              raise e\n          except Exception as e:\n              if self.silent_fail:\n                  logging.error(\"Sensor poke failed: \\n %s\", traceback.format_exc())\n                  poke_return = False\n              else:\n                  raise e\n\n          if poke_return: # poke_return = true이면 while loop 탈출\n              if isinstance(poke_return, PokeReturnValue):\n                  xcom_value = poke_return.xcom_value\n              break\n        (...)       \nBaseSensor 오퍼레이터 Mode 유형\n\nmode 유형\n\n\n\n\n\n\n\n\n\nComparison\nPoke Mode\nReschedule Mode\n\n\n\n\n원리\nDAG이 수행되는 내내 Running Slot(task가 수행될 때 차지하는 공간) 을 차지. sensor가 특정 조건을 체킹할때나 안할때나 항상 slot 차지. 다만 Slot 안에서 Sleep, active 를 반복\n센서가 조건을 체킹하는 동작 시기에만 Slot을 차지. 그 외에는 Slot을 점유하지 않음. slot을 들어갔다 나왔다를 반복\n\n\nWait에서의 Task 상태\nrunning (airflow web ui 에서 task bar가 연두색)\nup_for_reschedule (task bar가 민트색)\n\n\n유리한 적용 시점\n짧은 센싱 간격 (interval, 초 단위)\n긴 센싱 간격, 주로 분 단위 Reschedule될 때 (5분, 10분) 스케줄러의 부하 발생\n\n\n\nSlot의 이해\n\nPool\n\n모든 operator로 만들어진 Task는 특정 Pool에서 수행되며 Pool은 Slot이라는 것을 가지고 있음.\n기본적으로 Task 1개당 Slot 1개를 점유하며 Pool을 지정하지 않으면 default_pool에서 수행 \nairflow web service ui &gt;&gt; admin &gt;&gt; pools\n\npool: pool name\nslots: 128개의 공간\nRunning Slots, Queued Slots, Schedulued Slots.\n\n\n\n사용자 입장에서는 operator의 mode의 이해는 그렇게 중요하진 않지만 airflow를 운영하는 사람 입장에서는 중요한 변수가 될 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/10.sensor.html#bash-센서-명세-확인",
    "href": "docs/blog/posts/Engineering/airflow/10.sensor.html#bash-센서-명세-확인",
    "title": "Sensor",
    "section": "",
    "text": "airflow.sensors.bash\n\nParameters\n\nbash_command – 조건문을 여기에다가 적음\n\nReturn True if and only if the return code is 0.\nshell 스크립트에서 return True를 주는 방법\n\n파이썬에서의 return True와 같은 의미로 shell script에서는 exit 0 를 사용\n\n모든 shell은 수행을 마친 후 EXIT_STATUS를 가지고 있으며 0~255 사이의 값을 가짐.\n\nEXIT 0 만 정상이며 나머지는 모두 비정상의 의미를 가짐\n마지막 명령 수행의 EXIT_STATUS를 확인하려면 echo $? 로 확인\n\nkmkim@K100230201051:~/airflow$ ls\nairflow  custom_image  docker-compose.20230708  files  plugins\nconfig   dags          docker-compose.yaml      logs\nkmkim@K100230201051:~/airflow$ echo $?\n0 # ls란 명령이 정상적으로 실행됐기 때문에 0을 반환\nkmkim@K100230201051:~/airflow$ ls sdf\nls: cannot access 'sdf': No such file or directory\nkmkim@K100230201051:~/airflow$ echo $?\n2\nkmkim@K100230201051:~/airflow$ sdfsd\nsdfsd: command not found\nkmkim@K100230201051:~/airflow$ echo $?\n127\nexit status 변경하기\n\nvi test.sh #exit status 변경할 shell script\n# vi editor: test.sh\nls\nexit 1\nchmod +x test.sh #실행권한 부여\n./test.sh #실행\necho $? #1 출력됨\nenv – If env is not None, it must be a mapping that defines the environment variables for the new process; these are used instead of inheriting the current process environment, which is the default behavior. (templated)\noutput_encoding – output encoding of bash command.\nretry_exit_code (int | None) – If task exits with this code, treat the sensor as not-yet-complete and retry the check later according to the usual retry/timeout settings. Any other non-zero return code will be treated as an error, and cause the sensor to fail. If set to None (the default), any non-zero exit code will cause a retry and the task will never raise an error except on time-out.\n\n\nDag Example\n\ncsv file 있는지 없는지 확인\n\n\nfrom airflow.sensors.bash import BashSensor\nfrom airflow.operators.bash import BashOperator\nfrom airflow import DAG\nimport pendulum\n\nwith DAG(\n    dag_id='dags_bash_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='0 6 * * *',\n    catchup=False\n) as dag:\n\n    sensor_task_by_poke = BashSensor(\n        task_id='sensor_task_by_poke',\n        # 오늘 날짜로 tvCorona19VaccinestatNew.csv 가 있는지 없는지 확인 \n        env={'FILE':'/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv'},\n        bash_command=f'''echo $FILE && \n                        if [ -f $FILE ]; then \n                              exit 0\n                        else \n                              exit 1\n                        fi''',\n        poke_interval=30,      #30초\n        timeout=60*2,          #2분\n        mode='poke',\n        soft_fail=False\n    )\n\n    sensor_task_by_reschedule = BashSensor(\n        task_id='sensor_task_by_reschedule',\n        env={'FILE':'/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv'},\n        bash_command=f'''echo $FILE && \n                        if [ -f $FILE ]; then \n                              exit 0\n                        else \n                              exit 1\n                        fi''',\n        poke_interval=60*3,    # 3분\n        timeout=60*9,          #9분\n        mode='reschedule',\n        soft_fail=True\n    )\n\n    bash_task = BashOperator(\n        task_id='bash_task',\n        env={'FILE': '/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv'},\n        bash_command='echo \"건수: `cat $FILE | wc -l`\"',\n    )\n\n    [sensor_task_by_poke,sensor_task_by_reschedule] &gt;&gt; bash_task"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/10.sensor.html#file-센서-명세-확인",
    "href": "docs/blog/posts/Engineering/airflow/10.sensor.html#file-센서-명세-확인",
    "title": "Sensor",
    "section": "",
    "text": "airflow.sensors.filesystem\n\nWaits for a file or folder to land in a filesystem. (file이나 folder가 file system안에 들어왔는지 체크)\nParameters\n\nfs_conn_id – reference to the File (path) connection id (file connection id을 airflow web에 미리 등록해놔야함)\nfilepath – File or folder name (relative to the base path set within the connection), can be a glob. 상대 경로를 입력해야함\nrecursive – when set to True, enables recursive directory matching behavior of ** in glob filepath parameter. Defaults to False. (glob(**))\n\n구체적인 로직을 이해하기 위해 source를 봐야함\n\n모든 sensor를 만들때는 poke를 재정의해야만 함.\n즉, poke만 잘 이해하면 거의 모든 sensor를 잘 이해할 수 있다.\npoke()의 FSHook() 은 imported from airflow.hooks.filesystem\n\nhook = FSHook(self.fs_conn_id): hook을 만들 때 fs_conn_id를 넘겨받고 있음\n\nglob()을 이해하는 것이 핵심\n\nfile path안에 있는 files 또는 directories를 list로 반환\n\nglob('/home/kmkim') # 이 path 안에 있는 모든 files, directories이 리스트로 반환됨\nglob('/home/kmkim/docker-compose.yaml') # docker-compose.yaml 자체가 리스트로 반환됨\nglob('/home/kmkim/**', recursive=True) # /home/kmkim/ 하위에 있는 모든 files 또는 directories이 리스트로 반환됨\n\n주로 file을 찾기 위해 사용되는 함수이기 때문에 glob(‘/home/kmkim/docker-compose.yaml’) 이 형태가 가장 많이 사용된다.\n\n\n\nfrom __future__ import annotations\n\nimport datetime\nimport os\nfrom glob import glob\nfrom typing import Sequence\n\nfrom airflow.hooks.filesystem import FSHook\nfrom airflow.sensors.base import BaseSensorOperator\nfrom airflow.utils.context import Context\n\n\n[docs]class FileSensor(BaseSensorOperator):\n    \"\"\"\n    Waits for a file or folder to land in a filesystem.\n\n    If the path given is a directory then this sensor will only return true if\n    any files exist inside it (either directly, or within a subdirectory)\n\n    :param fs_conn_id: reference to the File (path)\n        connection id\n    :param filepath: File or folder name (relative to\n        the base path set within the connection), can be a glob.\n    :param recursive: when set to ``True``, enables recursive directory matching behavior of\n        ``**`` in glob filepath parameter. Defaults to ``False``.\n\n    .. seealso::\n        For more information on how to use this sensor, take a look at the guide:\n        :ref:`howto/operator:FileSensor`\n\n\n    \"\"\"\n\n[docs]    template_fields: Sequence[str] = (\"filepath\",)\n[docs]    ui_color = \"#91818a\"\n\n    def __init__(self, *, filepath, fs_conn_id=\"fs_default\", recursive=False, **kwargs):\n        super().__init__(**kwargs)\n        self.filepath = filepath\n        self.fs_conn_id = fs_conn_id\n        self.recursive = recursive\n\n[docs]    def poke(self, context: Context):\n        hook = FSHook(self.fs_conn_id)\n        basepath = hook.get_path() # hook이 가지고 있는 method\n        full_path = os.path.join(basepath, self.filepath)\n        self.log.info(\"Poking for file %s\", full_path)\n\n        for path in glob(full_path, recursive=self.recursive):\n            if os.path.isfile(path): # glob에 의한 결과물이 file이면 for loop 탈출\n                mod_time = datetime.datetime.fromtimestamp(os.path.getmtime(path)).strftime(\"%Y%m%d%H%M%S\")\n                self.log.info(\"Found File %s last modified: %s\", str(path), mod_time)\n                return True\n\n            for _, _, files in os.walk(path):\n                if len(files) &gt; 0:\n                    return True\n        return False\n\nglob: file path안에 있는 file이나 directory를 찾아서 list로 반환\n\nglob('/home/kmkim') : 모든 files and directories list 반환\nglob('/home/kmkim/docker-compose.yaml') : docker-compose.yaml만 리스트로 반환\nglob('/home/kmkim/**',recursive=True) : 하위 디렉토리 안에 있는 파일과 디렉토리들의 리스트로 반환\n\nFSHook() : 명세서\n[docs]class FSHook(BaseHook):\n\"\"\"\nAllows for interaction with an file server.\n\nConnection should have a name and a path specified under extra:\n\nexample:\nConnection Id: fs_test\nConnection Type: File (path)\nHost, Schema, Login, Password, Port: empty\nExtra: {\"path\": \"/tmp\"} # dictionary 형태\n\"\"\"\n\ndef __init__(self, conn_id: str = \"fs_default\"):\n    super().__init__()\n    conn = self.get_connection(conn_id)\n    self.basepath = conn.extra_dejson.get(\"path\", \"\")\n    self.conn = conn\n\n[docs]    def get_conn(self) -&gt; None:\n        pass\n\n\n[docs]    def get_path(self) -&gt; str:\n        \"\"\"\n        Get the path to the filesystem location.\n\n        :return: the path.\n        \"\"\"\n        return self.basepath\n\nget_path(): self.baspath return - 별 내용 없음\n생성자만 잘 이해하면 됨\n\nConnection should have a name and a path specified under extra (예시 잘 볼 것): \n\n\n\nConnection 작성\n\n\n\nConnection_id\nconn_file_opt_airflow_files\n\n\n\n\nConnection_type\nFile (path)\n\n\nHost\n\n\n\nSchema\n\n\n\nLogin\n\n\n\nPassword\n\n\n\nPort\n\n\n\nExtra\n{“path”:“/opt/airflow/files”}\n\n\n\nDag 작성\n\nfrom airflow import DAG\nfrom airflow.sensors.filesystem import FileSensor\nimport pendulum\n\nwith DAG(\n    dag_id='dags_file_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='0 7 * * *',\n    catchup=False\n) as dag:\n    tvCorona19VaccinestatNew_sensor = FileSensor(\n        task_id='tvCorona19VaccinestatNew_sensor',\n        fs_conn_id='conn_file_opt_airflow_files',\n        filepath='tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv',\n        recursive=False,\n        poke_interval=60,\n        timeout=60*60*24, # 1일\n        mode='reschedule'\n    )"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/10.sensor.html#python-센서-dag-작성",
    "href": "docs/blog/posts/Engineering/airflow/10.sensor.html#python-센서-dag-작성",
    "title": "Sensor",
    "section": "",
    "text": "무엇을 센싱할 것인가\n\n서울시 공공데이터에서 당일 날짜로 데이터가 생성되었는지 센싱하기(날짜 컬럼이 있는 경우)\n당일 날짜가 몇시에 업로드가 되는지 모름 (12시? ,00시, 1시?)\n\n서울 열린 데이터 광장\n\n검색창 &gt;&gt; ‘코로나’ 검색 &gt;&gt; 서울시 코로나19 확진자 발생동향 (2023.05.31.이전) &gt;&gt; 미리보기\n목표: 센서를 공용적으로 만들어 다른 dataset에도 적용할 수 있도록 작성\n\npython collable function(check_api_update())을 일반화 시켜서 작성 ```markdown from airflow import DAG from airflow.sensors.python import PythonSensor import pendulum from airflow.hooks.base import BaseHook\n\n\n\nwith DAG( dag_id=‘dags_python_sensor’, start_date=pendulum.datetime(2023,4,1, tz=‘Asia/Seoul’), schedule=’10 1 * * *’, catchup=False ) as dag: def check_api_update(http_conn_id, endpoint, base_dt_col, **kwargs): import requests import json from dateutil import relativedelta connection = BaseHook.get_connection(http_conn_id) url = f’http://{connection.host}:{connection.port}/{endpoint}/1/100’ #1부터 100행 까지만 가지고옴 response = requests.get(url)\n    contents = json.loads(response.text)\n    key_nm = list(contents.keys())[0]\n    row_data = contents.get(key_nm).get('row') \n      # row_data에 list형태로 데이터가 들어감\n      # [{1행},\n         {2행},\n         {3행},\n         {4행},...]\n    last_dt = row_data[0].get(base_dt_col) #row_data의 첫번째 행, base_dt_col의 key에 대한 value를 추출\n    last_date = last_dt[:10] \n      # 열번째 글자 까지만 slicing\n      # 왜냐면, 서울시 기준일(S_DT)가 date 형식이 아니라 string 형식으로 입력되어 있음\n      # 연/월/일 정보만 필요하기 때문에 시간은 제외\n    last_date = last_date.replace('.', '-').replace('/', '-')\n\n    # 밑에 try구문은 last_date 의 date 형식을 검증\n    try:\n        pendulum.from_format(last_date,'YYYY-MM-DD') # last_date가 'YYYY-MM-DD' 형식으로 parsing이 가능하면 코드 진행이 계속되고 안되면 except문으로 들어가게됨\n    except:\n        from airflow.exceptions import AirflowException\n        AirflowException(f'{base_dt_col} 컬럼은 YYYY.MM.DD 또는 YYYY/MM/DD 형태가 아닙니다.')\n        # 서울시 코로나19 확진자 발생동향 (2023.05.31.이전) 의 데이텉 명세에 따르면\n        # S_DT: 서울시 기준일 (데이터 기준일) 을 의미\n\n    today_ymd = kwargs.get('data_interval_end').in_timezone('Asia/Seoul').strftime('%Y-%m-%d') #2023-05-01 형태로 할당됨\n      # time stamp를 string형태로 바꿈\n      # today_ymd는 batch(DAG)가 도는 날짜\n    if last_date &gt;= today_ymd: # string 형태지만 크기는 비교 가능\n        print(f'생성 확인(배치 날짜: {today_ymd} / API Last 날짜: {last_date})')\n        return True\n    else:\n        print(f'Update 미완료 (배치 날짜: {today_ymd} / API Last 날짜:{last_date})')\n        return False\n\nsensor_task = PythonSensor(\n    task_id='sensor_task',\n    python_callable=check_api_update,\n    op_kwargs={'http_conn_id':'openapi.seoul.go.kr',\n               'endpoint':'{{var.value.apikey_openapi_seoul_go_kr}}/json/TbCorona19CountStatus',\n               'base_dt_col':'S_DT'},\n    poke_interval=600,   #10분\n    mode='reschedule'\n\n)\n위의 dag의 `check_api_update()` 모든 서울시 공공데이터에 적용할 수 있도록 적고 코로나 데이터에 대한 수행은 `sensor_task`로 이루어지도록 했다. 위와 같이, 코드의 재사용성이 있는 형태로 스크립트를 짜야함\n\n# ExternalTask 센서\n\n## DAG 간 의존관계 설정\n\n* DAG 의존관계 설정 방법\n  * TriggerDagRun 오퍼레이터    \n\n\n\n\n\n::::::{.cell layout-align=\"default\"}\n\n:::::{.cell-output-display}\n\n::::{}\n`&lt;figure class=''&gt;`{=html}\n\n:::{}\n```{=html}\n&lt;svg width=\"672\" height=\"480\" viewbox=\"0.00 0.00 189.55 209.00\" xmlns=\"http://www.w3.org/2000/svg\" xlink=\"http://www.w3.org/1999/xlink\" style=\"; max-width: none; max-height: none\"&gt;\n&lt;g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 205)\"&gt;\n&lt;title&gt;G&lt;/title&gt;\n&lt;polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-205 185.55,-205 185.55,4 -4,4\"&gt;&lt;/polygon&gt;\n&lt;g id=\"clust1\" class=\"cluster\"&gt;\n&lt;title&gt;cluster0&lt;/title&gt;\n&lt;polygon fill=\"none\" stroke=\"black\" points=\"8,-8 8,-193 173.55,-193 173.55,-8 8,-8\"&gt;&lt;/polygon&gt;\n&lt;text text-anchor=\"middle\" x=\"90.78\" y=\"-176.4\" font-family=\"Times,serif\" font-size=\"14.00\"&gt;Task Flow&lt;/text&gt;\n&lt;/g&gt;\n&lt;!-- task1 --&gt;\n&lt;g id=\"node1\" class=\"node\"&gt;\n&lt;title&gt;task1&lt;/title&gt;\n&lt;polygon fill=\"none\" stroke=\"black\" points=\"70,-106 16,-106 16,-70 70,-70 70,-106\"&gt;&lt;/polygon&gt;\n&lt;text text-anchor=\"middle\" x=\"43\" y=\"-83.8\" font-family=\"Times,serif\" font-size=\"14.00\"&gt;task1&lt;/text&gt;\n&lt;/g&gt;\n&lt;!-- task2_1 --&gt;\n&lt;g id=\"node2\" class=\"node\"&gt;\n&lt;title&gt;task2_1&lt;/title&gt;\n&lt;polygon fill=\"none\" stroke=\"black\" points=\"165.33,-52 106.22,-52 106.22,-16 165.33,-16 165.33,-52\"&gt;&lt;/polygon&gt;\n&lt;text text-anchor=\"middle\" x=\"135.78\" y=\"-29.8\" font-family=\"Times,serif\" font-size=\"14.00\"&gt;task2_1&lt;/text&gt;\n&lt;/g&gt;\n&lt;!-- task1&#45;&gt;task2_1 --&gt;\n&lt;g id=\"edge1\" class=\"edge\"&gt;\n&lt;title&gt;task1-&gt;task2_1&lt;/title&gt;\n&lt;path fill=\"none\" stroke=\"black\" d=\"M70.26,-72.38C78.66,-67.38 88.14,-61.74 97.18,-56.36\"&gt;&lt;/path&gt;\n&lt;polygon fill=\"black\" stroke=\"black\" points=\"99.23,-59.22 106.04,-51.1 95.65,-53.2 99.23,-59.22\"&gt;&lt;/polygon&gt;\n&lt;/g&gt;\n&lt;!-- task2_2 --&gt;\n&lt;g id=\"node3\" class=\"node\"&gt;\n&lt;title&gt;task2_2&lt;/title&gt;\n&lt;polygon fill=\"none\" stroke=\"black\" points=\"165.33,-106 106.22,-106 106.22,-70 165.33,-70 165.33,-106\"&gt;&lt;/polygon&gt;\n&lt;text text-anchor=\"middle\" x=\"135.78\" y=\"-83.8\" font-family=\"Times,serif\" font-size=\"14.00\"&gt;task2_2&lt;/text&gt;\n&lt;/g&gt;\n&lt;!-- task1&#45;&gt;task2_2 --&gt;\n&lt;g id=\"edge2\" class=\"edge\"&gt;\n&lt;title&gt;task1-&gt;task2_2&lt;/title&gt;\n&lt;path fill=\"none\" stroke=\"black\" d=\"M70.26,-88C78.22,-88 87.15,-88 95.77,-88\"&gt;&lt;/path&gt;\n&lt;polygon fill=\"black\" stroke=\"black\" points=\"96.04,-91.5 106.04,-88 96.04,-84.5 96.04,-91.5\"&gt;&lt;/polygon&gt;\n&lt;/g&gt;\n&lt;!-- task2_3 --&gt;\n&lt;g id=\"node4\" class=\"node\"&gt;\n&lt;title&gt;task2_3&lt;/title&gt;\n&lt;polygon fill=\"none\" stroke=\"black\" points=\"165.33,-160 106.22,-160 106.22,-124 165.33,-124 165.33,-160\"&gt;&lt;/polygon&gt;\n&lt;text text-anchor=\"middle\" x=\"135.78\" y=\"-137.8\" font-family=\"Times,serif\" font-size=\"14.00\"&gt;task2_3&lt;/text&gt;\n&lt;/g&gt;\n&lt;!-- task1&#45;&gt;task2_3 --&gt;\n&lt;g id=\"edge3\" class=\"edge\"&gt;\n&lt;title&gt;task1-&gt;task2_3&lt;/title&gt;\n&lt;path fill=\"none\" stroke=\"black\" d=\"M70.26,-103.62C78.66,-108.62 88.14,-114.26 97.18,-119.64\"&gt;&lt;/path&gt;\n&lt;polygon fill=\"black\" stroke=\"black\" points=\"95.65,-122.8 106.04,-124.9 99.23,-116.78 95.65,-122.8\"&gt;&lt;/polygon&gt;\n&lt;/g&gt;\n&lt;/g&gt;\n&lt;/svg&gt;\n:::  :::: ::::: ::::::\n* task1: PythonOperator\n* TriggerDagRun Operator: task1이 끝난 후 다른 dag을 돌리고 싶을 경우에 TriggerDagRun Operator사용. task2,3,4는 task1 후에 돌아가는 task로 triggerDagRun Operator에 의해 task2,3,4에 대응되는 각 각 다른 dag을 돌아가도록 수행된다.\n\nExternalTask 센서\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nsensor1\n\nsensor1\n\n\n\ntask2\n\ntask2\n\n\n\nsensor1-&gt;task2\n\n\n\n\n\nsensor2\n\nsensor2\n\n\n\nsensor2-&gt;task2\n\n\n\n\n\nsensor3\n\nsensor3\n\n\n\nsensor3-&gt;task2\n\n\n\n\n\nsensor4\n\nsensor4\n\n\n\nsensor4-&gt;task2\n\n\n\n\n\n\n\n\n\n\n* task2를 돌리기 위해 다른 dag의 task가 완료된 후에 돌아가야 하는 조건이 붙었을 때 사용\n* 예를 들어, 위의 그림 처럼, sensor1는 Dag A, sensor2는 Dag B, sensor3은 Dag C, sensor3은 Dag D에 속해있다고 할 때, 4개의 dag이 완료가 되어야 task2가 돌아갈 수 있다.\n\ndataset\n\nairflow 2.4 version에 나온 기능으로 dag의 dependency를 설정하는 새로운 방법"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/10.sensor.html#externaltask-센서-명세-확인",
    "href": "docs/blog/posts/Engineering/airflow/10.sensor.html#externaltask-센서-명세-확인",
    "title": "Sensor",
    "section": "",
    "text": "airflow.sensors.external_task 명세\n\nBases: airflow.sensors.base.BaseSensorOperator\nWaits for a different DAG, task group, or task to complete for a specific logical date.\n\nParameter\n\n\n\n\n\n\n\n\n\nParameter\n필수여부\n설명\n\n\n\n\nexternal_dag_id\nO\n센싱할 dag 명\n\n\nexternal_task_id\nX (셋 중 하나만 입력 가능 없으면 안써도 되고 없을 경우 dag만 센싱)\n센싱할 task_id 명 (string)\n\n\nexternal_task_ids\nX (셋 중 하나만 입력 가능 없으면 안써도 되고 없을 경우 dag만 센싱)\n센싱할 1 개 이상의 task_id 명 (list)\n\n\nexternal_task_group_id\nX (셋 중 하나만 입력 가능 없으면 dag만 센싱)\n센싱할 task_group_id명\n\n\nallowed_states\nX (같은 상태가 입력되면 안됨)\n센서가 Success 되기 위한 센싱 대상의 상태 (기본적으로 센싱할 task가 success로 끝나야함)\n\n\nskipped_states\nX (같은 상태가 입력되면 안됨)\n센서가 Skipped 되기 위한 센싱 대상의 상태 (기본적으로 none, 아무런 정의가 안되어있음)\n\n\nfailed_states\nX (같은 상태가 입력되면 안됨)\n센서가 Fail 되기 위한 센싱 대상의 상태 (기본적으로 none, 아무런 정의가 안되어있음)\n\n\nexecution_delta\nLogin\n현재 dag과 센싱할 dag의 data_interval_start의 차이 (스케쥴 차이)를 입력\n\n\nexecution_date_fn\nPassword\n스케쥴 차이를 상수로 구할 수 없는 경우, 센싱할 dag의 data_interval_start를 구하기 위한 함수를 넣어주면 됨\n\n\ncheck_existence\nPassword\nsensidng dag의 dag_id 또는 task_id 가 있는지 확인 true or false로 입력해야한다. default=false\n\n\n\n\n위의 allowed_states, skipped_states, failed_states의 states 적을 수 있는 항목은 airflow.utils.state import State (State class) 에 있는 member들만 적을 수 있다.\n\nState.SKIPPED, State.SUCCESS, State.FAILED, State.QUEUED, State.SCHEDULED, State.UP_FOR_RESCHEDULE 등 (from airflow.utils.state import State 필요)\n\n위의 execution_delta 에는 timedelta() 를 이용하여 입력.\n\nExternal Task가 있는 dag보다 센싱할 dag이 얼마나 과거에 있는지 양수로 입력\n예를 들어, 센싱할 dag이 6시간 앞선다면 timedelta(hours=6)으로 입력. 주의) -6으로 쓰면 안됨\n\nexecution_date_fn의 경우 함수가 들어가게 되는데 주로 주 dag의 주기가 다를 때 사용한다.\n\n예를 들어, externalTask의 dag이 시간단위 스케쥴이고 센싱할 dag의 스케쥴이 월단위로 적혀 있을때, data_interval_schedule값이 점점 벌어지게 된다.\n센싱할 dag의 스케쥴: dag(0 1 1 * *)\nexternal task의 dag의 스케쥴 : dag(0 7 * * *)\ndata_interval_schedule = 6h (1일차), 1d+6h(2일차), 2d+6h(3일차), \\(\\ldots\\)\n\n가장 빈번하게 쓰이고 중요한 parameter: external_dag_id와 execution_delta"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/10.sensor.html#dag-full-example",
    "href": "docs/blog/posts/Engineering/airflow/10.sensor.html#dag-full-example",
    "title": "Sensor",
    "section": "",
    "text": "Monitoring or sensing 할 dag: dags_branch_python_operator.py\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.python import BranchPythonOperator\n\nwith DAG(\n    dag_id='dags_branch_python_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'), \n    schedule='0 1 * * *',\n    catchup=False\n) as dag:\n    def select_random():\n        import random\n\n        item_lst = ['A','B','C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a' # task_id를 string 값으로 return해야함\n        elif selected_item in ['B','C']:\n            return ['task_b','task_c'] # 여러 task를 동시에 수행시킬 땐 리스트로 반환\n    \n    # 일반 operator의 parameter도 있음\n    python_branch_task = BranchPythonOperator(\n        task_id='python_branch_task',\n        python_callable=select_random\n    )\n    \n    # 후행 task 3개\n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    python_branch_task &gt;&gt; [task_a, task_b, task_c]\n\nExternalTask dag: dags_external_task_sensor.py\n\nfrom airflow import DAG\nfrom airflow.sensors.external_task import ExternalTaskSensor\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.utils.state import State \n\nwith DAG(\n    dag_id='dags_external_task_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='0 7 * * *',\n    catchup=False\n) as dag:\n    external_task_sensor_a = ExternalTaskSensor(\n        task_id='external_task_sensor_a',\n        external_dag_id='dags_branch_python_operator',\n        external_task_id='task_a',\n        allowed_states=[State.SKIPPED],\n        execution_delta=timedelta(hours=6),\n        poke_interval=10        #10초\n    )\n\n    external_task_sensor_b = ExternalTaskSensor(\n        task_id='external_task_sensor_b',\n        external_dag_id='dags_branch_python_operator',\n        external_task_id='task_b',\n        failed_states=[State.SKIPPED],\n        execution_delta=timedelta(hours=6),\n        poke_interval=10        #10초\n    )\n\n    external_task_sensor_c = ExternalTaskSensor(\n        task_id='external_task_sensor_c',\n        external_dag_id='dags_branch_python_operator',\n        external_task_id='task_c',\n        allowed_states=[State.SUCCESS],\n        execution_delta=timedelta(hours=6),\n        poke_interval=10        #10초\n    )"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/10.sensor.html#custom-sensor-만들기",
    "href": "docs/blog/posts/Engineering/airflow/10.sensor.html#custom-sensor-만들기",
    "title": "Sensor",
    "section": "",
    "text": "어떤 센서를 만들 것인가?\n\nPython 센서에서 만들었던 로직을 Custom Sensor화 하기 (서울시 공공데이터에서 날짜 컬럼이 있는 경우 날짜 기준 update되었는지 센싱)\n\n재활용성이 높아 다른 DAG에서 활용될 가능성이 높다면 가급적이면 Custom 오퍼레이터화 해놓는 것이 좋다. 그 이유는\n\n협업 환경에서 코드 중복 구현의 방지\n로직의 일원화 등\n\n위치: plugins/sensors/seoul_api_date_sensors.py"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/10.sensor.html#dag-example",
    "href": "docs/blog/posts/Engineering/airflow/10.sensor.html#dag-example",
    "title": "Sensor",
    "section": "",
    "text": "from airflow.sensors.base import BaseSensorOperator\nfrom airflow.hooks.base import BaseHook\n\n'''\n서울시 공공데이터 API 추출시 특정 날짜 컬럼을 조사하여 \n배치 날짜 기준 전날 데이터가 존재하는지 체크하는 센서 \n1. 데이터셋에 날짜 컬럼이 존재하고 \n2. API 사용시 그 날짜 컬럼으로 ORDER BY DESC 되어 가져온다는 가정하에 사용 가능\n'''\n\nclass SeoulApiDateSensor(BaseSensorOperator):\n    template_fields = ('endpoint',)\n    def __init__(self, dataset_nm, base_dt_col, day_off=0, **kwargs):\n        '''\n        dataset_nm: 서울시 공공데이터 포털에서 센싱하고자 하는 데이터셋 명\n        base_dt_col: 센싱 기준 컬럼 (yyyy.mm.dd... or yyyy/mm/dd... 형태만 가능)\n        day_off: 배치일 기준 생성여부를 확인하고자 하는 날짜 차이를 입력 (기본값: 0). 즉, 1일 전, 2일 전 데이터가 업데이트 되었는지 확인하는 것.\n        '''\n        super().__init__(**kwargs)\n        self.http_conn_id = 'openapi.seoul.go.kr'\n        self.endpoint = '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + dataset_nm + '/1/100'   # 100건만 추출\n        self.base_dt_col = base_dt_col\n        self.day_off = day_off\n\n        \n    def poke(self, context): # context에 templates 변수들을 호출할 수 있음\n        import requests\n        import json\n        from dateutil.relativedelta import relativedelta\n        connection = BaseHook.get_connection(self.http_conn_id)\n        url = f'http://{connection.host}:{connection.port}/{self.endpoint}'\n        self.log.info(f'request url:{url}')\n        response = requests.get(url)\n\n        contents = json.loads(response.text)\n        key_nm = list(contents.keys())[0]\n        row_data = contents.get(key_nm).get('row')\n        last_dt = row_data[0].get(self.base_dt_col)\n        last_date = last_dt[:10]\n        last_date = last_date.replace('.', '-').replace('/', '-')\n        search_ymd = (context.get('data_interval_end').in_timezone('Asia/Seoul') + relativedelta(days=self.day_off)).strftime('%Y-%m-%d')\n        try:\n            import pendulum\n            pendulum.from_format(last_date, 'YYYY-MM-DD')\n        except:\n            from airflow.exceptions import AirflowException\n            AirflowException(f'{self.base_dt_col} 컬럼은 YYYY.MM.DD 또는 YYYY/MM/DD 형태가 아닙니다.')\n\n        \n        if last_date &gt;= search_ymd: # last_date: API의 날짜, search_ymd: 조회할 날짜\n            self.log.info(f'생성 확인(기준 날짜: {search_ymd} / API Last 날짜: {last_date})')\n            return True\n        else:\n            self.log.info(f'Update 미완료 (기준 날짜: {search_ymd} / API Last 날짜:{last_date})')\n            return False"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n몰라도 되지만 알면 좋은 고급 기능들\n\n\n\n\nDAG간의 의존관계를 설정할 때 TriggerDagRun 과 ExternalTask Sensor를 이용한다. 그런데 이 2가지 방법을 이용하면 관리가 복잡해진다. 이를 해결하기 위해 고안된 기술이 dataset을 이용하여 DAG간의 의존관계를 설정하는 것이다. 어떤 상황에서 DAG 관리가 복잡해지고 dataset을 이용하면 어떻게 관리의 효율성이 증가하는지 다음의 cases를 통해 알아보자.\n\ncase 1\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nDAG A\n\n\ncluster1\n\nDAG B\n\n\n\ntask_1\n\ntask_1\n\n\n\ntask_3\n\ntask_3\n\n\n\ntask_1-&gt;task_3\n\n\n\n\n\ntask_k\n\ntask_k\n\n\n\n\n\n\n\n\n\n왼쪽 그림(요구사항) 에서, task3가 TriggerDagRun operator 으로 만들어진 task라 가정한다. task3가 DAG C의 task x를 trigger 한다고 가정한다. 이 경우를 변경하여 Task_1 뿐만 아니라 DAG_B의 task_k 에도 선행 의존관계가 걸려야 한다고 가정한다. 그렇게 하기위해서 External Task 센서를 하나 달아 개선해야 한다는 상황이 있다고 가정한다. 결국, task1과 task k가 성공적으로 끝나야 task3가 수행되고 이어서 DAG C의 task3가 돌아가야 하는 상황\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster1\n\nDAG C\n\n\ncluster0\n\nDAG A\n\n\ncluster2\n\nDAG B\n\n\n\ntask_1\n\ntask_1\n\n\n\nSensor_task_1\n\nSensor_task_1\n\n\n\nSensor_task_1-&gt;task_1\n\n\nsensing\n\n\n\ntask_x\n\ntask_x\n\n\n\nSensor_task_1-&gt;task_x\n\n\n\n\n\nSensor_task_2\n\nSensor_task_2\n\n\n\nSensor_task_2-&gt;task_x\n\n\n\n\n\ntask_k\n\ntask_k\n\n\n\nSensor_task_2-&gt;task_k\n\n\nsensing\n\n\n\n\n\n\n\n\n\n오른쪽 그림(해결책)은 왼쪽 그림을 개선하고자 만든 DAG의 의존관계, task3는 제거되어야 한다. 대신, DAG C를 external task sensor를 추가하도록 수정한다. sensor task1은 task1을 수행이 되고 sensor task2가 taskk의 수행이 성공적으로 완료가 된 후에 이 두 sensor task들이 taskx를 수행하도록 개선해야 한다.\n\n\n\n이렇게 DAG을 관리할 때 추가적인 DAG의 의존관계 설정을 해야할 때 추가적인 task들이 생기게 되는 요구사항을 해결해야할 때 오른쪽 그림과 같이 복잡한 workflow가 생기게 되는 case가 발생할 수 있다. 또 다른 case는 case2와 같다.\ncase 2\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nDAG A\n\n\ncluster\n\nDAG Y\n\n\n\ntask_1\n\ntask_1\n\n\n\ntask_3\n\ntask_3\n\n\n\ntask_1-&gt;task_3\n\n\n\n\n\ntask_4\n\ntask_4\n\n\n\ntask_y\n\ntask_y\n\n\n\n\n\n\n\n\n\n왼쪽 그림(요구사항)에서, Task_1이 완료되면 또 다른 dag 을 trigger 되고 싶은데 DAG A 를 수정해야 할 경우이다. 다시 말해서, task1이 완료되면 또 다른 dag Y의 task y를 trigger하는 TriggerDagRun task4를 만들어 주고 싶도록 dag A를 수정해야하는 상황. 즉, task y는 task 1 과 task 4가 성공적으로 완료되어야만 수행되도록 설정해주고 싶은 상황. 뿐만 아니라 DAG A에 지속적인 변경사항 있을시 계속 workflow를 수정해야하는 문제점이 있다.\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster\n\nDAG Y\n\n\n\ntask_1\n\ntask_1\n\n\n\ntask_3\n\ntask_3\n\n\n\ntask_1-&gt;task_3\n\n\n\n\n\ntask_4\n\ntask_4\n\n\n\ntask_1-&gt;task_4\n\n\n\n\n\ntask_y\n\ntask_y\n\n\n\ntask_4-&gt;task_y\n\n\n\n\n\n\n\n\n\n\n\n오른쪽 그림이 개선된 workflow\n\n\n\n이렇게, TriggerDagRun 오퍼레이터와 ExternalTask 센서를 많이 사용하다보면 아래 그림과 같이 연결관리에 많은 노력이 필요\n특히, ExternalTask은 execution_delta, 즉 data_interval_start 차이를 정확하게 연결해야 작동하도록 되어 있기 때문에 매우 강한 연결이 전제가 된다. 즉, parameter 중 일부가 안맞거나 틀리게 되면 제대로 sensing이 안됨\n\n\n\n\n\n\n\n\nG\n\n\ncluster\n\nDAG Flow\n\n\n\nDAG_A\n\nDAG_A\n\n\n\nDAG_B\n\nDAG_B\n\n\n\nDAG_A-&gt;DAG_B\n\n\n\n\n\nDAG_C\n\nDAG_C\n\n\n\nDAG_A-&gt;DAG_C\n\n\n\n\n\nDAG_D\n\nDAG_D\n\n\n\nDAG_D-&gt;DAG_C\n\n\n\n\n\nDAG_E\n\nDAG_E\n\n\n\nDAG_D-&gt;DAG_E\n\n\n\n\n\nDAG_G\n\nDAG_G\n\n\n\nDAG_E-&gt;DAG_G\n\n\n\n\n\n\n\n\n\n\n\n위와 같은 복잡하고 rigid한 workflow를 개선한 것이 dataset을 이용한 방식이다.\n\n큐시스템과 같이 Job 수행이 완료된 Task 는 Push 하고 센싱이 필요한 task 은 큐 시스템을 구독하는 방식을 쓴다면 더 유연한 workflow가 생성될 수 있다. (약한 연결)\n\n\n\n\n\n\n\n\nG\n\n\ncluster\n\nDAG Flow\n\n\n\nDAG_A\n\nDAG_A\n\n\n\nQueue\n\n\nQueue\n\n\n\nDAG_A-&gt;Queue\n\n\n\n\n\nDAG_B\n\nDAG_B\n\n\n\nDAG_C\n\nDAG_C\n\n\n\nDAG_D\n\nDAG_D\n\n\n\nDAG_D-&gt;Queue\n\n\n\n\n\nDAG_E\n\nDAG_E\n\n\n\nDAG_G\n\nDAG_G\n\n\n\nQueue-&gt;DAG_B\n\n\n\n\n\nQueue-&gt;DAG_C\n\n\n\n\n\nQueue-&gt;DAG_E\n\n\n\n\n\n\n\n\n\n\n\nDAG A가 작업이 끝났다라는 메시지를 큐에 넣고 DAG A가 완료되면 자동으로 DAG B가 triggering됨\n나중에 DAG F가 새로 생겨 DAG A가 끝났을 때 수행되도록 triggering 되어야 한다면 큐에 있는 메시지를 인식하고 돌아가도록 설정하면 된다.\n그러므로, DAG A를 구독하는 후행 DAG들이 아무리 많더라도 큐에 저장된 메시지만 바라보게 만들면 된다. DAG A는 수정할 필요가 없게 된다.\nProduce / Consume 구조를 이용하여 Task 완료를 알리기 위해 특정 키 값으로 Produce 하고 해당 키를 Consume 하는 DAG 을 트리거 스케줄 할 수 있는 기능\n\nproduce: queue에 메세지를 넣는 행위. 다른 솔루션에서는 produce를 publish라고도 부름\n\nconsume: queue로부터 메세지를 읽거나 꺼내는 행위. 보통은 이 consume다른 솔루션에서는 subscribe라고도 많이 부름. airflow에서는 subscribe가 consume으로 사용된다.\n선행 Dag이 produce하고 후행 Dag이 consume한다.\n실제 큐가 있는 것은 아니고 DB 에 Produce / Consume 내역을 기록\n\n\n\n\n\n\ndags_dataset_producer_1.py\n\nfrom airflow import Dataset #airflow의 dataset이라는 library 호출\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_1 = Dataset(\"dags_dataset_producer_1\")\n# Dataset의 인수값으로 string을 주는데 DAG이 큐에 메세지를 넣을 때 메세지가  dictionary형태로 저장이 되는데 그 키값을 input으로 넣게 된다. \n\n\nwith DAG(\n        dag_id='dags_dataset_producer_1',\n        schedule='0 7 * * *',\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        outlets=[dataset_dags_dataset_producer_1], # outlets 인수는 baseOperator로부터 상속된다. 모든 operator는 outlets인수를 가지고 있다. outlets에 다가는 리스트 구조로 선언된다. queue에다가 publish되는 메세지는 dag안에 있는 task('bash_task')가 dataset_dags_dataset_producer_2 이름으로  publish된다.  dataset_dags_dataset_producer_2가 들어가 있다. \n        bash_command='echo \"producer_1 수행 완료\"'\n    )\n\ndags_dataset_producer_2.py\n\nfrom airflow import Dataset\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_2 = Dataset(\"dags_dataset_producer_2\")\n\nwith DAG(\n        dag_id='dags_dataset_producer_2',\n        schedule='0 7 * * *',\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        outlets=[dataset_dags_dataset_producer_2], \n        bash_command='echo \"producer_2 수행 완료\"'\n    )\n\ndags_dataset_consumer_1.py\n\nfrom airflow import Dataset \nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_1 = Dataset(\"dags_dataset_producer_1\") # dags_dataset_producer_1.py의 producer1의 키값 'dags_dataset_producer_1'을 넣어 호출하고 있음\n\nwith DAG(\n        dag_id='dags_dataset_consumer_1',\n        schedule=[dataset_dags_dataset_producer_1], # 스케쥴이 다른 형식으로 들어가 있는데 dags_dataset_producer_1.py의 producer1 (정확히 말하면 task)을 구독하겠다는 뜻. 스케쥴은 없고 publish하고 있는 dag 수행이 끝나면 실행하겠다는 뜻.\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        bash_command='echo {{ ti.run_id }} && echo \"producer_1 이 완료되면 수행\"'\n    )\n\ndags_dataset_consumer_2.py\n\nfrom airflow import Dataset\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_1 = Dataset(\"dags_dataset_producer_1\")\ndataset_dags_dataset_producer_2 = Dataset(\"dags_dataset_producer_2\")\n# producer1,2 2개를 동시에 구독하고있음\n\nwith DAG(\n        dag_id='dags_dataset_consumer_2',\n        schedule=[dataset_dags_dataset_producer_1, dataset_dags_dataset_producer_2],\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        bash_command='echo {{ ti.run_id }} && echo \"producer_1 와 producer_2 완료되면 수행\"'\n    )\nairflow web service에서 dag 상태를 확인할 때 초록색 긴막대기를 누르면 Dataset Events에서 구독 depndencies를 확인할 수 있다. Run ID도 특이하게 나오게 되는 것을 확인 할 수 있다.\n\n\n\nAirflow Web Service의 dataset 메뉴를 클릭하면 publish와 subscribe의 의존관계를 graph로 볼 수 있다.\n\n\n\nAirflow Web Service의 Browse 메뉴의 Dag Dependencies를 클릭하면 Dag간의 의존관계를 볼 수 있다.\n\n\n\n\nDataset 은 Publish(produce)/subscribe(consume) 구조로 DAG 간 의존관계를 주는 방법\nUnique 한 String 형태의 Key 값을 부여하여 Dataset Publish\nDataset 을 Consume 하는 DAG 은 스케줄을 별도로 주지 않고 리스트 형태로 구독할 Dataset 요소들을 명시\nDataset에 의해 시작된 DAG 의 Run_id 는 dataset_triggered__{trigger 된 시간 } 으로 표현됨\nAirflow 화면 메뉴 중 Datasets 메뉴를 통해 별도로 Dataset 현황 모니터링 가능\nN개의 Dataset 을 구독할 때 스케줄링 시점은 ?\n\n마지막 수행 이후 N 개의 Dataset 이 모두 재 업데이트되는 시점 \n\n\n\n\n\n\n\n\n\n목적: DAG 하위 모든 오퍼레이터에 공통 적용될 파라미터를 입력\n어떤 파라미터들이 적용 가능할까?\n\n오퍼레이터에 공통 적용할 수 있는 파라미터들\nBaseOperator 클래스 생성자가 가지고 있는 파라미터\n\nairflow doc\nCode\n\n\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n) as dag:\n    \n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='sleep 10m',\n        sla: timedelta(seconds=70), #SLA 설정\n        email: 'sdf@sdfsfd.com'\n    )\n    \n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='sleep 2m',\n        sla: timedelta(seconds=70), #SLA 설정\n        email: 'sdf@sdfsfd.com'\n    )\n\n   task_1 &gt;&gt; task_2\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': 'sdf@sdfsfd.com'\n    }\n) as dag:\n    \n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='sleep 10m'\n    )\n    \n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='sleep 2m'\n    )\n\n   task_1 &gt;&gt; task_2\n\n\n\n\n\n\n\n\n\n\nDAG 파라미터는 DAG 단위로 적용될 파라미터\n개별 오퍼레이터에 적용되지 않음\nDAG 파라미터는 default_args 에 전달하면 안됨\n\n\n\n\n\n\nBase 오퍼레이터 파라미터는 개별 Task 단위로 적용될 파라미터.\nTask 마다 선언해줄 수 있지만 DAG 하위 모든 오퍼레이터에 적용 필요시 default_args 를 통해 전달 가능\n\n\n\n\n\ndefault_args 에 전달된 파라미터보다 개별 오퍼레이터에 선언된 파라미터가 우선순위를 가짐\n\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': 'sdf@sdfsfd.com'\n    }\n) as dag:\n    \n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='sleep 10m'\n    )\n    \n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='sleep 2m',\n        sla=timedelta(minutes=1)\n    )\n\n   task_1 &gt;&gt; task_2\n\n\n\n\n\n\n\nairflow doc\nemail 파라미터와 email_on_failure 파라미터를 이용\nemail 파라미터만 입력하면 email_on_failure 파라미터는 True 로 자동설정됨\n\n\n\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.decorators import task\nfrom airflow.exceptions import AirflowException\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_email_on_failure',\n    start_date=pendulum.datetime(2023,5,1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule='0 1 * * *',\n    dagrun_timeout=timedelta(minutes=2),\n    default_args={\n        'email_on_failure': True,\n        'email': email_lst\n    }\n) as dag:\n    @task(task_id='python_fail')\n    def python_task_func():\n        raise AirflowException('에러 발생')\n    python_task_func()\n\n    bash_fail = BashOperator(\n        task_id='bash_fail',\n        bash_command='exit 1',\n    )\n\n    bash_success = BashOperator(\n        task_id='bash_success',\n        bash_command='exit 0',\n    )\n\nEmail 받을 대상이 1 명이면 string 형식으로 , 2 명 이상이면 list 로 전달\n그런데 협업 환경에서 DAG 담당자는 수시로 바뀔 수 있고 인원도 수시로 바뀔 수 있는데 그때마다 DAG 을 뒤져서 email 리스트를 수정해야 할까 ?\n이럴때 Variable 을 이용하자\n\n\n\n\n\n\n\n\n개념: 오퍼레이터 수행시 정해놓은 시간을 초과하였는지를 판단할 수 있도록 설정해놓은 시간 값 파이썬의 timedelta 로 정의\n동작: 설정해놓은 SLA 를 초과하여 오퍼레이터 running 시 SLA Miss 발생 , Airflow UI 화면에서 Miss 건만 조회 가능 + email 발송 가능\n\nSLA Miss 발생시 task 가 실패되는 것은 아니며 단순 Miss 대상에 기록만 DB에 남기게 됨\nSLA 는 DAG 파라미터가 아니며 BaseOperator 의 파라미터\n\nairflow docs- def init\n def __init__(\n      self,\n      task_id: str,\n      owner: str = DEFAULT_OWNER,\n      email: str | Iterable[str] | None = None,\n      email_on_retry: bool = conf.getboolean(\"email\", \"default_email_on_retry\", fallback=True),\n      email_on_failure: bool = conf.getboolean(\"email\", \"default_email_on_failure\", fallback=True),\n      retries: int | None = DEFAULT_RETRIES,\n      retry_delay: timedelta | float = DEFAULT_RETRY_DELAY,\n      retry_exponential_backoff: bool = False,\n      max_retry_delay: timedelta | float | None = None,\n      start_date: datetime | None = None,\n      end_date: datetime | None = None,\n      depends_on_past: bool = False,\n      ignore_first_depends_on_past: bool = DEFAULT_IGNORE_FIRST_DEPENDS_ON_PAST,\n      wait_for_past_depends_before_skipping: bool = DEFAULT_WAIT_FOR_PAST_DEPENDS_BEFORE_SKIPPING,\n      wait_for_downstream: bool = False,\n      dag: DAG | None = None,\n      params: collections.abc.MutableMapping | None = None,\n      default_args: dict | None = None,\n      priority_weight: int = DEFAULT_PRIORITY_WEIGHT,\n      weight_rule: str = DEFAULT_WEIGHT_RULE,\n      queue: str = DEFAULT_QUEUE,\n      pool: str | None = None,\n      pool_slots: int = DEFAULT_POOL_SLOTS,\n      sla: timedelta | None = None, # sla 변수\n      execution_timeout: timedelta | None = DEFAULT_TASK_EXECUTION_TIMEOUT,\n      on_execute_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n      on_failure_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n      on_success_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n      on_retry_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n      pre_execute: TaskPreExecuteHook | None = None,\n      post_execute: TaskPostExecuteHook | None = None,\n      trigger_rule: str = DEFAULT_TRIGGER_RULE,\n      resources: dict[str, Any] | None = None,\n      run_as_user: str | None = None,\n      task_concurrency: int | None = None,\n      max_active_tis_per_dag: int | None = None,\n      max_active_tis_per_dagrun: int | None = None,\n      executor_config: dict | None = None,\n      do_xcom_push: bool = True,\n      inlets: Any | None = None,\n      outlets: Any | None = None,\n      task_group: TaskGroup | None = None,\n      doc: str | None = None,\n      doc_md: str | None = None,\n      doc_json: str | None = None,\n      doc_yaml: str | None = None,\n      doc_rst: str | None = None,\n      **kwargs,\n  ):\n\nsla: timedelta | None = None, # sla 변수\n\ntimedelta보다 오랜 시간동안 task가 더 오래 수행되면 SLA Miss 기록만 남기고 실패처리는 안하게 됨\ndefault_arg에 넣을 수 있기 때문에 모든 task, 즉 모든 operator에 공통 적용할 수 있다.\n\n\nsla_miss_callback: DAG에 있는 SLA 관련 파라미터로 SLA Miss시 수행할 함수 지정 가능-Source code for airflow.models.dag\n\nsla_miss_callback 파라미터\n\nsla miss시 수행할 함수명을 입력 받음\ndag에 있는 파라미터이기 때문에 default_arg에 넣을 수 없음\n\nDAG의 파라미터임 (BaseOperator 의 파라미터가 아니라는 것에 유의)\n\n\n\n\n\n\n1번째 제약사항: 각 Task 의 SLA timeout 카운트 시작은 DAG 의 시작시간 기준임\n\n\n\n\n\n\n\n처음 사용하는 사용자들은 다음과 같이 생각할 수 있다.\n\ntask1은 task1의 sla구간보다 수행시간이 짧게 완료됐기 때문에 성공 처리\ntask2는 sla 구간보다 더 길기 때문에 miss 처리됨\ntask3는 sla 구간보다 짧기 때문에 성공 처리\n\nairflow는 위와 같이 동작하지 않음 옆의 그림과 같이 동작함\n\n\n\n\n\n\n\n실제 airflow에서는 수행 시간을 dag의 최초 실행시점 부터 카운트 하기 시작함\ntask1의 수행완료 상태 및 시점과 상관없이 task2와 task3의 수행 시간은 task1 수행 시점 부터 카운트 되기 시작한다.\n그러므로 task2, task3는 수행된적이 없음에도 miss sla 상태로 처리됨\n\n\n\n\n\n2번째 제약사항: DAG의 시작시간 기준 \\(\\rightarrow\\) 명목적인 시작 시간 (data_interval_start)\n\n위의 SLA timeout 카운트 시작은 dag의 시작시간 기준이라는 제약 사항과 연결되는 제약사항\n만약 Pool 이 부족하거나 스케줄러의 부하로 인해 DAG 이 스케줄보다 늦게 시작했다면 늦게 시작한만큼 이미 SLA 시간 카운트는 진행되어 시간이 소요되고 있음\n\n실질적 DAG 시작 시간을 기준으로 카운트하지 않음\n\n즉, 특정 task 수행에 computing 부하가 걸리면 후차적인 task들은 모두 miss 처리됨\n대규모 프로젝트에서 흔히 발생하는 문제로 dag이 밀리는 현상이 자주 관찰되므로 sla는 자주쓰이는 방식은 아님.\n\n3번째 제약사항: 첫 스케줄에서는 SLA Miss 가 기록되지 않음 (두 번째 스케줄부터 기록)\n4번째 제약사항: sla miss처리가 분명히 발생됐는데 가끔 기록이 안될때가 있고 email역시 발송이 되지 않을때가 있음. 엄격이 관리가 되지 않는 기능\n\n\n\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': email_lst\n    }\n) as dag:\n    \n    # 30초 sleep\n    task_slp_30s_sla_70s = BashOperator( \n        task_id='task_slp_30s_sla_70s',\n        bash_command='sleep 30'\n    )\n    # 60초 sleep\n    task_slp_60_sla_70s = BashOperator(\n        task_id='task_slp_60_sla_70s',\n        bash_command='sleep 60'\n    )\n\n    # 10초 sleep\n    task_slp_10s_sla_70s = BashOperator( \n        task_id='task_slp_10s_sla_70s',\n        bash_command='sleep 10'\n    )\n\n    # 10초 sleep\n    # sla의 timedelta를 명시적으로 30초 선언\n    # default argument보다 명시적 선언이 우선 순위가 더 높음\n    # 그래서, 처음 3개의 task는 timedelta가 70초로 설정됐고 4번째 task는 30초로 설정됨 \n    task_slp_10s_sla_30s = BashOperator( \n        task_id='task_slp_10s_sla_30s',\n        bash_command='sleep 10',\n        sla=timedelta(seconds=30)\n    )\n\n    task_slp_30s_sla_70s &gt;&gt; task_slp_60_sla_70s &gt;&gt; task_slp_10s_sla_70s &gt;&gt; task_slp_10s_sla_30s\n\n위 코드를 보면,\n\n1번째 task는 30초 sleep이 70초 sla timedelta보다 짧기 때문에 성공 처리\n2번째 task는 30,40초 돌다가 miss 처리됨\n3,4 번째 task는 수행되기도 전에 miss처리됨\n4번째 timedelta가장 짧기 때문에 가장 먼저 miss 처리 됨\n\nAirflow web service에서 SLA Miss 현황 조회하기\n\nairflow web service &gt;&gt; browse\n\n\n\n\n\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': email_lst\n    }\n) as dag:\n    \n    task_slp_30s_sla_70s = BashOperator(\n        task_id='task_slp_30s_sla_70s',\n        bash_command='sleep 30'\n    )\n    \n    task_slp_60_sla_70s = BashOperator(\n        task_id='task_slp_60_sla_70s',\n        bash_command='sleep 60'\n    )\n\n    task_slp_10s_sla_70s = BashOperator(\n        task_id='task_slp_10s_sla_70s',\n        bash_command='sleep 10'\n    )\n\n    task_slp_10s_sla_30s = BashOperator(\n        task_id='task_slp_10s_sla_30s',\n        bash_command='sleep 10',\n        sla=timedelta(seconds=30)\n    )\n\n    task_slp_30s_sla_70s &gt;&gt; task_slp_60_sla_70s &gt;&gt; task_slp_10s_sla_70s &gt;&gt; task_slp_10s_sla_30s\n\n\n\n\n\n\n\nTask수준의 timeout 과 DAG 수준의 timeout 이 존재\nexecution_timeout: Task수준의 timeout 파라미터: baseOperator 명세에서 init 생성자에서 sla 파라미터 아래에 있음\n\nexecution_timeout: timedelta | None = DEFAULT_TASK_EXECUTION_TIMEOUT,\ntimedelta보다 오래 task가 수행됐을 때 task는 fail 처리됨\ntask가 fialure 됐을 때 init 생성자 안의 다른 파라미터인 email_on_failure: bool = conf.getboolean(\"email\", \"default_email_on_failure\", fallback=True) 과 email: str | Iterable[str] | None = None, 에 의해 지정 대상에게 email을 보낼 수 있다.\n요약하면, timedelta, email_on_failure, email 이렇게 3개의 파라미터를 이용하여 timedelta 를 초과하는 task 수행시 fail 처리를 하여 이메일을 보낼 수 있다.\n\ndagrun_timeout: DAG수준에서도 timeout 파라미터를 걸 수 도 있음 : dagrun_timeout\n\n[docs]class DAG(LoggingMixin): 의 init 생성자\n\n\n    def __init__(\n      self,\n      dag_id: str,\n      description: str | None = None,\n      schedule: ScheduleArg = NOTSET,\n      schedule_interval: ScheduleIntervalArg = NOTSET,\n      timetable: Timetable | None = None,\n      start_date: datetime | None = None,\n      end_date: datetime | None = None,\n      full_filepath: str | None = None,\n      template_searchpath: str | Iterable[str] | None = None,\n      template_undefined: type[jinja2.StrictUndefined] = jinja2.StrictUndefined,\n      user_defined_macros: dict | None = None,\n      user_defined_filters: dict | None = None,\n      default_args: dict | None = None,\n      concurrency: int | None = None,\n      max_active_tasks: int = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\"),\n      max_active_runs: int = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\"),\n      dagrun_timeout: timedelta | None = None,\n      sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None,\n      default_view: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),\n      orientation: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),\n      catchup: bool = airflow_conf.getboolean(\"scheduler\", \"catchup_by_default\"),\n      on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,\n      on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,\n      doc_md: str | None = None,\n      params: collections.abc.MutableMapping | None = None,\n      access_control: dict | None = None,\n      is_paused_upon_creation: bool | None = None,\n      jinja_environment_kwargs: dict | None = None,\n      render_template_as_native_obj: bool = False,\n      tags: list[str] | None = None,\n      owner_links: dict[str, str] | None = None,\n      auto_register: bool = True,\n  ):\n\n다음 파라미터를 이용하여 dag을 관리할 수 있다.\n\nschedule: ScheduleArg = NOTSET,\nstart_date: datetime | None = None,\ndefault_args: dict | None = None,\ndagrun_timeout: timedelta | None = None,\n\n\ntask1 &gt;&gt; task2 &gt;&gt; task3 예시\n\n\n\n\n\n\ndag은 timeout이 안됐지만 task가 timeout이 되는 case\n\n\n\ntask2의 실행시간이 task2의 execution_timeout보다 길게 수행되어 fail되었고 task3은 task2로 인해 upstream failed 이 발생\n하지만, task1,2,3 의 수행시간의 총합이 dagrun_timeout 보다 짧기 때문에 dag 자체는 실패처리 안됨\n\n\n\n\n\n\ntask는 성공 처리 됐지만 dag이 오랜시간동안 돌아 timeout되어 실패처리되는 case\n\n\n\n각 task들이 execution_timeout보다 짧아 모두 정상 처리 되었지만 task1,2,3의 총 실행시간이 dagrun_timeout보다 길어 dag자체는 failure이 된 상황\ndag이 failure되는 시점에서의 task는 skipped 상태로 처리된다.\n\n\n\n\n\n\n\n\ncase1: tasks는 실패, dargrun 정상\n\n# Package Import\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.models import Variable\n\n# email 수신자 리스트\nemail_str = Variable.get(\"email_target\") \nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_timeout_example_1',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule=None,\n    dagrun_timeout=timedelta(minutes=1),\n    default_args={\n        #각 task들이 20초안에 끝나야 성공 처리됨\n        'execution_timeout': timedelta(seconds=20), \n        'email_on_failure': True,\n        'email': email_lst\n    }\n) as dag:\n    # execution_timeout보다 길기 때문에 task는 실패 처리됨\n    bash_sleep_30 = BashOperator(\n        task_id='bash_sleep_30', \n        bash_command='sleep 30',\n    )\n    # execution_timeout보다 짧기 때문에 task는 성공 처리됨\n    bash_sleep_10 = BashOperator(\n        trigger_rule='all_done', # upstream fail에도 task 실행시키기 위해 triggering\n        task_id='bash_sleep_10',\n        bash_command='sleep 10',\n    )\n    \n    # upstream failure 발생해도 trigger_rule을 all_done을 줬기 때문에 bash_sleep_10 은 실행됨\n    # dagrun_timeout을 1분으로 설정했기 때문에 task run의 총합이 40초이기 때문에 dagrun은 정상 처리됨\n    bash_sleep_30 &gt;&gt; bash_sleep_10 \n    \n\ncase2: tasks는 정상, dargrun 실패\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_timeout_example_2',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule=None,\n    dagrun_timeout=timedelta(minutes=1),\n    default_args={\n        'execution_timeout': timedelta(seconds=40),\n        'email_on_failure': True,\n        'email': email_lst\n    }\n) as dag:\n    bash_sleep_35 = BashOperator(\n        task_id='bash_sleep_35',\n        bash_command='sleep 35',\n    )\n\n    bash_sleep_36 = BashOperator(\n        trigger_rule='all_done',\n        task_id='bash_sleep_36',\n        bash_command='sleep 36',\n    )\n\n    bash_go = BashOperator(\n        task_id='bash_go',\n        bash_command='exit 0',\n    )\n# 모든 task들이 40초안에 실행완료가 되기때문에 성공 처리됨\n# dagrun은 1분으로 설정됐기 때문에 2번째 task 가 실행될 때 dag이 fail되고\n# 2번째task는 skipped 처리가 됨, 이 task에 대해서는 email도 안감 \n# 3번째 task는  no status 처리됨 , 이 task에 대해서는 email도 안감\n    bash_sleep_35 &gt;&gt; bash_sleep_36 &gt;&gt; bash_go\n    \n\ndagrun_timeout의 한계점\n\n모든 task들이 40초안에 실행완료가 되기때문에 성공 처리됨\ndagrun은 1분으로 설정됐기 때문에 2번째 task 가 실행될 때 dag이 fail되고\n2번째task는 skipped 처리가 됨, 이 task에 대해서는 email도 안감\n3번째 task는 no status 처리됨 , 이 task에 대해서는 email도 안감\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparision\nsla\nexecution_timeout\ndagrun_timeout\n\n\n\n\n파라미터 정의 위치\nBaseOperator\nBaseOperator\nDAG\n\n\n적용 수준\nTask\ntask\nDAG\n\n\n기능\n지정한 시간 초과시 Miss 기록\n지정한 시간 초과시 task fail 처리\n지정한 시간 초과시 DAG fail 처리\n\n\nemail 발송 가능 여부\nO\nO\nX\n\n\ntimeout 발생시 후행 task 상태\n상관없이 지속\nUpstream_failed\nSkipped (current) /No status (not run)\n\n\n스케쥴 필요\nO\nX\nX\n\n\n\n\nsla, execution_timeout에는 email 발송 paraemeter가 있지만 execution_timeout에는 없다.\ndagrun timeout이 fail 됐을 때 반드시 email 발송 하고싶으면 dag의 파라미터 중 on_failure_callback 에 dag이 실패됐을 때 이메일을 전송하는 함수를 만들어 그 함수명을 할당해준다.\nupstream_failed 상태는 execution_timeout만이 갖는 특징이 아니라 airflow의 디폴트 설정이다. 상위 task들이 fail되면 후행 task들은 upstream_failed로 남는다.\n공통점: 파이썬의 timedelta 함수로 timeout 기준 시간 정의\n\n\n\n\n\n\nAirflow가 설치되어 있는 서버 또는 환경에서 shell 명령을 이용하여 Airflow를 컨트롤 할 수 있도록 많은 기능들을 제공하고 있음\nairflow cli doc\n\n대표적인 content\n\ndags: dag을 다룰 수 잇는 커맨드\n\nairflow dags [-h] COMMAND ...\nbackfill: airflow web ui 의 grid 기능을 보면 dag이 돌았던 이력을 볼 수 있는데 grid 상 가장 과거 날짜 뿐만 아니라 그 이전의 과거 날짜 또한 command의 옵션으로 모두 돌릴 수 있음\n\n  airflow dags backfill [-h] [-c CONF] [--continue-on-failures]\n                [--delay-on-limit DELAY_ON_LIMIT] [--disable-retry] [-x]\n                [-n] [-e END_DATE] [-i] [-I] [-l] [-m] [--pool POOL]\n                [--rerun-failed-tasks] [--reset-dagruns] [-B]\n                [-s START_DATE] [-S SUBDIR] [-t TASK_REGEX]\n                [--treat-dag-as-regex] [-v] [-y]\n                dag_id\n\ndelete: Delete all DB records related to the specified DAG\n\nairflow dags delete [-h] [-v] [-y] dag_id\n\ndetails: Get DAG details given a DAG id\n\nairflow dags details [-h] [-o table, json, yaml, plain] [-v] dag_id\n* list: List all the DAGs\n```markdown\nairflow dags list [-h] [-o table, json, yaml, plain] [-S SUBDIR] [-v]\nvariables: variables을 관리하는 command\n\nairflow variables [-h] COMMAND ...\ndelete: airflow variables delete [-h] [-v] key\n\n등록되있는 variables을 key값을 입력하여 삭제\n\nexport: airflow variables export [-h] [-v] file\n\n등록되있는 variables을 json file로 추출\n\nget: airflow variables get [-h] [-d VAL] [-j] [-v] key\n\n특정 variables의 key값을 주어 values을 꺼내옴\n\nimport: airflow variables import [-h] [-v] file\n\njson file에 variables을 작성해놓고 list를 한번에 입력한다.\n\nlist: airflow variables list [-h] [-o table, json, yaml, plain] [-v]\nset: airflow variables set [-h] [-j] [-v] key VALUE\n\n\n\nCli를 잘 쓰면 좋은 이유\n\n일괄작업: Airflow UI에서 할 수 없는 일괄 작업 방식을 제공\n\nex: connection 일괄 등록. 만약 airflow ui로 등록하면 일일히 등록해야한다.\n물론 CLI를 이용하는 방법 외에 metaDB table에 직접 insert하는 방법도 있음\n\n특수기능: Airflow UI에서는 할 수 없는 기능을 제공\n\nex: backfill 은 airflow ui 를 통해서는 실행 불가\n\n자동화: Airflow UI에서 직접 눈으로 보고 클릭하는 방식이 아닌 프로그래밍에 의한 제어가 가능해짐\n\nCLI 커맨드는 shell 명령어로 이루저있기 때문에 shell script를 작성하여 자동화 할 수 있다.\n\n\n\n\n\n\ndag trigger: airflow ui 상에서 manual 로 dag trigger 하거나 run_id 를 직접 넣어 trigger 할 수 있는 기능으로 CLI로 실행시킬 수 있다.\nCLI 명령은 WSL2에서 하는게 아니라 docker container안에서 해야함\nairflow dags trigger [-h] [-c CONF] [-e EXEC_DATE] \n                     [--no-replace-microseconds] [-o table, json, yaml, plain] [-r RUN_ID] [-S SUBDIR] [-v]\n                   dag_id\n\nEXEC_DATE: execution_date parameter는 모두 data_interval_start 기준이며 String 형식으로 입력하면 기본 UTC로 계산됨\nRUN_ID를 입력하면 기본적으로 manual__로 시작하며 run_id를 직접 입력도 가능\n예시: #&gt; airflow dags trigger dags_seoul_api_corona\n\nFull Example\n\n# docker container list 확인\nsudo docker ps \n\n# webserver container 선택 (어떤 것을 골라도 상관없음)\n\n# airflow container 들어가기: sudo docker exec -it [docker_container_id] bash\nsudo docker exec -it 8b755cb5aa70 bash \n\n# trigger 명령어 실행\nairflow dags trigger dags_base_branch_operator\n\n\n\n\n\n\ndag_run_id: manual__2023-07-22T05:05:59+00:00.\nrun type: manual__\n\nClI 로 돌렸기 때문에 manual_이 붙어있음\nrun types: schedule, manual, backfill 등이 있음\n\n\n\n\n\n\n입력 스케줄 구간에 대해 일괄 (재)실행 (스케줄 이력이 없는 과거 날짜도 가능)\n\n# start (-s), end(-ㄷ) 파라미터를 dashed string 형태로 입력하면 UTC로 간주(아래의 start 날짜에 시간:분:초가 나와있지 않지만 날짜뒤에 00:00:00 가 붙음) \nairflow dags backfill -s 2023-04-19 -e 2023-04-21 dags_seoul_api_corona\n\n# 타임스탬프 형태로 직접 작성도 가능 (run_id에서 해당하는 날짜 구간을 찾아 실행)\nairflow dags backfill -s 2023-04-19T22:00:00+00:00 -e 2023-04-20T22:00:00+00:00 —reset-dagruns dags_seoul_api_corona\n\n위의 예시에서, -s 2023-04-19 -e 2023-04-21 옵션이 있고 grid에서 task 수행 이력의 가장 최근 날짜가 2023-04-21 이라고 가정해보자.\n\nrun_id가 scheduled__2023-04-21T22:00:00+00:00 일때\n위의 날짜 구간 옵션에 있고 dag이 실행되지 않았던 04/20 22:00, 04/19 22:00 2개가 돌아가게 됨\n\n\n\n\n\n\nClear 작업을 start / end 구간으로 일괄 재실행\n\nbackfill의 경우 task가 수행이 됐건 안됐건 무조건 실행 (무조건 재실행)\n하지만 clear 이미 실행됐던 task에 한해서 재실행됨\nBackfill과 달리 수행되지 않은 스케줄 구간은 실행할 수 없음\n\nairflow tasks clear [-h] [-R] [-d] [-e END_DATE] [-X] [-x] [-f] [-r]\n                    [-s START_DATE] [-S SUBDIR] [-t TASK_REGEX] [-u] [-v] [-y]\n                    dag_id\nairflow tasks clear -s 2023-05-07 -e 2023-05-12 dags_seoul_api_corona\nairflow tasks clear -s 2023-05-07T22:00:00+00:00 -e 2023-05-12T22:00:00+00:00 dags_seoul_api_corona\nBackfill되었던 DAG은 clear 불가함. reset-dagruns 옵션과 함께 다시 Backfill 수행해야 함\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparision\ntrigger\nbackfill\nclear\n\n\n\n\n목적\n특정 날짜로 DAG Trigger\nStart ~ end 구간의 스케줄 실행\nStart ~ end 구간 내 이미 수\n\n\n행되었던 스케줄 재실행\n\n\n\n\n\nRun type\n-r 옵션으로 지정 가능. 없으면 Manual\nBackfill\n원래의 run_type\n\n\n기 수행된 run_id가 존재하는 경우\n동일 run_id 가 존재하는 경우 에러 발생\nRun_type 을 Backfill 로 덮어쓰며\n\n\n\n재실행\n재실행\n\n\n\n\n구간 지정\n불가\n가능\n가능\n\n\n과거 날짜 적용 가능\n가능\n가능\n불가\n\n\ntask 선택 가능\n불가\n가능\n가능\n\n\n\n\n공통점: CLI 명령으로 DAG 실행 가능\n\n\n\n\n\nScheduler, worker, webserver, triggerer containers 중 하나\n\n\n\nAirflow는 그 자체로 ETL 툴이라기보다 오케스트레이션 솔루션\n왜냐면 airflow와 연계되는 외부 솔루션에 작업 제출, 상태 확인, 완료 확인 등의 절차를 통해 관리\n예를 들어 airflow의 worker container가 python logic을 직접 처리하는게 아니라 python logic 을 python이 처리하도록 명령을 제출하고 로직 확인 및 결과 확인을 수행한다.\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\n\n\n\nairflow\n\nairflow\n\n\n\nPostgresqlDB\n\nPostgresqlDB\n\n\n\nairflow-&gt;PostgresqlDB\n\n\n작업 실행\n\n\n\nHive\n\nHive\n\n\n\nairflow-&gt;Hive\n\n\n\n\n\nHDFS\n\nHDFS\n\n\n\nairflow-&gt;HDFS\n\n\n작업 실행\n\n\n\nBigquerty\n\nBigquerty\n\n\n\nairflow-&gt;Bigquerty\n\n\n완료 확인\n\n\n\nPython_func\n\nPython_func\n\n\n\nairflow-&gt;Python_func\n\n\nPython func\n\n\n\nSpark\n\nSpark\n\n\n\nairflow-&gt;Spark\n\n\n상태 확인\n\n\n\n\n\n\n\n\n\nbigquerty: google에 있는 data 저장소 서비스\n외부 솔루션에 작업이 제출되어 완료될 때까지 Airflow의 Slot은 점유됨\n\n * airflow 의 task가 들어왔을 때 task는 airflow worker의 slot을 차지하게 됨 * 후에, 작업 대상에 작업을 제출하고 작업이 시작된다 * 작업대상: python function 또는 postgres, HDFS, Spark와 같은 외부 솔루션 * 작업이 진행되면 airflow는 작업이 완료 되었는지 작업상태를 polling 하면서 지속적으로 체크 * 작업 처리가 진행되는 동안 task는 차지했던 worker의 slot을 게속해서 차지한다. * task가 많아지면 airflow의 slot이 부족할 수도 있는 상황이 있음 * 그럼 작업 처리동안 task는 slot 점유할 필요는 없지 않나라는 생각이 들 수 있다. * triggerer가 이 문제를 해결 * 작업을 제출하고 Task는 작업 처리가 시작될 때 Slot을 비우고 작업 상태 Polling 작업은 Triggerer 에게 위임 * 작업 처리 시작 전까지는 slot을 점유 * 작업상태를 끊임없이 polling하면서 확인해야하는데 triggerer를 이용하면 이 작업이 없어짐 * triggerer는 작업 처리 완료가 되는 event (작업 완료 callback message를 수신)를 받아 scheduluer container에게 message를 전달하고 scheduler는 task가 다시 비워진 slot을 점유하게 한다.\n\n\n\n\n\n워커를 대신하여 작업 상태 완료를 수신하고, 그때까지 Slot을 비워둘 수 있도록 해주는 Airflow의 서비스\n\nairflow service: 스케줄러, 워커 같은 요소 중 하나\n\nPython의 비동기 작업 라이브러리인 asyncio를 이용하여 작업상태 수신\n\n사용 조건: Airflow 2.2 부터 & Python 3.7부터 사용 가능\n\n어떻게 사용하나?\n\nDeferrable Operator 이용하여 Task 생성\n기본 Operator 중에서는 아래의 Sensor 종류만 사용 가능\n\nTimeSensorAsync\nDateTimeSensorAsync\nTimeDeltaSensorAsync\n\n끝에 Async 가 붙은 오퍼레이터를 Deferrable Operator라 부르며 Triggerer에게 작업 완료 수신을 맡기는 오퍼레이터라는 의미\n\n\n\n\n\n비교 실험\n\ndags/dags_time_sensor.py (함수를 그냥 짬)\ndags/dags_time_sensor_with_async.py (asyncio library 이용)\n\n\n\n\n\ndag_time_sensor.py\n\nimport pendulum\nfrom airflow import DAG\nfrom airflow.sensors.date_time import DateTimeSensor\n\nwith DAG(\n    dag_id=\"dags_time_sensor\",\n    # 1시간 차이\n    start_date=pendulum.datetime(2023, 5, 1, 0, 0, 0), #5월1일 0시\n    end_date=pendulum.datetime(2023, 5, 1, 1, 0, 0), #5월1일 1시\n    schedule=\"*/10 * * * *\", #10 분마다 1시간안에 7번 돌게함\n    # 00분, 10분, 20분, 30분, 40분, 50분, 60분 총 7번\n    catchup=True, # catchup을 true이기 때문에 작업 상태bar 7개가 동시에 뜸\n    # airflow ui의 작업 pool을 보면 시작할 때 7개를 모두 차지 하도록 나옴\n) as dag:\n    # DateTimeSensor는 목표로 하는 시간까지 기다리는 sensor\n    sync_sensor = DateTimeSensor(\n        task_id=\"sync_sensor\",\n        # 현재 시간 + 5분\n        target_time=\"\"\"{{ macros.datetime.utcnow() + macros.timedelta(minutes=5) }}\"\"\",\n    )\n\ndags_time_sensor_with_async.py\n\nimport pendulum\nfrom airflow import DAG\nfrom airflow.sensors.date_time import DateTimeSensorAsync\n\nwith DAG(\n    dag_id=\"dags_time_sensor_with_async\",\n    start_date=pendulum.datetime(2023, 5, 1, 0, 0, 0),\n    end_date=pendulum.datetime(2023, 5, 1, 1, 0, 0),\n    schedule=\"*/10 * * * *\",\n    catchup=True,\n) as dag:\n    sync_sensor = DateTimeSensorAsync(\n        task_id=\"sync_sensor\",\n        target_time=\"\"\"{{ macros.datetime.utcnow() + macros.timedelta(minutes=5) }}\"\"\",\n    )\n\nairflow ui&gt;&gt; browse &gt;&gt; triggers 에서 trigger가 작업하는 대상 목록을 보여줌\ntrigger는 triggerer에게 작업을 맡길 event 또는 ticket이라고 생각하면 됨\ntriggerer id는 ticket id\ndefered status 는 보라색을 띄고 이 상태에서는 worker slot을 차지 않는 상태이다.\n\n\n\n\n\n끝에 Async가 붙은 오퍼레이터는 Deferrable Operator 라 부르며 Triggerer에 의해 Polling이 수행되는 오퍼레이터임을 의미\nDeferrable Operator는 작업 제출 후 Slot을 차지하지 않으며 Polling 내역에 대해 Trigger 제출 후 deferred 상태가 됨.\nTriggerer는 제출된 Trigger 내역을 보고 작업 완료시(조건 만족시) Worker에게 알려줘 작업이 마무리될 수 있도록 함.\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dataset의-필요성",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dataset의-필요성",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "DAG간의 의존관계를 설정할 때 TriggerDagRun 과 ExternalTask Sensor를 이용한다. 그런데 이 2가지 방법을 이용하면 관리가 복잡해진다. 이를 해결하기 위해 고안된 기술이 dataset을 이용하여 DAG간의 의존관계를 설정하는 것이다. 어떤 상황에서 DAG 관리가 복잡해지고 dataset을 이용하면 어떻게 관리의 효율성이 증가하는지 다음의 cases를 통해 알아보자.\n\ncase 1\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nDAG A\n\n\ncluster1\n\nDAG B\n\n\n\ntask_1\n\ntask_1\n\n\n\ntask_3\n\ntask_3\n\n\n\ntask_1-&gt;task_3\n\n\n\n\n\ntask_k\n\ntask_k\n\n\n\n\n\n\n\n\n\n왼쪽 그림(요구사항) 에서, task3가 TriggerDagRun operator 으로 만들어진 task라 가정한다. task3가 DAG C의 task x를 trigger 한다고 가정한다. 이 경우를 변경하여 Task_1 뿐만 아니라 DAG_B의 task_k 에도 선행 의존관계가 걸려야 한다고 가정한다. 그렇게 하기위해서 External Task 센서를 하나 달아 개선해야 한다는 상황이 있다고 가정한다. 결국, task1과 task k가 성공적으로 끝나야 task3가 수행되고 이어서 DAG C의 task3가 돌아가야 하는 상황\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster1\n\nDAG C\n\n\ncluster0\n\nDAG A\n\n\ncluster2\n\nDAG B\n\n\n\ntask_1\n\ntask_1\n\n\n\nSensor_task_1\n\nSensor_task_1\n\n\n\nSensor_task_1-&gt;task_1\n\n\nsensing\n\n\n\ntask_x\n\ntask_x\n\n\n\nSensor_task_1-&gt;task_x\n\n\n\n\n\nSensor_task_2\n\nSensor_task_2\n\n\n\nSensor_task_2-&gt;task_x\n\n\n\n\n\ntask_k\n\ntask_k\n\n\n\nSensor_task_2-&gt;task_k\n\n\nsensing\n\n\n\n\n\n\n\n\n\n오른쪽 그림(해결책)은 왼쪽 그림을 개선하고자 만든 DAG의 의존관계, task3는 제거되어야 한다. 대신, DAG C를 external task sensor를 추가하도록 수정한다. sensor task1은 task1을 수행이 되고 sensor task2가 taskk의 수행이 성공적으로 완료가 된 후에 이 두 sensor task들이 taskx를 수행하도록 개선해야 한다.\n\n\n\n이렇게 DAG을 관리할 때 추가적인 DAG의 의존관계 설정을 해야할 때 추가적인 task들이 생기게 되는 요구사항을 해결해야할 때 오른쪽 그림과 같이 복잡한 workflow가 생기게 되는 case가 발생할 수 있다. 또 다른 case는 case2와 같다.\ncase 2\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nDAG A\n\n\ncluster\n\nDAG Y\n\n\n\ntask_1\n\ntask_1\n\n\n\ntask_3\n\ntask_3\n\n\n\ntask_1-&gt;task_3\n\n\n\n\n\ntask_4\n\ntask_4\n\n\n\ntask_y\n\ntask_y\n\n\n\n\n\n\n\n\n\n왼쪽 그림(요구사항)에서, Task_1이 완료되면 또 다른 dag 을 trigger 되고 싶은데 DAG A 를 수정해야 할 경우이다. 다시 말해서, task1이 완료되면 또 다른 dag Y의 task y를 trigger하는 TriggerDagRun task4를 만들어 주고 싶도록 dag A를 수정해야하는 상황. 즉, task y는 task 1 과 task 4가 성공적으로 완료되어야만 수행되도록 설정해주고 싶은 상황. 뿐만 아니라 DAG A에 지속적인 변경사항 있을시 계속 workflow를 수정해야하는 문제점이 있다.\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster\n\nDAG Y\n\n\n\ntask_1\n\ntask_1\n\n\n\ntask_3\n\ntask_3\n\n\n\ntask_1-&gt;task_3\n\n\n\n\n\ntask_4\n\ntask_4\n\n\n\ntask_1-&gt;task_4\n\n\n\n\n\ntask_y\n\ntask_y\n\n\n\ntask_4-&gt;task_y\n\n\n\n\n\n\n\n\n\n\n\n오른쪽 그림이 개선된 workflow\n\n\n\n이렇게, TriggerDagRun 오퍼레이터와 ExternalTask 센서를 많이 사용하다보면 아래 그림과 같이 연결관리에 많은 노력이 필요\n특히, ExternalTask은 execution_delta, 즉 data_interval_start 차이를 정확하게 연결해야 작동하도록 되어 있기 때문에 매우 강한 연결이 전제가 된다. 즉, parameter 중 일부가 안맞거나 틀리게 되면 제대로 sensing이 안됨\n\n\n\n\n\n\n\n\nG\n\n\ncluster\n\nDAG Flow\n\n\n\nDAG_A\n\nDAG_A\n\n\n\nDAG_B\n\nDAG_B\n\n\n\nDAG_A-&gt;DAG_B\n\n\n\n\n\nDAG_C\n\nDAG_C\n\n\n\nDAG_A-&gt;DAG_C\n\n\n\n\n\nDAG_D\n\nDAG_D\n\n\n\nDAG_D-&gt;DAG_C\n\n\n\n\n\nDAG_E\n\nDAG_E\n\n\n\nDAG_D-&gt;DAG_E\n\n\n\n\n\nDAG_G\n\nDAG_G\n\n\n\nDAG_E-&gt;DAG_G\n\n\n\n\n\n\n\n\n\n\n\n위와 같은 복잡하고 rigid한 workflow를 개선한 것이 dataset을 이용한 방식이다.\n\n큐시스템과 같이 Job 수행이 완료된 Task 는 Push 하고 센싱이 필요한 task 은 큐 시스템을 구독하는 방식을 쓴다면 더 유연한 workflow가 생성될 수 있다. (약한 연결)\n\n\n\n\n\n\n\n\nG\n\n\ncluster\n\nDAG Flow\n\n\n\nDAG_A\n\nDAG_A\n\n\n\nQueue\n\n\nQueue\n\n\n\nDAG_A-&gt;Queue\n\n\n\n\n\nDAG_B\n\nDAG_B\n\n\n\nDAG_C\n\nDAG_C\n\n\n\nDAG_D\n\nDAG_D\n\n\n\nDAG_D-&gt;Queue\n\n\n\n\n\nDAG_E\n\nDAG_E\n\n\n\nDAG_G\n\nDAG_G\n\n\n\nQueue-&gt;DAG_B\n\n\n\n\n\nQueue-&gt;DAG_C\n\n\n\n\n\nQueue-&gt;DAG_E\n\n\n\n\n\n\n\n\n\n\n\nDAG A가 작업이 끝났다라는 메시지를 큐에 넣고 DAG A가 완료되면 자동으로 DAG B가 triggering됨\n나중에 DAG F가 새로 생겨 DAG A가 끝났을 때 수행되도록 triggering 되어야 한다면 큐에 있는 메시지를 인식하고 돌아가도록 설정하면 된다.\n그러므로, DAG A를 구독하는 후행 DAG들이 아무리 많더라도 큐에 저장된 메시지만 바라보게 만들면 된다. DAG A는 수정할 필요가 없게 된다.\nProduce / Consume 구조를 이용하여 Task 완료를 알리기 위해 특정 키 값으로 Produce 하고 해당 키를 Consume 하는 DAG 을 트리거 스케줄 할 수 있는 기능\n\nproduce: queue에 메세지를 넣는 행위. 다른 솔루션에서는 produce를 publish라고도 부름\n\nconsume: queue로부터 메세지를 읽거나 꺼내는 행위. 보통은 이 consume다른 솔루션에서는 subscribe라고도 많이 부름. airflow에서는 subscribe가 consume으로 사용된다.\n선행 Dag이 produce하고 후행 Dag이 consume한다.\n실제 큐가 있는 것은 아니고 DB 에 Produce / Consume 내역을 기록"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#정리",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#정리",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "Dataset 은 Publish(produce)/subscribe(consume) 구조로 DAG 간 의존관계를 주는 방법\nUnique 한 String 형태의 Key 값을 부여하여 Dataset Publish\nDataset 을 Consume 하는 DAG 은 스케줄을 별도로 주지 않고 리스트 형태로 구독할 Dataset 요소들을 명시\nDataset에 의해 시작된 DAG 의 Run_id 는 dataset_triggered__{trigger 된 시간 } 으로 표현됨\nAirflow 화면 메뉴 중 Datasets 메뉴를 통해 별도로 Dataset 현황 모니터링 가능\nN개의 Dataset 을 구독할 때 스케줄링 시점은 ?\n\n마지막 수행 이후 N 개의 Dataset 이 모두 재 업데이트되는 시점"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dag-의-default_args-파라미터",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dag-의-default_args-파라미터",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "목적: DAG 하위 모든 오퍼레이터에 공통 적용될 파라미터를 입력\n어떤 파라미터들이 적용 가능할까?\n\n오퍼레이터에 공통 적용할 수 있는 파라미터들\nBaseOperator 클래스 생성자가 가지고 있는 파라미터\n\nairflow doc\nCode\n\n\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n) as dag:\n    \n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='sleep 10m',\n        sla: timedelta(seconds=70), #SLA 설정\n        email: 'sdf@sdfsfd.com'\n    )\n    \n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='sleep 2m',\n        sla: timedelta(seconds=70), #SLA 설정\n        email: 'sdf@sdfsfd.com'\n    )\n\n   task_1 &gt;&gt; task_2\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': 'sdf@sdfsfd.com'\n    }\n) as dag:\n    \n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='sleep 10m'\n    )\n    \n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='sleep 2m'\n    )\n\n   task_1 &gt;&gt; task_2"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#baseoperator-파라미터-vs-dag-파라미터",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#baseoperator-파라미터-vs-dag-파라미터",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "DAG 파라미터는 DAG 단위로 적용될 파라미터\n개별 오퍼레이터에 적용되지 않음\nDAG 파라미터는 default_args 에 전달하면 안됨\n\n\n\n\n\n\nBase 오퍼레이터 파라미터는 개별 Task 단위로 적용될 파라미터.\nTask 마다 선언해줄 수 있지만 DAG 하위 모든 오퍼레이터에 적용 필요시 default_args 를 통해 전달 가능\n\n\n\n\n\ndefault_args 에 전달된 파라미터보다 개별 오퍼레이터에 선언된 파라미터가 우선순위를 가짐\n\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': 'sdf@sdfsfd.com'\n    }\n) as dag:\n    \n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='sleep 10m'\n    )\n    \n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='sleep 2m',\n        sla=timedelta(minutes=1)\n    )\n\n   task_1 &gt;&gt; task_2"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#email-발송-위한-파라미터-확인",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#email-발송-위한-파라미터-확인",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "airflow doc\nemail 파라미터와 email_on_failure 파라미터를 이용\nemail 파라미터만 입력하면 email_on_failure 파라미터는 True 로 자동설정됨"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#email-발송-대상-등록",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#email-발송-대상-등록",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.decorators import task\nfrom airflow.exceptions import AirflowException\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_email_on_failure',\n    start_date=pendulum.datetime(2023,5,1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule='0 1 * * *',\n    dagrun_timeout=timedelta(minutes=2),\n    default_args={\n        'email_on_failure': True,\n        'email': email_lst\n    }\n) as dag:\n    @task(task_id='python_fail')\n    def python_task_func():\n        raise AirflowException('에러 발생')\n    python_task_func()\n\n    bash_fail = BashOperator(\n        task_id='bash_fail',\n        bash_command='exit 1',\n    )\n\n    bash_success = BashOperator(\n        task_id='bash_success',\n        bash_command='exit 0',\n    )\n\nEmail 받을 대상이 1 명이면 string 형식으로 , 2 명 이상이면 list 로 전달\n그런데 협업 환경에서 DAG 담당자는 수시로 바뀔 수 있고 인원도 수시로 바뀔 수 있는데 그때마다 DAG 을 뒤져서 email 리스트를 수정해야 할까 ?\n이럴때 Variable 을 이용하자"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#sla파라미터-이해",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#sla파라미터-이해",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "개념: 오퍼레이터 수행시 정해놓은 시간을 초과하였는지를 판단할 수 있도록 설정해놓은 시간 값 파이썬의 timedelta 로 정의\n동작: 설정해놓은 SLA 를 초과하여 오퍼레이터 running 시 SLA Miss 발생 , Airflow UI 화면에서 Miss 건만 조회 가능 + email 발송 가능\n\nSLA Miss 발생시 task 가 실패되는 것은 아니며 단순 Miss 대상에 기록만 DB에 남기게 됨\nSLA 는 DAG 파라미터가 아니며 BaseOperator 의 파라미터\n\nairflow docs- def init\n def __init__(\n      self,\n      task_id: str,\n      owner: str = DEFAULT_OWNER,\n      email: str | Iterable[str] | None = None,\n      email_on_retry: bool = conf.getboolean(\"email\", \"default_email_on_retry\", fallback=True),\n      email_on_failure: bool = conf.getboolean(\"email\", \"default_email_on_failure\", fallback=True),\n      retries: int | None = DEFAULT_RETRIES,\n      retry_delay: timedelta | float = DEFAULT_RETRY_DELAY,\n      retry_exponential_backoff: bool = False,\n      max_retry_delay: timedelta | float | None = None,\n      start_date: datetime | None = None,\n      end_date: datetime | None = None,\n      depends_on_past: bool = False,\n      ignore_first_depends_on_past: bool = DEFAULT_IGNORE_FIRST_DEPENDS_ON_PAST,\n      wait_for_past_depends_before_skipping: bool = DEFAULT_WAIT_FOR_PAST_DEPENDS_BEFORE_SKIPPING,\n      wait_for_downstream: bool = False,\n      dag: DAG | None = None,\n      params: collections.abc.MutableMapping | None = None,\n      default_args: dict | None = None,\n      priority_weight: int = DEFAULT_PRIORITY_WEIGHT,\n      weight_rule: str = DEFAULT_WEIGHT_RULE,\n      queue: str = DEFAULT_QUEUE,\n      pool: str | None = None,\n      pool_slots: int = DEFAULT_POOL_SLOTS,\n      sla: timedelta | None = None, # sla 변수\n      execution_timeout: timedelta | None = DEFAULT_TASK_EXECUTION_TIMEOUT,\n      on_execute_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n      on_failure_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n      on_success_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n      on_retry_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n      pre_execute: TaskPreExecuteHook | None = None,\n      post_execute: TaskPostExecuteHook | None = None,\n      trigger_rule: str = DEFAULT_TRIGGER_RULE,\n      resources: dict[str, Any] | None = None,\n      run_as_user: str | None = None,\n      task_concurrency: int | None = None,\n      max_active_tis_per_dag: int | None = None,\n      max_active_tis_per_dagrun: int | None = None,\n      executor_config: dict | None = None,\n      do_xcom_push: bool = True,\n      inlets: Any | None = None,\n      outlets: Any | None = None,\n      task_group: TaskGroup | None = None,\n      doc: str | None = None,\n      doc_md: str | None = None,\n      doc_json: str | None = None,\n      doc_yaml: str | None = None,\n      doc_rst: str | None = None,\n      **kwargs,\n  ):\n\nsla: timedelta | None = None, # sla 변수\n\ntimedelta보다 오랜 시간동안 task가 더 오래 수행되면 SLA Miss 기록만 남기고 실패처리는 안하게 됨\ndefault_arg에 넣을 수 있기 때문에 모든 task, 즉 모든 operator에 공통 적용할 수 있다.\n\n\nsla_miss_callback: DAG에 있는 SLA 관련 파라미터로 SLA Miss시 수행할 함수 지정 가능-Source code for airflow.models.dag\n\nsla_miss_callback 파라미터\n\nsla miss시 수행할 함수명을 입력 받음\ndag에 있는 파라미터이기 때문에 default_arg에 넣을 수 없음\n\nDAG의 파라미터임 (BaseOperator 의 파라미터가 아니라는 것에 유의)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#sla-제약사항",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#sla-제약사항",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "1번째 제약사항: 각 Task 의 SLA timeout 카운트 시작은 DAG 의 시작시간 기준임\n\n\n\n\n\n\n\n처음 사용하는 사용자들은 다음과 같이 생각할 수 있다.\n\ntask1은 task1의 sla구간보다 수행시간이 짧게 완료됐기 때문에 성공 처리\ntask2는 sla 구간보다 더 길기 때문에 miss 처리됨\ntask3는 sla 구간보다 짧기 때문에 성공 처리\n\nairflow는 위와 같이 동작하지 않음 옆의 그림과 같이 동작함\n\n\n\n\n\n\n\n실제 airflow에서는 수행 시간을 dag의 최초 실행시점 부터 카운트 하기 시작함\ntask1의 수행완료 상태 및 시점과 상관없이 task2와 task3의 수행 시간은 task1 수행 시점 부터 카운트 되기 시작한다.\n그러므로 task2, task3는 수행된적이 없음에도 miss sla 상태로 처리됨\n\n\n\n\n\n2번째 제약사항: DAG의 시작시간 기준 \\(\\rightarrow\\) 명목적인 시작 시간 (data_interval_start)\n\n위의 SLA timeout 카운트 시작은 dag의 시작시간 기준이라는 제약 사항과 연결되는 제약사항\n만약 Pool 이 부족하거나 스케줄러의 부하로 인해 DAG 이 스케줄보다 늦게 시작했다면 늦게 시작한만큼 이미 SLA 시간 카운트는 진행되어 시간이 소요되고 있음\n\n실질적 DAG 시작 시간을 기준으로 카운트하지 않음\n\n즉, 특정 task 수행에 computing 부하가 걸리면 후차적인 task들은 모두 miss 처리됨\n대규모 프로젝트에서 흔히 발생하는 문제로 dag이 밀리는 현상이 자주 관찰되므로 sla는 자주쓰이는 방식은 아님.\n\n3번째 제약사항: 첫 스케줄에서는 SLA Miss 가 기록되지 않음 (두 번째 스케줄부터 기록)\n4번째 제약사항: sla miss처리가 분명히 발생됐는데 가끔 기록이 안될때가 있고 email역시 발송이 되지 않을때가 있음. 엄격이 관리가 되지 않는 기능"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#full-dag-example",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#full-dag-example",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': email_lst\n    }\n) as dag:\n    \n    # 30초 sleep\n    task_slp_30s_sla_70s = BashOperator( \n        task_id='task_slp_30s_sla_70s',\n        bash_command='sleep 30'\n    )\n    # 60초 sleep\n    task_slp_60_sla_70s = BashOperator(\n        task_id='task_slp_60_sla_70s',\n        bash_command='sleep 60'\n    )\n\n    # 10초 sleep\n    task_slp_10s_sla_70s = BashOperator( \n        task_id='task_slp_10s_sla_70s',\n        bash_command='sleep 10'\n    )\n\n    # 10초 sleep\n    # sla의 timedelta를 명시적으로 30초 선언\n    # default argument보다 명시적 선언이 우선 순위가 더 높음\n    # 그래서, 처음 3개의 task는 timedelta가 70초로 설정됐고 4번째 task는 30초로 설정됨 \n    task_slp_10s_sla_30s = BashOperator( \n        task_id='task_slp_10s_sla_30s',\n        bash_command='sleep 10',\n        sla=timedelta(seconds=30)\n    )\n\n    task_slp_30s_sla_70s &gt;&gt; task_slp_60_sla_70s &gt;&gt; task_slp_10s_sla_70s &gt;&gt; task_slp_10s_sla_30s\n\n위 코드를 보면,\n\n1번째 task는 30초 sleep이 70초 sla timedelta보다 짧기 때문에 성공 처리\n2번째 task는 30,40초 돌다가 miss 처리됨\n3,4 번째 task는 수행되기도 전에 miss처리됨\n4번째 timedelta가장 짧기 때문에 가장 먼저 miss 처리 됨\n\nAirflow web service에서 SLA Miss 현황 조회하기\n\nairflow web service &gt;&gt; browse"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#sla-miss-시-email-발송하기",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#sla-miss-시-email-발송하기",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': email_lst\n    }\n) as dag:\n    \n    task_slp_30s_sla_70s = BashOperator(\n        task_id='task_slp_30s_sla_70s',\n        bash_command='sleep 30'\n    )\n    \n    task_slp_60_sla_70s = BashOperator(\n        task_id='task_slp_60_sla_70s',\n        bash_command='sleep 60'\n    )\n\n    task_slp_10s_sla_70s = BashOperator(\n        task_id='task_slp_10s_sla_70s',\n        bash_command='sleep 10'\n    )\n\n    task_slp_10s_sla_30s = BashOperator(\n        task_id='task_slp_10s_sla_30s',\n        bash_command='sleep 10',\n        sla=timedelta(seconds=30)\n    )\n\n    task_slp_30s_sla_70s &gt;&gt; task_slp_60_sla_70s &gt;&gt; task_slp_10s_sla_70s &gt;&gt; task_slp_10s_sla_30s"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#timeout-파라미터-이해",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#timeout-파라미터-이해",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "Task수준의 timeout 과 DAG 수준의 timeout 이 존재\nexecution_timeout: Task수준의 timeout 파라미터: baseOperator 명세에서 init 생성자에서 sla 파라미터 아래에 있음\n\nexecution_timeout: timedelta | None = DEFAULT_TASK_EXECUTION_TIMEOUT,\ntimedelta보다 오래 task가 수행됐을 때 task는 fail 처리됨\ntask가 fialure 됐을 때 init 생성자 안의 다른 파라미터인 email_on_failure: bool = conf.getboolean(\"email\", \"default_email_on_failure\", fallback=True) 과 email: str | Iterable[str] | None = None, 에 의해 지정 대상에게 email을 보낼 수 있다.\n요약하면, timedelta, email_on_failure, email 이렇게 3개의 파라미터를 이용하여 timedelta 를 초과하는 task 수행시 fail 처리를 하여 이메일을 보낼 수 있다.\n\ndagrun_timeout: DAG수준에서도 timeout 파라미터를 걸 수 도 있음 : dagrun_timeout\n\n[docs]class DAG(LoggingMixin): 의 init 생성자\n\n\n    def __init__(\n      self,\n      dag_id: str,\n      description: str | None = None,\n      schedule: ScheduleArg = NOTSET,\n      schedule_interval: ScheduleIntervalArg = NOTSET,\n      timetable: Timetable | None = None,\n      start_date: datetime | None = None,\n      end_date: datetime | None = None,\n      full_filepath: str | None = None,\n      template_searchpath: str | Iterable[str] | None = None,\n      template_undefined: type[jinja2.StrictUndefined] = jinja2.StrictUndefined,\n      user_defined_macros: dict | None = None,\n      user_defined_filters: dict | None = None,\n      default_args: dict | None = None,\n      concurrency: int | None = None,\n      max_active_tasks: int = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\"),\n      max_active_runs: int = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\"),\n      dagrun_timeout: timedelta | None = None,\n      sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None,\n      default_view: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),\n      orientation: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),\n      catchup: bool = airflow_conf.getboolean(\"scheduler\", \"catchup_by_default\"),\n      on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,\n      on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,\n      doc_md: str | None = None,\n      params: collections.abc.MutableMapping | None = None,\n      access_control: dict | None = None,\n      is_paused_upon_creation: bool | None = None,\n      jinja_environment_kwargs: dict | None = None,\n      render_template_as_native_obj: bool = False,\n      tags: list[str] | None = None,\n      owner_links: dict[str, str] | None = None,\n      auto_register: bool = True,\n  ):\n\n다음 파라미터를 이용하여 dag을 관리할 수 있다.\n\nschedule: ScheduleArg = NOTSET,\nstart_date: datetime | None = None,\ndefault_args: dict | None = None,\ndagrun_timeout: timedelta | None = None,\n\n\ntask1 &gt;&gt; task2 &gt;&gt; task3 예시\n\n\n\n\n\n\ndag은 timeout이 안됐지만 task가 timeout이 되는 case\n\n\n\ntask2의 실행시간이 task2의 execution_timeout보다 길게 수행되어 fail되었고 task3은 task2로 인해 upstream failed 이 발생\n하지만, task1,2,3 의 수행시간의 총합이 dagrun_timeout 보다 짧기 때문에 dag 자체는 실패처리 안됨\n\n\n\n\n\n\ntask는 성공 처리 됐지만 dag이 오랜시간동안 돌아 timeout되어 실패처리되는 case\n\n\n\n각 task들이 execution_timeout보다 짧아 모두 정상 처리 되었지만 task1,2,3의 총 실행시간이 dagrun_timeout보다 길어 dag자체는 failure이 된 상황\ndag이 failure되는 시점에서의 task는 skipped 상태로 처리된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dag-full-example",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dag-full-example",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "dags_dataset_producer_1.py\n\nfrom airflow import Dataset #airflow의 dataset이라는 library 호출\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_1 = Dataset(\"dags_dataset_producer_1\")\n# Dataset의 인수값으로 string을 주는데 DAG이 큐에 메세지를 넣을 때 메세지가  dictionary형태로 저장이 되는데 그 키값을 input으로 넣게 된다. \n\n\nwith DAG(\n        dag_id='dags_dataset_producer_1',\n        schedule='0 7 * * *',\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        outlets=[dataset_dags_dataset_producer_1], # outlets 인수는 baseOperator로부터 상속된다. 모든 operator는 outlets인수를 가지고 있다. outlets에 다가는 리스트 구조로 선언된다. queue에다가 publish되는 메세지는 dag안에 있는 task('bash_task')가 dataset_dags_dataset_producer_2 이름으로  publish된다.  dataset_dags_dataset_producer_2가 들어가 있다. \n        bash_command='echo \"producer_1 수행 완료\"'\n    )\n\ndags_dataset_producer_2.py\n\nfrom airflow import Dataset\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_2 = Dataset(\"dags_dataset_producer_2\")\n\nwith DAG(\n        dag_id='dags_dataset_producer_2',\n        schedule='0 7 * * *',\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        outlets=[dataset_dags_dataset_producer_2], \n        bash_command='echo \"producer_2 수행 완료\"'\n    )\n\ndags_dataset_consumer_1.py\n\nfrom airflow import Dataset \nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_1 = Dataset(\"dags_dataset_producer_1\") # dags_dataset_producer_1.py의 producer1의 키값 'dags_dataset_producer_1'을 넣어 호출하고 있음\n\nwith DAG(\n        dag_id='dags_dataset_consumer_1',\n        schedule=[dataset_dags_dataset_producer_1], # 스케쥴이 다른 형식으로 들어가 있는데 dags_dataset_producer_1.py의 producer1 (정확히 말하면 task)을 구독하겠다는 뜻. 스케쥴은 없고 publish하고 있는 dag 수행이 끝나면 실행하겠다는 뜻.\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        bash_command='echo {{ ti.run_id }} && echo \"producer_1 이 완료되면 수행\"'\n    )\n\ndags_dataset_consumer_2.py\n\nfrom airflow import Dataset\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_1 = Dataset(\"dags_dataset_producer_1\")\ndataset_dags_dataset_producer_2 = Dataset(\"dags_dataset_producer_2\")\n# producer1,2 2개를 동시에 구독하고있음\n\nwith DAG(\n        dag_id='dags_dataset_consumer_2',\n        schedule=[dataset_dags_dataset_producer_1, dataset_dags_dataset_producer_2],\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        bash_command='echo {{ ti.run_id }} && echo \"producer_1 와 producer_2 완료되면 수행\"'\n    )\nairflow web service에서 dag 상태를 확인할 때 초록색 긴막대기를 누르면 Dataset Events에서 구독 depndencies를 확인할 수 있다. Run ID도 특이하게 나오게 되는 것을 확인 할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#정리-1",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#정리-1",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "Comparision\nsla\nexecution_timeout\ndagrun_timeout\n\n\n\n\n파라미터 정의 위치\nBaseOperator\nBaseOperator\nDAG\n\n\n적용 수준\nTask\ntask\nDAG\n\n\n기능\n지정한 시간 초과시 Miss 기록\n지정한 시간 초과시 task fail 처리\n지정한 시간 초과시 DAG fail 처리\n\n\nemail 발송 가능 여부\nO\nO\nX\n\n\ntimeout 발생시 후행 task 상태\n상관없이 지속\nUpstream_failed\nSkipped (current) /No status (not run)\n\n\n스케쥴 필요\nO\nX\nX\n\n\n\n\nsla, execution_timeout에는 email 발송 paraemeter가 있지만 execution_timeout에는 없다.\ndagrun timeout이 fail 됐을 때 반드시 email 발송 하고싶으면 dag의 파라미터 중 on_failure_callback 에 dag이 실패됐을 때 이메일을 전송하는 함수를 만들어 그 함수명을 할당해준다.\nupstream_failed 상태는 execution_timeout만이 갖는 특징이 아니라 airflow의 디폴트 설정이다. 상위 task들이 fail되면 후행 task들은 upstream_failed로 남는다.\n공통점: 파이썬의 timedelta 함수로 timeout 기준 시간 정의"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#cli---dag-trigger",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#cli---dag-trigger",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "dag trigger: airflow ui 상에서 manual 로 dag trigger 하거나 run_id 를 직접 넣어 trigger 할 수 있는 기능으로 CLI로 실행시킬 수 있다.\nCLI 명령은 WSL2에서 하는게 아니라 docker container안에서 해야함\nairflow dags trigger [-h] [-c CONF] [-e EXEC_DATE] \n                     [--no-replace-microseconds] [-o table, json, yaml, plain] [-r RUN_ID] [-S SUBDIR] [-v]\n                   dag_id\n\nEXEC_DATE: execution_date parameter는 모두 data_interval_start 기준이며 String 형식으로 입력하면 기본 UTC로 계산됨\nRUN_ID를 입력하면 기본적으로 manual__로 시작하며 run_id를 직접 입력도 가능\n예시: #&gt; airflow dags trigger dags_seoul_api_corona\n\nFull Example\n\n# docker container list 확인\nsudo docker ps \n\n# webserver container 선택 (어떤 것을 골라도 상관없음)\n\n# airflow container 들어가기: sudo docker exec -it [docker_container_id] bash\nsudo docker exec -it 8b755cb5aa70 bash \n\n# trigger 명령어 실행\nairflow dags trigger dags_base_branch_operator"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#결과",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#결과",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "dag_run_id: manual__2023-07-22T05:05:59+00:00.\nrun type: manual__\n\nClI 로 돌렸기 때문에 manual_이 붙어있음\nrun types: schedule, manual, backfill 등이 있음"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#cli---dag-backfill",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#cli---dag-backfill",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "입력 스케줄 구간에 대해 일괄 (재)실행 (스케줄 이력이 없는 과거 날짜도 가능)\n\n# start (-s), end(-ㄷ) 파라미터를 dashed string 형태로 입력하면 UTC로 간주(아래의 start 날짜에 시간:분:초가 나와있지 않지만 날짜뒤에 00:00:00 가 붙음) \nairflow dags backfill -s 2023-04-19 -e 2023-04-21 dags_seoul_api_corona\n\n# 타임스탬프 형태로 직접 작성도 가능 (run_id에서 해당하는 날짜 구간을 찾아 실행)\nairflow dags backfill -s 2023-04-19T22:00:00+00:00 -e 2023-04-20T22:00:00+00:00 —reset-dagruns dags_seoul_api_corona\n\n위의 예시에서, -s 2023-04-19 -e 2023-04-21 옵션이 있고 grid에서 task 수행 이력의 가장 최근 날짜가 2023-04-21 이라고 가정해보자.\n\nrun_id가 scheduled__2023-04-21T22:00:00+00:00 일때\n위의 날짜 구간 옵션에 있고 dag이 실행되지 않았던 04/20 22:00, 04/19 22:00 2개가 돌아가게 됨"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#cli---task-clear",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#cli---task-clear",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "Clear 작업을 start / end 구간으로 일괄 재실행\n\nbackfill의 경우 task가 수행이 됐건 안됐건 무조건 실행 (무조건 재실행)\n하지만 clear 이미 실행됐던 task에 한해서 재실행됨\nBackfill과 달리 수행되지 않은 스케줄 구간은 실행할 수 없음\n\nairflow tasks clear [-h] [-R] [-d] [-e END_DATE] [-X] [-x] [-f] [-r]\n                    [-s START_DATE] [-S SUBDIR] [-t TASK_REGEX] [-u] [-v] [-y]\n                    dag_id\nairflow tasks clear -s 2023-05-07 -e 2023-05-12 dags_seoul_api_corona\nairflow tasks clear -s 2023-05-07T22:00:00+00:00 -e 2023-05-12T22:00:00+00:00 dags_seoul_api_corona\nBackfill되었던 DAG은 clear 불가함. reset-dagruns 옵션과 함께 다시 Backfill 수행해야 함"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#정리-2",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#정리-2",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "Comparision\ntrigger\nbackfill\nclear\n\n\n\n\n목적\n특정 날짜로 DAG Trigger\nStart ~ end 구간의 스케줄 실행\nStart ~ end 구간 내 이미 수\n\n\n행되었던 스케줄 재실행\n\n\n\n\n\nRun type\n-r 옵션으로 지정 가능. 없으면 Manual\nBackfill\n원래의 run_type\n\n\n기 수행된 run_id가 존재하는 경우\n동일 run_id 가 존재하는 경우 에러 발생\nRun_type 을 Backfill 로 덮어쓰며\n\n\n\n재실행\n재실행\n\n\n\n\n구간 지정\n불가\n가능\n가능\n\n\n과거 날짜 적용 가능\n가능\n가능\n불가\n\n\ntask 선택 가능\n불가\n가능\n가능\n\n\n\n\n공통점: CLI 명령으로 DAG 실행 가능"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#airflow-triggerer의-필요성",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#airflow-triggerer의-필요성",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "Airflow는 그 자체로 ETL 툴이라기보다 오케스트레이션 솔루션\n왜냐면 airflow와 연계되는 외부 솔루션에 작업 제출, 상태 확인, 완료 확인 등의 절차를 통해 관리\n예를 들어 airflow의 worker container가 python logic을 직접 처리하는게 아니라 python logic 을 python이 처리하도록 명령을 제출하고 로직 확인 및 결과 확인을 수행한다.\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\n\n\n\nairflow\n\nairflow\n\n\n\nPostgresqlDB\n\nPostgresqlDB\n\n\n\nairflow-&gt;PostgresqlDB\n\n\n작업 실행\n\n\n\nHive\n\nHive\n\n\n\nairflow-&gt;Hive\n\n\n\n\n\nHDFS\n\nHDFS\n\n\n\nairflow-&gt;HDFS\n\n\n작업 실행\n\n\n\nBigquerty\n\nBigquerty\n\n\n\nairflow-&gt;Bigquerty\n\n\n완료 확인\n\n\n\nPython_func\n\nPython_func\n\n\n\nairflow-&gt;Python_func\n\n\nPython func\n\n\n\nSpark\n\nSpark\n\n\n\nairflow-&gt;Spark\n\n\n상태 확인\n\n\n\n\n\n\n\n\n\nbigquerty: google에 있는 data 저장소 서비스\n외부 솔루션에 작업이 제출되어 완료될 때까지 Airflow의 Slot은 점유됨\n\n * airflow 의 task가 들어왔을 때 task는 airflow worker의 slot을 차지하게 됨 * 후에, 작업 대상에 작업을 제출하고 작업이 시작된다 * 작업대상: python function 또는 postgres, HDFS, Spark와 같은 외부 솔루션 * 작업이 진행되면 airflow는 작업이 완료 되었는지 작업상태를 polling 하면서 지속적으로 체크 * 작업 처리가 진행되는 동안 task는 차지했던 worker의 slot을 게속해서 차지한다. * task가 많아지면 airflow의 slot이 부족할 수도 있는 상황이 있음 * 그럼 작업 처리동안 task는 slot 점유할 필요는 없지 않나라는 생각이 들 수 있다. * triggerer가 이 문제를 해결 * 작업을 제출하고 Task는 작업 처리가 시작될 때 Slot을 비우고 작업 상태 Polling 작업은 Triggerer 에게 위임 * 작업 처리 시작 전까지는 slot을 점유 * 작업상태를 끊임없이 polling하면서 확인해야하는데 triggerer를 이용하면 이 작업이 없어짐 * triggerer는 작업 처리 완료가 되는 event (작업 완료 callback message를 수신)를 받아 scheduluer container에게 message를 전달하고 scheduler는 task가 다시 비워진 slot을 점유하게 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#triggerer란",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#triggerer란",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "워커를 대신하여 작업 상태 완료를 수신하고, 그때까지 Slot을 비워둘 수 있도록 해주는 Airflow의 서비스\n\nairflow service: 스케줄러, 워커 같은 요소 중 하나\n\nPython의 비동기 작업 라이브러리인 asyncio를 이용하여 작업상태 수신\n\n사용 조건: Airflow 2.2 부터 & Python 3.7부터 사용 가능\n\n어떻게 사용하나?\n\nDeferrable Operator 이용하여 Task 생성\n기본 Operator 중에서는 아래의 Sensor 종류만 사용 가능\n\nTimeSensorAsync\nDateTimeSensorAsync\nTimeDeltaSensorAsync\n\n끝에 Async 가 붙은 오퍼레이터를 Deferrable Operator라 부르며 Triggerer에게 작업 완료 수신을 맡기는 오퍼레이터라는 의미"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#triggerer-실습",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#triggerer-실습",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "비교 실험\n\ndags/dags_time_sensor.py (함수를 그냥 짬)\ndags/dags_time_sensor_with_async.py (asyncio library 이용)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dag-full-example-1",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dag-full-example-1",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "case1: tasks는 실패, dargrun 정상\n\n# Package Import\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.models import Variable\n\n# email 수신자 리스트\nemail_str = Variable.get(\"email_target\") \nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_timeout_example_1',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule=None,\n    dagrun_timeout=timedelta(minutes=1),\n    default_args={\n        #각 task들이 20초안에 끝나야 성공 처리됨\n        'execution_timeout': timedelta(seconds=20), \n        'email_on_failure': True,\n        'email': email_lst\n    }\n) as dag:\n    # execution_timeout보다 길기 때문에 task는 실패 처리됨\n    bash_sleep_30 = BashOperator(\n        task_id='bash_sleep_30', \n        bash_command='sleep 30',\n    )\n    # execution_timeout보다 짧기 때문에 task는 성공 처리됨\n    bash_sleep_10 = BashOperator(\n        trigger_rule='all_done', # upstream fail에도 task 실행시키기 위해 triggering\n        task_id='bash_sleep_10',\n        bash_command='sleep 10',\n    )\n    \n    # upstream failure 발생해도 trigger_rule을 all_done을 줬기 때문에 bash_sleep_10 은 실행됨\n    # dagrun_timeout을 1분으로 설정했기 때문에 task run의 총합이 40초이기 때문에 dagrun은 정상 처리됨\n    bash_sleep_30 &gt;&gt; bash_sleep_10 \n    \n\ncase2: tasks는 정상, dargrun 실패\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_timeout_example_2',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule=None,\n    dagrun_timeout=timedelta(minutes=1),\n    default_args={\n        'execution_timeout': timedelta(seconds=40),\n        'email_on_failure': True,\n        'email': email_lst\n    }\n) as dag:\n    bash_sleep_35 = BashOperator(\n        task_id='bash_sleep_35',\n        bash_command='sleep 35',\n    )\n\n    bash_sleep_36 = BashOperator(\n        trigger_rule='all_done',\n        task_id='bash_sleep_36',\n        bash_command='sleep 36',\n    )\n\n    bash_go = BashOperator(\n        task_id='bash_go',\n        bash_command='exit 0',\n    )\n# 모든 task들이 40초안에 실행완료가 되기때문에 성공 처리됨\n# dagrun은 1분으로 설정됐기 때문에 2번째 task 가 실행될 때 dag이 fail되고\n# 2번째task는 skipped 처리가 됨, 이 task에 대해서는 email도 안감 \n# 3번째 task는  no status 처리됨 , 이 task에 대해서는 email도 안감\n    bash_sleep_35 &gt;&gt; bash_sleep_36 &gt;&gt; bash_go\n    \n\ndagrun_timeout의 한계점\n\n모든 task들이 40초안에 실행완료가 되기때문에 성공 처리됨\ndagrun은 1분으로 설정됐기 때문에 2번째 task 가 실행될 때 dag이 fail되고\n2번째task는 skipped 처리가 됨, 이 task에 대해서는 email도 안감\n3번째 task는 no status 처리됨 , 이 task에 대해서는 email도 안감"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#요약",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#요약",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "끝에 Async가 붙은 오퍼레이터는 Deferrable Operator 라 부르며 Triggerer에 의해 Polling이 수행되는 오퍼레이터임을 의미\nDeferrable Operator는 작업 제출 후 Slot을 차지하지 않으며 Polling 내역에 대해 Trigger 제출 후 deferred 상태가 됨.\nTriggerer는 제출된 Trigger 내역을 보고 작업 완료시(조건 만족시) Worker에게 알려줘 작업이 마무리될 수 있도록 함."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dag-full-example-2",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dag-full-example-2",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "dag_time_sensor.py\n\nimport pendulum\nfrom airflow import DAG\nfrom airflow.sensors.date_time import DateTimeSensor\n\nwith DAG(\n    dag_id=\"dags_time_sensor\",\n    # 1시간 차이\n    start_date=pendulum.datetime(2023, 5, 1, 0, 0, 0), #5월1일 0시\n    end_date=pendulum.datetime(2023, 5, 1, 1, 0, 0), #5월1일 1시\n    schedule=\"*/10 * * * *\", #10 분마다 1시간안에 7번 돌게함\n    # 00분, 10분, 20분, 30분, 40분, 50분, 60분 총 7번\n    catchup=True, # catchup을 true이기 때문에 작업 상태bar 7개가 동시에 뜸\n    # airflow ui의 작업 pool을 보면 시작할 때 7개를 모두 차지 하도록 나옴\n) as dag:\n    # DateTimeSensor는 목표로 하는 시간까지 기다리는 sensor\n    sync_sensor = DateTimeSensor(\n        task_id=\"sync_sensor\",\n        # 현재 시간 + 5분\n        target_time=\"\"\"{{ macros.datetime.utcnow() + macros.timedelta(minutes=5) }}\"\"\",\n    )\n\ndags_time_sensor_with_async.py\n\nimport pendulum\nfrom airflow import DAG\nfrom airflow.sensors.date_time import DateTimeSensorAsync\n\nwith DAG(\n    dag_id=\"dags_time_sensor_with_async\",\n    start_date=pendulum.datetime(2023, 5, 1, 0, 0, 0),\n    end_date=pendulum.datetime(2023, 5, 1, 1, 0, 0),\n    schedule=\"*/10 * * * *\",\n    catchup=True,\n) as dag:\n    sync_sensor = DateTimeSensorAsync(\n        task_id=\"sync_sensor\",\n        target_time=\"\"\"{{ macros.datetime.utcnow() + macros.timedelta(minutes=5) }}\"\"\",\n    )\n\nairflow ui&gt;&gt; browse &gt;&gt; triggers 에서 trigger가 작업하는 대상 목록을 보여줌\ntrigger는 triggerer에게 작업을 맡길 event 또는 ticket이라고 생각하면 됨\ntriggerer id는 ticket id\ndefered status 는 보라색을 띄고 이 상태에서는 worker slot을 차지 않는 상태이다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dataset-workflow",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dataset-workflow",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "Airflow Web Service의 dataset 메뉴를 클릭하면 publish와 subscribe의 의존관계를 graph로 볼 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dag-dependencies",
    "href": "docs/blog/posts/Engineering/airflow/11.airflow_functions.html#dag-dependencies",
    "title": "Airflow Additional Function",
    "section": "",
    "text": "Airflow Web Service의 Browse 메뉴의 Dag Dependencies를 클릭하면 Dag간의 의존관계를 볼 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/Python/pathlib.html",
    "href": "docs/blog/posts/Engineering/Python/pathlib.html",
    "title": "Pathlib Library",
    "section": "",
    "text": "pathlib은 산재된 데이터를 체계적으로 관리하고, 데이터 분석이나 엔지니어링 작업을 수행할 때 매우 유용한 도구이다. 파일 시스템에서의 데이터 접근, 조작, 관리를 간결하고 효율적으로 할 수 있게 해주기 때문이다.\n\n\n\n경로 조작: 경로를 쉽게 조합하고, 분해하며, 변경할 수 있다.\n파일 시스템 정보 조회: 파일의 존재 유무 확인, 파일 크기 조회, 수정 날짜 조회 등의 정보를 쉽게 얻을 수 있다.\n파일 시스템 작업: 파일 또는 디렉토리 생성, 읽기, 쓰기, 이름 변경, 삭제 등의 작업을 할 수 다.\n경로 탐색: 특정 패턴이나 조건에 맞는 파일을 경로 내에서 찾을 수 있다.\n\n\n\n\n\n데이터 파일 구조화 및 접근\n\n동적 경로 생성: 다양한 데이터 소스나 폴더 구조에 대한 동적 경로를 쉽게 생성할 수 있다. 예를 들어, 날짜별로 구분된 로그 파일을 처리할 때 Path 객체를 사용하여 해당 날짜의 경로를 쉽게 생성할 수 있다.\n파일 탐색: 특정 패턴이나 조건에 맞는 파일들을 glob 또는 rglob 메소드를 사용하여 쉽게 찾을 수 있다. 이는 분석 대상 파일을 자동으로 식별하는 데 유용하다.\n\n데이터 파일 읽기 및 쓰기 작업\n\n파일 읽기/쓰기: pathlib을 사용하면 파일을 열고 읽거나 쓰는 작업을 직관적으로 수행할 수 있다. Path 객체의 read_text, write_text, read_bytes, write_bytes 메소드를 활용하여 파일 내용을 쉽게 처리할 수 있다.\n\n파일 및 디렉토리 관리\n\n파일 생성 및 삭제: touch 메소드로 새 파일을 생성하거나, unlink 메소드로 파일을 삭제할 수 있다.\n디렉토리 생성 및 삭제: 생성에는 mkdir 메소드를, 삭제에는 rmdir 메소드를 사용할 수 있다.\n경로 유효성 검사: 파일이나 디렉토리의 존재 여부를 확인하고, 경로가 파일인지 디렉토리인지 등의 속성을 검사할 수 있다.\n\n플랫폼 독립적 경로 처리\n\n운영 체제 호환성: pathlib은 윈도우, 맥, 리눅스 등 다양한 운영 체제에서 동일하게 작동하므로, 코드의 이식성이 향상된다.\n\n\n\n\n\n\n동적 경로 생성 예시\n\n날짜별 로그 파일이 저장된 디렉토리 구조를 가정하고, 특정 날짜에 해당하는 로그 파일의 경로를 동적으로 생성하는 방법.\n\n\nCode\nfrom pathlib import Path\n\n# 현재 날짜를 기준으로 경로 생성\ncurrent_date = datetime.now().strftime('%Y-%m-%d')\nlog_dir = Path(f\"./logs/{current_date}\")\n\n# 해당 날짜의 로그 디렉토리가 없다면 생성\nlog_dir.mkdir(parents=True, exist_ok=True)\n\n# 예시 로그 파일 경로\nlog_file_path = log_dir / \"error.log\"\n\nprint(f\"오늘 날짜의 로그 파일 경로: {log_file_path}\")\n\n\n\n파일 탐색 예시\n\nglob 메소드를 사용하여 특정 패턴(예: 모든 .txt 파일)에 맞는 파일들을 탐색하는 방법\nrglob는 현재 디렉토리뿐만 아니라 모든 하위 디렉토리에서도 탐색을 수행\n\n\n\n\nCode\n# 현재 디렉토리의 모든 .txt 파일 탐색\nfor txt_file in Path(\".\").glob(\"*.txt\"):\n    print(f\"Found text file: {txt_file}\")\n\n# 현재 디렉토리 및 모든 하위 디렉토리의 .txt 파일 탐색\nfor txt_file in Path(\".\").rglob(\"*.txt\"):\n    print(f\"Found text file in current and subdirectories: {txt_file}\")\n\n# .csv 파일 탐색\nfor file_path in data_dir.glob(\"*.csv\"):\n    print(f\"CSV 파일 처리: {file_path}\")\n\n# 파일 읽기 및 처리\nfor file_path in data_dir.iterdir():\n    if file_path.is_file() and file_path.suffix == \".txt\":\n        content = file_path.read_text()\n        # 파일 내용 처리\n        print(f\"{file_path.name} 파일의 내용 처리\")\n\n\n\n파일 읽기/쓰기\n\n\n\nCode\n# 텍스트 파일 경로 생성\ntext_file_path = Path(\"./example.txt\")\n\n# 텍스트 파일 쓰기\ntext_file_path.write_text(\"Hello, pathlib! This is a text file.\")\n\n# 텍스트 파일 읽기\ntext_content = text_file_path.read_text()\nprint(\"Text file content:\", text_content)\n\n# 바이너리 파일 경로 생성\nbinary_file_path = Path(\"example.bin\")\n\n# 바이너리 파일 쓰기 (예제 데이터로 'Hello, pathlib!' 문자열의 바이트를 사용)\nbinary_file_path.write_bytes(b\"Hello, pathlib! This is a binary file.\")\n\n# 바이너리 파일 읽기\nbinary_content = binary_file_path.read_bytes()\nprint(\"Binary file content:\", binary_content)\n\n\n\n파일 생성 및 삭제\n\n\n\nCode\n# 새 파일 생성\nnew_file = Path(\"./new_file.txt\")\nnew_file.touch()  # 파일이 존재하지 않으면 생성\n\n# 파일 존재 여부 확인\nif new_file.exists():\n    print(f\"{new_file} exists.\")\n\n# 파일 삭제\nnew_file.unlink()\n\n# 파일 존재 여부 재확인\nif not new_file.exists():\n    print(f\"{new_file} has been deleted.\")\n\n\n\n디렉토리 생성 및 삭제\n\n\n\nCode\n# 새 디렉토리 생성\nnew_dir = Path(\"./new_directory\")\nnew_dir.mkdir(parent=True, exist_ok=True)  # 디렉토리가 존재하지 않으면 생성, 이미 있으면 오류 발생하지 않음\n\n# 디렉토리 존재 여부 확인\nif new_dir.exists():\n    print(f\"{new_dir} exists.\")\n\n# 디렉토리 삭제\n# 주의: rmdir()는 디렉토리가 비어 있을 때만 사용 가능\nnew_dir.rmdir()\n\n# 디렉토리 존재 여부 재확인\nif not new_dir.exists():\n    print(f\"{new_dir} has been deleted.\")\n\n# 현재 디렉토리에 데이터 파일 폴더 생성\ndata_dir = Path.cwd() / \"data_files\"\ndata_dir.mkdir(parents=True, exist_ok=True)\n\n\n\nparents=True는 상위 디렉토리가 없는 경우에도 필요한 모든 상위 디렉토리를 함께 생성하라는 의미이다. 예를 들어, /a/b/c/d와 같은 디렉토리를 만들고 싶지만, /a, /a/b, /a/b/c 디렉토리가 아직 없을 때:\n\nparents=False (기본값)로 설정하고 /a/b/c/d 디렉토리를 생성하려고 하면, 상위 디렉토리가 없기 때문에 FileNotFoundError 오류가 발생.\nparents=True로 설정하면, Python은 먼저 /a, 그 다음 /a/b, /a/b/c를 차례로 생성한 후, 마지막으로 /a/b/c/d를 생성한다. 즉, 지정된 경로에 필요한 모든 상위 디렉토리를 자동으로 생성해준다.\n\n\n\n\nCode\n# 상위 디렉토리가 없는 깊은 경로 설정\ndeep_directory_path = Path(\"./a/b/c/d\")\n# 모든 필요한 상위 디렉토리와 함께 디렉토리 생성\ndeep_directory_path.mkdir(parents=True, exist_ok=True)\n\n\n\nexist_ok=True는 지정된 경로에 디렉토리(폴더)를 생성하는 옵션이다. 이 옵션을 True로 설정하면, 만약 생성하려는 디렉토리가 이미 존재하는 경우에도 오류를 발생시키지 않고, 해당 명령을 무시한다. 즉, 해당 디렉토리의 존재 여부와 상관없이 프로그램이 계속 실행될 수 있도록 한다. exist_ok 매개변수의 기본값은 False 이다. 따라서, exist_ok를 명시적으로 지정하지 않고 디렉토리를 생성하려 할 때 해당 디렉토리가 이미 존재한다면, FileExistsError가 발생 및 프로그램이 중단된다.\n이 옵션들을 사용하면, 스크립트의 안정성을 높이고, 디렉토리가 이미 존재하는 상황에서도 원활하게 코드를 실행할 수 있다.\n경로 유효성 검사\n\n\n\nCode\n# 임의의 파일 및 디렉토리 경로\nfile_path = Path(\"example.txt\")\ndir_path = Path(\"example_dir\")\n\n# 파일 및 디렉토리 존재 여부 검사\nif file_path.exists():\n    print(f\"{file_path} exists.\")\nelse:\n    print(f\"{file_path} does not exist.\")\n\nif dir_path.exists():\n    print(f\"{dir_path} exists.\")\nelse:\n    print(f\"{dir_path} does not exist.\")\n\n# 경로가 파일인지 디렉토리인지 검사\nif file_path.is_file():\n    print(f\"{file_path} is a file.\")\n\nif dir_path.is_dir():\n    print(f\"{dir_path} is a directory.\")\n\n\n\n\n\n\n\nCode\nroot_path = path.cwd() # /home/kmkim/pda/dsp-research-strep-a/kkm\nprefix = 'data'\ndirectory_names = ['cfx-baseline-subtraction','pda-raw-sample']\nproduct_names = ['GI-B-I', 'GI-B-II', 'GI-P', 'GI-V', 'RP1', 'RP2', 'RP3', 'RP4', 'STI-CA', 'STI-EA', 'STI-GU']\nconsumables = ['8-strip','96-cap']\nplate_numbers = ['002','005','031','032','035','036','041']\n\ncfx_data = []\nraw_data = []\n\nfor directory_name in directory_names: \n    for product_name in ['GI-B-I']: #product_names:\n        for consumable in ['8-strip']: #consumables:\n            for plate_number in plate_numbers:\n                full_path = root_path / prefix / directory_name / product_name / consumable / plate_number\n                processed_path = full_path / \"processed\" / \"example1\"\n                processed_path.mkdir(parents=True, exist_ok=True)\n                exporting_path =  full_path / \"exported_pcrd\"\n                if 'cfx' in exporting_path: \n                    temp_cfx_data = bioradparse.load_pcrdata(exporting_path, datatype=\"cfx-xl\")\n                    cfx_data.append(temp_cfx_data)\n                temp_raw_data = bioradparse.load_pcrdata(exporting_path, datatype=\"cfx-batch-csv\")\n                raw_data.append(temp_raw_data)\n#pathlib.Path(f\"./data/baseline-subtracted/processed/example1\")\n\nfor pcrname, pcrdata in raw_data.items():\n    bioradparse.save_pcrdata(raw_data, root_path / \"pda-raw-sample\" / \"processed\" / \"example1\" / f\"{pcrname}.parquet\")\nfor pcrname, pcrdata in cfx_data.items():\n    bioradparse.save_pcrdata(cfx_data, root_path / \"cfx-baseline-subtraction\" / \"processed\" / \"example1\" / f\"{pcrname}.parquet\")"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Python/pathlib.html#pathlib",
    "href": "docs/blog/posts/Engineering/Python/pathlib.html#pathlib",
    "title": "Pathlib Library",
    "section": "",
    "text": "pathlib은 산재된 데이터를 체계적으로 관리하고, 데이터 분석이나 엔지니어링 작업을 수행할 때 매우 유용한 도구이다. 파일 시스템에서의 데이터 접근, 조작, 관리를 간결하고 효율적으로 할 수 있게 해주기 때문이다.\n\n\n\n경로 조작: 경로를 쉽게 조합하고, 분해하며, 변경할 수 있다.\n파일 시스템 정보 조회: 파일의 존재 유무 확인, 파일 크기 조회, 수정 날짜 조회 등의 정보를 쉽게 얻을 수 있다.\n파일 시스템 작업: 파일 또는 디렉토리 생성, 읽기, 쓰기, 이름 변경, 삭제 등의 작업을 할 수 다.\n경로 탐색: 특정 패턴이나 조건에 맞는 파일을 경로 내에서 찾을 수 있다.\n\n\n\n\n\n데이터 파일 구조화 및 접근\n\n동적 경로 생성: 다양한 데이터 소스나 폴더 구조에 대한 동적 경로를 쉽게 생성할 수 있다. 예를 들어, 날짜별로 구분된 로그 파일을 처리할 때 Path 객체를 사용하여 해당 날짜의 경로를 쉽게 생성할 수 있다.\n파일 탐색: 특정 패턴이나 조건에 맞는 파일들을 glob 또는 rglob 메소드를 사용하여 쉽게 찾을 수 있다. 이는 분석 대상 파일을 자동으로 식별하는 데 유용하다.\n\n데이터 파일 읽기 및 쓰기 작업\n\n파일 읽기/쓰기: pathlib을 사용하면 파일을 열고 읽거나 쓰는 작업을 직관적으로 수행할 수 있다. Path 객체의 read_text, write_text, read_bytes, write_bytes 메소드를 활용하여 파일 내용을 쉽게 처리할 수 있다.\n\n파일 및 디렉토리 관리\n\n파일 생성 및 삭제: touch 메소드로 새 파일을 생성하거나, unlink 메소드로 파일을 삭제할 수 있다.\n디렉토리 생성 및 삭제: 생성에는 mkdir 메소드를, 삭제에는 rmdir 메소드를 사용할 수 있다.\n경로 유효성 검사: 파일이나 디렉토리의 존재 여부를 확인하고, 경로가 파일인지 디렉토리인지 등의 속성을 검사할 수 있다.\n\n플랫폼 독립적 경로 처리\n\n운영 체제 호환성: pathlib은 윈도우, 맥, 리눅스 등 다양한 운영 체제에서 동일하게 작동하므로, 코드의 이식성이 향상된다.\n\n\n\n\n\n\n동적 경로 생성 예시\n\n날짜별 로그 파일이 저장된 디렉토리 구조를 가정하고, 특정 날짜에 해당하는 로그 파일의 경로를 동적으로 생성하는 방법.\n\n\nCode\nfrom pathlib import Path\n\n# 현재 날짜를 기준으로 경로 생성\ncurrent_date = datetime.now().strftime('%Y-%m-%d')\nlog_dir = Path(f\"./logs/{current_date}\")\n\n# 해당 날짜의 로그 디렉토리가 없다면 생성\nlog_dir.mkdir(parents=True, exist_ok=True)\n\n# 예시 로그 파일 경로\nlog_file_path = log_dir / \"error.log\"\n\nprint(f\"오늘 날짜의 로그 파일 경로: {log_file_path}\")\n\n\n\n파일 탐색 예시\n\nglob 메소드를 사용하여 특정 패턴(예: 모든 .txt 파일)에 맞는 파일들을 탐색하는 방법\nrglob는 현재 디렉토리뿐만 아니라 모든 하위 디렉토리에서도 탐색을 수행\n\n\n\n\nCode\n# 현재 디렉토리의 모든 .txt 파일 탐색\nfor txt_file in Path(\".\").glob(\"*.txt\"):\n    print(f\"Found text file: {txt_file}\")\n\n# 현재 디렉토리 및 모든 하위 디렉토리의 .txt 파일 탐색\nfor txt_file in Path(\".\").rglob(\"*.txt\"):\n    print(f\"Found text file in current and subdirectories: {txt_file}\")\n\n# .csv 파일 탐색\nfor file_path in data_dir.glob(\"*.csv\"):\n    print(f\"CSV 파일 처리: {file_path}\")\n\n# 파일 읽기 및 처리\nfor file_path in data_dir.iterdir():\n    if file_path.is_file() and file_path.suffix == \".txt\":\n        content = file_path.read_text()\n        # 파일 내용 처리\n        print(f\"{file_path.name} 파일의 내용 처리\")\n\n\n\n파일 읽기/쓰기\n\n\n\nCode\n# 텍스트 파일 경로 생성\ntext_file_path = Path(\"./example.txt\")\n\n# 텍스트 파일 쓰기\ntext_file_path.write_text(\"Hello, pathlib! This is a text file.\")\n\n# 텍스트 파일 읽기\ntext_content = text_file_path.read_text()\nprint(\"Text file content:\", text_content)\n\n# 바이너리 파일 경로 생성\nbinary_file_path = Path(\"example.bin\")\n\n# 바이너리 파일 쓰기 (예제 데이터로 'Hello, pathlib!' 문자열의 바이트를 사용)\nbinary_file_path.write_bytes(b\"Hello, pathlib! This is a binary file.\")\n\n# 바이너리 파일 읽기\nbinary_content = binary_file_path.read_bytes()\nprint(\"Binary file content:\", binary_content)\n\n\n\n파일 생성 및 삭제\n\n\n\nCode\n# 새 파일 생성\nnew_file = Path(\"./new_file.txt\")\nnew_file.touch()  # 파일이 존재하지 않으면 생성\n\n# 파일 존재 여부 확인\nif new_file.exists():\n    print(f\"{new_file} exists.\")\n\n# 파일 삭제\nnew_file.unlink()\n\n# 파일 존재 여부 재확인\nif not new_file.exists():\n    print(f\"{new_file} has been deleted.\")\n\n\n\n디렉토리 생성 및 삭제\n\n\n\nCode\n# 새 디렉토리 생성\nnew_dir = Path(\"./new_directory\")\nnew_dir.mkdir(parent=True, exist_ok=True)  # 디렉토리가 존재하지 않으면 생성, 이미 있으면 오류 발생하지 않음\n\n# 디렉토리 존재 여부 확인\nif new_dir.exists():\n    print(f\"{new_dir} exists.\")\n\n# 디렉토리 삭제\n# 주의: rmdir()는 디렉토리가 비어 있을 때만 사용 가능\nnew_dir.rmdir()\n\n# 디렉토리 존재 여부 재확인\nif not new_dir.exists():\n    print(f\"{new_dir} has been deleted.\")\n\n# 현재 디렉토리에 데이터 파일 폴더 생성\ndata_dir = Path.cwd() / \"data_files\"\ndata_dir.mkdir(parents=True, exist_ok=True)\n\n\n\nparents=True는 상위 디렉토리가 없는 경우에도 필요한 모든 상위 디렉토리를 함께 생성하라는 의미이다. 예를 들어, /a/b/c/d와 같은 디렉토리를 만들고 싶지만, /a, /a/b, /a/b/c 디렉토리가 아직 없을 때:\n\nparents=False (기본값)로 설정하고 /a/b/c/d 디렉토리를 생성하려고 하면, 상위 디렉토리가 없기 때문에 FileNotFoundError 오류가 발생.\nparents=True로 설정하면, Python은 먼저 /a, 그 다음 /a/b, /a/b/c를 차례로 생성한 후, 마지막으로 /a/b/c/d를 생성한다. 즉, 지정된 경로에 필요한 모든 상위 디렉토리를 자동으로 생성해준다.\n\n\n\n\nCode\n# 상위 디렉토리가 없는 깊은 경로 설정\ndeep_directory_path = Path(\"./a/b/c/d\")\n# 모든 필요한 상위 디렉토리와 함께 디렉토리 생성\ndeep_directory_path.mkdir(parents=True, exist_ok=True)\n\n\n\nexist_ok=True는 지정된 경로에 디렉토리(폴더)를 생성하는 옵션이다. 이 옵션을 True로 설정하면, 만약 생성하려는 디렉토리가 이미 존재하는 경우에도 오류를 발생시키지 않고, 해당 명령을 무시한다. 즉, 해당 디렉토리의 존재 여부와 상관없이 프로그램이 계속 실행될 수 있도록 한다. exist_ok 매개변수의 기본값은 False 이다. 따라서, exist_ok를 명시적으로 지정하지 않고 디렉토리를 생성하려 할 때 해당 디렉토리가 이미 존재한다면, FileExistsError가 발생 및 프로그램이 중단된다.\n이 옵션들을 사용하면, 스크립트의 안정성을 높이고, 디렉토리가 이미 존재하는 상황에서도 원활하게 코드를 실행할 수 있다.\n경로 유효성 검사\n\n\n\nCode\n# 임의의 파일 및 디렉토리 경로\nfile_path = Path(\"example.txt\")\ndir_path = Path(\"example_dir\")\n\n# 파일 및 디렉토리 존재 여부 검사\nif file_path.exists():\n    print(f\"{file_path} exists.\")\nelse:\n    print(f\"{file_path} does not exist.\")\n\nif dir_path.exists():\n    print(f\"{dir_path} exists.\")\nelse:\n    print(f\"{dir_path} does not exist.\")\n\n# 경로가 파일인지 디렉토리인지 검사\nif file_path.is_file():\n    print(f\"{file_path} is a file.\")\n\nif dir_path.is_dir():\n    print(f\"{dir_path} is a directory.\")\n\n\n\n\n\n\n\nCode\nroot_path = path.cwd() # /home/kmkim/pda/dsp-research-strep-a/kkm\nprefix = 'data'\ndirectory_names = ['cfx-baseline-subtraction','pda-raw-sample']\nproduct_names = ['GI-B-I', 'GI-B-II', 'GI-P', 'GI-V', 'RP1', 'RP2', 'RP3', 'RP4', 'STI-CA', 'STI-EA', 'STI-GU']\nconsumables = ['8-strip','96-cap']\nplate_numbers = ['002','005','031','032','035','036','041']\n\ncfx_data = []\nraw_data = []\n\nfor directory_name in directory_names: \n    for product_name in ['GI-B-I']: #product_names:\n        for consumable in ['8-strip']: #consumables:\n            for plate_number in plate_numbers:\n                full_path = root_path / prefix / directory_name / product_name / consumable / plate_number\n                processed_path = full_path / \"processed\" / \"example1\"\n                processed_path.mkdir(parents=True, exist_ok=True)\n                exporting_path =  full_path / \"exported_pcrd\"\n                if 'cfx' in exporting_path: \n                    temp_cfx_data = bioradparse.load_pcrdata(exporting_path, datatype=\"cfx-xl\")\n                    cfx_data.append(temp_cfx_data)\n                temp_raw_data = bioradparse.load_pcrdata(exporting_path, datatype=\"cfx-batch-csv\")\n                raw_data.append(temp_raw_data)\n#pathlib.Path(f\"./data/baseline-subtracted/processed/example1\")\n\nfor pcrname, pcrdata in raw_data.items():\n    bioradparse.save_pcrdata(raw_data, root_path / \"pda-raw-sample\" / \"processed\" / \"example1\" / f\"{pcrname}.parquet\")\nfor pcrname, pcrdata in cfx_data.items():\n    bioradparse.save_pcrdata(cfx_data, root_path / \"cfx-baseline-subtraction\" / \"processed\" / \"example1\" / f\"{pcrname}.parquet\")"
  },
  {
    "objectID": "docs/blog/posts/Engineering/image_process/03.operator_basic.html",
    "href": "docs/blog/posts/Engineering/image_process/03.operator_basic.html",
    "title": "OCR",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n1 DAG Basic\nfrom PIL import Image\nimport pytesseract\n\n# Load the image from file\nimage_path = '/mnt/data/캡처.PNG'\nimage = Image.open(image_path)\n\n# Perform OCR using tesseract\nextracted_text = pytesseract.image_to_string(image, lang='kor')\n\nextracted_text\n\n\n\n\n\n\n\n\n\n\n2 Go to Blog Content List\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html",
    "title": "인공신경망과 역전파",
    "section": "",
    "text": "마지막 과정으로 가장 간단한 인공신경망을 설계하고 이를 역전파 알고리즘을 통해 학습시키는 코드를 작성\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom mpl_toolkits import mplot3d\nfrom matplotlib.colors import ListedColormap\ncm2 = ListedColormap(['C6', 'C1'])\ncm2inv = ListedColormap(['C1', 'C6'])\n\n# 전역으로 지정한 numpy 출력 형식\nnp.set_printoptions(precision=4, linewidth=150)\n\n# matplotlib 스타일 지정\nmpl.style.use('bmh')\nmpl.style.use('seaborn-whitegrid')"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#보조함수-정의",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#보조함수-정의",
    "title": "인공신경망과 역전파",
    "section": "보조함수 정의",
    "text": "보조함수 정의\n\ndef logistic(x):\n    \"\"\"\n    logistic sigmoid function\n    \"\"\"\n    return 1 / (1+np.exp(-x))"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#데이터-로딩",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#데이터-로딩",
    "title": "인공신경망과 역전파",
    "section": "데이터 로딩",
    "text": "데이터 로딩\n\nneuralnet_make_samples.ipynb 파일에서 만든 데이터를 로딩\n\n\nD = np.load('C:/Users/kmkim/Desktop/projects/blog/docs/data/08-nndata.npz')\nsamples = D['samples']\ntarget = D['target']\n\n\nsamples.shape, target.shape\n\n((500, 2), (500,))\n\n\n\n데이터는 500개 2차원 점으로 구성되어 있고 각 점 마다 0, 1이 기록된 타겟이 함께 있음\n\n\n결정 경계와 타겟이 표시된 데이터\n\nfig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(1, 1, 1)\n\nax.xaxis.set_tick_params(labelsize=18)\nax.yaxis.set_tick_params(labelsize=18)\nax.set_xlabel('$x$', fontsize=25)\nax.set_ylabel('$y$', fontsize=25)\n\nax.plot(samples[target==1,0], samples[target==1,1], 'o', \n       markerfacecolor='C1', markeredgecolor='k', markersize=8)\nax.plot(samples[target==0,0], samples[target==0,1], '^', \n       markerfacecolor='C6', markeredgecolor='k', markersize=8)\n\nx1 = np.linspace(0, 15/7, 50)\nx2 = np.linspace(15/7, 3, 50)\n\n# 미리 정의된 pos, neg area\npos_area = np.array([0,3, 0,5, 5,5, 5,0, 3,0, 15/7,12/7]).reshape(6,2)\npos_polygon = mpl.patches.Polygon(pos_area, color='C1', alpha=0.3)\nax.add_patch(pos_polygon)\n\nneg_area = np.array([0,0, 0,3, 15/7,12/7, 3,0]).reshape(4,2)\nneg_polygon = mpl.patches.Polygon(neg_area, color='C6', alpha=0.3)\nax.add_patch(neg_polygon)\n\nax.set_ylim(0,5)\nax.set_xlim(0,5)\n    \nplt.show()"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#x-5y---15-0-로-분류",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#x-5y---15-0-로-분류",
    "title": "인공신경망과 역전파",
    "section": "$3x + 5y - 15 = 0 $ 로 분류",
    "text": "$3x + 5y - 15 = 0 $ 로 분류\n\n로지스틱 회귀에서 배운 선형 분류기 다른게 말하면 퍼셉트론으로 주언 데이터를 분류 하기 위해 매개변수 값을 3, 5, -15 로 설정\n\n\nx = np.linspace(0, 5, 50)\n\ndcs_bnd_1 = lambda x: -(3/5)*x + 3\ndcs_bnd_1_imp = lambda x, y: (3)*x + (5)*y + (-15) #implicity 음항수 형태\n\n\nfig = plt.figure(figsize=(7, 7))\nax = plt.axes()\n\nax.xaxis.set_tick_params(labelsize=18)\nax.yaxis.set_tick_params(labelsize=18)\nax.set_xlabel('$x$', fontsize=25)\nax.set_ylabel('$y$', fontsize=25)\n\nX1, X2 = np.meshgrid(x, x)\nY = dcs_bnd_1_imp(X1.ravel(), X2.ravel()).reshape(X1.shape)\n\nax.contourf(X1, X2, Y, [-100, 0, 100], cmap=cm2, alpha=0.3)\n\nax.plot(x, dcs_bnd_1(x), color='k', zorder=10)\n\ny1_bin = dcs_bnd_1_imp(samples[:,0], samples[:,1]) &gt; 0\n\nax.plot(samples[y1_bin, 0], \n        samples[y1_bin,1], 'o', \n        markerfacecolor='C1', markeredgecolor='k', markersize=8)\n\nnot_y1_bin = np.invert(y1_bin)\nnot_y1_and_target = np.logical_and(not_y1_bin,target)\n\nax.plot(samples[not_y1_and_target, 0], samples[not_y1_and_target, 1], 'o',\n        markerfacecolor='white', markeredgecolor='k', markersize=8, zorder=20)\n\nax.plot(samples[np.where(target==0)[0], 0], samples[np.where(target==0)[0], 1], '^', \n        markerfacecolor='C6', markeredgecolor='k', markersize=8, zorder=20)\n\nax.set_ylim(0,5)\nax.set_xlim(0,5)\n    \nplt.show()\n\n\n\n\n\n\n\n\n\n분류 결과를 보면 결정 경계 아래쪽에 있는 흰색 동그라미는 세모로 잘못 분류"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#x---3y-18-0-로-분류",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#x---3y-18-0-로-분류",
    "title": "인공신경망과 역전파",
    "section": "$ -6x - 3y + 18 = 0 $로 분류",
    "text": "$ -6x - 3y + 18 = 0 $로 분류\n\n가중치 값을 바꿔서 다른 선형 분류기로 분류\n\n\ndcs_bnd_2 = lambda x: -2*x + 6\ndcs_bnd_2_imp = lambda x, y: (-6)*x + (-3)*y + (18)\n\n\nfig = plt.figure(figsize=(7,7))\nax = fig.add_subplot(1,1,1)\n\nax.xaxis.set_tick_params(labelsize=18)\nax.yaxis.set_tick_params(labelsize=18)\nax.set_xlabel('$x$', fontsize=25)\nax.set_ylabel('$y$', fontsize=25)\n\nX1, X2 = np.meshgrid(x, x)\nY = dcs_bnd_2_imp(X1.ravel(), X2.ravel()).reshape(X1.shape)\n\nax.contourf(X1, X2, Y, [-100, 0, 100], cmap=cm2, alpha=0.3)\n\nax.plot(x, dcs_bnd_2(x), color='k')\n\ny2_bin = dcs_bnd_2_imp(samples[:,0], samples[:,1]) &lt; 0\n\nax.plot(samples[y2_bin, 0], samples[y2_bin, 1], 'o', \n        markerfacecolor='C6', markeredgecolor='k', markersize=8)\n\nnot_y2_bin = np.invert(y2_bin)\nnot_y2_and_target = np.logical_and(not_y2_bin,target)\n\nax.plot(samples[not_y2_and_target, 0], samples[not_y2_and_target, 1], 'o',\n        markerfacecolor='white', markeredgecolor='gray', markersize=8)\n\nax.plot(samples[np.where(target==0)[0], 0], samples[np.where(target==0)[0], 1], '^', \n        markerfacecolor='C1', markeredgecolor='k', markersize=8)\n\nax.set_ylim(0,5)\nax.set_xlim(0,5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n이번에도 두 샘플을 정확하게 분류할 수 없음\n결정경계 왼쪽 동그라미를 세모로 잘못 분류 하고 있음"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#위-두-경우를-합성하여-분류하는-경우",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#위-두-경우를-합성하여-분류하는-경우",
    "title": "인공신경망과 역전파",
    "section": "위 두 경우를 합성하여 분류하는 경우",
    "text": "위 두 경우를 합성하여 분류하는 경우\n\n앞서 만든 두 선형 분류기를 함께 혼합하여 또 다른 선형 분류기의 입력으로 사용\n\n\nX1, X2 = np.meshgrid(x, x)\n\n# 두 선형 분류기 합치기[+]\nw1, w2, bias = 10, -9, 4\na = lambda x, y: logistic( \n               w1*logistic(dcs_bnd_1_imp(x, y)) \n               + w2*logistic(dcs_bnd_2_imp(x, y)) \n               + bias\n)\n\nfig = plt.figure(figsize=(7,7))\nax = fig.add_subplot()\n\nax.xaxis.set_tick_params(labelsize=18)\nax.yaxis.set_tick_params(labelsize=18)\nax.set_xlabel('$x$', fontsize=25)\nax.set_ylabel('$y$', fontsize=25)\n\npred = a(samples[:,0], samples[:,1])\npred_pos = pred &gt;= 0.5\npred_neg = pred &lt; 0.5\n\nax.contour(X1, X2, a(X1,X2), cmap='gray', levels=[0.5])\nax.contourf(X1, X2, a(X1,X2), cmap=cm2, levels=[-10, 0.5, 10], alpha=0.3)\n\n# for positive samples\nTP = np.logical_and(target==1, pred_pos)\nFN = np.logical_and(target==1, pred_neg)\n\n# for negative samples\nTN = np.logical_and(target==0, pred_neg)\nFP = np.logical_and(target==0, pred_pos)\n\nax.plot(samples[TP,0], samples[TP,1], 'o', \n        markerfacecolor='C1', markeredgecolor='k', markersize=8)\nax.plot(samples[TN,0], samples[TN,1], '^', \n        markerfacecolor='C6', markeredgecolor='k', markersize=8)\n\nax.plot(samples[FN,0], samples[FN,1], 'o', \n        markerfacecolor='C6', markeredgecolor='k', markeredgewidth=2, markersize=15)\nax.plot(samples[FP,0], samples[FP,1], '^', \n         markerfacecolor='C1', markeredgecolor='k', markeredgewidth=2, markersize=15)\n\nax.set_ylim(0,5)\nax.set_xlim(0,5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n샘플 두 개만 틀리고 모두 바르게 분류\n결정경계는 비선형으로 곡선형태를 띄고 있음\n함수의 합성과정에서 비선형 함수인 로지스틱 시그모이드 함수가 끼어있어 전체 함수가 비선형성을 띄게됨"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#차원-표현",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#차원-표현",
    "title": "인공신경망과 역전파",
    "section": "3차원 표현",
    "text": "3차원 표현\n\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(projection='3d')\n\nplt.rc('xtick',labelsize=15)\nplt.rc('ytick',labelsize=15)\n\nx1 = np.linspace(0, 5, 101)\nx2 = np.linspace(0, 5, 101)\n\nX1, X2 = np.meshgrid(x1, x2)\n\nax.contour(X1, X2, a(X1, X2), linewidths=5, cmap='gray', levels=[0.5])\nax.plot_surface(X1, X2, a(X1, X2), color='w', edgecolor='k', alpha=0.5)\n\nax.plot3D(samples[TP,0], samples[TP,1], 1.0, 'o', \n        markerfacecolor='C1', markeredgecolor='k', markersize=8, zorder=10)\n\nax.plot3D(samples[TN,0], samples[TN,1], 0.0, '^', \n        markerfacecolor='C6', markeredgecolor='k', markersize=8, zorder=10)\n\nax.plot3D(samples[FN,0], samples[FN,1], 0.0, 'o', \n        markerfacecolor='C6', markeredgecolor='k', markeredgewidth=2, markersize=15, zorder=10)\nax.plot3D(samples[FP,0], samples[FP,1], 1.0, '^', \n        markerfacecolor='C1', markeredgecolor='k', markeredgewidth=2, markersize=15, zorder=10)\n\nax.xaxis.set_tick_params(labelsize=15) \nax.yaxis.set_tick_params(labelsize=15)\nax.zaxis.set_tick_params(labelsize=15)\nax.set_xlabel(r'$x$', fontsize=20)\nax.set_ylabel(r'$y$', fontsize=20)\nax.set_zlabel(r'$a$', fontsize=20)\nax.view_init(30, -80)\n    \nplt.show()\n\n\n\n\n\n\n\n\n\n이렇게 다층퍼셉트론(다른 말로 인공신경망)은 선형 분류기인 로지스틱 회귀를 여러개 겹쳐 사용한것 임을 확인\n이제 이렇게 만들어진 인공신경망을 계산하는 함수를 만들어 전체를 네트워크 하나로 처리"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#network-함수",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#network-함수",
    "title": "인공신경망과 역전파",
    "section": "network 함수",
    "text": "network 함수\n\n아래 함수는 샘플 점 X아 로지스틱 회귀 모델 세개에 해당하는 매개변수 아홉 개를 입력으로 받아 이 네트워크의 출력을 계산하는 함수\n이하 매개변수는 가중치 또는 가중치와 바이어스라는 용어를 혼용\n\n\ndef network(X, W):\n    \"\"\"\n    X : (N, D)\n    W : (3, 3)\n        [b^(1)_1, w^(1)_11, w^(1)_12]\n        [b^(1)_2, w^(1)_21, w^(1)_22]\n        [b^(2)_1, w^(2)_11, w^(2)_12]\n    \n    ret : (N,)\n    \n    D, H, A = 2, 2, 1\n    \"\"\"\n\n    ######################################\n    # WRITE YOUR CODE HERE\n    # 1st layer\n    Z1 = np.dot(W[:2,1:], X.T)\n    Z1 = Z1 + W[:2,0].reshape(-1,1)\n    A1 = logistic(Z1)\n    \n    \n    # 2nd layer\n    Z = np.dot(W[-1,1:], A1)\n    Z = Z + W[-1,0]\n    A = logistic(Z)\n    ######################################\n    \n    return A\n\n\nnetwork 함수 검증\n\n위 작성한 함수가 제대로 함숫값을 계산한다면 아래 셀 실행 결과는 2가 되어야 함\n\n\n############################################################\n# 이 테스트 코드의 결과는 2가 나와야 함\n\nW = np.array([ [-15, 3, 5], [18, -6, -3], [4, 10, -9] ])\npred = network(samples, W)\npred.shape\npred[pred&gt;=0.5] = 1\npred[pred&lt;0.5] = 0\n\nresult = pred != target\n\nprint(f\"오분류 데이터 개수: {result.sum()}\")\n\n오분류 데이터 개수: 2\n\n\n\n네트워크를 함수로 작동하게 만들었으므로 최적화 과정을 적용해서 이 함수를 학습시킬 수 있음"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#목적함수-정의와-최적화",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#목적함수-정의와-최적화",
    "title": "인공신경망과 역전파",
    "section": "목적함수 정의와 최적화",
    "text": "목적함수 정의와 최적화\n\n변수 초기화\n\nW\n\narray([[-15,   3,   5],\n       [ 18,  -6,  -3],\n       [  4,  10,  -9]])\n\n\n\n# 가중치를 무작위로 초기화\nnp.random.seed(17)\n\nW = np.random.randn(9)\n# W.reshape(3,3)\nW\n\narray([ 0.2763, -1.8546,  0.6239,  1.1453,  1.0372,  1.8866, -0.1117, -0.3621,  0.1487])\n\n\n\n가중치가 임의로 초기화 되었으므로 오분류 개수는 2보다 커져야 함\n\n\npred = network(samples, W.reshape(3,3))\n\npred[pred&gt;=0.5] = 1\npred[pred&lt;0.5] = 0\n\nresult = pred != target\n\nprint(f\"오분류 데이터 개수: {result.sum()}\")\n\n오분류 데이터 개수: 163\n\n\n\n\n목적함수 정의\n\n분류 문제는 크로스엔트로피 목적함수를 자주 사용하지만 여기서는 역전파 과정을 간략하게 설명하기 위해 간단한 오차 제곱합 함수를 목적함수로 사용\n\n\ndef J(W, X, T):\n    \"\"\"\n    W: 함숫값을 결정하는 변수, 가중치 (9,)\n    X: 주어진 점 데이터 X, X: (N,D)\n    T: 데이터에 대한 클래스 T, 0 또는 1, T: (N,)\n    \"\"\"\n    N = X.shape[0]\n    W = W.reshape(3,3)\n    \n    # 네트워크를 포워드 시키고 적당한 로스 함수를 계산하여 되돌림\n    Y = network(X, W)\n\n    return (1/(2*N)) * ((T-Y)**2).sum()\n\n\n# 초기 상태에서 목적함숫값\n# 0.1263148192185165\nJ(W, samples, target)\n\n0.1263148192185165"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#학습-scipy-사용",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#학습-scipy-사용",
    "title": "인공신경망과 역전파",
    "section": "학습: scipy 사용",
    "text": "학습: scipy 사용\n\n네트워크를 학습시키는 과정은 함수를 최적화하는 과정과 다르지 않음\n그렇기 때문에 기존 라이브러리에서 제공하는 최적화 모듈을 사용해도 충분히 학습이 가능\n여기서는 사이파이에서 제공하는 켤레기울기법을 사용\n\n\nfrom scipy import optimize\n\n# https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cg.html\nW_star = optimize.fmin_cg(J, W, args=(samples, target),  gtol=1e-06)\n\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 534\n         Function evaluations: 19760\n         Gradient evaluations: 1976\n\n\n\nW_star = W_star.reshape(3,3)\nW_star\n\narray([[ 48.0777, -10.5942, -15.3917],\n       [-27.7851,   9.6463,   3.5314],\n       [ 21.6201, -49.2293,  47.523 ]])\n\n\n\n학습된 네트워크로 분류\n\nx = np.linspace(0, 5, 200)\nX1, X2 = np.meshgrid(x, x)\n\ndcs_bnd_1_imp_ = lambda x, y: W_star[0, 1]*x + W_star[0, 2]*y + W_star[0, 0] \ndcs_bnd_2_imp_ = lambda x, y: W_star[1, 1]*x + W_star[1, 2]*y + W_star[1, 0] \n\na, b, c = W_star[2, 1], W_star[2, 2], W_star[2, 0] \no = lambda x, y: logistic( a*logistic(dcs_bnd_1_imp_(x, y)) \n                         + b*logistic(dcs_bnd_2_imp_(x, y)) \n                         + c )\n\nfig = plt.figure(figsize=(7,7))\nax = plt.axes()\n\nax.xaxis.set_tick_params(labelsize=18)\nax.yaxis.set_tick_params(labelsize=18)\nax.set_xlabel('$x$', fontsize=25)\nax.set_ylabel('$y$', fontsize=25)\n\npred = o(samples[:,0], samples[:,1])\npred_pos = pred &gt;= 0.5\npred_neg = pred &lt; 0.5\n\nax.contour(X1, X2, o(X1,X2), cmap='gray', levels=[0.5])\nax.contourf(X1, X2, o(X1,X2), cmap=cm2, levels=[-10, 0.5, 10], alpha=0.3)\n\n# for positive samples\nTP = np.logical_and(target==1, pred_pos)\nFN = np.logical_and(target==1, pred_neg)\n\n# for negative samples\nTN = np.logical_and(target==0, pred_neg)\nFP = np.logical_and(target==0, pred_pos)\n\n# True Positive and True Negative\nax.plot(samples[TP,0], samples[TP,1], 'o', \n        markerfacecolor='C1', markeredgecolor='k', markersize=8)\nax.plot(samples[TN,0], samples[TN,1], '^', \n        markerfacecolor='C6', markeredgecolor='k', markersize=8)\n\n# False Positive and False Negative\nax.plot(samples[FN,0], samples[FN,1], 'o', \n        markerfacecolor='white', markeredgecolor='k', \n        markeredgewidth=2, markersize=15)\nax.plot(samples[FP,0], samples[FP,1], '^', \n        markerfacecolor='white', markeredgecolor='k', \n        markeredgewidth=2, markersize=15)\n\nax.set_ylim(0,5)\nax.set_xlim(0,5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n학습이 잘되어 모든 데이터를 오류없이 잘 분류함을 확인할 수 있음"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#학습-직접-수치미분해서",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#학습-직접-수치미분해서",
    "title": "인공신경망과 역전파",
    "section": "학습: 직접 수치미분해서",
    "text": "학습: 직접 수치미분해서\n\n앞서 fmin_cg함수를 그냥 사용하기만 했는데 제대로 네트워크가 학습되는 것을 확인\nfmin_cg함수가 내부적으로 수치 미분을 수행하기 때문에 학습이 가능\n다음 단계로 수치 미분도 직접 해서 최적화를 수행\n수치 미분함수는 이전에 만들어 뒀던 함수를 그대로 재사용\n\n\n# 수치미분 함수\ndef numer_deriv(x, fun, args=(), h=None, method=\"central\", dtype='float32'):\n    \"\"\"\n    Find the first derivative of a function at a point x.\n    \n    x     : The point at which derivative is found.\n    fun   : Input function.\n    args  : Tuple extra arguments passed to fun\n    h     : step size\n    method: 'central' or 'forward'\n    dtype : Data type of gradient, must be set to 'longdouble' if numerically unstable \n    \"\"\"\n    \n    # [1] 필요변수 초기화\n    scalar = False\n    m = 1\n\n    # [2] x가 스칼라인지 벡터인지 확인 스칼라면 무조건 벡터로 고치고 시작\n    if not hasattr(type(x), '__iter__'):\n        x = np.array([x])\n        scalar = True\n\n    # [3] x 타입을 디폴트로 float32로 변경\n    x = x.astype(dtype)\n\n    # [4] 미분 계수 초기화\n    g = np.zeros(x.shape[0]).astype(dtype)\n    \n    # [5] 미분계수를 변수 개수만큼 루프를 돌면서 구하기\n    for i in range(x.shape[0]) :\n        # [5-1]변경된 위치를 설정할 변수 두개를 준비\n        # dx1[i], dx2[i]로 각 변수의 요소를 접근할 수 있음\n        dx1 = x.copy()\n        dx2 = x.copy()\n        \n        # [5-2] h 결정 대충 변수의 1%정도\n        # https://en.wikipedia.org/wiki/Numerical_differentiation\n        if h == None:\n            h = np.sqrt(np.finfo(np.float32).eps) if x[i] == 0.0 else np.sqrt(np.finfo(np.float32).eps) * x[i]\n        \n        # [5-3*] dx1[i] 변경\n        dx1[i] += h\n        \n        # [5-4*] central이면 dx[2]도 함께 변경\n        if method == \"central\":\n            dx2[i] -= h\n            m = 2\n\n        # [5-5*] 미분계수 계산\n        g[i] = ( fun(dx1, *args) - fun(dx2, *args) ) / (m*h)\n        \n    # [6] 결과값 리턴\n    if scalar:\n        return g[0]    \n    else:\n        return g\n\n\n\nscipy.optimize.fmin_cg\n\nscipy에서 제공하는 켤레 경사법 함수의 매개변수를 살펴보면\n\nscipy.optimize.fmin_cg(f, x0, fprime=None, args=(), \n                       gtol=1e-05, norm=inf, \n                       epsilon=1.4901161193847656e-08, \n                       maxiter=None, full_output=0, \n                       disp=1, retall=0, callback=None)\n\nf: callable, f(x, *args)\n\nObjective function to be minimized. Here x must be a 1-D array of the variables that are to be changed in the search for a minimum, and args are the other (fixed) parameters of f.\n\nx0: ndarray\n\nA user-supplied initial estimate of xopt, the optimal value of x. It must be a 1-D array of values.\n\nfprime: callable, fprime(x, *args), optional\n\nA function that returns the gradient of f at x. Here x and args are as described above for f. The returned value must be a 1-D array. Defaults to None, in which case the gradient is approximated numerically (see epsilon, below).\n\n앞선 실험과 최대한 비슷한 최적화 결과를 만들기 위해 fmin_cg함수를 그대로 사용하되 수치미분 함수만 직접 만든 함수로 전달하는 방법을 사용\n수치 미분함수를 직접 만들었으니 간단하게 경사하강법을 직접 구현해도 되지만 그렇게 할 경우 선탐색이나 강하 방향을 구하는 부분이 fmin_cg와 달라져서 앞선 실험과 동일한 결과를 얻기 힘들어 짐\nfmin_cg를 그대로 사용하기 위해 함수 도움말을 살펴보면 함숫값을 구하는 f와 미분계수를 구하는 fprime이 함수 헤더가 동일해야 함\n지금 f에 해당하는 J의 해더는 다음과 같고\n\ndef J(W, X, T):\n\nfprime 역할을 할 numer_deriv의 헤더는 다음과 같아서 일치하지 않음\n\nnumer_deriv(x, fun, args=(), h=None, method='central', dtype='float32'):\n\n헤더가 다르므로 다음처럼 fmin_cg를 호출할 수 없고, 따라서 목적함수 J와 미분계수를 계산하는 numer_deriv의 헤더를 동일하게 해줄 방법이 필요\n\nfmin_cg(J, W, fprime=numer_deriv)\n\n함수를 리턴하는 클로져 사용하여 이 문제를 간단하게 해결\n\n\n# make_fprime함수는 내부적으로 함수를 하나 만들어 반환하는데\n# 이 반환된 함수는 fprime(W, X, T)로 호출할 수 잇는 함수\n# 반환된 함수 내부에서 numer_derive가 호출되어 함수 f에 대한\n# 미분 계수를 구할 수 있음\n\ndef make_fprime(f):\n    def fprime(W, X, T, h=None, method='central', dtype='float32'):\n        c = numer_deriv(W, f, (X,T), h, method, dtype)\n        return c\n    return fprime\n\n\n클로저 문법을 사용하여 J를 수치미분하는 함수 fprime을 생성\n\n\nfprime = make_fprime(J)\n\n\n이제 fprime은 매개변수로 W, X, T를 받는 함수이므로 목적함수 J와 똑같은 호출 방식으로 미분 계수를 구할 수 있게 됨\n\n\nfprime(W, samples, target)\n\narray([ 1.2754e-03,  2.9345e-03,  7.6448e-03,  7.3761e-05,  4.8319e-05,  3.5792e-05, -7.4173e-02, -1.2350e-02, -7.4711e-02], dtype=float32)\n\n\n\n모든 부품이 완성되었으므로 최적화 수행\n\n\n# X = samples.copy()\n# y = target.copy()\n# W_scratch = W.copy()\n\nW_scratch = optimize.fmin_cg(J, W, fprime=fprime, \n                             args=(samples, target), gtol=1e-06  )\n\nWarning: Desired error not necessarily achieved due to precision loss.\n         Current function value: 0.000022\n         Iterations: 477\n         Function evaluations: 1893\n         Gradient evaluations: 1882\n\n\n\n최적회 결과를 사용하여 결과를 확인하면 잘 분류하는 것을 확인할 수 있음\n\n\nW_scratch = W_scratch.reshape(3,3)\nW_scratch\n\n# array([[ 40.6953,  -8.9941, -13.0385],\n#        [-24.8027,   8.6743,   2.8026],\n#        [ 18.5373, -42.7733,  42.151 ]])\n\narray([[ 37.5771,  -8.2841, -12.0733],\n       [-23.0983,   8.0599,   2.6867],\n       [ 14.0654, -33.0609,  33.4288]])\n\n\n\nx = np.linspace(0, 5, 200)\nX1, X2 = np.meshgrid(x, x)\n\ndcs_bnd_1_imp_ = lambda x, y: W_scratch[0, 1]*x + W_scratch[0, 2]*y + W_scratch[0, 0] \ndcs_bnd_2_imp_ = lambda x, y: W_scratch[1, 1]*x + W_scratch[1, 2]*y + W_scratch[1, 0] \n\na, b, c = W_scratch[2, 1], W_scratch[2, 2], W_scratch[2, 0] \no = lambda x, y: logistic( a*logistic(dcs_bnd_1_imp_(x, y)) \n                         + b*logistic(dcs_bnd_2_imp_(x, y)) \n                         + c )\n\nfig = plt.figure(figsize=(7,7))\nax = plt.axes()\n\nax.xaxis.set_tick_params(labelsize=18)\nax.yaxis.set_tick_params(labelsize=18)\nax.set_xlabel('$x$', fontsize=25)\nax.set_ylabel('$y$', fontsize=25)\n\npred = o(samples[:,0], samples[:,1])\npred_pos = pred &gt;= 0.5\npred_neg = pred &lt; 0.5\n\nax.contour(X1, X2, o(X1,X2), cmap='gray', levels=[0.5])\nax.contourf(X1, X2, o(X1,X2), cmap=cm2, levels=[-10, 0.5, 10], alpha=0.3)\n\n# for positive samples\nTP = np.logical_and(target==1, pred_pos)\nFN = np.logical_and(target==1, pred_neg)\n\n# for negative samples\nTN = np.logical_and(target==0, pred_neg)\nFP = np.logical_and(target==0, pred_pos)\n\n# True Positive and True Negative\nax.plot(samples[TP,0], samples[TP,1], 'o', \n        markerfacecolor='C1', markeredgecolor='k', markersize=8)\nax.plot(samples[TN,0], samples[TN,1], '^', \n        markerfacecolor='C6', markeredgecolor='k', markersize=8)\n\n# False Positive and False Negative\nax.plot(samples[FN,0], samples[FN,1], 'o', \n        markerfacecolor='white', markeredgecolor='k', \n        markeredgewidth=2, markersize=15)\nax.plot(samples[FP,0], samples[FP,1], '^', \n        markerfacecolor='white', markeredgecolor='k', \n        markeredgewidth=2, markersize=15)\n\nax.set_ylim(0,5)\nax.set_xlim(0,5)\n\nplt.show()"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#신경망-미분하기-데이터-하나에-대해서",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#신경망-미분하기-데이터-하나에-대해서",
    "title": "인공신경망과 역전파",
    "section": "신경망 미분하기 (데이터 하나에 대해서)",
    "text": "신경망 미분하기 (데이터 하나에 대해서)\n\n이제 fmin_cg에 전달했던 수치미분으로 미분 계수를 구하는 함수를 자동미분을 이용한 역전파 방식으로 바꿔서 전달할 차례\n지금부터 한단계 한단계 생략없이 주어진 네트워크에 자동미분을 적용하고 그 결과를 파이토치 자동미분과 비교하여 모든 결과의 유효성을 검증\n\n\n# pytorch를 임포트 한다.\nimport torch\n\n\n순전파 함수(넘파이)\n\ndef forward(X, W, T, retopt='all'):\n    \"\"\"\n    네트워크를 피드포워드 시킨다. numpy 버전\n    X : 네트워크의 입력벡터 shape:(N,2)\n    retopt : 네트워크가 순전파되면서 각 레이어에서 계산된 결과 값을 \n    되돌릴 방법을 설정한다.\n        - 'all'  : 모든 층에서 계산된 결과를 튜플 형태로 되돌린다.\n        - 'fval' : 함수의 최종 출력값만 되돌린다.\n    \"\"\"\n    N = X.shape[0]\n    \n    H1 = np.dot(W[:2,1:], X.T)\n    Z1 = H1 + W[:2,0].reshape(-1,1)\n    A1 = logistic(Z1)\n    \n    H2 = np.dot(W[2,1:], A1)\n    Z2 = H2 + W[2,0]\n    A2 = logistic(Z2)\n\n    C = (1/(2*N)) * ((T-A2)**2).sum()\n    \n    if retopt == 'all':\n        return (H1, Z1, A1, H2, Z2, A2, C)\n    elif retopt == 'fval':\n        return C\n\n\n\n순전파 함수(파이토치)\n\ndef forward_torch(X, W, T, retopt='all'):\n    \"\"\"\n    네트워크를 피드포워드 시킨다. pytorch 버전\n    X : 네트워크의 입력벡터 size:(N,2)\n    retopt : 네트워크가 순전파되면서 각 레이어에서 계산된 결과 값을 \n    되돌릴 방법을 설정한다.\n        - 'all'  : 모든 층에서 계산된 결과를 튜플 형태로 되돌린다.\n        - 'fval' : 함수의 최종 출력값만 되돌린다.\n    \"\"\"\n    N = X.size()[0]\n    T = torch.tensor(T, dtype=torch.double)\n    \n    # 계산 결과 검증을 위해 pytorch를 사용하므로 numpy 어레이 뿐 아니라\n    # pytorch tensor형태에 대해서도 동일한 연산을 한다.\n    H1 = torch.mm(W[:2,1:], torch.t(X))\n    Z1 = H1 + W[:2,0].view(-1,1)\n    A1 = torch.sigmoid(Z1)\n    \n    H2 = torch.mm(W[2:,1:], A1)\n    Z2 = H2 + W[2,0]\n    A2 = torch.sigmoid(Z2)\n\n    C = (1/(2*N)) * ((T-A2)**2).sum()\n    \n    if retopt == 'all':\n        return (H1, Z1, A1, H2, Z2, A2, C)\n    elif retopt == 'fval':\n        return C\n\n\n\n넘파이 어레이와 텐서 프린트 보조함수\n\nnp.set_printoptions(precision=4, linewidth =150)\n\ndef print_tensor(t):\n    \"\"\"\n    텐서형 자료를 보기 좋게 프린트하기 위한 보조 함수\n    \"\"\"\n    def namestr(obj, namespace):\n        return [name for name in namespace if namespace[name] is obj]\n    \n    var_name = namestr(t, globals())[0]\n    \n    print(\"{}:{},{}\".format(var_name, t.shape, t.dtype))\n    print(t)\n    print(\"-------------------------------------------\")\n\n\n\n가중치 무작위 초기화\n\nnp.random.seed(17)\n\nW = np.random.randn(9).reshape(3,3)\nW_torch = torch.tensor(W, dtype=torch.double); W_torch.requires_grad=True\n\nprint_tensor(W)\nprint_tensor(W_torch)\n\nW:(3, 3),float64\n[[ 0.2763 -1.8546  0.6239]\n [ 1.1453  1.0372  1.8866]\n [-0.1117 -0.3621  0.1487]]\n-------------------------------------------\nW_torch:torch.Size([3, 3]),torch.float64\ntensor([[ 0.2763, -1.8546,  0.6239],\n        [ 1.1453,  1.0372,  1.8866],\n        [-0.1117, -0.3621,  0.1487]], dtype=torch.float64, requires_grad=True)\n-------------------------------------------\n\n\n\n\n초기 상태에서 목적함숫값\n\n초기 상태에서 목적함숫값을 구하여 forward 함수가 제대로 네트워크를 포워드 패스하는지 확인\nJ()와 forward()의 출력값이 동일 입력에 대해서 동일해야 함\n\n\n# 초기 상태에서 목적함숫값\nJ(W, samples, target)\n\n0.1263148192185165\n\n\n\nH1, Z1, A1, H2, Z2, A2, C = forward(samples, W, target)\n\nC\n\n0.1263148192185165\n\n\n\n\n데이터 하나로 순전파 시키기\n\n500개 샘플 중에 0번 샘플 하나만 네트워크에 입력하여 순전파 시키고 이후 단계별로 역전파 시켜 가중치와 바이어스에 대한 미분계수를 계산\n이후 계산은 항상 직접 만든 코드와 파이토치의 계산 결과를 비교하면서 진행\n\n\nN = 1\nx = samples[[0]]\nx_torch = torch.tensor(x, dtype=torch.double); x_torch.requires_grad=True\nt = target[[0]]\n\nprint_tensor(x)\nprint_tensor(x_torch)\nprint_tensor(t)\n\nx:(1, 2),float64\n[[2.754  3.5407]]\n-------------------------------------------\nx_torch:torch.Size([1, 2]),torch.float64\ntensor([[2.7540, 3.5407]], dtype=torch.float64, requires_grad=True)\n-------------------------------------------\nt:(1,),float64\n[1.]\n-------------------------------------------\n\n\n\nH1, Z1, A1, H2, Z2, A2, C = forward(x, W, t)\n\nprint_tensor(H1)\nprint_tensor(Z1)\nprint_tensor(A1)\nprint_tensor(H2)\nprint_tensor(Z2)\nprint_tensor(A2)\nprint_tensor(C)\n\nH1:(2, 1),float64\n[[-2.8986]\n [ 9.5365]]\n-------------------------------------------\nZ1:(2, 1),float64\n[[-2.6223]\n [10.6818]]\n-------------------------------------------\nA1:(2, 1),float64\n[[0.0677]\n [1.    ]]\n-------------------------------------------\nH2:(1,),float64\n[0.1242]\n-------------------------------------------\nZ2:(1,),float64\n[0.0125]\n-------------------------------------------\nA2:(1,),float64\n[0.5031]\n-------------------------------------------\nC:(),float64\n0.12344827837124746\n-------------------------------------------\n\n\n\nH1_torch, Z1_torch, A1_torch, H2_torch, Z2_torch, A2_torch, C_torch = forward_torch(x_torch, W_torch, t)\n\nprint_tensor(H1_torch)\nprint_tensor(Z1_torch)\nprint_tensor(A1_torch)\nprint_tensor(H2_torch)\nprint_tensor(Z2_torch)\nprint_tensor(A2_torch)\nprint_tensor(C_torch)\n\nH1_torch:torch.Size([2, 1]),torch.float64\ntensor([[-2.8986],\n        [ 9.5365]], dtype=torch.float64, grad_fn=&lt;MmBackward0&gt;)\n-------------------------------------------\nZ1_torch:torch.Size([2, 1]),torch.float64\ntensor([[-2.6223],\n        [10.6818]], dtype=torch.float64, grad_fn=&lt;AddBackward0&gt;)\n-------------------------------------------\nA1_torch:torch.Size([2, 1]),torch.float64\ntensor([[0.0677],\n        [1.0000]], dtype=torch.float64, grad_fn=&lt;SigmoidBackward0&gt;)\n-------------------------------------------\nH2_torch:torch.Size([1, 1]),torch.float64\ntensor([[0.1242]], dtype=torch.float64, grad_fn=&lt;MmBackward0&gt;)\n-------------------------------------------\nZ2_torch:torch.Size([1, 1]),torch.float64\ntensor([[0.0125]], dtype=torch.float64, grad_fn=&lt;AddBackward0&gt;)\n-------------------------------------------\nA2_torch:torch.Size([1, 1]),torch.float64\ntensor([[0.5031]], dtype=torch.float64, grad_fn=&lt;SigmoidBackward0&gt;)\n-------------------------------------------\nC_torch:torch.Size([]),torch.float64\ntensor(0.1234, dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n-------------------------------------------\n\n\n\n\n단계별 계산\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{a}^{(2)}}\\): (1,1)\n\n목적함수를 목적함수의 직접적인 입력값인 네트워크의 최종 출력 \\(\\mathbf{a}^{(2)}\\)로 미분\n현재 예제로 다루는 네트워크는 최종출력이 노드 하나라 \\(\\mathbf{a}^{(2)}\\)가 스칼라지만 일반화를 위해 표기는 볼드인 벡터로 함\n\n\ndA2 = -(t-A2)/N\ndA2\n\narray([-0.4969])\n\n\n\n위 직접 미분한 결과와 파이토치 미분 결과를 비교\n파이토치 grad함수를 사용하여 종속변수와 독립변수를 지정하고 미분계수를 되돌려 받음\n\n\n#                              종속변수, 독립변수, 종속변수와 곱해지는 상위 그래디언트 \ndA2_torch = torch.autograd.grad(C_torch, A2_torch, torch.tensor(1, dtype=torch.double), \n                                retain_graph=True)[0]\ndA2_torch\n\ntensor([[-0.4969]], dtype=torch.float64)\n\n\n\n코드상의 노테이션\n\n목적함수에 대한 어떤 독립변수 x의 미분: 독립변수만 표시 dx\n중간변수 y에 대한 어떤 독립변수 x의 미분: dx: 두 변수를 언더바로 연결 dy_dx\n\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{z}^{(2)}}\\): (1,1)\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{a}^{(2)}}\\)에 대한 미분을 끝냈으면 그 다음 단계로 \\(\\mathbf{z}^{(2)}\\)에 대한 미분을 할 차례\n이를 위해서 \\(\\frac{d\\mathbf{a}^{(2)}}{d\\mathbf{z}^{(2)}}\\)를 계산하고 다음처럼 연쇄법칙 적용\n\n\\[\n\\frac{\\partial C}{\\partial \\mathbf{z}^{(2)}} = \\frac{d\\mathbf{a}^{(2)}}{d\\mathbf{z}^{(2)}} \\frac{\\partial C}{\\partial \\mathbf{a}^{(2)}}\n\\]\n\ndA2_dZ2 = logistic(Z2)*(1-logistic(Z2))\ndZ2 = dA2_dZ2 * dA2\ndZ2\n\narray([-0.1242])\n\n\n\n역시 파이토치 미분 결과와 비교\n두 결과는 완전히 동일해야 함\n\n\n#                              종속변수, 독립변수, 종속변수와 곱해지는 상위 그래디언트 \ndZ2_torch = torch.autograd.grad(C_torch, Z2_torch, torch.tensor(1, dtype=torch.double), \n                                retain_graph=True)[0]\ndZ2_torch\n\ntensor([[-0.1242]], dtype=torch.float64)\n\n\n\n이런 식으로 단위 함수에 대한 미분을 완성하고 지금까지 구한 미분계수를 곱하여 현재까지 미분계수를 계속 구해 나가는 방식을 역전파 알고리즘이라고 함\n그리고 특정층 \\(l\\)에 대한 가중합(가중치를 곱하고 바이어스를 더한 결과) \\(\\mathbf{z}^{(l)}\\)에 대한 미분은 특별한 역할을 하므로 다음처럼 \\(\\delta\\)로 주로 표시\n\n\\[\n\\boldsymbol{\\delta}^{(l)} = \\frac{\\partial C}{\\partial \\mathbf{z}^{(l)}}\n\\]\n\n이 문서에서 다음 세 표현은 모두 동일한 것들\n\n\\[\n\\boldsymbol{\\delta}^{(l)} = \\frac{\\partial C}{\\partial \\mathbf{z}^{(l)}} = \\frac{\\partial C}{\\partial \\mathbf{h}^{(l)}}\n\\]\n\n이 후 계속 진행되는 코드는 계속 반복되므로 슬라이드를 참조하면 됨\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{b}^{(2)}}\\): (1,1)\n\n\\(\\mathbf{b}^{(2)}\\)에 대한 미분은 더하기 연산의 백워드 패스 특성에 따라 그냥 \\(\\frac{\\partial C}{\\partial \\mathbf{z}^{(2)}}\\)가 됨\n\n\ndb2 = dZ2\ndb2\n\narray([-0.1242])\n\n\n\nW_torch\n\ntensor([[ 0.2763, -1.8546,  0.6239],\n        [ 1.1453,  1.0372,  1.8866],\n        [-0.1117, -0.3621,  0.1487]], dtype=torch.float64, requires_grad=True)\n\n\n\n파이토치로 결과 확인\n단 여기서는 모든 가중치와 바이어스가 모여있는 W_torch행렬로 미분을 해서 모든 매개변수에 대한 미분계수를 한꺼번에 계산\n그렇게 매개변수에 대한 미분 계수 dW_torch를 계산한 다음 적절한 위치의 숫자를 잘라와서 직접 계산한 미분 계수와 비교\n\n\n#                              종속변수, 독립변수, 종속변수와 곱해지는 상위 그래디언트 \ndW_torch = torch.autograd.grad(C_torch, W_torch, torch.tensor(1, dtype=torch.double), \n                               retain_graph=True)[0]\ndW_torch[2,0]\n\ntensor(-0.1242, dtype=torch.float64)\n\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{h}^{(2)}}\\): (1,1)\n\ndH2 = dZ2\ndH2_torch = dZ2_torch\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{W}^{(2)}}\\): (1,2)\n\n\\(\\mathbf{h}^{(2)}\\)를 \\(\\mathbf{W}^{(2)}\\)로 미분하는 단계에서는 벡터를 벡터로 미분하는 것이기 때문에 결과는 야코비안이 되야 함\n그런데 \\(\\mathbf{W}^{(2)}\\)는 어디까지나 행벡터이기 때문에 정확히 야코비안을 구하려면 \\(\\mathbf{W}^{(2)}\\)를 전치시켜서 열벡터로 만들고 나서 미분해야 함\n아래는 그렇게 구해진 야코비안을 계산하는 코드\n\n\ndH2_dW2 = A1.T #(1,2)\nnp.dot(dH2_dW2.T, dH2.reshape(-1,1))\n\narray([[-0.0084],\n       [-0.1242]])\n\n\n\n이렇게 구해진 야코비안을 다시 전치 시켜 현재까지 구한 그래디언테 dH2와 곱함\n여기서 역전파 알고리즘의 가장 일반적인 규칙인 ✅야코비안 전치 곱하기 그래디언트 가 처음으로 나타남\n\n\ndH2_dW2 = A1.T #(1,2)\n\ndW2 = np.dot(dH2_dW2.T, dH2.reshape(-1,1)) # (2,1) 야코비안 전치 곱하기 그레디언트\ndW2.T #여기서 결과를 다시 전치시켜야 (1,2)가 되는 이유는 일반화 야코비안과 관련있고 뒤에 다시 설명\n\narray([[-0.0084, -0.1242]])\n\n\n\ndW_torch[2,1:]\n\ntensor([-0.0084, -0.1242], dtype=torch.float64)\n\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{a}^{(1)}}\\): (2,1)\n\ndH2_dA1 = W[2:,1:] # dH2/dA1은 (1,2)인 야코비안이 맞고\ndH2_dA1.shape\n\n(1, 2)\n\n\n\ndA1 = np.dot(dH2_dA1.T, dH2.reshape(-1,1)) # 따라서 야코비안 전치 그래디언트가 성립, W.T * delta\n\ndA1\n\narray([[ 0.045 ],\n       [-0.0185]])\n\n\n\n#                              종속변수, 독립변수, 종속변수와 곱해지는 상위 그래디언트 \ndA1_torch = torch.autograd.grad(C_torch, A1_torch, torch.tensor(1, dtype=torch.double), \n                                retain_graph=True)[0]\ndA1_torch\n\ntensor([[ 0.0450],\n        [-0.0185]], dtype=torch.float64)\n\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{z}^{(1)}}\\): (2,1)\n\ndA1_dZ1 =  np.zeros((A1.shape[0], Z1.shape[0]))\ndA1_dZ1[np.diag_indices(Z1.shape[0])] = (logistic(Z1)*(1-logistic(Z1))).reshape(-1)\n\ndA1_dZ1\n\narray([[6.3132e-02, 0.0000e+00],\n       [0.0000e+00, 2.2958e-05]])\n\n\n\ndZ1 = np.dot(dA1_dZ1.T, dA1)\ndZ1\n\narray([[ 2.8396e-03],\n       [-4.2398e-07]])\n\n\n\n#                              종속변수, 독립변수, 종속변수와 곱해지는 상위 그래디언트 \ndZ1_torch = torch.autograd.grad(C_torch, Z1_torch, torch.tensor(1, dtype=torch.double), \n                                retain_graph=True)[0]\ndZ1_torch\n\ntensor([[ 2.8396e-03],\n        [-4.2398e-07]], dtype=torch.float64)\n\n\n\n중간층에 비선형 활성화 함수는 일변수 스칼라 함수이므로 야코비안은 항상 대각행렬 형태로만 나타남\n그래서 굳이 이렇게 어렵게 하지말고 그냥 엘리먼트 와이즈(아다마르 곱)로 처리하는 것이 더 간편하다.\n넘파이로는 그냥 *연산자로 곱하면 그만\n\n\ndA1_dZ1 = logistic(Z1)*(1-logistic(Z1))\n\ndA1_dZ1\n\narray([[6.3132e-02],\n       [2.2958e-05]])\n\n\n\ndZ1_ = dA1_dZ1 * dA1\ndZ1_\n\narray([[ 2.8396e-03],\n       [-4.2398e-07]])\n\n\n\n이후 계산은 \\(\\mathbf{W}^{(1)}\\)으로 미분할 때 까지 동일하게 진행\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{b}^{(1)}}\\): (2,1)\n\ndb1 = dZ1\ndb1\n\narray([[ 2.8396e-03],\n       [-4.2398e-07]])\n\n\n\ndW_torch[:2,0].view(-1,1)\n\ntensor([[ 2.8396e-03],\n        [-4.2398e-07]], dtype=torch.float64)\n\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{h}^{(1)}}\\): (2,1)\n\ndH1 = dZ1\ndH1_torch = dZ1_torch\n\ndH1_torch\n\ntensor([[ 2.8396e-03],\n        [-4.2398e-07]], dtype=torch.float64)\n\n\n\ndH1\n\narray([[ 2.8396e-03],\n       [-4.2398e-07]])\n\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{W}^{(1)}}\\): (2,2)\n\n최종적으로 \\(\\frac{\\partial C}{\\partial \\mathbf{W}^{(1)}}\\)를 구하기위해 \\(\\frac{\\partial \\mathbf{h}^{(1)}}{\\partial \\mathbf{W}^{(1)}}\\)를 계산해야 하는데 분자는 벡터, 분모는 행렬 일반적인 규칙으로 미분안됨\n방법1: 일반화 야코비안을 결과로 얻는 법\n\n\\(\\mathbf{h}^{(1)}\\)의 각 요소(스칼라)를 행렬 \\(\\mathbf{W}^{(1)}\\)로 미분하여 결과 행렬을 요소 개수 만큼 만들고\n이렇게 얻어진 행렬을 야코비안의 각 행으로 보는 방식, 즉 행이 두 개 있는데 각 행의 모양이 (2,2)인 행렬로 (2, (2,2))가 되는 식\n\n방법2: \\(\\mathbf{W}^{(1)}\\)을 \\(\\text{vec}\\)연산자를 사용하여 벡터로 바꾸거나 일반화 야코비안을 행렬로 바꾸는 방법\n\n잠시 후 코드로 같은 결과가 나옴을 확인할 예정\n\n\\(\\frac{\\partial \\mathbf{h}^{(1)}}{\\partial \\mathbf{W}^{(1)}}\\)를 계산하면 \\(\\mathbf{h}^{(1)}\\)가 계산되는 과정 때문에 각 행렬에 결과적으로 \\(\\mathbf{h}^{(1)}\\)를 계산하기 위해 \\(\\mathbf{W}^{(1)}\\)과 곱해지는 입력이 나타나게 됨\n\n\ndH1_dW1 = np.zeros((2,2,2))\ndH1_dW1[0,0,:] = x\ndH1_dW1[1,1,:] = x\n\nprint_tensor(dH1_dW1)\n\ndH1_dW1:(2, 2, 2),float64\n[[[2.754  3.5407]\n  [0.     0.    ]]\n\n [[0.     0.    ]\n  [2.754  3.5407]]]\n-------------------------------------------\n\n\n\ndH1.reshape(dH1.shape[0], 1, 1)\n\narray([[[ 2.8396e-03]],\n\n       [[-4.2398e-07]]])\n\n\n\n위 두 셀에서 구한 \\(\\frac{\\partial \\mathbf{h}^{(1)}}{\\partial \\mathbf{W}^{(1)}}\\)과 \\(\\frac{\\partial C}{\\partial \\mathbf{h}^{(1)}}\\)를 행렬곱\n여기서 행렬곱할 때 앞에서 곱하는 행렬의 열을 뒤에서 곱하는 벡터의 숫자로 선형조합하는 논리가 적용됨\n아래 코드는 그 논리를 그대로 코드로 옮긴 것\n\n\ndW1 = (dH1_dW1*dH1.reshape(dH1.shape[0], 1, 1)).sum(axis=0)\ndW1\n\narray([[ 7.8203e-03,  1.0054e-02],\n       [-1.1676e-06, -1.5012e-06]])\n\n\n\ndW_torch[:2,1:]\n\ntensor([[ 7.8203e-03,  1.0054e-02],\n        [-1.1676e-06, -1.5012e-06]], dtype=torch.float64)\n\n\n\n위 진행한 연산의 최종 결과를 정리하면 다음처럼 \\(l\\)층의 가중치 \\(\\mathbf{W}^{(l)}\\)에 대한 미분의 일반 공식을 알 수 있음\n\n\\[\n\\frac{\\partial C}{\\partial \\mathbf{W}^{(l)}} = \\boldsymbol{\\delta}^{(l)} \\left( \\mathbf{a}^{(l-1)}\\right)^T\n\\]\n\nprint(dH1.shape)\nprint(x.shape)\nnp.dot(dH1, x)\n\n(2, 1)\n(1, 2)\n\n\narray([[ 7.8203e-03,  1.0054e-02],\n       [-1.1676e-06, -1.5012e-06]])\n\n\n\n\n텐서 평활화\n\n\\(W^{(1)}\\)에 대한 미분 계수를 구할 때 행렬곱에서 알아본 열조합으로 설명되는 행렬곱을 적용\n여기서는 텐서 형태로 구해지는 일반화 야코비안을 행렬 형태로 펼쳐서 계산하는 방법을 실험\n결국은 같은 연산을 하게 되고 \\(\\mathbf{h}^{(1)}\\)을 \\(W^{(1)}\\)으로 미분할 때 \\(W^{(1)}\\)을 \\(\\text{vec}\\) 연산자로 벡터로 바꾼 다음 미분하는 것과 같은 결과를 얻게 됨\n행렬에 \\(\\text{vec}\\) 연산자와 \\(\\text{vec}\\) 역연산이 적용되는 규칙은 “벡터, 행렬에 대한 미분”(https://metamath1.github.io/2018/01/02/matrix-derivatives.html)에서 vec과 vec전치 부분 참고\n아래 코드는 (2,2,2)인 텐서를 reshape하여 (4,2)로 만들고 다시 원래 모양으로 되돌리는 실험 코드\n\n\nT = np.arange(8).reshape(2,2,2)\nT\n\narray([[[0, 1],\n        [2, 3]],\n\n       [[4, 5],\n        [6, 7]]])\n\n\n\nT.reshape(-1,2)\n\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7]])\n\n\n\nT.reshape(-1,2).reshape(2,2,2)\n\narray([[[0, 1],\n        [2, 3]],\n\n       [[4, 5],\n        [6, 7]]])\n\n\n\n텐서를 평활화해서 경사도벡터와 곱하는 한줄 코드\n\n텐서를 전치시키고, 평활화 한 다음 야코비안과 그래디언트를 곱하고 다시 원래 모양으로 돌려놓는 한줄 코드\n\n\nnp.dot(\n    dH1_dW1.transpose(1,2,0).reshape(-1,2), # Transposing and Flattening Jacobian \n    dH1 # Mutiply jacobian by gradient\n    ).reshape(2, 2, 1).squeeze() # reshaping\n\narray([[ 7.8203e-03,  1.0054e-02],\n       [-1.1676e-06, -1.5012e-06]])"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#데이터가-여러개인-경우",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#데이터가-여러개인-경우",
    "title": "인공신경망과 역전파",
    "section": "데이터가 여러개인 경우",
    "text": "데이터가 여러개인 경우\n\n이제 데이터 여러개가 한꺼번에 입력되는 경우에 대해 생각\n여기서는 확인을 쉽게 하기 위해 데이터 세 개 에 대해서 실험 즉 \\(N=3\\)\n\n\nN=3\nx = samples[[0,5,10]]\nx_torch = torch.tensor(x, dtype=torch.double); x_torch.requires_grad=True\nt = target[[0,5,10]]\n\nprint_tensor(x)\nprint_tensor(x_torch)\nprint_tensor(t)\n\nx:(3, 2),float64\n[[2.754  3.5407]\n [0.1494 2.2842]\n [1.4176 3.4657]]\n-------------------------------------------\nx_torch:torch.Size([3, 2]),torch.float64\ntensor([[2.7540, 3.5407],\n        [0.1494, 2.2842],\n        [1.4176, 3.4657]], dtype=torch.float64, requires_grad=True)\n-------------------------------------------\nt:(3,),float64\n[1. 0. 1.]\n-------------------------------------------\n\n\n\n순전파와 역전파를 동시에 하는 함수\n\n여러 데이터에 대해서 순전파와 역잔파를 동시에 하는 함수 작성\n함수 내부 코드는 지금까지 진행한 단계별 코드를 한군데 모은것\n\n\ndef forward_backward(X, W, T):\n    \"\"\"\n    네트워크를 포워드, 백워드 시킨다. numpy 버전\n    X : 네트워크의 입력벡터 shape:(N,2)\n    반환 : 목적함수, (W1에 대한 미분계수, b1에 대한 미분계수,\n                     W2에 대한 미분계수, b2에 대한 미분계수)\n    \"\"\"\n    # forward\n    N = X.shape[0]\n    \n    H1 = np.dot(W[:2,1:], X.T)\n    Z1 = H1 + W[:2,0].reshape(-1,1)\n    A1 = logistic(Z1)\n    \n    H2 = np.dot(W[2,1:], A1)\n    Z2 = H2 + W[2,0]\n    A2 = logistic(Z2)\n    \n    C = (1/(2*N)) * ((T-A2)**2).sum()\n    \n    # backward, dA-&gt;dZ-&gt;db-&gt;dW\n    dA2 = -(T-A2)/N\n    dA2_dZ2 = logistic(Z2)*(1-logistic(Z2))\n    dZ2 = dA2_dZ2 * dA2\n    \n    db2 = dZ2\n    dW2 = np.dot(dZ2, A1.T)\n\n    dH2_dA1 = W[2:,1:]\n    dA1 = np.dot(dH2_dA1.T, dZ2.reshape(-1,1))\n    dA1_dZ1 = logistic(Z1)*(1-logistic(Z1))\n    dZ1 = dA1_dZ1 * dA1\n\n    db1 = dZ1\n    dW1 = np.dot(dZ1, X) # X is A0.T\n    \n    return C, (dW1, db1, dW2, db2)\n    \n\n\n\n모든 데이터에 대해 각각 역전파한 미분계수를 평균하기\n\n목적함수는 항상 데이터에 대해서 합산되는 형태이기 때문에 샘플하나로 순전파 역전파 한 과정을 데이터 개수만큼 반복하고 그 결과를 모두 더해서 평균하면 데이터 여러개에 대한 미분계수를 쉽게 구할 수 있음\n아래 코드로 확인\n\n\nf, dW1, db1, dW2, db2 = 0, 0, 0, 0, 0\n\nfor x_, t_ in zip(x, t):\n    \n    x_ = np.array([x_])\n    t_ = np.array([t_])\n    \n    fv, derivs = forward_backward(x_, W, t_)\n    \n    f += fv\n    dW1 += derivs[0]\n    db1 += derivs[1]\n    dW2 += derivs[2]\n    db2 += derivs[3]\n\nf   /= N\ndW1 /= N\ndb1 /= N\ndW2 /= N\ndb2 /= N\n\nprint('C:', f)\nprint_tensor(dW1)\nprint_tensor(db1)\nprint_tensor(dW2)\nprint_tensor(db2)\n\nC: 0.12001678911062634\ndW1:(2, 2),float64\n[[7.9160e-03 1.2443e-02]\n [1.5168e-06 4.1280e-05]]\n-------------------------------------------\ndb1:(2, 1),float64\n[[2.8791e-03]\n [1.8509e-05]]\n-------------------------------------------\ndW2:(2,),float64\n[ 0.0061 -0.0499]\n-------------------------------------------\ndb2:(1,),float64\n[-0.0497]\n-------------------------------------------\n\n\n\n이 결과가 맞는지 아래에서 확인할 것임\n이런 방식은 데이터 개수만큼 계속 반복을 하면서 순전파 역전파를 하기 때문에 비효올적\n행렬 연산을 이용하여 한번 순전파와 역전파 만으로 모든 데이터에 대한 연산과 합산을 할 수 있음\n\n\n\n역전파 한번으로 미분계수 구하기\n\nx\n\narray([[2.754 , 3.5407],\n       [0.1494, 2.2842],\n       [1.4176, 3.4657]])\n\n\n\n\n순전파\n\nH1, Z1, A1, H2, Z2, A2, C = forward(x, W.reshape(3,3), t)\n\nprint_tensor(H1)\nprint_tensor(Z1)\nprint_tensor(A1)\nprint_tensor(H2)\nprint_tensor(Z2)\nprint_tensor(A2)\nprint_tensor(C)\n\nH1:(2, 3),float64\n[[-2.8986  1.148  -0.4669]\n [ 9.5365  4.4643  8.0089]]\n-------------------------------------------\nZ1:(2, 3),float64\n[[-2.6223  1.4243 -0.1907]\n [10.6818  5.6096  9.1542]]\n-------------------------------------------\nA1:(2, 3),float64\n[[0.0677 0.806  0.4525]\n [1.     0.9964 0.9999]]\n-------------------------------------------\nH2:(3,),float64\n[ 0.1242 -0.1437 -0.0152]\n-------------------------------------------\nZ2:(3,),float64\n[ 0.0125 -0.2554 -0.1269]\n-------------------------------------------\nA2:(3,),float64\n[0.5031 0.4365 0.4683]\n-------------------------------------------\nC:(),float64\n0.12001678911062633\n-------------------------------------------\n\n\n\nH1_torch, Z1_torch, A1_torch, H2_torch, Z2_torch, A2_torch, C_torch = forward_torch(x_torch, W_torch, t)\n\nprint_tensor(H1_torch)\nprint_tensor(Z1_torch)\nprint_tensor(A1_torch)\nprint_tensor(H2_torch)\nprint_tensor(Z2_torch)\nprint_tensor(A2_torch)\nprint_tensor(C_torch)\n\nH1_torch:torch.Size([2, 3]),torch.float64\ntensor([[-2.8986,  1.1480, -0.4669],\n        [ 9.5365,  4.4643,  8.0089]], dtype=torch.float64,\n       grad_fn=&lt;MmBackward0&gt;)\n-------------------------------------------\nZ1_torch:torch.Size([2, 3]),torch.float64\ntensor([[-2.6223,  1.4243, -0.1907],\n        [10.6818,  5.6096,  9.1542]], dtype=torch.float64,\n       grad_fn=&lt;AddBackward0&gt;)\n-------------------------------------------\nA1_torch:torch.Size([2, 3]),torch.float64\ntensor([[0.0677, 0.8060, 0.4525],\n        [1.0000, 0.9964, 0.9999]], dtype=torch.float64,\n       grad_fn=&lt;SigmoidBackward0&gt;)\n-------------------------------------------\nH2_torch:torch.Size([1, 3]),torch.float64\ntensor([[ 0.1242, -0.1437, -0.0152]], dtype=torch.float64,\n       grad_fn=&lt;MmBackward0&gt;)\n-------------------------------------------\nZ2_torch:torch.Size([1, 3]),torch.float64\ntensor([[ 0.0125, -0.2554, -0.1269]], dtype=torch.float64,\n       grad_fn=&lt;AddBackward0&gt;)\n-------------------------------------------\nA2_torch:torch.Size([1, 3]),torch.float64\ntensor([[0.5031, 0.4365, 0.4683]], dtype=torch.float64,\n       grad_fn=&lt;SigmoidBackward0&gt;)\n-------------------------------------------\nC_torch:torch.Size([]),torch.float64\ntensor(0.1200, dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n-------------------------------------------\n\n\n\n\n단계별 역전파\n\n데이터가 N개가 되었기 때문에 앞선 단계별 역전파를 하면서 얻어진 결과 행렬들에서 열 개수가 N개로 늘어났을 뿐 계산 과정과 코드는 완전히 동일\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{a}^{(2)}}\\): (1,N)\n\ndA2 = -(t-A2)/N\n\ndA2\n\narray([-0.1656,  0.1455, -0.1772])\n\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{z}^{(2)}}\\): (1,N)\n\ndA2_dZ2 = logistic(Z2)*(1-logistic(Z2))\ndZ2 = dA2_dZ2 * dA2\n\ndZ2\n\narray([-0.0414,  0.0358, -0.0441])\n\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{b}^{(2)}}\\): (1,1)\n\n앞서 샘플이 하나일 때 \\(\\frac{\\partial C}{\\partial \\mathbf{b}^{(l)}} = \\delta^{(l)}\\)이었음\n그런데 여기서는 \\(\\delta^{(l)}\\)이 데이터 개수만큼 있어서 (1,N) 형태이므로 데이터에 대해서 모두 합산\n\n\ndb2 = dZ2.sum(axis=0, keepdims=True)\ndH2 = dZ2\n\ndb2\n\narray([-0.0497])\n\n\n\n이 결과는 앞서 루프를 사용해서 계산한 db2와 일치해야 함\n여기서 \\(\\mathbf{b}^{(l)}\\)에 대한 일반 공식을 얻음\n\n\\[\n\\frac{\\partial C}{\\partial \\mathbf{b}^{(l)}} = \\sum_{j} \\boldsymbol{\\delta}^{(l)}_{:\\, , \\, j}\n\\]\n\n위 식에서 \\(\\boldsymbol{\\delta}^{(l)}_{:\\, , \\, j}\\)는 \\(\\boldsymbol{\\delta}^{(l)}\\)의 \\(j\\)번째 열을 나타냄\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{W}^{(2)}}\\): (1,2)\n\ndH2_dW2 = A1.T\ndW2 = np.dot(dH2, dH2_dW2) # 정리된 결과로 야코비안 전치 평활화 따위 하지 않고 바로 delta * a.T\n\ndW2\n\narray([ 0.0061, -0.0499])\n\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{a}^{(1)}}\\): (2,N)\n\ndH2_dA1 = W[2:,1:] # 분자레이아웃 아코비안 W2 (1,2)\ndA1 = np.dot(dH2_dA1.T, dH2.reshape(1,-1)) # W.T * delta\n\ndA1\n\narray([[ 0.015 , -0.013 ,  0.016 ],\n       [-0.0062,  0.0053, -0.0066]])\n\n\n\n여기서 \\(\\mathbf{a}^{l}\\)에 대한 미분의 일반 공식을 얻을 수 있음\n\n\\[\n\\frac{\\partial C}{\\partial \\mathbf{a}^{(l)}} =  \\frac{\\partial \\mathbf{z}^{(l+1)}}{\\partial \\mathbf{a}^{(l)}} \\frac{\\partial C}{\\partial \\mathbf{z}^{(l+1)}} ={\\mathbf{W}^{(l+1)}}^T \\delta^{(l+1)}\n\\]\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{z}^{(1)}}\\): (2,N)\n\ndA1_dZ1 =  logistic(Z1)*(1-logistic(Z1))\n    \ndA1_dZ1\n\narray([[6.3132e-02, 1.5636e-01, 2.4774e-01],\n       [2.2958e-05, 3.6357e-03, 1.0576e-04]])\n\n\n\ndZ1 = dA1_dZ1 * dA1 # 엘리먼트와이즈 곱\n\ndZ1\n\narray([[ 9.4654e-04, -2.0261e-03,  3.9587e-03],\n       [-1.4133e-07,  1.9344e-05, -6.9385e-07]])\n\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{b}^{(1)}}\\): (2,1)\n\ndb1 = dZ1.sum(axis=1, keepdims=True)\ndb1\n\narray([[2.8791e-03],\n       [1.8509e-05]])\n\n\n\n\n\\(\\frac{\\partial C}{\\partial \\mathbf{W}^{(1)}}\\): (2,2)\n\ndW1 = np.dot(dZ1, x) # delta * a.T\ndW1\n\narray([[7.9160e-03, 1.2443e-02],\n       [1.5168e-06, 4.1280e-05]])\n\n\n\n\\(\\mathbf{W}^{(1)}\\)로 미분하는 경우는 데이터에 대해 모두 합산하는 과정이 없는데 구해진 미분 계수를 보면 앞서 루프를 돌면서 구한 결과와 정확히 일치함\n합산 과정이 없어도 제대로 미분계수가 구해지는 이유는 행렬곱이 되면서 데이터에 대해서 합산되는 과정이 저절로 실행되기 때문\n이 과정을 바로 이해하기 위해서는 행렬곱을 앞에서 곱하는 행렬의 열과 뒤에서 곱하는 행렬의 행으로 외적하여 생기는 행렬들의 합으로 이해하는 시각이 필요\n자세한 그림 표현은 슬라이드 참조\n\n\n\n파이토치로 미분하기\n\n파이토치로 미분해서 위 데이터 세개에 대한 역전파가 정확한지 확인\n\n\n#                              종속변수, 독립변수, 종속변수와 곱해지는 상위 그래디언트 \ndW_torch = torch.autograd.grad(C_torch, W_torch, torch.tensor(1, dtype=torch.double), \n                               retain_graph=True)[0]\ndW_torch\n\ntensor([[ 2.8791e-03,  7.9160e-03,  1.2443e-02],\n        [ 1.8509e-05,  1.5168e-06,  4.1280e-05],\n        [-4.9747e-02,  6.0737e-03, -4.9872e-02]], dtype=torch.float64)\n\n\n\n파이토치로 미분한 결과와 루프를 돌리면서 구한 결과, 자동미분으로 구한 결과를 모두 비교하면 동일함을 확인할 수 있음"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#자동미분으로-신경망-학습하기",
    "href": "docs/blog/posts/Deep Learning/08-neuralnet_autobp.html#자동미분으로-신경망-학습하기",
    "title": "인공신경망과 역전파",
    "section": "자동미분으로 신경망 학습하기",
    "text": "자동미분으로 신경망 학습하기\n\n마지막으로 역전파를 이용해서 미분계수를 구하는 함수를 만들고 이 함수를 scipy.optimize.fmin_cg에 전달하여 수치 미분한 결과와 동일한 결과가 얻어지는지 확인\n샘플 한개에 대해서 순전파, 역전파 하는 함수 forward_backward를 약간 수정하여 foward_backward2로 작성\n매개변수로 W를 행렬형태로 변환하기 위하 행과 열에 해당하는 숫자 k, l을 받는 것만 다르고 나머지 코드는 모두 동일\n\n\ndef forward_backward2(W, X, T, k, l):\n    \"\"\"\n    W: 모든 weight, bias를 가지고 있는 1차원 벡터\n    X: 네트워크의 입력벡터 shape:(N,2)\n    T: 네트워크의 정답벡터 shape:(N,)\n    k, l : weight, bias가 들어있는 행렬의 모양\n    \n    반환 : dW\n    \"\"\"\n    # forward\n    N = X.shape[0]\n    W = W.reshape(k,l)\n    dW = np.zeros_like(W)\n\n    H1 = np.dot(W[:2,1:], X.T)\n    Z1 = H1 + W[:2,0].reshape(-1,1)\n    A1 = logistic(Z1)\n    \n    H2 = np.dot(W[2,1:], A1)\n    Z2 = H2 + W[2,0]\n    A2 = logistic(Z2)\n    \n    C = (1/(2*N)) * ((T-A2)**2).sum()\n    \n    # backward, dA-&gt;dZ-&gt;db-&gt;dW\n    ######################################################################\n    # WRITE YOUR CODE HERE\n    # 위 샘플 3개를 순전파 역전파 하나 코드를 참고하여 데이터 N개 에 대해서\n    # 순전파 역전파되는 코드를 단계별로 완성하세요.\n\n    ###########\n    # dA2 ~ dZ2\n    dA2 = -(T-A2)/N\n    dA2_dZ2 = logistic(Z2)*(1-logistic(Z2))\n    dZ2 = dA2_dZ2 * dA2\n    \n    #############\n    # db2 and dW2\n    db2 = dZ2.sum(axis=0, keepdims=True) #(1,)\n    dW2 = np.dot(dZ2, A1.T) #(2,)\n\n    # ------------------------------------------------------\n\n    ###########\n    # dA1 ~ dZ1\n    dZ2_dA1 = W[2:,1:]\n    dA1 = np.dot(dZ2_dA1.T, dZ2.reshape(1,-1))\n    dA1_dZ1 = logistic(Z1)*(1-logistic(Z1))\n    dZ1 = dA1_dZ1 * dA1\n    \n    #############\n    # db1 and dW1\n    db1 = dZ1.sum(axis=1, keepdims=True) # (2,1)\n    dW1 = np.dot(dZ1, X) # X is A0.T, (2,2)\n    #####################################################################\n    \n    # dW 모양 만들고 벡터로 펴서 리턴\n    dW[:2,[0]] = db1\n    dW[:2,1:] = dW1\n    dW[2,0] = db2\n    dW[2,1:] = dW2\n    \n    return dW.reshape(-1)\n    \n\n\nderivs = forward_backward2(W.reshape(-1), x, t, 3, 3)\nderivs\n\narray([ 2.8791e-03,  7.9160e-03,  1.2443e-02,  1.8509e-05,  1.5168e-06,  4.1280e-05, -4.9747e-02,  6.0737e-03, -4.9872e-02])\n\n\n\n미분하는 함수 forward_backward2와 호출을 동일하게 할 수 있는 목적함수 J2 작성\n\n\n# fmin_cg에서 최적화 시킬 목적함수 f와 도함수 fprime을 동일하게 호출하므로\n# 위 코딩한 forward_bacward2와 헤더를 맞춘 목적함수 J2를 만든다.\ndef J2(W, X, T, k, l):\n    \"\"\"\n    W: 함숫값을 결정하는 변수, 가중치 (9,)\n    X: 주어진 점 데이터 X, X: (N,D)\n    T: 데이터에 대한 클래스 T, 0 또는 1, T: (N,)\n    k, l : weight, bias가 들어있는 행렬의 모양\n    \"\"\"\n    N = X.shape[0]\n    W = W.reshape(k,l)\n    \n    # 네트워크를 포워드 시키고 적당한 로스 함수를 계산하여 되돌림\n    Y = network(X, W)\n    \n    return (1/(2*N)) * ((T-Y)**2).sum()\n    \n\n\n최종적으로 W를 다시 초기화 하고 fmin_cg에 역전파 알고리즘을 사용하는 함수 forward_backward2를 도함수로 제공하여 최적화 수행\n\n\nW\n\narray([[ 0.2763, -1.8546,  0.6239],\n       [ 1.1453,  1.0372,  1.8866],\n       [-0.1117, -0.3621,  0.1487]])\n\n\n\nW_star_bp = optimize.fmin_cg(J2, W, fprime=forward_backward2, \n                          args=(samples, target, 3, 3),  gtol=1e-06)\n\nOptimization terminated successfully.\n         Current function value: 0.000005\n         Iterations: 459\n         Function evaluations: 1851\n         Gradient evaluations: 1851\n\n\n\n결과를 확인해보면 거의 동일하게 학습되는 것을 확인할 수 있음\n\n\nW_star_bp = W_star_bp.reshape(3,3)\nW_star_bp\n\narray([[ 39.3466,  -8.6752, -12.6186],\n       [-26.0899,   9.0534,   3.3043],\n       [ 17.6746, -40.7086,  40.2004]])\n\n\n\nx = np.linspace(0, 5, 200)\nX1, X2 = np.meshgrid(x, x)\n\ndcs_bnd_1_imp_ = lambda x, y: W_star_bp[0, 1]*x + W_star_bp[0, 2]*y + W_star_bp[0, 0] \ndcs_bnd_2_imp_ = lambda x, y: W_star_bp[1, 1]*x + W_star_bp[1, 2]*y + W_star_bp[1, 0] \n\na, b, c = W_star_bp[2, 1], W_star_bp[2, 2], W_star_bp[2, 0] \no = lambda x, y: logistic( a*logistic(dcs_bnd_1_imp_(x, y)) \n                         + b*logistic(dcs_bnd_2_imp_(x, y)) \n                         + c )\n\nfig = plt.figure(figsize=(7,7))\nax = plt.axes()\n\nax.xaxis.set_tick_params(labelsize=18)\nax.yaxis.set_tick_params(labelsize=18)\nax.set_xlabel('$x$', fontsize=25)\nax.set_ylabel('$y$', fontsize=25)\n\npred = o(samples[:,0], samples[:,1])\npred_pos = pred &gt;= 0.5\npred_neg = pred &lt; 0.5\n\nax.contour(X1, X2, o(X1,X2), cmap='gray', levels=[0.5])\nax.contourf(X1, X2, o(X1,X2), cmap=cm2, levels=[-10, 0.5, 10], alpha=0.3)\n\n# for positive samples\nTP = np.logical_and(target==1, pred_pos)\nFN = np.logical_and(target==1, pred_neg)\n\n# for negative samples\nTN = np.logical_and(target==0, pred_neg)\nFP = np.logical_and(target==0, pred_pos)\n\n# True Positive and True Negative\nax.plot(samples[TP,0], samples[TP,1], 'o', \n        markerfacecolor='C1', markeredgecolor='k', markersize=8)\nax.plot(samples[TN,0], samples[TN,1], '^', \n        markerfacecolor='C6', markeredgecolor='k', markersize=8)\n\n# False Positive and False Negative\nax.plot(samples[FN,0], samples[FN,1], 'o', \n        markerfacecolor='white', markeredgecolor='k', \n        markeredgewidth=2, markersize=15)\nax.plot(samples[FP,0], samples[FP,1], '^', \n        markerfacecolor='white', markeredgecolor='k', \n        markeredgewidth=2, markersize=15)\n\nax.set_ylim(0,5)\nax.set_xlim(0,5)\n\nplt.show()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n비즈니스 운영: 고객 관리, 재고 관리, 주문 처리 등 다양한 비즈니스 프로세스를 지원합니다. 웹 애플리케이션: 사용자 정보, 콘텐츠 관리, 트랜잭션 데이터 등을 저장하고 관리합니다. 분석 및 보고: 데이터 웨어하우스를 통해 대규모 데이터 분석 및 비즈니스 인텔리전스 활동을 지원합니다.\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#요구사항-수집-및-분석",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#요구사항-수집-및-분석",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "사용자의 요구사항을 수집하고 분석하여 어떤 데이터가 필요한지, 어떤 데이터 관계가 있는지를 이해한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#개념적-데이터-모델링-conceptual-data-modeling",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#개념적-데이터-모델링-conceptual-data-modeling",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "데이터베이스에 저장될 데이터의 개념적 구조를 정의한다.\n개념적 모델은 보통 ERD(Entity-Relationship Diagram)와 같은 도구를 사용하여 엔티티(entity), 속성(attribute), 관계(relationship)를 시각적으로 표현한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#물리적-데이터-모델링-physical-data-modeling",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#물리적-데이터-모델링-physical-data-modeling",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "논리적 모델을 실제 데이터베이스 관리 시스템(DBMS)에서 구현 가능한 물리적 구조로 변환한다.\n테이블, 인덱스, 파티션 등 데이터베이스의 물리적 요소를 설계한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#데이터베이스-구현-및-관리",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#데이터베이스-구현-및-관리",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "물리적 모델을 실제 데이터베이스로 구현하고, 이를 유지 보수하며, 성능을 최적화한다.\n\n앞으로, Data Modeling의 단계를 구체적으로 이해하여 실무에 적용할 수 있는 염감을 얻도록 해보자."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#논리적-데이터-모델링-logical-data-modeling",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#논리적-데이터-모델링-logical-data-modeling",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "개념적 모델을 기반으로 데이터베이스의 논리적 구조를 정의한다.\n이 단계에서는 데이터 타입, 관계, 키(key) 등을 포함한 데이터베이스 스키마를 설계한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#구조화된-데이터",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#구조화된-데이터",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "데이터베이스는 데이터를 표(table), 행(row), 열(column)과 같은 형태로 조직하여 구조화된 방식으로 저장한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#데이터-무결성",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#데이터-무결성",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "데이터베이스는 데이터를 정확하고 일관되게 유지하기 위해 다양한 무결성 제약 조건을 적용한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#데이터-접근성",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#데이터-접근성",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "데이터베이스 관리 시스템(DBMS)은 사용자가 데이터를 쉽게 접근하고 검색할 수 있도록 다양한 쿼리 언어(예: SQL)를 제공한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#보안",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#보안",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "데이터베이스는 데이터에 대한 접근 권한을 제어하여 데이터의 보안을 유지한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#동시성-다중-사용자-지원",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#동시성-다중-사용자-지원",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "데이터베이스는 여러 사용자가 동시에 데이터를 접근하고 수정할 수 있도록 지원한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#데이터베이스가-필요한-경우",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#데이터베이스가-필요한-경우",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "수천에서 수백만 건 이상의 데이터를 효율적으로 저장하고 검색해야 할 때 데이터베이스가 필요하다.\n예시: 대규모 전자상거래 웹사이트의 상품 목록, 고객 정보, 주문 내역 관리.\n\n\n\n\n\n데이터를 여러 사용자가 동시에 접근하고 수정하는 환경에서 데이터의 일관성과 무결성을 유지해야 할 때 데이터베이스가 필요하다\n예시: 은행 시스템에서 계좌 거래 내역 관리.\n\n\n\n\n\n데이터베이스 시스템은 이를 효율적으로 수행할 수 있는 도구와 최적화된 알고리즘을 제공한다.\n예시: 마케팅 데이터를 분석하여 고객 행동 패턴을 추출하는 경우.\n\n\n\n\n\n민감한 데이터를 저장하고, 사용자별 접근 권한을 관리해야 할 때 데이터베이스가 필요하다.\n예시: 의료 기록 시스템에서 환자 정보 관리.\n\n\n\n\n\n데이터 손실에 대비하여 정기적인 백업과 복구 기능이 필요할 때 데이터베이스가 유용하다.\n예시: 기업의 중요한 문서와 기록 보관."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#데이터베이스가-필요하지-않은-경우",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#데이터베이스가-필요하지-않은-경우",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "사실, 위의 데이터베이스가 필요한 경우에 해당되지 않는 경우 소프트웨어 알고리즘만으로 왠만하면 처리가 가능한 경우\n\n\n\n간단한 게임의 점수 기록\n소규모 개인 프로젝트 데이터 관리.\n전자식 개폐장치의 비밀번호 관리\n\nData가 필요하지만 입력받은 번호를 저장해놔야 정답 비밀번호와 비교 가능 하지만 이것 역시 알고리즘 프로그램 회로로 대체 가능. 한명이 한번에 번호를 눌러 정답과 비교. DB가 필요하지 않음\n\n\n\n\n\n\n개인용 메모 애플리케이션.\n\n\n\n\n\n설정 파일이나 캐시 데이터 저장.\n\n\n\n\n\n실시간 데이터 처리 애플리케이션에서의 중간 계산 결과\n신호등의 램프 제어: 소규모 Data 필요(신호의 상태를 기억하고 있어야 다음 신호 상태를 결정할 수 있음), 하지만 알고리즘으로 관리할 수 있는 프로그램 회로가 기능 대체 가능. 실질적으론 DB가 필요하지 않음\n\n\n\n\n\n데이터 보안이 큰 우려사항이 아니며, 기본적인 접근 제어만 필요한 경우.\n공개된 데이터를 사용하는 단순 웹 크롤러."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#활용-예시",
    "href": "docs/blog/posts/Engineering/data_modeling/conceptual_data_modeling.html#활용-예시",
    "title": "Conceptual Data Modeling",
    "section": "",
    "text": "비즈니스 운영: 고객 관리, 재고 관리, 주문 처리 등 다양한 비즈니스 프로세스를 지원합니다. 웹 애플리케이션: 사용자 정보, 콘텐츠 관리, 트랜잭션 데이터 등을 저장하고 관리합니다. 분석 및 보고: 데이터 웨어하우스를 통해 대규모 데이터 분석 및 비즈니스 인텔리전스 활동을 지원합니다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/introduction.html",
    "href": "docs/blog/posts/Engineering/data_modeling/introduction.html",
    "title": "Introduction to Data Modeling",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n데이터 플랫폼은 data를 기반으로 플랫폼 생태계를 관리를 가능하게 하며 유저들의 needs관리와 경향 분석을 자동화할 수 있다. 구체적으로, 기업에겐 데이터를 수집, 저장, 처리, 분석 및 배포하여 비즈니스 의사결정 지원, 운영 효율성 향상, 고객 경험 개선, 비용 절감 및 혁신을 촉진하는 통합 시스템으로서 중요한 전략적 자산이 될 수 있다.\n데이터 모델링은 이러한 플랫폼의 성공적인 구현과 운영에 필수적이며, 데이터를 효율적으로 구조화하고 관리하여 데이터의 무결성, 일관성 및 성능을 보장한다. 따라서 데이터 모델링과 데이터 플랫폼은 상호 보완적으로 작용하여 데이터 중심의 비즈니스 환경을 구축하고 기업의 경쟁력을 강화할 수 있다.\n많은 한국의 기업은 미국의 기업보다 data platform의 이해와 도입이 한 박자 늦지만 data platform구축에 많은 노력을 기울이고 있다. 지금 다니고 있는 회사도 한창 data governance와 통계 플랫폼 구축에 힘을 쏟고 있지만 부서와의 의사소통, 체계, 문서 및 지식 부족으로 인해 상당한 난항을 겪고 있다. 이 블로깅을 통해 지식을 체계화하여 실무적 도움과 의사 소통에 도움이 되었으면 좋겠다.\n\n\n\n\n데이터 모델링(data modeling)은 데이터를 구조화하고 체계적으로 조직화하는 과정이다.\n\n즉, 데이터베이스 시스템 (Database System)에서 데이터를 어떻게 저장하고, 접근하며, 사용할지를 정의하는 데 중점을 둔다.\n\n나는 데이터 모델링이 데이터베이스 설계를 포함하는 상위 개념으로 이해하고 있다.\n데이터 모델링이 데이터의 구조와 관계를 정의하는 전체 과정을 포괄한다면\n데이터 모델링에서 얻은 정보를 바탕으로 데이터베이스 설계를 하여 데이터베이스 시스템의 논리적 및 물리적 구현에 중점을 둔다.\nData modeling과 DB design은 엄밀히 말하면 다르다곤 하지만 실무자들이나 많은 사람들은 혼용하고 있는 것 같아 보인다.\n\n\n\n\n\n\n\n사용자의 요구사항을 수집하고 분석하여 어떤 데이터가 필요한지, 어떤 데이터 관계가 있는지를 이해한다.\n\n\n\n\n\n데이터베이스에 저장될 데이터의 개념적 구조를 정의한다.\n개념적 모델은 보통 ERD(Entity-Relationship Diagram)와 같은 도구를 사용하여 엔티티(entity), 속성(attribute), 관계(relationship)를 시각적으로 표현한다.\n\n\n\n\n\n개념적 모델을 기반으로 데이터베이스의 논리적 구조를 정의한다.\n이 단계에서는 데이터 타입, 관계, 키(key) 등을 포함한 데이터베이스 스키마를 설계한다.\n\n\n\n\n\n논리적 모델을 실제 데이터베이스 관리 시스템(DBMS)에서 구현 가능한 물리적 구조로 변환한다.\n테이블, 인덱스, 파티션 등 데이터베이스의 물리적 요소를 설계한다.\n\n\n\n\n\n물리적 모델을 실제 데이터베이스로 구현하고, 이를 유지 보수하며, 성능을 최적화한다.\n\n앞으로, Data Modeling의 단계를 구체적으로 이해하여 실무에 적용할 수 있는 염감을 얻도록 해보자.\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/introduction.html#요구사항-수집-및-분석",
    "href": "docs/blog/posts/Engineering/data_modeling/introduction.html#요구사항-수집-및-분석",
    "title": "Introduction to Data Modeling",
    "section": "",
    "text": "사용자의 요구사항을 수집하고 분석하여 어떤 데이터가 필요한지, 어떤 데이터 관계가 있는지를 이해한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/introduction.html#개념적-데이터-모델링-conceptual-data-modeling",
    "href": "docs/blog/posts/Engineering/data_modeling/introduction.html#개념적-데이터-모델링-conceptual-data-modeling",
    "title": "Introduction to Data Modeling",
    "section": "",
    "text": "데이터베이스에 저장될 데이터의 개념적 구조를 정의한다.\n개념적 모델은 보통 ERD(Entity-Relationship Diagram)와 같은 도구를 사용하여 엔티티(entity), 속성(attribute), 관계(relationship)를 시각적으로 표현한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/introduction.html#논리적-데이터-모델링-logical-data-modeling",
    "href": "docs/blog/posts/Engineering/data_modeling/introduction.html#논리적-데이터-모델링-logical-data-modeling",
    "title": "Introduction to Data Modeling",
    "section": "",
    "text": "개념적 모델을 기반으로 데이터베이스의 논리적 구조를 정의한다.\n이 단계에서는 데이터 타입, 관계, 키(key) 등을 포함한 데이터베이스 스키마를 설계한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/introduction.html#물리적-데이터-모델링-physical-data-modeling",
    "href": "docs/blog/posts/Engineering/data_modeling/introduction.html#물리적-데이터-모델링-physical-data-modeling",
    "title": "Introduction to Data Modeling",
    "section": "",
    "text": "논리적 모델을 실제 데이터베이스 관리 시스템(DBMS)에서 구현 가능한 물리적 구조로 변환한다.\n테이블, 인덱스, 파티션 등 데이터베이스의 물리적 요소를 설계한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/introduction.html#데이터베이스-구현-및-관리",
    "href": "docs/blog/posts/Engineering/data_modeling/introduction.html#데이터베이스-구현-및-관리",
    "title": "Introduction to Data Modeling",
    "section": "",
    "text": "물리적 모델을 실제 데이터베이스로 구현하고, 이를 유지 보수하며, 성능을 최적화한다.\n\n앞으로, Data Modeling의 단계를 구체적으로 이해하여 실무에 적용할 수 있는 염감을 얻도록 해보자."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/database.html",
    "href": "docs/blog/posts/Engineering/data_modeling/database.html",
    "title": "Database의 기능",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n데이터베이스(database)는 구조화된 데이터의 집합체 (데이터 저장소)\n특정 목적을 위해 체계적으로 저장되고 관리되는 데이터의 모임이다.\n데이터베이스는 데이터를 효율적으로 저장하고 검색할 수 있도록 설계되었으며,\n다양한 응용 프로그램과 시스템에서 데이터를 쉽게 접근하고 조작할 수 있도록 지원하는 역할을 한다.\n\n\n\n\n\n\n데이터베이스는 데이터를 표(table), 행(row), 열(column)과 같은 형태로 조직하여 구조화된 방식으로 저장한다.\n\n\n\n데이터베이스는 데이터를 정확하고 일관되게 유지하기 위해 다양한 무결성 제약 조건을 적용한다.\n\n\n\n데이터베이스 관리 시스템(DBMS)은 사용자가 데이터를 쉽게 접근하고 검색할 수 있도록 다양한 쿼리 언어(예: SQL)를 제공한다.\n\n\n\n데이터베이스는 데이터에 대한 접근 권한을 제어하여 데이터의 보안을 유지한다.\n\n\n\n데이터베이스는 여러 사용자가 동시에 데이터를 접근하고 수정할 수 있도록 지원한다.\n\n\n\n\n\n\n\n\n\n수천에서 수백만 건 이상의 데이터를 효율적으로 저장하고 검색해야 할 때 데이터베이스가 필요하다.\n예시: 대규모 전자상거래 웹사이트의 상품 목록, 고객 정보, 주문 내역 관리.\n\n\n\n\n\n데이터를 여러 사용자가 동시에 접근하고 수정하는 환경에서 데이터의 일관성과 무결성을 유지해야 할 때 데이터베이스가 필요하다\n예시: 은행 시스템에서 계좌 거래 내역 관리.\n\n\n\n\n\n데이터베이스 시스템은 이를 효율적으로 수행할 수 있는 도구와 최적화된 알고리즘을 제공한다.\n예시: 마케팅 데이터를 분석하여 고객 행동 패턴을 추출하는 경우.\n\n\n\n\n\n민감한 데이터를 저장하고, 사용자별 접근 권한을 관리해야 할 때 데이터베이스가 필요하다.\n예시: 의료 기록 시스템에서 환자 정보 관리.\n\n\n\n\n\n데이터 손실에 대비하여 정기적인 백업과 복구 기능이 필요할 때 데이터베이스가 유용하다.\n예시: 기업의 중요한 문서와 기록 보관.\n\n\n\n\n\n사실, 위의 데이터베이스가 필요한 경우에 해당되지 않는 경우 소프트웨어 알고리즘만으로 왠만하면 처리가 가능한 경우\n\n\n\n간단한 게임의 점수 기록\n소규모 개인 프로젝트 데이터 관리.\n전자식 개폐장치의 비밀번호 관리\n\nData가 필요하지만 입력받은 번호를 저장해놔야 정답 비밀번호와 비교 가능 하지만 이것 역시 알고리즘 프로그램 회로로 대체 가능. 한명이 한번에 번호를 눌러 정답과 비교. DB가 필요하지 않음\n\n\n\n\n\n\n개인용 메모 애플리케이션.\n\n\n\n\n\n설정 파일이나 캐시 데이터 저장.\n\n\n\n\n\n실시간 데이터 처리 애플리케이션에서의 중간 계산 결과\n신호등의 램프 제어: 소규모 Data 필요(신호의 상태를 기억하고 있어야 다음 신호 상태를 결정할 수 있음), 하지만 알고리즘으로 관리할 수 있는 프로그램 회로가 기능 대체 가능. 실질적으론 DB가 필요하지 않음\n\n\n\n\n\n데이터 보안이 큰 우려사항이 아니며, 기본적인 접근 제어만 필요한 경우.\n공개된 데이터를 사용하는 단순 웹 크롤러.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/database.html#구조화된-데이터",
    "href": "docs/blog/posts/Engineering/data_modeling/database.html#구조화된-데이터",
    "title": "Database의 기능",
    "section": "",
    "text": "데이터베이스는 데이터를 표(table), 행(row), 열(column)과 같은 형태로 조직하여 구조화된 방식으로 저장한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/database.html#데이터-무결성",
    "href": "docs/blog/posts/Engineering/data_modeling/database.html#데이터-무결성",
    "title": "Database의 기능",
    "section": "",
    "text": "데이터베이스는 데이터를 정확하고 일관되게 유지하기 위해 다양한 무결성 제약 조건을 적용한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/database.html#데이터-접근성",
    "href": "docs/blog/posts/Engineering/data_modeling/database.html#데이터-접근성",
    "title": "Database의 기능",
    "section": "",
    "text": "데이터베이스 관리 시스템(DBMS)은 사용자가 데이터를 쉽게 접근하고 검색할 수 있도록 다양한 쿼리 언어(예: SQL)를 제공한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/database.html#보안",
    "href": "docs/blog/posts/Engineering/data_modeling/database.html#보안",
    "title": "Database의 기능",
    "section": "",
    "text": "데이터베이스는 데이터에 대한 접근 권한을 제어하여 데이터의 보안을 유지한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/database.html#동시성-다중-사용자-지원",
    "href": "docs/blog/posts/Engineering/data_modeling/database.html#동시성-다중-사용자-지원",
    "title": "Database의 기능",
    "section": "",
    "text": "데이터베이스는 여러 사용자가 동시에 데이터를 접근하고 수정할 수 있도록 지원한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/database.html#데이터베이스가-필요한-경우",
    "href": "docs/blog/posts/Engineering/data_modeling/database.html#데이터베이스가-필요한-경우",
    "title": "Database의 기능",
    "section": "",
    "text": "수천에서 수백만 건 이상의 데이터를 효율적으로 저장하고 검색해야 할 때 데이터베이스가 필요하다.\n예시: 대규모 전자상거래 웹사이트의 상품 목록, 고객 정보, 주문 내역 관리.\n\n\n\n\n\n데이터를 여러 사용자가 동시에 접근하고 수정하는 환경에서 데이터의 일관성과 무결성을 유지해야 할 때 데이터베이스가 필요하다\n예시: 은행 시스템에서 계좌 거래 내역 관리.\n\n\n\n\n\n데이터베이스 시스템은 이를 효율적으로 수행할 수 있는 도구와 최적화된 알고리즘을 제공한다.\n예시: 마케팅 데이터를 분석하여 고객 행동 패턴을 추출하는 경우.\n\n\n\n\n\n민감한 데이터를 저장하고, 사용자별 접근 권한을 관리해야 할 때 데이터베이스가 필요하다.\n예시: 의료 기록 시스템에서 환자 정보 관리.\n\n\n\n\n\n데이터 손실에 대비하여 정기적인 백업과 복구 기능이 필요할 때 데이터베이스가 유용하다.\n예시: 기업의 중요한 문서와 기록 보관."
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_modeling/database.html#데이터베이스가-필요하지-않은-경우",
    "href": "docs/blog/posts/Engineering/data_modeling/database.html#데이터베이스가-필요하지-않은-경우",
    "title": "Database의 기능",
    "section": "",
    "text": "사실, 위의 데이터베이스가 필요한 경우에 해당되지 않는 경우 소프트웨어 알고리즘만으로 왠만하면 처리가 가능한 경우\n\n\n\n간단한 게임의 점수 기록\n소규모 개인 프로젝트 데이터 관리.\n전자식 개폐장치의 비밀번호 관리\n\nData가 필요하지만 입력받은 번호를 저장해놔야 정답 비밀번호와 비교 가능 하지만 이것 역시 알고리즘 프로그램 회로로 대체 가능. 한명이 한번에 번호를 눌러 정답과 비교. DB가 필요하지 않음\n\n\n\n\n\n\n개인용 메모 애플리케이션.\n\n\n\n\n\n설정 파일이나 캐시 데이터 저장.\n\n\n\n\n\n실시간 데이터 처리 애플리케이션에서의 중간 계산 결과\n신호등의 램프 제어: 소규모 Data 필요(신호의 상태를 기억하고 있어야 다음 신호 상태를 결정할 수 있음), 하지만 알고리즘으로 관리할 수 있는 프로그램 회로가 기능 대체 가능. 실질적으론 DB가 필요하지 않음\n\n\n\n\n\n데이터 보안이 큰 우려사항이 아니며, 기본적인 접근 제어만 필요한 경우.\n공개된 데이터를 사용하는 단순 웹 크롤러."
  },
  {
    "objectID": "docs/blog/posts/Engineering/VScode/01.vscode_install.html",
    "href": "docs/blog/posts/Engineering/VScode/01.vscode_install.html",
    "title": "VS code Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n1 VScode Installation\n\nVScode란?\n\nMicrosoft사에서 2015년에 제작, 다양한 언어 개발을 돕는 IDE tool\nVisual Studio 라는 IDE 툴과는 엄연히 다른 툴\n\nActions\n\nVScode 다운로드\n\n설치 마법사에서 추가 작업 선택란에 code로 열기 작업을 windows탐색기 파일의 상황에 맞는 메뉴에 추가 선택할 것. programming file을 열때 VScode가 디폴트가 되도록함\n\nVScode 설치, 파이썬 확장팩 설치\n프로젝트 생성, 파이썬 가상환경 설정\n\nVScode가 file이나 directory단위로 관리하는 IDE tool이라 프로젝트 생성 개념이 없음\nwindows에 프로젝트 directory하나 만들고 VScode에서 open folder로 열면 그 folder를 최상위 folder로 인식 (project 생성됨)\n\npython interpreter 설정\n\nVScode &gt; Terminal &gt; New Terminal &gt; python version 확인\n\n\n파이썬 가상환경\n\n라이브러리 버전 충돌 방지를 위해 설치/사용되는 파이썬 인터프리터 환경을 격리시키는 기술\n파이썬은 라이브러리 설치 시점에 따라서도 설치되는 버전이 상이한 경우가 많음\n\n\n\n\n가상 환경의 필요성\n\n\n\npython을 global 환경에 설치할 경우 위의 그림처럼 C,D프로젝트가 동시에 진행될 때 둘 중하나의 library version이 차이가 나면 old version의 library 로 진행되는 프로젝트는 에러가 발생함\n\n2개의 다른 프로젝트가 같은 python interpreter를 바라보고 library를 설치하기 때문에 종속성 문제가 생김 (library 충돌 발생)\n그래서 다른 가상환경 venv안에 다른 프로젝트를 할당해서 독립적으로 프로젝트를 진행하는게 일반적임\n\npython 가상환경 만들기\n\nconda로 만들 경우 conda 설치 후 만들면 됨. 설치 링크\npython에 있는 가상환경 생성 기능으로 만들 경우 python -m airflow ./venv 실행\n\n./venv directory에 python 설치하고 version 관리하겠다는 의미\n\n\nVScode가 python 가상환경 참조하도록 설정\n\nhelp&gt;show all commands or ctrl+shift+p 누른후 interpreter 입력하여 가상환경에 있는 python 클릭\n\nterminal 에서 가상환경 잘 잡혔는지 확인\n\n\n\n\n\n\n\n\n\n\n\n2 Go to Blog Content List\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Git/01.git_install.html",
    "href": "docs/blog/posts/Engineering/Git/01.git_install.html",
    "title": "Git Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n1 Git\n\nversion 관리 솔루션\n\nprogramming시 version관리 하기 위해 만들어진 시스템\n중앙형 솔루션\n\n다수의 user가 원격에 있는 repository에 commit 모든 update는 중앙 repository에 최신화 됨.\n만약, 다수의 유저가 동일한 파일의 동일한 내용을 최신화시켜 commit하면 중앙 repository에 충돌이 발생\n대표적인 예로 SVN이 있음\n\n분산형 솔루현\n\n다수의 user가 본인들의 local repository에 commit 모든 update는 local repository에 최신화 됨.\n만약, 다수의 유저가 동일한 파일의 동일한 내용을 최신화시켜 commit하면 local repository에 최신화시키기 떄문에 충돌이 발생하지 않음\n한 user가 코드 공유를 하기 위해 원격 repository에 commited updates를 push\n다른 user가 최신화된 원격 중앙 repository로부터 updates를 pull 받아 자신의 local repository를 최신화 시킴\n만약 다수의 user가 같은 file의 같은 부분을 최신화 시켰다고 해도 local repository에서 충돌이 발생했기 때문에 원격 repository는 온전함\n원격 repository는 코드 공유를 위한 저장소의 역할을 할 뿐이다.\n대표적인 예로 Git이 있음\n\n\nGithub Repository 생성\n\nGit vs Github\n\nGit: 오픈소스 분산형 버전관리 solution, 시스템 또는 프로그램\nGithub: Git을 기반으로 소스를 공유할 수 있도록 만들어진 웹 서비스\n\nGit 레파지토리: Github 에서 생성(https://github.com)\n\nGit Push & Pull\n\n\n\nGit Workflow\n\n\nGit 설치 링크\nGit Repository 생성\n\ngithub에서 new repository 생성: remote 환경에 repository 생성\ngit init: local PC(환경)에 respository 생성. remote repository와 연동시킬 local folder를 만들고 command창에서 해당 위치로 이동후 실행\ngit status: local repository의 git status 확인\ntouch .gitignore: .gitignore파일이 생성되고 그 안에 commit되지 않기를 바라는 파일명 및 확장자 명을 기입한다.\ngit add .gitignore: stage .gitignore(git에 .gitignore가 최신화되었다는 것을 알려줌)\ngit commit -m \"message\": local repository에 변경사항을 기록을 함\n\n최초 commit 시 email과 user name을 등록시켜야할 수 있다.\ngit config --global user.email \"abd@sdfsd.com\" 입력\ngit config --global user.name \"name\" 입력\n\ngit log: commit을 하게 되면 log에 남음\ngit branch -M main: branch는 local repository의 version. branch를 여러개 둘 수 있는데 각 각 독립적으로 움직인다. 보통 운영용 branch와 개발용 branch를 나눠서 repository를 관리한다.\n\n개발용 branch로 개발 및 코드 공유를 하고 이상이 없다고 판단될 경우 운영용 branch에 merge를 한다.\nlocal repository에 merge된 운영용 branch를 최종적으로 remote repository의 운영용 branch에 push한다.\n\ngit remote add origin https://github.com/kmink3225/airflow.git: local repository와 remote repository를 연동 시켜 주는 것.\ngit push -u origin master\n\nWSL환경에서 remote git reposiotry pull 할 것\n\ngit clone {repository address}: remote git repository를 그대로 복제해서 내려 받음\n\nWSL환경에서 처음 commit 하면 다음과 같은 에러 뜸\n\n  Author identity unknown\n\n  *** Please tell me who you are.\n\n  Run\n\n    git config --global user.email \"you@example.com\"\n    git config --global user.name \"Your Name\"\n\n  to set your account's default identity.\n  Omit --global to set the identity only in this repository.\n\n  fatal: empty ident name (for &lt;kmkim@K100230201051.corp.seegene.com&gt;) not allowed\ngit config --global user.email \"you@example.com\" git config --global user.name \"Your Name\"\n그대로 실행해 주면 됨\n\nWSL환경에서 처음 push 하면 다음과 같은 창 뜸\n\nUsername for 'https://github.com'\nPassword for 'https://kmink3225@github.com':\n\ngithub 게정 비밀 번호 입력하면 다음과 같은 메세지 뜸 (보안상 취약점이 발견되서 암호 인증 방식은 제거됨) remote: Support for password authentication was removed on August 13, 2021.\ngithub 인증 방식 2가지\n\ngithub token 발행\n\ngithub login 후 profile&gt;&gt;settings&gt;&gt;Developer Settings&gt;&gt;Personal access tokens&gt;&gt;token(classic)&gt;&gt;generate personal access tokens&gt;&gt;note:‘some message’&gt;&gt;expiration:no expiration&gt;&gt;select scopes:repo box check&gt;&gt;github token local에다가 저장 (나중에 다시 못봄)\n\npublic key & private key 발행\n\n\n\n\n\n\n\n\n\n\n\n2 Go to Blog Content List\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/18.chatgpt.html",
    "href": "docs/blog/posts/Engineering/airflow/18.chatgpt.html",
    "title": "Template Variabler",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n문서(파일)에서 특정 양식으로 작성된 값을 런타임시 실제 값으로 치환해주는 처리 엔진\n템플릿 엔진은 여러 솔루션이 존재하며 그 중 Jinja 템플릿은 파이썬 언어에서 사용하는 엔진\nfrom jinja2 import Template\n\ntemplate = Template('my name is {{name}}')\nnew_template = template.render('name=hjkim')\nprint(new_template)\nJinja 템플릿, 어디서 쓰이나?\n\n파이썬 기반 웹 프레임워크인 Flask, Django에서 주로 사용 (주로 HTML 템플릿 저장 후 화면에 보여질 때 실제 값으로 변환해서 출력)\nSQL작성시에도 활용 가능\n\n\n\n\n\n오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html\n\n\n\n\n\n\n\n\n\nBash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nbash_command (str)\nenv (dict[str, str] | None)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)\n\n\n\n\n\n\n\nDaily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n데이터 관점의 시작일: 2023-02-24 데이터 관점의 종료일: 2023-02-25\n\n\n\n\n예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (과거버전)\n\nex. 2023-02-25 배치일\n\n= data_interval_end\n=\n=\n=\n= next_execution_date (과거버전)\n\n\n\n\n\n\n\n\n\n\nPython 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()\n\n\n\n\n\n\n\n\nMacro 변수의 필요성\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\nDAG 스케줄은 매일 말일에 도는 스케줄인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은데 어떻게 하지? 예를 들어 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되었으면 좋겠어. DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일 이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? data_interval_end 에서 하루 뺀 값이 나와야 하는데…\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta\n\n\nmacros.dateutil\nA reference to the dateutil package\n\n\nmacros.time\nThe standard lib’s time\n\n\nmacros.uuid\nThe standard lib’s uuid\n\n\nmacros.random\nThe standard lib’s random\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리\n\nMacro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함.\n\n\n\n\nfrom datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) # 1월로 변경\nprint(now.replace(month=1)) # 1월로 변경\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기\n\n\n\n\n예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''},\n)\n\n이 부분에 template + macro 활용\n\n\n\n\n\n어떤 파라미터가 Template 변수를 지원할까?\n패러미터\n\npython_callable (Callable | None)\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/python/index.html#airflow.operators.python.PythonOperator\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\")\n+ macros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}',\n'end_date': '{{\n(data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\nmacros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n    start_date = templates_dict.get('start_date') or 'start_date없음'\n    end_date = templates_dict.get('end_date') or 'end_date없음'\n    print(start_date)\n    print(end_date)\n그러나 Python 오퍼레이터에서 굳이 macro를 사용할 필요가 있을까? 날짜 연산을 DAG안에서 직접 할 수 있다면?\n\nmacro 사용\n\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") + macros.dateutil.relativedelta.relativedelta(months=-1,day=1)) | ds }}',\n    'end_date': '{{ (data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\n    macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n        start_date = templates_dict.get('start_date') or 'start_date없음'\n        end_date = templates_dict.get('end_date') or 'end_date없음'\n        print(start_date)\n        print(end_date)\n\n@task(task_id='task_direct_calc')\ndef get_datetime_calc(**kwargs):\n    from dateutil.relativedelta import relativedelta\n    data_interval_end = kwargs['data_interval_end']\n\n직접 연산\n\nprev_month_day_first = data_interval_end.in_timezone('Asia/Seoul') + relativedelta(months=-1, day=1)\nprev_month_day_last = data_interval_end.in_timezone('Asia/Seoul').replace(day=1) + relativedelta(days=-1)\nprint(prev_month_day_first.strftime('%Y-%m-%d'))\nprint(prev_month_day_last.strftime('%Y-%m-%d'))\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#airflow에서-사용법",
    "href": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#airflow에서-사용법",
    "title": "Template Variabler",
    "section": "",
    "text": "오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#bashoperator",
    "href": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#bashoperator",
    "title": "Template Variabler",
    "section": "",
    "text": "Bash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nbash_command (str)\nenv (dict[str, str] | None)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#데이터-추출-예시",
    "href": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#데이터-추출-예시",
    "title": "Template Variabler",
    "section": "",
    "text": "Daily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n데이터 관점의 시작일: 2023-02-24 데이터 관점의 종료일: 2023-02-25"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#airflow-날짜-template-변수",
    "href": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#airflow-날짜-template-변수",
    "title": "Template Variabler",
    "section": "",
    "text": "예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (과거버전)\n\nex. 2023-02-25 배치일\n\n= data_interval_end\n=\n=\n=\n= next_execution_date (과거버전)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#python-오퍼레이터에서-template-변수-사용",
    "href": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#python-오퍼레이터에서-template-변수-사용",
    "title": "Template Variabler",
    "section": "",
    "text": "Python 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#macro-변수의-이해",
    "href": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#macro-변수의-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "Macro 변수의 필요성\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\nDAG 스케줄은 매일 말일에 도는 스케줄인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은데 어떻게 하지? 예를 들어 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되었으면 좋겠어. DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일 이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? data_interval_end 에서 하루 뺀 값이 나와야 하는데…\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta\n\n\nmacros.dateutil\nA reference to the dateutil package\n\n\nmacros.time\nThe standard lib’s time\n\n\nmacros.uuid\nThe standard lib’s uuid\n\n\nmacros.random\nThe standard lib’s random\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리\n\nMacro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#파이썬-datetime-dateutil-라이브러리-이해",
    "href": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#파이썬-datetime-dateutil-라이브러리-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "from datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) # 1월로 변경\nprint(now.replace(month=1)) # 1월로 변경\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "href": "docs/blog/posts/Engineering/airflow/18.chatgpt.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "title": "Template Variabler",
    "section": "",
    "text": "예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''},\n)\n\n이 부분에 template + macro 활용"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html",
    "href": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html",
    "title": "Template Variabler",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n문서(파일)에서 특정 양식으로 작성된 값을 런타임시 실제 값으로 치환해주는 처리 엔진\n템플릿 엔진은 여러 솔루션이 존재하며 그 중 Jinja 템플릿은 파이썬 언어에서 사용하는 엔진\nfrom jinja2 import Template\n\ntemplate = Template('my name is {{name}}')\nnew_template = template.render('name=hjkim')\nprint(new_template)\nJinja 템플릿, 어디서 쓰이나?\n\n파이썬 기반 웹 프레임워크인 Flask, Django에서 주로 사용 (주로 HTML 템플릿 저장 후 화면에 보여질 때 실제 값으로 변환해서 출력)\nSQL작성시에도 활용 가능\n\n\n\n\n\n오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html\n\n\n\n\n\n\n\n\n\nBash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nbash_command (str)\nenv (dict[str, str] | None)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)\n\n\n\n\n\n\n\nDaily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n데이터 관점의 시작일: 2023-02-24 데이터 관점의 종료일: 2023-02-25\n\n\n\n\n예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (과거버전)\n\nex. 2023-02-25 배치일\n\n= data_interval_end\n=\n=\n=\n= next_execution_date (과거버전)\n\n\n\n\n\n\n\n\n\n\nPython 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()\n\n\n\n\n\n\n\n\nMacro 변수의 필요성\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\nDAG 스케줄은 매일 말일에 도는 스케줄인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은데 어떻게 하지? 예를 들어 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되었으면 좋겠어. DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일 이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? data_interval_end 에서 하루 뺀 값이 나와야 하는데…\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta\n\n\nmacros.dateutil\nA reference to the dateutil package\n\n\nmacros.time\nThe standard lib’s time\n\n\nmacros.uuid\nThe standard lib’s uuid\n\n\nmacros.random\nThe standard lib’s random\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리\n\nMacro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함.\n\n\n\n\nfrom datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) # 1월로 변경\nprint(now.replace(month=1)) # 1월로 변경\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기\n\n\n\n\n예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''},\n)\n\n이 부분에 template + macro 활용\n\n\n\n\n\n어떤 파라미터가 Template 변수를 지원할까?\n패러미터\n\npython_callable (Callable | None)\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/python/index.html#airflow.operators.python.PythonOperator\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\")\n+ macros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}',\n'end_date': '{{\n(data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\nmacros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n    start_date = templates_dict.get('start_date') or 'start_date없음'\n    end_date = templates_dict.get('end_date') or 'end_date없음'\n    print(start_date)\n    print(end_date)\n그러나 Python 오퍼레이터에서 굳이 macro를 사용할 필요가 있을까? 날짜 연산을 DAG안에서 직접 할 수 있다면?\n\nmacro 사용\n\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") + macros.dateutil.relativedelta.relativedelta(months=-1,day=1)) | ds }}',\n    'end_date': '{{ (data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\n    macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n        start_date = templates_dict.get('start_date') or 'start_date없음'\n        end_date = templates_dict.get('end_date') or 'end_date없음'\n        print(start_date)\n        print(end_date)\n\n@task(task_id='task_direct_calc')\ndef get_datetime_calc(**kwargs):\n    from dateutil.relativedelta import relativedelta\n    data_interval_end = kwargs['data_interval_end']\n\n직접 연산\n\nprev_month_day_first = data_interval_end.in_timezone('Asia/Seoul') + relativedelta(months=-1, day=1)\nprev_month_day_last = data_interval_end.in_timezone('Asia/Seoul').replace(day=1) + relativedelta(days=-1)\nprint(prev_month_day_first.strftime('%Y-%m-%d'))\nprint(prev_month_day_last.strftime('%Y-%m-%d'))\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#airflow에서-사용법",
    "href": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#airflow에서-사용법",
    "title": "Template Variabler",
    "section": "",
    "text": "오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#bashoperator",
    "href": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#bashoperator",
    "title": "Template Variabler",
    "section": "",
    "text": "Bash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nbash_command (str)\nenv (dict[str, str] | None)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#데이터-추출-예시",
    "href": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#데이터-추출-예시",
    "title": "Template Variabler",
    "section": "",
    "text": "Daily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n데이터 관점의 시작일: 2023-02-24 데이터 관점의 종료일: 2023-02-25"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#airflow-날짜-template-변수",
    "href": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#airflow-날짜-template-변수",
    "title": "Template Variabler",
    "section": "",
    "text": "예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (과거버전)\n\nex. 2023-02-25 배치일\n\n= data_interval_end\n=\n=\n=\n= next_execution_date (과거버전)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#python-오퍼레이터에서-template-변수-사용",
    "href": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#python-오퍼레이터에서-template-변수-사용",
    "title": "Template Variabler",
    "section": "",
    "text": "Python 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#macro-변수의-이해",
    "href": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#macro-변수의-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "Macro 변수의 필요성\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\nDAG 스케줄은 매일 말일에 도는 스케줄인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은데 어떻게 하지? 예를 들어 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되었으면 좋겠어. DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일 이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? data_interval_end 에서 하루 뺀 값이 나와야 하는데…\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta\n\n\nmacros.dateutil\nA reference to the dateutil package\n\n\nmacros.time\nThe standard lib’s time\n\n\nmacros.uuid\nThe standard lib’s uuid\n\n\nmacros.random\nThe standard lib’s random\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리\n\nMacro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#파이썬-datetime-dateutil-라이브러리-이해",
    "href": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#파이썬-datetime-dateutil-라이브러리-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "from datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) # 1월로 변경\nprint(now.replace(month=1)) # 1월로 변경\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "href": "docs/blog/posts/Engineering/airflow/16.airflow_architecture.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "title": "Template Variabler",
    "section": "",
    "text": "예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''},\n)\n\n이 부분에 template + macro 활용"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/14.airflow_management.html",
    "href": "docs/blog/posts/Engineering/airflow/14.airflow_management.html",
    "title": "Template Variabler",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n문서(파일)에서 특정 양식으로 작성된 값을 런타임시 실제 값으로 치환해주는 처리 엔진\n템플릿 엔진은 여러 솔루션이 존재하며 그 중 Jinja 템플릿은 파이썬 언어에서 사용하는 엔진\nfrom jinja2 import Template\n\ntemplate = Template('my name is {{name}}')\nnew_template = template.render('name=hjkim')\nprint(new_template)\nJinja 템플릿, 어디서 쓰이나?\n\n파이썬 기반 웹 프레임워크인 Flask, Django에서 주로 사용 (주로 HTML 템플릿 저장 후 화면에 보여질 때 실제 값으로 변환해서 출력)\nSQL작성시에도 활용 가능\n\n\n\n\n\n오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html\n\n\n\n\n\n\n\n\n\nBash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nbash_command (str)\nenv (dict[str, str] | None)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)\n\n\n\n\n\n\n\nDaily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n데이터 관점의 시작일: 2023-02-24 데이터 관점의 종료일: 2023-02-25\n\n\n\n\n예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (과거버전)\n\nex. 2023-02-25 배치일\n\n= data_interval_end\n=\n=\n=\n= next_execution_date (과거버전)\n\n\n\n\n\n\n\n\n\n\nPython 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()\n\n\n\n\n\n\n\n\nMacro 변수의 필요성\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\nDAG 스케줄은 매일 말일에 도는 스케줄인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은데 어떻게 하지? 예를 들어 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되었으면 좋겠어. DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일 이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? data_interval_end 에서 하루 뺀 값이 나와야 하는데…\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta\n\n\nmacros.dateutil\nA reference to the dateutil package\n\n\nmacros.time\nThe standard lib’s time\n\n\nmacros.uuid\nThe standard lib’s uuid\n\n\nmacros.random\nThe standard lib’s random\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리\n\nMacro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함.\n\n\n\n\nfrom datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) # 1월로 변경\nprint(now.replace(month=1)) # 1월로 변경\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기\n\n\n\n\n예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''},\n)\n\n이 부분에 template + macro 활용\n\n\n\n\n\n어떤 파라미터가 Template 변수를 지원할까?\n패러미터\n\npython_callable (Callable | None)\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/python/index.html#airflow.operators.python.PythonOperator\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\")\n+ macros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}',\n'end_date': '{{\n(data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\nmacros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n    start_date = templates_dict.get('start_date') or 'start_date없음'\n    end_date = templates_dict.get('end_date') or 'end_date없음'\n    print(start_date)\n    print(end_date)\n그러나 Python 오퍼레이터에서 굳이 macro를 사용할 필요가 있을까? 날짜 연산을 DAG안에서 직접 할 수 있다면?\n\nmacro 사용\n\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") + macros.dateutil.relativedelta.relativedelta(months=-1,day=1)) | ds }}',\n    'end_date': '{{ (data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\n    macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n        start_date = templates_dict.get('start_date') or 'start_date없음'\n        end_date = templates_dict.get('end_date') or 'end_date없음'\n        print(start_date)\n        print(end_date)\n\n@task(task_id='task_direct_calc')\ndef get_datetime_calc(**kwargs):\n    from dateutil.relativedelta import relativedelta\n    data_interval_end = kwargs['data_interval_end']\n\n직접 연산\n\nprev_month_day_first = data_interval_end.in_timezone('Asia/Seoul') + relativedelta(months=-1, day=1)\nprev_month_day_last = data_interval_end.in_timezone('Asia/Seoul').replace(day=1) + relativedelta(days=-1)\nprint(prev_month_day_first.strftime('%Y-%m-%d'))\nprint(prev_month_day_last.strftime('%Y-%m-%d'))\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#airflow에서-사용법",
    "href": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#airflow에서-사용법",
    "title": "Template Variabler",
    "section": "",
    "text": "오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#bashoperator",
    "href": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#bashoperator",
    "title": "Template Variabler",
    "section": "",
    "text": "Bash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nbash_command (str)\nenv (dict[str, str] | None)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#데이터-추출-예시",
    "href": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#데이터-추출-예시",
    "title": "Template Variabler",
    "section": "",
    "text": "Daily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n데이터 관점의 시작일: 2023-02-24 데이터 관점의 종료일: 2023-02-25"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#airflow-날짜-template-변수",
    "href": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#airflow-날짜-template-변수",
    "title": "Template Variabler",
    "section": "",
    "text": "예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (과거버전)\n\nex. 2023-02-25 배치일\n\n= data_interval_end\n=\n=\n=\n= next_execution_date (과거버전)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#python-오퍼레이터에서-template-변수-사용",
    "href": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#python-오퍼레이터에서-template-변수-사용",
    "title": "Template Variabler",
    "section": "",
    "text": "Python 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#macro-변수의-이해",
    "href": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#macro-변수의-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "Macro 변수의 필요성\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\nDAG 스케줄은 매일 말일에 도는 스케줄인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은데 어떻게 하지? 예를 들어 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되었으면 좋겠어. DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일 이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? data_interval_end 에서 하루 뺀 값이 나와야 하는데…\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta\n\n\nmacros.dateutil\nA reference to the dateutil package\n\n\nmacros.time\nThe standard lib’s time\n\n\nmacros.uuid\nThe standard lib’s uuid\n\n\nmacros.random\nThe standard lib’s random\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리\n\nMacro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#파이썬-datetime-dateutil-라이브러리-이해",
    "href": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#파이썬-datetime-dateutil-라이브러리-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "from datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) # 1월로 변경\nprint(now.replace(month=1)) # 1월로 변경\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "href": "docs/blog/posts/Engineering/airflow/14.airflow_management.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "title": "Template Variabler",
    "section": "",
    "text": "예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''},\n)\n\n이 부분에 template + macro 활용"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/12.dashboarding.html",
    "href": "docs/blog/posts/Engineering/airflow/12.dashboarding.html",
    "title": "Template Variabler",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nR로 대화형 웹을 만들 수 있게 해주는 R의 라이브러리 중 하나\n웹을 띄우기 위해선 세 가지 파일을 작성해야 함.\n\nui.R: 화면에 보여지는 UI에 대한 내용 (UI layout 정의)\nserver.R : ui.R 에서 입력을 받아 처리한 데이터를 ui.R 로 내보내는 내용. (데이터 처리 logic 정의)\nglobal.R: shiny에서 사용되는 전역 객체를 정의. 주로 라이브러리 임포트 등 수행. ui.R, server.R 에서 모두 사용됨\n\n\n\n\n\n\ndocker-compose.yaml 파일 수정하기\n\nservices:\n  rshiny_custom: # custom image 다른 이름 붙여도 상관없음\n    image: rocker/shiny-verse:latest\n    networks:\n      network_custom:\n        ipv4_address: 172.28.0.2\n  ports:\n  - 3838:3838\n\nsudo docker compose up 실행하면 rocker/shiny-verse:latest 이미지 자동으로 다운 받음\nDocker compose 재기동 후 localhost:3838 접속\n\n\n\n\n\n\n\n\nRshiny로 웹을 띄우기 위해선 세 가지 파일을 작성해야 함.\n\nui.R : 화면에 보여지는 UI에 대한 내용\nserver.R : ui.R 에서 입력을 받아 처리한 데이터를 ui.R 로 내보내는 내용\nglobal.R : shiny에서 사용되는 전역 객체를 정의. 주로 라이브러리 임포트 등 수행. ui.R, server.R 에서 모두 사용됨\n\n3가지 파일 작성 후 Rshiny 도커 이미지 Customizing 필요\n\n\n디렉토리 구조 및 파일 위치\n\n\n$home/airflow/custom_image \n\n\nglobal.R 작성\n\nif (!require(shiny)) {install.packages(\"shiny\"); library(shiny)}\n# DBI 와 glue는 DB에서 data 추출시에 사용하는 library\nif (!require(DBI)) {install.packages(\"DBI\"); library(DBI)} \nif (!require(glue)) {install.packages(\"glue\"); library(glue)}\nif (!require(ggplot2)) {install.packages(\"ggplot2\"); library(ggplot2)}\nsource(\"./ui.R\", local = TRUE)\nsource(\"./server.R\", local = TRUE)\nshinyApp(ui, server)\n\nui.R 작성\n\nui &lt;- fluidPage(\n    tags$h1(\"corona19\"),\n    sidebarPanel(\n    # dateRangeInput: date 구간을 from/to 형식(form)으로 입력할 수 있는 형태\n        dateRangeInput(\"dates\", # 형식(form) id 값\n                   \"Date range\", # 형식이 화면에 표시되는 이름\n                   start = as.Date(\"2023-01-01\"), # 화면에 기본적으로 보여질 날짜\n                   end = Sys.Date()),\n        br(),\n        br()\n    ),\n  # form을 여러개 나열하고 싶으면 sidebarPanel() 어러개 넣으면 됨\n\n  # mainPanel 역시 집어 넣을 수 있는 객체 (형식 또는 form)가 정해져 있음\n  # 대표적으로 자주 사용되는 객체: plotOUtput,imageOutput, tableOutput 등\n    mainPanel(plotOutput(\"daily_confirmed\"), plotOutput(\"total_confirmed\"))\n  # daily_confirmed: 1번째 plotOutput의 id\n  # total_confirmed: 2번째 plotOutput의 id\n)\n\n\nserver.R 작성\n\nserver &lt;- function(input, output){ # 무조건 오는 내용\n    selected_data &lt;- reactive({\n            # Connect to the DB\n            conn &lt;- dbConnect(\n                  RPostgres::Postgres(),\n                  dbname = \"kmkim\",\n                  host = \"172.28.0.3\",\n                  port = \"5432\",\n                  user = \"kmkim\",\n                  password = \"kmkim\"\n        )\n            # Get the data\n            corona &lt;- dbGetQuery(conn, glue(\"SELECT \n                         to_date(substring(\\\"S_DT\\\",1,10),'YYYY.MM.DD') as s_dt\n                            ,\\\"N_HJ\\\"::float as n_hj\n                        ,\\\"T_HJ\\\" as t_hj\n                        FROM \\\"TbCorona19CountStatus_bulk2\\\" \n                        WHERE to_date(substring(\\\"S_DT\\\",1,10),'YYYY.MM.DD') BETWEEN '{format(input$dates[1])}' AND '{format(input$dates[2])}'\"))\n            # Disconnect from the DB\n            dbDisconnect(conn)\n            # Convert to data.frame\n            data.frame(corona)\n    })\n    \n    output$daily_confirmed &lt;- renderPlot({\n        ggplot(data=selected_data(), aes(x=s_dt, y=n_hj)) + \n            geom_line(color='blue', linewidth = 1) + \n            geom_point(color='red') + \n            geom_smooth(method='lm') +\n            ggtitle(\"Daily confirmed cases\") +\n            labs(x='Date',y='Daily confirmed cases')\n    })\n    output$total_confirmed &lt;- renderPlot({\n               ggplot(data=selected_data(), aes(x=s_dt, y=t_hj)) +\n               geom_line(color='blue', linewidth = 1) +\n               geom_point(color='red') +\n               geom_smooth(method='lm') +\n           ggtitle(\"Total confirmed cases\") +\n               labs(x='Date',y='Total confirmed cases')\n        })\n\n}\n\nselected_data 는 ui.R에서 사용자에 의해 입력 받은 data를 처리한 결과값이 할당된 변수\noutput\\(daily_confirmed 은 server.R에서 ui.R로 데이터를 다시 내보내는 역할을 함\n* daily_confirmed 는 객체의 id\n* output\\)daily_confirmed: renderPlot의 처리 결과를 daily_confirmed에 보내는 과정 ## Custom 도커 이미지 만들기\n\nFROM rocker/shiny-verse:latest\nCOPY . /\nWORKDIR /\nEXPOSE 3838\nCMD R -e 'shiny::runApp(\"global.R\", port=3838, host=\"0.0.0.0\")'\n\nsudo docker build -t rshiny-custom\n\n\n\n\ndocker compose.yaml 파일 수정하고 서비스 up\n\nservices\n  rshiny_custom\n    image: rshiny custom # 기존 rocker/shiny verse:latest 변경\n    networks:\n      network_custom:\n        ipv4_address: 172.28.0.2\n    ports:\n    - 3838:3838\n\n\n\n\n\n\n\n\n무엇을 만들것인가\n\n현재는 서울시 공공데이터 API 를 통해 CSV 로 저장하는 오퍼레이터만 존재\nPostgresQL 데이터베이스로 바로 Insert 하는 오퍼레이터가 있다면\n\n오퍼레이터 개발 목표\n\n서울시 공공데이터 API 조회 후 데이터를 Postgres DB 로 바로 저장하는 오퍼레이 터\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/12.dashboarding.html#r-shiny",
    "href": "docs/blog/posts/Engineering/airflow/12.dashboarding.html#r-shiny",
    "title": "Template Variabler",
    "section": "",
    "text": "R로 대화형 웹을 만들 수 있게 해주는 R의 라이브러리 중 하나\n웹을 띄우기 위해선 세 가지 파일을 작성해야 함.\n\nui.R: 화면에 보여지는 UI에 대한 내용 (UI layout 정의)\nserver.R : ui.R 에서 입력을 받아 처리한 데이터를 ui.R 로 내보내는 내용. (데이터 처리 logic 정의)\nglobal.R: shiny에서 사용되는 전역 객체를 정의. 주로 라이브러리 임포트 등 수행. ui.R, server.R 에서 모두 사용됨"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/12.dashboarding.html#r-shiny-컨테이너-올리기",
    "href": "docs/blog/posts/Engineering/airflow/12.dashboarding.html#r-shiny-컨테이너-올리기",
    "title": "Template Variabler",
    "section": "",
    "text": "docker-compose.yaml 파일 수정하기\n\nservices:\n  rshiny_custom: # custom image 다른 이름 붙여도 상관없음\n    image: rocker/shiny-verse:latest\n    networks:\n      network_custom:\n        ipv4_address: 172.28.0.2\n  ports:\n  - 3838:3838\n\nsudo docker compose up 실행하면 rocker/shiny-verse:latest 이미지 자동으로 다운 받음\nDocker compose 재기동 후 localhost:3838 접속"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/12.dashboarding.html#rshiny-구성",
    "href": "docs/blog/posts/Engineering/airflow/12.dashboarding.html#rshiny-구성",
    "title": "Template Variabler",
    "section": "",
    "text": "Rshiny로 웹을 띄우기 위해선 세 가지 파일을 작성해야 함.\n\nui.R : 화면에 보여지는 UI에 대한 내용\nserver.R : ui.R 에서 입력을 받아 처리한 데이터를 ui.R 로 내보내는 내용\nglobal.R : shiny에서 사용되는 전역 객체를 정의. 주로 라이브러리 임포트 등 수행. ui.R, server.R 에서 모두 사용됨\n\n3가지 파일 작성 후 Rshiny 도커 이미지 Customizing 필요\n\n\n디렉토리 구조 및 파일 위치\n\n\n$home/airflow/custom_image \n\n\nglobal.R 작성\n\nif (!require(shiny)) {install.packages(\"shiny\"); library(shiny)}\n# DBI 와 glue는 DB에서 data 추출시에 사용하는 library\nif (!require(DBI)) {install.packages(\"DBI\"); library(DBI)} \nif (!require(glue)) {install.packages(\"glue\"); library(glue)}\nif (!require(ggplot2)) {install.packages(\"ggplot2\"); library(ggplot2)}\nsource(\"./ui.R\", local = TRUE)\nsource(\"./server.R\", local = TRUE)\nshinyApp(ui, server)\n\nui.R 작성\n\nui &lt;- fluidPage(\n    tags$h1(\"corona19\"),\n    sidebarPanel(\n    # dateRangeInput: date 구간을 from/to 형식(form)으로 입력할 수 있는 형태\n        dateRangeInput(\"dates\", # 형식(form) id 값\n                   \"Date range\", # 형식이 화면에 표시되는 이름\n                   start = as.Date(\"2023-01-01\"), # 화면에 기본적으로 보여질 날짜\n                   end = Sys.Date()),\n        br(),\n        br()\n    ),\n  # form을 여러개 나열하고 싶으면 sidebarPanel() 어러개 넣으면 됨\n\n  # mainPanel 역시 집어 넣을 수 있는 객체 (형식 또는 form)가 정해져 있음\n  # 대표적으로 자주 사용되는 객체: plotOUtput,imageOutput, tableOutput 등\n    mainPanel(plotOutput(\"daily_confirmed\"), plotOutput(\"total_confirmed\"))\n  # daily_confirmed: 1번째 plotOutput의 id\n  # total_confirmed: 2번째 plotOutput의 id\n)\n\n\nserver.R 작성\n\nserver &lt;- function(input, output){ # 무조건 오는 내용\n    selected_data &lt;- reactive({\n            # Connect to the DB\n            conn &lt;- dbConnect(\n                  RPostgres::Postgres(),\n                  dbname = \"kmkim\",\n                  host = \"172.28.0.3\",\n                  port = \"5432\",\n                  user = \"kmkim\",\n                  password = \"kmkim\"\n        )\n            # Get the data\n            corona &lt;- dbGetQuery(conn, glue(\"SELECT \n                         to_date(substring(\\\"S_DT\\\",1,10),'YYYY.MM.DD') as s_dt\n                            ,\\\"N_HJ\\\"::float as n_hj\n                        ,\\\"T_HJ\\\" as t_hj\n                        FROM \\\"TbCorona19CountStatus_bulk2\\\" \n                        WHERE to_date(substring(\\\"S_DT\\\",1,10),'YYYY.MM.DD') BETWEEN '{format(input$dates[1])}' AND '{format(input$dates[2])}'\"))\n            # Disconnect from the DB\n            dbDisconnect(conn)\n            # Convert to data.frame\n            data.frame(corona)\n    })\n    \n    output$daily_confirmed &lt;- renderPlot({\n        ggplot(data=selected_data(), aes(x=s_dt, y=n_hj)) + \n            geom_line(color='blue', linewidth = 1) + \n            geom_point(color='red') + \n            geom_smooth(method='lm') +\n            ggtitle(\"Daily confirmed cases\") +\n            labs(x='Date',y='Daily confirmed cases')\n    })\n    output$total_confirmed &lt;- renderPlot({\n               ggplot(data=selected_data(), aes(x=s_dt, y=t_hj)) +\n               geom_line(color='blue', linewidth = 1) +\n               geom_point(color='red') +\n               geom_smooth(method='lm') +\n           ggtitle(\"Total confirmed cases\") +\n               labs(x='Date',y='Total confirmed cases')\n        })\n\n}\n\nselected_data 는 ui.R에서 사용자에 의해 입력 받은 data를 처리한 결과값이 할당된 변수\noutput\\(daily_confirmed 은 server.R에서 ui.R로 데이터를 다시 내보내는 역할을 함\n* daily_confirmed 는 객체의 id\n* output\\)daily_confirmed: renderPlot의 처리 결과를 daily_confirmed에 보내는 과정 ## Custom 도커 이미지 만들기\n\nFROM rocker/shiny-verse:latest\nCOPY . /\nWORKDIR /\nEXPOSE 3838\nCMD R -e 'shiny::runApp(\"global.R\", port=3838, host=\"0.0.0.0\")'\n\nsudo docker build -t rshiny-custom"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/12.dashboarding.html#custom-컨테이너-올리기",
    "href": "docs/blog/posts/Engineering/airflow/12.dashboarding.html#custom-컨테이너-올리기",
    "title": "Template Variabler",
    "section": "",
    "text": "docker compose.yaml 파일 수정하고 서비스 up\n\nservices\n  rshiny_custom\n    image: rshiny custom # 기존 rocker/shiny verse:latest 변경\n    networks:\n      network_custom:\n        ipv4_address: 172.28.0.2\n    ports:\n    - 3838:3838"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/12.dashboarding.html#custom-operator-개발-1",
    "href": "docs/blog/posts/Engineering/airflow/12.dashboarding.html#custom-operator-개발-1",
    "title": "Template Variabler",
    "section": "",
    "text": "무엇을 만들것인가\n\n현재는 서울시 공공데이터 API 를 통해 CSV 로 저장하는 오퍼레이터만 존재\nPostgresQL 데이터베이스로 바로 Insert 하는 오퍼레이터가 있다면\n\n오퍼레이터 개발 목표\n\n서울시 공공데이터 API 조회 후 데이터를 Postgres DB 로 바로 저장하는 오퍼레이 터"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html",
    "title": "Storage and Database",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\nStorage Types \n\nBlock Storage: split into fixed size chunk of data\n\neasy to change small parts: change only block/piece of the data\nfor frequently modified data, or data with a high trasaction rate (like app or system files)\n\nObject Storage: each file = single unit of data\n\nto change even small parts: need to update the entire file\nfor WORM(write once, read many) model\n\naccessed often, but modified rarely (like photo)\n\n\nFile Storage: tree-like hierarchy (folders → subfolders)\n\nsimilar to windows file explorer or MacOS Finder\nfor files shared/managed by multiple host computers\n\nuser home directories, developmental environments\n\n\n\n\n\n\n\n\n\nblock storage\n\nboot volume for OS or data volume\n\nblock storages for EC2 instances\n\n\n\n\nblock storages for EC2 instances with EBS\n\n\n\nEC2 instance store: internal storage, ephemeral storage\n\ndirectly(physically) attached: fast, quick response\nlife cycle is tied to the instance: lose data when stop/terminate the instance\n\nAmazon Elastic Block Store(Amazon EBS): external storage, persistent storage\n\nseparate from EC2, user-configured size\none-to-one with EC2 instances (can’t be shared/attached to multiple instances): secure communication\n\ncan be detached → attached to another instance in the same AZ\nfor multiple attachements: need to use Amazon EBS Multi-Attach\n\nTypes of EBS\n\nSSD backed volumes: for random I/O\nHDD backed volumes: for sequential I/O\n\nbacking up data: (EBS)Snapshot\n\n\n\n\n\n\nfor employee photos: can’t use amazon EBS\n\ncan’t be attached to instances when the app scales\nhas size limitations\n\namazon simple storage service(Amazon S3)\n\nstandalone, don’t mount onto EC2 instances\naccess data through URL: storage for the internet\n\nsize limit for a single object: 5Tb\nflat structure: use unique identifiers(?)\ndistributed storage: store data across multiple different facilities within one AWS region\n\ndurable storage system\n\n\nS3 buckets: objects is stored in buckets\n\nneed to create buckets first\ncan make folders inside\nregion specific\nname: should be globally unique across AWS accounts, DNS compliant (no special characters, spaces, etc.)\n\nURL will be constructed using the name → should be reached with HTTP/HTTPS\n\nbucket URL → append object key to bucket URL\n\n\n\n\nBucket URL\n\n\n\nAccesss control\n\ndefault: everything in S3 is private → can give public access\n\nto make object publically access, need to change bucket settings\n\npolicies \n\nIAM policies\nS3 bucket policies\n\nJSON format (like IAM policies)\nonly placed on buckets (can apply for another AWS accounts or anonymous users)\n\nnot for folders/objects\n\n\n\n\n\n\n\n\nRelational database(RDB): data를 table 형태로 저장, 서로 다른 table간 data는 relationship으로 연결됨\n\n\n\nRDB\n\n\n\nTable은 row와 column으로 구성\nrow는 record라고도 부르며 특정 개체에 관련된 모든 정보를 담고 있음\ncolumn은 attribute라고도 부르며 개체의 각 속성을 나타냄\n\nSchema: 각 table 별 관계 및 column에 들어갈 data type 등을 정의한 것\n\nschema는 한 번 설정하고 나면 변경하기 어려움\n예시: MySQL, PostgresQL, Oracle, SQL server, Amazon Aurora\n일반적으로 SQL query를 사용해서 data 접근 및 수정\n\n장점\n\nJoins: table을 join하여 data간 관계를 쉽게 이해 가능\nReduced redundancy: 일부 attribute만 다른 경우 table을 나누어 중복 정보가 저장되는 것을 방지할 수 있음\nFamiliarity: 오래된 기술이기 때문에 자료가 많아서 적응하기 쉬움\nAccuracy: data의 안정성 및 결과 보장 참고\n\n\n사용처\n\nSchema가 거의 변경되지 않는 application들\n작업 및 data의 안정성이 필요한 분야 전반\nEnterprise Resource Planning (ERP) applications\nCustomer Relationship Management (CRM) applications\nCommerce and financial applications\n\nRDB on AWS\n\nManaged database: EC2 or AWS database service\n\n\n\nEC2 or AWS database service\n\n\n\nAmazon RDS: AWS에서 제공하는 RDB service\n\nCommercial: Oracle, SQL Server\nOpen Source: MySQL, PostgreSQL, MariaDB\nCloud Native: Amazon Aurora Note: The cloud native option, Amazon Aurora, is a MySQL and PostgreSQL-compatible database built for the cloud. It is more durable, more available, and provides faster performance than the Amazon RDS version of MySQL and PostgreSQL\n\nDB instance type - 아래 type 내에서 size 별 선택지 존재\n\nStandard, which include general-purpose instances\nMemory Optimized, which are optimized for memory-intensive applications\nBurstable Performance, which provides a baseline performance level, with the ability to burst to full CPU usage.\n\nDB storage - the DB instance uses Amazon Elastic Block Store (EBS) volumes as its storage layer\n\n용량: 20~65536Gb\nGeneral purpose (SSD)\nProvisioned IOPS (SSD)\nMagnetic storage (not recommended)\n\nDB subnet group\n\nDB를 사용하기 위해서 VPC 및 subnet 설정 필요 =&gt; availability zone 내 subnet 지정 필요\nDB subnet은 private해야 됨 - gateway에 직접 연결 금지 for 보안\n보안의 경우 ACL 및 security group으로 통제 가능 - network section 참고\n\nIAM policy\n\nDB subnet group은 traffic을 조절\nIAM policy는 data와 table에 대한 접근 및 수정 권한을 조절\n\nBackup\n\nAutomatic\n\ndefault로 설정\nlog 및 DB instance 자체를 백업\n주기: 0~35일 0일의 경우 automatic 백업을 disable, 기존 backup도 삭제됨\n방식: point-in-time =&gt; 특정 기간 내 일어난 transaction에 대해서 recovery\n\nManual snapshot\n\n35일보다 긴 기간에 대해 backup할 때 사용\n\nBackup recovery: 새 instance를 생성\n\nRedundancy\n\nMulti-AZ를 허용할 경우, Amazon RDS가 다른 AZ에 redundant copy 생성\nPrimary copy: 평소에 사용하는 copy\nStandby copy: primary copy에 접근이 불가한 경우 사용하는 copy\n두 copy간 싱크로는 자동 유지\nDB instance 생성시 DNS를 설정하면 AWS가 이를 인식하여 자동으로 failover 수행\nRedundant copy는 subnet에 존재해야 됨\n\nAmazon DynamoDB\n\nFully managed NoSQL database service: provides fast and predictable performance with seamless scalability\nServerless\n\nRDB와 달리 size 제한 없음\n자동 scale 조절\nSSD에 자동 저장되며 replication 또한 자동 수행\nNo schema\n\n\n저장된 데이터 양과 접근 횟수에 따라 과금\n구성 요소\n\nTable: RDB와 유사하게 item의 집합으로 구성\nItem: 다른 item과 unique하게 구분가능한 data, 개수 제한 없음, attribute의 조합으로 구성됨, RDB와 달리 각 item의 attribute 개수가 다를 수 있음 RDB의 row에 대응\nAttribute: RDB와 달리 같은 attribute라도 다양한 type의 정보를 저장할 수 있음? RDB의 column에 대응\n\nAWS Database Services \n\n\n\n\n\n\n\n\n\nDatabase Type\nUse Cases\nAWS Service\n\n\n\n\nRelational\nTraditional applications, ERP, CRM, e-commerce\nAmazon RDS, Amazon Aurora, Amazon Redshift\n\n\nKey-value\nHigh-traffic web apps, e-commerce systems, gaming applications\nAmazon DynamoDB\n\n\nIn-memory\nCaching, session management, gaming leaderboards, geospatial applications\nAmazon ElastiCache for Memcached, Amazon ElastiCache for Redis\n\n\nDocument\nContent management, catalogs, user profiles\nAmazon DocumentDB (with MongoDB compatibility)\n\n\nWide column\nHigh-scale industrial apps for equipment maintenance, fleet management, and route optimization\nAmazon Keyspaces (for Apache Cassandra)\n\n\nGraph\nFraud detection, social networking, recommendation engines\nAmazon Neptune\n\n\nTime series\nIoT applications, DevOps, industrial telemetry\nAmazon Timestream\n\n\nLedger\nSystems of record, supply chain, registrations, banking transactions\nAmazon QLDB\n\n\n\n\n선택 기준\n\nRDB: 데이터 간 관계가 복잡하고 별도 관리가 필요한 경우에 사용 복잡도에 의해 overhead가 발생하기 때문\nKey-value DB: Large scale, low latency 보장, 단순 데이터 저장 및 조회 목적으로 적합 =&gt; RDB에서는 여러 table에 나누어 저장해야 되는 정보를 한 table에 저장 가능\nGraph: SNS와 같은 관계형 자료구조에 적합\nLedger: 금융과 같은 안정성, 변경 불가가 필요한 자료를 저장하는 경우에 적합\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html#storage-on-aws",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html#storage-on-aws",
    "title": "Storage and Database",
    "section": "",
    "text": "Storage Types \n\nBlock Storage: split into fixed size chunk of data\n\neasy to change small parts: change only block/piece of the data\nfor frequently modified data, or data with a high trasaction rate (like app or system files)\n\nObject Storage: each file = single unit of data\n\nto change even small parts: need to update the entire file\nfor WORM(write once, read many) model\n\naccessed often, but modified rarely (like photo)\n\n\nFile Storage: tree-like hierarchy (folders → subfolders)\n\nsimilar to windows file explorer or MacOS Finder\nfor files shared/managed by multiple host computers\n\nuser home directories, developmental environments"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html#amazon-ec2-instance-storage-and-amazon-elastic-block-store",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html#amazon-ec2-instance-storage-and-amazon-elastic-block-store",
    "title": "Storage and Database",
    "section": "",
    "text": "block storage\n\nboot volume for OS or data volume\n\nblock storages for EC2 instances\n\n\n\n\nblock storages for EC2 instances with EBS\n\n\n\nEC2 instance store: internal storage, ephemeral storage\n\ndirectly(physically) attached: fast, quick response\nlife cycle is tied to the instance: lose data when stop/terminate the instance\n\nAmazon Elastic Block Store(Amazon EBS): external storage, persistent storage\n\nseparate from EC2, user-configured size\none-to-one with EC2 instances (can’t be shared/attached to multiple instances): secure communication\n\ncan be detached → attached to another instance in the same AZ\nfor multiple attachements: need to use Amazon EBS Multi-Attach\n\nTypes of EBS\n\nSSD backed volumes: for random I/O\nHDD backed volumes: for sequential I/O\n\nbacking up data: (EBS)Snapshot"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html#object-storage-with-amazon-s3",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html#object-storage-with-amazon-s3",
    "title": "Storage and Database",
    "section": "",
    "text": "for employee photos: can’t use amazon EBS\n\ncan’t be attached to instances when the app scales\nhas size limitations\n\namazon simple storage service(Amazon S3)\n\nstandalone, don’t mount onto EC2 instances\naccess data through URL: storage for the internet\n\nsize limit for a single object: 5Tb\nflat structure: use unique identifiers(?)\ndistributed storage: store data across multiple different facilities within one AWS region\n\ndurable storage system\n\n\nS3 buckets: objects is stored in buckets\n\nneed to create buckets first\ncan make folders inside\nregion specific\nname: should be globally unique across AWS accounts, DNS compliant (no special characters, spaces, etc.)\n\nURL will be constructed using the name → should be reached with HTTP/HTTPS\n\nbucket URL → append object key to bucket URL\n\n\n\n\nBucket URL\n\n\n\nAccesss control\n\ndefault: everything in S3 is private → can give public access\n\nto make object publically access, need to change bucket settings\n\npolicies \n\nIAM policies\nS3 bucket policies\n\nJSON format (like IAM policies)\nonly placed on buckets (can apply for another AWS accounts or anonymous users)\n\nnot for folders/objects"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html#databases-on-aws",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html#databases-on-aws",
    "title": "Storage and Database",
    "section": "",
    "text": "Relational database(RDB): data를 table 형태로 저장, 서로 다른 table간 data는 relationship으로 연결됨\n\n\n\nRDB\n\n\n\nTable은 row와 column으로 구성\nrow는 record라고도 부르며 특정 개체에 관련된 모든 정보를 담고 있음\ncolumn은 attribute라고도 부르며 개체의 각 속성을 나타냄\n\nSchema: 각 table 별 관계 및 column에 들어갈 data type 등을 정의한 것\n\nschema는 한 번 설정하고 나면 변경하기 어려움\n예시: MySQL, PostgresQL, Oracle, SQL server, Amazon Aurora\n일반적으로 SQL query를 사용해서 data 접근 및 수정\n\n장점\n\nJoins: table을 join하여 data간 관계를 쉽게 이해 가능\nReduced redundancy: 일부 attribute만 다른 경우 table을 나누어 중복 정보가 저장되는 것을 방지할 수 있음\nFamiliarity: 오래된 기술이기 때문에 자료가 많아서 적응하기 쉬움\nAccuracy: data의 안정성 및 결과 보장 참고\n\n\n사용처\n\nSchema가 거의 변경되지 않는 application들\n작업 및 data의 안정성이 필요한 분야 전반\nEnterprise Resource Planning (ERP) applications\nCustomer Relationship Management (CRM) applications\nCommerce and financial applications\n\nRDB on AWS\n\nManaged database: EC2 or AWS database service\n\n\n\nEC2 or AWS database service\n\n\n\nAmazon RDS: AWS에서 제공하는 RDB service\n\nCommercial: Oracle, SQL Server\nOpen Source: MySQL, PostgreSQL, MariaDB\nCloud Native: Amazon Aurora Note: The cloud native option, Amazon Aurora, is a MySQL and PostgreSQL-compatible database built for the cloud. It is more durable, more available, and provides faster performance than the Amazon RDS version of MySQL and PostgreSQL\n\nDB instance type - 아래 type 내에서 size 별 선택지 존재\n\nStandard, which include general-purpose instances\nMemory Optimized, which are optimized for memory-intensive applications\nBurstable Performance, which provides a baseline performance level, with the ability to burst to full CPU usage.\n\nDB storage - the DB instance uses Amazon Elastic Block Store (EBS) volumes as its storage layer\n\n용량: 20~65536Gb\nGeneral purpose (SSD)\nProvisioned IOPS (SSD)\nMagnetic storage (not recommended)\n\nDB subnet group\n\nDB를 사용하기 위해서 VPC 및 subnet 설정 필요 =&gt; availability zone 내 subnet 지정 필요\nDB subnet은 private해야 됨 - gateway에 직접 연결 금지 for 보안\n보안의 경우 ACL 및 security group으로 통제 가능 - network section 참고\n\nIAM policy\n\nDB subnet group은 traffic을 조절\nIAM policy는 data와 table에 대한 접근 및 수정 권한을 조절\n\nBackup\n\nAutomatic\n\ndefault로 설정\nlog 및 DB instance 자체를 백업\n주기: 0~35일 0일의 경우 automatic 백업을 disable, 기존 backup도 삭제됨\n방식: point-in-time =&gt; 특정 기간 내 일어난 transaction에 대해서 recovery\n\nManual snapshot\n\n35일보다 긴 기간에 대해 backup할 때 사용\n\nBackup recovery: 새 instance를 생성\n\nRedundancy\n\nMulti-AZ를 허용할 경우, Amazon RDS가 다른 AZ에 redundant copy 생성\nPrimary copy: 평소에 사용하는 copy\nStandby copy: primary copy에 접근이 불가한 경우 사용하는 copy\n두 copy간 싱크로는 자동 유지\nDB instance 생성시 DNS를 설정하면 AWS가 이를 인식하여 자동으로 failover 수행\nRedundant copy는 subnet에 존재해야 됨\n\nAmazon DynamoDB\n\nFully managed NoSQL database service: provides fast and predictable performance with seamless scalability\nServerless\n\nRDB와 달리 size 제한 없음\n자동 scale 조절\nSSD에 자동 저장되며 replication 또한 자동 수행\nNo schema\n\n\n저장된 데이터 양과 접근 횟수에 따라 과금\n구성 요소\n\nTable: RDB와 유사하게 item의 집합으로 구성\nItem: 다른 item과 unique하게 구분가능한 data, 개수 제한 없음, attribute의 조합으로 구성됨, RDB와 달리 각 item의 attribute 개수가 다를 수 있음 RDB의 row에 대응\nAttribute: RDB와 달리 같은 attribute라도 다양한 type의 정보를 저장할 수 있음? RDB의 column에 대응\n\nAWS Database Services \n\n\n\n\n\n\n\n\n\nDatabase Type\nUse Cases\nAWS Service\n\n\n\n\nRelational\nTraditional applications, ERP, CRM, e-commerce\nAmazon RDS, Amazon Aurora, Amazon Redshift\n\n\nKey-value\nHigh-traffic web apps, e-commerce systems, gaming applications\nAmazon DynamoDB\n\n\nIn-memory\nCaching, session management, gaming leaderboards, geospatial applications\nAmazon ElastiCache for Memcached, Amazon ElastiCache for Redis\n\n\nDocument\nContent management, catalogs, user profiles\nAmazon DocumentDB (with MongoDB compatibility)\n\n\nWide column\nHigh-scale industrial apps for equipment maintenance, fleet management, and route optimization\nAmazon Keyspaces (for Apache Cassandra)\n\n\nGraph\nFraud detection, social networking, recommendation engines\nAmazon Neptune\n\n\nTime series\nIoT applications, DevOps, industrial telemetry\nAmazon Timestream\n\n\nLedger\nSystems of record, supply chain, registrations, banking transactions\nAmazon QLDB\n\n\n\n\n선택 기준\n\nRDB: 데이터 간 관계가 복잡하고 별도 관리가 필요한 경우에 사용 복잡도에 의해 overhead가 발생하기 때문\nKey-value DB: Large scale, low latency 보장, 단순 데이터 저장 및 조회 목적으로 적합 =&gt; RDB에서는 여러 table에 나누어 저장해야 되는 정보를 한 table에 저장 가능\nGraph: SNS와 같은 관계형 자료구조에 적합\nLedger: 금융과 같은 안정성, 변경 불가가 필요한 자료를 저장하는 경우에 적합"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html",
    "title": "Infrastructure Security",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\nAWS has implemented network isolation through a limited number of access points to the cloud, allowing for comprehensive monitoring of inbound and outbound communications and network traffic.\n\n\n\n\n\n\nTip\n\n\n\ninbound and outbound communications involve observing and analyzing the data and traffic that is entering or leaving the AWS network to ensure security and compliance.\n\n\n\nEndpoints are URLs that serve as entry points for web services.\nSome services do not support regions (like IAM), so their endpoints do not include a region. But, some services (like US-West-2) do support regions.\nAWS offers Amazon Virtual Private Cloud (VPC) as a private network within the AWS Cloud that provides isolation from other customers and from the internet\n\nVPC allows you to allocate IP address spaces and build a private infrastructure with networks isolated from the internet.\nYou can also connect your on-premises environment or other VPN infrastructures to VPC using IPSec tunnels and AWS Direct Connect.\nVPC allows resources to communicate with the internet if desired.\n\nBuilding a fort on a barren planet to protect themselves and the bees, and further isolating hives inside the fort itself is similar to the concept of isolating resources within a secure environment, such as Amazon VPC, to protect them from potential external threats.\nNetwork Isolation VPC\n\nthe concept of Virtual Private Cloud (VPC) in AWS is a way to logically separate your AWS infrastructure from other customers.\nVPC is like creating a fort around your AWS account and isolating resources into hives, using subnets or logical subdivision of IPs.\n\nex) EC2 instances are able to access the internet and be accessed from by putting them in a public subnet via Network Access Control Lists (NACLs), which are used to control inbound and outbound traffic at the subnet level.\n\nsecurity groups\n\nact as firewalls for EC2 instances by controlling both inbound and outbound traffic at the instance level.\nThis fine-grained access is defined by allow rules and looks\n\n\nhow to secure traffic between VPCs in AWS using VPC endpoints and route tables?\n\nfurther secure communication between Virtual Private Clouds (VPCs) in AWS\n\nIt compares the traditional method of sending traffic between VPCs through the internet with the use of private links, which allow for direct communication between VPCs within the AWS infrastructure, resulting in a safer path of travel for data.\nthe concept of route tables in VPCs\n\nroute tables contain rules or routes used to determine where network traffic is directed, and the option to create custom route tables for routing traffic according to specific requirements.\n\n\n\n\n\n\n\n\nDetective controls: 감사(auditing), 자동화된 분석, 경보를 가능하게 하는 사건 모니터링 및 블록 처리의 지속적인 개선\n\n잠재적인 보안 위협 또는 사건을 식별할 수 있는 능력 향상\ngovernance frameworks에 필수적\n법이나 compliance 준수 의무, 보안 작업 등의 개선을 지원\n\nDifferent types of detective controls\n\n자산 인벤토리의 작성(conducting an inventory of AWS resources)\n내부 감사(internal auditing)\n정보 시스템과 관련된 제어가 정책 및 요구사항 충족하는지 검사\n\n이상 활동 범위를 식별하고 이해하는데 도움이 됨\n\n\n\n\n\nAWS infrastructure의 보안 및 compliance를 향상시키는 AWS services (일부는 무료, 일부는 유료)\n\nAWS CloudTrail: AWS infrastructure와 interact 하는 사람 추적 가능 (잘못된 변경/데이터 유출 추적에 도움)\nAWS Config: configuration 관리 및 변경 기록, 모든 실제 config 세부 사항의 inventory 제공\nAWS Inspector: 자동 보안 평가 실행\n\ndeploy된 applications의 보안 및 compliance를 향상시키기 위해, best practies와의 차이, EC2 instances의 노출, 취약점 등을 체크\n\nTrusted Advisor\n\nAWS resources의 프로비저닝 보조 - best practices를 사용해서 리포트 제공\n\n리포트 항목: 비용 최적화, 성능, 보안, 장애 허용 정도, 서비스 제한\n조사 또는 실행을 위해, 심각한 수준(녹색,주황,적색)에 따라 권장 사항 제공\n\nSecurity section: S3 bucket의 권한, 보안 그룹, IAM 사용, root 계정의 MFA, 노출된 access keys, IAM 비밀번호 정책 등을 스캔\n\n\n\n\n\n\n\nMonitoring: Infrastructure와 관련된 데이터와 통계를 수집, 추적, 표시하는 과정\nAWS의 CloudWatch: metrics repository역할 - repository에 넣은 metrics 기반으로 통계 검색\n\n사용자가 정한 threshold를 넘었을때 경보 생성 가능\n특정 기간 동안 하나의 metric을 감시 → threshold와 비교한 metric의 상대적인 값에 따라 하나 이상의 특정 action 수행 가능\n\nCloudWatch Logs: 여러 resources의 log files을 모니터링, 저장, 접근 가능한 tool\n\napplication, 서버 OS의 로그 수집 및 저장\nCloudTrail 사용해서 API activity 수집\nAmazon Route 53(Amazon의 DNS 웹 서비스)의 DNS queries를 기록\nS3의 로그 데이터 저장\n\nCloudWatch Logs Insights: 로그 데이터를 interactive하게 검색하고 분석\n\n\n\n\n\nAmazon GuardDuty: 위협 감지 서비스\n\nAWS 계정 및 리소스에 대한 허가되지 않거나 악성인 행동들을 계속 모니터링\n머신 러닝, 이상 감지, integrated threat intelligence를 사용해서 잠재적인 위협을 식별하고 우선 순위를 정함\n여러 AWS data resources에서 수백억건의 사건 분석\n잠재적인 위협을 세 단계(low, medium, high) 심각 수준으로 나눠서 대응 우선순위 결정\nHTTPs API, CLI tools, Amazon CloudWatch events를 제공해서 보안 관련 발견에 대한 자동화된 보안 제공 지원\n\nSecurity Hub: 여러 AWS service의 보안 경고나 발견을 모으고, 정리하고, 우선 순위를 정함 → 통합 dashboards에서 시각화 요약 제공\n\nAWS best practies 및 업계 표준을 기반으로, compliance check 자동화를 통해 환경을 지속적으로 모니터링할 수 있도록 함\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html#network-isolation",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html#network-isolation",
    "title": "Infrastructure Security",
    "section": "",
    "text": "AWS has implemented network isolation through a limited number of access points to the cloud, allowing for comprehensive monitoring of inbound and outbound communications and network traffic.\n\n\n\n\n\n\nTip\n\n\n\ninbound and outbound communications involve observing and analyzing the data and traffic that is entering or leaving the AWS network to ensure security and compliance.\n\n\n\nEndpoints are URLs that serve as entry points for web services.\nSome services do not support regions (like IAM), so their endpoints do not include a region. But, some services (like US-West-2) do support regions.\nAWS offers Amazon Virtual Private Cloud (VPC) as a private network within the AWS Cloud that provides isolation from other customers and from the internet\n\nVPC allows you to allocate IP address spaces and build a private infrastructure with networks isolated from the internet.\nYou can also connect your on-premises environment or other VPN infrastructures to VPC using IPSec tunnels and AWS Direct Connect.\nVPC allows resources to communicate with the internet if desired.\n\nBuilding a fort on a barren planet to protect themselves and the bees, and further isolating hives inside the fort itself is similar to the concept of isolating resources within a secure environment, such as Amazon VPC, to protect them from potential external threats.\nNetwork Isolation VPC\n\nthe concept of Virtual Private Cloud (VPC) in AWS is a way to logically separate your AWS infrastructure from other customers.\nVPC is like creating a fort around your AWS account and isolating resources into hives, using subnets or logical subdivision of IPs.\n\nex) EC2 instances are able to access the internet and be accessed from by putting them in a public subnet via Network Access Control Lists (NACLs), which are used to control inbound and outbound traffic at the subnet level.\n\nsecurity groups\n\nact as firewalls for EC2 instances by controlling both inbound and outbound traffic at the instance level.\nThis fine-grained access is defined by allow rules and looks\n\n\nhow to secure traffic between VPCs in AWS using VPC endpoints and route tables?\n\nfurther secure communication between Virtual Private Clouds (VPCs) in AWS\n\nIt compares the traditional method of sending traffic between VPCs through the internet with the use of private links, which allow for direct communication between VPCs within the AWS infrastructure, resulting in a safer path of travel for data.\nthe concept of route tables in VPCs\n\nroute tables contain rules or routes used to determine where network traffic is directed, and the option to create custom route tables for routing traffic according to specific requirements."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html#detective-controls",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html#detective-controls",
    "title": "Infrastructure Security",
    "section": "",
    "text": "Detective controls: 감사(auditing), 자동화된 분석, 경보를 가능하게 하는 사건 모니터링 및 블록 처리의 지속적인 개선\n\n잠재적인 보안 위협 또는 사건을 식별할 수 있는 능력 향상\ngovernance frameworks에 필수적\n법이나 compliance 준수 의무, 보안 작업 등의 개선을 지원\n\nDifferent types of detective controls\n\n자산 인벤토리의 작성(conducting an inventory of AWS resources)\n내부 감사(internal auditing)\n정보 시스템과 관련된 제어가 정책 및 요구사항 충족하는지 검사\n\n이상 활동 범위를 식별하고 이해하는데 도움이 됨"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html#auditing",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html#auditing",
    "title": "Infrastructure Security",
    "section": "",
    "text": "AWS infrastructure의 보안 및 compliance를 향상시키는 AWS services (일부는 무료, 일부는 유료)\n\nAWS CloudTrail: AWS infrastructure와 interact 하는 사람 추적 가능 (잘못된 변경/데이터 유출 추적에 도움)\nAWS Config: configuration 관리 및 변경 기록, 모든 실제 config 세부 사항의 inventory 제공\nAWS Inspector: 자동 보안 평가 실행\n\ndeploy된 applications의 보안 및 compliance를 향상시키기 위해, best practies와의 차이, EC2 instances의 노출, 취약점 등을 체크\n\nTrusted Advisor\n\nAWS resources의 프로비저닝 보조 - best practices를 사용해서 리포트 제공\n\n리포트 항목: 비용 최적화, 성능, 보안, 장애 허용 정도, 서비스 제한\n조사 또는 실행을 위해, 심각한 수준(녹색,주황,적색)에 따라 권장 사항 제공\n\nSecurity section: S3 bucket의 권한, 보안 그룹, IAM 사용, root 계정의 MFA, 노출된 access keys, IAM 비밀번호 정책 등을 스캔"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html#monitoring-cloudwatch-and-cloudwatch-log",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html#monitoring-cloudwatch-and-cloudwatch-log",
    "title": "Infrastructure Security",
    "section": "",
    "text": "Monitoring: Infrastructure와 관련된 데이터와 통계를 수집, 추적, 표시하는 과정\nAWS의 CloudWatch: metrics repository역할 - repository에 넣은 metrics 기반으로 통계 검색\n\n사용자가 정한 threshold를 넘었을때 경보 생성 가능\n특정 기간 동안 하나의 metric을 감시 → threshold와 비교한 metric의 상대적인 값에 따라 하나 이상의 특정 action 수행 가능\n\nCloudWatch Logs: 여러 resources의 log files을 모니터링, 저장, 접근 가능한 tool\n\napplication, 서버 OS의 로그 수집 및 저장\nCloudTrail 사용해서 API activity 수집\nAmazon Route 53(Amazon의 DNS 웹 서비스)의 DNS queries를 기록\nS3의 로그 데이터 저장\n\nCloudWatch Logs Insights: 로그 데이터를 interactive하게 검색하고 분석"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html#monitoring-guard-duty-and-security-hub",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html#monitoring-guard-duty-and-security-hub",
    "title": "Infrastructure Security",
    "section": "",
    "text": "Amazon GuardDuty: 위협 감지 서비스\n\nAWS 계정 및 리소스에 대한 허가되지 않거나 악성인 행동들을 계속 모니터링\n머신 러닝, 이상 감지, integrated threat intelligence를 사용해서 잠재적인 위협을 식별하고 우선 순위를 정함\n여러 AWS data resources에서 수백억건의 사건 분석\n잠재적인 위협을 세 단계(low, medium, high) 심각 수준으로 나눠서 대응 우선순위 결정\nHTTPs API, CLI tools, Amazon CloudWatch events를 제공해서 보안 관련 발견에 대한 자동화된 보안 제공 지원\n\nSecurity Hub: 여러 AWS service의 보안 경고나 발견을 모으고, 정리하고, 우선 순위를 정함 → 통합 dashboards에서 시각화 요약 제공\n\nAWS best practies 및 업계 표준을 기반으로, compliance check 자동화를 통해 환경을 지속적으로 모니터링할 수 있도록 함"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_graph/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-20_graph/index.html",
    "title": "Data Structure (10) Graph",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n• 그래프(graph)란 사물을 정점(vertex)과 간선(edge)으로 나타내기 위한 도구다.\n• 그래프는 두 가지 방식으로 구현할 수 있다.\n\n인접 행렬(adjacency matrix): 2차원 배열을 사용하는 방식\n인접 리스트(adjacency list): 연결 리스트를 이용하는 방식\n\n\n\n\n• 인접 행렬(adjacency matrix)에서는 그래프를 2차원 배열로 표현한다.\n\n\n• 모든 간선이 방향성을 가지지 않는 그래프를 무방향 그래프라고 한다.\n• 모든 간선에 가중치가 없는 그래프를 무가중치 그래프라고 한다.\n• 무방향 무가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 행렬로 출력할 수 있다.\n\n\n\n• 모든 간선이 방향을 가지는 그래프를 방향 그래프라고 한다.\n• 모든 간선에 가중치가 있는 그래프를 가중치 그래프라고 한다.\n• 방향 가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 행렬로 출력할 수 있다.\n\n\n\n\n• 인접 리스트(adjacency list)에서는 그래프를 리스트로 표현한다.\n\n\n• 모든 간선이 방향성을 가지지 않는 그래프를 무방향 그래프라고 한다.\n• 모든 간선에 가중치가 없는 그래프를 무가중치 그래프라고 한다.\n• 무방향 무가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 리스트로 출력할 수 있다.\n\n\n\n• 모든 간선이 방향을 가지는 그래프를 방향 그래프라고 한다.\n• 모든 간선에 가중치가 있는 그래프를 가중치 그래프라고 한다.\n• 방향 가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 리스트로 출력할 수 있다.\n\n\n\n\n\n인접 행렬: 모든 정점들의 연결 여부를 저장해 O V\n\n2 의 공간을 요구한다.\n• 공간 효율성이 떨어지지만, 두 노드의 연결 여부를 O 1 에 확인할 수 있다.\n\n인접 리스트: 연결된 간선의 정보만을 저장하여 O V + E 의 공간을 요구한다.\n\n• 공간 효율성이 우수하지만, 두 노드의 연결 여부를 확인하기 위해 O V 의 시간이 필요하다.\n\n\n\nTable 1: a list of the stack functions in Python\n\n\n\n\n\n\n\n\n\n\n\nNumber\nCategory\n필요한 메모리\n연결 여부 확인\n\n\n\n\n1\n인접 행렬\n\\(O(V^2)\\)\n\\(O(1)\\)\n\n\n2\n인접 리스트\n\\(O(V+E)\\)\n\\(O(V)\\)\n\n\n\n\n\n\nSee Table 1.\n\n\n\n• 최단 경로 알고리즘을 구현할 때, 어떤 자료구조가 유용할까?\n• 각각 근처의 노드와 연결되어 있는 경우가 많으므로, 간선 개수가 적어 인접 리스트가 유리하다.\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_graph/index.html#그래프graph",
    "href": "docs/blog/posts/Engineering/2023-01-20_graph/index.html#그래프graph",
    "title": "Data Structure (10) Graph",
    "section": "",
    "text": "• 그래프(graph)란 사물을 정점(vertex)과 간선(edge)으로 나타내기 위한 도구다.\n• 그래프는 두 가지 방식으로 구현할 수 있다.\n\n인접 행렬(adjacency matrix): 2차원 배열을 사용하는 방식\n인접 리스트(adjacency list): 연결 리스트를 이용하는 방식"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_graph/index.html#인접-행렬adjacency-matrix",
    "href": "docs/blog/posts/Engineering/2023-01-20_graph/index.html#인접-행렬adjacency-matrix",
    "title": "Data Structure (10) Graph",
    "section": "",
    "text": "• 인접 행렬(adjacency matrix)에서는 그래프를 2차원 배열로 표현한다.\n\n\n• 모든 간선이 방향성을 가지지 않는 그래프를 무방향 그래프라고 한다.\n• 모든 간선에 가중치가 없는 그래프를 무가중치 그래프라고 한다.\n• 무방향 무가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 행렬로 출력할 수 있다.\n\n\n\n• 모든 간선이 방향을 가지는 그래프를 방향 그래프라고 한다.\n• 모든 간선에 가중치가 있는 그래프를 가중치 그래프라고 한다.\n• 방향 가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 행렬로 출력할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_graph/index.html#인접-리스트adjacency-list",
    "href": "docs/blog/posts/Engineering/2023-01-20_graph/index.html#인접-리스트adjacency-list",
    "title": "Data Structure (10) Graph",
    "section": "",
    "text": "• 인접 리스트(adjacency list)에서는 그래프를 리스트로 표현한다.\n\n\n• 모든 간선이 방향성을 가지지 않는 그래프를 무방향 그래프라고 한다.\n• 모든 간선에 가중치가 없는 그래프를 무가중치 그래프라고 한다.\n• 무방향 무가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 리스트로 출력할 수 있다.\n\n\n\n• 모든 간선이 방향을 가지는 그래프를 방향 그래프라고 한다.\n• 모든 간선에 가중치가 있는 그래프를 가중치 그래프라고 한다.\n• 방향 가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 리스트로 출력할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_graph/index.html#그래프의-시간-복잡도",
    "href": "docs/blog/posts/Engineering/2023-01-20_graph/index.html#그래프의-시간-복잡도",
    "title": "Data Structure (10) Graph",
    "section": "",
    "text": "인접 행렬: 모든 정점들의 연결 여부를 저장해 O V\n\n2 의 공간을 요구한다.\n• 공간 효율성이 떨어지지만, 두 노드의 연결 여부를 O 1 에 확인할 수 있다.\n\n인접 리스트: 연결된 간선의 정보만을 저장하여 O V + E 의 공간을 요구한다.\n\n• 공간 효율성이 우수하지만, 두 노드의 연결 여부를 확인하기 위해 O V 의 시간이 필요하다.\n\n\n\nTable 1: a list of the stack functions in Python\n\n\n\n\n\n\n\n\n\n\n\nNumber\nCategory\n필요한 메모리\n연결 여부 확인\n\n\n\n\n1\n인접 행렬\n\\(O(V^2)\\)\n\\(O(1)\\)\n\n\n2\n인접 리스트\n\\(O(V+E)\\)\n\\(O(V)\\)\n\n\n\n\n\n\nSee Table 1."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_graph/index.html#인접-행렬-vs.-인접-리스트",
    "href": "docs/blog/posts/Engineering/2023-01-20_graph/index.html#인접-행렬-vs.-인접-리스트",
    "title": "Data Structure (10) Graph",
    "section": "",
    "text": "• 최단 경로 알고리즘을 구현할 때, 어떤 자료구조가 유용할까?\n• 각각 근처의 노드와 연결되어 있는 경우가 많으므로, 간선 개수가 적어 인접 리스트가 유리하다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_stack/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-19_stack/index.html",
    "title": "Data Structure (5) Stack",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n다양한 알고리즘과 프로그램에서 사용됨\n스택: 먼저 들어온 데이터가 나중에 나가는 자료구조\n흔히 박스가 쌓인 형태를 스택(stack)이라고 한다. 예) ‘Deep Learning 알고리즘의 구조가 stacked 되어 있는 구조다’ 라고 표현\n\n우리가 박스를 쌓은 뒤에 꺼낼 때는, 가장 마지막에 올렸던 박스부터 꺼내야 한다.\n\n새로운 원소를 삽입할 때는 마지막 위치에 삽입한다. (가장 최근에 삽입된 원소가 가장 끝에 위치)\n새로운 원소를 삭제할 때는 마지막 원소가 삭제된다. (가장 최근에 삽입된 원소가 제거됨)\nhead = 최상위 원소 = 가장 최근에 삽입이된 원소\n\n\n\n\n\n스택은 굉장히 기본적인 자료구조이다.\n기계 학습 분야뿐 아니라 다양한 프로그램을 개발할 때 빠지지 않고 사용된다.\n\n\n\n\n\n스택은 여러 가지 연산을 제공한다.\n\n\n\n\nTable 1: a list of the stack functions in Python\n\n\n\n\n\n\n\n\n\n\n\nNumber\nMethods\nTime Complexity\nDescription\n\n\n\n\n1\n삽입(Push)\n\\(O(1)\\)\n스택에 원소를 삽입하는 연산\n\n\n2\n추출(Pop)\n\\(O(1)\\)\n스택에서 원소를 추출하는 연산\n\n\n3\n최상위 원소 (Top)\n\\(O(1)\\)\n스택의 최상위 원소(마지막에 들어온 원소) 를 확인(조회)하는 연산\n\n\n4\nEmpty\n\\(O(1)\\)\n스택이 비어 있는지 확인하는 연산\n\n\n\n\n\n\nSee Table 1.\n\n\n\n\n파이썬의 기본적인 리스트 자료형은 다음의 두 가지 메서드를 제공한다.\nappend() 메서드: 마지막 위치에 원소를 삽입하며, 시간 복잡도는 \\(O(1)\\) 이다.\npop() 메서드: 마지막 위치에서 원소를 추출하며, 시간 복잡도는 \\(O(1)\\) 이다.\n따라서 일반적으로 스택을 구현할 때, 파이썬의 리스트(list) 자료형을 사용한다.\n\n\n\nCode\nclass Stack:\n    def __init__(self):\n        self.stack = []\n\n    def push(self, data):\n        # 마지막 위치에 원소 삽입\n        self.stack.append(data)\n\n    def pop(self):\n        if self.is_empty():\n            return None\n        # 마지막 원소 추출\n        return self.stack.pop()\n\n    def top(self):\n        if self.is_empty():\n            return None\n        # 마지막 원소 반환\n        return self.stack[-1]\n\n    def is_empty(self):\n        return len(self.stack) == 0\n\n\nstack = Stack()\narr = [9, 7, 2, 5, 6, 4, 2]\nfor x in arr:\n    stack.push(x)\n\nwhile not stack.is_empty():\n    print(stack.pop())\n\n\n2\n4\n6\n5\n2\n7\n9\n\n\n\n\n\n\n스택을 연결 리스트로 구현하면, 삽입과 삭제에 있어서 \\(O(1)\\) 을 보장한다.\n연결 리스트로 구현할 때는 머리(head)를 가리키는 하나의 포인터만 가진다.\n머리(head): 남아있는 원소 중 가장 마지막에 들어 온 데이터를 가리키는 포인터\n\n\n\n\n삽입할 때는 기존의 머리 뒤에 데이터가 들어가고 포인터가 가장 최근에 삽입된 데이터를 가리키도록 머리(head) 위치를 바꿔준다.\n삭제할 때는 머리(head) 위치에서 데이터를 꺼낸다.\n\n즉, 포인터를 삭제할 데이터에 앞에 있는 데이터로 머리 위치를 바꾸는 것만으로 삭제는 이루어진다.\n\n\n\n\n\n\n삭제할 때는 머리(head) 위치에서 데이터를 꺼낸다.\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\n\nclass Stack:\n    def __init__(self):\n        self.head = None\n\n    # 원소 삽입\n    def push(self, data):\n        node = Node(data)\n        node.next = self.head\n        self.head = node\n\n    # 원소 추출하기\n    def pop(self):\n        if self.is_empty():\n            return None\n\n        # 머리(head) 위치에서 노드 꺼내기\n        data = self.head.data\n        self.head = self.head.next\n\n        return data\n\n    # 최상위 원소(top)\n    def top(self):\n        if self.is_empty():\n            return None\n        return self.head.data\n\n    # 먼저 추출할 원소부터 출력\n    def show(self):\n        cur = self.head\n        while cur:\n            print(cur.data, end=\" \")\n            cur = cur.next\n\n    # 스택이 비어있는지 확인\n    def is_empty(self):\n        return self.head is None\n\n\nstack = Stack()\narr = [9, 7, 2, 5, 6, 4, 2]\nfor x in arr:\n    stack.push(x)\nstack.show()\nprint()\n\nwhile not stack.is_empty():\n    print(stack.pop())\n\n\n2 4 6 5 2 7 9 \n2\n4\n6\n5\n2\n7\n9\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_stack/index.html#stack",
    "href": "docs/blog/posts/Engineering/2023-01-19_stack/index.html#stack",
    "title": "Data Structure (5) Stack",
    "section": "",
    "text": "다양한 알고리즘과 프로그램에서 사용됨\n스택: 먼저 들어온 데이터가 나중에 나가는 자료구조\n흔히 박스가 쌓인 형태를 스택(stack)이라고 한다. 예) ‘Deep Learning 알고리즘의 구조가 stacked 되어 있는 구조다’ 라고 표현\n\n우리가 박스를 쌓은 뒤에 꺼낼 때는, 가장 마지막에 올렸던 박스부터 꺼내야 한다.\n\n새로운 원소를 삽입할 때는 마지막 위치에 삽입한다. (가장 최근에 삽입된 원소가 가장 끝에 위치)\n새로운 원소를 삭제할 때는 마지막 원소가 삭제된다. (가장 최근에 삽입된 원소가 제거됨)\nhead = 최상위 원소 = 가장 최근에 삽입이된 원소"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_stack/index.html#스택-자료구조의-중요성",
    "href": "docs/blog/posts/Engineering/2023-01-19_stack/index.html#스택-자료구조의-중요성",
    "title": "Data Structure (5) Stack",
    "section": "",
    "text": "스택은 굉장히 기본적인 자료구조이다.\n기계 학습 분야뿐 아니라 다양한 프로그램을 개발할 때 빠지지 않고 사용된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_stack/index.html#스택-자료구조의-시간-복잡도",
    "href": "docs/blog/posts/Engineering/2023-01-19_stack/index.html#스택-자료구조의-시간-복잡도",
    "title": "Data Structure (5) Stack",
    "section": "",
    "text": "스택은 여러 가지 연산을 제공한다.\n\n\n\n\nTable 1: a list of the stack functions in Python\n\n\n\n\n\n\n\n\n\n\n\nNumber\nMethods\nTime Complexity\nDescription\n\n\n\n\n1\n삽입(Push)\n\\(O(1)\\)\n스택에 원소를 삽입하는 연산\n\n\n2\n추출(Pop)\n\\(O(1)\\)\n스택에서 원소를 추출하는 연산\n\n\n3\n최상위 원소 (Top)\n\\(O(1)\\)\n스택의 최상위 원소(마지막에 들어온 원소) 를 확인(조회)하는 연산\n\n\n4\nEmpty\n\\(O(1)\\)\n스택이 비어 있는지 확인하는 연산\n\n\n\n\n\n\nSee Table 1."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_stack/index.html#python에서-스택을-구현하는-방법-1-리스트-자료형",
    "href": "docs/blog/posts/Engineering/2023-01-19_stack/index.html#python에서-스택을-구현하는-방법-1-리스트-자료형",
    "title": "Data Structure (5) Stack",
    "section": "",
    "text": "파이썬의 기본적인 리스트 자료형은 다음의 두 가지 메서드를 제공한다.\nappend() 메서드: 마지막 위치에 원소를 삽입하며, 시간 복잡도는 \\(O(1)\\) 이다.\npop() 메서드: 마지막 위치에서 원소를 추출하며, 시간 복잡도는 \\(O(1)\\) 이다.\n따라서 일반적으로 스택을 구현할 때, 파이썬의 리스트(list) 자료형을 사용한다.\n\n\n\nCode\nclass Stack:\n    def __init__(self):\n        self.stack = []\n\n    def push(self, data):\n        # 마지막 위치에 원소 삽입\n        self.stack.append(data)\n\n    def pop(self):\n        if self.is_empty():\n            return None\n        # 마지막 원소 추출\n        return self.stack.pop()\n\n    def top(self):\n        if self.is_empty():\n            return None\n        # 마지막 원소 반환\n        return self.stack[-1]\n\n    def is_empty(self):\n        return len(self.stack) == 0\n\n\nstack = Stack()\narr = [9, 7, 2, 5, 6, 4, 2]\nfor x in arr:\n    stack.push(x)\n\nwhile not stack.is_empty():\n    print(stack.pop())\n\n\n2\n4\n6\n5\n2\n7\n9"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_stack/index.html#연결-리스트로-스택-구현하기",
    "href": "docs/blog/posts/Engineering/2023-01-19_stack/index.html#연결-리스트로-스택-구현하기",
    "title": "Data Structure (5) Stack",
    "section": "",
    "text": "스택을 연결 리스트로 구현하면, 삽입과 삭제에 있어서 \\(O(1)\\) 을 보장한다.\n연결 리스트로 구현할 때는 머리(head)를 가리키는 하나의 포인터만 가진다.\n머리(head): 남아있는 원소 중 가장 마지막에 들어 온 데이터를 가리키는 포인터\n\n\n\n\n삽입할 때는 기존의 머리 뒤에 데이터가 들어가고 포인터가 가장 최근에 삽입된 데이터를 가리키도록 머리(head) 위치를 바꿔준다.\n삭제할 때는 머리(head) 위치에서 데이터를 꺼낸다.\n\n즉, 포인터를 삭제할 데이터에 앞에 있는 데이터로 머리 위치를 바꾸는 것만으로 삭제는 이루어진다.\n\n\n\n\n\n\n삭제할 때는 머리(head) 위치에서 데이터를 꺼낸다.\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\n\nclass Stack:\n    def __init__(self):\n        self.head = None\n\n    # 원소 삽입\n    def push(self, data):\n        node = Node(data)\n        node.next = self.head\n        self.head = node\n\n    # 원소 추출하기\n    def pop(self):\n        if self.is_empty():\n            return None\n\n        # 머리(head) 위치에서 노드 꺼내기\n        data = self.head.data\n        self.head = self.head.next\n\n        return data\n\n    # 최상위 원소(top)\n    def top(self):\n        if self.is_empty():\n            return None\n        return self.head.data\n\n    # 먼저 추출할 원소부터 출력\n    def show(self):\n        cur = self.head\n        while cur:\n            print(cur.data, end=\" \")\n            cur = cur.next\n\n    # 스택이 비어있는지 확인\n    def is_empty(self):\n        return self.head is None\n\n\nstack = Stack()\narr = [9, 7, 2, 5, 6, 4, 2]\nfor x in arr:\n    stack.push(x)\nstack.show()\nprint()\n\nwhile not stack.is_empty():\n    print(stack.pop())\n\n\n2 4 6 5 2 7 9 \n2\n4\n6\n5\n2\n7\n9"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_deque/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-19_deque/index.html",
    "title": "Data Structure (7) Deque",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n덱은 스택(stack)과 큐(queue)의 기능을 모두 가지고 있다.\n그래서, 스택과 큐대신 덱을 사용해도 괜찮음\n다만, 포인터 변수가 더 많이 필요하기 때문에, 메모리는 상대적으로 더 많이 필요하다.\nPython에서는 큐(queue)의 기능이 필요할 때 간단히 덱(deque)을 사용한다.\n데이터의 삭제와 삽입 모두에서 \\(O(1)\\) 의 시간 복잡도가 소요된다.\n덱에 여러 개의 데이터를 삽입하고 삭제하는 예시를 확인해 보자.\n\n[12개의 전체 연산]\n\n좌측으로부터 삽입 연산이 가능\n우측으로부터 삽입 연산이 가능\n삭제 연산시 우측/좌측 선택적 삭제가 가능\n\n\n\n\n• 데이터의 삭제와 삽입 모두에서 \\(O(1)\\) 의 시간 복잡도가 소요된다.\n\n\n\nTable 1: a list of the deque functions in Python\n\n\n\n\n\n\n\n\n\n\n\nNumber\nMethods\nTime Complexity\nDescription\n\n\n\n\n1\nappend left\n\\(O(1)\\)\n덱의 가장 왼쪽에 새 데이터를 삽입\n\n\n2\npop left\n\\(O(1)\\)\n덱의 가장 왼쪽에서 데이터를 추출\n\n\n3\nappend right\n\\(O(1)\\)\n덱의 가장 오른쪽에 새 데이터를 삽입\n\n\n4\npop right\n\\(O(1)\\)\n덱의 가장 오른쪽에서 데이터를 추출\n\n\n\n\n\n\nSee Table 1.\n\n\n\n\nPython에서는 덱(deque) 라이브러리를 사용할 수 있다.\n아래의 모든 메서드는 최악의 경우 시간 복잡도 O 1 을 보장한다.\n우측 삽입: append()\n좌측 삽입: appendleft()\n우측 추출: pop()\n좌측 추출: popleft()\n\n\n\nCode\nfrom collections import deque\n\n\nd = deque()\narr = [5, 6, 7, 8] \nfor x in arr:\n    d.append(x) # 오른쪽 삽입\narr = [4, 3, 2, 1]\nfor x in arr:\n    d.appendleft(x) # 좌측 삽입\nprint(d)\n\nwhile d:\n    print(d.popleft()) # 좌측 삭제\n\narr = [1, 2, 3, 4, 5, 6, 7, 8]\nfor x in arr:\n    d.appendleft(x)\nprint(d)\n\nwhile True:\n    print(d.pop())\n    if not d:\n        break\n    print(d.popleft())\n    if not d:\n        break\n\n\ndeque([1, 2, 3, 4, 5, 6, 7, 8])\n1\n2\n3\n4\n5\n6\n7\n8\ndeque([8, 7, 6, 5, 4, 3, 2, 1])\n1\n8\n2\n7\n3\n6\n4\n5\n\n\n\n\n\n기본적인 Python의 리스트 자료형은 큐(queue)의 기능을 제공하지 않는다.\n가능하다면 Python에서 제공하는 덱(deque) 라이브러리를 사용한다.\n큐(queue)의 기능이 필요할 때는 덱 라이브러리를 사용하는 것을 추천한다.\n삽입과 삭제에 대하여 모두 시간 복잡도 \\(O(1)\\) 이 요구된다.\n\n\n\n\n\n\n덱(deque)을 연결 리스트로 구현하면, 삽입과 삭제에 있어서 O 1 을 보장할 수 있다.\n연결 리스트로 구현할 때는 앞(front)과 뒤(rear) 두 개의 포인터를 가진다.\n앞(front): 가장 좌측에 있는 데이터를 가리키는 포인터\n뒤(rear): 가장 우측에 있는 데이터를 가리키는 포인터\n삽입과 삭제의 구현 방법은 스택 및 큐와 유사하다.\n앞(front)과 뒤(rear)에 대하여 대칭적으로 로직이 구현될 수 있다.\n\n\n\n\n좌측 삽입할 때는 앞(front) 위치에 데이터를 넣는다.\n새로운 데이터가 삽입되었을 때 front data와 연결이 먼저 된 후 front data의 이전 노드가 새로운 데이터가 되도록 설정\n\n\n\n\n\n삭제할 때는 앞(front) 위치에서 데이터를 꺼낸다. 즉, 그냥 front를 그 다음 데이터로 설정하면 됨\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\n\nclass Deque:\n    def __init__(self):\n        self.front = None\n        self.rear = None\n        self.size = 0\n\n    def appendleft(self, data):\n        node = Node(data)\n        if self.front == None:\n            self.front = node\n            self.rear = node\n        else:\n            node.next = self.front\n            self.front.prev = node\n            self.front = node\n        self.size += 1\n\n    def append(self, data):\n        node = Node(data)\n        if self.rear == None:\n            self.front = node\n            self.rear = node\n        else:\n            node.prev = self.rear\n            self.rear.next = node\n            self.rear = node\n        self.size += 1\n\n    def popleft(self):\n        if self.size == 0:\n            return None\n        # 앞에서 노드 꺼내기\n        data = self.front.data\n        self.front = self.front.next\n        # 삭제로 인해 노드가 하나도 없는 경우\n        if self.front == None:\n            self.rear = None\n        else:\n            self.front.prev = None\n        self.size -= 1\n        return data\n\n    def pop(self):\n        if self.size == 0:\n            return None\n        # 뒤에서 노드 꺼내기\n        data = self.rear.data\n        self.rear = self.rear.prev\n        # 삭제로 인해 노드가 하나도 없는 경우\n        if self.rear == None:\n            self.front = None\n        else:\n            self.rear.next = None\n        self.size -= 1\n        return data\n\n    def front(self):\n        if self.size == 0:\n            return None\n        return self.front.data\n\n    def rear(self):\n        if self.size == 0:\n            return None\n        return self.rear.data\n\n    # 앞에서부터 원소 출력\n    def show(self):\n        cur = self.front\n        while cur:\n            print(cur.data, end=\" \")\n            cur = cur.next\n\n\nd = Deque()\narr = [5, 6, 7, 8]\nfor x in arr:\n    d.append(x)\narr = [4, 3, 2, 1]\nfor x in arr:\n    d.appendleft(x)\nd.show()\n\nprint()\nwhile d.size != 0:\n    print(d.popleft())\n\narr = [1, 2, 3, 4, 5, 6, 7, 8]\nfor x in arr:\n    d.appendleft(x)\nd.show()\n\nprint()\nwhile True:\n    print(d.pop())\n    if d.size == 0:\n        break\n    print(d.popleft())\n    if d.size == 0:\n        break\n\n\n1 2 3 4 5 6 7 8 \n1\n2\n3\n4\n5\n6\n7\n8\n8 7 6 5 4 3 2 1 \n1\n8\n2\n7\n3\n6\n4\n5\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_deque/index.html#덱deque",
    "href": "docs/blog/posts/Engineering/2023-01-19_deque/index.html#덱deque",
    "title": "Data Structure (7) Deque",
    "section": "",
    "text": "덱은 스택(stack)과 큐(queue)의 기능을 모두 가지고 있다.\n그래서, 스택과 큐대신 덱을 사용해도 괜찮음\n다만, 포인터 변수가 더 많이 필요하기 때문에, 메모리는 상대적으로 더 많이 필요하다.\nPython에서는 큐(queue)의 기능이 필요할 때 간단히 덱(deque)을 사용한다.\n데이터의 삭제와 삽입 모두에서 \\(O(1)\\) 의 시간 복잡도가 소요된다.\n덱에 여러 개의 데이터를 삽입하고 삭제하는 예시를 확인해 보자.\n\n[12개의 전체 연산]\n\n좌측으로부터 삽입 연산이 가능\n우측으로부터 삽입 연산이 가능\n삭제 연산시 우측/좌측 선택적 삭제가 가능"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_deque/index.html#덱deque의-시간-복잡도",
    "href": "docs/blog/posts/Engineering/2023-01-19_deque/index.html#덱deque의-시간-복잡도",
    "title": "Data Structure (7) Deque",
    "section": "",
    "text": "• 데이터의 삭제와 삽입 모두에서 \\(O(1)\\) 의 시간 복잡도가 소요된다.\n\n\n\nTable 1: a list of the deque functions in Python\n\n\n\n\n\n\n\n\n\n\n\nNumber\nMethods\nTime Complexity\nDescription\n\n\n\n\n1\nappend left\n\\(O(1)\\)\n덱의 가장 왼쪽에 새 데이터를 삽입\n\n\n2\npop left\n\\(O(1)\\)\n덱의 가장 왼쪽에서 데이터를 추출\n\n\n3\nappend right\n\\(O(1)\\)\n덱의 가장 오른쪽에 새 데이터를 삽입\n\n\n4\npop right\n\\(O(1)\\)\n덱의 가장 오른쪽에서 데이터를 추출\n\n\n\n\n\n\nSee Table 1."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_deque/index.html#파이썬의-덱deque-라이브러리",
    "href": "docs/blog/posts/Engineering/2023-01-19_deque/index.html#파이썬의-덱deque-라이브러리",
    "title": "Data Structure (7) Deque",
    "section": "",
    "text": "Python에서는 덱(deque) 라이브러리를 사용할 수 있다.\n아래의 모든 메서드는 최악의 경우 시간 복잡도 O 1 을 보장한다.\n우측 삽입: append()\n좌측 삽입: appendleft()\n우측 추출: pop()\n좌측 추출: popleft()\n\n\n\nCode\nfrom collections import deque\n\n\nd = deque()\narr = [5, 6, 7, 8] \nfor x in arr:\n    d.append(x) # 오른쪽 삽입\narr = [4, 3, 2, 1]\nfor x in arr:\n    d.appendleft(x) # 좌측 삽입\nprint(d)\n\nwhile d:\n    print(d.popleft()) # 좌측 삭제\n\narr = [1, 2, 3, 4, 5, 6, 7, 8]\nfor x in arr:\n    d.appendleft(x)\nprint(d)\n\nwhile True:\n    print(d.pop())\n    if not d:\n        break\n    print(d.popleft())\n    if not d:\n        break\n\n\ndeque([1, 2, 3, 4, 5, 6, 7, 8])\n1\n2\n3\n4\n5\n6\n7\n8\ndeque([8, 7, 6, 5, 4, 3, 2, 1])\n1\n8\n2\n7\n3\n6\n4\n5\n\n\n\n\n\n기본적인 Python의 리스트 자료형은 큐(queue)의 기능을 제공하지 않는다.\n가능하다면 Python에서 제공하는 덱(deque) 라이브러리를 사용한다.\n큐(queue)의 기능이 필요할 때는 덱 라이브러리를 사용하는 것을 추천한다.\n삽입과 삭제에 대하여 모두 시간 복잡도 \\(O(1)\\) 이 요구된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_deque/index.html#연결-리스트로-덱-구현하기",
    "href": "docs/blog/posts/Engineering/2023-01-19_deque/index.html#연결-리스트로-덱-구현하기",
    "title": "Data Structure (7) Deque",
    "section": "",
    "text": "덱(deque)을 연결 리스트로 구현하면, 삽입과 삭제에 있어서 O 1 을 보장할 수 있다.\n연결 리스트로 구현할 때는 앞(front)과 뒤(rear) 두 개의 포인터를 가진다.\n앞(front): 가장 좌측에 있는 데이터를 가리키는 포인터\n뒤(rear): 가장 우측에 있는 데이터를 가리키는 포인터\n삽입과 삭제의 구현 방법은 스택 및 큐와 유사하다.\n앞(front)과 뒤(rear)에 대하여 대칭적으로 로직이 구현될 수 있다.\n\n\n\n\n좌측 삽입할 때는 앞(front) 위치에 데이터를 넣는다.\n새로운 데이터가 삽입되었을 때 front data와 연결이 먼저 된 후 front data의 이전 노드가 새로운 데이터가 되도록 설정\n\n\n\n\n\n삭제할 때는 앞(front) 위치에서 데이터를 꺼낸다. 즉, 그냥 front를 그 다음 데이터로 설정하면 됨\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\n\nclass Deque:\n    def __init__(self):\n        self.front = None\n        self.rear = None\n        self.size = 0\n\n    def appendleft(self, data):\n        node = Node(data)\n        if self.front == None:\n            self.front = node\n            self.rear = node\n        else:\n            node.next = self.front\n            self.front.prev = node\n            self.front = node\n        self.size += 1\n\n    def append(self, data):\n        node = Node(data)\n        if self.rear == None:\n            self.front = node\n            self.rear = node\n        else:\n            node.prev = self.rear\n            self.rear.next = node\n            self.rear = node\n        self.size += 1\n\n    def popleft(self):\n        if self.size == 0:\n            return None\n        # 앞에서 노드 꺼내기\n        data = self.front.data\n        self.front = self.front.next\n        # 삭제로 인해 노드가 하나도 없는 경우\n        if self.front == None:\n            self.rear = None\n        else:\n            self.front.prev = None\n        self.size -= 1\n        return data\n\n    def pop(self):\n        if self.size == 0:\n            return None\n        # 뒤에서 노드 꺼내기\n        data = self.rear.data\n        self.rear = self.rear.prev\n        # 삭제로 인해 노드가 하나도 없는 경우\n        if self.rear == None:\n            self.front = None\n        else:\n            self.rear.next = None\n        self.size -= 1\n        return data\n\n    def front(self):\n        if self.size == 0:\n            return None\n        return self.front.data\n\n    def rear(self):\n        if self.size == 0:\n            return None\n        return self.rear.data\n\n    # 앞에서부터 원소 출력\n    def show(self):\n        cur = self.front\n        while cur:\n            print(cur.data, end=\" \")\n            cur = cur.next\n\n\nd = Deque()\narr = [5, 6, 7, 8]\nfor x in arr:\n    d.append(x)\narr = [4, 3, 2, 1]\nfor x in arr:\n    d.appendleft(x)\nd.show()\n\nprint()\nwhile d.size != 0:\n    print(d.popleft())\n\narr = [1, 2, 3, 4, 5, 6, 7, 8]\nfor x in arr:\n    d.appendleft(x)\nd.show()\n\nprint()\nwhile True:\n    print(d.pop())\n    if d.size == 0:\n        break\n    print(d.popleft())\n    if d.size == 0:\n        break\n\n\n1 2 3 4 5 6 7 8 \n1\n2\n3\n4\n5\n6\n7\n8\n8 7 6 5 4 3 2 1 \n1\n8\n2\n7\n3\n6\n4\n5"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_linked_list/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-18_linked_list/index.html",
    "title": "Data Structure (3) Linked List",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n연결 리스트는 각 노드가 한 줄로 연결되어 있는 자료 구조다.\n각 노드는 (데이터, 포인터) 형태를 가진다.\n포인터: 다음 노드의 메모리 주소를 가리키는 목적으로 사용된다.\n연결성: 각 노드의 포인터는 다음 혹은 이전 노드를 가리킨다.\n\n연결 리스트를 이용하면 다양한 자료구조를 구현할 수 있다.\n\n예시) 스택, 큐 등을 구현 가능\n\nPython은 연결 리스트를 활용하는 자료구조를 제공한다.\n연결 리스트를 실제 구현해야 하는 경우는 적지만, 그 원리 이해는 자료 구조와 클래스를 작성하는데 도움이 된다.\n\n\n\n\n\n연결 리스트와 배열(array)을 비교하여 장단점을 이해할 필요가 있다.\n특정 위치의 데이터를 삭제할 때, 일반적인 배열에서는 \\(O(N)\\) 만큼의 시간이 소요된다.\n하지만, 연결 리스트를 이용하면 단순히 연결만 끊어주면 된다.\n따라서 삭제할 위치를 정확히 알고 있는 경우 \\(O(1)\\) 의 시간이 소요된다.\n하지만 삭제할 위치를 정확히 알아내기 위해 앞의 코드를 자세히 보게 되는 소요 시간이 증가할 수 있다.\n\n\n\n\n배열에 새로운 원소를 삽입할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 배열에서 인덱스 3에 원소 “59”를 삽입할 경우, 인덱스 4 이후의 공간에 있는 데이터를 한칸씩 밀어내는 \\(O(n)\\) 만큼 소요\n\n\n\n\n\n배열에 존재하는 원소를 삭제할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 배열에서 인덱스 3에 해당하는 원소를 삭제한 후 데이터를 한칸 씩 당겨 이동 시키는 \\(O(n)\\) 만큼 소요\n따라서, 최악의 경우 시간 복잡도는 \\(O(N)\\) 이다.\n\n\n\n\n\n삽입할 위치를 알고 있다면, 물리적인 위치를 한 칸씩 옮기지 않아도 삽입할 수 있다.\n인덱스 2의 위치에 원소를 삽입할 경우 인덱스 1의 Node에서 인덱스 2에 위치할 데이터를 가리키고 인덱스 2의 node가 인덱스 3의 node를 가리키도록 만들면 된다.\n\n\n\n\n\n삭제할 위치를 알고 을 경우 연결 리스트 사용\n인덱스 2의 위치에 원소를 삭제할 경우 인덱스 1의 Node가 인덱스 3의 node를 가리키게 만들면 됨\n\n\n\n\n\n뒤에 붙일 때는 남는 공간에 마지막 노드의 다음 위치에 원소를 할당 시키면 된다.\n마지막 위치에 새로운 원소를 추가\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data # 데이터 할당\n        self.next = None # 다음 노드\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None # 첫 번째  node\n\n    # 가장 뒤에 노드 삽입\n    def append(self, data):\n        \n        if self.head == None: # 헤드(head)가 비어있는 경우\n            self.head = Node(data)\n            return\n        \n        currrent = self.head # 그렇지 않다면 마지막 노드에 새로운 노드 추가\n\n        while currrent.next is not None: # 다음 노드가 없을 때까지  \n            currrent = currrent.next # 다음 원소로 넘어감\n        currrent.next = Node(data) # 다음 노드가 없으면 새로운 데이터를 추가 \n\n    # 모든 노드를 하나씩 출력\n    def show(self):\n        currrent = self.head\n        while currrent is not None:\n            print(currrent.data, end=\" \")\n            currrent = currrent.next\n\n    # 특정 인덱스(index)의 노드 찾기\n    def search(self, index):\n        node = self.head\n        for _ in range(index):\n            node = node.next\n        return node\n\n    # 특정 인덱스(index)에 노드 삽입\n    def insert(self, index, data):\n        new = Node(data)\n        # 첫 위치에 추가하는 경우\n        if index == 0:\n            new.next = self.head\n            self.head = new\n            return\n        # 삽입할 위치의 앞 노드\n        node = self.search(index - 1)\n        next = node.next\n        node.next = new\n        new.next = next\n\n    # 특정 인덱스(index)의 노드 삭제\n    def remove(self, index):\n        # 첫 위치를 삭제하는 경우\n        if index == 0:\n            self.head = self.head.next\n            return\n        # 삭제할 위치의 앞 노드\n        front = self.search(index - 1)\n        front.next = front.next.next\n\n\nlinked_list = LinkedList()\ndata_list = [3, 5, 9, 8, 5, 6, 1, 7]\n\nfor data in data_list:\n    linked_list.append(data)\n\nprint(\"전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\nlinked_list.insert(4, 4)\nprint(\"\\n전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\nlinked_list.remove(7)\nprint(\"\\n전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\nlinked_list.insert(7, 2)\nprint(\"\\n전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\n\n전체 노드 출력: 3 5 9 8 5 6 1 7 \n전체 노드 출력: 3 5 9 8 4 5 6 1 7 \n전체 노드 출력: 3 5 9 8 4 5 6 7 \n전체 노드 출력: 3 5 9 8 4 5 6 2 7 \n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_linked_list/index.html#개요",
    "href": "docs/blog/posts/Engineering/2023-01-18_linked_list/index.html#개요",
    "title": "Data Structure (3) Linked List",
    "section": "",
    "text": "연결 리스트는 각 노드가 한 줄로 연결되어 있는 자료 구조다.\n각 노드는 (데이터, 포인터) 형태를 가진다.\n포인터: 다음 노드의 메모리 주소를 가리키는 목적으로 사용된다.\n연결성: 각 노드의 포인터는 다음 혹은 이전 노드를 가리킨다.\n\n연결 리스트를 이용하면 다양한 자료구조를 구현할 수 있다.\n\n예시) 스택, 큐 등을 구현 가능\n\nPython은 연결 리스트를 활용하는 자료구조를 제공한다.\n연결 리스트를 실제 구현해야 하는 경우는 적지만, 그 원리 이해는 자료 구조와 클래스를 작성하는데 도움이 된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_linked_list/index.html#연결-리스트linked-list-vs.-배열array",
    "href": "docs/blog/posts/Engineering/2023-01-18_linked_list/index.html#연결-리스트linked-list-vs.-배열array",
    "title": "Data Structure (3) Linked List",
    "section": "",
    "text": "연결 리스트와 배열(array)을 비교하여 장단점을 이해할 필요가 있다.\n특정 위치의 데이터를 삭제할 때, 일반적인 배열에서는 \\(O(N)\\) 만큼의 시간이 소요된다.\n하지만, 연결 리스트를 이용하면 단순히 연결만 끊어주면 된다.\n따라서 삭제할 위치를 정확히 알고 있는 경우 \\(O(1)\\) 의 시간이 소요된다.\n하지만 삭제할 위치를 정확히 알아내기 위해 앞의 코드를 자세히 보게 되는 소요 시간이 증가할 수 있다.\n\n\n\n\n배열에 새로운 원소를 삽입할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 배열에서 인덱스 3에 원소 “59”를 삽입할 경우, 인덱스 4 이후의 공간에 있는 데이터를 한칸씩 밀어내는 \\(O(n)\\) 만큼 소요\n\n\n\n\n\n배열에 존재하는 원소를 삭제할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 배열에서 인덱스 3에 해당하는 원소를 삭제한 후 데이터를 한칸 씩 당겨 이동 시키는 \\(O(n)\\) 만큼 소요\n따라서, 최악의 경우 시간 복잡도는 \\(O(N)\\) 이다.\n\n\n\n\n\n삽입할 위치를 알고 있다면, 물리적인 위치를 한 칸씩 옮기지 않아도 삽입할 수 있다.\n인덱스 2의 위치에 원소를 삽입할 경우 인덱스 1의 Node에서 인덱스 2에 위치할 데이터를 가리키고 인덱스 2의 node가 인덱스 3의 node를 가리키도록 만들면 된다.\n\n\n\n\n\n삭제할 위치를 알고 을 경우 연결 리스트 사용\n인덱스 2의 위치에 원소를 삭제할 경우 인덱스 1의 Node가 인덱스 3의 node를 가리키게 만들면 됨\n\n\n\n\n\n뒤에 붙일 때는 남는 공간에 마지막 노드의 다음 위치에 원소를 할당 시키면 된다.\n마지막 위치에 새로운 원소를 추가\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data # 데이터 할당\n        self.next = None # 다음 노드\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None # 첫 번째  node\n\n    # 가장 뒤에 노드 삽입\n    def append(self, data):\n        \n        if self.head == None: # 헤드(head)가 비어있는 경우\n            self.head = Node(data)\n            return\n        \n        currrent = self.head # 그렇지 않다면 마지막 노드에 새로운 노드 추가\n\n        while currrent.next is not None: # 다음 노드가 없을 때까지  \n            currrent = currrent.next # 다음 원소로 넘어감\n        currrent.next = Node(data) # 다음 노드가 없으면 새로운 데이터를 추가 \n\n    # 모든 노드를 하나씩 출력\n    def show(self):\n        currrent = self.head\n        while currrent is not None:\n            print(currrent.data, end=\" \")\n            currrent = currrent.next\n\n    # 특정 인덱스(index)의 노드 찾기\n    def search(self, index):\n        node = self.head\n        for _ in range(index):\n            node = node.next\n        return node\n\n    # 특정 인덱스(index)에 노드 삽입\n    def insert(self, index, data):\n        new = Node(data)\n        # 첫 위치에 추가하는 경우\n        if index == 0:\n            new.next = self.head\n            self.head = new\n            return\n        # 삽입할 위치의 앞 노드\n        node = self.search(index - 1)\n        next = node.next\n        node.next = new\n        new.next = next\n\n    # 특정 인덱스(index)의 노드 삭제\n    def remove(self, index):\n        # 첫 위치를 삭제하는 경우\n        if index == 0:\n            self.head = self.head.next\n            return\n        # 삭제할 위치의 앞 노드\n        front = self.search(index - 1)\n        front.next = front.next.next\n\n\nlinked_list = LinkedList()\ndata_list = [3, 5, 9, 8, 5, 6, 1, 7]\n\nfor data in data_list:\n    linked_list.append(data)\n\nprint(\"전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\nlinked_list.insert(4, 4)\nprint(\"\\n전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\nlinked_list.remove(7)\nprint(\"\\n전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\nlinked_list.insert(7, 2)\nprint(\"\\n전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\n\n전체 노드 출력: 3 5 9 8 5 6 1 7 \n전체 노드 출력: 3 5 9 8 4 5 6 1 7 \n전체 노드 출력: 3 5 9 8 4 5 6 7 \n전체 노드 출력: 3 5 9 8 4 5 6 2 7"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/test.html",
    "href": "docs/blog/posts/Deep Learning/test.html",
    "title": "Kwangmin Kim",
    "section": "",
    "text": "import numpy as np\n\nclass Layer:\n    def __init__(self, input_size, output_size):\n        self.weights = np.random.randn(input_size, output_size) * 0.01\n        self.biases = np.zeros((1, output_size))\n        \n    def forward(self, inputs):\n        self.inputs = inputs\n        self.output = np.dot(inputs, self.weights) + self.biases\n        return self.output\n    \n    def backward(self, grad_output):\n        self.dweights = np.dot(self.inputs.T, grad_output)\n        self.dbiases = np.sum(grad_output, axis=0, keepdims=True)\n        return np.dot(grad_output, self.weights.T)\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_derivative(x):\n    return (x &gt; 0).astype(float)\n\nclass MLP:\n    def __init__(self, input_size, hidden_size, output_size):\n        self.layer1 = Layer(input_size, hidden_size)\n        self.layer2 = Layer(hidden_size, output_size)\n        \n    def forward(self, x):\n        self.h1 = relu(self.layer1.forward(x))\n        self.y_pred = self.layer2.forward(self.h1)\n        return self.y_pred\n    \n    def backward(self, grad_output):\n        grad_h2 = self.layer2.backward(grad_output)\n        grad_h1 = grad_h2 * relu_derivative(self.h1)\n        self.layer1.backward(grad_h1)\n        \n    def get_params(self):\n        return np.concatenate([self.layer1.weights.ravel(), self.layer1.biases.ravel(),\n                               self.layer2.weights.ravel(), self.layer2.biases.ravel()])\n    \n    def set_params(self, params):\n        idx = 0\n        self.layer1.weights = params[idx:idx+self.layer1.weights.size].reshape(self.layer1.weights.shape)\n        idx += self.layer1.weights.size\n        self.layer1.biases = params[idx:idx+self.layer1.biases.size].reshape(self.layer1.biases.shape)\n        idx += self.layer1.biases.size\n        self.layer2.weights = params[idx:idx+self.layer2.weights.size].reshape(self.layer2.weights.shape)\n        idx += self.layer2.weights.size\n        self.layer2.biases = params[idx:idx+self.layer2.biases.size].reshape(self.layer2.biases.shape)\n\ndef mse_loss(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\ndef mse_derivative(y_true, y_pred):\n    return 2 * (y_pred - y_true) / y_true.size\n\ndef f_alpha(alpha, fun, x, s, args):\n    return fun(x + alpha * s, *args)\n\ndef search_golden_section(fun, dfun, x, s, args=(), delta=1.0e-2, tol=1e-15):\n    gr = (np.sqrt(5) + 1) / 2\n        \n    AL = 0.\n    FL = f_alpha(AL, fun, x, s, args)\n    AA = delta\n    FA = f_alpha(AA, fun, x, s, args)\n    while FL &lt; FA:\n        delta = 0.1*delta\n        AA = delta\n        FA = f_alpha(AA, fun, x, s, args)\n    \n    j = 1\n    AU = AA + delta * (gr**j)\n    FU = f_alpha(AU, fun, x, s, args)\n    while FA &gt; FU:\n        AL = AA\n        AA = AU\n        FL = FA\n        FA = FU\n        \n        j += 1\n        AU = AA + delta * (gr**j)\n        FU = f_alpha(AU, fun, x, s, args)\n\n    AB = AL + (AU - AL) / gr\n    FB = f_alpha(AB, fun, x, s, args)\n    \n    while abs(AA - AB) &gt; tol:\n        if f_alpha(AA, fun, x, s, args) &lt; f_alpha(AB, fun, x, s, args):\n            AU = AB\n        else:\n            AL = AA\n\n        AA = AU - (AU - AL) / gr\n        AB = AL + (AU - AL) / gr\n\n    return (AU + AL) / 2,\n\ndef objective(params, model, X, Y):\n    model.set_params(params)\n    y_pred = model.forward(X)\n    return mse_loss(Y, y_pred)\n\ndef gradient(params, model, X, Y):\n    model.set_params(params)\n    y_pred = model.forward(X)\n    grad_output = mse_derivative(Y, y_pred)\n    model.backward(grad_output)\n    return np.concatenate([model.layer1.dweights.ravel(), model.layer1.dbiases.ravel(),\n                           model.layer2.dweights.ravel(), model.layer2.dbiases.ravel()])\n\ndef compute_conjugate_gradient(f, df, x, args=(), eps=1.0e-7, max_iter=2500, verbose=False, callback=None):\n    if verbose:\n        print(\"#####START OPTIMIZATION#####\")\n        print(\"INIT POINT : {}, dtype : {}\".format(x, x.dtype))\n\n    for k in range(max_iter):\n        c = df(x, *args)\n        if np.linalg.norm(c) &lt; eps :\n            if verbose:\n                print(\"Stop criterion break Iter: {:5d}, x: {}\".format(k, x))\n                print(\"\\n\")\n            break\n\n        if k == 0 :\n            d = -c\n        else:\n            beta = (np.linalg.norm(c) / np.linalg.norm(c_old))**2\n            d = -c + beta*d\n        \n        alpha = search_golden_section(f, df, x, d, args=args)[0]\n        x = x + alpha * d\n        c_old = c.copy()    \n\n        if callback :\n            callback(x)    \n\n    else:\n        print(\"Stop max iter:{:5d} x:{}\".format(k, x)) \n\n    return x\n\n# 데이터 생성\nnp.random.seed(42)\nt = np.linspace(0, 4*np.pi, 200)\ny = np.sin(t) + 0.1 * np.random.randn(200)\n\n# 시계열 데이터 준비\nwindow_size = 5\nX, Y = [], []\nfor i in range(len(y) - window_size):\n    X.append(y[i:i+window_size])\n    Y.append(y[i+window_size])\nX, Y = np.array(X), np.array(Y).reshape(-1, 1)\n\n# 학습 데이터와 테스트 데이터 분리\nsplit = 150\nX_train, X_test = X[:split], X[split:]\nY_train, Y_test = Y[:split], Y[split:]\n\n# 모델 초기화\nmodel = MLP(window_size, 10, 1)\n\n# 학습 과정을 기록하기 위한 리스트\nlosses = []\n\n# 콜백 함수 정의\ndef callback(params):\n    loss = objective(params, model, X_train, Y_train)\n    losses.append(loss)\n    if len(losses) % 100 == 0:\n        print(f\"Iteration {len(losses)}, Loss: {loss}\")\n\n# Conjugate Gradient 최적화\ninitial_params = model.get_params()\noptimized_params = compute_conjugate_gradient(\n    f=objective,\n    df=gradient,\n    x=initial_params,\n    args=(model, X_train, Y_train),\n    eps=1e-6,\n    max_iter=250,\n    verbose=True,\n    callback=callback\n)\n\n# 최적화된 파라미터 설정\nmodel.set_params(optimized_params)\n\n# 45개의 새로운 데이터 포인트 예측\npredictions = []\nlast_window = X_test[-1]\n\nfor _ in range(45):\n    next_pred = model.forward(last_window.reshape(1, -1))\n    predictions.append(next_pred[0, 0])\n    last_window = np.roll(last_window, -1)\n    last_window[-1] = next_pred[0, 0]\n\nprint(\"45개의 예측된 데이터 포인트:\", predictions)\n\n# 학습 과정 시각화\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(losses)\nplt.title('Training Loss over Iterations')\nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.yscale('log')\nplt.show()\n\n#####START OPTIMIZATION#####\nINIT POINT : [ 0.00357787  0.00560785  0.01083051  0.01053802 -0.01377669 -0.00937825\n  0.00515035  0.00513786  0.00515048  0.03852731  0.00570891  0.01135566\n  0.00954002  0.00651391 -0.00315269  0.00758969 -0.00772825 -0.00236819\n -0.00485364  0.00081874  0.02314659 -0.01867265  0.0068626  -0.01612716\n -0.00471932  0.01088951  0.0006428  -0.01077745 -0.00715304  0.00679598\n -0.00730367  0.00216459  0.00045572 -0.006516    0.02143944  0.00633919\n -0.02025143  0.00186454 -0.00661786  0.00852433 -0.00792521 -0.00114736\n  0.00504987  0.00865755 -0.01200296 -0.00334501 -0.00474945 -0.00653329\n  0.01765454  0.00404982  0.          0.          0.          0.\n  0.          0.          0.          0.          0.          0.\n -0.01260884  0.00917862  0.02122156  0.01032465 -0.0151937  -0.00484234\n  0.01266911 -0.00707669  0.00443819  0.00774634  0.        ], dtype : float64\nIteration 100, Loss: 0.11440042084266042\nIteration 200, Loss: 0.01507915707755106\nStop max iter:  249 x:[ 8.72096846e-01  3.07112637e-01  3.68544459e-01 -5.29833576e-03\n -1.48517452e-01 -1.50218171e-02 -3.93417207e-01  7.42587903e-02\n  2.50879823e-01 -7.49307753e-01  3.01391907e-01  1.72908260e-01\n -2.35471079e-02 -1.87798432e-01 -1.65741232e-01  3.45984751e-03\n  6.98100084e-02  4.97043163e-02  6.39536915e-02 -2.04070819e-01\n  3.90768317e-01  5.95579564e-02 -4.55714739e-02  1.25904551e-01\n -2.08916307e-01  6.41806980e-03  1.82456247e-02  1.20378258e-02\n  2.32207167e-02  2.95287914e-01  3.75158766e-01  2.13783850e-01\n  2.03101606e-01  9.84731976e-02 -2.40536449e-01  8.38452825e-03\n -6.24235393e-04 -4.42662306e-03  1.05630404e-01  3.03363299e-01\n -6.47670341e-01  3.42058342e-01  4.57045290e-01  4.42891375e-02\n -3.34055217e-01  6.55082739e-03  1.24474047e+00 -5.78019821e-02\n  1.37594237e-01  1.01113518e+00 -3.39541659e-01 -9.98842379e-02\n  1.35714493e+00  3.34535880e-01 -2.85506091e+00 -2.92903142e-01\n -7.37253627e-01 -1.88816279e+00  2.32034319e-01  4.51767622e-01\n  4.66335439e-01 -6.09988947e-01  6.85306593e-01 -9.52418691e-01\n  3.83872479e+00 -2.46248885e-01 -4.25623532e-01  1.94786281e+00\n  4.47072064e-02  8.76559455e-01 -9.85043229e-01]\n45개의 예측된 데이터 포인트: [0.017654573051130273, 0.10887787987354047, 0.2676292082498076, 0.4241587016663836, 0.5871353418493556, 0.7293565133567764, 0.769934058285144, 0.7483296118877864, 0.7939457222800178, 0.8004586147055256, 0.8118970013175318, 0.8270388115233821, 0.8343008397768034, 0.8433763251860792, 0.8507529488946862, 0.8565945040237039, 0.8621399573599745, 0.8666706024561442, 0.8706019202428911, 0.8740343965540912, 0.8769428768379698, 0.8794673285777729, 0.8816382565997251, 0.8835024228679039, 0.8851112218904079, 0.8864943010574838, 0.8876849899644846, 0.8887103679720806, 0.889592649615352, 0.8903522715153432, 0.8910061496451235, 0.8915689695293968, 0.8920534789274449, 0.8924705330687056, 0.8928295345074285, 0.8931385676439253, 0.8934045813156032, 0.8936335676829165, 0.893830679792316, 0.8940003540791018, 0.8941464104149822, 0.8942721360095246, 0.8943803609323687, 0.8944735212681764, 0.8945537139127117]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_array/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-18_array/index.html",
    "title": "Data Structure (2) Array",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n가장 기본적인 자료구조다.\n여러 개의 변수를 담는 공간으로 이해할 수 있다.\ndata가 연속적으로 들어가는 형태여서 배열은 인덱스(index)가 존재하며, 인덱스는 0부터 시작한다.\n특정한 인덱스에 직접적으로 접근 가능하여 수행 시간은 빠른 속도인 \\(O(1)\\) 이다.\n\n\n\n\n\n컴퓨터의 메인 메모리에서 배열의 공간은 연속적으로 할당된다.\n장점: Cache memory(속도면에서, \\(RAM&lt;Cache&lt;CPU\\), 공간면에서, \\(CPU&lt;RAM&lt;Cache\\), CPU옆에 위치) 히트(RAM에 있는 data를 Cache에 일부 옮기는 현상) 가능성이 높으며, 조회가 빠르다. 배열 같은 경우는 공간적으로 또는 연속적으로 붙어 있기때문에 cache memory 묶어서 옮길 수 있다.\n\nCache Hit: 원하는 data가 Cache Memory존재하는 것을 의미.\n특정 index에 접근하는 속도가 매우 빠르다, \\(O(1)\\).\n\n단점: 배열의 크기를 미리 지정해야 하는 것이 일반적이므로, 데이터의 추가 및 삭제에 한계가 있다.\n\n\n\n\n\n컴퓨터의 메인 메모리(RAM)상에서 주소가 연속적이지 않다.\n배열과 다르게 크기가 정해져 있지 않고, 리스트의 크기는 동적으로 변경 가능하다.\n장점: 포인터(pointer)를 통해 다음 데이터의 위치를 가리킨다는 점에서 삽입과 삭제가 간편하다.\n단점: 원소를 검색할 때는 포인터가 앞에서부터 원소를 찾아야 하므로, 데이터 검색 속도가 느리다.\n\n\n\n\n파이썬의 리스트(List) 자료형\n\n파이썬에서는 리스트 자료형을 제공한다. (컴퓨터 공학에서의 연결 리스트와는 다른 의미)\n일반적인 프로그래밍 언어에서의 배열로 이해할 수 있다. 그러므로, 파이썬의 리스트는 배열이라고 생각해야한다.\n\n파이썬의 리스트는 배열처럼 임의의 인덱스를 이용해 직접적인 접근이 가능하다.\n\n파이썬의 리스트 자료형은 동적 배열이다.\n\nappend를 이용해 데이터를 삽입할 때 배열의 용량이 가득 차면, 자동으로 크기를 증가시킨다.\n\n내부적으로 포인터(pointer)를 사용하여, 연결 리스트의 장점도 가지고 있다.\n배열(array) 혹은 스택(stack)의 기능이 필요할 때 리스트 자료형을 그대로 사용할 수 있다.\n큐(queue)의 기능을 제공하지 못한다. (비효율적)\n\n\n\n\n\n파이썬에서는 임의의 크기를 가지는 배열을 만들 수 있다.\n일반적으로 리스트를 초기화할 때 리스트 컴프리헨션(list comprehension)이 자주 사용된다. (매우 편리)\n크기가 N인 1차원 배열을 만드는 방법은 다음과 같다.\n\n\n\nCode\n# [0, 0, 0, 0, 0]\nn = 5\narr = [0] * n\nprint(arr)\n\n# [0, 1, 2, 3, 4]\nn = 5\narr = [i for i in range(n)]\nprint(arr)\n\n\n[0, 0, 0, 0, 0]\n[0, 1, 2, 3, 4]\n\n\n\n크기가 \\(N \\times M\\) 인 2차원 리스트(배열) 만들기 1\n\n2차원 배열이 필요할 때는 다음과 같이 초기화한다.\n\n\n\n\nCode\nn = 3\nm = 5\narr = [[0] * m for i in range(n)]\nprint(arr)\n\n\n[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n\n\n\n크기가 \\(N \\times M\\) 인 2차원 리스트(배열) 만들기 2\n\n\n\nCode\nn = 3\nm = 5\narr = [[i * m + j for j in range(m)] for i in range(n)]\nprint(arr)\n\n\n[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]\n\n\n\n\n\n\n리스트는 기본적으로 메모리 주소를 반환한다.\n따라서 단순히 [[0]∗m]∗n 형태로 배열을 초기화하면 안 된다.\n이렇게 초기화를 하게 되면, n개의 [0]∗m 리스트는 모두 같은 객체로 인식된다.\n다시 말해, 같은 메모리를(동일한 리스트를) 가리키는 n개의 원소를 담는 리스트가 된다.\n2차원 배열을 초기화할 때는 리스트 컴프리헨션을 이용하는 것이 일반적이다.\n\n\n\nCode\nn = 3\nm = 5\narr1 = [[0] * m] * n # 잘못된 방식\narr2 = [[0] * m for i in range(n)] # 옳은 방식\n\narr1[1][3] = 7\narr2[1][3] = 7\n\nprint(arr1)\nprint(arr2)\n\n\n[[0, 0, 0, 7, 0], [0, 0, 0, 7, 0], [0, 0, 0, 7, 0]]\n[[0, 0, 0, 0, 0], [0, 0, 0, 7, 0], [0, 0, 0, 0, 0]]\n\n\n\n\n위의 결과를 보면, 잘못된 방식으로 초기화된 배열 arr1은 [[0, 0, 0, 7, 0], [0, 0, 0, 7, 0], [0, 0, 0, 7, 0]]와 같이 7의 삽입이 모든 행에 걸쳐서 적용됐다. 반면에, 올바른 방식으로 초기화된 arr2는 [[0, 0, 0, 0, 0], [0, 0, 0, 7, 0], [0, 0, 0, 0, 0]]는 의도된 대로 하나의 element가 [1][3] index에 삽입이 된 것을 볼 수 있다.\n\n\n\n\n\n\n자신이 원하는 임의의 값을 넣어 곧바로 사용할 수 있다.\n\n\n\nCode\narr = [0, 1, 2, 3, 4, 5, 6, 7, 8]\nprint(arr)\n\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_array/index.html#배열의-개요",
    "href": "docs/blog/posts/Engineering/2023-01-18_array/index.html#배열의-개요",
    "title": "Data Structure (2) Array",
    "section": "",
    "text": "가장 기본적인 자료구조다.\n여러 개의 변수를 담는 공간으로 이해할 수 있다.\ndata가 연속적으로 들어가는 형태여서 배열은 인덱스(index)가 존재하며, 인덱스는 0부터 시작한다.\n특정한 인덱스에 직접적으로 접근 가능하여 수행 시간은 빠른 속도인 \\(O(1)\\) 이다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_array/index.html#배열의-특징",
    "href": "docs/blog/posts/Engineering/2023-01-18_array/index.html#배열의-특징",
    "title": "Data Structure (2) Array",
    "section": "",
    "text": "컴퓨터의 메인 메모리에서 배열의 공간은 연속적으로 할당된다.\n장점: Cache memory(속도면에서, \\(RAM&lt;Cache&lt;CPU\\), 공간면에서, \\(CPU&lt;RAM&lt;Cache\\), CPU옆에 위치) 히트(RAM에 있는 data를 Cache에 일부 옮기는 현상) 가능성이 높으며, 조회가 빠르다. 배열 같은 경우는 공간적으로 또는 연속적으로 붙어 있기때문에 cache memory 묶어서 옮길 수 있다.\n\nCache Hit: 원하는 data가 Cache Memory존재하는 것을 의미.\n특정 index에 접근하는 속도가 매우 빠르다, \\(O(1)\\).\n\n단점: 배열의 크기를 미리 지정해야 하는 것이 일반적이므로, 데이터의 추가 및 삭제에 한계가 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_array/index.html#연결리스트-linked-list",
    "href": "docs/blog/posts/Engineering/2023-01-18_array/index.html#연결리스트-linked-list",
    "title": "Data Structure (2) Array",
    "section": "",
    "text": "컴퓨터의 메인 메모리(RAM)상에서 주소가 연속적이지 않다.\n배열과 다르게 크기가 정해져 있지 않고, 리스트의 크기는 동적으로 변경 가능하다.\n장점: 포인터(pointer)를 통해 다음 데이터의 위치를 가리킨다는 점에서 삽입과 삭제가 간편하다.\n단점: 원소를 검색할 때는 포인터가 앞에서부터 원소를 찾아야 하므로, 데이터 검색 속도가 느리다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_array/index.html#파이썬의-리스트-list-자료형",
    "href": "docs/blog/posts/Engineering/2023-01-18_array/index.html#파이썬의-리스트-list-자료형",
    "title": "Data Structure (2) Array",
    "section": "",
    "text": "파이썬의 리스트(List) 자료형\n\n파이썬에서는 리스트 자료형을 제공한다. (컴퓨터 공학에서의 연결 리스트와는 다른 의미)\n일반적인 프로그래밍 언어에서의 배열로 이해할 수 있다. 그러므로, 파이썬의 리스트는 배열이라고 생각해야한다.\n\n파이썬의 리스트는 배열처럼 임의의 인덱스를 이용해 직접적인 접근이 가능하다.\n\n파이썬의 리스트 자료형은 동적 배열이다.\n\nappend를 이용해 데이터를 삽입할 때 배열의 용량이 가득 차면, 자동으로 크기를 증가시킨다.\n\n내부적으로 포인터(pointer)를 사용하여, 연결 리스트의 장점도 가지고 있다.\n배열(array) 혹은 스택(stack)의 기능이 필요할 때 리스트 자료형을 그대로 사용할 수 있다.\n큐(queue)의 기능을 제공하지 못한다. (비효율적)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_array/index.html#리스트-컴프리헨션-list-comprehension",
    "href": "docs/blog/posts/Engineering/2023-01-18_array/index.html#리스트-컴프리헨션-list-comprehension",
    "title": "Data Structure (2) Array",
    "section": "",
    "text": "파이썬에서는 임의의 크기를 가지는 배열을 만들 수 있다.\n일반적으로 리스트를 초기화할 때 리스트 컴프리헨션(list comprehension)이 자주 사용된다. (매우 편리)\n크기가 N인 1차원 배열을 만드는 방법은 다음과 같다.\n\n\n\nCode\n# [0, 0, 0, 0, 0]\nn = 5\narr = [0] * n\nprint(arr)\n\n# [0, 1, 2, 3, 4]\nn = 5\narr = [i for i in range(n)]\nprint(arr)\n\n\n[0, 0, 0, 0, 0]\n[0, 1, 2, 3, 4]\n\n\n\n크기가 \\(N \\times M\\) 인 2차원 리스트(배열) 만들기 1\n\n2차원 배열이 필요할 때는 다음과 같이 초기화한다.\n\n\n\n\nCode\nn = 3\nm = 5\narr = [[0] * m for i in range(n)]\nprint(arr)\n\n\n[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n\n\n\n크기가 \\(N \\times M\\) 인 2차원 리스트(배열) 만들기 2\n\n\n\nCode\nn = 3\nm = 5\narr = [[i * m + j for j in range(m)] for i in range(n)]\nprint(arr)\n\n\n[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_array/index.html#배열을-초기화할-때-유의할-점",
    "href": "docs/blog/posts/Engineering/2023-01-18_array/index.html#배열을-초기화할-때-유의할-점",
    "title": "Data Structure (2) Array",
    "section": "",
    "text": "리스트는 기본적으로 메모리 주소를 반환한다.\n따라서 단순히 [[0]∗m]∗n 형태로 배열을 초기화하면 안 된다.\n이렇게 초기화를 하게 되면, n개의 [0]∗m 리스트는 모두 같은 객체로 인식된다.\n다시 말해, 같은 메모리를(동일한 리스트를) 가리키는 n개의 원소를 담는 리스트가 된다.\n2차원 배열을 초기화할 때는 리스트 컴프리헨션을 이용하는 것이 일반적이다.\n\n\n\nCode\nn = 3\nm = 5\narr1 = [[0] * m] * n # 잘못된 방식\narr2 = [[0] * m for i in range(n)] # 옳은 방식\n\narr1[1][3] = 7\narr2[1][3] = 7\n\nprint(arr1)\nprint(arr2)\n\n\n[[0, 0, 0, 7, 0], [0, 0, 0, 7, 0], [0, 0, 0, 7, 0]]\n[[0, 0, 0, 0, 0], [0, 0, 0, 7, 0], [0, 0, 0, 0, 0]]\n\n\n\n\n위의 결과를 보면, 잘못된 방식으로 초기화된 배열 arr1은 [[0, 0, 0, 7, 0], [0, 0, 0, 7, 0], [0, 0, 0, 7, 0]]와 같이 7의 삽입이 모든 행에 걸쳐서 적용됐다. 반면에, 올바른 방식으로 초기화된 arr2는 [[0, 0, 0, 0, 0], [0, 0, 0, 7, 0], [0, 0, 0, 0, 0]]는 의도된 대로 하나의 element가 [1][3] index에 삽입이 된 것을 볼 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_array/index.html#배열-직접-초기화",
    "href": "docs/blog/posts/Engineering/2023-01-18_array/index.html#배열-직접-초기화",
    "title": "Data Structure (2) Array",
    "section": "",
    "text": "자신이 원하는 임의의 값을 넣어 곧바로 사용할 수 있다.\n\n\n\nCode\narr = [0, 1, 2, 3, 4, 5, 6, 7, 8]\nprint(arr)\n\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_python_list/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-18_python_list/index.html",
    "title": "Data Structure (4) Python List",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\nTable 1: a list of the list functions in Python\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nMethods\nTime Complexity\nExamples\nDescription\n\n\n\n\n1\nIndexing\n\\(O(1)\\)\narr[i]\n특정 i th 인덱스의 값 반환\n\n\n2\nStoring\n\\(O(1)\\)\narr[i] = 1\n특정 i th 인덱스에 값 (=1) 할당\n\n\n3\nAppend\n\\(O(1)\\)\narr.append(5)\n리스트의 가장 뒤에 데이터 추가\n\n\n4\nPop\n\\(O(1)\\)\narr.pop()\n리스트의 가장 뒤에서 원소 꺼내기\n\n\n5\nLength\n\\(O(1)\\)\nlen(arr)\n리스트의 길이 얻기\n\n\n6\nClear\n\\(O(1)\\)\narr.clear()\n리스트 내 모든 원소 제거하기\n\n\n7\nSlicing\n\\(O(b-a)\\)\narr[a:b]\n리스트에서 인덱스 a부터 b-1까지의 원소만 꺼내 새 리스트 만들기\n\n\n8\nExtend\n\\(O(len(other))\\)\narr.extend(list2)\n기존 리스트, list1에 다른 리스트, list2를 이어 붙이기\n\n\n9\nInsertion\n\\(O(N)\\)\narr.insert(index, x)\n특정 인덱스에 데이터 x를 삽입하기, 즉 i th index를 뒤로 밀고 추가\n\n\n10\nDelete\n\\(O(N)\\)\ndel arr[index]\n특정 인덱스의 데이터 삭제하기\n\n\n11\nConstruction\n\\(O(len(other))\\)\narr = list(other)\n다른 자료구조의 원소들을 이용해 리스트로 만들기\n\n\n12\nIn\n\\(O(N)\\)\nx in arr\n데이터 x가 리스트에 존재하는지 확인\n\n\n13\nNot in\n\\(O(N)\\)\nx not in arr\n데이터 x가 리스트에 존재하지 않는지 확인\n\n\n14\nPop\n\\(O(N)\\)\narr.pop(index)\n특정 인덱스의 데이터를 꺼내기 / 단, 가장 뒤 원소를 꺼내는 경우 O(1)\n\n\n15\nRemove\n\\(O(N)\\)\narr.remove(x)\n리스트 내에 존재하는 데이터 x를 삭제\n\n\n16\nCopy\n\\(O(N)\\)\narr.copy()\n리스트를 복제\n\n\n17\nMin\n\\(O(N)\\)\nmin(arr)\n리스트 내에 존재하는 가장 작은 원소\n\n\n18\nMax\n\\(O(N)\\)\nmax(arr)\n리스트 내에 존재하는 가장 큰 원소\n\n\n19\nIteration\n\\(O(N)\\)\nfor x in arr\n리스트 내에 존재하는 모든 원소 순회\n\n\n20\nMultiply\n\\(O(k*N)\\)\narr * k\n리스트를 k번 반복하여 길게 만들기\n\n\n21\nSort\n\\(O(NlogN)\\)\narr.sort()\n리스트 내 존재하는 원소를 정렬\n\n\n\n\n\n\nSee Table 1.\n\n1~6: 파이썬의 list는 동적 배열의 특징이 있다. 시간 복잡도는 모두 \\(O(1)\\) 이다.\n\n3~4: 사실상 stack의 기능과 동일\n\n\n\n\nCode\narr = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(arr[4]) # 인덱싱(indexing)\n\n# 저장(storing)\narr[7] = 10\n\n# 뒤에 붙이기(append)\narr.append(10)\nprint(arr)\n\n# 뒤에서 꺼내기(pop)\narr.pop()\nprint(arr)\n\n# 길이(length)\nprint(len(arr))\n\n# 배열 비우기(clear)\narr.clear()\nprint(arr)\n\n\n4\n[0, 1, 2, 3, 4, 5, 6, 10, 8, 9, 10]\n[0, 1, 2, 3, 4, 5, 6, 10, 8, 9]\n10\n[]\n\n\n\n7~11\n\n\n\nCode\narr = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nnew_arr = arr[2:7] # 슬라이싱(slicing)\nprint(new_arr)\n\narr1 = [0, 1, 2, 3, 4]\narr2 = [5, 6, 7, 8, 9]\narr1.extend(arr2) # 확장(extend)\nprint(arr1)\n\narr = [0, 1, 2, 3, 4]\narr.insert(3, 7) # 삽입(insertion)\nprint(arr)\n\ndel arr[3] # 삭제(delete)\nprint(arr)\n\ndata = {7, 8, 9}\narr = list(data) # 다른 자료구조로 리스트 만들기\nprint(arr)\n\n\n[2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1, 2, 7, 3, 4]\n[0, 1, 2, 3, 4]\n[8, 9, 7]\n\n\n\n12~16\n\n\n\nCode\narr = [0, 1, 2, 3, 4]\n\nprint(3 in arr) # 존재 여부(in)\nprint(7 not in arr) # 비존재 여부(not in)\n\narr.pop(1) # 인덱스 1에 해당하는 원소 꺼내기(pop)\nprint(arr)\n\narr.remove(3) # 리스트의 특정 원소 삭제(remove)\nprint(arr)\n\nnew_arr = arr.copy() # 복제(copy)\nprint(new_arr)\n\n\nTrue\nTrue\n[0, 2, 3, 4]\n[0, 2, 4]\n[0, 2, 4]\n\n\n\n17~21\n\n\n\nCode\narr = [3, 5, 4, 1, 2]\n\nprint(min(arr)) # 최소(min)\nprint(max(arr)) # 최대(max)\n\nfor x in arr: # 원소 순회(iteration)\n    print(x, end=\" \")\nprint()\n\nprint(arr * 2) # 리스트 반복하여 곱하기(multiply)\n\narr.sort() # 정렬(sorting)\nprint(arr)\n\n\n1\n5\n3 5 4 1 2 \n[3, 5, 4, 1, 2, 3, 5, 4, 1, 2]\n[1, 2, 3, 4, 5]\n\n\n\n\n\n\n\nGo to Blog Content List\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_queue/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-19_queue/index.html",
    "title": "Data Structure (6) Queue",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n큐(queue)는 먼저 삽입된 데이터가 먼저 추출되는 자료구조(data structure)다. (First-In First-Out)\n딥러닝 모델에 들어가는 데이터 순서대로 들어가는데 먼저 들어간 데이터는 먼저 나오게 할때 사용되는 자료 구조이다.\n\n\n\n\n\n큐를 연결 리스트로 구현하면, 삽입과 삭제에 있어서 \\(O(1)\\) 을 보장할 수 있다.\n연결 리스트로 구현할 때는 머리(head)와 꼬리(tail) 두 개의 포인터를 가진다.\n머리(head): 남아있는 원소 중 가장 먼저 들어 온 데이터를 가리키는 포인터\n꼬리(tail): 남아있는 원소 중 가장 마지막에 들어 온 데이터를 가리키는 포인터\n\n\n\n\n삽입할 때는 꼬리(tail) 위치에 데이터를 넣는다.\n값으로 8을 갖는 새로운 데이터가 삽입되었을 때 예시)\n\n\n\n\n\n삭제할 때는 머리(head) 위치에서 데이터를 꺼낸다.\n하나의 데이터를 삭제할 때의 예시)\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\n\nclass Queue:\n    def __init__(self):\n        self.head = None\n        self.tail = None\n\n    def enqueue(self, data):\n        node = Node(data)\n        if self.head == None:\n            self.head = node\n            self.tail = node\n        # 꼬리(tail) 위치에 새로운 노드 삽입\n        else:\n            self.tail.next = node\n            self.tail = self.tail.next\n\n    def dequeue(self):\n        if self.head == None:\n            return None\n\n        # 머리(head) 위치에서 노드 꺼내기\n        data = self.head.data\n        self.head = self.head.next\n\n        return data\n\n    def show(self):\n        cur = self.head\n        while cur:\n            print(cur.data, end=\" \")\n            cur = cur.next\n\n\nqueue = Queue()\ndata_list = [3, 5, 9, 8, 5, 6, 1, 7]\n\nfor data in data_list:\n    queue.enqueue(data)\n\nprint(\"\\n전체 노드 출력:\", end=\" \")\nqueue.show()\n\nprint(\"\\n[원소 삭제]\")\nprint(queue.dequeue())\nprint(queue.dequeue())\nprint(queue.dequeue())\n\nprint(\"[원소 삽입]\")\nqueue.enqueue(2)\nqueue.enqueue(5)\nqueue.enqueue(3)\n\nprint(\"전체 노드 출력:\", end=\" \")\nqueue.show()\n\n\n\n전체 노드 출력: 3 5 9 8 5 6 1 7 \n[원소 삭제]\n3\n5\n9\n[원소 삽입]\n전체 노드 출력: 8 5 6 1 7 2 5 3 \n\n\n\n\n\n\n다수의 데이터를 삽입 및 삭제할 때에 대하여, 수행 시간을 측정할 수 있다.\n단순히 Python의 리스트 자료형을 이용할 때보다 수행 시간 관점에서 효율적이다.\n\n\n\nCode\nimport time\n\ndata_list = [i for i in range(100000)]\n\nstart_time = time.time()\n\nqueue = []\nfor data in data_list:\n    queue.append(data)\nwhile queue:\n    queue.pop(0)\n\nprint(f\"Elapsed time: {time.time() - start_time} seconds.\")\nprint(queue)\n\nstart_time = time.time()\n\nqueue = Queue()\nfor data in data_list:\n    queue.enqueue(data)\nwhile queue.head != None:\n    queue.dequeue()\n\nprint(f\"Elapsed time: {time.time() - start_time} seconds.\")\nqueue.show()\n\n\nElapsed time: 0.8263566493988037 seconds.\n[]\nElapsed time: 0.19398808479309082 seconds.\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_queue/index.html#queue",
    "href": "docs/blog/posts/Engineering/2023-01-19_queue/index.html#queue",
    "title": "Data Structure (6) Queue",
    "section": "",
    "text": "큐(queue)는 먼저 삽입된 데이터가 먼저 추출되는 자료구조(data structure)다. (First-In First-Out)\n딥러닝 모델에 들어가는 데이터 순서대로 들어가는데 먼저 들어간 데이터는 먼저 나오게 할때 사용되는 자료 구조이다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_queue/index.html#연결-리스트로-큐-구현",
    "href": "docs/blog/posts/Engineering/2023-01-19_queue/index.html#연결-리스트로-큐-구현",
    "title": "Data Structure (6) Queue",
    "section": "",
    "text": "큐를 연결 리스트로 구현하면, 삽입과 삭제에 있어서 \\(O(1)\\) 을 보장할 수 있다.\n연결 리스트로 구현할 때는 머리(head)와 꼬리(tail) 두 개의 포인터를 가진다.\n머리(head): 남아있는 원소 중 가장 먼저 들어 온 데이터를 가리키는 포인터\n꼬리(tail): 남아있는 원소 중 가장 마지막에 들어 온 데이터를 가리키는 포인터\n\n\n\n\n삽입할 때는 꼬리(tail) 위치에 데이터를 넣는다.\n값으로 8을 갖는 새로운 데이터가 삽입되었을 때 예시)\n\n\n\n\n\n삭제할 때는 머리(head) 위치에서 데이터를 꺼낸다.\n하나의 데이터를 삭제할 때의 예시)\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\n\nclass Queue:\n    def __init__(self):\n        self.head = None\n        self.tail = None\n\n    def enqueue(self, data):\n        node = Node(data)\n        if self.head == None:\n            self.head = node\n            self.tail = node\n        # 꼬리(tail) 위치에 새로운 노드 삽입\n        else:\n            self.tail.next = node\n            self.tail = self.tail.next\n\n    def dequeue(self):\n        if self.head == None:\n            return None\n\n        # 머리(head) 위치에서 노드 꺼내기\n        data = self.head.data\n        self.head = self.head.next\n\n        return data\n\n    def show(self):\n        cur = self.head\n        while cur:\n            print(cur.data, end=\" \")\n            cur = cur.next\n\n\nqueue = Queue()\ndata_list = [3, 5, 9, 8, 5, 6, 1, 7]\n\nfor data in data_list:\n    queue.enqueue(data)\n\nprint(\"\\n전체 노드 출력:\", end=\" \")\nqueue.show()\n\nprint(\"\\n[원소 삭제]\")\nprint(queue.dequeue())\nprint(queue.dequeue())\nprint(queue.dequeue())\n\nprint(\"[원소 삽입]\")\nqueue.enqueue(2)\nqueue.enqueue(5)\nqueue.enqueue(3)\n\nprint(\"전체 노드 출력:\", end=\" \")\nqueue.show()\n\n\n\n전체 노드 출력: 3 5 9 8 5 6 1 7 \n[원소 삭제]\n3\n5\n9\n[원소 삽입]\n전체 노드 출력: 8 5 6 1 7 2 5 3 \n\n\n\n\n\n\n다수의 데이터를 삽입 및 삭제할 때에 대하여, 수행 시간을 측정할 수 있다.\n단순히 Python의 리스트 자료형을 이용할 때보다 수행 시간 관점에서 효율적이다.\n\n\n\nCode\nimport time\n\ndata_list = [i for i in range(100000)]\n\nstart_time = time.time()\n\nqueue = []\nfor data in data_list:\n    queue.append(data)\nwhile queue:\n    queue.pop(0)\n\nprint(f\"Elapsed time: {time.time() - start_time} seconds.\")\nprint(queue)\n\nstart_time = time.time()\n\nqueue = Queue()\nfor data in data_list:\n    queue.enqueue(data)\nwhile queue.head != None:\n    queue.dequeue()\n\nprint(f\"Elapsed time: {time.time() - start_time} seconds.\")\nqueue.show()\n\n\nElapsed time: 0.8263566493988037 seconds.\n[]\nElapsed time: 0.19398808479309082 seconds."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html",
    "title": "Data Structure (8) Binary Search Tree",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n트리는 가계도와 같이 계층적인 구조를 표현할 때 사용할 수 있는 자료구조다.\n나무(tree)의 형태를 뒤집은 것과 같이 생겼다.\n다수의 데이터를 관리하기에 적합한 트리 자료 구조의 가장 기본적인 형태\n\n\n\n\n\n루트 노드(root node): 부모가 없는 최상위 노드\n단말 노드(leaf node): 자식이 없는 노드\n트리(tree)에서는 부모와 자식 관계가 성립한다 (직계).\n형제 관계 (sibling, 방계): 부모 node로 부터 왼쪽 자식과 오른쪽 자식과의 관계\n깊이(depth): 루트 노드에서의 길이(length), 루트 노드로부터 손자까지의 depth=2\n\n이때, 길이란 출발 노드에서 목적지 노드까지 거쳐야 하는 간선의 수를 의미한다.\n\n트리의 높이(height)은 루트 노드에서 가장 깊은 노드까지의 길이를 의미한다.\n\n\n\n\n\n이진 트리는 최대 2개의 자식을 가질 수 있는 트리를 말한다.\n\n\n\n\n\n다수의 데이터를 관리(조회, 저장, 삭제)하기 위한 가장 기본적인 자료구조 중 하나다.\n이진 탐색 트리의 성질: 순서가 있음\n\n왼쪽 자식 노드 &lt; 부모 노드 &lt; 오른쪽 자식 노드\n루트 노드 기준 모든 왼쪽 노드들은 루트 노드보다 작음\n루트 노드 기준 모든 오른쪽 노드들은 루트 노드보다 큼\n2진 탐색을 가능하게 하는 구조\n\n\n\n\n\n특정한 노드의 키(key) 값보다 그 왼쪽 자식 노드의 키(key) 값이 더 작다.\n특정한 노드의 키(key) 값보다 그 오른쪽 자식 노드의 키(key) 값이 더 크다.\n특정한 노드의 왼쪽 서브 트리, 오른쪽 서브 트리 모두 이진 탐색 트리다.\nworst case: 찾는게 없을 때 혹은 찾고자 하는 데이터가 가장 마지막에 있을 때\n\n탐색시 재귀적으로 중앙값을 기준으로 오른쪽만 찾음\n매 실행마다 데이터의 개수가 절반씩 줄어듬\n그러면, 몇 번만에 사이즈가 1이 되는가?\n수식 유도, input size를 N이라고 가정했을때\n\\(N \\times {(\\frac{1}{2})}^{k}=1 \\rightarrow N=2^k \\rightarrow k = log_2N\\)\n위의 수식을 점근적 표기법으로 표현하면 \\(\\Theta(logN)\\)\n\nbest case: 한번에 찾았을 때\n\n\\(\\Theta(1)\\)\n\n그러므로, lower bound = \\(\\Theta(1)\\), upper bound = \\(O(logN)\\)\n\n\n\n\n\n루트 노드에서 출발하여 아래쪽으로 내려오면서, 삽입할 위치를 찾는다.\n\n삽입할 노드의 키(key)가 작으면 왼쪽으로,\n삽입할 노드의 키(key)가 크면 오른쪽으로 삽입\n\n삽입할 노드 목록 예시: [7,4,5,9,6,2,3,2,8]으로 트리 생성해보기\n\n\n\n\nBinary Tree\n\n\nSorcue: 코딩 테스트를 위한 트리(Tree) 자료구조 10분 핵심 요약 By 동빈나\n\n\n\n\n루트 노드에서 출발하여 아래쪽으로 내려오면서, 찾고자 하는 원소를 조회한다. 삽입 연산과 같은 로직을 따름\n1 삽입할 노드의 키(key)가 작으면 왼쪽으로, 2 삽입할 노드의 키(key)가 크면 오른쪽으로 조회\n조회할 노드 목록 예시: 5번 노드\n\n\n\n\n\n루트 노드에서 출발하여 아래쪽으로 내려오면서, 삭제할 원소에 접근한다.\n삭제할 노드 목록 예시: 7번 노드\n\nCase #1 왼쪽 자식이 없는 경우 → 오른쪽 자식으로 대체\nCase #2 오른쪽 자식이 없는 경우 → 왼쪽 자식으로 대체\nCase #3 왼쪽, 오른쪽이 모두 있는 경우 → 오른쪽 서브\n\n트리에서 가장 작은 노드로 대체\n삭제할 노드 목록 예시: 4번 노드\n\n\n\n\nBinary Tree Deletion\n\n\nSorcue: 코딩 테스트를 위한 트리(Tree) 자료구조 10분 핵심 요약 By 동빈나\n\n\n\n\n\n트리에 포함되어 있는 정보를 모두 출력하고자 할 때, 어떤 방식을 사용할 수 있을까?\n바로 순회(traversal)를 사용할 수 있다.\n트리의 모든 노드를 특정한 순서(조건)에 따라서 방문하는 방법을 순회(traversal)라고 한다.\n\n\n전위 순회(pre-order traverse): 루트 방문 → 왼쪽 자식 방문 → 오른쪽 자식 방문\n중위 순회(in-order traverse): 왼쪽 자식 방문 → 루트 방문 → 오른쪽 자식 방문\n후위 순회(post-order traverse): 왼쪽 자식 방문 → 오른쪽 자식 방문 → 루트 방문\n\n\n\n\n전위 순회(pre-order traverse): A → B → D → E → C → F → G\n중위 순회(in-order traverse): D → B → E → A → F → C → G\n후위 순회(post-order traverse): D → E → B → F → G → C → A\n\n\n\n\nBinary Tree Traverse\n\n\nSorcue: 코딩 테스트를 위한 트리(Tree) 자료구조 10분 핵심 요약 By 동빈나\n\n\n\n• 방문 방법: 현재 노드 → 왼쪽 자식 노드 → 오른쪽 자식 노드\n\n\nCode\ndef _preorder(self, node):\n  if node:\n    print(node.key, end=' ')\n    self._preorder(node.left)\n    self._preorder(node.right)\n\n\n\n\n\n\n방문 방법: 왼쪽 자식 노드 → 현재 노드 → 오른쪽 자식 노드\n\n\n\nCode\ndef _inorder(self, node):\n  if node:\n    self._inorder(node.left)\n    print(node.key, end=' ')\n    self._inorder(node.right)\n\n\n\n\n\n\n방문 방법: 왼쪽 자식 노드 → 오른쪽 자식 노드 → 현재 노드\n\n\n\nCode\ndef _postorder(self, node):\n  if node:\n    self._postorder(node.left)\n    self._postorder(node.right)\n    print(node.key, end=' ')\n\n\n\n\n\n\n낮은 레벨(루트)부터 높은 레벨까지 순차적으로 방문한다.\n단순히 루트 노드에서부터 너비 우선 탐색(BST)를 진행하면 된다.\n레벨 순회 순회(level-order traverse): A → B → C → D → E → F → G\n\n\n\n\n\n\n다른 메서드 안에서 사용되는 메서드는 이름 앞에 언더바(_) 기호를 붙인다.\n\n\n\nCode\ndef search(self, node, key):\n  return self._search(self.root, key) # search: recursively 조회\n\ndef _search(self, node, key):\n  if node is None or node.key == key:\n    return node\n\n  # 현재 노드의 key보다 작은 경우\n  if node.key &gt; key:\n    return self._search(node.left, key)\n\n  # 현재 노드의 key보다 큰 경우\n  elif node.key &lt; key:\n    return self._search(node.right, key)\n\n\n\n\n\n편향 이진 트리는 다음의 두 가지 속성을 가진다.\n\n\n같은 높이의 이진 트리 중 최소 개수의 노드 개수를 가진다.\n왼쪽 혹은 오른쪽으로 한 방향에 대한 서브 트리를 가진다.\n\n\n\n\n\n노드의 개수가 N개일 때, 시간 복잡도는 다음과 같다.\n트리의 높이(height)을 H라고 할 때, 엄밀한 시간 복잡도는 \\(O(H)\\) 다.\n이상적인 경우 H = log2 N로 볼 수 있다.\n하지만 최악의 경우(편향된 경우) H = N로 볼 수 있다.\n\n\n\n\nTable 1: a list of the time complexity of the binary search trees in Python\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nMethods\n조회\n삽입\n삭제\n수정\n\n\n\n\n1\n균형 잡힌 이진 탐색 트리\n\\(O(logN)\\)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\n\n2\n편향 이진 탐색 트리\n\\(O(N)\\)\n\\(O(N)\\)\n\\(O(N)\\)\n\\(O(N)\\)\n\n\n\n\n\n\nSee Table 1.\n\n\n\n\nAVL stands for Adelson-Velsky and Landis\n이진 탐색 트리는 편향 트리가 될 수 있으므로, 최악의 경우 \\(O(N)\\) 을 요구한다.\n반면에 AVL 트리는 균형이 갖춰진 이진 트리다.\n간단한 구현 과정으로 완전 이진 트리에 가까운 형태를 유지하도록 한다.\n\n\n\nCode\nfrom collections import deque\n\n\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.left = None\n        self.right = None\n\n\nclass BinarySearchTree:\n    def __init__(self):\n        self.root = None\n\n    def search(self, node, key):\n        return self._search(self.root, key)\n\n    def _search(self, node, key):\n        if node is None or node.key == key:\n            return node\n\n        # 현재 노드의 key보다 작은 경우\n        if node.key &gt; key:\n            return self._search(node.left, key)\n        # 현재 노드의 key보다 큰 경우\n        elif node.key &lt; key:\n            return self._search(node.right, key)\n\n    def insert(self, key):\n        self.root = self._insert(self.root, key)\n\n    def _insert(self, node, key):\n        if node is None:\n            return Node(key)\n\n        # 현재 노드의 key보다 작은 경우\n        if node.key &gt; key:\n            node.left = self._insert(node.left, key)\n        # 현재 노드의 key보다 큰 경우\n        elif node.key &lt; key:\n            node.right = self._insert(node.right, key)\n\n        return node\n\n    def delete(self, key):\n        self.root = self._delete(self.root, key)\n\n    def _delete(self, node, key):\n        if node is None:\n            return None\n\n        # 현재 노드의 key보다 작은 경우\n        if node.key &gt; key:\n            node.left = self._delete(node.left, key)\n        # 현재 노드의 key보다 큰 경우\n        elif node.key &lt; key:\n            node.right = self._delete(node.right, key)\n        # 삭제할 노드를 찾은 경우\n        else:\n            # 왼쪽 자식이 없는 경우\n            if node.left is None:\n                return node.right\n            # 오른쪽 자식이 없는 경우\n            elif node.right is None:\n                return node.left\n            # 왼쪽과 오른쪽 자식 모두 있는 경우\n            node.key = self._get_min(node.right)\n            node.right = self._delete(node.right, node.key)\n\n        return node\n\n    def _get_min(self, node):\n        key = node.key\n        while node.left:\n            key = node.left.key\n            node = node.left\n        return key\n\n    def preorder(self):\n        self._preorder(self.root)\n\n    def _preorder(self, node):\n        if node:\n            print(node.key, end=' ')\n            self._preorder(node.left)\n            self._preorder(node.right)\n\n    def inorder(self):\n        self._inorder(self.root)\n\n    def _inorder(self, node):\n        if node:\n            self._inorder(node.left)\n            print(node.key, end=' ')\n            self._inorder(node.right)\n\n    def postorder(self):\n        self._postorder(self.root)\n\n    def _postorder(self, node):\n        if node:\n            self._postorder(node.left)\n            self._postorder(node.right)\n            print(node.key, end=' ')\n\n    def levelorder(self):\n        return self._levelorder(self.root)\n\n    def _levelorder(self, node):\n        if node is None:\n            return\n\n        result = []\n\n        queue = deque()\n        queue.append((0, node))  # (level, node)\n\n        while queue:\n            level, node = queue.popleft()\n            if node:\n                result.append((level, node.key))\n                queue.append((level + 1, node.left))\n                queue.append((level + 1, node.right))\n\n        for level, key in result:\n            print(f\"level: {level}, key: {key}\")\n\n    def to_list(self):\n        return self._to_list(self.root)\n\n    def _to_list(self, node):\n        if node is None:\n            return []\n        return self._to_list(node.left) + [node.key] + self._to_list(\n            node.right)\n\n\narr = [7, 4, 5, 9, 6, 3, 2, 8]\nbst = BinarySearchTree()\nfor x in arr:\n    bst.insert(x)\nprint('전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nbst.delete(7)\nprint('\\n전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nbst.delete(4)\nprint('\\n전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nbst.delete(3)\nprint('\\n전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nprint(bst.to_list())\n\n\n전위 순회: 7 4 3 2 5 6 9 8 \n중위 순회: 2 3 4 5 6 7 8 9 \n후위 순회: 2 3 6 5 4 8 9 7 \n[레벨 순회]\nlevel: 0, key: 7\nlevel: 1, key: 4\nlevel: 1, key: 9\nlevel: 2, key: 3\nlevel: 2, key: 5\nlevel: 2, key: 8\nlevel: 3, key: 2\nlevel: 3, key: 6\n\n전위 순회: 8 4 3 2 5 6 9 \n중위 순회: 2 3 4 5 6 8 9 \n후위 순회: 2 3 6 5 4 9 8 \n[레벨 순회]\nlevel: 0, key: 8\nlevel: 1, key: 4\nlevel: 1, key: 9\nlevel: 2, key: 3\nlevel: 2, key: 5\nlevel: 3, key: 2\nlevel: 3, key: 6\n\n전위 순회: 8 5 3 2 6 9 \n중위 순회: 2 3 5 6 8 9 \n후위 순회: 2 3 6 5 9 8 \n[레벨 순회]\nlevel: 0, key: 8\nlevel: 1, key: 5\nlevel: 1, key: 9\nlevel: 2, key: 3\nlevel: 2, key: 6\nlevel: 3, key: 2\n\n전위 순회: 8 5 2 6 9 \n중위 순회: 2 5 6 8 9 \n후위 순회: 2 6 5 9 8 \n[레벨 순회]\nlevel: 0, key: 8\nlevel: 1, key: 5\nlevel: 1, key: 9\nlevel: 2, key: 2\nlevel: 2, key: 6\n[2, 5, 6, 8, 9]\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html#트리tree",
    "href": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html#트리tree",
    "title": "Data Structure (8) Binary Search Tree",
    "section": "",
    "text": "트리는 가계도와 같이 계층적인 구조를 표현할 때 사용할 수 있는 자료구조다.\n나무(tree)의 형태를 뒤집은 것과 같이 생겼다.\n다수의 데이터를 관리하기에 적합한 트리 자료 구조의 가장 기본적인 형태"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html#트리-용어-정리",
    "href": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html#트리-용어-정리",
    "title": "Data Structure (8) Binary Search Tree",
    "section": "",
    "text": "루트 노드(root node): 부모가 없는 최상위 노드\n단말 노드(leaf node): 자식이 없는 노드\n트리(tree)에서는 부모와 자식 관계가 성립한다 (직계).\n형제 관계 (sibling, 방계): 부모 node로 부터 왼쪽 자식과 오른쪽 자식과의 관계\n깊이(depth): 루트 노드에서의 길이(length), 루트 노드로부터 손자까지의 depth=2\n\n이때, 길이란 출발 노드에서 목적지 노드까지 거쳐야 하는 간선의 수를 의미한다.\n\n트리의 높이(height)은 루트 노드에서 가장 깊은 노드까지의 길이를 의미한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html#이진-트리binary-tree",
    "href": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html#이진-트리binary-tree",
    "title": "Data Structure (8) Binary Search Tree",
    "section": "",
    "text": "이진 트리는 최대 2개의 자식을 가질 수 있는 트리를 말한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html#이진-탐색-트리binary-search-tree",
    "href": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html#이진-탐색-트리binary-search-tree",
    "title": "Data Structure (8) Binary Search Tree",
    "section": "",
    "text": "다수의 데이터를 관리(조회, 저장, 삭제)하기 위한 가장 기본적인 자료구조 중 하나다.\n이진 탐색 트리의 성질: 순서가 있음\n\n왼쪽 자식 노드 &lt; 부모 노드 &lt; 오른쪽 자식 노드\n루트 노드 기준 모든 왼쪽 노드들은 루트 노드보다 작음\n루트 노드 기준 모든 오른쪽 노드들은 루트 노드보다 큼\n2진 탐색을 가능하게 하는 구조\n\n\n\n\n\n특정한 노드의 키(key) 값보다 그 왼쪽 자식 노드의 키(key) 값이 더 작다.\n특정한 노드의 키(key) 값보다 그 오른쪽 자식 노드의 키(key) 값이 더 크다.\n특정한 노드의 왼쪽 서브 트리, 오른쪽 서브 트리 모두 이진 탐색 트리다.\nworst case: 찾는게 없을 때 혹은 찾고자 하는 데이터가 가장 마지막에 있을 때\n\n탐색시 재귀적으로 중앙값을 기준으로 오른쪽만 찾음\n매 실행마다 데이터의 개수가 절반씩 줄어듬\n그러면, 몇 번만에 사이즈가 1이 되는가?\n수식 유도, input size를 N이라고 가정했을때\n\\(N \\times {(\\frac{1}{2})}^{k}=1 \\rightarrow N=2^k \\rightarrow k = log_2N\\)\n위의 수식을 점근적 표기법으로 표현하면 \\(\\Theta(logN)\\)\n\nbest case: 한번에 찾았을 때\n\n\\(\\Theta(1)\\)\n\n그러므로, lower bound = \\(\\Theta(1)\\), upper bound = \\(O(logN)\\)\n\n\n\n\n\n루트 노드에서 출발하여 아래쪽으로 내려오면서, 삽입할 위치를 찾는다.\n\n삽입할 노드의 키(key)가 작으면 왼쪽으로,\n삽입할 노드의 키(key)가 크면 오른쪽으로 삽입\n\n삽입할 노드 목록 예시: [7,4,5,9,6,2,3,2,8]으로 트리 생성해보기\n\n\n\n\nBinary Tree\n\n\nSorcue: 코딩 테스트를 위한 트리(Tree) 자료구조 10분 핵심 요약 By 동빈나\n\n\n\n\n루트 노드에서 출발하여 아래쪽으로 내려오면서, 찾고자 하는 원소를 조회한다. 삽입 연산과 같은 로직을 따름\n1 삽입할 노드의 키(key)가 작으면 왼쪽으로, 2 삽입할 노드의 키(key)가 크면 오른쪽으로 조회\n조회할 노드 목록 예시: 5번 노드\n\n\n\n\n\n루트 노드에서 출발하여 아래쪽으로 내려오면서, 삭제할 원소에 접근한다.\n삭제할 노드 목록 예시: 7번 노드\n\nCase #1 왼쪽 자식이 없는 경우 → 오른쪽 자식으로 대체\nCase #2 오른쪽 자식이 없는 경우 → 왼쪽 자식으로 대체\nCase #3 왼쪽, 오른쪽이 모두 있는 경우 → 오른쪽 서브\n\n트리에서 가장 작은 노드로 대체\n삭제할 노드 목록 예시: 4번 노드\n\n\n\n\nBinary Tree Deletion\n\n\nSorcue: 코딩 테스트를 위한 트리(Tree) 자료구조 10분 핵심 요약 By 동빈나"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html#트리의-순회",
    "href": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html#트리의-순회",
    "title": "Data Structure (8) Binary Search Tree",
    "section": "",
    "text": "트리에 포함되어 있는 정보를 모두 출력하고자 할 때, 어떤 방식을 사용할 수 있을까?\n바로 순회(traversal)를 사용할 수 있다.\n트리의 모든 노드를 특정한 순서(조건)에 따라서 방문하는 방법을 순회(traversal)라고 한다.\n\n\n전위 순회(pre-order traverse): 루트 방문 → 왼쪽 자식 방문 → 오른쪽 자식 방문\n중위 순회(in-order traverse): 왼쪽 자식 방문 → 루트 방문 → 오른쪽 자식 방문\n후위 순회(post-order traverse): 왼쪽 자식 방문 → 오른쪽 자식 방문 → 루트 방문\n\n\n\n\n전위 순회(pre-order traverse): A → B → D → E → C → F → G\n중위 순회(in-order traverse): D → B → E → A → F → C → G\n후위 순회(post-order traverse): D → E → B → F → G → C → A\n\n\n\n\nBinary Tree Traverse\n\n\nSorcue: 코딩 테스트를 위한 트리(Tree) 자료구조 10분 핵심 요약 By 동빈나\n\n\n\n• 방문 방법: 현재 노드 → 왼쪽 자식 노드 → 오른쪽 자식 노드\n\n\nCode\ndef _preorder(self, node):\n  if node:\n    print(node.key, end=' ')\n    self._preorder(node.left)\n    self._preorder(node.right)\n\n\n\n\n\n\n방문 방법: 왼쪽 자식 노드 → 현재 노드 → 오른쪽 자식 노드\n\n\n\nCode\ndef _inorder(self, node):\n  if node:\n    self._inorder(node.left)\n    print(node.key, end=' ')\n    self._inorder(node.right)\n\n\n\n\n\n\n방문 방법: 왼쪽 자식 노드 → 오른쪽 자식 노드 → 현재 노드\n\n\n\nCode\ndef _postorder(self, node):\n  if node:\n    self._postorder(node.left)\n    self._postorder(node.right)\n    print(node.key, end=' ')\n\n\n\n\n\n\n낮은 레벨(루트)부터 높은 레벨까지 순차적으로 방문한다.\n단순히 루트 노드에서부터 너비 우선 탐색(BST)를 진행하면 된다.\n레벨 순회 순회(level-order traverse): A → B → C → D → E → F → G"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html#이진-탐색-트리의-구현",
    "href": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html#이진-탐색-트리의-구현",
    "title": "Data Structure (8) Binary Search Tree",
    "section": "",
    "text": "다른 메서드 안에서 사용되는 메서드는 이름 앞에 언더바(_) 기호를 붙인다.\n\n\n\nCode\ndef search(self, node, key):\n  return self._search(self.root, key) # search: recursively 조회\n\ndef _search(self, node, key):\n  if node is None or node.key == key:\n    return node\n\n  # 현재 노드의 key보다 작은 경우\n  if node.key &gt; key:\n    return self._search(node.left, key)\n\n  # 현재 노드의 key보다 큰 경우\n  elif node.key &lt; key:\n    return self._search(node.right, key)\n\n\n\n\n\n편향 이진 트리는 다음의 두 가지 속성을 가진다.\n\n\n같은 높이의 이진 트리 중 최소 개수의 노드 개수를 가진다.\n왼쪽 혹은 오른쪽으로 한 방향에 대한 서브 트리를 가진다.\n\n\n\n\n\n노드의 개수가 N개일 때, 시간 복잡도는 다음과 같다.\n트리의 높이(height)을 H라고 할 때, 엄밀한 시간 복잡도는 \\(O(H)\\) 다.\n이상적인 경우 H = log2 N로 볼 수 있다.\n하지만 최악의 경우(편향된 경우) H = N로 볼 수 있다.\n\n\n\n\nTable 1: a list of the time complexity of the binary search trees in Python\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nMethods\n조회\n삽입\n삭제\n수정\n\n\n\n\n1\n균형 잡힌 이진 탐색 트리\n\\(O(logN)\\)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\n\n2\n편향 이진 탐색 트리\n\\(O(N)\\)\n\\(O(N)\\)\n\\(O(N)\\)\n\\(O(N)\\)\n\n\n\n\n\n\nSee Table 1.\n\n\n\n\nAVL stands for Adelson-Velsky and Landis\n이진 탐색 트리는 편향 트리가 될 수 있으므로, 최악의 경우 \\(O(N)\\) 을 요구한다.\n반면에 AVL 트리는 균형이 갖춰진 이진 트리다.\n간단한 구현 과정으로 완전 이진 트리에 가까운 형태를 유지하도록 한다.\n\n\n\nCode\nfrom collections import deque\n\n\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.left = None\n        self.right = None\n\n\nclass BinarySearchTree:\n    def __init__(self):\n        self.root = None\n\n    def search(self, node, key):\n        return self._search(self.root, key)\n\n    def _search(self, node, key):\n        if node is None or node.key == key:\n            return node\n\n        # 현재 노드의 key보다 작은 경우\n        if node.key &gt; key:\n            return self._search(node.left, key)\n        # 현재 노드의 key보다 큰 경우\n        elif node.key &lt; key:\n            return self._search(node.right, key)\n\n    def insert(self, key):\n        self.root = self._insert(self.root, key)\n\n    def _insert(self, node, key):\n        if node is None:\n            return Node(key)\n\n        # 현재 노드의 key보다 작은 경우\n        if node.key &gt; key:\n            node.left = self._insert(node.left, key)\n        # 현재 노드의 key보다 큰 경우\n        elif node.key &lt; key:\n            node.right = self._insert(node.right, key)\n\n        return node\n\n    def delete(self, key):\n        self.root = self._delete(self.root, key)\n\n    def _delete(self, node, key):\n        if node is None:\n            return None\n\n        # 현재 노드의 key보다 작은 경우\n        if node.key &gt; key:\n            node.left = self._delete(node.left, key)\n        # 현재 노드의 key보다 큰 경우\n        elif node.key &lt; key:\n            node.right = self._delete(node.right, key)\n        # 삭제할 노드를 찾은 경우\n        else:\n            # 왼쪽 자식이 없는 경우\n            if node.left is None:\n                return node.right\n            # 오른쪽 자식이 없는 경우\n            elif node.right is None:\n                return node.left\n            # 왼쪽과 오른쪽 자식 모두 있는 경우\n            node.key = self._get_min(node.right)\n            node.right = self._delete(node.right, node.key)\n\n        return node\n\n    def _get_min(self, node):\n        key = node.key\n        while node.left:\n            key = node.left.key\n            node = node.left\n        return key\n\n    def preorder(self):\n        self._preorder(self.root)\n\n    def _preorder(self, node):\n        if node:\n            print(node.key, end=' ')\n            self._preorder(node.left)\n            self._preorder(node.right)\n\n    def inorder(self):\n        self._inorder(self.root)\n\n    def _inorder(self, node):\n        if node:\n            self._inorder(node.left)\n            print(node.key, end=' ')\n            self._inorder(node.right)\n\n    def postorder(self):\n        self._postorder(self.root)\n\n    def _postorder(self, node):\n        if node:\n            self._postorder(node.left)\n            self._postorder(node.right)\n            print(node.key, end=' ')\n\n    def levelorder(self):\n        return self._levelorder(self.root)\n\n    def _levelorder(self, node):\n        if node is None:\n            return\n\n        result = []\n\n        queue = deque()\n        queue.append((0, node))  # (level, node)\n\n        while queue:\n            level, node = queue.popleft()\n            if node:\n                result.append((level, node.key))\n                queue.append((level + 1, node.left))\n                queue.append((level + 1, node.right))\n\n        for level, key in result:\n            print(f\"level: {level}, key: {key}\")\n\n    def to_list(self):\n        return self._to_list(self.root)\n\n    def _to_list(self, node):\n        if node is None:\n            return []\n        return self._to_list(node.left) + [node.key] + self._to_list(\n            node.right)\n\n\narr = [7, 4, 5, 9, 6, 3, 2, 8]\nbst = BinarySearchTree()\nfor x in arr:\n    bst.insert(x)\nprint('전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nbst.delete(7)\nprint('\\n전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nbst.delete(4)\nprint('\\n전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nbst.delete(3)\nprint('\\n전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nprint(bst.to_list())\n\n\n전위 순회: 7 4 3 2 5 6 9 8 \n중위 순회: 2 3 4 5 6 7 8 9 \n후위 순회: 2 3 6 5 4 8 9 7 \n[레벨 순회]\nlevel: 0, key: 7\nlevel: 1, key: 4\nlevel: 1, key: 9\nlevel: 2, key: 3\nlevel: 2, key: 5\nlevel: 2, key: 8\nlevel: 3, key: 2\nlevel: 3, key: 6\n\n전위 순회: 8 4 3 2 5 6 9 \n중위 순회: 2 3 4 5 6 8 9 \n후위 순회: 2 3 6 5 4 9 8 \n[레벨 순회]\nlevel: 0, key: 8\nlevel: 1, key: 4\nlevel: 1, key: 9\nlevel: 2, key: 3\nlevel: 2, key: 5\nlevel: 3, key: 2\nlevel: 3, key: 6\n\n전위 순회: 8 5 3 2 6 9 \n중위 순회: 2 3 5 6 8 9 \n후위 순회: 2 3 6 5 9 8 \n[레벨 순회]\nlevel: 0, key: 8\nlevel: 1, key: 5\nlevel: 1, key: 9\nlevel: 2, key: 3\nlevel: 2, key: 6\nlevel: 3, key: 2\n\n전위 순회: 8 5 2 6 9 \n중위 순회: 2 5 6 8 9 \n후위 순회: 2 6 5 9 8 \n[레벨 순회]\nlevel: 0, key: 8\nlevel: 1, key: 5\nlevel: 1, key: 9\nlevel: 2, key: 2\nlevel: 2, key: 6\n[2, 5, 6, 8, 9]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html",
    "title": "Data Structure (9) Priority Queue",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n우선순위 큐는 우선순위에 따라서 데이터를 추출하는 자료구조다.\n컴퓨터 운영체제, 온라인 게임 매칭 등에서 활용된다.\n우선순위 큐는 일반적으로 힙(heap)을 이용해 구현한다.\n\n\n\n\nTable 1: a list of data structure\n\n\n\n\n\nNumber\nData Structure\n추출되는 데이터\n\n\n\n\n1\nstack\n가장 나중에 삽입된 데이터\n\n\n2\nqueue\n가장 먼저 삽입된 데이터\n\n\n3\npriority queue\n가장 우선 순위가 높은 데이터\n\n\n\n\n\n\nSee Table 2.\n\n\n\n\n우선순위 큐는 다양한 방법으로 구현할 수 있다.\n데이터의 개수가 N개일 때, 구현 방식에 따른 시간 복잡도는 다음과 같다.\n삭제 할때는 우선 순위가 가장 높은 것을 찾아야 하기 때문에 \\(O(N)\\) 만큼 소요될 수 있다.\n\n\n\n\nTable 2: a list of the priority queue building methods in Python\n\n\n\n\n\nNumber\n우선 순위 큐 구현 방식\n삽입 시간\n삭제 시간\n\n\n\n\n1\n리스트 자료형\n\\(O(1)\\)\n\\(O(N)\\)\n\n\n2\n힙 (Heap)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\n\n\n\n\n\nSee Table 2.\n\n일반적인 형태의 큐는 선형적인 구조를 가진다.\n반면에 우선순위 큐는 이진 트리(binary tree) 구조를 사용하는 것이 일반적이다.\n\n\n\n\n\n이진 트리(binary tree)는 최대 2개까지의 자식을 가질 수 있다.\n\n\n\n\n포화 이진 트리는 리프 노드를 제외한 모든 노드가 두 자식을 가지고 있는 트리다.\n\n\n\n\n\n완전 이진 트리는 모든 노드가 왼쪽 자식부터 차근차근 채워진 트리다.\n\n\n\n\n\n왼쪽 자식 트리와 오른쪽 자식 트리의 높이가 1 이상 차이 나지 않는 트리다.\n\n\n\n\n\n\n힙(Heap)은 원소들 중에서 최댓값 혹은 최솟값을 빠르게 찾아내는 자료구조다.\n최대 힙(Max Heap): 값이 큰 원소부터 추출한다.\n최소 힙(Min Heap): 값이 작은 원소부터 추출한다.\n힙은 원소의 삽입과 삭제를 위해 \\(O(logN)\\) 의 수행 시간을 요구한다.\n단순한 N개의 데이터를 힙에 넣었다가 모두 꺼내는 작업은 정렬과 동일하다.\n이 경우 시간 복잡도는 \\(O(NlogN)\\) 이다.\n\n\n\n\n최대 힙(max heap)은 부모 노드가 자식 노드보다 값이 큰 완전 이진 트리를 의미한다.\n최대 힙(max heap)의 루트 노드는 전체 트리에서 가장 큰 값을 가진다는 특징이 있다.\n\n\n\n\n\n힙은 완전 이진 트리 자료구조를 따른다.\n힙에서는 우선순위가 높은 노드가 루트(root)에 위치한다.\n\n\n최대 힙(max heap)\n\n\n부모 노드의 키 값이 자식 노드의 키 값보다 항상 크다.\n루트 노드가 가장 크며, 값이 큰 데이터가 우선순위를 가진다.\n\n\n최소 힙(min heap)\n\n\n부모 노드의 키 값이 자식 노드의 키 값보다 항상 작다.\n루트 노드가 가장 작으며, 값이 작은 데이터가 우선순위를 가진다.\n힙의 삽입과 삭제 연산을 수행할 때를 고려해 보자.\n직관적으로, 거슬러 갈 때마다 처리해야 하는 범위에 포함된 원소의 개수가 절반씩 줄어든다.\n따라서 삽입과 삭제에 대한 시간 복잡도는 O logN 이다.\n\n\n\n\n\n(상향식) 부모로 거슬러 올라가며, 부모보다 자신이 더 작은 경우에 위치를 교체한다.\n\n\n\n\n\n(상향식) 부모로 거슬러 올라가며, 부모보다 자신이 더 작은 경우에 위치를 교체한다.\n새로운 원소가 삽입되었을 때 O logN 의 시간 복잡도로 힙 성질을 유지하도록 할 수 있다.\n\n\n\n\n\n원소가 제거되었을 때 O logN 의 시간 복잡도로 힙 성질을 유지하도록 할 수 있다.\n원소를 제거할 때는 가장 마지막 노드가 루트 노드의 위치에 오도록 한다.\n이후에 루트 노드에서부터 하향식으로(더 작은 자식 노드로) Heapify()를 진행한다.\n\n\n\n\n\n파이썬에서는 힙(Heap) 라이브러리를 제공한다.\nheapq 라이브러리의 삽입 및 삭제에 대한 시간 복잡도는 모두 O logN 이다.\n\n\n\n\n단순히 하나의 빈 리스트를 만들면, 그것을 힙(heap) 자료구조로 사용할 수 있다.\n\n\n\nCode\nimport heapq\n\nheap = []\nprint(heap)\n\n\n[]\n\n\n\n\n\n\n원소의 삽입: heappush() 메서드\n원소의 추출: heappop() 메서드\n\n\n\nCode\nimport heapq\n\nheap = []\n\nheapq.heappush(heap, 7)\nheapq.heappush(heap, 4)\nheapq.heappush(heap, 5)\nheapq.heappush(heap, 8)\n\nwhile heap:\n  element = heapq.heappop(heap)\n  print(element, end=\" \")\n\n\n4 5 7 8 \n\n\n\n\n\n\n원소의 삽입: heappush() 메서드\n원소의 추출: heappop() 메서드\n\n\n\nCode\nimport heapq\n\nheap = []\n\nheapq.heappush(heap, 7)\nheapq.heappush(heap, 4)\nheapq.heappush(heap, 5)\nheapq.heappush(heap, 8)\n\nprint(heap[0])\n\n\n4\n\n\n\n\n\n\nheappush() 메서드를 이용해 하나씩 원소를 삽입하지 않을 수 있다.\nheapify() 메서드는 새로운 리스트를 반환하지 않고, 리스트 내부를 직접 수정한다.\n\n\n\nCode\nimport heapq\n\nheap = [9, 1, 5, 4, 3, 8, 7]\nheapq.heapify(heap)\n\nwhile heap:\n  element = heapq.heappop(heap)\n  print(element, end=\" \")\n\n\n1 3 4 5 7 8 9 \n\n\n\n\n\n\n파이썬의 heapq 라이브러리는 기본적으로 최소 힙 기능을 제공한다.\n최대 힙을 위해서는 1 삽입과 2 추출할 때 키(key)에 음수(-) 부호를 취한다.\n\n\n\nCode\nimport heapq\n\narr = [9, 1, 5, 4, 3, 8, 7]\nheap = []\n\nfor x in arr:\n  heapq.heappush(heap, -x)\n\nwhile heap:\n  x = -heapq.heappop(heap)\n  print(x, end=\" \")\n\n\n9 8 7 5 4 3 1 \n\n\n\n\n\n\n단순히 힙에 원소를 넣었다가 꺼내는 것 만으로도 정렬을 수행할 수 있다.\n\n\n\nCode\nimport heapq\n\ndef heap_sort(arr):\n  heap = []\n  for x in arr:\n    heapq.heappush(heap, x)\n\n  result = []\n  while heap:\n    x = heapq.heappop(heap)\n    result.append(x)\n\n  return result\n\nprint(heap_sort([9, 1, 5, 4, 3, 8, 7]))\n\n\n[1, 3, 4, 5, 7, 8, 9]\n\n\n\n\n\n\n최소 힙이나 최대 힙을 사용하여 n번째로 작은 값이나 n번째로 큰 값을 얻을 수 있다.\n힙(heap)을 만든 뒤에 추출(pop) 함수를 n번 호출한다.\n\n\n\nCode\nimport heapq\n\ndef n_smallest(n, arr):\n  heap = []\n  for x in arr:\n    heapq.heappush(heap, x)\n    result = None\n  for _ in range(n):\n    result = heapq.heappop(heap)\n  return result\n\narr = [9, 1, 5, 4, 3, 8, 7]\nprint(n_smallest(3, arr))\n\n\n4\n\n\n\n파이썬에서는 N번째로 작은 값을 구하는 메서드를 제공한다.\nnsmallest() 메서드는 가장 작은 n개의 값을 반환한다.\n\n\n\nCode\nimport heapq\n\narr = [9, 1, 5, 4, 3, 8, 7]\nresult = heapq.nsmallest(3, arr)\nprint(result[-1])\n\n\n4\n\n\n\n\n\n\n파이썬에서는 N번째로 큰 값을 구하는 메서드를 제공한다.\nnlargest() 메서드는 가장 큰 n개의 값을 반환한다.\n\n\n\nCode\nimport heapq\n\narr = [9, 1, 5, 4, 3, 8, 7]\nresult = heapq.nlargest(3, arr)\nprint(result[-1])\n\n\n7\n\n\n\n\nCode\nclass Heap(object):\n    def __init__(self):\n        # 첫번째 원소는 사용하지 않음\n        self.arr = [None]\n\n    # 원소 삽입(push)\n    def push(self, x):\n        # 마지막 위치에 원소를 삽입\n        self.arr.append(x)\n        # 첫 원소인 경우 종료\n        if len(self.arr) == 2:\n            return\n        # 값의 크기를 비교하며 부모를 타고 올라감\n        i = len(self.arr) - 1\n        while True:\n            parent = i // 2\n            # 작은 값을 부모 쪽으로 계속 이동\n            if 1 &lt;= parent and self.arr[parent] &gt; self.arr[i]:\n                self.arr[parent], self.arr[i] = self.arr[i], self.arr[parent]\n                i = parent\n            else:\n                break\n\n    # 원소 추출(pop)\n    def pop(self):\n        # 마지막 원소\n        i = len(self.arr) - 1\n        # 남은 원소가 없다면 종료\n        if i &lt; 1:\n            return None\n        # 루트 원소와 마지막 원소를 교체하여, 마지막 원소 추출\n        self.arr[1], self.arr[i] = self.arr[i], self.arr[1]\n        result = self.arr.pop()\n        # 루트(root)에서부터 원소 정렬\n        self.heapify()\n        return result\n\n    # 루트(root)에서부터 자식 방향으로 내려가며 재정렬\n    def heapify(self):\n        # 남은 원소가 1개 이하라면 종료\n        if len(self.arr) &lt;= 2:\n            return\n        # 루트 원소\n        i = 1\n        while True:\n            # 왼쪽 자식\n            child = 2 * i\n            # 왼쪽 자식과 오른쪽 자식 중 더 작은 것을 선택\n            if child + 1 &lt; len(self.arr):\n                if self.arr[child] &gt; self.arr[child + 1]:\n                    child += 1\n            # 더 이상 자식이 없거나, 적절한 위치를 찾은 경우\n            if child &gt;= len(self.arr) or self.arr[child] &gt; self.arr[i]:\n                break\n            # 원소를 교체하며, 자식 방향으로 내려가기\n            self.arr[i], self.arr[child] = self.arr[child], self.arr[i]\n            i = child\n\n\n\n\nCode\narr = [9, 1, 5, 4, 3, 8, 7]\nheap = Heap()\n\nfor x in arr:\n    heap.push(x)\n\nwhile True:\n    x = heap.pop()\n    if x == None:\n        break\n    print(x, end=\" \")\n\n\n1 3 4 5 7 8 9 \n\n\n\n\nCode\nimport random\nimport time\n\n\n# N개의 무작위 데이터 생성\narr = []\nn = 100000\nfor _ in range(n):\n    arr.append(random.randint(0, 1000000))\n\n# 시간 측정 시작\nstart_time = time.time()\n\n# 힙에 모든 원소 삽입\nheap = Heap()\nfor x in arr:\n    heap.push(x)\n\n# 힙에서 모든 원소 추출\nresult = []\nwhile True:\n    x = heap.pop()\n    if x == None:\n        break\n    result.append(x)\n\n# 시간 측정 종료\nprint(f\"Elapsed time: {time.time() - start_time} seconds.\")\n\n# 오름차순 정렬 여부 확인\nascending = True\nfor i in range(n - 1):\n    if result[i] &gt; result[i + 1]:\n        ascending = False\nprint(\"Sorted:\", ascending)\n\n# 가장 작은 5개 원소와 가장 큰 5개 원소 출력\nprint(result[:5])\nprint(result[-5:])\n\n\nElapsed time: 1.1300086975097656 seconds.\nSorted: True\n[8, 15, 18, 40, 41]\n[999936, 999954, 999955, 999960, 999970]\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html#우선순위-큐priority-queue",
    "href": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html#우선순위-큐priority-queue",
    "title": "Data Structure (9) Priority Queue",
    "section": "",
    "text": "우선순위 큐는 우선순위에 따라서 데이터를 추출하는 자료구조다.\n컴퓨터 운영체제, 온라인 게임 매칭 등에서 활용된다.\n우선순위 큐는 일반적으로 힙(heap)을 이용해 구현한다.\n\n\n\n\nTable 1: a list of data structure\n\n\n\n\n\nNumber\nData Structure\n추출되는 데이터\n\n\n\n\n1\nstack\n가장 나중에 삽입된 데이터\n\n\n2\nqueue\n가장 먼저 삽입된 데이터\n\n\n3\npriority queue\n가장 우선 순위가 높은 데이터\n\n\n\n\n\n\nSee Table 2."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html#우선순위-큐를-구현하는-방법",
    "href": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html#우선순위-큐를-구현하는-방법",
    "title": "Data Structure (9) Priority Queue",
    "section": "",
    "text": "우선순위 큐는 다양한 방법으로 구현할 수 있다.\n데이터의 개수가 N개일 때, 구현 방식에 따른 시간 복잡도는 다음과 같다.\n삭제 할때는 우선 순위가 가장 높은 것을 찾아야 하기 때문에 \\(O(N)\\) 만큼 소요될 수 있다.\n\n\n\n\nTable 2: a list of the priority queue building methods in Python\n\n\n\n\n\nNumber\n우선 순위 큐 구현 방식\n삽입 시간\n삭제 시간\n\n\n\n\n1\n리스트 자료형\n\\(O(1)\\)\n\\(O(N)\\)\n\n\n2\n힙 (Heap)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\n\n\n\n\n\nSee Table 2.\n\n일반적인 형태의 큐는 선형적인 구조를 가진다.\n반면에 우선순위 큐는 이진 트리(binary tree) 구조를 사용하는 것이 일반적이다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html#이진-트리binary-tree",
    "href": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html#이진-트리binary-tree",
    "title": "Data Structure (9) Priority Queue",
    "section": "",
    "text": "이진 트리(binary tree)는 최대 2개까지의 자식을 가질 수 있다.\n\n\n\n\n포화 이진 트리는 리프 노드를 제외한 모든 노드가 두 자식을 가지고 있는 트리다.\n\n\n\n\n\n완전 이진 트리는 모든 노드가 왼쪽 자식부터 차근차근 채워진 트리다.\n\n\n\n\n\n왼쪽 자식 트리와 오른쪽 자식 트리의 높이가 1 이상 차이 나지 않는 트리다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html#힙heap",
    "href": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html#힙heap",
    "title": "Data Structure (9) Priority Queue",
    "section": "",
    "text": "힙(Heap)은 원소들 중에서 최댓값 혹은 최솟값을 빠르게 찾아내는 자료구조다.\n최대 힙(Max Heap): 값이 큰 원소부터 추출한다.\n최소 힙(Min Heap): 값이 작은 원소부터 추출한다.\n힙은 원소의 삽입과 삭제를 위해 \\(O(logN)\\) 의 수행 시간을 요구한다.\n단순한 N개의 데이터를 힙에 넣었다가 모두 꺼내는 작업은 정렬과 동일하다.\n이 경우 시간 복잡도는 \\(O(NlogN)\\) 이다.\n\n\n\n\n최대 힙(max heap)은 부모 노드가 자식 노드보다 값이 큰 완전 이진 트리를 의미한다.\n최대 힙(max heap)의 루트 노드는 전체 트리에서 가장 큰 값을 가진다는 특징이 있다.\n\n\n\n\n\n힙은 완전 이진 트리 자료구조를 따른다.\n힙에서는 우선순위가 높은 노드가 루트(root)에 위치한다.\n\n\n최대 힙(max heap)\n\n\n부모 노드의 키 값이 자식 노드의 키 값보다 항상 크다.\n루트 노드가 가장 크며, 값이 큰 데이터가 우선순위를 가진다.\n\n\n최소 힙(min heap)\n\n\n부모 노드의 키 값이 자식 노드의 키 값보다 항상 작다.\n루트 노드가 가장 작으며, 값이 작은 데이터가 우선순위를 가진다.\n힙의 삽입과 삭제 연산을 수행할 때를 고려해 보자.\n직관적으로, 거슬러 갈 때마다 처리해야 하는 범위에 포함된 원소의 개수가 절반씩 줄어든다.\n따라서 삽입과 삭제에 대한 시간 복잡도는 O logN 이다.\n\n\n\n\n\n(상향식) 부모로 거슬러 올라가며, 부모보다 자신이 더 작은 경우에 위치를 교체한다.\n\n\n\n\n\n(상향식) 부모로 거슬러 올라가며, 부모보다 자신이 더 작은 경우에 위치를 교체한다.\n새로운 원소가 삽입되었을 때 O logN 의 시간 복잡도로 힙 성질을 유지하도록 할 수 있다.\n\n\n\n\n\n원소가 제거되었을 때 O logN 의 시간 복잡도로 힙 성질을 유지하도록 할 수 있다.\n원소를 제거할 때는 가장 마지막 노드가 루트 노드의 위치에 오도록 한다.\n이후에 루트 노드에서부터 하향식으로(더 작은 자식 노드로) Heapify()를 진행한다.\n\n\n\n\n\n파이썬에서는 힙(Heap) 라이브러리를 제공한다.\nheapq 라이브러리의 삽입 및 삭제에 대한 시간 복잡도는 모두 O logN 이다.\n\n\n\n\n단순히 하나의 빈 리스트를 만들면, 그것을 힙(heap) 자료구조로 사용할 수 있다.\n\n\n\nCode\nimport heapq\n\nheap = []\nprint(heap)\n\n\n[]\n\n\n\n\n\n\n원소의 삽입: heappush() 메서드\n원소의 추출: heappop() 메서드\n\n\n\nCode\nimport heapq\n\nheap = []\n\nheapq.heappush(heap, 7)\nheapq.heappush(heap, 4)\nheapq.heappush(heap, 5)\nheapq.heappush(heap, 8)\n\nwhile heap:\n  element = heapq.heappop(heap)\n  print(element, end=\" \")\n\n\n4 5 7 8 \n\n\n\n\n\n\n원소의 삽입: heappush() 메서드\n원소의 추출: heappop() 메서드\n\n\n\nCode\nimport heapq\n\nheap = []\n\nheapq.heappush(heap, 7)\nheapq.heappush(heap, 4)\nheapq.heappush(heap, 5)\nheapq.heappush(heap, 8)\n\nprint(heap[0])\n\n\n4\n\n\n\n\n\n\nheappush() 메서드를 이용해 하나씩 원소를 삽입하지 않을 수 있다.\nheapify() 메서드는 새로운 리스트를 반환하지 않고, 리스트 내부를 직접 수정한다.\n\n\n\nCode\nimport heapq\n\nheap = [9, 1, 5, 4, 3, 8, 7]\nheapq.heapify(heap)\n\nwhile heap:\n  element = heapq.heappop(heap)\n  print(element, end=\" \")\n\n\n1 3 4 5 7 8 9 \n\n\n\n\n\n\n파이썬의 heapq 라이브러리는 기본적으로 최소 힙 기능을 제공한다.\n최대 힙을 위해서는 1 삽입과 2 추출할 때 키(key)에 음수(-) 부호를 취한다.\n\n\n\nCode\nimport heapq\n\narr = [9, 1, 5, 4, 3, 8, 7]\nheap = []\n\nfor x in arr:\n  heapq.heappush(heap, -x)\n\nwhile heap:\n  x = -heapq.heappop(heap)\n  print(x, end=\" \")\n\n\n9 8 7 5 4 3 1 \n\n\n\n\n\n\n단순히 힙에 원소를 넣었다가 꺼내는 것 만으로도 정렬을 수행할 수 있다.\n\n\n\nCode\nimport heapq\n\ndef heap_sort(arr):\n  heap = []\n  for x in arr:\n    heapq.heappush(heap, x)\n\n  result = []\n  while heap:\n    x = heapq.heappop(heap)\n    result.append(x)\n\n  return result\n\nprint(heap_sort([9, 1, 5, 4, 3, 8, 7]))\n\n\n[1, 3, 4, 5, 7, 8, 9]\n\n\n\n\n\n\n최소 힙이나 최대 힙을 사용하여 n번째로 작은 값이나 n번째로 큰 값을 얻을 수 있다.\n힙(heap)을 만든 뒤에 추출(pop) 함수를 n번 호출한다.\n\n\n\nCode\nimport heapq\n\ndef n_smallest(n, arr):\n  heap = []\n  for x in arr:\n    heapq.heappush(heap, x)\n    result = None\n  for _ in range(n):\n    result = heapq.heappop(heap)\n  return result\n\narr = [9, 1, 5, 4, 3, 8, 7]\nprint(n_smallest(3, arr))\n\n\n4\n\n\n\n파이썬에서는 N번째로 작은 값을 구하는 메서드를 제공한다.\nnsmallest() 메서드는 가장 작은 n개의 값을 반환한다.\n\n\n\nCode\nimport heapq\n\narr = [9, 1, 5, 4, 3, 8, 7]\nresult = heapq.nsmallest(3, arr)\nprint(result[-1])\n\n\n4\n\n\n\n\n\n\n파이썬에서는 N번째로 큰 값을 구하는 메서드를 제공한다.\nnlargest() 메서드는 가장 큰 n개의 값을 반환한다.\n\n\n\nCode\nimport heapq\n\narr = [9, 1, 5, 4, 3, 8, 7]\nresult = heapq.nlargest(3, arr)\nprint(result[-1])\n\n\n7\n\n\n\n\nCode\nclass Heap(object):\n    def __init__(self):\n        # 첫번째 원소는 사용하지 않음\n        self.arr = [None]\n\n    # 원소 삽입(push)\n    def push(self, x):\n        # 마지막 위치에 원소를 삽입\n        self.arr.append(x)\n        # 첫 원소인 경우 종료\n        if len(self.arr) == 2:\n            return\n        # 값의 크기를 비교하며 부모를 타고 올라감\n        i = len(self.arr) - 1\n        while True:\n            parent = i // 2\n            # 작은 값을 부모 쪽으로 계속 이동\n            if 1 &lt;= parent and self.arr[parent] &gt; self.arr[i]:\n                self.arr[parent], self.arr[i] = self.arr[i], self.arr[parent]\n                i = parent\n            else:\n                break\n\n    # 원소 추출(pop)\n    def pop(self):\n        # 마지막 원소\n        i = len(self.arr) - 1\n        # 남은 원소가 없다면 종료\n        if i &lt; 1:\n            return None\n        # 루트 원소와 마지막 원소를 교체하여, 마지막 원소 추출\n        self.arr[1], self.arr[i] = self.arr[i], self.arr[1]\n        result = self.arr.pop()\n        # 루트(root)에서부터 원소 정렬\n        self.heapify()\n        return result\n\n    # 루트(root)에서부터 자식 방향으로 내려가며 재정렬\n    def heapify(self):\n        # 남은 원소가 1개 이하라면 종료\n        if len(self.arr) &lt;= 2:\n            return\n        # 루트 원소\n        i = 1\n        while True:\n            # 왼쪽 자식\n            child = 2 * i\n            # 왼쪽 자식과 오른쪽 자식 중 더 작은 것을 선택\n            if child + 1 &lt; len(self.arr):\n                if self.arr[child] &gt; self.arr[child + 1]:\n                    child += 1\n            # 더 이상 자식이 없거나, 적절한 위치를 찾은 경우\n            if child &gt;= len(self.arr) or self.arr[child] &gt; self.arr[i]:\n                break\n            # 원소를 교체하며, 자식 방향으로 내려가기\n            self.arr[i], self.arr[child] = self.arr[child], self.arr[i]\n            i = child\n\n\n\n\nCode\narr = [9, 1, 5, 4, 3, 8, 7]\nheap = Heap()\n\nfor x in arr:\n    heap.push(x)\n\nwhile True:\n    x = heap.pop()\n    if x == None:\n        break\n    print(x, end=\" \")\n\n\n1 3 4 5 7 8 9 \n\n\n\n\nCode\nimport random\nimport time\n\n\n# N개의 무작위 데이터 생성\narr = []\nn = 100000\nfor _ in range(n):\n    arr.append(random.randint(0, 1000000))\n\n# 시간 측정 시작\nstart_time = time.time()\n\n# 힙에 모든 원소 삽입\nheap = Heap()\nfor x in arr:\n    heap.push(x)\n\n# 힙에서 모든 원소 추출\nresult = []\nwhile True:\n    x = heap.pop()\n    if x == None:\n        break\n    result.append(x)\n\n# 시간 측정 종료\nprint(f\"Elapsed time: {time.time() - start_time} seconds.\")\n\n# 오름차순 정렬 여부 확인\nascending = True\nfor i in range(n - 1):\n    if result[i] &gt; result[i + 1]:\n        ascending = False\nprint(\"Sorted:\", ascending)\n\n# 가장 작은 5개 원소와 가장 큰 5개 원소 출력\nprint(result[:5])\nprint(result[-5:])\n\n\nElapsed time: 1.1300086975097656 seconds.\nSorted: True\n[8, 15, 18, 40, 41]\n[999936, 999954, 999955, 999960, 999970]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html",
    "title": "Computing and Networking",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nAWS global infrastructure – Region / Availability zone (AZ) / Data Center\nAWS region consideration : Compliance – Latency – Pricing – Service availability\nInteracting with AWS\n\nAWS Management console\nAWS Command Line Interface (CLI) - unified tool to manage AWS services,\n\nAWS Software Development Kits (SDKs) - executing code with programming languages.\n\nSecurity in the AWS Cloud\n\nResponsibility - Customer & AWS\n\nAWS – security of the cloud, responsible for protecting and securing the infrastructure that runs all the services\nCustomer – security in the cloud, responsible for properly configuring the service and your applications, as well as ensuring your data is secure.\n\nRoot user (unristricted access)\nMulti-Factor Authentication\n\nAWS Identity and Access Management\n\nIntroduction to AWS IAM (Identity and Access Management)\nAWS account나 resource에 대한 권한을 관리하게 해주는 서비스\nAuthentication (인증)\nAuthorization (인가): resource에 대한 권한\n\nEx) 회사의 AWS account가 존재한다면, 회사 구성원들은 account 에 접근하기 위해 모두 IAM user가 되고 (authentication), account 내부의 각 AWS resource에 대한 접근 권한을 부여받아야 한다. (authorization)\n\nIAM policy: Effect (Allow/Deny), Action, Resource, Condition\nIAM Group\nIAM roles : IAM user나 AWS resource에 권한을 부여하기 위한 권한 세부 조정\nreference\n\n\n\n\n\n\n\n\nComputing\n\nit gives our application power in the form of CPU, memory, and networking capacity so that we can process our users’ requests.\nCompute Solution Examples\n\nEC2\ncontainer services: Amazon ECS(Elastic Container Service) and Amazon ECS (Elastic Container Service)\nserverless options AWS Lambda\n\ncompare them to one another.\n\nwhen and how to apply each service, to different use cases.\n\n\nNetworking\n\nneeded for being hands-on with Amazon EC2\nlaunch an EC2 instance to host the employee directory application.\nHow to Launch? → put an EC2 instance in a network, VPCs on AWS\n\n\n\n\n\n\nApps requiring computing capacity for running such as\n\nweb servers, batch jobs, databases, HR software, machine learning, or something else\non-premises computing resources are costly and have many things to deal with\nAWS already is ready to be used\n\nbuilt the infrastructure\nsecured the data centers.\nbought the servers,\nracked and stacked them\nare already online,\n\n3 computing services for different use cases.\n\nEC2 (Elastsic Compute Cloud)\ncontainer services\nserverless compute.\n\nEC2 (Flexible and scalable)\n\nEC2 instances Characteristics\n\na lot of flexibility and control in the cloud\nconfigure them to meet your needs.\nPayment\n\nat the end of the billing cycle, you only pay for what you use, either per second or per hour, depending on the type of instance.\nterminate (stop) the instance and you will stop incurring charges.\n\na range of operating systems including\n\nLinux, Mac OS, Ubuntu, Windows, and more\nTo select the OS for your server, you must choose\n\nAMI (Amazon Machine Image)\n\ncan set several configurations according to users’ use case\n\nYou can launch one or many instances from a single AMI, which would create multiple instances that all have the same configurations.\n\n\nThe instance types are grouped for use cases like\n\ncompute optimized\nmemory optimized\nstorage optimized, and more.\nread AWS documentation\nFor example,\n\nthe G instance family (graphics-intensive applications)\nthe M5 general purpose EC2 instance family (balance of resources)\nThe T3 or A1 is the instance family (the blend of the hardware capabilities)\nThen there’s the size like small, medium, large. It goes down to nano and up to many, many extra large sizes, depending on the instance type.\n\nthis type of selection: you are no longer locked into hardware decisions up front.\n\nchoose an initial EC2 instance type→ evaluate its performance for your specific use case, → change to a different type\n\nEC2 is also resizable with\n\na few clicks in the console or\ncan be done programmatically through an API call\n\n\nEC2 Instance Lifecycle\n\nyou launch an EC2 instance from an AMI\n\nit enters a pending state (booting up).\nit enters the running state (start being charged for the EC2 instance)\n\nrunning options\n\nreboot the instance\nstop your instance (stopping phase like powering down your laptop)\nstop hibernate (the stopping phase - no boot sequence required)\nthe terminate (the shutting down phase - get rid of an instance)\ntermination protection (back up in persistent storage in EC2)\n\ncharged if you are in the running state or if you are in the stopping state, preparing to hibernate.\n\n\n\n\n\nContainer Services\n\nefficiency and portability: container orchestration tools in AWS\n\nContainer orchestration\n\nprocesses to start, stop, restart and monitor containers running across not just one EC2 instance, but a number of them together that we call a cluster of EC2 instances.\nhundreds or thousands of containers - hard to manage them\nOrchestration tools : run and manage containers.\nAmazon ECS (Elastic Container Service)\n\nECS is designed to help you manage your own container orchestration software.\n\nAmazon EKS (Elastic Kubernetes Service)\n\nThe way you interact with these container services\n\nthe orchestration tool’s API\nthe orchestration tool carries out the management tasks.\nautomate scaling of your cluster\nautomate hosting your containers\nautomate the scaling of the containers themselves.\n\nsuper fast response to increasing demand when compared to virtual machines.\nhosting options : either ECS or EKS.\n\n\nServerless Compute Platform\n\nan alternative to hosting containers\n\nWhen using Amazon EC2 or Container Services running on top of EC2 as a compute platform, you are required to set up and manage your fleet of instances. This means that you are responsible for patching your instances when new software packages come out or when new security updates come out.\n\nServerless meaning\n\ncan not see or access the underlying infrastructure or instances that are hosting your solution.\nInstead, the management of the underlying environment from a provisioning, scaling, fault-tolerance and maintenance perspective is taken care of for a user.\nAll you need to do is focus on your application.\nserverless offerings are very convenient to use.\n\nAWS Fargate (the container hosting platform)\n\nserverless compute platform for ECS or EKS.\nThe scaling or fault-tolerance, OS or environments are built in\nFor user to do just\n\ndefine your container\nhow you want your container to be run\nit scales on-demand.\n\n\nAWS Lambda (the serverless compute platform)\n\npackage and upload your code to the Lambda service, creating what’s called a Lambda function.\nLambda functions run in response to triggers.\nYou configure a trigger\ncommon examples of triggers for Lambda functions\n\nan HTTP request\nan upload of a file to the storage service, Amazon S3\nevents originating from other AWS services or\neven in-app activity from mobile devices\n\nWhen the trigger is detected, the code is automatically run in a managed environment.\nLambda is currently designed to run code that has a runtime of under 15 minutes.\n\nSo, this isn’t for long running processes like deep learning or batch jobs, you wouldn’t host something like a WordPress site on AWS Lambda.\nIt’s more suited for quick processing, like a web backend for handling requests or a backend report processing service.\n\nnot billed for code that isn’t running, you only get billed for the resources that you use, down to 100 millisecond intervals.\n\n\n\n\n\n\n\n\nthe network, or VPC\n\nto launch instances, you needed to select a network. Building a custom VPC for our application that is more secure and provides more granular access to the Internet than the default option we originally chose.\nNetworking on AWS is the basis of most architectures. In this section, geared towards EC2-related services.\na Lambda function\n\nmight not need a network at all\n\n\nCreation and Concept of VPC\n\nCreation and Concept of VPC\n\nConcept\n\nIt creates a boundary where your applications and resources are isolated from any outside movement.\nnothing comes into and comes out of the VPC without your explicit permission.\n\nCreation\n\nTo create a VPC, two specific settings to declare\n\nthe region you’re selecting (In this example, the Oregon region)\nthe IP range in the form of CIDR notation. (In this example, 10.1.0.0/16)\n\nthe VPC name: app-vpc\n\nDivide VPC space into subnets\n\nput your resources such as your EC2 Instances inside of these subnets.\nThe goal of these subnets is to provide more granular controls over access to your resources.\nWith public resources, put those resources inside a subnet with internet connectivity.\nWith private resources like a database, create another subnet and have different controls to keep those resources private.\nTo create a subnet, you need three main things,\n\nthe VPC your subnet to live in which is this one,\nthe AZ (example. AZ-A = US-West-2a) your subnet to live in,\nIP range for your subnet which must be a subset of the VPC IP range\n\n\ninternet gateway for public resource\n\nenable internet connectivity\nWhen you create an internet gateway, you then need to attach it to your VPC.\n\nVGW (Virtual Private Gateway) for private resource\n\ncreate a VPN connection between a private network like an On-premise data center or internal corporate network to your VPC.\nestablish an encrypted VPN connection to your private internal AWS resources.\n\nhaving high availability: one option to make VPC better\n\nWhat that means is if this AZ goes down for whatever reason, what happens to our resources in that AZ? They go down too. So ideally, we would have resources in another easy to take on the traffic coming to our application.\nTo do this, we’d need to duplicate the resources in this AZ into the second AZ. So that means we need to create two additional subnets each within another AZ, say AZ b. All right\n\n\n\nAmazon VPC Routing\n\nThe example has two additional subnets, one public, one private in a different AZ for a total of four subnets including an EC2 instance hosting our employee directory inside of the public subnet in AZ A.\nroute tables\n\nprovide a path for the internet traffic to enter the internet gateway and find the right subnet.\nA route table contains a set of rules, called routes\n\ndetermine where network traffic is directed\nThese route tables can be applied on either the subnet level or at the VPC level.\nWhen creating a brand new VPC, AWS creates a route table called the main route table and applies it to the entire VPC.\nAWS assumes that when you create a new VPC with subnets, you want traffic to flow between those subnets.\nThe default configuration of the main route table is to allow traffic between all subnets local to the VPC.\nthe main route table of the VPC can be created in console\n\nclick on route tables on the side panel\ncreate route table.\ngive it a name such as app-route-table-public\nchoose the app-vpc and then click create.\n\n\n\n\nSecure Network with Amazone VPC Security\n\nat the base level, any new VPC is isolated from internet traffic to prevent risk.\nwhen allowing internet traffic by opening up routes, you need two options to keep your network secure, network access control lists\n\nnetwork ACLs\n\na firewall at the subnet level\ncontrol what kind of traffic is allowed to enter, and leave, your subnet\nThe default network ACL allows all traffic in and out of your subnet.\nUsing this default configuration is a good starting place but if needed, you can change the configuration of your network ACLs to further lock down your subnets.\nFor example, if you only wanted to allow HTTPS traffic into my subnet, you can do that by creating an inbound rule and outbound rule in my ACL, that allows HTTPS traffic from anywhere on port 443 and denies everything else.\n\nsecurity groups\n\nfirewalls that exist at the EC2 instance level.\nAny time you create an EC2 instance, you’ll need to place that EC2 instance inside a security group that allows the appropriate kinds of traffic to flow to your application.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html#some-basics",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html#some-basics",
    "title": "Computing and Networking",
    "section": "",
    "text": "AWS global infrastructure – Region / Availability zone (AZ) / Data Center\nAWS region consideration : Compliance – Latency – Pricing – Service availability\nInteracting with AWS\n\nAWS Management console\nAWS Command Line Interface (CLI) - unified tool to manage AWS services,\n\nAWS Software Development Kits (SDKs) - executing code with programming languages.\n\nSecurity in the AWS Cloud\n\nResponsibility - Customer & AWS\n\nAWS – security of the cloud, responsible for protecting and securing the infrastructure that runs all the services\nCustomer – security in the cloud, responsible for properly configuring the service and your applications, as well as ensuring your data is secure.\n\nRoot user (unristricted access)\nMulti-Factor Authentication\n\nAWS Identity and Access Management\n\nIntroduction to AWS IAM (Identity and Access Management)\nAWS account나 resource에 대한 권한을 관리하게 해주는 서비스\nAuthentication (인증)\nAuthorization (인가): resource에 대한 권한\n\nEx) 회사의 AWS account가 존재한다면, 회사 구성원들은 account 에 접근하기 위해 모두 IAM user가 되고 (authentication), account 내부의 각 AWS resource에 대한 접근 권한을 부여받아야 한다. (authorization)\n\nIAM policy: Effect (Allow/Deny), Action, Resource, Condition\nIAM Group\nIAM roles : IAM user나 AWS resource에 권한을 부여하기 위한 권한 세부 조정\nreference"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html#computing-and-networking",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html#computing-and-networking",
    "title": "Computing and Networking",
    "section": "",
    "text": "Computing\n\nit gives our application power in the form of CPU, memory, and networking capacity so that we can process our users’ requests.\nCompute Solution Examples\n\nEC2\ncontainer services: Amazon ECS(Elastic Container Service) and Amazon ECS (Elastic Container Service)\nserverless options AWS Lambda\n\ncompare them to one another.\n\nwhen and how to apply each service, to different use cases.\n\n\nNetworking\n\nneeded for being hands-on with Amazon EC2\nlaunch an EC2 instance to host the employee directory application.\nHow to Launch? → put an EC2 instance in a network, VPCs on AWS\n\n\n\n\n\n\nApps requiring computing capacity for running such as\n\nweb servers, batch jobs, databases, HR software, machine learning, or something else\non-premises computing resources are costly and have many things to deal with\nAWS already is ready to be used\n\nbuilt the infrastructure\nsecured the data centers.\nbought the servers,\nracked and stacked them\nare already online,\n\n3 computing services for different use cases.\n\nEC2 (Elastsic Compute Cloud)\ncontainer services\nserverless compute.\n\nEC2 (Flexible and scalable)\n\nEC2 instances Characteristics\n\na lot of flexibility and control in the cloud\nconfigure them to meet your needs.\nPayment\n\nat the end of the billing cycle, you only pay for what you use, either per second or per hour, depending on the type of instance.\nterminate (stop) the instance and you will stop incurring charges.\n\na range of operating systems including\n\nLinux, Mac OS, Ubuntu, Windows, and more\nTo select the OS for your server, you must choose\n\nAMI (Amazon Machine Image)\n\ncan set several configurations according to users’ use case\n\nYou can launch one or many instances from a single AMI, which would create multiple instances that all have the same configurations.\n\n\nThe instance types are grouped for use cases like\n\ncompute optimized\nmemory optimized\nstorage optimized, and more.\nread AWS documentation\nFor example,\n\nthe G instance family (graphics-intensive applications)\nthe M5 general purpose EC2 instance family (balance of resources)\nThe T3 or A1 is the instance family (the blend of the hardware capabilities)\nThen there’s the size like small, medium, large. It goes down to nano and up to many, many extra large sizes, depending on the instance type.\n\nthis type of selection: you are no longer locked into hardware decisions up front.\n\nchoose an initial EC2 instance type→ evaluate its performance for your specific use case, → change to a different type\n\nEC2 is also resizable with\n\na few clicks in the console or\ncan be done programmatically through an API call\n\n\nEC2 Instance Lifecycle\n\nyou launch an EC2 instance from an AMI\n\nit enters a pending state (booting up).\nit enters the running state (start being charged for the EC2 instance)\n\nrunning options\n\nreboot the instance\nstop your instance (stopping phase like powering down your laptop)\nstop hibernate (the stopping phase - no boot sequence required)\nthe terminate (the shutting down phase - get rid of an instance)\ntermination protection (back up in persistent storage in EC2)\n\ncharged if you are in the running state or if you are in the stopping state, preparing to hibernate.\n\n\n\n\n\nContainer Services\n\nefficiency and portability: container orchestration tools in AWS\n\nContainer orchestration\n\nprocesses to start, stop, restart and monitor containers running across not just one EC2 instance, but a number of them together that we call a cluster of EC2 instances.\nhundreds or thousands of containers - hard to manage them\nOrchestration tools : run and manage containers.\nAmazon ECS (Elastic Container Service)\n\nECS is designed to help you manage your own container orchestration software.\n\nAmazon EKS (Elastic Kubernetes Service)\n\nThe way you interact with these container services\n\nthe orchestration tool’s API\nthe orchestration tool carries out the management tasks.\nautomate scaling of your cluster\nautomate hosting your containers\nautomate the scaling of the containers themselves.\n\nsuper fast response to increasing demand when compared to virtual machines.\nhosting options : either ECS or EKS.\n\n\nServerless Compute Platform\n\nan alternative to hosting containers\n\nWhen using Amazon EC2 or Container Services running on top of EC2 as a compute platform, you are required to set up and manage your fleet of instances. This means that you are responsible for patching your instances when new software packages come out or when new security updates come out.\n\nServerless meaning\n\ncan not see or access the underlying infrastructure or instances that are hosting your solution.\nInstead, the management of the underlying environment from a provisioning, scaling, fault-tolerance and maintenance perspective is taken care of for a user.\nAll you need to do is focus on your application.\nserverless offerings are very convenient to use.\n\nAWS Fargate (the container hosting platform)\n\nserverless compute platform for ECS or EKS.\nThe scaling or fault-tolerance, OS or environments are built in\nFor user to do just\n\ndefine your container\nhow you want your container to be run\nit scales on-demand.\n\n\nAWS Lambda (the serverless compute platform)\n\npackage and upload your code to the Lambda service, creating what’s called a Lambda function.\nLambda functions run in response to triggers.\nYou configure a trigger\ncommon examples of triggers for Lambda functions\n\nan HTTP request\nan upload of a file to the storage service, Amazon S3\nevents originating from other AWS services or\neven in-app activity from mobile devices\n\nWhen the trigger is detected, the code is automatically run in a managed environment.\nLambda is currently designed to run code that has a runtime of under 15 minutes.\n\nSo, this isn’t for long running processes like deep learning or batch jobs, you wouldn’t host something like a WordPress site on AWS Lambda.\nIt’s more suited for quick processing, like a web backend for handling requests or a backend report processing service.\n\nnot billed for code that isn’t running, you only get billed for the resources that you use, down to 100 millisecond intervals.\n\n\n\n\n\n\n\n\nthe network, or VPC\n\nto launch instances, you needed to select a network. Building a custom VPC for our application that is more secure and provides more granular access to the Internet than the default option we originally chose.\nNetworking on AWS is the basis of most architectures. In this section, geared towards EC2-related services.\na Lambda function\n\nmight not need a network at all\n\n\nCreation and Concept of VPC\n\nCreation and Concept of VPC\n\nConcept\n\nIt creates a boundary where your applications and resources are isolated from any outside movement.\nnothing comes into and comes out of the VPC without your explicit permission.\n\nCreation\n\nTo create a VPC, two specific settings to declare\n\nthe region you’re selecting (In this example, the Oregon region)\nthe IP range in the form of CIDR notation. (In this example, 10.1.0.0/16)\n\nthe VPC name: app-vpc\n\nDivide VPC space into subnets\n\nput your resources such as your EC2 Instances inside of these subnets.\nThe goal of these subnets is to provide more granular controls over access to your resources.\nWith public resources, put those resources inside a subnet with internet connectivity.\nWith private resources like a database, create another subnet and have different controls to keep those resources private.\nTo create a subnet, you need three main things,\n\nthe VPC your subnet to live in which is this one,\nthe AZ (example. AZ-A = US-West-2a) your subnet to live in,\nIP range for your subnet which must be a subset of the VPC IP range\n\n\ninternet gateway for public resource\n\nenable internet connectivity\nWhen you create an internet gateway, you then need to attach it to your VPC.\n\nVGW (Virtual Private Gateway) for private resource\n\ncreate a VPN connection between a private network like an On-premise data center or internal corporate network to your VPC.\nestablish an encrypted VPN connection to your private internal AWS resources.\n\nhaving high availability: one option to make VPC better\n\nWhat that means is if this AZ goes down for whatever reason, what happens to our resources in that AZ? They go down too. So ideally, we would have resources in another easy to take on the traffic coming to our application.\nTo do this, we’d need to duplicate the resources in this AZ into the second AZ. So that means we need to create two additional subnets each within another AZ, say AZ b. All right\n\n\n\nAmazon VPC Routing\n\nThe example has two additional subnets, one public, one private in a different AZ for a total of four subnets including an EC2 instance hosting our employee directory inside of the public subnet in AZ A.\nroute tables\n\nprovide a path for the internet traffic to enter the internet gateway and find the right subnet.\nA route table contains a set of rules, called routes\n\ndetermine where network traffic is directed\nThese route tables can be applied on either the subnet level or at the VPC level.\nWhen creating a brand new VPC, AWS creates a route table called the main route table and applies it to the entire VPC.\nAWS assumes that when you create a new VPC with subnets, you want traffic to flow between those subnets.\nThe default configuration of the main route table is to allow traffic between all subnets local to the VPC.\nthe main route table of the VPC can be created in console\n\nclick on route tables on the side panel\ncreate route table.\ngive it a name such as app-route-table-public\nchoose the app-vpc and then click create.\n\n\n\n\nSecure Network with Amazone VPC Security\n\nat the base level, any new VPC is isolated from internet traffic to prevent risk.\nwhen allowing internet traffic by opening up routes, you need two options to keep your network secure, network access control lists\n\nnetwork ACLs\n\na firewall at the subnet level\ncontrol what kind of traffic is allowed to enter, and leave, your subnet\nThe default network ACL allows all traffic in and out of your subnet.\nUsing this default configuration is a good starting place but if needed, you can change the configuration of your network ACLs to further lock down your subnets.\nFor example, if you only wanted to allow HTTPS traffic into my subnet, you can do that by creating an inbound rule and outbound rule in my ACL, that allows HTTPS traffic from anywhere on port 443 and denies everything else.\n\nsecurity groups\n\nfirewalls that exist at the EC2 instance level.\nAny time you create an EC2 instance, you’ll need to place that EC2 instance inside a security group that allows the appropriate kinds of traffic to flow to your application."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html",
    "title": "Storage and Database",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n모니터링은 리소스의 운영 상태 및 사용에 대한 데이터를 수집하고 분석하는 기법으로 데이터를 수집, 분석 및 사용하여 IT 리소스 및 시스템에 대한 의사 결정을 내리거나 질문에 답변하는 작업이며 모니터링을 통해 수집한 데이터를 사용하여 리소스 과다 사용, 응용 프로그램 결함, 리소스 구성 오류 또는 보안 관련 이벤트와 같은 이벤트로 인해 발생하는 운영 문제를 확인 가능\n\n\n\n\n운영자가 문제에 대하여 능동적으로 대처가 가능\n\n운영 중단이 발생하였을 때 알려주는 것은 대처를 할 수 없으며 모니터링을 통해 Metric을 확인하여 사전에 운영 중단 방지를 위한 자동, 수동으로 수행하여 해결\n\n리소스의 성능과 안정성을 개선\n\n다양한 리소스를 모니터링하면 솔루션이 시스템으로 작동하는 방식을 전체적으로 파악할 수 있으며 병목 현상 또는 비효율적 아키텍처를 발견\n보안 위협 및 이벤트를 인식, 리소스에 액세스하는 비정상적인 트래픽 급증 또는 비정상적인 IP 주소와 같은 이상 징후를 발견\n\n비즈니스 의사 결정에 도움을 줌\n\n앱의 새 기능을 시작했는데 이 기능이 사용되고 있는지 확인할 때 애플리케이션 수준 Metric을 수집하고 새 기능을 사용하는 사용자 수를 체크\n\n\n\n\n\n\nAWS에서 제공하는 AWS서비스/App의 모니터링 서비스\nPublic 서비스(인터넷을 통해서 접근 또는 Interface Endpoint로 접근)\n로그, 지표, 이벤트 등의 운영데이터를 수집하여 시각화 처리\n경보 생성을 통해 자동화된 대응 가능\n\n\n\nVPC의 Private Service에 직접 접근은 불가하며 Interface EndPoint를 설정하여 접근이 가능  \n\n\n\n\n\n\n\nMetric은 CloudWatch에 게시된 시간순서 데이터 포인트 집합\nAWS 서비스/Application의 Performance를 모니터링 하기 위해 Metric을 생성\n생성 예시\n\n솔루션에 따라서 다양한 형태의 데이터 Metric을 생성\nEC2의 경우 CPU, 네트워크, 디스크 성능 등의 활용률 체크\nS3의 경우 CPU는 의미가 없어서 생성하지 않고 전체 크기 또는 버킷의 개체 수\nRDB의 경우 DB연결, 인스턴스의 CPU사용률, 디스크 공간 사용량\nCloudWatch Agent/ API를 활용하여 Custom Metric을 생성\n\nEC2의 Memory 사용량(외부 Public에서 볼 수 없는 것도 수집 가능)\nPrivate 영역은 CloudWatch 자체 서비스에서 확인 불가 #### Alarm\n\n\n수집된 Metric 값에 따라 Alarm 생성\n이에 따라, 다양한 방법으로 대응 가능(SNS로 Lambda 실행, 이메일 발송) #### Log수집 및 관리 #### 대시보드\n\n\n\n\n\n\nNamespace : CloudWatch metric의 Container이며 필수로 작성\nData Points : Metric을 구성하는 데이터 단위로 UTC를 권장\n\nPeriod : 시간을 기준으로 묶어서 보는 개념으로 60배수, 보관기간에 따라서 확인 가능한 조회기간도 상이함(1분 단위는 15일 -&gt; 15일 이후에는 5분 단위로 확인가능)\n2주 이상 데이터 업데이트 안된 Metric은 콘솔에서 확인 불가(CLI에서만 확인가능)\n\nDimension : Tag/Category이며 Key/Value로 구성되며 Metric을 구분할 때 사용\n\n예: Dimension : (Server=prod, Domain=Seoul)\n\n\n\n\n \n\n\n\n\n\nOptimizing Solutions on AWS\n\nImprove Application Availability\n\nredundancy\n\nS3, DynamoDB는 이미 이중화로 설계되어짐.\n문제는 EC2\n\nUse a Second Availability Zone\n\n서로 다른 AZ에 배치하는 것이 중요.\n하지만 이런 문제는 인스턴스가 두개 이상이므로 다른 문제 발생. \n\nManage Replication, Redirection, and High Availability\n\nCreate a Process for Replication\n\nDNS를 통한 접속 -&gt; 결국 ip를 변경해야하기 때문에 downtime 존재\n\nLoad balancer\n\n로드밸런서를 이용하면 수많은 요청을 분산 시킬 수 있음.\npublic ip를 여러 개 사용할 필요 없음 \n\n\n\n\nRoute Traffic with Amazon Elastic Load Balancing\n\nWhat’s a Load Balancer?\n\n작업을 분산 \n\nFeatures of ELB\n\n컨테이너, ip, aws lamda, ec2 모두 분산 가능\n\nHealth Checks \nELB Components\n\nListeners\n\nclient\n\nTarget groups\n\nEc2, lamda 등 타겟 대상\n\nRules\n\n매칭시키기 위한 룰 존재\nClient의 Source ip와 트래픽을 전송할 target groups \n\n\nALB(Application Load Balancer)\n\n특징\n\nALB routes traffic based on request data.\nSend responses directly to the client.\nALB supports TLS offloading.\nAuthenticate users.\nSecure traffic.\nALB uses the round-robin routing algorithm.\nALB uses the least outstanding request routing algorithm.\nALB has sticky sessions.\n\n\nNetwork Load Balancer\n\n특징\n\nNetwork Load Balancer supports TCP, UDP, and TLS protocols.\nNLB uses a flow hash routing algorithm.\nNLB has sticky sessions.\nNLB supports TLS offloading.\nNLB handles millions of requests per second.\nNLB supports static and elastic IP addresses.\nNLB preserves source IP address.\n\n\nELB types 그림 10\n\nGLB(Gateway Load Balancer)\n\n다른 곳의 application traffic으로 라우팅\n\n\n\n\n\nAmazon EC2 Auto Scaling 그림 11\n\nEC2의 과부화 발생하여 CloudWatch에 보고\nCloudWatch는 auto scailing을 실행\n그러면 각 인스턴스가 필요한 만큼 수평적으로 확장성을 제공.\nEC2가 다시 정상화가 되면 확장된 EC2 자동 종료.\n\n\n\n\n\n\n\n\n보안과 규정 준수는 AWS와 고객의 공동 책임입니다. 이 공유 모델은 AWS가 호스트 운영 체제 및 가상화 계층에서 서비스가 운영되는 시설의 물리적 보안에 이르기까지 구성 요소를 운영, 관리 및 제어하므로 고객의 운영 부담을 경감할 수 있습니다. 고객은 게스트 운영 체제(업데이트 및 보안 패치 포함) 및 다른 관련 애플리케이션 소프트웨어를 관리하고 AWS에서 제공한 보안 그룹 방화벽을 구성할 책임이 있습니다. 고객은 서비스를 선택할 때 신중하게 고려해야 합니다. 고객의 책임이 사용되는 서비스, IT 환경에서 이러한 서비스의 통합, 그리고 관계 법규에 다라 달라지기 때문입니다. 또한, 이러한 공동 책임의 특성은 배포를 허용하는 고객 제어권과 유연성을 제공합니다. 아래 차트에서 볼 수 있듯이 이러한 책임의 차이를 일반적으로 클라우드’의’ 보안과 클라우드’에서의’ 보안이라고 부릅니다.\n\n\nAWS는 AWS 클라우드에서 제공되는 모든 서비스를 실행하는 인프라를 보호할 책임이 있습니다. 이 인프라는 AWS 클라우드 서비스를 실행하는 하드웨어, 소프트웨어, 네트워킹 및 시설로 구성됩니다.\n\n\n고객 책임은 고객이 선택하는 AWS 클라우드 서비스에 따라 달라집니다. 이에 따라 고객이 보안 책임의 일부로 수행해야 하는 구성 작업량이 정해집니다. 예를 들어, Amazon Elastic Compute Cloud (Amazon EC2) 같은 서비스는 Iaas(Ifrastructure as a Service)로 분류되고 고객이 필요한 모든 보안 구성 및 관리 작업을 수행하도록 요구합니다. Amazon EC2 인스턴스를 배포하는 고객은 게스트 운영 체제의 관리(업데이트, 보안 패치 등), 고객이 인스턴스에 설치한 모든 애플리케이션 소프트웨어 또는 유틸리티의 관리, 인스턴스별로 AWS에서 제공한 방화벽(보안 그룹이라고 부름)의 구성 관리에 대한 책임이 있습니다. Amazon S3 및 Amazon DynamoDB와 같은 추상화 서비스의 경우, AWS는 인프라 계층, 운영 체제, 플랫폼을 운영하고 고객은 데이터를 저장하고 검색하기 위해 엔드포인트에 액세스합니다. 고객은 데이터 관리(암호화 옵션 포함), 자산 분류, 적절한 허가를 부여하는 IAM 도구 사용에 책임이 있습니다.\n\n\n\n\n\n예상되는 사용 사례, 피드백 및 수요를 기반으로 규정 준수 노력의 범위에 서비스를 포함합니다. 서비스가 현재 가장 최근 평가 범위에 포함되어 있지 않다고 해서 서비스를 사용할 수 없다는 의미는 아닙니다. 데이터의 특성을 결정하는 것은 조직의 공동 책임 의 일부입니다 . AWS에서 구축하는 항목의 특성에 따라 서비스가 고객 데이터를 처리하거나 저장할지 여부와 고객 데이터 환경의 규정 준수에 어떤 영향을 미칠지 또는 그렇지 않을지 결정해야 합니다.\nAWS 규정 준수 프로그램에 대한 자세한 내용은 https://aws.amazon.com/compliance/에서 확인할 수 있습니다.\n\n\n\nAWS Identity and Access Management(IAM)를 사용하면 AWS 서비스 및 리소스에 대한 액세스를 안전하게 관리할 수 있습니다. IAM을 사용하여 AWS 사용자 및 그룹을 생성 및 관리하고 권한을 사용하여 AWS 리소스에 대한 액세스를 허용 및 거부할 수 있습니다.\nIAM은 추가 비용 없이 제공되는 AWS 계정의 기능입니다. 사용자가 다른 AWS 서비스를 사용한 경우에만 비용이 청구됩니다.\nIAM 사용을 시작하거나 이미 AWS에 등록한 경우 AWS Management Console로 이동하여 이러한 IAM 모범 사례를 시작하십시오.\n\n\n\n\nQuiz\n질문 1\nWhat security mechanism can add an extra layer of protection to your AWS account in addition to a username password combination? ①Transport Layer Protocol or TCP ②Mult-factor Authentication or MFA ③Iris Scan Service or ISS ④Scure Bee Service or SBS\n질문 2\nIf a user wanted to read from a DynamoDB table what policy would you attach to their user profile? ①AmazonDynamoDBFullAccess ②AWSLambdaInvocation-DynamoDB ③AmazonDynamoDBReadOnlyAccess ④AWSLambdaDynamoDBExecutionRole\n질문 3\nWhat are valid MFA or Multi-factor Authentication options available to use on AWS? Select all that apply. ①Gemalto token ②Blizzard Authenticator ③yubiKey ④Google Authenticator\nAWS IoT button\n질문 4\nWhat format is an Identity and Access Management policy document in? ①XML ②HTML ③CSV ④JSON\n질문 5\nWhich are valid options for interacting with your AWS account? Select all that apply. ①Command Line Interface ②Software Development Kit ③Application Programming Interface ④AWS Console\n\n\n\nIAM user - IAM group – IAM policy\nHow to create permissions for a user\n\n\nAdd the user to IAM group\nCopy permissions from an existing user\nAttach existing policies to the user\n\nAWS Organization\n\n역할\n\nAutomate account creation and management\nCreate groups of accounts to reflect business needs\nGovern access to AWS services resources region by policies\nSet up single payment method for all AWS accounts with consolidated billing\nShare resources accross accounts\n\nAWS Organization을 만드는 연습\n\n\nPlan ahead for the structure of organization\nKeep the master account free of any operational AWS resources\nUse AWS CloudTrail – track all AWS usage\n\nApply least previlege practice to policies\nAssign policies to OU\nTest new and modified policies\nUse the APIs and AWS CloudFormation \n\n\n\n\n\nAuthentication and authorization\n\nThe user accesses an AWS account and resources\nAllowing resouces access to other resouces.\nAllow end users to access the applications\n\nServices\n\nAWS organizations : consolidated billing\nAWS IAM : users, groups and policies\nAWS Single Sign-ON (SSO)\nAmazon Cloud directory\nAmazon Congnito : Active directory\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html#monitoring-on-aws-optimization",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html#monitoring-on-aws-optimization",
    "title": "Storage and Database",
    "section": "",
    "text": "모니터링은 리소스의 운영 상태 및 사용에 대한 데이터를 수집하고 분석하는 기법으로 데이터를 수집, 분석 및 사용하여 IT 리소스 및 시스템에 대한 의사 결정을 내리거나 질문에 답변하는 작업이며 모니터링을 통해 수집한 데이터를 사용하여 리소스 과다 사용, 응용 프로그램 결함, 리소스 구성 오류 또는 보안 관련 이벤트와 같은 이벤트로 인해 발생하는 운영 문제를 확인 가능\n\n\n\n\n운영자가 문제에 대하여 능동적으로 대처가 가능\n\n운영 중단이 발생하였을 때 알려주는 것은 대처를 할 수 없으며 모니터링을 통해 Metric을 확인하여 사전에 운영 중단 방지를 위한 자동, 수동으로 수행하여 해결\n\n리소스의 성능과 안정성을 개선\n\n다양한 리소스를 모니터링하면 솔루션이 시스템으로 작동하는 방식을 전체적으로 파악할 수 있으며 병목 현상 또는 비효율적 아키텍처를 발견\n보안 위협 및 이벤트를 인식, 리소스에 액세스하는 비정상적인 트래픽 급증 또는 비정상적인 IP 주소와 같은 이상 징후를 발견\n\n비즈니스 의사 결정에 도움을 줌\n\n앱의 새 기능을 시작했는데 이 기능이 사용되고 있는지 확인할 때 애플리케이션 수준 Metric을 수집하고 새 기능을 사용하는 사용자 수를 체크\n\n\n\n\n\n\nAWS에서 제공하는 AWS서비스/App의 모니터링 서비스\nPublic 서비스(인터넷을 통해서 접근 또는 Interface Endpoint로 접근)\n로그, 지표, 이벤트 등의 운영데이터를 수집하여 시각화 처리\n경보 생성을 통해 자동화된 대응 가능\n\n\n\nVPC의 Private Service에 직접 접근은 불가하며 Interface EndPoint를 설정하여 접근이 가능  \n\n\n\n\n\n\n\nMetric은 CloudWatch에 게시된 시간순서 데이터 포인트 집합\nAWS 서비스/Application의 Performance를 모니터링 하기 위해 Metric을 생성\n생성 예시\n\n솔루션에 따라서 다양한 형태의 데이터 Metric을 생성\nEC2의 경우 CPU, 네트워크, 디스크 성능 등의 활용률 체크\nS3의 경우 CPU는 의미가 없어서 생성하지 않고 전체 크기 또는 버킷의 개체 수\nRDB의 경우 DB연결, 인스턴스의 CPU사용률, 디스크 공간 사용량\nCloudWatch Agent/ API를 활용하여 Custom Metric을 생성\n\nEC2의 Memory 사용량(외부 Public에서 볼 수 없는 것도 수집 가능)\nPrivate 영역은 CloudWatch 자체 서비스에서 확인 불가 #### Alarm\n\n\n수집된 Metric 값에 따라 Alarm 생성\n이에 따라, 다양한 방법으로 대응 가능(SNS로 Lambda 실행, 이메일 발송) #### Log수집 및 관리 #### 대시보드\n\n\n\n\n\n\nNamespace : CloudWatch metric의 Container이며 필수로 작성\nData Points : Metric을 구성하는 데이터 단위로 UTC를 권장\n\nPeriod : 시간을 기준으로 묶어서 보는 개념으로 60배수, 보관기간에 따라서 확인 가능한 조회기간도 상이함(1분 단위는 15일 -&gt; 15일 이후에는 5분 단위로 확인가능)\n2주 이상 데이터 업데이트 안된 Metric은 콘솔에서 확인 불가(CLI에서만 확인가능)\n\nDimension : Tag/Category이며 Key/Value로 구성되며 Metric을 구분할 때 사용\n\n예: Dimension : (Server=prod, Domain=Seoul)\n\n\n\n\n \n\n\n\n\n\nOptimizing Solutions on AWS\n\nImprove Application Availability\n\nredundancy\n\nS3, DynamoDB는 이미 이중화로 설계되어짐.\n문제는 EC2\n\nUse a Second Availability Zone\n\n서로 다른 AZ에 배치하는 것이 중요.\n하지만 이런 문제는 인스턴스가 두개 이상이므로 다른 문제 발생. \n\nManage Replication, Redirection, and High Availability\n\nCreate a Process for Replication\n\nDNS를 통한 접속 -&gt; 결국 ip를 변경해야하기 때문에 downtime 존재\n\nLoad balancer\n\n로드밸런서를 이용하면 수많은 요청을 분산 시킬 수 있음.\npublic ip를 여러 개 사용할 필요 없음 \n\n\n\n\nRoute Traffic with Amazon Elastic Load Balancing\n\nWhat’s a Load Balancer?\n\n작업을 분산 \n\nFeatures of ELB\n\n컨테이너, ip, aws lamda, ec2 모두 분산 가능\n\nHealth Checks \nELB Components\n\nListeners\n\nclient\n\nTarget groups\n\nEc2, lamda 등 타겟 대상\n\nRules\n\n매칭시키기 위한 룰 존재\nClient의 Source ip와 트래픽을 전송할 target groups \n\n\nALB(Application Load Balancer)\n\n특징\n\nALB routes traffic based on request data.\nSend responses directly to the client.\nALB supports TLS offloading.\nAuthenticate users.\nSecure traffic.\nALB uses the round-robin routing algorithm.\nALB uses the least outstanding request routing algorithm.\nALB has sticky sessions.\n\n\nNetwork Load Balancer\n\n특징\n\nNetwork Load Balancer supports TCP, UDP, and TLS protocols.\nNLB uses a flow hash routing algorithm.\nNLB has sticky sessions.\nNLB supports TLS offloading.\nNLB handles millions of requests per second.\nNLB supports static and elastic IP addresses.\nNLB preserves source IP address.\n\n\nELB types 그림 10\n\nGLB(Gateway Load Balancer)\n\n다른 곳의 application traffic으로 라우팅\n\n\n\n\n\nAmazon EC2 Auto Scaling 그림 11\n\nEC2의 과부화 발생하여 CloudWatch에 보고\nCloudWatch는 auto scailing을 실행\n그러면 각 인스턴스가 필요한 만큼 수평적으로 확장성을 제공.\nEC2가 다시 정상화가 되면 확장된 EC2 자동 종료."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html#shared-responsibility",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html#shared-responsibility",
    "title": "Storage and Database",
    "section": "",
    "text": "보안과 규정 준수는 AWS와 고객의 공동 책임입니다. 이 공유 모델은 AWS가 호스트 운영 체제 및 가상화 계층에서 서비스가 운영되는 시설의 물리적 보안에 이르기까지 구성 요소를 운영, 관리 및 제어하므로 고객의 운영 부담을 경감할 수 있습니다. 고객은 게스트 운영 체제(업데이트 및 보안 패치 포함) 및 다른 관련 애플리케이션 소프트웨어를 관리하고 AWS에서 제공한 보안 그룹 방화벽을 구성할 책임이 있습니다. 고객은 서비스를 선택할 때 신중하게 고려해야 합니다. 고객의 책임이 사용되는 서비스, IT 환경에서 이러한 서비스의 통합, 그리고 관계 법규에 다라 달라지기 때문입니다. 또한, 이러한 공동 책임의 특성은 배포를 허용하는 고객 제어권과 유연성을 제공합니다. 아래 차트에서 볼 수 있듯이 이러한 책임의 차이를 일반적으로 클라우드’의’ 보안과 클라우드’에서의’ 보안이라고 부릅니다.\n\n\nAWS는 AWS 클라우드에서 제공되는 모든 서비스를 실행하는 인프라를 보호할 책임이 있습니다. 이 인프라는 AWS 클라우드 서비스를 실행하는 하드웨어, 소프트웨어, 네트워킹 및 시설로 구성됩니다.\n\n\n고객 책임은 고객이 선택하는 AWS 클라우드 서비스에 따라 달라집니다. 이에 따라 고객이 보안 책임의 일부로 수행해야 하는 구성 작업량이 정해집니다. 예를 들어, Amazon Elastic Compute Cloud (Amazon EC2) 같은 서비스는 Iaas(Ifrastructure as a Service)로 분류되고 고객이 필요한 모든 보안 구성 및 관리 작업을 수행하도록 요구합니다. Amazon EC2 인스턴스를 배포하는 고객은 게스트 운영 체제의 관리(업데이트, 보안 패치 등), 고객이 인스턴스에 설치한 모든 애플리케이션 소프트웨어 또는 유틸리티의 관리, 인스턴스별로 AWS에서 제공한 방화벽(보안 그룹이라고 부름)의 구성 관리에 대한 책임이 있습니다. Amazon S3 및 Amazon DynamoDB와 같은 추상화 서비스의 경우, AWS는 인프라 계층, 운영 체제, 플랫폼을 운영하고 고객은 데이터를 저장하고 검색하기 위해 엔드포인트에 액세스합니다. 고객은 데이터 관리(암호화 옵션 포함), 자산 분류, 적절한 허가를 부여하는 IAM 도구 사용에 책임이 있습니다.\n\n\n\n\n\n예상되는 사용 사례, 피드백 및 수요를 기반으로 규정 준수 노력의 범위에 서비스를 포함합니다. 서비스가 현재 가장 최근 평가 범위에 포함되어 있지 않다고 해서 서비스를 사용할 수 없다는 의미는 아닙니다. 데이터의 특성을 결정하는 것은 조직의 공동 책임 의 일부입니다 . AWS에서 구축하는 항목의 특성에 따라 서비스가 고객 데이터를 처리하거나 저장할지 여부와 고객 데이터 환경의 규정 준수에 어떤 영향을 미칠지 또는 그렇지 않을지 결정해야 합니다.\nAWS 규정 준수 프로그램에 대한 자세한 내용은 https://aws.amazon.com/compliance/에서 확인할 수 있습니다.\n\n\n\nAWS Identity and Access Management(IAM)를 사용하면 AWS 서비스 및 리소스에 대한 액세스를 안전하게 관리할 수 있습니다. IAM을 사용하여 AWS 사용자 및 그룹을 생성 및 관리하고 권한을 사용하여 AWS 리소스에 대한 액세스를 허용 및 거부할 수 있습니다.\nIAM은 추가 비용 없이 제공되는 AWS 계정의 기능입니다. 사용자가 다른 AWS 서비스를 사용한 경우에만 비용이 청구됩니다.\nIAM 사용을 시작하거나 이미 AWS에 등록한 경우 AWS Management Console로 이동하여 이러한 IAM 모범 사례를 시작하십시오."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html#quiz",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html#quiz",
    "title": "Storage and Database",
    "section": "",
    "text": "Quiz\n질문 1\nWhat security mechanism can add an extra layer of protection to your AWS account in addition to a username password combination? ①Transport Layer Protocol or TCP ②Mult-factor Authentication or MFA ③Iris Scan Service or ISS ④Scure Bee Service or SBS\n질문 2\nIf a user wanted to read from a DynamoDB table what policy would you attach to their user profile? ①AmazonDynamoDBFullAccess ②AWSLambdaInvocation-DynamoDB ③AmazonDynamoDBReadOnlyAccess ④AWSLambdaDynamoDBExecutionRole\n질문 3\nWhat are valid MFA or Multi-factor Authentication options available to use on AWS? Select all that apply. ①Gemalto token ②Blizzard Authenticator ③yubiKey ④Google Authenticator\nAWS IoT button\n질문 4\nWhat format is an Identity and Access Management policy document in? ①XML ②HTML ③CSV ④JSON\n질문 5\nWhich are valid options for interacting with your AWS account? Select all that apply. ①Command Line Interface ②Software Development Kit ③Application Programming Interface ④AWS Console\n\n\n\nIAM user - IAM group – IAM policy\nHow to create permissions for a user\n\n\nAdd the user to IAM group\nCopy permissions from an existing user\nAttach existing policies to the user\n\nAWS Organization\n\n역할\n\nAutomate account creation and management\nCreate groups of accounts to reflect business needs\nGovern access to AWS services resources region by policies\nSet up single payment method for all AWS accounts with consolidated billing\nShare resources accross accounts\n\nAWS Organization을 만드는 연습\n\n\nPlan ahead for the structure of organization\nKeep the master account free of any operational AWS resources\nUse AWS CloudTrail – track all AWS usage\n\nApply least previlege practice to policies\nAssign policies to OU\nTest new and modified policies\nUse the APIs and AWS CloudFormation \n\n\n\n\n\nAuthentication and authorization\n\nThe user accesses an AWS account and resources\nAllowing resouces access to other resouces.\nAllow end users to access the applications\n\nServices\n\nAWS organizations : consolidated billing\nAWS IAM : users, groups and policies\nAWS Single Sign-ON (SSO)\nAmazon Cloud directory\nAmazon Congnito : Active directory"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/13.messenger.html",
    "href": "docs/blog/posts/Engineering/airflow/13.messenger.html",
    "title": "Template Variabler",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n문서(파일)에서 특정 양식으로 작성된 값을 런타임시 실제 값으로 치환해주는 처리 엔진\n템플릿 엔진은 여러 솔루션이 존재하며 그 중 Jinja 템플릿은 파이썬 언어에서 사용하는 엔진\nfrom jinja2 import Template\n\ntemplate = Template('my name is {{name}}')\nnew_template = template.render('name=hjkim')\nprint(new_template)\nJinja 템플릿, 어디서 쓰이나?\n\n파이썬 기반 웹 프레임워크인 Flask, Django에서 주로 사용 (주로 HTML 템플릿 저장 후 화면에 보여질 때 실제 값으로 변환해서 출력)\nSQL작성시에도 활용 가능\n\n\n\n\n\n오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html\n\n\n\n\n\n\n\n\n\nBash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nbash_command (str)\nenv (dict[str, str] | None)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)\n\n\n\n\n\n\n\nDaily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n데이터 관점의 시작일: 2023-02-24 데이터 관점의 종료일: 2023-02-25\n\n\n\n\n예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (과거버전)\n\nex. 2023-02-25 배치일\n\n= data_interval_end\n=\n=\n=\n= next_execution_date (과거버전)\n\n\n\n\n\n\n\n\n\n\nPython 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()\n\n\n\n\n\n\n\n\nMacro 변수의 필요성\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\nDAG 스케줄은 매일 말일에 도는 스케줄인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은데 어떻게 하지? 예를 들어 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되었으면 좋겠어. DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일 이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? data_interval_end 에서 하루 뺀 값이 나와야 하는데…\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta\n\n\nmacros.dateutil\nA reference to the dateutil package\n\n\nmacros.time\nThe standard lib’s time\n\n\nmacros.uuid\nThe standard lib’s uuid\n\n\nmacros.random\nThe standard lib’s random\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리\n\nMacro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함.\n\n\n\n\nfrom datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) # 1월로 변경\nprint(now.replace(month=1)) # 1월로 변경\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기\n\n\n\n\n예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''},\n)\n\n이 부분에 template + macro 활용\n\n\n\n\n\n어떤 파라미터가 Template 변수를 지원할까?\n패러미터\n\npython_callable (Callable | None)\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/python/index.html#airflow.operators.python.PythonOperator\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\")\n+ macros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}',\n'end_date': '{{\n(data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\nmacros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n    start_date = templates_dict.get('start_date') or 'start_date없음'\n    end_date = templates_dict.get('end_date') or 'end_date없음'\n    print(start_date)\n    print(end_date)\n그러나 Python 오퍼레이터에서 굳이 macro를 사용할 필요가 있을까? 날짜 연산을 DAG안에서 직접 할 수 있다면?\n\nmacro 사용\n\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") + macros.dateutil.relativedelta.relativedelta(months=-1,day=1)) | ds }}',\n    'end_date': '{{ (data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\n    macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n        start_date = templates_dict.get('start_date') or 'start_date없음'\n        end_date = templates_dict.get('end_date') or 'end_date없음'\n        print(start_date)\n        print(end_date)\n\n@task(task_id='task_direct_calc')\ndef get_datetime_calc(**kwargs):\n    from dateutil.relativedelta import relativedelta\n    data_interval_end = kwargs['data_interval_end']\n\n직접 연산\n\nprev_month_day_first = data_interval_end.in_timezone('Asia/Seoul') + relativedelta(months=-1, day=1)\nprev_month_day_last = data_interval_end.in_timezone('Asia/Seoul').replace(day=1) + relativedelta(days=-1)\nprint(prev_month_day_first.strftime('%Y-%m-%d'))\nprint(prev_month_day_last.strftime('%Y-%m-%d'))\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/13.messenger.html#airflow에서-사용법",
    "href": "docs/blog/posts/Engineering/airflow/13.messenger.html#airflow에서-사용법",
    "title": "Template Variabler",
    "section": "",
    "text": "오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/13.messenger.html#bashoperator",
    "href": "docs/blog/posts/Engineering/airflow/13.messenger.html#bashoperator",
    "title": "Template Variabler",
    "section": "",
    "text": "Bash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nbash_command (str)\nenv (dict[str, str] | None)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/13.messenger.html#데이터-추출-예시",
    "href": "docs/blog/posts/Engineering/airflow/13.messenger.html#데이터-추출-예시",
    "title": "Template Variabler",
    "section": "",
    "text": "Daily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n데이터 관점의 시작일: 2023-02-24 데이터 관점의 종료일: 2023-02-25"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/13.messenger.html#airflow-날짜-template-변수",
    "href": "docs/blog/posts/Engineering/airflow/13.messenger.html#airflow-날짜-template-변수",
    "title": "Template Variabler",
    "section": "",
    "text": "예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (과거버전)\n\nex. 2023-02-25 배치일\n\n= data_interval_end\n=\n=\n=\n= next_execution_date (과거버전)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/13.messenger.html#python-오퍼레이터에서-template-변수-사용",
    "href": "docs/blog/posts/Engineering/airflow/13.messenger.html#python-오퍼레이터에서-template-변수-사용",
    "title": "Template Variabler",
    "section": "",
    "text": "Python 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/13.messenger.html#macro-변수의-이해",
    "href": "docs/blog/posts/Engineering/airflow/13.messenger.html#macro-변수의-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "Macro 변수의 필요성\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\nDAG 스케줄은 매일 말일에 도는 스케줄인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은데 어떻게 하지? 예를 들어 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되었으면 좋겠어. DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일 이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? data_interval_end 에서 하루 뺀 값이 나와야 하는데…\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta\n\n\nmacros.dateutil\nA reference to the dateutil package\n\n\nmacros.time\nThe standard lib’s time\n\n\nmacros.uuid\nThe standard lib’s uuid\n\n\nmacros.random\nThe standard lib’s random\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리\n\nMacro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/13.messenger.html#파이썬-datetime-dateutil-라이브러리-이해",
    "href": "docs/blog/posts/Engineering/airflow/13.messenger.html#파이썬-datetime-dateutil-라이브러리-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "from datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) # 1월로 변경\nprint(now.replace(month=1)) # 1월로 변경\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/13.messenger.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "href": "docs/blog/posts/Engineering/airflow/13.messenger.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "title": "Template Variabler",
    "section": "",
    "text": "예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''},\n)\n\n이 부분에 template + macro 활용"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html",
    "href": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html",
    "title": "Template Variabler",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n문서(파일)에서 특정 양식으로 작성된 값을 런타임시 실제 값으로 치환해주는 처리 엔진\n템플릿 엔진은 여러 솔루션이 존재하며 그 중 Jinja 템플릿은 파이썬 언어에서 사용하는 엔진\nfrom jinja2 import Template\n\ntemplate = Template('my name is {{name}}')\nnew_template = template.render('name=hjkim')\nprint(new_template)\nJinja 템플릿, 어디서 쓰이나?\n\n파이썬 기반 웹 프레임워크인 Flask, Django에서 주로 사용 (주로 HTML 템플릿 저장 후 화면에 보여질 때 실제 값으로 변환해서 출력)\nSQL작성시에도 활용 가능\n\n\n\n\n\n오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html\n\n\n\n\n\n\n\n\n\nBash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nbash_command (str)\nenv (dict[str, str] | None)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)\n\n\n\n\n\n\n\nDaily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n데이터 관점의 시작일: 2023-02-24 데이터 관점의 종료일: 2023-02-25\n\n\n\n\n예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (과거버전)\n\nex. 2023-02-25 배치일\n\n= data_interval_end\n=\n=\n=\n= next_execution_date (과거버전)\n\n\n\n\n\n\n\n\n\n\nPython 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()\n\n\n\n\n\n\n\n\nMacro 변수의 필요성\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\nDAG 스케줄은 매일 말일에 도는 스케줄인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은데 어떻게 하지? 예를 들어 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되었으면 좋겠어. DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일 이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? data_interval_end 에서 하루 뺀 값이 나와야 하는데…\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta\n\n\nmacros.dateutil\nA reference to the dateutil package\n\n\nmacros.time\nThe standard lib’s time\n\n\nmacros.uuid\nThe standard lib’s uuid\n\n\nmacros.random\nThe standard lib’s random\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리\n\nMacro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함.\n\n\n\n\nfrom datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) # 1월로 변경\nprint(now.replace(month=1)) # 1월로 변경\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기\n\n\n\n\n예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''},\n)\n\n이 부분에 template + macro 활용\n\n\n\n\n\n어떤 파라미터가 Template 변수를 지원할까?\n패러미터\n\npython_callable (Callable | None)\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/python/index.html#airflow.operators.python.PythonOperator\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\")\n+ macros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}',\n'end_date': '{{\n(data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\nmacros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n    start_date = templates_dict.get('start_date') or 'start_date없음'\n    end_date = templates_dict.get('end_date') or 'end_date없음'\n    print(start_date)\n    print(end_date)\n그러나 Python 오퍼레이터에서 굳이 macro를 사용할 필요가 있을까? 날짜 연산을 DAG안에서 직접 할 수 있다면?\n\nmacro 사용\n\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") + macros.dateutil.relativedelta.relativedelta(months=-1,day=1)) | ds }}',\n    'end_date': '{{ (data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\n    macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n        start_date = templates_dict.get('start_date') or 'start_date없음'\n        end_date = templates_dict.get('end_date') or 'end_date없음'\n        print(start_date)\n        print(end_date)\n\n@task(task_id='task_direct_calc')\ndef get_datetime_calc(**kwargs):\n    from dateutil.relativedelta import relativedelta\n    data_interval_end = kwargs['data_interval_end']\n\n직접 연산\n\nprev_month_day_first = data_interval_end.in_timezone('Asia/Seoul') + relativedelta(months=-1, day=1)\nprev_month_day_last = data_interval_end.in_timezone('Asia/Seoul').replace(day=1) + relativedelta(days=-1)\nprint(prev_month_day_first.strftime('%Y-%m-%d'))\nprint(prev_month_day_last.strftime('%Y-%m-%d'))\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#airflow에서-사용법",
    "href": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#airflow에서-사용법",
    "title": "Template Variabler",
    "section": "",
    "text": "오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#bashoperator",
    "href": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#bashoperator",
    "title": "Template Variabler",
    "section": "",
    "text": "Bash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nbash_command (str)\nenv (dict[str, str] | None)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#데이터-추출-예시",
    "href": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#데이터-추출-예시",
    "title": "Template Variabler",
    "section": "",
    "text": "Daily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n데이터 관점의 시작일: 2023-02-24 데이터 관점의 종료일: 2023-02-25"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#airflow-날짜-template-변수",
    "href": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#airflow-날짜-template-변수",
    "title": "Template Variabler",
    "section": "",
    "text": "예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (과거버전)\n\nex. 2023-02-25 배치일\n\n= data_interval_end\n=\n=\n=\n= next_execution_date (과거버전)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#python-오퍼레이터에서-template-변수-사용",
    "href": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#python-오퍼레이터에서-template-변수-사용",
    "title": "Template Variabler",
    "section": "",
    "text": "Python 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#macro-변수의-이해",
    "href": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#macro-변수의-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "Macro 변수의 필요성\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\nDAG 스케줄은 매일 말일에 도는 스케줄인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은데 어떻게 하지? 예를 들어 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되었으면 좋겠어. DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일 이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? data_interval_end 에서 하루 뺀 값이 나와야 하는데…\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta\n\n\nmacros.dateutil\nA reference to the dateutil package\n\n\nmacros.time\nThe standard lib’s time\n\n\nmacros.uuid\nThe standard lib’s uuid\n\n\nmacros.random\nThe standard lib’s random\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리\n\nMacro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#파이썬-datetime-dateutil-라이브러리-이해",
    "href": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#파이썬-datetime-dateutil-라이브러리-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "from datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) # 1월로 변경\nprint(now.replace(month=1)) # 1월로 변경\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "href": "docs/blog/posts/Engineering/airflow/15.dag_monitoring.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "title": "Template Variabler",
    "section": "",
    "text": "예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''},\n)\n\n이 부분에 template + macro 활용"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/17.optimization copy.html",
    "href": "docs/blog/posts/Engineering/airflow/17.optimization copy.html",
    "title": "Template Variabler",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n문서(파일)에서 특정 양식으로 작성된 값을 런타임시 실제 값으로 치환해주는 처리 엔진\n템플릿 엔진은 여러 솔루션이 존재하며 그 중 Jinja 템플릿은 파이썬 언어에서 사용하는 엔진\nfrom jinja2 import Template\n\ntemplate = Template('my name is {{name}}')\nnew_template = template.render('name=hjkim')\nprint(new_template)\nJinja 템플릿, 어디서 쓰이나?\n\n파이썬 기반 웹 프레임워크인 Flask, Django에서 주로 사용 (주로 HTML 템플릿 저장 후 화면에 보여질 때 실제 값으로 변환해서 출력)\nSQL작성시에도 활용 가능\n\n\n\n\n\n오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html\n\n\n\n\n\n\n\n\n\nBash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nbash_command (str)\nenv (dict[str, str] | None)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)\n\n\n\n\n\n\n\nDaily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n데이터 관점의 시작일: 2023-02-24 데이터 관점의 종료일: 2023-02-25\n\n\n\n\n예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (과거버전)\n\nex. 2023-02-25 배치일\n\n= data_interval_end\n=\n=\n=\n= next_execution_date (과거버전)\n\n\n\n\n\n\n\n\n\n\nPython 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()\n\n\n\n\n\n\n\n\nMacro 변수의 필요성\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\nDAG 스케줄은 매일 말일에 도는 스케줄인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은데 어떻게 하지? 예를 들어 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되었으면 좋겠어. DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일 이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? data_interval_end 에서 하루 뺀 값이 나와야 하는데…\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta\n\n\nmacros.dateutil\nA reference to the dateutil package\n\n\nmacros.time\nThe standard lib’s time\n\n\nmacros.uuid\nThe standard lib’s uuid\n\n\nmacros.random\nThe standard lib’s random\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리\n\nMacro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함.\n\n\n\n\nfrom datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) # 1월로 변경\nprint(now.replace(month=1)) # 1월로 변경\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기\n\n\n\n\n예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''},\n)\n\n이 부분에 template + macro 활용\n\n\n\n\n\n어떤 파라미터가 Template 변수를 지원할까?\n패러미터\n\npython_callable (Callable | None)\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/python/index.html#airflow.operators.python.PythonOperator\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\")\n+ macros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}',\n'end_date': '{{\n(data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\nmacros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n    start_date = templates_dict.get('start_date') or 'start_date없음'\n    end_date = templates_dict.get('end_date') or 'end_date없음'\n    print(start_date)\n    print(end_date)\n그러나 Python 오퍼레이터에서 굳이 macro를 사용할 필요가 있을까? 날짜 연산을 DAG안에서 직접 할 수 있다면?\n\nmacro 사용\n\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") + macros.dateutil.relativedelta.relativedelta(months=-1,day=1)) | ds }}',\n    'end_date': '{{ (data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\n    macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n        start_date = templates_dict.get('start_date') or 'start_date없음'\n        end_date = templates_dict.get('end_date') or 'end_date없음'\n        print(start_date)\n        print(end_date)\n\n@task(task_id='task_direct_calc')\ndef get_datetime_calc(**kwargs):\n    from dateutil.relativedelta import relativedelta\n    data_interval_end = kwargs['data_interval_end']\n\n직접 연산\n\nprev_month_day_first = data_interval_end.in_timezone('Asia/Seoul') + relativedelta(months=-1, day=1)\nprev_month_day_last = data_interval_end.in_timezone('Asia/Seoul').replace(day=1) + relativedelta(days=-1)\nprint(prev_month_day_first.strftime('%Y-%m-%d'))\nprint(prev_month_day_last.strftime('%Y-%m-%d'))\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#airflow에서-사용법",
    "href": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#airflow에서-사용법",
    "title": "Template Variabler",
    "section": "",
    "text": "오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요\n\nhttps://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#bashoperator",
    "href": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#bashoperator",
    "title": "Template Variabler",
    "section": "",
    "text": "Bash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nbash_command (str)\nenv (dict[str, str] | None)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#데이터-추출-예시",
    "href": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#데이터-추출-예시",
    "title": "Template Variabler",
    "section": "",
    "text": "Daily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n데이터 관점의 시작일: 2023-02-24 데이터 관점의 종료일: 2023-02-25"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#airflow-날짜-template-변수",
    "href": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#airflow-날짜-template-변수",
    "title": "Template Variabler",
    "section": "",
    "text": "예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (과거버전)\n\nex. 2023-02-25 배치일\n\n= data_interval_end\n=\n=\n=\n= next_execution_date (과거버전)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#python-오퍼레이터에서-template-변수-사용",
    "href": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#python-오퍼레이터에서-template-변수-사용",
    "title": "Template Variabler",
    "section": "",
    "text": "Python 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs\nop_args\ntemplates_dict\ntemplates_exts\nshow_return_value_in_logs\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#macro-변수의-이해",
    "href": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#macro-변수의-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "Macro 변수의 필요성\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\nDAG 스케줄은 매일 말일에 도는 스케줄인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은데 어떻게 하지? 예를 들어 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되었으면 좋겠어. DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일 이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? data_interval_end 에서 하루 뺀 값이 나와야 하는데…\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta\n\n\nmacros.dateutil\nA reference to the dateutil package\n\n\nmacros.time\nThe standard lib’s time\n\n\nmacros.uuid\nThe standard lib’s uuid\n\n\nmacros.random\nThe standard lib’s random\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리\n\nMacro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#파이썬-datetime-dateutil-라이브러리-이해",
    "href": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#파이썬-datetime-dateutil-라이브러리-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "from datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) # 1월로 변경\nprint(now.replace(month=1)) # 1월로 변경\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "href": "docs/blog/posts/Engineering/airflow/17.optimization copy.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "title": "Template Variabler",
    "section": "",
    "text": "예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''},\n)\n\n이 부분에 template + macro 활용"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Linux/01.linux_comands.html",
    "href": "docs/blog/posts/Engineering/Linux/01.linux_comands.html",
    "title": "Linux Commands",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n$: command 상태\n#: shell 상태\n\n\n\n\n\n/bin : 기본 명령어\n/boot : for booting\n/dev : device file, cd-rom\n/etc : config (설정 파일들), passwd, rc.d\n/home : user home dir\n/lib : shared library\n/media : ssd (device와 연결되어 mount해서 쓰는 directory 들)\n/opt : application software package\n/proc : process info\n/root : root home dir\n/sbin : 관리자용, ifconfig\n/srv : system data\n/tmp : temporary dir\n/usr : source or programs\n/usr/local : mysql 보통 이경로에 설치함\n/var : logs, ftp (file transfer protocol), spool(print와 관련된 파일), mail\n/lost+found 휴지통\n\ncf. inode (ls -il) : file 정리 시스템으로 위와 같이 어느 디렉토리에 어떤 파일이 있는지 tree형태로 쉽게 알 수 있음\n\n\n\n\n! : history 중에서 ! 다음으로 시작하는 글자와 대응되는 명령어 찾아서 실행 ex) !l\n!! : 바로 이전 단계에서 실행했던 명령어 실행\nvmstat 1 모르면 !! 계속 치면서 확인해야함\nhistory\nman &lt;명령&gt; : 명령어의 manual ex) man mkdir\n\n\n\n\n\nctrl+alt+T : 터미널 오픈\nls : (list) 현재 디렉토리의 파일 목록 출력\n\nls -a : -a 옵션은 숨겨진 파일 출력\nls -l : -l 자세한 정보 출력\nls -al : 숨김 파일 (.파일명) 까지 볼 수 있음\n\ntouch &lt;파일명&gt; : 파일 만들기\ncat 파일 내용 보기\nhead -10 .bashrc : .bashrc파일안에 있는 데이터를 10줄 보여주기/ tail / less\npwd : (print working directory) 현재 디렉토리 경로 출력\nwhich &lt;폴더명&gt; : 폴더 경로 확인\nclear : terminal clear\necho $PATH: 출력된 경로에 있는 파일들은 어디서든 실행 가능\necho “aaaa” &gt;&gt; test.txt : “aaaa” 추가 입력됨. &gt;&gt; 는 append를 의미. &gt;는 새로씀을 의미\n\necho 기능 확인 echo ‘#!/bin/sh’ &gt; tt.sh → cat tt.sh → echo “echo 123” &gt;&gt; tt.sh → !cat→sh tt.sh 실행하면 123 화면에 출력되어야함\n\napt-get install locales\nmkdir : (make directory) 디렉토리 생성\n\ndirectory와 file의 구분은 ls -al에서 파일권한 column에서 맨앞이 file은 - directory는 d로 표시되어 있음\n\nrmdir : 디렉토리 제거\ncd : (change directory) 디렉토리 변경\n\ncd .. : 상위 디렉토리로 이동\ncd ~ or cd : 홈 디렉토리로 이동\ncd / : root(최상위) 디렉토리로 이동\ncd -: 바로 이전 폴더로 이동\n\nrm : (remove) 파일 삭제 (디렉토리 포함: 옵션 r, rm -r)\n\nrm -rf: 강제 삭제\n\nmv : (move) 파일 이동 및 이름 변경\n\n파일 이동: mv file1 dir1. 이때 dir1은 existing directory\n파일명 변경: mv file2 file3 file2를 file3로 renaming. 이때 file3은 non-existing file\n\ncp : (copy) file 및 directory 복사\n\ncp file1-to-copy file2-name-to-be-copied : cp file1 file2\ncp -r dir1-to-copy dir2-name-to-be-copied : cp dir1 dir2\n\ntar : (Tape ARchiver) 파일 및 디렉토리를 압축 (여러 directories 및 files을 하나의 file로 압축) 및 해제\n\n압축 : tar cvf name.tar file-to-zip \\(\\rightarrow\\) tar cvf dir_1.tar dir_1\n해제 : tar xvf name.tar \\(\\rightarrow\\) tar xvf dir_1.tar\n\nfind &lt;파일 이름&gt;: 현재 폴더에서 파일 찾기\nfind . &lt;파일 이름&gt;: 현재 폴더의 하위에 있는 모든 파일 찾기. 파일이 있어도 못찾았다는 명령어가 뜰때는 파일이름을 명시해줘야함\nfind . -name &lt;파일이름&gt; : 현재 폴더 아래에 파일이름을 찾게 됨\nwhoami: account name\npasswd: 비밀번호 변경할 때 사용 (root도 가능)\nexit : user logout\ncat : 파일을 읽는 명령어\n권한 부여\n\nchmod +x file_name: file_name에 실행 권한 부여\n\n\n\n\n\n\n\n\ncommand mode → editor mode\n\ni : (insert) 커서 왼쪽에 입력\na : (append) 커서 오른쪽에 입력\nA : shift+a 라인끝에서 입력\no : cursor 바로 밑 한줄 개행하면서 편집권한 생김\nO : 윗줄에 입력 모드 시작\n\narrows (vi editor에서의 방향키)\n\n&lt;&gt; : 방향키 (없다고 생각)\nh : 왼쪽\nj : 윗쪽\nk :아랫쪽\nl :오른쪽\nw : 어간 단위로 jump\nctrl+f : page down\nctrl+u/b : page up\nshift+h : 커서를 화면의 제일 윗부분으로 이동. 파일의 제일 윗부분이 아님\nshift+m: 커서를 화명 중간으로\nshift+l: 커서를 화면 끝으로\nG : 문서 맨 밑으로 이동\n\n\n\n\n\n\neditor mode → command mode\n\nesc 왼손으로 누름\n윗 방향키 누르면 이전 명령어들 나옴\n\nsearch 명령어\n\n/검색어 : 검색어 찾기\nn : 검색어 목록 다음\nb, N: 검색어 목록 이전\n\ncommand mode 명령어\n\ncw : change word 한 단어 바꾸기\ndw: delete word 한 단어 삭제\nshift+A : 라인의 젤 끝에서 입력모드\nshift+^, shift+$, 0\nv : 블록지정\ny : 블록 복사 ctrl+c (윈도우 명령 먹음)\nyy: 한개 라인 복사, 복제하고 싶은 라인에 커서를 놓고 yy를 누르면 복제됨\np: paste ctrl+v\ndd : 한개 라인 삭제\nu : undo (ctrl+z 윈도우 명령어 안먹음)\nx : 뒤의 글자 지워짐 (delete)\nX : 앞의 글자 지워짐 (backspace)\nr: change one character\n.\nShift+d : 커서 뒷부분 모두 지우기\nshift + $ : 윈도우의 end key\nshift + 6 : ^, 윈도우의 home key\nCtrl+r, Ctrl+e, Ctrl+y\n\n\n\n\n\n\n명령모드에서 : 누름\n:w : save ctrl+s\n:q: quit 나가기\n:qi : 저장x 나가기\n:wq: 저장 후 나가기\n\nESC + :wq!: 저장 후 exit\n\n:x\n:! : 커맨드라인 임시이동, enter치면 다시 복귀\n:!명령어: 커맨드라인에서 명령어 실행. :!ls, :!ls -al, !python a.py\n!%s: replace, 바꾸기\n:%s/검색어/새단어/g : g는 global 모든 검색어를 새단어로 바꿈\n:%s/검색어/새단어/ig : ignore case, 대소문자 구분 않고 새단어로 바꿈\n:n : n번째 줄로 이동\n:$ : 마지막 줄로 이동\n:set nu or :se nu: 라인 넘버 보여주기\n:set nonu : 라인번호 지우기\n:set paste : 복사 붙이기 하면 indentation 이 자동으로 되어 문단이 망가지는데 colon + paste + p(=ctrl+v) 실행하면 문단 유지. python 스크립트 짤때 유용\n\n\n\n\n\ndf : 현재 마운트 되어있는 디스크들\ndu -m /home: home directory 에 디스크 사용량 MB로 표시\ndu -sk /home: home directory 하위의 디스크 사용량 총합 표시 KB\ndu -sm /home: home directory 하위의 디스크 사용량 총합 표시 MB\nfree -m : 메모리 사용량 MB로 표시. Output상의 swap은 메모리가 부족할때 디스크를 사용량을 보여줌 swap 메모리가 많이 사용되면 도커의 메모리가 부족하다는 말로 메모리를 늘려줘야함\ntop : CPU와 Mem의 사용량 보여주기. space bar를 누르면 최신버전으로 업데이트 됨. load average는 하드웨어가 갖는 부담. Tasks의 zombie는 process가 비정상적으로 종료된 것을 의미. 1을 누르면 cpu가 펼쳐짐. 나가기는 ctrl+c\nvmstat: top보다 편리한 버전. cache는 메모리와 CPU의 속도차를 개선 시켜주기 위한 빠른 메모리. cput란의 id는 idle을 의미 놀고있는 프로세스 표시. 나가기는 ctrl +c\nps : process 상황 조회 ps -ef | grep bash\nchmod :mod is mode이고 mode는 읽기, 쓰기, 실행 권한이 있는데 .sh에는 실행권한이 없음. 그래서 ./[tt.sh](http://tt.sh)와 같은 명령어를 실행하면 permission denied. 아래 그림에서 첫번째 행을 예를 들면, mode를 이진수로 표현하면 rwx r-x r-x = 111 101 101 (2진수) = 755 (10진수) → 7은 권한을 모두 부여, 5는 읽기와 실행 권한만 부여, 4는 읽는 권한만 부여. 그래서 shell (.sh)에 모든 권한을 나에게 부여하려면 chmod 744 tt.sh 를 실행시켜줘야함. chmod +x tt.sh : 나와 그룹에 모두에게 실행 권한만 주기\nchown &lt;username&gt;: &lt;파일명&gt;: owner를 바꾸는 명령어 또는 명의 변경. 그룹이 없으면 내 이름이 그룹이 됨 \nln : symbolic link 걸기 ln   -s   목적지(존재하는)   링크명\nln -s /home/indiflex/ttt ttt : /home/indiflex/ttt를 ttt로 링크를 걸었음 (윈도우의 바로가기 링크 만든 것과 같음). rm ttt하면 링크만 지워짐\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Linux/01.linux_comands.html#editor-mode",
    "href": "docs/blog/posts/Engineering/Linux/01.linux_comands.html#editor-mode",
    "title": "Linux Commands",
    "section": "",
    "text": "command mode → editor mode\n\ni : (insert) 커서 왼쪽에 입력\na : (append) 커서 오른쪽에 입력\nA : shift+a 라인끝에서 입력\no : cursor 바로 밑 한줄 개행하면서 편집권한 생김\nO : 윗줄에 입력 모드 시작\n\narrows (vi editor에서의 방향키)\n\n&lt;&gt; : 방향키 (없다고 생각)\nh : 왼쪽\nj : 윗쪽\nk :아랫쪽\nl :오른쪽\nw : 어간 단위로 jump\nctrl+f : page down\nctrl+u/b : page up\nshift+h : 커서를 화면의 제일 윗부분으로 이동. 파일의 제일 윗부분이 아님\nshift+m: 커서를 화명 중간으로\nshift+l: 커서를 화면 끝으로\nG : 문서 맨 밑으로 이동"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Linux/01.linux_comands.html#command-mode",
    "href": "docs/blog/posts/Engineering/Linux/01.linux_comands.html#command-mode",
    "title": "Linux Commands",
    "section": "",
    "text": "editor mode → command mode\n\nesc 왼손으로 누름\n윗 방향키 누르면 이전 명령어들 나옴\n\nsearch 명령어\n\n/검색어 : 검색어 찾기\nn : 검색어 목록 다음\nb, N: 검색어 목록 이전\n\ncommand mode 명령어\n\ncw : change word 한 단어 바꾸기\ndw: delete word 한 단어 삭제\nshift+A : 라인의 젤 끝에서 입력모드\nshift+^, shift+$, 0\nv : 블록지정\ny : 블록 복사 ctrl+c (윈도우 명령 먹음)\nyy: 한개 라인 복사, 복제하고 싶은 라인에 커서를 놓고 yy를 누르면 복제됨\np: paste ctrl+v\ndd : 한개 라인 삭제\nu : undo (ctrl+z 윈도우 명령어 안먹음)\nx : 뒤의 글자 지워짐 (delete)\nX : 앞의 글자 지워짐 (backspace)\nr: change one character\n.\nShift+d : 커서 뒷부분 모두 지우기\nshift + $ : 윈도우의 end key\nshift + 6 : ^, 윈도우의 home key\nCtrl+r, Ctrl+e, Ctrl+y"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Linux/01.linux_comands.html#colon-command",
    "href": "docs/blog/posts/Engineering/Linux/01.linux_comands.html#colon-command",
    "title": "Linux Commands",
    "section": "",
    "text": "명령모드에서 : 누름\n:w : save ctrl+s\n:q: quit 나가기\n:qi : 저장x 나가기\n:wq: 저장 후 나가기\n\nESC + :wq!: 저장 후 exit\n\n:x\n:! : 커맨드라인 임시이동, enter치면 다시 복귀\n:!명령어: 커맨드라인에서 명령어 실행. :!ls, :!ls -al, !python a.py\n!%s: replace, 바꾸기\n:%s/검색어/새단어/g : g는 global 모든 검색어를 새단어로 바꿈\n:%s/검색어/새단어/ig : ignore case, 대소문자 구분 않고 새단어로 바꿈\n:n : n번째 줄로 이동\n:$ : 마지막 줄로 이동\n:set nu or :se nu: 라인 넘버 보여주기\n:set nonu : 라인번호 지우기\n:set paste : 복사 붙이기 하면 indentation 이 자동으로 되어 문단이 망가지는데 colon + paste + p(=ctrl+v) 실행하면 문단 유지. python 스크립트 짤때 유용"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Linux/01.linux_comands.html#system-적인-command",
    "href": "docs/blog/posts/Engineering/Linux/01.linux_comands.html#system-적인-command",
    "title": "Linux Commands",
    "section": "",
    "text": "df : 현재 마운트 되어있는 디스크들\ndu -m /home: home directory 에 디스크 사용량 MB로 표시\ndu -sk /home: home directory 하위의 디스크 사용량 총합 표시 KB\ndu -sm /home: home directory 하위의 디스크 사용량 총합 표시 MB\nfree -m : 메모리 사용량 MB로 표시. Output상의 swap은 메모리가 부족할때 디스크를 사용량을 보여줌 swap 메모리가 많이 사용되면 도커의 메모리가 부족하다는 말로 메모리를 늘려줘야함\ntop : CPU와 Mem의 사용량 보여주기. space bar를 누르면 최신버전으로 업데이트 됨. load average는 하드웨어가 갖는 부담. Tasks의 zombie는 process가 비정상적으로 종료된 것을 의미. 1을 누르면 cpu가 펼쳐짐. 나가기는 ctrl+c\nvmstat: top보다 편리한 버전. cache는 메모리와 CPU의 속도차를 개선 시켜주기 위한 빠른 메모리. cput란의 id는 idle을 의미 놀고있는 프로세스 표시. 나가기는 ctrl +c\nps : process 상황 조회 ps -ef | grep bash\nchmod :mod is mode이고 mode는 읽기, 쓰기, 실행 권한이 있는데 .sh에는 실행권한이 없음. 그래서 ./[tt.sh](http://tt.sh)와 같은 명령어를 실행하면 permission denied. 아래 그림에서 첫번째 행을 예를 들면, mode를 이진수로 표현하면 rwx r-x r-x = 111 101 101 (2진수) = 755 (10진수) → 7은 권한을 모두 부여, 5는 읽기와 실행 권한만 부여, 4는 읽는 권한만 부여. 그래서 shell (.sh)에 모든 권한을 나에게 부여하려면 chmod 744 tt.sh 를 실행시켜줘야함. chmod +x tt.sh : 나와 그룹에 모두에게 실행 권한만 주기\nchown &lt;username&gt;: &lt;파일명&gt;: owner를 바꾸는 명령어 또는 명의 변경. 그룹이 없으면 내 이름이 그룹이 됨 \nln : symbolic link 걸기 ln   -s   목적지(존재하는)   링크명\nln -s /home/indiflex/ttt ttt : /home/indiflex/ttt를 ttt로 링크를 걸었음 (윈도우의 바로가기 링크 만든 것과 같음). rm ttt하면 링크만 지워짐"
  },
  {
    "objectID": "docs/blog/posts/time_series/index.html",
    "href": "docs/blog/posts/time_series/index.html",
    "title": "Time Series (시계열) Data Analysis",
    "section": "",
    "text": "시계열 데이터란?\n\n시간에 따른 관측값의 순서\n시간 간격의 중요성 (일별, 월별, 분기별 등)\n\n시계열 데이터의 특성\n\n시간 독립변수\n자기 상관 관계 (Autocorrelation)\n추세(Trend)\n계절성(Seasonality)\n주기성(Cyclical)\n불규칙 요소(Irregular component)\n\n시계열 데이터의 시각화\n\n선 그래프 (Line Plot): 시간에 따른 데이터의 변화를 연속적인 선으로 표현\n영역 그래프 (Area Plot): 여러 시계열을 누적하여 표현할 때 유용\n막대 그래프 (Bar Plot): 각 시점의 값을 막대로 표현, 불연속적인 시계열이나 빈도 데이터에 적합\n캔들스틱 차트 (Candlestick Chart): 시가, 고가, 저가, 종가를 한 번에 표현\n히트맵 (Heatmap): 여러 변수나 범주의 시계열을 동시에 비교할 때 유용\n상자 그림 (Box Plot): 시간 구간별 데이터의 분포를 요약하여 표현, 이상치 탐지에 유용\n산점도 (Scatter Plot): 두 시계열 변수 간의 관계를 표현, 상관관계나 패턴을 파악하는 데 유용\n자기상관 그림 (Autocorrelation Plot): ACF, PACF를 시각화, 시계열의 자기상관 구조를 파악하는 데 사용\n계절성 분해 그래프 (Seasonal Decomposition Plot): 시계열을 트렌드, 계절성, 잔차로 분해하여 표현\n스펙트럼 분석 그래프 (Spectral Analysis Plot): 시계열의 주기성을 주파수 도메인에서 분석\n워터폴 차트 (Waterfall Chart): 시간에 따른 누적 변화를 단계별로 표현\n리플 그래프 (Ridge Plot): 여러 분포를 시간에 따라 중첩하여 표현, 분포의 변화를 시각적으로 파악하기 좋음\n\n시계열 데이터의 처리\n\n시간 정보 추출\n시간 기간 연산\n시간 반올림\n시간 그룹핑 (주간, 월간 합계 또는 평균)\n시간 저가 고가 종가 (OHLC)\n시간 롤링 (3일 평균, 5일 합계)\n필터링\n기간별 증감량\n기간별 비중 백분율\n기간별 누적 합계\n동 기간별 plot\n결측치 처리 (Handling Missing Values)\n\n선형 보간법 (Linear Interpolation)\n전방/후방 채우기 (Forward/Backward Fill)\n평균/중앙값 대체 (Mean/Median Imputation)\n시계열 예측 모델을 이용한 대체\n\n이상치 처리 (Outlier Detection and Treatment)\n\nZ-score 방법\nIQR (Interquartile Range) 방법\n이동 평균을 이용한 방법\n시계열 분해를 이용한 방법\n\n정규화 및 표준화 (Normalization and Standardization)\n\nMin-Max 정규화\nZ-score 표준화\n로그 변환 (Log Transformation)\n\n리샘플링 (Resampling)\n\n업샘플링 (Upsampling): 더 높은 빈도로 변환\n다운샘플링 (Downsampling): 더 낮은 빈도로 변환\n\n시간 정렬 (Time Alignment)\n\n시간대 조정\n날짜/시간 형식 통일\n\n차분 (Differencing)\n\n1차 차분\n계절 차분\n\n이동 평균 (Moving Average)\n\n단순 이동 평균\n가중 이동 평균\n지수 이동 평균\n\n윈도잉 (Windowing)\n\n고정 윈도우\n슬라이딩 윈도우\n\n트렌드 제거 (Detrending)\n\n선형 트렌드 제거\n비선형 트렌드 제거\n\n계절성 조정 (Seasonal Adjustment)\n\n계절 분해\nX-11, X-12-ARIMA 방법\n\n주기성 변환 (Periodicity Transformation)\n\n푸리에 변환 (Fourier Transform)\n웨이블릿 변환 (Wavelet Transform)\n\n특성 추출 (Feature Extraction)\n\n통계적 특성 (평균, 분산, 왜도, 첨도 등)\n주파수 도메인 특성\n시간 도메인 특성\n\n차원 축소 (Dimensionality Reduction)\n\n주성분 분석 (PCA)\n동적 시간 워핑 (DTW)\n\n데이터 증강 (Data Augmentation)\n\n시간 이동 (Time Shifting)\n스케일링 (Scaling)\n노이즈 추가 (Adding Noise)\n\n\n시계열 데이터 기초 개념\n\n정상성(stationarity)과 비정상성(non-stationarity)\n지연과 차분 (Lag and Differencing)\nACF와 PACF (ACF and PACF - Autocorrelation Function and Partial Autocorrelation Function)\n적합값과 잔차 (Fitted Values and Residuals)\n백색잡음 (White Noise)\n시계열 분해 (Time Series Decomposition)\n정상성 테스트 (Stationarity Test)\n계절성 검정 (Seasonality Test)\n\n시계열 예측 모델\n\n단순 모델 (Simple Models)\n\n단순 평균 (Simple Average)\n나이브 방법 (Naive Method)\n계절성 나이브 방법 (Seasonal Naive Method)\n드리프트 방법 (Drift Method)\n\n지수 평활법 (Exponential Smoothing)\n\n단순 지수 평활법 (Simple Exponential Smoothing)\n홀트의 선형 추세 방법 (Holt’s Linear Trend Method)\n홀트-윈터스 방법 (Holt-Winters Method)\n\n회귀 기반 모델 (Regression-based Models)\n\n선형 회귀 (Linear Regression)\n다항 회귀 (Polynomial Regression)\n동적 회귀 (Dynamic Regression)\n\n자기회귀 모델 (Autoregressive Models)\n\nAR (AutoRegressive) 모델\nMA (Moving Average) 모델\nARMA (AutoRegressive Moving Average) 모델\nARIMA (AutoRegressive Integrated Moving Average) 모델\nSARIMA (Seasonal ARIMA) 모델\nARIMAX (ARIMA with eXogenous variables) 모델\n\n고급 통계 모델 (Advanced Statistical Models)\n\nVAR (Vector AutoRegression)\nVARMA (Vector AutoRegressive Moving Average)\nGARCH (Generalized AutoRegressive Conditional Heteroskedasticity)\n상태 공간 모델 (State Space Models)\n\n기계학습 모델 (Machine Learning Models)\n\n랜덤 포레스트 (Random Forest)\n서포트 벡터 머신 (Support Vector Machine)\nK-최근접 이웃 (K-Nearest Neighbors)\n그래디언트 부스팅 (Gradient Boosting)\n\n딥러닝 모델 (Deep Learning Models)\n\nRNN (Recurrent Neural Networks)\nLSTM (Long Short-Term Memory)\nGRU (Gated Recurrent Units)\nCNN (Convolutional Neural Networks) for Time Series\nTransformer 모델\n\n혼합 및 앙상블 모델 (Hybrid and Ensemble Models)\n\nTBATS (Trigonometric, Box-Cox transform, ARMA errors, Trend, and Seasonal components)\nProphet (Facebook’s Time Series Forecasting Model)\n앙상블 방법 (여러 모델의 조합)\n\n베이지안 모델 (Bayesian Models)\n\n베이지안 구조적 시계열 모델 (Bayesian Structural Time Series)\n동적 선형 모델 (Dynamic Linear Models)\n\n특수 목적 모델 (Specialized Models)\n\n크로스 섹션 시계열 모델 (Cross-sectional Time Series Models)\n계층적 시계열 모델 (Hierarchical Time Series Models)\n간헐적 수요 예측 모델 (Intermittent Demand Forecasting Models)\n\n\n시계열 분석 frameworks\n\n통계 기반 프레임워크\n\nstatsmodels (Python)\nforecast (R)\nSPSS Time Series Modeler\n\n머신러닝 기반 프레임워크\n\nscikit-learn (Python)\nMLlib (Apache Spark)\nH2O.ai\n\n딥러닝 기반 프레임워크\n\nTensorFlow (Google)\nPyTorch (Facebook)\nKeras\n\n시계열 특화 프레임워크\n\nProphet (Facebook)\ntslearn (Python)\ntsfresh (Python)\n\n통합 분석 플랫폼\n\nDarts (Python)\nGluonTS (Amazon)\nKats (Facebook)\n\n비즈니스 인텔리전스 도구\n\nTableau\nPower BI (Microsoft)\nQlikView\n\n클라우드 기반 서비스\n\nAmazon Forecast\nGoogle Cloud AI Platform\nAzure Time Series Insights\n\n특수 목적 프레임워크\n\npmdarima (Python, ARIMA 모델링)\nPyFlux (Python, 베이지안 추론)\nBEAST (R, 베이지안 시계열 분석)\n\n실시간 처리 프레임워크\n\nApache Flink\nApache Kafka Streams\nStreamz (Python)\n\n시각화 특화 프레임워크\n\nPlotly\nBokeh\nAltair"
  },
  {
    "objectID": "docs/blog/posts/time_series/index.html#시계열-stduy-목차",
    "href": "docs/blog/posts/time_series/index.html#시계열-stduy-목차",
    "title": "Time Series (시계열) Data Analysis",
    "section": "",
    "text": "시계열 데이터란?\n\n시간에 따른 관측값의 순서\n시간 간격의 중요성 (일별, 월별, 분기별 등)\n\n시계열 데이터의 특성\n\n시간 독립변수\n자기 상관 관계 (Autocorrelation)\n추세(Trend)\n계절성(Seasonality)\n주기성(Cyclical)\n불규칙 요소(Irregular component)\n\n시계열 데이터의 시각화\n\n선 그래프 (Line Plot): 시간에 따른 데이터의 변화를 연속적인 선으로 표현\n영역 그래프 (Area Plot): 여러 시계열을 누적하여 표현할 때 유용\n막대 그래프 (Bar Plot): 각 시점의 값을 막대로 표현, 불연속적인 시계열이나 빈도 데이터에 적합\n캔들스틱 차트 (Candlestick Chart): 시가, 고가, 저가, 종가를 한 번에 표현\n히트맵 (Heatmap): 여러 변수나 범주의 시계열을 동시에 비교할 때 유용\n상자 그림 (Box Plot): 시간 구간별 데이터의 분포를 요약하여 표현, 이상치 탐지에 유용\n산점도 (Scatter Plot): 두 시계열 변수 간의 관계를 표현, 상관관계나 패턴을 파악하는 데 유용\n자기상관 그림 (Autocorrelation Plot): ACF, PACF를 시각화, 시계열의 자기상관 구조를 파악하는 데 사용\n계절성 분해 그래프 (Seasonal Decomposition Plot): 시계열을 트렌드, 계절성, 잔차로 분해하여 표현\n스펙트럼 분석 그래프 (Spectral Analysis Plot): 시계열의 주기성을 주파수 도메인에서 분석\n워터폴 차트 (Waterfall Chart): 시간에 따른 누적 변화를 단계별로 표현\n리플 그래프 (Ridge Plot): 여러 분포를 시간에 따라 중첩하여 표현, 분포의 변화를 시각적으로 파악하기 좋음\n\n시계열 데이터의 처리\n\n시간 정보 추출\n시간 기간 연산\n시간 반올림\n시간 그룹핑 (주간, 월간 합계 또는 평균)\n시간 저가 고가 종가 (OHLC)\n시간 롤링 (3일 평균, 5일 합계)\n필터링\n기간별 증감량\n기간별 비중 백분율\n기간별 누적 합계\n동 기간별 plot\n결측치 처리 (Handling Missing Values)\n\n선형 보간법 (Linear Interpolation)\n전방/후방 채우기 (Forward/Backward Fill)\n평균/중앙값 대체 (Mean/Median Imputation)\n시계열 예측 모델을 이용한 대체\n\n이상치 처리 (Outlier Detection and Treatment)\n\nZ-score 방법\nIQR (Interquartile Range) 방법\n이동 평균을 이용한 방법\n시계열 분해를 이용한 방법\n\n정규화 및 표준화 (Normalization and Standardization)\n\nMin-Max 정규화\nZ-score 표준화\n로그 변환 (Log Transformation)\n\n리샘플링 (Resampling)\n\n업샘플링 (Upsampling): 더 높은 빈도로 변환\n다운샘플링 (Downsampling): 더 낮은 빈도로 변환\n\n시간 정렬 (Time Alignment)\n\n시간대 조정\n날짜/시간 형식 통일\n\n차분 (Differencing)\n\n1차 차분\n계절 차분\n\n이동 평균 (Moving Average)\n\n단순 이동 평균\n가중 이동 평균\n지수 이동 평균\n\n윈도잉 (Windowing)\n\n고정 윈도우\n슬라이딩 윈도우\n\n트렌드 제거 (Detrending)\n\n선형 트렌드 제거\n비선형 트렌드 제거\n\n계절성 조정 (Seasonal Adjustment)\n\n계절 분해\nX-11, X-12-ARIMA 방법\n\n주기성 변환 (Periodicity Transformation)\n\n푸리에 변환 (Fourier Transform)\n웨이블릿 변환 (Wavelet Transform)\n\n특성 추출 (Feature Extraction)\n\n통계적 특성 (평균, 분산, 왜도, 첨도 등)\n주파수 도메인 특성\n시간 도메인 특성\n\n차원 축소 (Dimensionality Reduction)\n\n주성분 분석 (PCA)\n동적 시간 워핑 (DTW)\n\n데이터 증강 (Data Augmentation)\n\n시간 이동 (Time Shifting)\n스케일링 (Scaling)\n노이즈 추가 (Adding Noise)\n\n\n시계열 데이터 기초 개념\n\n정상성(stationarity)과 비정상성(non-stationarity)\n지연과 차분 (Lag and Differencing)\nACF와 PACF (ACF and PACF - Autocorrelation Function and Partial Autocorrelation Function)\n적합값과 잔차 (Fitted Values and Residuals)\n백색잡음 (White Noise)\n시계열 분해 (Time Series Decomposition)\n정상성 테스트 (Stationarity Test)\n계절성 검정 (Seasonality Test)\n\n시계열 예측 모델\n\n단순 모델 (Simple Models)\n\n단순 평균 (Simple Average)\n나이브 방법 (Naive Method)\n계절성 나이브 방법 (Seasonal Naive Method)\n드리프트 방법 (Drift Method)\n\n지수 평활법 (Exponential Smoothing)\n\n단순 지수 평활법 (Simple Exponential Smoothing)\n홀트의 선형 추세 방법 (Holt’s Linear Trend Method)\n홀트-윈터스 방법 (Holt-Winters Method)\n\n회귀 기반 모델 (Regression-based Models)\n\n선형 회귀 (Linear Regression)\n다항 회귀 (Polynomial Regression)\n동적 회귀 (Dynamic Regression)\n\n자기회귀 모델 (Autoregressive Models)\n\nAR (AutoRegressive) 모델\nMA (Moving Average) 모델\nARMA (AutoRegressive Moving Average) 모델\nARIMA (AutoRegressive Integrated Moving Average) 모델\nSARIMA (Seasonal ARIMA) 모델\nARIMAX (ARIMA with eXogenous variables) 모델\n\n고급 통계 모델 (Advanced Statistical Models)\n\nVAR (Vector AutoRegression)\nVARMA (Vector AutoRegressive Moving Average)\nGARCH (Generalized AutoRegressive Conditional Heteroskedasticity)\n상태 공간 모델 (State Space Models)\n\n기계학습 모델 (Machine Learning Models)\n\n랜덤 포레스트 (Random Forest)\n서포트 벡터 머신 (Support Vector Machine)\nK-최근접 이웃 (K-Nearest Neighbors)\n그래디언트 부스팅 (Gradient Boosting)\n\n딥러닝 모델 (Deep Learning Models)\n\nRNN (Recurrent Neural Networks)\nLSTM (Long Short-Term Memory)\nGRU (Gated Recurrent Units)\nCNN (Convolutional Neural Networks) for Time Series\nTransformer 모델\n\n혼합 및 앙상블 모델 (Hybrid and Ensemble Models)\n\nTBATS (Trigonometric, Box-Cox transform, ARMA errors, Trend, and Seasonal components)\nProphet (Facebook’s Time Series Forecasting Model)\n앙상블 방법 (여러 모델의 조합)\n\n베이지안 모델 (Bayesian Models)\n\n베이지안 구조적 시계열 모델 (Bayesian Structural Time Series)\n동적 선형 모델 (Dynamic Linear Models)\n\n특수 목적 모델 (Specialized Models)\n\n크로스 섹션 시계열 모델 (Cross-sectional Time Series Models)\n계층적 시계열 모델 (Hierarchical Time Series Models)\n간헐적 수요 예측 모델 (Intermittent Demand Forecasting Models)\n\n\n시계열 분석 frameworks\n\n통계 기반 프레임워크\n\nstatsmodels (Python)\nforecast (R)\nSPSS Time Series Modeler\n\n머신러닝 기반 프레임워크\n\nscikit-learn (Python)\nMLlib (Apache Spark)\nH2O.ai\n\n딥러닝 기반 프레임워크\n\nTensorFlow (Google)\nPyTorch (Facebook)\nKeras\n\n시계열 특화 프레임워크\n\nProphet (Facebook)\ntslearn (Python)\ntsfresh (Python)\n\n통합 분석 플랫폼\n\nDarts (Python)\nGluonTS (Amazon)\nKats (Facebook)\n\n비즈니스 인텔리전스 도구\n\nTableau\nPower BI (Microsoft)\nQlikView\n\n클라우드 기반 서비스\n\nAmazon Forecast\nGoogle Cloud AI Platform\nAzure Time Series Insights\n\n특수 목적 프레임워크\n\npmdarima (Python, ARIMA 모델링)\nPyFlux (Python, 베이지안 추론)\nBEAST (R, 베이지안 시계열 분석)\n\n실시간 처리 프레임워크\n\nApache Flink\nApache Kafka Streams\nStreamz (Python)\n\n시각화 특화 프레임워크\n\nPlotly\nBokeh\nAltair"
  },
  {
    "objectID": "docs/blog/posts/Deep Learning/test2.html",
    "href": "docs/blog/posts/Deep Learning/test2.html",
    "title": "Kwangmin Kim",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# 데이터 준비\ndata = np.array([10345.95344093, 10254.57894458, 10198.42042604, 10160.22832452, 10117.95232831, \n                 10096.21754303, 10068.98950732, 10048.45509888, 10029.97294258, 10012.6500211, \n                 9992.32002722, 9986.8216521, 9967.30002051, 9953.65734963, 9944.32238966, \n                 9924.22764642, 9918.56652301, 9906.78991636, 9896.77347545, 9881.72663955, \n                 9870.3843831, 9866.06461084, 9857.33460291, 9847.63331086, 9839.89791331, \n                 9829.26714378, 9819.53021078, 9803.83869316, 9802.0457347, 9792.98681928, \n                 9785.25389664, 9770.42734065, 9762.52207987, 9747.74130149, 9745.77012212, \n                 9734.95618568, 9731.59571095, 9728.38748027, 9713.43131064, 9706.91520543, \n                 9698.41219488, 9695.5943976, 9685.78235559, 9677.83567665, 9678.25361304])\n\n# 데이터 정규화\ndata_normalized = (data - np.min(data)) / (np.max(data) - np.min(data))\n\n# 입력 데이터 생성\nX = np.arange(len(data)).reshape(-1, 1) / len(data)\nY = data_normalized.reshape(-1, 1)\n\n\n\ndef search_golden_section(fun, dfun, x, s, args=(), delta=1.0e-2, tol=1e-15):\n    \"\"\"\n    https://en.wikipedia.org/wiki/Golden-section_search and [arora]\n    \n    fun   : Original objective function\n    dfun  : Objective function gradient which is not used\n    x     : Start point\n    s     : 1D search directin\n    args  : Tuple extra arguments passed to the objective function\n    delta : Init. guess interval determining initial interval of uncertainty\n    tol   : stop criterion\n    \"\"\"\n    gr = (np.sqrt(5) + 1) / 2\n        \n    AL = 0.\n    FL = f_alpha(AL, fun, x, s, args)\n    AA = delta\n    FA = f_alpha(AA, fun, x, s, args)\n    while  FL &lt; FA :\n        delta = 0.1*delta\n        AA = delta\n        FA = f_alpha(AA, fun, x, s, args)\n    \n    j = 1\n    AU = AA + delta * (gr**j)\n    FU = f_alpha(AU, fun, x, s, args)\n    while FA &gt; FU :\n        AL = AA\n        AA = AU\n        FL = FA\n        FA = FU\n        \n        j += 1\n        AU = AA + delta * (gr**j)\n        FU = f_alpha(AU, fun, x, s, args)\n\n    AB = AL + (AU - AL) / gr\n    FB = f_alpha(AB, fun, x, s, args)\n    \n    while abs(AA - AB) &gt; tol:\n        if f_alpha(AA, fun, x, s, args) &lt; f_alpha(AB, fun, x, s, args):\n            AU = AB\n        else:\n            AL = AA\n\n        # we recompute both c and d here to avoid loss of precision \n        # which may lead to incorrect results or infinite loop\n        AA = AU - (AU - AL) / gr\n        AB = AL + (AU - AL) / gr\n\n    return ( (AU + AL) / 2, )\n\n\ndef compute_conjugate_gradient(f, df, x, args=(), eps=1.0e-7, max_iter=2500, verbose=False, callback=None):\n    \"\"\"\n    f       : 최적화 할 함수\n    df      : 최적화할 함수의 도함수\n    x       : 초기값\n    args    : f(x, *args)로 f를 호출하기 위한 추가 매개변수\n    eps     : 정기 기준\n    max_iter: 최대 반복수\n    verbose : 실행 정보를 텍스트로 출력 \n    callback: x에 대한 기록을 위한 콜백함수\n    \"\"\"\n\n    if verbose:\n        print(\"#####START OPTIMIZATION#####\")\n        print(\"INIT POINT : {}, dtype : {}\".format(x, x.dtype))\n\n    for k in range(max_iter):\n        c = df(x, *args)\n        if np.linalg.norm(c) &lt; eps :\n            if verbose:\n                print(\"Stop criterion break Iter: {:5d}, x: {}\".format(k, x))\n                print(\"\\n\")\n            break\n\n        if k == 0 :\n            d = -c\n        else:\n            beta = (np.linalg.norm(c) / np.linalg.norm(c_old))**2\n            d = -c + beta*d\n        \n        alpha = search_golden_section(f, df, x, d, args=args)[0]\n        x = x + alpha * d\n        c_old = c.copy()    \n\n        if callback :\n            callback(x)    \n\n    else:\n        print(\"Stop max iter:{:5d} x:{}\".format(k, x)) \n\n    return x\n\n\n# 신경망 모델 정의\nclass NeuralNetwork:\n    def __init__(self, input_size, hidden_sizes, output_size):\n        self.layers = len(hidden_sizes) + 1\n        self.W = [np.random.randn(input_size, hidden_sizes[0]) * 0.01]\n        self.b = [np.zeros((1, hidden_sizes[0]))]\n        \n        for i in range(1, len(hidden_sizes)):\n            self.W.append(np.random.randn(hidden_sizes[i-1], hidden_sizes[i]) * 0.01)\n            self.b.append(np.zeros((1, hidden_sizes[i])))\n        \n        self.W.append(np.random.randn(hidden_sizes[-1], output_size) * 0.01)\n        self.b.append(np.zeros((1, output_size)))\n\n    def forward(self, X):\n        self.a = [X]\n        for i in range(self.layers):\n            z = np.dot(self.a[i], self.W[i]) + self.b[i]\n            a = np.tanh(z)\n            self.a.append(a)\n        return self.a[-1]\n\n    def get_params(self):\n        return np.concatenate([w.ravel() for w in self.W] + [b.ravel() for b in self.b])\n\n    def set_params(self, params):\n        start = 0\n        for i in range(self.layers):\n            end = start + self.W[i].size\n            self.W[i] = params[start:end].reshape(self.W[i].shape)\n            start = end\n            end = start + self.b[i].size\n            self.b[i] = params[start:end].reshape(self.b[i].shape)\n            start = end\n\n# 목적 함수 및 그래디언트 정의\ndef objective(params, nn, X, Y):\n    nn.set_params(params)\n    predictions = nn.forward(X)\n    return np.mean((predictions - Y) ** 2)\n\ndef gradient(params, nn, X, Y):\n    nn.set_params(params)\n    m = X.shape[0]\n    predictions = nn.forward(X)\n    \n    grad = []\n    delta = predictions - Y\n    for i in range(nn.layers - 1, -1, -1):\n        dW = np.dot(nn.a[i].T, delta) / m\n        db = np.sum(delta, axis=0, keepdims=True) / m\n        grad = [dW.ravel(), db.ravel()] + grad\n        \n        if i &gt; 0:\n            delta = np.dot(delta, nn.W[i].T) * (1 - nn.a[i]**2)\n    \n    return np.concatenate(grad)\n\n# Helper function for search_golden_section\ndef f_alpha(alpha, fun, x, s, args):\n    return fun(x + alpha * s, *args)\n\n# search_golden_section 및 compute_conjugate_gradient 함수는 이미 정의되어 있다고 가정합니다.\n\n# 모델 학습\nnn = NeuralNetwork(input_size=1, hidden_sizes=[32, 32], output_size=1)\ninitial_params = nn.get_params()\n\noptimized_params = compute_conjugate_gradient(\n    f=objective,\n    df=gradient,\n    x=initial_params,\n    args=(nn, X, Y),\n    eps=1e-6,\n    max_iter=1000,\n    verbose=True\n)\n\nnn.set_params(optimized_params)\n\n# 예측\npredictions_normalized = nn.forward(X)\npredictions = predictions_normalized * (np.max(data) - np.min(data)) + np.min(data)\n\n# 결과 시각화\nplt.figure(figsize=(10, 5))\nplt.plot(data, label='Actual')\nplt.plot(predictions, label='Predicted', linestyle='--')\nplt.title('Time Series Prediction with Conjugate Gradient Optimization')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.legend()\nplt.show()\n\n# 성능 평가\nmse = np.mean((data - predictions.flatten()) ** 2)\nprint(f'Mean Squared Error: {mse}')\n\nNameError: name 'compute_conjugate_gradient' is not defined"
  },
  {
    "objectID": "docs/blog/posts/time_series/basic_stationarity.html",
    "href": "docs/blog/posts/time_series/basic_stationarity.html",
    "title": "시계열 분석 기초 개념 - 정상성(stationarity)",
    "section": "",
    "text": "다른 데이터와 달리, 시계열 데이터는 시간이 흐름에 따라 연속적으로 기록되며, 동일한 데이터가 반복되지 않고, 항상 새로운 데이터가 시간 축을 따라 순차적으로 추가된다는 특성이 있다. 따라서 시계열 데이터 분석은 추세, 계절성, 자기 상관성 등의 패턴을 분석하는 과정을 포함한다.\n시계열 예측 모델은 이러한 시계열 데이터의 특성을 잘 반영하여 설계되어야 한다. 시계열 데이터의 주요 특성으로는 정상성, 비정상성, 지연, 차분, 자기 상관성, 백색 잡음 등이 있으며, 이러한 요소들을 고려한 모델링이 중요하다."
  },
  {
    "objectID": "docs/blog/posts/time_series/basic_stationarity.html#introduction",
    "href": "docs/blog/posts/time_series/basic_stationarity.html#introduction",
    "title": "시계열 분석 기초 개념 - 정상성(stationarity)",
    "section": "",
    "text": "다른 데이터와 달리, 시계열 데이터는 시간이 흐름에 따라 연속적으로 기록되며, 동일한 데이터가 반복되지 않고, 항상 새로운 데이터가 시간 축을 따라 순차적으로 추가된다는 특성이 있다. 따라서 시계열 데이터 분석은 추세, 계절성, 자기 상관성 등의 패턴을 분석하는 과정을 포함한다.\n시계열 예측 모델은 이러한 시계열 데이터의 특성을 잘 반영하여 설계되어야 한다. 시계열 데이터의 주요 특성으로는 정상성, 비정상성, 지연, 차분, 자기 상관성, 백색 잡음 등이 있으며, 이러한 요소들을 고려한 모델링이 중요하다."
  },
  {
    "objectID": "docs/blog/posts/time_series/basic_stationarity.html#정상성stationarity의-정의",
    "href": "docs/blog/posts/time_series/basic_stationarity.html#정상성stationarity의-정의",
    "title": "시계열 분석 기초 개념 - 정상성(stationarity)",
    "section": "2 정상성(Stationarity)의 정의",
    "text": "2 정상성(Stationarity)의 정의\n시계열이 정상성을 가진다는 것은 다음 조건들을 만족한다는 의미:\n\n평균이 일정, \\(E[Y_t] = \\mu \\quad \\text{모든 } t \\text{에 대해}\\)\n분산이 일정, \\(\\text{Var}(Y_t) = \\sigma^2 \\quad \\text{모든 } t \\text{에 대해}\\)\n공분산이 시차에만 의존하고 시간 자체에는 의존하지 않음.\n\n\\(\\text{Cov}(Y_t, Y_{t+k}) = \\gamma_k \\quad \\text{모든 시간 } t \\text{와 시간 간격 } k \\text{에 대해}\\)\n\n\n\n2.1 \\(\\text{Cov}(Y_t, Y_{t+k}) = \\gamma_k\\)\n두 시점 사이의 관계가 그 사이의 간격(시차)에만 영향을 받고, 실제 시간 위치는 중요하지 않다는 의미다. 특정 시점에서의 값이 그 시점 이후의 값들과 얼마나 상관관계가 있는지를 측정할 때 그 상관관계가 시간 \\(t\\) 의 특정 위치와 무관하다는 의미이다. 다시 말해, 공분산이 \\(t\\) 에 따라 변하지 않고 오직 시차 \\(k\\) 에만 의존한다는 뜻\n\nPythonR\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 시계열 데이터 생성\nnp.random.seed(0)\nn = 100  \nlags = 20  # 시차의 최대값\n\n# AR(1) 생성 (phi = 0.8)\nphi = 0.8\ny = np.zeros(n)\nepsilon = np.random.normal(0, 1, n)  # white noise\n\nfor t in range(1, n):\n    y[t] = phi * y[t - 1] + epsilon[t]\n\ncovariances = [np.cov(y[:-k], y[k:])[0, 1] for k in range(1, lags + 1)]\n\nplt.figure(figsize=(12, 6))\nplt.bar(range(1, lags + 1), covariances, color='blue', alpha=0.7)\n\n\n&lt;BarContainer object of 20 artists&gt;\n\n\nCode\nplt.title('Covariance as a Function of Lag (Cov(Yt, Yt+k) = γk)')\nplt.xlabel('Lag (k)')\nplt.ylabel('Covariance (γk)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(reshape2)\n\n# 시계열 데이터 생성\nset.seed(0)\nn &lt;- 100\nlags &lt;- 20  # 시차의 최대값\n\n# AR(1) 생성 (phi = 0.8)\nphi &lt;- 0.8\ny &lt;- numeric(n)\nepsilon &lt;- rnorm(n, mean = 0, sd = 1)  # white noise\n\nfor (t in 2:n) {\n  y[t] &lt;- phi * y[t - 1] + epsilon[t]\n}\n\n# 공분산 계산\ncovariances &lt;- sapply(1:lags, function(k) cov(y[1:(n-k)], y[(k+1):n]))\n\n# 그래프 그리기\nbarplot(covariances, names.arg = 1:lags, col = \"blue\", border = NA, \n        main = \"Covariance as a Function of Lag (Cov(Yt, Yt+k) = γk)\",\n        xlab = \"Lag (k)\", ylab = \"Covariance (γk)\")\ngrid()\n\n# 시점 t 선택 (여기서는 예시로 여러 개의 t를 선택하여 비교)\ntime_points &lt;- c(0, 10, 20, 30, 40, 50, 60, 70)\nmax_lag &lt;- 15  \n\n# 각 시점 t에서 공분산 계산\ncovariances_per_t &lt;- data.frame(Lag = 1:max_lag)\n\nfor (t in time_points) {\n  covariances &lt;- numeric(max_lag)\n  for (k in 1:max_lag) {\n    covariances[k] &lt;- cov(y[t:(t+max_lag-1)], y[(t+k):(t+k+max_lag-1)])\n  }\n  covariances_per_t[[paste0(\"t_\", t)]] &lt;- covariances\n}\n\ncovariances_long &lt;- melt(covariances_per_t, id.vars = \"Lag\", variable.name = \"Time\", value.name = \"Covariance\")\n\n# 시각화\nggplot(covariances_long, aes(x = Lag, y = Covariance, color = Time, group = Time)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Covariance as a Function of Lag for Different Time Points\",\n       x = \"Lag (k)\", y = \"Covariance (γk)\") +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n그래프에서 볼 수 있듯이, 특정 시차 \\(k\\) 에서의 공분산 값은 모든 시점 \\(t\\) 에 대해 동일한 패턴을 보인다. 이 같은 특성은 흔히 정상 시계열 이라고 불리며, 이는 시계열 데이터가 시간에 따른 일정한 통계적 특성을 갖고 있음을 의미한다.\n시차(Lag)에 대한 의존성\n\n시차 \\(k\\) 가 증가할수록 공분산의 값이 점차 감소하는 패턴을 보이고 있다. 이는 \\(k\\) 가 커질수록 시계열 데이터가 더 멀리 떨어진 시점과의 상관관계가 줄어든다는 것을 의미한다.\n\n시간 \\(t\\) 에 대한 독립성\n\n공분산이 특정 시차 \\(k\\) 에 대해서만 달라지고, 시간 \\(t\\) 자체에는 의존하지 않음을 의미. 즉, 동일한 시차 \\(k\\) 에 대해 어느 시간 \\(t\\) 에서 계산하든 동일한 공분산 값을 가진다.\n이를 그래프에서 직접적으로 확인하기는 어렵지만, 중요한 점은 모든 시차 \\(k\\) 에 대해 공분산이 일정한 패턴(감소하는 경향)을 유지하고 있다는 것. 이는 시계열 데이터가 시간에 따라 변화하지 않는 정상성을 갖고 있음을 암시한다.\n공분산이 시간 \\(t\\) 에 의존하지 않는다는 것은, 어느 시점에서 시작하든 동일한 시차 \\(k\\) 에 대한 공분산 값이 동일한 패턴을 보여야 한다는 것이다. 즉, 공분산 값의 감소 패턴은 특정 시점 \\(t\\) 에 국한되지 않고, 모든 시점 \\(t\\) 에 대해 동일하게 나타난다는 뜻이다.\n\n그래서, 시차 \\(k\\) 에 따라 공분산이 감소하는 것은, 특정한 시점에서만 나타나는 현상이 아니라, 모든 시점에서 동일하게 나타나는 패턴이라는 것이다. 이 때문에 공분산이 시간 \\(t\\) 에 의존하지 않는다고 할 수 있다.\n그래프에서 시점 \\(t\\) 가 달라져도 동일한 시차 \\(k\\) 에 대해 공분산이 같은 감소 패턴을 보인다는 점에서, 공분산이 시간 자체에는 의존하지 않고 시차에만 의존한다는 사실을 확인할 수 있다.\n특정 시점 \\(t\\) 에서만 계산한 것이 아니라, 모든 시점 \\(t\\) 를 반영하여 계산된 공분산의 평균값이다.\n\n\n\nCode\n\n# 시점 t 선택\ntime_points = [0, 10, 20, 30, 40, 50, 60, 70]\nmax_lag = 15  # 최대 시차를 줄여 데이터 길이 문제 해결\n\n# 각 시점 t에서 공분산 계산\ncovariances_per_t = {}\nfor t in time_points:\n    covariances_per_t[t] = [np.cov(y[t:t+max_lag], y[t+k:t+k+max_lag])[0, 1] for k in range(1, max_lag + 1)]\n\n# 시각화\nplt.figure(figsize=(12, 6))\n\nfor t in time_points:\n    plt.plot(range(1, max_lag + 1), covariances_per_t[t], label=f't={t}')\n\n\n\n\n\n\n\n\n\n\n위의 그래프는 여러 시점 \\(t\\) 에서 시차 \\(k\\) 에 따른 공분산 값을 보여준다.\n서로 다른 시점 \\(t\\) 에 대해서도 시차 \\(k\\) 에 따른 공분산 값은 거의 동일한 패턴을 보인다. 이는 공분산이 시간 \\(t\\) 에 의존하지 않고 시차 \\(k\\) 에만 의존한다는 것을 의미한다."
  },
  {
    "objectID": "docs/blog/posts/Governance/basis.html",
    "href": "docs/blog/posts/Governance/basis.html",
    "title": "Data Governance Study - Basic",
    "section": "",
    "text": "조직의 데이터 관리를 위한 핵심적인 프레임워크\n\n프레임워크란?\n\n특정 문제나 도메인에 대한 구조화된 접근 방식을 제공하는 개념적인 구조\n문제 해결을 위한 가이드라인, 원칙, 모범 사례를 제시\n\n\n조직 전체의 데이터 관리 전략과 실행 방법을 다룬다. 즉, 개념적, 전략적 접근 방식\n최종적으로는 조직의 정책, 프로세스, 문화 등에 반영되어야 한다.\n\n\n\n\n데이터 거버넌스는 조직 내에서 데이터의 가용성, 유용성, 무결성, 보안을 보장하기 위한 정책, 절차, 표준을 수립하고 실행하는 과정이다.\n이는 데이터를 비즈니스 자산으로 관리하고 활용하는 체계적인 접근 방식이다.\n데이터 거버넌스 구조\n\n\n\n\nData Governance Structure\n\n\n\n\n\n\n데이터 품질 향상\n데이터 보안 및 규정 준수 보장\n의사결정 프로세스 개선\n운영 효율성 및 생산성 증대\n데이터 기반 혁신 촉진\n\n\n\n\n거버넌스는 기술 구현 목적이 아니라 비즈니스 전략의 핵심 요소로 자리 잡아야 한다.\n\n데이터 품질 향상: 고품질 데이터는 정확한 분석과 신속한 비즈니스 의사결정의 기반이 된다.\n\n잘못된 데이터로 인한 비용 감소\n고객 만족도 증가\n운영 효율성 개선\n\n규정 준수 및 리스크 관리: 데이터 관련 법규(예: GDPR, CCPA)를 준수해야 함\n\n법적 제재 및 벌금 회피\n\n기업 평판 보호\n\n고객 신뢰 증진\n\n운영 및 개발 효율성 증가: 체계적인 데이터 관리는 업무 프로세스를 최적화\n\n개발 생산성 형상: 표준용어와 구조 정보를 관리하여 데이터 이해도 증가\n중복 작업 감소\n데이터 검색 및 활용 시간 단축\n부서간 협업 증진\n유지보수 효율성 향상\n평균 30~60% 이상의 비용, 공수, 기간 효율 제고\n비즈니스 목표 달성 및 노동 생산성 향상에도 기여함\n\n데이터 보안 강화: 민감한 데이터를 보호하고 무단 접근을 방지해야 함\n\n데이터 유출로 인한 손실 예방\n고객 및 파트너 신뢰 유지\n지적 재산권 보호\n\n비즈니스 인텔리전스 및 분석 개선: 일관되고 신뢰할 수 있는 데이터는 더 나은 분석을 가능하게 함\n\n시장 트렌드 신속 파악\n고객 인사이트 향상\n예측 분석의 정확도 증가\n\n데이터 자산의 가치 극대화\n\n데이터를 전략적 자산으로 관리\n데이터 기반 신규 비즈니스 모델 창출\n데이터 monetization 기회 발굴\n\n데이터 판매: 수집한 데이터를 다른 기업이나 연구기관에 직접 판매\n데이터 접근권 판매: API나 구독 모델을 통해 데이터 접근권을 제공\n제품/서비스 개선: 데이터를 활용해 기존 제품이나 서비스를 개선\n맞춤형 마케팅: 고객 데이터를 활용한 타겟 마케팅으로 매출 증대\n운영 효율성 개선: 내부 데이터 분석을 통한 비용 절감\n\n\n비용 절감: 효율적인 데이터 관리는 여러 영역에서 비용을 절감\n\n데이터 저장 및 관리 비용 최적화\n데이터 관련 오류 수정 비용 감소\n중복 시스템 및 프로세스 제거\n\n\n\n\n\n\n데이터 구조\n\n데이터의 조직화, 저장, 관리 방식을 정의하는 아키텍처와 모델링\n데이터 구조 설계 및 변경에 따른 표준 프로세스를 정의하고 구조 변경시 승인 절차를 통한 일관성 유지\n\n데이터 표준\n\n데이터의 정확성, 일관성, 신뢰성을 유지하기 위한 규칙과 가이드라인\n데이터의 품질을 향상시키고 데이터 활용 효율성을 높임\n\n데이터 정책\n\n데이터 보안 및 프라이버시 정책\n데이터 생명주기 관리 정책\n\n데이터 흐름도와 데이터 계보(lineage) 관리를 통해 데이터의 전체 생명주기를 명확히 파악\n\n\n데이터 품질 기준\n\n데이터의 정확성, 완전성, 일관성, 시의성 등을 관리\n데이터 분석 및 의사결정의 신뢰성을 좌우하는 핵심 요소\n\n조직 및 역할\n\n데이터 거버넌스 위원회\n데이터 소유자 (Data Owner)\n데이터 관리자 (Data Steward)\n데이터 사용자\n\n프로세스\n\n데이터 품질 관리 프로세스\n메타데이터 관리 프로세스\n데이터 접근 및 공유 프로세스\n\n기술\n\n데이터 표준 단어 사전\n데이터 표준 용어 사전\n데이터 카탈로그\n데이터 품질 도구\n메타데이터 관리 도구\nDBMS 사용\n\n주요 원칙\n\n책임성: 데이터에 대한 명확한 소유권과 책임을 정의\n투명성: 데이터 관련 프로세스와 결정을 투명하게 관리\n무결성: 데이터의 정확성과 일관성을 유지\n보안: 데이터를 안전하게 보호하고 적절한 접근 제어를 실시\n규정 준수: 관련 법규와 업계 표준을 준수\n가용성: 필요한 사람이 필요한 시점에 데이터에 접근할 수 있도록 함\n효율성: 데이터 관리 프로세스를 최적화하여 비용 효율성을 높임\n\n구현 단계\n\n현황 평가: 현재의 데이터 관리 실태를 분석\n전략 수립: 조직의 목표에 맞는 데이터 거버넌스 전략을 수립\n정책 및 표준 개발: 필요한 정책과 표준을 개발\n조직 구성: 데이터 거버넌스를 위한 조직 구조를 설계하고 역할을 할당\n프로세스 구현: 데이터 관리 프로세스를 설계하고 구현\n기술 도입: 필요한 데이터 관리 도구를 선택하고 도입\n교육 및 변화 관리: 조직 구성원들에게 필요한 교육을 제공하고 변화를 관리\n모니터링 및 개선: 지속적으로 성과를 모니터링하고 개선\n\n도전 과제\n\n조직 문화 변화의 어려움\n다양한 이해관계자 간의 조정\n지속적인 투자와 관심 유지\n레거시 시스템과의 통합\n\n현실적으로는 기존의 레거시 시스템은 유지하고 차세대 시스템에 거버넌스 정책을 적용\n레거시까지 거버넌즈 정책을 적용할 경우 실무자들의 거센 반발이 있음\n절충안으로 mapping table을 만들어 기존 레거시와 정책이 적용된 표준화 테이블을 연결\n\n\n\n\n\n\n\n의사결정 품질 저하\n\n부정확하거나 불완전한 데이터로 인해 잘못된 결정을 내릴 수 있다.\n시의적절한 데이터 접근이 어려워 기회를 놓칠 수 있다.\n\n운영 비효율성\n\n중복되거나 일관성 없는 데이터로 인해 작업 시간이 증가\n부서간 데이터 공유와 협업이 어려워진다.\n시간이 지남에 따라 비표준화된 데이터는 계속 누적되어 악순환에 빠짐\n\n규정 준수 리스크 증가\n\n데이터 관련 법규(예: GDPR, CCPA) 위반 가능성이 높아진다.\n감사 대응이 어려워지고, 이로 인한 법적 제재나 벌금 위험이 증가\n\n신뢰 상실\n\n데이터 구조와 정의가 불명확하여 데이터의 신뢰성이 떨어짐\n부정확한 고객 데이터로 인해 서비스 품질이 저하될 수 있다.\n데이터의 내용과 형식이 불분명하여 데이터 해석이 어렵습니다.\n개인정보 유출 위험이 증가하여 고객 신뢰를 잃을 수 있다.\n\n재무적 손실\n\n잘못된 데이터로 인한 전략적 실패로 재무적 손실이 발생할 수 있다.\n데이터 오류 수정에 많은 비용과 시간이 소요\n\n일관성 없는 명명 규칙으로 인해 데이터 이해와 통합이 어려워짐\n\n즉, 높은 의사소통 비용과 생산성 저하\n\n이 컬럼에 무슨 데이터가 들어가 있을까\n내가 필요한 데이터가 어디에 있을까\n내가 보고 있는 컬럼이 진짜 있는 컬럼일까\n데이터 흐름 관계는 어떻게 되어 있을까\n데이터를 저장시킬 컬럼명은 어떻게 지어야 할까\n\n\n경쟁력 약화\n\n낮은 데이터 품질로 의사결정의 정확도와 신속도가 저하됨\n데이터 기반 혁신이 어려워져 시장에서 뒤처질 수 있다.\n고객 인사이트 부족으로 시장 변화에 대응하기 어려움\n\nIT 시스템 복잡성 증가\n\n일관성 없는 데이터 구조로 인해 시스템 통합이 어려워진다.\n레거시 시스템 유지 비용이 증가\n\n데이터 보안 취약성\n\n데이터 접근 통제가 제대로 이루어지지 않아 보안 위험이 증가\n중요 데이터의 위치나 중요도를 파악하기 어려워 적절한 보호가 어렵다.\n\n분석 및 AI/ML 프로젝트 실패\n\n데이터 통합 실패 및 부족으로 모델링 불가\n낮은 품질의 데이터로 인해 분석 결과의 신뢰성이 떨어진다.\nAI/ML 모델의 성능이 저하되거나 편향된 결과를 도출할 수 있다.\n\n조직 문화 악화\n\n데이터에 대한 불신으로 인해 데이터 기반 문화 형성이 어렵다.\n부서간 데이터 사일로로 인해 협업이 저해\n\n비즈니스 기회 상실\n\n데이터의 전략적 가치를 활용하지 못해 새로운 비즈니스 모델 개발이 어렵다.\n데이터 monetization 기회를 놓친다.\n\n리소스 낭비\n\n중복된 데이터 저장 및 관리로 인해 불필요한 비용이 발생\n데이터 검색과 정제에 많은 시간을 소비.\n\n데이터의 위치와 저장 방식이 일관되지 않아 필요한 데이터를 찾기 어렵다\n\n\n데이터 계보 추적 불가\n\n데이터의 생성, 이동, 변환 과정이 불분명하여 데이터 계보 추적이 어렵다.\n\n\n\n\n\n\n마스 클라이밋 오비터 우주선\n\n미션: 화성의 기후와 대기를 관찰하기 위해 설계된 우주선\n특징: NASA와 록히드 마틴이 협력하여 우주선 제작\n\n실패 현상: 화성 대기권 진입으로 소멸 됨\n실패 원인: 협력 조직의 다른 단위 사용\n\nNASA : 뉴턴 단위로 계산\n록히드마틴 : 파운드 force 단위 사용\n\n결과\n\n3억2천만 달러 손실\n\n영향\n\n데이터 표준화 절차 재검토 촉발"
  },
  {
    "objectID": "docs/blog/posts/Governance/basis.html#데이터-거버넌스-기초",
    "href": "docs/blog/posts/Governance/basis.html#데이터-거버넌스-기초",
    "title": "Data Governance Study - Basic",
    "section": "",
    "text": "조직의 데이터 관리를 위한 핵심적인 프레임워크\n\n프레임워크란?\n\n특정 문제나 도메인에 대한 구조화된 접근 방식을 제공하는 개념적인 구조\n문제 해결을 위한 가이드라인, 원칙, 모범 사례를 제시\n\n\n조직 전체의 데이터 관리 전략과 실행 방법을 다룬다. 즉, 개념적, 전략적 접근 방식\n최종적으로는 조직의 정책, 프로세스, 문화 등에 반영되어야 한다.\n\n\n\n\n데이터 거버넌스는 조직 내에서 데이터의 가용성, 유용성, 무결성, 보안을 보장하기 위한 정책, 절차, 표준을 수립하고 실행하는 과정이다.\n이는 데이터를 비즈니스 자산으로 관리하고 활용하는 체계적인 접근 방식이다.\n데이터 거버넌스 구조\n\n\n\n\nData Governance Structure\n\n\n\n\n\n\n데이터 품질 향상\n데이터 보안 및 규정 준수 보장\n의사결정 프로세스 개선\n운영 효율성 및 생산성 증대\n데이터 기반 혁신 촉진\n\n\n\n\n거버넌스는 기술 구현 목적이 아니라 비즈니스 전략의 핵심 요소로 자리 잡아야 한다.\n\n데이터 품질 향상: 고품질 데이터는 정확한 분석과 신속한 비즈니스 의사결정의 기반이 된다.\n\n잘못된 데이터로 인한 비용 감소\n고객 만족도 증가\n운영 효율성 개선\n\n규정 준수 및 리스크 관리: 데이터 관련 법규(예: GDPR, CCPA)를 준수해야 함\n\n법적 제재 및 벌금 회피\n\n기업 평판 보호\n\n고객 신뢰 증진\n\n운영 및 개발 효율성 증가: 체계적인 데이터 관리는 업무 프로세스를 최적화\n\n개발 생산성 형상: 표준용어와 구조 정보를 관리하여 데이터 이해도 증가\n중복 작업 감소\n데이터 검색 및 활용 시간 단축\n부서간 협업 증진\n유지보수 효율성 향상\n평균 30~60% 이상의 비용, 공수, 기간 효율 제고\n비즈니스 목표 달성 및 노동 생산성 향상에도 기여함\n\n데이터 보안 강화: 민감한 데이터를 보호하고 무단 접근을 방지해야 함\n\n데이터 유출로 인한 손실 예방\n고객 및 파트너 신뢰 유지\n지적 재산권 보호\n\n비즈니스 인텔리전스 및 분석 개선: 일관되고 신뢰할 수 있는 데이터는 더 나은 분석을 가능하게 함\n\n시장 트렌드 신속 파악\n고객 인사이트 향상\n예측 분석의 정확도 증가\n\n데이터 자산의 가치 극대화\n\n데이터를 전략적 자산으로 관리\n데이터 기반 신규 비즈니스 모델 창출\n데이터 monetization 기회 발굴\n\n데이터 판매: 수집한 데이터를 다른 기업이나 연구기관에 직접 판매\n데이터 접근권 판매: API나 구독 모델을 통해 데이터 접근권을 제공\n제품/서비스 개선: 데이터를 활용해 기존 제품이나 서비스를 개선\n맞춤형 마케팅: 고객 데이터를 활용한 타겟 마케팅으로 매출 증대\n운영 효율성 개선: 내부 데이터 분석을 통한 비용 절감\n\n\n비용 절감: 효율적인 데이터 관리는 여러 영역에서 비용을 절감\n\n데이터 저장 및 관리 비용 최적화\n데이터 관련 오류 수정 비용 감소\n중복 시스템 및 프로세스 제거\n\n\n\n\n\n\n데이터 구조\n\n데이터의 조직화, 저장, 관리 방식을 정의하는 아키텍처와 모델링\n데이터 구조 설계 및 변경에 따른 표준 프로세스를 정의하고 구조 변경시 승인 절차를 통한 일관성 유지\n\n데이터 표준\n\n데이터의 정확성, 일관성, 신뢰성을 유지하기 위한 규칙과 가이드라인\n데이터의 품질을 향상시키고 데이터 활용 효율성을 높임\n\n데이터 정책\n\n데이터 보안 및 프라이버시 정책\n데이터 생명주기 관리 정책\n\n데이터 흐름도와 데이터 계보(lineage) 관리를 통해 데이터의 전체 생명주기를 명확히 파악\n\n\n데이터 품질 기준\n\n데이터의 정확성, 완전성, 일관성, 시의성 등을 관리\n데이터 분석 및 의사결정의 신뢰성을 좌우하는 핵심 요소\n\n조직 및 역할\n\n데이터 거버넌스 위원회\n데이터 소유자 (Data Owner)\n데이터 관리자 (Data Steward)\n데이터 사용자\n\n프로세스\n\n데이터 품질 관리 프로세스\n메타데이터 관리 프로세스\n데이터 접근 및 공유 프로세스\n\n기술\n\n데이터 표준 단어 사전\n데이터 표준 용어 사전\n데이터 카탈로그\n데이터 품질 도구\n메타데이터 관리 도구\nDBMS 사용\n\n주요 원칙\n\n책임성: 데이터에 대한 명확한 소유권과 책임을 정의\n투명성: 데이터 관련 프로세스와 결정을 투명하게 관리\n무결성: 데이터의 정확성과 일관성을 유지\n보안: 데이터를 안전하게 보호하고 적절한 접근 제어를 실시\n규정 준수: 관련 법규와 업계 표준을 준수\n가용성: 필요한 사람이 필요한 시점에 데이터에 접근할 수 있도록 함\n효율성: 데이터 관리 프로세스를 최적화하여 비용 효율성을 높임\n\n구현 단계\n\n현황 평가: 현재의 데이터 관리 실태를 분석\n전략 수립: 조직의 목표에 맞는 데이터 거버넌스 전략을 수립\n정책 및 표준 개발: 필요한 정책과 표준을 개발\n조직 구성: 데이터 거버넌스를 위한 조직 구조를 설계하고 역할을 할당\n프로세스 구현: 데이터 관리 프로세스를 설계하고 구현\n기술 도입: 필요한 데이터 관리 도구를 선택하고 도입\n교육 및 변화 관리: 조직 구성원들에게 필요한 교육을 제공하고 변화를 관리\n모니터링 및 개선: 지속적으로 성과를 모니터링하고 개선\n\n도전 과제\n\n조직 문화 변화의 어려움\n다양한 이해관계자 간의 조정\n지속적인 투자와 관심 유지\n레거시 시스템과의 통합\n\n현실적으로는 기존의 레거시 시스템은 유지하고 차세대 시스템에 거버넌스 정책을 적용\n레거시까지 거버넌즈 정책을 적용할 경우 실무자들의 거센 반발이 있음\n절충안으로 mapping table을 만들어 기존 레거시와 정책이 적용된 표준화 테이블을 연결\n\n\n\n\n\n\n\n의사결정 품질 저하\n\n부정확하거나 불완전한 데이터로 인해 잘못된 결정을 내릴 수 있다.\n시의적절한 데이터 접근이 어려워 기회를 놓칠 수 있다.\n\n운영 비효율성\n\n중복되거나 일관성 없는 데이터로 인해 작업 시간이 증가\n부서간 데이터 공유와 협업이 어려워진다.\n시간이 지남에 따라 비표준화된 데이터는 계속 누적되어 악순환에 빠짐\n\n규정 준수 리스크 증가\n\n데이터 관련 법규(예: GDPR, CCPA) 위반 가능성이 높아진다.\n감사 대응이 어려워지고, 이로 인한 법적 제재나 벌금 위험이 증가\n\n신뢰 상실\n\n데이터 구조와 정의가 불명확하여 데이터의 신뢰성이 떨어짐\n부정확한 고객 데이터로 인해 서비스 품질이 저하될 수 있다.\n데이터의 내용과 형식이 불분명하여 데이터 해석이 어렵습니다.\n개인정보 유출 위험이 증가하여 고객 신뢰를 잃을 수 있다.\n\n재무적 손실\n\n잘못된 데이터로 인한 전략적 실패로 재무적 손실이 발생할 수 있다.\n데이터 오류 수정에 많은 비용과 시간이 소요\n\n일관성 없는 명명 규칙으로 인해 데이터 이해와 통합이 어려워짐\n\n즉, 높은 의사소통 비용과 생산성 저하\n\n이 컬럼에 무슨 데이터가 들어가 있을까\n내가 필요한 데이터가 어디에 있을까\n내가 보고 있는 컬럼이 진짜 있는 컬럼일까\n데이터 흐름 관계는 어떻게 되어 있을까\n데이터를 저장시킬 컬럼명은 어떻게 지어야 할까\n\n\n경쟁력 약화\n\n낮은 데이터 품질로 의사결정의 정확도와 신속도가 저하됨\n데이터 기반 혁신이 어려워져 시장에서 뒤처질 수 있다.\n고객 인사이트 부족으로 시장 변화에 대응하기 어려움\n\nIT 시스템 복잡성 증가\n\n일관성 없는 데이터 구조로 인해 시스템 통합이 어려워진다.\n레거시 시스템 유지 비용이 증가\n\n데이터 보안 취약성\n\n데이터 접근 통제가 제대로 이루어지지 않아 보안 위험이 증가\n중요 데이터의 위치나 중요도를 파악하기 어려워 적절한 보호가 어렵다.\n\n분석 및 AI/ML 프로젝트 실패\n\n데이터 통합 실패 및 부족으로 모델링 불가\n낮은 품질의 데이터로 인해 분석 결과의 신뢰성이 떨어진다.\nAI/ML 모델의 성능이 저하되거나 편향된 결과를 도출할 수 있다.\n\n조직 문화 악화\n\n데이터에 대한 불신으로 인해 데이터 기반 문화 형성이 어렵다.\n부서간 데이터 사일로로 인해 협업이 저해\n\n비즈니스 기회 상실\n\n데이터의 전략적 가치를 활용하지 못해 새로운 비즈니스 모델 개발이 어렵다.\n데이터 monetization 기회를 놓친다.\n\n리소스 낭비\n\n중복된 데이터 저장 및 관리로 인해 불필요한 비용이 발생\n데이터 검색과 정제에 많은 시간을 소비.\n\n데이터의 위치와 저장 방식이 일관되지 않아 필요한 데이터를 찾기 어렵다\n\n\n데이터 계보 추적 불가\n\n데이터의 생성, 이동, 변환 과정이 불분명하여 데이터 계보 추적이 어렵다.\n\n\n\n\n\n\n마스 클라이밋 오비터 우주선\n\n미션: 화성의 기후와 대기를 관찰하기 위해 설계된 우주선\n특징: NASA와 록히드 마틴이 협력하여 우주선 제작\n\n실패 현상: 화성 대기권 진입으로 소멸 됨\n실패 원인: 협력 조직의 다른 단위 사용\n\nNASA : 뉴턴 단위로 계산\n록히드마틴 : 파운드 force 단위 사용\n\n결과\n\n3억2천만 달러 손실\n\n영향\n\n데이터 표준화 절차 재검토 촉발"
  },
  {
    "objectID": "docs/blog/posts/Governance/list.html",
    "href": "docs/blog/posts/Governance/list.html",
    "title": "Data Governance Study",
    "section": "",
    "text": "DAMA-DMBOK\n\n데이터 관리 지식 체계 개요\n11개 지식 영역 이해\n\nIBM Data Governance Council Maturity Model\n\n11개 범주 및 5단계 성숙도 모델 이해\n\n기타 프레임워크\n\nCOBIT (Control Objectives for Information and Related Technologies)\nISO/IEC 38500 IT Governance Standard\n\n\n\n\n\n\n준비 및 계획\n\n현황 분석 및 요구사항 정의\n이해관계자 식별 및 참여\n비전 및 목표 설정\n\n조직 구성\n\n데이터 거버넌스 위원회 구성\n데이터 스튜어드 지정\n역할 및 책임 정의\n\n정책 및 표준 수립\n\n데이터 품질 정책\n데이터 보안 및 프라이버시 정책\n데이터 아키텍처 표준\n\n프로세스 설계 및 구현\n\n데이터 생명주기 관리 프로세스\n메타데이터 관리 프로세스\n데이터 품질 관리 프로세스\n\n기술 도입\n\n데이터 카탈로그 도구\n데이터 품질 관리 도구\n메타데이터 관리 도구\n데이터 거버넌스를 위한 기술지원 솔루션\n\nSnowflake\nDatabricks\nPurview\nEncore\n\n\n모니터링 및 개선\n\n성과 지표(KPI) 설정 및 측정\n지속적인 개선 활동\n\n\n\n\n\n\n데이터 품질 관리\n\n데이터 프로파일링\n데이터 클렌징\n데이터 품질 모니터링\n\n메타데이터 관리\n\n비즈니스 메타데이터\n기술 메타데이터\n운영 메타데이터\n\n마스터 데이터 관리 (MDM)\n\n마스터 데이터 식별\n마스터 데이터 통합 및 동기화\n\n데이터 보안 및 프라이버시\n\n데이터 분류\n접근 제어\n데이터 암호화\n\n데이터 아키텍처 관리\n\n데이터 모델링\n데이터 플로우 관리\n\n\n\n\n\n\n데이터 카탈로그 도구\n\nCollibra\nAlation\nIBM Watson Knowledge Catalog\n\n데이터 품질 관리 도구\n\nInformatica Data Quality\nTalend Data Quality\nIBM InfoSphere Information Server for Data Quality\n\n메타데이터 관리 도구\n\nASG Enterprise Data Intelligence\nAdaptive Metadata Manager\nerwin Data Intelligence\n\n\n\n\n\n\n데이터 중심 문화 구축\n\n데이터 리터러시 향상\n데이터 윤리 교육\n\n조직 변화 관리\n\n커뮤니케이션 전략\n교육 및 훈련 프로그램 ### 규제 및 컴플라이언스\n\n주요 데이터 관련 규제\n\nGDPR (General Data Protection Regulation)\nCCPA (California Consumer Privacy Act)\n국내 개인정보보호법\n\n컴플라이언스 관리\n\n규제 요구사항 매핑\n컴플라이언스 모니터링 및 보고\n\n\n\n\n\n\n핵심 성과 지표 (KPI)\n\n데이터 품질 개선율\n데이터 관련 의사결정 시간 단축\n데이터 보안 사고 감소율\n\nROI 분석\n\n비용 절감 효과\n수익 증대 효과\n리스크 감소 효과\n\n\n\n\n\n\nAI/ML을 활용한 데이터 거버넌스\n클라우드 환경에서의 데이터 거버넌스\n데이터 윤리 및 책임 있는 AI"
  },
  {
    "objectID": "docs/blog/posts/Governance/list.html#데이터-거버넌스-지식-및-업무-절차-프레임워크",
    "href": "docs/blog/posts/Governance/list.html#데이터-거버넌스-지식-및-업무-절차-프레임워크",
    "title": "Data Governance Study",
    "section": "",
    "text": "데이터 거버넌스 기초\n\n정의 및 목적\n\n데이터 거버넌스의 정의\n데이터 거버넌스의 중요성과 비즈니스 가치\n\n\n핵심 원칙\n\n데이터 품질\n데이터 보안 및 프라이버시\n데이터 접근성 및 공유\n데이터 일관성 및 표준화\n\n주요 구성 요소\n\n정책 및 표준\n프로세스 및 절차\n역할 및 책임\n기술 및 도구\n\n데이터 거버넌스 프레임워크\n\nDAMA-DMBOK\n\n데이터 관리 지식 체계 개요\n11개 지식 영역 이해\n\nIBM Data Governance Council Maturity Model\n\n11개 범주 및 5단계 성숙도 모델 이해\n\n기타 프레임워크\n\nCOBIT (Control Objectives for Information and Related Technologies)\nISO/IEC 38500 IT Governance Standard\n\n\n데이터 거버넌스를 위한 기술지원 솔루션\n\nSnowflake\nDatabricks\nPurview\nEncore\n\n데이터 거버넌스 구현 단계\n\n준비 및 계획\n현황 분석 및 요구사항 정의\n이해관계자 식별 및 참여\n비전 및 목표 설정\n\n3.2 조직 구성\n\n데이터 거버넌스 위원회 구성\n데이터 스튜어드 지정\n역할 및 책임 정의\n\n3.3 정책 및 표준 수립\n\n데이터 품질 정책\n데이터 보안 및 프라이버시 정책\n데이터 아키텍처 표준\n\n3.4 프로세스 설계 및 구현\n\n데이터 생명주기 관리 프로세스\n메타데이터 관리 프로세스\n데이터 품질 관리 프로세스\n\n3.5 기술 도입\n\n데이터 카탈로그 도구\n데이터 품질 관리 도구\n메타데이터 관리 도구\n\n3.6 모니터링 및 개선\n\n성과 지표(KPI) 설정 및 측정\n지속적인 개선 활동\n\n\n데이터 거버넌스 주요 업무 영역\n\n4.1 데이터 품질 관리\n\n데이터 프로파일링\n데이터 클렌징\n데이터 품질 모니터링\n\n4.2 메타데이터 관리\n\n비즈니스 메타데이터\n기술 메타데이터\n운영 메타데이터\n\n4.3 마스터 데이터 관리 (MDM)\n\n마스터 데이터 식별\n마스터 데이터 통합 및 동기화\n\n4.4 데이터 보안 및 프라이버시\n\n데이터 분류\n접근 제어\n데이터 암호화\n\n4.5 데이터 아키텍처 관리\n\n데이터 모델링\n데이터 플로우 관리\n\n\n데이터 거버넌스 도구 및 기술\n\n5.1 데이터 카탈로그 도구\n\nCollibra\nAlation\nIBM Watson Knowledge Catalog\n\n5.2 데이터 품질 관리 도구\n\nInformatica Data Quality\nTalend Data Quality\nIBM InfoSphere Information Server for Data Quality\n\n5.3 메타데이터 관리 도구\n\nASG Enterprise Data Intelligence\nAdaptive Metadata Manager\nerwin Data Intelligence\n\n\n변화 관리 및 문화 조성\n\n6.1 데이터 중심 문화 구축\n\n데이터 리터러시 향상\n데이터 윤리 교육\n\n6.2 조직 변화 관리\n\n커뮤니케이션 전략\n교육 및 훈련 프로그램\n\n\n규제 및 컴플라이언스\n\n7.1 주요 데이터 관련 규제\n\nGDPR (General Data Protection Regulation)\nCCPA (California Consumer Privacy Act)\n국내 개인정보보호법\n\n7.2 컴플라이언스 관리\n\n규제 요구사항 매핑\n컴플라이언스 모니터링 및 보고\n\n\n데이터 거버넌스 성과 측정\n\n8.1 핵심 성과 지표 (KPI)\n\n데이터 품질 개선율\n데이터 관련 의사결정 시간 단축\n데이터 보안 사고 감소율\n\n8.2 ROI 분석\n\n비용 절감 효과\n수익 증대 효과\n리스크 감소 효과\n\n\n향후 트렌드 및 발전 방향\n\n9.1 AI/ML을 활용한 데이터 거버넌스\n9.2 클라우드 환경에서의 데이터 거버넌스\n9.3 데이터 윤리 및 책임 있는 AI"
  },
  {
    "objectID": "docs/blog/posts/time_series/basic_stationarity.html#정상성의-중요성",
    "href": "docs/blog/posts/time_series/basic_stationarity.html#정상성의-중요성",
    "title": "시계열 분석 기초 개념 - 정상성(stationarity)",
    "section": "3 정상성의 중요성",
    "text": "3 정상성의 중요성\n\n많은 시계열 모델(ARMA 포함)은 정상성을 가정한다.\n\n\n\nCode\n\nnp.random.seed(0)\nn = 100 \n\n# 정상성을 갖는 시계열: AR(1) (phi = 0.8)\nphi = 0.8\ny_stationary = np.zeros(n)\nepsilon = np.random.normal(0, 1, n)  # 백색잡음\n\nfor t in range(1, n):\n    y_stationary[t] = phi * y_stationary[t - 1] + epsilon[t]\n\n# 정상성을 갖지 않는 시계열 (예: 랜덤워크)\ny_non_stationary = np.zeros(n)\nepsilon_non_stationary = np.random.normal(0, 1, n)  # 백색잡음\n\nfor t in range(1, n):\n    y_non_stationary[t] = y_non_stationary[t - 1] + epsilon_non_stationary[t]\n\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(y_stationary, label='Stationary Time Series (AR(1))')\nplt.title('Stationary Time Series')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.grid(True)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(y_non_stationary, label='Non-Stationary Time Series (Random Walk)', color='orange')\nplt.title('Non-Stationary Time Series')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n정상성을 갖는 시계열 (Stationary Time Series):\n\n왼쪽 그래프는 AR(1) 프로세스에 의해 생성된 정상성을 갖는 시계열. 이 시계열은 평균과 분산이 시간에 따라 일정하게 유지된다. 즉, 데이터의 변동이 시간에 따라 크게 변화하지 않고 상대적으로 일정한 패턴을 보인다.\n\n정상성을 갖지 않는 시계열 (Non-Stationary Time Series):\n\n오른쪽 그래프는 랜덤 워크(Random Walk)에 의해 생성된 정상성을 갖지 않는 시계열. 이 시계열은 시간에 따라 평균이나 분산이 변하는 경향을 보이며, 데이터가 시간이 지남에 따라 특정 방향으로 이동하거나 더 넓은 범위로 퍼진다.\n\n\n정상성을 가진 데이터는 예측이 더 쉽고 신뢰할 수 있다.\n비정상 시계열은 과거 패턴이 미래에 그대로 적용되지 않을 수 있어 예측이 어렵다."
  },
  {
    "objectID": "docs/blog/posts/time_series/basic_stationarity.html#정상성-검정",
    "href": "docs/blog/posts/time_series/basic_stationarity.html#정상성-검정",
    "title": "시계열 분석 기초 개념 - 정상성(stationarity)",
    "section": "4 정상성 검정",
    "text": "4 정상성 검정\n\n시각적 검사: (정성적 방법) 시계열 플롯, ACF(자기상관함수) plot 등을 통해 대략적으로 파악\n통계적 검정: (정량적 방법) Augmented Dickey-Fuller 검정, KPSS 검정 등"
  },
  {
    "objectID": "docs/blog/posts/time_series/basic_stationarity.html#비정상-시계열의-정상화-방법",
    "href": "docs/blog/posts/time_series/basic_stationarity.html#비정상-시계열의-정상화-방법",
    "title": "시계열 분석 기초 개념 - 정상성(stationarity)",
    "section": "5 비정상 시계열의 정상화 방법",
    "text": "5 비정상 시계열의 정상화 방법\n\n차분(Differencing): 연속된 관측값의 차이를 구함\n로그 변환: 지수적 증가 추세를 선형화\n계절성 조정: 계절적 패턴 제거"
  },
  {
    "objectID": "docs/blog/posts/time_series/basic_stationarity.html#약정상성-weak-stationarity-vs-강정상성-strong-stationarity",
    "href": "docs/blog/posts/time_series/basic_stationarity.html#약정상성-weak-stationarity-vs-강정상성-strong-stationarity",
    "title": "시계열 분석 기초 개념 - 정상성(stationarity)",
    "section": "6 약정상성 (Weak Stationarity) vs 강정상성 (Strong Stationarity)",
    "text": "6 약정상성 (Weak Stationarity) vs 강정상성 (Strong Stationarity)\n\n약정상성\n\n정상성의 정의를 만족\n약 정상성은 시계열의 기본적인 통계적 특성인 평균과 분산, 공분산이 시간에 따라 변하지 않도록 요구하는데, 이는 주로 선형 모델이나 통계적 분석에서 주로 사용된다.\n\n강정상성\n\n모든 차수의 결합 확률 분포가 시간에 따라 불변\n\n\n\n실제 응용:\n\n경제 지표, 주가, 기상 데이터 등 많은 실제 시계열은 비정상적입니다.\n이러한 데이터는 적절한 변환을 통해 정상화한 후 분석합니다.\n\n\n정상성은 시계열 모델의 기본 가정이며, 데이터의 특성을 이해하고 적절한 모델을 선택하는 데 중요한 역할을 합니다."
  },
  {
    "objectID": "docs/blog/posts/Governance/architetucre.html",
    "href": "docs/blog/posts/Governance/architetucre.html",
    "title": "Data Governance Study - Data Architecture (구조 관리)",
    "section": "",
    "text": "네, Graphviz를 사용하여 앞서 보여드린 예시들을 다시 만들어보겠습니다. Graphviz는 DOT 언어를 사용하여 그래프를 정의합니다.\n\n엔터프라이즈 데이터 모델:\n\n\n\n\n\n\n\n\nG\n\n\n\n고객\n\n고객\n\n\n\n주문\n\n주문\n\n\n\n고객-&gt;주문\n\n\n\n\n\n계정\n\n계정\n\n\n\n고객-&gt;계정\n\n\n\n\n\n제품\n\n제품\n\n\n\n주문-&gt;제품\n\n\n\n\n\n배송\n\n배송\n\n\n\n주문-&gt;배송\n\n\n\n\n\n청구\n\n청구\n\n\n\n계정-&gt;청구\n\n\n\n\n\n재고\n\n재고\n\n\n\n제품-&gt;재고\n\n\n\n\n\n물류\n\n물류\n\n\n\n배송-&gt;물류\n\n\n\n\n\n\n\n\n\n\n\n데이터 흐름 아키텍처:\n\n\n\n\n\n\n\n\nG\n\n\n\n소스 시스템\n\n소스 시스템\n\n\n\n데이터 수집 레이어\n\n데이터 수집 레이어\n\n\n\n소스 시스템-&gt;데이터 수집 레이어\n\n\n\n\n\n데이터 저장소\n\n데이터 저장소\n\n\n\n데이터 수집 레이어-&gt;데이터 저장소\n\n\n\n\n\n데이터 처리 레이어\n\n데이터 처리 레이어\n\n\n\n데이터 저장소-&gt;데이터 처리 레이어\n\n\n\n\n\n데이터 서비스 레이어\n\n데이터 서비스 레이어\n\n\n\n데이터 처리 레이어-&gt;데이터 서비스 레이어\n\n\n\n\n\n비즈니스 애플리케이션\n\n비즈니스 애플리케이션\n\n\n\n데이터 서비스 레이어-&gt;비즈니스 애플리케이션\n\n\n\n\n\n\n\n\n\n\n\n기술 스택 아키텍처:\n\n\n\n\n\n\n\n\nG\n\n\n\n데이터 소스\n\n데이터 소스\n\n\n\nETL/ELT 도구\n\nETL/ELT 도구\n\n\n\n데이터 소스-&gt;ETL/ELT 도구\n\n\n\n\n\n데이터 저장소\n\n데이터 저장소\n\n\n\nETL/ELT 도구-&gt;데이터 저장소\n\n\n\n\n\n데이터 처리 엔진\n\n데이터 처리 엔진\n\n\n\n데이터 저장소-&gt;데이터 처리 엔진\n\n\n\n\n\nBI 및 분석 도구\n\nBI 및 분석 도구\n\n\n\n데이터 처리 엔진-&gt;BI 및 분석 도구\n\n\n\n\n\n데이터 카탈로그\n\n데이터 카탈로그\n\n\n\n데이터 카탈로그-&gt;데이터 저장소\n\n\n\n\n\n데이터 카탈로그-&gt;데이터 처리 엔진\n\n\n\n\n\n데이터 거버넌스 도구\n\n데이터 거버넌스 도구\n\n\n\n데이터 거버넌스 도구-&gt;데이터 카탈로그\n\n\n\n\n\n\n\n\n\n\n이 Graphviz 코드를 DOT 파일로 저장하고 Graphviz 도구를 사용하여 이미지로 렌더링할 수 있습니다. 예를 들어, 명령줄에서 다음과 같이 실행할 수 있습니다:\ndot -Tpng input.dot -o output.png\n이렇게 하면 PNG 형식의 이미지 파일이 생성됩니다.\nGraphviz는 복잡한 그래프와 다이어그램을 생성하는 데 매우 강력한 도구입니다. 노드의 모양, 색상, 레이아웃 등을 세밀하게 조정할 수 있어 더 복잡하고 세련된 다이어그램을 만들 수 있습니다. 또한 대규모 그래프를 자동으로 레이아웃하는 데 특히 유용합니다."
  },
  {
    "objectID": "docs/blog/posts/Governance/architetucre.html#data-architecure",
    "href": "docs/blog/posts/Governance/architetucre.html#data-architecure",
    "title": "Data Governance Study - Data Architecture (구조 관리)",
    "section": "",
    "text": "네, Graphviz를 사용하여 앞서 보여드린 예시들을 다시 만들어보겠습니다. Graphviz는 DOT 언어를 사용하여 그래프를 정의합니다.\n\n엔터프라이즈 데이터 모델:\n\n\n\n\n\n\n\n\nG\n\n\n\n고객\n\n고객\n\n\n\n주문\n\n주문\n\n\n\n고객-&gt;주문\n\n\n\n\n\n계정\n\n계정\n\n\n\n고객-&gt;계정\n\n\n\n\n\n제품\n\n제품\n\n\n\n주문-&gt;제품\n\n\n\n\n\n배송\n\n배송\n\n\n\n주문-&gt;배송\n\n\n\n\n\n청구\n\n청구\n\n\n\n계정-&gt;청구\n\n\n\n\n\n재고\n\n재고\n\n\n\n제품-&gt;재고\n\n\n\n\n\n물류\n\n물류\n\n\n\n배송-&gt;물류\n\n\n\n\n\n\n\n\n\n\n\n데이터 흐름 아키텍처:\n\n\n\n\n\n\n\n\nG\n\n\n\n소스 시스템\n\n소스 시스템\n\n\n\n데이터 수집 레이어\n\n데이터 수집 레이어\n\n\n\n소스 시스템-&gt;데이터 수집 레이어\n\n\n\n\n\n데이터 저장소\n\n데이터 저장소\n\n\n\n데이터 수집 레이어-&gt;데이터 저장소\n\n\n\n\n\n데이터 처리 레이어\n\n데이터 처리 레이어\n\n\n\n데이터 저장소-&gt;데이터 처리 레이어\n\n\n\n\n\n데이터 서비스 레이어\n\n데이터 서비스 레이어\n\n\n\n데이터 처리 레이어-&gt;데이터 서비스 레이어\n\n\n\n\n\n비즈니스 애플리케이션\n\n비즈니스 애플리케이션\n\n\n\n데이터 서비스 레이어-&gt;비즈니스 애플리케이션\n\n\n\n\n\n\n\n\n\n\n\n기술 스택 아키텍처:\n\n\n\n\n\n\n\n\nG\n\n\n\n데이터 소스\n\n데이터 소스\n\n\n\nETL/ELT 도구\n\nETL/ELT 도구\n\n\n\n데이터 소스-&gt;ETL/ELT 도구\n\n\n\n\n\n데이터 저장소\n\n데이터 저장소\n\n\n\nETL/ELT 도구-&gt;데이터 저장소\n\n\n\n\n\n데이터 처리 엔진\n\n데이터 처리 엔진\n\n\n\n데이터 저장소-&gt;데이터 처리 엔진\n\n\n\n\n\nBI 및 분석 도구\n\nBI 및 분석 도구\n\n\n\n데이터 처리 엔진-&gt;BI 및 분석 도구\n\n\n\n\n\n데이터 카탈로그\n\n데이터 카탈로그\n\n\n\n데이터 카탈로그-&gt;데이터 저장소\n\n\n\n\n\n데이터 카탈로그-&gt;데이터 처리 엔진\n\n\n\n\n\n데이터 거버넌스 도구\n\n데이터 거버넌스 도구\n\n\n\n데이터 거버넌스 도구-&gt;데이터 카탈로그\n\n\n\n\n\n\n\n\n\n\n이 Graphviz 코드를 DOT 파일로 저장하고 Graphviz 도구를 사용하여 이미지로 렌더링할 수 있습니다. 예를 들어, 명령줄에서 다음과 같이 실행할 수 있습니다:\ndot -Tpng input.dot -o output.png\n이렇게 하면 PNG 형식의 이미지 파일이 생성됩니다.\nGraphviz는 복잡한 그래프와 다이어그램을 생성하는 데 매우 강력한 도구입니다. 노드의 모양, 색상, 레이아웃 등을 세밀하게 조정할 수 있어 더 복잡하고 세련된 다이어그램을 만들 수 있습니다. 또한 대규모 그래프를 자동으로 레이아웃하는 데 특히 유용합니다."
  },
  {
    "objectID": "docs/blog/posts/References/index.html",
    "href": "docs/blog/posts/References/index.html",
    "title": "References",
    "section": "",
    "text": "Note\n\n\n\n\nScalars are denoted with a lower-case letter (ex a ) or a non-bolded lower-case Greek letter (ex \\(\\alpha\\) ).\nVectors are denoted using a bold-faced lower-case letter (ex \\(\\mathbf a\\)).\nMatrices are denoted using a bold-faced upper-case letter (ex \\(\\mathbf A\\), \\(\\mathbf \\phi\\)) or a bold-faced upper-case Greek letter (ex \\(\\mathbf \\Phi\\)).\nTensors are denoted using a bold-faced upper-case letter with multiple subscripts or superscripts, indicating the number of indices and the dimensions of the tensor along each axis.\n\nA second-order tensor (also known as a matrix) \\(\\mathbf A\\) with dimensions \\(n \\times m\\) can be represented as: \\(\\mathbf A_{ij}\\) where \\(i = 1,\\dots,m\\) and \\(j = 1,\\dots,n\\), which are the indices that run over the rows and columns of the matrix, respectively.\nA third-order tensor \\(T\\) with dimensions \\(n \\times m \\times p\\) can be represented as: \\(\\mathbf A_{ijk}\\) where \\(i = 1,\\dots,m\\), \\(j = 1,\\dots,n\\), which are \\(i\\), and \\(k = 1,\\dots,p\\) \\(j\\), and \\(k\\), which are the indices that run over the three dimensions of the tensor.\n\n\n\n\n\nContents\n\nEngineering\nSurveilance\n\n\n\nReference\n\nBioinformatics\n\nincoedu\n\nNGS 데이터 분석 기초편 김상수 / 숭실대학교\nCCNGS 데이터 변이 분석 기초편 김상우 / 연세대학교\n전사체 데이터 분석 이상혁 / 이화여자대학교\n\nWES 기초편 최무림 / 서울대학교\n단일세포 분석 김규태 / 아주대학교 의과대학 생리학교실\n김준일 / 숭실대학교 의생명시스템학부\n박지환 / 광주과학기술원 생명과학부\n황대희 / 서울대학교 생명과학부\n예제 데이터를 활용한 전사체 데이터 분석 김종환 / KOBIC\n예제 데이터를 활용한 단일세포 전사체 데이터 분석 김종환 / KOBIC\n후성 유전체 데이터 분석 노태영 / 포항공과대학교\n\n암 유전체 분석 주영석 / KAIST\nChIP-seq 정인경 / KAIST\n단백체 분석 황대희 / 서울대학교\n기계학습 및 딥러닝 기초 이론과 암 유전체 데이터 딥러닝 적용 실습 김권일 / 경희대학교\n롱리드 시퀀싱 데이터 분석 김준 / 서울대학교\n\n생명정보학 시작하기 안준용 / 고려대학교\n인공지능을 활용한 데이터 분석 및 신약개발 활용 김은영 / 삼진제약\n\neQTL 분석 최무림 / 서울대학교\nMicrobiome 이선재 / GIST\n이병욱 / KOBIC\n\n\nStatistics\n\nGeorge Casella & Rogeer L. Berger - Statistcal Inference, 2nd Edition\nDobson and Barnett (2008) An Introduction to Generalized Linear Model. 3rd Ed. Chapman & Hall.\nFitzmaurice, Laird and Ware (2011) Applied Longitudinal Analysis. 2nd Ed. Wiley.\nHosmer, Lemeshow and May (2008) Applied Survival Analysis. 2nd Ed. Wiley.\n슬기로운 통계생활 - https://www.youtube.com/@statisticsplaybook\n슬기로운 통계생활 - https://github.com/statisticsplaybook\nFast Campus, Coursera, Inflearn\n그 외 다수의 Youtube, and Documents from Googling\n\nMathematics\n\nJames Stewart - Calculus Early Transcedentals, 7th Eidition & any James Stewart series\nGILBERT STRANG - Introduction to Linear Algebra, 4th Edition.\n임장환 - 머신러닝, 인공지능, 컴퓨터 비전 전공자를 위한 최적화 이론\nFast Campus, Coursera, Inflearn\n8일간의 선형대수학 기초(이상준 경희대 교수)\nLinear Algebra(Prof. Gilbert Strang, MIT Open Courseware)\nComputational Linear Algebra for Coders\nImmersive linear Algebra\n3blue1brown\n그 외 다수의 Youtube, and Documents from Googling\n\nMachine Learning\n\nGareth M. James, Daniela Witten, Trevor Hastie, Robert Tibshirani - An Introduction to Statistical Learning: With Applications in R 2nd Edition\nTrevor Hastie, Robert Tibshirani, Jerome H. Friedman - The Elements of Statistical Learning 2nd Edition\nFast Campus, Coursera, Inflearn\n그 외 다수의 Youtube, and Documents from Googling\n\nDeep Learning\n\nSaito Koki - Deep Learning from Scratch 1,2,3 (밑바닥부터 시작하는 딥러닝 1,2,3)\n조준우 - 머신러닝·딥러닝에 필요한 기초 수학 with 파이썬\n조준우 - https://github.com/metamath1/noviceml\n동빈나 - https://www.youtube.com/c/dongbinna\n혁펜하임 - https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag\nFast Campus, Coursera, Inflearn\n다수의 Youtube, and Documents from Googling\n\nEngineering\n\nFast Campus, Coursera, Inflearn\n그 외 다수의 Youtube, and Documents from Googling"
  },
  {
    "objectID": "docs/blog/posts/Governance/list.html#데이터-거버넌스-프레임워크-및-업무-절차",
    "href": "docs/blog/posts/Governance/list.html#데이터-거버넌스-프레임워크-및-업무-절차",
    "title": "Data Governance Study",
    "section": "",
    "text": "DAMA-DMBOK\n\n데이터 관리 지식 체계 개요\n11개 지식 영역 이해\n\nIBM Data Governance Council Maturity Model\n\n11개 범주 및 5단계 성숙도 모델 이해\n\n기타 프레임워크\n\nCOBIT (Control Objectives for Information and Related Technologies)\nISO/IEC 38500 IT Governance Standard\n\n\n\n\n\n\n준비 및 계획\n\n현황 분석 및 요구사항 정의\n이해관계자 식별 및 참여\n비전 및 목표 설정\n\n조직 구성\n\n데이터 거버넌스 위원회 구성\n데이터 스튜어드 지정\n역할 및 책임 정의\n\n정책 및 표준 수립\n\n데이터 품질 정책\n데이터 보안 및 프라이버시 정책\n데이터 아키텍처 표준\n\n프로세스 설계 및 구현\n\n데이터 생명주기 관리 프로세스\n메타데이터 관리 프로세스\n데이터 품질 관리 프로세스\n\n기술 도입\n\n데이터 카탈로그 도구\n데이터 품질 관리 도구\n메타데이터 관리 도구\n데이터 거버넌스를 위한 기술지원 솔루션\n\nSnowflake\nDatabricks\nPurview\nEncore\n\n\n모니터링 및 개선\n\n성과 지표(KPI) 설정 및 측정\n지속적인 개선 활동\n\n\n\n\n\n\n데이터 품질 관리\n\n데이터 프로파일링\n데이터 클렌징\n데이터 품질 모니터링\n\n메타데이터 관리\n\n비즈니스 메타데이터\n기술 메타데이터\n운영 메타데이터\n\n마스터 데이터 관리 (MDM)\n\n마스터 데이터 식별\n마스터 데이터 통합 및 동기화\n\n데이터 보안 및 프라이버시\n\n데이터 분류\n접근 제어\n데이터 암호화\n\n데이터 아키텍처 관리\n\n데이터 모델링\n데이터 플로우 관리\n\n\n\n\n\n\n데이터 카탈로그 도구\n\nCollibra\nAlation\nIBM Watson Knowledge Catalog\n\n데이터 품질 관리 도구\n\nInformatica Data Quality\nTalend Data Quality\nIBM InfoSphere Information Server for Data Quality\n\n메타데이터 관리 도구\n\nASG Enterprise Data Intelligence\nAdaptive Metadata Manager\nerwin Data Intelligence\n\n\n\n\n\n\n데이터 중심 문화 구축\n\n데이터 리터러시 향상\n데이터 윤리 교육\n\n조직 변화 관리\n\n커뮤니케이션 전략\n교육 및 훈련 프로그램 ### 규제 및 컴플라이언스\n\n주요 데이터 관련 규제\n\nGDPR (General Data Protection Regulation)\nCCPA (California Consumer Privacy Act)\n국내 개인정보보호법\n\n컴플라이언스 관리\n\n규제 요구사항 매핑\n컴플라이언스 모니터링 및 보고\n\n\n\n\n\n\n핵심 성과 지표 (KPI)\n\n데이터 품질 개선율\n데이터 관련 의사결정 시간 단축\n데이터 보안 사고 감소율\n\nROI 분석\n\n비용 절감 효과\n수익 증대 효과\n리스크 감소 효과\n\n\n\n\n\n\nAI/ML을 활용한 데이터 거버넌스\n클라우드 환경에서의 데이터 거버넌스\n데이터 윤리 및 책임 있는 AI"
  },
  {
    "objectID": "docs/blog/posts/Governance/task_list.html",
    "href": "docs/blog/posts/Governance/task_list.html",
    "title": "Data Governance Study - Task Process",
    "section": "",
    "text": "DAMA-DMBOK\n\n데이터 관리 지식 체계 개요\n11개 지식 영역 이해\n\nIBM Data Governance Council Maturity Model\n\n11개 범주 및 5단계 성숙도 모델 이해\n\n기타 프레임워크\n\nCOBIT (Control Objectives for Information and Related Technologies)\nISO/IEC 38500 IT Governance Standard\n\n\n\n\n\n\n준비 및 계획\n\n현황 분석 및 요구사항 정의\n이해관계자 식별 및 참여\n비전 및 목표 설정\n\n조직 구성\n\n데이터 거버넌스 위원회 구성\n데이터 스튜어드 지정\n역할 및 책임 정의\n\n정책 및 표준 수립\n\n데이터 품질 정책\n데이터 보안 및 프라이버시 정책\n데이터 아키텍처 표준\n\n프로세스 설계 및 구현\n\n데이터 생명주기 관리 프로세스\n메타데이터 관리 프로세스\n데이터 품질 관리 프로세스\n\n기술 도입\n\n데이터 카탈로그 도구\n데이터 품질 관리 도구\n메타데이터 관리 도구\n데이터 거버넌스를 위한 기술지원 솔루션\n\nSnowflake\nDatabricks\nPurview\nEncore\n\n\n모니터링 및 개선\n\n성과 지표(KPI) 설정 및 측정\n지속적인 개선 활동\n\n\n\n\n\n\n데이터 품질 관리\n\n데이터 프로파일링\n데이터 클렌징\n데이터 품질 모니터링\n\n메타데이터 관리\n\n비즈니스 메타데이터\n기술 메타데이터\n운영 메타데이터\n\n마스터 데이터 관리 (MDM)\n\n마스터 데이터 식별\n마스터 데이터 통합 및 동기화\n\n데이터 보안 및 프라이버시\n\n데이터 분류\n접근 제어\n데이터 암호화\n\n데이터 아키텍처 관리\n\n데이터 모델링\n데이터 플로우 관리\n\n\n\n\n\n\n데이터 카탈로그 도구\n\nCollibra\nAlation\nIBM Watson Knowledge Catalog\n\n데이터 품질 관리 도구\n\nInformatica Data Quality\nTalend Data Quality\nIBM InfoSphere Information Server for Data Quality\n\n메타데이터 관리 도구\n\nASG Enterprise Data Intelligence\nAdaptive Metadata Manager\nerwin Data Intelligence\n\n\n\n\n\n\n데이터 중심 문화 구축\n\n데이터 리터러시 향상\n데이터 윤리 교육\n\n조직 변화 관리\n\n커뮤니케이션 전략\n교육 및 훈련 프로그램 ### 규제 및 컴플라이언스\n\n주요 데이터 관련 규제\n\nGDPR (General Data Protection Regulation)\nCCPA (California Consumer Privacy Act)\n국내 개인정보보호법\n\n컴플라이언스 관리\n\n규제 요구사항 매핑\n컴플라이언스 모니터링 및 보고\n\n\n\n\n\n\n핵심 성과 지표 (KPI)\n\n데이터 품질 개선율\n데이터 관련 의사결정 시간 단축\n데이터 보안 사고 감소율\n\nROI 분석\n\n비용 절감 효과\n수익 증대 효과\n리스크 감소 효과\n\n\n\n\n\n\nAI/ML을 활용한 데이터 거버넌스\n클라우드 환경에서의 데이터 거버넌스\n데이터 윤리 및 책임 있는 AI"
  },
  {
    "objectID": "docs/blog/posts/Governance/task_list.html#데이터-거버넌스-프레임워크-및-업무-절차",
    "href": "docs/blog/posts/Governance/task_list.html#데이터-거버넌스-프레임워크-및-업무-절차",
    "title": "Data Governance Study - Task Process",
    "section": "",
    "text": "DAMA-DMBOK\n\n데이터 관리 지식 체계 개요\n11개 지식 영역 이해\n\nIBM Data Governance Council Maturity Model\n\n11개 범주 및 5단계 성숙도 모델 이해\n\n기타 프레임워크\n\nCOBIT (Control Objectives for Information and Related Technologies)\nISO/IEC 38500 IT Governance Standard\n\n\n\n\n\n\n준비 및 계획\n\n현황 분석 및 요구사항 정의\n이해관계자 식별 및 참여\n비전 및 목표 설정\n\n조직 구성\n\n데이터 거버넌스 위원회 구성\n데이터 스튜어드 지정\n역할 및 책임 정의\n\n정책 및 표준 수립\n\n데이터 품질 정책\n데이터 보안 및 프라이버시 정책\n데이터 아키텍처 표준\n\n프로세스 설계 및 구현\n\n데이터 생명주기 관리 프로세스\n메타데이터 관리 프로세스\n데이터 품질 관리 프로세스\n\n기술 도입\n\n데이터 카탈로그 도구\n데이터 품질 관리 도구\n메타데이터 관리 도구\n데이터 거버넌스를 위한 기술지원 솔루션\n\nSnowflake\nDatabricks\nPurview\nEncore\n\n\n모니터링 및 개선\n\n성과 지표(KPI) 설정 및 측정\n지속적인 개선 활동\n\n\n\n\n\n\n데이터 품질 관리\n\n데이터 프로파일링\n데이터 클렌징\n데이터 품질 모니터링\n\n메타데이터 관리\n\n비즈니스 메타데이터\n기술 메타데이터\n운영 메타데이터\n\n마스터 데이터 관리 (MDM)\n\n마스터 데이터 식별\n마스터 데이터 통합 및 동기화\n\n데이터 보안 및 프라이버시\n\n데이터 분류\n접근 제어\n데이터 암호화\n\n데이터 아키텍처 관리\n\n데이터 모델링\n데이터 플로우 관리\n\n\n\n\n\n\n데이터 카탈로그 도구\n\nCollibra\nAlation\nIBM Watson Knowledge Catalog\n\n데이터 품질 관리 도구\n\nInformatica Data Quality\nTalend Data Quality\nIBM InfoSphere Information Server for Data Quality\n\n메타데이터 관리 도구\n\nASG Enterprise Data Intelligence\nAdaptive Metadata Manager\nerwin Data Intelligence\n\n\n\n\n\n\n데이터 중심 문화 구축\n\n데이터 리터러시 향상\n데이터 윤리 교육\n\n조직 변화 관리\n\n커뮤니케이션 전략\n교육 및 훈련 프로그램 ### 규제 및 컴플라이언스\n\n주요 데이터 관련 규제\n\nGDPR (General Data Protection Regulation)\nCCPA (California Consumer Privacy Act)\n국내 개인정보보호법\n\n컴플라이언스 관리\n\n규제 요구사항 매핑\n컴플라이언스 모니터링 및 보고\n\n\n\n\n\n\n핵심 성과 지표 (KPI)\n\n데이터 품질 개선율\n데이터 관련 의사결정 시간 단축\n데이터 보안 사고 감소율\n\nROI 분석\n\n비용 절감 효과\n수익 증대 효과\n리스크 감소 효과\n\n\n\n\n\n\nAI/ML을 활용한 데이터 거버넌스\n클라우드 환경에서의 데이터 거버넌스\n데이터 윤리 및 책임 있는 AI"
  },
  {
    "objectID": "docs/blog/posts/Governance/current_status.html",
    "href": "docs/blog/posts/Governance/current_status.html",
    "title": "Data Governance Study - Data Current Status Analysis",
    "section": "",
    "text": "0.1 데이터 거버넌스 구현 단계\n\n현황 분석 및 요구사항 정의\n\n분석 범위 및 목표 설정\n\n목적: 분석의 범위와 목표를 명확히 정의\n활동\n\n분석 대상 데이터 영역 정의 (예: 고객 데이터, 재무 데이터 등)\n분석의 깊이와 기간 설정\n주요 이해관계자와 목표 합의\n\nexample\n\n다른 거버넌스 프로젝트의 실폐 사례로 많이 언급되는 것이 광범위한 범위 설정으로 인해 실무진들이 수습을 못함\n본인의 경우, 자사에서 실현가능한 범위를 설정하기 위해 전사보다는 3개의 system과 연관된 데이터에 대해서 표준화를 진행\n범위 축소: 3개의 system과 연관된 데이터베이스 (전사보다는 8개의 팀이 사용하는 DB에 한정시킴)\n각 system에 속해있는 DB를 조사 및 파악하여 정보계 DB와 운영계 DB로 구분한다.\n범위 축소: 운영계 DB는 SCOPE에서 제외시키고 정보계 DB 표준화에 집중\n이 축소된 범위의 표준화를 성공하여 업무에 대한 효율성 best practice를 타부서에 제시하여 data governance에 대한 협조를 얻고 거버넌스 문화를 조성\n다른 시스템으로 점차적으로 확대함\n\n\n데이터 인벤토리 작성\n\n목적: 조직 내 모든 주요 데이터 자산을 파악\n활동\n\n데이터 소스 식별 (데이터베이스, 파일 시스템, 외부 소스 등)\n데이터 세트별 메타데이터 수집 (소유자, 용도, 볼륨, 갱신 주기 등)\n\n도구: 데이터 카탈로그 도구, 스프레드시트\nexample\n\nNAS 및 local에 있는 데이터 및 file은 우선 순위 설정에서 3순위로 지정\nDBMS에 정의된 데이터를 1순위로 지정하여 inventroy 작성\nDatabase 정보 작성 (DBMS 종류, 실무자, url, password, account 정보 등)\n\n\n데이터 프로파일링\n\n목적: 데이터의 구조, 내용, 품질을 분석\n활동\n\n데이터 구조 분석 (스키마, 데이터 타입 등)\n기술 통계 생성 (유니크 값 수, 최소/최대값, 평균, 중앙값 등)\n데이터 패턴 및 이상치 식별\n\n도구: IBM InfoSphere Information Analyzer, Informatica Data Quality, Talend Open Studio\n\n데이터 흐름 맵핑\n\n목적: 데이터의 생성, 이동, 변환, 소비 과정을 파악\n활동:\n\n데이터 흐름도 작성\n주요 데이터 통합 지점 식별\n데이터 중복 및 불일치 지점 파악\n\n도구: 데이터 흐름 다이어그램 도구 (예: Lucidchart, draw.io)\nexample\n\n본인은 회사에서 A Query Tool을 사용하여 이를 활용하고 있음\n\n\n데이터 관리 프로세스 평가\n\n목적: 현재의 데이터 관리 프로세스의 효과성을 평가\n활동:\n\n데이터 생성, 저장, 사용, 폐기 프로세스 검토\n데이터 품질 관리 프로세스 평가\n데이터 보안 및 접근 제어 프로세스 검토\n\n방법: 프로세스 워크숍, 인터뷰, 문서 리뷰\n\n규제 준수 상태 점검\n\n목적: 현재의 데이터 관리 관행이 관련 규제를 준수하는지 확인\n활동:\n\n관련 데이터 규제 식별 (예: GDPR, CCPA, 산업별 규제)\n현재의 준수 상태 평가\n주요 갭 및 리스크 식별\n\n방법: 규제 체크리스트, 컴플라이언스 감사\n\n기술 인프라 평가\n\n목적: 현재의 데이터 관리 기술 인프라를 평가\n활동:\n\n데이터 저장 및 처리 시스템 인벤토리 작성\n데이터 통합 및 품질 관리 도구 평가\n메타데이터 관리 및 데이터 보안 도구 검토\n\n방법: IT 인프라 평가 체크리스트, 기술 스택 다이어그램\n\n조직 및 거버넌스 구조 분석\n\n목적: 현재의 데이터 관련 의사결정 및 책임 구조를 파악\n활동\n\n데이터 관련 역할 및 책임 맵핑\n현재의 데이터 거버넌스 체계 (있다면) 평가\n데이터 관련 의사결정 프로세스 분석\n\n도구: 조직도, RACI 매트릭스\n\n이해관계자 인터뷰 및 설문조사\n\n목적: 다양한 이해관계자의 관점과 요구사항을 수집\n활동:\n\n주요 이해관계자 그룹 식별 (경영진, 데이터 소유자, 사용자 등)\n인터뷰 및 설문 설계\n결과 분석 및 주요 인사이트 도출\n\n방법: 구조화된 인터뷰, 온라인 설문조사\n\n분석 결과 종합 및 보고서 작성\n\n목적: 모든 분석 결과를 종합하여 현황을 포괄적으로 파악\n활동:\n\n주요 발견사항 정리\n강점, 약점, 기회, 위협 (SWOT) 분석\n개선 영역 및 우선순위 도출\n\n산출물: 종합 현황 분석 보고서, 경영진 요약 보고서\n\n\n\n\n\n\n\n\n\nG\n\n\n\n1. 분석 범위 및 목표 설정\n\n1. 분석 범위 및 목표 설정\n\n\n\n2. 데이터 인벤토리 작성\n\n2. 데이터 인벤토리 작성\n\n\n\n1. 분석 범위 및 목표 설정-&gt;2. 데이터 인벤토리 작성\n\n\n\n\n\n3. 데이터 프로파일링\n\n3. 데이터 프로파일링\n\n\n\n2. 데이터 인벤토리 작성-&gt;3. 데이터 프로파일링\n\n\n\n\n\n4. 데이터 흐름 맵핑\n\n4. 데이터 흐름 맵핑\n\n\n\n3. 데이터 프로파일링-&gt;4. 데이터 흐름 맵핑\n\n\n\n\n\n5. 데이터 관리 프로세스 평가\n\n5. 데이터 관리 프로세스 평가\n\n\n\n4. 데이터 흐름 맵핑-&gt;5. 데이터 관리 프로세스 평가\n\n\n\n\n\n6. 규제 준수 상태 점검\n\n6. 규제 준수 상태 점검\n\n\n\n5. 데이터 관리 프로세스 평가-&gt;6. 규제 준수 상태 점검\n\n\n\n\n\n7. 기술 인프라 평가\n\n7. 기술 인프라 평가\n\n\n\n6. 규제 준수 상태 점검-&gt;7. 기술 인프라 평가\n\n\n\n\n\n8. 조직 및 거버넌스 구조 분석\n\n8. 조직 및 거버넌스 구조 분석\n\n\n\n7. 기술 인프라 평가-&gt;8. 조직 및 거버넌스 구조 분석\n\n\n\n\n\n9. 이해관계자 인터뷰 및 설문조사\n\n9. 이해관계자 인터뷰 및 설문조사\n\n\n\n8. 조직 및 거버넌스 구조 분석-&gt;9. 이해관계자 인터뷰 및 설문조사\n\n\n\n\n\n10. 분석 결과 종합 및 보고서 작성\n\n10. 분석 결과 종합 및 보고서 작성\n\n\n\n9. 이해관계자 인터뷰 및 설문조사-&gt;10. 분석 결과 종합 및 보고서 작성"
  },
  {
    "objectID": "docs/blog/posts/Governance/3.architetucre.html",
    "href": "docs/blog/posts/Governance/3.architetucre.html",
    "title": "Data Governance Study - Data Architecture Management (구조 관리)",
    "section": "",
    "text": "데이터 아키텍처는 조직의 데이터 자산을 효과적으로 수집, 저장, 관리, 사용하기 위한 전체적인 구조와 계획을 의미한다.\n이는 비즈니스 요구사항, 기술적 구현, 데이터 거버넌스를 연결하는 청사진 역할을 한다.\n\n\n\n\n데이터 모델링: 데이터 엔티티, 관계, 속성 정의\n\n개념적 데이터 모델 (Conceptual Data Model)\n논리적 데이터 모델 (Logical Data Model)\n물리적 데이터 모델 (Physical Data Model)\n\n데이터 흐름\n\n데이터의 생성, 이동, 변환, 저장, 소비 과정 정의\nETL(Extract, Transform, Load) 프로세스 설계\n\n데이터 저장소\n\n데이터베이스, 데이터 레이크, 데이터 웨어하우스, 데이터 마트 등의 구조 설계\n데이터 저장 방식 (온프레미스, 클라우드, 하이브리드 등)\n\n메타데이터 관리\n\n데이터에 대한 데이터(메타데이터) 관리 방식 정의\n데이터 사전, 데이터 용어 사전, 데이터 카탈로그, 비즈니스 용어집 등 포함\n\n데이터 통합\n\n다양한 소스의 데이터를 통합하는 방식 정의\n마스터 데이터 관리(MDM) 전략 포함\n\n데이터 보안 및 프라이버시\n\n데이터 접근 제어, 암호화, 마스킹 등의 보안 정책\n개인정보 보호 규정 준수 방안\n\n데이터 품질 관리\n\n데이터 품질 측정 및 개선 프로세스 정의\n데이터 클렌징, 검증 방법론\n\n데이터 거버넌스 프레임워크\n\n데이터 관리에 대한 정책, 절차, 책임 정의\n데이터 스튜어드십 모델\n\n기술 스택\n\n데이터 관리 및 분석을 위한 기술 솔루션 선정\n\n\n\n\n\n종종 Data Architecture 한글로 데이터 구조관리라 번역되는데 Data Structgure와 혼동하지 말자.\n\n\n\n데이터 요소들을 조직화하고 저장하는 특정 방식을 의미하며 주로 프로그래밍과 데이터베이스 설계 수준에서 사용되는 개념이다.\n예: 배열, 리스트, 트리, 그래프, 테이블 등을 말하며 주로 개별 애플리케이션이나 데이터베이스 수준에 초점을 맞춘다.\n\n\n\n\n\n조직의 데이터 자산을 관리하기 위한 전체적인 구조와 모델을 의미하고 비즈니스 요구사항과 IT 전략을 연결하는 청사진 역할을 한다.\n데이터의 수집, 저장, 변환, 분배, 사용에 관한 전체적인 계획을 포함한다. (주로 그림이 많이 들어감)\n데이터 모델, 데이터 흐름, 통합 지점, 보안 정책 등을 포함합니다. (주로 그림이 많이 들어감)\n데이터 아키텍트, 비즈니스 분석가, IT 전략가가 주로 다룬다.\n범위: 조직 전체의 데이터 환경을 대상으로 하고 비즈니스 목표, 규제 요구사항, 기술 인프라를 모두 고려한다.\n예시  \n\n\n\n\n\n범위\n\nData Structure: 특정 데이터셋이나 애플리케이션에 국한됨\nData Architecture: 조직 전체의 데이터 환경을 다룸\n\n목적\n\nData Structure: 효율적인 데이터 저장 및 접근을 위한 기술적 설계\nData Architecture: 전략적 데이터 관리 및 활용을 위한 전체적인 프레임워크\n\n관점\n\nData Structure: 주로 기술적, 구현 중심적 관점\nData Architecture: 비즈니스와 기술을 연결하는 전략적 관점\n\n포함 요소\n\nData Structure: 데이터 유형, 관계, 제약조건 등\nData Architecture: 데이터 모델, 메타데이터, 데이터 흐름, 거버넌스 정책 등\n\n사용자\n\nData Structure: 주로 개발자, DBA\nData Architecture: 데이터 아키텍트, 비즈니스 분석가, IT 전략가, 경영진"
  },
  {
    "objectID": "docs/blog/posts/Governance/3.architetucre.html#data-architecure",
    "href": "docs/blog/posts/Governance/3.architetucre.html#data-architecure",
    "title": "Data Governance Study - Data Architecture Management (구조 관리)",
    "section": "",
    "text": "데이터 아키텍처는 조직의 데이터 자산을 효과적으로 수집, 저장, 관리, 사용하기 위한 전체적인 구조와 계획을 의미한다.\n이는 비즈니스 요구사항, 기술적 구현, 데이터 거버넌스를 연결하는 청사진 역할을 한다.\n\n\n\n\n데이터 모델링: 데이터 엔티티, 관계, 속성 정의\n\n개념적 데이터 모델 (Conceptual Data Model)\n논리적 데이터 모델 (Logical Data Model)\n물리적 데이터 모델 (Physical Data Model)\n\n데이터 흐름\n\n데이터의 생성, 이동, 변환, 저장, 소비 과정 정의\nETL(Extract, Transform, Load) 프로세스 설계\n\n데이터 저장소\n\n데이터베이스, 데이터 레이크, 데이터 웨어하우스, 데이터 마트 등의 구조 설계\n데이터 저장 방식 (온프레미스, 클라우드, 하이브리드 등)\n\n메타데이터 관리\n\n데이터에 대한 데이터(메타데이터) 관리 방식 정의\n데이터 사전, 데이터 용어 사전, 데이터 카탈로그, 비즈니스 용어집 등 포함\n\n데이터 통합\n\n다양한 소스의 데이터를 통합하는 방식 정의\n마스터 데이터 관리(MDM) 전략 포함\n\n데이터 보안 및 프라이버시\n\n데이터 접근 제어, 암호화, 마스킹 등의 보안 정책\n개인정보 보호 규정 준수 방안\n\n데이터 품질 관리\n\n데이터 품질 측정 및 개선 프로세스 정의\n데이터 클렌징, 검증 방법론\n\n데이터 거버넌스 프레임워크\n\n데이터 관리에 대한 정책, 절차, 책임 정의\n데이터 스튜어드십 모델\n\n기술 스택\n\n데이터 관리 및 분석을 위한 기술 솔루션 선정\n\n\n\n\n\n종종 Data Architecture 한글로 데이터 구조관리라 번역되는데 Data Structgure와 혼동하지 말자.\n\n\n\n데이터 요소들을 조직화하고 저장하는 특정 방식을 의미하며 주로 프로그래밍과 데이터베이스 설계 수준에서 사용되는 개념이다.\n예: 배열, 리스트, 트리, 그래프, 테이블 등을 말하며 주로 개별 애플리케이션이나 데이터베이스 수준에 초점을 맞춘다.\n\n\n\n\n\n조직의 데이터 자산을 관리하기 위한 전체적인 구조와 모델을 의미하고 비즈니스 요구사항과 IT 전략을 연결하는 청사진 역할을 한다.\n데이터의 수집, 저장, 변환, 분배, 사용에 관한 전체적인 계획을 포함한다. (주로 그림이 많이 들어감)\n데이터 모델, 데이터 흐름, 통합 지점, 보안 정책 등을 포함합니다. (주로 그림이 많이 들어감)\n데이터 아키텍트, 비즈니스 분석가, IT 전략가가 주로 다룬다.\n범위: 조직 전체의 데이터 환경을 대상으로 하고 비즈니스 목표, 규제 요구사항, 기술 인프라를 모두 고려한다.\n예시  \n\n\n\n\n\n범위\n\nData Structure: 특정 데이터셋이나 애플리케이션에 국한됨\nData Architecture: 조직 전체의 데이터 환경을 다룸\n\n목적\n\nData Structure: 효율적인 데이터 저장 및 접근을 위한 기술적 설계\nData Architecture: 전략적 데이터 관리 및 활용을 위한 전체적인 프레임워크\n\n관점\n\nData Structure: 주로 기술적, 구현 중심적 관점\nData Architecture: 비즈니스와 기술을 연결하는 전략적 관점\n\n포함 요소\n\nData Structure: 데이터 유형, 관계, 제약조건 등\nData Architecture: 데이터 모델, 메타데이터, 데이터 흐름, 거버넌스 정책 등\n\n사용자\n\nData Structure: 주로 개발자, DBA\nData Architecture: 데이터 아키텍트, 비즈니스 분석가, IT 전략가, 경영진"
  },
  {
    "objectID": "docs/blog/posts/Governance/2.task_list.html",
    "href": "docs/blog/posts/Governance/2.task_list.html",
    "title": "Data Governance Study - Task Process",
    "section": "",
    "text": "DAMA-DMBOK\n\n데이터 관리 지식 체계 개요\n11개 지식 영역 이해\n\nIBM Data Governance Council Maturity Model\n\n11개 범주 및 5단계 성숙도 모델 이해\n\n기타 프레임워크\n\nCOBIT (Control Objectives for Information and Related Technologies)\nISO/IEC 38500 IT Governance Standard\n\n\n\n\n\n\n준비 및 계획\n\n현황 분석 및 요구사항 정의\n이해관계자 식별 및 참여\n비전 및 목표 설정\n\n조직 구성\n\n데이터 거버넌스 위원회 구성\n데이터 스튜어드 지정\n역할 및 책임 정의\n\n정책 및 표준 수립\n\n데이터 품질 정책\n데이터 보안 및 프라이버시 정책\n데이터 아키텍처 표준\n\n프로세스 설계 및 구현\n\n데이터 생명주기 관리 프로세스\n메타데이터 관리 프로세스\n데이터 품질 관리 프로세스\n\n기술 도입\n\n데이터 카탈로그 도구\n데이터 품질 관리 도구\n메타데이터 관리 도구\n데이터 거버넌스를 위한 기술지원 솔루션\n\nSnowflake\nDatabricks\nPurview\nEncore\n\n\n모니터링 및 개선\n\n성과 지표(KPI) 설정 및 측정\n지속적인 개선 활동\n\n\n\n\n\n\n데이터 품질 관리\n\n데이터 프로파일링\n데이터 클렌징\n데이터 품질 모니터링\n\n메타데이터 관리\n\n비즈니스 메타데이터\n기술 메타데이터\n운영 메타데이터\n\n마스터 데이터 관리 (MDM)\n\n마스터 데이터 식별\n마스터 데이터 통합 및 동기화\n\n데이터 보안 및 프라이버시\n\n데이터 분류\n접근 제어\n데이터 암호화\n\n데이터 아키텍처 관리\n\n데이터 모델링\n데이터 플로우 관리\n\n\n\n\n\n\n데이터 사전: 한국에만 있는 개념으로 형태소 분석의 결과이다 (자세한 사항은 다음 블로그에…)\n데이터 용어 사전: 한국에만 있는 개념으로 데이터 사전을 기반으로 제작된다. (자세한 사항은 다음 블로그에…)\n데이터 카탈로그 도구: 데이터 사전과 데이터 용어 사전을 기반으로 만들어져야한다.\n\nCollibra\nAlation\nIBM Watson Knowledge Catalog\n\n데이터 품질 관리 도구\n\nInformatica Data Quality\nTalend Data Quality\nIBM InfoSphere Information Server for Data Quality\n\n메타데이터 관리 도구\n\nASG Enterprise Data Intelligence\nAdaptive Metadata Manager\nerwin Data Intelligence\n\n\n\n\n\n\n데이터 중심 문화 구축\n\n데이터 리터러시 향상\n데이터 윤리 교육\n\n조직 변화 관리\n\n커뮤니케이션 전략\n교육 및 훈련 프로그램\n\n\n\n\n\n\n주요 데이터 관련 규제\n\nGDPR (General Data Protection Regulation)\nCCPA (California Consumer Privacy Act)\n국내 개인정보보호법\n\n컴플라이언스 관리\n\n규제 요구사항 매핑\n컴플라이언스 모니터링 및 보고\n\n\n\n\n\n\n핵심 성과 지표 (KPI)\n\n데이터 품질 개선율\n데이터 관련 의사결정 시간 단축\n데이터 보안 사고 감소율\n\nROI 분석\n\n비용 절감 효과\n수익 증대 효과\n리스크 감소 효과\n\n\n\n\n\n\nAI/ML을 활용한 데이터 거버넌스\n클라우드 환경에서의 데이터 거버넌스\n데이터 윤리 및 책임 있는 AI"
  },
  {
    "objectID": "docs/blog/posts/Governance/2.task_list.html#데이터-거버넌스-프레임워크-및-업무-절차",
    "href": "docs/blog/posts/Governance/2.task_list.html#데이터-거버넌스-프레임워크-및-업무-절차",
    "title": "Data Governance Study - Task Process",
    "section": "",
    "text": "DAMA-DMBOK\n\n데이터 관리 지식 체계 개요\n11개 지식 영역 이해\n\nIBM Data Governance Council Maturity Model\n\n11개 범주 및 5단계 성숙도 모델 이해\n\n기타 프레임워크\n\nCOBIT (Control Objectives for Information and Related Technologies)\nISO/IEC 38500 IT Governance Standard\n\n\n\n\n\n\n준비 및 계획\n\n현황 분석 및 요구사항 정의\n이해관계자 식별 및 참여\n비전 및 목표 설정\n\n조직 구성\n\n데이터 거버넌스 위원회 구성\n데이터 스튜어드 지정\n역할 및 책임 정의\n\n정책 및 표준 수립\n\n데이터 품질 정책\n데이터 보안 및 프라이버시 정책\n데이터 아키텍처 표준\n\n프로세스 설계 및 구현\n\n데이터 생명주기 관리 프로세스\n메타데이터 관리 프로세스\n데이터 품질 관리 프로세스\n\n기술 도입\n\n데이터 카탈로그 도구\n데이터 품질 관리 도구\n메타데이터 관리 도구\n데이터 거버넌스를 위한 기술지원 솔루션\n\nSnowflake\nDatabricks\nPurview\nEncore\n\n\n모니터링 및 개선\n\n성과 지표(KPI) 설정 및 측정\n지속적인 개선 활동\n\n\n\n\n\n\n데이터 품질 관리\n\n데이터 프로파일링\n데이터 클렌징\n데이터 품질 모니터링\n\n메타데이터 관리\n\n비즈니스 메타데이터\n기술 메타데이터\n운영 메타데이터\n\n마스터 데이터 관리 (MDM)\n\n마스터 데이터 식별\n마스터 데이터 통합 및 동기화\n\n데이터 보안 및 프라이버시\n\n데이터 분류\n접근 제어\n데이터 암호화\n\n데이터 아키텍처 관리\n\n데이터 모델링\n데이터 플로우 관리\n\n\n\n\n\n\n데이터 사전: 한국에만 있는 개념으로 형태소 분석의 결과이다 (자세한 사항은 다음 블로그에…)\n데이터 용어 사전: 한국에만 있는 개념으로 데이터 사전을 기반으로 제작된다. (자세한 사항은 다음 블로그에…)\n데이터 카탈로그 도구: 데이터 사전과 데이터 용어 사전을 기반으로 만들어져야한다.\n\nCollibra\nAlation\nIBM Watson Knowledge Catalog\n\n데이터 품질 관리 도구\n\nInformatica Data Quality\nTalend Data Quality\nIBM InfoSphere Information Server for Data Quality\n\n메타데이터 관리 도구\n\nASG Enterprise Data Intelligence\nAdaptive Metadata Manager\nerwin Data Intelligence\n\n\n\n\n\n\n데이터 중심 문화 구축\n\n데이터 리터러시 향상\n데이터 윤리 교육\n\n조직 변화 관리\n\n커뮤니케이션 전략\n교육 및 훈련 프로그램\n\n\n\n\n\n\n주요 데이터 관련 규제\n\nGDPR (General Data Protection Regulation)\nCCPA (California Consumer Privacy Act)\n국내 개인정보보호법\n\n컴플라이언스 관리\n\n규제 요구사항 매핑\n컴플라이언스 모니터링 및 보고\n\n\n\n\n\n\n핵심 성과 지표 (KPI)\n\n데이터 품질 개선율\n데이터 관련 의사결정 시간 단축\n데이터 보안 사고 감소율\n\nROI 분석\n\n비용 절감 효과\n수익 증대 효과\n리스크 감소 효과\n\n\n\n\n\n\nAI/ML을 활용한 데이터 거버넌스\n클라우드 환경에서의 데이터 거버넌스\n데이터 윤리 및 책임 있는 AI"
  },
  {
    "objectID": "docs/blog/posts/Governance/1.basis.html",
    "href": "docs/blog/posts/Governance/1.basis.html",
    "title": "Data Governance Study - Basic",
    "section": "",
    "text": "조직의 데이터 관리를 위한 핵심적인 프레임워크\n\n프레임워크란?\n\n특정 문제나 도메인에 대한 구조화된 접근 방식을 제공하는 개념적인 구조\n문제 해결을 위한 가이드라인, 원칙, 모범 사례를 제시\n\n\n조직 전체의 데이터 관리 전략과 실행 방법을 다룬다. 즉, 개념적, 전략적 접근 방식\n최종적으로는 조직의 정책, 프로세스, 문화 등에 반영되어야 한다.\n\n\n\n\n데이터 거버넌스는 조직 내에서 데이터의 가용성, 유용성, 무결성, 보안을 보장하기 위한 정책, 절차, 표준을 수립하고 실행하는 과정이다.\n이는 데이터를 비즈니스 자산으로 관리하고 활용하는 체계적인 접근 방식이다.\n데이터 거버넌스 구조\n\n\n\n\nData Governance Structure\n\n\n\n\n\n\n데이터 품질 향상\n데이터 보안 및 규정 준수 보장\n의사결정 프로세스 개선\n운영 효율성 및 생산성 증대\n데이터 기반 혁신 촉진\n\n\n\n\n거버넌스는 기술 구현 목적이 아니라 비즈니스 전략의 핵심 요소로 자리 잡아야 한다.\n\n데이터 품질 향상: 고품질 데이터는 정확한 분석과 신속한 비즈니스 의사결정의 기반이 된다.\n\n잘못된 데이터로 인한 비용 감소\n고객 만족도 증가\n운영 효율성 개선\n\n규정 준수 및 리스크 관리: 데이터 관련 법규(예: GDPR, CCPA)를 준수해야 함\n\n법적 제재 및 벌금 회피\n\n기업 평판 보호\n\n고객 신뢰 증진\n\n운영 및 개발 효율성 증가: 체계적인 데이터 관리는 업무 프로세스를 최적화\n\n개발 생산성 형상: 표준용어와 구조 정보를 관리하여 데이터 이해도 증가\n중복 작업 감소\n데이터 검색 및 활용 시간 단축\n부서간 협업 증진\n유지보수 효율성 향상\n평균 30~60% 이상의 비용, 공수, 기간 효율 제고\n비즈니스 목표 달성 및 노동 생산성 향상에도 기여함\n\n데이터 보안 강화: 민감한 데이터를 보호하고 무단 접근을 방지해야 함\n\n데이터 유출로 인한 손실 예방\n고객 및 파트너 신뢰 유지\n지적 재산권 보호\n\n비즈니스 인텔리전스 및 분석 개선: 일관되고 신뢰할 수 있는 데이터는 더 나은 분석을 가능하게 함\n\n시장 트렌드 신속 파악\n고객 인사이트 향상\n예측 분석의 정확도 증가\n\n데이터 자산의 가치 극대화\n\n데이터를 전략적 자산으로 관리\n데이터 기반 신규 비즈니스 모델 창출\n데이터 monetization 기회 발굴\n\n데이터 판매: 수집한 데이터를 다른 기업이나 연구기관에 직접 판매\n데이터 접근권 판매: API나 구독 모델을 통해 데이터 접근권을 제공\n제품/서비스 개선: 데이터를 활용해 기존 제품이나 서비스를 개선\n맞춤형 마케팅: 고객 데이터를 활용한 타겟 마케팅으로 매출 증대\n운영 효율성 개선: 내부 데이터 분석을 통한 비용 절감\n\n\n비용 절감: 효율적인 데이터 관리는 여러 영역에서 비용을 절감\n\n데이터 저장 및 관리 비용 최적화\n데이터 관련 오류 수정 비용 감소\n중복 시스템 및 프로세스 제거\n\n\n\n\n\n데이터 거버넌스의 구성 요소는 데이터 관리 조직과 데이터 관리 체계로 구성되어 있지만 여러 사정상 데이터 관리 체계에 대한 정리에 집중한다. (후차적인 업데이트를 통해 데이터 관리 조직에 대한 정보를 정리하여 업로드 할 것)\n\n\n\n데이터의 조직화, 저장, 관리 방식을 정의하는 아키텍처와 모델링\n데이터 구조 설계 및 변경에 따른 표준 프로세스를 정의하고 구조 변경시 승인 절차를 통한 일관성 유지\n데이터 구조의 구성\n\n데이터 주제 영역 (Data Subject Area)\n\n비즈니스 관점에서 관련된 데이터를 그룹화한 상위 수준의 분류\n데이터의 논리적 구조화와 관리를 용이하게 하기 떄문에 명확히 정의가 되어있어야 한다.\n예시: 고객, 제품, 주문, 재무 등\n\n개념 데이터 모델 (Conceptual Data Model)\n\n비즈니스 요구사항을 반영한 고수준의 데이터 구조 표현\n비즈니스 용어를 사용하며, 기술적 세부사항은 포함하지 않음\n비즈니스 관계자와 기술팀 간의 커뮤니케이션 촉진\n구성요소: 주요 엔티티와 그들 간의 관계\n\n논리 데이터 모델 (Logical Data Model)\n\n개념 모델을 더 상세화한 것으로, 데이터 구조와 관계를 명확히 정의\n특정 데이터베이스 시스템에 독립적\n비즈니스 규칙과 데이터 요구사항을 상세히 표현\n구성요소: 엔티티, 속성, 관계, 키\n\n물리 데이터 모델 (Physical Data Model)\n\n논리 모델을 특정 데이터베이스 시스템에 맞게 구현한 모델\n데이터베이스 특정 용어와 개념 사용 (테이블, 컬럼, 인덱스 등)\n실제 데이터베이스 구현을 위한 상세 명세 제공\n구성요소: 테이블, 컬럼, 데이터 타입, 제약조건, 인덱스, 파티션 등\n\n데이터 라이프 사이클 (Data Lifecycle)\n\n데이터의 생성부터 폐기까지의 전체 과정에 대한 원칙 및 관리 방법 수립\n예를 들어, 데이터 보안 및 프라이버시 정책 수립\n생성, 저장, 사용, 공유, 보관, 폐기와 같은 데이터 생명주기 관리 정책\n\n데이터 흐름도와 데이터 계보(lineage) 관리를 통해 데이터의 전체 생명주기를 명확히 파악\n\n데이터의 효율적 관리와 가치 최적화\n고려사항: 데이터 품질, 보안, 규정 준수, 저장 최적화\n\n\n\n\n\n\n\n데이터의 정확성, 일관성, 신뢰성을 유지하기 위한 규칙과 가이드라인\n데이터의 품질을 향상시키고 데이터 활용 효율성을 높임\n데이터 표준 관리의 구성 요소\n\n표준 단어 (Standard Words)\n\n데이터 요소를 구성하는 가장 기본적인 의미 단위\n일관된 용어 사용을 통한 데이터 의미의 명확성 확보하기 위해 제작되어야 한다.\n동의어, 약어, 금칙어 등을 정의하여 단어 사용의 일관성 유지를 할 수 있음\n예시: ‘고객’, ‘주문’, ‘제품’, ‘날짜’ 등\n\n표준 용어 (Standard Terms)\n\n표준 단어들의 조합으로 만들어진 비즈니스에서 사용되는 용어\n업무 영역에서 일관된 의미를 가진 용어 사용 보장을 할 수 있음\n명명 규칙을 정의하여 용어 생성의 일관성을 유지할 수 있음\n예시: ‘고객ID’, ‘주문일자’, ‘제품코드’ 등\n\n표준 도메인 (Standard Domains)\n\n데이터 요소의 형식, 길이, 값의 범위 등을 정의한 것\n데이터 타입과 제약조건의 일관성 확보\n특징: 데이터 타입, 길이, 소수점 자릿수, 값 범위 등을 포함\n\n예시: ‘날짜형’, ‘금액형’, ‘코드형’ 등\n\n표준 코드 (Standard Codes)\n\n특정 의미를 나타내는 고유한 식별자 또는 약어\n데이터의 일관성과 효율성 향상\n코드 값, 의미, 사용 규칙 등을 정의한다.\n예시: 국가코드, 통화코드, 상태코드 등\n\n서비스 DB별 매핑 정보 (Mapping Information for Service Databases)\n\n표준화된 데이터 요소와 실제 서비스 데이터베이스 간의 연결 정보\n표준과 실제 구현 간의 일관성 유지 및 변환 지원\n표준 요소와 실제 DB 요소 간의 대응 관계, 변환 규칙 등을 포함\n데이터 모델이 이미 완성된 Legacy System(또는 시스템) 통합을 하기 위해선 legacy를 유지하고 표준화된 데이터 모델을 다시 만들어야한다.\n매핑 정보를 통해 기존 legacy와 표준화된 데이터 모델의 관계는 매핑 정보로 연결시킨다.\n\n예시: 표준 용어 ‘CustomerID’가 서비스 A의 DB에서는 ’CustID’, 서비스 B의 DB에서는 ’UserNo’로 사용되는 경우의 매핑 정보\n\n\n\n\n\n\n\n데이터의 정확성, 완전성, 일관성, 시의성 등을 관리\n데이터 분석 및 의사결정의 신뢰성을 좌우하는 핵심 요소\n조직 및 역할\n\n데이터 거버넌스 위원회\n데이터 소유자 (Data Owner)\n데이터 관리자 (Data Steward)\n데이터 사용자\n\n프로세스\n\n데이터 품질 관리 프로세스\n메타데이터 관리 프로세스\n데이터 접근 및 공유 프로세스\n\n기술\n\n데이터 표준 단어 사전\n데이터 표준 용어 사전\n데이터 카탈로그\n데이터 품질 도구\n메타데이터 관리 도구\nDBMS 사용\n\n주요 원칙\n\n책임성: 데이터에 대한 명확한 소유권과 책임을 정의\n투명성: 데이터 관련 프로세스와 결정을 투명하게 관리\n무결성: 데이터의 정확성과 일관성을 유지\n보안: 데이터를 안전하게 보호하고 적절한 접근 제어를 실시\n규정 준수: 관련 법규와 업계 표준을 준수\n가용성: 필요한 사람이 필요한 시점에 데이터에 접근할 수 있도록 함\n효율성: 데이터 관리 프로세스를 최적화하여 비용 효율성을 높임\n\n구현 단계\n\n현황 평가: 현재의 데이터 관리 실태를 분석\n전략 수립: 조직의 목표에 맞는 데이터 거버넌스 전략을 수립\n정책 및 표준 개발: 필요한 정책과 표준을 개발\n조직 구성: 데이터 거버넌스를 위한 조직 구조를 설계하고 역할을 할당\n프로세스 구현: 데이터 관리 프로세스를 설계하고 구현\n기술 도입: 필요한 데이터 관리 도구를 선택하고 도입\n교육 및 변화 관리: 조직 구성원들에게 필요한 교육을 제공하고 변화를 관리\n모니터링 및 개선: 지속적으로 성과를 모니터링하고 개선\n\n도전 과제\n\n조직 문화 변화의 어려움\n다양한 이해관계자 간의 조정\n지속적인 투자와 관심 유지\n레거시 시스템과의 통합\n\n현실적으로는 기존의 레거시 시스템은 유지하고 차세대 시스템에 거버넌스 정책을 적용\n레거시까지 거버넌즈 정책을 적용할 경우 실무자들의 거센 반발이 있음\n절충안으로 mapping table을 만들어 기존 레거시와 정책이 적용된 표준화 테이블을 연결\n\n\n\n\n\n\n\n데이터 품질 진단 구성 요소\n\n품질관리 대상 (Quality Management Targets)\n\n품질 관리를 수행할 데이터 요소나 데이터셋\n품질 관리의 범위와 우선순위 설정\n비즈니스 중요도, 데이터 볼륨, 사용 빈도 등을 고려하여 선정\n예시: 고객 정보 테이블, 주문 이력 데이터, 재무 보고서 데이터 등\n\n품질 진단 지표 (Quality Diagnosis Indicators)\n\n데이터 품질을 측정하고 평가하기 위한 기준\n객관적이고 정량적인 품질 평가 가능\n각 지표별로 측정 방법과 허용 기준을 정의\n예시: 정확성, 완전성, 일관성, 유효성, 적시성 등\n\n핵심 품질 항목 (Core Quality Items)\n\n특정 데이터셋이나 비즈니스 영역에서 중요하게 관리해야 할 품질 요소\n중점적인 품질 관리를 통한 효율성 제고\n비즈니스 영향도가 높은 항목들을 선별하여 집중 관리\n예시: 고객 데이터의 경우 ‘이메일 주소 유효성’, ‘중복 고객 비율’ 등\n\n업무 규칙 (Business Rules)\n\n데이터가 준수해야 할 비즈니스 로직이나 제약 조건\n비즈니스 요구사항에 맞는 데이터 품질 보장\n도메인 전문가와 협력하여 정의, 지속적으로 업데이트 필요\n예시: “주문 금액은 항상 양수여야 한다”, “고객 연령은 0에서 120 사이여야 한다” 등\n\n품질 관리 지침 (Quality Management Guidelines)\n\n데이터 품질을 유지하고 개선하기 위한 정책, 절차, 베스트 프랙티스\n일관되고 체계적인 품질 관리 활동 지원\n조직의 데이터 거버넌스 정책과 연계, 문서화하여 공유\n예시: 데이터 입력 표준, 품질 모니터링 주기, 이상치 처리 절차 등\n\n\n\n\n\n\n\n\n의사결정 품질 저하\n\n부정확하거나 불완전한 데이터로 인해 잘못된 결정을 내릴 수 있다.\n시의적절한 데이터 접근이 어려워 기회를 놓칠 수 있다.\n\n운영 비효율성\n\n중복되거나 일관성 없는 데이터로 인해 작업 시간이 증가\n부서간 데이터 공유와 협업이 어려워진다.\n시간이 지남에 따라 비표준화된 데이터는 계속 누적되어 악순환에 빠짐\n\n규정 준수 리스크 증가\n\n데이터 관련 법규(예: GDPR, CCPA) 위반 가능성이 높아진다.\n감사 대응이 어려워지고, 이로 인한 법적 제재나 벌금 위험이 증가\n\n신뢰 상실\n\n데이터 구조와 정의가 불명확하여 데이터의 신뢰성이 떨어짐\n부정확한 고객 데이터로 인해 서비스 품질이 저하될 수 있다.\n데이터의 내용과 형식이 불분명하여 데이터 해석이 어렵습니다.\n개인정보 유출 위험이 증가하여 고객 신뢰를 잃을 수 있다.\n\n재무적 손실\n\n잘못된 데이터로 인한 전략적 실패로 재무적 손실이 발생할 수 있다.\n데이터 오류 수정에 많은 비용과 시간이 소요\n\n일관성 없는 명명 규칙으로 인해 데이터 이해와 통합이 어려워짐\n\n즉, 높은 의사소통 비용과 생산성 저하\n\n이 컬럼에 무슨 데이터가 들어가 있을까\n내가 필요한 데이터가 어디에 있을까\n내가 보고 있는 컬럼이 진짜 있는 컬럼일까\n데이터 흐름 관계는 어떻게 되어 있을까\n데이터를 저장시킬 컬럼명은 어떻게 지어야 할까\n\n\n경쟁력 약화\n\n낮은 데이터 품질로 의사결정의 정확도와 신속도가 저하됨\n데이터 기반 혁신이 어려워져 시장에서 뒤처질 수 있다.\n고객 인사이트 부족으로 시장 변화에 대응하기 어려움\n\nIT 시스템 복잡성 증가\n\n일관성 없는 데이터 구조로 인해 시스템 통합이 어려워진다.\n레거시 시스템 유지 비용이 증가\n\n데이터 보안 취약성\n\n데이터 접근 통제가 제대로 이루어지지 않아 보안 위험이 증가\n중요 데이터의 위치나 중요도를 파악하기 어려워 적절한 보호가 어렵다.\n\n분석 및 AI/ML 프로젝트 실패\n\n데이터 통합 실패 및 부족으로 모델링 불가\n낮은 품질의 데이터로 인해 분석 결과의 신뢰성이 떨어진다.\nAI/ML 모델의 성능이 저하되거나 편향된 결과를 도출할 수 있다.\n\n조직 문화 악화\n\n데이터에 대한 불신으로 인해 데이터 기반 문화 형성이 어렵다.\n부서간 데이터 사일로로 인해 협업이 저해\n\n비즈니스 기회 상실\n\n데이터의 전략적 가치를 활용하지 못해 새로운 비즈니스 모델 개발이 어렵다.\n데이터 monetization 기회를 놓친다.\n\n리소스 낭비\n\n중복된 데이터 저장 및 관리로 인해 불필요한 비용이 발생\n데이터 검색과 정제에 많은 시간을 소비.\n\n데이터의 위치와 저장 방식이 일관되지 않아 필요한 데이터를 찾기 어렵다\n\n\n데이터 계보 추적 불가\n\n데이터의 생성, 이동, 변환 과정이 불분명하여 데이터 계보 추적이 어렵다.\n\n\n\n\n\n\n마스 클라이밋 오비터 우주선\n\n미션: 화성의 기후와 대기를 관찰하기 위해 설계된 우주선\n특징: NASA와 록히드 마틴이 협력하여 우주선 제작\n\n실패 현상: 화성 대기권 진입으로 소멸 됨\n실패 원인: 협력 조직의 다른 단위 사용\n\nNASA : 뉴턴 단위로 계산\n록히드마틴 : 파운드 force 단위 사용\n\n결과\n\n3억2천만 달러 손실\n\n영향\n\n데이터 표준화 절차 재검토 촉발"
  },
  {
    "objectID": "docs/blog/posts/Governance/1.basis.html#데이터-거버넌스-기초",
    "href": "docs/blog/posts/Governance/1.basis.html#데이터-거버넌스-기초",
    "title": "Data Governance Study - Basic",
    "section": "",
    "text": "조직의 데이터 관리를 위한 핵심적인 프레임워크\n\n프레임워크란?\n\n특정 문제나 도메인에 대한 구조화된 접근 방식을 제공하는 개념적인 구조\n문제 해결을 위한 가이드라인, 원칙, 모범 사례를 제시\n\n\n조직 전체의 데이터 관리 전략과 실행 방법을 다룬다. 즉, 개념적, 전략적 접근 방식\n최종적으로는 조직의 정책, 프로세스, 문화 등에 반영되어야 한다.\n\n\n\n\n데이터 거버넌스는 조직 내에서 데이터의 가용성, 유용성, 무결성, 보안을 보장하기 위한 정책, 절차, 표준을 수립하고 실행하는 과정이다.\n이는 데이터를 비즈니스 자산으로 관리하고 활용하는 체계적인 접근 방식이다.\n데이터 거버넌스 구조\n\n\n\n\nData Governance Structure\n\n\n\n\n\n\n데이터 품질 향상\n데이터 보안 및 규정 준수 보장\n의사결정 프로세스 개선\n운영 효율성 및 생산성 증대\n데이터 기반 혁신 촉진\n\n\n\n\n거버넌스는 기술 구현 목적이 아니라 비즈니스 전략의 핵심 요소로 자리 잡아야 한다.\n\n데이터 품질 향상: 고품질 데이터는 정확한 분석과 신속한 비즈니스 의사결정의 기반이 된다.\n\n잘못된 데이터로 인한 비용 감소\n고객 만족도 증가\n운영 효율성 개선\n\n규정 준수 및 리스크 관리: 데이터 관련 법규(예: GDPR, CCPA)를 준수해야 함\n\n법적 제재 및 벌금 회피\n\n기업 평판 보호\n\n고객 신뢰 증진\n\n운영 및 개발 효율성 증가: 체계적인 데이터 관리는 업무 프로세스를 최적화\n\n개발 생산성 형상: 표준용어와 구조 정보를 관리하여 데이터 이해도 증가\n중복 작업 감소\n데이터 검색 및 활용 시간 단축\n부서간 협업 증진\n유지보수 효율성 향상\n평균 30~60% 이상의 비용, 공수, 기간 효율 제고\n비즈니스 목표 달성 및 노동 생산성 향상에도 기여함\n\n데이터 보안 강화: 민감한 데이터를 보호하고 무단 접근을 방지해야 함\n\n데이터 유출로 인한 손실 예방\n고객 및 파트너 신뢰 유지\n지적 재산권 보호\n\n비즈니스 인텔리전스 및 분석 개선: 일관되고 신뢰할 수 있는 데이터는 더 나은 분석을 가능하게 함\n\n시장 트렌드 신속 파악\n고객 인사이트 향상\n예측 분석의 정확도 증가\n\n데이터 자산의 가치 극대화\n\n데이터를 전략적 자산으로 관리\n데이터 기반 신규 비즈니스 모델 창출\n데이터 monetization 기회 발굴\n\n데이터 판매: 수집한 데이터를 다른 기업이나 연구기관에 직접 판매\n데이터 접근권 판매: API나 구독 모델을 통해 데이터 접근권을 제공\n제품/서비스 개선: 데이터를 활용해 기존 제품이나 서비스를 개선\n맞춤형 마케팅: 고객 데이터를 활용한 타겟 마케팅으로 매출 증대\n운영 효율성 개선: 내부 데이터 분석을 통한 비용 절감\n\n\n비용 절감: 효율적인 데이터 관리는 여러 영역에서 비용을 절감\n\n데이터 저장 및 관리 비용 최적화\n데이터 관련 오류 수정 비용 감소\n중복 시스템 및 프로세스 제거\n\n\n\n\n\n데이터 거버넌스의 구성 요소는 데이터 관리 조직과 데이터 관리 체계로 구성되어 있지만 여러 사정상 데이터 관리 체계에 대한 정리에 집중한다. (후차적인 업데이트를 통해 데이터 관리 조직에 대한 정보를 정리하여 업로드 할 것)\n\n\n\n데이터의 조직화, 저장, 관리 방식을 정의하는 아키텍처와 모델링\n데이터 구조 설계 및 변경에 따른 표준 프로세스를 정의하고 구조 변경시 승인 절차를 통한 일관성 유지\n데이터 구조의 구성\n\n데이터 주제 영역 (Data Subject Area)\n\n비즈니스 관점에서 관련된 데이터를 그룹화한 상위 수준의 분류\n데이터의 논리적 구조화와 관리를 용이하게 하기 떄문에 명확히 정의가 되어있어야 한다.\n예시: 고객, 제품, 주문, 재무 등\n\n개념 데이터 모델 (Conceptual Data Model)\n\n비즈니스 요구사항을 반영한 고수준의 데이터 구조 표현\n비즈니스 용어를 사용하며, 기술적 세부사항은 포함하지 않음\n비즈니스 관계자와 기술팀 간의 커뮤니케이션 촉진\n구성요소: 주요 엔티티와 그들 간의 관계\n\n논리 데이터 모델 (Logical Data Model)\n\n개념 모델을 더 상세화한 것으로, 데이터 구조와 관계를 명확히 정의\n특정 데이터베이스 시스템에 독립적\n비즈니스 규칙과 데이터 요구사항을 상세히 표현\n구성요소: 엔티티, 속성, 관계, 키\n\n물리 데이터 모델 (Physical Data Model)\n\n논리 모델을 특정 데이터베이스 시스템에 맞게 구현한 모델\n데이터베이스 특정 용어와 개념 사용 (테이블, 컬럼, 인덱스 등)\n실제 데이터베이스 구현을 위한 상세 명세 제공\n구성요소: 테이블, 컬럼, 데이터 타입, 제약조건, 인덱스, 파티션 등\n\n데이터 라이프 사이클 (Data Lifecycle)\n\n데이터의 생성부터 폐기까지의 전체 과정에 대한 원칙 및 관리 방법 수립\n예를 들어, 데이터 보안 및 프라이버시 정책 수립\n생성, 저장, 사용, 공유, 보관, 폐기와 같은 데이터 생명주기 관리 정책\n\n데이터 흐름도와 데이터 계보(lineage) 관리를 통해 데이터의 전체 생명주기를 명확히 파악\n\n데이터의 효율적 관리와 가치 최적화\n고려사항: 데이터 품질, 보안, 규정 준수, 저장 최적화\n\n\n\n\n\n\n\n데이터의 정확성, 일관성, 신뢰성을 유지하기 위한 규칙과 가이드라인\n데이터의 품질을 향상시키고 데이터 활용 효율성을 높임\n데이터 표준 관리의 구성 요소\n\n표준 단어 (Standard Words)\n\n데이터 요소를 구성하는 가장 기본적인 의미 단위\n일관된 용어 사용을 통한 데이터 의미의 명확성 확보하기 위해 제작되어야 한다.\n동의어, 약어, 금칙어 등을 정의하여 단어 사용의 일관성 유지를 할 수 있음\n예시: ‘고객’, ‘주문’, ‘제품’, ‘날짜’ 등\n\n표준 용어 (Standard Terms)\n\n표준 단어들의 조합으로 만들어진 비즈니스에서 사용되는 용어\n업무 영역에서 일관된 의미를 가진 용어 사용 보장을 할 수 있음\n명명 규칙을 정의하여 용어 생성의 일관성을 유지할 수 있음\n예시: ‘고객ID’, ‘주문일자’, ‘제품코드’ 등\n\n표준 도메인 (Standard Domains)\n\n데이터 요소의 형식, 길이, 값의 범위 등을 정의한 것\n데이터 타입과 제약조건의 일관성 확보\n특징: 데이터 타입, 길이, 소수점 자릿수, 값 범위 등을 포함\n\n예시: ‘날짜형’, ‘금액형’, ‘코드형’ 등\n\n표준 코드 (Standard Codes)\n\n특정 의미를 나타내는 고유한 식별자 또는 약어\n데이터의 일관성과 효율성 향상\n코드 값, 의미, 사용 규칙 등을 정의한다.\n예시: 국가코드, 통화코드, 상태코드 등\n\n서비스 DB별 매핑 정보 (Mapping Information for Service Databases)\n\n표준화된 데이터 요소와 실제 서비스 데이터베이스 간의 연결 정보\n표준과 실제 구현 간의 일관성 유지 및 변환 지원\n표준 요소와 실제 DB 요소 간의 대응 관계, 변환 규칙 등을 포함\n데이터 모델이 이미 완성된 Legacy System(또는 시스템) 통합을 하기 위해선 legacy를 유지하고 표준화된 데이터 모델을 다시 만들어야한다.\n매핑 정보를 통해 기존 legacy와 표준화된 데이터 모델의 관계는 매핑 정보로 연결시킨다.\n\n예시: 표준 용어 ‘CustomerID’가 서비스 A의 DB에서는 ’CustID’, 서비스 B의 DB에서는 ’UserNo’로 사용되는 경우의 매핑 정보\n\n\n\n\n\n\n\n데이터의 정확성, 완전성, 일관성, 시의성 등을 관리\n데이터 분석 및 의사결정의 신뢰성을 좌우하는 핵심 요소\n조직 및 역할\n\n데이터 거버넌스 위원회\n데이터 소유자 (Data Owner)\n데이터 관리자 (Data Steward)\n데이터 사용자\n\n프로세스\n\n데이터 품질 관리 프로세스\n메타데이터 관리 프로세스\n데이터 접근 및 공유 프로세스\n\n기술\n\n데이터 표준 단어 사전\n데이터 표준 용어 사전\n데이터 카탈로그\n데이터 품질 도구\n메타데이터 관리 도구\nDBMS 사용\n\n주요 원칙\n\n책임성: 데이터에 대한 명확한 소유권과 책임을 정의\n투명성: 데이터 관련 프로세스와 결정을 투명하게 관리\n무결성: 데이터의 정확성과 일관성을 유지\n보안: 데이터를 안전하게 보호하고 적절한 접근 제어를 실시\n규정 준수: 관련 법규와 업계 표준을 준수\n가용성: 필요한 사람이 필요한 시점에 데이터에 접근할 수 있도록 함\n효율성: 데이터 관리 프로세스를 최적화하여 비용 효율성을 높임\n\n구현 단계\n\n현황 평가: 현재의 데이터 관리 실태를 분석\n전략 수립: 조직의 목표에 맞는 데이터 거버넌스 전략을 수립\n정책 및 표준 개발: 필요한 정책과 표준을 개발\n조직 구성: 데이터 거버넌스를 위한 조직 구조를 설계하고 역할을 할당\n프로세스 구현: 데이터 관리 프로세스를 설계하고 구현\n기술 도입: 필요한 데이터 관리 도구를 선택하고 도입\n교육 및 변화 관리: 조직 구성원들에게 필요한 교육을 제공하고 변화를 관리\n모니터링 및 개선: 지속적으로 성과를 모니터링하고 개선\n\n도전 과제\n\n조직 문화 변화의 어려움\n다양한 이해관계자 간의 조정\n지속적인 투자와 관심 유지\n레거시 시스템과의 통합\n\n현실적으로는 기존의 레거시 시스템은 유지하고 차세대 시스템에 거버넌스 정책을 적용\n레거시까지 거버넌즈 정책을 적용할 경우 실무자들의 거센 반발이 있음\n절충안으로 mapping table을 만들어 기존 레거시와 정책이 적용된 표준화 테이블을 연결\n\n\n\n\n\n\n\n데이터 품질 진단 구성 요소\n\n품질관리 대상 (Quality Management Targets)\n\n품질 관리를 수행할 데이터 요소나 데이터셋\n품질 관리의 범위와 우선순위 설정\n비즈니스 중요도, 데이터 볼륨, 사용 빈도 등을 고려하여 선정\n예시: 고객 정보 테이블, 주문 이력 데이터, 재무 보고서 데이터 등\n\n품질 진단 지표 (Quality Diagnosis Indicators)\n\n데이터 품질을 측정하고 평가하기 위한 기준\n객관적이고 정량적인 품질 평가 가능\n각 지표별로 측정 방법과 허용 기준을 정의\n예시: 정확성, 완전성, 일관성, 유효성, 적시성 등\n\n핵심 품질 항목 (Core Quality Items)\n\n특정 데이터셋이나 비즈니스 영역에서 중요하게 관리해야 할 품질 요소\n중점적인 품질 관리를 통한 효율성 제고\n비즈니스 영향도가 높은 항목들을 선별하여 집중 관리\n예시: 고객 데이터의 경우 ‘이메일 주소 유효성’, ‘중복 고객 비율’ 등\n\n업무 규칙 (Business Rules)\n\n데이터가 준수해야 할 비즈니스 로직이나 제약 조건\n비즈니스 요구사항에 맞는 데이터 품질 보장\n도메인 전문가와 협력하여 정의, 지속적으로 업데이트 필요\n예시: “주문 금액은 항상 양수여야 한다”, “고객 연령은 0에서 120 사이여야 한다” 등\n\n품질 관리 지침 (Quality Management Guidelines)\n\n데이터 품질을 유지하고 개선하기 위한 정책, 절차, 베스트 프랙티스\n일관되고 체계적인 품질 관리 활동 지원\n조직의 데이터 거버넌스 정책과 연계, 문서화하여 공유\n예시: 데이터 입력 표준, 품질 모니터링 주기, 이상치 처리 절차 등\n\n\n\n\n\n\n\n\n의사결정 품질 저하\n\n부정확하거나 불완전한 데이터로 인해 잘못된 결정을 내릴 수 있다.\n시의적절한 데이터 접근이 어려워 기회를 놓칠 수 있다.\n\n운영 비효율성\n\n중복되거나 일관성 없는 데이터로 인해 작업 시간이 증가\n부서간 데이터 공유와 협업이 어려워진다.\n시간이 지남에 따라 비표준화된 데이터는 계속 누적되어 악순환에 빠짐\n\n규정 준수 리스크 증가\n\n데이터 관련 법규(예: GDPR, CCPA) 위반 가능성이 높아진다.\n감사 대응이 어려워지고, 이로 인한 법적 제재나 벌금 위험이 증가\n\n신뢰 상실\n\n데이터 구조와 정의가 불명확하여 데이터의 신뢰성이 떨어짐\n부정확한 고객 데이터로 인해 서비스 품질이 저하될 수 있다.\n데이터의 내용과 형식이 불분명하여 데이터 해석이 어렵습니다.\n개인정보 유출 위험이 증가하여 고객 신뢰를 잃을 수 있다.\n\n재무적 손실\n\n잘못된 데이터로 인한 전략적 실패로 재무적 손실이 발생할 수 있다.\n데이터 오류 수정에 많은 비용과 시간이 소요\n\n일관성 없는 명명 규칙으로 인해 데이터 이해와 통합이 어려워짐\n\n즉, 높은 의사소통 비용과 생산성 저하\n\n이 컬럼에 무슨 데이터가 들어가 있을까\n내가 필요한 데이터가 어디에 있을까\n내가 보고 있는 컬럼이 진짜 있는 컬럼일까\n데이터 흐름 관계는 어떻게 되어 있을까\n데이터를 저장시킬 컬럼명은 어떻게 지어야 할까\n\n\n경쟁력 약화\n\n낮은 데이터 품질로 의사결정의 정확도와 신속도가 저하됨\n데이터 기반 혁신이 어려워져 시장에서 뒤처질 수 있다.\n고객 인사이트 부족으로 시장 변화에 대응하기 어려움\n\nIT 시스템 복잡성 증가\n\n일관성 없는 데이터 구조로 인해 시스템 통합이 어려워진다.\n레거시 시스템 유지 비용이 증가\n\n데이터 보안 취약성\n\n데이터 접근 통제가 제대로 이루어지지 않아 보안 위험이 증가\n중요 데이터의 위치나 중요도를 파악하기 어려워 적절한 보호가 어렵다.\n\n분석 및 AI/ML 프로젝트 실패\n\n데이터 통합 실패 및 부족으로 모델링 불가\n낮은 품질의 데이터로 인해 분석 결과의 신뢰성이 떨어진다.\nAI/ML 모델의 성능이 저하되거나 편향된 결과를 도출할 수 있다.\n\n조직 문화 악화\n\n데이터에 대한 불신으로 인해 데이터 기반 문화 형성이 어렵다.\n부서간 데이터 사일로로 인해 협업이 저해\n\n비즈니스 기회 상실\n\n데이터의 전략적 가치를 활용하지 못해 새로운 비즈니스 모델 개발이 어렵다.\n데이터 monetization 기회를 놓친다.\n\n리소스 낭비\n\n중복된 데이터 저장 및 관리로 인해 불필요한 비용이 발생\n데이터 검색과 정제에 많은 시간을 소비.\n\n데이터의 위치와 저장 방식이 일관되지 않아 필요한 데이터를 찾기 어렵다\n\n\n데이터 계보 추적 불가\n\n데이터의 생성, 이동, 변환 과정이 불분명하여 데이터 계보 추적이 어렵다.\n\n\n\n\n\n\n마스 클라이밋 오비터 우주선\n\n미션: 화성의 기후와 대기를 관찰하기 위해 설계된 우주선\n특징: NASA와 록히드 마틴이 협력하여 우주선 제작\n\n실패 현상: 화성 대기권 진입으로 소멸 됨\n실패 원인: 협력 조직의 다른 단위 사용\n\nNASA : 뉴턴 단위로 계산\n록히드마틴 : 파운드 force 단위 사용\n\n결과\n\n3억2천만 달러 손실\n\n영향\n\n데이터 표준화 절차 재검토 촉발"
  },
  {
    "objectID": "docs/blog/posts/Governance/4.data_model.html",
    "href": "docs/blog/posts/Governance/4.data_model.html",
    "title": "Data Governance Study - Data Model",
    "section": "",
    "text": "데이터 모델링은 현실 세계의 정보를 컴퓨터 시스템에서 표현할 수 있는 논리적 구조로 변환하는 과정이다.\n이는 데이터 요소, 그들 간의 관계, 그리고 데이터에 대한 제약 조건을 정의한다.\n\n\n\n\n엔티티(Entity): 데이터를 저장할 대상 (예: 고객, 주문)\n속성(Attribute): 엔티티의 특성 (예: 고객명, 주문일자)\n관계(Relationship): 엔티티 간의 연관성\n키(Key): 데이터를 고유하게 식별하는 속성\n제약조건(Constraint): 데이터의 무결성을 보장하는 규칙\n\n\n\n\n\n\n\n목적: 비즈니스 관점에서 높은 수준의 데이터 구조 정의\n특징: 주요 엔티티와 그들 간의 관계를 식별\n산출물: 개체-관계 다이어그램(ERD, Entity Relation Diagram)\n\n\n\n\n\n목적: 비즈니스 규칙과 데이터 요구사항을 상세히 표현\n특징: 엔티티의 속성 정의, 정규화 수행, 키 설정\n산출물: 상세 ERD, 데이터 사전\n\n\n\n\n\n목적: 실제 데이터베이스 구현을 위한 스키마 설계\n특징: 테이블, 컬럼, 인덱스 등의 물리적 구조 정의\n산출물: 데이터베이스 스키마, SQL 스크립트\n\n\n\n\n\n\n데이터 품질 향상: 일관성 있고 정확한 데이터 구조 제공\n시스템 성능 최적화: 효율적인 데이터 접근 및 저장 구조 설계\n유지보수 용이성: 시스템 변경 및 확장이 쉬워짐\n비즈니스 이해 증진: 데이터 관점에서 비즈니스 프로세스 이해\n의사소통 도구: 기술팀과 비즈니스팀 간의 소통 촉진"
  },
  {
    "objectID": "docs/blog/posts/Governance/4.data_model.html#data-architecure",
    "href": "docs/blog/posts/Governance/4.data_model.html#data-architecure",
    "title": "Data Governance Study - Data Model",
    "section": "",
    "text": "데이터 아키텍처는 조직의 데이터 자산을 효과적으로 수집, 저장, 관리, 사용하기 위한 전체적인 구조와 계획을 의미한다.\n이는 비즈니스 요구사항, 기술적 구현, 데이터 거버넌스를 연결하는 청사진 역할을 한다.\n\n\n\n\n데이터 모델링: 데이터 엔티티, 관계, 속성 정의\n\n개념적 데이터 모델 (Conceptual Data Model)\n논리적 데이터 모델 (Logical Data Model)\n물리적 데이터 모델 (Physical Data Model)\n\n데이터 흐름\n\n데이터의 생성, 이동, 변환, 저장, 소비 과정 정의\nETL(Extract, Transform, Load) 프로세스 설계\n\n데이터 저장소\n\n데이터베이스, 데이터 레이크, 데이터 웨어하우스, 데이터 마트 등의 구조 설계\n데이터 저장 방식 (온프레미스, 클라우드, 하이브리드 등)\n\n메타데이터 관리\n\n데이터에 대한 데이터(메타데이터) 관리 방식 정의\n데이터 사전, 데이터 용어 사전, 데이터 카탈로그, 비즈니스 용어집 등 포함\n\n데이터 통합\n\n다양한 소스의 데이터를 통합하는 방식 정의\n마스터 데이터 관리(MDM) 전략 포함\n\n데이터 보안 및 프라이버시\n\n데이터 접근 제어, 암호화, 마스킹 등의 보안 정책\n개인정보 보호 규정 준수 방안\n\n데이터 품질 관리\n\n데이터 품질 측정 및 개선 프로세스 정의\n데이터 클렌징, 검증 방법론\n\n데이터 거버넌스 프레임워크\n\n데이터 관리에 대한 정책, 절차, 책임 정의\n데이터 스튜어드십 모델\n\n기술 스택\n\n데이터 관리 및 분석을 위한 기술 솔루션 선정\n\n\n\n\n\n종종 Data Architecture 한글로 데이터 구조관리라 번역되는데 Data Structgure와 혼동하지 말자.\n\n\n\n데이터 요소들을 조직화하고 저장하는 특정 방식을 의미하며 주로 프로그래밍과 데이터베이스 설계 수준에서 사용되는 개념이다.\n예: 배열, 리스트, 트리, 그래프, 테이블 등을 말하며 주로 개별 애플리케이션이나 데이터베이스 수준에 초점을 맞춘다.\n\n\n\n\n\n조직의 데이터 자산을 관리하기 위한 전체적인 구조와 모델을 의미하고 비즈니스 요구사항과 IT 전략을 연결하는 청사진 역할을 한다.\n데이터의 수집, 저장, 변환, 분배, 사용에 관한 전체적인 계획을 포함한다. (주로 그림이 많이 들어감)\n데이터 모델, 데이터 흐름, 통합 지점, 보안 정책 등을 포함합니다. (주로 그림이 많이 들어감)\n데이터 아키텍트, 비즈니스 분석가, IT 전략가가 주로 다룬다.\n범위: 조직 전체의 데이터 환경을 대상으로 하고 비즈니스 목표, 규제 요구사항, 기술 인프라를 모두 고려한다.\n\n\n\n\n\n범위\n\nData Structure: 특정 데이터셋이나 애플리케이션에 국한됨\nData Architecture: 조직 전체의 데이터 환경을 다룸\n\n목적\n\nData Structure: 효율적인 데이터 저장 및 접근을 위한 기술적 설계\nData Architecture: 전략적 데이터 관리 및 활용을 위한 전체적인 프레임워크\n\n관점\n\nData Structure: 주로 기술적, 구현 중심적 관점\nData Architecture: 비즈니스와 기술을 연결하는 전략적 관점\n\n포함 요소\n\nData Structure: 데이터 유형, 관계, 제약조건 등\nData Architecture: 데이터 모델, 메타데이터, 데이터 흐름, 거버넌스 정책 등\n\n사용자\n\nData Structure: 주로 개발자, DBA\nData Architecture: 데이터 아키텍트, 비즈니스 분석가, IT 전략가, 경영진"
  },
  {
    "objectID": "docs/blog/posts/Governance/4.data_model.html#data-modeling",
    "href": "docs/blog/posts/Governance/4.data_model.html#data-modeling",
    "title": "Data Governance Study - Data Model",
    "section": "",
    "text": "데이터 모델링은 현실 세계의 정보를 컴퓨터 시스템에서 표현할 수 있는 논리적 구조로 변환하는 과정이다.\n이는 데이터 요소, 그들 간의 관계, 그리고 데이터에 대한 제약 조건을 정의한다.\n\n\n\n\n엔티티(Entity): 데이터를 저장할 대상 (예: 고객, 주문)\n속성(Attribute): 엔티티의 특성 (예: 고객명, 주문일자)\n관계(Relationship): 엔티티 간의 연관성\n키(Key): 데이터를 고유하게 식별하는 속성\n제약조건(Constraint): 데이터의 무결성을 보장하는 규칙\n\n\n\n\n\n\n\n목적: 비즈니스 관점에서 높은 수준의 데이터 구조 정의\n특징: 주요 엔티티와 그들 간의 관계를 식별\n산출물: 개체-관계 다이어그램(ERD, Entity Relation Diagram)\n\n\n\n\n\n목적: 비즈니스 규칙과 데이터 요구사항을 상세히 표현\n특징: 엔티티의 속성 정의, 정규화 수행, 키 설정\n산출물: 상세 ERD, 데이터 사전\n\n\n\n\n\n목적: 실제 데이터베이스 구현을 위한 스키마 설계\n특징: 테이블, 컬럼, 인덱스 등의 물리적 구조 정의\n산출물: 데이터베이스 스키마, SQL 스크립트\n\n\n\n\n\n\n데이터 품질 향상: 일관성 있고 정확한 데이터 구조 제공\n시스템 성능 최적화: 효율적인 데이터 접근 및 저장 구조 설계\n유지보수 용이성: 시스템 변경 및 확장이 쉬워짐\n비즈니스 이해 증진: 데이터 관점에서 비즈니스 프로세스 이해\n의사소통 도구: 기술팀과 비즈니스팀 간의 소통 촉진"
  },
  {
    "objectID": "docs/blog/posts/Governance/4.data_model.html#도식",
    "href": "docs/blog/posts/Governance/4.data_model.html#도식",
    "title": "Data Governance Study - Data Model",
    "section": "2 도식",
    "text": "2 도식\n\n\n\n\n\n\n\nG\n\n\ncluster_2\n\n물리적 모델링\n\n\ncluster_1\n\n논리적 모델링\n\n\ncluster_0\n\n개념적 모델링\n\n\n\n엔티티 식별\n\n엔티티 식별\n\n\n\n관계 정의\n\n관계 정의\n\n\n\n엔티티 식별-&gt;관계 정의\n\n\n\n\n\nERD 작성\n\nERD 작성\n\n\n\n관계 정의-&gt;ERD 작성\n\n\n\n\n\n속성 정의\n\n속성 정의\n\n\n\n정규화\n\n정규화\n\n\n\n속성 정의-&gt;정규화\n\n\n\n\n\n키 설정\n\n키 설정\n\n\n\n정규화-&gt;키 설정\n\n\n\n\n\n상세 ERD 작성\n\n상세 ERD 작성\n\n\n\n키 설정-&gt;상세 ERD 작성\n\n\n\n\n\n테이블 정의\n\n테이블 정의\n\n\n\n컬럼 설계\n\n컬럼 설계\n\n\n\n테이블 정의-&gt;컬럼 설계\n\n\n\n\n\n인덱스 생성\n\n인덱스 생성\n\n\n\n컬럼 설계-&gt;인덱스 생성\n\n\n\n\n\nDB 스키마 작성\n\nDB 스키마 작성\n\n\n\n인덱스 생성-&gt;DB 스키마 작성"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-1.data_model_basic.html",
    "href": "docs/blog/posts/Governance/4-1.data_model_basic.html",
    "title": "Data Governance Study - Data Model (2)",
    "section": "",
    "text": "쉽게 말해, 데이터베이스는 데이터 저장소\n어렵게 말해, 일정한 체계 속에 저장된 데이터 및 데이터간 관계의 집합\n\n데이터는 보통 table(표) 단위로 저장됨\nDB는 보통 여러 table을 가지고 있을 수 있음\n\n\n\n\n\n현대 사회는 거의 모든 전기 기기에 DB가 있기때문에 데이터를 효율적으로 관리하려면 DB 시스템이 필요함\n하지만, 모든 상황에 DB가 필요한건 아님\n언제 필요함? 대용량의 데이터를 다룰 때\n예\n\n입금 출금 이체 등 은행 거래 (인터넷 뱅킹 포함) : Data 필요, 대용량 Data, 많은 사용자 (DB 필요)\n호텔 객실 예약 관리는 기록이 필요함 : Data 필요, 대용량 Data, 많은 사용자 (DB 필요)\n신호등의 램프 제어: 소규모 Data 필요 (신호의 상태를 기억하고 있어야 다음 신호 상태를 결정할 수 있음, DB 불필요)\n\n알고리즘으로 관리할 수 있는 프로그램 회로가 기능 대체 가능. 실질적으론 DB가 필요하지 않음\n\n온라인 쇼핑몰에서의 물품 구매: Data 필요, 대용량 Data, 많은 사용자 (DB 필요)\n전자식 개폐장치의 비밀번호 관리: Data 필요 (한명이 한번에 번호를 눌러 정답과 비교. DB가 필요하지 않음 , DB 불필요)\n\n입력받은 번호를 저장해놔야 정답 비밀번호와 비교 가능 하지만 이것 역시 알고리즘 프로그램 회로로 대체 가능\n\n\n\n\n\n\n\n데이터베이스 자체도 여러개 만들 수 있음\n최초 적재 (loading): 서비스를 위한 초기 대량 데이터 적재\n데이터 축적 (Interaction): 이벤트 발생에 따른 잦은 변경\n대용량의 데이터를 다룰때만 DB가 필요\n\n1) 많은 사용자들이 2)원하는 순간 3)데이터에 접근하거나 4)동시 접근 할때 관리 가능\n대용량의 데이터가 체계적으로 조직화되어 있어야함\n원하는 조건에 맞는 데이터에 접근가능\n\n데이터베이스 시스템\n\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nDatabase System\n\n\ncluster_1\n\nDBMS Software\n\n\n\na0\n\nUsers/Programmers\n\n\n\na1\n\nApplication Programs/Queries\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nSoftware to Process\n(Programs Queries)\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nSoftware to Access\n(Stored Data)\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nStored Database Definition\n(Metadata)\n\n\n\na3-&gt;a4\n\n\n\n\n\n\na5\n\nStored Database\n\n\n\na3-&gt;a5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntable에는 row(행)와 column(열) 이 있다.\nrow: table에 대한 체계적인 정보가 set로 구성되어 있는 단위, 개체\ncolumn: table이 갖고 있는 속성\n예: 학생 table\n\nrow: 각 각의 학생 개체 (김철수 개체 , 이영희 개체 등)이 갖고 있는 정보\n\n행 (Row): SQL 등에서 실제로 데이터를 다룰 때 자주 사용되는 용어\n레코드 (Record): 가장 일반적으로 사용되는 용어, 하나의 완전한 데이터 항목을 나타냄\n튜플 (Tuple): 관계형 데이터베이스 이론에서 주로 사용되는 용어, 수학적인 의미에서 속성들의 순서화된 집합을 나타냄\n엔티티 (Entity): 주로 데이터 모델링 단계에서 사용되는 용어, 실세계의 개체나 개념을 나타냄.\n인스턴스 (Instance): 특정 엔티티의 구체적인 예를 나타냄, 객체 지향 개념에서 차용된 용어.\n\ncolumn: 학생(table)이 갖고 있는 속성\n\n열 (Column): SQL 등에서 실제로 데이터를 다룰 때 자주 사용되는 용어, 테이블의 수직적 구성 요소를 지칭\n필드 (Field): 가장 일반적으로 사용되는 용어, 특정 유형의 데이터를 저장하는 테이블의 한 부분을 나타냄\n속성 (Attribute): 데이터 모델링과 관계형 데이터베이스 이론에서 주로 사용되는 용어, 엔티티의 특성이나 성질을 나타냄\n도메인 (Domain): 특정 속성이 가질 수 있는 모든 가능한 값의 집합(or 정의역)을 의미하는데 때로는 열 자체를 지칭하는 데 사용되기도 한다.\n변수 (Variable): 프로그래밍 관점에서 데이터베이스를 다룰 때 사용되는 용어\n\n\n\n\n\n학번\n이름\n전공\n성별\n\n\n\n\n20240001\n김철수\n컴퓨터공학\n남\n\n\n20240002\n이영희\n경영학\n여\n\n\n20240003\n박민준\n생물학\n남\n\n\n20240004\n정수연\n물리학\n여\n\n\n20240005\n강지원\n심리학\n여\n\n\n\n\n\n\n\n\nDB를 다루는 언어\n대표적인 예: SQL\n\n다른 언어에서 SQL문을 string 의 형태로 DB에 접근할수도 있고 따로 SQL을 써서 접근할 수 있음\nSQL 질의를 입력하면 (=SQL 쿼리을 날리면) 쿼리문이 stored database definition에서 DB구조를 파악한 후 Stored DB에 접근해서 쿼리문에 대응되는 원하는 data를 가지고와서 사용자에게 전달\n\nDBMS: DB를 다루는 프로그램\n\nDataBase Management System (DB 관리 시스템)\n사용자가 DB에 접근할 수 있도록 지원해주는 프로그램의 집합 (모듈의 집합)\n\n사용자 ID생성, 권한부여, 보안 관리, 동시성 제어, 사용자의 쿼리문을 처리 등의 모듈의 집합\n\nBusiness 요구사항에 맞는 DBMS를 선택해야함\nDBMS종류: 오라클, MS-SQL, MS-Access, SQLite, MySQL, MariaDB, SQL Server, SQLite, PostgreSQL, etc.\nDBMS 종류마다 시스템이 다르긴 하지만 모두 SQL 로 다룰 수 있다.\nSQL은 국제 표준이 존재하기 때문에 국제 표준을 습득하면 모든 DBMS를 어느 정도 사용할 수 있다. (하지만, 각 DBMS마다 고유의 문법이 있어 약간 씩 다름)\n\n\n\n\n\n\n학적 관리\n\nCourse Table\n\nattribute: course_name, credit_hours, department\n\n\n\n\ncourse_name\ncredit_hours\ndepartment\n\n\n\n\nIntroduction to Computer Science\n3\nComputer Science\n\n\nCalculus I\n4\nMathematics\n\n\nWorld History\n3\nHistory\n\n\nOrganic Chemistry\n4\nChemistry\n\n\n\nStudent Table\n\nattribute: name, student_number\n\n\n\n\nname\nstudent_number\n\n\n\n\nJohn Smith\n20240001\n\n\nEmma Johnson\n20240002\n\n\nMichael Lee\n20240003\n\n\nSophia Chen\n20240004\n\n\n\nSection Table\n\nattribute: section_ideintifier, course_number, semester, year, instructor\n\n\n\n\nsection_identifier\ncourse_number\nsemester\nyear\ninstructor\n\n\n\n\nCS101-1\nCS101\nFall\n2023\nDr. Alan Turing\n\n\nMATH201-2\nMATH201\nSpring\n2023\nDr. Katherine Johnson\n\n\nHIST100-3\nHIST100\nFall\n2023\nProf. Howard Zinn\n\n\nCHEM302-1\nCHEM302\nSpring\n2023\nDr. Marie Curie\n\n\n\nGrade Report Table\n\nattribute: student_number, section_identifier, grade\n\n\n\n\nstudent_number\nsection_identifier\ngrade\n\n\n\n\n20240001\nCS101-1\nA\n\n\n20240002\nMATH201-2\nB+\n\n\n20240003\nHIST100-3\nA-\n\n\n20240004\nCHEM302-1\nB"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-1.data_model_basic.html#데이터베이스-database-db",
    "href": "docs/blog/posts/Governance/4-1.data_model_basic.html#데이터베이스-database-db",
    "title": "Data Governance Study - Data Model (2)",
    "section": "",
    "text": "쉽게 말해, 데이터베이스는 데이터 저장소\n어렵게 말해, 일정한 체계 속에 저장된 데이터 및 데이터간 관계의 집합\n\n데이터는 보통 table(표) 단위로 저장됨\nDB는 보통 여러 table을 가지고 있을 수 있음\n\n\n\n\n\n현대 사회는 거의 모든 전기 기기에 DB가 있기때문에 데이터를 효율적으로 관리하려면 DB 시스템이 필요함\n하지만, 모든 상황에 DB가 필요한건 아님\n언제 필요함? 대용량의 데이터를 다룰 때\n예\n\n입금 출금 이체 등 은행 거래 (인터넷 뱅킹 포함) : Data 필요, 대용량 Data, 많은 사용자 (DB 필요)\n호텔 객실 예약 관리는 기록이 필요함 : Data 필요, 대용량 Data, 많은 사용자 (DB 필요)\n신호등의 램프 제어: 소규모 Data 필요 (신호의 상태를 기억하고 있어야 다음 신호 상태를 결정할 수 있음, DB 불필요)\n\n알고리즘으로 관리할 수 있는 프로그램 회로가 기능 대체 가능. 실질적으론 DB가 필요하지 않음\n\n온라인 쇼핑몰에서의 물품 구매: Data 필요, 대용량 Data, 많은 사용자 (DB 필요)\n전자식 개폐장치의 비밀번호 관리: Data 필요 (한명이 한번에 번호를 눌러 정답과 비교. DB가 필요하지 않음 , DB 불필요)\n\n입력받은 번호를 저장해놔야 정답 비밀번호와 비교 가능 하지만 이것 역시 알고리즘 프로그램 회로로 대체 가능\n\n\n\n\n\n\n\n데이터베이스 자체도 여러개 만들 수 있음\n최초 적재 (loading): 서비스를 위한 초기 대량 데이터 적재\n데이터 축적 (Interaction): 이벤트 발생에 따른 잦은 변경\n대용량의 데이터를 다룰때만 DB가 필요\n\n1) 많은 사용자들이 2)원하는 순간 3)데이터에 접근하거나 4)동시 접근 할때 관리 가능\n대용량의 데이터가 체계적으로 조직화되어 있어야함\n원하는 조건에 맞는 데이터에 접근가능\n\n데이터베이스 시스템\n\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nDatabase System\n\n\ncluster_1\n\nDBMS Software\n\n\n\na0\n\nUsers/Programmers\n\n\n\na1\n\nApplication Programs/Queries\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nSoftware to Process\n(Programs Queries)\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nSoftware to Access\n(Stored Data)\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nStored Database Definition\n(Metadata)\n\n\n\na3-&gt;a4\n\n\n\n\n\n\na5\n\nStored Database\n\n\n\na3-&gt;a5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntable에는 row(행)와 column(열) 이 있다.\nrow: table에 대한 체계적인 정보가 set로 구성되어 있는 단위, 개체\ncolumn: table이 갖고 있는 속성\n예: 학생 table\n\nrow: 각 각의 학생 개체 (김철수 개체 , 이영희 개체 등)이 갖고 있는 정보\n\n행 (Row): SQL 등에서 실제로 데이터를 다룰 때 자주 사용되는 용어\n레코드 (Record): 가장 일반적으로 사용되는 용어, 하나의 완전한 데이터 항목을 나타냄\n튜플 (Tuple): 관계형 데이터베이스 이론에서 주로 사용되는 용어, 수학적인 의미에서 속성들의 순서화된 집합을 나타냄\n엔티티 (Entity): 주로 데이터 모델링 단계에서 사용되는 용어, 실세계의 개체나 개념을 나타냄.\n인스턴스 (Instance): 특정 엔티티의 구체적인 예를 나타냄, 객체 지향 개념에서 차용된 용어.\n\ncolumn: 학생(table)이 갖고 있는 속성\n\n열 (Column): SQL 등에서 실제로 데이터를 다룰 때 자주 사용되는 용어, 테이블의 수직적 구성 요소를 지칭\n필드 (Field): 가장 일반적으로 사용되는 용어, 특정 유형의 데이터를 저장하는 테이블의 한 부분을 나타냄\n속성 (Attribute): 데이터 모델링과 관계형 데이터베이스 이론에서 주로 사용되는 용어, 엔티티의 특성이나 성질을 나타냄\n도메인 (Domain): 특정 속성이 가질 수 있는 모든 가능한 값의 집합(or 정의역)을 의미하는데 때로는 열 자체를 지칭하는 데 사용되기도 한다.\n변수 (Variable): 프로그래밍 관점에서 데이터베이스를 다룰 때 사용되는 용어\n\n\n\n\n\n학번\n이름\n전공\n성별\n\n\n\n\n20240001\n김철수\n컴퓨터공학\n남\n\n\n20240002\n이영희\n경영학\n여\n\n\n20240003\n박민준\n생물학\n남\n\n\n20240004\n정수연\n물리학\n여\n\n\n20240005\n강지원\n심리학\n여\n\n\n\n\n\n\n\n\nDB를 다루는 언어\n대표적인 예: SQL\n\n다른 언어에서 SQL문을 string 의 형태로 DB에 접근할수도 있고 따로 SQL을 써서 접근할 수 있음\nSQL 질의를 입력하면 (=SQL 쿼리을 날리면) 쿼리문이 stored database definition에서 DB구조를 파악한 후 Stored DB에 접근해서 쿼리문에 대응되는 원하는 data를 가지고와서 사용자에게 전달\n\nDBMS: DB를 다루는 프로그램\n\nDataBase Management System (DB 관리 시스템)\n사용자가 DB에 접근할 수 있도록 지원해주는 프로그램의 집합 (모듈의 집합)\n\n사용자 ID생성, 권한부여, 보안 관리, 동시성 제어, 사용자의 쿼리문을 처리 등의 모듈의 집합\n\nBusiness 요구사항에 맞는 DBMS를 선택해야함\nDBMS종류: 오라클, MS-SQL, MS-Access, SQLite, MySQL, MariaDB, SQL Server, SQLite, PostgreSQL, etc.\nDBMS 종류마다 시스템이 다르긴 하지만 모두 SQL 로 다룰 수 있다.\nSQL은 국제 표준이 존재하기 때문에 국제 표준을 습득하면 모든 DBMS를 어느 정도 사용할 수 있다. (하지만, 각 DBMS마다 고유의 문법이 있어 약간 씩 다름)\n\n\n\n\n\n\n학적 관리\n\nCourse Table\n\nattribute: course_name, credit_hours, department\n\n\n\n\ncourse_name\ncredit_hours\ndepartment\n\n\n\n\nIntroduction to Computer Science\n3\nComputer Science\n\n\nCalculus I\n4\nMathematics\n\n\nWorld History\n3\nHistory\n\n\nOrganic Chemistry\n4\nChemistry\n\n\n\nStudent Table\n\nattribute: name, student_number\n\n\n\n\nname\nstudent_number\n\n\n\n\nJohn Smith\n20240001\n\n\nEmma Johnson\n20240002\n\n\nMichael Lee\n20240003\n\n\nSophia Chen\n20240004\n\n\n\nSection Table\n\nattribute: section_ideintifier, course_number, semester, year, instructor\n\n\n\n\nsection_identifier\ncourse_number\nsemester\nyear\ninstructor\n\n\n\n\nCS101-1\nCS101\nFall\n2023\nDr. Alan Turing\n\n\nMATH201-2\nMATH201\nSpring\n2023\nDr. Katherine Johnson\n\n\nHIST100-3\nHIST100\nFall\n2023\nProf. Howard Zinn\n\n\nCHEM302-1\nCHEM302\nSpring\n2023\nDr. Marie Curie\n\n\n\nGrade Report Table\n\nattribute: student_number, section_identifier, grade\n\n\n\n\nstudent_number\nsection_identifier\ngrade\n\n\n\n\n20240001\nCS101-1\nA\n\n\n20240002\nMATH201-2\nB+\n\n\n20240003\nHIST100-3\nA-\n\n\n20240004\nCHEM302-1\nB"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-1.data_model_basic.html#도식",
    "href": "docs/blog/posts/Governance/4-1.data_model_basic.html#도식",
    "title": "Data Governance Study - Data Model (1)",
    "section": "2 도식",
    "text": "2 도식\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\n개념적 모델링\n\n\ncluster_1\n\n논리적 모델링\n\n\ncluster_2\n\n물리적 모델링\n\n\n\n엔티티 식별\n\n엔티티 식별\n\n\n\n관계 정의\n\n관계 정의\n\n\n\n엔티티 식별-&gt;관계 정의\n\n\n\n\n\nERD 작성\n\nERD 작성\n\n\n\n관계 정의-&gt;ERD 작성\n\n\n\n\n\n속성 정의\n\n속성 정의\n\n\n\n정규화\n\n정규화\n\n\n\n속성 정의-&gt;정규화\n\n\n\n\n\n키 설정\n\n키 설정\n\n\n\n정규화-&gt;키 설정\n\n\n\n\n\n상세 ERD 작성\n\n상세 ERD 작성\n\n\n\n키 설정-&gt;상세 ERD 작성\n\n\n\n\n\n테이블 정의\n\n테이블 정의\n\n\n\n컬럼 설계\n\n컬럼 설계\n\n\n\n테이블 정의-&gt;컬럼 설계\n\n\n\n\n\n인덱스 생성\n\n인덱스 생성\n\n\n\n컬럼 설계-&gt;인덱스 생성\n\n\n\n\n\nDB 스키마 작성\n\nDB 스키마 작성\n\n\n\n인덱스 생성-&gt;DB 스키마 작성"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-0.data_model.html",
    "href": "docs/blog/posts/Governance/4-0.data_model.html",
    "title": "Data Governance Study - Data Model (1)",
    "section": "",
    "text": "데이터 모델링은 현실 세계의 정보를 컴퓨터 시스템에서 표현할 수 있는 논리적 구조로 변환하는 과정이다.\n이는 데이터 요소, 그들 간의 관계, 그리고 데이터에 대한 제약 조건을 정의한다.\n\n\n\n\n엔티티(Entity): 데이터를 저장할 대상 (예: 고객, 주문)\n속성(Attribute): 엔티티의 특성 (예: 고객명, 주문일자)\n관계(Relationship): 엔티티 간의 연관성\n키(Key): 데이터를 고유하게 식별하는 속성\n제약조건(Constraint): 데이터의 무결성을 보장하는 규칙\n\n\n\n\n\n\n\n목적: 비즈니스 관점에서 높은 수준의 데이터 구조 정의\n특징: 주요 엔티티와 그들 간의 관계를 식별\n산출물: 개체-관계 다이어그램(ERD, Entity Relation Diagram)\n\n\n\n\n\n목적: 비즈니스 규칙과 데이터 요구사항을 상세히 표현\n특징: 엔티티의 속성 정의, 정규화 수행, 키 설정\n산출물: 상세 ERD, 데이터 사전\n\n\n\n\n\n목적: 실제 데이터베이스 구현을 위한 스키마 설계\n특징: 테이블, 컬럼, 인덱스 등의 물리적 구조 정의\n산출물: 데이터베이스 스키마, SQL 스크립트\n\n\n\n\n\n\n데이터 품질 향상: 일관성 있고 정확한 데이터 구조 제공\n시스템 성능 최적화: 효율적인 데이터 접근 및 저장 구조 설계\n유지보수 용이성: 시스템 변경 및 확장이 쉬워짐\n비즈니스 이해 증진: 데이터 관점에서 비즈니스 프로세스 이해\n의사소통 도구: 기술팀과 비즈니스팀 간의 소통 촉진"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-0.data_model.html#data-modeling",
    "href": "docs/blog/posts/Governance/4-0.data_model.html#data-modeling",
    "title": "Data Governance Study - Data Model (1)",
    "section": "",
    "text": "데이터 모델링은 현실 세계의 정보를 컴퓨터 시스템에서 표현할 수 있는 논리적 구조로 변환하는 과정이다.\n이는 데이터 요소, 그들 간의 관계, 그리고 데이터에 대한 제약 조건을 정의한다.\n\n\n\n\n엔티티(Entity): 데이터를 저장할 대상 (예: 고객, 주문)\n속성(Attribute): 엔티티의 특성 (예: 고객명, 주문일자)\n관계(Relationship): 엔티티 간의 연관성\n키(Key): 데이터를 고유하게 식별하는 속성\n제약조건(Constraint): 데이터의 무결성을 보장하는 규칙\n\n\n\n\n\n\n\n목적: 비즈니스 관점에서 높은 수준의 데이터 구조 정의\n특징: 주요 엔티티와 그들 간의 관계를 식별\n산출물: 개체-관계 다이어그램(ERD, Entity Relation Diagram)\n\n\n\n\n\n목적: 비즈니스 규칙과 데이터 요구사항을 상세히 표현\n특징: 엔티티의 속성 정의, 정규화 수행, 키 설정\n산출물: 상세 ERD, 데이터 사전\n\n\n\n\n\n목적: 실제 데이터베이스 구현을 위한 스키마 설계\n특징: 테이블, 컬럼, 인덱스 등의 물리적 구조 정의\n산출물: 데이터베이스 스키마, SQL 스크립트\n\n\n\n\n\n\n데이터 품질 향상: 일관성 있고 정확한 데이터 구조 제공\n시스템 성능 최적화: 효율적인 데이터 접근 및 저장 구조 설계\n유지보수 용이성: 시스템 변경 및 확장이 쉬워짐\n비즈니스 이해 증진: 데이터 관점에서 비즈니스 프로세스 이해\n의사소통 도구: 기술팀과 비즈니스팀 간의 소통 촉진"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-0.data_model.html#도식",
    "href": "docs/blog/posts/Governance/4-0.data_model.html#도식",
    "title": "Data Governance Study - Data Model (1)",
    "section": "2 도식",
    "text": "2 도식\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\n개념적 모델링\n\n\ncluster_1\n\n논리적 모델링\n\n\ncluster_2\n\n물리적 모델링\n\n\n\n엔티티 식별\n\n엔티티 식별\n\n\n\n관계 정의\n\n관계 정의\n\n\n\n엔티티 식별-&gt;관계 정의\n\n\n\n\n\n초안 ERD 작성\n\n초안 ERD 작성\n\n\n\n관계 정의-&gt;초안 ERD 작성\n\n\n\n\n\n속성 정의\n\n속성 정의\n\n\n\n정규화\n\n정규화\n\n\n\n속성 정의-&gt;정규화\n\n\n\n\n\n키 설정\n\n키 설정\n\n\n\n정규화-&gt;키 설정\n\n\n\n\n\n상세 ERD 작성\n\n상세 ERD 작성\n\n\n\n키 설정-&gt;상세 ERD 작성\n\n\n\n\n\n테이블 정의\n\n테이블 정의\n\n\n\n컬럼 설계\n\n컬럼 설계\n\n\n\n테이블 정의-&gt;컬럼 설계\n\n\n\n\n\n인덱스 생성\n\n인덱스 생성\n\n\n\n컬럼 설계-&gt;인덱스 생성\n\n\n\n\n\nDB 스키마 작성\n\nDB 스키마 작성\n\n\n\n인덱스 생성-&gt;DB 스키마 작성\n\n\n\n\n\n\n\n\n\n\n\n2.1 ERD vs Schema\n간단히 말하면, 둘의 차이는 데이터의 개념적, 논리적 모델링에서 그려진 ERD를 물리적 단계에서 DB스키마로 구체화 시킨다.\n\n정의\n\n스키마: 데이터베이스의 구조와 제약 조건을 정의하는 공식적인 설명\nERD: 데이터베이스의 개체(엔티티), 속성, 관계를 시각적으로 표현한 다이어그램\n\n표현 방식\n\n스키마: 주로 텍스트 형식으로 표현되며, SQL 문 등으로 작성된다.\nERD: 그래픽 형식으로 표현되어 시각적 이해를 제공\n\n상세도\n\n스키마: 데이터 타입, 키, 제약 조건 등 세부적인 정보를 포함.\nERD: 전체적인 구조와 관계에 초점을 맞추며, 세부 사항은 생략될 수 있다.\n\n사용 목적\n\n스키마: 데이터베이스 시스템에서 실제 구현을 위해 사용\nERD: 설계 단계에서 개념적 모델링을 위해 주로 사용\n\n시점\n\n스키마: 주로 물리적 데이터베이스 설계 단계에서 사용\nERD: 주로 개념적, 논리적 설계 단계에서 사용"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-2.data_model_sql.html",
    "href": "docs/blog/posts/Governance/4-2.data_model_sql.html",
    "title": "Data Governance Study - Data Model (3)",
    "section": "",
    "text": "SQL은 영문명을 풀이하면 구조를 갖는 질의 언어라는 의미인데 DBMS (Database Management System)를 다루는 언어이다.\nSQL(Structured Query Language)\n\nStructure = DDL(Data Definition Language) + DML (Data Manipulation Language) + DCL(Data Control Language) + TCL (Transaction Control Language)\n\n4가지 모두 명령어들의 집합체 (SQL 함수의 집합체) 4가지의 다른 명령어 집합이 있음\n\n\nSQL은 단순하게 질의만을 수행하는 것이 아니라 데이터베이스의 모든 작업을 통제하는 비절차적(Non-procedural) 언어\n\n다시 말해, 비절차적 언어는 사용자가 “무엇을” 원하는지만 명시하고, “어떻게” 그것을 수행할지는 시스템에 맡기는 프로그래밍 방식이다.\nSQL의 비절차적 특성\n\n사용자는 원하는 결과만 명시 (예: 어떤 데이터를 조회하고 싶은지).\n데이터베이스 시스템이 그 결과를 얻기 위한 최적의 방법을 결정\n\n절차적 언어와의 차이\n\n절차적 언어: 모든 단계를 순서대로 명시해야 함\n비절차적 언어: 원하는 결과만 명시하면 됨\n\n예시: SQL 쿼리 markdown       SELECT * FROM Customers WHERE City = 'London';\n\n사용자: London에 있는 모든 고객 정보를 원한다고만 명시\n시스템: 어떤 인덱스를 사용할지, 어떤 순서로 데이터를 읽을지 등을 결정\n\n장점\n\n사용 편의성: 상세한 기술 지식 없이도 사용 가능\n최적화: 시스템이 최적의 실행 계획을 선택\n높은 생산성: 적은 코드로 복잡한 작업 수행 가능\n\n특징\n\n데이터베이스 작업의 효율성 증대\n사용자는 데이터 자체에 집중 가능\n시스템의 내부 변경에도 쿼리는 그대로 사용 가능\n\n\n독립 실행형 / 내장형\n\n독립 실행형 : SQL 인터페이스를 이용하여 SQL 쿼리를 직접 DBMS에 입력\n내장형\n\nC, C++, Java 등의 프로그래밍 언어에 내장됨\nHost Language + Data sublanguage로 구성됨\n\n\n\n\n\n\n스키마에 대한 명령어 : 스키마를 기술하기 위해 사용되며, 주로 DB 설계자가 사용\n예시 (DBMS마다 문법이 조금씩 다를 수 있다.)\n\n    -- 테이블 생성\n    CREATE TABLE Employees (\n        EmployeeID INT PRIMARY KEY,\n        FirstName VARCHAR(50),\n        LastName VARCHAR(50),\n        Department VARCHAR(50)\n    );\n\n    -- 테이블 구조 변경\n    ALTER TABLE Employees ADD Salary DECIMAL(10, 2);\n\n    -- 테이블 삭제\n    DROP TABLE Employees;\n\n    -- 테이블 이름 변경\n    RENAME TABLE Employees TO Staff;\n\n    -- Oracle에서의 다른 문법\n    ALTER TABLE Employees RENAME TO Staff;\n\n    -- 열 이름 변경 (DBMS에 따라 문법이 다를 수 있음)\n    ALTER TABLE Staff RENAME COLUMN EmployeeID TO StaffID;\n\n    -- MySQL에서 데이터베이스 이름 변경\n    RENAME DATABASE old_db TO new_db;\n\n    -- TRUNCATE는 테이블의 모든 데이터를 빠르게 삭제하는 데 사용\n    -- 테이블 구조는 유지됩\n    \n    -- Employees 테이블의 모든 데이터 삭제\n    TRUNCATE TABLE Employees;\n\n    -- 특정 파티션 truncate (Oracle)\n    TRUNCATE TABLE Sales PARTITION (sales_q1);\n\n    -- 여러 테이블 동시에 truncate (일부 DBMS에서 지원)\n    TRUNCATE TABLE Table1, Table2, Table3;\n\nTRUNCATE의 특징\n\nDELETE보다 빠르게 실행됨.\n일반적으로 롤백할 수 없음\n자동으로 COMMIT 됨\n테이블의 AUTO_INCREMENT 값을 리셋 (DBMS에 따라 다를 수 있음).\n\n주의사항\n\nRENAME과 TRUNCATE는 DDL 명령어이므로 자동 커밋되며, 일반적으로 롤백할 수 없다.\n데이터베이스 시스템에 따라 문법이 조금씩 다를 수 있으므로, 사용 중인 DBMS의 문서를 참조하는 것이 좋다.\n이러한 명령어들은 데이터베이스 구조나 데이터에 큰 영향을 미치므로 신중하게 사용해야 한다.\n\n\n\n\n\n\n주로 instance를 다루는 명령어\n\n데이터의 조회 (Retrieval), 삽입 (Insertion) , 삭제 (Deletion), 갱신(Update)이\n\n예시\n\n    -- 데이터 삽입\n    INSERT INTO Employees (EmployeeID, FirstName, LastName, Department)\n    VALUES (1, 'John', 'Doe', 'IT');\n\n    -- 데이터 조회\n    SELECT * FROM Employees WHERE Department = 'IT';\n\n    -- 데이터 갱신\n    UPDATE Employees SET Salary = 50000 WHERE EmployeeID = 1;\n\n    -- 데이터 삭제\n    DELETE FROM Employees WHERE EmployeeID = 1;\n\n\n\n\n사용자 권한 관리: 데이터베이스에 접근하고 객체들을 사용하도록 권한을 주고 회수하는 명령어들을 말함\n예시\n\n-- 사용자에게 권한 부여\nGRANT SELECT, INSERT ON Employees TO user1;\n\n-- 사용자로부터 권한 회수\nREVOKE INSERT ON Employees FROM user1;\n\n\n\n\n트랜잭션 관리: 논리적인 작업의 단위를 묶어서 DML에 의해 조작된 결과를 작업단위(트랜잭션) 별로 제어하는 명령어를 말함.\nTCL은 특정 상황에서 데이터의 일관성과 무결성을 유지하기 위해 사용\n\n복잡한 작업 수행 시\n\n여러 관련 테이블을 업데이트하는 경우\n예: 은행 계좌 간 송금 (한 계좌에서 출금, 다른 계좌에 입금)\n\n대량의 데이터 처리 시\n\n많은 양의 데이터를 삽입, 수정, 삭제하는 경우\n중간에 오류가 발생하면 부분적 업데이트를 방지하기 위해\n\n데이터 일관성이 중요한 경우:\n\n여러 단계의 작업이 모두 성공해야 의미가 있는 경우\n예: 주문 처리 (재고 확인, 주문 생성, 결제 처리)\n\n오류 복구가 필요한 경우\n\n작업 중 오류 발생 시 이전 상태로 롤백해야 할 때\n\n테스트 및 개발 환경\n\n데이터 변경 사항을 테스트하고 필요 시 롤백할 때\n\n동시성 제어\n\n여러 사용자가 동시에 데이터를 수정할 때 일관성 유지를 위해\n\n배치 처리\n\n대규모 배치 작업에서 전체 작업의 성공 또는 실패를 관리할 때\n\n\n예시\n\n    -- 트랜잭션 시작 (일부 DBMS에서는 명시적으로 시작할 필요가 없을 수 있다)\n    BEGIN TRANSACTION;\n\n    -- 데이터 삽입\n    INSERT INTO Employees (EmployeeID, FirstName, LastName, Salary)\n    VALUES (1, 'John', 'Doe', 50000);\n\n    -- 첫 번째 저장점 생성\n    SAVEPOINT sp1;\n\n    -- 데이터 수정\n    UPDATE Employees SET Salary = 55000 WHERE EmployeeID = 1;\n\n    -- 두 번째 저장점 생성\n    SAVEPOINT sp2;\n\n    -- 데이터 삭제\n    DELETE FROM Employees WHERE EmployeeID = 1;\n\n    -- sp2 저장점까지 롤백 (DELETE 작업만 취소)\n    ROLLBACK TO SAVEPOINT sp2;\n\n    -- 여기서 COMMIT하면 INSERT와 UPDATE 작업이 저장\n    COMMIT;\n\n    -- 새로운 트랜잭션 시작\n    BEGIN TRANSACTION;\n\n    -- 다른 데이터 수정\n    UPDATE Employees SET Salary = 60000 WHERE EmployeeID = 1;\n\n    -- 전체 트랜잭션 롤백 (이 UPDATE 작업 취소)\n    ROLLBACK;\n\n\n\n\n테이블 생성\n-- example: the Student table\nCREATE TABLE Student (\n    student_id INT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL\n);\n-- example: the Course table\nCREATE TABLE Course (\n    course_id INT PRIMARY KEY,\n    course_name VARCHAR(100) NOT NULL\n);\n관계 (relationship) 생성\n-- 수강 테이블 (학생과 과목의 M:N 관계를 표현)\nCREATE TABLE Enrollment (\n    student_id INT,\n    course_id INT,\n    enrollment_date DATE,\n    PRIMARY KEY (student_id, course_id),\n    FOREIGN KEY (student_id) REFERENCES Student(student_id),\n    FOREIGN KEY (course_id) REFERENCES Course(course_id)\n);\n학적 관리\n\nCourse Table\n\nattribute: course_name, credit_hours, department\n\n\n\n\ncourse_name\ncredit_hours\ndepartment\n\n\n\n\nIntroduction to Computer Science\n3\nComputer Science\n\n\nCalculus I\n4\nMathematics\n\n\nWorld History\n3\nHistory\n\n\nOrganic Chemistry\n4\nChemistry\n\n\n\nStudent Table\n\nattribute: name, student_number\n\n\n\n\nname\nstudent_number\n\n\n\n\nJohn Smith\n20240001\n\n\nEmma Johnson\n20240002\n\n\nMichael Lee\n20240003\n\n\nSophia Chen\n20240004\n\n\n\nSection Table\n\nattribute: section_ideintifier, course_number, semester, year, instructor\n\n\n\n\nsection_identifier\ncourse_number\nsemester\nyear\ninstructor\n\n\n\n\nCS101-1\nCS101\nFall\n2023\nDr. Alan Turing\n\n\nMATH201-2\nMATH201\nSpring\n2023\nDr. Katherine Johnson\n\n\nHIST100-3\nHIST100\nFall\n2023\nProf. Howard Zinn\n\n\nCHEM302-1\nCHEM302\nSpring\n2023\nDr. Marie Curie\n\n\n\nGrade Report Table\n\nattribute: student_number, section_identifier, grade\n\n\n\n\nstudent_number\nsection_identifier\ngrade\n\n\n\n\n20240001\nCS101-1\nA\n\n\n20240002\nMATH201-2\nB+\n\n\n20240003\nHIST100-3\nA-\n\n\n20240004\nCHEM302-1\nB\n\n\n\ncalculus 1 course의 section을 하나라도 수강한 학생을 찾으시오\n\n질의의 정보를 각 table을 통해 추적이 가능해야 한다.\n결과적으로 course table &gt;&gt; section table &gt;&gt; grade_report table &gt;&gt; student table 순으로 추적함 (join의 원리)\n\nSELECT DISTINCT s.student_number, s.name\n    FROM Course c\n    JOIN Section sec ON c.course_name = sec.course_number\n    JOIN Grade_Report gr ON sec.section_identifier = gr.section_identifier\n    JOIN Student s ON gr.student_number = s.student_number\n    WHERE c.course_name = 'Calculus I';\nMATH201-2, student 20240001의 grade를 B로 수정하시오\nUPDATE Grade_Report\nSET grade = 'B'\nWHERE section_identifier = 'MATH201-2'\n    AND student_number = '20240001';\n\n위 처럼 table의 추적이 가능하게 하려면 DB 설계를 잘해야한다\n\n설계: table의 정의를 잘 내려야 하고 table들간 관계를 잘 설정 해야한다."
  },
  {
    "objectID": "docs/blog/posts/Governance/4-2.data_model_sql.html#sql-structured-query-language",
    "href": "docs/blog/posts/Governance/4-2.data_model_sql.html#sql-structured-query-language",
    "title": "Data Governance Study - Data Model (3)",
    "section": "",
    "text": "SQL은 영문명을 풀이하면 구조를 갖는 질의 언어라는 의미인데 DBMS (Database Management System)를 다루는 언어이다.\nSQL(Structured Query Language)\n\nStructure = DDL(Data Definition Language) + DML (Data Manipulation Language) + DCL(Data Control Language) + TCL (Transaction Control Language)\n\n4가지 모두 명령어들의 집합체 (SQL 함수의 집합체) 4가지의 다른 명령어 집합이 있음\n\n\nSQL은 단순하게 질의만을 수행하는 것이 아니라 데이터베이스의 모든 작업을 통제하는 비절차적(Non-procedural) 언어\n\n다시 말해, 비절차적 언어는 사용자가 “무엇을” 원하는지만 명시하고, “어떻게” 그것을 수행할지는 시스템에 맡기는 프로그래밍 방식이다.\nSQL의 비절차적 특성\n\n사용자는 원하는 결과만 명시 (예: 어떤 데이터를 조회하고 싶은지).\n데이터베이스 시스템이 그 결과를 얻기 위한 최적의 방법을 결정\n\n절차적 언어와의 차이\n\n절차적 언어: 모든 단계를 순서대로 명시해야 함\n비절차적 언어: 원하는 결과만 명시하면 됨\n\n예시: SQL 쿼리 markdown       SELECT * FROM Customers WHERE City = 'London';\n\n사용자: London에 있는 모든 고객 정보를 원한다고만 명시\n시스템: 어떤 인덱스를 사용할지, 어떤 순서로 데이터를 읽을지 등을 결정\n\n장점\n\n사용 편의성: 상세한 기술 지식 없이도 사용 가능\n최적화: 시스템이 최적의 실행 계획을 선택\n높은 생산성: 적은 코드로 복잡한 작업 수행 가능\n\n특징\n\n데이터베이스 작업의 효율성 증대\n사용자는 데이터 자체에 집중 가능\n시스템의 내부 변경에도 쿼리는 그대로 사용 가능\n\n\n독립 실행형 / 내장형\n\n독립 실행형 : SQL 인터페이스를 이용하여 SQL 쿼리를 직접 DBMS에 입력\n내장형\n\nC, C++, Java 등의 프로그래밍 언어에 내장됨\nHost Language + Data sublanguage로 구성됨\n\n\n\n\n\n\n스키마에 대한 명령어 : 스키마를 기술하기 위해 사용되며, 주로 DB 설계자가 사용\n예시 (DBMS마다 문법이 조금씩 다를 수 있다.)\n\n    -- 테이블 생성\n    CREATE TABLE Employees (\n        EmployeeID INT PRIMARY KEY,\n        FirstName VARCHAR(50),\n        LastName VARCHAR(50),\n        Department VARCHAR(50)\n    );\n\n    -- 테이블 구조 변경\n    ALTER TABLE Employees ADD Salary DECIMAL(10, 2);\n\n    -- 테이블 삭제\n    DROP TABLE Employees;\n\n    -- 테이블 이름 변경\n    RENAME TABLE Employees TO Staff;\n\n    -- Oracle에서의 다른 문법\n    ALTER TABLE Employees RENAME TO Staff;\n\n    -- 열 이름 변경 (DBMS에 따라 문법이 다를 수 있음)\n    ALTER TABLE Staff RENAME COLUMN EmployeeID TO StaffID;\n\n    -- MySQL에서 데이터베이스 이름 변경\n    RENAME DATABASE old_db TO new_db;\n\n    -- TRUNCATE는 테이블의 모든 데이터를 빠르게 삭제하는 데 사용\n    -- 테이블 구조는 유지됩\n    \n    -- Employees 테이블의 모든 데이터 삭제\n    TRUNCATE TABLE Employees;\n\n    -- 특정 파티션 truncate (Oracle)\n    TRUNCATE TABLE Sales PARTITION (sales_q1);\n\n    -- 여러 테이블 동시에 truncate (일부 DBMS에서 지원)\n    TRUNCATE TABLE Table1, Table2, Table3;\n\nTRUNCATE의 특징\n\nDELETE보다 빠르게 실행됨.\n일반적으로 롤백할 수 없음\n자동으로 COMMIT 됨\n테이블의 AUTO_INCREMENT 값을 리셋 (DBMS에 따라 다를 수 있음).\n\n주의사항\n\nRENAME과 TRUNCATE는 DDL 명령어이므로 자동 커밋되며, 일반적으로 롤백할 수 없다.\n데이터베이스 시스템에 따라 문법이 조금씩 다를 수 있으므로, 사용 중인 DBMS의 문서를 참조하는 것이 좋다.\n이러한 명령어들은 데이터베이스 구조나 데이터에 큰 영향을 미치므로 신중하게 사용해야 한다.\n\n\n\n\n\n\n주로 instance를 다루는 명령어\n\n데이터의 조회 (Retrieval), 삽입 (Insertion) , 삭제 (Deletion), 갱신(Update)이\n\n예시\n\n    -- 데이터 삽입\n    INSERT INTO Employees (EmployeeID, FirstName, LastName, Department)\n    VALUES (1, 'John', 'Doe', 'IT');\n\n    -- 데이터 조회\n    SELECT * FROM Employees WHERE Department = 'IT';\n\n    -- 데이터 갱신\n    UPDATE Employees SET Salary = 50000 WHERE EmployeeID = 1;\n\n    -- 데이터 삭제\n    DELETE FROM Employees WHERE EmployeeID = 1;\n\n\n\n\n사용자 권한 관리: 데이터베이스에 접근하고 객체들을 사용하도록 권한을 주고 회수하는 명령어들을 말함\n예시\n\n-- 사용자에게 권한 부여\nGRANT SELECT, INSERT ON Employees TO user1;\n\n-- 사용자로부터 권한 회수\nREVOKE INSERT ON Employees FROM user1;\n\n\n\n\n트랜잭션 관리: 논리적인 작업의 단위를 묶어서 DML에 의해 조작된 결과를 작업단위(트랜잭션) 별로 제어하는 명령어를 말함.\nTCL은 특정 상황에서 데이터의 일관성과 무결성을 유지하기 위해 사용\n\n복잡한 작업 수행 시\n\n여러 관련 테이블을 업데이트하는 경우\n예: 은행 계좌 간 송금 (한 계좌에서 출금, 다른 계좌에 입금)\n\n대량의 데이터 처리 시\n\n많은 양의 데이터를 삽입, 수정, 삭제하는 경우\n중간에 오류가 발생하면 부분적 업데이트를 방지하기 위해\n\n데이터 일관성이 중요한 경우:\n\n여러 단계의 작업이 모두 성공해야 의미가 있는 경우\n예: 주문 처리 (재고 확인, 주문 생성, 결제 처리)\n\n오류 복구가 필요한 경우\n\n작업 중 오류 발생 시 이전 상태로 롤백해야 할 때\n\n테스트 및 개발 환경\n\n데이터 변경 사항을 테스트하고 필요 시 롤백할 때\n\n동시성 제어\n\n여러 사용자가 동시에 데이터를 수정할 때 일관성 유지를 위해\n\n배치 처리\n\n대규모 배치 작업에서 전체 작업의 성공 또는 실패를 관리할 때\n\n\n예시\n\n    -- 트랜잭션 시작 (일부 DBMS에서는 명시적으로 시작할 필요가 없을 수 있다)\n    BEGIN TRANSACTION;\n\n    -- 데이터 삽입\n    INSERT INTO Employees (EmployeeID, FirstName, LastName, Salary)\n    VALUES (1, 'John', 'Doe', 50000);\n\n    -- 첫 번째 저장점 생성\n    SAVEPOINT sp1;\n\n    -- 데이터 수정\n    UPDATE Employees SET Salary = 55000 WHERE EmployeeID = 1;\n\n    -- 두 번째 저장점 생성\n    SAVEPOINT sp2;\n\n    -- 데이터 삭제\n    DELETE FROM Employees WHERE EmployeeID = 1;\n\n    -- sp2 저장점까지 롤백 (DELETE 작업만 취소)\n    ROLLBACK TO SAVEPOINT sp2;\n\n    -- 여기서 COMMIT하면 INSERT와 UPDATE 작업이 저장\n    COMMIT;\n\n    -- 새로운 트랜잭션 시작\n    BEGIN TRANSACTION;\n\n    -- 다른 데이터 수정\n    UPDATE Employees SET Salary = 60000 WHERE EmployeeID = 1;\n\n    -- 전체 트랜잭션 롤백 (이 UPDATE 작업 취소)\n    ROLLBACK;\n\n\n\n\n테이블 생성\n-- example: the Student table\nCREATE TABLE Student (\n    student_id INT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL\n);\n-- example: the Course table\nCREATE TABLE Course (\n    course_id INT PRIMARY KEY,\n    course_name VARCHAR(100) NOT NULL\n);\n관계 (relationship) 생성\n-- 수강 테이블 (학생과 과목의 M:N 관계를 표현)\nCREATE TABLE Enrollment (\n    student_id INT,\n    course_id INT,\n    enrollment_date DATE,\n    PRIMARY KEY (student_id, course_id),\n    FOREIGN KEY (student_id) REFERENCES Student(student_id),\n    FOREIGN KEY (course_id) REFERENCES Course(course_id)\n);\n학적 관리\n\nCourse Table\n\nattribute: course_name, credit_hours, department\n\n\n\n\ncourse_name\ncredit_hours\ndepartment\n\n\n\n\nIntroduction to Computer Science\n3\nComputer Science\n\n\nCalculus I\n4\nMathematics\n\n\nWorld History\n3\nHistory\n\n\nOrganic Chemistry\n4\nChemistry\n\n\n\nStudent Table\n\nattribute: name, student_number\n\n\n\n\nname\nstudent_number\n\n\n\n\nJohn Smith\n20240001\n\n\nEmma Johnson\n20240002\n\n\nMichael Lee\n20240003\n\n\nSophia Chen\n20240004\n\n\n\nSection Table\n\nattribute: section_ideintifier, course_number, semester, year, instructor\n\n\n\n\nsection_identifier\ncourse_number\nsemester\nyear\ninstructor\n\n\n\n\nCS101-1\nCS101\nFall\n2023\nDr. Alan Turing\n\n\nMATH201-2\nMATH201\nSpring\n2023\nDr. Katherine Johnson\n\n\nHIST100-3\nHIST100\nFall\n2023\nProf. Howard Zinn\n\n\nCHEM302-1\nCHEM302\nSpring\n2023\nDr. Marie Curie\n\n\n\nGrade Report Table\n\nattribute: student_number, section_identifier, grade\n\n\n\n\nstudent_number\nsection_identifier\ngrade\n\n\n\n\n20240001\nCS101-1\nA\n\n\n20240002\nMATH201-2\nB+\n\n\n20240003\nHIST100-3\nA-\n\n\n20240004\nCHEM302-1\nB\n\n\n\ncalculus 1 course의 section을 하나라도 수강한 학생을 찾으시오\n\n질의의 정보를 각 table을 통해 추적이 가능해야 한다.\n결과적으로 course table &gt;&gt; section table &gt;&gt; grade_report table &gt;&gt; student table 순으로 추적함 (join의 원리)\n\nSELECT DISTINCT s.student_number, s.name\n    FROM Course c\n    JOIN Section sec ON c.course_name = sec.course_number\n    JOIN Grade_Report gr ON sec.section_identifier = gr.section_identifier\n    JOIN Student s ON gr.student_number = s.student_number\n    WHERE c.course_name = 'Calculus I';\nMATH201-2, student 20240001의 grade를 B로 수정하시오\nUPDATE Grade_Report\nSET grade = 'B'\nWHERE section_identifier = 'MATH201-2'\n    AND student_number = '20240001';\n\n위 처럼 table의 추적이 가능하게 하려면 DB 설계를 잘해야한다\n\n설계: table의 정의를 잘 내려야 하고 table들간 관계를 잘 설정 해야한다."
  },
  {
    "objectID": "docs/blog/posts/Governance/4-3.data_model_schema.html",
    "href": "docs/blog/posts/Governance/4-3.data_model_schema.html",
    "title": "Data Governance Study - Data Model (4)",
    "section": "",
    "text": "Data modeling은 DB Schema를 설계하는 과정이라 할 수 있다.\n\n\n\nDB 구조(table 종류), 데이터 타입, 그리고 제약 조건에 대한 명세 (Specification)\n테이블 (table or relation), 필드 (field), 관계 (relationship), 뷰 (view), 인덱스 등 데이터베이스 객체들의 논리적 구조를 설명\n데이터 베이스 설계 단계에서 명시 되며 자주 변경되지 않음\n실제 데이터베이스 시스템에서 구현될 수 있는 형태로 정의된다.\nSQL DDL(Data Definition Language)로 표현될 수 있다.\n\n테이블 (table or relation) & 필드 (field)\n\n예시\n\n-- example: the Student table\nCREATE TABLE Student (\n    student_id INT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL\n);\n-- example: the Course table\nCREATE TABLE Course (\n    course_id INT PRIMARY KEY,\n    course_name VARCHAR(100) NOT NULL\n);\n관계 (relationship)\n\n외래 키(Foreign Key)를 통한 관계 표현: 주로 외래 키를 사용하여 테이블 간의 관계를 표현\n즉, 한 테이블의 컬럼이 다른 테이블의 기본 키를 참조하도록 설정\n관계 표현 방법\n\nONE-TO-ONE (1:1) 관계\n\n한 엔티티의 하나의 인스턴스가 다른 엔티티의 하나의 인스턴스와 연관된다.\n한 테이블에 다른 테이블의 기본 키를 외래 키로 추가하고, 이를 유니크 제약조건으로 설정한다. -\n예: 한 사람은 하나의 여권을 가진다.\n\nONE-TO-MANY (1:N) 관계\n\n가장 흔한 관계로, 자식 테이블에 부모 테이블의 기본 키를 외래 키로 추가한다.\n한 엔티티의 하나의 인스턴스가 다른 엔티티의 여러 인스턴스와 연관된다.\n예: 한 부서는 여러 직원을 가질 수 있다.\n\nMANY-TO-MANY (M:N) 관계\n\n중간 테이블(연결 테이블)을 생성하여 두 테이블의 관계를 표현한다.\n한 엔티티의 여러 인스턴스가 다른 엔티티의 여러 인스턴스와 연관된다.\n예: 학생들은 여러 과목을 수강하고, 각 과목은 여러 학생들에 의해 수강된다.\n\n\n관계의 특성\n\n선택성(Optionality): 관계가 필수적인지 선택적인지를 나타낸다.\n기수성(Cardinality): 관계에 참여하는 엔티티 인스턴스의 수를 나타낸다.\n\n관계의 역할\n\n각 엔티티가 관계에서 어떤 역할을 하는지 설명할 수 있다.\n예: ‘직원’과 ’부서’ 간의 관계에서 직원은 ‘소속됨’, 부서는 ’고용함’의 역할을 한다.\n\n관계 엔티티\n\n때로는 관계 자체가 속성을 가질 수 있으며, 이를 관계 엔티티로 모델링한다.\n주로 다대다 관계를 해결하기 위해 사용됩니다.\n\n재귀적 관계\n\n한 엔티티가 자기 자신과 관계를 맺는 경우다.\n예: ‘직원’ 엔티티에서 ‘관리자’와 ’부하직원’ 관계\n\n\n -- Course 테이블 생성\nCREATE TABLE Course (\n    course_id VARCHAR(10) PRIMARY KEY,\n    course_name VARCHAR(100) NOT NULL,\n    credit_hours INT NOT NULL,\n    department VARCHAR(50) NOT NULL\n);\n\n-- Student 테이블 생성\nCREATE TABLE Student (\n    student_id VARCHAR(8) PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    date_of_birth DATE,\n    major VARCHAR(50)\n);\n\n-- Section 테이블 생성\nCREATE TABLE Section (\n    section_id VARCHAR(15) PRIMARY KEY,\n    course_id VARCHAR(10) NOT NULL,\n    semester VARCHAR(10) NOT NULL,\n    year INT NOT NULL,\n    instructor VARCHAR(100),\n    FOREIGN KEY (course_id) REFERENCES Course(course_id)\n);\n\n-- Grade_Report 테이블 생성\nCREATE TABLE Grade_Report (\n    student_id VARCHAR(8),\n    section_id VARCHAR(15),\n    grade CHAR(2),\n    PRIMARY KEY (student_id, section_id),\n    FOREIGN KEY (student_id) REFERENCES Student(student_id),\n    FOREIGN KEY (section_id) REFERENCES Section(section_id)\n);\n\n-- 수강 테이블 (학생과 과목의 M:N 관계를 표현)\nCREATE TABLE Enrollment (\n    student_id INT,\n    course_id INT,\n    enrollment_date DATE,\n    PRIMARY KEY (student_id, course_id),\n    FOREIGN KEY (student_id) REFERENCES Student(student_id),\n    FOREIGN KEY (course_id) REFERENCES Course(course_id)\n);\n\n\n\n\n\n\nDB schema를 만드는 과정을 Data Modeling 또는 DB design이라고 부름 (늬앙스 차이가있음)\n\nData Modeling\n\n현실 세계의 데이터를 추상화하여 컴퓨터 세계의 데이터로 표현하는 과정\n특징 (데이터 거버넌스와 상대적으로 더 밀접한 관련이 있음)\n\n주로 개념적, 논리적 수준에서 이루어짐\n더 추상적이고 개념적인 수준\n비즈니스 요구사항을 데이터 구조로 변환\nERD(Entity-Relationship Diagram) 등을 사용하여 시각화\n\n결과물: ERD, 개념적/논리적 데이터 모델\n단계: 프로젝트 초기 단계\n\n개념적 모델링: 핵심 엔티티와 관계 식별\n논리적 모델링: 속성 정의, 정규화 수행\n\n목적\n\n비즈니스 프로세스와 규칙을 이해하고 표현\n데이터의 구조와 관계를 명확히 정의\n\n\nDB Design\n\n데이터 모델을 실제 데이터베이스 구조로 변환하는 과정\n특징\n\n논리적, 물리적 수준에서 이루어짐\n더 구체적이고 구현에 가까운 수준\n특정 DBMS와 하드웨어 환경을 고려\nSQL DDL 등을 사용하여 실제 스키마 생성\n\n결과물: 데이터베이스 스키마, 테이블 정의, 인덱스 구조 등\n단계: Data Modeling 이후, 실제 구현 전 단계\n\n논리적 설계: 데이터 모델을 DBMS에 독립적인 형태로 변환\n물리적 설계: 특정 DBMS에 맞는 물리적 구조 설계 (인덱스, 파티션 등)\n\n목적\n\n효율적인 데이터 저장, 접근, 관리 구조 설계\n성능, 보안, 확장성 등을 고려한 최적화\n\n\n\n학계에서는 엄격하게 구별지으나 실무나 industry에서는 Data Modeling 이나 DB Design을 혼용해서 씀\n\n편의상 이 블로그에선 Data Modeling = DB Design로 설정\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n데이터베이스의 구조를 시각적으로 표현한 다이어그램이다.\nEntity(개체), 속성 (Attribute), 관계(Relationship)를 그래픽 요소로 나타낸다.\nEntity(개체)\n\n현실 세계에서 독립적으로 존재하는 객체나 개념을 나타냄\n데이터 모델링 단계에서 사용되는 추상적인 개념\n특징\n\n보통 명사로 표현 (예: 학생, 강의, 교수)\n속성(Attribute)을 가진다\nERD에서 사각형으로 표현된다.\n일반적으로 ERD의 엔티티는 데이터베이스 설계 과정에서 테이블로 변환된다.\n하나의 엔티티는 대부분 하나의 테이블로 매핑된다.\n\n\n관계(Relationship)\n\n엔티티(Entity) 간의 연관성이나 상호작용을 나타내고 Schema의 관계도를\n두 개 이상의 엔티티 간의 논리적 연결을 표현\n표현 방식: 일반적으로 선으로 연결되며, 관계의 특성을 나타내는 기호나 레이블이 추가됨\n\n예시\n\n\n\nERD Example - Stack overflow by snowflake\n\n\nlink\n\n\n\n\n\n특정 시점에 DB에 실제로 저장되어 있는 데이터로 자주 변동된다."
  },
  {
    "objectID": "docs/blog/posts/Governance/4-3.data_model_schema.html#schemas-vs-erd-vs-instances",
    "href": "docs/blog/posts/Governance/4-3.data_model_schema.html#schemas-vs-erd-vs-instances",
    "title": "Data Governance Study - Data Model (3)",
    "section": "",
    "text": "DB 구조(table 종류), 데이터 타입, 그리고 제약 조건에 대한 명세\n테이블 (table or relation), 필드 (field), 관계 (relationship), 뷰 (view), 인덱스 등 데이터베이스 객체들의 논리적 구조를 설명\n데이터 베이스 설계 단계에서 명시 되며 자주 변경되지 않음\n실제 데이터베이스 시스템에서 구현될 수 있는 형태로 정의된다.\nSQL DDL(Data Definition Language)로 표현될 수 있다.\n\n테이블 (table or relation) & 필드 (field)\n\n예시\n\n-- example: the Student table\nCREATE TABLE Student (\n    student_id INT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL\n);\n-- example: the Course table\nCREATE TABLE Course (\n    course_id INT PRIMARY KEY,\n    course_name VARCHAR(100) NOT NULL\n);\n관계 (relationship)\n\n외래 키(Foreign Key)를 통한 관계 표현: 주로 외래 키를 사용하여 테이블 간의 관계를 표현\n즉, 한 테이블의 컬럼이 다른 테이블의 기본 키를 참조하도록 설정\n관계 표현 방법\n\nONE-TO-MANY (1:N) 관계: 가장 흔한 관계로, 자식 테이블에 부모 테이블의 기본 키를 외래 키로 추가한다.\nONE-TO-ONE (1:1) 관계: 한 테이블에 다른 테이블의 기본 키를 외래 키로 추가하고, 이를 유니크 제약조건으로 설정한다.\nMANY-TO-MANY (M:N) 관계: 중간 테이블(연결 테이블)을 생성하여 두 테이블의 관계를 표현한다.\n\n-- 수강 테이블 (학생과 과목의 M:N 관계를 표현)\nCREATE TABLE Enrollment (\nstudent_id INT,\ncourse_id INT,\nenrollment_date DATE,\nPRIMARY KEY (student_id, course_id),\nFOREIGN KEY (student_id) REFERENCES Student(student_id),\nFOREIGN KEY (course_id) REFERENCES Course(course_id)\n);\n\n\n\n\n\n```makrdown\n-- Course 테이블 생성\nCREATE TABLE Course (\n    course_id VARCHAR(10) PRIMARY KEY,\n    course_name VARCHAR(100) NOT NULL,\n    credit_hours INT NOT NULL,\n    department VARCHAR(50) NOT NULL\n);\n\n-- Student 테이블 생성\nCREATE TABLE Student (\n    student_number VARCHAR(8) PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    date_of_birth DATE,\n    major VARCHAR(50)\n);\n\n-- Section 테이블 생성\nCREATE TABLE Section (\n    section_id VARCHAR(15) PRIMARY KEY,\n    course_id VARCHAR(10) NOT NULL,\n    semester VARCHAR(10) NOT NULL,\n    year INT NOT NULL,\n    instructor VARCHAR(100),\n    FOREIGN KEY (course_id) REFERENCES Course(course_id)\n);\n\n-- Grade_Report 테이블 생성\nCREATE TABLE Grade_Report (\n    student_number VARCHAR(8),\n    section_id VARCHAR(15),\n    grade CHAR(2),\n    PRIMARY KEY (student_number, section_id),\n    FOREIGN KEY (student_number) REFERENCES Student(student_number),\n    FOREIGN KEY (section_id) REFERENCES Section(section_id)\n);\n```\n\n\n\n\nDB schema를 만드는 과정을 Data Modeling 또는 DB design이라고 부름 (늬앙스 차이가있음)\n\nData Modeling\n\n현실 세계의 데이터를 추상화하여 컴퓨터 세계의 데이터로 표현하는 과정\n특징 (데이터 거버넌스와 상대적으로 더 밀접한 관련이 있음)\n\n주로 개념적, 논리적 수준에서 이루어짐\n더 추상적이고 개념적인 수준\n비즈니스 요구사항을 데이터 구조로 변환\nERD(Entity-Relationship Diagram) 등을 사용하여 시각화\n\n결과물: ERD, 개념적/논리적 데이터 모델\n단계: 프로젝트 초기 단계\n\n개념적 모델링: 핵심 엔티티와 관계 식별\n논리적 모델링: 속성 정의, 정규화 수행\n\n목적\n\n비즈니스 프로세스와 규칙을 이해하고 표현\n데이터의 구조와 관계를 명확히 정의\n\n\nDB Design\n\n데이터 모델을 실제 데이터베이스 구조로 변환하는 과정\n특징\n\n논리적, 물리적 수준에서 이루어짐\n더 구체적이고 구현에 가까운 수준\n특정 DBMS와 하드웨어 환경을 고려\nSQL DDL 등을 사용하여 실제 스키마 생성\n\n결과물: 데이터베이스 스키마, 테이블 정의, 인덱스 구조 등\n단계: Data Modeling 이후, 실제 구현 전 단계\n\n논리적 설계: 데이터 모델을 DBMS에 독립적인 형태로 변환\n물리적 설계: 특정 DBMS에 맞는 물리적 구조 설계 (인덱스, 파티션 등)\n\n목적\n\n효율적인 데이터 저장, 접근, 관리 구조 설계\n성능, 보안, 확장성 등을 고려한 최적화\n\n\n\n\n\n\n\n\n\n데이터베이스의 구조를 시각적으로 표현한 다이어그램이다.\nEntity(개체), 속성 (Attribute), 관계(Relationship)를 그래픽 요소로 나타낸다.\nEntity(개체)\n\n현실 세계에서 독립적으로 존재하는 객체나 개념을 나타냄\n데이터 모델링 단계에서 사용되는 추상적인 개념\n특징\n\n보통 명사로 표현 (예: 학생, 강의, 교수)\n속성(Attribute)을 가진다\nERD에서 사각형으로 표현된다.\n일반적으로 ERD의 엔티티는 데이터베이스 설계 과정에서 테이블로 변환된다.\n하나의 엔티티는 대부분 하나의 테이블로 매핑된다.\n\n\n관계(Relationship)\n\n엔티티(Entity) 간의 연관성이나 상호작용을 나타낸다.\n두 개 이상의 엔티티 간의 논리적 연결을 표현\n표현 방식: 일반적으로 선으로 연결되며, 관계의 특성을 나타내는 기호나 레이블이 추가됨\n주요 관계 유형\n\n일대일 (One-to-One, 1:1)\n\n한 엔티티의 하나의 인스턴스가 다른 엔티티의 하나의 인스턴스와 연관된다.\n예: 한 사람은 하나의 여권을 가진다.\n\n일대다 (One-to-Many, 1:N)\n\n한 엔티티의 하나의 인스턴스가 다른 엔티티의 여러 인스턴스와 연관된다.\n예: 한 부서는 여러 직원을 가질 수 있다.\n\n다대다 (Many-to-Many, M:N)\n\n한 엔티티의 여러 인스턴스가 다른 엔티티의 여러 인스턴스와 연관된다.\n예: 학생들은 여러 과목을 수강하고, 각 과목은 여러 학생들에 의해 수강된다.\n\n\n관계의 특성\n\n선택성(Optionality): 관계가 필수적인지 선택적인지를 나타낸다.\n기수성(Cardinality): 관계에 참여하는 엔티티 인스턴스의 수를 나타낸다.\n\n관계의 역할\n\n각 엔티티가 관계에서 어떤 역할을 하는지 설명할 수 있다.\n예: ‘직원’과 ’부서’ 간의 관계에서 직원은 ‘소속됨’, 부서는 ’고용함’의 역할을 한다.\n\n관계 엔티티\n\n때로는 관계 자체가 속성을 가질 수 있으며, 이를 관계 엔티티로 모델링한다.\n주로 다대다 관계를 해결하기 위해 사용됩니다.\n\n재귀적 관계\n\n한 엔티티가 자기 자신과 관계를 맺는 경우다.\n예: ‘직원’ 엔티티에서 ‘관리자’와 ’부하직원’ 관계\n\n\n예시\n\n\n\nERD Example - Stack overflow by snowflake\n\n\nlink\n\n\n\n\n\n특정 시점에 DB에 실제로 저장되어 있는 데이터로 자주 변동된다."
  },
  {
    "objectID": "docs/blog/posts/Governance/4-3.data_model_schema.html#data-modeling",
    "href": "docs/blog/posts/Governance/4-3.data_model_schema.html#data-modeling",
    "title": "Data Governance Study - Data Model (4)",
    "section": "",
    "text": "Data modeling은 DB Schema를 설계하는 과정이라 할 수 있다.\n\n\n\nDB 구조(table 종류), 데이터 타입, 그리고 제약 조건에 대한 명세 (Specification)\n테이블 (table or relation), 필드 (field), 관계 (relationship), 뷰 (view), 인덱스 등 데이터베이스 객체들의 논리적 구조를 설명\n데이터 베이스 설계 단계에서 명시 되며 자주 변경되지 않음\n실제 데이터베이스 시스템에서 구현될 수 있는 형태로 정의된다.\nSQL DDL(Data Definition Language)로 표현될 수 있다.\n\n테이블 (table or relation) & 필드 (field)\n\n예시\n\n-- example: the Student table\nCREATE TABLE Student (\n    student_id INT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL\n);\n-- example: the Course table\nCREATE TABLE Course (\n    course_id INT PRIMARY KEY,\n    course_name VARCHAR(100) NOT NULL\n);\n관계 (relationship)\n\n외래 키(Foreign Key)를 통한 관계 표현: 주로 외래 키를 사용하여 테이블 간의 관계를 표현\n즉, 한 테이블의 컬럼이 다른 테이블의 기본 키를 참조하도록 설정\n관계 표현 방법\n\nONE-TO-ONE (1:1) 관계\n\n한 엔티티의 하나의 인스턴스가 다른 엔티티의 하나의 인스턴스와 연관된다.\n한 테이블에 다른 테이블의 기본 키를 외래 키로 추가하고, 이를 유니크 제약조건으로 설정한다. -\n예: 한 사람은 하나의 여권을 가진다.\n\nONE-TO-MANY (1:N) 관계\n\n가장 흔한 관계로, 자식 테이블에 부모 테이블의 기본 키를 외래 키로 추가한다.\n한 엔티티의 하나의 인스턴스가 다른 엔티티의 여러 인스턴스와 연관된다.\n예: 한 부서는 여러 직원을 가질 수 있다.\n\nMANY-TO-MANY (M:N) 관계\n\n중간 테이블(연결 테이블)을 생성하여 두 테이블의 관계를 표현한다.\n한 엔티티의 여러 인스턴스가 다른 엔티티의 여러 인스턴스와 연관된다.\n예: 학생들은 여러 과목을 수강하고, 각 과목은 여러 학생들에 의해 수강된다.\n\n\n관계의 특성\n\n선택성(Optionality): 관계가 필수적인지 선택적인지를 나타낸다.\n기수성(Cardinality): 관계에 참여하는 엔티티 인스턴스의 수를 나타낸다.\n\n관계의 역할\n\n각 엔티티가 관계에서 어떤 역할을 하는지 설명할 수 있다.\n예: ‘직원’과 ’부서’ 간의 관계에서 직원은 ‘소속됨’, 부서는 ’고용함’의 역할을 한다.\n\n관계 엔티티\n\n때로는 관계 자체가 속성을 가질 수 있으며, 이를 관계 엔티티로 모델링한다.\n주로 다대다 관계를 해결하기 위해 사용됩니다.\n\n재귀적 관계\n\n한 엔티티가 자기 자신과 관계를 맺는 경우다.\n예: ‘직원’ 엔티티에서 ‘관리자’와 ’부하직원’ 관계\n\n\n -- Course 테이블 생성\nCREATE TABLE Course (\n    course_id VARCHAR(10) PRIMARY KEY,\n    course_name VARCHAR(100) NOT NULL,\n    credit_hours INT NOT NULL,\n    department VARCHAR(50) NOT NULL\n);\n\n-- Student 테이블 생성\nCREATE TABLE Student (\n    student_id VARCHAR(8) PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    date_of_birth DATE,\n    major VARCHAR(50)\n);\n\n-- Section 테이블 생성\nCREATE TABLE Section (\n    section_id VARCHAR(15) PRIMARY KEY,\n    course_id VARCHAR(10) NOT NULL,\n    semester VARCHAR(10) NOT NULL,\n    year INT NOT NULL,\n    instructor VARCHAR(100),\n    FOREIGN KEY (course_id) REFERENCES Course(course_id)\n);\n\n-- Grade_Report 테이블 생성\nCREATE TABLE Grade_Report (\n    student_id VARCHAR(8),\n    section_id VARCHAR(15),\n    grade CHAR(2),\n    PRIMARY KEY (student_id, section_id),\n    FOREIGN KEY (student_id) REFERENCES Student(student_id),\n    FOREIGN KEY (section_id) REFERENCES Section(section_id)\n);\n\n-- 수강 테이블 (학생과 과목의 M:N 관계를 표현)\nCREATE TABLE Enrollment (\n    student_id INT,\n    course_id INT,\n    enrollment_date DATE,\n    PRIMARY KEY (student_id, course_id),\n    FOREIGN KEY (student_id) REFERENCES Student(student_id),\n    FOREIGN KEY (course_id) REFERENCES Course(course_id)\n);\n\n\n\n\n\n\nDB schema를 만드는 과정을 Data Modeling 또는 DB design이라고 부름 (늬앙스 차이가있음)\n\nData Modeling\n\n현실 세계의 데이터를 추상화하여 컴퓨터 세계의 데이터로 표현하는 과정\n특징 (데이터 거버넌스와 상대적으로 더 밀접한 관련이 있음)\n\n주로 개념적, 논리적 수준에서 이루어짐\n더 추상적이고 개념적인 수준\n비즈니스 요구사항을 데이터 구조로 변환\nERD(Entity-Relationship Diagram) 등을 사용하여 시각화\n\n결과물: ERD, 개념적/논리적 데이터 모델\n단계: 프로젝트 초기 단계\n\n개념적 모델링: 핵심 엔티티와 관계 식별\n논리적 모델링: 속성 정의, 정규화 수행\n\n목적\n\n비즈니스 프로세스와 규칙을 이해하고 표현\n데이터의 구조와 관계를 명확히 정의\n\n\nDB Design\n\n데이터 모델을 실제 데이터베이스 구조로 변환하는 과정\n특징\n\n논리적, 물리적 수준에서 이루어짐\n더 구체적이고 구현에 가까운 수준\n특정 DBMS와 하드웨어 환경을 고려\nSQL DDL 등을 사용하여 실제 스키마 생성\n\n결과물: 데이터베이스 스키마, 테이블 정의, 인덱스 구조 등\n단계: Data Modeling 이후, 실제 구현 전 단계\n\n논리적 설계: 데이터 모델을 DBMS에 독립적인 형태로 변환\n물리적 설계: 특정 DBMS에 맞는 물리적 구조 설계 (인덱스, 파티션 등)\n\n목적\n\n효율적인 데이터 저장, 접근, 관리 구조 설계\n성능, 보안, 확장성 등을 고려한 최적화\n\n\n\n학계에서는 엄격하게 구별지으나 실무나 industry에서는 Data Modeling 이나 DB Design을 혼용해서 씀\n\n편의상 이 블로그에선 Data Modeling = DB Design로 설정\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n데이터베이스의 구조를 시각적으로 표현한 다이어그램이다.\nEntity(개체), 속성 (Attribute), 관계(Relationship)를 그래픽 요소로 나타낸다.\nEntity(개체)\n\n현실 세계에서 독립적으로 존재하는 객체나 개념을 나타냄\n데이터 모델링 단계에서 사용되는 추상적인 개념\n특징\n\n보통 명사로 표현 (예: 학생, 강의, 교수)\n속성(Attribute)을 가진다\nERD에서 사각형으로 표현된다.\n일반적으로 ERD의 엔티티는 데이터베이스 설계 과정에서 테이블로 변환된다.\n하나의 엔티티는 대부분 하나의 테이블로 매핑된다.\n\n\n관계(Relationship)\n\n엔티티(Entity) 간의 연관성이나 상호작용을 나타내고 Schema의 관계도를\n두 개 이상의 엔티티 간의 논리적 연결을 표현\n표현 방식: 일반적으로 선으로 연결되며, 관계의 특성을 나타내는 기호나 레이블이 추가됨\n\n예시\n\n\n\nERD Example - Stack overflow by snowflake\n\n\nlink\n\n\n\n\n\n특정 시점에 DB에 실제로 저장되어 있는 데이터로 자주 변동된다."
  },
  {
    "objectID": "docs/blog/posts/Governance/4-4.data_model_requirements.html",
    "href": "docs/blog/posts/Governance/4-4.data_model_requirements.html",
    "title": "Data Governance Study - Data Model (5)",
    "section": "",
    "text": "DB 설계는 각 설계자마다 다르게 설계하므로 science가 아니라 art라고 부른다. 후에 게시되는 블로그를 읽어보면 주관적인 판단이 들어가는 요소가 많이 있음을 파악할 수 있다.\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\n보통 데이터 거버넌스를 비롯한 데이터 모델링을 실패하는 이유는 너무 광범위한 범위를 설정하여 비지니스를 구체화 및 명시화하지 못하기 떄문이라고 한다 (많은 article과 papers가 있으니 각자가 찾아서 참고하시기 바람).\n단기 및 중장기적인 기간에 가시적인 산출물을 낼 수 있는 범위로 줄여 best practice를 제시하여 많은 유저들을 확보하는 것이 프로젝트 지속성을 증가시킬 수 있는 tip이다.\n\n\n\n\n\n업무기술서 작성\n\n명확한 목적 설정\n\n업무기술서의 목적과 범위를 명확히 정의한다.\n누구를 위한 문서인지, 어떤 정보를 담아야 하는지 파악한다.\n\n이해관계자 식별 및 참여\n\n관련된 모든 이해관계자를 식별하고 참여시킨다.\n업무 담당자, 관리자, 개발자 등 다양한 관점을 포함한다.\n\n구조화된 형식 사용\n\n일관된 템플릿이나 형식을 사용한다.\n섹션별로 명확히 구분하여 정보를 구조화한다.\n\n상세한 업무 프로세스 기술\n\n사실상, 이 단계에서 데이터 모델링 가능/여부가 결정된다.\nData Platform을 처음 도입하는 많은 회사들이 본인들의 업무 절차의 표준화가 되어 있지 않기 떄문에 메뉴얼화 할 수 없고 업무 프로세스를 표준화를 할 수 없다.\n회사의 규모가 클 수록 업무 프로세스가 표준화되는데도 많은 시간이 소요된다.\n범위를 줄여 현실적으로 실행가능한 항목들을 추려내야한다.\n업무 흐름을 단계별로 상세히 기술한다.\n각 단계에서 사용되는 데이터를 명확히 식별한다.\n\n데이터 요소 정의\n\n각 업무 프로세스에서 사용되는 데이터 요소를 정의한다.\n데이터의 의미, 형식, 제약조건 등을 명시한다\n\n비즈니스 규칙 명시\n\n데이터와 관련된 모든 비즈니스 규칙을 명확히 기술한다.\n데이터 무결성, 계산 규칙, 제약조건 등을 포함한다.\n\n용어 정의\n\n업무 도메인의 주요 용어를 정의하고 일관되게 사용한다.\n용어집을 만들어 문서에 포함시킨다.\n\n다이어그램 활용\n\n업무 흐름도, 개념적 데이터 모델 등을 사용하여 시각화한다.\n복잡한 관계나 프로세스를 이해하기 쉽게 표현한다.\n\n실제 업무 사례나 데이터 예시를 포함하여 이해를 돕는다.\n이해관계자들의 검토를 받고 피드백을 반영한다.\n여러 차례의 반복적인 검토와 수정을 거친다.\n문서 전체에 걸쳐 용어, 형식, 스타일의 일관성을 유지한다.\n문서의 버전을 관리하고 변경 이력을 기록한다.\n문서를 쉽게 이해하고 참조할 수 있도록 구성한다.\n목차, 색인 등을 포함하여 필요한 정보를 빠르게 찾을 수 있게 합니다.\n향후 발생할 수 있는 업무 변화나 확장을 고려하여 작성한다.\n필요한 경우 데이터 보안 및 규정 준수 요구사항을 포함한다.\n\n예시\n\n수강신청 시스템 업무기술서\n\n목적 본 문서는 대학교 수강신청 시스템의 데이터베이스 설계를 위한 업무 요구사항을 기술합니다.\n시스템 개요 수강신청 시스템은 학생들이 학기별로 수강할 과목을 선택하고 등록할 수 있게 하며, 교수와 관리자가 관련 정보를 관리할 수 있도록 합니다.\n주요 엔티티: 학생, 교수, 과목, 수강신청, 학과, 강의실\n업무 프로세스\n\n수강신청 프로세스\n\n학생이 시스템에 로그인합니다.\n학생이 해당 학기의 개설 과목 목록을 조회합니다.\n학생이 희망 과목을 선택하고 수강신청을 합니다.\n시스템은 수강 가능 여부(선수과목, 정원 등)를 확인합니다.\n신청이 승인되면 수강신청 내역에 추가됩니다.\n학생은 수강신청 내역을 조회하고 필요시 변경/취소할 수 있습니다.\n\n과목 관리 프로세스\n\n교수가 새로운 과목을 제안합니다.\n관리자가 과목 정보를 검토하고 승인합니다.\n승인된 과목은 개설 과목 목록에 추가됩니다.\n교수는 자신의 과목에 대한 정보(강의계획서 등)를 업데이트할 수 있습니다.\n\n\n데이터 요구사항\n\n학생 정보\n\n학번 (기본키)\n이름\n학과\n학년\n연락처\n이메일\n\n과목 정보\n\n과목코드 (기본키)\n과목명\n담당교수\n학점\n수강정원\n강의시간\n강의실\n선수과목 (있는 경우)\n\n수강신청 정보\n\n수강신청ID (기본키)\n학생ID (외래키)\n과목코드 (외래키)\n신청일시\n상태 (신청완료, 대기, 취소 등)\n\n\n비즈니스 규칙\n\n학생은 한 학기에 최대 21학점까지 신청할 수 있습니다.\n동일 시간대에 중복된 과목 신청은 불가능합니다.\n수강정원이 초과된 경우, 대기 목록에 등록됩니다.\n선수과목이 있는 경우, 해당 과목 이수 여부를 확인해야 합니다.\n수강신청 변경 및 취소는 지정된 기간 내에만 가능합니다.\n\n보고서 요구사항\n\n학생별 수강신청 내역\n과목별 수강신청 학생 목록\n학과별 개설 과목 통계\n시간대별 강의실 사용 현황\n\n인터페이스 요구사항\n\n학사관리 시스템과의 연동 (학생 정보, 성적 정보 등)\n모바일 앱을 통한 수강신청 지원\n\n보안 요구사항\n\n사용자 인증 및 권한 관리\n개인정보 보호를 위한 암호화\n데이터 접근 로그 관리"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-4.data_model_requirements.html#db-design",
    "href": "docs/blog/posts/Governance/4-4.data_model_requirements.html#db-design",
    "title": "Data Governance Study - Data Model (5)",
    "section": "",
    "text": "DB 설계는 각 설계자마다 다르게 설계하므로 science가 아니라 art라고 부른다. 후에 게시되는 블로그를 읽어보면 주관적인 판단이 들어가는 요소가 많이 있음을 파악할 수 있다.\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\n보통 데이터 거버넌스를 비롯한 데이터 모델링을 실패하는 이유는 너무 광범위한 범위를 설정하여 비지니스를 구체화 및 명시화하지 못하기 떄문이라고 한다 (많은 article과 papers가 있으니 각자가 찾아서 참고하시기 바람).\n단기 및 중장기적인 기간에 가시적인 산출물을 낼 수 있는 범위로 줄여 best practice를 제시하여 많은 유저들을 확보하는 것이 프로젝트 지속성을 증가시킬 수 있는 tip이다.\n\n\n\n\n\n업무기술서 작성\n\n명확한 목적 설정\n\n업무기술서의 목적과 범위를 명확히 정의한다.\n누구를 위한 문서인지, 어떤 정보를 담아야 하는지 파악한다.\n\n이해관계자 식별 및 참여\n\n관련된 모든 이해관계자를 식별하고 참여시킨다.\n업무 담당자, 관리자, 개발자 등 다양한 관점을 포함한다.\n\n구조화된 형식 사용\n\n일관된 템플릿이나 형식을 사용한다.\n섹션별로 명확히 구분하여 정보를 구조화한다.\n\n상세한 업무 프로세스 기술\n\n사실상, 이 단계에서 데이터 모델링 가능/여부가 결정된다.\nData Platform을 처음 도입하는 많은 회사들이 본인들의 업무 절차의 표준화가 되어 있지 않기 떄문에 메뉴얼화 할 수 없고 업무 프로세스를 표준화를 할 수 없다.\n회사의 규모가 클 수록 업무 프로세스가 표준화되는데도 많은 시간이 소요된다.\n범위를 줄여 현실적으로 실행가능한 항목들을 추려내야한다.\n업무 흐름을 단계별로 상세히 기술한다.\n각 단계에서 사용되는 데이터를 명확히 식별한다.\n\n데이터 요소 정의\n\n각 업무 프로세스에서 사용되는 데이터 요소를 정의한다.\n데이터의 의미, 형식, 제약조건 등을 명시한다\n\n비즈니스 규칙 명시\n\n데이터와 관련된 모든 비즈니스 규칙을 명확히 기술한다.\n데이터 무결성, 계산 규칙, 제약조건 등을 포함한다.\n\n용어 정의\n\n업무 도메인의 주요 용어를 정의하고 일관되게 사용한다.\n용어집을 만들어 문서에 포함시킨다.\n\n다이어그램 활용\n\n업무 흐름도, 개념적 데이터 모델 등을 사용하여 시각화한다.\n복잡한 관계나 프로세스를 이해하기 쉽게 표현한다.\n\n실제 업무 사례나 데이터 예시를 포함하여 이해를 돕는다.\n이해관계자들의 검토를 받고 피드백을 반영한다.\n여러 차례의 반복적인 검토와 수정을 거친다.\n문서 전체에 걸쳐 용어, 형식, 스타일의 일관성을 유지한다.\n문서의 버전을 관리하고 변경 이력을 기록한다.\n문서를 쉽게 이해하고 참조할 수 있도록 구성한다.\n목차, 색인 등을 포함하여 필요한 정보를 빠르게 찾을 수 있게 합니다.\n향후 발생할 수 있는 업무 변화나 확장을 고려하여 작성한다.\n필요한 경우 데이터 보안 및 규정 준수 요구사항을 포함한다.\n\n예시\n\n수강신청 시스템 업무기술서\n\n목적 본 문서는 대학교 수강신청 시스템의 데이터베이스 설계를 위한 업무 요구사항을 기술합니다.\n시스템 개요 수강신청 시스템은 학생들이 학기별로 수강할 과목을 선택하고 등록할 수 있게 하며, 교수와 관리자가 관련 정보를 관리할 수 있도록 합니다.\n주요 엔티티: 학생, 교수, 과목, 수강신청, 학과, 강의실\n업무 프로세스\n\n수강신청 프로세스\n\n학생이 시스템에 로그인합니다.\n학생이 해당 학기의 개설 과목 목록을 조회합니다.\n학생이 희망 과목을 선택하고 수강신청을 합니다.\n시스템은 수강 가능 여부(선수과목, 정원 등)를 확인합니다.\n신청이 승인되면 수강신청 내역에 추가됩니다.\n학생은 수강신청 내역을 조회하고 필요시 변경/취소할 수 있습니다.\n\n과목 관리 프로세스\n\n교수가 새로운 과목을 제안합니다.\n관리자가 과목 정보를 검토하고 승인합니다.\n승인된 과목은 개설 과목 목록에 추가됩니다.\n교수는 자신의 과목에 대한 정보(강의계획서 등)를 업데이트할 수 있습니다.\n\n\n데이터 요구사항\n\n학생 정보\n\n학번 (기본키)\n이름\n학과\n학년\n연락처\n이메일\n\n과목 정보\n\n과목코드 (기본키)\n과목명\n담당교수\n학점\n수강정원\n강의시간\n강의실\n선수과목 (있는 경우)\n\n수강신청 정보\n\n수강신청ID (기본키)\n학생ID (외래키)\n과목코드 (외래키)\n신청일시\n상태 (신청완료, 대기, 취소 등)\n\n\n비즈니스 규칙\n\n학생은 한 학기에 최대 21학점까지 신청할 수 있습니다.\n동일 시간대에 중복된 과목 신청은 불가능합니다.\n수강정원이 초과된 경우, 대기 목록에 등록됩니다.\n선수과목이 있는 경우, 해당 과목 이수 여부를 확인해야 합니다.\n수강신청 변경 및 취소는 지정된 기간 내에만 가능합니다.\n\n보고서 요구사항\n\n학생별 수강신청 내역\n과목별 수강신청 학생 목록\n학과별 개설 과목 통계\n시간대별 강의실 사용 현황\n\n인터페이스 요구사항\n\n학사관리 시스템과의 연동 (학생 정보, 성적 정보 등)\n모바일 앱을 통한 수강신청 지원\n\n보안 요구사항\n\n사용자 인증 및 권한 관리\n개인정보 보호를 위한 암호화\n데이터 접근 로그 관리"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5.data_model_conceptual.html",
    "href": "docs/blog/posts/Governance/4-5.data_model_conceptual.html",
    "title": "Data Governance Study - Data Model (5)",
    "section": "",
    "text": "G\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling(개념적 데이터 모델링)은 데이터베이스 설계의 초기 단계\n비즈니스 요구사항 (업무기술서의 내용)을 이해하기 좋게 개념화 또는 추상화 (i.e. 도식화)하여 high level (고수준)의 데이터 구조를 정의하는 과정이다.\n\nhigh level은 사람이 이해할 수 있는 수준이란 뜻이며 low level은 컴퓨터가 이해할 수 있는 수준이라고 생각하면 편하다.\n\n\n\n\n\n\n\n\n\nG\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0-&gt;a1\n\n\n\n\na2\n\nSubject\n\n\n\na1-&gt;a2\n\n\n\n\na3\n\nLecture\n\n\n\na2-&gt;a3\n\n\n\n\na4\n\nProfessor\n\n\n\na3-&gt;a4\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling은 복잡한 비즈니스 요구사항을 단순화하여 표현하므로, 프로젝트의 초기 단계에서 이해관계자 간의 합의를 도출하는 데 매우 유용하다.\n후속 데이터베이스 설계 단계의 기반이 되어 전체 프로젝트의 성공에 중요한 역할을 한다.\n\n\n\n\n비즈니스 관점에서 데이터 구조를 이해하고 표현\n주요 엔티티와 그들 간의 관계를 식별\n시스템의 범위를 정의\n\n\n\n\n\n엔티티(Entity): 비즈니스에서 중요한 객체나 개념\n속성(Attribute): 엔티티의 특성 (이 단계에서는 상세하게 다루지 않을 수 있음)\n관계(Relationship): 엔티티 간의 연관성\n\nRelationship (관계) \\(\\ne\\) Relation\nRelationship 은 [Student] - &lt; Register &gt; - [Subject] 에서 &lt; Register &gt;에 해당, 한글로 관계로 표시\n도식에서 마름모에 해당\nRelation = Table (테이블), 보통 relation을 한글로 표현할 때 릴레이션 으로 표시 (관계라고 표시안함)\n\n수학자들은 테이블의 한 행을 relation 이라 부름\n즉, 관계형 데이터 모델의 수학적, 논리적 개념이고 튜플(tuple)의 집합이며 속성(attribute)의 집합으로 구성된다.\n테이블의 추상적인 개념으로 테이블은 Relation을 기술하는 하나의 구체적 표현\nDB에서 하나의 table로 구현된다.\n\n\n\n\n\n\n\n기술적 세부사항을 배제하고 비즈니스 개념에 집중\n높은 수준의 추상화\nDBMS에 독립적\n\n\n\n\n\n간단한 텍스트 설명이나 다이어그램\n주로 Entity-Relationship Diagram (ERD)을 사용\n\n개념적 설계에 가장 많이 쓰는 모델로서 Entity-Relationship Model (ERM or ER-model 개체 관계 모형)을 사용하고 그 산출물이 ERD이다.\n\n\n\n\n\n\n개체 (Entity)\n\n실 세계에 존재하는 의미있는 하나의 정보 단위\n표현: 사각형으로 표시\n물리적 개체 뿐 아니라 추상적(개념적) 개체도 포함\n\n물리적 개체: (학생, 자동차, 강의실, 등)\n추상적 개체 : (프로젝트, 직업, 교과목)\n\n개체는 둥근 직사각형으로 표시\n\n관계 (Relationship)\n\n개체들 사이의 연관성\n\n학생과 교과목 사이의 수강 관계\n표현: 마름모로 표시, 선으로 관련 엔티티에 연결, ex) [Student] - &lt; Register &gt; -[Subject]\n\n실제로는 개체와 관계를 구분짓기 매우 힘듦 \\(\\rightarrow\\) ER modeling 할때 의미가 없어짐 (깊게 생각하지 말것)\n\nex) 결혼을 개체로 둘건지 관계로 둘건지 애매\n\n[결혼] - &lt;진행&gt; - [예식장] vs [남자] - &lt;결혼&gt; - &lt;여자&gt;\n\n\n관계는 마름모로 표시\n\n속성 (attribute) (=필드명)\n\n개체 또는 관계의 본질적 특성이나 성질\n그러므로 instance는 속성들의 값의 집합\n표현: 타원형으로 표시, 선으로 엔티티에 연결\n예시\n\n학생(개체)이 가지는 속성은 학번, 혈액형, 나이, 핸폰 번호, 성별, 학년 등이 있음\n과목(개체)이 가지는 속성은 학점(credit), 시간(hour), 부서(department), 장소(location) 등이 있음\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0--a1\n\n\n\n\na3\n\nage\n\n\n\na0--a3\n\n\n\n\na4\n\nstudent_id\n\n\n\na0--a4\n\n\n\n\na5\n\nsex\n\n\n\na0--a5\n\n\n\n\na2\n\nSubject\n\n\n\na1--a2\n\n\n\n\na6\n\ncredit\n\n\n\na2--a6\n\n\n\n\na7\n\nhour\n\n\n\na2--a7\n\n\n\n\na8\n\ndepartment\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\n개체나 관계에서 파생되는 수많은 속성을 나열하고 명확하게 분리하는 것은 어려움, why?\n\n다음 개체 및 관계에서 주어진 속성의 주인(Owner)은?\n\n개체: [학생], [교과목]\n관계: &lt;수강&gt;\n속성: (성별), (나이), (과목), (학점), (평점), (이수구분)\n\n[학생]: (성별), (나이)\n\n(성별), (나이) 는 비교적 명확하게 [학생] 개체에 대응되는 속성이다\n\n[교과목] : (과목), (학점)\n\n(과목명), (학점) 는 비교적 명확하게 [교과목] 개체에 대응되는 속성이다\n\n개체 기준으로 (평점), (이수구분) 속성은 구분짓기 애매함\n\n[학생]이 (평점) 속성을 갖게 되면 학생 A에게 평점을 물어볼경우 대답을 할 수가 없음\n왜냐하면, 여러 과목에 대한 평점이 존재하기 때문에 어떤 교과목에 대한 평점을 얘기해야하는지 모름.\n즉, (평점)은 [학생]의 고유 속성이 아님.\n반대로, (평점) 속성의 주인이 [교과목] 개체라 가정할 경우, 교과목에 평점을 물어보면 학생이 몇 십명이기 때문에 어떤 학생의 평점을 얘기해야하는지 애매해짐.\n즉 (평점)은 [교과목]의 고유 속성이 아님\n\n\n(평점)과 (이수구분) 과 같은 애매한 속성은 관계로 구분 지으면 해결될 경우가 있음!\n\n관계: 개체 사이에 관계를 맺어주는 이벤트 또는 함수\n&lt;수강&gt;: 학생이 교과목을 수강한 이벤트\n\n(평점) : 학생 1명이 과목 1개를 수강하여 평점을 산출\n(이수구분): 학생 1명이 과목 1개를 수강하여 이수여부 산출\n\n&lt;수강&gt;: (평점), (이수구분)\n사실, (평점)과 (이수구분)과 같이 관계에 의하여 파생되는 속성은 해당 배경지식이 없는 외부인이라면 파악하기 매우 힘듦 (업무기술서가 명확히 적혀 있어야 해결 가능).\n\n\n\n\n\n\n\n주요 비즈니스 개체 식별\n개체 간 관계 정의\n높은 수준의 속성 식별 (선택적)\n비즈니스 규칙 반영\n논리적 데이터 모델링의 기초가 됨 (더 상세한 데이터 모델로 발전)"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5.data_model_conceptual.html#db-design",
    "href": "docs/blog/posts/Governance/4-5.data_model_conceptual.html#db-design",
    "title": "Data Governance Study - Data Model (5)",
    "section": "",
    "text": "G\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling(개념적 데이터 모델링)은 데이터베이스 설계의 초기 단계\n비즈니스 요구사항 (업무기술서의 내용)을 이해하기 좋게 개념화 또는 추상화 (i.e. 도식화)하여 high level (고수준)의 데이터 구조를 정의하는 과정이다.\n\nhigh level은 사람이 이해할 수 있는 수준이란 뜻이며 low level은 컴퓨터가 이해할 수 있는 수준이라고 생각하면 편하다.\n\n\n\n\n\n\n\n\n\nG\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0-&gt;a1\n\n\n\n\na2\n\nSubject\n\n\n\na1-&gt;a2\n\n\n\n\na3\n\nLecture\n\n\n\na2-&gt;a3\n\n\n\n\na4\n\nProfessor\n\n\n\na3-&gt;a4\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling은 복잡한 비즈니스 요구사항을 단순화하여 표현하므로, 프로젝트의 초기 단계에서 이해관계자 간의 합의를 도출하는 데 매우 유용하다.\n후속 데이터베이스 설계 단계의 기반이 되어 전체 프로젝트의 성공에 중요한 역할을 한다.\n\n\n\n\n비즈니스 관점에서 데이터 구조를 이해하고 표현\n주요 엔티티와 그들 간의 관계를 식별\n시스템의 범위를 정의\n\n\n\n\n\n엔티티(Entity): 비즈니스에서 중요한 객체나 개념\n속성(Attribute): 엔티티의 특성 (이 단계에서는 상세하게 다루지 않을 수 있음)\n관계(Relationship): 엔티티 간의 연관성\n\nRelationship (관계) \\(\\ne\\) Relation\nRelationship 은 [Student] - &lt; Register &gt; - [Subject] 에서 &lt; Register &gt;에 해당, 한글로 관계로 표시\n도식에서 마름모에 해당\nRelation = Table (테이블), 보통 relation을 한글로 표현할 때 릴레이션 으로 표시 (관계라고 표시안함)\n\n수학자들은 테이블의 한 행을 relation 이라 부름\n즉, 관계형 데이터 모델의 수학적, 논리적 개념이고 튜플(tuple)의 집합이며 속성(attribute)의 집합으로 구성된다.\n테이블의 추상적인 개념으로 테이블은 Relation을 기술하는 하나의 구체적 표현\nDB에서 하나의 table로 구현된다.\n\n\n\n\n\n\n\n기술적 세부사항을 배제하고 비즈니스 개념에 집중\n높은 수준의 추상화\nDBMS에 독립적\n\n\n\n\n\n간단한 텍스트 설명이나 다이어그램\n주로 Entity-Relationship Diagram (ERD)을 사용\n\n개념적 설계에 가장 많이 쓰는 모델로서 Entity-Relationship Model (ERM or ER-model 개체 관계 모형)을 사용하고 그 산출물이 ERD이다.\n\n\n\n\n\n\n개체 (Entity)\n\n실 세계에 존재하는 의미있는 하나의 정보 단위\n표현: 사각형으로 표시\n물리적 개체 뿐 아니라 추상적(개념적) 개체도 포함\n\n물리적 개체: (학생, 자동차, 강의실, 등)\n추상적 개체 : (프로젝트, 직업, 교과목)\n\n개체는 둥근 직사각형으로 표시\n\n관계 (Relationship)\n\n개체들 사이의 연관성\n\n학생과 교과목 사이의 수강 관계\n표현: 마름모로 표시, 선으로 관련 엔티티에 연결, ex) [Student] - &lt; Register &gt; -[Subject]\n\n실제로는 개체와 관계를 구분짓기 매우 힘듦 \\(\\rightarrow\\) ER modeling 할때 의미가 없어짐 (깊게 생각하지 말것)\n\nex) 결혼을 개체로 둘건지 관계로 둘건지 애매\n\n[결혼] - &lt;진행&gt; - [예식장] vs [남자] - &lt;결혼&gt; - &lt;여자&gt;\n\n\n관계는 마름모로 표시\n\n속성 (attribute) (=필드명)\n\n개체 또는 관계의 본질적 특성이나 성질\n그러므로 instance는 속성들의 값의 집합\n표현: 타원형으로 표시, 선으로 엔티티에 연결\n예시\n\n학생(개체)이 가지는 속성은 학번, 혈액형, 나이, 핸폰 번호, 성별, 학년 등이 있음\n과목(개체)이 가지는 속성은 학점(credit), 시간(hour), 부서(department), 장소(location) 등이 있음\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0--a1\n\n\n\n\na3\n\nage\n\n\n\na0--a3\n\n\n\n\na4\n\nstudent_id\n\n\n\na0--a4\n\n\n\n\na5\n\nsex\n\n\n\na0--a5\n\n\n\n\na2\n\nSubject\n\n\n\na1--a2\n\n\n\n\na6\n\ncredit\n\n\n\na2--a6\n\n\n\n\na7\n\nhour\n\n\n\na2--a7\n\n\n\n\na8\n\ndepartment\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\n개체나 관계에서 파생되는 수많은 속성을 나열하고 명확하게 분리하는 것은 어려움, why?\n\n다음 개체 및 관계에서 주어진 속성의 주인(Owner)은?\n\n개체: [학생], [교과목]\n관계: &lt;수강&gt;\n속성: (성별), (나이), (과목), (학점), (평점), (이수구분)\n\n[학생]: (성별), (나이)\n\n(성별), (나이) 는 비교적 명확하게 [학생] 개체에 대응되는 속성이다\n\n[교과목] : (과목), (학점)\n\n(과목명), (학점) 는 비교적 명확하게 [교과목] 개체에 대응되는 속성이다\n\n개체 기준으로 (평점), (이수구분) 속성은 구분짓기 애매함\n\n[학생]이 (평점) 속성을 갖게 되면 학생 A에게 평점을 물어볼경우 대답을 할 수가 없음\n왜냐하면, 여러 과목에 대한 평점이 존재하기 때문에 어떤 교과목에 대한 평점을 얘기해야하는지 모름.\n즉, (평점)은 [학생]의 고유 속성이 아님.\n반대로, (평점) 속성의 주인이 [교과목] 개체라 가정할 경우, 교과목에 평점을 물어보면 학생이 몇 십명이기 때문에 어떤 학생의 평점을 얘기해야하는지 애매해짐.\n즉 (평점)은 [교과목]의 고유 속성이 아님\n\n\n(평점)과 (이수구분) 과 같은 애매한 속성은 관계로 구분 지으면 해결될 경우가 있음!\n\n관계: 개체 사이에 관계를 맺어주는 이벤트 또는 함수\n&lt;수강&gt;: 학생이 교과목을 수강한 이벤트\n\n(평점) : 학생 1명이 과목 1개를 수강하여 평점을 산출\n(이수구분): 학생 1명이 과목 1개를 수강하여 이수여부 산출\n\n&lt;수강&gt;: (평점), (이수구분)\n사실, (평점)과 (이수구분)과 같이 관계에 의하여 파생되는 속성은 해당 배경지식이 없는 외부인이라면 파악하기 매우 힘듦 (업무기술서가 명확히 적혀 있어야 해결 가능).\n\n\n\n\n\n\n\n주요 비즈니스 개체 식별\n개체 간 관계 정의\n높은 수준의 속성 식별 (선택적)\n비즈니스 규칙 반영\n논리적 데이터 모델링의 기초가 됨 (더 상세한 데이터 모델로 발전)"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5_0..data_model_conceptual.html",
    "href": "docs/blog/posts/Governance/4-5_0..data_model_conceptual.html",
    "title": "Data Governance Study - Data Model (5)",
    "section": "",
    "text": "DB 설계는 각 설계자마다 다르게 설계하므로 science가 아니라 art라고 부른다. 후에 게시되는 블로그를 읽어보면 주관적인 판단이 들어가는 요소가 많이 있음을 파악할 수 있다.\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling(개념적 데이터 모델링)은 데이터베이스 설계의 초기 단계\n비즈니스 요구사항 (업무기술서의 내용)을 이해하기 좋게 개념화 또는 추상화 (i.e. 도식화)하여 high level (고수준)의 데이터 구조를 정의하는 과정이다.\n\nhigh level은 사람이 이해할 수 있는 수준이란 뜻이며 low level은 컴퓨터가 이해할 수 있는 수준이라고 생각하면 편하다.\n\n\n\n\n\n\n\n\n\nG\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0-&gt;a1\n\n\n\n\na2\n\nSubject\n\n\n\na1-&gt;a2\n\n\n\n\na3\n\nLecture\n\n\n\na2-&gt;a3\n\n\n\n\na4\n\nProfessor\n\n\n\na3-&gt;a4\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling은 복잡한 비즈니스 요구사항을 단순화하여 표현하므로, 프로젝트의 초기 단계에서 이해관계자 간의 합의를 도출하는 데 매우 유용하다.\n후속 데이터베이스 설계 단계의 기반이 되어 전체 프로젝트의 성공에 중요한 역할을 한다.\n\n\n\n\n비즈니스 관점에서 데이터 구조를 이해하고 표현\n주요 엔티티와 그들 간의 관계를 식별\n시스템의 범위를 정의\n\n\n\n\n\n엔티티(Entity): 비즈니스에서 중요한 객체나 개념\n속성(Attribute): 엔티티의 특성 (이 단계에서는 상세하게 다루지 않을 수 있음)\n관계(Relationship): 엔티티 간의 연관성\n\nRelationship (관계) \\(\\ne\\) Relation\nRelationship 은 [Student] - &lt; Register &gt; - [Subject] 에서 &lt; Register &gt;에 해당, 한글로 관계로 표시\n도식에서 마름모에 해당\nRelation = Table (테이블), 보통 relation을 한글로 표현할 때 릴레이션 으로 표시 (관계라고 표시안함)\n\n수학자들은 테이블의 한 행을 relation 이라 부름\n즉, 관계형 데이터 모델의 수학적, 논리적 개념이고 튜플(tuple)의 집합이며 속성(attribute)의 집합으로 구성된다.\n테이블의 추상적인 개념으로 테이블은 Relation을 기술하는 하나의 구체적 표현\nDB에서 하나의 table로 구현된다.\n\n\n\n\n\n\n\n기술적 세부사항을 배제하고 비즈니스 개념에 집중\n높은 수준의 추상화\nDBMS에 독립적\n\n\n\n\n\n간단한 텍스트 설명이나 다이어그램\n주로 Entity-Relationship Diagram (ERD)을 사용\n\n개념적 설계에 가장 많이 쓰는 모델로서 Entity-Relationship Model (ERM or ER-model 개체 관계 모형)을 사용하고 그 산출물이 ERD이다.\n\n\n\n\n\n\n주요 비즈니스 개체 식별\n개체 간 관계 정의\n높은 수준의 속성 식별 (선택적)\n비즈니스 규칙 반영\n논리적 데이터 모델링의 기초가 됨 (더 상세한 데이터 모델로 발전)"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5_0..data_model_conceptual.html#db-design",
    "href": "docs/blog/posts/Governance/4-5_0..data_model_conceptual.html#db-design",
    "title": "Data Governance Study - Data Model (5)",
    "section": "",
    "text": "DB 설계는 각 설계자마다 다르게 설계하므로 science가 아니라 art라고 부른다. 후에 게시되는 블로그를 읽어보면 주관적인 판단이 들어가는 요소가 많이 있음을 파악할 수 있다.\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling(개념적 데이터 모델링)은 데이터베이스 설계의 초기 단계\n비즈니스 요구사항 (업무기술서의 내용)을 이해하기 좋게 개념화 또는 추상화 (i.e. 도식화)하여 high level (고수준)의 데이터 구조를 정의하는 과정이다.\n\nhigh level은 사람이 이해할 수 있는 수준이란 뜻이며 low level은 컴퓨터가 이해할 수 있는 수준이라고 생각하면 편하다.\n\n\n\n\n\n\n\n\n\nG\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0-&gt;a1\n\n\n\n\na2\n\nSubject\n\n\n\na1-&gt;a2\n\n\n\n\na3\n\nLecture\n\n\n\na2-&gt;a3\n\n\n\n\na4\n\nProfessor\n\n\n\na3-&gt;a4\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling은 복잡한 비즈니스 요구사항을 단순화하여 표현하므로, 프로젝트의 초기 단계에서 이해관계자 간의 합의를 도출하는 데 매우 유용하다.\n후속 데이터베이스 설계 단계의 기반이 되어 전체 프로젝트의 성공에 중요한 역할을 한다.\n\n\n\n\n비즈니스 관점에서 데이터 구조를 이해하고 표현\n주요 엔티티와 그들 간의 관계를 식별\n시스템의 범위를 정의\n\n\n\n\n\n엔티티(Entity): 비즈니스에서 중요한 객체나 개념\n속성(Attribute): 엔티티의 특성 (이 단계에서는 상세하게 다루지 않을 수 있음)\n관계(Relationship): 엔티티 간의 연관성\n\nRelationship (관계) \\(\\ne\\) Relation\nRelationship 은 [Student] - &lt; Register &gt; - [Subject] 에서 &lt; Register &gt;에 해당, 한글로 관계로 표시\n도식에서 마름모에 해당\nRelation = Table (테이블), 보통 relation을 한글로 표현할 때 릴레이션 으로 표시 (관계라고 표시안함)\n\n수학자들은 테이블의 한 행을 relation 이라 부름\n즉, 관계형 데이터 모델의 수학적, 논리적 개념이고 튜플(tuple)의 집합이며 속성(attribute)의 집합으로 구성된다.\n테이블의 추상적인 개념으로 테이블은 Relation을 기술하는 하나의 구체적 표현\nDB에서 하나의 table로 구현된다.\n\n\n\n\n\n\n\n기술적 세부사항을 배제하고 비즈니스 개념에 집중\n높은 수준의 추상화\nDBMS에 독립적\n\n\n\n\n\n간단한 텍스트 설명이나 다이어그램\n주로 Entity-Relationship Diagram (ERD)을 사용\n\n개념적 설계에 가장 많이 쓰는 모델로서 Entity-Relationship Model (ERM or ER-model 개체 관계 모형)을 사용하고 그 산출물이 ERD이다.\n\n\n\n\n\n\n주요 비즈니스 개체 식별\n개체 간 관계 정의\n높은 수준의 속성 식별 (선택적)\n비즈니스 규칙 반영\n논리적 데이터 모델링의 기초가 됨 (더 상세한 데이터 모델로 발전)"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5_1.data_model_conceptual_ER_model.html",
    "href": "docs/blog/posts/Governance/4-5_1.data_model_conceptual_ER_model.html",
    "title": "Data Governance Study - Data Model (6)",
    "section": "",
    "text": "G\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n실 세계에 존재하는 의미있는 하나의 정보 단위\n표현: 사각형으로 표시\n물리적 개체 뿐 아니라 추상적(개념적) 개체도 포함\n\n물리적 개체: (학생, 자동차, 강의실, 등)\n추상적 개체 : (프로젝트, 직업, 교과목)\n\n개체는 둥근 직사각형으로 표시\n\n\n\n\n\n개체들 사이의 연관성\n\n학생과 교과목 사이의 수강 관계\n표현: 마름모로 표시, 선으로 관련 엔티티에 연결, ex) [Student] - &lt; Register &gt; -[Subject]\n\n실제로는 개체와 관계를 구분짓기 매우 힘듦 \\(\\rightarrow\\) ER modeling 할때 의미가 없어짐 (깊게 생각하지 말것)\n\nex) 결혼을 개체로 둘건지 관계로 둘건지 애매\n\n[결혼] - &lt;진행&gt; - [예식장] vs [남자] - &lt;결혼&gt; - &lt;여자&gt;\n\n\n관계는 마름모로 표시\n\n\n\n\n\n개체 또는 관계의 본질적 특성이나 성질\n그러므로 instance는 속성들의 값의 집합\n표현: 타원형으로 표시, 선으로 엔티티에 연결\n예시\n\n학생(개체)이 가지는 속성은 학번, 혈액형, 나이, 핸폰 번호, 성별, 학년 등이 있음\n과목(개체)이 가지는 속성은 학점(credit), 시간(hour), 부서(department), 장소(location) 등이 있음\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0--a1\n\n\n\n\na3\n\nage\n\n\n\na0--a3\n\n\n\n\na4\n\nstudent_id\n\n\n\na0--a4\n\n\n\n\na5\n\nsex\n\n\n\na0--a5\n\n\n\n\na2\n\nSubject\n\n\n\na1--a2\n\n\n\n\na6\n\ncredit\n\n\n\na2--a6\n\n\n\n\na7\n\nhour\n\n\n\na2--a7\n\n\n\n\na8\n\ndepartment\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\n개체나 관계에서 파생되는 수많은 속성을 나열하고 명확하게 분리하는 것은 어려움, why?\n\n다음 개체 및 관계에서 주어진 속성의 주인(Owner)은?\n\n개체: [학생], [교과목]\n관계: &lt;수강&gt;\n속성: (성별), (나이), (과목), (학점), (평점), (이수구분)\n\n[학생]: (성별), (나이)\n\n(성별), (나이) 는 비교적 명확하게 [학생] 개체에 대응되는 속성이다\n\n[교과목] : (과목), (학점)\n\n(과목명), (학점) 는 비교적 명확하게 [교과목] 개체에 대응되는 속성이다\n\n개체 기준으로 (평점), (이수구분) 속성은 구분짓기 애매함\n\n[학생]이 (평점) 속성을 갖게 되면 학생 A에게 평점을 물어볼경우 대답을 할 수가 없음\n왜냐하면, 여러 과목에 대한 평점이 존재하기 때문에 어떤 교과목에 대한 평점을 얘기해야하는지 모름.\n즉, (평점)은 [학생]의 고유 속성이 아님.\n반대로, (평점) 속성의 주인이 [교과목] 개체라 가정할 경우, 교과목에 평점을 물어보면 학생이 몇 십명이기 때문에 어떤 학생의 평점을 얘기해야하는지 애매해짐.\n즉 (평점)은 [교과목]의 고유 속성이 아님\n\n\n(평점)과 (이수구분) 과 같은 애매한 속성은 관계로 구분 지으면 해결될 경우가 있음!\n\n관계: 개체 사이에 관계를 맺어주는 이벤트 또는 함수\n&lt;수강&gt;: 학생이 교과목을 수강한 이벤트\n\n(평점) : 학생 1명이 과목 1개를 수강하여 평점을 산출\n(이수구분): 학생 1명이 과목 1개를 수강하여 이수여부 산출\n\n&lt;수강&gt;: (평점), (이수구분)\n사실, (평점)과 (이수구분)과 같이 관계에 의하여 파생되는 속성은 해당 배경지식이 없는 외부인이라면 파악하기 매우 힘듦 (업무기술서가 명확히 적혀 있어야 해결 가능).\n\n\n\n\n\n\n속성의 유형을 여러 기준으로 분류할 수 있음\nMulti-valued (다중값 속성)\n\n하나의 엔티티 인스턴스가 여러 값을 가질 수 있는 속성\n표기: single valued (동그라미 1개로 표기) vs Multi-valued (동그라미 2개로 표기)\n예)\n\n(나이)는 한 시점에 여러 개의 값을 가질 수 없음\n(취미)는 한 시점에 여러 개의 값을 가질 수 있음\nCAR(자동차) ERD\n\ncolor가 multi-valued가 2개인 이유\n\n차 한대에 여러 색상이 들어갈 수 있기 때문에\n\n싱글 value가 multi value보다 훨 씬 많음\n\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nCar\n\n\n\na1\n\nyear\n\n\n\na0--a1\n\n\n\n\na2\n\nregistration\n\n\n\na0--a2\n\n\n\n\na3\n\nvehicle_id\n\n\n\na0--a3\n\n\n\n\na4\n\nmodel\n\n\n\na0--a4\n\n\n\n\na5\n\nmake\n\n\n\na0--a5\n\n\n\n\na6\n\n\ncolor\n\n\n\na0--a6\n\n\n\n\na7\n\nstate\n\n\n\na2--a7\n\n\n\n\na8\n\nnumber\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\ncomposite attribute(복합 속성)\n\nsimple attribute : 더이상 쪼개지지 않는 원자값을 갖는 속성\n\nex) 나이, 학번\n\ncomposite attribute\n\n여러 하위 속성으로 구성된 속성\n즉, 몇 개의 요소로 분해 될 수 있는 속성, 쪼개어 져도 의미를 갖을 수 있어야함\nex) 주소: 시 + 군 + 구 + 번지\n\nsimple 와 composite attribute를 구분짓는 기준은 나라의 사회 제도나 단체의 시스템의 특성에 따라 변한다\n\nex) 한국은 우편 수집 창고와 우체국의 시스템에 따라 주소를 시 + 군 + 구 + 번지 또는 시 + 군 + 구 가 필요로 할 수 있다\n하지만 다른 나라는 주소 전체를 쓰는 시스템이라면 주소 속성이 simple attribute 로 남을 수 있다.\n이름의 경우 성과 이름을 가르는 시스템이 필요한 미국과 full name을 사용하는 한국의 시스템의 경우 각 상황에 맞게 modeling을 해야한다.\n어떤 속성이든 분해해야할 용도가 있다면 쪼개야한다.\n\n위의 그림 예시에서\n\ncomposite attribute: registration (자동차 번호판)\nsimple attribute: state(주이름) & number(자동차 고유번호)\n\n\nderived vs stored attributes\n\nderived attribute (점선으로 표기) : 저장된 다른 데이터로부터 유도 가능한 속성\n\n총 학생 수: 그냥 instance나 학생 수를 카운트 하면 됨, 총 학생 수라는 속성은 없어도 됨\n각 과목의 성적 : 총점(derived), 평점(derived)\n주민등록번호: 나이(derived), 생일(derived)\nderived attribute는 자주 쓰이는 통계치를 구할 때 자주 쓰임. DB에 저장할 필요는 없다.\n\nstored attribute: 위에서 말한 총 학생 수, 총점, 평점, 나이, 생일과 같은 derived attribute의 연산이 너무 무겁거나 너무 빈번하게 사용되는 상황이라면 DB에 data를 적재하여 연산량을 줄이는 방법 도 있다.\n\n설계자의 재량에 따라 stored와 derived의 구분 짓는다.\n실무에서는 파생 변수(derived)가 자주 쓰인다면 stored (실선)로 남긴다\n학계에서는 derived는 가급적 점선으로 표시하여 derived상태로 남긴다\n\n\nKey Attributes\n\nkey attributes: 유일성 + 최소성 을 만족시켜야함\n\n어떤 개체에 대해서 그 인스턴스가 항상 유일한 값을 갖는 속성 또는 속성들의 집합\n\n중복되는 값을 가지면 안됨\n키 속성은 밑줄을 그어 표시\nex) 학생의 학번, 책의 ISBN, 차량번호\n\n\n특정 스냅샷이 아닌 해당 개체의 모든 가능한 스냅샷의 집합을 고려하여 파악되어야함 (개체가 아무리 많아 지더라도 항상 유일한 값을 가져야함)\n\nex) 다음의 ssn, 이름, 혈액형 중 키 속성은 ssn\n\ncomposite key(복합키)\n\nentity에서 키 속성자체가 없을 경우 attributes의 조합으로도 생성가능\n복합키는 최소성을 가져야함 : 최소한의 attributes로 복합키를 만들어야함\n\n용어 정리\n\nconceptual design: key는 identifier라고 부르고\nlogical design: key는 primary key라고 부름.\n\ntable을 만들게 되면서 primary key라 부름\n\n하지만 identifier ≠ primary key.\n보통 identifier로 primary key를 만듦\n\nprimary key 한 table에 반드시 1개만 있어야함. 없어도 안됨. primary key없으면 DB가 아님\n\nEntity Types\n\n강성 개체(strong entity)\n\n각 개체는 하나 이상의 key 속성을 가질 수 있음\n대부분의 개체는 key를 갖기 때문에 강성 개체라 부르는 경우는 별로 없다. 그냥 개체라 부름\n\n약성 개체 (weak entity)\n\n어떤 개체는 key를 갖지 않을 수 있음\n자체적으로 식별될 수 없고, 다른 엔티티에 의존하는 엔티티\n\n\n\n\n\n\n\n\n관계에 참여하는 엔티티 인스턴스의 수\n표현: 1:1, 1:N, M:N 등으로 표시\nex) 한 학생은 여러 강의를 수강할 수 있음 (1:N)\n\n\n\n\n\n엔티티의 관계 참여 여부\n필수 참여 (전체 참여): 이중선으로 표시\n선택 참여 (부분 참여): 단일선으로 표시\n\n\n\n\n\n상위 엔티티와 하위 엔티티 간의 관계\n표현: 삼각형으로 연결\nex) ‘사람’ 엔티티의 특화로 ‘학생’과 ’교수’ 엔티티"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5_1.data_model_conceptual_ER_model.html#db-design",
    "href": "docs/blog/posts/Governance/4-5_1.data_model_conceptual_ER_model.html#db-design",
    "title": "Data Governance Study - Data Model (5)",
    "section": "",
    "text": "G\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling(개념적 데이터 모델링)은 데이터베이스 설계의 초기 단계\n비즈니스 요구사항 (업무기술서의 내용)을 이해하기 좋게 개념화 또는 추상화 (i.e. 도식화)하여 high level (고수준)의 데이터 구조를 정의하는 과정이다.\n\nhigh level은 사람이 이해할 수 있는 수준이란 뜻이며 low level은 컴퓨터가 이해할 수 있는 수준이라고 생각하면 편하다.\n\n\n\n\n\n\n\n\n\nG\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0-&gt;a1\n\n\n\n\na2\n\nSubject\n\n\n\na1-&gt;a2\n\n\n\n\na3\n\nLecture\n\n\n\na2-&gt;a3\n\n\n\n\na4\n\nProfessor\n\n\n\na3-&gt;a4\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling은 복잡한 비즈니스 요구사항을 단순화하여 표현하므로, 프로젝트의 초기 단계에서 이해관계자 간의 합의를 도출하는 데 매우 유용하다.\n후속 데이터베이스 설계 단계의 기반이 되어 전체 프로젝트의 성공에 중요한 역할을 한다.\n\n\n\n\n비즈니스 관점에서 데이터 구조를 이해하고 표현\n주요 엔티티와 그들 간의 관계를 식별\n시스템의 범위를 정의\n\n\n\n\n\n엔티티(Entity): 비즈니스에서 중요한 객체나 개념\n속성(Attribute): 엔티티의 특성 (이 단계에서는 상세하게 다루지 않을 수 있음)\n관계(Relationship): 엔티티 간의 연관성\n\nRelationship (관계) \\(\\ne\\) Relation\nRelationship 은 [Student] - &lt; Register &gt; - [Subject] 에서 &lt; Register &gt;에 해당, 한글로 관계로 표시\n도식에서 마름모에 해당\nRelation = Table (테이블), 보통 relation을 한글로 표현할 때 릴레이션 으로 표시 (관계라고 표시안함)\n\n수학자들은 테이블의 한 행을 relation 이라 부름\n즉, 관계형 데이터 모델의 수학적, 논리적 개념이고 튜플(tuple)의 집합이며 속성(attribute)의 집합으로 구성된다.\n테이블의 추상적인 개념으로 테이블은 Relation을 기술하는 하나의 구체적 표현\nDB에서 하나의 table로 구현된다.\n\n\n\n\n\n\n\n기술적 세부사항을 배제하고 비즈니스 개념에 집중\n높은 수준의 추상화\nDBMS에 독립적\n\n\n\n\n\n간단한 텍스트 설명이나 다이어그램\n주로 Entity-Relationship Diagram (ERD)을 사용\n\n개념적 설계에 가장 많이 쓰는 모델로서 Entity-Relationship Model (ERM or ER-model 개체 관계 모형)을 사용하고 그 산출물이 ERD이다.\n\n\n\n\n\n\n개체 (Entity)\n\n실 세계에 존재하는 의미있는 하나의 정보 단위\n표현: 사각형으로 표시\n물리적 개체 뿐 아니라 추상적(개념적) 개체도 포함\n\n물리적 개체: (학생, 자동차, 강의실, 등)\n추상적 개체 : (프로젝트, 직업, 교과목)\n\n개체는 둥근 직사각형으로 표시\n\n관계 (Relationship)\n\n개체들 사이의 연관성\n\n학생과 교과목 사이의 수강 관계\n표현: 마름모로 표시, 선으로 관련 엔티티에 연결, ex) [Student] - &lt; Register &gt; -[Subject]\n\n실제로는 개체와 관계를 구분짓기 매우 힘듦 \\(\\rightarrow\\) ER modeling 할때 의미가 없어짐 (깊게 생각하지 말것)\n\nex) 결혼을 개체로 둘건지 관계로 둘건지 애매\n\n[결혼] - &lt;진행&gt; - [예식장] vs [남자] - &lt;결혼&gt; - &lt;여자&gt;\n\n\n관계는 마름모로 표시\n\n속성 (attribute) (=필드명)\n\n개체 또는 관계의 본질적 특성이나 성질\n그러므로 instance는 속성들의 값의 집합\n표현: 타원형으로 표시, 선으로 엔티티에 연결\n예시\n\n학생(개체)이 가지는 속성은 학번, 혈액형, 나이, 핸폰 번호, 성별, 학년 등이 있음\n과목(개체)이 가지는 속성은 학점(credit), 시간(hour), 부서(department), 장소(location) 등이 있음\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0--a1\n\n\n\n\na3\n\nage\n\n\n\na0--a3\n\n\n\n\na4\n\nstudent_id\n\n\n\na0--a4\n\n\n\n\na5\n\nsex\n\n\n\na0--a5\n\n\n\n\na2\n\nSubject\n\n\n\na1--a2\n\n\n\n\na6\n\ncredit\n\n\n\na2--a6\n\n\n\n\na7\n\nhour\n\n\n\na2--a7\n\n\n\n\na8\n\ndepartment\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\n개체나 관계에서 파생되는 수많은 속성을 나열하고 명확하게 분리하는 것은 어려움, why?\n\n다음 개체 및 관계에서 주어진 속성의 주인(Owner)은?\n\n개체: [학생], [교과목]\n관계: &lt;수강&gt;\n속성: (성별), (나이), (과목), (학점), (평점), (이수구분)\n\n[학생]: (성별), (나이)\n\n(성별), (나이) 는 비교적 명확하게 [학생] 개체에 대응되는 속성이다\n\n[교과목] : (과목), (학점)\n\n(과목명), (학점) 는 비교적 명확하게 [교과목] 개체에 대응되는 속성이다\n\n개체 기준으로 (평점), (이수구분) 속성은 구분짓기 애매함\n\n[학생]이 (평점) 속성을 갖게 되면 학생 A에게 평점을 물어볼경우 대답을 할 수가 없음\n왜냐하면, 여러 과목에 대한 평점이 존재하기 때문에 어떤 교과목에 대한 평점을 얘기해야하는지 모름.\n즉, (평점)은 [학생]의 고유 속성이 아님.\n반대로, (평점) 속성의 주인이 [교과목] 개체라 가정할 경우, 교과목에 평점을 물어보면 학생이 몇 십명이기 때문에 어떤 학생의 평점을 얘기해야하는지 애매해짐.\n즉 (평점)은 [교과목]의 고유 속성이 아님\n\n\n(평점)과 (이수구분) 과 같은 애매한 속성은 관계로 구분 지으면 해결될 경우가 있음!\n\n관계: 개체 사이에 관계를 맺어주는 이벤트 또는 함수\n&lt;수강&gt;: 학생이 교과목을 수강한 이벤트\n\n(평점) : 학생 1명이 과목 1개를 수강하여 평점을 산출\n(이수구분): 학생 1명이 과목 1개를 수강하여 이수여부 산출\n\n&lt;수강&gt;: (평점), (이수구분)\n사실, (평점)과 (이수구분)과 같이 관계에 의하여 파생되는 속성은 해당 배경지식이 없는 외부인이라면 파악하기 매우 힘듦 (업무기술서가 명확히 적혀 있어야 해결 가능).\n\n\n\n\n\n\n\n주요 비즈니스 개체 식별\n개체 간 관계 정의\n높은 수준의 속성 식별 (선택적)\n비즈니스 규칙 반영\n논리적 데이터 모델링의 기초가 됨 (더 상세한 데이터 모델로 발전)"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5_1.data_model_conceptual_ER_model.html#er-model-concept-및-구성-요소",
    "href": "docs/blog/posts/Governance/4-5_1.data_model_conceptual_ER_model.html#er-model-concept-및-구성-요소",
    "title": "Data Governance Study - Data Model (6)",
    "section": "",
    "text": "실 세계에 존재하는 의미있는 하나의 정보 단위\n표현: 사각형으로 표시\n물리적 개체 뿐 아니라 추상적(개념적) 개체도 포함\n\n물리적 개체: (학생, 자동차, 강의실, 등)\n추상적 개체 : (프로젝트, 직업, 교과목)\n\n개체는 둥근 직사각형으로 표시\n\n\n\n\n\n개체들 사이의 연관성\n\n학생과 교과목 사이의 수강 관계\n표현: 마름모로 표시, 선으로 관련 엔티티에 연결, ex) [Student] - &lt; Register &gt; -[Subject]\n\n실제로는 개체와 관계를 구분짓기 매우 힘듦 \\(\\rightarrow\\) ER modeling 할때 의미가 없어짐 (깊게 생각하지 말것)\n\nex) 결혼을 개체로 둘건지 관계로 둘건지 애매\n\n[결혼] - &lt;진행&gt; - [예식장] vs [남자] - &lt;결혼&gt; - &lt;여자&gt;\n\n\n관계는 마름모로 표시\n\n\n\n\n\n개체 또는 관계의 본질적 특성이나 성질\n그러므로 instance는 속성들의 값의 집합\n표현: 타원형으로 표시, 선으로 엔티티에 연결\n예시\n\n학생(개체)이 가지는 속성은 학번, 혈액형, 나이, 핸폰 번호, 성별, 학년 등이 있음\n과목(개체)이 가지는 속성은 학점(credit), 시간(hour), 부서(department), 장소(location) 등이 있음\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0--a1\n\n\n\n\na3\n\nage\n\n\n\na0--a3\n\n\n\n\na4\n\nstudent_id\n\n\n\na0--a4\n\n\n\n\na5\n\nsex\n\n\n\na0--a5\n\n\n\n\na2\n\nSubject\n\n\n\na1--a2\n\n\n\n\na6\n\ncredit\n\n\n\na2--a6\n\n\n\n\na7\n\nhour\n\n\n\na2--a7\n\n\n\n\na8\n\ndepartment\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\n개체나 관계에서 파생되는 수많은 속성을 나열하고 명확하게 분리하는 것은 어려움, why?\n\n다음 개체 및 관계에서 주어진 속성의 주인(Owner)은?\n\n개체: [학생], [교과목]\n관계: &lt;수강&gt;\n속성: (성별), (나이), (과목), (학점), (평점), (이수구분)\n\n[학생]: (성별), (나이)\n\n(성별), (나이) 는 비교적 명확하게 [학생] 개체에 대응되는 속성이다\n\n[교과목] : (과목), (학점)\n\n(과목명), (학점) 는 비교적 명확하게 [교과목] 개체에 대응되는 속성이다\n\n개체 기준으로 (평점), (이수구분) 속성은 구분짓기 애매함\n\n[학생]이 (평점) 속성을 갖게 되면 학생 A에게 평점을 물어볼경우 대답을 할 수가 없음\n왜냐하면, 여러 과목에 대한 평점이 존재하기 때문에 어떤 교과목에 대한 평점을 얘기해야하는지 모름.\n즉, (평점)은 [학생]의 고유 속성이 아님.\n반대로, (평점) 속성의 주인이 [교과목] 개체라 가정할 경우, 교과목에 평점을 물어보면 학생이 몇 십명이기 때문에 어떤 학생의 평점을 얘기해야하는지 애매해짐.\n즉 (평점)은 [교과목]의 고유 속성이 아님\n\n\n(평점)과 (이수구분) 과 같은 애매한 속성은 관계로 구분 지으면 해결될 경우가 있음!\n\n관계: 개체 사이에 관계를 맺어주는 이벤트 또는 함수\n&lt;수강&gt;: 학생이 교과목을 수강한 이벤트\n\n(평점) : 학생 1명이 과목 1개를 수강하여 평점을 산출\n(이수구분): 학생 1명이 과목 1개를 수강하여 이수여부 산출\n\n&lt;수강&gt;: (평점), (이수구분)\n사실, (평점)과 (이수구분)과 같이 관계에 의하여 파생되는 속성은 해당 배경지식이 없는 외부인이라면 파악하기 매우 힘듦 (업무기술서가 명확히 적혀 있어야 해결 가능).\n\n\n\n\n\n\n속성의 유형을 여러 기준으로 분류할 수 있음\nMulti-valued (다중값 속성)\n\n하나의 엔티티 인스턴스가 여러 값을 가질 수 있는 속성\n표기: single valued (동그라미 1개로 표기) vs Multi-valued (동그라미 2개로 표기)\n예)\n\n(나이)는 한 시점에 여러 개의 값을 가질 수 없음\n(취미)는 한 시점에 여러 개의 값을 가질 수 있음\nCAR(자동차) ERD\n\ncolor가 multi-valued가 2개인 이유\n\n차 한대에 여러 색상이 들어갈 수 있기 때문에\n\n싱글 value가 multi value보다 훨 씬 많음\n\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nCar\n\n\n\na1\n\nyear\n\n\n\na0--a1\n\n\n\n\na2\n\nregistration\n\n\n\na0--a2\n\n\n\n\na3\n\nvehicle_id\n\n\n\na0--a3\n\n\n\n\na4\n\nmodel\n\n\n\na0--a4\n\n\n\n\na5\n\nmake\n\n\n\na0--a5\n\n\n\n\na6\n\n\ncolor\n\n\n\na0--a6\n\n\n\n\na7\n\nstate\n\n\n\na2--a7\n\n\n\n\na8\n\nnumber\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\ncomposite attribute(복합 속성)\n\nsimple attribute : 더이상 쪼개지지 않는 원자값을 갖는 속성\n\nex) 나이, 학번\n\ncomposite attribute\n\n여러 하위 속성으로 구성된 속성\n즉, 몇 개의 요소로 분해 될 수 있는 속성, 쪼개어 져도 의미를 갖을 수 있어야함\nex) 주소: 시 + 군 + 구 + 번지\n\nsimple 와 composite attribute를 구분짓는 기준은 나라의 사회 제도나 단체의 시스템의 특성에 따라 변한다\n\nex) 한국은 우편 수집 창고와 우체국의 시스템에 따라 주소를 시 + 군 + 구 + 번지 또는 시 + 군 + 구 가 필요로 할 수 있다\n하지만 다른 나라는 주소 전체를 쓰는 시스템이라면 주소 속성이 simple attribute 로 남을 수 있다.\n이름의 경우 성과 이름을 가르는 시스템이 필요한 미국과 full name을 사용하는 한국의 시스템의 경우 각 상황에 맞게 modeling을 해야한다.\n어떤 속성이든 분해해야할 용도가 있다면 쪼개야한다.\n\n위의 그림 예시에서\n\ncomposite attribute: registration (자동차 번호판)\nsimple attribute: state(주이름) & number(자동차 고유번호)\n\n\nderived vs stored attributes\n\nderived attribute (점선으로 표기) : 저장된 다른 데이터로부터 유도 가능한 속성\n\n총 학생 수: 그냥 instance나 학생 수를 카운트 하면 됨, 총 학생 수라는 속성은 없어도 됨\n각 과목의 성적 : 총점(derived), 평점(derived)\n주민등록번호: 나이(derived), 생일(derived)\nderived attribute는 자주 쓰이는 통계치를 구할 때 자주 쓰임. DB에 저장할 필요는 없다.\n\nstored attribute: 위에서 말한 총 학생 수, 총점, 평점, 나이, 생일과 같은 derived attribute의 연산이 너무 무겁거나 너무 빈번하게 사용되는 상황이라면 DB에 data를 적재하여 연산량을 줄이는 방법 도 있다.\n\n설계자의 재량에 따라 stored와 derived의 구분 짓는다.\n실무에서는 파생 변수(derived)가 자주 쓰인다면 stored (실선)로 남긴다\n학계에서는 derived는 가급적 점선으로 표시하여 derived상태로 남긴다\n\n\nKey Attributes\n\nkey attributes: 유일성 + 최소성 을 만족시켜야함\n\n어떤 개체에 대해서 그 인스턴스가 항상 유일한 값을 갖는 속성 또는 속성들의 집합\n\n중복되는 값을 가지면 안됨\n키 속성은 밑줄을 그어 표시\nex) 학생의 학번, 책의 ISBN, 차량번호\n\n\n특정 스냅샷이 아닌 해당 개체의 모든 가능한 스냅샷의 집합을 고려하여 파악되어야함 (개체가 아무리 많아 지더라도 항상 유일한 값을 가져야함)\n\nex) 다음의 ssn, 이름, 혈액형 중 키 속성은 ssn\n\ncomposite key(복합키)\n\nentity에서 키 속성자체가 없을 경우 attributes의 조합으로도 생성가능\n복합키는 최소성을 가져야함 : 최소한의 attributes로 복합키를 만들어야함\n\n용어 정리\n\nconceptual design: key는 identifier라고 부르고\nlogical design: key는 primary key라고 부름.\n\ntable을 만들게 되면서 primary key라 부름\n\n하지만 identifier ≠ primary key.\n보통 identifier로 primary key를 만듦\n\nprimary key 한 table에 반드시 1개만 있어야함. 없어도 안됨. primary key없으면 DB가 아님\n\nEntity Types\n\n강성 개체(strong entity)\n\n각 개체는 하나 이상의 key 속성을 가질 수 있음\n대부분의 개체는 key를 갖기 때문에 강성 개체라 부르는 경우는 별로 없다. 그냥 개체라 부름\n\n약성 개체 (weak entity)\n\n어떤 개체는 key를 갖지 않을 수 있음\n자체적으로 식별될 수 없고, 다른 엔티티에 의존하는 엔티티\n\n\n\n\n\n\n\n\n관계에 참여하는 엔티티 인스턴스의 수\n표현: 1:1, 1:N, M:N 등으로 표시\nex) 한 학생은 여러 강의를 수강할 수 있음 (1:N)\n\n\n\n\n\n엔티티의 관계 참여 여부\n필수 참여 (전체 참여): 이중선으로 표시\n선택 참여 (부분 참여): 단일선으로 표시\n\n\n\n\n\n상위 엔티티와 하위 엔티티 간의 관계\n표현: 삼각형으로 연결\nex) ‘사람’ 엔티티의 특화로 ‘학생’과 ’교수’ 엔티티"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_0.data_standard.html",
    "href": "docs/blog/posts/Governance/5_0.data_standard.html",
    "title": "Data Governance Study - Data Standard Governance",
    "section": "",
    "text": "각 데이터 항목에 대한 명명 규칙을 정의\n표준단어 및 표준용어, 표준도메인, 표준코드 등을 전사관리기준 형태로 정의\n이러한 표준들을 변경 및 관리하는 활동 수행\n\n\n\n\n\n데이터 표준관리는 데이터 항목들에 대해서 전사적으로 일관된 명명과 정의를 부여하고 이를 정제, 개선하는 활동이다.\n\n\n\n\n데이터 표준관리에서 중점적으로 다루는 정제 및 개선 사항은 다음과 같다:\n\n\n\n대상\n정제 및 개선 결과물\n\n\n\n\n명칭 + 정의\n단어 및 용어\n\n\n형식\n도메인\n\n\n규칙\n코드\n\n\n\n이러한 표준화 활동을 통해 조직 전체에서 일관된 데이터 관리가 가능해진다.\n\n\n\n\n\n\n각 용어에 대한 부가 정보(예: 데이터 타입, 길이, 허용 값 등)를 관리할 수 있다.\n데이터의 의미와 구조에 대한 종합적인 이해를 제공한다.\n\n\n\n\n\n비즈니스 사용자와 IT 전문가 사이의 의사소통을 원활하게 한다\n동일한 데이터에 대해서는 동일한 명칭을 사용함으로써 개발자-현업, 운영자-현업, 운영자-운영자 등 다양한 계층간에 명확하고 신속한 의사소통이 가능\n편의나 관습에 따라 동일한 의미로 사용되는 이음동의어를 대표성을 지닌 한 개의 단어로 정의함으로써 의사소통과 데이터에 대한 인식의 오류를 방지할 수 있다\n용어의 오해로 인한 프로젝트 지연이나 오류를 방지한다.\n\n\n\n\n\n데이터 소재 파악의 시간 및 노력 감소\n즉, 정보 사용자는 데이터의 의미와 데이터의 위치 등을 신속하게 파악할 수 있어 적시에 정확한 정보를 활용\n\n\n\n\n\n데이터 관련 규제 요구사항을 충족하는 데 도움을 준다.\n\n데이터 관련 규제 요구사항: 데이터의 수집, 저장, 처리, 공유, 및 폐기와 관련된 법적, 규범적 기준을 의미\n데이터 보안, 개인정보 보호, 데이터 품질, 및 데이터 관리의 투명성을 보장하기 위한 규제\n\n데이터의 의미와 사용에 대한 명확한 문서화를 제공한다.\n데이터 규제 종류\n\nGDPR (General Data Protection Regulation)\n\nEU의 개인정보 보호법\n예시: 개인 데이터의 수집 및 처리에 대한 명시적 동의 요구, 데이터 삭제 권리(잊힐 권리) 보장\n\nCCPA (California Consumer Privacy Act)\n\n캘리포니아 주의 소비자 개인정보 보호법\n예시: 소비자의 개인정보 접근 권리, 개인정보 판매 거부 권리\n\nHIPAA (Health Insurance Portability and Accountability Act)\n\n미국의 의료정보 보호법\n예시: 환자 의료 정보의 비밀성 보장, 의료 정보 접근 로그 유지\n\nPCI DSS (Payment Card Industry Data Security Standard)\n\n신용카드 정보 보호 표준\n예시: 카드 소지자 데이터의 암호화, 정기적인 보안 시스템 및 프로세스 검사\n\nSOX (Sarbanes-Oxley Act)\n\n미국의 기업 회계 개혁법\n예시: 재무 보고의 정확성과 신뢰성을 보장하기 위한 데이터 관리 요구사항\n\n개인정보 보호법\n\n한국\n예시: 개인정보 수집 시 동의 획득, 개인정보의 안전한 보관 및 파기\n\n전자금융거래법\n\n한국\n예시: 전자금융 거래 기록의 보관 및 보안 요구사항\n\n정보통신망법\n\n한국\n예시: 개인정보 유출 시 신고 의무, 정보보호 관리체계 인증\n\n데이터 현지화 법률\n\n여러 국가\n예시: 러시아의 데이터 현지화법, 중국의 사이버보안법에 따른 데이터 현지 저장 요구\n\n\n이러한 규제 요구사항을 충족하기 위해서는 데이터의 정의, 분류, 처리 방법 등이 명확히 문서화되고 관리되어야 한다.\n표준 단어 사전은 데이터 요소를 일관되게 정의하고 분류하는 데 도움을 줌으로써, 규제 준수를 위한 기반을 제공한다.\n예를 들어, 개인식별정보(PII)가 무엇인지, 어떤 데이터 필드가 이에 해당하는지를 명확히 정의하고 관리할 수 있게 해준다.\n\n\n\n\n\n일관된 데이터 정의와 사용으로 전반적인 데이터 품질을 개선한다.\n데이터의 입력 오류를 예방함으로써 데이터의 품질을 향상\n데이터 통합과 분석의 정확성을 높인다.\n\n\n\n\n\n조직의 데이터 관련 지식을 체계적으로 축적하고 공유할 수 있다.\n신규 직원의 온보딩과 지식 전달을 용이하게 한다.\n\n\n\n\n\n서로 다른 시스템 간의 데이터 매핑과 통합을 지원한다.\n시스템 간 데이터 불일치 문제를 줄인다.\n데이터 변환, 정제 비용 감소: 데이터의 전송, 공유, 가공을 위해 정보시스템 간 또는 정보시스템 내에서 별도의 포맷 변환이나 정제 작업 불 필요\n\n\n\n\n\n\n\n\n데이터 표준관리 지침\n데이터 관리 가이드\n표준 데이터\n구조 데이터\n\n각 요소는 서로 밀접하게 연관되어 있으며, 전체적인 데이터 거버넌스 체계를 형성한다.\n\n\n\n\n데이터 표준을 정의하고 변경 관리하는 데 필요한 지침을 제공한다.\n조직 내에서 데이터 표준을 어떻게 설정하고, 유지하며, 업데이트할 것인지에 대한 가이드라인을 제시한다.\n\n\n\n\n\n데이터 표준, 구조, 흐름을 관리하기 위한 프로세스, 조직, 역할을 정의한다.\n이는 데이터 거버넌스의 실행 측면을 다루며, 누가 어떤 책임을 지고 데이터 표준화 활동을 수행할지 명확히 한다.\n\n\n\n\n\n표준 데이터는 실제 데이터 요소들을 표준화하는 핵심 부분이다. 여기에는 다음과 같은 세부 요소들이 포함됩니다\n\n\n표준 단어\n\n명칭 (한글/영문)\n영문약어\n단어 정의\n표준 단어를 기반으로 자료형을 정의하여 표준 단어 금칙어를 만든다.\n\n표준 도메인\n\n표준 단어를 기반으로 자료형을 정의하여 표준 도메인을 작성한다.\n데이터 타입 결정\n데이터 길이 결정\n\n표준 용어\n\n표준 단어, 자료형, 표준 도메인을 기반으로 용어를 정의한다.\n표준 단어 2개 이상의 조합으로 용어를 생성한다.\n표준 용어는 1개의 표준 도메인으로 구성된다.\n\n표준코드\n\n표준 도메인을 기반으로 유효값을 정의한다.\n유효값을 기반으로 코드값을 만들어낸다.\n\n제작 순서\n\n표준 단어 \\(\\rightarrow\\) 표준 도메인 \\(\\rightarrow\\) 표준 용어 \\(\\rightarrow\\) 표준 코드 \\(\\rightarrow\\) 코드 값\n\n\n\n표준 단어와 표준 용어는 한국어에만 존재하는 개념으로 다른 외국어에는 없는 개념이다. 외국어는 형태소 분석이 한국어 만큼 세분화되어 있지 않아 표준 단어와 표준 용어가 합쳐진 개념인 데이터 카탈로그란 용어를 사용한다.\n\n\n\n\n\n데이터 모델링의 영역이다.\n표준 데이터가 정의되면, 이를 바탕으로 실제 데이터 모델을 구축합니다. 이 과정은 크게 세 단계로 나뉜다:\n\n개념 데이터 모델\n논리 데이터 모델\n물리 데이터 모델\n\n개념 데이터 모델 + 논리 데이터 모델링을 통해 주제영역, 엔티티, 속성을 차례로 규명한다.\n물리 데이터 모델을 통해 엔티티를 물리적으로 구현한 테이블과 속성을 물리적으로 구현한 컬럼을 만들어낸다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5_0.data_standard.html#표준-데이터-사전이란",
    "href": "docs/blog/posts/Governance/5_0.data_standard.html#표준-데이터-사전이란",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "조직 내에서 사용되는 데이터 용어를 표준화하고 관리하기 위한 도구이다. 이는 데이터 거버넌스와 데이터 품질 관리의 중요한 구성 요소이다.\n조직에서 사용되는 모든 데이터 용어의 공식적인 정의를 제공\n각 용어의 의미, 사용 맥락, 형식 등을 명확히 기술\n데이터 표준관리를 위한 표준 단어 사전을 만드는 과정은 체계적이고 협력적인 접근이 필요하다.\n표준 단어 사전을 만드는 과정은 반복적이고 지속적으로 이루어져야 하며, 조직의 변화와 새로운 요구사항을 반영하여 계속 발전시켜 나가야 한다.\n\n표준 단어 사전은 단순한 용어 목록이 아니라 조직의 데이터 자산을 효과적으로 관리하고 활용하기 위한 중요한 도구이다.\n조직의 모든 구성원이 쉽게 접근하고 활용할 수 있어야 한다.\n\n또한, 기술적인 구현뿐만 아니라 조직 문화와 프로세스의 변화도 함께 고려해야 한다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5_0.data_standard.html#필요성",
    "href": "docs/blog/posts/Governance/5_0.data_standard.html#필요성",
    "title": "Data Governance Study - Data Standard Governance",
    "section": "",
    "text": "각 용어에 대한 부가 정보(예: 데이터 타입, 길이, 허용 값 등)를 관리할 수 있다.\n데이터의 의미와 구조에 대한 종합적인 이해를 제공한다.\n\n\n\n\n\n비즈니스 사용자와 IT 전문가 사이의 의사소통을 원활하게 한다\n동일한 데이터에 대해서는 동일한 명칭을 사용함으로써 개발자-현업, 운영자-현업, 운영자-운영자 등 다양한 계층간에 명확하고 신속한 의사소통이 가능\n편의나 관습에 따라 동일한 의미로 사용되는 이음동의어를 대표성을 지닌 한 개의 단어로 정의함으로써 의사소통과 데이터에 대한 인식의 오류를 방지할 수 있다\n용어의 오해로 인한 프로젝트 지연이나 오류를 방지한다.\n\n\n\n\n\n데이터 소재 파악의 시간 및 노력 감소\n즉, 정보 사용자는 데이터의 의미와 데이터의 위치 등을 신속하게 파악할 수 있어 적시에 정확한 정보를 활용\n\n\n\n\n\n데이터 관련 규제 요구사항을 충족하는 데 도움을 준다.\n\n데이터 관련 규제 요구사항: 데이터의 수집, 저장, 처리, 공유, 및 폐기와 관련된 법적, 규범적 기준을 의미\n데이터 보안, 개인정보 보호, 데이터 품질, 및 데이터 관리의 투명성을 보장하기 위한 규제\n\n데이터의 의미와 사용에 대한 명확한 문서화를 제공한다.\n데이터 규제 종류\n\nGDPR (General Data Protection Regulation)\n\nEU의 개인정보 보호법\n예시: 개인 데이터의 수집 및 처리에 대한 명시적 동의 요구, 데이터 삭제 권리(잊힐 권리) 보장\n\nCCPA (California Consumer Privacy Act)\n\n캘리포니아 주의 소비자 개인정보 보호법\n예시: 소비자의 개인정보 접근 권리, 개인정보 판매 거부 권리\n\nHIPAA (Health Insurance Portability and Accountability Act)\n\n미국의 의료정보 보호법\n예시: 환자 의료 정보의 비밀성 보장, 의료 정보 접근 로그 유지\n\nPCI DSS (Payment Card Industry Data Security Standard)\n\n신용카드 정보 보호 표준\n예시: 카드 소지자 데이터의 암호화, 정기적인 보안 시스템 및 프로세스 검사\n\nSOX (Sarbanes-Oxley Act)\n\n미국의 기업 회계 개혁법\n예시: 재무 보고의 정확성과 신뢰성을 보장하기 위한 데이터 관리 요구사항\n\n개인정보 보호법\n\n한국\n예시: 개인정보 수집 시 동의 획득, 개인정보의 안전한 보관 및 파기\n\n전자금융거래법\n\n한국\n예시: 전자금융 거래 기록의 보관 및 보안 요구사항\n\n정보통신망법\n\n한국\n예시: 개인정보 유출 시 신고 의무, 정보보호 관리체계 인증\n\n데이터 현지화 법률\n\n여러 국가\n예시: 러시아의 데이터 현지화법, 중국의 사이버보안법에 따른 데이터 현지 저장 요구\n\n\n이러한 규제 요구사항을 충족하기 위해서는 데이터의 정의, 분류, 처리 방법 등이 명확히 문서화되고 관리되어야 한다.\n표준 단어 사전은 데이터 요소를 일관되게 정의하고 분류하는 데 도움을 줌으로써, 규제 준수를 위한 기반을 제공한다.\n예를 들어, 개인식별정보(PII)가 무엇인지, 어떤 데이터 필드가 이에 해당하는지를 명확히 정의하고 관리할 수 있게 해준다.\n\n\n\n\n\n일관된 데이터 정의와 사용으로 전반적인 데이터 품질을 개선한다.\n데이터의 입력 오류를 예방함으로써 데이터의 품질을 향상\n데이터 통합과 분석의 정확성을 높인다.\n\n\n\n\n\n조직의 데이터 관련 지식을 체계적으로 축적하고 공유할 수 있다.\n신규 직원의 온보딩과 지식 전달을 용이하게 한다.\n\n\n\n\n\n서로 다른 시스템 간의 데이터 매핑과 통합을 지원한다.\n시스템 간 데이터 불일치 문제를 줄인다.\n데이터 변환, 정제 비용 감소: 데이터의 전송, 공유, 가공을 위해 정보시스템 간 또는 정보시스템 내에서 별도의 포맷 변환이나 정제 작업 불 필요"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_0.data_standard.html#제작시-유념해야할-사항",
    "href": "docs/blog/posts/Governance/5_0.data_standard.html#제작시-유념해야할-사항",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "조직 전체에서 데이터 용어를 일관되게 사용할 수 있도록 한다.\n동일한 개념에 대해 서로 다른 용어를 사용하는 문제를 방지한다.\n\n\n\n\n\n유사하거나 중복된 용어를 식별하고 제거\n\n\n\n\n\n명명 규칙, 약어 사용, 데이터 형식 등을 표준화\n데이터 모델링과 시스템 개발에서 일관된 기준을 제공\n데이터 표준 단어 용도의 목적은 조직내 정보 공유가 제 1 목적이기 때문에 반드시 공공 기관이나 권위있는 조직의 양식을 따를 필요는 없다.\n이상적인 접근 방식은 외부 표준과 조직 내부의 요구사항을 균형 있게 고려하는 것이다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5_0.data_standard.html#제작-과정",
    "href": "docs/blog/posts/Governance/5_0.data_standard.html#제작-과정",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "표준 단어 사전의 목적과 범위를 명확히 한다.\n\n목적이라 함은 데이터 일관성 확보(동일한 의미의 데이터 사용), 데이터 품질 향상 (데이터 처리 과정 비용 감소), 시스템 통합 지원 (여러 시스템 간 데이터 매핑과 통합), 규제 준수 지원 등을 의미한다.\n범위는 대상 데이터 범위, 조직적 범위, 시스템 범위 등을 의미하며, 너무 광범위한 영역은 제작 실패로 이어진다.\n\n이해관계자 식별: 관련 부서와 담당자들을 파악한다.\n거버넌스 체계 수립: 단어 사전 관리를 위한 조직과 프로세스를 정립한다.\n\n\n\n\n\n기존 데이터 모델, 데이터베이스 스키마, 업무 문서 등에서 사용 중인 단어들을 수집한다.\n업무 도메인별로 사용되는 용어들을 취합한다.\n\n\n\n\n\n동의어, 유사어, 약어 등을 식별한다.\n업무 영역별 용어의 의미와 사용 맥락을 분석한다.\n불필요하거나 중복된 단어들을 제거한다.\n\n\n\n\n\n약어 사용 규칙, 대소문자 규칙 등의 명명 규칙을 정의합니다.\n단어 정의 형식을 standardization 합니다.\n도메인별 특수 규칙을 설정한다.\n\n유용한 데이터 표준안을 만들기 위해 조직 내의 각 업무 영역 또는 데이터 도메인에 맞는 고유한 규칙이나 지침을 만들어야 한다.\n도메인 식별: 재무, 인사, 마케팅, 제조, 고객 서비스 등\n각 도메인별 특수성 파악: 해당 도메인에서만 사용되는 용어나 개념과 도메인 특유의 데이터 형식이나 제약 조건\n예시\n\n재무 도메인의 통화 표기 규칙: “USD 1,000.00” 또는 “1,000,000 원” 형식 사용\n재무 도메인의 회계 기간 표현: “FY2023Q2” (2023 회계연도 2분기)\n인사 도메인의 직급 코드 체계: “M” (매니저), “D” (디렉터)\n인사 도메인의 근속 연수 계산 규칙: 입사일 기준, 월 단위 반올림\n마케팅 도메인의 캠페인 코드 형식: “CAM_2023_SUMMER_01”\n마케팅 도메인의 고객 세그먼트 분류 기준: “VIP”, “REGULAR”, “NEW”\n제조 도메인의 제품 코드 체계: “PROD-A01-R” (제품군-모델번호-버전)\n제조 도메인의 품질 등급 표기: “A”, “B”, “C” 등급 사용\n\n\n\n\n\n\n\n분석된 단어들 중 표준으로 사용할 단어들을 선정\n선정 기준을 명확히 하고, 이해관계자들의 합의를 도출\n선정 기준 예시\n\n명확성\n\n의미가 명확하고 모호하지 않은 단어 선정 (일반적인 단어는 한정시킬 것)\n예: “고객” 대신 “활성고객”과 “비활성고객”으로 구분\n\n일관성\n\n조직 전체에서 일관되게 사용할 수 있는 단어\n예: 부서별로 다르게 사용되던 용어를 하나로 통일\n\n간결성\n\n가능한 간단하고 간결한 단어\n예: “제품구매고객정보” 대신 “구매자정보”\n\n유일성\n\n중복되지 않는 고유한 의미를 가진 단어\n예: 동음이의어 피하기\n\n업계 표준 부합성\n\n가능한 업계에서 널리 사용되는 표준 용어 선택\n예: 금융업계의 “ROI” (Return on Investment)\n\n확장성\n\n향후 변화나 확장을 고려한 단어 선택\n예: “2023년예산” 대신 “연간예산”\n\n이해 용이성\n\n비전문가도 이해하기 쉬운 단어\n예: 전문 용어보다는 일반적인 비즈니스 용어 선호\n\n번역 가능성\n\n다국어 지원이 필요한 경우, 번역이 용이한 단어\n관용구나 은유적 표현 피하기\n\n기존 시스템 호환성\n\n기존 시스템과의 호환성을 고려한 단어\n예: 레거시 시스템의 주요 용어 유지\n\n법규 및 규제 준수\n\n관련 법규나 규제를 준수하는 단어\n예: 개인정보보호법에 부합하는 용어 선택\n\n도메인 적합성\n\n해당 업무 도메인에 적합한 단어\n예: 금융 도메인에서는 “이자율”, 제조 도메인에서는 “불량률”\n\n측정 가능성\n\n정량적 측정이 가능한 개념을 나타내는 단어\n예: “고객만족도” (1-5 척도로 측정 가능)\n\n약어 사용 규칙\n\n약어 사용 시 일관된 규칙 적용\n규칙에 부합하지 않는 관용어는 관용어 사용을 유지하는 것이 좋다.\n예: “고객번호”를 “CUST_NO”로 통일\n\n\n\n\n\n\n\n각 단어에 대한 상세 정보를 정의 (정의, 동의어, 사용 예시, 관련 업무 영역 등).\n단어 간의 관계를 정의 (상위어, 하위어, 관련어 등)\n\n\n\n\n\n선정된 표준 단어들에 대해 관련 부서와 전문가들의 검토를 받는다.\n필요시 수정하고 최종 승인을 받는다.\n\n\n\n\n\n승인된 단어들을 데이터베이스나 전문 도구에 등록한다. (엔코아, Microsoft Purview 등)\n검색, 조회, 관리가 용이한 형태로 구성한다.\n\n\n\n\n\n완성된 표준 단어 사전을 조직 내에 공유\n사용 방법과 중요성에 대한 교육을 실시\n\n\n\n\n\n새로운 단어 추가, 기존 단어 수정, 폐기 등의 프로세스를 수립한다.\n정기적인 검토와 업데이트를 수행한다.\n사용 현황을 모니터링하고 피드백을 수집한다.\n\n\n\n\n\n데이터 모델링, 시스템 개발, 보고서 작성 등의 프로세스와 표준 단어 사전을 연계한다\n\n연계 순서는 각 상황마다 다르다.\n이미 ERD와 DB가 존재하는 시스템들을 통합하는 상황이라면 표준 단어 사전을 순서상 나중에 제작하는 것이 유리하다\n그 반대라면 표준 단어사전을 먼저 만들어 DB를 만드는 것이 유리할 수 있다.\n\n다른 데이터 관리 도구들과의 통합을 고려한다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5_0.data_standard.html#표준-단어-사전-예시",
    "href": "docs/blog/posts/Governance/5_0.data_standard.html#표준-단어-사전-예시",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "표준 단어 사전 예시"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5_1.data_model_conceptual_ER_model copy.html",
    "href": "docs/blog/posts/Governance/4-5_1.data_model_conceptual_ER_model copy.html",
    "title": "Data Governance Study - Data Model (6)",
    "section": "",
    "text": "G\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n실 세계에 존재하는 의미있는 하나의 정보 단위\n표현: 사각형으로 표시\n물리적 개체 뿐 아니라 추상적(개념적) 개체도 포함\n\n물리적 개체: (학생, 자동차, 강의실, 등)\n추상적 개체 : (프로젝트, 직업, 교과목)\n\n개체는 둥근 직사각형으로 표시\n\n\n\n\n\n개체들 사이의 연관성\n\n학생과 교과목 사이의 수강 관계\n표현: 마름모로 표시, 선으로 관련 엔티티에 연결, ex) [Student] - &lt; Register &gt; -[Subject]\n\n실제로는 개체와 관계를 구분짓기 매우 힘듦 \\(\\rightarrow\\) ER modeling 할때 의미가 없어짐 (깊게 생각하지 말것)\n\nex) 결혼을 개체로 둘건지 관계로 둘건지 애매\n\n[결혼] - &lt;진행&gt; - [예식장] vs [남자] - &lt;결혼&gt; - &lt;여자&gt;\n\n\n관계는 마름모로 표시\n\n\n\n\n\n개체 또는 관계의 본질적 특성이나 성질\n그러므로 instance는 속성들의 값의 집합\n표현: 타원형으로 표시, 선으로 엔티티에 연결\n예시\n\n학생(개체)이 가지는 속성은 학번, 혈액형, 나이, 핸폰 번호, 성별, 학년 등이 있음\n과목(개체)이 가지는 속성은 학점(credit), 시간(hour), 부서(department), 장소(location) 등이 있음\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0--a1\n\n\n\n\na3\n\nage\n\n\n\na0--a3\n\n\n\n\na4\n\nstudent_id\n\n\n\na0--a4\n\n\n\n\na5\n\nsex\n\n\n\na0--a5\n\n\n\n\na2\n\nSubject\n\n\n\na1--a2\n\n\n\n\na6\n\ncredit\n\n\n\na2--a6\n\n\n\n\na7\n\nhour\n\n\n\na2--a7\n\n\n\n\na8\n\ndepartment\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\n개체나 관계에서 파생되는 수많은 속성을 나열하고 명확하게 분리하는 것은 어려움, why?\n\n다음 개체 및 관계에서 주어진 속성의 주인(Owner)은?\n\n개체: [학생], [교과목]\n관계: &lt;수강&gt;\n속성: (성별), (나이), (과목), (학점), (평점), (이수구분)\n\n[학생]: (성별), (나이)\n\n(성별), (나이) 는 비교적 명확하게 [학생] 개체에 대응되는 속성이다\n\n[교과목] : (과목), (학점)\n\n(과목명), (학점) 는 비교적 명확하게 [교과목] 개체에 대응되는 속성이다\n\n개체 기준으로 (평점), (이수구분) 속성은 구분짓기 애매함\n\n[학생]이 (평점) 속성을 갖게 되면 학생 A에게 평점을 물어볼경우 대답을 할 수가 없음\n왜냐하면, 여러 과목에 대한 평점이 존재하기 때문에 어떤 교과목에 대한 평점을 얘기해야하는지 모름.\n즉, (평점)은 [학생]의 고유 속성이 아님.\n반대로, (평점) 속성의 주인이 [교과목] 개체라 가정할 경우, 교과목에 평점을 물어보면 학생이 몇 십명이기 때문에 어떤 학생의 평점을 얘기해야하는지 애매해짐.\n즉 (평점)은 [교과목]의 고유 속성이 아님\n\n\n(평점)과 (이수구분) 과 같은 애매한 속성은 관계로 구분 지으면 해결될 경우가 있음!\n\n관계: 개체 사이에 관계를 맺어주는 이벤트 또는 함수\n&lt;수강&gt;: 학생이 교과목을 수강한 이벤트\n\n(평점) : 학생 1명이 과목 1개를 수강하여 평점을 산출\n(이수구분): 학생 1명이 과목 1개를 수강하여 이수여부 산출\n\n&lt;수강&gt;: (평점), (이수구분)\n사실, (평점)과 (이수구분)과 같이 관계에 의하여 파생되는 속성은 해당 배경지식이 없는 외부인이라면 파악하기 매우 힘듦 (업무기술서가 명확히 적혀 있어야 해결 가능).\n\n\n\n\n\n\n속성의 유형을 여러 기준으로 분류할 수 있음\nMulti-valued (다중값 속성)\n\n하나의 엔티티 인스턴스가 여러 값을 가질 수 있는 속성\n표기: single valued (동그라미 1개로 표기) vs Multi-valued (동그라미 2개로 표기)\n예)\n\n(나이)는 한 시점에 여러 개의 값을 가질 수 없음\n(취미)는 한 시점에 여러 개의 값을 가질 수 있음\nCAR(자동차) ERD\n\ncolor가 multi-valued가 2개인 이유\n\n차 한대에 여러 색상이 들어갈 수 있기 때문에\n\n싱글 value가 multi value보다 훨 씬 많음\n\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nCar\n\n\n\na1\n\nyear\n\n\n\na0--a1\n\n\n\n\na2\n\nregistration\n\n\n\na0--a2\n\n\n\n\na3\n\nvehicle_id\n\n\n\na0--a3\n\n\n\n\na4\n\nmodel\n\n\n\na0--a4\n\n\n\n\na5\n\nmake\n\n\n\na0--a5\n\n\n\n\na6\n\n\ncolor\n\n\n\na0--a6\n\n\n\n\na7\n\nstate\n\n\n\na2--a7\n\n\n\n\na8\n\nnumber\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\ncomposite attribute(복합 속성)\n\nsimple attribute : 더이상 쪼개지지 않는 원자값을 갖는 속성\n\nex) 나이, 학번\n\ncomposite attribute\n\n여러 하위 속성으로 구성된 속성\n즉, 몇 개의 요소로 분해 될 수 있는 속성, 쪼개어 져도 의미를 갖을 수 있어야함\nex) 주소: 시 + 군 + 구 + 번지\n\nsimple 와 composite attribute를 구분짓는 기준은 나라의 사회 제도나 단체의 시스템의 특성에 따라 변한다\n\nex) 한국은 우편 수집 창고와 우체국의 시스템에 따라 주소를 시 + 군 + 구 + 번지 또는 시 + 군 + 구 가 필요로 할 수 있다\n하지만 다른 나라는 주소 전체를 쓰는 시스템이라면 주소 속성이 simple attribute 로 남을 수 있다.\n이름의 경우 성과 이름을 가르는 시스템이 필요한 미국과 full name을 사용하는 한국의 시스템의 경우 각 상황에 맞게 modeling을 해야한다.\n어떤 속성이든 분해해야할 용도가 있다면 쪼개야한다.\n\n위의 그림 예시에서\n\ncomposite attribute: registration (자동차 번호판)\nsimple attribute: state(주이름) & number(자동차 고유번호)\n\n\nderived vs stored attributes\n\nderived attribute (점선으로 표기) : 저장된 다른 데이터로부터 유도 가능한 속성\n\n총 학생 수: 그냥 instance나 학생 수를 카운트 하면 됨, 총 학생 수라는 속성은 없어도 됨\n각 과목의 성적 : 총점(derived), 평점(derived)\n주민등록번호: 나이(derived), 생일(derived)\nderived attribute는 자주 쓰이는 통계치를 구할 때 자주 쓰임. DB에 저장할 필요는 없다.\n\nstored attribute: 위에서 말한 총 학생 수, 총점, 평점, 나이, 생일과 같은 derived attribute의 연산이 너무 무겁거나 너무 빈번하게 사용되는 상황이라면 DB에 data를 적재하여 연산량을 줄이는 방법 도 있다.\n\n설계자의 재량에 따라 stored와 derived의 구분 짓는다.\n실무에서는 파생 변수(derived)가 자주 쓰인다면 stored (실선)로 남긴다\n학계에서는 derived는 가급적 점선으로 표시하여 derived상태로 남긴다\n\n\nKey Attributes\n\nkey attributes: 유일성 + 최소성 을 만족시켜야함\n\n어떤 개체에 대해서 그 인스턴스가 항상 유일한 값을 갖는 속성 또는 속성들의 집합\n\n중복되는 값을 가지면 안됨\n키 속성은 밑줄을 그어 표시\nex) 학생의 학번, 책의 ISBN, 차량번호\n\n\n특정 스냅샷이 아닌 해당 개체의 모든 가능한 스냅샷의 집합을 고려하여 파악되어야함 (개체가 아무리 많아 지더라도 항상 유일한 값을 가져야함)\n\nex) 다음의 ssn, 이름, 혈액형 중 키 속성은 ssn\n\ncomposite key(복합키)\n\nentity에서 키 속성자체가 없을 경우 attributes의 조합으로도 생성가능\n복합키는 최소성을 가져야함 : 최소한의 attributes로 복합키를 만들어야함\n\n용어 정리\n\nconceptual design: key는 identifier라고 부르고\nlogical design: key는 primary key라고 부름.\n\ntable을 만들게 되면서 primary key라 부름\n\n하지만 identifier ≠ primary key.\n보통 identifier로 primary key를 만듦\n\nprimary key 한 table에 반드시 1개만 있어야함. 없어도 안됨. primary key없으면 DB가 아님\n\nEntity Types\n\n강성 개체(strong entity)\n\n각 개체는 하나 이상의 key 속성을 가질 수 있음\n대부분의 개체는 key를 갖기 때문에 강성 개체라 부르는 경우는 별로 없다. 그냥 개체라 부름\n\n약성 개체 (weak entity)\n\n어떤 개체는 key를 갖지 않을 수 있음\n자체적으로 식별될 수 없고, 다른 엔티티에 의존하는 엔티티\n\n\n\n\n\n\n\n\n관계에 참여하는 엔티티 인스턴스의 수\n표현: 1:1, 1:N, M:N 등으로 표시\nex) 한 학생은 여러 강의를 수강할 수 있음 (1:N)\n\n\n\n\n\n엔티티의 관계 참여 여부\n필수 참여 (전체 참여): 이중선으로 표시\n선택 참여 (부분 참여): 단일선으로 표시\n\n\n\n\n\n상위 엔티티와 하위 엔티티 간의 관계\n표현: 삼각형으로 연결\nex) ‘사람’ 엔티티의 특화로 ‘학생’과 ’교수’ 엔티티"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5_1.data_model_conceptual_ER_model copy.html#er-model-concept-및-구성-요소",
    "href": "docs/blog/posts/Governance/4-5_1.data_model_conceptual_ER_model copy.html#er-model-concept-및-구성-요소",
    "title": "Data Governance Study - Data Model (6)",
    "section": "",
    "text": "실 세계에 존재하는 의미있는 하나의 정보 단위\n표현: 사각형으로 표시\n물리적 개체 뿐 아니라 추상적(개념적) 개체도 포함\n\n물리적 개체: (학생, 자동차, 강의실, 등)\n추상적 개체 : (프로젝트, 직업, 교과목)\n\n개체는 둥근 직사각형으로 표시\n\n\n\n\n\n개체들 사이의 연관성\n\n학생과 교과목 사이의 수강 관계\n표현: 마름모로 표시, 선으로 관련 엔티티에 연결, ex) [Student] - &lt; Register &gt; -[Subject]\n\n실제로는 개체와 관계를 구분짓기 매우 힘듦 \\(\\rightarrow\\) ER modeling 할때 의미가 없어짐 (깊게 생각하지 말것)\n\nex) 결혼을 개체로 둘건지 관계로 둘건지 애매\n\n[결혼] - &lt;진행&gt; - [예식장] vs [남자] - &lt;결혼&gt; - &lt;여자&gt;\n\n\n관계는 마름모로 표시\n\n\n\n\n\n개체 또는 관계의 본질적 특성이나 성질\n그러므로 instance는 속성들의 값의 집합\n표현: 타원형으로 표시, 선으로 엔티티에 연결\n예시\n\n학생(개체)이 가지는 속성은 학번, 혈액형, 나이, 핸폰 번호, 성별, 학년 등이 있음\n과목(개체)이 가지는 속성은 학점(credit), 시간(hour), 부서(department), 장소(location) 등이 있음\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0--a1\n\n\n\n\na3\n\nage\n\n\n\na0--a3\n\n\n\n\na4\n\nstudent_id\n\n\n\na0--a4\n\n\n\n\na5\n\nsex\n\n\n\na0--a5\n\n\n\n\na2\n\nSubject\n\n\n\na1--a2\n\n\n\n\na6\n\ncredit\n\n\n\na2--a6\n\n\n\n\na7\n\nhour\n\n\n\na2--a7\n\n\n\n\na8\n\ndepartment\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\n개체나 관계에서 파생되는 수많은 속성을 나열하고 명확하게 분리하는 것은 어려움, why?\n\n다음 개체 및 관계에서 주어진 속성의 주인(Owner)은?\n\n개체: [학생], [교과목]\n관계: &lt;수강&gt;\n속성: (성별), (나이), (과목), (학점), (평점), (이수구분)\n\n[학생]: (성별), (나이)\n\n(성별), (나이) 는 비교적 명확하게 [학생] 개체에 대응되는 속성이다\n\n[교과목] : (과목), (학점)\n\n(과목명), (학점) 는 비교적 명확하게 [교과목] 개체에 대응되는 속성이다\n\n개체 기준으로 (평점), (이수구분) 속성은 구분짓기 애매함\n\n[학생]이 (평점) 속성을 갖게 되면 학생 A에게 평점을 물어볼경우 대답을 할 수가 없음\n왜냐하면, 여러 과목에 대한 평점이 존재하기 때문에 어떤 교과목에 대한 평점을 얘기해야하는지 모름.\n즉, (평점)은 [학생]의 고유 속성이 아님.\n반대로, (평점) 속성의 주인이 [교과목] 개체라 가정할 경우, 교과목에 평점을 물어보면 학생이 몇 십명이기 때문에 어떤 학생의 평점을 얘기해야하는지 애매해짐.\n즉 (평점)은 [교과목]의 고유 속성이 아님\n\n\n(평점)과 (이수구분) 과 같은 애매한 속성은 관계로 구분 지으면 해결될 경우가 있음!\n\n관계: 개체 사이에 관계를 맺어주는 이벤트 또는 함수\n&lt;수강&gt;: 학생이 교과목을 수강한 이벤트\n\n(평점) : 학생 1명이 과목 1개를 수강하여 평점을 산출\n(이수구분): 학생 1명이 과목 1개를 수강하여 이수여부 산출\n\n&lt;수강&gt;: (평점), (이수구분)\n사실, (평점)과 (이수구분)과 같이 관계에 의하여 파생되는 속성은 해당 배경지식이 없는 외부인이라면 파악하기 매우 힘듦 (업무기술서가 명확히 적혀 있어야 해결 가능).\n\n\n\n\n\n\n속성의 유형을 여러 기준으로 분류할 수 있음\nMulti-valued (다중값 속성)\n\n하나의 엔티티 인스턴스가 여러 값을 가질 수 있는 속성\n표기: single valued (동그라미 1개로 표기) vs Multi-valued (동그라미 2개로 표기)\n예)\n\n(나이)는 한 시점에 여러 개의 값을 가질 수 없음\n(취미)는 한 시점에 여러 개의 값을 가질 수 있음\nCAR(자동차) ERD\n\ncolor가 multi-valued가 2개인 이유\n\n차 한대에 여러 색상이 들어갈 수 있기 때문에\n\n싱글 value가 multi value보다 훨 씬 많음\n\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nCar\n\n\n\na1\n\nyear\n\n\n\na0--a1\n\n\n\n\na2\n\nregistration\n\n\n\na0--a2\n\n\n\n\na3\n\nvehicle_id\n\n\n\na0--a3\n\n\n\n\na4\n\nmodel\n\n\n\na0--a4\n\n\n\n\na5\n\nmake\n\n\n\na0--a5\n\n\n\n\na6\n\n\ncolor\n\n\n\na0--a6\n\n\n\n\na7\n\nstate\n\n\n\na2--a7\n\n\n\n\na8\n\nnumber\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\ncomposite attribute(복합 속성)\n\nsimple attribute : 더이상 쪼개지지 않는 원자값을 갖는 속성\n\nex) 나이, 학번\n\ncomposite attribute\n\n여러 하위 속성으로 구성된 속성\n즉, 몇 개의 요소로 분해 될 수 있는 속성, 쪼개어 져도 의미를 갖을 수 있어야함\nex) 주소: 시 + 군 + 구 + 번지\n\nsimple 와 composite attribute를 구분짓는 기준은 나라의 사회 제도나 단체의 시스템의 특성에 따라 변한다\n\nex) 한국은 우편 수집 창고와 우체국의 시스템에 따라 주소를 시 + 군 + 구 + 번지 또는 시 + 군 + 구 가 필요로 할 수 있다\n하지만 다른 나라는 주소 전체를 쓰는 시스템이라면 주소 속성이 simple attribute 로 남을 수 있다.\n이름의 경우 성과 이름을 가르는 시스템이 필요한 미국과 full name을 사용하는 한국의 시스템의 경우 각 상황에 맞게 modeling을 해야한다.\n어떤 속성이든 분해해야할 용도가 있다면 쪼개야한다.\n\n위의 그림 예시에서\n\ncomposite attribute: registration (자동차 번호판)\nsimple attribute: state(주이름) & number(자동차 고유번호)\n\n\nderived vs stored attributes\n\nderived attribute (점선으로 표기) : 저장된 다른 데이터로부터 유도 가능한 속성\n\n총 학생 수: 그냥 instance나 학생 수를 카운트 하면 됨, 총 학생 수라는 속성은 없어도 됨\n각 과목의 성적 : 총점(derived), 평점(derived)\n주민등록번호: 나이(derived), 생일(derived)\nderived attribute는 자주 쓰이는 통계치를 구할 때 자주 쓰임. DB에 저장할 필요는 없다.\n\nstored attribute: 위에서 말한 총 학생 수, 총점, 평점, 나이, 생일과 같은 derived attribute의 연산이 너무 무겁거나 너무 빈번하게 사용되는 상황이라면 DB에 data를 적재하여 연산량을 줄이는 방법 도 있다.\n\n설계자의 재량에 따라 stored와 derived의 구분 짓는다.\n실무에서는 파생 변수(derived)가 자주 쓰인다면 stored (실선)로 남긴다\n학계에서는 derived는 가급적 점선으로 표시하여 derived상태로 남긴다\n\n\nKey Attributes\n\nkey attributes: 유일성 + 최소성 을 만족시켜야함\n\n어떤 개체에 대해서 그 인스턴스가 항상 유일한 값을 갖는 속성 또는 속성들의 집합\n\n중복되는 값을 가지면 안됨\n키 속성은 밑줄을 그어 표시\nex) 학생의 학번, 책의 ISBN, 차량번호\n\n\n특정 스냅샷이 아닌 해당 개체의 모든 가능한 스냅샷의 집합을 고려하여 파악되어야함 (개체가 아무리 많아 지더라도 항상 유일한 값을 가져야함)\n\nex) 다음의 ssn, 이름, 혈액형 중 키 속성은 ssn\n\ncomposite key(복합키)\n\nentity에서 키 속성자체가 없을 경우 attributes의 조합으로도 생성가능\n복합키는 최소성을 가져야함 : 최소한의 attributes로 복합키를 만들어야함\n\n용어 정리\n\nconceptual design: key는 identifier라고 부르고\nlogical design: key는 primary key라고 부름.\n\ntable을 만들게 되면서 primary key라 부름\n\n하지만 identifier ≠ primary key.\n보통 identifier로 primary key를 만듦\n\nprimary key 한 table에 반드시 1개만 있어야함. 없어도 안됨. primary key없으면 DB가 아님\n\nEntity Types\n\n강성 개체(strong entity)\n\n각 개체는 하나 이상의 key 속성을 가질 수 있음\n대부분의 개체는 key를 갖기 때문에 강성 개체라 부르는 경우는 별로 없다. 그냥 개체라 부름\n\n약성 개체 (weak entity)\n\n어떤 개체는 key를 갖지 않을 수 있음\n자체적으로 식별될 수 없고, 다른 엔티티에 의존하는 엔티티\n\n\n\n\n\n\n\n\n관계에 참여하는 엔티티 인스턴스의 수\n표현: 1:1, 1:N, M:N 등으로 표시\nex) 한 학생은 여러 강의를 수강할 수 있음 (1:N)\n\n\n\n\n\n엔티티의 관계 참여 여부\n필수 참여 (전체 참여): 이중선으로 표시\n선택 참여 (부분 참여): 단일선으로 표시\n\n\n\n\n\n상위 엔티티와 하위 엔티티 간의 관계\n표현: 삼각형으로 연결\nex) ‘사람’ 엔티티의 특화로 ‘학생’과 ’교수’ 엔티티"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_0.data_standard.html#데이터-표준관리-목적",
    "href": "docs/blog/posts/Governance/5_0.data_standard.html#데이터-표준관리-목적",
    "title": "Data Governance Study - Data Standard Governance",
    "section": "",
    "text": "각 데이터 항목에 대한 명명 규칙을 정의\n표준단어 및 표준용어, 표준도메인, 표준코드 등을 전사관리기준 형태로 정의\n이러한 표준들을 변경 및 관리하는 활동 수행"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_0.data_standard.html#데이터-표준관리-정의",
    "href": "docs/blog/posts/Governance/5_0.data_standard.html#데이터-표준관리-정의",
    "title": "Data Governance Study - Data Standard Governance",
    "section": "",
    "text": "데이터 표준관리는 데이터 항목들에 대해서 전사적으로 일관된 명명과 정의를 부여하고 이를 정제, 개선하는 활동이다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5_0.data_standard.html#주요-정제-및-개선-사항",
    "href": "docs/blog/posts/Governance/5_0.data_standard.html#주요-정제-및-개선-사항",
    "title": "Data Governance Study - Data Standard Governance",
    "section": "",
    "text": "데이터 표준관리에서 중점적으로 다루는 정제 및 개선 사항은 다음과 같다:\n\n\n\n대상\n정제 및 개선 결과물\n\n\n\n\n명칭 + 정의\n단어 및 용어\n\n\n형식\n도메인\n\n\n규칙\n코드\n\n\n\n이러한 표준화 활동을 통해 조직 전체에서 일관된 데이터 관리가 가능해진다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5_1.data_word_dictionary.html",
    "href": "docs/blog/posts/Governance/5_1.data_word_dictionary.html",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "조직 내에서 사용되는 데이터 용어를 표준화하고 관리하기 위한 도구이다. 이는 데이터 거버넌스와 데이터 품질 관리의 중요한 구성 요소이다.\n조직에서 사용되는 모든 데이터 용어의 공식적인 정의를 제공\n각 용어의 의미, 사용 맥락, 형식 등을 명확히 기술\n데이터 표준관리를 위한 표준 단어 사전을 만드는 과정은 체계적이고 협력적인 접근이 필요하다.\n표준 단어 사전을 만드는 과정은 반복적이고 지속적으로 이루어져야 하며, 조직의 변화와 새로운 요구사항을 반영하여 계속 발전시켜 나가야 한다.\n\n표준 단어 사전은 단순한 용어 목록이 아니라 조직의 데이터 자산을 효과적으로 관리하고 활용하기 위한 중요한 도구이다.\n조직의 모든 구성원이 쉽게 접근하고 활용할 수 있어야 한다.\n\n또한, 기술적인 구현뿐만 아니라 조직 문화와 프로세스의 변화도 함께 고려해야 한다.\n표준 용어 사전과 데이터 카탈로그를 제작하기 위한 성분이 된다.\n\n\n\n\n일반적으로 단어란 문법상 일정한 뜻과 구실을 가지는 말의 최소 단위를 의미한다.\n정보 시스템에서 사용하는 표준 단어란 회사에서 업무상 사용하며 일정한 의미를 갖고 있는 최소 단위의 단어를 말한다.\n표준용어를 구성 하는데 사용한다.\n예시\n\n표준 단어: 정산, 승인, 금액\n표준 용어: 정산승인금액\nDB 속성 또는 Data Modeling을 위한 속성\n\n\n\n\n속성명\n데이터타입\n도메인\n\n\n\n\n정산승인금액\nNUMBER(10)\n금액n10\n\n\n\n\n\n\n\n\n\n\n\n한 개의 단어에 대해 표준화된 영문약어를 사용하여 일관성을 확보한다.\n조직 전체에서 데이터 용어를 일관되게 사용할 수 있도록 한다.\n동일한 개념에 대해 서로 다른 용어를 사용하는 문제를 방지한다.\n\n\n\n\n\n유사하거나 중복된 용어를 식별하고 제거\n\n\n\n\n\n명명 규칙, 약어 사용, 데이터 형식 등을 표준화\n데이터 모델링과 시스템 개발에서 일관된 기준을 제공\n데이터 표준 단어 용도의 목적은 조직내 정보 공유가 제 1 목적이기 때문에 반드시 공공 기관이나 권위있는 조직의 양식을 따를 필요는 없다.\n이상적인 접근 방식은 외부 표준과 조직 내부의 요구사항을 균형 있게 고려하는 것이다.\n\n\n\n\n\n\n표준단어는 단어명, 영문명, 영문약어명 및 정의 등으로 구성되며 약어는 영문명을 축약하여 작성한다.\n\n\n[단어명]\n\n표준단어를 구성하는 최소단위의 단위를 의미하며 한글 및 영문, 숫자로 정의한다.\n표준단어의 최대길이는 15자로 하며, 10자 이내로 작성할 것을 권장한다.\n\n[영문명]\n\n표준단어명의 영문 명칭을 의미한다.\n영문명의 첫 자리의 알파벳은 대문자로 하고 나머지 부분은 소문자로 하며,영문 단어 간에는 띄어쓰기를 한다.\n\n[정의]\n\n해당 단어가 뜻하는 것 혹은 내용을 의미한다.\n데이터 명칭을 그대로 서술하거나 약어 또는 전문 용어를 이용한 기술은 가급적 지양한다.\n\n[영문약어명]\n\n영문명의 축약된 형태의 영문명칭을 의미하며 영문명을 바탕으로 영문약어를 정의한다.\n영문약어의 최대 길이는 4자로 한다.(권장)\n단, 고유명사나 관용적인 표현 등의 경우 글자수에 대한 예외를 허용한다.\n\n[단어 유형]\n\n기본단어(수식어성)\n\n용어를 구성하는 단어로 용어의 마지막에 존재할 수 없는 단어\n주제어, 수식어, 접두사/접미사, 복합단어 등이 이에 속한다\n예시, 주문,고객, 상품,거래\n\n분류단어 (도메인성)\n\n용어를 구성하는 단어로 용어의 마지막에 존재하는 단어\n용어가 가질 수 있는 속성 정보(데이터 형식 및 길이)를 구체적으로 표현하기 위해 사용한다\n분류단어는 도메인과 매핑 되어 그 속성정보를 정의한다\n예시, 금액,수량, 성명,코드, 번호,일자\n\n예시\n\n기본 단어: 정산, 승인\n분류 단어: 금액\n표준 용어: 정산승인금액\n\n\n예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n단어명\n한자\n영문명\n정의\n영문약어명\n단어종류\n단어유형\n\n\n\n\n가격\n價格\nPrice\n물건이 지니고 있는 가치를 돈으로 나타낸 것\nPRC\n기본단어\n단일어\n\n\n연령\n年齡\nAge\n나이(사람 등 세상에서 나서 살아온 햇수)\nAGE\n기본단어\n분류어\n\n\n우편번호\n郵便番號\nZip Code\n우편물을 쉽게 분류하기 위하여 정보 통신부에서 각 지역마다 매긴 번호\nZPCD 또는 ZIP(관용적표현)\n분류단어\n복합어\n\n\n\n\n\n\n단일어\n\n하나의 형태소(形態素: 의미의 기능을 부여하는 언어의 형태론적 수준에서의 최소단위)로 성립된 단어\n\n고유명사(기관명 포함), 지명(명사) 단일어로 사용함\n접사(접두사 및 접미사)와 합성된 단어\n두 개의 단일어로 구성되나 영문단어가 각각의 단일어의 영문단어의 조합과 일치 하지 않고 다른 영문단어가 존재하는 경우\n\n예시) 고객,지점, 가격,시설\n\n복합어\n\n둘 이상의 어근(실질 형태소)이 결합 이루어진 단어\n예시) 전화번호, 휴대폰번호, 차용지점\n\n외래어\n\n원래 외국어였던 것이 국어의 체계에 동화되어 사회적으로 그 사용이 허용된 단어\n예시) 이메일, 팩스, 네비게이션\n\n관용어\n\n한글 단어와 외래어가 결합되어 사용되거나 기타 국어 체계에 부합하지 않더라도 관용적으로 자주 사용되며 의미가 명확한 단어\n일반 사회나 기관에서 관습적으로 널리 쓰는 말\n예시) VDC,ABS, 셀프계약서, MT\n\n접사 (접두사/접미사)\n\n접두사(Prefix): 어떤 낱말 앞에 붙어서 의미를 첨가하여 한 다른 낱말을 이루는 말\n접미사(Suffix): 낱말의 끝에 붙어 의미를 첨가하여 다른 낱말을 이루는 말\n예시) 미사용, 보증료\n\n\n\n\n\n\n동음이의어\n\n발음은 동일하나 의미가 다른 단어(Homonym)\n예시: 매수(Sheets Count) / 매수(Buying)\n\n이음동의어\n\n동일한 의미를 표현하는 두 개의 다른 단어 (Synonym)\n즉, 발음은 다르나 동일한 의미를 표현하는 단어\n예시: 이수(Completion) / 수료(Completion)\n\n금칙어\n\n표준단어 사용의 일관성을 위하여 사용하지 못하게 지정된 단어\n예시: 이해당사자(X), 이해관계자(O)\n\n유사어\n\n표준단어 사용의 일관성을 위하여 권장어(대체어) 사용을 권고하는 단어\n예시: 수정, 변경, 정정\n\n축약어\n\n줄여서 간략하게 표현하는 단어\n주민번호(X), 주민등록번호(O)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준단어는 단일어 형태의 명사형 낱말을 표준단어로 정의하는 것을 기본 원칙으로 한다.\n고객, 가격, 금액, 지점\n\n\n2\n• 동일한 의미의 단어를 한글과 영문으로 중복해서 정의하지 않는다.\nRENT(X) → 렌트(O)\n\n\n3\n• 축약된 형태의 단어로 정의하지 않는다. 단, 범용 또는 공식적으로 사용이 승인된 약어는 표준 원칙에 의거하여 사용할 수 있다.• 또한 원래 단어가 너무 길거나 잘 활용하지 않아서 업무적으로 축약된 단어를 주로 사용하는 경우에 한하여 사용할 수 있다• 용어명 길이 제약을 해결하기 위해 부득이하게 약어를 사용해야 할 경우에는 약어와 전체 단어를 모두 표준단어로 등록하도록 한다\n주민번호(X) → 주민등록번호(O)등평(X) → 등급평가(O)\n\n\n4\n• 한글 축약어는 다른 단어와 붙여서 쓸 경우 혼동이 될 우려가 있으므로 가급적 풀어 쓴 단어를 사용한다. 단, 어감상 필요 시는 예외로 적용할 수 있다. (이음동의어)\n가(假)(X) → 임시(O)현(X) → 현재(O)전(X) → 이전(O)후(X) → 이후(O)\n\n\n5\n• 과거 단어와 현대 단어를 함께 사용하고 있는 경우 가급적 현대 단어를 사용한다.\n\n\n\n6\n• 접두사 및 접미사는 별도로 분리 하지 않으며 표준단어(단일어)로 등록함을 원칙으로 한다\n\n\n\n7\n• 고유명사(기관명 포함)는 한글 단어를 사용하는 것을 원칙으로 한다\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 외국에서 들어온 말로 국어처럼 쓰이는 외래어는 영문을 쓰지 않고 외래어 한글 표기법에 따라 정의하는 것을 원칙으로 한다.\nFax(X) → 팩스(O)Email(X) → 이메일(O)Database(X) → 데이터베이스(O)\n\n\n2\n• 영문을 대체할 적절한 한글이 없거나 영문자체에 고유한 업무적 의미를 담고 있는 경우, 또한 관용적으로 두음문자 표현이 사용되고 있는 경우에는 해당 영문을 그대로 사용한다. 단, 표준단어로 등록 시 해당 영문의 전체명이 아닌 두음문자 형식의 대문자로 등록한다.\nMMF, HTML, SQL\n\n\n3\n• 영문 단어 첫글자는 알파벳 대문자로 나머지는 소문자로 작성한다.\nSWIFT(X) → Swift(O)\n\n\n4\n• 영문 단어를 한글화 할 경우, 영문 의미로 한글화 한 경우와 소리 나는 대로 한글화한 경우 둘 다 자주 사용될 경우, 소리 나는 대로 한글화한 단어를 표준으로 하고, 의미로 한글화 한 단어는 금칙어로 등록한다.\nERROR(X), 오류(X) → 에러(O)\n\n\n5\n• 한글 단어 보다 더 친숙하게 사용되는 영문 단어의 경우에는 그 영문단어를 사용한다.\n인터넷프로토콜(X), 아이피(X) → IP(O)\n\n\n6\n• 관용적으로 사용하는 단어나 고유명사인 경우 특수문자 중 ‘/’, ‘-’, ’&’만을 사용할 수 있으나, 가급적 특수문자를 사용하지 않는다\nM&A(O)\n\n\n7\n• 한글, 영문, 숫자의 혼용이 가능하다\n회사(O), 12월(O)\n\n\n8\n• 한글과 영문만 표준 단어로 인정하며 기타 외국어(한자, 일본어 등)는 사용하지 않는다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 업무적 또는 관용적으로 자주 쓰이는 표현이나 단일어 단위로 구분해서 사용할 경우 의미 전달이 불분명해질 수 있는 단어에 대해서는 복합어로 구성 사용한다\n계좌번호(Account Number) = 계좌(Account)+번호(Number)\n\n\n2\n• 고유명사(기관명 포함)는 단일어 형태로 사용한다.\n회사\n\n\n3\n• 접두사 및 접미사와 합성된 단어는 단일어 형태로 사용한다\n재[再]발급, 지급처[處]\n\n\n4\n• 두 개 이상의 단일어로 이루어졌으나 별도의 영문 단어가 존재하며 각 단일어의 조합과는 다른 의미를 지니게 되는 경우 단일어 형태로 사용한다.\n감가상각(Depreciation) ≠ 감가(Reduction)+ 상각(Repayment)\n\n\n5\n• 한글과 외래어가 결합되어 사용되거나 기타 사용 원칙에 부합하지 않더라도 관용적으로 자주 사용되며 의미가 명확한 경우 사용한다. (관용적 표현)\n파레트수량\n\n\n6\n• 유형의 구분을 나타내는 복합 단어는 단어로 식별하지 않도록 한다. 이때에는 유형을 대표하는 다른 단어 또는 용어로 대체하도록 한다. 단, 관용적으로 자주 쓰이는 표현이면서 대체 단어 또는 용어 구성이 어렵다면 사용을 허용한다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 숫자 사용 시 아라비아 숫자 사용을 원칙으로 한다\n삼순위(X) → 3순위(O)\n\n\n2\n• 숫자만으로는 단어가 될 수 없고 해당 숫자의 의미를 나타내는 단어와 함께 조합하여 사용한다\n6(X) → 6개월(O)\n\n\n3\n• 숫자와 단위의 합성어는 단일어로 등록한다\n100퍼센트, 91일\n\n\n4\n• 단위 앞에 올 수 있는 숫자의 유효값이 제한적인 경우 유효한 단어를 모두 등록한다.\n1월,2월 ~ 12월1분기,2분기,3분기,4분기1순위,2순위,3순위\n\n\n5\n• 단위 앞에 올 수 있는 숫자의 유효값이나 범위가 제한이 없는 경우 해당 단위를 등록하고 최소 1단위와 합성된 단어를 등록한다\n퍼센트 - 1퍼센트개월 -1개월차 - 1차급 - 1급\n\n\n6\n• 숫자 단위 대에 따라 표준을 다르게 정의할 경우 각각을 단어로 등록할 수 있다\n1원,1십원,1천원,1만원,1십만원,1백만원,1천만원\n\n\n7\n• 숫자와 조합된 단어의 의미가 불분명한 경우는 해당 단어의 사용을 지양한다 단, 의미가 불분명한 단어임에도 업무상의 필요로 인해 등록이 불가피한 경우 단어 뒤에 숫자를 붙여서 정의한다\n컬럼1(X)※ 단 불가피한 경우 사용\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 동음이의어(Homonym)는 허용하지 않는다 - (대체방안1) 다른 한글단어로 교체하여 표준 단어로 등록하여 사용한다. - (대체방안2) 어감상 동음이의어인데도 불구하고 ’의사’라는 단오를 꼭 써야 하는 경우는 ’의사결정’식의 복합 단어를 표준단어로 등록하여 사용한다• 단, 대체 단어가 없는 경우 사용할 수 있다 (한글 명이 같더라도 영문 명은 반드시 달라야 한다) 이 경우 데이터 표준관리자 및 모델관리자에게 검토 요청을 신청 한다\n다리(leg) : 다리다리(bridge) : 교량의사(doctor) : 의사의사(idea)결정(Decision):의사결정\n\n\n2\n• 이음동의어(Synonym)는 용어의 혼돈과 용어생성 시 중복발생 가능성 때문에 가급적 사용하지 않는다.• 대표단어를 정하고, 그 대표 단어만을 사용하도록 하며 나머지는 금칙어로 등록하여 사용을 제한하도록 한다\n핸드폰(X) → 휴대폰(O)※ 핸드폰은 금치어로 등록\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 이음동의어는 대표 단어만을 표준단어로 정의하여 사용하도록 하며, 나머지 이음 동의어의 단어들은 금칙어로 정의하여 사용을 사전에 방지한다.\n나이(X) → 연령(O)금일(X) → 당일(O)\n\n\n2\n• 축약된 형태의 단어(축약어)는 금칙어로 정의하여 사용을 사전에 방지한다.\n주민번호(X) → 주민등록번호(O)\n\n\n3\n• 사람에 해당하는 접미사 “자”와 “인”으로 구성된 복합어의 경우에는 사용빈도가 높은 단어를 표준단어로 정의하고, 다른 나머지 단어를 금칙어로 관리하여 사용을 사전에 방지한다\n장애자(X) → 장애인(O)신청인(X) → 신청자(O)\n\n\n4\n• 한글 맞춤법을 고려하지 않고 관용적으로 사용되던 단어들은 금칙어로 정의하여 사용을 사전에 방지한다.\n갯수(X) → 개수(O)써비스(X) → 서비스(O)\n\n\n5\n• 영문 단어의 경우 한글화를 원칙으로 한다. 단, 한글 단어 보다 더 친숙하게 사용되는 영문 단어의 경우에는 그 영문 단어를 사용한다. 이렇게 선정된 단어에 대응되는 영문 혹은 한글 단어는 금칙어로 등록하고 사용을 제한하도록 한다\nERROR(X) → 에러(O)FAX(X) → 팩스(O)EMAIL(X) → 이메일(O)아이피(X) → IP(O)\n\n\n6\n• 범용적으로 사용되는 외래어의 경우 사용할 수 있지만 표기법을 고려하여 표준 단어를 지정하고, 나머지 단어는 금칙어로 등록하여 사용을 제한하도록 한다\nEXPOSURE(X), 익스포저(X) → 익스포져(O),화일(X), FILE(X) →파일(O)\n\n\n7\n• 전문 업무용어 중 관용적으로 영문의 두문자어(頭文字語:Acronyms)를 사용하는 경우 한글단어는 금칙어로 정의한다.\n\n\n\n8\n• 기관명 등이나 고유명사 등은 전체 단어를 표준으로 사용하고, 한글 약어는 금칙어로 등록한다.\n\n\n\n9\n• 시스템명중 관용적으로 영문의 두문자어(頭文字語:Acronyms)를 사용하는 경우 한글단어는 금칙어로 정의한다\n데이터웨어하우스(X) → DW(O)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 단어의 대표 자음으로 영문 약어를 구성한다. 대표 자음의 적용 우선 순위는 앞 자리의 자음부터 4 글자까지 적용한다. 모음(A,E,I,O,U)외 모든 알파벳이 자음이다.• 약어 중복 발생 시 모음을 순차적으로 추가해서 약어를 생성한다.\n대여: Rental → RNTL\n\n\n2\n• 가급적 4글자의 약어로 구성하되, 고유명사나 관용적인 표현 등의 경우 글자수에 대한 예외를 허용한다\n배치: BATCH\n\n\n3\n• 영문 약어의 글자수가 4글자 이하인 경우에는 가장 앞자리의 모음을 대표 자음으로 간주하여, 이 모음 앞뒤에 나오는 대표 자음과 결합하여 4자리 약어를 적용한다.• Y와 같이 자음이지만 모음으로 혼동하는 알파벳도 모두 자음으로 간주한다.\n급여: Color → COLR\n\n\n4\n• 영문 약어명의 사용시 한글 발음 식의 약어는 사용하지 않으나 순수 한글이거나 대체할 영문이 없는 경우 허용한다\n주소: JUSO → ADR, 시: SI, 군: GUN, 구: GU\n\n\n5\n• 영문 약어는 항상 유일성을 유지하도록 관리한다\n\n\n\n6\n• 두 개의 자음이 연속할 경우, 가급적 한 개의 자음은 생략하는 형태로 표현한다.\n결제: Settlement → STTL (X) → STL (O)\n\n\n7\n• 범용적으로 사용되는 두문자어(頭文字語: Acronyms)가 있는 경우 되도록 두문자어를 사용하며 글자수가 지나치게 긴 경우 새로운 영문약어를 작성한다.\nSQL(Structured Query Language), UDM(Universal Data Model)\n\n\n8\n• 한글단어에 대해 관용적으로 사용되는 영문 약어가 있으면 이를 사용한다.\nNumber→NMBR(X)→ NO(O)\n\n\n9\n• 영문명을 일반적으로 축약해서 사용되는 영문 약어가 있는 경우 이를 사용한다.\nPackage → PCKG (X) → PKG (O)\n\n\n10\n• 한 영문명의 글자수가 4글자 이하의 경우는 그대로 사용한다. 단, 합성어의 경우 4글자 이상을 사용할 수 있다\n면적 : Area → AREA (O), 역할 : Role → ROLE (O)\n\n\n\n\n\n\n\n\n\n\n표준 단어 사전의 목적과 범위를 명확히 한다.\n\n목적이라 함은 데이터 일관성 확보(동일한 의미의 데이터 사용), 데이터 품질 향상 (데이터 처리 과정 비용 감소), 시스템 통합 지원 (여러 시스템 간 데이터 매핑과 통합), 규제 준수 지원 등을 의미한다.\n범위는 대상 데이터 범위, 조직적 범위, 시스템 범위 등을 의미하며, 너무 광범위한 영역은 제작 실패로 이어진다.\n\n이해관계자 식별: 관련 부서와 담당자들을 파악한다.\n거버넌스 체계 수립: 단어 사전 관리를 위한 조직과 프로세스를 정립한다.\n\n\n\n\n\n기존 데이터 모델, 데이터베이스 스키마, 업무 문서 등에서 사용 중인 단어들을 수집한다.\n업무 도메인별로 사용되는 용어들을 취합한다.\n\n\n\n\n\n동의어, 유사어, 약어 등을 식별한다.\n업무 영역별 용어의 의미와 사용 맥락을 분석한다.\n불필요하거나 중복된 단어들을 제거한다.\n\n\n\n\n\n약어 사용 규칙, 대소문자 규칙 등의 명명 규칙을 정의합니다.\n단어 정의 형식을 standardization 합니다.\n도메인별 특수 규칙을 설정한다.\n\n유용한 데이터 표준안을 만들기 위해 조직 내의 각 업무 영역 또는 데이터 도메인에 맞는 고유한 규칙이나 지침을 만들어야 한다.\n도메인 식별: 재무, 인사, 마케팅, 제조, 고객 서비스 등\n각 도메인별 특수성 파악: 해당 도메인에서만 사용되는 용어나 개념과 도메인 특유의 데이터 형식이나 제약 조건\n예시\n\n재무 도메인의 통화 표기 규칙: “USD 1,000.00” 또는 “1,000,000 원” 형식 사용\n재무 도메인의 회계 기간 표현: “FY2023Q2” (2023 회계연도 2분기)\n인사 도메인의 직급 코드 체계: “M” (매니저), “D” (디렉터)\n인사 도메인의 근속 연수 계산 규칙: 입사일 기준, 월 단위 반올림\n마케팅 도메인의 캠페인 코드 형식: “CAM_2023_SUMMER_01”\n마케팅 도메인의 고객 세그먼트 분류 기준: “VIP”, “REGULAR”, “NEW”\n제조 도메인의 제품 코드 체계: “PROD-A01-R” (제품군-모델번호-버전)\n제조 도메인의 품질 등급 표기: “A”, “B”, “C” 등급 사용\n\n\n\n\n\n\n\n분석된 단어들 중 표준으로 사용할 단어들을 선정\n선정 기준을 명확히 하고, 이해관계자들의 합의를 도출\n선정 기준 예시\n\n명확성\n\n의미가 명확하고 모호하지 않은 단어 선정 (일반적인 단어는 한정시킬 것)\n예: “고객” 대신 “활성고객”과 “비활성고객”으로 구분\n\n일관성\n\n조직 전체에서 일관되게 사용할 수 있는 단어\n예: 부서별로 다르게 사용되던 용어를 하나로 통일\n\n간결성\n\n가능한 간단하고 간결한 단어\n예: “제품구매고객정보” 대신 “구매자정보”\n\n유일성\n\n중복되지 않는 고유한 의미를 가진 단어\n예: 동음이의어 피하기\n\n업계 표준 부합성\n\n가능한 업계에서 널리 사용되는 표준 용어 선택\n예: 금융업계의 “ROI” (Return on Investment)\n\n확장성\n\n향후 변화나 확장을 고려한 단어 선택\n예: “2023년예산” 대신 “연간예산”\n\n이해 용이성\n\n비전문가도 이해하기 쉬운 단어\n예: 전문 용어보다는 일반적인 비즈니스 용어 선호\n\n번역 가능성\n\n다국어 지원이 필요한 경우, 번역이 용이한 단어\n관용구나 은유적 표현 피하기\n\n기존 시스템 호환성\n\n기존 시스템과의 호환성을 고려한 단어\n예: 레거시 시스템의 주요 용어 유지\n\n법규 및 규제 준수\n\n관련 법규나 규제를 준수하는 단어\n예: 개인정보보호법에 부합하는 용어 선택\n\n도메인 적합성\n\n해당 업무 도메인에 적합한 단어\n예: 금융 도메인에서는 “이자율”, 제조 도메인에서는 “불량률”\n\n측정 가능성\n\n정량적 측정이 가능한 개념을 나타내는 단어\n예: “고객만족도” (1-5 척도로 측정 가능)\n\n약어 사용 규칙\n\n약어 사용 시 일관된 규칙 적용\n규칙에 부합하지 않는 관용어는 관용어 사용을 유지하는 것이 좋다.\n예: “고객번호”를 “CUST_NO”로 통일\n\n\n\n\n\n\n\n각 단어에 대한 상세 정보를 정의 (정의, 동의어, 사용 예시, 관련 업무 영역 등).\n단어 간의 관계를 정의 (상위어, 하위어, 관련어 등)\n\n\n\n\n\n선정된 표준 단어들에 대해 관련 부서와 전문가들의 검토를 받는다.\n필요시 수정하고 최종 승인을 받는다.\n\n\n\n\n\n승인된 단어들을 데이터베이스나 전문 도구에 등록한다. (엔코아, Microsoft Purview 등)\n검색, 조회, 관리가 용이한 형태로 구성한다.\n\n\n\n\n\n완성된 표준 단어 사전을 조직 내에 공유\n사용 방법과 중요성에 대한 교육을 실시\n\n\n\n\n\n새로운 단어 추가, 기존 단어 수정, 폐기 등의 프로세스를 수립한다.\n정기적인 검토와 업데이트를 수행한다.\n사용 현황을 모니터링하고 피드백을 수집한다.\n\n\n\n\n\n데이터 모델링, 시스템 개발, 보고서 작성 등의 프로세스와 표준 단어 사전을 연계한다\n\n연계 순서는 각 상황마다 다르다.\n이미 ERD와 DB가 존재하는 시스템들을 통합하는 상황이라면 표준 단어 사전을 순서상 나중에 제작하는 것이 유리하다\n그 반대라면 표준 단어사전을 먼저 만들어 DB를 만드는 것이 유리할 수 있다.\n\n다른 데이터 관리 도구들과의 통합을 고려한다.\n\n\n\n\n\n\n예시 1\n\n\n\n\n표준 단어 사전 예시\n\n\n\n예시 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n단어 ID\n한글명\n한자\n영문명\n영문약어\n정의\n도메인\n데이터 타입\n길이\n허용값\n관련 업무 영역\n사용 예시\n동의어/유사어\n상위어\n하위어\n등록일\n최종 수정일\n승인 상태\n\n\n\n\nW001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW002\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW003\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n칼럼 설명:\n\n단어 ID: 각 단어의 고유 식별자\n한글명: 한글로 된 단어명\n한자: 해당 단어의 한자 표기 (필요한 경우)\n영문명: 영문으로 된 단어명\n영문약어: 영문 약어 (필요한 경우)\n정의: 단어의 명확한 정의\n도메인: 해당 단어가 속한 비즈니스 도메인\n데이터 타입: 해당 단어의 데이터 타입 (예: VARCHAR, INTEGER 등)\n길이: 데이터 길이\n허용값: 허용되는 값의 범위 또는 목록\n관련 업무 영역: 해당 단어가 주로 사용되는 업무 영역\n사용 예시: 실제 사용 예시\n동의어/유사어: 관련된 동의어나 유사어\n상위어: 해당 단어의 상위 개념 단어 (여러 개일 경우 쉼표로 구분)\n\n상위어: 이해관계자\n\n하위어: 해당 단어의 하위 개념 단어들 (여러 개일 경우 쉼표로 구분)\n\n하위어: 개인고객, 법인고객, VIP고객\n\n등록일: 단어가 사전에 처음 등록된 날짜\n최종 수정일: 마지막으로 수정된 날짜\n승인 상태: 현재 승인 상태 (예: 승인됨, 검토 중, 폐기 등)\n\n이 템플릿은 조직의 필요에 따라 수정하거나 필드를 추가/제거할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#데이터-표준-단어-사전이란",
    "href": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#데이터-표준-단어-사전이란",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "조직 내에서 사용되는 데이터 용어를 표준화하고 관리하기 위한 도구이다. 이는 데이터 거버넌스와 데이터 품질 관리의 중요한 구성 요소이다.\n조직에서 사용되는 모든 데이터 용어의 공식적인 정의를 제공\n각 용어의 의미, 사용 맥락, 형식 등을 명확히 기술\n데이터 표준관리를 위한 표준 단어 사전을 만드는 과정은 체계적이고 협력적인 접근이 필요하다.\n표준 단어 사전을 만드는 과정은 반복적이고 지속적으로 이루어져야 하며, 조직의 변화와 새로운 요구사항을 반영하여 계속 발전시켜 나가야 한다.\n\n표준 단어 사전은 단순한 용어 목록이 아니라 조직의 데이터 자산을 효과적으로 관리하고 활용하기 위한 중요한 도구이다.\n조직의 모든 구성원이 쉽게 접근하고 활용할 수 있어야 한다.\n\n또한, 기술적인 구현뿐만 아니라 조직 문화와 프로세스의 변화도 함께 고려해야 한다.\n표준 용어 사전과 데이터 카탈로그를 제작하기 위한 성분이 된다.\n\n\n\n\n일반적으로 단어란 문법상 일정한 뜻과 구실을 가지는 말의 최소 단위를 의미한다.\n정보 시스템에서 사용하는 표준 단어란 회사에서 업무상 사용하며 일정한 의미를 갖고 있는 최소 단위의 단어를 말한다.\n표준용어를 구성 하는데 사용한다.\n예시\n\n표준 단어: 정산, 승인, 금액\n표준 용어: 정산승인금액\nDB 속성 또는 Data Modeling을 위한 속성\n\n\n\n\n속성명\n데이터타입\n도메인\n\n\n\n\n정산승인금액\nNUMBER(10)\n금액n10"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#필요성",
    "href": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#필요성",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "각 용어에 대한 부가 정보(예: 데이터 타입, 길이, 허용 값 등)를 관리할 수 있다.\n데이터의 의미와 구조에 대한 종합적인 이해를 제공한다.\n\n\n\n\n\n비즈니스 사용자와 IT 전문가 사이의 의사소통을 원활하게 한다\n편의나 관습에 따라 동일한 의미로 사용되는 이음동의어를 대표성을 지닌 한 개의 단어로 정의함으로써 의사소통과 데이터에 대한 인식의 오류를 방지할 수 있다\n용어의 오해로 인한 프로젝트 지연이나 오류를 방지한다.\n\n\n\n\n\n데이터 관련 규제 요구사항을 충족하는 데 도움을 준다.\n\n데이터 관련 규제 요구사항: 데이터의 수집, 저장, 처리, 공유, 및 폐기와 관련된 법적, 규범적 기준을 의미\n데이터 보안, 개인정보 보호, 데이터 품질, 및 데이터 관리의 투명성을 보장하기 위한 규제\n\n데이터의 의미와 사용에 대한 명확한 문서화를 제공한다.\n데이터 규제 종류\n\nGDPR (General Data Protection Regulation)\n\nEU의 개인정보 보호법\n예시: 개인 데이터의 수집 및 처리에 대한 명시적 동의 요구, 데이터 삭제 권리(잊힐 권리) 보장\n\nCCPA (California Consumer Privacy Act)\n\n캘리포니아 주의 소비자 개인정보 보호법\n예시: 소비자의 개인정보 접근 권리, 개인정보 판매 거부 권리\n\nHIPAA (Health Insurance Portability and Accountability Act)\n\n미국의 의료정보 보호법\n예시: 환자 의료 정보의 비밀성 보장, 의료 정보 접근 로그 유지\n\nPCI DSS (Payment Card Industry Data Security Standard)\n\n신용카드 정보 보호 표준\n예시: 카드 소지자 데이터의 암호화, 정기적인 보안 시스템 및 프로세스 검사\n\nSOX (Sarbanes-Oxley Act)\n\n미국의 기업 회계 개혁법\n예시: 재무 보고의 정확성과 신뢰성을 보장하기 위한 데이터 관리 요구사항\n\n개인정보 보호법\n\n한국\n예시: 개인정보 수집 시 동의 획득, 개인정보의 안전한 보관 및 파기\n\n전자금융거래법\n\n한국\n예시: 전자금융 거래 기록의 보관 및 보안 요구사항\n\n정보통신망법\n\n한국\n예시: 개인정보 유출 시 신고 의무, 정보보호 관리체계 인증\n\n데이터 현지화 법률\n\n여러 국가\n예시: 러시아의 데이터 현지화법, 중국의 사이버보안법에 따른 데이터 현지 저장 요구\n\n\n이러한 규제 요구사항을 충족하기 위해서는 데이터의 정의, 분류, 처리 방법 등이 명확히 문서화되고 관리되어야 한다.\n표준 단어 사전은 데이터 요소를 일관되게 정의하고 분류하는 데 도움을 줌으로써, 규제 준수를 위한 기반을 제공한다.\n예를 들어, 개인식별정보(PII)가 무엇인지, 어떤 데이터 필드가 이에 해당하는지를 명확히 정의하고 관리할 수 있게 해준다.\n\n\n\n\n\n일관된 데이터 정의와 사용으로 전반적인 데이터 품질을 개선한다.\n데이터 통합과 분석의 정확성을 높인다.\n\n\n\n\n\n조직의 데이터 관련 지식을 체계적으로 축적하고 공유할 수 있다.\n신규 직원의 온보딩과 지식 전달을 용이하게 한다.\n\n\n\n\n\n서로 다른 시스템 간의 데이터 매핑과 통합을 지원한다.\n시스템 간 데이터 불일치 문제를 줄인다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#제작시-유념해야할-사항",
    "href": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#제작시-유념해야할-사항",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "한 개의 단어에 대해 표준화된 영문약어를 사용하여 일관성을 확보한다.\n조직 전체에서 데이터 용어를 일관되게 사용할 수 있도록 한다.\n동일한 개념에 대해 서로 다른 용어를 사용하는 문제를 방지한다.\n\n\n\n\n\n유사하거나 중복된 용어를 식별하고 제거\n\n\n\n\n\n명명 규칙, 약어 사용, 데이터 형식 등을 표준화\n데이터 모델링과 시스템 개발에서 일관된 기준을 제공\n데이터 표준 단어 용도의 목적은 조직내 정보 공유가 제 1 목적이기 때문에 반드시 공공 기관이나 권위있는 조직의 양식을 따를 필요는 없다.\n이상적인 접근 방식은 외부 표준과 조직 내부의 요구사항을 균형 있게 고려하는 것이다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#제작-과정",
    "href": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#제작-과정",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "표준 단어 사전의 목적과 범위를 명확히 한다.\n\n목적이라 함은 데이터 일관성 확보(동일한 의미의 데이터 사용), 데이터 품질 향상 (데이터 처리 과정 비용 감소), 시스템 통합 지원 (여러 시스템 간 데이터 매핑과 통합), 규제 준수 지원 등을 의미한다.\n범위는 대상 데이터 범위, 조직적 범위, 시스템 범위 등을 의미하며, 너무 광범위한 영역은 제작 실패로 이어진다.\n\n이해관계자 식별: 관련 부서와 담당자들을 파악한다.\n거버넌스 체계 수립: 단어 사전 관리를 위한 조직과 프로세스를 정립한다.\n\n\n\n\n\n기존 데이터 모델, 데이터베이스 스키마, 업무 문서 등에서 사용 중인 단어들을 수집한다.\n업무 도메인별로 사용되는 용어들을 취합한다.\n\n\n\n\n\n동의어, 유사어, 약어 등을 식별한다.\n업무 영역별 용어의 의미와 사용 맥락을 분석한다.\n불필요하거나 중복된 단어들을 제거한다.\n\n\n\n\n\n약어 사용 규칙, 대소문자 규칙 등의 명명 규칙을 정의합니다.\n단어 정의 형식을 standardization 합니다.\n도메인별 특수 규칙을 설정한다.\n\n유용한 데이터 표준안을 만들기 위해 조직 내의 각 업무 영역 또는 데이터 도메인에 맞는 고유한 규칙이나 지침을 만들어야 한다.\n도메인 식별: 재무, 인사, 마케팅, 제조, 고객 서비스 등\n각 도메인별 특수성 파악: 해당 도메인에서만 사용되는 용어나 개념과 도메인 특유의 데이터 형식이나 제약 조건\n예시\n\n재무 도메인의 통화 표기 규칙: “USD 1,000.00” 또는 “1,000,000 원” 형식 사용\n재무 도메인의 회계 기간 표현: “FY2023Q2” (2023 회계연도 2분기)\n인사 도메인의 직급 코드 체계: “M” (매니저), “D” (디렉터)\n인사 도메인의 근속 연수 계산 규칙: 입사일 기준, 월 단위 반올림\n마케팅 도메인의 캠페인 코드 형식: “CAM_2023_SUMMER_01”\n마케팅 도메인의 고객 세그먼트 분류 기준: “VIP”, “REGULAR”, “NEW”\n제조 도메인의 제품 코드 체계: “PROD-A01-R” (제품군-모델번호-버전)\n제조 도메인의 품질 등급 표기: “A”, “B”, “C” 등급 사용\n\n\n\n\n\n\n\n분석된 단어들 중 표준으로 사용할 단어들을 선정\n선정 기준을 명확히 하고, 이해관계자들의 합의를 도출\n선정 기준 예시\n\n명확성\n\n의미가 명확하고 모호하지 않은 단어 선정 (일반적인 단어는 한정시킬 것)\n예: “고객” 대신 “활성고객”과 “비활성고객”으로 구분\n\n일관성\n\n조직 전체에서 일관되게 사용할 수 있는 단어\n예: 부서별로 다르게 사용되던 용어를 하나로 통일\n\n간결성\n\n가능한 간단하고 간결한 단어\n예: “제품구매고객정보” 대신 “구매자정보”\n\n유일성\n\n중복되지 않는 고유한 의미를 가진 단어\n예: 동음이의어 피하기\n\n업계 표준 부합성\n\n가능한 업계에서 널리 사용되는 표준 용어 선택\n예: 금융업계의 “ROI” (Return on Investment)\n\n확장성\n\n향후 변화나 확장을 고려한 단어 선택\n예: “2023년예산” 대신 “연간예산”\n\n이해 용이성\n\n비전문가도 이해하기 쉬운 단어\n예: 전문 용어보다는 일반적인 비즈니스 용어 선호\n\n번역 가능성\n\n다국어 지원이 필요한 경우, 번역이 용이한 단어\n관용구나 은유적 표현 피하기\n\n기존 시스템 호환성\n\n기존 시스템과의 호환성을 고려한 단어\n예: 레거시 시스템의 주요 용어 유지\n\n법규 및 규제 준수\n\n관련 법규나 규제를 준수하는 단어\n예: 개인정보보호법에 부합하는 용어 선택\n\n도메인 적합성\n\n해당 업무 도메인에 적합한 단어\n예: 금융 도메인에서는 “이자율”, 제조 도메인에서는 “불량률”\n\n측정 가능성\n\n정량적 측정이 가능한 개념을 나타내는 단어\n예: “고객만족도” (1-5 척도로 측정 가능)\n\n약어 사용 규칙\n\n약어 사용 시 일관된 규칙 적용\n규칙에 부합하지 않는 관용어는 관용어 사용을 유지하는 것이 좋다.\n예: “고객번호”를 “CUST_NO”로 통일\n\n\n\n\n\n\n\n각 단어에 대한 상세 정보를 정의 (정의, 동의어, 사용 예시, 관련 업무 영역 등).\n단어 간의 관계를 정의 (상위어, 하위어, 관련어 등)\n\n\n\n\n\n선정된 표준 단어들에 대해 관련 부서와 전문가들의 검토를 받는다.\n필요시 수정하고 최종 승인을 받는다.\n\n\n\n\n\n승인된 단어들을 데이터베이스나 전문 도구에 등록한다. (엔코아, Microsoft Purview 등)\n검색, 조회, 관리가 용이한 형태로 구성한다.\n\n\n\n\n\n완성된 표준 단어 사전을 조직 내에 공유\n사용 방법과 중요성에 대한 교육을 실시\n\n\n\n\n\n새로운 단어 추가, 기존 단어 수정, 폐기 등의 프로세스를 수립한다.\n정기적인 검토와 업데이트를 수행한다.\n사용 현황을 모니터링하고 피드백을 수집한다.\n\n\n\n\n\n데이터 모델링, 시스템 개발, 보고서 작성 등의 프로세스와 표준 단어 사전을 연계한다\n\n연계 순서는 각 상황마다 다르다.\n이미 ERD와 DB가 존재하는 시스템들을 통합하는 상황이라면 표준 단어 사전을 순서상 나중에 제작하는 것이 유리하다\n그 반대라면 표준 단어사전을 먼저 만들어 DB를 만드는 것이 유리할 수 있다.\n\n다른 데이터 관리 도구들과의 통합을 고려한다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#표준-단어-사전-예시",
    "href": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#표준-단어-사전-예시",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "예시 1\n\n\n\n\n표준 단어 사전 예시\n\n\n\n예시 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n단어 ID\n한글명\n한자\n영문명\n영문약어\n정의\n도메인\n데이터 타입\n길이\n허용값\n관련 업무 영역\n사용 예시\n동의어/유사어\n상위어\n하위어\n등록일\n최종 수정일\n승인 상태\n\n\n\n\nW001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW002\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW003\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n칼럼 설명:\n\n단어 ID: 각 단어의 고유 식별자\n한글명: 한글로 된 단어명\n한자: 해당 단어의 한자 표기 (필요한 경우)\n영문명: 영문으로 된 단어명\n영문약어: 영문 약어 (필요한 경우)\n정의: 단어의 명확한 정의\n도메인: 해당 단어가 속한 비즈니스 도메인\n데이터 타입: 해당 단어의 데이터 타입 (예: VARCHAR, INTEGER 등)\n길이: 데이터 길이\n허용값: 허용되는 값의 범위 또는 목록\n관련 업무 영역: 해당 단어가 주로 사용되는 업무 영역\n사용 예시: 실제 사용 예시\n동의어/유사어: 관련된 동의어나 유사어\n상위어: 해당 단어의 상위 개념 단어 (여러 개일 경우 쉼표로 구분)\n\n상위어: 이해관계자\n\n하위어: 해당 단어의 하위 개념 단어들 (여러 개일 경우 쉼표로 구분)\n\n하위어: 개인고객, 법인고객, VIP고객\n\n등록일: 단어가 사전에 처음 등록된 날짜\n최종 수정일: 마지막으로 수정된 날짜\n승인 상태: 현재 승인 상태 (예: 승인됨, 검토 중, 폐기 등)\n\n이 템플릿은 조직의 필요에 따라 수정하거나 필드를 추가/제거할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5_0.data_standard.html#데이터-표준화-요소-간-관계",
    "href": "docs/blog/posts/Governance/5_0.data_standard.html#데이터-표준화-요소-간-관계",
    "title": "Data Governance Study - Data Standard Governance",
    "section": "",
    "text": "데이터 표준관리 지침\n데이터 관리 가이드\n표준 데이터\n구조 데이터\n\n각 요소는 서로 밀접하게 연관되어 있으며, 전체적인 데이터 거버넌스 체계를 형성한다.\n\n\n\n\n데이터 표준을 정의하고 변경 관리하는 데 필요한 지침을 제공한다.\n조직 내에서 데이터 표준을 어떻게 설정하고, 유지하며, 업데이트할 것인지에 대한 가이드라인을 제시한다.\n\n\n\n\n\n데이터 표준, 구조, 흐름을 관리하기 위한 프로세스, 조직, 역할을 정의한다.\n이는 데이터 거버넌스의 실행 측면을 다루며, 누가 어떤 책임을 지고 데이터 표준화 활동을 수행할지 명확히 한다.\n\n\n\n\n\n표준 데이터는 실제 데이터 요소들을 표준화하는 핵심 부분이다. 여기에는 다음과 같은 세부 요소들이 포함됩니다\n\n\n표준 단어\n\n명칭 (한글/영문)\n영문약어\n단어 정의\n표준 단어를 기반으로 자료형을 정의하여 표준 단어 금칙어를 만든다.\n\n표준 도메인\n\n표준 단어를 기반으로 자료형을 정의하여 표준 도메인을 작성한다.\n데이터 타입 결정\n데이터 길이 결정\n\n표준 용어\n\n표준 단어, 자료형, 표준 도메인을 기반으로 용어를 정의한다.\n표준 단어 2개 이상의 조합으로 용어를 생성한다.\n표준 용어는 1개의 표준 도메인으로 구성된다.\n\n표준코드\n\n표준 도메인을 기반으로 유효값을 정의한다.\n유효값을 기반으로 코드값을 만들어낸다.\n\n제작 순서\n\n표준 단어 \\(\\rightarrow\\) 표준 도메인 \\(\\rightarrow\\) 표준 용어 \\(\\rightarrow\\) 표준 코드 \\(\\rightarrow\\) 코드 값\n\n\n\n표준 단어와 표준 용어는 한국어에만 존재하는 개념으로 다른 외국어에는 없는 개념이다. 외국어는 형태소 분석이 한국어 만큼 세분화되어 있지 않아 표준 단어와 표준 용어가 합쳐진 개념인 데이터 카탈로그란 용어를 사용한다.\n\n\n\n\n\n데이터 모델링의 영역이다.\n표준 데이터가 정의되면, 이를 바탕으로 실제 데이터 모델을 구축합니다. 이 과정은 크게 세 단계로 나뉜다:\n\n개념 데이터 모델\n논리 데이터 모델\n물리 데이터 모델\n\n개념 데이터 모델 + 논리 데이터 모델링을 통해 주제영역, 엔티티, 속성을 차례로 규명한다.\n물리 데이터 모델을 통해 엔티티를 물리적으로 구현한 테이블과 속성을 물리적으로 구현한 컬럼을 만들어낸다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#표준단어-구성-요소",
    "href": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#표준단어-구성-요소",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "표준단어는 단어명, 영문명, 영문약어명 및 정의 등으로 구성되며 약어는 영문명을 축약하여 작성한다.\n\n\n[단어명]\n\n표준단어를 구성하는 최소단위의 단위를 의미하며 한글 및 영문, 숫자로 정의한다.\n표준단어의 최대길이는 15자로 하며, 10자 이내로 작성할 것을 권장한다.\n\n[영문명]\n\n표준단어명의 영문 명칭을 의미한다.\n영문명의 첫 자리의 알파벳은 대문자로 하고 나머지 부분은 소문자로 하며,영문 단어 간에는 띄어쓰기를 한다.\n\n[정의]\n\n해당 단어가 뜻하는 것 혹은 내용을 의미한다.\n데이터 명칭을 그대로 서술하거나 약어 또는 전문 용어를 이용한 기술은 가급적 지양한다.\n\n[영문약어명]\n\n영문명의 축약된 형태의 영문명칭을 의미하며 영문명을 바탕으로 영문약어를 정의한다.\n영문약어의 최대 길이는 4자로 한다.(권장)\n단, 고유명사나 관용적인 표현 등의 경우 글자수에 대한 예외를 허용한다.\n\n[단어 유형]\n\n기본단어(수식어성)\n\n용어를 구성하는 단어로 용어의 마지막에 존재할 수 없는 단어\n주제어, 수식어, 접두사/접미사, 복합단어 등이 이에 속한다\n예시, 주문,고객, 상품,거래\n\n분류단어 (도메인성)\n\n용어를 구성하는 단어로 용어의 마지막에 존재하는 단어\n용어가 가질 수 있는 속성 정보(데이터 형식 및 길이)를 구체적으로 표현하기 위해 사용한다\n분류단어는 도메인과 매핑 되어 그 속성정보를 정의한다\n예시, 금액,수량, 성명,코드, 번호,일자\n\n예시\n\n기본 단어: 정산, 승인\n분류 단어: 금액\n표준 용어: 정산승인금액\n\n\n예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n단어명\n한자\n영문명\n정의\n영문약어명\n단어종류\n단어유형\n\n\n\n\n가격\n價格\nPrice\n물건이 지니고 있는 가치를 돈으로 나타낸 것\nPRC\n기본단어\n단일어\n\n\n연령\n年齡\nAge\n나이(사람 등 세상에서 나서 살아온 햇수)\nAGE\n기본단어\n분류어\n\n\n우편번호\n郵便番號\nZip Code\n우편물을 쉽게 분류하기 위하여 정보 통신부에서 각 지역마다 매긴 번호\nZPCD 또는 ZIP(관용적표현)\n분류단어\n복합어\n\n\n\n\n\n\n단일어\n\n하나의 형태소(形態素: 의미의 기능을 부여하는 언어의 형태론적 수준에서의 최소단위)로 성립된 단어\n\n고유명사(기관명 포함), 지명(명사) 단일어로 사용함\n접사(접두사 및 접미사)와 합성된 단어\n두 개의 단일어로 구성되나 영문단어가 각각의 단일어의 영문단어의 조합과 일치 하지 않고 다른 영문단어가 존재하는 경우\n\n예시) 고객,지점, 가격,시설\n\n복합어\n\n둘 이상의 어근(실질 형태소)이 결합 이루어진 단어\n예시) 전화번호, 휴대폰번호, 차용지점\n\n외래어\n\n원래 외국어였던 것이 국어의 체계에 동화되어 사회적으로 그 사용이 허용된 단어\n예시) 이메일, 팩스, 네비게이션\n\n관용어\n\n한글 단어와 외래어가 결합되어 사용되거나 기타 국어 체계에 부합하지 않더라도 관용적으로 자주 사용되며 의미가 명확한 단어\n일반 사회나 기관에서 관습적으로 널리 쓰는 말\n예시) VDC,ABS, 셀프계약서, MT\n\n접사 (접두사/접미사)\n\n접두사(Prefix): 어떤 낱말 앞에 붙어서 의미를 첨가하여 한 다른 낱말을 이루는 말\n접미사(Suffix): 낱말의 끝에 붙어 의미를 첨가하여 다른 낱말을 이루는 말\n예시) 미사용, 보증료\n\n\n\n\n\n\n동음이의어\n\n발음은 동일하나 의미가 다른 단어(Homonym)\n예시: 매수(Sheets Count) / 매수(Buying)\n\n이음동의어\n\n동일한 의미를 표현하는 두 개의 다른 단어 (Synonym)\n즉, 발음은 다르나 동일한 의미를 표현하는 단어\n예시: 이수(Completion) / 수료(Completion)\n\n금칙어\n\n표준단어 사용의 일관성을 위하여 사용하지 못하게 지정된 단어\n예시: 이해당사자(X), 이해관계자(O)\n\n유사어\n\n표준단어 사용의 일관성을 위하여 권장어(대체어) 사용을 권고하는 단어\n예시: 수정, 변경, 정정\n\n축약어\n\n줄여서 간략하게 표현하는 단어\n주민번호(X), 주민등록번호(O)"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#표준-단어-사용-원칙",
    "href": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#표준-단어-사용-원칙",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준단어는 단일어 형태의 명사형 낱말을 표준단어로 정의하는 것을 기본 원칙으로 한다.\n고객, 가격, 금액, 지점\n\n\n2\n• 동일한 의미의 단어를 한글과 영문으로 중복해서 정의하지 않는다.\nRENT(X) → 렌트(O)\n\n\n3\n• 축약된 형태의 단어로 정의하지 않는다. 단, 범용 또는 공식적으로 사용이 승인된 약어는 표준 원칙에 의거하여 사용할 수 있다.• 또한 원래 단어가 너무 길거나 잘 활용하지 않아서 업무적으로 축약된 단어를 주로 사용하는 경우에 한하여 사용할 수 있다• 용어명 길이 제약을 해결하기 위해 부득이하게 약어를 사용해야 할 경우에는 약어와 전체 단어를 모두 표준단어로 등록하도록 한다\n주민번호(X) → 주민등록번호(O)등평(X) → 등급평가(O)\n\n\n4\n• 한글 축약어는 다른 단어와 붙여서 쓸 경우 혼동이 될 우려가 있으므로 가급적 풀어 쓴 단어를 사용한다. 단, 어감상 필요 시는 예외로 적용할 수 있다. (이음동의어)\n가(假)(X) → 임시(O)현(X) → 현재(O)전(X) → 이전(O)후(X) → 이후(O)\n\n\n5\n• 과거 단어와 현대 단어를 함께 사용하고 있는 경우 가급적 현대 단어를 사용한다.\n\n\n\n6\n• 접두사 및 접미사는 별도로 분리 하지 않으며 표준단어(단일어)로 등록함을 원칙으로 한다\n\n\n\n7\n• 고유명사(기관명 포함)는 한글 단어를 사용하는 것을 원칙으로 한다\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 외국에서 들어온 말로 국어처럼 쓰이는 외래어는 영문을 쓰지 않고 외래어 한글 표기법에 따라 정의하는 것을 원칙으로 한다.\nFax(X) → 팩스(O)Email(X) → 이메일(O)Database(X) → 데이터베이스(O)\n\n\n2\n• 영문을 대체할 적절한 한글이 없거나 영문자체에 고유한 업무적 의미를 담고 있는 경우, 또한 관용적으로 두음문자 표현이 사용되고 있는 경우에는 해당 영문을 그대로 사용한다. 단, 표준단어로 등록 시 해당 영문의 전체명이 아닌 두음문자 형식의 대문자로 등록한다.\nMMF, HTML, SQL\n\n\n3\n• 영문 단어 첫글자는 알파벳 대문자로 나머지는 소문자로 작성한다.\nSWIFT(X) → Swift(O)\n\n\n4\n• 영문 단어를 한글화 할 경우, 영문 의미로 한글화 한 경우와 소리 나는 대로 한글화한 경우 둘 다 자주 사용될 경우, 소리 나는 대로 한글화한 단어를 표준으로 하고, 의미로 한글화 한 단어는 금칙어로 등록한다.\nERROR(X), 오류(X) → 에러(O)\n\n\n5\n• 한글 단어 보다 더 친숙하게 사용되는 영문 단어의 경우에는 그 영문단어를 사용한다.\n인터넷프로토콜(X), 아이피(X) → IP(O)\n\n\n6\n• 관용적으로 사용하는 단어나 고유명사인 경우 특수문자 중 ‘/’, ‘-’, ’&’만을 사용할 수 있으나, 가급적 특수문자를 사용하지 않는다\nM&A(O)\n\n\n7\n• 한글, 영문, 숫자의 혼용이 가능하다\n회사(O), 12월(O)\n\n\n8\n• 한글과 영문만 표준 단어로 인정하며 기타 외국어(한자, 일본어 등)는 사용하지 않는다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 업무적 또는 관용적으로 자주 쓰이는 표현이나 단일어 단위로 구분해서 사용할 경우 의미 전달이 불분명해질 수 있는 단어에 대해서는 복합어로 구성 사용한다\n계좌번호(Account Number) = 계좌(Account)+번호(Number)\n\n\n2\n• 고유명사(기관명 포함)는 단일어 형태로 사용한다.\n회사\n\n\n3\n• 접두사 및 접미사와 합성된 단어는 단일어 형태로 사용한다\n재[再]발급, 지급처[處]\n\n\n4\n• 두 개 이상의 단일어로 이루어졌으나 별도의 영문 단어가 존재하며 각 단일어의 조합과는 다른 의미를 지니게 되는 경우 단일어 형태로 사용한다.\n감가상각(Depreciation) ≠ 감가(Reduction)+ 상각(Repayment)\n\n\n5\n• 한글과 외래어가 결합되어 사용되거나 기타 사용 원칙에 부합하지 않더라도 관용적으로 자주 사용되며 의미가 명확한 경우 사용한다. (관용적 표현)\n파레트수량\n\n\n6\n• 유형의 구분을 나타내는 복합 단어는 단어로 식별하지 않도록 한다. 이때에는 유형을 대표하는 다른 단어 또는 용어로 대체하도록 한다. 단, 관용적으로 자주 쓰이는 표현이면서 대체 단어 또는 용어 구성이 어렵다면 사용을 허용한다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 숫자 사용 시 아라비아 숫자 사용을 원칙으로 한다\n삼순위(X) → 3순위(O)\n\n\n2\n• 숫자만으로는 단어가 될 수 없고 해당 숫자의 의미를 나타내는 단어와 함께 조합하여 사용한다\n6(X) → 6개월(O)\n\n\n3\n• 숫자와 단위의 합성어는 단일어로 등록한다\n100퍼센트, 91일\n\n\n4\n• 단위 앞에 올 수 있는 숫자의 유효값이 제한적인 경우 유효한 단어를 모두 등록한다.\n1월,2월 ~ 12월1분기,2분기,3분기,4분기1순위,2순위,3순위\n\n\n5\n• 단위 앞에 올 수 있는 숫자의 유효값이나 범위가 제한이 없는 경우 해당 단위를 등록하고 최소 1단위와 합성된 단어를 등록한다\n퍼센트 - 1퍼센트개월 -1개월차 - 1차급 - 1급\n\n\n6\n• 숫자 단위 대에 따라 표준을 다르게 정의할 경우 각각을 단어로 등록할 수 있다\n1원,1십원,1천원,1만원,1십만원,1백만원,1천만원\n\n\n7\n• 숫자와 조합된 단어의 의미가 불분명한 경우는 해당 단어의 사용을 지양한다 단, 의미가 불분명한 단어임에도 업무상의 필요로 인해 등록이 불가피한 경우 단어 뒤에 숫자를 붙여서 정의한다\n컬럼1(X)※ 단 불가피한 경우 사용\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 동음이의어(Homonym)는 허용하지 않는다 - (대체방안1) 다른 한글단어로 교체하여 표준 단어로 등록하여 사용한다. - (대체방안2) 어감상 동음이의어인데도 불구하고 ’의사’라는 단오를 꼭 써야 하는 경우는 ’의사결정’식의 복합 단어를 표준단어로 등록하여 사용한다• 단, 대체 단어가 없는 경우 사용할 수 있다 (한글 명이 같더라도 영문 명은 반드시 달라야 한다) 이 경우 데이터 표준관리자 및 모델관리자에게 검토 요청을 신청 한다\n다리(leg) : 다리다리(bridge) : 교량의사(doctor) : 의사의사(idea)결정(Decision):의사결정\n\n\n2\n• 이음동의어(Synonym)는 용어의 혼돈과 용어생성 시 중복발생 가능성 때문에 가급적 사용하지 않는다.• 대표단어를 정하고, 그 대표 단어만을 사용하도록 하며 나머지는 금칙어로 등록하여 사용을 제한하도록 한다\n핸드폰(X) → 휴대폰(O)※ 핸드폰은 금치어로 등록\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 이음동의어는 대표 단어만을 표준단어로 정의하여 사용하도록 하며, 나머지 이음 동의어의 단어들은 금칙어로 정의하여 사용을 사전에 방지한다.\n나이(X) → 연령(O)금일(X) → 당일(O)\n\n\n2\n• 축약된 형태의 단어(축약어)는 금칙어로 정의하여 사용을 사전에 방지한다.\n주민번호(X) → 주민등록번호(O)\n\n\n3\n• 사람에 해당하는 접미사 “자”와 “인”으로 구성된 복합어의 경우에는 사용빈도가 높은 단어를 표준단어로 정의하고, 다른 나머지 단어를 금칙어로 관리하여 사용을 사전에 방지한다\n장애자(X) → 장애인(O)신청인(X) → 신청자(O)\n\n\n4\n• 한글 맞춤법을 고려하지 않고 관용적으로 사용되던 단어들은 금칙어로 정의하여 사용을 사전에 방지한다.\n갯수(X) → 개수(O)써비스(X) → 서비스(O)\n\n\n5\n• 영문 단어의 경우 한글화를 원칙으로 한다. 단, 한글 단어 보다 더 친숙하게 사용되는 영문 단어의 경우에는 그 영문 단어를 사용한다. 이렇게 선정된 단어에 대응되는 영문 혹은 한글 단어는 금칙어로 등록하고 사용을 제한하도록 한다\nERROR(X) → 에러(O)FAX(X) → 팩스(O)EMAIL(X) → 이메일(O)아이피(X) → IP(O)\n\n\n6\n• 범용적으로 사용되는 외래어의 경우 사용할 수 있지만 표기법을 고려하여 표준 단어를 지정하고, 나머지 단어는 금칙어로 등록하여 사용을 제한하도록 한다\nEXPOSURE(X), 익스포저(X) → 익스포져(O),화일(X), FILE(X) →파일(O)\n\n\n7\n• 전문 업무용어 중 관용적으로 영문의 두문자어(頭文字語:Acronyms)를 사용하는 경우 한글단어는 금칙어로 정의한다.\n\n\n\n8\n• 기관명 등이나 고유명사 등은 전체 단어를 표준으로 사용하고, 한글 약어는 금칙어로 등록한다.\n\n\n\n9\n• 시스템명중 관용적으로 영문의 두문자어(頭文字語:Acronyms)를 사용하는 경우 한글단어는 금칙어로 정의한다\n데이터웨어하우스(X) → DW(O)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 단어의 대표 자음으로 영문 약어를 구성한다. 대표 자음의 적용 우선 순위는 앞 자리의 자음부터 4 글자까지 적용한다. 모음(A,E,I,O,U)외 모든 알파벳이 자음이다.• 약어 중복 발생 시 모음을 순차적으로 추가해서 약어를 생성한다.\n대여: Rental → RNTL\n\n\n2\n• 가급적 4글자의 약어로 구성하되, 고유명사나 관용적인 표현 등의 경우 글자수에 대한 예외를 허용한다\n배치: BATCH\n\n\n3\n• 영문 약어의 글자수가 4글자 이하인 경우에는 가장 앞자리의 모음을 대표 자음으로 간주하여, 이 모음 앞뒤에 나오는 대표 자음과 결합하여 4자리 약어를 적용한다.• Y와 같이 자음이지만 모음으로 혼동하는 알파벳도 모두 자음으로 간주한다.\n급여: Color → COLR\n\n\n4\n• 영문 약어명의 사용시 한글 발음 식의 약어는 사용하지 않으나 순수 한글이거나 대체할 영문이 없는 경우 허용한다\n주소: JUSO → ADR, 시: SI, 군: GUN, 구: GU\n\n\n5\n• 영문 약어는 항상 유일성을 유지하도록 관리한다\n\n\n\n6\n• 두 개의 자음이 연속할 경우, 가급적 한 개의 자음은 생략하는 형태로 표현한다.\n결제: Settlement → STTL (X) → STL (O)\n\n\n7\n• 범용적으로 사용되는 두문자어(頭文字語: Acronyms)가 있는 경우 되도록 두문자어를 사용하며 글자수가 지나치게 긴 경우 새로운 영문약어를 작성한다.\nSQL(Structured Query Language), UDM(Universal Data Model)\n\n\n8\n• 한글단어에 대해 관용적으로 사용되는 영문 약어가 있으면 이를 사용한다.\nNumber→NMBR(X)→ NO(O)\n\n\n9\n• 영문명을 일반적으로 축약해서 사용되는 영문 약어가 있는 경우 이를 사용한다.\nPackage → PCKG (X) → PKG (O)\n\n\n10\n• 한 영문명의 글자수가 4글자 이하의 경우는 그대로 사용한다. 단, 합성어의 경우 4글자 이상을 사용할 수 있다\n면적 : Area → AREA (O), 역할 : Role → ROLE (O)"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_2.data_word_glossary.html",
    "href": "docs/blog/posts/Governance/5_2.data_word_glossary.html",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "조직에서 사용되는 모든 데이터 관련 용어의 공식적인 정의를 제공하는 중앙 집중식 저장소이다.\n업무상 사용되는 용어를 정보시스템에서 사용하는 기술적인 용어로 전환하여 이것을 일관되게 사용 할 수 있도록 정의한 것을 지칭한다.\n각 용어의 의미, 사용 맥락, 형식 등을 명확히 기술한다.\n표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다.\n모델링에서는 속성명으로 사용되며 전사관점에서 유일하다.\n예시\n\n표준 단어 (주제어): 장기\n표준 단어 (수식어): 대여\n표준 단어 (분류단어): 금액\n표준 용어 : 장기대여금액\n데이터 모델 (속성명): 장기대여금액 with datatype = NUMBER(10)\n도메인: 금액N10\n\n\n\n\n\n데이터의 일관성을 확보하고 품질을 향상\n조직 전체에서 데이터 용어를 일관되게 사용할 수 있도록 한다.\n\n즉, 전사적으로 표준화된 용어를 사용함으로써 데이터 모델 구성 요소의 명칭을 부여하는데 일관성을 유지할 수 있다\n\n시스템 간 데이터 통합과 매핑을 용이하게 한다.\n표준화된 명칭을 부여함으로써 데이터의 중복 정의 방지와 의미 전달의 명확성을 확보하여 의사 소통을 원활하게 한다\n\n\n\n\n\n용어 논리명 (보통 한글)\n\n논리명은 데이터의 의미를 나타내는 명칭으로 표준단어 명으로 구성된다.\n용어 논리명의 최대길이는 30자로 한다. 단 20자 이내로 작성할 것을 권장한다\n표준속성 구성 시 5개 단어를 넘지 않도록 하며, 구분자나 띄어쓰기 없이 한 단어로 붙인다 (월_계좌잔액(X) → 월계좌잔액)\n\n용어 물리명 (보통 영어)\n\n물리명은 단어의 영문 약어 조합으로 이루어지며 단어의 영문약어들끼리 연결 할 때는 언더바(_)를 사용한다.\n용어의 물리명은 최종적으로 데이터베이스를 구성 할 때 테이블의 컬럼명으로 사용한다.\n용어 물리명의 최대길이는 28자로 한다. 단 20자 이내로 작성할 것을 권장한다. (단, 용어 논리 및 물리명의 길이는 DBMS에 따라 달라질 수 있음)\n\n정의\n\n용어가 업무적으로 사용되는 의미를 기술한 내용이다.\n\n도메인 정보\n\n특정 비즈니스 컨텍스트에서 데이터 값의 허용 범위를 정의\n즉, 데이터 값의 범위를 한정하는 데이터 타입과 길이,소수점을 의미한다.\n데이터 타입을 포함하며, 추가적인 제약조건이나 비즈니스 규칙을 포함할 수 있습니다.\n비즈니스 로직과 데이터 무결성 규칙을 포함할 수 있다.\n예시\n\n나이: INTEGER, 0-150 사이의 값만 허용\n이메일: VARCHAR(100), 이메일 형식 검증 규칙 포함\n급여: DECIMAL(10,2), 0보다 큰 값만 허용\n\n\n데이터 타입 및 형식\n\n데이터의 기본적인 저장 형태와 구조를 나타낸다.\n일반적이고 기본적인 데이터 유형을 지정\n보통 도메인 정보가 데이터 타입 및 형식 정보를 모두 포함한다.\n예시\n\n문자열(VARCHAR, CHAR)\n숫자(INTEGER, DECIMAL)\n날짜/시간(DATE, TIMESTAMP)\n불리언(BOOLEAN)\n\n\n코드\n\n입력할 수 있는 유효 값 데이터 값을 정의할 수 있다면 용어는 코드와 매핑 한다.\n\n관련 업무 영역\n\n해당 용어가 주로 사용되는 비즈니스 또는 조직 내의 특정 부서나 기능 영역을 나타낸다.\n목적: 용어의 사용 맥락을 제공하고, 해당 용어가 어떤 비즈니스 프로세스나 기능과 관련있는지 이해하는 데 도움을 준다.\n예시\n\n“고객ID” - 관련 업무 영역: 고객 관리, 마케팅, 영업\n“재고수량” - 관련 업무 영역: 재고 관리, 물류, 구매\n“급여액” - 관련 업무 영역: 인사, 재무\n\n\n사용 예시 등\n\n“생년월일” - 사용 예시: “1990-05-15”\n“주문상태” - 사용 예시: “접수”, “처리중”, “배송완료”\n“계좌잔액” - 사용 예시: “1,000,000원”\n\n용어 사전 예시\n\n네, 관련 업무 영역과 사용 예시 칼럼을 추가한 마크다운 테이블을 만들어 드리겠습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n용어 논리명\n용어 물리명\n도메인(인포타입)\n코드\n정의\n관련 업무 영역\n사용 예시\n\n\n\n\n계약종료일자\nCNTR_END_DATE\n일자VC8\n\n거래 계약의 종료일자이다. YYYYMMDD로 작성한다.\n계약관리, 고객관리\n“20231231”\n\n\n렌탈등록비공급가액\nRENT_RGST_SPAM\n금액N10\n\n렌탈 등록 공급 금액을 뜻한다. 숫자로 작성한다.\n렌탈관리, 재무\n“50000”\n\n\n관청은행코드\nGOV_BANK_CD\n은행코드\n은행코드\n해당 관할 은행의 코드이다.\n재무, 회계\n“004” (국민은행)\n\n\n\n이 테이블에서 ‘관련 업무 영역’과 ’사용 예시’ 칼럼을 추가하여 각 용어의 사용 맥락과 실제 적용 예를 보여주고 있습니다. 실제 환경에서는 이 정보들을 조직의 특성과 요구사항에 맞게 더 구체적으로 작성할 수 있습니다.\n\n\n\n\n표준용어 작성 시 누구나 이해하기 쉽도록 간결하되 명확하고 모호함 없이 표현하도록 해야 하며, 다음과 같은 기본 원칙에 위배되지 않도록 한다.\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어는 관용적으로 사용하는 용어를 우선적으로 사용한다\n\n\n\n2\n• 표준용어를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n등록일자(X) → 장비자산등록일자(O)\n\n\n3\n• 용어 구성 시 단어는 반드시 표준 단어 사전에 등록된 단어를 사용하며, 단어 사전에 등록되어 있지 않은 경우에는 표준 담당자와 협의 후에 신규 단어로 등록하도록 한다.\n단어 부재 시 신규 요청 요망\n\n\n4\n• 일반적인 의미와 전혀 다르게 사용된 용어는 적절한 다른 용어로 대체하고, 유사한 의미의 용어가 중복 개발되어 혼재되지 않도록 하며 새로운 용어의 개발은 자제한다.\n반환일자(X) 반납일자(O)\n\n\n5\n• 표준용어로 등록된 명칭의 전사적으로 사용되어야 함으로 명 선정 시 신중하게 고려하여야 한다\n\n\n\n6\n• 표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다\n렌탈 + 가능(X) 렌탈 + 가능 + 금액(O)\n\n\n7\n• 표준용어 명명 규칙 - 표준용어는 누구나 이해하기 쉽도록 구체적이고 명확하고 간결하게 정의한다 - 복합어를 단일어 보다 우선 적용한다 - 복합어가 중첩되어 사용될 경우 도메인이 포함된 복합어를 우선 적용한다 - 의미 있는 숫자를 포함한 용어의 경우에는 숫자를 포함하여 하나의 표준단어를 등록한 후 그 표준 단어를 사용하여 용어를 정의한다 - 용어의 의미를 모호하게 하는 의미 없는 일련번호를 부여하기 위한 숫자는 사용하지 않으며 용어에 수식어를 사용하여 용어가 유일하게 식별되도록 정의하는 것을 원칙으로 한다\n\n\n\n\n\n\n\n\n표준 용어 = 수식어 (표준 단어) + 주제어 (표준 단어) + \\(\\dots\\) + 수식어 (표준 단어) + 주제어 (표준 단어) + 분류어\n수식어 예시\n\n\n\n수식어\n예시\n\n\n\n\n기간 수식어\n최초, 최종, 과거, 최근 등\n\n\n기간/시간\n6개월, 당월, 월말, 년초, 년말 등\n\n\n장소\n국외, 국내, 지점, 본점 등\n\n\n특징\n순수, 사용, 처리, 거래 등\n\n\n계산\n합계: 한데 모아서 합산함누계: 계속하여 덧붙여 합산함\n\n\n\n표준 용어 = 표준 용어 + 표준 도메인\n원칙 항목 및 설명\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 용어는 수식과 수식의 대상이 되는 단어가 여러 개 존재할 수 있고 도메인을 수식하는 분류어는 용어의 끝에 위치한다\n\n\n\n2\n• 수식어는 기간, 장소, 특징, 계산의 성격을 가지는 단어가 순서대로 위치하고 기간을 수식하는 단어는 맨 앞에 위치한다\n\n\n\n3\n• 수식어 중 계산의 성격을 가진 단어는 합계, 누계, 총합계, 총누계, 소계 중 하나를 선택하여 사용해야 한다. (도메인 그룹이 금액과 수량과 같이 계산이 필요한 도메인을 수식함)\n\n\n\n4\n• 대상이 되는 단어가 여러 개일 때는 중 범위가 큰 것 순서대로 용어의 앞부분에 위치한다\n\n\n\n\n주의 사항\n\n\n\n\n\n\n\n\nAS-IS 용어 (비권장)\nTO-BE 용어(권장)\n비고\n\n\n\n\n회사에게 하고 싶은 말 내용\n사용자건의사항\n서술형용어\n\n\n공임금액구분별금액\n공임구분별금액\n단어 반복\n\n\n법인번호\n?+법인등록번호\n주제어 누락 및 약어 사용\n\n\n상세순번\n?+상세일련번호\n주제어 누락\n\n\n수정자\n?+수정자+?\n주제어 및 분류어 누락\n\n\n스케줄추가(청구)정보 수정여부\n스케줄추가정보수정여부\n특수문자사용\n\n\n요청일\n장기렌트입고요청일자\n주제어누락\n\n\n운전자\n렌트카운전자성명\n주제어 및 분류어 누락\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어에 적절한 명칭을 부여하고 전사적 공유가 가능하기 위해서는 구체적인 정의가 반드시 있어야 한다- 표준용어정의는 활동을 나타내는 단어와 그 주체가 되는 단어와 무엇을 어떻게 했는지의 내용이 담겨 있어야 한다.\n대여차량입고일자: 렌트차량이 반환되어 차고지에 입고 된 일자이다.\n\n\n2\n• 표준용어의 명칭이 주는 의미가 불분명하면 좀더 상세하게 정의해야 한다.\n유효기간(X) → 회원멤버쉽유효기간(O)\n\n\n3\n• 동일한 의미의 용어가 중복되지 않도록 표준용어 구성 순서를 고려해 생성한다\n현금서비스최종3개월 총합계금액(X) → 최종3개월 현금서비스 총합계금액(O)\n\n\n4\n• 표준용어의 한글명 또는 영문명 길이 제한으로 인해 축약된 형태로 사용해야 하는 경우 용어를 구성하는 단어 중 연관도와 활용도가 높은 단어들을 합하여 복합어를 정의해야 한다\n고객차량등급^코드 → 고객차량등급코드\n\n\n5\n• 표준용어 중 ‘여부’ 도메인으로 끝나는 것들은 대표로 하나의 코드명과 코드 값(Y, N)을 등록하여 모든 공통적으로 사용하도록 한다.\n코드일련번호 : B011\n\n\n6\n• ‘계약번호’ 자체로는 그 의미가 불분명하여 대차계약번호, 유지보수계약번호 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n7\n• 일반적으로 ‘렌탈’은 ’자동차대여’를 말하는 것이나 ’렌탈’ 자체로는 그 의미가 불분명하여 단기렌탈, 장기렌탈 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n8\n• 신청사용자아이디 → 사용자아이디 에서 ’신청’의 단어처럼 생략해도 의미가 통하는 경우에는 표준용어는 축약된 형태로 정의한다\n\n\n\n\n\n\n\n\n\n\n\n각 업무 영역에서 필요한 용어들을 수집\n기존 시스템, 문서, 보고서 등에서 사용 중인 비표준 용어들을 식별\n\n\n\n\n\n수집된 용어들의 의미와 사용 맥락을 분석\n유사하거나 중복된 용어들을 식별\n\n\n\n\n\n분석된 용어들을 기존의 표준 단어 사전과 매핑\n각 용어를 구성하는 단어들이 표준 단어 사전에 있는지 확인\n\n\n\n\n\n표준 단어들을 조합하여 새로운 표준 용어를 구성\n용어 구성 원칙(예: 수식어 순서, 도메인 위치 등)을 따른다.\n\n\n\n\n\n구성된 용어가 의미를 명확히 전달하는지 확인\n기존 표준 용어와 중복되지 않는지 검토\n전문가나 제 3의 부서와 cross check\n\n\n\n\n\n용어의 정의, 사용 예시, 관련 업무 영역 등 상세 정보를 작성\n용어와 관련된 도메인 정보를 지정\n\n\n\n\n\n구성된 용어에 대해 관련 부서와 데이터 관리자의 검토를 받는다.\n필요시 수정하고 최종 승인을 받는다.\n\n\n\n\n\n승인된 용어를 표준 용어 사전에 등록\n용어의 물리명(영문명)을 생성\n\n\n\n\n\n새로 등록된 표준 용어를 조직 내에 공지\n필요시 사용 방법에 대한 교육을 실시\n\n\n\n\n\n새로 등록된 표준 용어의 사용 현황을 모니터링\n사용자 피드백을 수집\n\n\n\n\n\n정기적 검토 및 피드백 수집\n\n분기별로 용어 사전 검토 일정을 수립\n사용자로부터 피드백을 수집하고, 사용 현황을 모니터링\n\n변경 관리 프로세스 운영\n\n용어 추가, 수정, 폐기를 위한 공식적인 변경 요청 프로세스를 구축\n변경 요청에 대한 영향 분석을 수행하고, 승인 절차를 거친다.\n\n업데이트 및 버전 관리\n\n승인된 변경사항을 용어 사전에 반영\n버전 관리를 통해 변경 이력을 추적하고, 주요 변경사항을 공지\n\n교육 및 홍보\n\n변경된 용어나 새로운 용어에 대한 교육을 실시\n내부 커뮤니케이션 채널을 통해 주요 업데이트 사항을 공유\n\n성과 측정 및 개선\n\n용어 표준화로 인한 데이터 품질 개선, 업무 효율성 증가 등의 성과를 측정\n측정 결과를 바탕으로 용어 관리 전략을 지속적으로 개선\n\n\n\n\n\n\n\n예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n용어ID\n용어논리명\n용어물리명\n도메인\n정의\n관련업무영역\n사용예시\n표준단어구성\n승인상태\n등록일\n\n\n\n\nT001\n고객번호\nCUST_NO\n일련번호VC10\n고객을 유일하게 식별하는 번호\n고객관리, 영업\n“C0001234567”\n고객(주제어) + 번호(분류어)\n승인\n2023-01-15\n\n\nT002\n장기렌트계약시작일자\nLTRM_RENT_CNTR_STRT_DT\n일자VC8\n장기 렌트 계약이 시작되는 날짜\n계약관리, 렌트관리\n“20230107”\n장기(수식어) + 렌트(주제어) + 계약(주제어) + 시작(수식어) + 일자(분류어)\n승인\n2023-01-16\n\n\nT003\n월별렌탈등록비합계금액\nMTHLY_RENT_RGST_FEE_TOT_AMT\n금액N15\n한 달 동안의 렌탈 등록비 총액\n재무, 렌트관리\n“5000000”\n월별(수식어) + 렌탈(주제어) + 등록(주제어) + 비(주제어) + 합계(수식어) + 금액(분류어)\n검토중\n2023-01-17\n\n\n\n* VC10: Variable Character의 약자로 가변 길이 문자열을 나타냄 (최대 10 자리)  \n* N15: N은 숫자(Numeric) 데이터 타입, 최대 15자리의 숫자  \n* D: Date (날짜)  \n* T: Time (시간)  \n* B: Boolean (참/거짓)  \n\n칼럼 설명:\n\n\n용어ID: 각 용어의 고유 식별자\n용어논리명: 업무에서 사용되는 한글 용어명 (최대 30자, 권장 20자 이내)\n용어물리명: 데이터베이스 등에서 사용되는 영문 약어명 (최대 28자, 권장 20자 이내)\n도메인: 해당 용어의 데이터 타입과 제약조건\n정의: 용어에 대한 명확한 설명\n관련업무영역: 해당 용어가 주로 사용되는 업무 분야\n사용예시: 실제 데이터 예시\n표준단어구성: 용어를 구성하는 표준 단어들과 각 단어의 역할 (수식어, 주제어, 분류어)\n승인상태: 용어의 현재 승인 상태 (예: 승인, 검토중, 폐기 등)\n등록일: 용어가 사전에 처음 등록된 날짜\n최종수정일: 용어 정보가 마지막으로 수정된 날짜"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_2.data_word_glossary.html#데이터-표준-용어-사전이란",
    "href": "docs/blog/posts/Governance/5_2.data_word_glossary.html#데이터-표준-용어-사전이란",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "조직에서 사용되는 모든 데이터 관련 용어의 공식적인 정의를 제공하는 중앙 집중식 저장소이다.\n업무상 사용되는 용어를 정보시스템에서 사용하는 기술적인 용어로 전환하여 이것을 일관되게 사용 할 수 있도록 정의한 것을 지칭한다.\n각 용어의 의미, 사용 맥락, 형식 등을 명확히 기술한다.\n표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다.\n모델링에서는 속성명으로 사용되며 전사관점에서 유일하다.\n예시\n\n표준 단어 (주제어): 장기\n표준 단어 (수식어): 대여\n표준 단어 (분류단어): 금액\n표준 용어 : 장기대여금액\n데이터 모델 (속성명): 장기대여금액 with datatype = NUMBER(10)\n도메인: 금액N10\n\n\n\n\n\n데이터의 일관성을 확보하고 품질을 향상\n조직 전체에서 데이터 용어를 일관되게 사용할 수 있도록 한다.\n\n즉, 전사적으로 표준화된 용어를 사용함으로써 데이터 모델 구성 요소의 명칭을 부여하는데 일관성을 유지할 수 있다\n\n시스템 간 데이터 통합과 매핑을 용이하게 한다.\n표준화된 명칭을 부여함으로써 데이터의 중복 정의 방지와 의미 전달의 명확성을 확보하여 의사 소통을 원활하게 한다\n\n\n\n\n\n용어 논리명 (보통 한글)\n\n논리명은 데이터의 의미를 나타내는 명칭으로 표준단어 명으로 구성된다.\n용어 논리명의 최대길이는 30자로 한다. 단 20자 이내로 작성할 것을 권장한다\n표준속성 구성 시 5개 단어를 넘지 않도록 하며, 구분자나 띄어쓰기 없이 한 단어로 붙인다 (월_계좌잔액(X) → 월계좌잔액)\n\n용어 물리명 (보통 영어)\n\n물리명은 단어의 영문 약어 조합으로 이루어지며 단어의 영문약어들끼리 연결 할 때는 언더바(_)를 사용한다.\n용어의 물리명은 최종적으로 데이터베이스를 구성 할 때 테이블의 컬럼명으로 사용한다.\n용어 물리명의 최대길이는 28자로 한다. 단 20자 이내로 작성할 것을 권장한다. (단, 용어 논리 및 물리명의 길이는 DBMS에 따라 달라질 수 있음)\n\n정의\n\n용어가 업무적으로 사용되는 의미를 기술한 내용이다.\n\n도메인 정보\n\n특정 비즈니스 컨텍스트에서 데이터 값의 허용 범위를 정의\n즉, 데이터 값의 범위를 한정하는 데이터 타입과 길이,소수점을 의미한다.\n데이터 타입을 포함하며, 추가적인 제약조건이나 비즈니스 규칙을 포함할 수 있습니다.\n비즈니스 로직과 데이터 무결성 규칙을 포함할 수 있다.\n예시\n\n나이: INTEGER, 0-150 사이의 값만 허용\n이메일: VARCHAR(100), 이메일 형식 검증 규칙 포함\n급여: DECIMAL(10,2), 0보다 큰 값만 허용\n\n\n데이터 타입 및 형식\n\n데이터의 기본적인 저장 형태와 구조를 나타낸다.\n일반적이고 기본적인 데이터 유형을 지정\n보통 도메인 정보가 데이터 타입 및 형식 정보를 모두 포함한다.\n예시\n\n문자열(VARCHAR, CHAR)\n숫자(INTEGER, DECIMAL)\n날짜/시간(DATE, TIMESTAMP)\n불리언(BOOLEAN)\n\n\n코드\n\n입력할 수 있는 유효 값 데이터 값을 정의할 수 있다면 용어는 코드와 매핑 한다.\n\n관련 업무 영역\n\n해당 용어가 주로 사용되는 비즈니스 또는 조직 내의 특정 부서나 기능 영역을 나타낸다.\n목적: 용어의 사용 맥락을 제공하고, 해당 용어가 어떤 비즈니스 프로세스나 기능과 관련있는지 이해하는 데 도움을 준다.\n예시\n\n“고객ID” - 관련 업무 영역: 고객 관리, 마케팅, 영업\n“재고수량” - 관련 업무 영역: 재고 관리, 물류, 구매\n“급여액” - 관련 업무 영역: 인사, 재무\n\n\n사용 예시 등\n\n“생년월일” - 사용 예시: “1990-05-15”\n“주문상태” - 사용 예시: “접수”, “처리중”, “배송완료”\n“계좌잔액” - 사용 예시: “1,000,000원”\n\n용어 사전 예시\n\n네, 관련 업무 영역과 사용 예시 칼럼을 추가한 마크다운 테이블을 만들어 드리겠습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n용어 논리명\n용어 물리명\n도메인(인포타입)\n코드\n정의\n관련 업무 영역\n사용 예시\n\n\n\n\n계약종료일자\nCNTR_END_DATE\n일자VC8\n\n거래 계약의 종료일자이다. YYYYMMDD로 작성한다.\n계약관리, 고객관리\n“20231231”\n\n\n렌탈등록비공급가액\nRENT_RGST_SPAM\n금액N10\n\n렌탈 등록 공급 금액을 뜻한다. 숫자로 작성한다.\n렌탈관리, 재무\n“50000”\n\n\n관청은행코드\nGOV_BANK_CD\n은행코드\n은행코드\n해당 관할 은행의 코드이다.\n재무, 회계\n“004” (국민은행)\n\n\n\n이 테이블에서 ‘관련 업무 영역’과 ’사용 예시’ 칼럼을 추가하여 각 용어의 사용 맥락과 실제 적용 예를 보여주고 있습니다. 실제 환경에서는 이 정보들을 조직의 특성과 요구사항에 맞게 더 구체적으로 작성할 수 있습니다.\n\n\n\n\n표준용어 작성 시 누구나 이해하기 쉽도록 간결하되 명확하고 모호함 없이 표현하도록 해야 하며, 다음과 같은 기본 원칙에 위배되지 않도록 한다.\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어는 관용적으로 사용하는 용어를 우선적으로 사용한다\n\n\n\n2\n• 표준용어를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n등록일자(X) → 장비자산등록일자(O)\n\n\n3\n• 용어 구성 시 단어는 반드시 표준 단어 사전에 등록된 단어를 사용하며, 단어 사전에 등록되어 있지 않은 경우에는 표준 담당자와 협의 후에 신규 단어로 등록하도록 한다.\n단어 부재 시 신규 요청 요망\n\n\n4\n• 일반적인 의미와 전혀 다르게 사용된 용어는 적절한 다른 용어로 대체하고, 유사한 의미의 용어가 중복 개발되어 혼재되지 않도록 하며 새로운 용어의 개발은 자제한다.\n반환일자(X) 반납일자(O)\n\n\n5\n• 표준용어로 등록된 명칭의 전사적으로 사용되어야 함으로 명 선정 시 신중하게 고려하여야 한다\n\n\n\n6\n• 표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다\n렌탈 + 가능(X) 렌탈 + 가능 + 금액(O)\n\n\n7\n• 표준용어 명명 규칙 - 표준용어는 누구나 이해하기 쉽도록 구체적이고 명확하고 간결하게 정의한다 - 복합어를 단일어 보다 우선 적용한다 - 복합어가 중첩되어 사용될 경우 도메인이 포함된 복합어를 우선 적용한다 - 의미 있는 숫자를 포함한 용어의 경우에는 숫자를 포함하여 하나의 표준단어를 등록한 후 그 표준 단어를 사용하여 용어를 정의한다 - 용어의 의미를 모호하게 하는 의미 없는 일련번호를 부여하기 위한 숫자는 사용하지 않으며 용어에 수식어를 사용하여 용어가 유일하게 식별되도록 정의하는 것을 원칙으로 한다\n\n\n\n\n\n\n\n\n표준 용어 = 수식어 (표준 단어) + 주제어 (표준 단어) + \\(\\dots\\) + 수식어 (표준 단어) + 주제어 (표준 단어) + 분류어\n수식어 예시\n\n\n\n수식어\n예시\n\n\n\n\n기간 수식어\n최초, 최종, 과거, 최근 등\n\n\n기간/시간\n6개월, 당월, 월말, 년초, 년말 등\n\n\n장소\n국외, 국내, 지점, 본점 등\n\n\n특징\n순수, 사용, 처리, 거래 등\n\n\n계산\n합계: 한데 모아서 합산함누계: 계속하여 덧붙여 합산함\n\n\n\n표준 용어 = 표준 용어 + 표준 도메인\n원칙 항목 및 설명\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 용어는 수식과 수식의 대상이 되는 단어가 여러 개 존재할 수 있고 도메인을 수식하는 분류어는 용어의 끝에 위치한다\n\n\n\n2\n• 수식어는 기간, 장소, 특징, 계산의 성격을 가지는 단어가 순서대로 위치하고 기간을 수식하는 단어는 맨 앞에 위치한다\n\n\n\n3\n• 수식어 중 계산의 성격을 가진 단어는 합계, 누계, 총합계, 총누계, 소계 중 하나를 선택하여 사용해야 한다. (도메인 그룹이 금액과 수량과 같이 계산이 필요한 도메인을 수식함)\n\n\n\n4\n• 대상이 되는 단어가 여러 개일 때는 중 범위가 큰 것 순서대로 용어의 앞부분에 위치한다\n\n\n\n\n주의 사항\n\n\n\n\n\n\n\n\nAS-IS 용어 (비권장)\nTO-BE 용어(권장)\n비고\n\n\n\n\n회사에게 하고 싶은 말 내용\n사용자건의사항\n서술형용어\n\n\n공임금액구분별금액\n공임구분별금액\n단어 반복\n\n\n법인번호\n?+법인등록번호\n주제어 누락 및 약어 사용\n\n\n상세순번\n?+상세일련번호\n주제어 누락\n\n\n수정자\n?+수정자+?\n주제어 및 분류어 누락\n\n\n스케줄추가(청구)정보 수정여부\n스케줄추가정보수정여부\n특수문자사용\n\n\n요청일\n장기렌트입고요청일자\n주제어누락\n\n\n운전자\n렌트카운전자성명\n주제어 및 분류어 누락\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어에 적절한 명칭을 부여하고 전사적 공유가 가능하기 위해서는 구체적인 정의가 반드시 있어야 한다- 표준용어정의는 활동을 나타내는 단어와 그 주체가 되는 단어와 무엇을 어떻게 했는지의 내용이 담겨 있어야 한다.\n대여차량입고일자: 렌트차량이 반환되어 차고지에 입고 된 일자이다.\n\n\n2\n• 표준용어의 명칭이 주는 의미가 불분명하면 좀더 상세하게 정의해야 한다.\n유효기간(X) → 회원멤버쉽유효기간(O)\n\n\n3\n• 동일한 의미의 용어가 중복되지 않도록 표준용어 구성 순서를 고려해 생성한다\n현금서비스최종3개월 총합계금액(X) → 최종3개월 현금서비스 총합계금액(O)\n\n\n4\n• 표준용어의 한글명 또는 영문명 길이 제한으로 인해 축약된 형태로 사용해야 하는 경우 용어를 구성하는 단어 중 연관도와 활용도가 높은 단어들을 합하여 복합어를 정의해야 한다\n고객차량등급^코드 → 고객차량등급코드\n\n\n5\n• 표준용어 중 ‘여부’ 도메인으로 끝나는 것들은 대표로 하나의 코드명과 코드 값(Y, N)을 등록하여 모든 공통적으로 사용하도록 한다.\n코드일련번호 : B011\n\n\n6\n• ‘계약번호’ 자체로는 그 의미가 불분명하여 대차계약번호, 유지보수계약번호 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n7\n• 일반적으로 ‘렌탈’은 ’자동차대여’를 말하는 것이나 ’렌탈’ 자체로는 그 의미가 불분명하여 단기렌탈, 장기렌탈 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n8\n• 신청사용자아이디 → 사용자아이디 에서 ’신청’의 단어처럼 생략해도 의미가 통하는 경우에는 표준용어는 축약된 형태로 정의한다"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_2.data_word_glossary.html#제작-과정",
    "href": "docs/blog/posts/Governance/5_2.data_word_glossary.html#제작-과정",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "각 업무 영역에서 필요한 용어들을 수집\n기존 시스템, 문서, 보고서 등에서 사용 중인 비표준 용어들을 식별\n\n\n\n\n\n수집된 용어들의 의미와 사용 맥락을 분석\n유사하거나 중복된 용어들을 식별\n\n\n\n\n\n분석된 용어들을 기존의 표준 단어 사전과 매핑\n각 용어를 구성하는 단어들이 표준 단어 사전에 있는지 확인\n\n\n\n\n\n표준 단어들을 조합하여 새로운 표준 용어를 구성\n용어 구성 원칙(예: 수식어 순서, 도메인 위치 등)을 따른다.\n\n\n\n\n\n구성된 용어가 의미를 명확히 전달하는지 확인\n기존 표준 용어와 중복되지 않는지 검토\n전문가나 제 3의 부서와 cross check\n\n\n\n\n\n용어의 정의, 사용 예시, 관련 업무 영역 등 상세 정보를 작성\n용어와 관련된 도메인 정보를 지정\n\n\n\n\n\n구성된 용어에 대해 관련 부서와 데이터 관리자의 검토를 받는다.\n필요시 수정하고 최종 승인을 받는다.\n\n\n\n\n\n승인된 용어를 표준 용어 사전에 등록\n용어의 물리명(영문명)을 생성\n\n\n\n\n\n새로 등록된 표준 용어를 조직 내에 공지\n필요시 사용 방법에 대한 교육을 실시\n\n\n\n\n\n새로 등록된 표준 용어의 사용 현황을 모니터링\n사용자 피드백을 수집\n\n\n\n\n\n정기적 검토 및 피드백 수집\n\n분기별로 용어 사전 검토 일정을 수립\n사용자로부터 피드백을 수집하고, 사용 현황을 모니터링\n\n변경 관리 프로세스 운영\n\n용어 추가, 수정, 폐기를 위한 공식적인 변경 요청 프로세스를 구축\n변경 요청에 대한 영향 분석을 수행하고, 승인 절차를 거친다.\n\n업데이트 및 버전 관리\n\n승인된 변경사항을 용어 사전에 반영\n버전 관리를 통해 변경 이력을 추적하고, 주요 변경사항을 공지\n\n교육 및 홍보\n\n변경된 용어나 새로운 용어에 대한 교육을 실시\n내부 커뮤니케이션 채널을 통해 주요 업데이트 사항을 공유\n\n성과 측정 및 개선\n\n용어 표준화로 인한 데이터 품질 개선, 업무 효율성 증가 등의 성과를 측정\n측정 결과를 바탕으로 용어 관리 전략을 지속적으로 개선"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_2.data_word_glossary.html#표준-단어-사전-예시",
    "href": "docs/blog/posts/Governance/5_2.data_word_glossary.html#표준-단어-사전-예시",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n용어ID\n용어논리명\n용어물리명\n도메인\n정의\n관련업무영역\n사용예시\n표준단어구성\n승인상태\n등록일\n\n\n\n\nT001\n고객번호\nCUST_NO\n일련번호VC10\n고객을 유일하게 식별하는 번호\n고객관리, 영업\n“C0001234567”\n고객(주제어) + 번호(분류어)\n승인\n2023-01-15\n\n\nT002\n장기렌트계약시작일자\nLTRM_RENT_CNTR_STRT_DT\n일자VC8\n장기 렌트 계약이 시작되는 날짜\n계약관리, 렌트관리\n“20230107”\n장기(수식어) + 렌트(주제어) + 계약(주제어) + 시작(수식어) + 일자(분류어)\n승인\n2023-01-16\n\n\nT003\n월별렌탈등록비합계금액\nMTHLY_RENT_RGST_FEE_TOT_AMT\n금액N15\n한 달 동안의 렌탈 등록비 총액\n재무, 렌트관리\n“5000000”\n월별(수식어) + 렌탈(주제어) + 등록(주제어) + 비(주제어) + 합계(수식어) + 금액(분류어)\n검토중\n2023-01-17\n\n\n\n* VC10: Variable Character의 약자로 가변 길이 문자열을 나타냄 (최대 10 자리)  \n* N15: N은 숫자(Numeric) 데이터 타입, 최대 15자리의 숫자  \n* D: Date (날짜)  \n* T: Time (시간)  \n* B: Boolean (참/거짓)  \n\n칼럼 설명:\n\n\n용어ID: 각 용어의 고유 식별자\n용어논리명: 업무에서 사용되는 한글 용어명 (최대 30자, 권장 20자 이내)\n용어물리명: 데이터베이스 등에서 사용되는 영문 약어명 (최대 28자, 권장 20자 이내)\n도메인: 해당 용어의 데이터 타입과 제약조건\n정의: 용어에 대한 명확한 설명\n관련업무영역: 해당 용어가 주로 사용되는 업무 분야\n사용예시: 실제 데이터 예시\n표준단어구성: 용어를 구성하는 표준 단어들과 각 단어의 역할 (수식어, 주제어, 분류어)\n승인상태: 용어의 현재 승인 상태 (예: 승인, 검토중, 폐기 등)\n등록일: 용어가 사전에 처음 등록된 날짜\n최종수정일: 용어 정보가 마지막으로 수정된 날짜"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_3.data_word_glossary.html",
    "href": "docs/blog/posts/Governance/5_3.data_word_glossary.html",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "조직에서 사용되는 모든 데이터 관련 용어의 공식적인 정의를 제공하는 중앙 집중식 저장소이다.\n업무상 사용되는 용어를 정보시스템에서 사용하는 기술적인 용어로 전환하여 이것을 일관되게 사용 할 수 있도록 정의한 것을 지칭한다.\n각 용어의 의미, 사용 맥락, 형식 등을 명확히 기술한다.\n표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다.\n모델링에서는 속성명으로 사용되며 전사관점에서 유일하다.\n예시\n\n표준 단어 (주제어): 장기\n표준 단어 (수식어): 대여\n표준 단어 (분류단어): 금액\n표준 용어 : 장기대여금액\n데이터 모델 (속성명): 장기대여금액 with datatype = NUMBER(10)\n도메인: 금액N10\n\n\n\n\n\n데이터의 일관성을 확보하고 품질을 향상\n조직 전체에서 데이터 용어를 일관되게 사용할 수 있도록 한다.\n\n즉, 전사적으로 표준화된 용어를 사용함으로써 데이터 모델 구성 요소의 명칭을 부여하는데 일관성을 유지할 수 있다\n\n시스템 간 데이터 통합과 매핑을 용이하게 한다.\n표준화된 명칭을 부여함으로써 데이터의 중복 정의 방지와 의미 전달의 명확성을 확보하여 의사 소통을 원활하게 한다\n\n\n\n\n\n용어 논리명 (보통 한글)\n\n논리명은 데이터의 의미를 나타내는 명칭으로 표준단어 명으로 구성된다.\n용어 논리명의 최대길이는 30자로 한다. 단 20자 이내로 작성할 것을 권장한다\n표준속성 구성 시 5개 단어를 넘지 않도록 하며, 구분자나 띄어쓰기 없이 한 단어로 붙인다 (월_계좌잔액(X) → 월계좌잔액)\n\n용어 물리명 (보통 영어)\n\n물리명은 단어의 영문 약어 조합으로 이루어지며 단어의 영문약어들끼리 연결 할 때는 언더바(_)를 사용한다.\n용어의 물리명은 최종적으로 데이터베이스를 구성 할 때 테이블의 컬럼명으로 사용한다.\n용어 물리명의 최대길이는 28자로 한다. 단 20자 이내로 작성할 것을 권장한다. (단, 용어 논리 및 물리명의 길이는 DBMS에 따라 달라질 수 있음)\n\n정의\n\n용어가 업무적으로 사용되는 의미를 기술한 내용이다.\n\n도메인 정보\n\n특정 비즈니스 컨텍스트에서 데이터 값의 허용 범위를 정의\n즉, 데이터 값의 범위를 한정하는 데이터 타입과 길이,소수점을 의미한다.\n데이터 타입을 포함하며, 추가적인 제약조건이나 비즈니스 규칙을 포함할 수 있습니다.\n비즈니스 로직과 데이터 무결성 규칙을 포함할 수 있다.\n예시\n\n나이: INTEGER, 0-150 사이의 값만 허용\n이메일: VARCHAR(100), 이메일 형식 검증 규칙 포함\n급여: DECIMAL(10,2), 0보다 큰 값만 허용\n\n\n데이터 타입 및 형식\n\n데이터의 기본적인 저장 형태와 구조를 나타낸다.\n일반적이고 기본적인 데이터 유형을 지정\n보통 도메인 정보가 데이터 타입 및 형식 정보를 모두 포함한다.\n예시\n\n문자열(VARCHAR, CHAR)\n숫자(INTEGER, DECIMAL)\n날짜/시간(DATE, TIMESTAMP)\n불리언(BOOLEAN)\n\n\n코드\n\n입력할 수 있는 유효 값 데이터 값을 정의할 수 있다면 용어는 코드와 매핑 한다.\n\n관련 업무 영역\n\n해당 용어가 주로 사용되는 비즈니스 또는 조직 내의 특정 부서나 기능 영역을 나타낸다.\n목적: 용어의 사용 맥락을 제공하고, 해당 용어가 어떤 비즈니스 프로세스나 기능과 관련있는지 이해하는 데 도움을 준다.\n예시\n\n“고객ID” - 관련 업무 영역: 고객 관리, 마케팅, 영업\n“재고수량” - 관련 업무 영역: 재고 관리, 물류, 구매\n“급여액” - 관련 업무 영역: 인사, 재무\n\n\n사용 예시 등\n\n“생년월일” - 사용 예시: “1990-05-15”\n“주문상태” - 사용 예시: “접수”, “처리중”, “배송완료”\n“계좌잔액” - 사용 예시: “1,000,000원”\n\n용어 사전 예시\n\n네, 관련 업무 영역과 사용 예시 칼럼을 추가한 마크다운 테이블을 만들어 드리겠습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n용어 논리명\n용어 물리명\n도메인(인포타입)\n코드\n정의\n관련 업무 영역\n사용 예시\n\n\n\n\n계약종료일자\nCNTR_END_DATE\n일자VC8\n\n거래 계약의 종료일자이다. YYYYMMDD로 작성한다.\n계약관리, 고객관리\n“20231231”\n\n\n렌탈등록비공급가액\nRENT_RGST_SPAM\n금액N10\n\n렌탈 등록 공급 금액을 뜻한다. 숫자로 작성한다.\n렌탈관리, 재무\n“50000”\n\n\n관청은행코드\nGOV_BANK_CD\n은행코드\n은행코드\n해당 관할 은행의 코드이다.\n재무, 회계\n“004” (국민은행)\n\n\n\n이 테이블에서 ‘관련 업무 영역’과 ’사용 예시’ 칼럼을 추가하여 각 용어의 사용 맥락과 실제 적용 예를 보여주고 있습니다. 실제 환경에서는 이 정보들을 조직의 특성과 요구사항에 맞게 더 구체적으로 작성할 수 있습니다.\n\n\n\n\n표준용어 작성 시 누구나 이해하기 쉽도록 간결하되 명확하고 모호함 없이 표현하도록 해야 하며, 다음과 같은 기본 원칙에 위배되지 않도록 한다.\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어는 관용적으로 사용하는 용어를 우선적으로 사용한다\n\n\n\n2\n• 표준용어를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n등록일자(X) → 장비자산등록일자(O)\n\n\n3\n• 용어 구성 시 단어는 반드시 표준 단어 사전에 등록된 단어를 사용하며, 단어 사전에 등록되어 있지 않은 경우에는 표준 담당자와 협의 후에 신규 단어로 등록하도록 한다.\n단어 부재 시 신규 요청 요망\n\n\n4\n• 일반적인 의미와 전혀 다르게 사용된 용어는 적절한 다른 용어로 대체하고, 유사한 의미의 용어가 중복 개발되어 혼재되지 않도록 하며 새로운 용어의 개발은 자제한다.\n반환일자(X) 반납일자(O)\n\n\n5\n• 표준용어로 등록된 명칭의 전사적으로 사용되어야 함으로 명 선정 시 신중하게 고려하여야 한다\n\n\n\n6\n• 표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다\n렌탈 + 가능(X) 렌탈 + 가능 + 금액(O)\n\n\n7\n• 표준용어 명명 규칙 - 표준용어는 누구나 이해하기 쉽도록 구체적이고 명확하고 간결하게 정의한다 - 복합어를 단일어 보다 우선 적용한다 - 복합어가 중첩되어 사용될 경우 도메인이 포함된 복합어를 우선 적용한다 - 의미 있는 숫자를 포함한 용어의 경우에는 숫자를 포함하여 하나의 표준단어를 등록한 후 그 표준 단어를 사용하여 용어를 정의한다 - 용어의 의미를 모호하게 하는 의미 없는 일련번호를 부여하기 위한 숫자는 사용하지 않으며 용어에 수식어를 사용하여 용어가 유일하게 식별되도록 정의하는 것을 원칙으로 한다\n\n\n\n\n\n\n\n\n표준 용어 = 수식어 (표준 단어) + 주제어 (표준 단어) + \\(\\dots\\) + 수식어 (표준 단어) + 주제어 (표준 단어) + 분류어\n수식어 예시\n\n\n\n수식어\n예시\n\n\n\n\n기간 수식어\n최초, 최종, 과거, 최근 등\n\n\n기간/시간\n6개월, 당월, 월말, 년초, 년말 등\n\n\n장소\n국외, 국내, 지점, 본점 등\n\n\n특징\n순수, 사용, 처리, 거래 등\n\n\n계산\n합계: 한데 모아서 합산함누계: 계속하여 덧붙여 합산함\n\n\n\n표준 용어 = 표준 용어 + 표준 도메인\n원칙 항목 및 설명\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 용어는 수식과 수식의 대상이 되는 단어가 여러 개 존재할 수 있고 도메인을 수식하는 분류어는 용어의 끝에 위치한다\n\n\n\n2\n• 수식어는 기간, 장소, 특징, 계산의 성격을 가지는 단어가 순서대로 위치하고 기간을 수식하는 단어는 맨 앞에 위치한다\n\n\n\n3\n• 수식어 중 계산의 성격을 가진 단어는 합계, 누계, 총합계, 총누계, 소계 중 하나를 선택하여 사용해야 한다. (도메인 그룹이 금액과 수량과 같이 계산이 필요한 도메인을 수식함)\n\n\n\n4\n• 대상이 되는 단어가 여러 개일 때는 중 범위가 큰 것 순서대로 용어의 앞부분에 위치한다\n\n\n\n\n주의 사항\n\n\n\n\n\n\n\n\nAS-IS 용어 (비권장)\nTO-BE 용어(권장)\n비고\n\n\n\n\n회사에게 하고 싶은 말 내용\n사용자건의사항\n서술형용어\n\n\n공임금액구분별금액\n공임구분별금액\n단어 반복\n\n\n법인번호\n?+법인등록번호\n주제어 누락 및 약어 사용\n\n\n상세순번\n?+상세일련번호\n주제어 누락\n\n\n수정자\n?+수정자+?\n주제어 및 분류어 누락\n\n\n스케줄추가(청구)정보 수정여부\n스케줄추가정보수정여부\n특수문자사용\n\n\n요청일\n장기렌트입고요청일자\n주제어누락\n\n\n운전자\n렌트카운전자성명\n주제어 및 분류어 누락\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어에 적절한 명칭을 부여하고 전사적 공유가 가능하기 위해서는 구체적인 정의가 반드시 있어야 한다- 표준용어정의는 활동을 나타내는 단어와 그 주체가 되는 단어와 무엇을 어떻게 했는지의 내용이 담겨 있어야 한다.\n대여차량입고일자: 렌트차량이 반환되어 차고지에 입고 된 일자이다.\n\n\n2\n• 표준용어의 명칭이 주는 의미가 불분명하면 좀더 상세하게 정의해야 한다.\n유효기간(X) → 회원멤버쉽유효기간(O)\n\n\n3\n• 동일한 의미의 용어가 중복되지 않도록 표준용어 구성 순서를 고려해 생성한다\n현금서비스최종3개월 총합계금액(X) → 최종3개월 현금서비스 총합계금액(O)\n\n\n4\n• 표준용어의 한글명 또는 영문명 길이 제한으로 인해 축약된 형태로 사용해야 하는 경우 용어를 구성하는 단어 중 연관도와 활용도가 높은 단어들을 합하여 복합어를 정의해야 한다\n고객차량등급^코드 → 고객차량등급코드\n\n\n5\n• 표준용어 중 ‘여부’ 도메인으로 끝나는 것들은 대표로 하나의 코드명과 코드 값(Y, N)을 등록하여 모든 공통적으로 사용하도록 한다.\n코드일련번호 : B011\n\n\n6\n• ‘계약번호’ 자체로는 그 의미가 불분명하여 대차계약번호, 유지보수계약번호 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n7\n• 일반적으로 ‘렌탈’은 ’자동차대여’를 말하는 것이나 ’렌탈’ 자체로는 그 의미가 불분명하여 단기렌탈, 장기렌탈 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n8\n• 신청사용자아이디 → 사용자아이디 에서 ’신청’의 단어처럼 생략해도 의미가 통하는 경우에는 표준용어는 축약된 형태로 정의한다\n\n\n\n\n\n\n\n\n\n\n\n각 업무 영역에서 필요한 용어들을 수집\n기존 시스템, 문서, 보고서 등에서 사용 중인 비표준 용어들을 식별\n\n\n\n\n\n수집된 용어들의 의미와 사용 맥락을 분석\n유사하거나 중복된 용어들을 식별\n\n\n\n\n\n분석된 용어들을 기존의 표준 단어 사전과 매핑\n각 용어를 구성하는 단어들이 표준 단어 사전에 있는지 확인\n\n\n\n\n\n표준 단어들을 조합하여 새로운 표준 용어를 구성\n용어 구성 원칙(예: 수식어 순서, 도메인 위치 등)을 따른다.\n\n\n\n\n\n구성된 용어가 의미를 명확히 전달하는지 확인\n기존 표준 용어와 중복되지 않는지 검토\n전문가나 제 3의 부서와 cross check\n\n\n\n\n\n용어의 정의, 사용 예시, 관련 업무 영역 등 상세 정보를 작성\n용어와 관련된 도메인 정보를 지정\n\n\n\n\n\n구성된 용어에 대해 관련 부서와 데이터 관리자의 검토를 받는다.\n필요시 수정하고 최종 승인을 받는다.\n\n\n\n\n\n승인된 용어를 표준 용어 사전에 등록\n용어의 물리명(영문명)을 생성\n\n\n\n\n\n새로 등록된 표준 용어를 조직 내에 공지\n필요시 사용 방법에 대한 교육을 실시\n\n\n\n\n\n새로 등록된 표준 용어의 사용 현황을 모니터링\n사용자 피드백을 수집\n\n\n\n\n\n정기적 검토 및 피드백 수집\n\n분기별로 용어 사전 검토 일정을 수립\n사용자로부터 피드백을 수집하고, 사용 현황을 모니터링\n\n변경 관리 프로세스 운영\n\n용어 추가, 수정, 폐기를 위한 공식적인 변경 요청 프로세스를 구축\n변경 요청에 대한 영향 분석을 수행하고, 승인 절차를 거친다.\n\n업데이트 및 버전 관리\n\n승인된 변경사항을 용어 사전에 반영\n버전 관리를 통해 변경 이력을 추적하고, 주요 변경사항을 공지\n\n교육 및 홍보\n\n변경된 용어나 새로운 용어에 대한 교육을 실시\n내부 커뮤니케이션 채널을 통해 주요 업데이트 사항을 공유\n\n성과 측정 및 개선\n\n용어 표준화로 인한 데이터 품질 개선, 업무 효율성 증가 등의 성과를 측정\n측정 결과를 바탕으로 용어 관리 전략을 지속적으로 개선\n\n\n\n\n\n\n\n예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n용어ID\n용어논리명\n용어물리명\n도메인\n정의\n관련업무영역\n사용예시\n표준단어구성\n승인상태\n등록일\n\n\n\n\nT001\n고객번호\nCUST_NO\n일련번호VC10\n고객을 유일하게 식별하는 번호\n고객관리, 영업\n“C0001234567”\n고객(주제어) + 번호(분류어)\n승인\n2023-01-15\n\n\nT002\n장기렌트계약시작일자\nLTRM_RENT_CNTR_STRT_DT\n일자VC8\n장기 렌트 계약이 시작되는 날짜\n계약관리, 렌트관리\n“20230107”\n장기(수식어) + 렌트(주제어) + 계약(주제어) + 시작(수식어) + 일자(분류어)\n승인\n2023-01-16\n\n\nT003\n월별렌탈등록비합계금액\nMTHLY_RENT_RGST_FEE_TOT_AMT\n금액N15\n한 달 동안의 렌탈 등록비 총액\n재무, 렌트관리\n“5000000”\n월별(수식어) + 렌탈(주제어) + 등록(주제어) + 비(주제어) + 합계(수식어) + 금액(분류어)\n검토중\n2023-01-17\n\n\n\n* VC10: Variable Character의 약자로 가변 길이 문자열을 나타냄 (최대 10 자리)  \n* N15: N은 숫자(Numeric) 데이터 타입, 최대 15자리의 숫자  \n* D: Date (날짜)  \n* T: Time (시간)  \n* B: Boolean (참/거짓)  \n\n칼럼 설명\n\n용어ID: 각 용어의 고유 식별자\n용어논리명: 업무에서 사용되는 한글 용어명 (최대 30자, 권장 20자 이내)\n용어물리명: 데이터베이스 등에서 사용되는 영문 약어명 (최대 28자, 권장 20자 이내)\n도메인: 해당 용어의 데이터 타입과 제약조건\n정의: 용어에 대한 명확한 설명\n관련업무영역: 해당 용어가 주로 사용되는 업무 분야\n사용예시: 실제 데이터 예시\n표준단어구성: 용어를 구성하는 표준 단어들과 각 단어의 역할 (수식어, 주제어, 분류어)\n승인상태: 용어의 현재 승인 상태 (예: 승인, 검토중, 폐기 등)\n등록일: 용어가 사전에 처음 등록된 날짜\n최종수정일: 용어 정보가 마지막으로 수정된 날짜"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_3.data_word_glossary.html#데이터-표준-용어-사전이란",
    "href": "docs/blog/posts/Governance/5_3.data_word_glossary.html#데이터-표준-용어-사전이란",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "조직에서 사용되는 모든 데이터 관련 용어의 공식적인 정의를 제공하는 중앙 집중식 저장소이다.\n업무상 사용되는 용어를 정보시스템에서 사용하는 기술적인 용어로 전환하여 이것을 일관되게 사용 할 수 있도록 정의한 것을 지칭한다.\n각 용어의 의미, 사용 맥락, 형식 등을 명확히 기술한다.\n표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다.\n모델링에서는 속성명으로 사용되며 전사관점에서 유일하다.\n예시\n\n표준 단어 (주제어): 장기\n표준 단어 (수식어): 대여\n표준 단어 (분류단어): 금액\n표준 용어 : 장기대여금액\n데이터 모델 (속성명): 장기대여금액 with datatype = NUMBER(10)\n도메인: 금액N10\n\n\n\n\n\n데이터의 일관성을 확보하고 품질을 향상\n조직 전체에서 데이터 용어를 일관되게 사용할 수 있도록 한다.\n\n즉, 전사적으로 표준화된 용어를 사용함으로써 데이터 모델 구성 요소의 명칭을 부여하는데 일관성을 유지할 수 있다\n\n시스템 간 데이터 통합과 매핑을 용이하게 한다.\n표준화된 명칭을 부여함으로써 데이터의 중복 정의 방지와 의미 전달의 명확성을 확보하여 의사 소통을 원활하게 한다\n\n\n\n\n\n용어 논리명 (보통 한글)\n\n논리명은 데이터의 의미를 나타내는 명칭으로 표준단어 명으로 구성된다.\n용어 논리명의 최대길이는 30자로 한다. 단 20자 이내로 작성할 것을 권장한다\n표준속성 구성 시 5개 단어를 넘지 않도록 하며, 구분자나 띄어쓰기 없이 한 단어로 붙인다 (월_계좌잔액(X) → 월계좌잔액)\n\n용어 물리명 (보통 영어)\n\n물리명은 단어의 영문 약어 조합으로 이루어지며 단어의 영문약어들끼리 연결 할 때는 언더바(_)를 사용한다.\n용어의 물리명은 최종적으로 데이터베이스를 구성 할 때 테이블의 컬럼명으로 사용한다.\n용어 물리명의 최대길이는 28자로 한다. 단 20자 이내로 작성할 것을 권장한다. (단, 용어 논리 및 물리명의 길이는 DBMS에 따라 달라질 수 있음)\n\n정의\n\n용어가 업무적으로 사용되는 의미를 기술한 내용이다.\n\n도메인 정보\n\n특정 비즈니스 컨텍스트에서 데이터 값의 허용 범위를 정의\n즉, 데이터 값의 범위를 한정하는 데이터 타입과 길이,소수점을 의미한다.\n데이터 타입을 포함하며, 추가적인 제약조건이나 비즈니스 규칙을 포함할 수 있습니다.\n비즈니스 로직과 데이터 무결성 규칙을 포함할 수 있다.\n예시\n\n나이: INTEGER, 0-150 사이의 값만 허용\n이메일: VARCHAR(100), 이메일 형식 검증 규칙 포함\n급여: DECIMAL(10,2), 0보다 큰 값만 허용\n\n\n데이터 타입 및 형식\n\n데이터의 기본적인 저장 형태와 구조를 나타낸다.\n일반적이고 기본적인 데이터 유형을 지정\n보통 도메인 정보가 데이터 타입 및 형식 정보를 모두 포함한다.\n예시\n\n문자열(VARCHAR, CHAR)\n숫자(INTEGER, DECIMAL)\n날짜/시간(DATE, TIMESTAMP)\n불리언(BOOLEAN)\n\n\n코드\n\n입력할 수 있는 유효 값 데이터 값을 정의할 수 있다면 용어는 코드와 매핑 한다.\n\n관련 업무 영역\n\n해당 용어가 주로 사용되는 비즈니스 또는 조직 내의 특정 부서나 기능 영역을 나타낸다.\n목적: 용어의 사용 맥락을 제공하고, 해당 용어가 어떤 비즈니스 프로세스나 기능과 관련있는지 이해하는 데 도움을 준다.\n예시\n\n“고객ID” - 관련 업무 영역: 고객 관리, 마케팅, 영업\n“재고수량” - 관련 업무 영역: 재고 관리, 물류, 구매\n“급여액” - 관련 업무 영역: 인사, 재무\n\n\n사용 예시 등\n\n“생년월일” - 사용 예시: “1990-05-15”\n“주문상태” - 사용 예시: “접수”, “처리중”, “배송완료”\n“계좌잔액” - 사용 예시: “1,000,000원”\n\n용어 사전 예시\n\n네, 관련 업무 영역과 사용 예시 칼럼을 추가한 마크다운 테이블을 만들어 드리겠습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n용어 논리명\n용어 물리명\n도메인(인포타입)\n코드\n정의\n관련 업무 영역\n사용 예시\n\n\n\n\n계약종료일자\nCNTR_END_DATE\n일자VC8\n\n거래 계약의 종료일자이다. YYYYMMDD로 작성한다.\n계약관리, 고객관리\n“20231231”\n\n\n렌탈등록비공급가액\nRENT_RGST_SPAM\n금액N10\n\n렌탈 등록 공급 금액을 뜻한다. 숫자로 작성한다.\n렌탈관리, 재무\n“50000”\n\n\n관청은행코드\nGOV_BANK_CD\n은행코드\n은행코드\n해당 관할 은행의 코드이다.\n재무, 회계\n“004” (국민은행)\n\n\n\n이 테이블에서 ‘관련 업무 영역’과 ’사용 예시’ 칼럼을 추가하여 각 용어의 사용 맥락과 실제 적용 예를 보여주고 있습니다. 실제 환경에서는 이 정보들을 조직의 특성과 요구사항에 맞게 더 구체적으로 작성할 수 있습니다.\n\n\n\n\n표준용어 작성 시 누구나 이해하기 쉽도록 간결하되 명확하고 모호함 없이 표현하도록 해야 하며, 다음과 같은 기본 원칙에 위배되지 않도록 한다.\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어는 관용적으로 사용하는 용어를 우선적으로 사용한다\n\n\n\n2\n• 표준용어를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n등록일자(X) → 장비자산등록일자(O)\n\n\n3\n• 용어 구성 시 단어는 반드시 표준 단어 사전에 등록된 단어를 사용하며, 단어 사전에 등록되어 있지 않은 경우에는 표준 담당자와 협의 후에 신규 단어로 등록하도록 한다.\n단어 부재 시 신규 요청 요망\n\n\n4\n• 일반적인 의미와 전혀 다르게 사용된 용어는 적절한 다른 용어로 대체하고, 유사한 의미의 용어가 중복 개발되어 혼재되지 않도록 하며 새로운 용어의 개발은 자제한다.\n반환일자(X) 반납일자(O)\n\n\n5\n• 표준용어로 등록된 명칭의 전사적으로 사용되어야 함으로 명 선정 시 신중하게 고려하여야 한다\n\n\n\n6\n• 표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다\n렌탈 + 가능(X) 렌탈 + 가능 + 금액(O)\n\n\n7\n• 표준용어 명명 규칙 - 표준용어는 누구나 이해하기 쉽도록 구체적이고 명확하고 간결하게 정의한다 - 복합어를 단일어 보다 우선 적용한다 - 복합어가 중첩되어 사용될 경우 도메인이 포함된 복합어를 우선 적용한다 - 의미 있는 숫자를 포함한 용어의 경우에는 숫자를 포함하여 하나의 표준단어를 등록한 후 그 표준 단어를 사용하여 용어를 정의한다 - 용어의 의미를 모호하게 하는 의미 없는 일련번호를 부여하기 위한 숫자는 사용하지 않으며 용어에 수식어를 사용하여 용어가 유일하게 식별되도록 정의하는 것을 원칙으로 한다\n\n\n\n\n\n\n\n\n표준 용어 = 수식어 (표준 단어) + 주제어 (표준 단어) + \\(\\dots\\) + 수식어 (표준 단어) + 주제어 (표준 단어) + 분류어\n수식어 예시\n\n\n\n수식어\n예시\n\n\n\n\n기간 수식어\n최초, 최종, 과거, 최근 등\n\n\n기간/시간\n6개월, 당월, 월말, 년초, 년말 등\n\n\n장소\n국외, 국내, 지점, 본점 등\n\n\n특징\n순수, 사용, 처리, 거래 등\n\n\n계산\n합계: 한데 모아서 합산함누계: 계속하여 덧붙여 합산함\n\n\n\n표준 용어 = 표준 용어 + 표준 도메인\n원칙 항목 및 설명\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 용어는 수식과 수식의 대상이 되는 단어가 여러 개 존재할 수 있고 도메인을 수식하는 분류어는 용어의 끝에 위치한다\n\n\n\n2\n• 수식어는 기간, 장소, 특징, 계산의 성격을 가지는 단어가 순서대로 위치하고 기간을 수식하는 단어는 맨 앞에 위치한다\n\n\n\n3\n• 수식어 중 계산의 성격을 가진 단어는 합계, 누계, 총합계, 총누계, 소계 중 하나를 선택하여 사용해야 한다. (도메인 그룹이 금액과 수량과 같이 계산이 필요한 도메인을 수식함)\n\n\n\n4\n• 대상이 되는 단어가 여러 개일 때는 중 범위가 큰 것 순서대로 용어의 앞부분에 위치한다\n\n\n\n\n주의 사항\n\n\n\n\n\n\n\n\nAS-IS 용어 (비권장)\nTO-BE 용어(권장)\n비고\n\n\n\n\n회사에게 하고 싶은 말 내용\n사용자건의사항\n서술형용어\n\n\n공임금액구분별금액\n공임구분별금액\n단어 반복\n\n\n법인번호\n?+법인등록번호\n주제어 누락 및 약어 사용\n\n\n상세순번\n?+상세일련번호\n주제어 누락\n\n\n수정자\n?+수정자+?\n주제어 및 분류어 누락\n\n\n스케줄추가(청구)정보 수정여부\n스케줄추가정보수정여부\n특수문자사용\n\n\n요청일\n장기렌트입고요청일자\n주제어누락\n\n\n운전자\n렌트카운전자성명\n주제어 및 분류어 누락\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어에 적절한 명칭을 부여하고 전사적 공유가 가능하기 위해서는 구체적인 정의가 반드시 있어야 한다- 표준용어정의는 활동을 나타내는 단어와 그 주체가 되는 단어와 무엇을 어떻게 했는지의 내용이 담겨 있어야 한다.\n대여차량입고일자: 렌트차량이 반환되어 차고지에 입고 된 일자이다.\n\n\n2\n• 표준용어의 명칭이 주는 의미가 불분명하면 좀더 상세하게 정의해야 한다.\n유효기간(X) → 회원멤버쉽유효기간(O)\n\n\n3\n• 동일한 의미의 용어가 중복되지 않도록 표준용어 구성 순서를 고려해 생성한다\n현금서비스최종3개월 총합계금액(X) → 최종3개월 현금서비스 총합계금액(O)\n\n\n4\n• 표준용어의 한글명 또는 영문명 길이 제한으로 인해 축약된 형태로 사용해야 하는 경우 용어를 구성하는 단어 중 연관도와 활용도가 높은 단어들을 합하여 복합어를 정의해야 한다\n고객차량등급^코드 → 고객차량등급코드\n\n\n5\n• 표준용어 중 ‘여부’ 도메인으로 끝나는 것들은 대표로 하나의 코드명과 코드 값(Y, N)을 등록하여 모든 공통적으로 사용하도록 한다.\n코드일련번호 : B011\n\n\n6\n• ‘계약번호’ 자체로는 그 의미가 불분명하여 대차계약번호, 유지보수계약번호 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n7\n• 일반적으로 ‘렌탈’은 ’자동차대여’를 말하는 것이나 ’렌탈’ 자체로는 그 의미가 불분명하여 단기렌탈, 장기렌탈 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n8\n• 신청사용자아이디 → 사용자아이디 에서 ’신청’의 단어처럼 생략해도 의미가 통하는 경우에는 표준용어는 축약된 형태로 정의한다"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_3.data_word_glossary.html#제작-과정",
    "href": "docs/blog/posts/Governance/5_3.data_word_glossary.html#제작-과정",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "각 업무 영역에서 필요한 용어들을 수집\n기존 시스템, 문서, 보고서 등에서 사용 중인 비표준 용어들을 식별\n\n\n\n\n\n수집된 용어들의 의미와 사용 맥락을 분석\n유사하거나 중복된 용어들을 식별\n\n\n\n\n\n분석된 용어들을 기존의 표준 단어 사전과 매핑\n각 용어를 구성하는 단어들이 표준 단어 사전에 있는지 확인\n\n\n\n\n\n표준 단어들을 조합하여 새로운 표준 용어를 구성\n용어 구성 원칙(예: 수식어 순서, 도메인 위치 등)을 따른다.\n\n\n\n\n\n구성된 용어가 의미를 명확히 전달하는지 확인\n기존 표준 용어와 중복되지 않는지 검토\n전문가나 제 3의 부서와 cross check\n\n\n\n\n\n용어의 정의, 사용 예시, 관련 업무 영역 등 상세 정보를 작성\n용어와 관련된 도메인 정보를 지정\n\n\n\n\n\n구성된 용어에 대해 관련 부서와 데이터 관리자의 검토를 받는다.\n필요시 수정하고 최종 승인을 받는다.\n\n\n\n\n\n승인된 용어를 표준 용어 사전에 등록\n용어의 물리명(영문명)을 생성\n\n\n\n\n\n새로 등록된 표준 용어를 조직 내에 공지\n필요시 사용 방법에 대한 교육을 실시\n\n\n\n\n\n새로 등록된 표준 용어의 사용 현황을 모니터링\n사용자 피드백을 수집\n\n\n\n\n\n정기적 검토 및 피드백 수집\n\n분기별로 용어 사전 검토 일정을 수립\n사용자로부터 피드백을 수집하고, 사용 현황을 모니터링\n\n변경 관리 프로세스 운영\n\n용어 추가, 수정, 폐기를 위한 공식적인 변경 요청 프로세스를 구축\n변경 요청에 대한 영향 분석을 수행하고, 승인 절차를 거친다.\n\n업데이트 및 버전 관리\n\n승인된 변경사항을 용어 사전에 반영\n버전 관리를 통해 변경 이력을 추적하고, 주요 변경사항을 공지\n\n교육 및 홍보\n\n변경된 용어나 새로운 용어에 대한 교육을 실시\n내부 커뮤니케이션 채널을 통해 주요 업데이트 사항을 공유\n\n성과 측정 및 개선\n\n용어 표준화로 인한 데이터 품질 개선, 업무 효율성 증가 등의 성과를 측정\n측정 결과를 바탕으로 용어 관리 전략을 지속적으로 개선"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_3.data_word_glossary.html#표준-단어-사전-예시",
    "href": "docs/blog/posts/Governance/5_3.data_word_glossary.html#표준-단어-사전-예시",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n용어ID\n용어논리명\n용어물리명\n도메인\n정의\n관련업무영역\n사용예시\n표준단어구성\n승인상태\n등록일\n\n\n\n\nT001\n고객번호\nCUST_NO\n일련번호VC10\n고객을 유일하게 식별하는 번호\n고객관리, 영업\n“C0001234567”\n고객(주제어) + 번호(분류어)\n승인\n2023-01-15\n\n\nT002\n장기렌트계약시작일자\nLTRM_RENT_CNTR_STRT_DT\n일자VC8\n장기 렌트 계약이 시작되는 날짜\n계약관리, 렌트관리\n“20230107”\n장기(수식어) + 렌트(주제어) + 계약(주제어) + 시작(수식어) + 일자(분류어)\n승인\n2023-01-16\n\n\nT003\n월별렌탈등록비합계금액\nMTHLY_RENT_RGST_FEE_TOT_AMT\n금액N15\n한 달 동안의 렌탈 등록비 총액\n재무, 렌트관리\n“5000000”\n월별(수식어) + 렌탈(주제어) + 등록(주제어) + 비(주제어) + 합계(수식어) + 금액(분류어)\n검토중\n2023-01-17\n\n\n\n* VC10: Variable Character의 약자로 가변 길이 문자열을 나타냄 (최대 10 자리)  \n* N15: N은 숫자(Numeric) 데이터 타입, 최대 15자리의 숫자  \n* D: Date (날짜)  \n* T: Time (시간)  \n* B: Boolean (참/거짓)  \n\n칼럼 설명\n\n용어ID: 각 용어의 고유 식별자\n용어논리명: 업무에서 사용되는 한글 용어명 (최대 30자, 권장 20자 이내)\n용어물리명: 데이터베이스 등에서 사용되는 영문 약어명 (최대 28자, 권장 20자 이내)\n도메인: 해당 용어의 데이터 타입과 제약조건\n정의: 용어에 대한 명확한 설명\n관련업무영역: 해당 용어가 주로 사용되는 업무 분야\n사용예시: 실제 데이터 예시\n표준단어구성: 용어를 구성하는 표준 단어들과 각 단어의 역할 (수식어, 주제어, 분류어)\n승인상태: 용어의 현재 승인 상태 (예: 승인, 검토중, 폐기 등)\n등록일: 용어가 사전에 처음 등록된 날짜\n최종수정일: 용어 정보가 마지막으로 수정된 날짜"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_2.data_word_domain.html",
    "href": "docs/blog/posts/Governance/5_2.data_word_domain.html",
    "title": "Data Governance Study - Data Domain Standardization",
    "section": "",
    "text": "비즈니스적으로 의미 있고 데이터의 성격을 분류한 것으로 동일한 형식을 가진 집합을 도메인이라 하며, 하나의 용어는 하나의 도메인만 지정한다.\n동일한 형식을 가진 데이터에 대해서 같은 도메인을 적용함으로써 속성의 의미 및 데이터의 범위를 명확히 할 수 있고 컬럼에 대한 일관적인 관리가 가능하다\n\n데이터의 형식, 길이, 허용 가능한 값의 범위 등을 명시\n\n표준 단어 조합의 마지막에 위치하는 분류단어(도메인성) 단어가 도메인의 후보가 된다.\n예시\n\n표준 단어 (주제어): 장기\n표준 단어 (수식어): 대여\n표준 단어 (분류단어): 금액\n표준 용어 : 장기대여금액\n데이터 모델 (속성명): 장기대여금액 with datatype = NUMBER(10)\n도메인: 금액N10\n\n\n\n\n\n동일한 형식을 가진 데이터에 대해서 같은 도메인을 적용함으로써 속성의 의미 및 유효한 데이터범위를 명확히 할 수 있고, 칼럼에 대한 일관적인 관리가 가능하다\n동일한 도메인을 사용하는 칼럼의 속성 성격을 변경하고자 할 때 도메인만을 변경함으로써 데이터 타입 및 길이를 동시에 부여하여 변경할 수 있다\n\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nDomain Group\n\n\n\na1\n\nDomain\n\n\n\na0--a1\n\n\n1:N\nN\n1\n\n\n\na2\n\nInfo Type\n\n\n\na1--a2\n\n\n1:N\nN\n1\n\n\n\n\n\n\n\n\n\n도메인그룹 (Domain Group) : 도메인 그룹은 성격이 유사한 도메인들을 그룹화 해서 관리하는 관리 단위이며, 예를 들면 금액, 날짜, 내용, 율 등의 도메인 그룹이 존재하며, 금액 도메인 그룹의 하위에는 금액, 외화금액,세액 등의 도메인이 존재 한다\n도메인 (Domain): 도메인은 데이터 값의 범위를 컬럼(속성)의 특성에 따라 분류한 것이므로 업무적으로 또는 비지니스적으로 의미가 있는 도메인명을 부여해야 하며, 표준단어를 사용하여 명명한다.\n인포타입 (Info Type): 인포타입은 Information Type의 약어로, 해당 도메인에서 사용할 수 있는 데이터 타입과 길이가 결합된 형태로 속성이 가질 수 있는 데이터 타입과 길이를 나타낸다\nDBMS별 데이터 타입 (Data Type): 표준 인포타입을 정의한 뒤에는 각 DBMS의 특성에 적합한 데이터 타입을 정의한다. DBMS는 다양한 제약 사항이 존재하기 때문에 데이터베이스를 설계할 DBMS의 특성도 반영해야 한다. 따라서 DBMS별 인포타입을 정의하고 이는 전사 표준 인포타입과 매핑 정보를 유지한다.\n\n\n\n\n\n예를 들어, 회사의 전사 도메인 그룹은 9개 도메인 그룹으로 관리한다고 하면 도메인은 다음과 같이 관리 될 수 있다.\n[도메인 그룹 – 도메인 – 인포타입 – DBMS별 데이터 타입] 예시\n\n\n\n\n\n\n\n\n\n\n\n\n도메인그룹\n도메인\n인포타입\nDBMS별 데이터 타입\n\n예시\n\n\n\n\n\n\n\n데이터 타입\n길이\n\n\n\n날짜\n일자\n일자VC8\nVARCHAR2\n8\n대손처리 일자\n\n\n날짜\n일시\n일시VC16\nVARCHAR2\n16\n대여시작 일시\n\n\n명\n명\n명VC30\nVARCHAR2\n30\n송금정비업체 명\n\n\n명\n성명\n성명VC10\nVARCHAR2\n10\n담당자 성명\n\n\n내용\n내용\n내용VC200\nVARCHAR2\n200\n수리요청 내용\n\n\n수\n수\n수N9\nNUMBER\n4\n에어백보유 수\n\n\n수\n건수\n건수N4\nNUMBER\n4\n신청 건수\n\n\n수\n수량\n수량N4\nNUMBER\n4\n신품출고 수량\n\n\n율\n이율\n이율N5,3\nNUMBER\n5,3\n연체 이율\n\n\n율\n금리\n금리N5,2\nNUMBER\n5,2\n적용 금리\n\n\n금액\n금액\n금액N10\nNUMBER\n10\n정산승인 금액\n\n\n금액\n잔액\n잔액N10\nNUMBER\n10\n매출 잔액\n\n\n번호\n고객번호\n법인고객번호\nVARCHAR2\n9\n법인고객번호\n\n\n번호\n법인등록번호\n법인등록번호\nVARCHAR2\n13\n협력업체 법인등록번호\n\n\n코드\n구분코드\n가족관계유형코드\nVARCHAR2\n5\n지점 시도구분코드\n\n\n분류\n여부\n여부VC1\nVARCHAR2\n1\n신규사업MT 여부\n\n\n\n\nNUMBER(5,3)은 다음을 의미\n\n총 5자리의 숫자를 저장\n그 중 3자리는 소수점 이하 숫자\n결과적으로 소수점 앞에는 2자리의 숫자만 올 수 있다.\n예시,\n\n12.345 (유효)\n1.234 (유효)\n0.123 (유효)\n123.45 (무효 - 소수점 이상 자릿수 초과)\n1.2345 (무효 - 소수점 이하 자릿수 초과)\n\n\n\n\n\n\n\n표준 도메인 구성 요소\n\n범위형 도메인: 속성(컬럼)에 허용되는 데이터 값을 데이터의 유형과 길이로 범위를 제한\n\n열거형 도메인: 속성(컬럼)에 허용되는 데이터 값을 정의된 범위 내에서 구체적으로 열거 또는 목록화하여 범위를 제한\n예시\n\n\n\n\n\n\n\n\n\n\n도메인 유형\n도메인그룹\n정의\n도메인 예시\n\n\n\n\n범위형 도메인\n날짜\n• 특정 사건이 일어난 시점 또는 시점과 시점간의 정해진 기간을 표현하기 위한 도메인\n일자, 일시, 년도, 년월, 월, 일, 시각, 시분, 분기, 반기 등\n\n\n범위형 도메인\n명칭\n• 문자 형식으로 객체에 대한 식별을 표현하기 위한 도메인\n명, 성명, 영문성명, 주소, 우편번호주소, 상세주소, 이메일주소 등\n\n\n범위형 도메인\n내용\n• 서술 형식의 상세 내용을 자유 형식의 텍스트로 표현하기 위한 도메인\n내용\n\n\n범위형 도메인\n수량\n• 객체의 개수나 양을 수로써 표현하기 위한 도메인• 일반적인 측량 단위도 포함됨\n수, 일수, 개월수, 매수, 좌수, 건수, 량, 평점, 연령, 면적, 평형 등\n\n\n범위형 도메인\n율\n• 비율을 수로 표현하기 위한 도메인\n율, 이율, 이자율, 지분율, 할인율 등\n\n\n범위형 도메인\n금액\n• 화폐의 가치를 수로 표현하기 위한 도메인\n금액, 잔액, 차액, 보증금, 수수료, 할인료, 보험료 등\n\n\n범위형 도메인\n번호\n• 각 자리 별 특정 의미를 가지거나 체계를 가지고 관리되어야 하는 속성을 정의하기 위한 도메인• 용어 별 고유의 번호 도메인을 부여\n전사고객번호, 계좌번호, 직원번호, 단말번호, 거래번호, 법인등록번호, 일련번호 등\n\n\n열거형 도메인\n코드\n• 코드화 하여 관리되는 속성을 정의하기 위한 도메인\n그룹내기관코드, 거래상태코드 등\n\n\n열거형 도메인\n분류\n• 상반된 상태의 값을 갖는 속성을 정의하기 위한 도메인\n여부, 유무\n\n\n\n\n\n\n\n\n\n\n[날짜] 도메인그룹은 특정 사건이 일어난 시점 또는 시점과 시점간의 정해진 기간을 표현하기 위한 도메인들을 그룹화하여 관리한다.\n\n\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n데이터타입\n길이\n유효값 범위\n예시\n\n\n\n\n일\nDD형태의 데이터 값을 갖는 도메인\nVARCHAR2\n2\n01~31\n계약시작 일\n\n\n시각\nHHMISS 형태의 시각을 나타내는 도메인\nVARCHAR2\n6\n000000~235959\n차량사용시작 시각\n\n\n일자\nYYYYMMDD형태의 데이터 값을 갖는 도메인\nVARCHAR2\n8\n00010101 ~ 99991231\n계약해지 일자\n\n\n생년월일\nYYMMDD형태의 데이터 값을 갖는 도메인\nVARCHAR2\n6\n000101 ~ 991231\n회원 생년월일\n\n\n일시\nYYYYMMDDHH24MISS형태의 데이터 값을 갖는 도메인\nVARCHAR2\n16\n\n가격정책등록 일시\n\n\n타임스템프\nYYYYMMDDHH24MISSFF3형태의 데이터 값을 갖는 도메인\nTIMESTAMP\n20\n\n\n\n\n년도\nYYYY형태의 데이터 값을 갖는 도메인\nVARCHAR2\n4\n0001 ~ 9999\n기준 년도\n\n\n년월\nYYYYMM형태의 데이터 값을 갖는 도메인\nVARCHAR2\n6\n\n지불 년월\n\n\n월\nMM형태의 데이터 값을 갖는 도메인\nVARCHAR2\n2\n\n청구 월\n\n\n\n\n년월일 형식은 ‘YYYYMMDD’ 로 통일하며, 도메인명은 ‘일자’ 로 정의하여 사용한다\n\n(예시) 계약해지 일자 : 20160420\n\n‘YYYY’ 형식의 도메인명은 ‘년도’ 로 정의하여 사용한다\n\n(예시) 출생 년도 : 1966, 기준 년도 : 2016\n\n‘YYYYMM’ 형식의 도메인명은 ‘년월’ 로 정의하여 사용한다\n\n(예시) 포인트적립 년월 : 201604\n\n‘MM’ 형식의 도메인명은 ‘월’ 로 정의하여 사용한다\n\n(예시) 결산 월 : 04\n\n‘MMDD’ 형식의 도메인명은 ‘월일’ 로 정의하여 사용한다\n\n(예시) 기혼기념 월일 : 1010\n\n‘DD’ 형식의 도메인명은 ‘일’ 로 정의하여 사용한다\n\n(예시) 자동이체지정 일 : 25\n\n년월일+시분초 형식의 도메인명은 ‘일시’ 로 정의하여 사용한다. ‘일시’ 도메인의 DataType은 Date 와 Variable Character의 두 가지를 지원한다\n시스템 로그일시(YYYYMMDDTTMMSSFF3)를 표시하기 위해서는 타임스탬프 도메인을 사용한다. DataType은 Timestamp 형태이다\n\n\n\n\n\n[명] 도메인그룹은 객체에 대한 식별을 표현하기 위한 도메인들을 그룹화하여 관리한다.\n사람을 제외한 포함한 모든 명칭은 ‘명’ 을 도메인으로 정의하여 사용하며, 사람은 ’성명’을 사용한다.\n‘명’ 도메인을 속성(컬럼)에 사용할 경우 도메인 앞에 ‘한글’,’영문’,‘한자’,‘약어’ 등과 같은 수식어가 생략된 경우는 ‘한글’+’명을 ’명’으로 갈음한다\n\n(예시) 종목 명(한글), 종목 영문 명(영문)\n\n사람이 살고 있는 곳이나 기관, 회사 따위가 자리 잡고 있는 곳을 행정 구역으로 나타낸 이름은 ‘주소’ 를 도메인으로 정의하여 사용한다\n’명’은 단독으로 사용하지 않고 사전을 참조하여 복합어로 생성한다\n예시\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n명\n상품, 사물 등을 식별하기 위한 명칭에는 ‘명’ 도메인을 사용한다\n상품 명\n\n\n\n성명\n사람의 성명을 관리할 경우 ’성명’이라는 도메인을 사용한다\n고객 영문성명\n\n\n\n주소\n사람이 살고 있는 곳이나 기관, 회사 따위가 자리 잡고 있는 곳을 행정 구역으로 나타낸 이름은 ‘주소’ 도메인을 사용한다\n직장 주소\n\n\n\n이메일주소\n전자우편주소\n수신자 이메일주소\n\n\n\nID\n시스템 오브젝트 등을 식별하기 위해 사용되는 도메인으로서 체계 없이 순차적으로 체번하여 사용하는 일련번호와는 구별하여 사용한다\n스마트빌 ID\n\n\n\n\n\n\n\n\n“내용” 단일 도메인으로 정의 한다.\n“명세”, “설명”, ”비고”, “적요”, “내역”, “의견”, “사유”, “사항” 등 유사 도메인은 별도로 정의하지 않고 “내용” 도메인으로 통합관리 한다.\n\n(예시) 평가자 의견 (X) → 평가자 의견 내용 (O)\n\n’값’은 단일단어로 허용하지 않고, 사전을 참조하여 복합어로 생성한다.\n예시\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n내용\n사실이나 사물에 대해 전하고자 하는 정보를 형식 없이 서술형으로 저장하는 경우 사용된다.\n사고 내용\n\n\n\n값\n평가값, 항목값 등 값을 의미하는 용어에 대해 값 도메인을 사용한다.\n항목 값, 입력 값\n\n\n\n\n\n\n\n\n[수] 도메인그룹은 객체의 개수나 양을 수로써 표현하기 위한 도메인들을 그룹화하여 관리한다.\n[수 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n수\n셀 수 있는 사물의 크기를 나타내는 값(복합어로 생성)\n종업원 수\n\n\n\n일수\n일을 기준으로 헤아리는 수.\n연체 일수\n\n\n\n개월수\n월을 기준으로 헤아리는 수\n견적 개월수\n\n\n\n년수\n년을 기준으로 헤아리는 수\n근무 년수\n\n\n\n매수\n종이나 유리 따위의 장으로 셀 수 있는 물건의 수효\n기본 매수\n\n\n\n개수\n한 개씩 낱으로 셀 수 있는 물건의 수효\n보유 개수\n\n\n\n건수\n사물이나 사건의 가짓수\n조회 건수\n\n\n\n횟수\n돌아오는 차례의 수효\n기존인출 횟수\n\n\n\n점수\n성적을 나타내는 숫자\n평가대상 점수\n\n\n\n연령\n나이, 사람이 세상에 나서 현재 또는 기준이 되는 때까지 살아 온 햇수\n보험 연령\n\n\n\n수량\n수와 량이 혼재된 수량을 자연수로 표현한 수\n수량\n\n\n\n\n\n금액을 제외한 정보의 수치 및 합계 등을 정의하는 경우에 사용한다\n\n(예시) 고객 수, 연체 일수, 거래 량\n\n되풀이되는 일이나 차례의 수효를 나타내는 경우는 ’횟수’를 도메인으로 정의하여 사용한다.\n\n(예시) 지로자동이체 횟수\n\n나이와 관련된 용어는 ‘연령’ 을 도메인으로 정의하여 사용한다\n\n(예시) 보험 연령\n\n기간을 나타내는 용어의 경우 ‘기간’을 사용하지 않고, ‘년수’, ‘개월수’, ‘일수’ 등으로 구체 적으로 정의한다\n\n(예시) 대출 년수, 연장 개월수, 연체 일수\n\n\n\n\n\n\n[율] 도메인그룹은 둘 이상의 수를 비교하여 그 중 하나의 수를 기준으로 하여 나타낸 다른 수의 비교 값을 표현하기 위한 도메인들을 그룹화하여 관리한다.\n‘율’ 또는 ’률＇은 단일단어로 사용하지 않고 사전을 참조하여 복합어로 생성한다\n확률, 비율 등 ‘%’ 로 관리되는 속성에 대해 ‘율/비율’ 을 도메인으로 정의하여 사용한다\n\n(예시) 연체 율, 담보 비율, 적용 비율\n\n[율 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n율\n비율의 뜻을 나타내는 말 “~율”은 모음으로 끝나거나 ‘ㄴ’ 받침을 가진 일부 명사 뒤에 붙임(복합어로 생성)\n할인 율\n\n\n\n이율\n원금에 대한 이자의 비율, 즉 이자 산출에 기초가 되는 비율\n연체 이율\n\n\n\n세율\n과세 표준에 의하여 세금을 계산하여 매기는 법정률\n\n\n\n\n요율\n요금의 정도나 비율\n보증 요율\n\n\n\n금리\n자금의 사용료로 대외적으로 공시되는 기준의 의미로 사용\n대출 금리\n\n\n\n환율\n외국환 시세\n기준 환율\n\n\n\n비율\n둘 이상의 수를 비교하여 나타낼 때 그 중 한 개의 수를 기준으로 하여 나타낸 다른 수의 비교 값\n사고MT 비율\n\n\n\n\n\n\n\n\n[금액] 도메인그룹은 돈의 액수나 화폐 가치를 표현하기 위한 도메인들을 그룹화하여 관리한다\n금액을 의미하는 도메인은 중복해서 사용하는 것을 피한다\n\n(예시) 물품 + 원가(도메인) + 금액(도메인) → 물품 + 원가(도메인)\n\n기본적으로 ‘금액’ 도메인으로 표현될 수 있는 속성에 대해서는 ‘금액’ 도메인의 사용을 권장한다. 하지만 관용적으로 ‘금’ 도메인이 사용되는 표현에는 별도의 도메인으로 분류하여 사용한다\n합산금액을 의미하는 총계, 합계, 누계 등의 수식어는 반드시 금액 앞에 붙인다\n\n(예시) 감가상각누계금액, 자산총계금액\n\n부득이하게 ’금＇을 사용해야 하는 경우 복합어 생성한다\n[금액 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n\n도메인\n세부 도메인\n설명\n예시\n비고\n\n\n\n\n요금\n료\n대여료, 렌탈료, 리스료 등을 나타내는 금액 단위(복합어로 생성)\n수수 료, 과태 료\n\n\n\n금액\n금액\n돈의 액수\n고객청구 금액\n\n\n\n금액\n잔액\n남은 금액\n연체이자 잔액\n\n\n\n세\n세\n조세의 액수(복합어로 생성)\n부가 세\n\n\n\n가격\n단가\n물건 한 단위(單位)의 가격\n계약 단가\n\n\n\n가격\n원가\n상품의 제조 판매 배급 따위에 든 재화와 용역을 단위에 따라 계산한 가격\n물품 원가\n\n\n\n\n\n\n\n\n[번호] 도메인그룹은 일정한 체계를 가지거나 특정 자리에 존재하는 의미를 표현하기 위한 도메인들을 그룹화하여 관리한다.\n번호도메인의 공통 도메인은 생성하지 않는다.\n‘번호’ 도메인은 일정한 체계를 가지거나 특정 자리의 의미가 존재하는 속성을 정의할 때 사용하는 것을 원칙으로 한다.\n번호 자체에 특별한 의미를 가진 경우 해당 번호를 번호도메인으로 정의하여 사용한다\n\n(예시) 주민등록번호, 사업자등록번호, 전화번호, 법인등록번호 등\n\n순차적으로 채번되는 번호는 ‘일련번호’ 도메인으로 정의하여 사용한다.\n번호도메인의 공통인 도메인은 생성하지 않는다.\n\n(예시) 번호N10 (Number(10))과 같이 분류어가 번호인 용어의 도메인이 공통적으로 쓰는 도메인 생성 금지.\n\n[번호 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n전화번호\n가입된 전화마다 매겨져 있는 일정한 번호\n계약담당 전화번호\n\n\n\n우편번호\n우편물을 쉽게 분류하기 위하여 정보 통신부에서 각 지역마다 매긴 번호\n계약자 우편번호\n\n\n\n주민등록번호\n주민등록을 할 때에, 국가에서 국민에게 부여하는 고유 번호\n계약자 주민등록번호\n\n\n\n사업자등록번호\n세무에서, 신규로 개업하는 사업자에게 부여하는 사업체의 고유번호이다\n공급자 사업자등록번호\n\n\n\n법인등록번호\n사무소의 소재지에서 설립등기(設立登記)를 함으로써 성립하는데 이때 부여된 일련번호이다\n~ 법인등록번호\n\n\n\n계좌번호\n금융 기관에 예금하려고 설정한 개인명이나 법인명의 계좌에 부여된 번호\n입금 계좌번호\n\n\n\n휴대전화번호\n지니고 다니면서 걸고 받을 수 있는 소형 무선 전화기 번호\n\n\n\n\n비밀번호\n본인임을 확인하기 위해 설정한 암호\n\n\n\n\n신용카드번호\n신용카드식별번호\n~ 신용카드번호\n\n\n\n직원번호\n회사 직원 식별번호\n~ 사원번호\n\n\n\n여권번호\n외국을 여행하는 사람의 신분이나 국적을 증명하고 상대국에 그 보호를 의뢰하는 문서번호.\n고객 여권번호\n\n\n\n외국인등록번호\n외국인등록번호\n외국인등록번호\n\n\n\n일련번호\n일률적으로 연속되어 있는 번호\n구성품 일련번호\n\n\n\n\n\n\n\n\n코드는 다른 도메인들과 달리 특정 도메인 값(즉, 코드값)과 이 값에 대한 의미(즉, 코드값명)를 표현하기 위해 코드도메인을 그룹화하여 표준코드로 분류하여 별도로 관리한다\n코드값을 가지는 속성은 반드시 ‘코드’ 를 도메인으로 정의하여 사용한다\n속성명은 해당 속성이 사용하는 코드도메인명과 일치시키는 것을 원칙으로 하나, 속성명에 수식어를 붙여서 사용할 수 있다\n\n(예시) 코드도메인 ‘거래코드’ : 속성명 → 거래코드(O), 수신거래코드(O)\n\n코드의 의미가 ‘여부’ 또는 ‘유무’ 인 경우 ‘코드’를 도메인으로 사용할 수 없고, ‘여부/유무’ 를 도메인으로 정의하여 사용한다\n코드도메인은 ‘수식어 + 코드유형수식어 + 코드’ 로 명명한다\n[코드 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n도메인\n설명\n비고\n\n\n\n\n가상계좌은행코드\n[가상계좌은행코드] 코드는 회사 전사에서 사용하는 가상계좌 은행코드\n\n\n\n제품카테고리코드\n[제품카테고리코드] 코드는 제품의 카테코리에 대하여 조회할 수 있도록 구분하는 코드\n\n\n\n주차장코드\n[주차장구분코드] 코드는 자산 제고의 위치를 구분하는 코드\n\n\n\n수리항목코드\n[수리항목코드] 수리항목 목록을 구분하는 코드\n\n\n\n\n\n\n\n\n\n“여부” 도메인의 인스턴스는 반드시 ‘Y’ 또는 ‘N’ 만 허용되며, NULL, N/A, SAPCE 등의 값은 허용하지 않는다.\n“유무” 도메인의 인스턴스는 반드시 ‘Y’ 또는 ‘N’ 만 허용되며, NULL, N/A, SAPCE 등의 값은 허용하지 않는다.\n“여부/유무” 도메인명은 ‘Y/N’ 이외의 값을 허용하지 않으므로, 기타 값을 사용해야 하는 경우는 코드도메인으로 분류하여 사용하고, 특히 미 정의된 값은 속성의 인스턴스로 사용할 수 없으며, ‘미정의’ 값을 표현하기 위해서는 ’*’ (미정의)와 같이 반드시 대체값을 정의하여 인스턴스로 사용한다. (Not Null)\n단, 대외 인터페이스 테이블의 경우 업무 특성에 따라 Null을 에외로 허용할 수 있다\n[분류 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n여부\n특정 사실이나 행위의 ’그러함/그러하지 아니함’을 의미\n반출 여부\n\n\n\n유무\n존재나 소유의 ’있음/없음’을 의미\n보증금 유무\n\n\n\n\n\n이 두 도메인을 명확히 구분하여 사용하면 데이터의 의미를 더 정확하게 전달할 수 있다.\n\n예를 들어, ’계약 여부’는 계약이 체결되었는지 아닌지를 나타내는 반면, ’계약서 유무’는 물리적인 계약서 문서가 존재하는지 않는지를 나타낼 수 있다.\n\n\n\n\n\n\n\n도메인 등록 기준\n\n\n\n\n\n\n\n\n\n순번\n원칙\n예시\n\n\n\n\n1\n• 속성 데이터가 규칙을 가지는 경우 도메인 명으로 지정한다.• 속성값의 자릿수가 항상 일정해야 하는 경우• 특정 자릿수의 데이터가 의미를 가지는 경우• 속성값 데이터 내의 규칙이 존재하는 경우\n• 고객번호는 20자리• 사업자등록번호, 여권번호, 법인등록번호\n\n\n2\n속성 데이터 값의 유효값이나 유효 범위의 제한이 있는 경우 도메인으로 지정한다.\n• 분기 – 1,2,3,4 만 유효함• 일(Day) – 1~31만 유효함\n\n\n3\n속성값의 성격을 식별하고자 하는 경우 도메인 명으로 지정한다\n지점구분코드\n\n\n4\n속성명 명명 시에 합성어 등이 관용적으로 사용되어 분리하여 사용하기 힘든 경우 도메인 명으로 지정한다.\n잔액, 특소세\n\n\n5\n코드값 체계를 가지는 모든 코드는 도메인 명으로 지정한다\n은행코드, 부품코드\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n도메인 그룹\n도메인\n인포타입\n데이터 타입\n길이\n설명\n예시\n\n\n\n\n날짜\n일자\n일자VC8\nVARCHAR2\n8\nYYYYMMDD 형식의 날짜\n20230501\n\n\n날짜\n일시\n일시VC14\nVARCHAR2\n14\nYYYYMMDDHHMMSS 형식의 날짜와 시간\n20230501143000\n\n\n명칭\n성명\n성명VC50\nVARCHAR2\n50\n개인의 이름\n홍길동\n\n\n명칭\n상품명\n명VC100\nVARCHAR2\n100\n상품의 이름\n스마트폰 갤럭시 S23\n\n\n내용\n내용\n내용VC4000\nVARCHAR2\n4000\n자유 형식의 텍스트\n이 제품은 최신 기술을 적용한…\n\n\n수량\n수량\n수량N10\nNUMBER\n10\n물품의 개수\n1000\n\n\n율\n이율\n이율N5_2\nNUMBER\n5,2\n비율을 나타내는 수치 (소수점 2자리)\n3.75\n\n\n금액\n금액\n금액N15\nNUMBER\n15\n화폐 금액\n10000000\n\n\n번호\n전화번호\n전화번호VC20\nVARCHAR2\n20\n전화번호 형식\n010-1234-5678\n\n\n코드\n상품코드\n코드VC10\nVARCHAR2\n10\n상품을 구분하는 고유 코드\nPRD0001234\n\n\n분류\n여부\n여부VC1\nVARCHAR2\n1\nY/N으로 표현되는 여부\nY"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_2.data_word_domain.html#데이터-표준-용어-사전이란",
    "href": "docs/blog/posts/Governance/5_2.data_word_domain.html#데이터-표준-용어-사전이란",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "조직에서 사용되는 모든 데이터 관련 용어의 공식적인 정의를 제공하는 중앙 집중식 저장소이다.\n업무상 사용되는 용어를 정보시스템에서 사용하는 기술적인 용어로 전환하여 이것을 일관되게 사용 할 수 있도록 정의한 것을 지칭한다.\n각 용어의 의미, 사용 맥락, 형식 등을 명확히 기술한다.\n표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다.\n모델링에서는 속성명으로 사용되며 전사관점에서 유일하다.\n예시\n\n표준 단어 (주제어): 장기\n표준 단어 (수식어): 대여\n표준 단어 (분류단어): 금액\n표준 용어 : 장기대여금액\n데이터 모델 (속성명): 장기대여금액 with datatype = NUMBER(10)\n도메인: 금액N10\n\n\n\n\n\n데이터의 일관성을 확보하고 품질을 향상\n조직 전체에서 데이터 용어를 일관되게 사용할 수 있도록 한다.\n\n즉, 전사적으로 표준화된 용어를 사용함으로써 데이터 모델 구성 요소의 명칭을 부여하는데 일관성을 유지할 수 있다\n\n시스템 간 데이터 통합과 매핑을 용이하게 한다.\n표준화된 명칭을 부여함으로써 데이터의 중복 정의 방지와 의미 전달의 명확성을 확보하여 의사 소통을 원활하게 한다\n\n\n\n\n\n용어 논리명 (보통 한글)\n\n논리명은 데이터의 의미를 나타내는 명칭으로 표준단어 명으로 구성된다.\n용어 논리명의 최대길이는 30자로 한다. 단 20자 이내로 작성할 것을 권장한다\n표준속성 구성 시 5개 단어를 넘지 않도록 하며, 구분자나 띄어쓰기 없이 한 단어로 붙인다 (월_계좌잔액(X) → 월계좌잔액)\n\n용어 물리명 (보통 영어)\n\n물리명은 단어의 영문 약어 조합으로 이루어지며 단어의 영문약어들끼리 연결 할 때는 언더바(_)를 사용한다.\n용어의 물리명은 최종적으로 데이터베이스를 구성 할 때 테이블의 컬럼명으로 사용한다.\n용어 물리명의 최대길이는 28자로 한다. 단 20자 이내로 작성할 것을 권장한다. (단, 용어 논리 및 물리명의 길이는 DBMS에 따라 달라질 수 있음)\n\n정의\n\n용어가 업무적으로 사용되는 의미를 기술한 내용이다.\n\n도메인 정보\n\n특정 비즈니스 컨텍스트에서 데이터 값의 허용 범위를 정의\n즉, 데이터 값의 범위를 한정하는 데이터 타입과 길이,소수점을 의미한다.\n데이터 타입을 포함하며, 추가적인 제약조건이나 비즈니스 규칙을 포함할 수 있습니다.\n비즈니스 로직과 데이터 무결성 규칙을 포함할 수 있다.\n예시\n\n나이: INTEGER, 0-150 사이의 값만 허용\n이메일: VARCHAR(100), 이메일 형식 검증 규칙 포함\n급여: DECIMAL(10,2), 0보다 큰 값만 허용\n\n\n데이터 타입 및 형식\n\n데이터의 기본적인 저장 형태와 구조를 나타낸다.\n일반적이고 기본적인 데이터 유형을 지정\n보통 도메인 정보가 데이터 타입 및 형식 정보를 모두 포함한다.\n예시\n\n문자열(VARCHAR, CHAR)\n숫자(INTEGER, DECIMAL)\n날짜/시간(DATE, TIMESTAMP)\n불리언(BOOLEAN)\n\n\n코드\n\n입력할 수 있는 유효 값 데이터 값을 정의할 수 있다면 용어는 코드와 매핑 한다.\n\n관련 업무 영역\n\n해당 용어가 주로 사용되는 비즈니스 또는 조직 내의 특정 부서나 기능 영역을 나타낸다.\n목적: 용어의 사용 맥락을 제공하고, 해당 용어가 어떤 비즈니스 프로세스나 기능과 관련있는지 이해하는 데 도움을 준다.\n예시\n\n“고객ID” - 관련 업무 영역: 고객 관리, 마케팅, 영업\n“재고수량” - 관련 업무 영역: 재고 관리, 물류, 구매\n“급여액” - 관련 업무 영역: 인사, 재무\n\n\n사용 예시 등\n\n“생년월일” - 사용 예시: “1990-05-15”\n“주문상태” - 사용 예시: “접수”, “처리중”, “배송완료”\n“계좌잔액” - 사용 예시: “1,000,000원”\n\n용어 사전 예시\n\n네, 관련 업무 영역과 사용 예시 칼럼을 추가한 마크다운 테이블을 만들어 드리겠습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n용어 논리명\n용어 물리명\n도메인(인포타입)\n코드\n정의\n관련 업무 영역\n사용 예시\n\n\n\n\n계약종료일자\nCNTR_END_DATE\n일자VC8\n\n거래 계약의 종료일자이다. YYYYMMDD로 작성한다.\n계약관리, 고객관리\n“20231231”\n\n\n렌탈등록비공급가액\nRENT_RGST_SPAM\n금액N10\n\n렌탈 등록 공급 금액을 뜻한다. 숫자로 작성한다.\n렌탈관리, 재무\n“50000”\n\n\n관청은행코드\nGOV_BANK_CD\n은행코드\n은행코드\n해당 관할 은행의 코드이다.\n재무, 회계\n“004” (국민은행)\n\n\n\n이 테이블에서 ‘관련 업무 영역’과 ’사용 예시’ 칼럼을 추가하여 각 용어의 사용 맥락과 실제 적용 예를 보여주고 있습니다. 실제 환경에서는 이 정보들을 조직의 특성과 요구사항에 맞게 더 구체적으로 작성할 수 있습니다.\n\n\n\n\n표준용어 작성 시 누구나 이해하기 쉽도록 간결하되 명확하고 모호함 없이 표현하도록 해야 하며, 다음과 같은 기본 원칙에 위배되지 않도록 한다.\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어는 관용적으로 사용하는 용어를 우선적으로 사용한다\n\n\n\n2\n• 표준용어를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n등록일자(X) → 장비자산등록일자(O)\n\n\n3\n• 용어 구성 시 단어는 반드시 표준 단어 사전에 등록된 단어를 사용하며, 단어 사전에 등록되어 있지 않은 경우에는 표준 담당자와 협의 후에 신규 단어로 등록하도록 한다.\n단어 부재 시 신규 요청 요망\n\n\n4\n• 일반적인 의미와 전혀 다르게 사용된 용어는 적절한 다른 용어로 대체하고, 유사한 의미의 용어가 중복 개발되어 혼재되지 않도록 하며 새로운 용어의 개발은 자제한다.\n반환일자(X) 반납일자(O)\n\n\n5\n• 표준용어로 등록된 명칭의 전사적으로 사용되어야 함으로 명 선정 시 신중하게 고려하여야 한다\n\n\n\n6\n• 표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다\n렌탈 + 가능(X) 렌탈 + 가능 + 금액(O)\n\n\n7\n• 표준용어 명명 규칙 - 표준용어는 누구나 이해하기 쉽도록 구체적이고 명확하고 간결하게 정의한다 - 복합어를 단일어 보다 우선 적용한다 - 복합어가 중첩되어 사용될 경우 도메인이 포함된 복합어를 우선 적용한다 - 의미 있는 숫자를 포함한 용어의 경우에는 숫자를 포함하여 하나의 표준단어를 등록한 후 그 표준 단어를 사용하여 용어를 정의한다 - 용어의 의미를 모호하게 하는 의미 없는 일련번호를 부여하기 위한 숫자는 사용하지 않으며 용어에 수식어를 사용하여 용어가 유일하게 식별되도록 정의하는 것을 원칙으로 한다\n\n\n\n\n\n\n\n\n표준 용어 = 수식어 (표준 단어) + 주제어 (표준 단어) + \\(\\dots\\) + 수식어 (표준 단어) + 주제어 (표준 단어) + 분류어\n수식어 예시\n\n\n\n수식어\n예시\n\n\n\n\n기간 수식어\n최초, 최종, 과거, 최근 등\n\n\n기간/시간\n6개월, 당월, 월말, 년초, 년말 등\n\n\n장소\n국외, 국내, 지점, 본점 등\n\n\n특징\n순수, 사용, 처리, 거래 등\n\n\n계산\n합계: 한데 모아서 합산함누계: 계속하여 덧붙여 합산함\n\n\n\n표준 용어 = 표준 용어 + 표준 도메인\n원칙 항목 및 설명\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 용어는 수식과 수식의 대상이 되는 단어가 여러 개 존재할 수 있고 도메인을 수식하는 분류어는 용어의 끝에 위치한다\n\n\n\n2\n• 수식어는 기간, 장소, 특징, 계산의 성격을 가지는 단어가 순서대로 위치하고 기간을 수식하는 단어는 맨 앞에 위치한다\n\n\n\n3\n• 수식어 중 계산의 성격을 가진 단어는 합계, 누계, 총합계, 총누계, 소계 중 하나를 선택하여 사용해야 한다. (도메인 그룹이 금액과 수량과 같이 계산이 필요한 도메인을 수식함)\n\n\n\n4\n• 대상이 되는 단어가 여러 개일 때는 중 범위가 큰 것 순서대로 용어의 앞부분에 위치한다\n\n\n\n\n주의 사항\n\n\n\n\n\n\n\n\nAS-IS 용어 (비권장)\nTO-BE 용어(권장)\n비고\n\n\n\n\n회사에게 하고 싶은 말 내용\n사용자건의사항\n서술형용어\n\n\n공임금액구분별금액\n공임구분별금액\n단어 반복\n\n\n법인번호\n?+법인등록번호\n주제어 누락 및 약어 사용\n\n\n상세순번\n?+상세일련번호\n주제어 누락\n\n\n수정자\n?+수정자+?\n주제어 및 분류어 누락\n\n\n스케줄추가(청구)정보 수정여부\n스케줄추가정보수정여부\n특수문자사용\n\n\n요청일\n장기렌트입고요청일자\n주제어누락\n\n\n운전자\n렌트카운전자성명\n주제어 및 분류어 누락\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어에 적절한 명칭을 부여하고 전사적 공유가 가능하기 위해서는 구체적인 정의가 반드시 있어야 한다- 표준용어정의는 활동을 나타내는 단어와 그 주체가 되는 단어와 무엇을 어떻게 했는지의 내용이 담겨 있어야 한다.\n대여차량입고일자: 렌트차량이 반환되어 차고지에 입고 된 일자이다.\n\n\n2\n• 표준용어의 명칭이 주는 의미가 불분명하면 좀더 상세하게 정의해야 한다.\n유효기간(X) → 회원멤버쉽유효기간(O)\n\n\n3\n• 동일한 의미의 용어가 중복되지 않도록 표준용어 구성 순서를 고려해 생성한다\n현금서비스최종3개월 총합계금액(X) → 최종3개월 현금서비스 총합계금액(O)\n\n\n4\n• 표준용어의 한글명 또는 영문명 길이 제한으로 인해 축약된 형태로 사용해야 하는 경우 용어를 구성하는 단어 중 연관도와 활용도가 높은 단어들을 합하여 복합어를 정의해야 한다\n고객차량등급^코드 → 고객차량등급코드\n\n\n5\n• 표준용어 중 ‘여부’ 도메인으로 끝나는 것들은 대표로 하나의 코드명과 코드 값(Y, N)을 등록하여 모든 공통적으로 사용하도록 한다.\n코드일련번호 : B011\n\n\n6\n• ‘계약번호’ 자체로는 그 의미가 불분명하여 대차계약번호, 유지보수계약번호 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n7\n• 일반적으로 ‘렌탈’은 ’자동차대여’를 말하는 것이나 ’렌탈’ 자체로는 그 의미가 불분명하여 단기렌탈, 장기렌탈 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n8\n• 신청사용자아이디 → 사용자아이디 에서 ’신청’의 단어처럼 생략해도 의미가 통하는 경우에는 표준용어는 축약된 형태로 정의한다"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_2.data_word_domain.html#제작-과정",
    "href": "docs/blog/posts/Governance/5_2.data_word_domain.html#제작-과정",
    "title": "Data Governance Study - Data Domain Standardization",
    "section": "",
    "text": "각 업무 영역에서 필요한 용어들을 수집\n기존 시스템, 문서, 보고서 등에서 사용 중인 비표준 용어들을 식별\n\n\n\n\n\n수집된 용어들의 의미와 사용 맥락을 분석\n유사하거나 중복된 용어들을 식별\n\n\n\n\n\n분석된 용어들을 기존의 표준 단어 사전과 매핑\n각 용어를 구성하는 단어들이 표준 단어 사전에 있는지 확인\n\n\n\n\n\n표준 단어들을 조합하여 새로운 표준 용어를 구성\n용어 구성 원칙(예: 수식어 순서, 도메인 위치 등)을 따른다.\n\n\n\n\n\n구성된 용어가 의미를 명확히 전달하는지 확인\n기존 표준 용어와 중복되지 않는지 검토\n전문가나 제 3의 부서와 cross check\n\n\n\n\n\n용어의 정의, 사용 예시, 관련 업무 영역 등 상세 정보를 작성\n용어와 관련된 도메인 정보를 지정\n\n\n\n\n\n구성된 용어에 대해 관련 부서와 데이터 관리자의 검토를 받는다.\n필요시 수정하고 최종 승인을 받는다.\n\n\n\n\n\n승인된 용어를 표준 용어 사전에 등록\n용어의 물리명(영문명)을 생성\n\n\n\n\n\n새로 등록된 표준 용어를 조직 내에 공지\n필요시 사용 방법에 대한 교육을 실시\n\n\n\n\n\n새로 등록된 표준 용어의 사용 현황을 모니터링\n사용자 피드백을 수집\n\n\n\n\n\n정기적 검토 및 피드백 수집\n\n분기별로 용어 사전 검토 일정을 수립\n사용자로부터 피드백을 수집하고, 사용 현황을 모니터링\n\n변경 관리 프로세스 운영\n\n용어 추가, 수정, 폐기를 위한 공식적인 변경 요청 프로세스를 구축\n변경 요청에 대한 영향 분석을 수행하고, 승인 절차를 거친다.\n\n업데이트 및 버전 관리\n\n승인된 변경사항을 용어 사전에 반영\n버전 관리를 통해 변경 이력을 추적하고, 주요 변경사항을 공지\n\n교육 및 홍보\n\n변경된 용어나 새로운 용어에 대한 교육을 실시\n내부 커뮤니케이션 채널을 통해 주요 업데이트 사항을 공유\n\n성과 측정 및 개선\n\n용어 표준화로 인한 데이터 품질 개선, 업무 효율성 증가 등의 성과를 측정\n측정 결과를 바탕으로 용어 관리 전략을 지속적으로 개선"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_2.data_word_domain.html#표준-단어-사전-예시",
    "href": "docs/blog/posts/Governance/5_2.data_word_domain.html#표준-단어-사전-예시",
    "title": "Data Governance Study - Data Domain Standardization",
    "section": "",
    "text": "예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n용어ID\n용어논리명\n용어물리명\n도메인\n정의\n관련업무영역\n사용예시\n표준단어구성\n승인상태\n등록일\n\n\n\n\nT001\n고객번호\nCUST_NO\n일련번호VC10\n고객을 유일하게 식별하는 번호\n고객관리, 영업\n“C0001234567”\n고객(주제어) + 번호(분류어)\n승인\n2023-01-15\n\n\nT002\n장기렌트계약시작일자\nLTRM_RENT_CNTR_STRT_DT\n일자VC8\n장기 렌트 계약이 시작되는 날짜\n계약관리, 렌트관리\n“20230107”\n장기(수식어) + 렌트(주제어) + 계약(주제어) + 시작(수식어) + 일자(분류어)\n승인\n2023-01-16\n\n\nT003\n월별렌탈등록비합계금액\nMTHLY_RENT_RGST_FEE_TOT_AMT\n금액N15\n한 달 동안의 렌탈 등록비 총액\n재무, 렌트관리\n“5000000”\n월별(수식어) + 렌탈(주제어) + 등록(주제어) + 비(주제어) + 합계(수식어) + 금액(분류어)\n검토중\n2023-01-17\n\n\n\n* VC10: Variable Character의 약자로 가변 길이 문자열을 나타냄 (최대 10 자리)  \n* N15: N은 숫자(Numeric) 데이터 타입, 최대 15자리의 숫자  \n* D: Date (날짜)  \n* T: Time (시간)  \n* B: Boolean (참/거짓)  \n\n칼럼 설명:\n\n\n용어ID: 각 용어의 고유 식별자\n용어논리명: 업무에서 사용되는 한글 용어명 (최대 30자, 권장 20자 이내)\n용어물리명: 데이터베이스 등에서 사용되는 영문 약어명 (최대 28자, 권장 20자 이내)\n도메인: 해당 용어의 데이터 타입과 제약조건\n정의: 용어에 대한 명확한 설명\n관련업무영역: 해당 용어가 주로 사용되는 업무 분야\n사용예시: 실제 데이터 예시\n표준단어구성: 용어를 구성하는 표준 단어들과 각 단어의 역할 (수식어, 주제어, 분류어)\n승인상태: 용어의 현재 승인 상태 (예: 승인, 검토중, 폐기 등)\n등록일: 용어가 사전에 처음 등록된 날짜\n최종수정일: 용어 정보가 마지막으로 수정된 날짜"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_2.data_word_domain.html#데이터-표준-도메인이란",
    "href": "docs/blog/posts/Governance/5_2.data_word_domain.html#데이터-표준-도메인이란",
    "title": "Data Governance Study - Data Domain Standardization",
    "section": "",
    "text": "비즈니스적으로 의미 있고 데이터의 성격을 분류한 것으로 동일한 형식을 가진 집합을 도메인이라 하며, 하나의 용어는 하나의 도메인만 지정한다.\n동일한 형식을 가진 데이터에 대해서 같은 도메인을 적용함으로써 속성의 의미 및 데이터의 범위를 명확히 할 수 있고 컬럼에 대한 일관적인 관리가 가능하다\n\n데이터의 형식, 길이, 허용 가능한 값의 범위 등을 명시\n\n표준 단어 조합의 마지막에 위치하는 분류단어(도메인성) 단어가 도메인의 후보가 된다.\n예시\n\n표준 단어 (주제어): 장기\n표준 단어 (수식어): 대여\n표준 단어 (분류단어): 금액\n표준 용어 : 장기대여금액\n데이터 모델 (속성명): 장기대여금액 with datatype = NUMBER(10)\n도메인: 금액N10\n\n\n\n\n\n동일한 형식을 가진 데이터에 대해서 같은 도메인을 적용함으로써 속성의 의미 및 유효한 데이터범위를 명확히 할 수 있고, 칼럼에 대한 일관적인 관리가 가능하다\n동일한 도메인을 사용하는 칼럼의 속성 성격을 변경하고자 할 때 도메인만을 변경함으로써 데이터 타입 및 길이를 동시에 부여하여 변경할 수 있다\n\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nDomain Group\n\n\n\na1\n\nDomain\n\n\n\na0--a1\n\n\n1:N\nN\n1\n\n\n\na2\n\nInfo Type\n\n\n\na1--a2\n\n\n1:N\nN\n1\n\n\n\n\n\n\n\n\n\n도메인그룹 (Domain Group) : 도메인 그룹은 성격이 유사한 도메인들을 그룹화 해서 관리하는 관리 단위이며, 예를 들면 금액, 날짜, 내용, 율 등의 도메인 그룹이 존재하며, 금액 도메인 그룹의 하위에는 금액, 외화금액,세액 등의 도메인이 존재 한다\n도메인 (Domain): 도메인은 데이터 값의 범위를 컬럼(속성)의 특성에 따라 분류한 것이므로 업무적으로 또는 비지니스적으로 의미가 있는 도메인명을 부여해야 하며, 표준단어를 사용하여 명명한다.\n인포타입 (Info Type): 인포타입은 Information Type의 약어로, 해당 도메인에서 사용할 수 있는 데이터 타입과 길이가 결합된 형태로 속성이 가질 수 있는 데이터 타입과 길이를 나타낸다\nDBMS별 데이터 타입 (Data Type): 표준 인포타입을 정의한 뒤에는 각 DBMS의 특성에 적합한 데이터 타입을 정의한다. DBMS는 다양한 제약 사항이 존재하기 때문에 데이터베이스를 설계할 DBMS의 특성도 반영해야 한다. 따라서 DBMS별 인포타입을 정의하고 이는 전사 표준 인포타입과 매핑 정보를 유지한다.\n\n\n\n\n\n예를 들어, 회사의 전사 도메인 그룹은 9개 도메인 그룹으로 관리한다고 하면 도메인은 다음과 같이 관리 될 수 있다.\n[도메인 그룹 – 도메인 – 인포타입 – DBMS별 데이터 타입] 예시\n\n\n\n\n\n\n\n\n\n\n\n\n도메인그룹\n도메인\n인포타입\nDBMS별 데이터 타입\n\n예시\n\n\n\n\n\n\n\n데이터 타입\n길이\n\n\n\n날짜\n일자\n일자VC8\nVARCHAR2\n8\n대손처리 일자\n\n\n날짜\n일시\n일시VC16\nVARCHAR2\n16\n대여시작 일시\n\n\n명\n명\n명VC30\nVARCHAR2\n30\n송금정비업체 명\n\n\n명\n성명\n성명VC10\nVARCHAR2\n10\n담당자 성명\n\n\n내용\n내용\n내용VC200\nVARCHAR2\n200\n수리요청 내용\n\n\n수\n수\n수N9\nNUMBER\n4\n에어백보유 수\n\n\n수\n건수\n건수N4\nNUMBER\n4\n신청 건수\n\n\n수\n수량\n수량N4\nNUMBER\n4\n신품출고 수량\n\n\n율\n이율\n이율N5,3\nNUMBER\n5,3\n연체 이율\n\n\n율\n금리\n금리N5,2\nNUMBER\n5,2\n적용 금리\n\n\n금액\n금액\n금액N10\nNUMBER\n10\n정산승인 금액\n\n\n금액\n잔액\n잔액N10\nNUMBER\n10\n매출 잔액\n\n\n번호\n고객번호\n법인고객번호\nVARCHAR2\n9\n법인고객번호\n\n\n번호\n법인등록번호\n법인등록번호\nVARCHAR2\n13\n협력업체 법인등록번호\n\n\n코드\n구분코드\n가족관계유형코드\nVARCHAR2\n5\n지점 시도구분코드\n\n\n분류\n여부\n여부VC1\nVARCHAR2\n1\n신규사업MT 여부\n\n\n\n\nNUMBER(5,3)은 다음을 의미\n\n총 5자리의 숫자를 저장\n그 중 3자리는 소수점 이하 숫자\n결과적으로 소수점 앞에는 2자리의 숫자만 올 수 있다.\n예시,\n\n12.345 (유효)\n1.234 (유효)\n0.123 (유효)\n123.45 (무효 - 소수점 이상 자릿수 초과)\n1.2345 (무효 - 소수점 이하 자릿수 초과)\n\n\n\n\n\n\n\n표준 도메인 구성 요소\n\n범위형 도메인: 속성(컬럼)에 허용되는 데이터 값을 데이터의 유형과 길이로 범위를 제한\n\n열거형 도메인: 속성(컬럼)에 허용되는 데이터 값을 정의된 범위 내에서 구체적으로 열거 또는 목록화하여 범위를 제한\n예시\n\n\n\n\n\n\n\n\n\n\n도메인 유형\n도메인그룹\n정의\n도메인 예시\n\n\n\n\n범위형 도메인\n날짜\n• 특정 사건이 일어난 시점 또는 시점과 시점간의 정해진 기간을 표현하기 위한 도메인\n일자, 일시, 년도, 년월, 월, 일, 시각, 시분, 분기, 반기 등\n\n\n범위형 도메인\n명칭\n• 문자 형식으로 객체에 대한 식별을 표현하기 위한 도메인\n명, 성명, 영문성명, 주소, 우편번호주소, 상세주소, 이메일주소 등\n\n\n범위형 도메인\n내용\n• 서술 형식의 상세 내용을 자유 형식의 텍스트로 표현하기 위한 도메인\n내용\n\n\n범위형 도메인\n수량\n• 객체의 개수나 양을 수로써 표현하기 위한 도메인• 일반적인 측량 단위도 포함됨\n수, 일수, 개월수, 매수, 좌수, 건수, 량, 평점, 연령, 면적, 평형 등\n\n\n범위형 도메인\n율\n• 비율을 수로 표현하기 위한 도메인\n율, 이율, 이자율, 지분율, 할인율 등\n\n\n범위형 도메인\n금액\n• 화폐의 가치를 수로 표현하기 위한 도메인\n금액, 잔액, 차액, 보증금, 수수료, 할인료, 보험료 등\n\n\n범위형 도메인\n번호\n• 각 자리 별 특정 의미를 가지거나 체계를 가지고 관리되어야 하는 속성을 정의하기 위한 도메인• 용어 별 고유의 번호 도메인을 부여\n전사고객번호, 계좌번호, 직원번호, 단말번호, 거래번호, 법인등록번호, 일련번호 등\n\n\n열거형 도메인\n코드\n• 코드화 하여 관리되는 속성을 정의하기 위한 도메인\n그룹내기관코드, 거래상태코드 등\n\n\n열거형 도메인\n분류\n• 상반된 상태의 값을 갖는 속성을 정의하기 위한 도메인\n여부, 유무\n\n\n\n\n\n\n\n\n\n\n[날짜] 도메인그룹은 특정 사건이 일어난 시점 또는 시점과 시점간의 정해진 기간을 표현하기 위한 도메인들을 그룹화하여 관리한다.\n\n\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n데이터타입\n길이\n유효값 범위\n예시\n\n\n\n\n일\nDD형태의 데이터 값을 갖는 도메인\nVARCHAR2\n2\n01~31\n계약시작 일\n\n\n시각\nHHMISS 형태의 시각을 나타내는 도메인\nVARCHAR2\n6\n000000~235959\n차량사용시작 시각\n\n\n일자\nYYYYMMDD형태의 데이터 값을 갖는 도메인\nVARCHAR2\n8\n00010101 ~ 99991231\n계약해지 일자\n\n\n생년월일\nYYMMDD형태의 데이터 값을 갖는 도메인\nVARCHAR2\n6\n000101 ~ 991231\n회원 생년월일\n\n\n일시\nYYYYMMDDHH24MISS형태의 데이터 값을 갖는 도메인\nVARCHAR2\n16\n\n가격정책등록 일시\n\n\n타임스템프\nYYYYMMDDHH24MISSFF3형태의 데이터 값을 갖는 도메인\nTIMESTAMP\n20\n\n\n\n\n년도\nYYYY형태의 데이터 값을 갖는 도메인\nVARCHAR2\n4\n0001 ~ 9999\n기준 년도\n\n\n년월\nYYYYMM형태의 데이터 값을 갖는 도메인\nVARCHAR2\n6\n\n지불 년월\n\n\n월\nMM형태의 데이터 값을 갖는 도메인\nVARCHAR2\n2\n\n청구 월\n\n\n\n\n년월일 형식은 ‘YYYYMMDD’ 로 통일하며, 도메인명은 ‘일자’ 로 정의하여 사용한다\n\n(예시) 계약해지 일자 : 20160420\n\n‘YYYY’ 형식의 도메인명은 ‘년도’ 로 정의하여 사용한다\n\n(예시) 출생 년도 : 1966, 기준 년도 : 2016\n\n‘YYYYMM’ 형식의 도메인명은 ‘년월’ 로 정의하여 사용한다\n\n(예시) 포인트적립 년월 : 201604\n\n‘MM’ 형식의 도메인명은 ‘월’ 로 정의하여 사용한다\n\n(예시) 결산 월 : 04\n\n‘MMDD’ 형식의 도메인명은 ‘월일’ 로 정의하여 사용한다\n\n(예시) 기혼기념 월일 : 1010\n\n‘DD’ 형식의 도메인명은 ‘일’ 로 정의하여 사용한다\n\n(예시) 자동이체지정 일 : 25\n\n년월일+시분초 형식의 도메인명은 ‘일시’ 로 정의하여 사용한다. ‘일시’ 도메인의 DataType은 Date 와 Variable Character의 두 가지를 지원한다\n시스템 로그일시(YYYYMMDDTTMMSSFF3)를 표시하기 위해서는 타임스탬프 도메인을 사용한다. DataType은 Timestamp 형태이다\n\n\n\n\n\n[명] 도메인그룹은 객체에 대한 식별을 표현하기 위한 도메인들을 그룹화하여 관리한다.\n사람을 제외한 포함한 모든 명칭은 ‘명’ 을 도메인으로 정의하여 사용하며, 사람은 ’성명’을 사용한다.\n‘명’ 도메인을 속성(컬럼)에 사용할 경우 도메인 앞에 ‘한글’,’영문’,‘한자’,‘약어’ 등과 같은 수식어가 생략된 경우는 ‘한글’+’명을 ’명’으로 갈음한다\n\n(예시) 종목 명(한글), 종목 영문 명(영문)\n\n사람이 살고 있는 곳이나 기관, 회사 따위가 자리 잡고 있는 곳을 행정 구역으로 나타낸 이름은 ‘주소’ 를 도메인으로 정의하여 사용한다\n’명’은 단독으로 사용하지 않고 사전을 참조하여 복합어로 생성한다\n예시\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n명\n상품, 사물 등을 식별하기 위한 명칭에는 ‘명’ 도메인을 사용한다\n상품 명\n\n\n\n성명\n사람의 성명을 관리할 경우 ’성명’이라는 도메인을 사용한다\n고객 영문성명\n\n\n\n주소\n사람이 살고 있는 곳이나 기관, 회사 따위가 자리 잡고 있는 곳을 행정 구역으로 나타낸 이름은 ‘주소’ 도메인을 사용한다\n직장 주소\n\n\n\n이메일주소\n전자우편주소\n수신자 이메일주소\n\n\n\nID\n시스템 오브젝트 등을 식별하기 위해 사용되는 도메인으로서 체계 없이 순차적으로 체번하여 사용하는 일련번호와는 구별하여 사용한다\n스마트빌 ID\n\n\n\n\n\n\n\n\n“내용” 단일 도메인으로 정의 한다.\n“명세”, “설명”, ”비고”, “적요”, “내역”, “의견”, “사유”, “사항” 등 유사 도메인은 별도로 정의하지 않고 “내용” 도메인으로 통합관리 한다.\n\n(예시) 평가자 의견 (X) → 평가자 의견 내용 (O)\n\n’값’은 단일단어로 허용하지 않고, 사전을 참조하여 복합어로 생성한다.\n예시\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n내용\n사실이나 사물에 대해 전하고자 하는 정보를 형식 없이 서술형으로 저장하는 경우 사용된다.\n사고 내용\n\n\n\n값\n평가값, 항목값 등 값을 의미하는 용어에 대해 값 도메인을 사용한다.\n항목 값, 입력 값\n\n\n\n\n\n\n\n\n[수] 도메인그룹은 객체의 개수나 양을 수로써 표현하기 위한 도메인들을 그룹화하여 관리한다.\n[수 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n수\n셀 수 있는 사물의 크기를 나타내는 값(복합어로 생성)\n종업원 수\n\n\n\n일수\n일을 기준으로 헤아리는 수.\n연체 일수\n\n\n\n개월수\n월을 기준으로 헤아리는 수\n견적 개월수\n\n\n\n년수\n년을 기준으로 헤아리는 수\n근무 년수\n\n\n\n매수\n종이나 유리 따위의 장으로 셀 수 있는 물건의 수효\n기본 매수\n\n\n\n개수\n한 개씩 낱으로 셀 수 있는 물건의 수효\n보유 개수\n\n\n\n건수\n사물이나 사건의 가짓수\n조회 건수\n\n\n\n횟수\n돌아오는 차례의 수효\n기존인출 횟수\n\n\n\n점수\n성적을 나타내는 숫자\n평가대상 점수\n\n\n\n연령\n나이, 사람이 세상에 나서 현재 또는 기준이 되는 때까지 살아 온 햇수\n보험 연령\n\n\n\n수량\n수와 량이 혼재된 수량을 자연수로 표현한 수\n수량\n\n\n\n\n\n금액을 제외한 정보의 수치 및 합계 등을 정의하는 경우에 사용한다\n\n(예시) 고객 수, 연체 일수, 거래 량\n\n되풀이되는 일이나 차례의 수효를 나타내는 경우는 ’횟수’를 도메인으로 정의하여 사용한다.\n\n(예시) 지로자동이체 횟수\n\n나이와 관련된 용어는 ‘연령’ 을 도메인으로 정의하여 사용한다\n\n(예시) 보험 연령\n\n기간을 나타내는 용어의 경우 ‘기간’을 사용하지 않고, ‘년수’, ‘개월수’, ‘일수’ 등으로 구체 적으로 정의한다\n\n(예시) 대출 년수, 연장 개월수, 연체 일수\n\n\n\n\n\n\n[율] 도메인그룹은 둘 이상의 수를 비교하여 그 중 하나의 수를 기준으로 하여 나타낸 다른 수의 비교 값을 표현하기 위한 도메인들을 그룹화하여 관리한다.\n‘율’ 또는 ’률＇은 단일단어로 사용하지 않고 사전을 참조하여 복합어로 생성한다\n확률, 비율 등 ‘%’ 로 관리되는 속성에 대해 ‘율/비율’ 을 도메인으로 정의하여 사용한다\n\n(예시) 연체 율, 담보 비율, 적용 비율\n\n[율 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n율\n비율의 뜻을 나타내는 말 “~율”은 모음으로 끝나거나 ‘ㄴ’ 받침을 가진 일부 명사 뒤에 붙임(복합어로 생성)\n할인 율\n\n\n\n이율\n원금에 대한 이자의 비율, 즉 이자 산출에 기초가 되는 비율\n연체 이율\n\n\n\n세율\n과세 표준에 의하여 세금을 계산하여 매기는 법정률\n\n\n\n\n요율\n요금의 정도나 비율\n보증 요율\n\n\n\n금리\n자금의 사용료로 대외적으로 공시되는 기준의 의미로 사용\n대출 금리\n\n\n\n환율\n외국환 시세\n기준 환율\n\n\n\n비율\n둘 이상의 수를 비교하여 나타낼 때 그 중 한 개의 수를 기준으로 하여 나타낸 다른 수의 비교 값\n사고MT 비율\n\n\n\n\n\n\n\n\n[금액] 도메인그룹은 돈의 액수나 화폐 가치를 표현하기 위한 도메인들을 그룹화하여 관리한다\n금액을 의미하는 도메인은 중복해서 사용하는 것을 피한다\n\n(예시) 물품 + 원가(도메인) + 금액(도메인) → 물품 + 원가(도메인)\n\n기본적으로 ‘금액’ 도메인으로 표현될 수 있는 속성에 대해서는 ‘금액’ 도메인의 사용을 권장한다. 하지만 관용적으로 ‘금’ 도메인이 사용되는 표현에는 별도의 도메인으로 분류하여 사용한다\n합산금액을 의미하는 총계, 합계, 누계 등의 수식어는 반드시 금액 앞에 붙인다\n\n(예시) 감가상각누계금액, 자산총계금액\n\n부득이하게 ’금＇을 사용해야 하는 경우 복합어 생성한다\n[금액 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n\n도메인\n세부 도메인\n설명\n예시\n비고\n\n\n\n\n요금\n료\n대여료, 렌탈료, 리스료 등을 나타내는 금액 단위(복합어로 생성)\n수수 료, 과태 료\n\n\n\n금액\n금액\n돈의 액수\n고객청구 금액\n\n\n\n금액\n잔액\n남은 금액\n연체이자 잔액\n\n\n\n세\n세\n조세의 액수(복합어로 생성)\n부가 세\n\n\n\n가격\n단가\n물건 한 단위(單位)의 가격\n계약 단가\n\n\n\n가격\n원가\n상품의 제조 판매 배급 따위에 든 재화와 용역을 단위에 따라 계산한 가격\n물품 원가\n\n\n\n\n\n\n\n\n[번호] 도메인그룹은 일정한 체계를 가지거나 특정 자리에 존재하는 의미를 표현하기 위한 도메인들을 그룹화하여 관리한다.\n번호도메인의 공통 도메인은 생성하지 않는다.\n‘번호’ 도메인은 일정한 체계를 가지거나 특정 자리의 의미가 존재하는 속성을 정의할 때 사용하는 것을 원칙으로 한다.\n번호 자체에 특별한 의미를 가진 경우 해당 번호를 번호도메인으로 정의하여 사용한다\n\n(예시) 주민등록번호, 사업자등록번호, 전화번호, 법인등록번호 등\n\n순차적으로 채번되는 번호는 ‘일련번호’ 도메인으로 정의하여 사용한다.\n번호도메인의 공통인 도메인은 생성하지 않는다.\n\n(예시) 번호N10 (Number(10))과 같이 분류어가 번호인 용어의 도메인이 공통적으로 쓰는 도메인 생성 금지.\n\n[번호 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n전화번호\n가입된 전화마다 매겨져 있는 일정한 번호\n계약담당 전화번호\n\n\n\n우편번호\n우편물을 쉽게 분류하기 위하여 정보 통신부에서 각 지역마다 매긴 번호\n계약자 우편번호\n\n\n\n주민등록번호\n주민등록을 할 때에, 국가에서 국민에게 부여하는 고유 번호\n계약자 주민등록번호\n\n\n\n사업자등록번호\n세무에서, 신규로 개업하는 사업자에게 부여하는 사업체의 고유번호이다\n공급자 사업자등록번호\n\n\n\n법인등록번호\n사무소의 소재지에서 설립등기(設立登記)를 함으로써 성립하는데 이때 부여된 일련번호이다\n~ 법인등록번호\n\n\n\n계좌번호\n금융 기관에 예금하려고 설정한 개인명이나 법인명의 계좌에 부여된 번호\n입금 계좌번호\n\n\n\n휴대전화번호\n지니고 다니면서 걸고 받을 수 있는 소형 무선 전화기 번호\n\n\n\n\n비밀번호\n본인임을 확인하기 위해 설정한 암호\n\n\n\n\n신용카드번호\n신용카드식별번호\n~ 신용카드번호\n\n\n\n직원번호\n회사 직원 식별번호\n~ 사원번호\n\n\n\n여권번호\n외국을 여행하는 사람의 신분이나 국적을 증명하고 상대국에 그 보호를 의뢰하는 문서번호.\n고객 여권번호\n\n\n\n외국인등록번호\n외국인등록번호\n외국인등록번호\n\n\n\n일련번호\n일률적으로 연속되어 있는 번호\n구성품 일련번호\n\n\n\n\n\n\n\n\n코드는 다른 도메인들과 달리 특정 도메인 값(즉, 코드값)과 이 값에 대한 의미(즉, 코드값명)를 표현하기 위해 코드도메인을 그룹화하여 표준코드로 분류하여 별도로 관리한다\n코드값을 가지는 속성은 반드시 ‘코드’ 를 도메인으로 정의하여 사용한다\n속성명은 해당 속성이 사용하는 코드도메인명과 일치시키는 것을 원칙으로 하나, 속성명에 수식어를 붙여서 사용할 수 있다\n\n(예시) 코드도메인 ‘거래코드’ : 속성명 → 거래코드(O), 수신거래코드(O)\n\n코드의 의미가 ‘여부’ 또는 ‘유무’ 인 경우 ‘코드’를 도메인으로 사용할 수 없고, ‘여부/유무’ 를 도메인으로 정의하여 사용한다\n코드도메인은 ‘수식어 + 코드유형수식어 + 코드’ 로 명명한다\n[코드 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n도메인\n설명\n비고\n\n\n\n\n가상계좌은행코드\n[가상계좌은행코드] 코드는 회사 전사에서 사용하는 가상계좌 은행코드\n\n\n\n제품카테고리코드\n[제품카테고리코드] 코드는 제품의 카테코리에 대하여 조회할 수 있도록 구분하는 코드\n\n\n\n주차장코드\n[주차장구분코드] 코드는 자산 제고의 위치를 구분하는 코드\n\n\n\n수리항목코드\n[수리항목코드] 수리항목 목록을 구분하는 코드\n\n\n\n\n\n\n\n\n\n“여부” 도메인의 인스턴스는 반드시 ‘Y’ 또는 ‘N’ 만 허용되며, NULL, N/A, SAPCE 등의 값은 허용하지 않는다.\n“유무” 도메인의 인스턴스는 반드시 ‘Y’ 또는 ‘N’ 만 허용되며, NULL, N/A, SAPCE 등의 값은 허용하지 않는다.\n“여부/유무” 도메인명은 ‘Y/N’ 이외의 값을 허용하지 않으므로, 기타 값을 사용해야 하는 경우는 코드도메인으로 분류하여 사용하고, 특히 미 정의된 값은 속성의 인스턴스로 사용할 수 없으며, ‘미정의’ 값을 표현하기 위해서는 ’*’ (미정의)와 같이 반드시 대체값을 정의하여 인스턴스로 사용한다. (Not Null)\n단, 대외 인터페이스 테이블의 경우 업무 특성에 따라 Null을 에외로 허용할 수 있다\n[분류 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n여부\n특정 사실이나 행위의 ’그러함/그러하지 아니함’을 의미\n반출 여부\n\n\n\n유무\n존재나 소유의 ’있음/없음’을 의미\n보증금 유무\n\n\n\n\n\n이 두 도메인을 명확히 구분하여 사용하면 데이터의 의미를 더 정확하게 전달할 수 있다.\n\n예를 들어, ’계약 여부’는 계약이 체결되었는지 아닌지를 나타내는 반면, ’계약서 유무’는 물리적인 계약서 문서가 존재하는지 않는지를 나타낼 수 있다.\n\n\n\n\n\n\n\n도메인 등록 기준\n\n\n\n\n\n\n\n\n\n순번\n원칙\n예시\n\n\n\n\n1\n• 속성 데이터가 규칙을 가지는 경우 도메인 명으로 지정한다.• 속성값의 자릿수가 항상 일정해야 하는 경우• 특정 자릿수의 데이터가 의미를 가지는 경우• 속성값 데이터 내의 규칙이 존재하는 경우\n• 고객번호는 20자리• 사업자등록번호, 여권번호, 법인등록번호\n\n\n2\n속성 데이터 값의 유효값이나 유효 범위의 제한이 있는 경우 도메인으로 지정한다.\n• 분기 – 1,2,3,4 만 유효함• 일(Day) – 1~31만 유효함\n\n\n3\n속성값의 성격을 식별하고자 하는 경우 도메인 명으로 지정한다\n지점구분코드\n\n\n4\n속성명 명명 시에 합성어 등이 관용적으로 사용되어 분리하여 사용하기 힘든 경우 도메인 명으로 지정한다.\n잔액, 특소세\n\n\n5\n코드값 체계를 가지는 모든 코드는 도메인 명으로 지정한다\n은행코드, 부품코드\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n도메인 그룹\n도메인\n인포타입\n데이터 타입\n길이\n설명\n예시\n\n\n\n\n날짜\n일자\n일자VC8\nVARCHAR2\n8\nYYYYMMDD 형식의 날짜\n20230501\n\n\n날짜\n일시\n일시VC14\nVARCHAR2\n14\nYYYYMMDDHHMMSS 형식의 날짜와 시간\n20230501143000\n\n\n명칭\n성명\n성명VC50\nVARCHAR2\n50\n개인의 이름\n홍길동\n\n\n명칭\n상품명\n명VC100\nVARCHAR2\n100\n상품의 이름\n스마트폰 갤럭시 S23\n\n\n내용\n내용\n내용VC4000\nVARCHAR2\n4000\n자유 형식의 텍스트\n이 제품은 최신 기술을 적용한…\n\n\n수량\n수량\n수량N10\nNUMBER\n10\n물품의 개수\n1000\n\n\n율\n이율\n이율N5_2\nNUMBER\n5,2\n비율을 나타내는 수치 (소수점 2자리)\n3.75\n\n\n금액\n금액\n금액N15\nNUMBER\n15\n화폐 금액\n10000000\n\n\n번호\n전화번호\n전화번호VC20\nVARCHAR2\n20\n전화번호 형식\n010-1234-5678\n\n\n코드\n상품코드\n코드VC10\nVARCHAR2\n10\n상품을 구분하는 고유 코드\nPRD0001234\n\n\n분류\n여부\n여부VC1\nVARCHAR2\n1\nY/N으로 표현되는 여부\nY"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_3.data_word_glossary.html#표준-용어-사전-예시",
    "href": "docs/blog/posts/Governance/5_3.data_word_glossary.html#표준-용어-사전-예시",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n용어ID\n용어논리명\n용어물리명\n도메인\n정의\n관련업무영역\n사용예시\n표준단어구성\n승인상태\n등록일\n\n\n\n\nT001\n고객번호\nCUST_NO\n일련번호VC10\n고객을 유일하게 식별하는 번호\n고객관리, 영업\n“C0001234567”\n고객(주제어) + 번호(분류어)\n승인\n2023-01-15\n\n\nT002\n장기렌트계약시작일자\nLTRM_RENT_CNTR_STRT_DT\n일자VC8\n장기 렌트 계약이 시작되는 날짜\n계약관리, 렌트관리\n“20230107”\n장기(수식어) + 렌트(주제어) + 계약(주제어) + 시작(수식어) + 일자(분류어)\n승인\n2023-01-16\n\n\nT003\n월별렌탈등록비합계금액\nMTHLY_RENT_RGST_FEE_TOT_AMT\n금액N15\n한 달 동안의 렌탈 등록비 총액\n재무, 렌트관리\n“5000000”\n월별(수식어) + 렌탈(주제어) + 등록(주제어) + 비(주제어) + 합계(수식어) + 금액(분류어)\n검토중\n2023-01-17\n\n\n\n* VC10: Variable Character의 약자로 가변 길이 문자열을 나타냄 (최대 10 자리)  \n* N15: N은 숫자(Numeric) 데이터 타입, 최대 15자리의 숫자  \n* D: Date (날짜)  \n* T: Time (시간)  \n* B: Boolean (참/거짓)  \n\n칼럼 설명\n\n용어ID: 각 용어의 고유 식별자\n용어논리명: 업무에서 사용되는 한글 용어명 (최대 30자, 권장 20자 이내)\n용어물리명: 데이터베이스 등에서 사용되는 영문 약어명 (최대 28자, 권장 20자 이내)\n도메인: 해당 용어의 데이터 타입과 제약조건\n정의: 용어에 대한 명확한 설명\n관련업무영역: 해당 용어가 주로 사용되는 업무 분야\n사용예시: 실제 데이터 예시\n표준단어구성: 용어를 구성하는 표준 단어들과 각 단어의 역할 (수식어, 주제어, 분류어)\n승인상태: 용어의 현재 승인 상태 (예: 승인, 검토중, 폐기 등)\n등록일: 용어가 사전에 처음 등록된 날짜\n최종수정일: 용어 정보가 마지막으로 수정된 날짜"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_4.data_code.html",
    "href": "docs/blog/posts/Governance/5_4.data_code.html",
    "title": "Data Governance Study - Data Standard Code",
    "section": "",
    "text": "데이터 표준 코드는 특정 개념이나 항목을 나타내기 위해 일관되게 사용되는 약속된 값의 집합이다.\n\n즉, 코드란 활용하고자 하는 데이터를 약어 혹은 기호로 함축하여 사용하는 데이터를 말한다.\n\n도메인의 한 유형으로서, 속성(컬럼)에 허용된 데이터 값을 제한된 범위 내에서 구체적으로 열거하여 정의한 것을 지칭한다\n이 데이터 값을 코드값 또는 코드 유효값이라 하며 각각의 코드값에는 의미를 부여한다. 이 의미를 ‘코드값명’ 또는 ’코드유효값정의’라 한다\n예시\n\n성별 코드: M (남성), F (여성)\n은행 코드: 004 (국민은행), 020 (우리은행)\n국가 코드: KR (대한민국), US (미국)\n\n코드명은 “국가코드”이다.\n코드값(코드 유효값)은 ISO 3166-1 alpha-2 표준을 따르는 2자리 국가 코드이다.\n코드값 명은 해당 국가의 한글 명칭이다.\n코드 유효값 정의(설명)는 모든 코드가 ISO 3166-1 alpha-2 기준을 따르며, 영문 대문자 2자리로 구성됨을 명시한다..\n\n\n\n\n\n\n\n\n\n\n\n코드명\n코드값\n코드값 명\n코드 유효값 정의\n\n\n\n\n국가코드\nKR\n대한민국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nUS\n미국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nJP\n일본\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nCN\n중국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nGB\n영국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nDE\n독일\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nFR\n프랑스\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n\n\n\n\n\n데이터의 일관성 유지\n시스템 간 데이터 교환 용이\n데이터 해석의 명확성 제공\n\n전사적으로 표준코드를 사용하도록 함으로써 업무영역간 운영데이터의 불일치를 방지하고 데이터의 정합성을 향상 시킨다.\n\n데이터 입력 오류 감소\n\n\n\n\n\n고유성: 각 코드는 유일한 의미를 가짐\n\n코드의 중복방지(유사한 코드 검토 포함) 통해 효율적 운영이 가능해야 한다\n\n간결성: 일반적으로 짧고 간단한 형태\n체계성: 논리적인 구조를 가짐\n확장성: 새로운 항목 추가가 가능한 구조\n\n\n\n\n\n코드 값: 실제 사용되는 코드\n코드 명: 코드가 나타내는 항목의 이름\n설명: 코드의 의미나 사용 목적\n유효 기간: 코드의 사용 가능 기간\n\n\n\n\n\n코드 관리 시스템 구축\n정기적인 검토 및 업데이트\n코드 변경 이력 관리\n\n\n\n\n\n업계 표준이나 국제 표준 고려\n코드의 의미가 시간이 지나도 변하지 않도록 설계\n코드 체계의 일관성 유지\n\n\n\n\n\n표준코드 작성 시 의미를 충분히 파악할 수 있도록 작성을 하며 구성은 표준용어 작성 기준 및 관리원칙을 따른다.\n\n\n\n\n\n\n\n\n\n순번\n코드 표준화 대상 및 관리 원칙\n비고\n\n\n\n\n1\n• 코드 정보가 저장되며 코드 테이블에 그 내용이 존재하는 경우\n표준화 대상\n\n\n2\n• 애플리케이션 내부에 코드의 실제 내용이 존재하는 경우\n표준화 대상\n\n\n3\n• 사용 가능한 데이터의 종류가 2개 이상인 경우\n표준화 대상\n\n\n4\n• Yes or No 값 외에 미확정 값(Null)을 가질 수 있는 경우\n표준화 대상\n\n\n5\n• 현재는 Yes or No 처럼 Boolean값을 갖지만 추후 그 이외의 데이터가 추가 될 가능성이 있는 경우\n표준화 대상\n\n\n6\n• Yes or No 처럼 Boolean 값만을 데이터로 가질 경우 ‘Y’/’N’으로 통일 함\n관리원칙\n\n\n7\n• 표준코드도메인은 관용적으로 사용하는 용어를 우선적으로 사용한다\n관리원칙\n\n\n8\n• 표준코드를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n관리원칙\n\n\n9\n• 단일 코드는 하나의 공통 엔티티로 관리한다.\n관리원칙\n\n\n10\n• 계층코드는 내용을 분석하여 단일코드 형태로 변경 조정 할 수 있다.\n관리원칙\n\n\n11\n• 목록성 코드의 인스턴스 값은 각각 별도의 엔티티로 관리하며 공통코드 엔티티에서는 해당 코드값을 관리하는 테이블 정보를 관리한다.\n관리원칙\n\n\n\n\n\n\n\n표준코드는 신규 모델링 시 데이터 모델 관리자 또는 응용팀에서 도출 신청 후 데이터 표준 담당자가 최종 관리한다. 죄송합니다. 제가 이해를 돕기 위해 추가 설명을 드리겠습니다. 귀하께서 언급하신 내용은 코드 관리 프로세스의 중요한 부분을 강조하고 있습니다. 이를 반영하여 테이블을 수정해 보겠습니다.\n\n\n\n\n\n\n\n\n\n\n순번\n담당\n코드 표준 관리 담당 별 역할\n비고\n\n\n\n\n1\n응용팀\n• 기능 정의시 데이터 항목에 코드가 필요한 경우 모델러와 협의• 목록성 코드에 대한 요건 제시• 각 업무영역별로 생성된 목록성 코드에 대한 코드값 관리\n운영시 코드 신청은 업무담당자(현업)가 수행함현업: 코드 신청, 활용\n\n\n2\n데이터 모델관리자\n• 단일코드, 계층코드 등 공통코드를 관리하기 위한 테이블 설계• 업무영역별 목록성 코드 테이블 설계\n\n\n\n3\n표준담당자\n• 단일코드/계층코드 신청을 위한 템플릿 제공• 코드명에 대한 표준 준수 검증• 코드 취합/조정 및 공통코드 확정• 코드 중복 조정 작업 수행(인스턴스명 간 유사성 검증)• 데이터 타입 검증(코드 도메인화)• AS-IS 코드와 매핑 정보 관리• 코드 등록, 공통 코드 및 코드 도메인 관리(메타시스템 or Excel)• 신규 코드 생성 및 AS-IS 코드의 코드 값에 대한 재정비 수행• 코드에 대한 Ownership 관리 및 승인\n표준담당자: 코드 등록 및 관리\n\n\n\n\n응용팀(현업)은 업무 수행 중 필요한 코드를 식별하고 신청\n\n응용팀(현업)의 비고 항목에 “현업: 코드 신청, 활용”은 실제 업무를 수행하는 현업 담당자가 코드를 신청하고 사용한다\n\n표준담당자는 이 신청을 검토하고, 적절한 경우 코드를 등록하며, 전체적인 코드 체계를 관리한다.\n\n표준담당자의 비고 항목에 “표준담당자: 코드 등록 및 관리”는 표준담당자가 신청된 코드를 검토하고, 실제로 시스템에 등록하며, 지속적으로 관리한다는 점을 명확히 한다.\n\n\n\n\n\n\n코드의 구성에 따른 유형으로는 단일, 계층, 목록, 복합코드가 있으며, 내용은 다음과 같다.\n\n\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n단일코드(S)\n• [코드값] + [코드내용]의 형태를 갖추는 가장 일반적인 형태의 코드로서 한 개의 코드로 Key가 구성됨• 단일코드의 코드값은 시스템에 등록/관리하며, 등록된 단일코드(코드명, 코드값, 코드값 한글정의)는 프로젝트 내 공통코드 테이블의 형태로 만들어져 전사공통으로 활용됨\n\n\n\n계층코드(C)\n• 하나 이상의 코드를 상속받거나 계층 구조를 통해 생성되어진 코드로 Key 가 구성된 경우• 대분류 / 중분류 / 소분류 와 같은 분류체계를 가짐\n\n\n\n목록코드(L)\n• 목록성코드는 코드명, 코드값, 코드한글정의 외에 부가적인 정보를 관리해야 하는 코드를 의미하며, 해당 업무팀에서 테이블의 형태로 관리한다\n\n\n\n복합코드(M)\n• 두개이상의 코드도메인을 하나의 코드도메인에서 활용하기 위하여 구성.• 복합코드는 단일코드의 코드도메인을 관리함.\n\n\n\n\n\n각 코드 유형(단일코드, 계층코드, 목록코드, 복합코드)의 특성과 용도를 명확히 구분하고 있다.\n예시\n\n단일코드(S)\n\n단일코드(S) 유형: 각 코드가 하나의 고유한 의미를 가진다.\n코드값(고객 구분 코드)은 숫자로 구성되어 있으며, 일반적으로 2자리 숫자를 사용한다.\n코드명(고객 구분 명)은 해당 코드의 의미를 명확하게 설명한다.\n사용 조건\n\n간단하고 평면적인 분류가 필요할 때\n코드 값과 의미가 1:1로 대응될 때\n코드의 수가 제한적이고 변경이 적을 때\n\n예시: 고객 구분 코드, 성별코드, 결혼여부코드, 직급코드\n\n\n\n\n고객 구분 코드\n고객 구분 명\n\n\n\n\n01\n개인\n\n\n02\n법인\n\n\n03\n개인사업자\n\n\n04\n외국인\n\n\n05\n공공기관\n\n\n06\n비영리단체\n\n\n07\nVIP\n\n\n08\n임직원\n\n\n09\n제휴사\n\n\n10\n기타\n\n\n\n계층코드(C)\n\n하나 이상의 코드를 상속받거나 계층 구조를 통해 생성되어진 코드로 Key 가 구성된 경우\n대분류 / 중분류 / 소분류 와 같은 분류체계를 가짐\n\n정규화: 각 분류 수준이 별도의 테이블로 분리되어 있어 데이터 중복이 최소화된다.\n참조 무결성: 외래 키 관계를 통해 데이터의 일관성이 유지된다.\n유연성: 각 분류 수준에서 독립적으로 항목을 추가, 수정, 삭제할 수 있다.\n확장성: 새로운 분류 항목을 쉽게 추가할 수 있다.\n\n각 분류 수준에 대한 추가 정보(예: 생성일, 수정일, 설명 등)를 쉽게 추가할 수 있는 장점이 있다.\n\n쿼리 효율성: 필요에 따라 조인을 통해 전체 계층 구조를 조회하거나, 특정 수준만 조회할 수 있다.\n\n사용 조건\n\n데이터가 계층적 구조를 가질 때\n상위 개념과 하위 개념의 관계를 표현해야 할 때\ndrill-down 분석이 필요한 경우\n예시: 조직코드, 상품분류코드, 지역코드\n\n\n대분류 코드 테이블:\n\n\n\n\n대분류 코드\n대분류명\n\n\n\n\nA\n전자제품\n\n\nB\n가전제품\n\n\nC\n의류\n\n\n\n\n중분류 코드 테이블:\n\n\n\n\n중분류 코드\n중분류명\n대분류 코드 (FK)\n\n\n\n\nA1\n컴퓨터\nA\n\n\nA2\n휴대폰\nA\n\n\nB1\n주방가전\nB\n\n\nB2\n생활가전\nB\n\n\nC1\n남성복\nC\n\n\nC2\n여성복\nC\n\n\n\n\n소분류 코드 테이블:\n\n\n\n\n소분류 코드\n소분류명\n대분류 코드 (FK)\n중분류 코드 (FK)\n\n\n\n\nA11\n데스크톱\nA\nA1\n\n\nA12\n노트북\nA\nA1\n\n\nA13\n태블릿\nA\nA1\n\n\nA21\n스마트폰\nA\nA2\n\n\nA22\n피처폰\nA\nA2\n\n\nB11\n냉장고\nB\nB1\n\n\nB12\n전자레인지\nB\nB1\n\n\nB21\n청소기\nB\nB2\n\n\nB22\n세탁기\nB\nB2\n\n\nC11\n셔츠\nC\nC1\n\n\nC12\n바지\nC\nC1\n\n\nC21\n원피스\nC\nC2\n\n\nC22\n스커트\nC\nC2\n\n\n\n\n목록코드(L)\n\n코드값(은행코드)과 코드명(은행명) 외에 여러 부가 정보를 포함한다.\n약칭, 영문명, 주소, 전화번호 등 해당 코드와 관련된 상세 정보를 관리한다.\n설립일과 같은 날짜 정보도 포함될 수 있다.\n사용여부와 같은 관리 정보도 포함될 수 있다.\n목록코드는 일반적으로 해당 업무팀에서 직접 관리하며, 시스템 전반에서 참조되어 사용됨\n\n목록코드의 장점\n\n상세 정보 관리: 코드와 관련된 다양한 부가 정보를 함께 관리할 수 있다.\n업무 특성 반영: 특정 업무 영역의 특성을 반영한 정보를 포함할 수 있다.\n데이터 일관성: 코드와 관련된 정보를 중앙에서 관리함으로써 데이터의 일관성을 유지할 수 있다.\n확장성: 필요에 따라 새로운 정보 항목을 쉽게 추가할 수 있다.\n\n\n사용 조건\n\n코드와 함께 추가적인 속성 정보가 필요할 때\n\n업무 요구사항 분석\n\n사용자나 부서가 코드 외에 추가 정보를 자주 요청하는 경우\n코드만으로는 업무 처리에 충분한 정보를 제공하지 못하는 경우\n\n데이터 활용도 검토\n\n보고서나 분석에서 코드 관련 부가 정보가 자주 필요한 경우\n데이터 조인이나 lookup 작업이 빈번하게 발생하는 경우\n\n시스템 통합 요구사항\n\n다른 시스템과 데이터를 교환할 때 코드 외 추가 정보가 필요한 경우\n외부 시스템이나 API가 코드와 관련된 부가 정보를 요구하는 경우\n\n변경 관리 필요성\n\n코드 값이 시간에 따라 변경되거나 이력 관리가 필요한 경우\n코드의 유효 기간이나 사용 상태를 관리해야 하는 경우\n\n복잡한 비즈니스 로직\n\n코드를 기반으로 복잡한 비즈니스 규칙이나 계산이 필요한 경우\n코드에 따라 다른 처리 로직이 적용되어야 하는 경우\n\n\n사용자 인터페이스 요구사항\n\n코드 선택 시 사용자에게 추가 정보를 제공해야 하는 경우\n코드 검색이나 필터링 시 다양한 기준이 필요한 경우\n\n감사 및 규제 요구사항\n\n코드 사용에 대한 상세한 이력이나 근거를 유지해야 하는 경우\n규제 준수를 위해 코드와 관련된 부가 정보를 관리해야 하는 경우\n\n\n코드 정보가 자주 변경되거나 확장될 가능성이 있을 때\n코드 정보가 특정 업무 영역에 국한되어 관리될 때\n\n예시: 은행코드, 국가코드, 통화코드\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n은행코드\n은행명\n약칭\n영문명\n본점주소\n대표전화\n설립일\n사용여부\n\n\n\n\n001\n한국은행\n한은\nBank of Korea\n서울특별시 중구 남대문로 39\n02-759-4114\n1950-06-12\nY\n\n\n002\n산업은행\n산은\nKorea Development Bank\n서울특별시 영등포구 은행로 14\n1588-1500\n1954-04-01\nY\n\n\n003\n기업은행\nIBK\nIndustrial Bank of Korea\n서울특별시 중구 을지로 79\n1566-2566\n1961-08-01\nY\n\n\n004\nKB국민은행\n국민\nKB Kookmin Bank\n서울특별시 영등포구 국제금융로8길 26\n1588-9999\n2001-11-01\nY\n\n\n005\n하나은행\n하나\nKEB Hana Bank\n서울특별시 중구 을지로 35\n1599-1111\n1967-01-30\nY\n\n\n007\n수협은행\n수협\nSuhyup Bank\n서울특별시 송파구 오금로 62\n1588-1515\n1962-04-01\nY\n\n\n\n복합코드 (M)\n\n코드 구성: 복합코드는 ’상품카테고리코드’와 ’지역코드’를 조합하여 만들어진다.\n의미 결합: 두 개의 단일 코드의 의미를 결합하여 새로운 의미를 만든다.\n추가 정보: 복합코드에는 단순히 두 코드를 붙인 것 외에도 추가적인 정보(설명, 담당부서, 적용일자 등)를 포함할 수 있다.\n유연성: 새로운 상품 카테고리나 지역이 추가될 때 쉽게 확장할 수 있다.\n복합코드의 장점\n\n데이터 압축: 여러 정보를 하나의 코드로 표현할 수 있다.\n의미 전달: 코드만으로도 여러 차원의 정보를 전달할 수 있다.\n유연한 확장: 기존 단일 코드 체계를 유지하면서 새로운 의미를 부여할 수 있다.\n\n데이터 분석: 복합코드를 분해하여 다양한 관점에서 데이터를 분석할 수 있다.\n\n이러한 복합코드는 조직의 복잡한 구조나 다차원적인 정보를 효율적으로 표현하고 관리하는 데 유용하다.\n\n이러한 목록코드는 일반적으로 해당 업무팀에서 직접 관리하며, 시스템 전반에서 참조되어 사용된다.\n사용 조건\n\n두 개 이상의 독립적인 코드 체계를 조합해야 할 때\n다차원적인 정보를 하나의 코드로 표현해야 할 때\n기존 코드 체계를 유지하면서 새로운 의미를 부여해야 할 때\n\n먼저, 각 단일 코드 도메인을 정의한다.\n예시: 지역별 상품코드, 부서별 프로젝트코드\n\n상품 카테고리 코드 (단일코드)\n\n\n\n\n코드\n카테고리명\n\n\n\n\nA\n전자제품\n\n\nB\n의류\n\n\nC\n식품\n\n\n\n\n지역 코드 (단일코드)\n\n\n\n\n코드\n지역명\n\n\n\n\n01\n서울\n\n\n02\n부산\n\n\n03\n대구\n\n\n\n\n지역별 상품 코드 (복합코드)\n\n\n\n\n복합코드\n상품카테고리코드\n지역코드\n설명\n담당부서\n적용일자\n\n\n\n\nA01\nA\n01\n서울 전자제품\n서울영업1팀\n2023-01-01\n\n\nA02\nA\n02\n부산 전자제품\n부산영업팀\n2023-01-01\n\n\nA03\nA\n03\n대구 전자제품\n대구영업팀\n2023-01-01\n\n\nB01\nB\n01\n서울 의류\n서울영업2팀\n2023-01-01\n\n\nB02\nB\n02\n부산 의류\n부산영업팀\n2023-01-01\n\n\nB03\nB\n03\n대구 의류\n대구영업팀\n2023-01-01\n\n\nC01\nC\n01\n서울 식품\n서울영업3팀\n2023-01-01\n\n\nC02\nC\n02\n부산 식품\n부산영업팀\n2023-01-01\n\n\nC03\nC\n03\n대구 식품\n대구영업팀\n2023-01-01\n\n\n\n\n\n선택 시 고려사항\n\n데이터의 구조: 데이터가 계층적인지, 평면적인지 파악\n확장성: 향후 코드 추가나 변경 가능성 고려\n사용 목적: 데이터 분석, 보고, 시스템 통합 등의 용도 파악\n관리 용이성: 코드 관리의 복잡성과 유지보수 고려\n업무 특성: 특정 업무 도메인의 요구사항 반영\n\n\n\n\n\n\n회사에서 사용하는 표준 코드의 기준 관리항목은 아래와 같다\n표준코드 관리항목 구성\n\n신규 모델링 단계에서 코드 값에 대한 신청은 오프라인으로 수행된다.\n변경 모델링 단계에서 코드 값에 대한 신청은 표준화 담당자를 통해 이루어진다.\n모든 코드는 코드 도메인과 매핑 관계를 가지며 ERP 공통 코드 테이블에 대한 데이터 SYNC 작업이 수행된다\n\n\n\n\n\n유형\n설명\n\n\n\n\n코드구분값\n• 코드목록값 혹은 계층코드일 경우 최상위 코드 목록값\n\n\n코드값\n• 코드 목록에 따른 코드 Value값\n\n\n코드명\n• 코드 도메인명과 동일함\n\n\n코드설명\n• 코드명 설명\n\n\n코드영문명\n• 코드 도메인 영문명과 동일함\n\n\n코드길이\n• 실제 코드 값의 길이\n\n\n코드구분\n• 단일코드/계층코드/목록성코드로 구분함\n\n\n업무구분\n• 코드에 대한 ownership을 가진 담당 업무영역\n\n\n상위코드값\n• 상위 코드 Value값\n\n\n상위코드구분값\n• 상위코드 목록값\n\n\n엔티티명\n• 목록성 코드인 경우 대상 엔티티명\n\n\n테이블명\n• 목록성 코드인 경우 대상 테이블명\n\n\n\n\n\n\n\n[주제어] + [코드 수식어 유형] + 코드 형태로 정의하여 사용한다.\n표준코드 구성 체계\n\n수식어 없이 코드용어 생성 가능\n\n\n\n\n\n\n\n\n\n\n\n분류\n유형\n설명\n예시\n\n\n\n\n기본\n유형\n어떤 비슷한 것들의 본질을 개체로서 나타낸 것, 또는 그것들의 공통되는 성질이나 모양을 정의할 때 사용되는 코드 유형\n거래 유형 코드\n\n\n기본\n분류\n코드 값을 체계화 하여 관리하는 경우 사용하며, 주로 대 / 중 / 소 / 세 등의 분류 체계를 갖는 코드에 대해서 ’분류’를 사용\n제품 소분류 코드\n\n\n기본\n종류\n가급적 사용을 제한하되 ’유형’이나 ’분류’의 사용 시 의미 전달이 모호해질 경우 혹은 통상적으로 사용되는 경우에 한해서 사용\n거래 종류 코드\n\n\n기본\n구분\n따로따로 갈라서 나누는 것으로 ’유형’보다는 단순하고 값의 종류가 10개 이내로 제한적이고 값의 범위가 명확한 경우 사용\n상품항목 구분 코드\n\n\n기본\n항목\n목록을 나열한 경우에 한해 사용\n점검 항목 코드\n\n\n확장\n사유\n인식 작용, 분석, 종합, 추리, 판단 등의 정신 작용에 대한 근거 및 동기\n취소 사유 코드\n\n\n확장\n상태\n사물이나 현상이 처해 있는 현재의 모양 또는 형편\n계약 상태 코드\n\n\n확장\n관계\n둘 이상의 사람, 사물, 현상 따위가 서로 관련을 맺거나 관련이 있음\n계약자 관계 코드\n\n\n확장\n용도\n사용되는 곳 혹은 사용되는 목적을 정의\n자금 용도 코드\n\n\n확장\n등급\n높고 낮음이나 좋고 나쁨 따위의 차이를 여러 층으로 구분한 단계\n차량 등급 코드\n\n\n확장\n지역\n전체 영역을 어떤 특징으로 나눈 일정한 공간 영역\n등록 지역 코드\n\n\n확장\n단위\n어떤 물리량(物理量)의 크기를 나타낼 때 비교의 기준 되는 크기\n회계 단위 코드\n\n\n\n\n\n\n\n코드값(인스턴스)을 부여하는 방식에 대한 4가지 분류가 있으며 코드의 형식을 결정할 수 있다\n계층 분류형(H)은 조직 구조와 같이 계층적인 관계를 표현하는 데 적합합니다.\n순차 채번형(S)은 순서가 있는 항목들을 나열할 때 유용합니다.\n표준약어 부여형(A)은 국제적으로 통용되는 표준 코드를 사용할 때 적합합니다.\n복합 분류형(C)은 계층 구조와 순차적 번호 부여가 동시에 필요한 경우에 사용됩니다.\n표준코드 구성 체계\n\n[A : Alphabet N : Numeric S : Sequence Number]\n\n\n\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n계층 분류형(H)\n• 대/중/소 등의 분류에 의한 구분이 필요한 경우 적용• 일반적으로 10진 분류 체계로 구성• 코드형식: N + N + N - NN(대분류) + NN(중분류) + NN(소분류) - NN(본/지점 분류) + NN(실/부 분류) + NN(팀 분류)\n[조직구분코드]100000: 본사총괄101000: 기획실101010: 회계팀101020: 자금팀\n\n\n순차 채번형(S)\n• 일련번호와 같이 순차적으로 번호를 부여하며 부여된 자리를 넘지 않도록 구성• 가능한 결번이 없도록 정의함. 코드 길이 만큼을 앞에 0을 채워서 번호 부여(숫자형 문자)• 코드 형식: SS\n[가족구분코드]01: 부02: 모03: 배우자99: 기타\n\n\n표준약어 부여형(A)\n• 대부분 국제 표준 코드 및 국가표준코드, 업종표준코드 등이 이에 속함.• 코드 형식: AAA\n[국가구분코드]CAN: 캐나다CHN: 중국\n\n\n복합 분류형(C)\n• 계층분류와 순차채번이 결합된 형태의 분류• 코드 형식: ASSSSS\n[담보구분코드]A00001: 건물A00002: 토지B00001: 예금\n\n\n\n\n\n\n\n표준코드 구성 체계\n기본 원칙\n\n원칙적으로 회사 렌터카 시스템 구축에서 사용하는 모든 코드는 통합 관리한다\n업무적으로 동일한 의미의 코드나 유사한 코드를 통합 후 표준화된 코드값과 코드내용을 부여한다\n목록성 코드의 경우 참조정보(DB명, 테이블명, 컬럼명) 만 관리하며 별도 코드값, 코드내용을 관리하지 않는다. 코드값과 코드내용 이외에 부가적인 정보가 존재하고, 코드에 따라 부가적인 정보의 개수가 다르기 때문에 표준 코드 테이블에서 관리하기 어렵기 때문이다\n\n코드값(인스턴스) 부여 원칙\n\n코드값의 부여는 원칙적으로 숫자형 문자 형태의 일련번호(01,02..)를 부여한다\n특별한 사유가 없는 한 현업에서 부여한 코드값을 최우선 사용함을 원칙으로 한다\n코드값 부여는 가능한 연속적으로 부여한다\n코드값 길이는 향후 확장성을 고려해서 부여한다\n숫자로만 이루어진 코드는 원칙적으로 허용하지 않으며 코드 길이만큼 숫자형 문자를 이용해서 ’0’을 채워서 코드를 부여한다\n‘기타’, ‘해당없음’ 등의 내용을 갖는 코드는 가급적 사용하지 않는 것을 원칙으로 하되, 반드시 사용해야 할 경우 해당 자리의 ‘00’, ‘99’ 등의 최대값을 이용한다\n’ 여부’, ’유무’의 모든 코드값은 ’Y＇과 ’N＇로 사용된다.\n\n코드값(인스턴스) 부여 원칙 예외\n\n기존 As-Is에서 특별한 의미를 가지는 코드 값으로 사용되었을 경우 그대로 채택한다\n외부에서 정의되어서 표준 약어로 널리 사용되는 있는 코드들은 표준화 대상에서 제외하며, 그대로 사용하도록 한다.\n\n국가구분코드 등"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_4.data_code.html#데이터-표준-코드-사전이란",
    "href": "docs/blog/posts/Governance/5_4.data_code.html#데이터-표준-코드-사전이란",
    "title": "Data Governance Study - Data Standard Code",
    "section": "",
    "text": "데이터 표준 코드는 특정 개념이나 항목을 나타내기 위해 일관되게 사용되는 약속된 값의 집합이다.\n\n즉, 코드란 활용하고자 하는 데이터를 약어 혹은 기호로 함축하여 사용하는 데이터를 말한다.\n\n도메인의 한 유형으로서, 속성(컬럼)에 허용된 데이터 값을 제한된 범위 내에서 구체적으로 열거하여 정의한 것을 지칭한다\n이 데이터 값을 코드값 또는 코드 유효값이라 하며 각각의 코드값에는 의미를 부여한다. 이 의미를 ‘코드값명’ 또는 ’코드유효값정의’라 한다\n예시\n\n성별 코드: M (남성), F (여성)\n은행 코드: 004 (국민은행), 020 (우리은행)\n국가 코드: KR (대한민국), US (미국)\n\n코드명은 “국가코드”이다.\n코드값(코드 유효값)은 ISO 3166-1 alpha-2 표준을 따르는 2자리 국가 코드이다.\n코드값 명은 해당 국가의 한글 명칭이다.\n코드 유효값 정의(설명)는 모든 코드가 ISO 3166-1 alpha-2 기준을 따르며, 영문 대문자 2자리로 구성됨을 명시한다..\n\n\n\n\n\n\n\n\n\n\n\n코드명\n코드값\n코드값 명\n코드 유효값 정의\n\n\n\n\n국가코드\nKR\n대한민국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nUS\n미국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nJP\n일본\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nCN\n중국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nGB\n영국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nDE\n독일\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nFR\n프랑스\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n\n\n\n\n\n데이터의 일관성 유지\n시스템 간 데이터 교환 용이\n데이터 해석의 명확성 제공\n\n전사적으로 표준코드를 사용하도록 함으로써 업무영역간 운영데이터의 불일치를 방지하고 데이터의 정합성을 향상 시킨다.\n\n데이터 입력 오류 감소\n\n\n\n\n\n고유성: 각 코드는 유일한 의미를 가짐\n\n코드의 중복방지(유사한 코드 검토 포함) 통해 효율적 운영이 가능해야 한다\n\n간결성: 일반적으로 짧고 간단한 형태\n체계성: 논리적인 구조를 가짐\n확장성: 새로운 항목 추가가 가능한 구조\n\n\n\n\n\n코드 값: 실제 사용되는 코드\n코드 명: 코드가 나타내는 항목의 이름\n설명: 코드의 의미나 사용 목적\n유효 기간: 코드의 사용 가능 기간\n\n\n\n\n\n코드 관리 시스템 구축\n정기적인 검토 및 업데이트\n코드 변경 이력 관리\n\n\n\n\n\n업계 표준이나 국제 표준 고려\n코드의 의미가 시간이 지나도 변하지 않도록 설계\n코드 체계의 일관성 유지\n\n\n\n\n\n표준코드 작성 시 의미를 충분히 파악할 수 있도록 작성을 하며 구성은 표준용어 작성 기준 및 관리원칙을 따른다.\n\n\n\n\n\n\n\n\n\n순번\n코드 표준화 대상 및 관리 원칙\n비고\n\n\n\n\n1\n• 코드 정보가 저장되며 코드 테이블에 그 내용이 존재하는 경우\n표준화 대상\n\n\n2\n• 애플리케이션 내부에 코드의 실제 내용이 존재하는 경우\n표준화 대상\n\n\n3\n• 사용 가능한 데이터의 종류가 2개 이상인 경우\n표준화 대상\n\n\n4\n• Yes or No 값 외에 미확정 값(Null)을 가질 수 있는 경우\n표준화 대상\n\n\n5\n• 현재는 Yes or No 처럼 Boolean값을 갖지만 추후 그 이외의 데이터가 추가 될 가능성이 있는 경우\n표준화 대상\n\n\n6\n• Yes or No 처럼 Boolean 값만을 데이터로 가질 경우 ‘Y’/’N’으로 통일 함\n관리원칙\n\n\n7\n• 표준코드도메인은 관용적으로 사용하는 용어를 우선적으로 사용한다\n관리원칙\n\n\n8\n• 표준코드를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n관리원칙\n\n\n9\n• 단일 코드는 하나의 공통 엔티티로 관리한다.\n관리원칙\n\n\n10\n• 계층코드는 내용을 분석하여 단일코드 형태로 변경 조정 할 수 있다.\n관리원칙\n\n\n11\n• 목록성 코드의 인스턴스 값은 각각 별도의 엔티티로 관리하며 공통코드 엔티티에서는 해당 코드값을 관리하는 테이블 정보를 관리한다.\n관리원칙\n\n\n\n\n\n\n\n표준코드는 신규 모델링 시 데이터 모델 관리자 또는 응용팀에서 도출 신청 후 데이터 표준 담당자가 최종 관리한다. 죄송합니다. 제가 이해를 돕기 위해 추가 설명을 드리겠습니다. 귀하께서 언급하신 내용은 코드 관리 프로세스의 중요한 부분을 강조하고 있습니다. 이를 반영하여 테이블을 수정해 보겠습니다.\n\n\n\n\n\n\n\n\n\n\n순번\n담당\n코드 표준 관리 담당 별 역할\n비고\n\n\n\n\n1\n응용팀\n• 기능 정의시 데이터 항목에 코드가 필요한 경우 모델러와 협의• 목록성 코드에 대한 요건 제시• 각 업무영역별로 생성된 목록성 코드에 대한 코드값 관리\n운영시 코드 신청은 업무담당자(현업)가 수행함현업: 코드 신청, 활용\n\n\n2\n데이터 모델관리자\n• 단일코드, 계층코드 등 공통코드를 관리하기 위한 테이블 설계• 업무영역별 목록성 코드 테이블 설계\n\n\n\n3\n표준담당자\n• 단일코드/계층코드 신청을 위한 템플릿 제공• 코드명에 대한 표준 준수 검증• 코드 취합/조정 및 공통코드 확정• 코드 중복 조정 작업 수행(인스턴스명 간 유사성 검증)• 데이터 타입 검증(코드 도메인화)• AS-IS 코드와 매핑 정보 관리• 코드 등록, 공통 코드 및 코드 도메인 관리(메타시스템 or Excel)• 신규 코드 생성 및 AS-IS 코드의 코드 값에 대한 재정비 수행• 코드에 대한 Ownership 관리 및 승인\n표준담당자: 코드 등록 및 관리\n\n\n\n\n응용팀(현업)은 업무 수행 중 필요한 코드를 식별하고 신청\n\n응용팀(현업)의 비고 항목에 “현업: 코드 신청, 활용”은 실제 업무를 수행하는 현업 담당자가 코드를 신청하고 사용한다\n\n표준담당자는 이 신청을 검토하고, 적절한 경우 코드를 등록하며, 전체적인 코드 체계를 관리한다.\n\n표준담당자의 비고 항목에 “표준담당자: 코드 등록 및 관리”는 표준담당자가 신청된 코드를 검토하고, 실제로 시스템에 등록하며, 지속적으로 관리한다는 점을 명확히 한다.\n\n\n\n\n\n\n코드의 구성에 따른 유형으로는 단일, 계층, 목록, 복합코드가 있으며, 내용은 다음과 같다.\n\n\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n단일코드(S)\n• [코드값] + [코드내용]의 형태를 갖추는 가장 일반적인 형태의 코드로서 한 개의 코드로 Key가 구성됨• 단일코드의 코드값은 시스템에 등록/관리하며, 등록된 단일코드(코드명, 코드값, 코드값 한글정의)는 프로젝트 내 공통코드 테이블의 형태로 만들어져 전사공통으로 활용됨\n\n\n\n계층코드(C)\n• 하나 이상의 코드를 상속받거나 계층 구조를 통해 생성되어진 코드로 Key 가 구성된 경우• 대분류 / 중분류 / 소분류 와 같은 분류체계를 가짐\n\n\n\n목록코드(L)\n• 목록성코드는 코드명, 코드값, 코드한글정의 외에 부가적인 정보를 관리해야 하는 코드를 의미하며, 해당 업무팀에서 테이블의 형태로 관리한다\n\n\n\n복합코드(M)\n• 두개이상의 코드도메인을 하나의 코드도메인에서 활용하기 위하여 구성.• 복합코드는 단일코드의 코드도메인을 관리함.\n\n\n\n\n\n각 코드 유형(단일코드, 계층코드, 목록코드, 복합코드)의 특성과 용도를 명확히 구분하고 있다.\n예시\n\n단일코드(S)\n\n단일코드(S) 유형: 각 코드가 하나의 고유한 의미를 가진다.\n코드값(고객 구분 코드)은 숫자로 구성되어 있으며, 일반적으로 2자리 숫자를 사용한다.\n코드명(고객 구분 명)은 해당 코드의 의미를 명확하게 설명한다.\n사용 조건\n\n간단하고 평면적인 분류가 필요할 때\n코드 값과 의미가 1:1로 대응될 때\n코드의 수가 제한적이고 변경이 적을 때\n\n예시: 고객 구분 코드, 성별코드, 결혼여부코드, 직급코드\n\n\n\n\n고객 구분 코드\n고객 구분 명\n\n\n\n\n01\n개인\n\n\n02\n법인\n\n\n03\n개인사업자\n\n\n04\n외국인\n\n\n05\n공공기관\n\n\n06\n비영리단체\n\n\n07\nVIP\n\n\n08\n임직원\n\n\n09\n제휴사\n\n\n10\n기타\n\n\n\n계층코드(C)\n\n하나 이상의 코드를 상속받거나 계층 구조를 통해 생성되어진 코드로 Key 가 구성된 경우\n대분류 / 중분류 / 소분류 와 같은 분류체계를 가짐\n\n정규화: 각 분류 수준이 별도의 테이블로 분리되어 있어 데이터 중복이 최소화된다.\n참조 무결성: 외래 키 관계를 통해 데이터의 일관성이 유지된다.\n유연성: 각 분류 수준에서 독립적으로 항목을 추가, 수정, 삭제할 수 있다.\n확장성: 새로운 분류 항목을 쉽게 추가할 수 있다.\n\n각 분류 수준에 대한 추가 정보(예: 생성일, 수정일, 설명 등)를 쉽게 추가할 수 있는 장점이 있다.\n\n쿼리 효율성: 필요에 따라 조인을 통해 전체 계층 구조를 조회하거나, 특정 수준만 조회할 수 있다.\n\n사용 조건\n\n데이터가 계층적 구조를 가질 때\n상위 개념과 하위 개념의 관계를 표현해야 할 때\ndrill-down 분석이 필요한 경우\n예시: 조직코드, 상품분류코드, 지역코드\n\n\n대분류 코드 테이블:\n\n\n\n\n대분류 코드\n대분류명\n\n\n\n\nA\n전자제품\n\n\nB\n가전제품\n\n\nC\n의류\n\n\n\n\n중분류 코드 테이블:\n\n\n\n\n중분류 코드\n중분류명\n대분류 코드 (FK)\n\n\n\n\nA1\n컴퓨터\nA\n\n\nA2\n휴대폰\nA\n\n\nB1\n주방가전\nB\n\n\nB2\n생활가전\nB\n\n\nC1\n남성복\nC\n\n\nC2\n여성복\nC\n\n\n\n\n소분류 코드 테이블:\n\n\n\n\n소분류 코드\n소분류명\n대분류 코드 (FK)\n중분류 코드 (FK)\n\n\n\n\nA11\n데스크톱\nA\nA1\n\n\nA12\n노트북\nA\nA1\n\n\nA13\n태블릿\nA\nA1\n\n\nA21\n스마트폰\nA\nA2\n\n\nA22\n피처폰\nA\nA2\n\n\nB11\n냉장고\nB\nB1\n\n\nB12\n전자레인지\nB\nB1\n\n\nB21\n청소기\nB\nB2\n\n\nB22\n세탁기\nB\nB2\n\n\nC11\n셔츠\nC\nC1\n\n\nC12\n바지\nC\nC1\n\n\nC21\n원피스\nC\nC2\n\n\nC22\n스커트\nC\nC2\n\n\n\n\n목록코드(L)\n\n코드값(은행코드)과 코드명(은행명) 외에 여러 부가 정보를 포함한다.\n약칭, 영문명, 주소, 전화번호 등 해당 코드와 관련된 상세 정보를 관리한다.\n설립일과 같은 날짜 정보도 포함될 수 있다.\n사용여부와 같은 관리 정보도 포함될 수 있다.\n목록코드는 일반적으로 해당 업무팀에서 직접 관리하며, 시스템 전반에서 참조되어 사용됨\n\n목록코드의 장점\n\n상세 정보 관리: 코드와 관련된 다양한 부가 정보를 함께 관리할 수 있다.\n업무 특성 반영: 특정 업무 영역의 특성을 반영한 정보를 포함할 수 있다.\n데이터 일관성: 코드와 관련된 정보를 중앙에서 관리함으로써 데이터의 일관성을 유지할 수 있다.\n확장성: 필요에 따라 새로운 정보 항목을 쉽게 추가할 수 있다.\n\n\n사용 조건\n\n코드와 함께 추가적인 속성 정보가 필요할 때\n\n업무 요구사항 분석\n\n사용자나 부서가 코드 외에 추가 정보를 자주 요청하는 경우\n코드만으로는 업무 처리에 충분한 정보를 제공하지 못하는 경우\n\n데이터 활용도 검토\n\n보고서나 분석에서 코드 관련 부가 정보가 자주 필요한 경우\n데이터 조인이나 lookup 작업이 빈번하게 발생하는 경우\n\n시스템 통합 요구사항\n\n다른 시스템과 데이터를 교환할 때 코드 외 추가 정보가 필요한 경우\n외부 시스템이나 API가 코드와 관련된 부가 정보를 요구하는 경우\n\n변경 관리 필요성\n\n코드 값이 시간에 따라 변경되거나 이력 관리가 필요한 경우\n코드의 유효 기간이나 사용 상태를 관리해야 하는 경우\n\n복잡한 비즈니스 로직\n\n코드를 기반으로 복잡한 비즈니스 규칙이나 계산이 필요한 경우\n코드에 따라 다른 처리 로직이 적용되어야 하는 경우\n\n\n사용자 인터페이스 요구사항\n\n코드 선택 시 사용자에게 추가 정보를 제공해야 하는 경우\n코드 검색이나 필터링 시 다양한 기준이 필요한 경우\n\n감사 및 규제 요구사항\n\n코드 사용에 대한 상세한 이력이나 근거를 유지해야 하는 경우\n규제 준수를 위해 코드와 관련된 부가 정보를 관리해야 하는 경우\n\n\n코드 정보가 자주 변경되거나 확장될 가능성이 있을 때\n코드 정보가 특정 업무 영역에 국한되어 관리될 때\n\n예시: 은행코드, 국가코드, 통화코드\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n은행코드\n은행명\n약칭\n영문명\n본점주소\n대표전화\n설립일\n사용여부\n\n\n\n\n001\n한국은행\n한은\nBank of Korea\n서울특별시 중구 남대문로 39\n02-759-4114\n1950-06-12\nY\n\n\n002\n산업은행\n산은\nKorea Development Bank\n서울특별시 영등포구 은행로 14\n1588-1500\n1954-04-01\nY\n\n\n003\n기업은행\nIBK\nIndustrial Bank of Korea\n서울특별시 중구 을지로 79\n1566-2566\n1961-08-01\nY\n\n\n004\nKB국민은행\n국민\nKB Kookmin Bank\n서울특별시 영등포구 국제금융로8길 26\n1588-9999\n2001-11-01\nY\n\n\n005\n하나은행\n하나\nKEB Hana Bank\n서울특별시 중구 을지로 35\n1599-1111\n1967-01-30\nY\n\n\n007\n수협은행\n수협\nSuhyup Bank\n서울특별시 송파구 오금로 62\n1588-1515\n1962-04-01\nY\n\n\n\n복합코드 (M)\n\n코드 구성: 복합코드는 ’상품카테고리코드’와 ’지역코드’를 조합하여 만들어진다.\n의미 결합: 두 개의 단일 코드의 의미를 결합하여 새로운 의미를 만든다.\n추가 정보: 복합코드에는 단순히 두 코드를 붙인 것 외에도 추가적인 정보(설명, 담당부서, 적용일자 등)를 포함할 수 있다.\n유연성: 새로운 상품 카테고리나 지역이 추가될 때 쉽게 확장할 수 있다.\n복합코드의 장점\n\n데이터 압축: 여러 정보를 하나의 코드로 표현할 수 있다.\n의미 전달: 코드만으로도 여러 차원의 정보를 전달할 수 있다.\n유연한 확장: 기존 단일 코드 체계를 유지하면서 새로운 의미를 부여할 수 있다.\n\n데이터 분석: 복합코드를 분해하여 다양한 관점에서 데이터를 분석할 수 있다.\n\n이러한 복합코드는 조직의 복잡한 구조나 다차원적인 정보를 효율적으로 표현하고 관리하는 데 유용하다.\n\n이러한 목록코드는 일반적으로 해당 업무팀에서 직접 관리하며, 시스템 전반에서 참조되어 사용된다.\n사용 조건\n\n두 개 이상의 독립적인 코드 체계를 조합해야 할 때\n다차원적인 정보를 하나의 코드로 표현해야 할 때\n기존 코드 체계를 유지하면서 새로운 의미를 부여해야 할 때\n\n먼저, 각 단일 코드 도메인을 정의한다.\n예시: 지역별 상품코드, 부서별 프로젝트코드\n\n상품 카테고리 코드 (단일코드)\n\n\n\n\n코드\n카테고리명\n\n\n\n\nA\n전자제품\n\n\nB\n의류\n\n\nC\n식품\n\n\n\n\n지역 코드 (단일코드)\n\n\n\n\n코드\n지역명\n\n\n\n\n01\n서울\n\n\n02\n부산\n\n\n03\n대구\n\n\n\n\n지역별 상품 코드 (복합코드)\n\n\n\n\n복합코드\n상품카테고리코드\n지역코드\n설명\n담당부서\n적용일자\n\n\n\n\nA01\nA\n01\n서울 전자제품\n서울영업1팀\n2023-01-01\n\n\nA02\nA\n02\n부산 전자제품\n부산영업팀\n2023-01-01\n\n\nA03\nA\n03\n대구 전자제품\n대구영업팀\n2023-01-01\n\n\nB01\nB\n01\n서울 의류\n서울영업2팀\n2023-01-01\n\n\nB02\nB\n02\n부산 의류\n부산영업팀\n2023-01-01\n\n\nB03\nB\n03\n대구 의류\n대구영업팀\n2023-01-01\n\n\nC01\nC\n01\n서울 식품\n서울영업3팀\n2023-01-01\n\n\nC02\nC\n02\n부산 식품\n부산영업팀\n2023-01-01\n\n\nC03\nC\n03\n대구 식품\n대구영업팀\n2023-01-01\n\n\n\n\n\n선택 시 고려사항\n\n데이터의 구조: 데이터가 계층적인지, 평면적인지 파악\n확장성: 향후 코드 추가나 변경 가능성 고려\n사용 목적: 데이터 분석, 보고, 시스템 통합 등의 용도 파악\n관리 용이성: 코드 관리의 복잡성과 유지보수 고려\n업무 특성: 특정 업무 도메인의 요구사항 반영\n\n\n\n\n\n\n회사에서 사용하는 표준 코드의 기준 관리항목은 아래와 같다\n표준코드 관리항목 구성\n\n신규 모델링 단계에서 코드 값에 대한 신청은 오프라인으로 수행된다.\n변경 모델링 단계에서 코드 값에 대한 신청은 표준화 담당자를 통해 이루어진다.\n모든 코드는 코드 도메인과 매핑 관계를 가지며 ERP 공통 코드 테이블에 대한 데이터 SYNC 작업이 수행된다\n\n\n\n\n\n유형\n설명\n\n\n\n\n코드구분값\n• 코드목록값 혹은 계층코드일 경우 최상위 코드 목록값\n\n\n코드값\n• 코드 목록에 따른 코드 Value값\n\n\n코드명\n• 코드 도메인명과 동일함\n\n\n코드설명\n• 코드명 설명\n\n\n코드영문명\n• 코드 도메인 영문명과 동일함\n\n\n코드길이\n• 실제 코드 값의 길이\n\n\n코드구분\n• 단일코드/계층코드/목록성코드로 구분함\n\n\n업무구분\n• 코드에 대한 ownership을 가진 담당 업무영역\n\n\n상위코드값\n• 상위 코드 Value값\n\n\n상위코드구분값\n• 상위코드 목록값\n\n\n엔티티명\n• 목록성 코드인 경우 대상 엔티티명\n\n\n테이블명\n• 목록성 코드인 경우 대상 테이블명\n\n\n\n\n\n\n\n[주제어] + [코드 수식어 유형] + 코드 형태로 정의하여 사용한다.\n표준코드 구성 체계\n\n수식어 없이 코드용어 생성 가능\n\n\n\n\n\n\n\n\n\n\n\n분류\n유형\n설명\n예시\n\n\n\n\n기본\n유형\n어떤 비슷한 것들의 본질을 개체로서 나타낸 것, 또는 그것들의 공통되는 성질이나 모양을 정의할 때 사용되는 코드 유형\n거래 유형 코드\n\n\n기본\n분류\n코드 값을 체계화 하여 관리하는 경우 사용하며, 주로 대 / 중 / 소 / 세 등의 분류 체계를 갖는 코드에 대해서 ’분류’를 사용\n제품 소분류 코드\n\n\n기본\n종류\n가급적 사용을 제한하되 ’유형’이나 ’분류’의 사용 시 의미 전달이 모호해질 경우 혹은 통상적으로 사용되는 경우에 한해서 사용\n거래 종류 코드\n\n\n기본\n구분\n따로따로 갈라서 나누는 것으로 ’유형’보다는 단순하고 값의 종류가 10개 이내로 제한적이고 값의 범위가 명확한 경우 사용\n상품항목 구분 코드\n\n\n기본\n항목\n목록을 나열한 경우에 한해 사용\n점검 항목 코드\n\n\n확장\n사유\n인식 작용, 분석, 종합, 추리, 판단 등의 정신 작용에 대한 근거 및 동기\n취소 사유 코드\n\n\n확장\n상태\n사물이나 현상이 처해 있는 현재의 모양 또는 형편\n계약 상태 코드\n\n\n확장\n관계\n둘 이상의 사람, 사물, 현상 따위가 서로 관련을 맺거나 관련이 있음\n계약자 관계 코드\n\n\n확장\n용도\n사용되는 곳 혹은 사용되는 목적을 정의\n자금 용도 코드\n\n\n확장\n등급\n높고 낮음이나 좋고 나쁨 따위의 차이를 여러 층으로 구분한 단계\n차량 등급 코드\n\n\n확장\n지역\n전체 영역을 어떤 특징으로 나눈 일정한 공간 영역\n등록 지역 코드\n\n\n확장\n단위\n어떤 물리량(物理量)의 크기를 나타낼 때 비교의 기준 되는 크기\n회계 단위 코드\n\n\n\n\n\n\n\n코드값(인스턴스)을 부여하는 방식에 대한 4가지 분류가 있으며 코드의 형식을 결정할 수 있다\n계층 분류형(H)은 조직 구조와 같이 계층적인 관계를 표현하는 데 적합합니다.\n순차 채번형(S)은 순서가 있는 항목들을 나열할 때 유용합니다.\n표준약어 부여형(A)은 국제적으로 통용되는 표준 코드를 사용할 때 적합합니다.\n복합 분류형(C)은 계층 구조와 순차적 번호 부여가 동시에 필요한 경우에 사용됩니다.\n표준코드 구성 체계\n\n[A : Alphabet N : Numeric S : Sequence Number]\n\n\n\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n계층 분류형(H)\n• 대/중/소 등의 분류에 의한 구분이 필요한 경우 적용• 일반적으로 10진 분류 체계로 구성• 코드형식: N + N + N - NN(대분류) + NN(중분류) + NN(소분류) - NN(본/지점 분류) + NN(실/부 분류) + NN(팀 분류)\n[조직구분코드]100000: 본사총괄101000: 기획실101010: 회계팀101020: 자금팀\n\n\n순차 채번형(S)\n• 일련번호와 같이 순차적으로 번호를 부여하며 부여된 자리를 넘지 않도록 구성• 가능한 결번이 없도록 정의함. 코드 길이 만큼을 앞에 0을 채워서 번호 부여(숫자형 문자)• 코드 형식: SS\n[가족구분코드]01: 부02: 모03: 배우자99: 기타\n\n\n표준약어 부여형(A)\n• 대부분 국제 표준 코드 및 국가표준코드, 업종표준코드 등이 이에 속함.• 코드 형식: AAA\n[국가구분코드]CAN: 캐나다CHN: 중국\n\n\n복합 분류형(C)\n• 계층분류와 순차채번이 결합된 형태의 분류• 코드 형식: ASSSSS\n[담보구분코드]A00001: 건물A00002: 토지B00001: 예금\n\n\n\n\n\n\n\n표준코드 구성 체계\n기본 원칙\n\n원칙적으로 회사 렌터카 시스템 구축에서 사용하는 모든 코드는 통합 관리한다\n업무적으로 동일한 의미의 코드나 유사한 코드를 통합 후 표준화된 코드값과 코드내용을 부여한다\n목록성 코드의 경우 참조정보(DB명, 테이블명, 컬럼명) 만 관리하며 별도 코드값, 코드내용을 관리하지 않는다. 코드값과 코드내용 이외에 부가적인 정보가 존재하고, 코드에 따라 부가적인 정보의 개수가 다르기 때문에 표준 코드 테이블에서 관리하기 어렵기 때문이다\n\n코드값(인스턴스) 부여 원칙\n\n코드값의 부여는 원칙적으로 숫자형 문자 형태의 일련번호(01,02..)를 부여한다\n특별한 사유가 없는 한 현업에서 부여한 코드값을 최우선 사용함을 원칙으로 한다\n코드값 부여는 가능한 연속적으로 부여한다\n코드값 길이는 향후 확장성을 고려해서 부여한다\n숫자로만 이루어진 코드는 원칙적으로 허용하지 않으며 코드 길이만큼 숫자형 문자를 이용해서 ’0’을 채워서 코드를 부여한다\n‘기타’, ‘해당없음’ 등의 내용을 갖는 코드는 가급적 사용하지 않는 것을 원칙으로 하되, 반드시 사용해야 할 경우 해당 자리의 ‘00’, ‘99’ 등의 최대값을 이용한다\n’ 여부’, ’유무’의 모든 코드값은 ’Y＇과 ’N＇로 사용된다.\n\n코드값(인스턴스) 부여 원칙 예외\n\n기존 As-Is에서 특별한 의미를 가지는 코드 값으로 사용되었을 경우 그대로 채택한다\n외부에서 정의되어서 표준 약어로 널리 사용되는 있는 코드들은 표준화 대상에서 제외하며, 그대로 사용하도록 한다.\n\n국가구분코드 등"
  },
  {
    "objectID": "docs/blog/posts/Governance/6.data_registration.html",
    "href": "docs/blog/posts/Governance/6.data_registration.html",
    "title": "Data Governance Study - Data Standard Code",
    "section": "",
    "text": "데이터 표준 코드는 특정 개념이나 항목을 나타내기 위해 일관되게 사용되는 약속된 값의 집합이다.\n\n즉, 코드란 활용하고자 하는 데이터를 약어 혹은 기호로 함축하여 사용하는 데이터를 말한다.\n\n도메인의 한 유형으로서, 속성(컬럼)에 허용된 데이터 값을 제한된 범위 내에서 구체적으로 열거하여 정의한 것을 지칭한다\n이 데이터 값을 코드값 또는 코드 유효값이라 하며 각각의 코드값에는 의미를 부여한다. 이 의미를 ‘코드값명’ 또는 ’코드유효값정의’라 한다\n예시\n\n성별 코드: M (남성), F (여성)\n은행 코드: 004 (국민은행), 020 (우리은행)\n국가 코드: KR (대한민국), US (미국)\n\n코드명은 “국가코드”이다.\n코드값(코드 유효값)은 ISO 3166-1 alpha-2 표준을 따르는 2자리 국가 코드이다.\n코드값 명은 해당 국가의 한글 명칭이다.\n코드 유효값 정의(설명)는 모든 코드가 ISO 3166-1 alpha-2 기준을 따르며, 영문 대문자 2자리로 구성됨을 명시한다..\n\n\n\n\n\n\n\n\n\n\n\n코드명\n코드값\n코드값 명\n코드 유효값 정의\n\n\n\n\n국가코드\nKR\n대한민국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nUS\n미국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nJP\n일본\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nCN\n중국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nGB\n영국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nDE\n독일\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nFR\n프랑스\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n\n\n\n\n\n데이터의 일관성 유지\n시스템 간 데이터 교환 용이\n데이터 해석의 명확성 제공\n\n전사적으로 표준코드를 사용하도록 함으로써 업무영역간 운영데이터의 불일치를 방지하고 데이터의 정합성을 향상 시킨다.\n\n데이터 입력 오류 감소\n\n\n\n\n\n고유성: 각 코드는 유일한 의미를 가짐\n\n코드의 중복방지(유사한 코드 검토 포함) 통해 효율적 운영이 가능해야 한다\n\n간결성: 일반적으로 짧고 간단한 형태\n체계성: 논리적인 구조를 가짐\n확장성: 새로운 항목 추가가 가능한 구조\n\n\n\n\n\n코드 값: 실제 사용되는 코드\n코드 명: 코드가 나타내는 항목의 이름\n설명: 코드의 의미나 사용 목적\n유효 기간: 코드의 사용 가능 기간\n\n\n\n\n\n코드 관리 시스템 구축\n정기적인 검토 및 업데이트\n코드 변경 이력 관리\n\n\n\n\n\n업계 표준이나 국제 표준 고려\n코드의 의미가 시간이 지나도 변하지 않도록 설계\n코드 체계의 일관성 유지\n\n\n\n\n\n표준코드 작성 시 의미를 충분히 파악할 수 있도록 작성을 하며 구성은 표준용어 작성 기준 및 관리원칙을 따른다.\n\n\n\n\n\n\n\n\n\n순번\n코드 표준화 대상 및 관리 원칙\n비고\n\n\n\n\n1\n• 코드 정보가 저장되며 코드 테이블에 그 내용이 존재하는 경우\n표준화 대상\n\n\n2\n• 애플리케이션 내부에 코드의 실제 내용이 존재하는 경우\n표준화 대상\n\n\n3\n• 사용 가능한 데이터의 종류가 2개 이상인 경우\n표준화 대상\n\n\n4\n• Yes or No 값 외에 미확정 값(Null)을 가질 수 있는 경우\n표준화 대상\n\n\n5\n• 현재는 Yes or No 처럼 Boolean값을 갖지만 추후 그 이외의 데이터가 추가 될 가능성이 있는 경우\n표준화 대상\n\n\n6\n• Yes or No 처럼 Boolean 값만을 데이터로 가질 경우 ‘Y’/’N’으로 통일 함\n관리원칙\n\n\n7\n• 표준코드도메인은 관용적으로 사용하는 용어를 우선적으로 사용한다\n관리원칙\n\n\n8\n• 표준코드를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n관리원칙\n\n\n9\n• 단일 코드는 하나의 공통 엔티티로 관리한다.\n관리원칙\n\n\n10\n• 계층코드는 내용을 분석하여 단일코드 형태로 변경 조정 할 수 있다.\n관리원칙\n\n\n11\n• 목록성 코드의 인스턴스 값은 각각 별도의 엔티티로 관리하며 공통코드 엔티티에서는 해당 코드값을 관리하는 테이블 정보를 관리한다.\n관리원칙\n\n\n\n\n\n\n\n표준코드는 신규 모델링 시 데이터 모델 관리자 또는 응용팀에서 도출 신청 후 데이터 표준 담당자가 최종 관리한다. 죄송합니다. 제가 이해를 돕기 위해 추가 설명을 드리겠습니다. 귀하께서 언급하신 내용은 코드 관리 프로세스의 중요한 부분을 강조하고 있습니다. 이를 반영하여 테이블을 수정해 보겠습니다.\n\n\n\n\n\n\n\n\n\n\n순번\n담당\n코드 표준 관리 담당 별 역할\n비고\n\n\n\n\n1\n응용팀\n• 기능 정의시 데이터 항목에 코드가 필요한 경우 모델러와 협의• 목록성 코드에 대한 요건 제시• 각 업무영역별로 생성된 목록성 코드에 대한 코드값 관리\n운영시 코드 신청은 업무담당자(현업)가 수행함현업: 코드 신청, 활용\n\n\n2\n데이터 모델관리자\n• 단일코드, 계층코드 등 공통코드를 관리하기 위한 테이블 설계• 업무영역별 목록성 코드 테이블 설계\n\n\n\n3\n표준담당자\n• 단일코드/계층코드 신청을 위한 템플릿 제공• 코드명에 대한 표준 준수 검증• 코드 취합/조정 및 공통코드 확정• 코드 중복 조정 작업 수행(인스턴스명 간 유사성 검증)• 데이터 타입 검증(코드 도메인화)• AS-IS 코드와 매핑 정보 관리• 코드 등록, 공통 코드 및 코드 도메인 관리(메타시스템 or Excel)• 신규 코드 생성 및 AS-IS 코드의 코드 값에 대한 재정비 수행• 코드에 대한 Ownership 관리 및 승인\n표준담당자: 코드 등록 및 관리\n\n\n\n\n응용팀(현업)은 업무 수행 중 필요한 코드를 식별하고 신청\n\n응용팀(현업)의 비고 항목에 “현업: 코드 신청, 활용”은 실제 업무를 수행하는 현업 담당자가 코드를 신청하고 사용한다\n\n표준담당자는 이 신청을 검토하고, 적절한 경우 코드를 등록하며, 전체적인 코드 체계를 관리한다.\n\n표준담당자의 비고 항목에 “표준담당자: 코드 등록 및 관리”는 표준담당자가 신청된 코드를 검토하고, 실제로 시스템에 등록하며, 지속적으로 관리한다는 점을 명확히 한다.\n\n\n\n\n\n\n코드의 구성에 따른 유형으로는 단일, 계층, 목록, 복합코드가 있으며, 내용은 다음과 같다.\n\n\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n단일코드(S)\n• [코드값] + [코드내용]의 형태를 갖추는 가장 일반적인 형태의 코드로서 한 개의 코드로 Key가 구성됨• 단일코드의 코드값은 시스템에 등록/관리하며, 등록된 단일코드(코드명, 코드값, 코드값 한글정의)는 프로젝트 내 공통코드 테이블의 형태로 만들어져 전사공통으로 활용됨\n\n\n\n계층코드(C)\n• 하나 이상의 코드를 상속받거나 계층 구조를 통해 생성되어진 코드로 Key 가 구성된 경우• 대분류 / 중분류 / 소분류 와 같은 분류체계를 가짐\n\n\n\n목록코드(L)\n• 목록성코드는 코드명, 코드값, 코드한글정의 외에 부가적인 정보를 관리해야 하는 코드를 의미하며, 해당 업무팀에서 테이블의 형태로 관리한다\n\n\n\n복합코드(M)\n• 두개이상의 코드도메인을 하나의 코드도메인에서 활용하기 위하여 구성.• 복합코드는 단일코드의 코드도메인을 관리함.\n\n\n\n\n\n각 코드 유형(단일코드, 계층코드, 목록코드, 복합코드)의 특성과 용도를 명확히 구분하고 있다.\n예시\n\n단일코드(S)\n\n단일코드(S) 유형: 각 코드가 하나의 고유한 의미를 가진다.\n코드값(고객 구분 코드)은 숫자로 구성되어 있으며, 일반적으로 2자리 숫자를 사용한다.\n코드명(고객 구분 명)은 해당 코드의 의미를 명확하게 설명한다.\n사용 조건\n\n간단하고 평면적인 분류가 필요할 때\n코드 값과 의미가 1:1로 대응될 때\n코드의 수가 제한적이고 변경이 적을 때\n\n예시: 고객 구분 코드, 성별코드, 결혼여부코드, 직급코드\n\n\n\n\n고객 구분 코드\n고객 구분 명\n\n\n\n\n01\n개인\n\n\n02\n법인\n\n\n03\n개인사업자\n\n\n04\n외국인\n\n\n05\n공공기관\n\n\n06\n비영리단체\n\n\n07\nVIP\n\n\n08\n임직원\n\n\n09\n제휴사\n\n\n10\n기타\n\n\n\n계층코드(C)\n\n하나 이상의 코드를 상속받거나 계층 구조를 통해 생성되어진 코드로 Key 가 구성된 경우\n대분류 / 중분류 / 소분류 와 같은 분류체계를 가짐\n\n정규화: 각 분류 수준이 별도의 테이블로 분리되어 있어 데이터 중복이 최소화된다.\n참조 무결성: 외래 키 관계를 통해 데이터의 일관성이 유지된다.\n유연성: 각 분류 수준에서 독립적으로 항목을 추가, 수정, 삭제할 수 있다.\n확장성: 새로운 분류 항목을 쉽게 추가할 수 있다.\n\n각 분류 수준에 대한 추가 정보(예: 생성일, 수정일, 설명 등)를 쉽게 추가할 수 있는 장점이 있다.\n\n쿼리 효율성: 필요에 따라 조인을 통해 전체 계층 구조를 조회하거나, 특정 수준만 조회할 수 있다.\n\n사용 조건\n\n데이터가 계층적 구조를 가질 때\n상위 개념과 하위 개념의 관계를 표현해야 할 때\ndrill-down 분석이 필요한 경우\n예시: 조직코드, 상품분류코드, 지역코드\n\n\n대분류 코드 테이블:\n\n\n\n\n대분류 코드\n대분류명\n\n\n\n\nA\n전자제품\n\n\nB\n가전제품\n\n\nC\n의류\n\n\n\n\n중분류 코드 테이블:\n\n\n\n\n중분류 코드\n중분류명\n대분류 코드 (FK)\n\n\n\n\nA1\n컴퓨터\nA\n\n\nA2\n휴대폰\nA\n\n\nB1\n주방가전\nB\n\n\nB2\n생활가전\nB\n\n\nC1\n남성복\nC\n\n\nC2\n여성복\nC\n\n\n\n\n소분류 코드 테이블:\n\n\n\n\n소분류 코드\n소분류명\n대분류 코드 (FK)\n중분류 코드 (FK)\n\n\n\n\nA11\n데스크톱\nA\nA1\n\n\nA12\n노트북\nA\nA1\n\n\nA13\n태블릿\nA\nA1\n\n\nA21\n스마트폰\nA\nA2\n\n\nA22\n피처폰\nA\nA2\n\n\nB11\n냉장고\nB\nB1\n\n\nB12\n전자레인지\nB\nB1\n\n\nB21\n청소기\nB\nB2\n\n\nB22\n세탁기\nB\nB2\n\n\nC11\n셔츠\nC\nC1\n\n\nC12\n바지\nC\nC1\n\n\nC21\n원피스\nC\nC2\n\n\nC22\n스커트\nC\nC2\n\n\n\n\n목록코드(L)\n\n코드값(은행코드)과 코드명(은행명) 외에 여러 부가 정보를 포함한다.\n약칭, 영문명, 주소, 전화번호 등 해당 코드와 관련된 상세 정보를 관리한다.\n설립일과 같은 날짜 정보도 포함될 수 있다.\n사용여부와 같은 관리 정보도 포함될 수 있다.\n목록코드는 일반적으로 해당 업무팀에서 직접 관리하며, 시스템 전반에서 참조되어 사용됨\n\n목록코드의 장점\n\n상세 정보 관리: 코드와 관련된 다양한 부가 정보를 함께 관리할 수 있다.\n업무 특성 반영: 특정 업무 영역의 특성을 반영한 정보를 포함할 수 있다.\n데이터 일관성: 코드와 관련된 정보를 중앙에서 관리함으로써 데이터의 일관성을 유지할 수 있다.\n확장성: 필요에 따라 새로운 정보 항목을 쉽게 추가할 수 있다.\n\n\n사용 조건\n\n코드와 함께 추가적인 속성 정보가 필요할 때\n\n업무 요구사항 분석\n\n사용자나 부서가 코드 외에 추가 정보를 자주 요청하는 경우\n코드만으로는 업무 처리에 충분한 정보를 제공하지 못하는 경우\n\n데이터 활용도 검토\n\n보고서나 분석에서 코드 관련 부가 정보가 자주 필요한 경우\n데이터 조인이나 lookup 작업이 빈번하게 발생하는 경우\n\n시스템 통합 요구사항\n\n다른 시스템과 데이터를 교환할 때 코드 외 추가 정보가 필요한 경우\n외부 시스템이나 API가 코드와 관련된 부가 정보를 요구하는 경우\n\n변경 관리 필요성\n\n코드 값이 시간에 따라 변경되거나 이력 관리가 필요한 경우\n코드의 유효 기간이나 사용 상태를 관리해야 하는 경우\n\n복잡한 비즈니스 로직\n\n코드를 기반으로 복잡한 비즈니스 규칙이나 계산이 필요한 경우\n코드에 따라 다른 처리 로직이 적용되어야 하는 경우\n\n\n사용자 인터페이스 요구사항\n\n코드 선택 시 사용자에게 추가 정보를 제공해야 하는 경우\n코드 검색이나 필터링 시 다양한 기준이 필요한 경우\n\n감사 및 규제 요구사항\n\n코드 사용에 대한 상세한 이력이나 근거를 유지해야 하는 경우\n규제 준수를 위해 코드와 관련된 부가 정보를 관리해야 하는 경우\n\n\n코드 정보가 자주 변경되거나 확장될 가능성이 있을 때\n코드 정보가 특정 업무 영역에 국한되어 관리될 때\n\n예시: 은행코드, 국가코드, 통화코드\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n은행코드\n은행명\n약칭\n영문명\n본점주소\n대표전화\n설립일\n사용여부\n\n\n\n\n001\n한국은행\n한은\nBank of Korea\n서울특별시 중구 남대문로 39\n02-759-4114\n1950-06-12\nY\n\n\n002\n산업은행\n산은\nKorea Development Bank\n서울특별시 영등포구 은행로 14\n1588-1500\n1954-04-01\nY\n\n\n003\n기업은행\nIBK\nIndustrial Bank of Korea\n서울특별시 중구 을지로 79\n1566-2566\n1961-08-01\nY\n\n\n004\nKB국민은행\n국민\nKB Kookmin Bank\n서울특별시 영등포구 국제금융로8길 26\n1588-9999\n2001-11-01\nY\n\n\n005\n하나은행\n하나\nKEB Hana Bank\n서울특별시 중구 을지로 35\n1599-1111\n1967-01-30\nY\n\n\n007\n수협은행\n수협\nSuhyup Bank\n서울특별시 송파구 오금로 62\n1588-1515\n1962-04-01\nY\n\n\n\n복합코드 (M)\n\n코드 구성: 복합코드는 ’상품카테고리코드’와 ’지역코드’를 조합하여 만들어진다.\n의미 결합: 두 개의 단일 코드의 의미를 결합하여 새로운 의미를 만든다.\n추가 정보: 복합코드에는 단순히 두 코드를 붙인 것 외에도 추가적인 정보(설명, 담당부서, 적용일자 등)를 포함할 수 있다.\n유연성: 새로운 상품 카테고리나 지역이 추가될 때 쉽게 확장할 수 있다.\n복합코드의 장점\n\n데이터 압축: 여러 정보를 하나의 코드로 표현할 수 있다.\n의미 전달: 코드만으로도 여러 차원의 정보를 전달할 수 있다.\n유연한 확장: 기존 단일 코드 체계를 유지하면서 새로운 의미를 부여할 수 있다.\n\n데이터 분석: 복합코드를 분해하여 다양한 관점에서 데이터를 분석할 수 있다.\n\n이러한 복합코드는 조직의 복잡한 구조나 다차원적인 정보를 효율적으로 표현하고 관리하는 데 유용하다.\n\n이러한 목록코드는 일반적으로 해당 업무팀에서 직접 관리하며, 시스템 전반에서 참조되어 사용된다.\n사용 조건\n\n두 개 이상의 독립적인 코드 체계를 조합해야 할 때\n다차원적인 정보를 하나의 코드로 표현해야 할 때\n기존 코드 체계를 유지하면서 새로운 의미를 부여해야 할 때\n\n먼저, 각 단일 코드 도메인을 정의한다.\n예시: 지역별 상품코드, 부서별 프로젝트코드\n\n상품 카테고리 코드 (단일코드)\n\n\n\n\n코드\n카테고리명\n\n\n\n\nA\n전자제품\n\n\nB\n의류\n\n\nC\n식품\n\n\n\n\n지역 코드 (단일코드)\n\n\n\n\n코드\n지역명\n\n\n\n\n01\n서울\n\n\n02\n부산\n\n\n03\n대구\n\n\n\n\n지역별 상품 코드 (복합코드)\n\n\n\n\n복합코드\n상품카테고리코드\n지역코드\n설명\n담당부서\n적용일자\n\n\n\n\nA01\nA\n01\n서울 전자제품\n서울영업1팀\n2023-01-01\n\n\nA02\nA\n02\n부산 전자제품\n부산영업팀\n2023-01-01\n\n\nA03\nA\n03\n대구 전자제품\n대구영업팀\n2023-01-01\n\n\nB01\nB\n01\n서울 의류\n서울영업2팀\n2023-01-01\n\n\nB02\nB\n02\n부산 의류\n부산영업팀\n2023-01-01\n\n\nB03\nB\n03\n대구 의류\n대구영업팀\n2023-01-01\n\n\nC01\nC\n01\n서울 식품\n서울영업3팀\n2023-01-01\n\n\nC02\nC\n02\n부산 식품\n부산영업팀\n2023-01-01\n\n\nC03\nC\n03\n대구 식품\n대구영업팀\n2023-01-01\n\n\n\n\n\n선택 시 고려사항\n\n데이터의 구조: 데이터가 계층적인지, 평면적인지 파악\n확장성: 향후 코드 추가나 변경 가능성 고려\n사용 목적: 데이터 분석, 보고, 시스템 통합 등의 용도 파악\n관리 용이성: 코드 관리의 복잡성과 유지보수 고려\n업무 특성: 특정 업무 도메인의 요구사항 반영\n\n\n\n\n\n\n회사에서 사용하는 표준 코드의 기준 관리항목은 아래와 같다\n표준코드 관리항목 구성\n\n신규 모델링 단계에서 코드 값에 대한 신청은 오프라인으로 수행된다.\n변경 모델링 단계에서 코드 값에 대한 신청은 표준화 담당자를 통해 이루어진다.\n모든 코드는 코드 도메인과 매핑 관계를 가지며 ERP 공통 코드 테이블에 대한 데이터 SYNC 작업이 수행된다\n\n\n\n\n\n유형\n설명\n\n\n\n\n코드구분값\n• 코드목록값 혹은 계층코드일 경우 최상위 코드 목록값\n\n\n코드값\n• 코드 목록에 따른 코드 Value값\n\n\n코드명\n• 코드 도메인명과 동일함\n\n\n코드설명\n• 코드명 설명\n\n\n코드영문명\n• 코드 도메인 영문명과 동일함\n\n\n코드길이\n• 실제 코드 값의 길이\n\n\n코드구분\n• 단일코드/계층코드/목록성코드로 구분함\n\n\n업무구분\n• 코드에 대한 ownership을 가진 담당 업무영역\n\n\n상위코드값\n• 상위 코드 Value값\n\n\n상위코드구분값\n• 상위코드 목록값\n\n\n엔티티명\n• 목록성 코드인 경우 대상 엔티티명\n\n\n테이블명\n• 목록성 코드인 경우 대상 테이블명\n\n\n\n\n\n\n\n[주제어] + [코드 수식어 유형] + 코드 형태로 정의하여 사용한다.\n표준코드 구성 체계\n\n수식어 없이 코드용어 생성 가능\n\n\n\n\n\n\n\n\n\n\n\n분류\n유형\n설명\n예시\n\n\n\n\n기본\n유형\n어떤 비슷한 것들의 본질을 개체로서 나타낸 것, 또는 그것들의 공통되는 성질이나 모양을 정의할 때 사용되는 코드 유형\n거래 유형 코드\n\n\n기본\n분류\n코드 값을 체계화 하여 관리하는 경우 사용하며, 주로 대 / 중 / 소 / 세 등의 분류 체계를 갖는 코드에 대해서 ’분류’를 사용\n제품 소분류 코드\n\n\n기본\n종류\n가급적 사용을 제한하되 ’유형’이나 ’분류’의 사용 시 의미 전달이 모호해질 경우 혹은 통상적으로 사용되는 경우에 한해서 사용\n거래 종류 코드\n\n\n기본\n구분\n따로따로 갈라서 나누는 것으로 ’유형’보다는 단순하고 값의 종류가 10개 이내로 제한적이고 값의 범위가 명확한 경우 사용\n상품항목 구분 코드\n\n\n기본\n항목\n목록을 나열한 경우에 한해 사용\n점검 항목 코드\n\n\n확장\n사유\n인식 작용, 분석, 종합, 추리, 판단 등의 정신 작용에 대한 근거 및 동기\n취소 사유 코드\n\n\n확장\n상태\n사물이나 현상이 처해 있는 현재의 모양 또는 형편\n계약 상태 코드\n\n\n확장\n관계\n둘 이상의 사람, 사물, 현상 따위가 서로 관련을 맺거나 관련이 있음\n계약자 관계 코드\n\n\n확장\n용도\n사용되는 곳 혹은 사용되는 목적을 정의\n자금 용도 코드\n\n\n확장\n등급\n높고 낮음이나 좋고 나쁨 따위의 차이를 여러 층으로 구분한 단계\n차량 등급 코드\n\n\n확장\n지역\n전체 영역을 어떤 특징으로 나눈 일정한 공간 영역\n등록 지역 코드\n\n\n확장\n단위\n어떤 물리량(物理量)의 크기를 나타낼 때 비교의 기준 되는 크기\n회계 단위 코드\n\n\n\n\n\n\n\n코드값(인스턴스)을 부여하는 방식에 대한 4가지 분류가 있으며 코드의 형식을 결정할 수 있다\n계층 분류형(H)은 조직 구조와 같이 계층적인 관계를 표현하는 데 적합합니다.\n순차 채번형(S)은 순서가 있는 항목들을 나열할 때 유용합니다.\n표준약어 부여형(A)은 국제적으로 통용되는 표준 코드를 사용할 때 적합합니다.\n복합 분류형(C)은 계층 구조와 순차적 번호 부여가 동시에 필요한 경우에 사용됩니다.\n표준코드 구성 체계\n\n[A : Alphabet N : Numeric S : Sequence Number]\n\n\n\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n계층 분류형(H)\n• 대/중/소 등의 분류에 의한 구분이 필요한 경우 적용• 일반적으로 10진 분류 체계로 구성• 코드형식: N + N + N - NN(대분류) + NN(중분류) + NN(소분류) - NN(본/지점 분류) + NN(실/부 분류) + NN(팀 분류)\n[조직구분코드]100000: 본사총괄101000: 기획실101010: 회계팀101020: 자금팀\n\n\n순차 채번형(S)\n• 일련번호와 같이 순차적으로 번호를 부여하며 부여된 자리를 넘지 않도록 구성• 가능한 결번이 없도록 정의함. 코드 길이 만큼을 앞에 0을 채워서 번호 부여(숫자형 문자)• 코드 형식: SS\n[가족구분코드]01: 부02: 모03: 배우자99: 기타\n\n\n표준약어 부여형(A)\n• 대부분 국제 표준 코드 및 국가표준코드, 업종표준코드 등이 이에 속함.• 코드 형식: AAA\n[국가구분코드]CAN: 캐나다CHN: 중국\n\n\n복합 분류형(C)\n• 계층분류와 순차채번이 결합된 형태의 분류• 코드 형식: ASSSSS\n[담보구분코드]A00001: 건물A00002: 토지B00001: 예금\n\n\n\n\n\n\n\n표준코드 구성 체계\n기본 원칙\n\n원칙적으로 회사 렌터카 시스템 구축에서 사용하는 모든 코드는 통합 관리한다\n업무적으로 동일한 의미의 코드나 유사한 코드를 통합 후 표준화된 코드값과 코드내용을 부여한다\n목록성 코드의 경우 참조정보(DB명, 테이블명, 컬럼명) 만 관리하며 별도 코드값, 코드내용을 관리하지 않는다. 코드값과 코드내용 이외에 부가적인 정보가 존재하고, 코드에 따라 부가적인 정보의 개수가 다르기 때문에 표준 코드 테이블에서 관리하기 어렵기 때문이다\n\n코드값(인스턴스) 부여 원칙\n\n코드값의 부여는 원칙적으로 숫자형 문자 형태의 일련번호(01,02..)를 부여한다\n특별한 사유가 없는 한 현업에서 부여한 코드값을 최우선 사용함을 원칙으로 한다\n코드값 부여는 가능한 연속적으로 부여한다\n코드값 길이는 향후 확장성을 고려해서 부여한다\n숫자로만 이루어진 코드는 원칙적으로 허용하지 않으며 코드 길이만큼 숫자형 문자를 이용해서 ’0’을 채워서 코드를 부여한다\n‘기타’, ‘해당없음’ 등의 내용을 갖는 코드는 가급적 사용하지 않는 것을 원칙으로 하되, 반드시 사용해야 할 경우 해당 자리의 ‘00’, ‘99’ 등의 최대값을 이용한다\n’ 여부’, ’유무’의 모든 코드값은 ’Y＇과 ’N＇로 사용된다.\n\n코드값(인스턴스) 부여 원칙 예외\n\n기존 As-Is에서 특별한 의미를 가지는 코드 값으로 사용되었을 경우 그대로 채택한다\n외부에서 정의되어서 표준 약어로 널리 사용되는 있는 코드들은 표준화 대상에서 제외하며, 그대로 사용하도록 한다.\n\n국가구분코드 등"
  },
  {
    "objectID": "docs/blog/posts/Governance/6.data_registration.html#데이터-표준-코드-사전이란",
    "href": "docs/blog/posts/Governance/6.data_registration.html#데이터-표준-코드-사전이란",
    "title": "Data Governance Study - Data Standard Code",
    "section": "",
    "text": "데이터 표준 코드는 특정 개념이나 항목을 나타내기 위해 일관되게 사용되는 약속된 값의 집합이다.\n\n즉, 코드란 활용하고자 하는 데이터를 약어 혹은 기호로 함축하여 사용하는 데이터를 말한다.\n\n도메인의 한 유형으로서, 속성(컬럼)에 허용된 데이터 값을 제한된 범위 내에서 구체적으로 열거하여 정의한 것을 지칭한다\n이 데이터 값을 코드값 또는 코드 유효값이라 하며 각각의 코드값에는 의미를 부여한다. 이 의미를 ‘코드값명’ 또는 ’코드유효값정의’라 한다\n예시\n\n성별 코드: M (남성), F (여성)\n은행 코드: 004 (국민은행), 020 (우리은행)\n국가 코드: KR (대한민국), US (미국)\n\n코드명은 “국가코드”이다.\n코드값(코드 유효값)은 ISO 3166-1 alpha-2 표준을 따르는 2자리 국가 코드이다.\n코드값 명은 해당 국가의 한글 명칭이다.\n코드 유효값 정의(설명)는 모든 코드가 ISO 3166-1 alpha-2 기준을 따르며, 영문 대문자 2자리로 구성됨을 명시한다..\n\n\n\n\n\n\n\n\n\n\n\n코드명\n코드값\n코드값 명\n코드 유효값 정의\n\n\n\n\n국가코드\nKR\n대한민국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nUS\n미국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nJP\n일본\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nCN\n중국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nGB\n영국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nDE\n독일\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nFR\n프랑스\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n\n\n\n\n\n데이터의 일관성 유지\n시스템 간 데이터 교환 용이\n데이터 해석의 명확성 제공\n\n전사적으로 표준코드를 사용하도록 함으로써 업무영역간 운영데이터의 불일치를 방지하고 데이터의 정합성을 향상 시킨다.\n\n데이터 입력 오류 감소\n\n\n\n\n\n고유성: 각 코드는 유일한 의미를 가짐\n\n코드의 중복방지(유사한 코드 검토 포함) 통해 효율적 운영이 가능해야 한다\n\n간결성: 일반적으로 짧고 간단한 형태\n체계성: 논리적인 구조를 가짐\n확장성: 새로운 항목 추가가 가능한 구조\n\n\n\n\n\n코드 값: 실제 사용되는 코드\n코드 명: 코드가 나타내는 항목의 이름\n설명: 코드의 의미나 사용 목적\n유효 기간: 코드의 사용 가능 기간\n\n\n\n\n\n코드 관리 시스템 구축\n정기적인 검토 및 업데이트\n코드 변경 이력 관리\n\n\n\n\n\n업계 표준이나 국제 표준 고려\n코드의 의미가 시간이 지나도 변하지 않도록 설계\n코드 체계의 일관성 유지\n\n\n\n\n\n표준코드 작성 시 의미를 충분히 파악할 수 있도록 작성을 하며 구성은 표준용어 작성 기준 및 관리원칙을 따른다.\n\n\n\n\n\n\n\n\n\n순번\n코드 표준화 대상 및 관리 원칙\n비고\n\n\n\n\n1\n• 코드 정보가 저장되며 코드 테이블에 그 내용이 존재하는 경우\n표준화 대상\n\n\n2\n• 애플리케이션 내부에 코드의 실제 내용이 존재하는 경우\n표준화 대상\n\n\n3\n• 사용 가능한 데이터의 종류가 2개 이상인 경우\n표준화 대상\n\n\n4\n• Yes or No 값 외에 미확정 값(Null)을 가질 수 있는 경우\n표준화 대상\n\n\n5\n• 현재는 Yes or No 처럼 Boolean값을 갖지만 추후 그 이외의 데이터가 추가 될 가능성이 있는 경우\n표준화 대상\n\n\n6\n• Yes or No 처럼 Boolean 값만을 데이터로 가질 경우 ‘Y’/’N’으로 통일 함\n관리원칙\n\n\n7\n• 표준코드도메인은 관용적으로 사용하는 용어를 우선적으로 사용한다\n관리원칙\n\n\n8\n• 표준코드를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n관리원칙\n\n\n9\n• 단일 코드는 하나의 공통 엔티티로 관리한다.\n관리원칙\n\n\n10\n• 계층코드는 내용을 분석하여 단일코드 형태로 변경 조정 할 수 있다.\n관리원칙\n\n\n11\n• 목록성 코드의 인스턴스 값은 각각 별도의 엔티티로 관리하며 공통코드 엔티티에서는 해당 코드값을 관리하는 테이블 정보를 관리한다.\n관리원칙\n\n\n\n\n\n\n\n표준코드는 신규 모델링 시 데이터 모델 관리자 또는 응용팀에서 도출 신청 후 데이터 표준 담당자가 최종 관리한다. 죄송합니다. 제가 이해를 돕기 위해 추가 설명을 드리겠습니다. 귀하께서 언급하신 내용은 코드 관리 프로세스의 중요한 부분을 강조하고 있습니다. 이를 반영하여 테이블을 수정해 보겠습니다.\n\n\n\n\n\n\n\n\n\n\n순번\n담당\n코드 표준 관리 담당 별 역할\n비고\n\n\n\n\n1\n응용팀\n• 기능 정의시 데이터 항목에 코드가 필요한 경우 모델러와 협의• 목록성 코드에 대한 요건 제시• 각 업무영역별로 생성된 목록성 코드에 대한 코드값 관리\n운영시 코드 신청은 업무담당자(현업)가 수행함현업: 코드 신청, 활용\n\n\n2\n데이터 모델관리자\n• 단일코드, 계층코드 등 공통코드를 관리하기 위한 테이블 설계• 업무영역별 목록성 코드 테이블 설계\n\n\n\n3\n표준담당자\n• 단일코드/계층코드 신청을 위한 템플릿 제공• 코드명에 대한 표준 준수 검증• 코드 취합/조정 및 공통코드 확정• 코드 중복 조정 작업 수행(인스턴스명 간 유사성 검증)• 데이터 타입 검증(코드 도메인화)• AS-IS 코드와 매핑 정보 관리• 코드 등록, 공통 코드 및 코드 도메인 관리(메타시스템 or Excel)• 신규 코드 생성 및 AS-IS 코드의 코드 값에 대한 재정비 수행• 코드에 대한 Ownership 관리 및 승인\n표준담당자: 코드 등록 및 관리\n\n\n\n\n응용팀(현업)은 업무 수행 중 필요한 코드를 식별하고 신청\n\n응용팀(현업)의 비고 항목에 “현업: 코드 신청, 활용”은 실제 업무를 수행하는 현업 담당자가 코드를 신청하고 사용한다\n\n표준담당자는 이 신청을 검토하고, 적절한 경우 코드를 등록하며, 전체적인 코드 체계를 관리한다.\n\n표준담당자의 비고 항목에 “표준담당자: 코드 등록 및 관리”는 표준담당자가 신청된 코드를 검토하고, 실제로 시스템에 등록하며, 지속적으로 관리한다는 점을 명확히 한다.\n\n\n\n\n\n\n코드의 구성에 따른 유형으로는 단일, 계층, 목록, 복합코드가 있으며, 내용은 다음과 같다.\n\n\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n단일코드(S)\n• [코드값] + [코드내용]의 형태를 갖추는 가장 일반적인 형태의 코드로서 한 개의 코드로 Key가 구성됨• 단일코드의 코드값은 시스템에 등록/관리하며, 등록된 단일코드(코드명, 코드값, 코드값 한글정의)는 프로젝트 내 공통코드 테이블의 형태로 만들어져 전사공통으로 활용됨\n\n\n\n계층코드(C)\n• 하나 이상의 코드를 상속받거나 계층 구조를 통해 생성되어진 코드로 Key 가 구성된 경우• 대분류 / 중분류 / 소분류 와 같은 분류체계를 가짐\n\n\n\n목록코드(L)\n• 목록성코드는 코드명, 코드값, 코드한글정의 외에 부가적인 정보를 관리해야 하는 코드를 의미하며, 해당 업무팀에서 테이블의 형태로 관리한다\n\n\n\n복합코드(M)\n• 두개이상의 코드도메인을 하나의 코드도메인에서 활용하기 위하여 구성.• 복합코드는 단일코드의 코드도메인을 관리함.\n\n\n\n\n\n각 코드 유형(단일코드, 계층코드, 목록코드, 복합코드)의 특성과 용도를 명확히 구분하고 있다.\n예시\n\n단일코드(S)\n\n단일코드(S) 유형: 각 코드가 하나의 고유한 의미를 가진다.\n코드값(고객 구분 코드)은 숫자로 구성되어 있으며, 일반적으로 2자리 숫자를 사용한다.\n코드명(고객 구분 명)은 해당 코드의 의미를 명확하게 설명한다.\n사용 조건\n\n간단하고 평면적인 분류가 필요할 때\n코드 값과 의미가 1:1로 대응될 때\n코드의 수가 제한적이고 변경이 적을 때\n\n예시: 고객 구분 코드, 성별코드, 결혼여부코드, 직급코드\n\n\n\n\n고객 구분 코드\n고객 구분 명\n\n\n\n\n01\n개인\n\n\n02\n법인\n\n\n03\n개인사업자\n\n\n04\n외국인\n\n\n05\n공공기관\n\n\n06\n비영리단체\n\n\n07\nVIP\n\n\n08\n임직원\n\n\n09\n제휴사\n\n\n10\n기타\n\n\n\n계층코드(C)\n\n하나 이상의 코드를 상속받거나 계층 구조를 통해 생성되어진 코드로 Key 가 구성된 경우\n대분류 / 중분류 / 소분류 와 같은 분류체계를 가짐\n\n정규화: 각 분류 수준이 별도의 테이블로 분리되어 있어 데이터 중복이 최소화된다.\n참조 무결성: 외래 키 관계를 통해 데이터의 일관성이 유지된다.\n유연성: 각 분류 수준에서 독립적으로 항목을 추가, 수정, 삭제할 수 있다.\n확장성: 새로운 분류 항목을 쉽게 추가할 수 있다.\n\n각 분류 수준에 대한 추가 정보(예: 생성일, 수정일, 설명 등)를 쉽게 추가할 수 있는 장점이 있다.\n\n쿼리 효율성: 필요에 따라 조인을 통해 전체 계층 구조를 조회하거나, 특정 수준만 조회할 수 있다.\n\n사용 조건\n\n데이터가 계층적 구조를 가질 때\n상위 개념과 하위 개념의 관계를 표현해야 할 때\ndrill-down 분석이 필요한 경우\n예시: 조직코드, 상품분류코드, 지역코드\n\n\n대분류 코드 테이블:\n\n\n\n\n대분류 코드\n대분류명\n\n\n\n\nA\n전자제품\n\n\nB\n가전제품\n\n\nC\n의류\n\n\n\n\n중분류 코드 테이블:\n\n\n\n\n중분류 코드\n중분류명\n대분류 코드 (FK)\n\n\n\n\nA1\n컴퓨터\nA\n\n\nA2\n휴대폰\nA\n\n\nB1\n주방가전\nB\n\n\nB2\n생활가전\nB\n\n\nC1\n남성복\nC\n\n\nC2\n여성복\nC\n\n\n\n\n소분류 코드 테이블:\n\n\n\n\n소분류 코드\n소분류명\n대분류 코드 (FK)\n중분류 코드 (FK)\n\n\n\n\nA11\n데스크톱\nA\nA1\n\n\nA12\n노트북\nA\nA1\n\n\nA13\n태블릿\nA\nA1\n\n\nA21\n스마트폰\nA\nA2\n\n\nA22\n피처폰\nA\nA2\n\n\nB11\n냉장고\nB\nB1\n\n\nB12\n전자레인지\nB\nB1\n\n\nB21\n청소기\nB\nB2\n\n\nB22\n세탁기\nB\nB2\n\n\nC11\n셔츠\nC\nC1\n\n\nC12\n바지\nC\nC1\n\n\nC21\n원피스\nC\nC2\n\n\nC22\n스커트\nC\nC2\n\n\n\n\n목록코드(L)\n\n코드값(은행코드)과 코드명(은행명) 외에 여러 부가 정보를 포함한다.\n약칭, 영문명, 주소, 전화번호 등 해당 코드와 관련된 상세 정보를 관리한다.\n설립일과 같은 날짜 정보도 포함될 수 있다.\n사용여부와 같은 관리 정보도 포함될 수 있다.\n목록코드는 일반적으로 해당 업무팀에서 직접 관리하며, 시스템 전반에서 참조되어 사용됨\n\n목록코드의 장점\n\n상세 정보 관리: 코드와 관련된 다양한 부가 정보를 함께 관리할 수 있다.\n업무 특성 반영: 특정 업무 영역의 특성을 반영한 정보를 포함할 수 있다.\n데이터 일관성: 코드와 관련된 정보를 중앙에서 관리함으로써 데이터의 일관성을 유지할 수 있다.\n확장성: 필요에 따라 새로운 정보 항목을 쉽게 추가할 수 있다.\n\n\n사용 조건\n\n코드와 함께 추가적인 속성 정보가 필요할 때\n\n업무 요구사항 분석\n\n사용자나 부서가 코드 외에 추가 정보를 자주 요청하는 경우\n코드만으로는 업무 처리에 충분한 정보를 제공하지 못하는 경우\n\n데이터 활용도 검토\n\n보고서나 분석에서 코드 관련 부가 정보가 자주 필요한 경우\n데이터 조인이나 lookup 작업이 빈번하게 발생하는 경우\n\n시스템 통합 요구사항\n\n다른 시스템과 데이터를 교환할 때 코드 외 추가 정보가 필요한 경우\n외부 시스템이나 API가 코드와 관련된 부가 정보를 요구하는 경우\n\n변경 관리 필요성\n\n코드 값이 시간에 따라 변경되거나 이력 관리가 필요한 경우\n코드의 유효 기간이나 사용 상태를 관리해야 하는 경우\n\n복잡한 비즈니스 로직\n\n코드를 기반으로 복잡한 비즈니스 규칙이나 계산이 필요한 경우\n코드에 따라 다른 처리 로직이 적용되어야 하는 경우\n\n\n사용자 인터페이스 요구사항\n\n코드 선택 시 사용자에게 추가 정보를 제공해야 하는 경우\n코드 검색이나 필터링 시 다양한 기준이 필요한 경우\n\n감사 및 규제 요구사항\n\n코드 사용에 대한 상세한 이력이나 근거를 유지해야 하는 경우\n규제 준수를 위해 코드와 관련된 부가 정보를 관리해야 하는 경우\n\n\n코드 정보가 자주 변경되거나 확장될 가능성이 있을 때\n코드 정보가 특정 업무 영역에 국한되어 관리될 때\n\n예시: 은행코드, 국가코드, 통화코드\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n은행코드\n은행명\n약칭\n영문명\n본점주소\n대표전화\n설립일\n사용여부\n\n\n\n\n001\n한국은행\n한은\nBank of Korea\n서울특별시 중구 남대문로 39\n02-759-4114\n1950-06-12\nY\n\n\n002\n산업은행\n산은\nKorea Development Bank\n서울특별시 영등포구 은행로 14\n1588-1500\n1954-04-01\nY\n\n\n003\n기업은행\nIBK\nIndustrial Bank of Korea\n서울특별시 중구 을지로 79\n1566-2566\n1961-08-01\nY\n\n\n004\nKB국민은행\n국민\nKB Kookmin Bank\n서울특별시 영등포구 국제금융로8길 26\n1588-9999\n2001-11-01\nY\n\n\n005\n하나은행\n하나\nKEB Hana Bank\n서울특별시 중구 을지로 35\n1599-1111\n1967-01-30\nY\n\n\n007\n수협은행\n수협\nSuhyup Bank\n서울특별시 송파구 오금로 62\n1588-1515\n1962-04-01\nY\n\n\n\n복합코드 (M)\n\n코드 구성: 복합코드는 ’상품카테고리코드’와 ’지역코드’를 조합하여 만들어진다.\n의미 결합: 두 개의 단일 코드의 의미를 결합하여 새로운 의미를 만든다.\n추가 정보: 복합코드에는 단순히 두 코드를 붙인 것 외에도 추가적인 정보(설명, 담당부서, 적용일자 등)를 포함할 수 있다.\n유연성: 새로운 상품 카테고리나 지역이 추가될 때 쉽게 확장할 수 있다.\n복합코드의 장점\n\n데이터 압축: 여러 정보를 하나의 코드로 표현할 수 있다.\n의미 전달: 코드만으로도 여러 차원의 정보를 전달할 수 있다.\n유연한 확장: 기존 단일 코드 체계를 유지하면서 새로운 의미를 부여할 수 있다.\n\n데이터 분석: 복합코드를 분해하여 다양한 관점에서 데이터를 분석할 수 있다.\n\n이러한 복합코드는 조직의 복잡한 구조나 다차원적인 정보를 효율적으로 표현하고 관리하는 데 유용하다.\n\n이러한 목록코드는 일반적으로 해당 업무팀에서 직접 관리하며, 시스템 전반에서 참조되어 사용된다.\n사용 조건\n\n두 개 이상의 독립적인 코드 체계를 조합해야 할 때\n다차원적인 정보를 하나의 코드로 표현해야 할 때\n기존 코드 체계를 유지하면서 새로운 의미를 부여해야 할 때\n\n먼저, 각 단일 코드 도메인을 정의한다.\n예시: 지역별 상품코드, 부서별 프로젝트코드\n\n상품 카테고리 코드 (단일코드)\n\n\n\n\n코드\n카테고리명\n\n\n\n\nA\n전자제품\n\n\nB\n의류\n\n\nC\n식품\n\n\n\n\n지역 코드 (단일코드)\n\n\n\n\n코드\n지역명\n\n\n\n\n01\n서울\n\n\n02\n부산\n\n\n03\n대구\n\n\n\n\n지역별 상품 코드 (복합코드)\n\n\n\n\n복합코드\n상품카테고리코드\n지역코드\n설명\n담당부서\n적용일자\n\n\n\n\nA01\nA\n01\n서울 전자제품\n서울영업1팀\n2023-01-01\n\n\nA02\nA\n02\n부산 전자제품\n부산영업팀\n2023-01-01\n\n\nA03\nA\n03\n대구 전자제품\n대구영업팀\n2023-01-01\n\n\nB01\nB\n01\n서울 의류\n서울영업2팀\n2023-01-01\n\n\nB02\nB\n02\n부산 의류\n부산영업팀\n2023-01-01\n\n\nB03\nB\n03\n대구 의류\n대구영업팀\n2023-01-01\n\n\nC01\nC\n01\n서울 식품\n서울영업3팀\n2023-01-01\n\n\nC02\nC\n02\n부산 식품\n부산영업팀\n2023-01-01\n\n\nC03\nC\n03\n대구 식품\n대구영업팀\n2023-01-01\n\n\n\n\n\n선택 시 고려사항\n\n데이터의 구조: 데이터가 계층적인지, 평면적인지 파악\n확장성: 향후 코드 추가나 변경 가능성 고려\n사용 목적: 데이터 분석, 보고, 시스템 통합 등의 용도 파악\n관리 용이성: 코드 관리의 복잡성과 유지보수 고려\n업무 특성: 특정 업무 도메인의 요구사항 반영\n\n\n\n\n\n\n회사에서 사용하는 표준 코드의 기준 관리항목은 아래와 같다\n표준코드 관리항목 구성\n\n신규 모델링 단계에서 코드 값에 대한 신청은 오프라인으로 수행된다.\n변경 모델링 단계에서 코드 값에 대한 신청은 표준화 담당자를 통해 이루어진다.\n모든 코드는 코드 도메인과 매핑 관계를 가지며 ERP 공통 코드 테이블에 대한 데이터 SYNC 작업이 수행된다\n\n\n\n\n\n유형\n설명\n\n\n\n\n코드구분값\n• 코드목록값 혹은 계층코드일 경우 최상위 코드 목록값\n\n\n코드값\n• 코드 목록에 따른 코드 Value값\n\n\n코드명\n• 코드 도메인명과 동일함\n\n\n코드설명\n• 코드명 설명\n\n\n코드영문명\n• 코드 도메인 영문명과 동일함\n\n\n코드길이\n• 실제 코드 값의 길이\n\n\n코드구분\n• 단일코드/계층코드/목록성코드로 구분함\n\n\n업무구분\n• 코드에 대한 ownership을 가진 담당 업무영역\n\n\n상위코드값\n• 상위 코드 Value값\n\n\n상위코드구분값\n• 상위코드 목록값\n\n\n엔티티명\n• 목록성 코드인 경우 대상 엔티티명\n\n\n테이블명\n• 목록성 코드인 경우 대상 테이블명\n\n\n\n\n\n\n\n[주제어] + [코드 수식어 유형] + 코드 형태로 정의하여 사용한다.\n표준코드 구성 체계\n\n수식어 없이 코드용어 생성 가능\n\n\n\n\n\n\n\n\n\n\n\n분류\n유형\n설명\n예시\n\n\n\n\n기본\n유형\n어떤 비슷한 것들의 본질을 개체로서 나타낸 것, 또는 그것들의 공통되는 성질이나 모양을 정의할 때 사용되는 코드 유형\n거래 유형 코드\n\n\n기본\n분류\n코드 값을 체계화 하여 관리하는 경우 사용하며, 주로 대 / 중 / 소 / 세 등의 분류 체계를 갖는 코드에 대해서 ’분류’를 사용\n제품 소분류 코드\n\n\n기본\n종류\n가급적 사용을 제한하되 ’유형’이나 ’분류’의 사용 시 의미 전달이 모호해질 경우 혹은 통상적으로 사용되는 경우에 한해서 사용\n거래 종류 코드\n\n\n기본\n구분\n따로따로 갈라서 나누는 것으로 ’유형’보다는 단순하고 값의 종류가 10개 이내로 제한적이고 값의 범위가 명확한 경우 사용\n상품항목 구분 코드\n\n\n기본\n항목\n목록을 나열한 경우에 한해 사용\n점검 항목 코드\n\n\n확장\n사유\n인식 작용, 분석, 종합, 추리, 판단 등의 정신 작용에 대한 근거 및 동기\n취소 사유 코드\n\n\n확장\n상태\n사물이나 현상이 처해 있는 현재의 모양 또는 형편\n계약 상태 코드\n\n\n확장\n관계\n둘 이상의 사람, 사물, 현상 따위가 서로 관련을 맺거나 관련이 있음\n계약자 관계 코드\n\n\n확장\n용도\n사용되는 곳 혹은 사용되는 목적을 정의\n자금 용도 코드\n\n\n확장\n등급\n높고 낮음이나 좋고 나쁨 따위의 차이를 여러 층으로 구분한 단계\n차량 등급 코드\n\n\n확장\n지역\n전체 영역을 어떤 특징으로 나눈 일정한 공간 영역\n등록 지역 코드\n\n\n확장\n단위\n어떤 물리량(物理量)의 크기를 나타낼 때 비교의 기준 되는 크기\n회계 단위 코드\n\n\n\n\n\n\n\n코드값(인스턴스)을 부여하는 방식에 대한 4가지 분류가 있으며 코드의 형식을 결정할 수 있다\n계층 분류형(H)은 조직 구조와 같이 계층적인 관계를 표현하는 데 적합합니다.\n순차 채번형(S)은 순서가 있는 항목들을 나열할 때 유용합니다.\n표준약어 부여형(A)은 국제적으로 통용되는 표준 코드를 사용할 때 적합합니다.\n복합 분류형(C)은 계층 구조와 순차적 번호 부여가 동시에 필요한 경우에 사용됩니다.\n표준코드 구성 체계\n\n[A : Alphabet N : Numeric S : Sequence Number]\n\n\n\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n계층 분류형(H)\n• 대/중/소 등의 분류에 의한 구분이 필요한 경우 적용• 일반적으로 10진 분류 체계로 구성• 코드형식: N + N + N - NN(대분류) + NN(중분류) + NN(소분류) - NN(본/지점 분류) + NN(실/부 분류) + NN(팀 분류)\n[조직구분코드]100000: 본사총괄101000: 기획실101010: 회계팀101020: 자금팀\n\n\n순차 채번형(S)\n• 일련번호와 같이 순차적으로 번호를 부여하며 부여된 자리를 넘지 않도록 구성• 가능한 결번이 없도록 정의함. 코드 길이 만큼을 앞에 0을 채워서 번호 부여(숫자형 문자)• 코드 형식: SS\n[가족구분코드]01: 부02: 모03: 배우자99: 기타\n\n\n표준약어 부여형(A)\n• 대부분 국제 표준 코드 및 국가표준코드, 업종표준코드 등이 이에 속함.• 코드 형식: AAA\n[국가구분코드]CAN: 캐나다CHN: 중국\n\n\n복합 분류형(C)\n• 계층분류와 순차채번이 결합된 형태의 분류• 코드 형식: ASSSSS\n[담보구분코드]A00001: 건물A00002: 토지B00001: 예금\n\n\n\n\n\n\n\n표준코드 구성 체계\n기본 원칙\n\n원칙적으로 회사 렌터카 시스템 구축에서 사용하는 모든 코드는 통합 관리한다\n업무적으로 동일한 의미의 코드나 유사한 코드를 통합 후 표준화된 코드값과 코드내용을 부여한다\n목록성 코드의 경우 참조정보(DB명, 테이블명, 컬럼명) 만 관리하며 별도 코드값, 코드내용을 관리하지 않는다. 코드값과 코드내용 이외에 부가적인 정보가 존재하고, 코드에 따라 부가적인 정보의 개수가 다르기 때문에 표준 코드 테이블에서 관리하기 어렵기 때문이다\n\n코드값(인스턴스) 부여 원칙\n\n코드값의 부여는 원칙적으로 숫자형 문자 형태의 일련번호(01,02..)를 부여한다\n특별한 사유가 없는 한 현업에서 부여한 코드값을 최우선 사용함을 원칙으로 한다\n코드값 부여는 가능한 연속적으로 부여한다\n코드값 길이는 향후 확장성을 고려해서 부여한다\n숫자로만 이루어진 코드는 원칙적으로 허용하지 않으며 코드 길이만큼 숫자형 문자를 이용해서 ’0’을 채워서 코드를 부여한다\n‘기타’, ‘해당없음’ 등의 내용을 갖는 코드는 가급적 사용하지 않는 것을 원칙으로 하되, 반드시 사용해야 할 경우 해당 자리의 ‘00’, ‘99’ 등의 최대값을 이용한다\n’ 여부’, ’유무’의 모든 코드값은 ’Y＇과 ’N＇로 사용된다.\n\n코드값(인스턴스) 부여 원칙 예외\n\n기존 As-Is에서 특별한 의미를 가지는 코드 값으로 사용되었을 경우 그대로 채택한다\n외부에서 정의되어서 표준 약어로 널리 사용되는 있는 코드들은 표준화 대상에서 제외하며, 그대로 사용하도록 한다.\n\n국가구분코드 등"
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_engineering/data_storage_system.html",
    "href": "docs/blog/posts/Engineering/data_engineering/data_storage_system.html",
    "title": "데이터 저장소의 핵심 개념",
    "section": "",
    "text": "데이터 엔지니어링은 데이터의 수집, 처리, 저장, 변환, 배포 등 데이터를 효율적으로 관리하고 분석에 적합한 형태로 제공하는 작업입니다. Data Lake, Data Warehouse, Data Mart는 데이터 엔지니어링의 중요한 구성 요소이다. Data Lake, Data Warehouse, 그리고 Data Mart는 데이터 저장 및 관리 아키텍처의 개념이다.\n\n\n\nData Lake는 원시 데이터(raw data)를 거의 가공하지 않고 저장하는 대규모 저장소이다.\n구조화된 데이터(예: 테이블 형식 데이터)뿐만 아니라 비구조화된 데이터(예: 이미지, 비디오, 로그 파일 등)도 함께 저장할 수 있다.\n특징\n\n데이터를 변환하거나 구조화하기 전에 저장.\n저장 비용이 저렴하고 대용량 데이터를 다룰 수 있음.\n데이터 처리는 나중에 필요할 때 적용(ELT 방식: Extract, Load, Transform).\n데이터 분석, 머신러닝 등의 고급 분석에 자주 사용.\n주요 사용처: 빅데이터 분석, 머신러닝 모델 훈련, 비정형 데이터를 다루는 경우.\n\n\n\n\n\n\nData Warehouse는 구조화된 데이터를 저장하는 중앙화된 데이터 저장소이다.\n여러 소스에서 데이터를 수집한 후, 데이터를 변환하고 정제하여 저장하므로 정형화된 데이터만 저장한다.\n특징\n\nETL 프로세스(Extract, Transform, Load)를 사용해 데이터 변환 후 저장.\n분석 및 보고를 위한 데이터가 사전 정의된 스키마에 맞춰 저장됨.\n실시간 쿼리 성능이 중요하며, 비즈니스 인텔리전스(BI) 분석에 주로 사용.\n주요 사용처: 비즈니스 분석, 데이터 마이닝, 보고서 작성.\n\n\n\n\n\n\nData Mart는 특정 부서 또는 팀이 필요로 하는 데이터만 모아둔 소규모 데이터 저장소이다.\nData Warehouse의 하위 집합으로 볼 수 있다.\n특징\n\n비즈니스 단위에 맞춘 데이터 서브셋.\nData Warehouse보다 가볍고 빠른 접근이 가능.\n특정 목적(예: 마케팅 분석, 재무 보고)을 위해 구조화된 데이터를 사용.\n주요 사용처: 특정 부서의 분석 요구(예: 재무부, 마케팅부) 또는 특정 주제별 분석.\n\n\n\n\n\n\n데이터의 가공 여부\n\nData Lake는 원시 데이터를 저장\nData Warehouse는 정제된 데이터를 저장\nData Mart는 특정 부서 또는 목적에 맞춘 데이터를 저장.\n\n데이터 구조\n\nData Lake는 비정형 데이터와 정형 데이터를 모두 저장\nData Warehouse와 Data Mart는 구조화된 데이터에 초점을 맞춤.\n\n사용 목적\n\nData Lake는 분석과 머신러닝에 사용될 수 있음\nData Warehouse는 비즈니스 인텔리전스와 보고에 사용될 수 있음\nData Mart는 특정 부서나 팀의 요구에 맞는 데이터 분석에 사용될 수 있음.\n\n\n\n\n\n\nOperational Data Store (ODS)\n\n실시간 데이터를 저장하는 저장소\n주로 운영 데이터 처리를 위한 시스템\n실시간 데이터 통합과 분석을 위해 사용되며, ETL 처리를 기다릴 필요 없이 데이터를 즉시 사용할 수 있다.\n\nData Lakehouse\n\nData Lake와 Data Warehouse의 장점을 결합한 하이브리드 아키텍처\n비정형 데이터와 정형 데이터를 한곳에 저장하고, 분석을 위한 스키마와 ETL 프로세스도 제공하는 형태로 발전한 개념\n\nNoSQL Databases\n\n전통적인 관계형 데이터베이스(RDBMS)와는 달리, 다양한 데이터 형식을 저장\n스키마가 필요 없다.\n대규모 분산 시스템에서 특히 효과적이다.\n예시: MongoDB, Cassandra\n\nGraph Databases\n\n노드와 엣지로 구성된 그래프 형태로 데이터를 저장\n데이터 간의 관계가 중요한 상황에서 유용\n예시: Neo4j, Amazon Neptune"
  },
  {
    "objectID": "docs/blog/posts/Engineering/data_engineering/data_storage_system.html#데이터-저장소",
    "href": "docs/blog/posts/Engineering/data_engineering/data_storage_system.html#데이터-저장소",
    "title": "데이터 저장소의 핵심 개념",
    "section": "",
    "text": "데이터 엔지니어링은 데이터의 수집, 처리, 저장, 변환, 배포 등 데이터를 효율적으로 관리하고 분석에 적합한 형태로 제공하는 작업입니다. Data Lake, Data Warehouse, Data Mart는 데이터 엔지니어링의 중요한 구성 요소이다. Data Lake, Data Warehouse, 그리고 Data Mart는 데이터 저장 및 관리 아키텍처의 개념이다.\n\n\n\nData Lake는 원시 데이터(raw data)를 거의 가공하지 않고 저장하는 대규모 저장소이다.\n구조화된 데이터(예: 테이블 형식 데이터)뿐만 아니라 비구조화된 데이터(예: 이미지, 비디오, 로그 파일 등)도 함께 저장할 수 있다.\n특징\n\n데이터를 변환하거나 구조화하기 전에 저장.\n저장 비용이 저렴하고 대용량 데이터를 다룰 수 있음.\n데이터 처리는 나중에 필요할 때 적용(ELT 방식: Extract, Load, Transform).\n데이터 분석, 머신러닝 등의 고급 분석에 자주 사용.\n주요 사용처: 빅데이터 분석, 머신러닝 모델 훈련, 비정형 데이터를 다루는 경우.\n\n\n\n\n\n\nData Warehouse는 구조화된 데이터를 저장하는 중앙화된 데이터 저장소이다.\n여러 소스에서 데이터를 수집한 후, 데이터를 변환하고 정제하여 저장하므로 정형화된 데이터만 저장한다.\n특징\n\nETL 프로세스(Extract, Transform, Load)를 사용해 데이터 변환 후 저장.\n분석 및 보고를 위한 데이터가 사전 정의된 스키마에 맞춰 저장됨.\n실시간 쿼리 성능이 중요하며, 비즈니스 인텔리전스(BI) 분석에 주로 사용.\n주요 사용처: 비즈니스 분석, 데이터 마이닝, 보고서 작성.\n\n\n\n\n\n\nData Mart는 특정 부서 또는 팀이 필요로 하는 데이터만 모아둔 소규모 데이터 저장소이다.\nData Warehouse의 하위 집합으로 볼 수 있다.\n특징\n\n비즈니스 단위에 맞춘 데이터 서브셋.\nData Warehouse보다 가볍고 빠른 접근이 가능.\n특정 목적(예: 마케팅 분석, 재무 보고)을 위해 구조화된 데이터를 사용.\n주요 사용처: 특정 부서의 분석 요구(예: 재무부, 마케팅부) 또는 특정 주제별 분석.\n\n\n\n\n\n\n데이터의 가공 여부\n\nData Lake는 원시 데이터를 저장\nData Warehouse는 정제된 데이터를 저장\nData Mart는 특정 부서 또는 목적에 맞춘 데이터를 저장.\n\n데이터 구조\n\nData Lake는 비정형 데이터와 정형 데이터를 모두 저장\nData Warehouse와 Data Mart는 구조화된 데이터에 초점을 맞춤.\n\n사용 목적\n\nData Lake는 분석과 머신러닝에 사용될 수 있음\nData Warehouse는 비즈니스 인텔리전스와 보고에 사용될 수 있음\nData Mart는 특정 부서나 팀의 요구에 맞는 데이터 분석에 사용될 수 있음.\n\n\n\n\n\n\nOperational Data Store (ODS)\n\n실시간 데이터를 저장하는 저장소\n주로 운영 데이터 처리를 위한 시스템\n실시간 데이터 통합과 분석을 위해 사용되며, ETL 처리를 기다릴 필요 없이 데이터를 즉시 사용할 수 있다.\n\nData Lakehouse\n\nData Lake와 Data Warehouse의 장점을 결합한 하이브리드 아키텍처\n비정형 데이터와 정형 데이터를 한곳에 저장하고, 분석을 위한 스키마와 ETL 프로세스도 제공하는 형태로 발전한 개념\n\nNoSQL Databases\n\n전통적인 관계형 데이터베이스(RDBMS)와는 달리, 다양한 데이터 형식을 저장\n스키마가 필요 없다.\n대규모 분산 시스템에서 특히 효과적이다.\n예시: MongoDB, Cassandra\n\nGraph Databases\n\n노드와 엣지로 구성된 그래프 형태로 데이터를 저장\n데이터 간의 관계가 중요한 상황에서 유용\n예시: Neo4j, Amazon Neptune"
  },
  {
    "objectID": "docs/blog/posts/Governance/6_0..data_registration_process.html",
    "href": "docs/blog/posts/Governance/6_0..data_registration_process.html",
    "title": "Data Governance Study - Data Registration Process",
    "section": "",
    "text": "데이터 모델 담당자가 단어(분류어일 경우 도메인 포함) 및 용어와 도메인을 신청하고 표준 담당자가 승인한다.\n표준담당자가 회사 표준담당자가 아닐 경우 표준단어/표준용어에 등록 시 회사 담당자 확인이 필요함\n메타시스템 도입 전까지 회사 렌터카 표준 담당자가 엑셀로 관리함\n\n\n\n\n단어 신청 절차 순서도\n\n\n\n신청 단계 (데이터 모델 담당자):\n\n새로운 단어, 용어, 도메인의 필요성 확인\n기존 표준 목록 검토 (중복 여부 확인)\n신청 양식 작성\n\n단어의 경우: 단어명, 정의, 영문명, 약어 등\n분류어일 경우: 위 내용 + 도메인 정보 (데이터 타입, 길이 등)\n용어의 경우: 용어명, 정의, 구성 단어, 도메인 등\n\n신청 시스템이나 지정된 채널을 통해 제출\n\n검토 단계 (표준 담당자):\n\n신청 내용의 완전성 확인\n기존 표준과의 일관성 검토\n명명 규칙 준수 여부 확인\n정의의 명확성과 적절성 검토\n도메인 정보의 적절성 확인 (해당되는 경우)\n\n피드백 및 수정 단계:\n\n필요시 데이터 모델 담당자에게 추가 정보 요청 또는 수정 제안\n데이터 모델 담당자는 요청받은 사항에 대해 보완하여 재제출\n\n승인 단계 (표준 담당자):\n\n최종 검토 후 승인 결정\n승인된 내용을 표준 사전에 등록\n필요시 관련 시스템 업데이트 (예: 데이터 모델링 도구, 메타데이터 저장소 등)\n\n결과 통보:\n\n데이터 모델 담당자에게 승인 결과 통보\n승인된 경우 적용 방법 및 시기 안내\n반려된 경우 사유 설명 및 대안 제시\n\n적용 및 모니터링:\n\n데이터 모델 담당자는 승인된 표준을 모델에 적용\n표준 담당자는 적용 현황을 모니터링하고 필요시 지원 제공\n\n피드백 및 개선:\n\n사용 과정에서 발생하는 이슈나 개선사항 수집\n필요시 표준 개정 절차 진행\n\n\n\n\n개발 과정의 유연성과 표준화 사이의 균형을 잡는 것은 중요하다.\n\n개발 단계 구분:\n\nPoC / 프로토타입 단계\n\n개발자들에게 최대한의 자유도 부여\n임시 명명 규칙 사용 (예: tmp_, poc_, dev_ 접두어)\n표준화 절차 적용하지 않음\n\n알파 / 베타 단계\n\n느슨한 명명 규칙 적용\n주요 변수, 속성에 대해서만 표준화 검토\n간소화된 승인 프로세스 사용\n\n프로덕션 준비 단계\n\n엄격한 표준화 적용\n모든 주요 변수, 속성, 코드에 대한 표준화 검토\n정식 승인 프로세스 적용\n\n\n명명 규칙 계층화\n\n개인 / 팀 레벨\n\n개발자 또는 팀 내에서 사용하는 임시 명명 규칙\n문서화는 하되, 공식 승인 불필요\n\n프로젝트 레벨\n\n프로젝트 내에서 합의된 명명 규칙\n프로젝트 매니저 또는 기술 리더의 승인\n\n부서 / 도메인 레벨\n\n특정 비즈니스 도메인이나 부서에서 사용하는 표준\n도메인 전문가의 검토 필요\n\n전사 레벨\n\n조직 전체에서 사용되는 공식 표준\n데이터 거버넌스 위원회의 승인 필요\n\n\n자동화 도구 활용\n\n코드 분석 도구를 사용하여 명명 규칙 준수 여부 자동 검사\nCI/CD 파이프라인에 표준화 검사 단계 추가\n데이터 모델링 도구와 연동하여 표준 용어 자동 적용\n\n점진적 표준화\n\n개발 초기에는 핵심 개념에 대해서만 표준화 적용\n프로젝트 진행에 따라 점진적으로 표준화 범위 확대\n리팩토링 과정에서 비표준 명칭을 표준화된 명칭으로 대체\n\n예외 관리 프로세스\n\n표준을 적용하기 어려운 특수 상황에 대한 예외 처리 절차 마련\n예외 사유 문서화 및 승인 프로세스 간소화\n\n교육 및 가이드라인\n\n개발자들에게 표준화의 중요성과 이점에 대한 교육 제공\n쉽게 참조할 수 있는 명명 규칙 가이드라인 제공\n자주 사용되는 표준 용어 목록 공유\n\n정기적인 리뷰 및 정리\n\n주기적으로 사용 중인 변수명, 코드명 등을 검토\n프로젝트 마일스톤마다 표준화 작업 수행\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\nTASK\n설명\n담당\n비고\n\n\n\n\n1\n데이터 모델 신규 용어 발생\n데이터 모델 담당자가 속성 추가 등으로 신규 용어를 추가요건 발생\n모델담당\n\n\n\n2\n표준 용어 검색\n데이터 모델 담당자가 추가한 속성에 해당하는 용어를 데이터 사전에서 검색\n모델담당\n\n\n\n3\n표준 도메인 검색\n신청하는 용어의 도메인이 존재하는지 데이터 사전에서 검색\n모델담당\n\n\n\n4\n표준 단어 검색\n데이터 모델 담당자는 표준데이터 사전에서 신규 속성 구성 단어를 검색 함\n모델담당\n\n\n\n5\n표준 단어 신청\n표준데이터 사전에 구성 단어가 없을 경우 DA는 표준담당자에게 단어신청 함\n모델담당\n\n\n\n6\n단어 신청 접수\n표준담당자는 신규 단어 신청을 접수 함\n표준담당\nEXCEL\n\n\n7\n타당성 검토\n기존 표준단어 및 지침을 참고로 신규 신청 단어의 표준 등록 가능 여부를 판단하고 결과를 데이터 모델 담당자에 통보 함\n표준담당\n\n\n\n8\n표준 단어 등록\n신규 신청 단어가 표준 단어로 적정할 경우 표준담당자는 표준 사전에 등록 함\n표준담당\n\n\n\n9\n표준 용어/도메인 신청\n신규 속성 용어/도메인이 없을 경우 데이터 모델 담당자는 표준담당자에게 용어신청 함\n모델담당\n\n\n\n10\n타당성 검토\n신청 된 용어/도메인이 표준 지침에 맞게 구성 되었는지 검토 후 표준 등록 여부 DA 통보\n표준담당\n\n\n\n11\n표준 용어/도메인 등록\n신규 신청 용어/도메인를 표준 사전에 등록\n표준담당\nEXCEL\n\n\n12\n데이터 모델 반영\n변경 된 표준 용어 및 도메인을 데이터 모델에 반영\n모델담당"
  },
  {
    "objectID": "docs/blog/posts/Governance/6_0..data_registration_process.html#표준-단어-용어-및-도메인-신청-절차",
    "href": "docs/blog/posts/Governance/6_0..data_registration_process.html#표준-단어-용어-및-도메인-신청-절차",
    "title": "Data Governance Study - Data Registration Process",
    "section": "",
    "text": "데이터 모델 담당자가 단어(분류어일 경우 도메인 포함) 및 용어와 도메인을 신청하고 표준 담당자가 승인한다.\n표준담당자가 회사 표준담당자가 아닐 경우 표준단어/표준용어에 등록 시 회사 담당자 확인이 필요함\n메타시스템 도입 전까지 회사 렌터카 표준 담당자가 엑셀로 관리함\n\n\n\n\n단어 신청 절차 순서도\n\n\n\n신청 단계 (데이터 모델 담당자):\n\n새로운 단어, 용어, 도메인의 필요성 확인\n기존 표준 목록 검토 (중복 여부 확인)\n신청 양식 작성\n\n단어의 경우: 단어명, 정의, 영문명, 약어 등\n분류어일 경우: 위 내용 + 도메인 정보 (데이터 타입, 길이 등)\n용어의 경우: 용어명, 정의, 구성 단어, 도메인 등\n\n신청 시스템이나 지정된 채널을 통해 제출\n\n검토 단계 (표준 담당자):\n\n신청 내용의 완전성 확인\n기존 표준과의 일관성 검토\n명명 규칙 준수 여부 확인\n정의의 명확성과 적절성 검토\n도메인 정보의 적절성 확인 (해당되는 경우)\n\n피드백 및 수정 단계:\n\n필요시 데이터 모델 담당자에게 추가 정보 요청 또는 수정 제안\n데이터 모델 담당자는 요청받은 사항에 대해 보완하여 재제출\n\n승인 단계 (표준 담당자):\n\n최종 검토 후 승인 결정\n승인된 내용을 표준 사전에 등록\n필요시 관련 시스템 업데이트 (예: 데이터 모델링 도구, 메타데이터 저장소 등)\n\n결과 통보:\n\n데이터 모델 담당자에게 승인 결과 통보\n승인된 경우 적용 방법 및 시기 안내\n반려된 경우 사유 설명 및 대안 제시\n\n적용 및 모니터링:\n\n데이터 모델 담당자는 승인된 표준을 모델에 적용\n표준 담당자는 적용 현황을 모니터링하고 필요시 지원 제공\n\n피드백 및 개선:\n\n사용 과정에서 발생하는 이슈나 개선사항 수집\n필요시 표준 개정 절차 진행\n\n\n\n\n개발 과정의 유연성과 표준화 사이의 균형을 잡는 것은 중요하다.\n\n개발 단계 구분:\n\nPoC / 프로토타입 단계\n\n개발자들에게 최대한의 자유도 부여\n임시 명명 규칙 사용 (예: tmp_, poc_, dev_ 접두어)\n표준화 절차 적용하지 않음\n\n알파 / 베타 단계\n\n느슨한 명명 규칙 적용\n주요 변수, 속성에 대해서만 표준화 검토\n간소화된 승인 프로세스 사용\n\n프로덕션 준비 단계\n\n엄격한 표준화 적용\n모든 주요 변수, 속성, 코드에 대한 표준화 검토\n정식 승인 프로세스 적용\n\n\n명명 규칙 계층화\n\n개인 / 팀 레벨\n\n개발자 또는 팀 내에서 사용하는 임시 명명 규칙\n문서화는 하되, 공식 승인 불필요\n\n프로젝트 레벨\n\n프로젝트 내에서 합의된 명명 규칙\n프로젝트 매니저 또는 기술 리더의 승인\n\n부서 / 도메인 레벨\n\n특정 비즈니스 도메인이나 부서에서 사용하는 표준\n도메인 전문가의 검토 필요\n\n전사 레벨\n\n조직 전체에서 사용되는 공식 표준\n데이터 거버넌스 위원회의 승인 필요\n\n\n자동화 도구 활용\n\n코드 분석 도구를 사용하여 명명 규칙 준수 여부 자동 검사\nCI/CD 파이프라인에 표준화 검사 단계 추가\n데이터 모델링 도구와 연동하여 표준 용어 자동 적용\n\n점진적 표준화\n\n개발 초기에는 핵심 개념에 대해서만 표준화 적용\n프로젝트 진행에 따라 점진적으로 표준화 범위 확대\n리팩토링 과정에서 비표준 명칭을 표준화된 명칭으로 대체\n\n예외 관리 프로세스\n\n표준을 적용하기 어려운 특수 상황에 대한 예외 처리 절차 마련\n예외 사유 문서화 및 승인 프로세스 간소화\n\n교육 및 가이드라인\n\n개발자들에게 표준화의 중요성과 이점에 대한 교육 제공\n쉽게 참조할 수 있는 명명 규칙 가이드라인 제공\n자주 사용되는 표준 용어 목록 공유\n\n정기적인 리뷰 및 정리\n\n주기적으로 사용 중인 변수명, 코드명 등을 검토\n프로젝트 마일스톤마다 표준화 작업 수행\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\nTASK\n설명\n담당\n비고\n\n\n\n\n1\n데이터 모델 신규 용어 발생\n데이터 모델 담당자가 속성 추가 등으로 신규 용어를 추가요건 발생\n모델담당\n\n\n\n2\n표준 용어 검색\n데이터 모델 담당자가 추가한 속성에 해당하는 용어를 데이터 사전에서 검색\n모델담당\n\n\n\n3\n표준 도메인 검색\n신청하는 용어의 도메인이 존재하는지 데이터 사전에서 검색\n모델담당\n\n\n\n4\n표준 단어 검색\n데이터 모델 담당자는 표준데이터 사전에서 신규 속성 구성 단어를 검색 함\n모델담당\n\n\n\n5\n표준 단어 신청\n표준데이터 사전에 구성 단어가 없을 경우 DA는 표준담당자에게 단어신청 함\n모델담당\n\n\n\n6\n단어 신청 접수\n표준담당자는 신규 단어 신청을 접수 함\n표준담당\nEXCEL\n\n\n7\n타당성 검토\n기존 표준단어 및 지침을 참고로 신규 신청 단어의 표준 등록 가능 여부를 판단하고 결과를 데이터 모델 담당자에 통보 함\n표준담당\n\n\n\n8\n표준 단어 등록\n신규 신청 단어가 표준 단어로 적정할 경우 표준담당자는 표준 사전에 등록 함\n표준담당\n\n\n\n9\n표준 용어/도메인 신청\n신규 속성 용어/도메인이 없을 경우 데이터 모델 담당자는 표준담당자에게 용어신청 함\n모델담당\n\n\n\n10\n타당성 검토\n신청 된 용어/도메인이 표준 지침에 맞게 구성 되었는지 검토 후 표준 등록 여부 DA 통보\n표준담당\n\n\n\n11\n표준 용어/도메인 등록\n신규 신청 용어/도메인를 표준 사전에 등록\n표준담당\nEXCEL\n\n\n12\n데이터 모델 반영\n변경 된 표준 용어 및 도메인을 데이터 모델에 반영\n모델담당"
  },
  {
    "objectID": "docs/blog/posts/Governance/6_1.data_review_process.html",
    "href": "docs/blog/posts/Governance/6_1.data_review_process.html",
    "title": "Data Governance Study - Data Glossary Review Process",
    "section": "",
    "text": "1 Data Standard Governance &gt; Data Glossary Review Process\n데이터 모델 수정 발생시 표준담당자가 표준 점검 후 신규 용어를 데이터 모델 담당자에게 전달하고 모델에 반영한다.\n\n1.0.1 데이터 모델 표준 용어 점검 절차\n\n데이터 모델 수정 발생시 표준담당자가 표준 점검 후 신규 용어를 데이터 모델 담당자에게 전달하고 모델에 반영한다.\n표준담당자가 회사 표준담당자가 아닐 경우 표준단어/표준용어에 등록 시 회사 담당자 확인이 필요함\n메타시스템 도입 전까지 회사 표준 담당자가 엑셀로 관리함\n\n\n\n\n데이터 모델 표준 용어 점검 절차\n\n\n\n데이터 모델의 표준 용어 점검 절차를 TASK 별 상세 설명\n\n모델 담당자와 표준 담당자 간의 명확한 역할 구분\n단계별 검토 및 피드백 프로세스\n비표준 용어 발견 시 신규 용어 생성 및 확인 절차\n최종적으로 모델에 표준 용어 반영\n\n\n\n\n\n\n\n\n\n\n\n순번\nTASK\n설명\n담당\n비고\n\n\n\n\n1\n데이터 모델 마트 적재\n데이터 모델 담당자는 설계한 데이터 모델을 회사 데이터 마트에 적재하여 공유함\n모델담당\n\n\n\n2\n데이터 표준 준수 여부 점검 요청\n신규 또는 변경된 데이터 모델에 대하여 표준 담당자에게 표준 준수 여부 점검을 요청함\n모델담당\n\n\n\n3\n표준 준수 점검 신청 접수\n표준 담당자는 데이터 모델 담당자의 데이터 모델 표준 준수 여부 요청을 접수함\n표준담당\n\n\n\n4\n데이터 모델 마트 접속\n데이터 모델 마트에 접속하여 모델의 속성명 정보를 내려받아 점검 대상을 추출함\n표준담당\n\n\n\n5\n용어 및 단어 표준 준수 여부 점검\n점검 대상을 표준 단어 및 용어 기준으로 표준 준수 여부를 점검함\n표준담당\n\n\n\n6\n비표준 용어 발생 여부 확인\n추출한 데이터 모델의 용어 중 비표준 용어 발생 여부를 확인함\n표준담당\n\n\n\n7\n신규 단어/용어 생성 및 모델 담당자 전달\n비표준 용어에 대한 용어 및 단어 생성 후 모델러에게 전달\n표준담당\nEXCEL\n\n\n8\n신규 용어 확인\n표준 담당자에게 전달받은 신규 용어가 적정한지 확인\n모델담당\n\n\n\n9\n데이터 표준 점검 확인 통보\n데이터 모델에 비표준 용어가 없을 경우 점검 확인을 데이터 모델 담당자에게 통보함\n표준담당\n\n\n\n10\n표준 점검 완료\n데이터 모델 담당자는 신규 용어를 데이터 모델에 반영함\n모델담당"
  },
  {
    "objectID": "docs/blog/posts/Governance/6_2.data_code_registration.html",
    "href": "docs/blog/posts/Governance/6_2.data_code_registration.html",
    "title": "Data Governance Study - Data Code Registration Process",
    "section": "",
    "text": "애플리케이션 파트(개발)가 코드 정보 도출하여 신규 및 변경 신청하고 표준 담당자가 승인한다.\n\n\n\n\n애플리케이션 파트(개발)가 코드 정보 도출하여 신규 및 변경 신청하고 표준 담당자가 승인한다.\n\n\n\n\n데이터 모델 표준 코드 등록 절차\n\n\n\n\n\n\n\n\n\n\n\n\n순번\nTASK\n설명\n담당\n비고\n\n\n\n\n1\n표준 코드 요건 발생\n애플리케이션 개발 중 신규 코드 요건 발생\n개발자\n\n\n\n2\n표준 코드 도출\n신규 등록이 필요한 코드 명, 값 등을 도출\n개발자\nEXCEL\n\n\n3\n코드 검색\n표준 코드 사전에서 해당 코드 검색\n개발자\n코드 사전\n\n\n4\n표준 코드 신청\n표준 코드 사전에 도출된 코드가 없을 경우 표준 담당자에게 신청\n개발자\n\n\n\n5\n코드 신청 접수\n표준 담당자는 개발자가 신청한 코드 표준 등록 요청 접수\n표준담당\n\n\n\n6\n타당성 검토\n기존 코드 사전을 기준으로 신규 코드 타당성 검토\n표준담당\n\n\n\n7\n표준 코드 등록\n검토를 마친 신청 표준 코드를 표준 코드 사전에 등록 후 개발자에게 통보\n표준담당\nEXCEL\n\n\n8\n표준 코드 반영\n등록된 코드 반영\n개발자"
  },
  {
    "objectID": "docs/blog/posts/Governance/6_2.data_code_registration.html#data-standard-governance-data-code-review-process",
    "href": "docs/blog/posts/Governance/6_2.data_code_registration.html#data-standard-governance-data-code-review-process",
    "title": "Data Governance Study - Data Code Registration Process",
    "section": "",
    "text": "애플리케이션 파트(개발)가 코드 정보 도출하여 신규 및 변경 신청하고 표준 담당자가 승인한다.\n\n\n\n\n애플리케이션 파트(개발)가 코드 정보 도출하여 신규 및 변경 신청하고 표준 담당자가 승인한다.\n\n\n\n\n데이터 모델 표준 코드 등록 절차\n\n\n\n\n\n\n\n\n\n\n\n\n순번\nTASK\n설명\n담당\n비고\n\n\n\n\n1\n표준 코드 요건 발생\n애플리케이션 개발 중 신규 코드 요건 발생\n개발자\n\n\n\n2\n표준 코드 도출\n신규 등록이 필요한 코드 명, 값 등을 도출\n개발자\nEXCEL\n\n\n3\n코드 검색\n표준 코드 사전에서 해당 코드 검색\n개발자\n코드 사전\n\n\n4\n표준 코드 신청\n표준 코드 사전에 도출된 코드가 없을 경우 표준 담당자에게 신청\n개발자\n\n\n\n5\n코드 신청 접수\n표준 담당자는 개발자가 신청한 코드 표준 등록 요청 접수\n표준담당\n\n\n\n6\n타당성 검토\n기존 코드 사전을 기준으로 신규 코드 타당성 검토\n표준담당\n\n\n\n7\n표준 코드 등록\n검토를 마친 신청 표준 코드를 표준 코드 사전에 등록 후 개발자에게 통보\n표준담당\nEXCEL\n\n\n8\n표준 코드 반영\n등록된 코드 반영\n개발자"
  },
  {
    "objectID": "docs/blog/posts/Governance/7.0..contents_agrement.html",
    "href": "docs/blog/posts/Governance/7.0..contents_agrement.html",
    "title": "Data Governance Study - the contents agreement",
    "section": "",
    "text": "번호\n도메인\n질의내용\n확정\n\n\n\n\n1\n수\n1,2,3,4, … , 01, 02, 03 등 숫자형 금칙어 등록- 숫자는 문자와 함께 씀- 운전자1 (Driver1)\n숫자 단일 사용은 금지\n\n\n2\n수\n갯수 -&gt; 개수\n개수로 사용\n\n\n3\n금액\n금, 비, 액, 가격, 가액(공금가액, 증가액, 차량가액) -&gt; 금액으로 통일- 금액으로 사용하지 못하는 단어일 경우 복합어 사용\nfee : 요금price : 가격cost : 비용amount : 금액\n\n\n4\n금액\n금액을 나타내는 ‘료’ 단어 단일어 인정 여부- 렌탈료-&gt;렌탈요금, 휴차료, 담보료\n요금으로 사용하며, 관용단어 -&gt; 료 복합어 생성\n\n\n5\n명\n’명’을 단일어로 사용 여부(성명, 명 모두 명으로 사용)\n사물 : 명사람 : 성명\n\n\n6\n일자\n일 -&gt; 일자로 사용(취득일, 종료일, 승인일, 발행일 -&gt; 취득일자, 종료일자) 하고 DD의 형태는 ’일’로 사용\n’일자’로 사용\n\n\n7\n-\n차대 -&gt; 차대번호 고유단어로 등록- CARBODYNO(차대번호), CAR_BODY_NO로 사용 중\n차대번호 복합어 생성\n\n\n8\n-\n상품(상품코드, 주문상품) -&gt; 제품- 별도로 사용하면 상품 영문명(Goods), 제품(Product)- 상품은 Good과 Product를 사용중- 제품은 Item을 사용중- 상품을 Commondity로 등록\n상품 : Product자산 : Asset제품 : Goods부품 : Components소모품 : Consumption Goods\n\n\n9\n-\n회사(실사용자회사명, 사용회사, 소속회사) -&gt; 업체- 회사 영문명(Corporation) 업체(Company)\n회사 : 사용업체 : 사용하지 않고 ’비즈니스파트너’로 대체\n\n\n10\n일자\n월 -&gt; 월단가, 월렌탈료, 월리스료 등으로 단어 생성- 월(Month)은 1월~12월을 표현하는 용도로만 사용\n필요시 복합어 생성ex) 월별렌탈금액 단어 생성\n\n\n11\n수\n수 -&gt; 단일어 허용할지 여부(고객수, 직원수, 회전수)\n단일어 사용하지 않고 복합어 생성\n\n\n12\n수\n건수(Count/CNT, 처리건수, 발생건수)와 개수(Count/CO, 반품갯수, 쿠폰갯수)로 분리해서 사용\n건수는 CNT약어로 생성\n\n\n13\n명\n이름(금칙어 등록), 성명 -&gt; 명으로 사용\n사물 : 명사람 : 성명\n\n\n14\n내용\n사항(특기사항, 특약사항, 참고사항) -&gt; 내용으로 사용\n내용으로 사용하며 명시적으로 사항을 사용하는 경우 복합어 생성\n\n\n15\n번호\n사업자번호(인증서 사업자번호) -&gt; 사업자등록번호 : 사업자번호가 사업자등록번호와 다른 의미이면 단어 생성\n사업자등록번호로 사용\n\n\n16\n번호\n연락처 -&gt; 전화번호로 사용 (ASIS 컬럼은 대부분 TEL로 전화번호 내용임)- 연락을 취할수 있는 모든 내용의 의미로 사용예정이면 등록\n연락처 단어 생성(연락처 Pool에서 사용)ASIS에서 연락처는 전화번호의 의미이므로 ASIS의 연락처 용어는 전화번호로 대체\n\n\n17\n번호\n핸드폰번호(가장많음), 휴대전화번호, 휴대폰 -&gt; 휴대전화번호(표준어)로 통일\n휴대전화번호로 사용\n\n\n18\n-\n대여 (대여종료일자, 대여지점, 월간대여료)-&gt; 랜탈\n렌탈로 사용하며 대여는 금칙어\n\n\n19\n-\n제작사(MAKERCODE, CONFIRMDATE)와 제조사(Maker) 차이\n제작사 : 사용제조사 : 금칙어\n\n\n20\n율\n백분율을 나타내는 ‘율’ 단어 단일어 인정 여부- 감가율, 공채율, 대비율\n복합어 생성(사전 참조)단일어 불허\n\n\n21\n세\n세금을 나타내는 ‘세’ 단어 단일어 인정 여부- 과세, 교육세\n복합어 생성(사전 참조)단일어 불허\n\n\n22\n-\n구입(구입가격, 차량구입비, 구입일자) -&gt; 구매- 구입가격, 구입일\n구매구입 : 금칙어\n\n\n23\n명\n이름을 나타내는 ‘명’ 단어 단일어 인정 여부- 파일명, 차량명\n사물 : 명사람 : 성명\n\n\n24\n-\n구입(구입가격, 차량구입비, 구입일자) -&gt; 구매- 구입가격, 구입일\n구매구입 : 금칙어\n\n\n25\n-\n제원(specification) (제원), 스팩(SPEC) (제품스팩, 제품스팩아이디) -&gt; 스펙\n스펙제원 : 금칙어\n\n\n26\n-\n제휴협력사 vs 협력사 vs 파트너\n비즈니스파트너\n\n\n27\n-\n제휴사 -&gt; ex) 대한항공\n-\n\n\n28\n-\n협력사 -&gt; 정비업체\n-\n\n\n29\n-\n이해당사자 vs 이해관계자\n이해관계자\n\n\n30\n량\n양을 나타내는 의미로 ‘량’ 단어 단일어 인정 여부- 월평균소요량, 연료량 등\n’량’으로 복합어 생성(사전 참조)단일어 불허\n\n\n31\n값\n값을 나타내는 의미로 ‘값’ 단어 단일어 인정 여부- 변경값, 관리키값, 상태값 등\n복합어 생성(사전 참조)단일어 불허\n\n\n32\n번호\n번호 그룹 도메인에 속하는 용어들의 데이터 타입 및 길이가 상이하므로, 용어별 도메인 별도 생성\n번호용어 별 도메인 생성\n\n\n33\n-\n매각제품 별 단가, 회차 별 산출금액에서 ‘별’ 단어 허용 여부\n별, 및 : 금칙어필요시 복합어 생성\n\n\n34\n명\n‘처’ (유지보수 처, 송금 처) 단어 허용 여부- 처의 의미\n처 : 금칙어-&gt; 대상 등으로 사용\n\n\n35\n번호\n순번과 일련번호 중 일련번호 사용 (순번 -&gt; 금칙어)- 일련번호를 Serial Number로 사용하고 순번을 단순한 순차적인 번호(Sequence)로 사용할지 에 대한 의사결정 필요\n순번 : 금칙어일련번호 : 순차적인 번호 (KEY)고유번호(시리얼번호) : 제품고유번호"
  },
  {
    "objectID": "docs/blog/posts/Governance/7.0..contents_agrement.html#data-standard-governance-contents-agreement",
    "href": "docs/blog/posts/Governance/7.0..contents_agrement.html#data-standard-governance-contents-agreement",
    "title": "Data Governance Study - the contents agreement",
    "section": "",
    "text": "번호\n도메인\n질의내용\n확정\n\n\n\n\n1\n수\n1,2,3,4, … , 01, 02, 03 등 숫자형 금칙어 등록- 숫자는 문자와 함께 씀- 운전자1 (Driver1)\n숫자 단일 사용은 금지\n\n\n2\n수\n갯수 -&gt; 개수\n개수로 사용\n\n\n3\n금액\n금, 비, 액, 가격, 가액(공금가액, 증가액, 차량가액) -&gt; 금액으로 통일- 금액으로 사용하지 못하는 단어일 경우 복합어 사용\nfee : 요금price : 가격cost : 비용amount : 금액\n\n\n4\n금액\n금액을 나타내는 ‘료’ 단어 단일어 인정 여부- 렌탈료-&gt;렌탈요금, 휴차료, 담보료\n요금으로 사용하며, 관용단어 -&gt; 료 복합어 생성\n\n\n5\n명\n’명’을 단일어로 사용 여부(성명, 명 모두 명으로 사용)\n사물 : 명사람 : 성명\n\n\n6\n일자\n일 -&gt; 일자로 사용(취득일, 종료일, 승인일, 발행일 -&gt; 취득일자, 종료일자) 하고 DD의 형태는 ’일’로 사용\n’일자’로 사용\n\n\n7\n-\n차대 -&gt; 차대번호 고유단어로 등록- CARBODYNO(차대번호), CAR_BODY_NO로 사용 중\n차대번호 복합어 생성\n\n\n8\n-\n상품(상품코드, 주문상품) -&gt; 제품- 별도로 사용하면 상품 영문명(Goods), 제품(Product)- 상품은 Good과 Product를 사용중- 제품은 Item을 사용중- 상품을 Commondity로 등록\n상품 : Product자산 : Asset제품 : Goods부품 : Components소모품 : Consumption Goods\n\n\n9\n-\n회사(실사용자회사명, 사용회사, 소속회사) -&gt; 업체- 회사 영문명(Corporation) 업체(Company)\n회사 : 사용업체 : 사용하지 않고 ’비즈니스파트너’로 대체\n\n\n10\n일자\n월 -&gt; 월단가, 월렌탈료, 월리스료 등으로 단어 생성- 월(Month)은 1월~12월을 표현하는 용도로만 사용\n필요시 복합어 생성ex) 월별렌탈금액 단어 생성\n\n\n11\n수\n수 -&gt; 단일어 허용할지 여부(고객수, 직원수, 회전수)\n단일어 사용하지 않고 복합어 생성\n\n\n12\n수\n건수(Count/CNT, 처리건수, 발생건수)와 개수(Count/CO, 반품갯수, 쿠폰갯수)로 분리해서 사용\n건수는 CNT약어로 생성\n\n\n13\n명\n이름(금칙어 등록), 성명 -&gt; 명으로 사용\n사물 : 명사람 : 성명\n\n\n14\n내용\n사항(특기사항, 특약사항, 참고사항) -&gt; 내용으로 사용\n내용으로 사용하며 명시적으로 사항을 사용하는 경우 복합어 생성\n\n\n15\n번호\n사업자번호(인증서 사업자번호) -&gt; 사업자등록번호 : 사업자번호가 사업자등록번호와 다른 의미이면 단어 생성\n사업자등록번호로 사용\n\n\n16\n번호\n연락처 -&gt; 전화번호로 사용 (ASIS 컬럼은 대부분 TEL로 전화번호 내용임)- 연락을 취할수 있는 모든 내용의 의미로 사용예정이면 등록\n연락처 단어 생성(연락처 Pool에서 사용)ASIS에서 연락처는 전화번호의 의미이므로 ASIS의 연락처 용어는 전화번호로 대체\n\n\n17\n번호\n핸드폰번호(가장많음), 휴대전화번호, 휴대폰 -&gt; 휴대전화번호(표준어)로 통일\n휴대전화번호로 사용\n\n\n18\n-\n대여 (대여종료일자, 대여지점, 월간대여료)-&gt; 랜탈\n렌탈로 사용하며 대여는 금칙어\n\n\n19\n-\n제작사(MAKERCODE, CONFIRMDATE)와 제조사(Maker) 차이\n제작사 : 사용제조사 : 금칙어\n\n\n20\n율\n백분율을 나타내는 ‘율’ 단어 단일어 인정 여부- 감가율, 공채율, 대비율\n복합어 생성(사전 참조)단일어 불허\n\n\n21\n세\n세금을 나타내는 ‘세’ 단어 단일어 인정 여부- 과세, 교육세\n복합어 생성(사전 참조)단일어 불허\n\n\n22\n-\n구입(구입가격, 차량구입비, 구입일자) -&gt; 구매- 구입가격, 구입일\n구매구입 : 금칙어\n\n\n23\n명\n이름을 나타내는 ‘명’ 단어 단일어 인정 여부- 파일명, 차량명\n사물 : 명사람 : 성명\n\n\n24\n-\n구입(구입가격, 차량구입비, 구입일자) -&gt; 구매- 구입가격, 구입일\n구매구입 : 금칙어\n\n\n25\n-\n제원(specification) (제원), 스팩(SPEC) (제품스팩, 제품스팩아이디) -&gt; 스펙\n스펙제원 : 금칙어\n\n\n26\n-\n제휴협력사 vs 협력사 vs 파트너\n비즈니스파트너\n\n\n27\n-\n제휴사 -&gt; ex) 대한항공\n-\n\n\n28\n-\n협력사 -&gt; 정비업체\n-\n\n\n29\n-\n이해당사자 vs 이해관계자\n이해관계자\n\n\n30\n량\n양을 나타내는 의미로 ‘량’ 단어 단일어 인정 여부- 월평균소요량, 연료량 등\n’량’으로 복합어 생성(사전 참조)단일어 불허\n\n\n31\n값\n값을 나타내는 의미로 ‘값’ 단어 단일어 인정 여부- 변경값, 관리키값, 상태값 등\n복합어 생성(사전 참조)단일어 불허\n\n\n32\n번호\n번호 그룹 도메인에 속하는 용어들의 데이터 타입 및 길이가 상이하므로, 용어별 도메인 별도 생성\n번호용어 별 도메인 생성\n\n\n33\n-\n매각제품 별 단가, 회차 별 산출금액에서 ‘별’ 단어 허용 여부\n별, 및 : 금칙어필요시 복합어 생성\n\n\n34\n명\n‘처’ (유지보수 처, 송금 처) 단어 허용 여부- 처의 의미\n처 : 금칙어-&gt; 대상 등으로 사용\n\n\n35\n번호\n순번과 일련번호 중 일련번호 사용 (순번 -&gt; 금칙어)- 일련번호를 Serial Number로 사용하고 순번을 단순한 순차적인 번호(Sequence)로 사용할지 에 대한 의사결정 필요\n순번 : 금칙어일련번호 : 순차적인 번호 (KEY)고유번호(시리얼번호) : 제품고유번호"
  },
  {
    "objectID": "docs/blog/posts/Governance/6_2.data_code_registration copy.html",
    "href": "docs/blog/posts/Governance/6_2.data_code_registration copy.html",
    "title": "Data Governance Study - Data Code Registration Process",
    "section": "",
    "text": "애플리케이션 파트(개발)가 코드 정보 도출하여 신규 및 변경 신청하고 표준 담당자가 승인한다.\n\n\n\n\n애플리케이션 파트(개발)가 코드 정보 도출하여 신규 및 변경 신청하고 표준 담당자가 승인한다.\n\n\n\n\n데이터 모델 표준 코드 등록 절차\n\n\n\n\n\n\n\n\n\n\n\n\n순번\nTASK\n설명\n담당\n비고\n\n\n\n\n1\n표준 코드 요건 발생\n애플리케이션 개발 중 신규 코드 요건 발생\n개발자\n\n\n\n2\n표준 코드 도출\n신규 등록이 필요한 코드 명, 값 등을 도출\n개발자\nEXCEL\n\n\n3\n코드 검색\n표준 코드 사전에서 해당 코드 검색\n개발자\n코드 사전\n\n\n4\n표준 코드 신청\n표준 코드 사전에 도출된 코드가 없을 경우 표준 담당자에게 신청\n개발자\n\n\n\n5\n코드 신청 접수\n표준 담당자는 개발자가 신청한 코드 표준 등록 요청 접수\n표준담당\n\n\n\n6\n타당성 검토\n기존 코드 사전을 기준으로 신규 코드 타당성 검토\n표준담당\n\n\n\n7\n표준 코드 등록\n검토를 마친 신청 표준 코드를 표준 코드 사전에 등록 후 개발자에게 통보\n표준담당\nEXCEL\n\n\n8\n표준 코드 반영\n등록된 코드 반영\n개발자"
  },
  {
    "objectID": "docs/blog/posts/Governance/6_2.data_code_registration copy.html#data-standard-governance-data-code-review-process",
    "href": "docs/blog/posts/Governance/6_2.data_code_registration copy.html#data-standard-governance-data-code-review-process",
    "title": "Data Governance Study - Data Code Registration Process",
    "section": "",
    "text": "애플리케이션 파트(개발)가 코드 정보 도출하여 신규 및 변경 신청하고 표준 담당자가 승인한다.\n\n\n\n\n애플리케이션 파트(개발)가 코드 정보 도출하여 신규 및 변경 신청하고 표준 담당자가 승인한다.\n\n\n\n\n데이터 모델 표준 코드 등록 절차\n\n\n\n\n\n\n\n\n\n\n\n\n순번\nTASK\n설명\n담당\n비고\n\n\n\n\n1\n표준 코드 요건 발생\n애플리케이션 개발 중 신규 코드 요건 발생\n개발자\n\n\n\n2\n표준 코드 도출\n신규 등록이 필요한 코드 명, 값 등을 도출\n개발자\nEXCEL\n\n\n3\n코드 검색\n표준 코드 사전에서 해당 코드 검색\n개발자\n코드 사전\n\n\n4\n표준 코드 신청\n표준 코드 사전에 도출된 코드가 없을 경우 표준 담당자에게 신청\n개발자\n\n\n\n5\n코드 신청 접수\n표준 담당자는 개발자가 신청한 코드 표준 등록 요청 접수\n표준담당\n\n\n\n6\n타당성 검토\n기존 코드 사전을 기준으로 신규 코드 타당성 검토\n표준담당\n\n\n\n7\n표준 코드 등록\n검토를 마친 신청 표준 코드를 표준 코드 사전에 등록 후 개발자에게 통보\n표준담당\nEXCEL\n\n\n8\n표준 코드 반영\n등록된 코드 반영\n개발자"
  },
  {
    "objectID": "docs/blog/posts/Governance/8.data_quality_control.html",
    "href": "docs/blog/posts/Governance/8.data_quality_control.html",
    "title": "Data Governance Study - Data Quality Management",
    "section": "",
    "text": "데이터가 실제 값이나 참조 소스와 일치하는 정도\n측정 방법\n\n샘플링을 통한 수동 검증\n\n(정확한 샘플 수 / 전체 샘플 수) * 100\n방법: 무작위로 선택된 데이터 샘플을 실제 값과 비교\n\n참조 데이터와의 비교\n\n예: (참조 데이터와 일치하는 레코드 수 / 전체 레코드 수) * 100\n방법: 신뢰할 수 있는 외부 데이터 소스와 비교\n\n비즈니스 규칙 위반 검사\n\n예: (비즈니스 규칙을 준수하는 레코드 수 / 전체 레코드 수) * 100\n방법: 미리 정의된 비즈니스 규칙에 대한 준수 여부 확인\n\n데이터 검증 알고리즘 사용\n\n예: (알고리즘 검증을 통과한 데이터 수 / 전체 데이터 수) * 100\n방법: 체크섬, 유효성 검사 알고리즘 등을 사용\n\n\n중요성\n\n신뢰성 있는 의사결정: 정확한 데이터는 올바른 비즈니스 결정을 내리는 기반이 된다.\n운영 효율성: 부정확한 데이터로 인한 오류와 재작업을 줄일 수 있다.\n고객 만족: 정확한 고객 정보는 더 나은 서비스 제공으로 이어진다.\n규제 준수: 많은 산업에서 데이터의 정확성은 법적 요구사항이다.\n비용 절감: 정확한 데이터는 불필요한 비용 발생을 방지한다.\n\n예시\n\n고객 주소 정확성\n\n측정: 우편번호와 주소의 일치 여부\n방법: 우편번호 데이터베이스와 비교\n목표: 95% 이상의 주소가 정확해야 함\n\n제품 가격 정확성\n\n측정: 시스템에 등록된 가격과 실제 판매 가격의 일치 여부\n방법: POS 데이터와 제품 카탈로그 비교\n목표: 99.9% 이상의 가격 정보가 정확해야 함\n\n재고 수량 정확성\n\n측정: 시스템상 재고량과 실제 재고량의 일치 여부\n방법: 정기적인 실사를 통한 비교\n목표: 97% 이상의 재고 정보가 정확해야 함\n\n금융 거래 정확성\n\n측정: 거래 기록의 정확성\n방법: 더블 엔트리 회계 시스템을 통한 검증\n목표: 100% 정확성 (모든 불일치는 조사 및 수정되어야 함)\n\n고객 연락처 정확성\n\n측정: 이메일 주소와 전화번호의 유효성\n방법: 이메일 발송 테스트, 전화번호 형식 검증\n목표: 90% 이상의 연락처 정보가 유효해야 함\n\n\n정확성 개선 전략\n\n데이터 입력 시 자동 검증 시스템 구축\n정기적인 데이터 클렌징 및 품질 검사 수행\n데이터 소스의 신뢰성 평가 및 관리\n직원 교육을 통한 데이터 입력 오류 최소화\n데이터 품질 관리 도구 활용\n데이터 정확성에 대한 책임자 지정\n지속적인 모니터링 및 피드백 시스템 구축\n\n\n\n\n\n\n완전성은 필요한 모든 데이터가 존재하는 정도\n즉, 데이터셋이 얼마나 빠짐없이 채워져 있는지, 그리고 필요한 모든 정보를 포함하고 있는지를 측정\n측정 방법\n\n널(Null) 값 또는 빈 값 검사\n\n(채워진 필드 수 / 전체 필드 수) * 100\n\n필수 필드 존재 여부 확인\n\n(모든 필수 필드가 채워진 레코드 수 / 전체 레코드 수) * 100\n\n데이터셋 완전성 검사\n\n(실제 레코드 수 / 예상되는 총 레코드 수) * 100\n\n시계열 데이터 완전성 검사\n\n(데이터가 있는 시간 단위 수 / 전체 시간 단위 수) * 100\n\n\n중요성\n\n정확한 분석: 완전한 데이터셋은 더 정확하고 신뢰할 수 있는 분석 결과 제공\n의사결정 지원: 누락된 데이터 없이 전체 그림을 볼 수 있어 더 나은 의사결정 가능\n프로세스 효율성: 필요한 모든 데이터가 있으면 업무 프로세스가 원활하게 진행\n고객 만족: 완전한 고객 정보는 더 나은 서비스와 경험 제공 가능\n규제 준수: 많은 산업에서 데이터의 완전성은 규제 요구사항의 일부\n\n구체적인 예시\n\n고객 프로필 완전성\n\n측정: 필수 고객 정보 필드의 완전성\n방법: (모든 필수 필드가 채워진 고객 프로필 수 / 전체 고객 프로필 수) * 100\n목표: 95% 이상의 고객 프로필이 모든 필수 정보를 포함해야 함\n필수 필드 예: 이름, 연락처, 이메일, 주소\n\n주문 데이터 완전성\n\n측정: 주문 관련 모든 필요 정보의 존재 여부\n방법: (모든 필요 정보가 있는 주문 수 / 전체 주문 수) * 100\n목표: 99% 이상의 주문이 모든 필요 정보를 포함해야 함\n필요 정보: 주문 ID, 고객 ID, 주문 날짜, 제품 ID, 수량, 가격, 배송 주소\n\n재무 보고 데이터 완전성\n\n측정: 월별 재무 보고서의 모든 필요 항목 존재 여부\n방법: (모든 필요 항목이 보고된 월 수 / 전체 보고 월 수) * 100\n목표: 100% (모든 월의 재무 보고서가 완전해야 함)\n필요 항목: 매출, 비용, 순이익, 자산, 부채, 자본 등\n\n센서 데이터 완전성\n\n측정: IoT 센서에서 수집된 데이터의 시간별 완전성\n방법: (데이터가 수집된 시간 단위 수 / 24시간) * 100 (일일 기준)\n목표: 99.9% 이상 (하루 중 대부분의 시간에 데이터가 수집되어야 함)\n\n제품 카탈로그 완전성\n\n측정: 제품 정보의 완전성\n방법: (모든 필요 정보가 있는 제품 수 / 전체 제품 수) * 100\n목표: 98% 이상의 제품이 모든 필요 정보를 포함해야 함\n필요 정보: 제품명, 설명, 가격, 카테고리, 이미지, 재고 상태 등\n\n\n완전성 개선 전략\n\n데이터 입력 시 필수 필드 설정 및 유효성 검사 구현\n데이터 수집 프로세스 자동화\n데이터 품질 모니터링 도구 사용\n정기적인 데이터 감사 및 클렌징 작업 수행\n사용자 교육 및 데이터 입력 가이드라인 제공\n데이터 보완을 위한 외부 데이터 소스 활용\n데이터 거버넌스 정책 수립 및 시행\n\n\n\n\n\n\n일관성은 데이터가 여러 위치, 시스템, 또는 표현 방식에서 서로 모순 없이 일치하는 정도\n이는 데이터의 내부적 일관성(동일 데이터셋 내)과 외부적 일관성(여러 데이터셋 간)을 모두 포함\n측정 방법\n\n크로스 체크 (여러 테이블/시스템 간 데이터 비교)\n\n(일치하는 데이터 항목 수 / 전체 비교 데이터 항목 수) * 100\n\n중복 데이터 검사\n\n(고유한 데이터 항목 수 / 전체 데이터 항목 수) * 100\n\n데이터 형식의 일관성 검사\n\n(표준 형식을 따르는 데이터 항목 수 / 전체 데이터 항목 수) * 100\n\n참조 무결성 검사\n\n(유효한 참조를 가진 외래 키 수 / 전체 외래 키 수) * 100\n\n\n중요성\n\n데이터 신뢰성\n\n일관된 데이터는 신뢰할 수 있는 정보 제공\n여러 시스템이나 채널에서 일관된 정보를 제공함으로써 고객과 내부 사용자의 신뢰를 얻을 수 있다.\n\n효율적인 운영\n\n일관된 데이터는 업무 프로세스의 효율성을 높이고, 데이터 불일치로 인한 추가 작업을 줄일 수 있다.\n\n\n의사결정 지원\n\n모순 없는 데이터를 기반으로 한 일관된 의사결정 가능\n\n시스템 통합\n\n여러 시스템 간 원활한 데이터 교환 및 통합 지원\n\n사용자 경험\n\n일관된 데이터로 인한 사용자 혼란 방지\n\n정확한 보고 및 분석\n\n여러 소스의 데이터가 일관될 때, 더 정확하고 신뢰할 수 있는 비즈니스 인텔리전스와 분석이 가능\n\n비용 절감\n\n데이터 불일치로 인한 오류 수정 비용 감소\n\n규제 준수\n\n많은 산업에서 데이터의 일관성은 규제 요구사항의 일부이다. 일관된 데이터 관리는 컴플라이언스를 지원.\n\n고객 경험 향상\n\n고객이 모든 접점에서 일관된 정보를 받을 때, 더 나은 고객 경험을 제공\n\n\n예시\n\n고객 정보의 일관성\n\n측정: CRM 시스템과 주문 시스템 간 고객 정보 일치 여부\n방법: 두 시스템의 고객 데이터를 주기적으로 비교\n목표: 95% 이상의 고객 정보가 두 시스템에서 일치해야 함\n\n제품 가격의 일관성\n\n측정: 온라인 스토어와 POS 시스템 간 제품 가격 일치 여부\n방법: 실시간 또는 일일 기준으로 두 시스템의 제품 가격 비교\n목표: 99.9% 이상의 제품 가격이 모든 판매 채널에서 일치해야 함\n\n재무 데이터의 일관성\n\n측정: 총계정원장과 보조원장 간 잔액 일치 여부\n방법: 월말 결산 시 원장 간 잔액 비교\n목표: 100% 일치 (모든 차이는 조정되고 설명되어야 함)\n\n주소 형식의 일관성\n\n측정: 정의된 주소 형식 준수 여부\n방법: 정규 표현식을 사용하여 주소 형식 검증\n목표: 90% 이상의 주소가 표준 형식을 따라야 함\n\n제품 카테고리의 일관성\n\n측정: 여러 시스템에서 동일한 제품에 대한 카테고리 분류 일치 여부\n방법: 제품 마스터 데이터와 각 시스템의 카테고리 정보 비교\n목표: 98% 이상의 제품이 모든 시스템에서 동일한 카테고리로 분류되어야 함\n\n\n일관성 개선 전략\n\n데이터 통합 솔루션 구현 (예: 마스터 데이터 관리 시스템)\n데이터 동기화 메커니즘 개선\n데이터 거버넌스 정책 수립 및 시행\n데이터 입력 및 수정 프로세스 표준화\n정기적인 데이터 감사 및 정화 작업 수행\n시스템 간 실시간 데이터 교환 체계 구축\n데이터 소유권 및 책임 명확화\n\n\n\n\n\n\n유효성은 데이터가 정의된 비즈니스 규칙, 데이터 타입, 범위, 형식 등을 준수하는 정도\n즉, 데이터가 논리적으로 타당하고 비즈니스 컨텍스트에서 의미 있는지를 측정하는 지표\n중요성\n\n데이터 무결성: 시스템의 전반적인 데이터 무결성 보장\n오류 방지: 잘못된 데이터로 인한 비즈니스 프로세스 오류 예방\n분석 신뢰성: 유효한 데이터를 기반으로 한 신뢰할 수 있는 분석 결과 도출\n시스템 호환성: 다양한 시스템 간 데이터 교환 시 문제 방지\n\n측정 방법\n\n데이터 타입 검사\n\n(올바른 데이터 타입을 가진 필드 수 / 전체 필드 수) * 100\n\n값 범위 검사\n\n(정의된 범위 내의 값을 가진 레코드 수 / 전체 레코드 수) * 100\n\n형식 검사\n\n(올바른 형식을 가진 데이터 항목 수 / 전체 데이터 항목 수) * 100\n\n비즈니스 규칙 준수 검사\n\n(비즈니스 규칙을 준수하는 레코드 수 / 전체 레코드 수) * 100\n\n\n예시\n\n이메일 주소 유효성\n\n측정: 올바른 이메일 형식 준수 여부\n방법: 정규 표현식을 사용하여 이메일 주소 형식 검증\n예시 규칙: 1+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$\n목표: 95% 이상의 이메일 주소가 올바른 형식을 가져야 함\n\n나이 데이터 유효성\n\n측정: 나이 값의 논리적 범위 준수\n방법: 0 &lt; 나이 &lt;= 120 범위 검사\n목표: 100%의 나이 데이터가 유효한 범위 내에 있어야 함\n\n주문 데이터 유효성\n\n측정: 주문 금액과 주문 항목 수량의 논리적 일관성\n방법: 주문 총액 = Σ(항목 가격 * 수량) 검증\n목표: 99.9% 이상의 주문 데이터가 이 규칙을 만족해야 함\n\n신용카드 번호 유효성\n\n측정: Luhn 알고리즘을 사용한 신용카드 번호 유효성 검사\n방법: Luhn 알고리즘 적용 후 유효성 확인\n목표: 100%의 신용카드 번호가 Luhn 알고리즘을 통과해야 함\n\n날짜 데이터 유효성\n\n측정: 날짜 형식 및 논리적 타당성 검사\n방법: YYYY-MM-DD 형식 준수 및 존재하는 날짜인지 확인 (예: 2023-02-30은 유효하지 않음)\n목표: 100%의 날짜 데이터가 올바른 형식과 유효한 날짜여야 함\n\n\n유효성 개선 전략\n\n데이터 입력 시점의 유효성 검사 구현\n정기적인 데이터 클렌징 프로세스 수립\n비즈니스 규칙 엔진 도입\n데이터 품질 관리 도구 활용\n사용자 교육 및 가이드라인 제공\n데이터 모델링 및 스키마 설계 시 제약조건 적용\n\n\n\n\n\n\n데이터가 필요한 시점에 이용 가능한 정도와 최신 상태인 정도를 나타냅\n\n즉, 데이터가 현실 세계의 상태를 얼마나 잘 반영하고 있는지를 측정하는 지표\n\n중요성\n\n의사결정: 최신 데이터를 기반으로 한 신속하고 정확한 의사결정 가능 중요성\n운영 효율성: 실시간 또는 최신 데이터로 업무 프로세스 최적화 중요성\n고객 만족: 최신 정보를 기반으로 한 서비스 제공으로 고객 경험 향상\n\n측정 방법\n\n데이터 갱신 주기 확인\n\n(정해진 주기 내 업데이트된 레코드 수 / 전체 레코드 수) * 100\n\n실시간 데이터와 저장된 데이터의 시간 차이 측정\n\n평균 데이터 지연 시간 = Σ(현재 시간 - 데이터 최종 업데이트 시간) / 전체 레코드 수\n\n데이터 생성 시점과 사용 가능 시점의 차이 측정\n\n평균 데이터 가용 지연 = Σ(데이터 사용 가능 시간 - 데이터 생성 시간) / 전체 데이터 수\n\n\n예시\n\n재고 관리 시스템\n\n측정: 실제 재고량과 시스템상 재고량의 일치 비율\n방법: (1시간 이내 업데이트된 재고 항목 수 / 전체 재고 항목 수) * 100\n목표: 95% 이상의 재고 정보가 1시간 이내에 업데이트되어야 함\n\n금융 거래 시스템\n\n측정: 거래 발생부터 시스템 반영까지의 평균 시간\n방법: Σ(거래 반영 시간 - 거래 발생 시간) / 전체 거래 수\n목표: 평균 지연 시간 5초 이내\n\n고객 정보 관리 시스템\n\n측정: 고객 정보 변경사항의 반영 속도\n방법: (24시간 이내 업데이트된 고객 정보 변경 건수 / 전체 고객 정보 변경 요청 건수) * 100\n목표: 99% 이상의 고객 정보 변경사항이 24시간 이내에 반영되어야 함\n\n적시성 개선 전략\n\n실시간 데이터 처리 시스템 구축\n데이터 동기화 주기 최적화\n데이터 파이프라인 효율성 향상\n중요 데이터에 대한 우선순위 처리 체계 수립\n데이터 갱신 알림 시스템 구축"
  },
  {
    "objectID": "docs/blog/posts/Governance/8.data_quality_control.html#data-standard-governance-data-quality-management",
    "href": "docs/blog/posts/Governance/8.data_quality_control.html#data-standard-governance-data-quality-management",
    "title": "Data Governance Study - Data Quality Management",
    "section": "",
    "text": "데이터가 실제 값이나 참조 소스와 일치하는 정도\n측정 방법\n\n샘플링을 통한 수동 검증\n\n(정확한 샘플 수 / 전체 샘플 수) * 100\n방법: 무작위로 선택된 데이터 샘플을 실제 값과 비교\n\n참조 데이터와의 비교\n\n예: (참조 데이터와 일치하는 레코드 수 / 전체 레코드 수) * 100\n방법: 신뢰할 수 있는 외부 데이터 소스와 비교\n\n비즈니스 규칙 위반 검사\n\n예: (비즈니스 규칙을 준수하는 레코드 수 / 전체 레코드 수) * 100\n방법: 미리 정의된 비즈니스 규칙에 대한 준수 여부 확인\n\n데이터 검증 알고리즘 사용\n\n예: (알고리즘 검증을 통과한 데이터 수 / 전체 데이터 수) * 100\n방법: 체크섬, 유효성 검사 알고리즘 등을 사용\n\n\n중요성\n\n신뢰성 있는 의사결정: 정확한 데이터는 올바른 비즈니스 결정을 내리는 기반이 된다.\n운영 효율성: 부정확한 데이터로 인한 오류와 재작업을 줄일 수 있다.\n고객 만족: 정확한 고객 정보는 더 나은 서비스 제공으로 이어진다.\n규제 준수: 많은 산업에서 데이터의 정확성은 법적 요구사항이다.\n비용 절감: 정확한 데이터는 불필요한 비용 발생을 방지한다.\n\n예시\n\n고객 주소 정확성\n\n측정: 우편번호와 주소의 일치 여부\n방법: 우편번호 데이터베이스와 비교\n목표: 95% 이상의 주소가 정확해야 함\n\n제품 가격 정확성\n\n측정: 시스템에 등록된 가격과 실제 판매 가격의 일치 여부\n방법: POS 데이터와 제품 카탈로그 비교\n목표: 99.9% 이상의 가격 정보가 정확해야 함\n\n재고 수량 정확성\n\n측정: 시스템상 재고량과 실제 재고량의 일치 여부\n방법: 정기적인 실사를 통한 비교\n목표: 97% 이상의 재고 정보가 정확해야 함\n\n금융 거래 정확성\n\n측정: 거래 기록의 정확성\n방법: 더블 엔트리 회계 시스템을 통한 검증\n목표: 100% 정확성 (모든 불일치는 조사 및 수정되어야 함)\n\n고객 연락처 정확성\n\n측정: 이메일 주소와 전화번호의 유효성\n방법: 이메일 발송 테스트, 전화번호 형식 검증\n목표: 90% 이상의 연락처 정보가 유효해야 함\n\n\n정확성 개선 전략\n\n데이터 입력 시 자동 검증 시스템 구축\n정기적인 데이터 클렌징 및 품질 검사 수행\n데이터 소스의 신뢰성 평가 및 관리\n직원 교육을 통한 데이터 입력 오류 최소화\n데이터 품질 관리 도구 활용\n데이터 정확성에 대한 책임자 지정\n지속적인 모니터링 및 피드백 시스템 구축\n\n\n\n\n\n\n완전성은 필요한 모든 데이터가 존재하는 정도\n즉, 데이터셋이 얼마나 빠짐없이 채워져 있는지, 그리고 필요한 모든 정보를 포함하고 있는지를 측정\n측정 방법\n\n널(Null) 값 또는 빈 값 검사\n\n(채워진 필드 수 / 전체 필드 수) * 100\n\n필수 필드 존재 여부 확인\n\n(모든 필수 필드가 채워진 레코드 수 / 전체 레코드 수) * 100\n\n데이터셋 완전성 검사\n\n(실제 레코드 수 / 예상되는 총 레코드 수) * 100\n\n시계열 데이터 완전성 검사\n\n(데이터가 있는 시간 단위 수 / 전체 시간 단위 수) * 100\n\n\n중요성\n\n정확한 분석: 완전한 데이터셋은 더 정확하고 신뢰할 수 있는 분석 결과 제공\n의사결정 지원: 누락된 데이터 없이 전체 그림을 볼 수 있어 더 나은 의사결정 가능\n프로세스 효율성: 필요한 모든 데이터가 있으면 업무 프로세스가 원활하게 진행\n고객 만족: 완전한 고객 정보는 더 나은 서비스와 경험 제공 가능\n규제 준수: 많은 산업에서 데이터의 완전성은 규제 요구사항의 일부\n\n구체적인 예시\n\n고객 프로필 완전성\n\n측정: 필수 고객 정보 필드의 완전성\n방법: (모든 필수 필드가 채워진 고객 프로필 수 / 전체 고객 프로필 수) * 100\n목표: 95% 이상의 고객 프로필이 모든 필수 정보를 포함해야 함\n필수 필드 예: 이름, 연락처, 이메일, 주소\n\n주문 데이터 완전성\n\n측정: 주문 관련 모든 필요 정보의 존재 여부\n방법: (모든 필요 정보가 있는 주문 수 / 전체 주문 수) * 100\n목표: 99% 이상의 주문이 모든 필요 정보를 포함해야 함\n필요 정보: 주문 ID, 고객 ID, 주문 날짜, 제품 ID, 수량, 가격, 배송 주소\n\n재무 보고 데이터 완전성\n\n측정: 월별 재무 보고서의 모든 필요 항목 존재 여부\n방법: (모든 필요 항목이 보고된 월 수 / 전체 보고 월 수) * 100\n목표: 100% (모든 월의 재무 보고서가 완전해야 함)\n필요 항목: 매출, 비용, 순이익, 자산, 부채, 자본 등\n\n센서 데이터 완전성\n\n측정: IoT 센서에서 수집된 데이터의 시간별 완전성\n방법: (데이터가 수집된 시간 단위 수 / 24시간) * 100 (일일 기준)\n목표: 99.9% 이상 (하루 중 대부분의 시간에 데이터가 수집되어야 함)\n\n제품 카탈로그 완전성\n\n측정: 제품 정보의 완전성\n방법: (모든 필요 정보가 있는 제품 수 / 전체 제품 수) * 100\n목표: 98% 이상의 제품이 모든 필요 정보를 포함해야 함\n필요 정보: 제품명, 설명, 가격, 카테고리, 이미지, 재고 상태 등\n\n\n완전성 개선 전략\n\n데이터 입력 시 필수 필드 설정 및 유효성 검사 구현\n데이터 수집 프로세스 자동화\n데이터 품질 모니터링 도구 사용\n정기적인 데이터 감사 및 클렌징 작업 수행\n사용자 교육 및 데이터 입력 가이드라인 제공\n데이터 보완을 위한 외부 데이터 소스 활용\n데이터 거버넌스 정책 수립 및 시행\n\n\n\n\n\n\n일관성은 데이터가 여러 위치, 시스템, 또는 표현 방식에서 서로 모순 없이 일치하는 정도\n이는 데이터의 내부적 일관성(동일 데이터셋 내)과 외부적 일관성(여러 데이터셋 간)을 모두 포함\n측정 방법\n\n크로스 체크 (여러 테이블/시스템 간 데이터 비교)\n\n(일치하는 데이터 항목 수 / 전체 비교 데이터 항목 수) * 100\n\n중복 데이터 검사\n\n(고유한 데이터 항목 수 / 전체 데이터 항목 수) * 100\n\n데이터 형식의 일관성 검사\n\n(표준 형식을 따르는 데이터 항목 수 / 전체 데이터 항목 수) * 100\n\n참조 무결성 검사\n\n(유효한 참조를 가진 외래 키 수 / 전체 외래 키 수) * 100\n\n\n중요성\n\n데이터 신뢰성\n\n일관된 데이터는 신뢰할 수 있는 정보 제공\n여러 시스템이나 채널에서 일관된 정보를 제공함으로써 고객과 내부 사용자의 신뢰를 얻을 수 있다.\n\n효율적인 운영\n\n일관된 데이터는 업무 프로세스의 효율성을 높이고, 데이터 불일치로 인한 추가 작업을 줄일 수 있다.\n\n\n의사결정 지원\n\n모순 없는 데이터를 기반으로 한 일관된 의사결정 가능\n\n시스템 통합\n\n여러 시스템 간 원활한 데이터 교환 및 통합 지원\n\n사용자 경험\n\n일관된 데이터로 인한 사용자 혼란 방지\n\n정확한 보고 및 분석\n\n여러 소스의 데이터가 일관될 때, 더 정확하고 신뢰할 수 있는 비즈니스 인텔리전스와 분석이 가능\n\n비용 절감\n\n데이터 불일치로 인한 오류 수정 비용 감소\n\n규제 준수\n\n많은 산업에서 데이터의 일관성은 규제 요구사항의 일부이다. 일관된 데이터 관리는 컴플라이언스를 지원.\n\n고객 경험 향상\n\n고객이 모든 접점에서 일관된 정보를 받을 때, 더 나은 고객 경험을 제공\n\n\n예시\n\n고객 정보의 일관성\n\n측정: CRM 시스템과 주문 시스템 간 고객 정보 일치 여부\n방법: 두 시스템의 고객 데이터를 주기적으로 비교\n목표: 95% 이상의 고객 정보가 두 시스템에서 일치해야 함\n\n제품 가격의 일관성\n\n측정: 온라인 스토어와 POS 시스템 간 제품 가격 일치 여부\n방법: 실시간 또는 일일 기준으로 두 시스템의 제품 가격 비교\n목표: 99.9% 이상의 제품 가격이 모든 판매 채널에서 일치해야 함\n\n재무 데이터의 일관성\n\n측정: 총계정원장과 보조원장 간 잔액 일치 여부\n방법: 월말 결산 시 원장 간 잔액 비교\n목표: 100% 일치 (모든 차이는 조정되고 설명되어야 함)\n\n주소 형식의 일관성\n\n측정: 정의된 주소 형식 준수 여부\n방법: 정규 표현식을 사용하여 주소 형식 검증\n목표: 90% 이상의 주소가 표준 형식을 따라야 함\n\n제품 카테고리의 일관성\n\n측정: 여러 시스템에서 동일한 제품에 대한 카테고리 분류 일치 여부\n방법: 제품 마스터 데이터와 각 시스템의 카테고리 정보 비교\n목표: 98% 이상의 제품이 모든 시스템에서 동일한 카테고리로 분류되어야 함\n\n\n일관성 개선 전략\n\n데이터 통합 솔루션 구현 (예: 마스터 데이터 관리 시스템)\n데이터 동기화 메커니즘 개선\n데이터 거버넌스 정책 수립 및 시행\n데이터 입력 및 수정 프로세스 표준화\n정기적인 데이터 감사 및 정화 작업 수행\n시스템 간 실시간 데이터 교환 체계 구축\n데이터 소유권 및 책임 명확화\n\n\n\n\n\n\n유효성은 데이터가 정의된 비즈니스 규칙, 데이터 타입, 범위, 형식 등을 준수하는 정도\n즉, 데이터가 논리적으로 타당하고 비즈니스 컨텍스트에서 의미 있는지를 측정하는 지표\n중요성\n\n데이터 무결성: 시스템의 전반적인 데이터 무결성 보장\n오류 방지: 잘못된 데이터로 인한 비즈니스 프로세스 오류 예방\n분석 신뢰성: 유효한 데이터를 기반으로 한 신뢰할 수 있는 분석 결과 도출\n시스템 호환성: 다양한 시스템 간 데이터 교환 시 문제 방지\n\n측정 방법\n\n데이터 타입 검사\n\n(올바른 데이터 타입을 가진 필드 수 / 전체 필드 수) * 100\n\n값 범위 검사\n\n(정의된 범위 내의 값을 가진 레코드 수 / 전체 레코드 수) * 100\n\n형식 검사\n\n(올바른 형식을 가진 데이터 항목 수 / 전체 데이터 항목 수) * 100\n\n비즈니스 규칙 준수 검사\n\n(비즈니스 규칙을 준수하는 레코드 수 / 전체 레코드 수) * 100\n\n\n예시\n\n이메일 주소 유효성\n\n측정: 올바른 이메일 형식 준수 여부\n방법: 정규 표현식을 사용하여 이메일 주소 형식 검증\n예시 규칙: 1+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$\n목표: 95% 이상의 이메일 주소가 올바른 형식을 가져야 함\n\n나이 데이터 유효성\n\n측정: 나이 값의 논리적 범위 준수\n방법: 0 &lt; 나이 &lt;= 120 범위 검사\n목표: 100%의 나이 데이터가 유효한 범위 내에 있어야 함\n\n주문 데이터 유효성\n\n측정: 주문 금액과 주문 항목 수량의 논리적 일관성\n방법: 주문 총액 = Σ(항목 가격 * 수량) 검증\n목표: 99.9% 이상의 주문 데이터가 이 규칙을 만족해야 함\n\n신용카드 번호 유효성\n\n측정: Luhn 알고리즘을 사용한 신용카드 번호 유효성 검사\n방법: Luhn 알고리즘 적용 후 유효성 확인\n목표: 100%의 신용카드 번호가 Luhn 알고리즘을 통과해야 함\n\n날짜 데이터 유효성\n\n측정: 날짜 형식 및 논리적 타당성 검사\n방법: YYYY-MM-DD 형식 준수 및 존재하는 날짜인지 확인 (예: 2023-02-30은 유효하지 않음)\n목표: 100%의 날짜 데이터가 올바른 형식과 유효한 날짜여야 함\n\n\n유효성 개선 전략\n\n데이터 입력 시점의 유효성 검사 구현\n정기적인 데이터 클렌징 프로세스 수립\n비즈니스 규칙 엔진 도입\n데이터 품질 관리 도구 활용\n사용자 교육 및 가이드라인 제공\n데이터 모델링 및 스키마 설계 시 제약조건 적용\n\n\n\n\n\n\n데이터가 필요한 시점에 이용 가능한 정도와 최신 상태인 정도를 나타냅\n\n즉, 데이터가 현실 세계의 상태를 얼마나 잘 반영하고 있는지를 측정하는 지표\n\n중요성\n\n의사결정: 최신 데이터를 기반으로 한 신속하고 정확한 의사결정 가능 중요성\n운영 효율성: 실시간 또는 최신 데이터로 업무 프로세스 최적화 중요성\n고객 만족: 최신 정보를 기반으로 한 서비스 제공으로 고객 경험 향상\n\n측정 방법\n\n데이터 갱신 주기 확인\n\n(정해진 주기 내 업데이트된 레코드 수 / 전체 레코드 수) * 100\n\n실시간 데이터와 저장된 데이터의 시간 차이 측정\n\n평균 데이터 지연 시간 = Σ(현재 시간 - 데이터 최종 업데이트 시간) / 전체 레코드 수\n\n데이터 생성 시점과 사용 가능 시점의 차이 측정\n\n평균 데이터 가용 지연 = Σ(데이터 사용 가능 시간 - 데이터 생성 시간) / 전체 데이터 수\n\n\n예시\n\n재고 관리 시스템\n\n측정: 실제 재고량과 시스템상 재고량의 일치 비율\n방법: (1시간 이내 업데이트된 재고 항목 수 / 전체 재고 항목 수) * 100\n목표: 95% 이상의 재고 정보가 1시간 이내에 업데이트되어야 함\n\n금융 거래 시스템\n\n측정: 거래 발생부터 시스템 반영까지의 평균 시간\n방법: Σ(거래 반영 시간 - 거래 발생 시간) / 전체 거래 수\n목표: 평균 지연 시간 5초 이내\n\n고객 정보 관리 시스템\n\n측정: 고객 정보 변경사항의 반영 속도\n방법: (24시간 이내 업데이트된 고객 정보 변경 건수 / 전체 고객 정보 변경 요청 건수) * 100\n목표: 99% 이상의 고객 정보 변경사항이 24시간 이내에 반영되어야 함\n\n적시성 개선 전략\n\n실시간 데이터 처리 시스템 구축\n데이터 동기화 주기 최적화\n데이터 파이프라인 효율성 향상\n중요 데이터에 대한 우선순위 처리 체계 수립\n데이터 갱신 알림 시스템 구축"
  },
  {
    "objectID": "docs/blog/posts/Governance/8.data_quality_control.html#footnotes",
    "href": "docs/blog/posts/Governance/8.data_quality_control.html#footnotes",
    "title": "Data Governance Study - Data Quality Management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\na-zA-Z0-9._%+-↩︎"
  },
  {
    "objectID": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#learning-more",
    "href": "docs/blog/posts/Governance/5_1.data_word_dictionary.html#learning-more",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "표준 도메인 사전\n데이터 표준화\n\n\n이전 블로그 다음 블로그"
  },
  {
    "objectID": "docs/blog/posts/Governance/8-0.data_quality_control.html",
    "href": "docs/blog/posts/Governance/8-0.data_quality_control.html",
    "title": "Data Governance Study - Data Quality Management",
    "section": "",
    "text": "데이터가 실제 값이나 참조 소스와 일치하는 정도\n측정 방법\n\n샘플링을 통한 수동 검증\n\n(정확한 샘플 수 / 전체 샘플 수) * 100\n방법: 무작위로 선택된 데이터 샘플을 실제 값과 비교\n\n참조 데이터와의 비교\n\n예: (참조 데이터와 일치하는 레코드 수 / 전체 레코드 수) * 100\n방법: 신뢰할 수 있는 외부 데이터 소스와 비교\n\n비즈니스 규칙 위반 검사\n\n예: (비즈니스 규칙을 준수하는 레코드 수 / 전체 레코드 수) * 100\n방법: 미리 정의된 비즈니스 규칙에 대한 준수 여부 확인\n\n데이터 검증 알고리즘 사용\n\n예: (알고리즘 검증을 통과한 데이터 수 / 전체 데이터 수) * 100\n방법: 체크섬, 유효성 검사 알고리즘 등을 사용\n\n\n중요성\n\n신뢰성 있는 의사결정: 정확한 데이터는 올바른 비즈니스 결정을 내리는 기반이 된다.\n운영 효율성: 부정확한 데이터로 인한 오류와 재작업을 줄일 수 있다.\n고객 만족: 정확한 고객 정보는 더 나은 서비스 제공으로 이어진다.\n규제 준수: 많은 산업에서 데이터의 정확성은 법적 요구사항이다.\n비용 절감: 정확한 데이터는 불필요한 비용 발생을 방지한다.\n\n예시\n\n고객 주소 정확성\n\n측정: 우편번호와 주소의 일치 여부\n방법: 우편번호 데이터베이스와 비교\n목표: 95% 이상의 주소가 정확해야 함\n\n제품 가격 정확성\n\n측정: 시스템에 등록된 가격과 실제 판매 가격의 일치 여부\n방법: POS 데이터와 제품 카탈로그 비교\n목표: 99.9% 이상의 가격 정보가 정확해야 함\n\n재고 수량 정확성\n\n측정: 시스템상 재고량과 실제 재고량의 일치 여부\n방법: 정기적인 실사를 통한 비교\n목표: 97% 이상의 재고 정보가 정확해야 함\n\n금융 거래 정확성\n\n측정: 거래 기록의 정확성\n방법: 더블 엔트리 회계 시스템을 통한 검증\n목표: 100% 정확성 (모든 불일치는 조사 및 수정되어야 함)\n\n고객 연락처 정확성\n\n측정: 이메일 주소와 전화번호의 유효성\n방법: 이메일 발송 테스트, 전화번호 형식 검증\n목표: 90% 이상의 연락처 정보가 유효해야 함\n\n\n정확성 개선 전략\n\n데이터 입력 시 자동 검증 시스템 구축\n정기적인 데이터 클렌징 및 품질 검사 수행\n데이터 소스의 신뢰성 평가 및 관리\n직원 교육을 통한 데이터 입력 오류 최소화\n데이터 품질 관리 도구 활용\n데이터 정확성에 대한 책임자 지정\n지속적인 모니터링 및 피드백 시스템 구축\n\n\n\n\n\n\n완전성은 필요한 모든 데이터가 존재하는 정도\n즉, 데이터셋이 얼마나 빠짐없이 채워져 있는지, 그리고 필요한 모든 정보를 포함하고 있는지를 측정\n측정 방법\n\n널(Null) 값 또는 빈 값 검사\n\n(채워진 필드 수 / 전체 필드 수) * 100\n\n필수 필드 존재 여부 확인\n\n(모든 필수 필드가 채워진 레코드 수 / 전체 레코드 수) * 100\n\n데이터셋 완전성 검사\n\n(실제 레코드 수 / 예상되는 총 레코드 수) * 100\n\n시계열 데이터 완전성 검사\n\n(데이터가 있는 시간 단위 수 / 전체 시간 단위 수) * 100\n\n\n중요성\n\n정확한 분석: 완전한 데이터셋은 더 정확하고 신뢰할 수 있는 분석 결과 제공\n의사결정 지원: 누락된 데이터 없이 전체 그림을 볼 수 있어 더 나은 의사결정 가능\n프로세스 효율성: 필요한 모든 데이터가 있으면 업무 프로세스가 원활하게 진행\n고객 만족: 완전한 고객 정보는 더 나은 서비스와 경험 제공 가능\n규제 준수: 많은 산업에서 데이터의 완전성은 규제 요구사항의 일부\n\n구체적인 예시\n\n고객 프로필 완전성\n\n측정: 필수 고객 정보 필드의 완전성\n방법: (모든 필수 필드가 채워진 고객 프로필 수 / 전체 고객 프로필 수) * 100\n목표: 95% 이상의 고객 프로필이 모든 필수 정보를 포함해야 함\n필수 필드 예: 이름, 연락처, 이메일, 주소\n\n주문 데이터 완전성\n\n측정: 주문 관련 모든 필요 정보의 존재 여부\n방법: (모든 필요 정보가 있는 주문 수 / 전체 주문 수) * 100\n목표: 99% 이상의 주문이 모든 필요 정보를 포함해야 함\n필요 정보: 주문 ID, 고객 ID, 주문 날짜, 제품 ID, 수량, 가격, 배송 주소\n\n재무 보고 데이터 완전성\n\n측정: 월별 재무 보고서의 모든 필요 항목 존재 여부\n방법: (모든 필요 항목이 보고된 월 수 / 전체 보고 월 수) * 100\n목표: 100% (모든 월의 재무 보고서가 완전해야 함)\n필요 항목: 매출, 비용, 순이익, 자산, 부채, 자본 등\n\n센서 데이터 완전성\n\n측정: IoT 센서에서 수집된 데이터의 시간별 완전성\n방법: (데이터가 수집된 시간 단위 수 / 24시간) * 100 (일일 기준)\n목표: 99.9% 이상 (하루 중 대부분의 시간에 데이터가 수집되어야 함)\n\n제품 카탈로그 완전성\n\n측정: 제품 정보의 완전성\n방법: (모든 필요 정보가 있는 제품 수 / 전체 제품 수) * 100\n목표: 98% 이상의 제품이 모든 필요 정보를 포함해야 함\n필요 정보: 제품명, 설명, 가격, 카테고리, 이미지, 재고 상태 등\n\n\n완전성 개선 전략\n\n데이터 입력 시 필수 필드 설정 및 유효성 검사 구현\n데이터 수집 프로세스 자동화\n데이터 품질 모니터링 도구 사용\n정기적인 데이터 감사 및 클렌징 작업 수행\n사용자 교육 및 데이터 입력 가이드라인 제공\n데이터 보완을 위한 외부 데이터 소스 활용\n데이터 거버넌스 정책 수립 및 시행\n\n\n\n\n\n\n일관성은 데이터가 여러 위치, 시스템, 또는 표현 방식에서 서로 모순 없이 일치하는 정도\n이는 데이터의 내부적 일관성(동일 데이터셋 내)과 외부적 일관성(여러 데이터셋 간)을 모두 포함\n측정 방법\n\n크로스 체크 (여러 테이블/시스템 간 데이터 비교)\n\n(일치하는 데이터 항목 수 / 전체 비교 데이터 항목 수) * 100\n\n중복 데이터 검사\n\n(고유한 데이터 항목 수 / 전체 데이터 항목 수) * 100\n\n데이터 형식의 일관성 검사\n\n(표준 형식을 따르는 데이터 항목 수 / 전체 데이터 항목 수) * 100\n\n참조 무결성 검사\n\n(유효한 참조를 가진 외래 키 수 / 전체 외래 키 수) * 100\n\n\n중요성\n\n데이터 신뢰성\n\n일관된 데이터는 신뢰할 수 있는 정보 제공\n여러 시스템이나 채널에서 일관된 정보를 제공함으로써 고객과 내부 사용자의 신뢰를 얻을 수 있다.\n\n효율적인 운영\n\n일관된 데이터는 업무 프로세스의 효율성을 높이고, 데이터 불일치로 인한 추가 작업을 줄일 수 있다.\n\n\n의사결정 지원\n\n모순 없는 데이터를 기반으로 한 일관된 의사결정 가능\n\n시스템 통합\n\n여러 시스템 간 원활한 데이터 교환 및 통합 지원\n\n사용자 경험\n\n일관된 데이터로 인한 사용자 혼란 방지\n\n정확한 보고 및 분석\n\n여러 소스의 데이터가 일관될 때, 더 정확하고 신뢰할 수 있는 비즈니스 인텔리전스와 분석이 가능\n\n비용 절감\n\n데이터 불일치로 인한 오류 수정 비용 감소\n\n규제 준수\n\n많은 산업에서 데이터의 일관성은 규제 요구사항의 일부이다. 일관된 데이터 관리는 컴플라이언스를 지원.\n\n고객 경험 향상\n\n고객이 모든 접점에서 일관된 정보를 받을 때, 더 나은 고객 경험을 제공\n\n\n예시\n\n고객 정보의 일관성\n\n측정: CRM 시스템과 주문 시스템 간 고객 정보 일치 여부\n방법: 두 시스템의 고객 데이터를 주기적으로 비교\n목표: 95% 이상의 고객 정보가 두 시스템에서 일치해야 함\n\n제품 가격의 일관성\n\n측정: 온라인 스토어와 POS 시스템 간 제품 가격 일치 여부\n방법: 실시간 또는 일일 기준으로 두 시스템의 제품 가격 비교\n목표: 99.9% 이상의 제품 가격이 모든 판매 채널에서 일치해야 함\n\n재무 데이터의 일관성\n\n측정: 총계정원장과 보조원장 간 잔액 일치 여부\n방법: 월말 결산 시 원장 간 잔액 비교\n목표: 100% 일치 (모든 차이는 조정되고 설명되어야 함)\n\n주소 형식의 일관성\n\n측정: 정의된 주소 형식 준수 여부\n방법: 정규 표현식을 사용하여 주소 형식 검증\n목표: 90% 이상의 주소가 표준 형식을 따라야 함\n\n제품 카테고리의 일관성\n\n측정: 여러 시스템에서 동일한 제품에 대한 카테고리 분류 일치 여부\n방법: 제품 마스터 데이터와 각 시스템의 카테고리 정보 비교\n목표: 98% 이상의 제품이 모든 시스템에서 동일한 카테고리로 분류되어야 함\n\n\n일관성 개선 전략\n\n데이터 통합 솔루션 구현 (예: 마스터 데이터 관리 시스템)\n데이터 동기화 메커니즘 개선\n데이터 거버넌스 정책 수립 및 시행\n데이터 입력 및 수정 프로세스 표준화\n정기적인 데이터 감사 및 정화 작업 수행\n시스템 간 실시간 데이터 교환 체계 구축\n데이터 소유권 및 책임 명확화\n\n\n\n\n\n\n유효성은 데이터가 정의된 비즈니스 규칙, 데이터 타입, 범위, 형식 등을 준수하는 정도\n즉, 데이터가 논리적으로 타당하고 비즈니스 컨텍스트에서 의미 있는지를 측정하는 지표\n중요성\n\n데이터 무결성: 시스템의 전반적인 데이터 무결성 보장\n오류 방지: 잘못된 데이터로 인한 비즈니스 프로세스 오류 예방\n분석 신뢰성: 유효한 데이터를 기반으로 한 신뢰할 수 있는 분석 결과 도출\n시스템 호환성: 다양한 시스템 간 데이터 교환 시 문제 방지\n\n측정 방법\n\n데이터 타입 검사\n\n(올바른 데이터 타입을 가진 필드 수 / 전체 필드 수) * 100\n\n값 범위 검사\n\n(정의된 범위 내의 값을 가진 레코드 수 / 전체 레코드 수) * 100\n\n형식 검사\n\n(올바른 형식을 가진 데이터 항목 수 / 전체 데이터 항목 수) * 100\n\n비즈니스 규칙 준수 검사\n\n(비즈니스 규칙을 준수하는 레코드 수 / 전체 레코드 수) * 100\n\n\n예시\n\n이메일 주소 유효성\n\n측정: 올바른 이메일 형식 준수 여부\n방법: 정규 표현식을 사용하여 이메일 주소 형식 검증\n예시 규칙: 1+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$\n목표: 95% 이상의 이메일 주소가 올바른 형식을 가져야 함\n\n나이 데이터 유효성\n\n측정: 나이 값의 논리적 범위 준수\n방법: 0 &lt; 나이 &lt;= 120 범위 검사\n목표: 100%의 나이 데이터가 유효한 범위 내에 있어야 함\n\n주문 데이터 유효성\n\n측정: 주문 금액과 주문 항목 수량의 논리적 일관성\n방법: 주문 총액 = Σ(항목 가격 * 수량) 검증\n목표: 99.9% 이상의 주문 데이터가 이 규칙을 만족해야 함\n\n신용카드 번호 유효성\n\n측정: Luhn 알고리즘을 사용한 신용카드 번호 유효성 검사\n방법: Luhn 알고리즘 적용 후 유효성 확인\n목표: 100%의 신용카드 번호가 Luhn 알고리즘을 통과해야 함\n\n날짜 데이터 유효성\n\n측정: 날짜 형식 및 논리적 타당성 검사\n방법: YYYY-MM-DD 형식 준수 및 존재하는 날짜인지 확인 (예: 2023-02-30은 유효하지 않음)\n목표: 100%의 날짜 데이터가 올바른 형식과 유효한 날짜여야 함\n\n\n유효성 개선 전략\n\n데이터 입력 시점의 유효성 검사 구현\n정기적인 데이터 클렌징 프로세스 수립\n비즈니스 규칙 엔진 도입\n데이터 품질 관리 도구 활용\n사용자 교육 및 가이드라인 제공\n데이터 모델링 및 스키마 설계 시 제약조건 적용\n\n\n\n\n\n\n데이터가 필요한 시점에 이용 가능한 정도와 최신 상태인 정도를 나타냅\n\n즉, 데이터가 현실 세계의 상태를 얼마나 잘 반영하고 있는지를 측정하는 지표\n\n중요성\n\n의사결정: 최신 데이터를 기반으로 한 신속하고 정확한 의사결정 가능 중요성\n운영 효율성: 실시간 또는 최신 데이터로 업무 프로세스 최적화 중요성\n고객 만족: 최신 정보를 기반으로 한 서비스 제공으로 고객 경험 향상\n\n측정 방법\n\n데이터 갱신 주기 확인\n\n(정해진 주기 내 업데이트된 레코드 수 / 전체 레코드 수) * 100\n\n실시간 데이터와 저장된 데이터의 시간 차이 측정\n\n평균 데이터 지연 시간 = Σ(현재 시간 - 데이터 최종 업데이트 시간) / 전체 레코드 수\n\n데이터 생성 시점과 사용 가능 시점의 차이 측정\n\n평균 데이터 가용 지연 = Σ(데이터 사용 가능 시간 - 데이터 생성 시간) / 전체 데이터 수\n\n\n예시\n\n재고 관리 시스템\n\n측정: 실제 재고량과 시스템상 재고량의 일치 비율\n방법: (1시간 이내 업데이트된 재고 항목 수 / 전체 재고 항목 수) * 100\n목표: 95% 이상의 재고 정보가 1시간 이내에 업데이트되어야 함\n\n금융 거래 시스템\n\n측정: 거래 발생부터 시스템 반영까지의 평균 시간\n방법: Σ(거래 반영 시간 - 거래 발생 시간) / 전체 거래 수\n목표: 평균 지연 시간 5초 이내\n\n고객 정보 관리 시스템\n\n측정: 고객 정보 변경사항의 반영 속도\n방법: (24시간 이내 업데이트된 고객 정보 변경 건수 / 전체 고객 정보 변경 요청 건수) * 100\n목표: 99% 이상의 고객 정보 변경사항이 24시간 이내에 반영되어야 함\n\n적시성 개선 전략\n\n실시간 데이터 처리 시스템 구축\n데이터 동기화 주기 최적화\n데이터 파이프라인 효율성 향상\n중요 데이터에 대한 우선순위 처리 체계 수립\n데이터 갱신 알림 시스템 구축"
  },
  {
    "objectID": "docs/blog/posts/Governance/8-0.data_quality_control.html#data-standard-governance-data-quality-management",
    "href": "docs/blog/posts/Governance/8-0.data_quality_control.html#data-standard-governance-data-quality-management",
    "title": "Data Governance Study - Data Quality Management",
    "section": "",
    "text": "데이터가 실제 값이나 참조 소스와 일치하는 정도\n측정 방법\n\n샘플링을 통한 수동 검증\n\n(정확한 샘플 수 / 전체 샘플 수) * 100\n방법: 무작위로 선택된 데이터 샘플을 실제 값과 비교\n\n참조 데이터와의 비교\n\n예: (참조 데이터와 일치하는 레코드 수 / 전체 레코드 수) * 100\n방법: 신뢰할 수 있는 외부 데이터 소스와 비교\n\n비즈니스 규칙 위반 검사\n\n예: (비즈니스 규칙을 준수하는 레코드 수 / 전체 레코드 수) * 100\n방법: 미리 정의된 비즈니스 규칙에 대한 준수 여부 확인\n\n데이터 검증 알고리즘 사용\n\n예: (알고리즘 검증을 통과한 데이터 수 / 전체 데이터 수) * 100\n방법: 체크섬, 유효성 검사 알고리즘 등을 사용\n\n\n중요성\n\n신뢰성 있는 의사결정: 정확한 데이터는 올바른 비즈니스 결정을 내리는 기반이 된다.\n운영 효율성: 부정확한 데이터로 인한 오류와 재작업을 줄일 수 있다.\n고객 만족: 정확한 고객 정보는 더 나은 서비스 제공으로 이어진다.\n규제 준수: 많은 산업에서 데이터의 정확성은 법적 요구사항이다.\n비용 절감: 정확한 데이터는 불필요한 비용 발생을 방지한다.\n\n예시\n\n고객 주소 정확성\n\n측정: 우편번호와 주소의 일치 여부\n방법: 우편번호 데이터베이스와 비교\n목표: 95% 이상의 주소가 정확해야 함\n\n제품 가격 정확성\n\n측정: 시스템에 등록된 가격과 실제 판매 가격의 일치 여부\n방법: POS 데이터와 제품 카탈로그 비교\n목표: 99.9% 이상의 가격 정보가 정확해야 함\n\n재고 수량 정확성\n\n측정: 시스템상 재고량과 실제 재고량의 일치 여부\n방법: 정기적인 실사를 통한 비교\n목표: 97% 이상의 재고 정보가 정확해야 함\n\n금융 거래 정확성\n\n측정: 거래 기록의 정확성\n방법: 더블 엔트리 회계 시스템을 통한 검증\n목표: 100% 정확성 (모든 불일치는 조사 및 수정되어야 함)\n\n고객 연락처 정확성\n\n측정: 이메일 주소와 전화번호의 유효성\n방법: 이메일 발송 테스트, 전화번호 형식 검증\n목표: 90% 이상의 연락처 정보가 유효해야 함\n\n\n정확성 개선 전략\n\n데이터 입력 시 자동 검증 시스템 구축\n정기적인 데이터 클렌징 및 품질 검사 수행\n데이터 소스의 신뢰성 평가 및 관리\n직원 교육을 통한 데이터 입력 오류 최소화\n데이터 품질 관리 도구 활용\n데이터 정확성에 대한 책임자 지정\n지속적인 모니터링 및 피드백 시스템 구축\n\n\n\n\n\n\n완전성은 필요한 모든 데이터가 존재하는 정도\n즉, 데이터셋이 얼마나 빠짐없이 채워져 있는지, 그리고 필요한 모든 정보를 포함하고 있는지를 측정\n측정 방법\n\n널(Null) 값 또는 빈 값 검사\n\n(채워진 필드 수 / 전체 필드 수) * 100\n\n필수 필드 존재 여부 확인\n\n(모든 필수 필드가 채워진 레코드 수 / 전체 레코드 수) * 100\n\n데이터셋 완전성 검사\n\n(실제 레코드 수 / 예상되는 총 레코드 수) * 100\n\n시계열 데이터 완전성 검사\n\n(데이터가 있는 시간 단위 수 / 전체 시간 단위 수) * 100\n\n\n중요성\n\n정확한 분석: 완전한 데이터셋은 더 정확하고 신뢰할 수 있는 분석 결과 제공\n의사결정 지원: 누락된 데이터 없이 전체 그림을 볼 수 있어 더 나은 의사결정 가능\n프로세스 효율성: 필요한 모든 데이터가 있으면 업무 프로세스가 원활하게 진행\n고객 만족: 완전한 고객 정보는 더 나은 서비스와 경험 제공 가능\n규제 준수: 많은 산업에서 데이터의 완전성은 규제 요구사항의 일부\n\n구체적인 예시\n\n고객 프로필 완전성\n\n측정: 필수 고객 정보 필드의 완전성\n방법: (모든 필수 필드가 채워진 고객 프로필 수 / 전체 고객 프로필 수) * 100\n목표: 95% 이상의 고객 프로필이 모든 필수 정보를 포함해야 함\n필수 필드 예: 이름, 연락처, 이메일, 주소\n\n주문 데이터 완전성\n\n측정: 주문 관련 모든 필요 정보의 존재 여부\n방법: (모든 필요 정보가 있는 주문 수 / 전체 주문 수) * 100\n목표: 99% 이상의 주문이 모든 필요 정보를 포함해야 함\n필요 정보: 주문 ID, 고객 ID, 주문 날짜, 제품 ID, 수량, 가격, 배송 주소\n\n재무 보고 데이터 완전성\n\n측정: 월별 재무 보고서의 모든 필요 항목 존재 여부\n방법: (모든 필요 항목이 보고된 월 수 / 전체 보고 월 수) * 100\n목표: 100% (모든 월의 재무 보고서가 완전해야 함)\n필요 항목: 매출, 비용, 순이익, 자산, 부채, 자본 등\n\n센서 데이터 완전성\n\n측정: IoT 센서에서 수집된 데이터의 시간별 완전성\n방법: (데이터가 수집된 시간 단위 수 / 24시간) * 100 (일일 기준)\n목표: 99.9% 이상 (하루 중 대부분의 시간에 데이터가 수집되어야 함)\n\n제품 카탈로그 완전성\n\n측정: 제품 정보의 완전성\n방법: (모든 필요 정보가 있는 제품 수 / 전체 제품 수) * 100\n목표: 98% 이상의 제품이 모든 필요 정보를 포함해야 함\n필요 정보: 제품명, 설명, 가격, 카테고리, 이미지, 재고 상태 등\n\n\n완전성 개선 전략\n\n데이터 입력 시 필수 필드 설정 및 유효성 검사 구현\n데이터 수집 프로세스 자동화\n데이터 품질 모니터링 도구 사용\n정기적인 데이터 감사 및 클렌징 작업 수행\n사용자 교육 및 데이터 입력 가이드라인 제공\n데이터 보완을 위한 외부 데이터 소스 활용\n데이터 거버넌스 정책 수립 및 시행\n\n\n\n\n\n\n일관성은 데이터가 여러 위치, 시스템, 또는 표현 방식에서 서로 모순 없이 일치하는 정도\n이는 데이터의 내부적 일관성(동일 데이터셋 내)과 외부적 일관성(여러 데이터셋 간)을 모두 포함\n측정 방법\n\n크로스 체크 (여러 테이블/시스템 간 데이터 비교)\n\n(일치하는 데이터 항목 수 / 전체 비교 데이터 항목 수) * 100\n\n중복 데이터 검사\n\n(고유한 데이터 항목 수 / 전체 데이터 항목 수) * 100\n\n데이터 형식의 일관성 검사\n\n(표준 형식을 따르는 데이터 항목 수 / 전체 데이터 항목 수) * 100\n\n참조 무결성 검사\n\n(유효한 참조를 가진 외래 키 수 / 전체 외래 키 수) * 100\n\n\n중요성\n\n데이터 신뢰성\n\n일관된 데이터는 신뢰할 수 있는 정보 제공\n여러 시스템이나 채널에서 일관된 정보를 제공함으로써 고객과 내부 사용자의 신뢰를 얻을 수 있다.\n\n효율적인 운영\n\n일관된 데이터는 업무 프로세스의 효율성을 높이고, 데이터 불일치로 인한 추가 작업을 줄일 수 있다.\n\n\n의사결정 지원\n\n모순 없는 데이터를 기반으로 한 일관된 의사결정 가능\n\n시스템 통합\n\n여러 시스템 간 원활한 데이터 교환 및 통합 지원\n\n사용자 경험\n\n일관된 데이터로 인한 사용자 혼란 방지\n\n정확한 보고 및 분석\n\n여러 소스의 데이터가 일관될 때, 더 정확하고 신뢰할 수 있는 비즈니스 인텔리전스와 분석이 가능\n\n비용 절감\n\n데이터 불일치로 인한 오류 수정 비용 감소\n\n규제 준수\n\n많은 산업에서 데이터의 일관성은 규제 요구사항의 일부이다. 일관된 데이터 관리는 컴플라이언스를 지원.\n\n고객 경험 향상\n\n고객이 모든 접점에서 일관된 정보를 받을 때, 더 나은 고객 경험을 제공\n\n\n예시\n\n고객 정보의 일관성\n\n측정: CRM 시스템과 주문 시스템 간 고객 정보 일치 여부\n방법: 두 시스템의 고객 데이터를 주기적으로 비교\n목표: 95% 이상의 고객 정보가 두 시스템에서 일치해야 함\n\n제품 가격의 일관성\n\n측정: 온라인 스토어와 POS 시스템 간 제품 가격 일치 여부\n방법: 실시간 또는 일일 기준으로 두 시스템의 제품 가격 비교\n목표: 99.9% 이상의 제품 가격이 모든 판매 채널에서 일치해야 함\n\n재무 데이터의 일관성\n\n측정: 총계정원장과 보조원장 간 잔액 일치 여부\n방법: 월말 결산 시 원장 간 잔액 비교\n목표: 100% 일치 (모든 차이는 조정되고 설명되어야 함)\n\n주소 형식의 일관성\n\n측정: 정의된 주소 형식 준수 여부\n방법: 정규 표현식을 사용하여 주소 형식 검증\n목표: 90% 이상의 주소가 표준 형식을 따라야 함\n\n제품 카테고리의 일관성\n\n측정: 여러 시스템에서 동일한 제품에 대한 카테고리 분류 일치 여부\n방법: 제품 마스터 데이터와 각 시스템의 카테고리 정보 비교\n목표: 98% 이상의 제품이 모든 시스템에서 동일한 카테고리로 분류되어야 함\n\n\n일관성 개선 전략\n\n데이터 통합 솔루션 구현 (예: 마스터 데이터 관리 시스템)\n데이터 동기화 메커니즘 개선\n데이터 거버넌스 정책 수립 및 시행\n데이터 입력 및 수정 프로세스 표준화\n정기적인 데이터 감사 및 정화 작업 수행\n시스템 간 실시간 데이터 교환 체계 구축\n데이터 소유권 및 책임 명확화\n\n\n\n\n\n\n유효성은 데이터가 정의된 비즈니스 규칙, 데이터 타입, 범위, 형식 등을 준수하는 정도\n즉, 데이터가 논리적으로 타당하고 비즈니스 컨텍스트에서 의미 있는지를 측정하는 지표\n중요성\n\n데이터 무결성: 시스템의 전반적인 데이터 무결성 보장\n오류 방지: 잘못된 데이터로 인한 비즈니스 프로세스 오류 예방\n분석 신뢰성: 유효한 데이터를 기반으로 한 신뢰할 수 있는 분석 결과 도출\n시스템 호환성: 다양한 시스템 간 데이터 교환 시 문제 방지\n\n측정 방법\n\n데이터 타입 검사\n\n(올바른 데이터 타입을 가진 필드 수 / 전체 필드 수) * 100\n\n값 범위 검사\n\n(정의된 범위 내의 값을 가진 레코드 수 / 전체 레코드 수) * 100\n\n형식 검사\n\n(올바른 형식을 가진 데이터 항목 수 / 전체 데이터 항목 수) * 100\n\n비즈니스 규칙 준수 검사\n\n(비즈니스 규칙을 준수하는 레코드 수 / 전체 레코드 수) * 100\n\n\n예시\n\n이메일 주소 유효성\n\n측정: 올바른 이메일 형식 준수 여부\n방법: 정규 표현식을 사용하여 이메일 주소 형식 검증\n예시 규칙: 1+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$\n목표: 95% 이상의 이메일 주소가 올바른 형식을 가져야 함\n\n나이 데이터 유효성\n\n측정: 나이 값의 논리적 범위 준수\n방법: 0 &lt; 나이 &lt;= 120 범위 검사\n목표: 100%의 나이 데이터가 유효한 범위 내에 있어야 함\n\n주문 데이터 유효성\n\n측정: 주문 금액과 주문 항목 수량의 논리적 일관성\n방법: 주문 총액 = Σ(항목 가격 * 수량) 검증\n목표: 99.9% 이상의 주문 데이터가 이 규칙을 만족해야 함\n\n신용카드 번호 유효성\n\n측정: Luhn 알고리즘을 사용한 신용카드 번호 유효성 검사\n방법: Luhn 알고리즘 적용 후 유효성 확인\n목표: 100%의 신용카드 번호가 Luhn 알고리즘을 통과해야 함\n\n날짜 데이터 유효성\n\n측정: 날짜 형식 및 논리적 타당성 검사\n방법: YYYY-MM-DD 형식 준수 및 존재하는 날짜인지 확인 (예: 2023-02-30은 유효하지 않음)\n목표: 100%의 날짜 데이터가 올바른 형식과 유효한 날짜여야 함\n\n\n유효성 개선 전략\n\n데이터 입력 시점의 유효성 검사 구현\n정기적인 데이터 클렌징 프로세스 수립\n비즈니스 규칙 엔진 도입\n데이터 품질 관리 도구 활용\n사용자 교육 및 가이드라인 제공\n데이터 모델링 및 스키마 설계 시 제약조건 적용\n\n\n\n\n\n\n데이터가 필요한 시점에 이용 가능한 정도와 최신 상태인 정도를 나타냅\n\n즉, 데이터가 현실 세계의 상태를 얼마나 잘 반영하고 있는지를 측정하는 지표\n\n중요성\n\n의사결정: 최신 데이터를 기반으로 한 신속하고 정확한 의사결정 가능 중요성\n운영 효율성: 실시간 또는 최신 데이터로 업무 프로세스 최적화 중요성\n고객 만족: 최신 정보를 기반으로 한 서비스 제공으로 고객 경험 향상\n\n측정 방법\n\n데이터 갱신 주기 확인\n\n(정해진 주기 내 업데이트된 레코드 수 / 전체 레코드 수) * 100\n\n실시간 데이터와 저장된 데이터의 시간 차이 측정\n\n평균 데이터 지연 시간 = Σ(현재 시간 - 데이터 최종 업데이트 시간) / 전체 레코드 수\n\n데이터 생성 시점과 사용 가능 시점의 차이 측정\n\n평균 데이터 가용 지연 = Σ(데이터 사용 가능 시간 - 데이터 생성 시간) / 전체 데이터 수\n\n\n예시\n\n재고 관리 시스템\n\n측정: 실제 재고량과 시스템상 재고량의 일치 비율\n방법: (1시간 이내 업데이트된 재고 항목 수 / 전체 재고 항목 수) * 100\n목표: 95% 이상의 재고 정보가 1시간 이내에 업데이트되어야 함\n\n금융 거래 시스템\n\n측정: 거래 발생부터 시스템 반영까지의 평균 시간\n방법: Σ(거래 반영 시간 - 거래 발생 시간) / 전체 거래 수\n목표: 평균 지연 시간 5초 이내\n\n고객 정보 관리 시스템\n\n측정: 고객 정보 변경사항의 반영 속도\n방법: (24시간 이내 업데이트된 고객 정보 변경 건수 / 전체 고객 정보 변경 요청 건수) * 100\n목표: 99% 이상의 고객 정보 변경사항이 24시간 이내에 반영되어야 함\n\n적시성 개선 전략\n\n실시간 데이터 처리 시스템 구축\n데이터 동기화 주기 최적화\n데이터 파이프라인 효율성 향상\n중요 데이터에 대한 우선순위 처리 체계 수립\n데이터 갱신 알림 시스템 구축"
  },
  {
    "objectID": "docs/blog/posts/Governance/8-0.data_quality_control.html#footnotes",
    "href": "docs/blog/posts/Governance/8-0.data_quality_control.html#footnotes",
    "title": "Data Governance Study - Data Quality Management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\na-zA-Z0-9._%+-↩︎"
  },
  {
    "objectID": "docs/blog/posts/Governance/6-2.data_code_registration copy.html",
    "href": "docs/blog/posts/Governance/6-2.data_code_registration copy.html",
    "title": "Data Governance Study - Data Code Registration Process",
    "section": "",
    "text": "애플리케이션 파트(개발)가 코드 정보 도출하여 신규 및 변경 신청하고 표준 담당자가 승인한다.\n\n\n\n\n애플리케이션 파트(개발)가 코드 정보 도출하여 신규 및 변경 신청하고 표준 담당자가 승인한다.\n\n\n\n\n데이터 모델 표준 코드 등록 절차\n\n\n\n\n\n\n\n\n\n\n\n\n순번\nTASK\n설명\n담당\n비고\n\n\n\n\n1\n표준 코드 요건 발생\n애플리케이션 개발 중 신규 코드 요건 발생\n개발자\n\n\n\n2\n표준 코드 도출\n신규 등록이 필요한 코드 명, 값 등을 도출\n개발자\nEXCEL\n\n\n3\n코드 검색\n표준 코드 사전에서 해당 코드 검색\n개발자\n코드 사전\n\n\n4\n표준 코드 신청\n표준 코드 사전에 도출된 코드가 없을 경우 표준 담당자에게 신청\n개발자\n\n\n\n5\n코드 신청 접수\n표준 담당자는 개발자가 신청한 코드 표준 등록 요청 접수\n표준담당\n\n\n\n6\n타당성 검토\n기존 코드 사전을 기준으로 신규 코드 타당성 검토\n표준담당\n\n\n\n7\n표준 코드 등록\n검토를 마친 신청 표준 코드를 표준 코드 사전에 등록 후 개발자에게 통보\n표준담당\nEXCEL\n\n\n8\n표준 코드 반영\n등록된 코드 반영\n개발자"
  },
  {
    "objectID": "docs/blog/posts/Governance/6-2.data_code_registration copy.html#data-standard-governance-data-code-review-process",
    "href": "docs/blog/posts/Governance/6-2.data_code_registration copy.html#data-standard-governance-data-code-review-process",
    "title": "Data Governance Study - Data Code Registration Process",
    "section": "",
    "text": "애플리케이션 파트(개발)가 코드 정보 도출하여 신규 및 변경 신청하고 표준 담당자가 승인한다.\n\n\n\n\n애플리케이션 파트(개발)가 코드 정보 도출하여 신규 및 변경 신청하고 표준 담당자가 승인한다.\n\n\n\n\n데이터 모델 표준 코드 등록 절차\n\n\n\n\n\n\n\n\n\n\n\n\n순번\nTASK\n설명\n담당\n비고\n\n\n\n\n1\n표준 코드 요건 발생\n애플리케이션 개발 중 신규 코드 요건 발생\n개발자\n\n\n\n2\n표준 코드 도출\n신규 등록이 필요한 코드 명, 값 등을 도출\n개발자\nEXCEL\n\n\n3\n코드 검색\n표준 코드 사전에서 해당 코드 검색\n개발자\n코드 사전\n\n\n4\n표준 코드 신청\n표준 코드 사전에 도출된 코드가 없을 경우 표준 담당자에게 신청\n개발자\n\n\n\n5\n코드 신청 접수\n표준 담당자는 개발자가 신청한 코드 표준 등록 요청 접수\n표준담당\n\n\n\n6\n타당성 검토\n기존 코드 사전을 기준으로 신규 코드 타당성 검토\n표준담당\n\n\n\n7\n표준 코드 등록\n검토를 마친 신청 표준 코드를 표준 코드 사전에 등록 후 개발자에게 통보\n표준담당\nEXCEL\n\n\n8\n표준 코드 반영\n등록된 코드 반영\n개발자"
  },
  {
    "objectID": "docs/blog/posts/Governance/6-0..data_registration_process.html",
    "href": "docs/blog/posts/Governance/6-0..data_registration_process.html",
    "title": "Data Governance Study - Data Registration Process",
    "section": "",
    "text": "DB 스키마 (물리적 데이터 모델)에 포함된 단어들만 신청 가능하도록 한다.\n데이터 모델 담당자가 단어(분류어일 경우 도메인 포함) 및 용어와 도메인을 신청하고 표준 담당자가 승인한다.\n표준담당자가 회사 표준담당자가 아닐 경우 표준단어/표준용어에 등록 시 회사 담당자 확인이 필요함\n메타시스템 도입 전까지 회사 렌터카 표준 담당자가 엑셀로 관리함\n\n\n\n\n단어 신청 절차 순서도\n\n\n\n신청 단계 (데이터 모델 담당자):\n\n새로운 단어, 용어, 도메인의 필요성 확인\n기존 표준 목록 검토 (중복 여부 확인)\n신청 양식 작성\n\n단어의 경우: 단어명, 정의, 영문명, 약어 등\n분류어일 경우: 위 내용 + 도메인 정보 (데이터 타입, 길이 등)\n용어의 경우: 용어명, 정의, 구성 단어, 도메인 등\n\n신청 시스템이나 지정된 채널을 통해 제출\n\n검토 단계 (표준 담당자):\n\n신청 내용의 완전성 확인\n기존 표준과의 일관성 검토\n명명 규칙 준수 여부 확인\n정의의 명확성과 적절성 검토\n도메인 정보의 적절성 확인 (해당되는 경우)\n\n피드백 및 수정 단계:\n\n필요시 데이터 모델 담당자에게 추가 정보 요청 또는 수정 제안\n데이터 모델 담당자는 요청받은 사항에 대해 보완하여 재제출\n\n승인 단계 (표준 담당자):\n\n최종 검토 후 승인 결정\n승인된 내용을 표준 사전에 등록\n필요시 관련 시스템 업데이트 (예: 데이터 모델링 도구, 메타데이터 저장소 등)\n\n결과 통보:\n\n데이터 모델 담당자에게 승인 결과 통보\n승인된 경우 적용 방법 및 시기 안내\n반려된 경우 사유 설명 및 대안 제시\n\n적용 및 모니터링:\n\n데이터 모델 담당자는 승인된 표준을 모델에 적용\n표준 담당자는 적용 현황을 모니터링하고 필요시 지원 제공\n\n피드백 및 개선:\n\n사용 과정에서 발생하는 이슈나 개선사항 수집\n필요시 표준 개정 절차 진행\n\n\n\n\n개발 과정의 유연성과 표준화 사이의 균형을 잡는 것은 중요하다.\n\n개발 단계 구분:\n\nPoC / 프로토타입 단계\n\n개발자들에게 최대한의 자유도 부여\n임시 명명 규칙 사용 (예: tmp_, poc_, dev_ 접두어)\n표준화 절차 적용하지 않음\n\n알파 / 베타 단계\n\n느슨한 명명 규칙 적용\n주요 변수, 속성에 대해서만 표준화 검토\n간소화된 승인 프로세스 사용\n\n프로덕션 준비 단계\n\n엄격한 표준화 적용\n모든 주요 변수, 속성, 코드에 대한 표준화 검토\n정식 승인 프로세스 적용\n\n\n명명 규칙 계층화\n\n개인 / 팀 레벨\n\n개발자 또는 팀 내에서 사용하는 임시 명명 규칙\n문서화는 하되, 공식 승인 불필요\n\n프로젝트 레벨\n\n프로젝트 내에서 합의된 명명 규칙\n프로젝트 매니저 또는 기술 리더의 승인\n\n부서 / 도메인 레벨\n\n특정 비즈니스 도메인이나 부서에서 사용하는 표준\n도메인 전문가의 검토 필요\n\n전사 레벨\n\n조직 전체에서 사용되는 공식 표준\n데이터 거버넌스 위원회의 승인 필요\n\n\n자동화 도구 활용\n\n코드 분석 도구를 사용하여 명명 규칙 준수 여부 자동 검사\nCI/CD 파이프라인에 표준화 검사 단계 추가\n데이터 모델링 도구와 연동하여 표준 용어 자동 적용\n\n점진적 표준화\n\n개발 초기에는 핵심 개념에 대해서만 표준화 적용\n프로젝트 진행에 따라 점진적으로 표준화 범위 확대\n리팩토링 과정에서 비표준 명칭을 표준화된 명칭으로 대체\n\n예외 관리 프로세스\n\n표준을 적용하기 어려운 특수 상황에 대한 예외 처리 절차 마련\n예외 사유 문서화 및 승인 프로세스 간소화\n\n교육 및 가이드라인\n\n개발자들에게 표준화의 중요성과 이점에 대한 교육 제공\n쉽게 참조할 수 있는 명명 규칙 가이드라인 제공\n자주 사용되는 표준 용어 목록 공유\n\n정기적인 리뷰 및 정리\n\n주기적으로 사용 중인 변수명, 코드명 등을 검토\n프로젝트 마일스톤마다 표준화 작업 수행\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\nTASK\n설명\n담당\n비고\n\n\n\n\n1\n데이터 모델 신규 용어 발생\n데이터 모델 담당자가 속성 추가 등으로 신규 용어를 추가요건 발생\n모델담당\n\n\n\n2\n표준 용어 검색\n데이터 모델 담당자가 추가한 속성에 해당하는 용어를 데이터 사전에서 검색\n모델담당\n\n\n\n3\n표준 도메인 검색\n신청하는 용어의 도메인이 존재하는지 데이터 사전에서 검색\n모델담당\n\n\n\n4\n표준 단어 검색\n데이터 모델 담당자는 표준데이터 사전에서 신규 속성 구성 단어를 검색 함\n모델담당\n\n\n\n5\n표준 단어 신청\n표준데이터 사전에 구성 단어가 없을 경우 DA는 표준담당자에게 단어신청 함\n모델담당\n\n\n\n6\n단어 신청 접수\n표준담당자는 신규 단어 신청을 접수 함\n표준담당\nEXCEL\n\n\n7\n타당성 검토\n기존 표준단어 및 지침을 참고로 신규 신청 단어의 표준 등록 가능 여부를 판단하고 결과를 데이터 모델 담당자에 통보 함\n표준담당\n\n\n\n8\n표준 단어 등록\n신규 신청 단어가 표준 단어로 적정할 경우 표준담당자는 표준 사전에 등록 함\n표준담당\n\n\n\n9\n표준 용어/도메인 신청\n신규 속성 용어/도메인이 없을 경우 데이터 모델 담당자는 표준담당자에게 용어신청 함\n모델담당\n\n\n\n10\n타당성 검토\n신청 된 용어/도메인이 표준 지침에 맞게 구성 되었는지 검토 후 표준 등록 여부 DA 통보\n표준담당\n\n\n\n11\n표준 용어/도메인 등록\n신규 신청 용어/도메인를 표준 사전에 등록\n표준담당\nEXCEL\n\n\n12\n데이터 모델 반영\n변경 된 표준 용어 및 도메인을 데이터 모델에 반영\n모델담당"
  },
  {
    "objectID": "docs/blog/posts/Governance/6-0..data_registration_process.html#표준-단어-용어-및-도메인-신청-절차",
    "href": "docs/blog/posts/Governance/6-0..data_registration_process.html#표준-단어-용어-및-도메인-신청-절차",
    "title": "Data Governance Study - Data Registration Process",
    "section": "",
    "text": "DB 스키마 (물리적 데이터 모델)에 포함된 단어들만 신청 가능하도록 한다.\n데이터 모델 담당자가 단어(분류어일 경우 도메인 포함) 및 용어와 도메인을 신청하고 표준 담당자가 승인한다.\n표준담당자가 회사 표준담당자가 아닐 경우 표준단어/표준용어에 등록 시 회사 담당자 확인이 필요함\n메타시스템 도입 전까지 회사 렌터카 표준 담당자가 엑셀로 관리함\n\n\n\n\n단어 신청 절차 순서도\n\n\n\n신청 단계 (데이터 모델 담당자):\n\n새로운 단어, 용어, 도메인의 필요성 확인\n기존 표준 목록 검토 (중복 여부 확인)\n신청 양식 작성\n\n단어의 경우: 단어명, 정의, 영문명, 약어 등\n분류어일 경우: 위 내용 + 도메인 정보 (데이터 타입, 길이 등)\n용어의 경우: 용어명, 정의, 구성 단어, 도메인 등\n\n신청 시스템이나 지정된 채널을 통해 제출\n\n검토 단계 (표준 담당자):\n\n신청 내용의 완전성 확인\n기존 표준과의 일관성 검토\n명명 규칙 준수 여부 확인\n정의의 명확성과 적절성 검토\n도메인 정보의 적절성 확인 (해당되는 경우)\n\n피드백 및 수정 단계:\n\n필요시 데이터 모델 담당자에게 추가 정보 요청 또는 수정 제안\n데이터 모델 담당자는 요청받은 사항에 대해 보완하여 재제출\n\n승인 단계 (표준 담당자):\n\n최종 검토 후 승인 결정\n승인된 내용을 표준 사전에 등록\n필요시 관련 시스템 업데이트 (예: 데이터 모델링 도구, 메타데이터 저장소 등)\n\n결과 통보:\n\n데이터 모델 담당자에게 승인 결과 통보\n승인된 경우 적용 방법 및 시기 안내\n반려된 경우 사유 설명 및 대안 제시\n\n적용 및 모니터링:\n\n데이터 모델 담당자는 승인된 표준을 모델에 적용\n표준 담당자는 적용 현황을 모니터링하고 필요시 지원 제공\n\n피드백 및 개선:\n\n사용 과정에서 발생하는 이슈나 개선사항 수집\n필요시 표준 개정 절차 진행\n\n\n\n\n개발 과정의 유연성과 표준화 사이의 균형을 잡는 것은 중요하다.\n\n개발 단계 구분:\n\nPoC / 프로토타입 단계\n\n개발자들에게 최대한의 자유도 부여\n임시 명명 규칙 사용 (예: tmp_, poc_, dev_ 접두어)\n표준화 절차 적용하지 않음\n\n알파 / 베타 단계\n\n느슨한 명명 규칙 적용\n주요 변수, 속성에 대해서만 표준화 검토\n간소화된 승인 프로세스 사용\n\n프로덕션 준비 단계\n\n엄격한 표준화 적용\n모든 주요 변수, 속성, 코드에 대한 표준화 검토\n정식 승인 프로세스 적용\n\n\n명명 규칙 계층화\n\n개인 / 팀 레벨\n\n개발자 또는 팀 내에서 사용하는 임시 명명 규칙\n문서화는 하되, 공식 승인 불필요\n\n프로젝트 레벨\n\n프로젝트 내에서 합의된 명명 규칙\n프로젝트 매니저 또는 기술 리더의 승인\n\n부서 / 도메인 레벨\n\n특정 비즈니스 도메인이나 부서에서 사용하는 표준\n도메인 전문가의 검토 필요\n\n전사 레벨\n\n조직 전체에서 사용되는 공식 표준\n데이터 거버넌스 위원회의 승인 필요\n\n\n자동화 도구 활용\n\n코드 분석 도구를 사용하여 명명 규칙 준수 여부 자동 검사\nCI/CD 파이프라인에 표준화 검사 단계 추가\n데이터 모델링 도구와 연동하여 표준 용어 자동 적용\n\n점진적 표준화\n\n개발 초기에는 핵심 개념에 대해서만 표준화 적용\n프로젝트 진행에 따라 점진적으로 표준화 범위 확대\n리팩토링 과정에서 비표준 명칭을 표준화된 명칭으로 대체\n\n예외 관리 프로세스\n\n표준을 적용하기 어려운 특수 상황에 대한 예외 처리 절차 마련\n예외 사유 문서화 및 승인 프로세스 간소화\n\n교육 및 가이드라인\n\n개발자들에게 표준화의 중요성과 이점에 대한 교육 제공\n쉽게 참조할 수 있는 명명 규칙 가이드라인 제공\n자주 사용되는 표준 용어 목록 공유\n\n정기적인 리뷰 및 정리\n\n주기적으로 사용 중인 변수명, 코드명 등을 검토\n프로젝트 마일스톤마다 표준화 작업 수행\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\nTASK\n설명\n담당\n비고\n\n\n\n\n1\n데이터 모델 신규 용어 발생\n데이터 모델 담당자가 속성 추가 등으로 신규 용어를 추가요건 발생\n모델담당\n\n\n\n2\n표준 용어 검색\n데이터 모델 담당자가 추가한 속성에 해당하는 용어를 데이터 사전에서 검색\n모델담당\n\n\n\n3\n표준 도메인 검색\n신청하는 용어의 도메인이 존재하는지 데이터 사전에서 검색\n모델담당\n\n\n\n4\n표준 단어 검색\n데이터 모델 담당자는 표준데이터 사전에서 신규 속성 구성 단어를 검색 함\n모델담당\n\n\n\n5\n표준 단어 신청\n표준데이터 사전에 구성 단어가 없을 경우 DA는 표준담당자에게 단어신청 함\n모델담당\n\n\n\n6\n단어 신청 접수\n표준담당자는 신규 단어 신청을 접수 함\n표준담당\nEXCEL\n\n\n7\n타당성 검토\n기존 표준단어 및 지침을 참고로 신규 신청 단어의 표준 등록 가능 여부를 판단하고 결과를 데이터 모델 담당자에 통보 함\n표준담당\n\n\n\n8\n표준 단어 등록\n신규 신청 단어가 표준 단어로 적정할 경우 표준담당자는 표준 사전에 등록 함\n표준담당\n\n\n\n9\n표준 용어/도메인 신청\n신규 속성 용어/도메인이 없을 경우 데이터 모델 담당자는 표준담당자에게 용어신청 함\n모델담당\n\n\n\n10\n타당성 검토\n신청 된 용어/도메인이 표준 지침에 맞게 구성 되었는지 검토 후 표준 등록 여부 DA 통보\n표준담당\n\n\n\n11\n표준 용어/도메인 등록\n신규 신청 용어/도메인를 표준 사전에 등록\n표준담당\nEXCEL\n\n\n12\n데이터 모델 반영\n변경 된 표준 용어 및 도메인을 데이터 모델에 반영\n모델담당"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-3.data_word_glossary.html",
    "href": "docs/blog/posts/Governance/5-3.data_word_glossary.html",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "조직에서 사용되는 모든 데이터 관련 용어의 공식적인 정의를 제공하는 중앙 집중식 저장소이다.\n업무상 사용되는 용어를 정보시스템에서 사용하는 기술적인 용어로 전환하여 이것을 일관되게 사용 할 수 있도록 정의한 것을 지칭한다.\n각 용어의 의미, 사용 맥락, 형식 등을 명확히 기술한다.\n표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다.\n모델링에서는 속성명으로 사용되며 전사관점에서 유일하다.\n예시\n\n표준 단어 (주제어): 장기\n표준 단어 (수식어): 대여\n표준 단어 (분류단어): 금액\n표준 용어 : 장기대여금액\n데이터 모델 (속성명): 장기대여금액 with datatype = NUMBER(10)\n도메인: 금액N10\n\n\n\n\n\n데이터의 일관성을 확보하고 품질을 향상\n조직 전체에서 데이터 용어를 일관되게 사용할 수 있도록 한다.\n\n즉, 전사적으로 표준화된 용어를 사용함으로써 데이터 모델 구성 요소의 명칭을 부여하는데 일관성을 유지할 수 있다\n\n시스템 간 데이터 통합과 매핑을 용이하게 한다.\n표준화된 명칭을 부여함으로써 데이터의 중복 정의 방지와 의미 전달의 명확성을 확보하여 의사 소통을 원활하게 한다\n\n\n\n\n\n용어 논리명 (보통 한글)\n\n논리명은 데이터의 의미를 나타내는 명칭으로 표준단어 명으로 구성된다.\n용어 논리명의 최대길이는 30자로 한다. 단 20자 이내로 작성할 것을 권장한다\n표준속성 구성 시 5개 단어를 넘지 않도록 하며, 구분자나 띄어쓰기 없이 한 단어로 붙인다 (월_계좌잔액(X) → 월계좌잔액)\n\n용어 물리명 (보통 영어)\n\n물리명은 단어의 영문 약어 조합으로 이루어지며 단어의 영문약어들끼리 연결 할 때는 언더바(_)를 사용한다.\n용어의 물리명은 최종적으로 데이터베이스를 구성 할 때 테이블의 컬럼명으로 사용한다.\n용어 물리명의 최대길이는 28자로 한다. 단 20자 이내로 작성할 것을 권장한다. (단, 용어 논리 및 물리명의 길이는 DBMS에 따라 달라질 수 있음)\n\n정의\n\n용어가 업무적으로 사용되는 의미를 기술한 내용이다.\n\n도메인 정보\n\n특정 비즈니스 컨텍스트에서 데이터 값의 허용 범위를 정의\n즉, 데이터 값의 범위를 한정하는 데이터 타입과 길이,소수점을 의미한다.\n데이터 타입을 포함하며, 추가적인 제약조건이나 비즈니스 규칙을 포함할 수 있습니다.\n비즈니스 로직과 데이터 무결성 규칙을 포함할 수 있다.\n예시\n\n나이: INTEGER, 0-150 사이의 값만 허용\n이메일: VARCHAR(100), 이메일 형식 검증 규칙 포함\n급여: DECIMAL(10,2), 0보다 큰 값만 허용\n\n\n데이터 타입 및 형식\n\n데이터의 기본적인 저장 형태와 구조를 나타낸다.\n일반적이고 기본적인 데이터 유형을 지정\n보통 도메인 정보가 데이터 타입 및 형식 정보를 모두 포함한다.\n예시\n\n문자열(VARCHAR, CHAR)\n숫자(INTEGER, DECIMAL)\n날짜/시간(DATE, TIMESTAMP)\n불리언(BOOLEAN)\n\n\n코드\n\n입력할 수 있는 유효 값 데이터 값을 정의할 수 있다면 용어는 코드와 매핑 한다.\n\n관련 업무 영역\n\n해당 용어가 주로 사용되는 비즈니스 또는 조직 내의 특정 부서나 기능 영역을 나타낸다.\n목적: 용어의 사용 맥락을 제공하고, 해당 용어가 어떤 비즈니스 프로세스나 기능과 관련있는지 이해하는 데 도움을 준다.\n예시\n\n“고객ID” - 관련 업무 영역: 고객 관리, 마케팅, 영업\n“재고수량” - 관련 업무 영역: 재고 관리, 물류, 구매\n“급여액” - 관련 업무 영역: 인사, 재무\n\n\n사용 예시 등\n\n“생년월일” - 사용 예시: “1990-05-15”\n“주문상태” - 사용 예시: “접수”, “처리중”, “배송완료”\n“계좌잔액” - 사용 예시: “1,000,000원”\n\n용어 사전 예시\n\n\n\n\n\n\n\n\n\n\n\n\n용어 논리명\n용어 물리명\n도메인(인포타입)\n코드\n정의\n관련 업무 영역\n사용 예시\n\n\n\n\n실험종료일자\nEXPR_END_DATE\n일자VC8\n\nPCR 실험의 종료일자이다. YYYYMMDD로 작성한다.\n실험관리, 품질관리\n“20240315”\n\n\n기기시리얼번호\nINSTRMNT_SERIAL_NO\n문자열VC30\n\nReal Time PCR 기기의 고유 시리얼 번호이다.\n장비관리, 실험관리\n“787BR13738_BR205241”\n\n\n시약제품명\nREAG_PROD_NM\n문자열VC30\n\n실험에 사용되는 시약의 제품 이름이다.\n실험관리, 제품관리, 품질관리\n“SARS-CoV-2 D Plus”\n\n\n\n\n\n\n\n\n표준용어 작성 시 누구나 이해하기 쉽도록 간결하되 명확하고 모호함 없이 표현하도록 해야 하며, 다음과 같은 기본 원칙에 위배되지 않도록 한다.\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어는 관용적으로 사용하는 용어를 우선적으로 사용한다\n\n\n\n2\n• 표준용어를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n등록일자(X) → 장비자산등록일자(O)\n\n\n3\n• 용어 구성 시 단어는 반드시 표준 단어 사전에 등록된 단어를 사용하며, 단어 사전에 등록되어 있지 않은 경우에는 표준 담당자와 협의 후에 신규 단어로 등록하도록 한다.\n단어 부재 시 신규 요청 요망\n\n\n4\n• 일반적인 의미와 전혀 다르게 사용된 용어는 적절한 다른 용어로 대체하고, 유사한 의미의 용어가 중복 개발되어 혼재되지 않도록 하며 새로운 용어의 개발은 자제한다.\n반환일자(X) 반납일자(O)\n\n\n5\n• 표준용어로 등록된 명칭의 전사적으로 사용되어야 함으로 명 선정 시 신중하게 고려하여야 한다\n\n\n\n6\n• 표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다\n렌탈 + 가능(X) 렌탈 + 가능 + 금액(O)\n\n\n7\n• 표준용어 명명 규칙 - 표준용어는 누구나 이해하기 쉽도록 구체적이고 명확하고 간결하게 정의한다 - 복합어를 단일어 보다 우선 적용한다 - 복합어가 중첩되어 사용될 경우 도메인이 포함된 복합어를 우선 적용한다 - 의미 있는 숫자를 포함한 용어의 경우에는 숫자를 포함하여 하나의 표준단어를 등록한 후 그 표준 단어를 사용하여 용어를 정의한다 - 용어의 의미를 모호하게 하는 의미 없는 일련번호를 부여하기 위한 숫자는 사용하지 않으며 용어에 수식어를 사용하여 용어가 유일하게 식별되도록 정의하는 것을 원칙으로 한다\n\n\n\n\n\n\n\n\n표준 용어 = 수식어 (표준 단어) + 주제어 (표준 단어) + \\(\\dots\\) + 수식어 (표준 단어) + 주제어 (표준 단어) + 분류어\n수식어 예시\n\n\n\n수식어\n예시\n\n\n\n\n기간 수식어\n최초, 최종, 과거, 최근 등\n\n\n기간/시간\n6개월, 당월, 월말, 년초, 년말 등\n\n\n장소\n국외, 국내, 지점, 본점 등\n\n\n특징\n순수, 사용, 처리, 거래 등\n\n\n계산\n합계: 한데 모아서 합산함누계: 계속하여 덧붙여 합산함\n\n\n\n표준 용어 = 표준 용어 + 표준 도메인\n원칙 항목 및 설명\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 용어는 수식과 수식의 대상이 되는 단어가 여러 개 존재할 수 있고 도메인을 수식하는 분류어는 용어의 끝에 위치한다\n\n\n\n2\n• 수식어는 기간, 장소, 특징, 계산의 성격을 가지는 단어가 순서대로 위치하고 기간을 수식하는 단어는 맨 앞에 위치한다\n\n\n\n3\n• 수식어 중 계산의 성격을 가진 단어는 합계, 누계, 총합계, 총누계, 소계 중 하나를 선택하여 사용해야 한다. (도메인 그룹이 금액과 수량과 같이 계산이 필요한 도메인을 수식함)\n\n\n\n4\n• 대상이 되는 단어가 여러 개일 때는 중 범위가 큰 것 순서대로 용어의 앞부분에 위치한다\n\n\n\n\n주의 사항\n\n\n\n\n\n\n\n\nAS-IS 용어 (비권장)\nTO-BE 용어(권장)\n비고\n\n\n\n\n회사에게 하고 싶은 말 내용\n사용자건의사항\n서술형용어\n\n\n공임금액구분별금액\n공임구분별금액\n단어 반복\n\n\n법인번호\n?+법인등록번호\n주제어 누락 및 약어 사용\n\n\n상세순번\n?+상세일련번호\n주제어 누락\n\n\n수정자\n?+수정자+?\n주제어 및 분류어 누락\n\n\n스케줄추가(청구)정보 수정여부\n스케줄추가정보수정여부\n특수문자사용\n\n\n요청일\n장기렌트입고요청일자\n주제어누락\n\n\n운전자\n렌트카운전자성명\n주제어 및 분류어 누락\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어에 적절한 명칭을 부여하고 전사적 공유가 가능하기 위해서는 구체적인 정의가 반드시 있어야 한다- 표준용어정의는 활동을 나타내는 단어와 그 주체가 되는 단어와 무엇을 어떻게 했는지의 내용이 담겨 있어야 한다.\n대여차량입고일자: 렌트차량이 반환되어 차고지에 입고 된 일자이다.\n\n\n2\n• 표준용어의 명칭이 주는 의미가 불분명하면 좀더 상세하게 정의해야 한다.\n유효기간(X) → 회원멤버쉽유효기간(O)\n\n\n3\n• 동일한 의미의 용어가 중복되지 않도록 표준용어 구성 순서를 고려해 생성한다\n현금서비스최종3개월 총합계금액(X) → 최종3개월 현금서비스 총합계금액(O)\n\n\n4\n• 표준용어의 한글명 또는 영문명 길이 제한으로 인해 축약된 형태로 사용해야 하는 경우 용어를 구성하는 단어 중 연관도와 활용도가 높은 단어들을 합하여 복합어를 정의해야 한다\n고객차량등급^코드 → 고객차량등급코드\n\n\n5\n• 표준용어 중 ‘여부’ 도메인으로 끝나는 것들은 대표로 하나의 코드명과 코드 값(Y, N)을 등록하여 모든 공통적으로 사용하도록 한다.\n코드일련번호 : B011\n\n\n6\n• ‘계약번호’ 자체로는 그 의미가 불분명하여 대차계약번호, 유지보수계약번호 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n7\n• 일반적으로 ‘렌탈’은 ’자동차대여’를 말하는 것이나 ’렌탈’ 자체로는 그 의미가 불분명하여 단기렌탈, 장기렌탈 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n8\n• 신청사용자아이디 → 사용자아이디 에서 ’신청’의 단어처럼 생략해도 의미가 통하는 경우에는 표준용어는 축약된 형태로 정의한다\n\n\n\n\n\n\n\n\n\n\n\n각 업무 영역에서 필요한 용어들을 수집\n기존 시스템, 문서, 보고서 등에서 사용 중인 비표준 용어들을 식별\n\n\n\n\n\n수집된 용어들의 의미와 사용 맥락을 분석\n유사하거나 중복된 용어들을 식별\n\n\n\n\n\n분석된 용어들을 기존의 표준 단어 사전과 매핑\n각 용어를 구성하는 단어들이 표준 단어 사전에 있는지 확인\n\n\n\n\n\n표준 단어들을 조합하여 새로운 표준 용어를 구성\n용어 구성 원칙(예: 수식어 순서, 도메인 위치 등)을 따른다.\n\n\n\n\n\n구성된 용어가 의미를 명확히 전달하는지 확인\n기존 표준 용어와 중복되지 않는지 검토\n전문가나 제 3의 부서와 cross check\n\n\n\n\n\n용어의 정의, 사용 예시, 관련 업무 영역 등 상세 정보를 작성\n용어와 관련된 도메인 정보를 지정\n\n\n\n\n\n구성된 용어에 대해 관련 부서와 데이터 관리자의 검토를 받는다.\n필요시 수정하고 최종 승인을 받는다.\n\n\n\n\n\n승인된 용어를 표준 용어 사전에 등록\n용어의 물리명(영문명)을 생성\n\n\n\n\n\n새로 등록된 표준 용어를 조직 내에 공지\n필요시 사용 방법에 대한 교육을 실시\n\n\n\n\n\n새로 등록된 표준 용어의 사용 현황을 모니터링\n사용자 피드백을 수집\n\n\n\n\n\n정기적 검토 및 피드백 수집\n\n분기별로 용어 사전 검토 일정을 수립\n사용자로부터 피드백을 수집하고, 사용 현황을 모니터링\n\n변경 관리 프로세스 운영\n\n용어 추가, 수정, 폐기를 위한 공식적인 변경 요청 프로세스를 구축\n변경 요청에 대한 영향 분석을 수행하고, 승인 절차를 거친다.\n\n업데이트 및 버전 관리\n\n승인된 변경사항을 용어 사전에 반영\n버전 관리를 통해 변경 이력을 추적하고, 주요 변경사항을 공지\n\n교육 및 홍보\n\n변경된 용어나 새로운 용어에 대한 교육을 실시\n내부 커뮤니케이션 채널을 통해 주요 업데이트 사항을 공유\n\n성과 측정 및 개선\n\n용어 표준화로 인한 데이터 품질 개선, 업무 효율성 증가 등의 성과를 측정\n측정 결과를 바탕으로 용어 관리 전략을 지속적으로 개선\n\n\n\n\n\n\n\n예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n용어ID\n용어논리명\n용어물리명\n도메인\n정의\n관련업무영역\n사용예시\n표준단어구성\n승인상태\n등록일\n\n\n\n\nT001\n고객번호\nCUST_NO\n일련번호VC10\n고객을 유일하게 식별하는 번호\n고객관리, 영업\n“C0001234567”\n고객(주제어) + 번호(분류어)\n승인\n2023-01-15\n\n\nT002\n장기렌트계약시작일자\nLTRM_RENT_CNTR_STRT_DT\n일자VC8\n장기 렌트 계약이 시작되는 날짜\n계약관리, 렌트관리\n“20230107”\n장기(수식어) + 렌트(주제어) + 계약(주제어) + 시작(수식어) + 일자(분류어)\n승인\n2023-01-16\n\n\nT003\n월별렌탈등록비합계금액\nMTHLY_RENT_RGST_FEE_TOT_AMT\n금액N15\n한 달 동안의 렌탈 등록비 총액\n재무, 렌트관리\n“5000000”\n월별(수식어) + 렌탈(주제어) + 등록(주제어) + 비(주제어) + 합계(수식어) + 금액(분류어)\n검토중\n2023-01-17\n\n\n\n* VC10: Variable Character의 약자로 가변 길이 문자열을 나타냄 (최대 10 자리)  \n* N15: N은 숫자(Numeric) 데이터 타입, 최대 15자리의 숫자  \n* D: Date (날짜)  \n* T: Time (시간)  \n* B: Boolean (참/거짓)  \n\n칼럼 설명\n\n용어ID: 각 용어의 고유 식별자\n용어논리명: 업무에서 사용되는 한글 용어명 (최대 30자, 권장 20자 이내)\n용어물리명: 데이터베이스 등에서 사용되는 영문 약어명 (최대 28자, 권장 20자 이내)\n도메인: 해당 용어의 데이터 타입과 제약조건\n정의: 용어에 대한 명확한 설명\n관련업무영역: 해당 용어가 주로 사용되는 업무 분야\n사용예시: 실제 데이터 예시\n표준단어구성: 용어를 구성하는 표준 단어들과 각 단어의 역할 (수식어, 주제어, 분류어)\n승인상태: 용어의 현재 승인 상태 (예: 승인, 검토중, 폐기 등)\n등록일: 용어가 사전에 처음 등록된 날짜\n최종수정일: 용어 정보가 마지막으로 수정된 날짜"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-3.data_word_glossary.html#데이터-표준-용어-사전이란",
    "href": "docs/blog/posts/Governance/5-3.data_word_glossary.html#데이터-표준-용어-사전이란",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "조직에서 사용되는 모든 데이터 관련 용어의 공식적인 정의를 제공하는 중앙 집중식 저장소이다.\n업무상 사용되는 용어를 정보시스템에서 사용하는 기술적인 용어로 전환하여 이것을 일관되게 사용 할 수 있도록 정의한 것을 지칭한다.\n각 용어의 의미, 사용 맥락, 형식 등을 명확히 기술한다.\n표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다.\n모델링에서는 속성명으로 사용되며 전사관점에서 유일하다.\n예시\n\n표준 단어 (주제어): 장기\n표준 단어 (수식어): 대여\n표준 단어 (분류단어): 금액\n표준 용어 : 장기대여금액\n데이터 모델 (속성명): 장기대여금액 with datatype = NUMBER(10)\n도메인: 금액N10\n\n\n\n\n\n데이터의 일관성을 확보하고 품질을 향상\n조직 전체에서 데이터 용어를 일관되게 사용할 수 있도록 한다.\n\n즉, 전사적으로 표준화된 용어를 사용함으로써 데이터 모델 구성 요소의 명칭을 부여하는데 일관성을 유지할 수 있다\n\n시스템 간 데이터 통합과 매핑을 용이하게 한다.\n표준화된 명칭을 부여함으로써 데이터의 중복 정의 방지와 의미 전달의 명확성을 확보하여 의사 소통을 원활하게 한다\n\n\n\n\n\n용어 논리명 (보통 한글)\n\n논리명은 데이터의 의미를 나타내는 명칭으로 표준단어 명으로 구성된다.\n용어 논리명의 최대길이는 30자로 한다. 단 20자 이내로 작성할 것을 권장한다\n표준속성 구성 시 5개 단어를 넘지 않도록 하며, 구분자나 띄어쓰기 없이 한 단어로 붙인다 (월_계좌잔액(X) → 월계좌잔액)\n\n용어 물리명 (보통 영어)\n\n물리명은 단어의 영문 약어 조합으로 이루어지며 단어의 영문약어들끼리 연결 할 때는 언더바(_)를 사용한다.\n용어의 물리명은 최종적으로 데이터베이스를 구성 할 때 테이블의 컬럼명으로 사용한다.\n용어 물리명의 최대길이는 28자로 한다. 단 20자 이내로 작성할 것을 권장한다. (단, 용어 논리 및 물리명의 길이는 DBMS에 따라 달라질 수 있음)\n\n정의\n\n용어가 업무적으로 사용되는 의미를 기술한 내용이다.\n\n도메인 정보\n\n특정 비즈니스 컨텍스트에서 데이터 값의 허용 범위를 정의\n즉, 데이터 값의 범위를 한정하는 데이터 타입과 길이,소수점을 의미한다.\n데이터 타입을 포함하며, 추가적인 제약조건이나 비즈니스 규칙을 포함할 수 있습니다.\n비즈니스 로직과 데이터 무결성 규칙을 포함할 수 있다.\n예시\n\n나이: INTEGER, 0-150 사이의 값만 허용\n이메일: VARCHAR(100), 이메일 형식 검증 규칙 포함\n급여: DECIMAL(10,2), 0보다 큰 값만 허용\n\n\n데이터 타입 및 형식\n\n데이터의 기본적인 저장 형태와 구조를 나타낸다.\n일반적이고 기본적인 데이터 유형을 지정\n보통 도메인 정보가 데이터 타입 및 형식 정보를 모두 포함한다.\n예시\n\n문자열(VARCHAR, CHAR)\n숫자(INTEGER, DECIMAL)\n날짜/시간(DATE, TIMESTAMP)\n불리언(BOOLEAN)\n\n\n코드\n\n입력할 수 있는 유효 값 데이터 값을 정의할 수 있다면 용어는 코드와 매핑 한다.\n\n관련 업무 영역\n\n해당 용어가 주로 사용되는 비즈니스 또는 조직 내의 특정 부서나 기능 영역을 나타낸다.\n목적: 용어의 사용 맥락을 제공하고, 해당 용어가 어떤 비즈니스 프로세스나 기능과 관련있는지 이해하는 데 도움을 준다.\n예시\n\n“고객ID” - 관련 업무 영역: 고객 관리, 마케팅, 영업\n“재고수량” - 관련 업무 영역: 재고 관리, 물류, 구매\n“급여액” - 관련 업무 영역: 인사, 재무\n\n\n사용 예시 등\n\n“생년월일” - 사용 예시: “1990-05-15”\n“주문상태” - 사용 예시: “접수”, “처리중”, “배송완료”\n“계좌잔액” - 사용 예시: “1,000,000원”\n\n용어 사전 예시\n\n\n\n\n\n\n\n\n\n\n\n\n용어 논리명\n용어 물리명\n도메인(인포타입)\n코드\n정의\n관련 업무 영역\n사용 예시\n\n\n\n\n실험종료일자\nEXPR_END_DATE\n일자VC8\n\nPCR 실험의 종료일자이다. YYYYMMDD로 작성한다.\n실험관리, 품질관리\n“20240315”\n\n\n기기시리얼번호\nINSTRMNT_SERIAL_NO\n문자열VC30\n\nReal Time PCR 기기의 고유 시리얼 번호이다.\n장비관리, 실험관리\n“787BR13738_BR205241”\n\n\n시약제품명\nREAG_PROD_NM\n문자열VC30\n\n실험에 사용되는 시약의 제품 이름이다.\n실험관리, 제품관리, 품질관리\n“SARS-CoV-2 D Plus”\n\n\n\n\n\n\n\n\n표준용어 작성 시 누구나 이해하기 쉽도록 간결하되 명확하고 모호함 없이 표현하도록 해야 하며, 다음과 같은 기본 원칙에 위배되지 않도록 한다.\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어는 관용적으로 사용하는 용어를 우선적으로 사용한다\n\n\n\n2\n• 표준용어를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n등록일자(X) → 장비자산등록일자(O)\n\n\n3\n• 용어 구성 시 단어는 반드시 표준 단어 사전에 등록된 단어를 사용하며, 단어 사전에 등록되어 있지 않은 경우에는 표준 담당자와 협의 후에 신규 단어로 등록하도록 한다.\n단어 부재 시 신규 요청 요망\n\n\n4\n• 일반적인 의미와 전혀 다르게 사용된 용어는 적절한 다른 용어로 대체하고, 유사한 의미의 용어가 중복 개발되어 혼재되지 않도록 하며 새로운 용어의 개발은 자제한다.\n반환일자(X) 반납일자(O)\n\n\n5\n• 표준용어로 등록된 명칭의 전사적으로 사용되어야 함으로 명 선정 시 신중하게 고려하여야 한다\n\n\n\n6\n• 표준용어는 표준단어와 표준도메인을 조합하여 구성하며 한 개의 표준도메인으로 구성된다\n렌탈 + 가능(X) 렌탈 + 가능 + 금액(O)\n\n\n7\n• 표준용어 명명 규칙 - 표준용어는 누구나 이해하기 쉽도록 구체적이고 명확하고 간결하게 정의한다 - 복합어를 단일어 보다 우선 적용한다 - 복합어가 중첩되어 사용될 경우 도메인이 포함된 복합어를 우선 적용한다 - 의미 있는 숫자를 포함한 용어의 경우에는 숫자를 포함하여 하나의 표준단어를 등록한 후 그 표준 단어를 사용하여 용어를 정의한다 - 용어의 의미를 모호하게 하는 의미 없는 일련번호를 부여하기 위한 숫자는 사용하지 않으며 용어에 수식어를 사용하여 용어가 유일하게 식별되도록 정의하는 것을 원칙으로 한다\n\n\n\n\n\n\n\n\n표준 용어 = 수식어 (표준 단어) + 주제어 (표준 단어) + \\(\\dots\\) + 수식어 (표준 단어) + 주제어 (표준 단어) + 분류어\n수식어 예시\n\n\n\n수식어\n예시\n\n\n\n\n기간 수식어\n최초, 최종, 과거, 최근 등\n\n\n기간/시간\n6개월, 당월, 월말, 년초, 년말 등\n\n\n장소\n국외, 국내, 지점, 본점 등\n\n\n특징\n순수, 사용, 처리, 거래 등\n\n\n계산\n합계: 한데 모아서 합산함누계: 계속하여 덧붙여 합산함\n\n\n\n표준 용어 = 표준 용어 + 표준 도메인\n원칙 항목 및 설명\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 용어는 수식과 수식의 대상이 되는 단어가 여러 개 존재할 수 있고 도메인을 수식하는 분류어는 용어의 끝에 위치한다\n\n\n\n2\n• 수식어는 기간, 장소, 특징, 계산의 성격을 가지는 단어가 순서대로 위치하고 기간을 수식하는 단어는 맨 앞에 위치한다\n\n\n\n3\n• 수식어 중 계산의 성격을 가진 단어는 합계, 누계, 총합계, 총누계, 소계 중 하나를 선택하여 사용해야 한다. (도메인 그룹이 금액과 수량과 같이 계산이 필요한 도메인을 수식함)\n\n\n\n4\n• 대상이 되는 단어가 여러 개일 때는 중 범위가 큰 것 순서대로 용어의 앞부분에 위치한다\n\n\n\n\n주의 사항\n\n\n\n\n\n\n\n\nAS-IS 용어 (비권장)\nTO-BE 용어(권장)\n비고\n\n\n\n\n회사에게 하고 싶은 말 내용\n사용자건의사항\n서술형용어\n\n\n공임금액구분별금액\n공임구분별금액\n단어 반복\n\n\n법인번호\n?+법인등록번호\n주제어 누락 및 약어 사용\n\n\n상세순번\n?+상세일련번호\n주제어 누락\n\n\n수정자\n?+수정자+?\n주제어 및 분류어 누락\n\n\n스케줄추가(청구)정보 수정여부\n스케줄추가정보수정여부\n특수문자사용\n\n\n요청일\n장기렌트입고요청일자\n주제어누락\n\n\n운전자\n렌트카운전자성명\n주제어 및 분류어 누락\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준용어에 적절한 명칭을 부여하고 전사적 공유가 가능하기 위해서는 구체적인 정의가 반드시 있어야 한다- 표준용어정의는 활동을 나타내는 단어와 그 주체가 되는 단어와 무엇을 어떻게 했는지의 내용이 담겨 있어야 한다.\n대여차량입고일자: 렌트차량이 반환되어 차고지에 입고 된 일자이다.\n\n\n2\n• 표준용어의 명칭이 주는 의미가 불분명하면 좀더 상세하게 정의해야 한다.\n유효기간(X) → 회원멤버쉽유효기간(O)\n\n\n3\n• 동일한 의미의 용어가 중복되지 않도록 표준용어 구성 순서를 고려해 생성한다\n현금서비스최종3개월 총합계금액(X) → 최종3개월 현금서비스 총합계금액(O)\n\n\n4\n• 표준용어의 한글명 또는 영문명 길이 제한으로 인해 축약된 형태로 사용해야 하는 경우 용어를 구성하는 단어 중 연관도와 활용도가 높은 단어들을 합하여 복합어를 정의해야 한다\n고객차량등급^코드 → 고객차량등급코드\n\n\n5\n• 표준용어 중 ‘여부’ 도메인으로 끝나는 것들은 대표로 하나의 코드명과 코드 값(Y, N)을 등록하여 모든 공통적으로 사용하도록 한다.\n코드일련번호 : B011\n\n\n6\n• ‘계약번호’ 자체로는 그 의미가 불분명하여 대차계약번호, 유지보수계약번호 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n7\n• 일반적으로 ‘렌탈’은 ’자동차대여’를 말하는 것이나 ’렌탈’ 자체로는 그 의미가 불분명하여 단기렌탈, 장기렌탈 등으로 좀더 상세화하여 표준용어를 정의한다\n\n\n\n8\n• 신청사용자아이디 → 사용자아이디 에서 ’신청’의 단어처럼 생략해도 의미가 통하는 경우에는 표준용어는 축약된 형태로 정의한다"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-3.data_word_glossary.html#제작-과정",
    "href": "docs/blog/posts/Governance/5-3.data_word_glossary.html#제작-과정",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "각 업무 영역에서 필요한 용어들을 수집\n기존 시스템, 문서, 보고서 등에서 사용 중인 비표준 용어들을 식별\n\n\n\n\n\n수집된 용어들의 의미와 사용 맥락을 분석\n유사하거나 중복된 용어들을 식별\n\n\n\n\n\n분석된 용어들을 기존의 표준 단어 사전과 매핑\n각 용어를 구성하는 단어들이 표준 단어 사전에 있는지 확인\n\n\n\n\n\n표준 단어들을 조합하여 새로운 표준 용어를 구성\n용어 구성 원칙(예: 수식어 순서, 도메인 위치 등)을 따른다.\n\n\n\n\n\n구성된 용어가 의미를 명확히 전달하는지 확인\n기존 표준 용어와 중복되지 않는지 검토\n전문가나 제 3의 부서와 cross check\n\n\n\n\n\n용어의 정의, 사용 예시, 관련 업무 영역 등 상세 정보를 작성\n용어와 관련된 도메인 정보를 지정\n\n\n\n\n\n구성된 용어에 대해 관련 부서와 데이터 관리자의 검토를 받는다.\n필요시 수정하고 최종 승인을 받는다.\n\n\n\n\n\n승인된 용어를 표준 용어 사전에 등록\n용어의 물리명(영문명)을 생성\n\n\n\n\n\n새로 등록된 표준 용어를 조직 내에 공지\n필요시 사용 방법에 대한 교육을 실시\n\n\n\n\n\n새로 등록된 표준 용어의 사용 현황을 모니터링\n사용자 피드백을 수집\n\n\n\n\n\n정기적 검토 및 피드백 수집\n\n분기별로 용어 사전 검토 일정을 수립\n사용자로부터 피드백을 수집하고, 사용 현황을 모니터링\n\n변경 관리 프로세스 운영\n\n용어 추가, 수정, 폐기를 위한 공식적인 변경 요청 프로세스를 구축\n변경 요청에 대한 영향 분석을 수행하고, 승인 절차를 거친다.\n\n업데이트 및 버전 관리\n\n승인된 변경사항을 용어 사전에 반영\n버전 관리를 통해 변경 이력을 추적하고, 주요 변경사항을 공지\n\n교육 및 홍보\n\n변경된 용어나 새로운 용어에 대한 교육을 실시\n내부 커뮤니케이션 채널을 통해 주요 업데이트 사항을 공유\n\n성과 측정 및 개선\n\n용어 표준화로 인한 데이터 품질 개선, 업무 효율성 증가 등의 성과를 측정\n측정 결과를 바탕으로 용어 관리 전략을 지속적으로 개선"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-3.data_word_glossary.html#표준-용어-사전-예시",
    "href": "docs/blog/posts/Governance/5-3.data_word_glossary.html#표준-용어-사전-예시",
    "title": "Data Governance Study - Data Standard Glossary",
    "section": "",
    "text": "예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n용어ID\n용어논리명\n용어물리명\n도메인\n정의\n관련업무영역\n사용예시\n표준단어구성\n승인상태\n등록일\n\n\n\n\nT001\n고객번호\nCUST_NO\n일련번호VC10\n고객을 유일하게 식별하는 번호\n고객관리, 영업\n“C0001234567”\n고객(주제어) + 번호(분류어)\n승인\n2023-01-15\n\n\nT002\n장기렌트계약시작일자\nLTRM_RENT_CNTR_STRT_DT\n일자VC8\n장기 렌트 계약이 시작되는 날짜\n계약관리, 렌트관리\n“20230107”\n장기(수식어) + 렌트(주제어) + 계약(주제어) + 시작(수식어) + 일자(분류어)\n승인\n2023-01-16\n\n\nT003\n월별렌탈등록비합계금액\nMTHLY_RENT_RGST_FEE_TOT_AMT\n금액N15\n한 달 동안의 렌탈 등록비 총액\n재무, 렌트관리\n“5000000”\n월별(수식어) + 렌탈(주제어) + 등록(주제어) + 비(주제어) + 합계(수식어) + 금액(분류어)\n검토중\n2023-01-17\n\n\n\n* VC10: Variable Character의 약자로 가변 길이 문자열을 나타냄 (최대 10 자리)  \n* N15: N은 숫자(Numeric) 데이터 타입, 최대 15자리의 숫자  \n* D: Date (날짜)  \n* T: Time (시간)  \n* B: Boolean (참/거짓)  \n\n칼럼 설명\n\n용어ID: 각 용어의 고유 식별자\n용어논리명: 업무에서 사용되는 한글 용어명 (최대 30자, 권장 20자 이내)\n용어물리명: 데이터베이스 등에서 사용되는 영문 약어명 (최대 28자, 권장 20자 이내)\n도메인: 해당 용어의 데이터 타입과 제약조건\n정의: 용어에 대한 명확한 설명\n관련업무영역: 해당 용어가 주로 사용되는 업무 분야\n사용예시: 실제 데이터 예시\n표준단어구성: 용어를 구성하는 표준 단어들과 각 단어의 역할 (수식어, 주제어, 분류어)\n승인상태: 용어의 현재 승인 상태 (예: 승인, 검토중, 폐기 등)\n등록일: 용어가 사전에 처음 등록된 날짜\n최종수정일: 용어 정보가 마지막으로 수정된 날짜"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-1.data_word_dictionary.html",
    "href": "docs/blog/posts/Governance/5-1.data_word_dictionary.html",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "조직 내에서 사용되는 데이터 용어를 표준화하고 관리하기 위한 도구이다. 이는 데이터 거버넌스와 데이터 품질 관리의 중요한 구성 요소이다.\n조직에서 사용되는 모든 데이터 용어의 공식적인 정의를 제공\n각 용어의 의미, 사용 맥락, 형식 등을 명확히 기술\n데이터 표준관리를 위한 표준 단어 사전을 만드는 과정은 체계적이고 협력적인 접근이 필요하다.\n표준 단어 사전을 만드는 과정은 반복적이고 지속적으로 이루어져야 하며, 조직의 변화와 새로운 요구사항을 반영하여 계속 발전시켜 나가야 한다.\n\n표준 단어 사전은 단순한 용어 목록이 아니라 조직의 데이터 자산을 효과적으로 관리하고 활용하기 위한 중요한 도구이다.\n조직의 모든 구성원이 쉽게 접근하고 활용할 수 있어야 한다.\n\n또한, 기술적인 구현뿐만 아니라 조직 문화와 프로세스의 변화도 함께 고려해야 한다.\n표준 용어 사전과 데이터 카탈로그를 제작하기 위한 성분이 된다.\n\n\n\n\n일반적으로 단어란 문법상 일정한 뜻과 구실을 가지는 말의 최소 단위를 의미한다.\n정보 시스템에서 사용하는 표준 단어란 회사에서 업무상 사용하며 일정한 의미를 갖고 있는 최소 단위의 단어를 말한다.\n표준용어를 구성 하는데 사용한다.\n예시\n\n표준 단어: 정산, 승인, 금액\n표준 용어: 정산승인금액\nDB 속성 또는 Data Modeling을 위한 속성\n\n\n\n\n속성명\n데이터타입\n도메인\n\n\n\n\n정산승인금액\nNUMBER(10)\n금액n10\n\n\n\n\n\n\n\n\n\n\n\n한 개의 단어에 대해 표준화된 영문약어를 사용하여 일관성을 확보한다.\n조직 전체에서 데이터 용어를 일관되게 사용할 수 있도록 한다.\n동일한 개념에 대해 서로 다른 용어를 사용하는 문제를 방지한다.\n\n\n\n\n\n유사하거나 중복된 용어를 식별하고 제거\n\n\n\n\n\n명명 규칙, 약어 사용, 데이터 형식 등을 표준화\n데이터 모델링과 시스템 개발에서 일관된 기준을 제공\n데이터 표준 단어 용도의 목적은 조직내 정보 공유가 제 1 목적이기 때문에 반드시 공공 기관이나 권위있는 조직의 양식을 따를 필요는 없다.\n이상적인 접근 방식은 외부 표준과 조직 내부의 요구사항을 균형 있게 고려하는 것이다.\n\n\n\n\n\n\n표준단어는 단어명, 영문명, 영문약어명 및 정의 등으로 구성되며 약어는 영문명을 축약하여 작성한다.\n\n\n[단어명]\n\n표준단어를 구성하는 최소단위의 단위를 의미하며 한글 및 영문, 숫자로 정의한다.\n표준단어의 최대길이는 15자로 하며, 10자 이내로 작성할 것을 권장한다.\n\n[영문명]\n\n표준단어명의 영문 명칭을 의미한다.\n영문명의 첫 자리의 알파벳은 대문자로 하고 나머지 부분은 소문자로 하며,영문 단어 간에는 띄어쓰기를 한다.\n\n[정의]\n\n해당 단어가 뜻하는 것 혹은 내용을 의미한다.\n데이터 명칭을 그대로 서술하거나 약어 또는 전문 용어를 이용한 기술은 가급적 지양한다.\n\n[영문약어명]\n\n영문명의 축약된 형태의 영문명칭을 의미하며 영문명을 바탕으로 영문약어를 정의한다.\n영문약어의 최대 길이는 4자로 한다.(권장)\n단, 고유명사나 관용적인 표현 등의 경우 글자수에 대한 예외를 허용한다.\n\n[단어 유형]\n\n기본단어(수식어성)\n\n용어를 구성하는 단어로 용어의 마지막에 존재할 수 없는 단어\n주제어, 수식어, 접두사/접미사, 복합단어 등이 이에 속한다\n예시, 주문,고객, 상품,거래\n\n분류단어 (도메인성)\n\n용어를 구성하는 단어로 용어의 마지막에 존재하는 단어\n용어가 가질 수 있는 속성 정보(데이터 형식 및 길이)를 구체적으로 표현하기 위해 사용한다\n분류단어는 도메인과 매핑 되어 그 속성정보를 정의한다\n예시, 금액,수량, 성명,코드, 번호,일자\n\n예시\n\n기본 단어: 정산, 승인\n분류 단어: 금액\n표준 용어: 정산승인금액\n\n\n예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n단어명\n한자\n영문명\n정의\n영문약어명\n단어종류\n단어유형\n\n\n\n\n가격\n價格\nPrice\n물건이 지니고 있는 가치를 돈으로 나타낸 것\nPRC\n기본단어\n단일어\n\n\n연령\n年齡\nAge\n나이(사람 등 세상에서 나서 살아온 햇수)\nAGE\n기본단어\n분류어\n\n\n우편번호\n郵便番號\nZip Code\n우편물을 쉽게 분류하기 위하여 정보 통신부에서 각 지역마다 매긴 번호\nZPCD 또는 ZIP(관용적표현)\n분류단어\n복합어\n\n\n\n\n\n\n단일어\n\n하나의 형태소(形態素: 의미의 기능을 부여하는 언어의 형태론적 수준에서의 최소단위)로 성립된 단어\n\n고유명사(기관명 포함), 지명(명사) 단일어로 사용함\n접사(접두사 및 접미사)와 합성된 단어\n두 개의 단일어로 구성되나 영문단어가 각각의 단일어의 영문단어의 조합과 일치 하지 않고 다른 영문단어가 존재하는 경우\n\n예시) 고객,지점, 가격,시설\n\n복합어\n\n둘 이상의 어근(실질 형태소)이 결합 이루어진 단어\n예시) 전화번호, 휴대폰번호, 차용지점\n\n외래어\n\n원래 외국어였던 것이 국어의 체계에 동화되어 사회적으로 그 사용이 허용된 단어\n예시) 이메일, 팩스, 네비게이션\n\n관용어\n\n한글 단어와 외래어가 결합되어 사용되거나 기타 국어 체계에 부합하지 않더라도 관용적으로 자주 사용되며 의미가 명확한 단어\n일반 사회나 기관에서 관습적으로 널리 쓰는 말\n예시) VDC,ABS, 셀프계약서, MT\n\n접사 (접두사/접미사)\n\n접두사(Prefix): 어떤 낱말 앞에 붙어서 의미를 첨가하여 한 다른 낱말을 이루는 말\n접미사(Suffix): 낱말의 끝에 붙어 의미를 첨가하여 다른 낱말을 이루는 말\n예시) 미사용, 보증료\n\n\n\n\n\n\n동음이의어\n\n발음은 동일하나 의미가 다른 단어(Homonym)\n예시: 매수(Sheets Count) / 매수(Buying)\n\n이음동의어\n\n동일한 의미를 표현하는 두 개의 다른 단어 (Synonym)\n즉, 발음은 다르나 동일한 의미를 표현하는 단어\n예시: 이수(Completion) / 수료(Completion)\n\n금칙어\n\n표준단어 사용의 일관성을 위하여 사용하지 못하게 지정된 단어\n예시: 이해당사자(X), 이해관계자(O)\n\n유사어\n\n표준단어 사용의 일관성을 위하여 권장어(대체어) 사용을 권고하는 단어\n예시: 수정, 변경, 정정\n\n축약어\n\n줄여서 간략하게 표현하는 단어\n주민번호(X), 주민등록번호(O)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준단어는 단일어 형태의 명사형 낱말을 표준단어로 정의하는 것을 기본 원칙으로 한다.\n고객, 가격, 금액, 지점\n\n\n2\n• 동일한 의미의 단어를 한글과 영문으로 중복해서 정의하지 않는다.\nRENT(X) → 렌트(O)\n\n\n3\n• 축약된 형태의 단어로 정의하지 않는다. 단, 범용 또는 공식적으로 사용이 승인된 약어는 표준 원칙에 의거하여 사용할 수 있다.• 또한 원래 단어가 너무 길거나 잘 활용하지 않아서 업무적으로 축약된 단어를 주로 사용하는 경우에 한하여 사용할 수 있다• 용어명 길이 제약을 해결하기 위해 부득이하게 약어를 사용해야 할 경우에는 약어와 전체 단어를 모두 표준단어로 등록하도록 한다\n주민번호(X) → 주민등록번호(O)등평(X) → 등급평가(O)\n\n\n4\n• 한글 축약어는 다른 단어와 붙여서 쓸 경우 혼동이 될 우려가 있으므로 가급적 풀어 쓴 단어를 사용한다. 단, 어감상 필요 시는 예외로 적용할 수 있다. (이음동의어)\n가(假)(X) → 임시(O)현(X) → 현재(O)전(X) → 이전(O)후(X) → 이후(O)\n\n\n5\n• 과거 단어와 현대 단어를 함께 사용하고 있는 경우 가급적 현대 단어를 사용한다.\n\n\n\n6\n• 접두사 및 접미사는 별도로 분리 하지 않으며 표준단어(단일어)로 등록함을 원칙으로 한다\n\n\n\n7\n• 고유명사(기관명 포함)는 한글 단어를 사용하는 것을 원칙으로 한다\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 외국에서 들어온 말로 국어처럼 쓰이는 외래어는 영문을 쓰지 않고 외래어 한글 표기법에 따라 정의하는 것을 원칙으로 한다.\nFax(X) → 팩스(O)Email(X) → 이메일(O)Database(X) → 데이터베이스(O)\n\n\n2\n• 영문을 대체할 적절한 한글이 없거나 영문자체에 고유한 업무적 의미를 담고 있는 경우, 또한 관용적으로 두음문자 표현이 사용되고 있는 경우에는 해당 영문을 그대로 사용한다. 단, 표준단어로 등록 시 해당 영문의 전체명이 아닌 두음문자 형식의 대문자로 등록한다.\nMMF, HTML, SQL\n\n\n3\n• 영문 단어 첫글자는 알파벳 대문자로 나머지는 소문자로 작성한다.\nSWIFT(X) → Swift(O)\n\n\n4\n• 영문 단어를 한글화 할 경우, 영문 의미로 한글화 한 경우와 소리 나는 대로 한글화한 경우 둘 다 자주 사용될 경우, 소리 나는 대로 한글화한 단어를 표준으로 하고, 의미로 한글화 한 단어는 금칙어로 등록한다.\nERROR(X), 오류(X) → 에러(O)\n\n\n5\n• 한글 단어 보다 더 친숙하게 사용되는 영문 단어의 경우에는 그 영문단어를 사용한다.\n인터넷프로토콜(X), 아이피(X) → IP(O)\n\n\n6\n• 관용적으로 사용하는 단어나 고유명사인 경우 특수문자 중 ‘/’, ‘-’, ’&’만을 사용할 수 있으나, 가급적 특수문자를 사용하지 않는다\nM&A(O)\n\n\n7\n• 한글, 영문, 숫자의 혼용이 가능하다\n회사(O), 12월(O)\n\n\n8\n• 한글과 영문만 표준 단어로 인정하며 기타 외국어(한자, 일본어 등)는 사용하지 않는다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 단어의 대표 자음으로 영문 약어를 구성한다. 대표 자음의 적용 우선 순위는 앞 자리의 자음부터 4 글자까지 적용한다. 모음(A,E,I,O,U)외 모든 알파벳이 자음이다.• 약어 중복 발생 시 모음을 순차적으로 추가해서 약어를 생성한다.\n대여: Rental → RNTL\n\n\n2\n• 가급적 4글자의 약어로 구성하되, 고유명사나 관용적인 표현 등의 경우 글자수에 대한 예외를 허용한다\n배치: BATCH\n\n\n3\n• 영문 약어의 글자수가 4글자 이하인 경우에는 가장 앞자리의 모음을 대표 자음으로 간주하여, 이 모음 앞뒤에 나오는 대표 자음과 결합하여 4자리 약어를 적용한다.• Y와 같이 자음이지만 모음으로 혼동하는 알파벳도 모두 자음으로 간주한다.\n급여: Color → COLR\n\n\n4\n• 영문 약어명의 사용시 한글 발음 식의 약어는 사용하지 않으나 순수 한글이거나 대체할 영문이 없는 경우 허용한다\n주소: JUSO → ADR, 시: SI, 군: GUN, 구: GU\n\n\n5\n• 영문 약어는 항상 유일성을 유지하도록 관리한다\n\n\n\n6\n• 두 개의 자음이 연속할 경우, 가급적 한 개의 자음은 생략하는 형태로 표현한다.\n결제: Settlement → STTL (X) → STL (O)\n\n\n7\n• 범용적으로 사용되는 두문자어(頭文字語: Acronyms)가 있는 경우 되도록 두문자어를 사용하며 글자수가 지나치게 긴 경우 새로운 영문약어를 작성한다.\nSQL(Structured Query Language), UDM(Universal Data Model)\n\n\n8\n• 한글단어에 대해 관용적으로 사용되는 영문 약어가 있으면 이를 사용한다.\nNumber→NMBR(X)→ NO(O)\n\n\n9\n• 영문명을 일반적으로 축약해서 사용되는 영문 약어가 있는 경우 이를 사용한다.\nPackage → PCKG (X) → PKG (O)\n\n\n10\n• 한 영문명의 글자수가 4글자 이하의 경우는 그대로 사용한다. 단, 합성어의 경우 4글자 이상을 사용할 수 있다\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 업무적 또는 관용적으로 자주 쓰이는 표현이나 단일어 단위로 구분해서 사용할 경우 의미 전달이 불분명해질 수 있는 단어에 대해서는 복합어로 구성 사용한다\n계좌번호(Account Number) = 계좌(Account)+번호(Number)\n\n\n2\n• 고유명사(기관명 포함)는 단일어 형태로 사용한다.\n회사\n\n\n3\n• 접두사 및 접미사와 합성된 단어는 단일어 형태로 사용한다\n재[再]발급, 지급처[處]\n\n\n4\n• 두 개 이상의 단일어로 이루어졌으나 별도의 영문 단어가 존재하며 각 단일어의 조합과는 다른 의미를 지니게 되는 경우 단일어 형태로 사용한다.\n감가상각(Depreciation) ≠ 감가(Reduction)+ 상각(Repayment)\n\n\n5\n• 한글과 외래어가 결합되어 사용되거나 기타 사용 원칙에 부합하지 않더라도 관용적으로 자주 사용되며 의미가 명확한 경우 사용한다. (관용적 표현)\n파레트수량\n\n\n6\n• 유형의 구분을 나타내는 복합 단어는 단어로 식별하지 않도록 한다. 이때에는 유형을 대표하는 다른 단어 또는 용어로 대체하도록 한다. 단, 관용적으로 자주 쓰이는 표현이면서 대체 단어 또는 용어 구성이 어렵다면 사용을 허용한다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 숫자 사용 시 아라비아 숫자 사용을 원칙으로 한다\n삼순위(X) → 3순위(O)\n\n\n2\n• 숫자만으로는 단어가 될 수 없고 해당 숫자의 의미를 나타내는 단어와 함께 조합하여 사용한다\n6(X) → 6개월(O)\n\n\n3\n• 숫자와 단위의 합성어는 단일어로 등록한다\n100퍼센트, 91일\n\n\n4\n• 단위 앞에 올 수 있는 숫자의 유효값이 제한적인 경우 유효한 단어를 모두 등록한다.\n1월,2월 ~ 12월1분기,2분기,3분기,4분기1순위,2순위,3순위\n\n\n5\n• 단위 앞에 올 수 있는 숫자의 유효값이나 범위가 제한이 없는 경우 해당 단위를 등록하고 최소 1단위와 합성된 단어를 등록한다\n퍼센트 - 1퍼센트개월 -1개월차 - 1차급 - 1급\n\n\n6\n• 숫자 단위 대에 따라 표준을 다르게 정의할 경우 각각을 단어로 등록할 수 있다\n1원,1십원,1천원,1만원,1십만원,1백만원,1천만원\n\n\n7\n• 숫자와 조합된 단어의 의미가 불분명한 경우는 해당 단어의 사용을 지양한다 단, 의미가 불분명한 단어임에도 업무상의 필요로 인해 등록이 불가피한 경우 단어 뒤에 숫자를 붙여서 정의한다\n컬럼1(X)※ 단 불가피한 경우 사용\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 동음이의어(Homonym)는 허용하지 않는다 - (대체방안1) 다른 한글단어로 교체하여 표준 단어로 등록하여 사용한다. - (대체방안2) 어감상 동음이의어인데도 불구하고 ’의사’라는 단오를 꼭 써야 하는 경우는 ’의사결정’식의 복합 단어를 표준단어로 등록하여 사용한다• 단, 대체 단어가 없는 경우 사용할 수 있다 (한글 명이 같더라도 영문 명은 반드시 달라야 한다) 이 경우 데이터 표준관리자 및 모델관리자에게 검토 요청을 신청 한다\n다리(leg) : 다리다리(bridge) : 교량의사(doctor) : 의사의사(idea)결정(Decision):의사결정\n\n\n2\n• 이음동의어(Synonym)는 용어의 혼돈과 용어생성 시 중복발생 가능성 때문에 가급적 사용하지 않는다.• 대표단어를 정하고, 그 대표 단어만을 사용하도록 하며 나머지는 금칙어로 등록하여 사용을 제한하도록 한다\n핸드폰(X) → 휴대폰(O)※ 핸드폰은 금치어로 등록\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 이음동의어는 대표 단어만을 표준단어로 정의하여 사용하도록 하며, 나머지 이음 동의어의 단어들은 금칙어로 정의하여 사용을 사전에 방지한다.\n나이(X) → 연령(O)금일(X) → 당일(O)\n\n\n2\n• 축약된 형태의 단어(축약어)는 금칙어로 정의하여 사용을 사전에 방지한다.\n주민번호(X) → 주민등록번호(O)\n\n\n3\n• 사람에 해당하는 접미사 “자”와 “인”으로 구성된 복합어의 경우에는 사용빈도가 높은 단어를 표준단어로 정의하고, 다른 나머지 단어를 금칙어로 관리하여 사용을 사전에 방지한다\n장애자(X) → 장애인(O)신청인(X) → 신청자(O)\n\n\n4\n• 한글 맞춤법을 고려하지 않고 관용적으로 사용되던 단어들은 금칙어로 정의하여 사용을 사전에 방지한다.\n갯수(X) → 개수(O)써비스(X) → 서비스(O)\n\n\n5\n• 영문 단어의 경우 한글화를 원칙으로 한다. 단, 한글 단어 보다 더 친숙하게 사용되는 영문 단어의 경우에는 그 영문 단어를 사용한다. 이렇게 선정된 단어에 대응되는 영문 혹은 한글 단어는 금칙어로 등록하고 사용을 제한하도록 한다\nERROR(X) → 에러(O)FAX(X) → 팩스(O)EMAIL(X) → 이메일(O)아이피(X) → IP(O)\n\n\n6\n• 범용적으로 사용되는 외래어의 경우 사용할 수 있지만 표기법을 고려하여 표준 단어를 지정하고, 나머지 단어는 금칙어로 등록하여 사용을 제한하도록 한다\nEXPOSURE(X), 익스포저(X) → 익스포져(O),화일(X), FILE(X) →파일(O)\n\n\n7\n• 전문 업무용어 중 관용적으로 영문의 두문자어(頭文字語:Acronyms)를 사용하는 경우 한글단어는 금칙어로 정의한다.\n\n\n\n8\n• 기관명 등이나 고유명사 등은 전체 단어를 표준으로 사용하고, 한글 약어는 금칙어로 등록한다.\n\n\n\n9\n• 시스템명중 관용적으로 영문의 두문자어(頭文字語:Acronyms)를 사용하는 경우 한글단어는 금칙어로 정의한다\n데이터웨어하우스(X) → DW(O)\n\n\n면적 : Area → AREA (O), 역할 : Role → ROLE (O)\n\n\n\n\n\n\n\n\n\n\n\n\n표준 단어 사전의 목적과 범위를 명확히 한다.\n\n목적이라 함은 데이터 일관성 확보(동일한 의미의 데이터 사용), 데이터 품질 향상 (데이터 처리 과정 비용 감소), 시스템 통합 지원 (여러 시스템 간 데이터 매핑과 통합), 규제 준수 지원 등을 의미한다.\n범위는 대상 데이터 범위, 조직적 범위, 시스템 범위 등을 의미하며, 너무 광범위한 영역은 제작 실패로 이어진다.\n\n이해관계자 식별: 관련 부서와 담당자들을 파악한다.\n거버넌스 체계 수립: 단어 사전 관리를 위한 조직과 프로세스를 정립한다.\n\n\n\n\n\n기존 데이터 모델, 데이터베이스 스키마, 업무 문서 등에서 사용 중인 단어들을 수집한다.\n업무 도메인별로 사용되는 용어들을 취합한다.\n\n\n\n\n\n동의어, 유사어, 약어 등을 식별한다.\n업무 영역별 용어의 의미와 사용 맥락을 분석한다.\n불필요하거나 중복된 단어들을 제거한다.\n\n\n\n\n\n약어 사용 규칙, 대소문자 규칙 등의 명명 규칙을 정의합니다.\n단어 정의 형식을 standardization 합니다.\n도메인별 특수 규칙을 설정한다.\n\n유용한 데이터 표준안을 만들기 위해 조직 내의 각 업무 영역 또는 데이터 도메인에 맞는 고유한 규칙이나 지침을 만들어야 한다.\n도메인 식별: 재무, 인사, 마케팅, 제조, 고객 서비스 등\n각 도메인별 특수성 파악: 해당 도메인에서만 사용되는 용어나 개념과 도메인 특유의 데이터 형식이나 제약 조건\n예시\n\n재무 도메인의 통화 표기 규칙: “USD 1,000.00” 또는 “1,000,000 원” 형식 사용\n재무 도메인의 회계 기간 표현: “FY2023Q2” (2023 회계연도 2분기)\n인사 도메인의 직급 코드 체계: “M” (매니저), “D” (디렉터)\n인사 도메인의 근속 연수 계산 규칙: 입사일 기준, 월 단위 반올림\n마케팅 도메인의 캠페인 코드 형식: “CAM_2023_SUMMER_01”\n마케팅 도메인의 고객 세그먼트 분류 기준: “VIP”, “REGULAR”, “NEW”\n제조 도메인의 제품 코드 체계: “PROD-A01-R” (제품군-모델번호-버전)\n제조 도메인의 품질 등급 표기: “A”, “B”, “C” 등급 사용\n\n\n\n\n\n\n\n분석된 단어들 중 표준으로 사용할 단어들을 선정\n선정 기준을 명확히 하고, 이해관계자들의 합의를 도출\n선정 기준 예시\n\n명확성\n\n의미가 명확하고 모호하지 않은 단어 선정 (일반적인 단어는 한정시킬 것)\n예: “고객” 대신 “활성고객”과 “비활성고객”으로 구분\n\n일관성\n\n조직 전체에서 일관되게 사용할 수 있는 단어\n예: 부서별로 다르게 사용되던 용어를 하나로 통일\n\n간결성\n\n가능한 간단하고 간결한 단어\n예: “제품구매고객정보” 대신 “구매자정보”\n\n유일성\n\n중복되지 않는 고유한 의미를 가진 단어\n예: 동음이의어 피하기\n\n업계 표준 부합성\n\n가능한 업계에서 널리 사용되는 표준 용어 선택\n예: 금융업계의 “ROI” (Return on Investment)\n\n확장성\n\n향후 변화나 확장을 고려한 단어 선택\n예: “2023년예산” 대신 “연간예산”\n\n이해 용이성\n\n비전문가도 이해하기 쉬운 단어\n예: 전문 용어보다는 일반적인 비즈니스 용어 선호\n\n번역 가능성\n\n다국어 지원이 필요한 경우, 번역이 용이한 단어\n관용구나 은유적 표현 피하기\n\n기존 시스템 호환성\n\n기존 시스템과의 호환성을 고려한 단어\n예: 레거시 시스템의 주요 용어 유지\n\n법규 및 규제 준수\n\n관련 법규나 규제를 준수하는 단어\n예: 개인정보보호법에 부합하는 용어 선택\n\n도메인 적합성\n\n해당 업무 도메인에 적합한 단어\n예: 금융 도메인에서는 “이자율”, 제조 도메인에서는 “불량률”\n\n측정 가능성\n\n정량적 측정이 가능한 개념을 나타내는 단어\n예: “고객만족도” (1-5 척도로 측정 가능)\n\n약어 사용 규칙\n\n약어 사용 시 일관된 규칙 적용\n규칙에 부합하지 않는 관용어는 관용어 사용을 유지하는 것이 좋다.\n예: “고객번호”를 “CUST_NO”로 통일\n\n\n\n\n\n\n\n각 단어에 대한 상세 정보를 정의 (정의, 동의어, 사용 예시, 관련 업무 영역 등).\n단어 간의 관계를 정의 (상위어, 하위어, 관련어 등)\n\n\n\n\n\n선정된 표준 단어들에 대해 관련 부서와 전문가들의 검토를 받는다.\n필요시 수정하고 최종 승인을 받는다.\n\n\n\n\n\n승인된 단어들을 데이터베이스나 전문 도구에 등록한다. (엔코아, Microsoft Purview 등)\n검색, 조회, 관리가 용이한 형태로 구성한다.\n\n\n\n\n\n완성된 표준 단어 사전을 조직 내에 공유\n사용 방법과 중요성에 대한 교육을 실시\n\n\n\n\n\n새로운 단어 추가, 기존 단어 수정, 폐기 등의 프로세스를 수립한다.\n정기적인 검토와 업데이트를 수행한다.\n사용 현황을 모니터링하고 피드백을 수집한다.\n\n\n\n\n\n데이터 모델링, 시스템 개발, 보고서 작성 등의 프로세스와 표준 단어 사전을 연계한다\n\n연계 순서는 각 상황마다 다르다.\n이미 ERD와 DB가 존재하는 시스템들을 통합하는 상황이라면 표준 단어 사전을 순서상 나중에 제작하는 것이 유리하다\n그 반대라면 표준 단어사전을 먼저 만들어 DB를 만드는 것이 유리할 수 있다.\n\n다른 데이터 관리 도구들과의 통합을 고려한다.\n\n\n\n\n\n\n예시 1\n\n\n\n\n표준 단어 사전 예시\n\n\n\n예시 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n단어 ID\n한글명\n한자\n영문명\n영문약어\n정의\n도메인\n데이터 타입\n길이\n허용값\n관련 업무 영역\n사용 예시\n동의어/유사어\n상위어\n하위어\n등록일\n최종 수정일\n승인 상태\n\n\n\n\nW001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW002\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW003\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n칼럼 설명:\n\n단어 ID: 각 단어의 고유 식별자\n한글명: 한글로 된 단어명\n한자: 해당 단어의 한자 표기 (필요한 경우)\n영문명: 영문으로 된 단어명\n영문약어: 영문 약어 (필요한 경우)\n정의: 단어의 명확한 정의\n도메인: 해당 단어가 속한 비즈니스 도메인\n데이터 타입: 해당 단어의 데이터 타입 (예: VARCHAR, INTEGER 등)\n길이: 데이터 길이\n허용값: 허용되는 값의 범위 또는 목록\n관련 업무 영역: 해당 단어가 주로 사용되는 업무 영역\n사용 예시: 실제 사용 예시\n동의어/유사어: 관련된 동의어나 유사어\n상위어: 해당 단어의 상위 개념 단어 (여러 개일 경우 쉼표로 구분)\n\n상위어: 이해관계자\n\n하위어: 해당 단어의 하위 개념 단어들 (여러 개일 경우 쉼표로 구분)\n\n하위어: 개인고객, 법인고객, VIP고객\n\n등록일: 단어가 사전에 처음 등록된 날짜\n최종 수정일: 마지막으로 수정된 날짜\n승인 상태: 현재 승인 상태 (예: 승인됨, 검토 중, 폐기 등)\n\n이 템플릿은 조직의 필요에 따라 수정하거나 필드를 추가/제거할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5-1.data_word_dictionary.html#데이터-표준-단어-사전이란",
    "href": "docs/blog/posts/Governance/5-1.data_word_dictionary.html#데이터-표준-단어-사전이란",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "조직 내에서 사용되는 데이터 용어를 표준화하고 관리하기 위한 도구이다. 이는 데이터 거버넌스와 데이터 품질 관리의 중요한 구성 요소이다.\n조직에서 사용되는 모든 데이터 용어의 공식적인 정의를 제공\n각 용어의 의미, 사용 맥락, 형식 등을 명확히 기술\n데이터 표준관리를 위한 표준 단어 사전을 만드는 과정은 체계적이고 협력적인 접근이 필요하다.\n표준 단어 사전을 만드는 과정은 반복적이고 지속적으로 이루어져야 하며, 조직의 변화와 새로운 요구사항을 반영하여 계속 발전시켜 나가야 한다.\n\n표준 단어 사전은 단순한 용어 목록이 아니라 조직의 데이터 자산을 효과적으로 관리하고 활용하기 위한 중요한 도구이다.\n조직의 모든 구성원이 쉽게 접근하고 활용할 수 있어야 한다.\n\n또한, 기술적인 구현뿐만 아니라 조직 문화와 프로세스의 변화도 함께 고려해야 한다.\n표준 용어 사전과 데이터 카탈로그를 제작하기 위한 성분이 된다.\n\n\n\n\n일반적으로 단어란 문법상 일정한 뜻과 구실을 가지는 말의 최소 단위를 의미한다.\n정보 시스템에서 사용하는 표준 단어란 회사에서 업무상 사용하며 일정한 의미를 갖고 있는 최소 단위의 단어를 말한다.\n표준용어를 구성 하는데 사용한다.\n예시\n\n표준 단어: 정산, 승인, 금액\n표준 용어: 정산승인금액\nDB 속성 또는 Data Modeling을 위한 속성\n\n\n\n\n속성명\n데이터타입\n도메인\n\n\n\n\n정산승인금액\nNUMBER(10)\n금액n10"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-1.data_word_dictionary.html#제작시-유념해야할-사항",
    "href": "docs/blog/posts/Governance/5-1.data_word_dictionary.html#제작시-유념해야할-사항",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "한 개의 단어에 대해 표준화된 영문약어를 사용하여 일관성을 확보한다.\n조직 전체에서 데이터 용어를 일관되게 사용할 수 있도록 한다.\n동일한 개념에 대해 서로 다른 용어를 사용하는 문제를 방지한다.\n\n\n\n\n\n유사하거나 중복된 용어를 식별하고 제거\n\n\n\n\n\n명명 규칙, 약어 사용, 데이터 형식 등을 표준화\n데이터 모델링과 시스템 개발에서 일관된 기준을 제공\n데이터 표준 단어 용도의 목적은 조직내 정보 공유가 제 1 목적이기 때문에 반드시 공공 기관이나 권위있는 조직의 양식을 따를 필요는 없다.\n이상적인 접근 방식은 외부 표준과 조직 내부의 요구사항을 균형 있게 고려하는 것이다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5-1.data_word_dictionary.html#표준단어-구성-요소",
    "href": "docs/blog/posts/Governance/5-1.data_word_dictionary.html#표준단어-구성-요소",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "표준단어는 단어명, 영문명, 영문약어명 및 정의 등으로 구성되며 약어는 영문명을 축약하여 작성한다.\n\n\n[단어명]\n\n표준단어를 구성하는 최소단위의 단위를 의미하며 한글 및 영문, 숫자로 정의한다.\n표준단어의 최대길이는 15자로 하며, 10자 이내로 작성할 것을 권장한다.\n\n[영문명]\n\n표준단어명의 영문 명칭을 의미한다.\n영문명의 첫 자리의 알파벳은 대문자로 하고 나머지 부분은 소문자로 하며,영문 단어 간에는 띄어쓰기를 한다.\n\n[정의]\n\n해당 단어가 뜻하는 것 혹은 내용을 의미한다.\n데이터 명칭을 그대로 서술하거나 약어 또는 전문 용어를 이용한 기술은 가급적 지양한다.\n\n[영문약어명]\n\n영문명의 축약된 형태의 영문명칭을 의미하며 영문명을 바탕으로 영문약어를 정의한다.\n영문약어의 최대 길이는 4자로 한다.(권장)\n단, 고유명사나 관용적인 표현 등의 경우 글자수에 대한 예외를 허용한다.\n\n[단어 유형]\n\n기본단어(수식어성)\n\n용어를 구성하는 단어로 용어의 마지막에 존재할 수 없는 단어\n주제어, 수식어, 접두사/접미사, 복합단어 등이 이에 속한다\n예시, 주문,고객, 상품,거래\n\n분류단어 (도메인성)\n\n용어를 구성하는 단어로 용어의 마지막에 존재하는 단어\n용어가 가질 수 있는 속성 정보(데이터 형식 및 길이)를 구체적으로 표현하기 위해 사용한다\n분류단어는 도메인과 매핑 되어 그 속성정보를 정의한다\n예시, 금액,수량, 성명,코드, 번호,일자\n\n예시\n\n기본 단어: 정산, 승인\n분류 단어: 금액\n표준 용어: 정산승인금액\n\n\n예시\n\n\n\n\n\n\n\n\n\n\n\n\n\n단어명\n한자\n영문명\n정의\n영문약어명\n단어종류\n단어유형\n\n\n\n\n가격\n價格\nPrice\n물건이 지니고 있는 가치를 돈으로 나타낸 것\nPRC\n기본단어\n단일어\n\n\n연령\n年齡\nAge\n나이(사람 등 세상에서 나서 살아온 햇수)\nAGE\n기본단어\n분류어\n\n\n우편번호\n郵便番號\nZip Code\n우편물을 쉽게 분류하기 위하여 정보 통신부에서 각 지역마다 매긴 번호\nZPCD 또는 ZIP(관용적표현)\n분류단어\n복합어\n\n\n\n\n\n\n단일어\n\n하나의 형태소(形態素: 의미의 기능을 부여하는 언어의 형태론적 수준에서의 최소단위)로 성립된 단어\n\n고유명사(기관명 포함), 지명(명사) 단일어로 사용함\n접사(접두사 및 접미사)와 합성된 단어\n두 개의 단일어로 구성되나 영문단어가 각각의 단일어의 영문단어의 조합과 일치 하지 않고 다른 영문단어가 존재하는 경우\n\n예시) 고객,지점, 가격,시설\n\n복합어\n\n둘 이상의 어근(실질 형태소)이 결합 이루어진 단어\n예시) 전화번호, 휴대폰번호, 차용지점\n\n외래어\n\n원래 외국어였던 것이 국어의 체계에 동화되어 사회적으로 그 사용이 허용된 단어\n예시) 이메일, 팩스, 네비게이션\n\n관용어\n\n한글 단어와 외래어가 결합되어 사용되거나 기타 국어 체계에 부합하지 않더라도 관용적으로 자주 사용되며 의미가 명확한 단어\n일반 사회나 기관에서 관습적으로 널리 쓰는 말\n예시) VDC,ABS, 셀프계약서, MT\n\n접사 (접두사/접미사)\n\n접두사(Prefix): 어떤 낱말 앞에 붙어서 의미를 첨가하여 한 다른 낱말을 이루는 말\n접미사(Suffix): 낱말의 끝에 붙어 의미를 첨가하여 다른 낱말을 이루는 말\n예시) 미사용, 보증료\n\n\n\n\n\n\n동음이의어\n\n발음은 동일하나 의미가 다른 단어(Homonym)\n예시: 매수(Sheets Count) / 매수(Buying)\n\n이음동의어\n\n동일한 의미를 표현하는 두 개의 다른 단어 (Synonym)\n즉, 발음은 다르나 동일한 의미를 표현하는 단어\n예시: 이수(Completion) / 수료(Completion)\n\n금칙어\n\n표준단어 사용의 일관성을 위하여 사용하지 못하게 지정된 단어\n예시: 이해당사자(X), 이해관계자(O)\n\n유사어\n\n표준단어 사용의 일관성을 위하여 권장어(대체어) 사용을 권고하는 단어\n예시: 수정, 변경, 정정\n\n축약어\n\n줄여서 간략하게 표현하는 단어\n주민번호(X), 주민등록번호(O)"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-1.data_word_dictionary.html#표준-단어-사용-원칙",
    "href": "docs/blog/posts/Governance/5-1.data_word_dictionary.html#표준-단어-사용-원칙",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 표준단어는 단일어 형태의 명사형 낱말을 표준단어로 정의하는 것을 기본 원칙으로 한다.\n고객, 가격, 금액, 지점\n\n\n2\n• 동일한 의미의 단어를 한글과 영문으로 중복해서 정의하지 않는다.\nRENT(X) → 렌트(O)\n\n\n3\n• 축약된 형태의 단어로 정의하지 않는다. 단, 범용 또는 공식적으로 사용이 승인된 약어는 표준 원칙에 의거하여 사용할 수 있다.• 또한 원래 단어가 너무 길거나 잘 활용하지 않아서 업무적으로 축약된 단어를 주로 사용하는 경우에 한하여 사용할 수 있다• 용어명 길이 제약을 해결하기 위해 부득이하게 약어를 사용해야 할 경우에는 약어와 전체 단어를 모두 표준단어로 등록하도록 한다\n주민번호(X) → 주민등록번호(O)등평(X) → 등급평가(O)\n\n\n4\n• 한글 축약어는 다른 단어와 붙여서 쓸 경우 혼동이 될 우려가 있으므로 가급적 풀어 쓴 단어를 사용한다. 단, 어감상 필요 시는 예외로 적용할 수 있다. (이음동의어)\n가(假)(X) → 임시(O)현(X) → 현재(O)전(X) → 이전(O)후(X) → 이후(O)\n\n\n5\n• 과거 단어와 현대 단어를 함께 사용하고 있는 경우 가급적 현대 단어를 사용한다.\n\n\n\n6\n• 접두사 및 접미사는 별도로 분리 하지 않으며 표준단어(단일어)로 등록함을 원칙으로 한다\n\n\n\n7\n• 고유명사(기관명 포함)는 한글 단어를 사용하는 것을 원칙으로 한다\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 외국에서 들어온 말로 국어처럼 쓰이는 외래어는 영문을 쓰지 않고 외래어 한글 표기법에 따라 정의하는 것을 원칙으로 한다.\nFax(X) → 팩스(O)Email(X) → 이메일(O)Database(X) → 데이터베이스(O)\n\n\n2\n• 영문을 대체할 적절한 한글이 없거나 영문자체에 고유한 업무적 의미를 담고 있는 경우, 또한 관용적으로 두음문자 표현이 사용되고 있는 경우에는 해당 영문을 그대로 사용한다. 단, 표준단어로 등록 시 해당 영문의 전체명이 아닌 두음문자 형식의 대문자로 등록한다.\nMMF, HTML, SQL\n\n\n3\n• 영문 단어 첫글자는 알파벳 대문자로 나머지는 소문자로 작성한다.\nSWIFT(X) → Swift(O)\n\n\n4\n• 영문 단어를 한글화 할 경우, 영문 의미로 한글화 한 경우와 소리 나는 대로 한글화한 경우 둘 다 자주 사용될 경우, 소리 나는 대로 한글화한 단어를 표준으로 하고, 의미로 한글화 한 단어는 금칙어로 등록한다.\nERROR(X), 오류(X) → 에러(O)\n\n\n5\n• 한글 단어 보다 더 친숙하게 사용되는 영문 단어의 경우에는 그 영문단어를 사용한다.\n인터넷프로토콜(X), 아이피(X) → IP(O)\n\n\n6\n• 관용적으로 사용하는 단어나 고유명사인 경우 특수문자 중 ‘/’, ‘-’, ’&’만을 사용할 수 있으나, 가급적 특수문자를 사용하지 않는다\nM&A(O)\n\n\n7\n• 한글, 영문, 숫자의 혼용이 가능하다\n회사(O), 12월(O)\n\n\n8\n• 한글과 영문만 표준 단어로 인정하며 기타 외국어(한자, 일본어 등)는 사용하지 않는다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 단어의 대표 자음으로 영문 약어를 구성한다. 대표 자음의 적용 우선 순위는 앞 자리의 자음부터 4 글자까지 적용한다. 모음(A,E,I,O,U)외 모든 알파벳이 자음이다.• 약어 중복 발생 시 모음을 순차적으로 추가해서 약어를 생성한다.\n대여: Rental → RNTL\n\n\n2\n• 가급적 4글자의 약어로 구성하되, 고유명사나 관용적인 표현 등의 경우 글자수에 대한 예외를 허용한다\n배치: BATCH\n\n\n3\n• 영문 약어의 글자수가 4글자 이하인 경우에는 가장 앞자리의 모음을 대표 자음으로 간주하여, 이 모음 앞뒤에 나오는 대표 자음과 결합하여 4자리 약어를 적용한다.• Y와 같이 자음이지만 모음으로 혼동하는 알파벳도 모두 자음으로 간주한다.\n급여: Color → COLR\n\n\n4\n• 영문 약어명의 사용시 한글 발음 식의 약어는 사용하지 않으나 순수 한글이거나 대체할 영문이 없는 경우 허용한다\n주소: JUSO → ADR, 시: SI, 군: GUN, 구: GU\n\n\n5\n• 영문 약어는 항상 유일성을 유지하도록 관리한다\n\n\n\n6\n• 두 개의 자음이 연속할 경우, 가급적 한 개의 자음은 생략하는 형태로 표현한다.\n결제: Settlement → STTL (X) → STL (O)\n\n\n7\n• 범용적으로 사용되는 두문자어(頭文字語: Acronyms)가 있는 경우 되도록 두문자어를 사용하며 글자수가 지나치게 긴 경우 새로운 영문약어를 작성한다.\nSQL(Structured Query Language), UDM(Universal Data Model)\n\n\n8\n• 한글단어에 대해 관용적으로 사용되는 영문 약어가 있으면 이를 사용한다.\nNumber→NMBR(X)→ NO(O)\n\n\n9\n• 영문명을 일반적으로 축약해서 사용되는 영문 약어가 있는 경우 이를 사용한다.\nPackage → PCKG (X) → PKG (O)\n\n\n10\n• 한 영문명의 글자수가 4글자 이하의 경우는 그대로 사용한다. 단, 합성어의 경우 4글자 이상을 사용할 수 있다\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 업무적 또는 관용적으로 자주 쓰이는 표현이나 단일어 단위로 구분해서 사용할 경우 의미 전달이 불분명해질 수 있는 단어에 대해서는 복합어로 구성 사용한다\n계좌번호(Account Number) = 계좌(Account)+번호(Number)\n\n\n2\n• 고유명사(기관명 포함)는 단일어 형태로 사용한다.\n회사\n\n\n3\n• 접두사 및 접미사와 합성된 단어는 단일어 형태로 사용한다\n재[再]발급, 지급처[處]\n\n\n4\n• 두 개 이상의 단일어로 이루어졌으나 별도의 영문 단어가 존재하며 각 단일어의 조합과는 다른 의미를 지니게 되는 경우 단일어 형태로 사용한다.\n감가상각(Depreciation) ≠ 감가(Reduction)+ 상각(Repayment)\n\n\n5\n• 한글과 외래어가 결합되어 사용되거나 기타 사용 원칙에 부합하지 않더라도 관용적으로 자주 사용되며 의미가 명확한 경우 사용한다. (관용적 표현)\n파레트수량\n\n\n6\n• 유형의 구분을 나타내는 복합 단어는 단어로 식별하지 않도록 한다. 이때에는 유형을 대표하는 다른 단어 또는 용어로 대체하도록 한다. 단, 관용적으로 자주 쓰이는 표현이면서 대체 단어 또는 용어 구성이 어렵다면 사용을 허용한다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 숫자 사용 시 아라비아 숫자 사용을 원칙으로 한다\n삼순위(X) → 3순위(O)\n\n\n2\n• 숫자만으로는 단어가 될 수 없고 해당 숫자의 의미를 나타내는 단어와 함께 조합하여 사용한다\n6(X) → 6개월(O)\n\n\n3\n• 숫자와 단위의 합성어는 단일어로 등록한다\n100퍼센트, 91일\n\n\n4\n• 단위 앞에 올 수 있는 숫자의 유효값이 제한적인 경우 유효한 단어를 모두 등록한다.\n1월,2월 ~ 12월1분기,2분기,3분기,4분기1순위,2순위,3순위\n\n\n5\n• 단위 앞에 올 수 있는 숫자의 유효값이나 범위가 제한이 없는 경우 해당 단위를 등록하고 최소 1단위와 합성된 단어를 등록한다\n퍼센트 - 1퍼센트개월 -1개월차 - 1차급 - 1급\n\n\n6\n• 숫자 단위 대에 따라 표준을 다르게 정의할 경우 각각을 단어로 등록할 수 있다\n1원,1십원,1천원,1만원,1십만원,1백만원,1천만원\n\n\n7\n• 숫자와 조합된 단어의 의미가 불분명한 경우는 해당 단어의 사용을 지양한다 단, 의미가 불분명한 단어임에도 업무상의 필요로 인해 등록이 불가피한 경우 단어 뒤에 숫자를 붙여서 정의한다\n컬럼1(X)※ 단 불가피한 경우 사용\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 동음이의어(Homonym)는 허용하지 않는다 - (대체방안1) 다른 한글단어로 교체하여 표준 단어로 등록하여 사용한다. - (대체방안2) 어감상 동음이의어인데도 불구하고 ’의사’라는 단오를 꼭 써야 하는 경우는 ’의사결정’식의 복합 단어를 표준단어로 등록하여 사용한다• 단, 대체 단어가 없는 경우 사용할 수 있다 (한글 명이 같더라도 영문 명은 반드시 달라야 한다) 이 경우 데이터 표준관리자 및 모델관리자에게 검토 요청을 신청 한다\n다리(leg) : 다리다리(bridge) : 교량의사(doctor) : 의사의사(idea)결정(Decision):의사결정\n\n\n2\n• 이음동의어(Synonym)는 용어의 혼돈과 용어생성 시 중복발생 가능성 때문에 가급적 사용하지 않는다.• 대표단어를 정하고, 그 대표 단어만을 사용하도록 하며 나머지는 금칙어로 등록하여 사용을 제한하도록 한다\n핸드폰(X) → 휴대폰(O)※ 핸드폰은 금치어로 등록\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n순번\n원칙 및 설명\n예시\n\n\n\n\n1\n• 이음동의어는 대표 단어만을 표준단어로 정의하여 사용하도록 하며, 나머지 이음 동의어의 단어들은 금칙어로 정의하여 사용을 사전에 방지한다.\n나이(X) → 연령(O)금일(X) → 당일(O)\n\n\n2\n• 축약된 형태의 단어(축약어)는 금칙어로 정의하여 사용을 사전에 방지한다.\n주민번호(X) → 주민등록번호(O)\n\n\n3\n• 사람에 해당하는 접미사 “자”와 “인”으로 구성된 복합어의 경우에는 사용빈도가 높은 단어를 표준단어로 정의하고, 다른 나머지 단어를 금칙어로 관리하여 사용을 사전에 방지한다\n장애자(X) → 장애인(O)신청인(X) → 신청자(O)\n\n\n4\n• 한글 맞춤법을 고려하지 않고 관용적으로 사용되던 단어들은 금칙어로 정의하여 사용을 사전에 방지한다.\n갯수(X) → 개수(O)써비스(X) → 서비스(O)\n\n\n5\n• 영문 단어의 경우 한글화를 원칙으로 한다. 단, 한글 단어 보다 더 친숙하게 사용되는 영문 단어의 경우에는 그 영문 단어를 사용한다. 이렇게 선정된 단어에 대응되는 영문 혹은 한글 단어는 금칙어로 등록하고 사용을 제한하도록 한다\nERROR(X) → 에러(O)FAX(X) → 팩스(O)EMAIL(X) → 이메일(O)아이피(X) → IP(O)\n\n\n6\n• 범용적으로 사용되는 외래어의 경우 사용할 수 있지만 표기법을 고려하여 표준 단어를 지정하고, 나머지 단어는 금칙어로 등록하여 사용을 제한하도록 한다\nEXPOSURE(X), 익스포저(X) → 익스포져(O),화일(X), FILE(X) →파일(O)\n\n\n7\n• 전문 업무용어 중 관용적으로 영문의 두문자어(頭文字語:Acronyms)를 사용하는 경우 한글단어는 금칙어로 정의한다.\n\n\n\n8\n• 기관명 등이나 고유명사 등은 전체 단어를 표준으로 사용하고, 한글 약어는 금칙어로 등록한다.\n\n\n\n9\n• 시스템명중 관용적으로 영문의 두문자어(頭文字語:Acronyms)를 사용하는 경우 한글단어는 금칙어로 정의한다\n데이터웨어하우스(X) → DW(O)\n\n\n면적 : Area → AREA (O), 역할 : Role → ROLE (O)"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-1.data_word_dictionary.html#제작-과정",
    "href": "docs/blog/posts/Governance/5-1.data_word_dictionary.html#제작-과정",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "표준 단어 사전의 목적과 범위를 명확히 한다.\n\n목적이라 함은 데이터 일관성 확보(동일한 의미의 데이터 사용), 데이터 품질 향상 (데이터 처리 과정 비용 감소), 시스템 통합 지원 (여러 시스템 간 데이터 매핑과 통합), 규제 준수 지원 등을 의미한다.\n범위는 대상 데이터 범위, 조직적 범위, 시스템 범위 등을 의미하며, 너무 광범위한 영역은 제작 실패로 이어진다.\n\n이해관계자 식별: 관련 부서와 담당자들을 파악한다.\n거버넌스 체계 수립: 단어 사전 관리를 위한 조직과 프로세스를 정립한다.\n\n\n\n\n\n기존 데이터 모델, 데이터베이스 스키마, 업무 문서 등에서 사용 중인 단어들을 수집한다.\n업무 도메인별로 사용되는 용어들을 취합한다.\n\n\n\n\n\n동의어, 유사어, 약어 등을 식별한다.\n업무 영역별 용어의 의미와 사용 맥락을 분석한다.\n불필요하거나 중복된 단어들을 제거한다.\n\n\n\n\n\n약어 사용 규칙, 대소문자 규칙 등의 명명 규칙을 정의합니다.\n단어 정의 형식을 standardization 합니다.\n도메인별 특수 규칙을 설정한다.\n\n유용한 데이터 표준안을 만들기 위해 조직 내의 각 업무 영역 또는 데이터 도메인에 맞는 고유한 규칙이나 지침을 만들어야 한다.\n도메인 식별: 재무, 인사, 마케팅, 제조, 고객 서비스 등\n각 도메인별 특수성 파악: 해당 도메인에서만 사용되는 용어나 개념과 도메인 특유의 데이터 형식이나 제약 조건\n예시\n\n재무 도메인의 통화 표기 규칙: “USD 1,000.00” 또는 “1,000,000 원” 형식 사용\n재무 도메인의 회계 기간 표현: “FY2023Q2” (2023 회계연도 2분기)\n인사 도메인의 직급 코드 체계: “M” (매니저), “D” (디렉터)\n인사 도메인의 근속 연수 계산 규칙: 입사일 기준, 월 단위 반올림\n마케팅 도메인의 캠페인 코드 형식: “CAM_2023_SUMMER_01”\n마케팅 도메인의 고객 세그먼트 분류 기준: “VIP”, “REGULAR”, “NEW”\n제조 도메인의 제품 코드 체계: “PROD-A01-R” (제품군-모델번호-버전)\n제조 도메인의 품질 등급 표기: “A”, “B”, “C” 등급 사용\n\n\n\n\n\n\n\n분석된 단어들 중 표준으로 사용할 단어들을 선정\n선정 기준을 명확히 하고, 이해관계자들의 합의를 도출\n선정 기준 예시\n\n명확성\n\n의미가 명확하고 모호하지 않은 단어 선정 (일반적인 단어는 한정시킬 것)\n예: “고객” 대신 “활성고객”과 “비활성고객”으로 구분\n\n일관성\n\n조직 전체에서 일관되게 사용할 수 있는 단어\n예: 부서별로 다르게 사용되던 용어를 하나로 통일\n\n간결성\n\n가능한 간단하고 간결한 단어\n예: “제품구매고객정보” 대신 “구매자정보”\n\n유일성\n\n중복되지 않는 고유한 의미를 가진 단어\n예: 동음이의어 피하기\n\n업계 표준 부합성\n\n가능한 업계에서 널리 사용되는 표준 용어 선택\n예: 금융업계의 “ROI” (Return on Investment)\n\n확장성\n\n향후 변화나 확장을 고려한 단어 선택\n예: “2023년예산” 대신 “연간예산”\n\n이해 용이성\n\n비전문가도 이해하기 쉬운 단어\n예: 전문 용어보다는 일반적인 비즈니스 용어 선호\n\n번역 가능성\n\n다국어 지원이 필요한 경우, 번역이 용이한 단어\n관용구나 은유적 표현 피하기\n\n기존 시스템 호환성\n\n기존 시스템과의 호환성을 고려한 단어\n예: 레거시 시스템의 주요 용어 유지\n\n법규 및 규제 준수\n\n관련 법규나 규제를 준수하는 단어\n예: 개인정보보호법에 부합하는 용어 선택\n\n도메인 적합성\n\n해당 업무 도메인에 적합한 단어\n예: 금융 도메인에서는 “이자율”, 제조 도메인에서는 “불량률”\n\n측정 가능성\n\n정량적 측정이 가능한 개념을 나타내는 단어\n예: “고객만족도” (1-5 척도로 측정 가능)\n\n약어 사용 규칙\n\n약어 사용 시 일관된 규칙 적용\n규칙에 부합하지 않는 관용어는 관용어 사용을 유지하는 것이 좋다.\n예: “고객번호”를 “CUST_NO”로 통일\n\n\n\n\n\n\n\n각 단어에 대한 상세 정보를 정의 (정의, 동의어, 사용 예시, 관련 업무 영역 등).\n단어 간의 관계를 정의 (상위어, 하위어, 관련어 등)\n\n\n\n\n\n선정된 표준 단어들에 대해 관련 부서와 전문가들의 검토를 받는다.\n필요시 수정하고 최종 승인을 받는다.\n\n\n\n\n\n승인된 단어들을 데이터베이스나 전문 도구에 등록한다. (엔코아, Microsoft Purview 등)\n검색, 조회, 관리가 용이한 형태로 구성한다.\n\n\n\n\n\n완성된 표준 단어 사전을 조직 내에 공유\n사용 방법과 중요성에 대한 교육을 실시\n\n\n\n\n\n새로운 단어 추가, 기존 단어 수정, 폐기 등의 프로세스를 수립한다.\n정기적인 검토와 업데이트를 수행한다.\n사용 현황을 모니터링하고 피드백을 수집한다.\n\n\n\n\n\n데이터 모델링, 시스템 개발, 보고서 작성 등의 프로세스와 표준 단어 사전을 연계한다\n\n연계 순서는 각 상황마다 다르다.\n이미 ERD와 DB가 존재하는 시스템들을 통합하는 상황이라면 표준 단어 사전을 순서상 나중에 제작하는 것이 유리하다\n그 반대라면 표준 단어사전을 먼저 만들어 DB를 만드는 것이 유리할 수 있다.\n\n다른 데이터 관리 도구들과의 통합을 고려한다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5-1.data_word_dictionary.html#표준-단어-사전-예시",
    "href": "docs/blog/posts/Governance/5-1.data_word_dictionary.html#표준-단어-사전-예시",
    "title": "Data Governance Study - Data Standard Word Dictionary",
    "section": "",
    "text": "예시 1\n\n\n\n\n표준 단어 사전 예시\n\n\n\n예시 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n단어 ID\n한글명\n한자\n영문명\n영문약어\n정의\n도메인\n데이터 타입\n길이\n허용값\n관련 업무 영역\n사용 예시\n동의어/유사어\n상위어\n하위어\n등록일\n최종 수정일\n승인 상태\n\n\n\n\nW001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW002\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW003\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n칼럼 설명:\n\n단어 ID: 각 단어의 고유 식별자\n한글명: 한글로 된 단어명\n한자: 해당 단어의 한자 표기 (필요한 경우)\n영문명: 영문으로 된 단어명\n영문약어: 영문 약어 (필요한 경우)\n정의: 단어의 명확한 정의\n도메인: 해당 단어가 속한 비즈니스 도메인\n데이터 타입: 해당 단어의 데이터 타입 (예: VARCHAR, INTEGER 등)\n길이: 데이터 길이\n허용값: 허용되는 값의 범위 또는 목록\n관련 업무 영역: 해당 단어가 주로 사용되는 업무 영역\n사용 예시: 실제 사용 예시\n동의어/유사어: 관련된 동의어나 유사어\n상위어: 해당 단어의 상위 개념 단어 (여러 개일 경우 쉼표로 구분)\n\n상위어: 이해관계자\n\n하위어: 해당 단어의 하위 개념 단어들 (여러 개일 경우 쉼표로 구분)\n\n하위어: 개인고객, 법인고객, VIP고객\n\n등록일: 단어가 사전에 처음 등록된 날짜\n최종 수정일: 마지막으로 수정된 날짜\n승인 상태: 현재 승인 상태 (예: 승인됨, 검토 중, 폐기 등)\n\n이 템플릿은 조직의 필요에 따라 수정하거나 필드를 추가/제거할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5-1.data_model_conceptual_ER_model copy.html",
    "href": "docs/blog/posts/Governance/4-5-1.data_model_conceptual_ER_model copy.html",
    "title": "Data Governance Study - Data Model (7)",
    "section": "",
    "text": "G\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n실 세계에 존재하는 의미있는 하나의 정보 단위\n표현: 사각형으로 표시\n물리적 개체 뿐 아니라 추상적(개념적) 개체도 포함\n\n물리적 개체: (학생, 자동차, 강의실, 등)\n추상적 개체 : (프로젝트, 직업, 교과목)\n\n개체는 둥근 직사각형으로 표시\n\n\n\n\n\n개체들 사이의 연관성\n\n학생과 교과목 사이의 수강 관계\n표현: 마름모로 표시, 선으로 관련 엔티티에 연결, ex) [Student] - &lt; Register &gt; -[Subject]\n\n실제로는 개체와 관계를 구분짓기 매우 힘듦 \\(\\rightarrow\\) ER modeling 할때 의미가 없어짐 (깊게 생각하지 말것)\n\nex) 결혼을 개체로 둘건지 관계로 둘건지 애매\n\n[결혼] - &lt;진행&gt; - [예식장] vs [남자] - &lt;결혼&gt; - &lt;여자&gt;\n\n\n관계는 마름모로 표시\n\n\n\n\n\n개체 또는 관계의 본질적 특성이나 성질\n그러므로 instance는 속성들의 값의 집합\n표현: 타원형으로 표시, 선으로 엔티티에 연결\n예시\n\n학생(개체)이 가지는 속성은 학번, 혈액형, 나이, 핸폰 번호, 성별, 학년 등이 있음\n과목(개체)이 가지는 속성은 학점(credit), 시간(hour), 부서(department), 장소(location) 등이 있음\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0--a1\n\n\n\n\na3\n\nage\n\n\n\na0--a3\n\n\n\n\na4\n\nstudent_id\n\n\n\na0--a4\n\n\n\n\na5\n\nsex\n\n\n\na0--a5\n\n\n\n\na2\n\nSubject\n\n\n\na1--a2\n\n\n\n\na6\n\ncredit\n\n\n\na2--a6\n\n\n\n\na7\n\nhour\n\n\n\na2--a7\n\n\n\n\na8\n\ndepartment\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\n개체나 관계에서 파생되는 수많은 속성을 나열하고 명확하게 분리하는 것은 어려움, why?\n\n다음 개체 및 관계에서 주어진 속성의 주인(Owner)은?\n\n개체: [학생], [교과목]\n관계: &lt;수강&gt;\n속성: (성별), (나이), (과목), (학점), (평점), (이수구분)\n\n[학생]: (성별), (나이)\n\n(성별), (나이) 는 비교적 명확하게 [학생] 개체에 대응되는 속성이다\n\n[교과목] : (과목), (학점)\n\n(과목명), (학점) 는 비교적 명확하게 [교과목] 개체에 대응되는 속성이다\n\n개체 기준으로 (평점), (이수구분) 속성은 구분짓기 애매함\n\n[학생]이 (평점) 속성을 갖게 되면 학생 A에게 평점을 물어볼경우 대답을 할 수가 없음\n왜냐하면, 여러 과목에 대한 평점이 존재하기 때문에 어떤 교과목에 대한 평점을 얘기해야하는지 모름.\n즉, (평점)은 [학생]의 고유 속성이 아님.\n반대로, (평점) 속성의 주인이 [교과목] 개체라 가정할 경우, 교과목에 평점을 물어보면 학생이 몇 십명이기 때문에 어떤 학생의 평점을 얘기해야하는지 애매해짐.\n즉 (평점)은 [교과목]의 고유 속성이 아님\n\n\n(평점)과 (이수구분) 과 같은 애매한 속성은 관계로 구분 지으면 해결될 경우가 있음!\n\n관계: 개체 사이에 관계를 맺어주는 이벤트 또는 함수\n&lt;수강&gt;: 학생이 교과목을 수강한 이벤트\n\n(평점) : 학생 1명이 과목 1개를 수강하여 평점을 산출\n(이수구분): 학생 1명이 과목 1개를 수강하여 이수여부 산출\n\n&lt;수강&gt;: (평점), (이수구분)\n사실, (평점)과 (이수구분)과 같이 관계에 의하여 파생되는 속성은 해당 배경지식이 없는 외부인이라면 파악하기 매우 힘듦 (업무기술서가 명확히 적혀 있어야 해결 가능).\n\n\n\n\n\n\n속성의 유형을 여러 기준으로 분류할 수 있음\nMulti-valued (다중값 속성)\n\n하나의 엔티티 인스턴스가 여러 값을 가질 수 있는 속성\n표기: single valued (동그라미 1개로 표기) vs Multi-valued (동그라미 2개로 표기)\n예)\n\n(나이)는 한 시점에 여러 개의 값을 가질 수 없음\n(취미)는 한 시점에 여러 개의 값을 가질 수 있음\nCAR(자동차) ERD\n\ncolor가 multi-valued가 2개인 이유\n\n차 한대에 여러 색상이 들어갈 수 있기 때문에\n\n싱글 value가 multi value보다 훨 씬 많음\n\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nCar\n\n\n\na1\n\nyear\n\n\n\na0--a1\n\n\n\n\na2\n\nregistration\n\n\n\na0--a2\n\n\n\n\na3\n\nvehicle_id\n\n\n\na0--a3\n\n\n\n\na4\n\nmodel\n\n\n\na0--a4\n\n\n\n\na5\n\nmake\n\n\n\na0--a5\n\n\n\n\na6\n\n\ncolor\n\n\n\na0--a6\n\n\n\n\na7\n\nstate\n\n\n\na2--a7\n\n\n\n\na8\n\nnumber\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\ncomposite attribute(복합 속성)\n\nsimple attribute : 더이상 쪼개지지 않는 원자값을 갖는 속성\n\nex) 나이, 학번\n\ncomposite attribute\n\n여러 하위 속성으로 구성된 속성\n즉, 몇 개의 요소로 분해 될 수 있는 속성, 쪼개어 져도 의미를 갖을 수 있어야함\nex) 주소: 시 + 군 + 구 + 번지\n\nsimple 와 composite attribute를 구분짓는 기준은 나라의 사회 제도나 단체의 시스템의 특성에 따라 변한다\n\nex) 한국은 우편 수집 창고와 우체국의 시스템에 따라 주소를 시 + 군 + 구 + 번지 또는 시 + 군 + 구 가 필요로 할 수 있다\n하지만 다른 나라는 주소 전체를 쓰는 시스템이라면 주소 속성이 simple attribute 로 남을 수 있다.\n이름의 경우 성과 이름을 가르는 시스템이 필요한 미국과 full name을 사용하는 한국의 시스템의 경우 각 상황에 맞게 modeling을 해야한다.\n어떤 속성이든 분해해야할 용도가 있다면 쪼개야한다.\n\n위의 그림 예시에서\n\ncomposite attribute: registration (자동차 번호판)\nsimple attribute: state(주이름) & number(자동차 고유번호)\n\n\nderived vs stored attributes\n\nderived attribute (점선으로 표기) : 저장된 다른 데이터로부터 유도 가능한 속성\n\n총 학생 수: 그냥 instance나 학생 수를 카운트 하면 됨, 총 학생 수라는 속성은 없어도 됨\n각 과목의 성적 : 총점(derived), 평점(derived)\n주민등록번호: 나이(derived), 생일(derived)\nderived attribute는 자주 쓰이는 통계치를 구할 때 자주 쓰임. DB에 저장할 필요는 없다.\n\nstored attribute: 위에서 말한 총 학생 수, 총점, 평점, 나이, 생일과 같은 derived attribute의 연산이 너무 무겁거나 너무 빈번하게 사용되는 상황이라면 DB에 data를 적재하여 연산량을 줄이는 방법 도 있다.\n\n설계자의 재량에 따라 stored와 derived의 구분 짓는다.\n실무에서는 파생 변수(derived)가 자주 쓰인다면 stored (실선)로 남긴다\n학계에서는 derived는 가급적 점선으로 표시하여 derived상태로 남긴다\n\n\nKey Attributes\n\nkey attributes: 유일성 + 최소성 을 만족시켜야함\n\n어떤 개체에 대해서 그 인스턴스가 항상 유일한 값을 갖는 속성 또는 속성들의 집합\n\n중복되는 값을 가지면 안됨\n키 속성은 밑줄을 그어 표시\nex) 학생의 학번, 책의 ISBN, 차량번호\n\n\n특정 스냅샷이 아닌 해당 개체의 모든 가능한 스냅샷의 집합을 고려하여 파악되어야함 (개체가 아무리 많아 지더라도 항상 유일한 값을 가져야함)\n\nex) 다음의 ssn, 이름, 혈액형 중 키 속성은 ssn\n\ncomposite key(복합키)\n\nentity에서 키 속성자체가 없을 경우 attributes의 조합으로도 생성가능\n복합키는 최소성을 가져야함 : 최소한의 attributes로 복합키를 만들어야함\n\n용어 정리\n\nconceptual design: key는 identifier라고 부르고\nlogical design: key는 primary key라고 부름.\n\ntable을 만들게 되면서 primary key라 부름\n\n하지만 identifier ≠ primary key.\n보통 identifier로 primary key를 만듦\n\nprimary key 한 table에 반드시 1개만 있어야함. 없어도 안됨. primary key없으면 DB가 아님\n\nEntity Types\n\n강성 개체(strong entity)\n\n각 개체는 하나 이상의 key 속성을 가질 수 있음\n대부분의 개체는 key를 갖기 때문에 강성 개체라 부르는 경우는 별로 없다. 그냥 개체라 부름\n\n약성 개체 (weak entity)\n\n어떤 개체는 key를 갖지 않을 수 있음\n자체적으로 식별될 수 없고, 다른 엔티티에 의존하는 엔티티\n\n\n\n\n\n\n\n\n관계에 참여하는 엔티티 인스턴스의 수\n표현: 1:1, 1:N, M:N 등으로 표시\nex) 한 학생은 여러 강의를 수강할 수 있음 (1:N)\n\n\n\n\n\n엔티티의 관계 참여 여부\n필수 참여 (전체 참여): 이중선으로 표시\n선택 참여 (부분 참여): 단일선으로 표시\n\n\n\n\n\n상위 엔티티와 하위 엔티티 간의 관계\n표현: 삼각형으로 연결\nex) ‘사람’ 엔티티의 특화로 ‘학생’과 ’교수’ 엔티티"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5-1.data_model_conceptual_ER_model copy.html#er-model-concept-및-구성-요소",
    "href": "docs/blog/posts/Governance/4-5-1.data_model_conceptual_ER_model copy.html#er-model-concept-및-구성-요소",
    "title": "Data Governance Study - Data Model (7)",
    "section": "",
    "text": "실 세계에 존재하는 의미있는 하나의 정보 단위\n표현: 사각형으로 표시\n물리적 개체 뿐 아니라 추상적(개념적) 개체도 포함\n\n물리적 개체: (학생, 자동차, 강의실, 등)\n추상적 개체 : (프로젝트, 직업, 교과목)\n\n개체는 둥근 직사각형으로 표시\n\n\n\n\n\n개체들 사이의 연관성\n\n학생과 교과목 사이의 수강 관계\n표현: 마름모로 표시, 선으로 관련 엔티티에 연결, ex) [Student] - &lt; Register &gt; -[Subject]\n\n실제로는 개체와 관계를 구분짓기 매우 힘듦 \\(\\rightarrow\\) ER modeling 할때 의미가 없어짐 (깊게 생각하지 말것)\n\nex) 결혼을 개체로 둘건지 관계로 둘건지 애매\n\n[결혼] - &lt;진행&gt; - [예식장] vs [남자] - &lt;결혼&gt; - &lt;여자&gt;\n\n\n관계는 마름모로 표시\n\n\n\n\n\n개체 또는 관계의 본질적 특성이나 성질\n그러므로 instance는 속성들의 값의 집합\n표현: 타원형으로 표시, 선으로 엔티티에 연결\n예시\n\n학생(개체)이 가지는 속성은 학번, 혈액형, 나이, 핸폰 번호, 성별, 학년 등이 있음\n과목(개체)이 가지는 속성은 학점(credit), 시간(hour), 부서(department), 장소(location) 등이 있음\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0--a1\n\n\n\n\na3\n\nage\n\n\n\na0--a3\n\n\n\n\na4\n\nstudent_id\n\n\n\na0--a4\n\n\n\n\na5\n\nsex\n\n\n\na0--a5\n\n\n\n\na2\n\nSubject\n\n\n\na1--a2\n\n\n\n\na6\n\ncredit\n\n\n\na2--a6\n\n\n\n\na7\n\nhour\n\n\n\na2--a7\n\n\n\n\na8\n\ndepartment\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\n개체나 관계에서 파생되는 수많은 속성을 나열하고 명확하게 분리하는 것은 어려움, why?\n\n다음 개체 및 관계에서 주어진 속성의 주인(Owner)은?\n\n개체: [학생], [교과목]\n관계: &lt;수강&gt;\n속성: (성별), (나이), (과목), (학점), (평점), (이수구분)\n\n[학생]: (성별), (나이)\n\n(성별), (나이) 는 비교적 명확하게 [학생] 개체에 대응되는 속성이다\n\n[교과목] : (과목), (학점)\n\n(과목명), (학점) 는 비교적 명확하게 [교과목] 개체에 대응되는 속성이다\n\n개체 기준으로 (평점), (이수구분) 속성은 구분짓기 애매함\n\n[학생]이 (평점) 속성을 갖게 되면 학생 A에게 평점을 물어볼경우 대답을 할 수가 없음\n왜냐하면, 여러 과목에 대한 평점이 존재하기 때문에 어떤 교과목에 대한 평점을 얘기해야하는지 모름.\n즉, (평점)은 [학생]의 고유 속성이 아님.\n반대로, (평점) 속성의 주인이 [교과목] 개체라 가정할 경우, 교과목에 평점을 물어보면 학생이 몇 십명이기 때문에 어떤 학생의 평점을 얘기해야하는지 애매해짐.\n즉 (평점)은 [교과목]의 고유 속성이 아님\n\n\n(평점)과 (이수구분) 과 같은 애매한 속성은 관계로 구분 지으면 해결될 경우가 있음!\n\n관계: 개체 사이에 관계를 맺어주는 이벤트 또는 함수\n&lt;수강&gt;: 학생이 교과목을 수강한 이벤트\n\n(평점) : 학생 1명이 과목 1개를 수강하여 평점을 산출\n(이수구분): 학생 1명이 과목 1개를 수강하여 이수여부 산출\n\n&lt;수강&gt;: (평점), (이수구분)\n사실, (평점)과 (이수구분)과 같이 관계에 의하여 파생되는 속성은 해당 배경지식이 없는 외부인이라면 파악하기 매우 힘듦 (업무기술서가 명확히 적혀 있어야 해결 가능).\n\n\n\n\n\n\n속성의 유형을 여러 기준으로 분류할 수 있음\nMulti-valued (다중값 속성)\n\n하나의 엔티티 인스턴스가 여러 값을 가질 수 있는 속성\n표기: single valued (동그라미 1개로 표기) vs Multi-valued (동그라미 2개로 표기)\n예)\n\n(나이)는 한 시점에 여러 개의 값을 가질 수 없음\n(취미)는 한 시점에 여러 개의 값을 가질 수 있음\nCAR(자동차) ERD\n\ncolor가 multi-valued가 2개인 이유\n\n차 한대에 여러 색상이 들어갈 수 있기 때문에\n\n싱글 value가 multi value보다 훨 씬 많음\n\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nCar\n\n\n\na1\n\nyear\n\n\n\na0--a1\n\n\n\n\na2\n\nregistration\n\n\n\na0--a2\n\n\n\n\na3\n\nvehicle_id\n\n\n\na0--a3\n\n\n\n\na4\n\nmodel\n\n\n\na0--a4\n\n\n\n\na5\n\nmake\n\n\n\na0--a5\n\n\n\n\na6\n\n\ncolor\n\n\n\na0--a6\n\n\n\n\na7\n\nstate\n\n\n\na2--a7\n\n\n\n\na8\n\nnumber\n\n\n\na2--a8\n\n\n\n\n\n\n\n\n\n\ncomposite attribute(복합 속성)\n\nsimple attribute : 더이상 쪼개지지 않는 원자값을 갖는 속성\n\nex) 나이, 학번\n\ncomposite attribute\n\n여러 하위 속성으로 구성된 속성\n즉, 몇 개의 요소로 분해 될 수 있는 속성, 쪼개어 져도 의미를 갖을 수 있어야함\nex) 주소: 시 + 군 + 구 + 번지\n\nsimple 와 composite attribute를 구분짓는 기준은 나라의 사회 제도나 단체의 시스템의 특성에 따라 변한다\n\nex) 한국은 우편 수집 창고와 우체국의 시스템에 따라 주소를 시 + 군 + 구 + 번지 또는 시 + 군 + 구 가 필요로 할 수 있다\n하지만 다른 나라는 주소 전체를 쓰는 시스템이라면 주소 속성이 simple attribute 로 남을 수 있다.\n이름의 경우 성과 이름을 가르는 시스템이 필요한 미국과 full name을 사용하는 한국의 시스템의 경우 각 상황에 맞게 modeling을 해야한다.\n어떤 속성이든 분해해야할 용도가 있다면 쪼개야한다.\n\n위의 그림 예시에서\n\ncomposite attribute: registration (자동차 번호판)\nsimple attribute: state(주이름) & number(자동차 고유번호)\n\n\nderived vs stored attributes\n\nderived attribute (점선으로 표기) : 저장된 다른 데이터로부터 유도 가능한 속성\n\n총 학생 수: 그냥 instance나 학생 수를 카운트 하면 됨, 총 학생 수라는 속성은 없어도 됨\n각 과목의 성적 : 총점(derived), 평점(derived)\n주민등록번호: 나이(derived), 생일(derived)\nderived attribute는 자주 쓰이는 통계치를 구할 때 자주 쓰임. DB에 저장할 필요는 없다.\n\nstored attribute: 위에서 말한 총 학생 수, 총점, 평점, 나이, 생일과 같은 derived attribute의 연산이 너무 무겁거나 너무 빈번하게 사용되는 상황이라면 DB에 data를 적재하여 연산량을 줄이는 방법 도 있다.\n\n설계자의 재량에 따라 stored와 derived의 구분 짓는다.\n실무에서는 파생 변수(derived)가 자주 쓰인다면 stored (실선)로 남긴다\n학계에서는 derived는 가급적 점선으로 표시하여 derived상태로 남긴다\n\n\nKey Attributes\n\nkey attributes: 유일성 + 최소성 을 만족시켜야함\n\n어떤 개체에 대해서 그 인스턴스가 항상 유일한 값을 갖는 속성 또는 속성들의 집합\n\n중복되는 값을 가지면 안됨\n키 속성은 밑줄을 그어 표시\nex) 학생의 학번, 책의 ISBN, 차량번호\n\n\n특정 스냅샷이 아닌 해당 개체의 모든 가능한 스냅샷의 집합을 고려하여 파악되어야함 (개체가 아무리 많아 지더라도 항상 유일한 값을 가져야함)\n\nex) 다음의 ssn, 이름, 혈액형 중 키 속성은 ssn\n\ncomposite key(복합키)\n\nentity에서 키 속성자체가 없을 경우 attributes의 조합으로도 생성가능\n복합키는 최소성을 가져야함 : 최소한의 attributes로 복합키를 만들어야함\n\n용어 정리\n\nconceptual design: key는 identifier라고 부르고\nlogical design: key는 primary key라고 부름.\n\ntable을 만들게 되면서 primary key라 부름\n\n하지만 identifier ≠ primary key.\n보통 identifier로 primary key를 만듦\n\nprimary key 한 table에 반드시 1개만 있어야함. 없어도 안됨. primary key없으면 DB가 아님\n\nEntity Types\n\n강성 개체(strong entity)\n\n각 개체는 하나 이상의 key 속성을 가질 수 있음\n대부분의 개체는 key를 갖기 때문에 강성 개체라 부르는 경우는 별로 없다. 그냥 개체라 부름\n\n약성 개체 (weak entity)\n\n어떤 개체는 key를 갖지 않을 수 있음\n자체적으로 식별될 수 없고, 다른 엔티티에 의존하는 엔티티\n\n\n\n\n\n\n\n\n관계에 참여하는 엔티티 인스턴스의 수\n표현: 1:1, 1:N, M:N 등으로 표시\nex) 한 학생은 여러 강의를 수강할 수 있음 (1:N)\n\n\n\n\n\n엔티티의 관계 참여 여부\n필수 참여 (전체 참여): 이중선으로 표시\n선택 참여 (부분 참여): 단일선으로 표시\n\n\n\n\n\n상위 엔티티와 하위 엔티티 간의 관계\n표현: 삼각형으로 연결\nex) ‘사람’ 엔티티의 특화로 ‘학생’과 ’교수’ 엔티티"
  },
  {
    "objectID": "docs/blog/posts/Governance/2.task_process.html",
    "href": "docs/blog/posts/Governance/2.task_process.html",
    "title": "Data Governance Study - Task Process",
    "section": "",
    "text": "DAMA-DMBOK\n\n데이터 관리 지식 체계 개요\n11개 지식 영역 이해\n\nIBM Data Governance Council Maturity Model\n\n11개 범주 및 5단계 성숙도 모델 이해\n\n기타 프레임워크\n\nCOBIT (Control Objectives for Information and Related Technologies)\nISO/IEC 38500 IT Governance Standard\n\n\n\n\n\n\n준비 및 계획\n\n현황 분석 및 요구사항 정의\n이해관계자 식별 및 참여\n비전 및 목표 설정\n\n조직 구성\n\n데이터 거버넌스 위원회 구성\n데이터 스튜어드 지정\n역할 및 책임 정의\n\n정책 및 표준 수립\n\n데이터 품질 정책\n데이터 보안 및 프라이버시 정책\n데이터 아키텍처 표준\n\n프로세스 설계 및 구현\n\n데이터 생명주기 관리 프로세스\n메타데이터 관리 프로세스\n데이터 품질 관리 프로세스\n\n기술 도입\n\n데이터 카탈로그 도구\n데이터 품질 관리 도구\n메타데이터 관리 도구\n데이터 거버넌스를 위한 기술지원 솔루션\n\nSnowflake\nDatabricks\nPurview\nEncore\n\n\n모니터링 및 개선\n\n성과 지표(KPI) 설정 및 측정\n지속적인 개선 활동\n\n\n\n\n\n\n데이터 품질 관리\n\n데이터 프로파일링\n데이터 클렌징\n데이터 품질 모니터링\n\n메타데이터 관리\n\n비즈니스 메타데이터\n기술 메타데이터\n운영 메타데이터\n\n마스터 데이터 관리 (MDM)\n\n마스터 데이터 식별\n마스터 데이터 통합 및 동기화\n\n데이터 보안 및 프라이버시\n\n데이터 분류\n접근 제어\n데이터 암호화\n\n데이터 아키텍처 관리\n\n데이터 모델링\n데이터 플로우 관리\n\n\n\n\n\n\n데이터 사전: 한국에만 있는 개념으로 형태소 분석의 결과이다 (자세한 사항은 다음 블로그에…)\n데이터 용어 사전: 한국에만 있는 개념으로 데이터 사전을 기반으로 제작된다. (자세한 사항은 다음 블로그에…)\n데이터 카탈로그 도구: 데이터 사전과 데이터 용어 사전을 기반으로 만들어져야한다.\n\nCollibra\nAlation\nIBM Watson Knowledge Catalog\n\n데이터 품질 관리 도구\n\nInformatica Data Quality\nTalend Data Quality\nIBM InfoSphere Information Server for Data Quality\n\n메타데이터 관리 도구\n\nASG Enterprise Data Intelligence\nAdaptive Metadata Manager\nerwin Data Intelligence\n\n\n\n\n\n\n데이터 중심 문화 구축\n\n데이터 리터러시 향상\n데이터 윤리 교육\n\n조직 변화 관리\n\n커뮤니케이션 전략\n교육 및 훈련 프로그램\n\n\n\n\n\n\n주요 데이터 관련 규제\n\nGDPR (General Data Protection Regulation)\nCCPA (California Consumer Privacy Act)\n국내 개인정보보호법\n\n컴플라이언스 관리\n\n규제 요구사항 매핑\n컴플라이언스 모니터링 및 보고\n\n\n\n\n\n\n핵심 성과 지표 (KPI)\n\n데이터 품질 개선율\n데이터 관련 의사결정 시간 단축\n데이터 보안 사고 감소율\n\nROI 분석\n\n비용 절감 효과\n수익 증대 효과\n리스크 감소 효과\n\n\n\n\n\n\nAI/ML을 활용한 데이터 거버넌스\n클라우드 환경에서의 데이터 거버넌스\n데이터 윤리 및 책임 있는 AI"
  },
  {
    "objectID": "docs/blog/posts/Governance/2.task_process.html#데이터-거버넌스-프레임워크-및-업무-절차",
    "href": "docs/blog/posts/Governance/2.task_process.html#데이터-거버넌스-프레임워크-및-업무-절차",
    "title": "Data Governance Study - Task Process",
    "section": "",
    "text": "DAMA-DMBOK\n\n데이터 관리 지식 체계 개요\n11개 지식 영역 이해\n\nIBM Data Governance Council Maturity Model\n\n11개 범주 및 5단계 성숙도 모델 이해\n\n기타 프레임워크\n\nCOBIT (Control Objectives for Information and Related Technologies)\nISO/IEC 38500 IT Governance Standard\n\n\n\n\n\n\n준비 및 계획\n\n현황 분석 및 요구사항 정의\n이해관계자 식별 및 참여\n비전 및 목표 설정\n\n조직 구성\n\n데이터 거버넌스 위원회 구성\n데이터 스튜어드 지정\n역할 및 책임 정의\n\n정책 및 표준 수립\n\n데이터 품질 정책\n데이터 보안 및 프라이버시 정책\n데이터 아키텍처 표준\n\n프로세스 설계 및 구현\n\n데이터 생명주기 관리 프로세스\n메타데이터 관리 프로세스\n데이터 품질 관리 프로세스\n\n기술 도입\n\n데이터 카탈로그 도구\n데이터 품질 관리 도구\n메타데이터 관리 도구\n데이터 거버넌스를 위한 기술지원 솔루션\n\nSnowflake\nDatabricks\nPurview\nEncore\n\n\n모니터링 및 개선\n\n성과 지표(KPI) 설정 및 측정\n지속적인 개선 활동\n\n\n\n\n\n\n데이터 품질 관리\n\n데이터 프로파일링\n데이터 클렌징\n데이터 품질 모니터링\n\n메타데이터 관리\n\n비즈니스 메타데이터\n기술 메타데이터\n운영 메타데이터\n\n마스터 데이터 관리 (MDM)\n\n마스터 데이터 식별\n마스터 데이터 통합 및 동기화\n\n데이터 보안 및 프라이버시\n\n데이터 분류\n접근 제어\n데이터 암호화\n\n데이터 아키텍처 관리\n\n데이터 모델링\n데이터 플로우 관리\n\n\n\n\n\n\n데이터 사전: 한국에만 있는 개념으로 형태소 분석의 결과이다 (자세한 사항은 다음 블로그에…)\n데이터 용어 사전: 한국에만 있는 개념으로 데이터 사전을 기반으로 제작된다. (자세한 사항은 다음 블로그에…)\n데이터 카탈로그 도구: 데이터 사전과 데이터 용어 사전을 기반으로 만들어져야한다.\n\nCollibra\nAlation\nIBM Watson Knowledge Catalog\n\n데이터 품질 관리 도구\n\nInformatica Data Quality\nTalend Data Quality\nIBM InfoSphere Information Server for Data Quality\n\n메타데이터 관리 도구\n\nASG Enterprise Data Intelligence\nAdaptive Metadata Manager\nerwin Data Intelligence\n\n\n\n\n\n\n데이터 중심 문화 구축\n\n데이터 리터러시 향상\n데이터 윤리 교육\n\n조직 변화 관리\n\n커뮤니케이션 전략\n교육 및 훈련 프로그램\n\n\n\n\n\n\n주요 데이터 관련 규제\n\nGDPR (General Data Protection Regulation)\nCCPA (California Consumer Privacy Act)\n국내 개인정보보호법\n\n컴플라이언스 관리\n\n규제 요구사항 매핑\n컴플라이언스 모니터링 및 보고\n\n\n\n\n\n\n핵심 성과 지표 (KPI)\n\n데이터 품질 개선율\n데이터 관련 의사결정 시간 단축\n데이터 보안 사고 감소율\n\nROI 분석\n\n비용 절감 효과\n수익 증대 효과\n리스크 감소 효과\n\n\n\n\n\n\nAI/ML을 활용한 데이터 거버넌스\n클라우드 환경에서의 데이터 거버넌스\n데이터 윤리 및 책임 있는 AI"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5-0..data_model_conceptual.html",
    "href": "docs/blog/posts/Governance/4-5-0..data_model_conceptual.html",
    "title": "Data Governance Study - Data Model (6)",
    "section": "",
    "text": "DB 설계는 각 설계자마다 다르게 설계하므로 science가 아니라 art라고 부른다. 후에 게시되는 블로그를 읽어보면 주관적인 판단이 들어가는 요소가 많이 있음을 파악할 수 있다.\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling(개념적 데이터 모델링)은 데이터베이스 설계의 초기 단계\n비즈니스 요구사항 (업무기술서의 내용)을 이해하기 좋게 개념화 또는 추상화 (i.e. 도식화)하여 high level (고수준)의 데이터 구조를 정의하는 과정이다.\n\nhigh level은 사람이 이해할 수 있는 수준이란 뜻이며 low level은 컴퓨터가 이해할 수 있는 수준이라고 생각하면 편하다.\n\n\n\n\n\n\n\n\n\nG\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0-&gt;a1\n\n\n\n\na2\n\nSubject\n\n\n\na1-&gt;a2\n\n\n\n\na3\n\nLecture\n\n\n\na2-&gt;a3\n\n\n\n\na4\n\nProfessor\n\n\n\na3-&gt;a4\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling은 복잡한 비즈니스 요구사항을 단순화하여 표현하므로, 프로젝트의 초기 단계에서 이해관계자 간의 합의를 도출하는 데 매우 유용하다.\n후속 데이터베이스 설계 단계의 기반이 되어 전체 프로젝트의 성공에 중요한 역할을 한다.\n\n\n\n\n비즈니스 관점에서 데이터 구조를 이해하고 표현\n주요 엔티티와 그들 간의 관계를 식별\n시스템의 범위를 정의\n\n\n\n\n\n엔티티(Entity): 비즈니스에서 중요한 객체나 개념\n속성(Attribute): 엔티티의 특성 (이 단계에서는 상세하게 다루지 않을 수 있음)\n관계(Relationship): 엔티티 간의 연관성\n\nRelationship (관계) \\(\\ne\\) Relation\nRelationship 은 [Student] - &lt; Register &gt; - [Subject] 에서 &lt; Register &gt;에 해당, 한글로 관계로 표시\n도식에서 마름모에 해당\nRelation = Table (테이블), 보통 relation을 한글로 표현할 때 릴레이션 으로 표시 (관계라고 표시안함)\n\n수학자들은 테이블의 한 행을 relation 이라 부름\n즉, 관계형 데이터 모델의 수학적, 논리적 개념이고 튜플(tuple)의 집합이며 속성(attribute)의 집합으로 구성된다.\n테이블의 추상적인 개념으로 테이블은 Relation을 기술하는 하나의 구체적 표현\nDB에서 하나의 table로 구현된다.\n\n\n\n\n\n\n\n기술적 세부사항을 배제하고 비즈니스 개념에 집중\n높은 수준의 추상화\nDBMS에 독립적\n\n\n\n\n\n간단한 텍스트 설명이나 다이어그램\n주로 Entity-Relationship Diagram (ERD)을 사용\n\n개념적 설계에 가장 많이 쓰는 모델로서 Entity-Relationship Model (ERM or ER-model 개체 관계 모형)을 사용하고 그 산출물이 ERD이다.\n\n\n\n\n\n\n주요 비즈니스 개체 식별\n개체 간 관계 정의\n높은 수준의 속성 식별 (선택적)\n비즈니스 규칙 반영\n논리적 데이터 모델링의 기초가 됨 (더 상세한 데이터 모델로 발전)"
  },
  {
    "objectID": "docs/blog/posts/Governance/4-5-0..data_model_conceptual.html#db-design",
    "href": "docs/blog/posts/Governance/4-5-0..data_model_conceptual.html#db-design",
    "title": "Data Governance Study - Data Model (6)",
    "section": "",
    "text": "DB 설계는 각 설계자마다 다르게 설계하므로 science가 아니라 art라고 부른다. 후에 게시되는 블로그를 읽어보면 주관적인 판단이 들어가는 요소가 많이 있음을 파악할 수 있다.\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nDB Design(Data Modeling)\n\n\ncluster_1\n\nConceptual Design\n(Data Modeling)\n\n\ncluster_3\n\nPhysical Design\n(Data Modeling)\n\n\ncluster_2\n\nLogical Design\n(Data Modeling)\n\n\n\na0\n\nReal World\n\n\n\na1\n\nMini-world\n(Reduced Scope)\n\n\n\na0-&gt;a1\n\n\n\n\n\na2\n\nRequirements\nCollection & Analysis\n\n\n\na1-&gt;a2\n\n\n\n\n\na3\n\nDocumenting\nData Requirements\n\n\n\na2-&gt;a3\n\n\n\n\n\na4\n\nConceptual Schema\n(High Level Data Model)\n\n\n\na3-&gt;a4\n\n\n\n\n\na5\n\nEntity-Relational Model\n(Output:ERD)\n\n\n\na4-&gt;a5\n\n\n\n\n\na6\n\nLogical Schema\n(Middle Level Data Model)\n\n\n\na5-&gt;a6\n\n\n\n\n\na7\n\nRelational Model\n(Output:ERD)\n(Specific DBMS)\n\n\n\na6-&gt;a7\n\n\n\n\n\na8\n\nPhysical Schema\n(Low Level Data Model)\n\n\n\na7-&gt;a8\n\n\n\n\n\na9\n\nEntity-Relational Model\nOutput:ERD\n\n\n\na8-&gt;a9\n\n\n\n\n\na10\n\nLogical\nDesign(Data Modeling)\n\n\n\na9-&gt;a10\n\n\n\n\n\na11\n\nInternal Schema\n\n\n\na10-&gt;a11\n\n\n\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling(개념적 데이터 모델링)은 데이터베이스 설계의 초기 단계\n비즈니스 요구사항 (업무기술서의 내용)을 이해하기 좋게 개념화 또는 추상화 (i.e. 도식화)하여 high level (고수준)의 데이터 구조를 정의하는 과정이다.\n\nhigh level은 사람이 이해할 수 있는 수준이란 뜻이며 low level은 컴퓨터가 이해할 수 있는 수준이라고 생각하면 편하다.\n\n\n\n\n\n\n\n\n\nG\n\n\n\na0\n\nStudent\n\n\n\na1\n\nRegister\n\n\n\na0-&gt;a1\n\n\n\n\na2\n\nSubject\n\n\n\na1-&gt;a2\n\n\n\n\na3\n\nLecture\n\n\n\na2-&gt;a3\n\n\n\n\na4\n\nProfessor\n\n\n\na3-&gt;a4\n\n\n\n\n\n\n\n\n\n\nConceptual data modeling은 복잡한 비즈니스 요구사항을 단순화하여 표현하므로, 프로젝트의 초기 단계에서 이해관계자 간의 합의를 도출하는 데 매우 유용하다.\n후속 데이터베이스 설계 단계의 기반이 되어 전체 프로젝트의 성공에 중요한 역할을 한다.\n\n\n\n\n비즈니스 관점에서 데이터 구조를 이해하고 표현\n주요 엔티티와 그들 간의 관계를 식별\n시스템의 범위를 정의\n\n\n\n\n\n엔티티(Entity): 비즈니스에서 중요한 객체나 개념\n속성(Attribute): 엔티티의 특성 (이 단계에서는 상세하게 다루지 않을 수 있음)\n관계(Relationship): 엔티티 간의 연관성\n\nRelationship (관계) \\(\\ne\\) Relation\nRelationship 은 [Student] - &lt; Register &gt; - [Subject] 에서 &lt; Register &gt;에 해당, 한글로 관계로 표시\n도식에서 마름모에 해당\nRelation = Table (테이블), 보통 relation을 한글로 표현할 때 릴레이션 으로 표시 (관계라고 표시안함)\n\n수학자들은 테이블의 한 행을 relation 이라 부름\n즉, 관계형 데이터 모델의 수학적, 논리적 개념이고 튜플(tuple)의 집합이며 속성(attribute)의 집합으로 구성된다.\n테이블의 추상적인 개념으로 테이블은 Relation을 기술하는 하나의 구체적 표현\nDB에서 하나의 table로 구현된다.\n\n\n\n\n\n\n\n기술적 세부사항을 배제하고 비즈니스 개념에 집중\n높은 수준의 추상화\nDBMS에 독립적\n\n\n\n\n\n간단한 텍스트 설명이나 다이어그램\n주로 Entity-Relationship Diagram (ERD)을 사용\n\n개념적 설계에 가장 많이 쓰는 모델로서 Entity-Relationship Model (ERM or ER-model 개체 관계 모형)을 사용하고 그 산출물이 ERD이다.\n\n\n\n\n\n\n주요 비즈니스 개체 식별\n개체 간 관계 정의\n높은 수준의 속성 식별 (선택적)\n비즈니스 규칙 반영\n논리적 데이터 모델링의 기초가 됨 (더 상세한 데이터 모델로 발전)"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-0.data_standard.html",
    "href": "docs/blog/posts/Governance/5-0.data_standard.html",
    "title": "Data Governance Study - Data Standard Governance",
    "section": "",
    "text": "각 데이터 항목에 대한 명명 규칙을 정의\n표준단어 및 표준용어, 표준도메인, 표준코드 등을 전사관리기준 형태로 정의\n이러한 표준들을 변경 및 관리하는 활동 수행\n\n\n\n\n\n데이터 표준관리는 데이터 항목들에 대해서 전사적으로 일관된 명명과 정의를 부여하고 이를 정제, 개선하는 활동이다.\n\n\n\n\n데이터 표준관리에서 중점적으로 다루는 정제 및 개선 사항은 다음과 같다:\n\n\n\n대상\n정제 및 개선 결과물\n\n\n\n\n명칭 + 정의\n단어 및 용어\n\n\n형식\n도메인\n\n\n규칙\n코드\n\n\n\n이러한 표준화 활동을 통해 조직 전체에서 일관된 데이터 관리가 가능해진다.\n\n\n\n\n\n\n각 용어에 대한 부가 정보(예: 데이터 타입, 길이, 허용 값 등)를 관리할 수 있다.\n데이터의 의미와 구조에 대한 종합적인 이해를 제공한다.\n\n\n\n\n\n비즈니스 사용자와 IT 전문가 사이의 의사소통을 원활하게 한다\n동일한 데이터에 대해서는 동일한 명칭을 사용함으로써 개발자-현업, 운영자-현업, 운영자-운영자 등 다양한 계층간에 명확하고 신속한 의사소통이 가능\n편의나 관습에 따라 동일한 의미로 사용되는 이음동의어를 대표성을 지닌 한 개의 단어로 정의함으로써 의사소통과 데이터에 대한 인식의 오류를 방지할 수 있다\n용어의 오해로 인한 프로젝트 지연이나 오류를 방지한다.\n\n\n\n\n\n데이터 소재 파악의 시간 및 노력 감소\n즉, 정보 사용자는 데이터의 의미와 데이터의 위치 등을 신속하게 파악할 수 있어 적시에 정확한 정보를 활용\n\n\n\n\n\n데이터 관련 규제 요구사항을 충족하는 데 도움을 준다.\n\n데이터 관련 규제 요구사항: 데이터의 수집, 저장, 처리, 공유, 및 폐기와 관련된 법적, 규범적 기준을 의미\n데이터 보안, 개인정보 보호, 데이터 품질, 및 데이터 관리의 투명성을 보장하기 위한 규제\n\n데이터의 의미와 사용에 대한 명확한 문서화를 제공한다.\n데이터 규제 종류\n\nGDPR (General Data Protection Regulation)\n\nEU의 개인정보 보호법\n예시: 개인 데이터의 수집 및 처리에 대한 명시적 동의 요구, 데이터 삭제 권리(잊힐 권리) 보장\n\nCCPA (California Consumer Privacy Act)\n\n캘리포니아 주의 소비자 개인정보 보호법\n예시: 소비자의 개인정보 접근 권리, 개인정보 판매 거부 권리\n\nHIPAA (Health Insurance Portability and Accountability Act)\n\n미국의 의료정보 보호법\n예시: 환자 의료 정보의 비밀성 보장, 의료 정보 접근 로그 유지\n\nPCI DSS (Payment Card Industry Data Security Standard)\n\n신용카드 정보 보호 표준\n예시: 카드 소지자 데이터의 암호화, 정기적인 보안 시스템 및 프로세스 검사\n\nSOX (Sarbanes-Oxley Act)\n\n미국의 기업 회계 개혁법\n예시: 재무 보고의 정확성과 신뢰성을 보장하기 위한 데이터 관리 요구사항\n\n개인정보 보호법\n\n한국\n예시: 개인정보 수집 시 동의 획득, 개인정보의 안전한 보관 및 파기\n\n전자금융거래법\n\n한국\n예시: 전자금융 거래 기록의 보관 및 보안 요구사항\n\n정보통신망법\n\n한국\n예시: 개인정보 유출 시 신고 의무, 정보보호 관리체계 인증\n\n데이터 현지화 법률\n\n여러 국가\n예시: 러시아의 데이터 현지화법, 중국의 사이버보안법에 따른 데이터 현지 저장 요구\n\n\n이러한 규제 요구사항을 충족하기 위해서는 데이터의 정의, 분류, 처리 방법 등이 명확히 문서화되고 관리되어야 한다.\n표준 단어 사전은 데이터 요소를 일관되게 정의하고 분류하는 데 도움을 줌으로써, 규제 준수를 위한 기반을 제공한다.\n예를 들어, 개인식별정보(PII)가 무엇인지, 어떤 데이터 필드가 이에 해당하는지를 명확히 정의하고 관리할 수 있게 해준다.\n\n\n\n\n\n일관된 데이터 정의와 사용으로 전반적인 데이터 품질을 개선한다.\n데이터의 입력 오류를 예방함으로써 데이터의 품질을 향상\n데이터 통합과 분석의 정확성을 높인다.\n\n\n\n\n\n조직의 데이터 관련 지식을 체계적으로 축적하고 공유할 수 있다.\n신규 직원의 온보딩과 지식 전달을 용이하게 한다.\n\n\n\n\n\n서로 다른 시스템 간의 데이터 매핑과 통합을 지원한다.\n시스템 간 데이터 불일치 문제를 줄인다.\n데이터 변환, 정제 비용 감소: 데이터의 전송, 공유, 가공을 위해 정보시스템 간 또는 정보시스템 내에서 별도의 포맷 변환이나 정제 작업 불 필요\n\n\n\n\n\n\n\n\n데이터 표준관리 지침\n데이터 관리 가이드\n표준 데이터\n구조 데이터\n\n각 요소는 서로 밀접하게 연관되어 있으며, 전체적인 데이터 거버넌스 체계를 형성한다.\n\n\n\n\n데이터 표준을 정의하고 변경 관리하는 데 필요한 지침을 제공한다.\n조직 내에서 데이터 표준을 어떻게 설정하고, 유지하며, 업데이트할 것인지에 대한 가이드라인을 제시한다.\n\n\n\n\n\n데이터 표준, 구조, 흐름을 관리하기 위한 프로세스, 조직, 역할을 정의한다.\n이는 데이터 거버넌스의 실행 측면을 다루며, 누가 어떤 책임을 지고 데이터 표준화 활동을 수행할지 명확히 한다.\n\n\n\n\n\n표준 데이터는 실제 데이터 요소들을 표준화하는 핵심 부분이다. 여기에는 다음과 같은 세부 요소들이 포함됩니다\n\n\n표준 단어\n\n명칭 (한글/영문)\n영문약어\n단어 정의\n표준 단어를 기반으로 자료형을 정의하여 표준 단어 금칙어를 만든다.\n\n표준 도메인\n\n표준 단어를 기반으로 자료형을 정의하여 표준 도메인을 작성한다.\n데이터 타입 결정\n데이터 길이 결정\n\n표준 용어\n\n표준 단어, 자료형, 표준 도메인을 기반으로 용어를 정의한다.\n표준 단어 2개 이상의 조합으로 용어를 생성한다.\n표준 용어는 1개의 표준 도메인으로 구성된다.\n\n표준코드\n\n표준 도메인을 기반으로 유효값을 정의한다.\n유효값을 기반으로 코드값을 만들어낸다.\n\n제작 순서\n\n표준 단어 \\(\\rightarrow\\) 표준 도메인 \\(\\rightarrow\\) 표준 용어 \\(\\rightarrow\\) 표준 코드 \\(\\rightarrow\\) 코드 값\n\n\n\n표준 단어와 표준 용어는 한국어에만 존재하는 개념으로 다른 외국어에는 없는 개념이다. 외국어는 형태소 분석이 한국어 만큼 세분화되어 있지 않아 표준 단어와 표준 용어가 합쳐진 개념인 데이터 카탈로그란 용어를 사용한다.\n\n\n\n\n\n데이터 모델링의 영역이다.\n표준 데이터가 정의되면, 이를 바탕으로 실제 데이터 모델을 구축합니다. 이 과정은 크게 세 단계로 나뉜다:\n\n개념 데이터 모델\n논리 데이터 모델\n물리 데이터 모델\n\n개념 데이터 모델 + 논리 데이터 모델링을 통해 주제영역, 엔티티, 속성을 차례로 규명한다.\n물리 데이터 모델을 통해 엔티티를 물리적으로 구현한 테이블과 속성을 물리적으로 구현한 컬럼을 만들어낸다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5-0.data_standard.html#데이터-표준관리-목적",
    "href": "docs/blog/posts/Governance/5-0.data_standard.html#데이터-표준관리-목적",
    "title": "Data Governance Study - Data Standard Governance",
    "section": "",
    "text": "각 데이터 항목에 대한 명명 규칙을 정의\n표준단어 및 표준용어, 표준도메인, 표준코드 등을 전사관리기준 형태로 정의\n이러한 표준들을 변경 및 관리하는 활동 수행"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-0.data_standard.html#데이터-표준관리-정의",
    "href": "docs/blog/posts/Governance/5-0.data_standard.html#데이터-표준관리-정의",
    "title": "Data Governance Study - Data Standard Governance",
    "section": "",
    "text": "데이터 표준관리는 데이터 항목들에 대해서 전사적으로 일관된 명명과 정의를 부여하고 이를 정제, 개선하는 활동이다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5-0.data_standard.html#주요-정제-및-개선-사항",
    "href": "docs/blog/posts/Governance/5-0.data_standard.html#주요-정제-및-개선-사항",
    "title": "Data Governance Study - Data Standard Governance",
    "section": "",
    "text": "데이터 표준관리에서 중점적으로 다루는 정제 및 개선 사항은 다음과 같다:\n\n\n\n대상\n정제 및 개선 결과물\n\n\n\n\n명칭 + 정의\n단어 및 용어\n\n\n형식\n도메인\n\n\n규칙\n코드\n\n\n\n이러한 표준화 활동을 통해 조직 전체에서 일관된 데이터 관리가 가능해진다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5-0.data_standard.html#필요성",
    "href": "docs/blog/posts/Governance/5-0.data_standard.html#필요성",
    "title": "Data Governance Study - Data Standard Governance",
    "section": "",
    "text": "각 용어에 대한 부가 정보(예: 데이터 타입, 길이, 허용 값 등)를 관리할 수 있다.\n데이터의 의미와 구조에 대한 종합적인 이해를 제공한다.\n\n\n\n\n\n비즈니스 사용자와 IT 전문가 사이의 의사소통을 원활하게 한다\n동일한 데이터에 대해서는 동일한 명칭을 사용함으로써 개발자-현업, 운영자-현업, 운영자-운영자 등 다양한 계층간에 명확하고 신속한 의사소통이 가능\n편의나 관습에 따라 동일한 의미로 사용되는 이음동의어를 대표성을 지닌 한 개의 단어로 정의함으로써 의사소통과 데이터에 대한 인식의 오류를 방지할 수 있다\n용어의 오해로 인한 프로젝트 지연이나 오류를 방지한다.\n\n\n\n\n\n데이터 소재 파악의 시간 및 노력 감소\n즉, 정보 사용자는 데이터의 의미와 데이터의 위치 등을 신속하게 파악할 수 있어 적시에 정확한 정보를 활용\n\n\n\n\n\n데이터 관련 규제 요구사항을 충족하는 데 도움을 준다.\n\n데이터 관련 규제 요구사항: 데이터의 수집, 저장, 처리, 공유, 및 폐기와 관련된 법적, 규범적 기준을 의미\n데이터 보안, 개인정보 보호, 데이터 품질, 및 데이터 관리의 투명성을 보장하기 위한 규제\n\n데이터의 의미와 사용에 대한 명확한 문서화를 제공한다.\n데이터 규제 종류\n\nGDPR (General Data Protection Regulation)\n\nEU의 개인정보 보호법\n예시: 개인 데이터의 수집 및 처리에 대한 명시적 동의 요구, 데이터 삭제 권리(잊힐 권리) 보장\n\nCCPA (California Consumer Privacy Act)\n\n캘리포니아 주의 소비자 개인정보 보호법\n예시: 소비자의 개인정보 접근 권리, 개인정보 판매 거부 권리\n\nHIPAA (Health Insurance Portability and Accountability Act)\n\n미국의 의료정보 보호법\n예시: 환자 의료 정보의 비밀성 보장, 의료 정보 접근 로그 유지\n\nPCI DSS (Payment Card Industry Data Security Standard)\n\n신용카드 정보 보호 표준\n예시: 카드 소지자 데이터의 암호화, 정기적인 보안 시스템 및 프로세스 검사\n\nSOX (Sarbanes-Oxley Act)\n\n미국의 기업 회계 개혁법\n예시: 재무 보고의 정확성과 신뢰성을 보장하기 위한 데이터 관리 요구사항\n\n개인정보 보호법\n\n한국\n예시: 개인정보 수집 시 동의 획득, 개인정보의 안전한 보관 및 파기\n\n전자금융거래법\n\n한국\n예시: 전자금융 거래 기록의 보관 및 보안 요구사항\n\n정보통신망법\n\n한국\n예시: 개인정보 유출 시 신고 의무, 정보보호 관리체계 인증\n\n데이터 현지화 법률\n\n여러 국가\n예시: 러시아의 데이터 현지화법, 중국의 사이버보안법에 따른 데이터 현지 저장 요구\n\n\n이러한 규제 요구사항을 충족하기 위해서는 데이터의 정의, 분류, 처리 방법 등이 명확히 문서화되고 관리되어야 한다.\n표준 단어 사전은 데이터 요소를 일관되게 정의하고 분류하는 데 도움을 줌으로써, 규제 준수를 위한 기반을 제공한다.\n예를 들어, 개인식별정보(PII)가 무엇인지, 어떤 데이터 필드가 이에 해당하는지를 명확히 정의하고 관리할 수 있게 해준다.\n\n\n\n\n\n일관된 데이터 정의와 사용으로 전반적인 데이터 품질을 개선한다.\n데이터의 입력 오류를 예방함으로써 데이터의 품질을 향상\n데이터 통합과 분석의 정확성을 높인다.\n\n\n\n\n\n조직의 데이터 관련 지식을 체계적으로 축적하고 공유할 수 있다.\n신규 직원의 온보딩과 지식 전달을 용이하게 한다.\n\n\n\n\n\n서로 다른 시스템 간의 데이터 매핑과 통합을 지원한다.\n시스템 간 데이터 불일치 문제를 줄인다.\n데이터 변환, 정제 비용 감소: 데이터의 전송, 공유, 가공을 위해 정보시스템 간 또는 정보시스템 내에서 별도의 포맷 변환이나 정제 작업 불 필요"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-0.data_standard.html#데이터-표준화-요소-간-관계",
    "href": "docs/blog/posts/Governance/5-0.data_standard.html#데이터-표준화-요소-간-관계",
    "title": "Data Governance Study - Data Standard Governance",
    "section": "",
    "text": "데이터 표준관리 지침\n데이터 관리 가이드\n표준 데이터\n구조 데이터\n\n각 요소는 서로 밀접하게 연관되어 있으며, 전체적인 데이터 거버넌스 체계를 형성한다.\n\n\n\n\n데이터 표준을 정의하고 변경 관리하는 데 필요한 지침을 제공한다.\n조직 내에서 데이터 표준을 어떻게 설정하고, 유지하며, 업데이트할 것인지에 대한 가이드라인을 제시한다.\n\n\n\n\n\n데이터 표준, 구조, 흐름을 관리하기 위한 프로세스, 조직, 역할을 정의한다.\n이는 데이터 거버넌스의 실행 측면을 다루며, 누가 어떤 책임을 지고 데이터 표준화 활동을 수행할지 명확히 한다.\n\n\n\n\n\n표준 데이터는 실제 데이터 요소들을 표준화하는 핵심 부분이다. 여기에는 다음과 같은 세부 요소들이 포함됩니다\n\n\n표준 단어\n\n명칭 (한글/영문)\n영문약어\n단어 정의\n표준 단어를 기반으로 자료형을 정의하여 표준 단어 금칙어를 만든다.\n\n표준 도메인\n\n표준 단어를 기반으로 자료형을 정의하여 표준 도메인을 작성한다.\n데이터 타입 결정\n데이터 길이 결정\n\n표준 용어\n\n표준 단어, 자료형, 표준 도메인을 기반으로 용어를 정의한다.\n표준 단어 2개 이상의 조합으로 용어를 생성한다.\n표준 용어는 1개의 표준 도메인으로 구성된다.\n\n표준코드\n\n표준 도메인을 기반으로 유효값을 정의한다.\n유효값을 기반으로 코드값을 만들어낸다.\n\n제작 순서\n\n표준 단어 \\(\\rightarrow\\) 표준 도메인 \\(\\rightarrow\\) 표준 용어 \\(\\rightarrow\\) 표준 코드 \\(\\rightarrow\\) 코드 값\n\n\n\n표준 단어와 표준 용어는 한국어에만 존재하는 개념으로 다른 외국어에는 없는 개념이다. 외국어는 형태소 분석이 한국어 만큼 세분화되어 있지 않아 표준 단어와 표준 용어가 합쳐진 개념인 데이터 카탈로그란 용어를 사용한다.\n\n\n\n\n\n데이터 모델링의 영역이다.\n표준 데이터가 정의되면, 이를 바탕으로 실제 데이터 모델을 구축합니다. 이 과정은 크게 세 단계로 나뉜다:\n\n개념 데이터 모델\n논리 데이터 모델\n물리 데이터 모델\n\n개념 데이터 모델 + 논리 데이터 모델링을 통해 주제영역, 엔티티, 속성을 차례로 규명한다.\n물리 데이터 모델을 통해 엔티티를 물리적으로 구현한 테이블과 속성을 물리적으로 구현한 컬럼을 만들어낸다."
  },
  {
    "objectID": "docs/blog/posts/Governance/5-2.data_word_domain.html",
    "href": "docs/blog/posts/Governance/5-2.data_word_domain.html",
    "title": "Data Governance Study - Data Domain Standardization",
    "section": "",
    "text": "비즈니스적으로 의미 있고 데이터의 성격을 분류한 것으로 동일한 형식을 가진 집합을 도메인이라 하며, 하나의 용어는 하나의 도메인만 지정한다.\n동일한 형식을 가진 데이터에 대해서 같은 도메인을 적용함으로써 속성의 의미 및 데이터의 범위를 명확히 할 수 있고 컬럼에 대한 일관적인 관리가 가능하다\n\n데이터의 형식, 길이, 허용 가능한 값의 범위 등을 명시\n\n표준 단어 조합의 마지막에 위치하는 분류단어(도메인성) 단어가 도메인의 후보가 된다.\n예시\n\n표준 단어 (주제어): 장기\n표준 단어 (수식어): 대여\n표준 단어 (분류단어): 금액\n표준 용어 : 장기대여금액\n데이터 모델 (속성명): 장기대여금액 with datatype = NUMBER(10)\n도메인: 금액N10\n\n\n\n\n\n동일한 형식을 가진 데이터에 대해서 같은 도메인을 적용함으로써 속성의 의미 및 유효한 데이터범위를 명확히 할 수 있고, 칼럼에 대한 일관적인 관리가 가능하다\n동일한 도메인을 사용하는 칼럼의 속성 성격을 변경하고자 할 때 도메인만을 변경함으로써 데이터 타입 및 길이를 동시에 부여하여 변경할 수 있다\n\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nDomain Group\n\n\n\na1\n\nDomain\n\n\n\na0--a1\n\n\n1:N\nN\n1\n\n\n\na2\n\nInfo Type\n\n\n\na1--a2\n\n\n1:N\nN\n1\n\n\n\n\n\n\n\n\n\n도메인그룹 (Domain Group) : 도메인 그룹은 성격이 유사한 도메인들을 그룹화 해서 관리하는 관리 단위이며, 예를 들면 금액, 날짜, 내용, 율 등의 도메인 그룹이 존재하며, 금액 도메인 그룹의 하위에는 금액, 외화금액,세액 등의 도메인이 존재 한다\n도메인 (Domain): 도메인은 데이터 값의 범위를 컬럼(속성)의 특성에 따라 분류한 것이므로 업무적으로 또는 비지니스적으로 의미가 있는 도메인명을 부여해야 하며, 표준단어를 사용하여 명명한다.\n인포타입 (Info Type): 인포타입은 Information Type의 약어로, 해당 도메인에서 사용할 수 있는 데이터 타입과 길이가 결합된 형태로 속성이 가질 수 있는 데이터 타입과 길이를 나타낸다\nDBMS별 데이터 타입 (Data Type): 표준 인포타입을 정의한 뒤에는 각 DBMS의 특성에 적합한 데이터 타입을 정의한다. DBMS는 다양한 제약 사항이 존재하기 때문에 데이터베이스를 설계할 DBMS의 특성도 반영해야 한다. 따라서 DBMS별 인포타입을 정의하고 이는 전사 표준 인포타입과 매핑 정보를 유지한다.\n\n\n\n\n\n예를 들어, 회사의 전사 도메인 그룹은 9개 도메인 그룹으로 관리한다고 하면 도메인은 다음과 같이 관리 될 수 있다.\n[도메인 그룹 – 도메인 – 인포타입 – DBMS별 데이터 타입] 예시\n\n\n\n\n\n\n\n\n\n\n\n\n도메인그룹\n도메인\n인포타입\nDBMS별 데이터 타입\n\n예시\n\n\n\n\n\n\n\n데이터 타입\n길이\n\n\n\n날짜\n일자\n일자VC8\nVARCHAR2\n8\n대손처리 일자\n\n\n날짜\n일시\n일시VC16\nVARCHAR2\n16\n대여시작 일시\n\n\n명\n명\n명VC30\nVARCHAR2\n30\n송금정비업체 명\n\n\n명\n성명\n성명VC10\nVARCHAR2\n10\n담당자 성명\n\n\n내용\n내용\n내용VC200\nVARCHAR2\n200\n수리요청 내용\n\n\n수\n수\n수N9\nNUMBER\n4\n에어백보유 수\n\n\n수\n건수\n건수N4\nNUMBER\n4\n신청 건수\n\n\n수\n수량\n수량N4\nNUMBER\n4\n신품출고 수량\n\n\n율\n이율\n이율N5,3\nNUMBER\n5,3\n연체 이율\n\n\n율\n금리\n금리N5,2\nNUMBER\n5,2\n적용 금리\n\n\n금액\n금액\n금액N10\nNUMBER\n10\n정산승인 금액\n\n\n금액\n잔액\n잔액N10\nNUMBER\n10\n매출 잔액\n\n\n번호\n고객번호\n법인고객번호\nVARCHAR2\n9\n법인고객번호\n\n\n번호\n법인등록번호\n법인등록번호\nVARCHAR2\n13\n협력업체 법인등록번호\n\n\n코드\n구분코드\n가족관계유형코드\nVARCHAR2\n5\n지점 시도구분코드\n\n\n분류\n여부\n여부VC1\nVARCHAR2\n1\n신규사업MT 여부\n\n\n\n\nNUMBER(5,3)은 다음을 의미\n\n총 5자리의 숫자를 저장\n그 중 3자리는 소수점 이하 숫자\n결과적으로 소수점 앞에는 2자리의 숫자만 올 수 있다.\n예시,\n\n12.345 (유효)\n1.234 (유효)\n0.123 (유효)\n123.45 (무효 - 소수점 이상 자릿수 초과)\n1.2345 (무효 - 소수점 이하 자릿수 초과)\n\n\n\n\n\n\n\n표준 도메인 구성 요소\n\n범위형 도메인: 속성(컬럼)에 허용되는 데이터 값을 데이터의 유형과 길이로 범위를 제한\n\n열거형 도메인: 속성(컬럼)에 허용되는 데이터 값을 정의된 범위 내에서 구체적으로 열거 또는 목록화하여 범위를 제한\n예시\n\n\n\n\n\n\n\n\n\n\n도메인 유형\n도메인그룹\n정의\n도메인 예시\n\n\n\n\n범위형 도메인\n날짜\n• 특정 사건이 일어난 시점 또는 시점과 시점간의 정해진 기간을 표현하기 위한 도메인\n일자, 일시, 년도, 년월, 월, 일, 시각, 시분, 분기, 반기 등\n\n\n범위형 도메인\n명칭\n• 문자 형식으로 객체에 대한 식별을 표현하기 위한 도메인\n명, 성명, 영문성명, 주소, 우편번호주소, 상세주소, 이메일주소 등\n\n\n범위형 도메인\n내용\n• 서술 형식의 상세 내용을 자유 형식의 텍스트로 표현하기 위한 도메인\n내용\n\n\n범위형 도메인\n수량\n• 객체의 개수나 양을 수로써 표현하기 위한 도메인• 일반적인 측량 단위도 포함됨\n수, 일수, 개월수, 매수, 좌수, 건수, 량, 평점, 연령, 면적, 평형 등\n\n\n범위형 도메인\n율\n• 비율을 수로 표현하기 위한 도메인\n율, 이율, 이자율, 지분율, 할인율 등\n\n\n범위형 도메인\n금액\n• 화폐의 가치를 수로 표현하기 위한 도메인\n금액, 잔액, 차액, 보증금, 수수료, 할인료, 보험료 등\n\n\n범위형 도메인\n번호\n• 각 자리 별 특정 의미를 가지거나 체계를 가지고 관리되어야 하는 속성을 정의하기 위한 도메인• 용어 별 고유의 번호 도메인을 부여\n전사고객번호, 계좌번호, 직원번호, 단말번호, 거래번호, 법인등록번호, 일련번호 등\n\n\n열거형 도메인\n코드\n• 코드화 하여 관리되는 속성을 정의하기 위한 도메인\n그룹내기관코드, 거래상태코드 등\n\n\n열거형 도메인\n분류\n• 상반된 상태의 값을 갖는 속성을 정의하기 위한 도메인\n여부, 유무\n\n\n\n\n\n\n\n\n\n\n[날짜] 도메인그룹은 특정 사건이 일어난 시점 또는 시점과 시점간의 정해진 기간을 표현하기 위한 도메인들을 그룹화하여 관리한다.\n\n\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n데이터타입\n길이\n유효값 범위\n예시\n\n\n\n\n일\nDD형태의 데이터 값을 갖는 도메인\nVARCHAR2\n2\n01~31\n계약시작 일\n\n\n시각\nHHMISS 형태의 시각을 나타내는 도메인\nVARCHAR2\n6\n000000~235959\n차량사용시작 시각\n\n\n일자\nYYYYMMDD형태의 데이터 값을 갖는 도메인\nVARCHAR2\n8\n00010101 ~ 99991231\n계약해지 일자\n\n\n생년월일\nYYMMDD형태의 데이터 값을 갖는 도메인\nVARCHAR2\n6\n000101 ~ 991231\n회원 생년월일\n\n\n일시\nYYYYMMDDHH24MISS형태의 데이터 값을 갖는 도메인\nVARCHAR2\n16\n\n가격정책등록 일시\n\n\n타임스템프\nYYYYMMDDHH24MISSFF3형태의 데이터 값을 갖는 도메인\nTIMESTAMP\n20\n\n\n\n\n년도\nYYYY형태의 데이터 값을 갖는 도메인\nVARCHAR2\n4\n0001 ~ 9999\n기준 년도\n\n\n년월\nYYYYMM형태의 데이터 값을 갖는 도메인\nVARCHAR2\n6\n\n지불 년월\n\n\n월\nMM형태의 데이터 값을 갖는 도메인\nVARCHAR2\n2\n\n청구 월\n\n\n\n\n년월일 형식은 ‘YYYYMMDD’ 로 통일하며, 도메인명은 ‘일자’ 로 정의하여 사용한다\n\n(예시) 계약해지 일자 : 20160420\n\n‘YYYY’ 형식의 도메인명은 ‘년도’ 로 정의하여 사용한다\n\n(예시) 출생 년도 : 1966, 기준 년도 : 2016\n\n‘YYYYMM’ 형식의 도메인명은 ‘년월’ 로 정의하여 사용한다\n\n(예시) 포인트적립 년월 : 201604\n\n‘MM’ 형식의 도메인명은 ‘월’ 로 정의하여 사용한다\n\n(예시) 결산 월 : 04\n\n‘MMDD’ 형식의 도메인명은 ‘월일’ 로 정의하여 사용한다\n\n(예시) 기혼기념 월일 : 1010\n\n‘DD’ 형식의 도메인명은 ‘일’ 로 정의하여 사용한다\n\n(예시) 자동이체지정 일 : 25\n\n년월일+시분초 형식의 도메인명은 ‘일시’ 로 정의하여 사용한다. ‘일시’ 도메인의 DataType은 Date 와 Variable Character의 두 가지를 지원한다\n시스템 로그일시(YYYYMMDDTTMMSSFF3)를 표시하기 위해서는 타임스탬프 도메인을 사용한다. DataType은 Timestamp 형태이다\n\n\n\n\n\n[명] 도메인그룹은 객체에 대한 식별을 표현하기 위한 도메인들을 그룹화하여 관리한다.\n사람을 제외한 포함한 모든 명칭은 ‘명’ 을 도메인으로 정의하여 사용하며, 사람은 ’성명’을 사용한다.\n‘명’ 도메인을 속성(컬럼)에 사용할 경우 도메인 앞에 ‘한글’,’영문’,‘한자’,‘약어’ 등과 같은 수식어가 생략된 경우는 ‘한글’+’명을 ’명’으로 갈음한다\n\n(예시) 종목 명(한글), 종목 영문 명(영문)\n\n사람이 살고 있는 곳이나 기관, 회사 따위가 자리 잡고 있는 곳을 행정 구역으로 나타낸 이름은 ‘주소’ 를 도메인으로 정의하여 사용한다\n’명’은 단독으로 사용하지 않고 사전을 참조하여 복합어로 생성한다\n예시\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n명\n상품, 사물 등을 식별하기 위한 명칭에는 ‘명’ 도메인을 사용한다\n상품 명\n\n\n\n성명\n사람의 성명을 관리할 경우 ’성명’이라는 도메인을 사용한다\n고객 영문성명\n\n\n\n주소\n사람이 살고 있는 곳이나 기관, 회사 따위가 자리 잡고 있는 곳을 행정 구역으로 나타낸 이름은 ‘주소’ 도메인을 사용한다\n직장 주소\n\n\n\n이메일주소\n전자우편주소\n수신자 이메일주소\n\n\n\nID\n시스템 오브젝트 등을 식별하기 위해 사용되는 도메인으로서 체계 없이 순차적으로 체번하여 사용하는 일련번호와는 구별하여 사용한다\n스마트빌 ID\n\n\n\n\n\n\n\n\n“내용” 단일 도메인으로 정의 한다.\n“명세”, “설명”, ”비고”, “적요”, “내역”, “의견”, “사유”, “사항” 등 유사 도메인은 별도로 정의하지 않고 “내용” 도메인으로 통합관리 한다.\n\n(예시) 평가자 의견 (X) → 평가자 의견 내용 (O)\n\n’값’은 단일단어로 허용하지 않고, 사전을 참조하여 복합어로 생성한다.\n예시\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n내용\n사실이나 사물에 대해 전하고자 하는 정보를 형식 없이 서술형으로 저장하는 경우 사용된다.\n사고 내용\n\n\n\n값\n평가값, 항목값 등 값을 의미하는 용어에 대해 값 도메인을 사용한다.\n항목 값, 입력 값\n\n\n\n\n\n\n\n\n[수] 도메인그룹은 객체의 개수나 양을 수로써 표현하기 위한 도메인들을 그룹화하여 관리한다.\n[수 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n수\n셀 수 있는 사물의 크기를 나타내는 값(복합어로 생성)\n종업원 수\n\n\n\n일수\n일을 기준으로 헤아리는 수.\n연체 일수\n\n\n\n개월수\n월을 기준으로 헤아리는 수\n견적 개월수\n\n\n\n년수\n년을 기준으로 헤아리는 수\n근무 년수\n\n\n\n매수\n종이나 유리 따위의 장으로 셀 수 있는 물건의 수효\n기본 매수\n\n\n\n개수\n한 개씩 낱으로 셀 수 있는 물건의 수효\n보유 개수\n\n\n\n건수\n사물이나 사건의 가짓수\n조회 건수\n\n\n\n횟수\n돌아오는 차례의 수효\n기존인출 횟수\n\n\n\n점수\n성적을 나타내는 숫자\n평가대상 점수\n\n\n\n연령\n나이, 사람이 세상에 나서 현재 또는 기준이 되는 때까지 살아 온 햇수\n보험 연령\n\n\n\n수량\n수와 량이 혼재된 수량을 자연수로 표현한 수\n수량\n\n\n\n\n\n금액을 제외한 정보의 수치 및 합계 등을 정의하는 경우에 사용한다\n\n(예시) 고객 수, 연체 일수, 거래 량\n\n되풀이되는 일이나 차례의 수효를 나타내는 경우는 ’횟수’를 도메인으로 정의하여 사용한다.\n\n(예시) 지로자동이체 횟수\n\n나이와 관련된 용어는 ‘연령’ 을 도메인으로 정의하여 사용한다\n\n(예시) 보험 연령\n\n기간을 나타내는 용어의 경우 ‘기간’을 사용하지 않고, ‘년수’, ‘개월수’, ‘일수’ 등으로 구체 적으로 정의한다\n\n(예시) 대출 년수, 연장 개월수, 연체 일수\n\n\n\n\n\n\n[율] 도메인그룹은 둘 이상의 수를 비교하여 그 중 하나의 수를 기준으로 하여 나타낸 다른 수의 비교 값을 표현하기 위한 도메인들을 그룹화하여 관리한다.\n‘율’ 또는 ’률＇은 단일단어로 사용하지 않고 사전을 참조하여 복합어로 생성한다\n확률, 비율 등 ‘%’ 로 관리되는 속성에 대해 ‘율/비율’ 을 도메인으로 정의하여 사용한다\n\n(예시) 연체 율, 담보 비율, 적용 비율\n\n[율 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n율\n비율의 뜻을 나타내는 말 “~율”은 모음으로 끝나거나 ‘ㄴ’ 받침을 가진 일부 명사 뒤에 붙임(복합어로 생성)\n할인 율\n\n\n\n이율\n원금에 대한 이자의 비율, 즉 이자 산출에 기초가 되는 비율\n연체 이율\n\n\n\n세율\n과세 표준에 의하여 세금을 계산하여 매기는 법정률\n\n\n\n\n요율\n요금의 정도나 비율\n보증 요율\n\n\n\n금리\n자금의 사용료로 대외적으로 공시되는 기준의 의미로 사용\n대출 금리\n\n\n\n환율\n외국환 시세\n기준 환율\n\n\n\n비율\n둘 이상의 수를 비교하여 나타낼 때 그 중 한 개의 수를 기준으로 하여 나타낸 다른 수의 비교 값\n사고MT 비율\n\n\n\n\n\n\n\n\n[금액] 도메인그룹은 돈의 액수나 화폐 가치를 표현하기 위한 도메인들을 그룹화하여 관리한다\n금액을 의미하는 도메인은 중복해서 사용하는 것을 피한다\n\n(예시) 물품 + 원가(도메인) + 금액(도메인) → 물품 + 원가(도메인)\n\n기본적으로 ‘금액’ 도메인으로 표현될 수 있는 속성에 대해서는 ‘금액’ 도메인의 사용을 권장한다. 하지만 관용적으로 ‘금’ 도메인이 사용되는 표현에는 별도의 도메인으로 분류하여 사용한다\n합산금액을 의미하는 총계, 합계, 누계 등의 수식어는 반드시 금액 앞에 붙인다\n\n(예시) 감가상각누계금액, 자산총계금액\n\n부득이하게 ’금＇을 사용해야 하는 경우 복합어 생성한다\n[금액 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n\n도메인\n세부 도메인\n설명\n예시\n비고\n\n\n\n\n요금\n료\n대여료, 렌탈료, 리스료 등을 나타내는 금액 단위(복합어로 생성)\n수수 료, 과태 료\n\n\n\n금액\n금액\n돈의 액수\n고객청구 금액\n\n\n\n금액\n잔액\n남은 금액\n연체이자 잔액\n\n\n\n세\n세\n조세의 액수(복합어로 생성)\n부가 세\n\n\n\n가격\n단가\n물건 한 단위(單位)의 가격\n계약 단가\n\n\n\n가격\n원가\n상품의 제조 판매 배급 따위에 든 재화와 용역을 단위에 따라 계산한 가격\n물품 원가\n\n\n\n\n\n\n\n\n[번호] 도메인그룹은 일정한 체계를 가지거나 특정 자리에 존재하는 의미를 표현하기 위한 도메인들을 그룹화하여 관리한다.\n번호도메인의 공통 도메인은 생성하지 않는다.\n‘번호’ 도메인은 일정한 체계를 가지거나 특정 자리의 의미가 존재하는 속성을 정의할 때 사용하는 것을 원칙으로 한다.\n번호 자체에 특별한 의미를 가진 경우 해당 번호를 번호도메인으로 정의하여 사용한다\n\n(예시) 주민등록번호, 사업자등록번호, 전화번호, 법인등록번호 등\n\n순차적으로 채번되는 번호는 ‘일련번호’ 도메인으로 정의하여 사용한다.\n번호도메인의 공통인 도메인은 생성하지 않는다.\n\n(예시) 번호N10 (Number(10))과 같이 분류어가 번호인 용어의 도메인이 공통적으로 쓰는 도메인 생성 금지.\n\n[번호 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n전화번호\n가입된 전화마다 매겨져 있는 일정한 번호\n계약담당 전화번호\n\n\n\n우편번호\n우편물을 쉽게 분류하기 위하여 정보 통신부에서 각 지역마다 매긴 번호\n계약자 우편번호\n\n\n\n주민등록번호\n주민등록을 할 때에, 국가에서 국민에게 부여하는 고유 번호\n계약자 주민등록번호\n\n\n\n사업자등록번호\n세무에서, 신규로 개업하는 사업자에게 부여하는 사업체의 고유번호이다\n공급자 사업자등록번호\n\n\n\n법인등록번호\n사무소의 소재지에서 설립등기(設立登記)를 함으로써 성립하는데 이때 부여된 일련번호이다\n~ 법인등록번호\n\n\n\n계좌번호\n금융 기관에 예금하려고 설정한 개인명이나 법인명의 계좌에 부여된 번호\n입금 계좌번호\n\n\n\n휴대전화번호\n지니고 다니면서 걸고 받을 수 있는 소형 무선 전화기 번호\n\n\n\n\n비밀번호\n본인임을 확인하기 위해 설정한 암호\n\n\n\n\n신용카드번호\n신용카드식별번호\n~ 신용카드번호\n\n\n\n직원번호\n회사 직원 식별번호\n~ 사원번호\n\n\n\n여권번호\n외국을 여행하는 사람의 신분이나 국적을 증명하고 상대국에 그 보호를 의뢰하는 문서번호.\n고객 여권번호\n\n\n\n외국인등록번호\n외국인등록번호\n외국인등록번호\n\n\n\n일련번호\n일률적으로 연속되어 있는 번호\n구성품 일련번호\n\n\n\n\n\n\n\n\n코드는 다른 도메인들과 달리 특정 도메인 값(즉, 코드값)과 이 값에 대한 의미(즉, 코드값명)를 표현하기 위해 코드도메인을 그룹화하여 표준코드로 분류하여 별도로 관리한다\n코드값을 가지는 속성은 반드시 ‘코드’ 를 도메인으로 정의하여 사용한다\n속성명은 해당 속성이 사용하는 코드도메인명과 일치시키는 것을 원칙으로 하나, 속성명에 수식어를 붙여서 사용할 수 있다\n\n(예시) 코드도메인 ‘거래코드’ : 속성명 → 거래코드(O), 수신거래코드(O)\n\n코드의 의미가 ‘여부’ 또는 ‘유무’ 인 경우 ‘코드’를 도메인으로 사용할 수 없고, ‘여부/유무’ 를 도메인으로 정의하여 사용한다\n코드도메인은 ‘수식어 + 코드유형수식어 + 코드’ 로 명명한다\n[코드 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n도메인\n설명\n비고\n\n\n\n\n가상계좌은행코드\n[가상계좌은행코드] 코드는 회사 전사에서 사용하는 가상계좌 은행코드\n\n\n\n제품카테고리코드\n[제품카테고리코드] 코드는 제품의 카테코리에 대하여 조회할 수 있도록 구분하는 코드\n\n\n\n주차장코드\n[주차장구분코드] 코드는 자산 제고의 위치를 구분하는 코드\n\n\n\n수리항목코드\n[수리항목코드] 수리항목 목록을 구분하는 코드\n\n\n\n\n\n\n\n\n\n“여부” 도메인의 인스턴스는 반드시 ‘Y’ 또는 ‘N’ 만 허용되며, NULL, N/A, SAPCE 등의 값은 허용하지 않는다.\n“유무” 도메인의 인스턴스는 반드시 ‘Y’ 또는 ‘N’ 만 허용되며, NULL, N/A, SAPCE 등의 값은 허용하지 않는다.\n“여부/유무” 도메인명은 ‘Y/N’ 이외의 값을 허용하지 않으므로, 기타 값을 사용해야 하는 경우는 코드도메인으로 분류하여 사용하고, 특히 미 정의된 값은 속성의 인스턴스로 사용할 수 없으며, ‘미정의’ 값을 표현하기 위해서는 ’*’ (미정의)와 같이 반드시 대체값을 정의하여 인스턴스로 사용한다. (Not Null)\n단, 대외 인터페이스 테이블의 경우 업무 특성에 따라 Null을 에외로 허용할 수 있다\n[분류 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n여부\n특정 사실이나 행위의 ’그러함/그러하지 아니함’을 의미\n반출 여부\n\n\n\n유무\n존재나 소유의 ’있음/없음’을 의미\n보증금 유무\n\n\n\n\n\n이 두 도메인을 명확히 구분하여 사용하면 데이터의 의미를 더 정확하게 전달할 수 있다.\n\n예를 들어, ’계약 여부’는 계약이 체결되었는지 아닌지를 나타내는 반면, ’계약서 유무’는 물리적인 계약서 문서가 존재하는지 않는지를 나타낼 수 있다.\n\n\n\n\n\n\n\n도메인 등록 기준\n\n\n\n\n\n\n\n\n\n순번\n원칙\n예시\n\n\n\n\n1\n• 속성 데이터가 규칙을 가지는 경우 도메인 명으로 지정한다.• 속성값의 자릿수가 항상 일정해야 하는 경우• 특정 자릿수의 데이터가 의미를 가지는 경우• 속성값 데이터 내의 규칙이 존재하는 경우\n• 고객번호는 20자리• 사업자등록번호, 여권번호, 법인등록번호\n\n\n2\n속성 데이터 값의 유효값이나 유효 범위의 제한이 있는 경우 도메인으로 지정한다.\n• 분기 – 1,2,3,4 만 유효함• 일(Day) – 1~31만 유효함\n\n\n3\n속성값의 성격을 식별하고자 하는 경우 도메인 명으로 지정한다\n지점구분코드\n\n\n4\n속성명 명명 시에 합성어 등이 관용적으로 사용되어 분리하여 사용하기 힘든 경우 도메인 명으로 지정한다.\n잔액, 특소세\n\n\n5\n코드값 체계를 가지는 모든 코드는 도메인 명으로 지정한다\n은행코드, 부품코드\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n도메인 그룹\n도메인\n인포타입\n데이터 타입\n길이\n설명\n예시\n\n\n\n\n날짜\n일자\n일자VC8\nVARCHAR2\n8\nYYYYMMDD 형식의 날짜\n20230501\n\n\n날짜\n일시\n일시VC14\nVARCHAR2\n14\nYYYYMMDDHHMMSS 형식의 날짜와 시간\n20230501143000\n\n\n명칭\n성명\n성명VC50\nVARCHAR2\n50\n개인의 이름\n홍길동\n\n\n명칭\n상품명\n명VC100\nVARCHAR2\n100\n상품의 이름\n스마트폰 갤럭시 S23\n\n\n내용\n내용\n내용VC4000\nVARCHAR2\n4000\n자유 형식의 텍스트\n이 제품은 최신 기술을 적용한…\n\n\n수량\n수량\n수량N10\nNUMBER\n10\n물품의 개수\n1000\n\n\n율\n이율\n이율N5_2\nNUMBER\n5,2\n비율을 나타내는 수치 (소수점 2자리)\n3.75\n\n\n금액\n금액\n금액N15\nNUMBER\n15\n화폐 금액\n10000000\n\n\n번호\n전화번호\n전화번호VC20\nVARCHAR2\n20\n전화번호 형식\n010-1234-5678\n\n\n코드\n상품코드\n코드VC10\nVARCHAR2\n10\n상품을 구분하는 고유 코드\nPRD0001234\n\n\n분류\n여부\n여부VC1\nVARCHAR2\n1\nY/N으로 표현되는 여부\nY"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-2.data_word_domain.html#데이터-표준-도메인이란",
    "href": "docs/blog/posts/Governance/5-2.data_word_domain.html#데이터-표준-도메인이란",
    "title": "Data Governance Study - Data Domain Standardization",
    "section": "",
    "text": "비즈니스적으로 의미 있고 데이터의 성격을 분류한 것으로 동일한 형식을 가진 집합을 도메인이라 하며, 하나의 용어는 하나의 도메인만 지정한다.\n동일한 형식을 가진 데이터에 대해서 같은 도메인을 적용함으로써 속성의 의미 및 데이터의 범위를 명확히 할 수 있고 컬럼에 대한 일관적인 관리가 가능하다\n\n데이터의 형식, 길이, 허용 가능한 값의 범위 등을 명시\n\n표준 단어 조합의 마지막에 위치하는 분류단어(도메인성) 단어가 도메인의 후보가 된다.\n예시\n\n표준 단어 (주제어): 장기\n표준 단어 (수식어): 대여\n표준 단어 (분류단어): 금액\n표준 용어 : 장기대여금액\n데이터 모델 (속성명): 장기대여금액 with datatype = NUMBER(10)\n도메인: 금액N10\n\n\n\n\n\n동일한 형식을 가진 데이터에 대해서 같은 도메인을 적용함으로써 속성의 의미 및 유효한 데이터범위를 명확히 할 수 있고, 칼럼에 대한 일관적인 관리가 가능하다\n동일한 도메인을 사용하는 칼럼의 속성 성격을 변경하고자 할 때 도메인만을 변경함으로써 데이터 타입 및 길이를 동시에 부여하여 변경할 수 있다\n\n\n\n\n\n\n\n\n\n\n\nER\n\n\n\na0\n\nDomain Group\n\n\n\na1\n\nDomain\n\n\n\na0--a1\n\n\n1:N\nN\n1\n\n\n\na2\n\nInfo Type\n\n\n\na1--a2\n\n\n1:N\nN\n1\n\n\n\n\n\n\n\n\n\n도메인그룹 (Domain Group) : 도메인 그룹은 성격이 유사한 도메인들을 그룹화 해서 관리하는 관리 단위이며, 예를 들면 금액, 날짜, 내용, 율 등의 도메인 그룹이 존재하며, 금액 도메인 그룹의 하위에는 금액, 외화금액,세액 등의 도메인이 존재 한다\n도메인 (Domain): 도메인은 데이터 값의 범위를 컬럼(속성)의 특성에 따라 분류한 것이므로 업무적으로 또는 비지니스적으로 의미가 있는 도메인명을 부여해야 하며, 표준단어를 사용하여 명명한다.\n인포타입 (Info Type): 인포타입은 Information Type의 약어로, 해당 도메인에서 사용할 수 있는 데이터 타입과 길이가 결합된 형태로 속성이 가질 수 있는 데이터 타입과 길이를 나타낸다\nDBMS별 데이터 타입 (Data Type): 표준 인포타입을 정의한 뒤에는 각 DBMS의 특성에 적합한 데이터 타입을 정의한다. DBMS는 다양한 제약 사항이 존재하기 때문에 데이터베이스를 설계할 DBMS의 특성도 반영해야 한다. 따라서 DBMS별 인포타입을 정의하고 이는 전사 표준 인포타입과 매핑 정보를 유지한다.\n\n\n\n\n\n예를 들어, 회사의 전사 도메인 그룹은 9개 도메인 그룹으로 관리한다고 하면 도메인은 다음과 같이 관리 될 수 있다.\n[도메인 그룹 – 도메인 – 인포타입 – DBMS별 데이터 타입] 예시\n\n\n\n\n\n\n\n\n\n\n\n\n도메인그룹\n도메인\n인포타입\nDBMS별 데이터 타입\n\n예시\n\n\n\n\n\n\n\n데이터 타입\n길이\n\n\n\n날짜\n일자\n일자VC8\nVARCHAR2\n8\n대손처리 일자\n\n\n날짜\n일시\n일시VC16\nVARCHAR2\n16\n대여시작 일시\n\n\n명\n명\n명VC30\nVARCHAR2\n30\n송금정비업체 명\n\n\n명\n성명\n성명VC10\nVARCHAR2\n10\n담당자 성명\n\n\n내용\n내용\n내용VC200\nVARCHAR2\n200\n수리요청 내용\n\n\n수\n수\n수N9\nNUMBER\n4\n에어백보유 수\n\n\n수\n건수\n건수N4\nNUMBER\n4\n신청 건수\n\n\n수\n수량\n수량N4\nNUMBER\n4\n신품출고 수량\n\n\n율\n이율\n이율N5,3\nNUMBER\n5,3\n연체 이율\n\n\n율\n금리\n금리N5,2\nNUMBER\n5,2\n적용 금리\n\n\n금액\n금액\n금액N10\nNUMBER\n10\n정산승인 금액\n\n\n금액\n잔액\n잔액N10\nNUMBER\n10\n매출 잔액\n\n\n번호\n고객번호\n법인고객번호\nVARCHAR2\n9\n법인고객번호\n\n\n번호\n법인등록번호\n법인등록번호\nVARCHAR2\n13\n협력업체 법인등록번호\n\n\n코드\n구분코드\n가족관계유형코드\nVARCHAR2\n5\n지점 시도구분코드\n\n\n분류\n여부\n여부VC1\nVARCHAR2\n1\n신규사업MT 여부\n\n\n\n\nNUMBER(5,3)은 다음을 의미\n\n총 5자리의 숫자를 저장\n그 중 3자리는 소수점 이하 숫자\n결과적으로 소수점 앞에는 2자리의 숫자만 올 수 있다.\n예시,\n\n12.345 (유효)\n1.234 (유효)\n0.123 (유효)\n123.45 (무효 - 소수점 이상 자릿수 초과)\n1.2345 (무효 - 소수점 이하 자릿수 초과)\n\n\n\n\n\n\n\n표준 도메인 구성 요소\n\n범위형 도메인: 속성(컬럼)에 허용되는 데이터 값을 데이터의 유형과 길이로 범위를 제한\n\n열거형 도메인: 속성(컬럼)에 허용되는 데이터 값을 정의된 범위 내에서 구체적으로 열거 또는 목록화하여 범위를 제한\n예시\n\n\n\n\n\n\n\n\n\n\n도메인 유형\n도메인그룹\n정의\n도메인 예시\n\n\n\n\n범위형 도메인\n날짜\n• 특정 사건이 일어난 시점 또는 시점과 시점간의 정해진 기간을 표현하기 위한 도메인\n일자, 일시, 년도, 년월, 월, 일, 시각, 시분, 분기, 반기 등\n\n\n범위형 도메인\n명칭\n• 문자 형식으로 객체에 대한 식별을 표현하기 위한 도메인\n명, 성명, 영문성명, 주소, 우편번호주소, 상세주소, 이메일주소 등\n\n\n범위형 도메인\n내용\n• 서술 형식의 상세 내용을 자유 형식의 텍스트로 표현하기 위한 도메인\n내용\n\n\n범위형 도메인\n수량\n• 객체의 개수나 양을 수로써 표현하기 위한 도메인• 일반적인 측량 단위도 포함됨\n수, 일수, 개월수, 매수, 좌수, 건수, 량, 평점, 연령, 면적, 평형 등\n\n\n범위형 도메인\n율\n• 비율을 수로 표현하기 위한 도메인\n율, 이율, 이자율, 지분율, 할인율 등\n\n\n범위형 도메인\n금액\n• 화폐의 가치를 수로 표현하기 위한 도메인\n금액, 잔액, 차액, 보증금, 수수료, 할인료, 보험료 등\n\n\n범위형 도메인\n번호\n• 각 자리 별 특정 의미를 가지거나 체계를 가지고 관리되어야 하는 속성을 정의하기 위한 도메인• 용어 별 고유의 번호 도메인을 부여\n전사고객번호, 계좌번호, 직원번호, 단말번호, 거래번호, 법인등록번호, 일련번호 등\n\n\n열거형 도메인\n코드\n• 코드화 하여 관리되는 속성을 정의하기 위한 도메인\n그룹내기관코드, 거래상태코드 등\n\n\n열거형 도메인\n분류\n• 상반된 상태의 값을 갖는 속성을 정의하기 위한 도메인\n여부, 유무\n\n\n\n\n\n\n\n\n\n\n[날짜] 도메인그룹은 특정 사건이 일어난 시점 또는 시점과 시점간의 정해진 기간을 표현하기 위한 도메인들을 그룹화하여 관리한다.\n\n\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n데이터타입\n길이\n유효값 범위\n예시\n\n\n\n\n일\nDD형태의 데이터 값을 갖는 도메인\nVARCHAR2\n2\n01~31\n계약시작 일\n\n\n시각\nHHMISS 형태의 시각을 나타내는 도메인\nVARCHAR2\n6\n000000~235959\n차량사용시작 시각\n\n\n일자\nYYYYMMDD형태의 데이터 값을 갖는 도메인\nVARCHAR2\n8\n00010101 ~ 99991231\n계약해지 일자\n\n\n생년월일\nYYMMDD형태의 데이터 값을 갖는 도메인\nVARCHAR2\n6\n000101 ~ 991231\n회원 생년월일\n\n\n일시\nYYYYMMDDHH24MISS형태의 데이터 값을 갖는 도메인\nVARCHAR2\n16\n\n가격정책등록 일시\n\n\n타임스템프\nYYYYMMDDHH24MISSFF3형태의 데이터 값을 갖는 도메인\nTIMESTAMP\n20\n\n\n\n\n년도\nYYYY형태의 데이터 값을 갖는 도메인\nVARCHAR2\n4\n0001 ~ 9999\n기준 년도\n\n\n년월\nYYYYMM형태의 데이터 값을 갖는 도메인\nVARCHAR2\n6\n\n지불 년월\n\n\n월\nMM형태의 데이터 값을 갖는 도메인\nVARCHAR2\n2\n\n청구 월\n\n\n\n\n년월일 형식은 ‘YYYYMMDD’ 로 통일하며, 도메인명은 ‘일자’ 로 정의하여 사용한다\n\n(예시) 계약해지 일자 : 20160420\n\n‘YYYY’ 형식의 도메인명은 ‘년도’ 로 정의하여 사용한다\n\n(예시) 출생 년도 : 1966, 기준 년도 : 2016\n\n‘YYYYMM’ 형식의 도메인명은 ‘년월’ 로 정의하여 사용한다\n\n(예시) 포인트적립 년월 : 201604\n\n‘MM’ 형식의 도메인명은 ‘월’ 로 정의하여 사용한다\n\n(예시) 결산 월 : 04\n\n‘MMDD’ 형식의 도메인명은 ‘월일’ 로 정의하여 사용한다\n\n(예시) 기혼기념 월일 : 1010\n\n‘DD’ 형식의 도메인명은 ‘일’ 로 정의하여 사용한다\n\n(예시) 자동이체지정 일 : 25\n\n년월일+시분초 형식의 도메인명은 ‘일시’ 로 정의하여 사용한다. ‘일시’ 도메인의 DataType은 Date 와 Variable Character의 두 가지를 지원한다\n시스템 로그일시(YYYYMMDDTTMMSSFF3)를 표시하기 위해서는 타임스탬프 도메인을 사용한다. DataType은 Timestamp 형태이다\n\n\n\n\n\n[명] 도메인그룹은 객체에 대한 식별을 표현하기 위한 도메인들을 그룹화하여 관리한다.\n사람을 제외한 포함한 모든 명칭은 ‘명’ 을 도메인으로 정의하여 사용하며, 사람은 ’성명’을 사용한다.\n‘명’ 도메인을 속성(컬럼)에 사용할 경우 도메인 앞에 ‘한글’,’영문’,‘한자’,‘약어’ 등과 같은 수식어가 생략된 경우는 ‘한글’+’명을 ’명’으로 갈음한다\n\n(예시) 종목 명(한글), 종목 영문 명(영문)\n\n사람이 살고 있는 곳이나 기관, 회사 따위가 자리 잡고 있는 곳을 행정 구역으로 나타낸 이름은 ‘주소’ 를 도메인으로 정의하여 사용한다\n’명’은 단독으로 사용하지 않고 사전을 참조하여 복합어로 생성한다\n예시\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n명\n상품, 사물 등을 식별하기 위한 명칭에는 ‘명’ 도메인을 사용한다\n상품 명\n\n\n\n성명\n사람의 성명을 관리할 경우 ’성명’이라는 도메인을 사용한다\n고객 영문성명\n\n\n\n주소\n사람이 살고 있는 곳이나 기관, 회사 따위가 자리 잡고 있는 곳을 행정 구역으로 나타낸 이름은 ‘주소’ 도메인을 사용한다\n직장 주소\n\n\n\n이메일주소\n전자우편주소\n수신자 이메일주소\n\n\n\nID\n시스템 오브젝트 등을 식별하기 위해 사용되는 도메인으로서 체계 없이 순차적으로 체번하여 사용하는 일련번호와는 구별하여 사용한다\n스마트빌 ID\n\n\n\n\n\n\n\n\n“내용” 단일 도메인으로 정의 한다.\n“명세”, “설명”, ”비고”, “적요”, “내역”, “의견”, “사유”, “사항” 등 유사 도메인은 별도로 정의하지 않고 “내용” 도메인으로 통합관리 한다.\n\n(예시) 평가자 의견 (X) → 평가자 의견 내용 (O)\n\n’값’은 단일단어로 허용하지 않고, 사전을 참조하여 복합어로 생성한다.\n예시\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n내용\n사실이나 사물에 대해 전하고자 하는 정보를 형식 없이 서술형으로 저장하는 경우 사용된다.\n사고 내용\n\n\n\n값\n평가값, 항목값 등 값을 의미하는 용어에 대해 값 도메인을 사용한다.\n항목 값, 입력 값\n\n\n\n\n\n\n\n\n[수] 도메인그룹은 객체의 개수나 양을 수로써 표현하기 위한 도메인들을 그룹화하여 관리한다.\n[수 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n수\n셀 수 있는 사물의 크기를 나타내는 값(복합어로 생성)\n종업원 수\n\n\n\n일수\n일을 기준으로 헤아리는 수.\n연체 일수\n\n\n\n개월수\n월을 기준으로 헤아리는 수\n견적 개월수\n\n\n\n년수\n년을 기준으로 헤아리는 수\n근무 년수\n\n\n\n매수\n종이나 유리 따위의 장으로 셀 수 있는 물건의 수효\n기본 매수\n\n\n\n개수\n한 개씩 낱으로 셀 수 있는 물건의 수효\n보유 개수\n\n\n\n건수\n사물이나 사건의 가짓수\n조회 건수\n\n\n\n횟수\n돌아오는 차례의 수효\n기존인출 횟수\n\n\n\n점수\n성적을 나타내는 숫자\n평가대상 점수\n\n\n\n연령\n나이, 사람이 세상에 나서 현재 또는 기준이 되는 때까지 살아 온 햇수\n보험 연령\n\n\n\n수량\n수와 량이 혼재된 수량을 자연수로 표현한 수\n수량\n\n\n\n\n\n금액을 제외한 정보의 수치 및 합계 등을 정의하는 경우에 사용한다\n\n(예시) 고객 수, 연체 일수, 거래 량\n\n되풀이되는 일이나 차례의 수효를 나타내는 경우는 ’횟수’를 도메인으로 정의하여 사용한다.\n\n(예시) 지로자동이체 횟수\n\n나이와 관련된 용어는 ‘연령’ 을 도메인으로 정의하여 사용한다\n\n(예시) 보험 연령\n\n기간을 나타내는 용어의 경우 ‘기간’을 사용하지 않고, ‘년수’, ‘개월수’, ‘일수’ 등으로 구체 적으로 정의한다\n\n(예시) 대출 년수, 연장 개월수, 연체 일수\n\n\n\n\n\n\n[율] 도메인그룹은 둘 이상의 수를 비교하여 그 중 하나의 수를 기준으로 하여 나타낸 다른 수의 비교 값을 표현하기 위한 도메인들을 그룹화하여 관리한다.\n‘율’ 또는 ’률＇은 단일단어로 사용하지 않고 사전을 참조하여 복합어로 생성한다\n확률, 비율 등 ‘%’ 로 관리되는 속성에 대해 ‘율/비율’ 을 도메인으로 정의하여 사용한다\n\n(예시) 연체 율, 담보 비율, 적용 비율\n\n[율 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n율\n비율의 뜻을 나타내는 말 “~율”은 모음으로 끝나거나 ‘ㄴ’ 받침을 가진 일부 명사 뒤에 붙임(복합어로 생성)\n할인 율\n\n\n\n이율\n원금에 대한 이자의 비율, 즉 이자 산출에 기초가 되는 비율\n연체 이율\n\n\n\n세율\n과세 표준에 의하여 세금을 계산하여 매기는 법정률\n\n\n\n\n요율\n요금의 정도나 비율\n보증 요율\n\n\n\n금리\n자금의 사용료로 대외적으로 공시되는 기준의 의미로 사용\n대출 금리\n\n\n\n환율\n외국환 시세\n기준 환율\n\n\n\n비율\n둘 이상의 수를 비교하여 나타낼 때 그 중 한 개의 수를 기준으로 하여 나타낸 다른 수의 비교 값\n사고MT 비율\n\n\n\n\n\n\n\n\n[금액] 도메인그룹은 돈의 액수나 화폐 가치를 표현하기 위한 도메인들을 그룹화하여 관리한다\n금액을 의미하는 도메인은 중복해서 사용하는 것을 피한다\n\n(예시) 물품 + 원가(도메인) + 금액(도메인) → 물품 + 원가(도메인)\n\n기본적으로 ‘금액’ 도메인으로 표현될 수 있는 속성에 대해서는 ‘금액’ 도메인의 사용을 권장한다. 하지만 관용적으로 ‘금’ 도메인이 사용되는 표현에는 별도의 도메인으로 분류하여 사용한다\n합산금액을 의미하는 총계, 합계, 누계 등의 수식어는 반드시 금액 앞에 붙인다\n\n(예시) 감가상각누계금액, 자산총계금액\n\n부득이하게 ’금＇을 사용해야 하는 경우 복합어 생성한다\n[금액 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n\n도메인\n세부 도메인\n설명\n예시\n비고\n\n\n\n\n요금\n료\n대여료, 렌탈료, 리스료 등을 나타내는 금액 단위(복합어로 생성)\n수수 료, 과태 료\n\n\n\n금액\n금액\n돈의 액수\n고객청구 금액\n\n\n\n금액\n잔액\n남은 금액\n연체이자 잔액\n\n\n\n세\n세\n조세의 액수(복합어로 생성)\n부가 세\n\n\n\n가격\n단가\n물건 한 단위(單位)의 가격\n계약 단가\n\n\n\n가격\n원가\n상품의 제조 판매 배급 따위에 든 재화와 용역을 단위에 따라 계산한 가격\n물품 원가\n\n\n\n\n\n\n\n\n[번호] 도메인그룹은 일정한 체계를 가지거나 특정 자리에 존재하는 의미를 표현하기 위한 도메인들을 그룹화하여 관리한다.\n번호도메인의 공통 도메인은 생성하지 않는다.\n‘번호’ 도메인은 일정한 체계를 가지거나 특정 자리의 의미가 존재하는 속성을 정의할 때 사용하는 것을 원칙으로 한다.\n번호 자체에 특별한 의미를 가진 경우 해당 번호를 번호도메인으로 정의하여 사용한다\n\n(예시) 주민등록번호, 사업자등록번호, 전화번호, 법인등록번호 등\n\n순차적으로 채번되는 번호는 ‘일련번호’ 도메인으로 정의하여 사용한다.\n번호도메인의 공통인 도메인은 생성하지 않는다.\n\n(예시) 번호N10 (Number(10))과 같이 분류어가 번호인 용어의 도메인이 공통적으로 쓰는 도메인 생성 금지.\n\n[번호 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n전화번호\n가입된 전화마다 매겨져 있는 일정한 번호\n계약담당 전화번호\n\n\n\n우편번호\n우편물을 쉽게 분류하기 위하여 정보 통신부에서 각 지역마다 매긴 번호\n계약자 우편번호\n\n\n\n주민등록번호\n주민등록을 할 때에, 국가에서 국민에게 부여하는 고유 번호\n계약자 주민등록번호\n\n\n\n사업자등록번호\n세무에서, 신규로 개업하는 사업자에게 부여하는 사업체의 고유번호이다\n공급자 사업자등록번호\n\n\n\n법인등록번호\n사무소의 소재지에서 설립등기(設立登記)를 함으로써 성립하는데 이때 부여된 일련번호이다\n~ 법인등록번호\n\n\n\n계좌번호\n금융 기관에 예금하려고 설정한 개인명이나 법인명의 계좌에 부여된 번호\n입금 계좌번호\n\n\n\n휴대전화번호\n지니고 다니면서 걸고 받을 수 있는 소형 무선 전화기 번호\n\n\n\n\n비밀번호\n본인임을 확인하기 위해 설정한 암호\n\n\n\n\n신용카드번호\n신용카드식별번호\n~ 신용카드번호\n\n\n\n직원번호\n회사 직원 식별번호\n~ 사원번호\n\n\n\n여권번호\n외국을 여행하는 사람의 신분이나 국적을 증명하고 상대국에 그 보호를 의뢰하는 문서번호.\n고객 여권번호\n\n\n\n외국인등록번호\n외국인등록번호\n외국인등록번호\n\n\n\n일련번호\n일률적으로 연속되어 있는 번호\n구성품 일련번호\n\n\n\n\n\n\n\n\n코드는 다른 도메인들과 달리 특정 도메인 값(즉, 코드값)과 이 값에 대한 의미(즉, 코드값명)를 표현하기 위해 코드도메인을 그룹화하여 표준코드로 분류하여 별도로 관리한다\n코드값을 가지는 속성은 반드시 ‘코드’ 를 도메인으로 정의하여 사용한다\n속성명은 해당 속성이 사용하는 코드도메인명과 일치시키는 것을 원칙으로 하나, 속성명에 수식어를 붙여서 사용할 수 있다\n\n(예시) 코드도메인 ‘거래코드’ : 속성명 → 거래코드(O), 수신거래코드(O)\n\n코드의 의미가 ‘여부’ 또는 ‘유무’ 인 경우 ‘코드’를 도메인으로 사용할 수 없고, ‘여부/유무’ 를 도메인으로 정의하여 사용한다\n코드도메인은 ‘수식어 + 코드유형수식어 + 코드’ 로 명명한다\n[코드 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n도메인\n설명\n비고\n\n\n\n\n가상계좌은행코드\n[가상계좌은행코드] 코드는 회사 전사에서 사용하는 가상계좌 은행코드\n\n\n\n제품카테고리코드\n[제품카테고리코드] 코드는 제품의 카테코리에 대하여 조회할 수 있도록 구분하는 코드\n\n\n\n주차장코드\n[주차장구분코드] 코드는 자산 제고의 위치를 구분하는 코드\n\n\n\n수리항목코드\n[수리항목코드] 수리항목 목록을 구분하는 코드\n\n\n\n\n\n\n\n\n\n“여부” 도메인의 인스턴스는 반드시 ‘Y’ 또는 ‘N’ 만 허용되며, NULL, N/A, SAPCE 등의 값은 허용하지 않는다.\n“유무” 도메인의 인스턴스는 반드시 ‘Y’ 또는 ‘N’ 만 허용되며, NULL, N/A, SAPCE 등의 값은 허용하지 않는다.\n“여부/유무” 도메인명은 ‘Y/N’ 이외의 값을 허용하지 않으므로, 기타 값을 사용해야 하는 경우는 코드도메인으로 분류하여 사용하고, 특히 미 정의된 값은 속성의 인스턴스로 사용할 수 없으며, ‘미정의’ 값을 표현하기 위해서는 ’*’ (미정의)와 같이 반드시 대체값을 정의하여 인스턴스로 사용한다. (Not Null)\n단, 대외 인터페이스 테이블의 경우 업무 특성에 따라 Null을 에외로 허용할 수 있다\n[분류 도메인그룹] 도메인 상세\n\n\n\n\n\n\n\n\n\n\n도메인\n설명\n예시\n비고\n\n\n\n\n여부\n특정 사실이나 행위의 ’그러함/그러하지 아니함’을 의미\n반출 여부\n\n\n\n유무\n존재나 소유의 ’있음/없음’을 의미\n보증금 유무\n\n\n\n\n\n이 두 도메인을 명확히 구분하여 사용하면 데이터의 의미를 더 정확하게 전달할 수 있다.\n\n예를 들어, ’계약 여부’는 계약이 체결되었는지 아닌지를 나타내는 반면, ’계약서 유무’는 물리적인 계약서 문서가 존재하는지 않는지를 나타낼 수 있다.\n\n\n\n\n\n\n\n도메인 등록 기준\n\n\n\n\n\n\n\n\n\n순번\n원칙\n예시\n\n\n\n\n1\n• 속성 데이터가 규칙을 가지는 경우 도메인 명으로 지정한다.• 속성값의 자릿수가 항상 일정해야 하는 경우• 특정 자릿수의 데이터가 의미를 가지는 경우• 속성값 데이터 내의 규칙이 존재하는 경우\n• 고객번호는 20자리• 사업자등록번호, 여권번호, 법인등록번호\n\n\n2\n속성 데이터 값의 유효값이나 유효 범위의 제한이 있는 경우 도메인으로 지정한다.\n• 분기 – 1,2,3,4 만 유효함• 일(Day) – 1~31만 유효함\n\n\n3\n속성값의 성격을 식별하고자 하는 경우 도메인 명으로 지정한다\n지점구분코드\n\n\n4\n속성명 명명 시에 합성어 등이 관용적으로 사용되어 분리하여 사용하기 힘든 경우 도메인 명으로 지정한다.\n잔액, 특소세\n\n\n5\n코드값 체계를 가지는 모든 코드는 도메인 명으로 지정한다\n은행코드, 부품코드\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n도메인 그룹\n도메인\n인포타입\n데이터 타입\n길이\n설명\n예시\n\n\n\n\n날짜\n일자\n일자VC8\nVARCHAR2\n8\nYYYYMMDD 형식의 날짜\n20230501\n\n\n날짜\n일시\n일시VC14\nVARCHAR2\n14\nYYYYMMDDHHMMSS 형식의 날짜와 시간\n20230501143000\n\n\n명칭\n성명\n성명VC50\nVARCHAR2\n50\n개인의 이름\n홍길동\n\n\n명칭\n상품명\n명VC100\nVARCHAR2\n100\n상품의 이름\n스마트폰 갤럭시 S23\n\n\n내용\n내용\n내용VC4000\nVARCHAR2\n4000\n자유 형식의 텍스트\n이 제품은 최신 기술을 적용한…\n\n\n수량\n수량\n수량N10\nNUMBER\n10\n물품의 개수\n1000\n\n\n율\n이율\n이율N5_2\nNUMBER\n5,2\n비율을 나타내는 수치 (소수점 2자리)\n3.75\n\n\n금액\n금액\n금액N15\nNUMBER\n15\n화폐 금액\n10000000\n\n\n번호\n전화번호\n전화번호VC20\nVARCHAR2\n20\n전화번호 형식\n010-1234-5678\n\n\n코드\n상품코드\n코드VC10\nVARCHAR2\n10\n상품을 구분하는 고유 코드\nPRD0001234\n\n\n분류\n여부\n여부VC1\nVARCHAR2\n1\nY/N으로 표현되는 여부\nY"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-4.data_code.html",
    "href": "docs/blog/posts/Governance/5-4.data_code.html",
    "title": "Data Governance Study - Data Standard Code",
    "section": "",
    "text": "데이터 표준 코드는 특정 개념이나 항목을 나타내기 위해 일관되게 사용되는 약속된 값의 집합이다.\n\n즉, 코드란 활용하고자 하는 데이터를 약어 혹은 기호로 함축하여 사용하는 데이터를 말한다.\n\n도메인의 한 유형으로서, 속성(컬럼)에 허용된 데이터 값을 제한된 범위 내에서 구체적으로 열거하여 정의한 것을 지칭한다\n이 데이터 값을 코드값 또는 코드 유효값이라 하며 각각의 코드값에는 의미를 부여한다. 이 의미를 ‘코드값명’ 또는 ’코드유효값정의’라 한다\n예시\n\n성별 코드: M (남성), F (여성)\n은행 코드: 004 (국민은행), 020 (우리은행)\n국가 코드: KR (대한민국), US (미국)\n\n코드명은 “국가코드”이다.\n코드값(코드 유효값)은 ISO 3166-1 alpha-2 표준을 따르는 2자리 국가 코드이다.\n코드값 명은 해당 국가의 한글 명칭이다.\n코드 유효값 정의(설명)는 모든 코드가 ISO 3166-1 alpha-2 기준을 따르며, 영문 대문자 2자리로 구성됨을 명시한다..\n\n\n\n\n\n\n\n\n\n\n\n코드명\n코드값\n코드값 명\n코드 유효값 정의\n\n\n\n\n국가코드\nKR\n대한민국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nUS\n미국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nJP\n일본\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nCN\n중국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nGB\n영국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nDE\n독일\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nFR\n프랑스\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n\n\n\n\n\n데이터의 일관성 유지\n시스템 간 데이터 교환 용이\n데이터 해석의 명확성 제공\n\n전사적으로 표준코드를 사용하도록 함으로써 업무영역간 운영데이터의 불일치를 방지하고 데이터의 정합성을 향상 시킨다.\n\n데이터 입력 오류 감소\n\n\n\n\n\n고유성: 각 코드는 유일한 의미를 가짐\n\n코드의 중복방지(유사한 코드 검토 포함) 통해 효율적 운영이 가능해야 한다\n\n간결성: 일반적으로 짧고 간단한 형태\n체계성: 논리적인 구조를 가짐\n확장성: 새로운 항목 추가가 가능한 구조\n\n\n\n\n\n코드 값: 실제 사용되는 코드\n코드 명: 코드가 나타내는 항목의 이름\n설명: 코드의 의미나 사용 목적\n유효 기간: 코드의 사용 가능 기간\n\n\n\n\n\n코드 관리 시스템 구축\n정기적인 검토 및 업데이트\n코드 변경 이력 관리\n\n\n\n\n\n업계 표준이나 국제 표준 고려\n코드의 의미가 시간이 지나도 변하지 않도록 설계\n코드 체계의 일관성 유지\n\n\n\n\n\n표준코드 작성 시 의미를 충분히 파악할 수 있도록 작성을 하며 구성은 표준용어 작성 기준 및 관리원칙을 따른다.\n\n\n\n\n\n\n\n\n\n순번\n코드 표준화 대상 및 관리 원칙\n비고\n\n\n\n\n1\n• 코드 정보가 저장되며 코드 테이블에 그 내용이 존재하는 경우\n표준화 대상\n\n\n2\n• 애플리케이션 내부에 코드의 실제 내용이 존재하는 경우\n표준화 대상\n\n\n3\n• 사용 가능한 데이터의 종류가 2개 이상인 경우\n표준화 대상\n\n\n4\n• Yes or No 값 외에 미확정 값(Null)을 가질 수 있는 경우\n표준화 대상\n\n\n5\n• 현재는 Yes or No 처럼 Boolean값을 갖지만 추후 그 이외의 데이터가 추가 될 가능성이 있는 경우\n표준화 대상\n\n\n6\n• Yes or No 처럼 Boolean 값만을 데이터로 가질 경우 ‘Y’/’N’으로 통일 함\n관리원칙\n\n\n7\n• 표준코드도메인은 관용적으로 사용하는 용어를 우선적으로 사용한다\n관리원칙\n\n\n8\n• 표준코드를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n관리원칙\n\n\n9\n• 단일 코드는 하나의 공통 엔티티로 관리한다.\n관리원칙\n\n\n10\n• 계층코드는 내용을 분석하여 단일코드 형태로 변경 조정 할 수 있다.\n관리원칙\n\n\n11\n• 목록성 코드의 인스턴스 값은 각각 별도의 엔티티로 관리하며 공통코드 엔티티에서는 해당 코드값을 관리하는 테이블 정보를 관리한다.\n관리원칙\n\n\n\n\n\n\n\n표준코드는 신규 모델링 시 데이터 모델 관리자 또는 응용팀에서 도출 신청 후 데이터 표준 담당자가 최종 관리한다. 죄송합니다. 제가 이해를 돕기 위해 추가 설명을 드리겠습니다. 귀하께서 언급하신 내용은 코드 관리 프로세스의 중요한 부분을 강조하고 있습니다. 이를 반영하여 테이블을 수정해 보겠습니다.\n\n\n\n\n\n\n\n\n\n\n순번\n담당\n코드 표준 관리 담당 별 역할\n비고\n\n\n\n\n1\n응용팀\n• 기능 정의시 데이터 항목에 코드가 필요한 경우 모델러와 협의• 목록성 코드에 대한 요건 제시• 각 업무영역별로 생성된 목록성 코드에 대한 코드값 관리\n운영시 코드 신청은 업무담당자(현업)가 수행함현업: 코드 신청, 활용\n\n\n2\n데이터 모델관리자\n• 단일코드, 계층코드 등 공통코드를 관리하기 위한 테이블 설계• 업무영역별 목록성 코드 테이블 설계\n\n\n\n3\n표준담당자\n• 단일코드/계층코드 신청을 위한 템플릿 제공• 코드명에 대한 표준 준수 검증• 코드 취합/조정 및 공통코드 확정• 코드 중복 조정 작업 수행(인스턴스명 간 유사성 검증)• 데이터 타입 검증(코드 도메인화)• AS-IS 코드와 매핑 정보 관리• 코드 등록, 공통 코드 및 코드 도메인 관리(메타시스템 or Excel)• 신규 코드 생성 및 AS-IS 코드의 코드 값에 대한 재정비 수행• 코드에 대한 Ownership 관리 및 승인\n표준담당자: 코드 등록 및 관리\n\n\n\n\n응용팀(현업)은 업무 수행 중 필요한 코드를 식별하고 신청\n\n응용팀(현업)의 비고 항목에 “현업: 코드 신청, 활용”은 실제 업무를 수행하는 현업 담당자가 코드를 신청하고 사용한다\n\n표준담당자는 이 신청을 검토하고, 적절한 경우 코드를 등록하며, 전체적인 코드 체계를 관리한다.\n\n표준담당자의 비고 항목에 “표준담당자: 코드 등록 및 관리”는 표준담당자가 신청된 코드를 검토하고, 실제로 시스템에 등록하며, 지속적으로 관리한다는 점을 명확히 한다.\n\n\n\n\n\n\n코드의 구성에 따른 유형으로는 단일, 계층, 목록, 복합코드가 있으며, 내용은 다음과 같다.\n\n\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n단일코드(S)\n• [코드값] + [코드내용]의 형태를 갖추는 가장 일반적인 형태의 코드로서 한 개의 코드로 Key가 구성됨• 단일코드의 코드값은 시스템에 등록/관리하며, 등록된 단일코드(코드명, 코드값, 코드값 한글정의)는 프로젝트 내 공통코드 테이블의 형태로 만들어져 전사공통으로 활용됨\n\n\n\n계층코드(C)\n• 하나 이상의 코드를 상속받거나 계층 구조를 통해 생성되어진 코드로 Key 가 구성된 경우• 대분류 / 중분류 / 소분류 와 같은 분류체계를 가짐\n\n\n\n목록코드(L)\n• 목록성코드는 코드명, 코드값, 코드한글정의 외에 부가적인 정보를 관리해야 하는 코드를 의미하며, 해당 업무팀에서 테이블의 형태로 관리한다\n\n\n\n복합코드(M)\n• 두개이상의 코드도메인을 하나의 코드도메인에서 활용하기 위하여 구성.• 복합코드는 단일코드의 코드도메인을 관리함.\n\n\n\n\n\n각 코드 유형(단일코드, 계층코드, 목록코드, 복합코드)의 특성과 용도를 명확히 구분하고 있다.\n예시\n\n단일코드(S)\n\n단일코드(S) 유형: 각 코드가 하나의 고유한 의미를 가진다.\n코드값(고객 구분 코드)은 숫자로 구성되어 있으며, 일반적으로 2자리 숫자를 사용한다.\n코드명(고객 구분 명)은 해당 코드의 의미를 명확하게 설명한다.\n사용 조건\n\n간단하고 평면적인 분류가 필요할 때\n코드 값과 의미가 1:1로 대응될 때\n코드의 수가 제한적이고 변경이 적을 때\n\n예시: 고객 구분 코드, 성별코드, 결혼여부코드, 직급코드\n\n\n\n\n고객 구분 코드\n고객 구분 명\n\n\n\n\n01\n개인\n\n\n02\n법인\n\n\n03\n개인사업자\n\n\n04\n외국인\n\n\n05\n공공기관\n\n\n06\n비영리단체\n\n\n07\nVIP\n\n\n08\n임직원\n\n\n09\n제휴사\n\n\n10\n기타\n\n\n\n계층코드(C)\n\n하나 이상의 코드를 상속받거나 계층 구조를 통해 생성되어진 코드로 Key 가 구성된 경우\n대분류 / 중분류 / 소분류 와 같은 분류체계를 가짐\n\n정규화: 각 분류 수준이 별도의 테이블로 분리되어 있어 데이터 중복이 최소화된다.\n참조 무결성: 외래 키 관계를 통해 데이터의 일관성이 유지된다.\n유연성: 각 분류 수준에서 독립적으로 항목을 추가, 수정, 삭제할 수 있다.\n확장성: 새로운 분류 항목을 쉽게 추가할 수 있다.\n\n각 분류 수준에 대한 추가 정보(예: 생성일, 수정일, 설명 등)를 쉽게 추가할 수 있는 장점이 있다.\n\n쿼리 효율성: 필요에 따라 조인을 통해 전체 계층 구조를 조회하거나, 특정 수준만 조회할 수 있다.\n\n사용 조건\n\n데이터가 계층적 구조를 가질 때\n상위 개념과 하위 개념의 관계를 표현해야 할 때\ndrill-down 분석이 필요한 경우\n예시: 조직코드, 상품분류코드, 지역코드\n\n\n대분류 코드 테이블:\n\n\n\n\n대분류 코드\n대분류명\n\n\n\n\nA\n전자제품\n\n\nB\n가전제품\n\n\nC\n의류\n\n\n\n\n중분류 코드 테이블:\n\n\n\n\n중분류 코드\n중분류명\n대분류 코드 (FK)\n\n\n\n\nA1\n컴퓨터\nA\n\n\nA2\n휴대폰\nA\n\n\nB1\n주방가전\nB\n\n\nB2\n생활가전\nB\n\n\nC1\n남성복\nC\n\n\nC2\n여성복\nC\n\n\n\n\n소분류 코드 테이블:\n\n\n\n\n소분류 코드\n소분류명\n대분류 코드 (FK)\n중분류 코드 (FK)\n\n\n\n\nA11\n데스크톱\nA\nA1\n\n\nA12\n노트북\nA\nA1\n\n\nA13\n태블릿\nA\nA1\n\n\nA21\n스마트폰\nA\nA2\n\n\nA22\n피처폰\nA\nA2\n\n\nB11\n냉장고\nB\nB1\n\n\nB12\n전자레인지\nB\nB1\n\n\nB21\n청소기\nB\nB2\n\n\nB22\n세탁기\nB\nB2\n\n\nC11\n셔츠\nC\nC1\n\n\nC12\n바지\nC\nC1\n\n\nC21\n원피스\nC\nC2\n\n\nC22\n스커트\nC\nC2\n\n\n\n\n목록코드(L)\n\n코드값(은행코드)과 코드명(은행명) 외에 여러 부가 정보를 포함한다.\n약칭, 영문명, 주소, 전화번호 등 해당 코드와 관련된 상세 정보를 관리한다.\n설립일과 같은 날짜 정보도 포함될 수 있다.\n사용여부와 같은 관리 정보도 포함될 수 있다.\n목록코드는 일반적으로 해당 업무팀에서 직접 관리하며, 시스템 전반에서 참조되어 사용됨\n\n목록코드의 장점\n\n상세 정보 관리: 코드와 관련된 다양한 부가 정보를 함께 관리할 수 있다.\n업무 특성 반영: 특정 업무 영역의 특성을 반영한 정보를 포함할 수 있다.\n데이터 일관성: 코드와 관련된 정보를 중앙에서 관리함으로써 데이터의 일관성을 유지할 수 있다.\n확장성: 필요에 따라 새로운 정보 항목을 쉽게 추가할 수 있다.\n\n\n사용 조건\n\n코드와 함께 추가적인 속성 정보가 필요할 때\n\n업무 요구사항 분석\n\n사용자나 부서가 코드 외에 추가 정보를 자주 요청하는 경우\n코드만으로는 업무 처리에 충분한 정보를 제공하지 못하는 경우\n\n데이터 활용도 검토\n\n보고서나 분석에서 코드 관련 부가 정보가 자주 필요한 경우\n데이터 조인이나 lookup 작업이 빈번하게 발생하는 경우\n\n시스템 통합 요구사항\n\n다른 시스템과 데이터를 교환할 때 코드 외 추가 정보가 필요한 경우\n외부 시스템이나 API가 코드와 관련된 부가 정보를 요구하는 경우\n\n변경 관리 필요성\n\n코드 값이 시간에 따라 변경되거나 이력 관리가 필요한 경우\n코드의 유효 기간이나 사용 상태를 관리해야 하는 경우\n\n복잡한 비즈니스 로직\n\n코드를 기반으로 복잡한 비즈니스 규칙이나 계산이 필요한 경우\n코드에 따라 다른 처리 로직이 적용되어야 하는 경우\n\n\n사용자 인터페이스 요구사항\n\n코드 선택 시 사용자에게 추가 정보를 제공해야 하는 경우\n코드 검색이나 필터링 시 다양한 기준이 필요한 경우\n\n감사 및 규제 요구사항\n\n코드 사용에 대한 상세한 이력이나 근거를 유지해야 하는 경우\n규제 준수를 위해 코드와 관련된 부가 정보를 관리해야 하는 경우\n\n\n코드 정보가 자주 변경되거나 확장될 가능성이 있을 때\n코드 정보가 특정 업무 영역에 국한되어 관리될 때\n\n예시: 은행코드, 국가코드, 통화코드\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n은행코드\n은행명\n약칭\n영문명\n본점주소\n대표전화\n설립일\n사용여부\n\n\n\n\n001\n한국은행\n한은\nBank of Korea\n서울특별시 중구 남대문로 39\n02-759-4114\n1950-06-12\nY\n\n\n002\n산업은행\n산은\nKorea Development Bank\n서울특별시 영등포구 은행로 14\n1588-1500\n1954-04-01\nY\n\n\n003\n기업은행\nIBK\nIndustrial Bank of Korea\n서울특별시 중구 을지로 79\n1566-2566\n1961-08-01\nY\n\n\n004\nKB국민은행\n국민\nKB Kookmin Bank\n서울특별시 영등포구 국제금융로8길 26\n1588-9999\n2001-11-01\nY\n\n\n005\n하나은행\n하나\nKEB Hana Bank\n서울특별시 중구 을지로 35\n1599-1111\n1967-01-30\nY\n\n\n007\n수협은행\n수협\nSuhyup Bank\n서울특별시 송파구 오금로 62\n1588-1515\n1962-04-01\nY\n\n\n\n복합코드 (M)\n\n코드 구성: 복합코드는 ’상품카테고리코드’와 ’지역코드’를 조합하여 만들어진다.\n의미 결합: 두 개의 단일 코드의 의미를 결합하여 새로운 의미를 만든다.\n추가 정보: 복합코드에는 단순히 두 코드를 붙인 것 외에도 추가적인 정보(설명, 담당부서, 적용일자 등)를 포함할 수 있다.\n유연성: 새로운 상품 카테고리나 지역이 추가될 때 쉽게 확장할 수 있다.\n복합코드의 장점\n\n데이터 압축: 여러 정보를 하나의 코드로 표현할 수 있다.\n의미 전달: 코드만으로도 여러 차원의 정보를 전달할 수 있다.\n유연한 확장: 기존 단일 코드 체계를 유지하면서 새로운 의미를 부여할 수 있다.\n\n데이터 분석: 복합코드를 분해하여 다양한 관점에서 데이터를 분석할 수 있다.\n\n이러한 복합코드는 조직의 복잡한 구조나 다차원적인 정보를 효율적으로 표현하고 관리하는 데 유용하다.\n\n이러한 목록코드는 일반적으로 해당 업무팀에서 직접 관리하며, 시스템 전반에서 참조되어 사용된다.\n사용 조건\n\n두 개 이상의 독립적인 코드 체계를 조합해야 할 때\n다차원적인 정보를 하나의 코드로 표현해야 할 때\n기존 코드 체계를 유지하면서 새로운 의미를 부여해야 할 때\n\n먼저, 각 단일 코드 도메인을 정의한다.\n예시: 지역별 상품코드, 부서별 프로젝트코드\n\n상품 카테고리 코드 (단일코드)\n\n\n\n\n코드\n카테고리명\n\n\n\n\nA\n전자제품\n\n\nB\n의류\n\n\nC\n식품\n\n\n\n\n지역 코드 (단일코드)\n\n\n\n\n코드\n지역명\n\n\n\n\n01\n서울\n\n\n02\n부산\n\n\n03\n대구\n\n\n\n\n지역별 상품 코드 (복합코드)\n\n\n\n\n복합코드\n상품카테고리코드\n지역코드\n설명\n담당부서\n적용일자\n\n\n\n\nA01\nA\n01\n서울 전자제품\n서울영업1팀\n2023-01-01\n\n\nA02\nA\n02\n부산 전자제품\n부산영업팀\n2023-01-01\n\n\nA03\nA\n03\n대구 전자제품\n대구영업팀\n2023-01-01\n\n\nB01\nB\n01\n서울 의류\n서울영업2팀\n2023-01-01\n\n\nB02\nB\n02\n부산 의류\n부산영업팀\n2023-01-01\n\n\nB03\nB\n03\n대구 의류\n대구영업팀\n2023-01-01\n\n\nC01\nC\n01\n서울 식품\n서울영업3팀\n2023-01-01\n\n\nC02\nC\n02\n부산 식품\n부산영업팀\n2023-01-01\n\n\nC03\nC\n03\n대구 식품\n대구영업팀\n2023-01-01\n\n\n\n\n\n선택 시 고려사항\n\n데이터의 구조: 데이터가 계층적인지, 평면적인지 파악\n확장성: 향후 코드 추가나 변경 가능성 고려\n사용 목적: 데이터 분석, 보고, 시스템 통합 등의 용도 파악\n관리 용이성: 코드 관리의 복잡성과 유지보수 고려\n업무 특성: 특정 업무 도메인의 요구사항 반영\n\n\n\n\n\n\n회사에서 사용하는 표준 코드의 기준 관리항목은 아래와 같다\n표준코드 관리항목 구성\n\n신규 모델링 단계에서 코드 값에 대한 신청은 오프라인으로 수행된다.\n변경 모델링 단계에서 코드 값에 대한 신청은 표준화 담당자를 통해 이루어진다.\n모든 코드는 코드 도메인과 매핑 관계를 가지며 ERP 공통 코드 테이블에 대한 데이터 SYNC 작업이 수행된다\n\n\n\n\n\n유형\n설명\n\n\n\n\n코드구분값\n• 코드목록값 혹은 계층코드일 경우 최상위 코드 목록값\n\n\n코드값\n• 코드 목록에 따른 코드 Value값\n\n\n코드명\n• 코드 도메인명과 동일함\n\n\n코드설명\n• 코드명 설명\n\n\n코드영문명\n• 코드 도메인 영문명과 동일함\n\n\n코드길이\n• 실제 코드 값의 길이\n\n\n코드구분\n• 단일코드/계층코드/목록성코드로 구분함\n\n\n업무구분\n• 코드에 대한 ownership을 가진 담당 업무영역\n\n\n상위코드값\n• 상위 코드 Value값\n\n\n상위코드구분값\n• 상위코드 목록값\n\n\n엔티티명\n• 목록성 코드인 경우 대상 엔티티명\n\n\n테이블명\n• 목록성 코드인 경우 대상 테이블명\n\n\n\n\n\n\n\n[주제어] + [코드 수식어 유형] + 코드 형태로 정의하여 사용한다.\n표준코드 구성 체계\n\n수식어 없이 코드용어 생성 가능\n\n\n\n\n\n\n\n\n\n\n\n분류\n유형\n설명\n예시\n\n\n\n\n기본\n유형\n어떤 비슷한 것들의 본질을 개체로서 나타낸 것, 또는 그것들의 공통되는 성질이나 모양을 정의할 때 사용되는 코드 유형\n거래 유형 코드\n\n\n기본\n분류\n코드 값을 체계화 하여 관리하는 경우 사용하며, 주로 대 / 중 / 소 / 세 등의 분류 체계를 갖는 코드에 대해서 ’분류’를 사용\n제품 소분류 코드\n\n\n기본\n종류\n가급적 사용을 제한하되 ’유형’이나 ’분류’의 사용 시 의미 전달이 모호해질 경우 혹은 통상적으로 사용되는 경우에 한해서 사용\n거래 종류 코드\n\n\n기본\n구분\n따로따로 갈라서 나누는 것으로 ’유형’보다는 단순하고 값의 종류가 10개 이내로 제한적이고 값의 범위가 명확한 경우 사용\n상품항목 구분 코드\n\n\n기본\n항목\n목록을 나열한 경우에 한해 사용\n점검 항목 코드\n\n\n확장\n사유\n인식 작용, 분석, 종합, 추리, 판단 등의 정신 작용에 대한 근거 및 동기\n취소 사유 코드\n\n\n확장\n상태\n사물이나 현상이 처해 있는 현재의 모양 또는 형편\n계약 상태 코드\n\n\n확장\n관계\n둘 이상의 사람, 사물, 현상 따위가 서로 관련을 맺거나 관련이 있음\n계약자 관계 코드\n\n\n확장\n용도\n사용되는 곳 혹은 사용되는 목적을 정의\n자금 용도 코드\n\n\n확장\n등급\n높고 낮음이나 좋고 나쁨 따위의 차이를 여러 층으로 구분한 단계\n차량 등급 코드\n\n\n확장\n지역\n전체 영역을 어떤 특징으로 나눈 일정한 공간 영역\n등록 지역 코드\n\n\n확장\n단위\n어떤 물리량(物理量)의 크기를 나타낼 때 비교의 기준 되는 크기\n회계 단위 코드\n\n\n\n\n\n\n\n코드값(인스턴스)을 부여하는 방식에 대한 4가지 분류가 있으며 코드의 형식을 결정할 수 있다\n계층 분류형(H)은 조직 구조와 같이 계층적인 관계를 표현하는 데 적합합니다.\n순차 채번형(S)은 순서가 있는 항목들을 나열할 때 유용합니다.\n표준약어 부여형(A)은 국제적으로 통용되는 표준 코드를 사용할 때 적합합니다.\n복합 분류형(C)은 계층 구조와 순차적 번호 부여가 동시에 필요한 경우에 사용됩니다.\n표준코드 구성 체계\n\n[A : Alphabet N : Numeric S : Sequence Number]\n\n\n\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n계층 분류형(H)\n• 대/중/소 등의 분류에 의한 구분이 필요한 경우 적용• 일반적으로 10진 분류 체계로 구성• 코드형식: N + N + N - NN(대분류) + NN(중분류) + NN(소분류) - NN(본/지점 분류) + NN(실/부 분류) + NN(팀 분류)\n[조직구분코드]100000: 본사총괄101000: 기획실101010: 회계팀101020: 자금팀\n\n\n순차 채번형(S)\n• 일련번호와 같이 순차적으로 번호를 부여하며 부여된 자리를 넘지 않도록 구성• 가능한 결번이 없도록 정의함. 코드 길이 만큼을 앞에 0을 채워서 번호 부여(숫자형 문자)• 코드 형식: SS\n[가족구분코드]01: 부02: 모03: 배우자99: 기타\n\n\n표준약어 부여형(A)\n• 대부분 국제 표준 코드 및 국가표준코드, 업종표준코드 등이 이에 속함.• 코드 형식: AAA\n[국가구분코드]CAN: 캐나다CHN: 중국\n\n\n복합 분류형(C)\n• 계층분류와 순차채번이 결합된 형태의 분류• 코드 형식: ASSSSS\n[담보구분코드]A00001: 건물A00002: 토지B00001: 예금\n\n\n\n\n\n\n\n표준코드 구성 체계\n기본 원칙\n\n원칙적으로 회사 렌터카 시스템 구축에서 사용하는 모든 코드는 통합 관리한다\n업무적으로 동일한 의미의 코드나 유사한 코드를 통합 후 표준화된 코드값과 코드내용을 부여한다\n목록성 코드의 경우 참조정보(DB명, 테이블명, 컬럼명) 만 관리하며 별도 코드값, 코드내용을 관리하지 않는다. 코드값과 코드내용 이외에 부가적인 정보가 존재하고, 코드에 따라 부가적인 정보의 개수가 다르기 때문에 표준 코드 테이블에서 관리하기 어렵기 때문이다\n\n코드값(인스턴스) 부여 원칙\n\n코드값의 부여는 원칙적으로 숫자형 문자 형태의 일련번호(01,02..)를 부여한다\n특별한 사유가 없는 한 현업에서 부여한 코드값을 최우선 사용함을 원칙으로 한다\n코드값 부여는 가능한 연속적으로 부여한다\n코드값 길이는 향후 확장성을 고려해서 부여한다\n숫자로만 이루어진 코드는 원칙적으로 허용하지 않으며 코드 길이만큼 숫자형 문자를 이용해서 ’0’을 채워서 코드를 부여한다\n‘기타’, ‘해당없음’ 등의 내용을 갖는 코드는 가급적 사용하지 않는 것을 원칙으로 하되, 반드시 사용해야 할 경우 해당 자리의 ‘00’, ‘99’ 등의 최대값을 이용한다\n’ 여부’, ’유무’의 모든 코드값은 ’Y＇과 ’N＇로 사용된다.\n\n코드값(인스턴스) 부여 원칙 예외\n\n기존 As-Is에서 특별한 의미를 가지는 코드 값으로 사용되었을 경우 그대로 채택한다\n외부에서 정의되어서 표준 약어로 널리 사용되는 있는 코드들은 표준화 대상에서 제외하며, 그대로 사용하도록 한다.\n\n국가구분코드 등"
  },
  {
    "objectID": "docs/blog/posts/Governance/5-4.data_code.html#데이터-표준-코드-사전이란",
    "href": "docs/blog/posts/Governance/5-4.data_code.html#데이터-표준-코드-사전이란",
    "title": "Data Governance Study - Data Standard Code",
    "section": "",
    "text": "데이터 표준 코드는 특정 개념이나 항목을 나타내기 위해 일관되게 사용되는 약속된 값의 집합이다.\n\n즉, 코드란 활용하고자 하는 데이터를 약어 혹은 기호로 함축하여 사용하는 데이터를 말한다.\n\n도메인의 한 유형으로서, 속성(컬럼)에 허용된 데이터 값을 제한된 범위 내에서 구체적으로 열거하여 정의한 것을 지칭한다\n이 데이터 값을 코드값 또는 코드 유효값이라 하며 각각의 코드값에는 의미를 부여한다. 이 의미를 ‘코드값명’ 또는 ’코드유효값정의’라 한다\n예시\n\n성별 코드: M (남성), F (여성)\n은행 코드: 004 (국민은행), 020 (우리은행)\n국가 코드: KR (대한민국), US (미국)\n\n코드명은 “국가코드”이다.\n코드값(코드 유효값)은 ISO 3166-1 alpha-2 표준을 따르는 2자리 국가 코드이다.\n코드값 명은 해당 국가의 한글 명칭이다.\n코드 유효값 정의(설명)는 모든 코드가 ISO 3166-1 alpha-2 기준을 따르며, 영문 대문자 2자리로 구성됨을 명시한다..\n\n\n\n\n\n\n\n\n\n\n\n코드명\n코드값\n코드값 명\n코드 유효값 정의\n\n\n\n\n국가코드\nKR\n대한민국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nUS\n미국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nJP\n일본\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nCN\n중국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nGB\n영국\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nDE\n독일\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n국가코드\nFR\n프랑스\nISO 3166-1 alpha-2 기준, 영문 대문자 2자리\n\n\n\n\n\n\n\n데이터의 일관성 유지\n시스템 간 데이터 교환 용이\n데이터 해석의 명확성 제공\n\n전사적으로 표준코드를 사용하도록 함으로써 업무영역간 운영데이터의 불일치를 방지하고 데이터의 정합성을 향상 시킨다.\n\n데이터 입력 오류 감소\n\n\n\n\n\n고유성: 각 코드는 유일한 의미를 가짐\n\n코드의 중복방지(유사한 코드 검토 포함) 통해 효율적 운영이 가능해야 한다\n\n간결성: 일반적으로 짧고 간단한 형태\n체계성: 논리적인 구조를 가짐\n확장성: 새로운 항목 추가가 가능한 구조\n\n\n\n\n\n코드 값: 실제 사용되는 코드\n코드 명: 코드가 나타내는 항목의 이름\n설명: 코드의 의미나 사용 목적\n유효 기간: 코드의 사용 가능 기간\n\n\n\n\n\n코드 관리 시스템 구축\n정기적인 검토 및 업데이트\n코드 변경 이력 관리\n\n\n\n\n\n업계 표준이나 국제 표준 고려\n코드의 의미가 시간이 지나도 변하지 않도록 설계\n코드 체계의 일관성 유지\n\n\n\n\n\n표준코드 작성 시 의미를 충분히 파악할 수 있도록 작성을 하며 구성은 표준용어 작성 기준 및 관리원칙을 따른다.\n\n\n\n\n\n\n\n\n\n순번\n코드 표준화 대상 및 관리 원칙\n비고\n\n\n\n\n1\n• 코드 정보가 저장되며 코드 테이블에 그 내용이 존재하는 경우\n표준화 대상\n\n\n2\n• 애플리케이션 내부에 코드의 실제 내용이 존재하는 경우\n표준화 대상\n\n\n3\n• 사용 가능한 데이터의 종류가 2개 이상인 경우\n표준화 대상\n\n\n4\n• Yes or No 값 외에 미확정 값(Null)을 가질 수 있는 경우\n표준화 대상\n\n\n5\n• 현재는 Yes or No 처럼 Boolean값을 갖지만 추후 그 이외의 데이터가 추가 될 가능성이 있는 경우\n표준화 대상\n\n\n6\n• Yes or No 처럼 Boolean 값만을 데이터로 가질 경우 ‘Y’/’N’으로 통일 함\n관리원칙\n\n\n7\n• 표준코드도메인은 관용적으로 사용하는 용어를 우선적으로 사용한다\n관리원칙\n\n\n8\n• 표준코드를 구성할 때에는 가독성을 높이고, 의미를 명확히 전달하기 위해 수식어를 사용하여 구성하도록 한다.\n관리원칙\n\n\n9\n• 단일 코드는 하나의 공통 엔티티로 관리한다.\n관리원칙\n\n\n10\n• 계층코드는 내용을 분석하여 단일코드 형태로 변경 조정 할 수 있다.\n관리원칙\n\n\n11\n• 목록성 코드의 인스턴스 값은 각각 별도의 엔티티로 관리하며 공통코드 엔티티에서는 해당 코드값을 관리하는 테이블 정보를 관리한다.\n관리원칙\n\n\n\n\n\n\n\n표준코드는 신규 모델링 시 데이터 모델 관리자 또는 응용팀에서 도출 신청 후 데이터 표준 담당자가 최종 관리한다. 죄송합니다. 제가 이해를 돕기 위해 추가 설명을 드리겠습니다. 귀하께서 언급하신 내용은 코드 관리 프로세스의 중요한 부분을 강조하고 있습니다. 이를 반영하여 테이블을 수정해 보겠습니다.\n\n\n\n\n\n\n\n\n\n\n순번\n담당\n코드 표준 관리 담당 별 역할\n비고\n\n\n\n\n1\n응용팀\n• 기능 정의시 데이터 항목에 코드가 필요한 경우 모델러와 협의• 목록성 코드에 대한 요건 제시• 각 업무영역별로 생성된 목록성 코드에 대한 코드값 관리\n운영시 코드 신청은 업무담당자(현업)가 수행함현업: 코드 신청, 활용\n\n\n2\n데이터 모델관리자\n• 단일코드, 계층코드 등 공통코드를 관리하기 위한 테이블 설계• 업무영역별 목록성 코드 테이블 설계\n\n\n\n3\n표준담당자\n• 단일코드/계층코드 신청을 위한 템플릿 제공• 코드명에 대한 표준 준수 검증• 코드 취합/조정 및 공통코드 확정• 코드 중복 조정 작업 수행(인스턴스명 간 유사성 검증)• 데이터 타입 검증(코드 도메인화)• AS-IS 코드와 매핑 정보 관리• 코드 등록, 공통 코드 및 코드 도메인 관리(메타시스템 or Excel)• 신규 코드 생성 및 AS-IS 코드의 코드 값에 대한 재정비 수행• 코드에 대한 Ownership 관리 및 승인\n표준담당자: 코드 등록 및 관리\n\n\n\n\n응용팀(현업)은 업무 수행 중 필요한 코드를 식별하고 신청\n\n응용팀(현업)의 비고 항목에 “현업: 코드 신청, 활용”은 실제 업무를 수행하는 현업 담당자가 코드를 신청하고 사용한다\n\n표준담당자는 이 신청을 검토하고, 적절한 경우 코드를 등록하며, 전체적인 코드 체계를 관리한다.\n\n표준담당자의 비고 항목에 “표준담당자: 코드 등록 및 관리”는 표준담당자가 신청된 코드를 검토하고, 실제로 시스템에 등록하며, 지속적으로 관리한다는 점을 명확히 한다.\n\n\n\n\n\n\n코드의 구성에 따른 유형으로는 단일, 계층, 목록, 복합코드가 있으며, 내용은 다음과 같다.\n\n\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n단일코드(S)\n• [코드값] + [코드내용]의 형태를 갖추는 가장 일반적인 형태의 코드로서 한 개의 코드로 Key가 구성됨• 단일코드의 코드값은 시스템에 등록/관리하며, 등록된 단일코드(코드명, 코드값, 코드값 한글정의)는 프로젝트 내 공통코드 테이블의 형태로 만들어져 전사공통으로 활용됨\n\n\n\n계층코드(C)\n• 하나 이상의 코드를 상속받거나 계층 구조를 통해 생성되어진 코드로 Key 가 구성된 경우• 대분류 / 중분류 / 소분류 와 같은 분류체계를 가짐\n\n\n\n목록코드(L)\n• 목록성코드는 코드명, 코드값, 코드한글정의 외에 부가적인 정보를 관리해야 하는 코드를 의미하며, 해당 업무팀에서 테이블의 형태로 관리한다\n\n\n\n복합코드(M)\n• 두개이상의 코드도메인을 하나의 코드도메인에서 활용하기 위하여 구성.• 복합코드는 단일코드의 코드도메인을 관리함.\n\n\n\n\n\n각 코드 유형(단일코드, 계층코드, 목록코드, 복합코드)의 특성과 용도를 명확히 구분하고 있다.\n예시\n\n단일코드(S)\n\n단일코드(S) 유형: 각 코드가 하나의 고유한 의미를 가진다.\n코드값(고객 구분 코드)은 숫자로 구성되어 있으며, 일반적으로 2자리 숫자를 사용한다.\n코드명(고객 구분 명)은 해당 코드의 의미를 명확하게 설명한다.\n사용 조건\n\n간단하고 평면적인 분류가 필요할 때\n코드 값과 의미가 1:1로 대응될 때\n코드의 수가 제한적이고 변경이 적을 때\n\n예시: 고객 구분 코드, 성별코드, 결혼여부코드, 직급코드\n\n\n\n\n고객 구분 코드\n고객 구분 명\n\n\n\n\n01\n개인\n\n\n02\n법인\n\n\n03\n개인사업자\n\n\n04\n외국인\n\n\n05\n공공기관\n\n\n06\n비영리단체\n\n\n07\nVIP\n\n\n08\n임직원\n\n\n09\n제휴사\n\n\n10\n기타\n\n\n\n계층코드(C)\n\n하나 이상의 코드를 상속받거나 계층 구조를 통해 생성되어진 코드로 Key 가 구성된 경우\n대분류 / 중분류 / 소분류 와 같은 분류체계를 가짐\n\n정규화: 각 분류 수준이 별도의 테이블로 분리되어 있어 데이터 중복이 최소화된다.\n참조 무결성: 외래 키 관계를 통해 데이터의 일관성이 유지된다.\n유연성: 각 분류 수준에서 독립적으로 항목을 추가, 수정, 삭제할 수 있다.\n확장성: 새로운 분류 항목을 쉽게 추가할 수 있다.\n\n각 분류 수준에 대한 추가 정보(예: 생성일, 수정일, 설명 등)를 쉽게 추가할 수 있는 장점이 있다.\n\n쿼리 효율성: 필요에 따라 조인을 통해 전체 계층 구조를 조회하거나, 특정 수준만 조회할 수 있다.\n\n사용 조건\n\n데이터가 계층적 구조를 가질 때\n상위 개념과 하위 개념의 관계를 표현해야 할 때\ndrill-down 분석이 필요한 경우\n예시: 조직코드, 상품분류코드, 지역코드\n\n\n대분류 코드 테이블:\n\n\n\n\n대분류 코드\n대분류명\n\n\n\n\nA\n전자제품\n\n\nB\n가전제품\n\n\nC\n의류\n\n\n\n\n중분류 코드 테이블:\n\n\n\n\n중분류 코드\n중분류명\n대분류 코드 (FK)\n\n\n\n\nA1\n컴퓨터\nA\n\n\nA2\n휴대폰\nA\n\n\nB1\n주방가전\nB\n\n\nB2\n생활가전\nB\n\n\nC1\n남성복\nC\n\n\nC2\n여성복\nC\n\n\n\n\n소분류 코드 테이블:\n\n\n\n\n소분류 코드\n소분류명\n대분류 코드 (FK)\n중분류 코드 (FK)\n\n\n\n\nA11\n데스크톱\nA\nA1\n\n\nA12\n노트북\nA\nA1\n\n\nA13\n태블릿\nA\nA1\n\n\nA21\n스마트폰\nA\nA2\n\n\nA22\n피처폰\nA\nA2\n\n\nB11\n냉장고\nB\nB1\n\n\nB12\n전자레인지\nB\nB1\n\n\nB21\n청소기\nB\nB2\n\n\nB22\n세탁기\nB\nB2\n\n\nC11\n셔츠\nC\nC1\n\n\nC12\n바지\nC\nC1\n\n\nC21\n원피스\nC\nC2\n\n\nC22\n스커트\nC\nC2\n\n\n\n\n목록코드(L)\n\n코드값(은행코드)과 코드명(은행명) 외에 여러 부가 정보를 포함한다.\n약칭, 영문명, 주소, 전화번호 등 해당 코드와 관련된 상세 정보를 관리한다.\n설립일과 같은 날짜 정보도 포함될 수 있다.\n사용여부와 같은 관리 정보도 포함될 수 있다.\n목록코드는 일반적으로 해당 업무팀에서 직접 관리하며, 시스템 전반에서 참조되어 사용됨\n\n목록코드의 장점\n\n상세 정보 관리: 코드와 관련된 다양한 부가 정보를 함께 관리할 수 있다.\n업무 특성 반영: 특정 업무 영역의 특성을 반영한 정보를 포함할 수 있다.\n데이터 일관성: 코드와 관련된 정보를 중앙에서 관리함으로써 데이터의 일관성을 유지할 수 있다.\n확장성: 필요에 따라 새로운 정보 항목을 쉽게 추가할 수 있다.\n\n\n사용 조건\n\n코드와 함께 추가적인 속성 정보가 필요할 때\n\n업무 요구사항 분석\n\n사용자나 부서가 코드 외에 추가 정보를 자주 요청하는 경우\n코드만으로는 업무 처리에 충분한 정보를 제공하지 못하는 경우\n\n데이터 활용도 검토\n\n보고서나 분석에서 코드 관련 부가 정보가 자주 필요한 경우\n데이터 조인이나 lookup 작업이 빈번하게 발생하는 경우\n\n시스템 통합 요구사항\n\n다른 시스템과 데이터를 교환할 때 코드 외 추가 정보가 필요한 경우\n외부 시스템이나 API가 코드와 관련된 부가 정보를 요구하는 경우\n\n변경 관리 필요성\n\n코드 값이 시간에 따라 변경되거나 이력 관리가 필요한 경우\n코드의 유효 기간이나 사용 상태를 관리해야 하는 경우\n\n복잡한 비즈니스 로직\n\n코드를 기반으로 복잡한 비즈니스 규칙이나 계산이 필요한 경우\n코드에 따라 다른 처리 로직이 적용되어야 하는 경우\n\n\n사용자 인터페이스 요구사항\n\n코드 선택 시 사용자에게 추가 정보를 제공해야 하는 경우\n코드 검색이나 필터링 시 다양한 기준이 필요한 경우\n\n감사 및 규제 요구사항\n\n코드 사용에 대한 상세한 이력이나 근거를 유지해야 하는 경우\n규제 준수를 위해 코드와 관련된 부가 정보를 관리해야 하는 경우\n\n\n코드 정보가 자주 변경되거나 확장될 가능성이 있을 때\n코드 정보가 특정 업무 영역에 국한되어 관리될 때\n\n예시: 은행코드, 국가코드, 통화코드\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n은행코드\n은행명\n약칭\n영문명\n본점주소\n대표전화\n설립일\n사용여부\n\n\n\n\n001\n한국은행\n한은\nBank of Korea\n서울특별시 중구 남대문로 39\n02-759-4114\n1950-06-12\nY\n\n\n002\n산업은행\n산은\nKorea Development Bank\n서울특별시 영등포구 은행로 14\n1588-1500\n1954-04-01\nY\n\n\n003\n기업은행\nIBK\nIndustrial Bank of Korea\n서울특별시 중구 을지로 79\n1566-2566\n1961-08-01\nY\n\n\n004\nKB국민은행\n국민\nKB Kookmin Bank\n서울특별시 영등포구 국제금융로8길 26\n1588-9999\n2001-11-01\nY\n\n\n005\n하나은행\n하나\nKEB Hana Bank\n서울특별시 중구 을지로 35\n1599-1111\n1967-01-30\nY\n\n\n007\n수협은행\n수협\nSuhyup Bank\n서울특별시 송파구 오금로 62\n1588-1515\n1962-04-01\nY\n\n\n\n복합코드 (M)\n\n코드 구성: 복합코드는 ’상품카테고리코드’와 ’지역코드’를 조합하여 만들어진다.\n의미 결합: 두 개의 단일 코드의 의미를 결합하여 새로운 의미를 만든다.\n추가 정보: 복합코드에는 단순히 두 코드를 붙인 것 외에도 추가적인 정보(설명, 담당부서, 적용일자 등)를 포함할 수 있다.\n유연성: 새로운 상품 카테고리나 지역이 추가될 때 쉽게 확장할 수 있다.\n복합코드의 장점\n\n데이터 압축: 여러 정보를 하나의 코드로 표현할 수 있다.\n의미 전달: 코드만으로도 여러 차원의 정보를 전달할 수 있다.\n유연한 확장: 기존 단일 코드 체계를 유지하면서 새로운 의미를 부여할 수 있다.\n\n데이터 분석: 복합코드를 분해하여 다양한 관점에서 데이터를 분석할 수 있다.\n\n이러한 복합코드는 조직의 복잡한 구조나 다차원적인 정보를 효율적으로 표현하고 관리하는 데 유용하다.\n\n이러한 목록코드는 일반적으로 해당 업무팀에서 직접 관리하며, 시스템 전반에서 참조되어 사용된다.\n사용 조건\n\n두 개 이상의 독립적인 코드 체계를 조합해야 할 때\n다차원적인 정보를 하나의 코드로 표현해야 할 때\n기존 코드 체계를 유지하면서 새로운 의미를 부여해야 할 때\n\n먼저, 각 단일 코드 도메인을 정의한다.\n예시: 지역별 상품코드, 부서별 프로젝트코드\n\n상품 카테고리 코드 (단일코드)\n\n\n\n\n코드\n카테고리명\n\n\n\n\nA\n전자제품\n\n\nB\n의류\n\n\nC\n식품\n\n\n\n\n지역 코드 (단일코드)\n\n\n\n\n코드\n지역명\n\n\n\n\n01\n서울\n\n\n02\n부산\n\n\n03\n대구\n\n\n\n\n지역별 상품 코드 (복합코드)\n\n\n\n\n복합코드\n상품카테고리코드\n지역코드\n설명\n담당부서\n적용일자\n\n\n\n\nA01\nA\n01\n서울 전자제품\n서울영업1팀\n2023-01-01\n\n\nA02\nA\n02\n부산 전자제품\n부산영업팀\n2023-01-01\n\n\nA03\nA\n03\n대구 전자제품\n대구영업팀\n2023-01-01\n\n\nB01\nB\n01\n서울 의류\n서울영업2팀\n2023-01-01\n\n\nB02\nB\n02\n부산 의류\n부산영업팀\n2023-01-01\n\n\nB03\nB\n03\n대구 의류\n대구영업팀\n2023-01-01\n\n\nC01\nC\n01\n서울 식품\n서울영업3팀\n2023-01-01\n\n\nC02\nC\n02\n부산 식품\n부산영업팀\n2023-01-01\n\n\nC03\nC\n03\n대구 식품\n대구영업팀\n2023-01-01\n\n\n\n\n\n선택 시 고려사항\n\n데이터의 구조: 데이터가 계층적인지, 평면적인지 파악\n확장성: 향후 코드 추가나 변경 가능성 고려\n사용 목적: 데이터 분석, 보고, 시스템 통합 등의 용도 파악\n관리 용이성: 코드 관리의 복잡성과 유지보수 고려\n업무 특성: 특정 업무 도메인의 요구사항 반영\n\n\n\n\n\n\n회사에서 사용하는 표준 코드의 기준 관리항목은 아래와 같다\n표준코드 관리항목 구성\n\n신규 모델링 단계에서 코드 값에 대한 신청은 오프라인으로 수행된다.\n변경 모델링 단계에서 코드 값에 대한 신청은 표준화 담당자를 통해 이루어진다.\n모든 코드는 코드 도메인과 매핑 관계를 가지며 ERP 공통 코드 테이블에 대한 데이터 SYNC 작업이 수행된다\n\n\n\n\n\n유형\n설명\n\n\n\n\n코드구분값\n• 코드목록값 혹은 계층코드일 경우 최상위 코드 목록값\n\n\n코드값\n• 코드 목록에 따른 코드 Value값\n\n\n코드명\n• 코드 도메인명과 동일함\n\n\n코드설명\n• 코드명 설명\n\n\n코드영문명\n• 코드 도메인 영문명과 동일함\n\n\n코드길이\n• 실제 코드 값의 길이\n\n\n코드구분\n• 단일코드/계층코드/목록성코드로 구분함\n\n\n업무구분\n• 코드에 대한 ownership을 가진 담당 업무영역\n\n\n상위코드값\n• 상위 코드 Value값\n\n\n상위코드구분값\n• 상위코드 목록값\n\n\n엔티티명\n• 목록성 코드인 경우 대상 엔티티명\n\n\n테이블명\n• 목록성 코드인 경우 대상 테이블명\n\n\n\n\n\n\n\n[주제어] + [코드 수식어 유형] + 코드 형태로 정의하여 사용한다.\n표준코드 구성 체계\n\n수식어 없이 코드용어 생성 가능\n\n\n\n\n\n\n\n\n\n\n\n분류\n유형\n설명\n예시\n\n\n\n\n기본\n유형\n어떤 비슷한 것들의 본질을 개체로서 나타낸 것, 또는 그것들의 공통되는 성질이나 모양을 정의할 때 사용되는 코드 유형\n거래 유형 코드\n\n\n기본\n분류\n코드 값을 체계화 하여 관리하는 경우 사용하며, 주로 대 / 중 / 소 / 세 등의 분류 체계를 갖는 코드에 대해서 ’분류’를 사용\n제품 소분류 코드\n\n\n기본\n종류\n가급적 사용을 제한하되 ’유형’이나 ’분류’의 사용 시 의미 전달이 모호해질 경우 혹은 통상적으로 사용되는 경우에 한해서 사용\n거래 종류 코드\n\n\n기본\n구분\n따로따로 갈라서 나누는 것으로 ’유형’보다는 단순하고 값의 종류가 10개 이내로 제한적이고 값의 범위가 명확한 경우 사용\n상품항목 구분 코드\n\n\n기본\n항목\n목록을 나열한 경우에 한해 사용\n점검 항목 코드\n\n\n확장\n사유\n인식 작용, 분석, 종합, 추리, 판단 등의 정신 작용에 대한 근거 및 동기\n취소 사유 코드\n\n\n확장\n상태\n사물이나 현상이 처해 있는 현재의 모양 또는 형편\n계약 상태 코드\n\n\n확장\n관계\n둘 이상의 사람, 사물, 현상 따위가 서로 관련을 맺거나 관련이 있음\n계약자 관계 코드\n\n\n확장\n용도\n사용되는 곳 혹은 사용되는 목적을 정의\n자금 용도 코드\n\n\n확장\n등급\n높고 낮음이나 좋고 나쁨 따위의 차이를 여러 층으로 구분한 단계\n차량 등급 코드\n\n\n확장\n지역\n전체 영역을 어떤 특징으로 나눈 일정한 공간 영역\n등록 지역 코드\n\n\n확장\n단위\n어떤 물리량(物理量)의 크기를 나타낼 때 비교의 기준 되는 크기\n회계 단위 코드\n\n\n\n\n\n\n\n코드값(인스턴스)을 부여하는 방식에 대한 4가지 분류가 있으며 코드의 형식을 결정할 수 있다\n계층 분류형(H)은 조직 구조와 같이 계층적인 관계를 표현하는 데 적합합니다.\n순차 채번형(S)은 순서가 있는 항목들을 나열할 때 유용합니다.\n표준약어 부여형(A)은 국제적으로 통용되는 표준 코드를 사용할 때 적합합니다.\n복합 분류형(C)은 계층 구조와 순차적 번호 부여가 동시에 필요한 경우에 사용됩니다.\n표준코드 구성 체계\n\n[A : Alphabet N : Numeric S : Sequence Number]\n\n\n\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n계층 분류형(H)\n• 대/중/소 등의 분류에 의한 구분이 필요한 경우 적용• 일반적으로 10진 분류 체계로 구성• 코드형식: N + N + N - NN(대분류) + NN(중분류) + NN(소분류) - NN(본/지점 분류) + NN(실/부 분류) + NN(팀 분류)\n[조직구분코드]100000: 본사총괄101000: 기획실101010: 회계팀101020: 자금팀\n\n\n순차 채번형(S)\n• 일련번호와 같이 순차적으로 번호를 부여하며 부여된 자리를 넘지 않도록 구성• 가능한 결번이 없도록 정의함. 코드 길이 만큼을 앞에 0을 채워서 번호 부여(숫자형 문자)• 코드 형식: SS\n[가족구분코드]01: 부02: 모03: 배우자99: 기타\n\n\n표준약어 부여형(A)\n• 대부분 국제 표준 코드 및 국가표준코드, 업종표준코드 등이 이에 속함.• 코드 형식: AAA\n[국가구분코드]CAN: 캐나다CHN: 중국\n\n\n복합 분류형(C)\n• 계층분류와 순차채번이 결합된 형태의 분류• 코드 형식: ASSSSS\n[담보구분코드]A00001: 건물A00002: 토지B00001: 예금\n\n\n\n\n\n\n\n표준코드 구성 체계\n기본 원칙\n\n원칙적으로 회사 렌터카 시스템 구축에서 사용하는 모든 코드는 통합 관리한다\n업무적으로 동일한 의미의 코드나 유사한 코드를 통합 후 표준화된 코드값과 코드내용을 부여한다\n목록성 코드의 경우 참조정보(DB명, 테이블명, 컬럼명) 만 관리하며 별도 코드값, 코드내용을 관리하지 않는다. 코드값과 코드내용 이외에 부가적인 정보가 존재하고, 코드에 따라 부가적인 정보의 개수가 다르기 때문에 표준 코드 테이블에서 관리하기 어렵기 때문이다\n\n코드값(인스턴스) 부여 원칙\n\n코드값의 부여는 원칙적으로 숫자형 문자 형태의 일련번호(01,02..)를 부여한다\n특별한 사유가 없는 한 현업에서 부여한 코드값을 최우선 사용함을 원칙으로 한다\n코드값 부여는 가능한 연속적으로 부여한다\n코드값 길이는 향후 확장성을 고려해서 부여한다\n숫자로만 이루어진 코드는 원칙적으로 허용하지 않으며 코드 길이만큼 숫자형 문자를 이용해서 ’0’을 채워서 코드를 부여한다\n‘기타’, ‘해당없음’ 등의 내용을 갖는 코드는 가급적 사용하지 않는 것을 원칙으로 하되, 반드시 사용해야 할 경우 해당 자리의 ‘00’, ‘99’ 등의 최대값을 이용한다\n’ 여부’, ’유무’의 모든 코드값은 ’Y＇과 ’N＇로 사용된다.\n\n코드값(인스턴스) 부여 원칙 예외\n\n기존 As-Is에서 특별한 의미를 가지는 코드 값으로 사용되었을 경우 그대로 채택한다\n외부에서 정의되어서 표준 약어로 널리 사용되는 있는 코드들은 표준화 대상에서 제외하며, 그대로 사용하도록 한다.\n\n국가구분코드 등"
  },
  {
    "objectID": "docs/blog/posts/Governance/6-1.data_review_process.html",
    "href": "docs/blog/posts/Governance/6-1.data_review_process.html",
    "title": "Data Governance Study - Data Glossary Review Process",
    "section": "",
    "text": "1 Data Standard Governance &gt; Data Glossary Review Process\n데이터 모델 수정 발생시 표준담당자가 표준 점검 후 신규 용어를 데이터 모델 담당자에게 전달하고 모델에 반영한다.\n\n1.0.1 데이터 모델 표준 용어 점검 절차\n\n데이터 모델 수정 발생시 표준담당자가 표준 점검 후 신규 용어를 데이터 모델 담당자에게 전달하고 모델에 반영한다.\n표준담당자가 회사 표준담당자가 아닐 경우 표준단어/표준용어에 등록 시 회사 담당자 확인이 필요함\n메타시스템 도입 전까지 회사 표준 담당자가 엑셀로 관리함\n\n\n\n\n데이터 모델 표준 용어 점검 절차\n\n\n\n데이터 모델의 표준 용어 점검 절차를 TASK 별 상세 설명\n\n모델 담당자와 표준 담당자 간의 명확한 역할 구분\n단계별 검토 및 피드백 프로세스\n비표준 용어 발견 시 신규 용어 생성 및 확인 절차\n최종적으로 모델에 표준 용어 반영\n\n\n\n\n\n\n\n\n\n\n\n순번\nTASK\n설명\n담당\n비고\n\n\n\n\n1\n데이터 모델 마트 적재\n데이터 모델 담당자는 설계한 데이터 모델을 회사 데이터 마트에 적재하여 공유함\n모델담당\n\n\n\n2\n데이터 표준 준수 여부 점검 요청\n신규 또는 변경된 데이터 모델에 대하여 표준 담당자에게 표준 준수 여부 점검을 요청함\n모델담당\n\n\n\n3\n표준 준수 점검 신청 접수\n표준 담당자는 데이터 모델 담당자의 데이터 모델 표준 준수 여부 요청을 접수함\n표준담당\n\n\n\n4\n데이터 모델 마트 접속\n데이터 모델 마트에 접속하여 모델의 속성명 정보를 내려받아 점검 대상을 추출함\n표준담당\n\n\n\n5\n용어 및 단어 표준 준수 여부 점검\n점검 대상을 표준 단어 및 용어 기준으로 표준 준수 여부를 점검함\n표준담당\n\n\n\n6\n비표준 용어 발생 여부 확인\n추출한 데이터 모델의 용어 중 비표준 용어 발생 여부를 확인함\n표준담당\n\n\n\n7\n신규 단어/용어 생성 및 모델 담당자 전달\n비표준 용어에 대한 용어 및 단어 생성 후 모델러에게 전달\n표준담당\nEXCEL\n\n\n8\n신규 용어 확인\n표준 담당자에게 전달받은 신규 용어가 적정한지 확인\n모델담당\n\n\n\n9\n데이터 표준 점검 확인 통보\n데이터 모델에 비표준 용어가 없을 경우 점검 확인을 데이터 모델 담당자에게 통보함\n표준담당\n\n\n\n10\n표준 점검 완료\n데이터 모델 담당자는 신규 용어를 데이터 모델에 반영함\n모델담당"
  },
  {
    "objectID": "docs/blog/posts/Governance/7-0.contents_agrement.html",
    "href": "docs/blog/posts/Governance/7-0.contents_agrement.html",
    "title": "Data Governance Study - the contents agreement",
    "section": "",
    "text": "번호\n도메인\n질의내용\n확정\n\n\n\n\n1\n수\n1,2,3,4, … , 01, 02, 03 등 숫자형 금칙어 등록- 숫자는 문자와 함께 씀- 운전자1 (Driver1)\n숫자 단일 사용은 금지\n\n\n2\n수\n갯수 -&gt; 개수\n개수로 사용\n\n\n3\n금액\n금, 비, 액, 가격, 가액(공금가액, 증가액, 차량가액) -&gt; 금액으로 통일- 금액으로 사용하지 못하는 단어일 경우 복합어 사용\nfee : 요금price : 가격cost : 비용amount : 금액\n\n\n4\n금액\n금액을 나타내는 ‘료’ 단어 단일어 인정 여부- 렌탈료-&gt;렌탈요금, 휴차료, 담보료\n요금으로 사용하며, 관용단어 -&gt; 료 복합어 생성\n\n\n5\n명\n’명’을 단일어로 사용 여부(성명, 명 모두 명으로 사용)\n사물 : 명사람 : 성명\n\n\n6\n일자\n일 -&gt; 일자로 사용(취득일, 종료일, 승인일, 발행일 -&gt; 취득일자, 종료일자) 하고 DD의 형태는 ’일’로 사용\n’일자’로 사용\n\n\n7\n-\n차대 -&gt; 차대번호 고유단어로 등록- CARBODYNO(차대번호), CAR_BODY_NO로 사용 중\n차대번호 복합어 생성\n\n\n8\n-\n상품(상품코드, 주문상품) -&gt; 제품- 별도로 사용하면 상품 영문명(Goods), 제품(Product)- 상품은 Good과 Product를 사용중- 제품은 Item을 사용중- 상품을 Commondity로 등록\n상품 : Product자산 : Asset제품 : Goods부품 : Components소모품 : Consumption Goods\n\n\n9\n-\n회사(실사용자회사명, 사용회사, 소속회사) -&gt; 업체- 회사 영문명(Corporation) 업체(Company)\n회사 : 사용업체 : 사용하지 않고 ’비즈니스파트너’로 대체\n\n\n10\n일자\n월 -&gt; 월단가, 월렌탈료, 월리스료 등으로 단어 생성- 월(Month)은 1월~12월을 표현하는 용도로만 사용\n필요시 복합어 생성ex) 월별렌탈금액 단어 생성\n\n\n11\n수\n수 -&gt; 단일어 허용할지 여부(고객수, 직원수, 회전수)\n단일어 사용하지 않고 복합어 생성\n\n\n12\n수\n건수(Count/CNT, 처리건수, 발생건수)와 개수(Count/CO, 반품갯수, 쿠폰갯수)로 분리해서 사용\n건수는 CNT약어로 생성\n\n\n13\n명\n이름(금칙어 등록), 성명 -&gt; 명으로 사용\n사물 : 명사람 : 성명\n\n\n14\n내용\n사항(특기사항, 특약사항, 참고사항) -&gt; 내용으로 사용\n내용으로 사용하며 명시적으로 사항을 사용하는 경우 복합어 생성\n\n\n15\n번호\n사업자번호(인증서 사업자번호) -&gt; 사업자등록번호 : 사업자번호가 사업자등록번호와 다른 의미이면 단어 생성\n사업자등록번호로 사용\n\n\n16\n번호\n연락처 -&gt; 전화번호로 사용 (ASIS 컬럼은 대부분 TEL로 전화번호 내용임)- 연락을 취할수 있는 모든 내용의 의미로 사용예정이면 등록\n연락처 단어 생성(연락처 Pool에서 사용)ASIS에서 연락처는 전화번호의 의미이므로 ASIS의 연락처 용어는 전화번호로 대체\n\n\n17\n번호\n핸드폰번호(가장많음), 휴대전화번호, 휴대폰 -&gt; 휴대전화번호(표준어)로 통일\n휴대전화번호로 사용\n\n\n18\n-\n대여 (대여종료일자, 대여지점, 월간대여료)-&gt; 랜탈\n렌탈로 사용하며 대여는 금칙어\n\n\n19\n-\n제작사(MAKERCODE, CONFIRMDATE)와 제조사(Maker) 차이\n제작사 : 사용제조사 : 금칙어\n\n\n20\n율\n백분율을 나타내는 ‘율’ 단어 단일어 인정 여부- 감가율, 공채율, 대비율\n복합어 생성(사전 참조)단일어 불허\n\n\n21\n세\n세금을 나타내는 ‘세’ 단어 단일어 인정 여부- 과세, 교육세\n복합어 생성(사전 참조)단일어 불허\n\n\n22\n-\n구입(구입가격, 차량구입비, 구입일자) -&gt; 구매- 구입가격, 구입일\n구매구입 : 금칙어\n\n\n23\n명\n이름을 나타내는 ‘명’ 단어 단일어 인정 여부- 파일명, 차량명\n사물 : 명사람 : 성명\n\n\n24\n-\n구입(구입가격, 차량구입비, 구입일자) -&gt; 구매- 구입가격, 구입일\n구매구입 : 금칙어\n\n\n25\n-\n제원(specification) (제원), 스팩(SPEC) (제품스팩, 제품스팩아이디) -&gt; 스펙\n스펙제원 : 금칙어\n\n\n26\n-\n제휴협력사 vs 협력사 vs 파트너\n비즈니스파트너\n\n\n27\n-\n제휴사 -&gt; ex) 대한항공\n-\n\n\n28\n-\n협력사 -&gt; 정비업체\n-\n\n\n29\n-\n이해당사자 vs 이해관계자\n이해관계자\n\n\n30\n량\n양을 나타내는 의미로 ‘량’ 단어 단일어 인정 여부- 월평균소요량, 연료량 등\n’량’으로 복합어 생성(사전 참조)단일어 불허\n\n\n31\n값\n값을 나타내는 의미로 ‘값’ 단어 단일어 인정 여부- 변경값, 관리키값, 상태값 등\n복합어 생성(사전 참조)단일어 불허\n\n\n32\n번호\n번호 그룹 도메인에 속하는 용어들의 데이터 타입 및 길이가 상이하므로, 용어별 도메인 별도 생성\n번호용어 별 도메인 생성\n\n\n33\n-\n매각제품 별 단가, 회차 별 산출금액에서 ‘별’ 단어 허용 여부\n별, 및 : 금칙어필요시 복합어 생성\n\n\n34\n명\n‘처’ (유지보수 처, 송금 처) 단어 허용 여부- 처의 의미\n처 : 금칙어-&gt; 대상 등으로 사용\n\n\n35\n번호\n순번과 일련번호 중 일련번호 사용 (순번 -&gt; 금칙어)- 일련번호를 Serial Number로 사용하고 순번을 단순한 순차적인 번호(Sequence)로 사용할지 에 대한 의사결정 필요\n순번 : 금칙어일련번호 : 순차적인 번호 (KEY)고유번호(시리얼번호) : 제품고유번호"
  },
  {
    "objectID": "docs/blog/posts/Governance/7-0.contents_agrement.html#data-standard-governance-contents-agreement",
    "href": "docs/blog/posts/Governance/7-0.contents_agrement.html#data-standard-governance-contents-agreement",
    "title": "Data Governance Study - the contents agreement",
    "section": "",
    "text": "번호\n도메인\n질의내용\n확정\n\n\n\n\n1\n수\n1,2,3,4, … , 01, 02, 03 등 숫자형 금칙어 등록- 숫자는 문자와 함께 씀- 운전자1 (Driver1)\n숫자 단일 사용은 금지\n\n\n2\n수\n갯수 -&gt; 개수\n개수로 사용\n\n\n3\n금액\n금, 비, 액, 가격, 가액(공금가액, 증가액, 차량가액) -&gt; 금액으로 통일- 금액으로 사용하지 못하는 단어일 경우 복합어 사용\nfee : 요금price : 가격cost : 비용amount : 금액\n\n\n4\n금액\n금액을 나타내는 ‘료’ 단어 단일어 인정 여부- 렌탈료-&gt;렌탈요금, 휴차료, 담보료\n요금으로 사용하며, 관용단어 -&gt; 료 복합어 생성\n\n\n5\n명\n’명’을 단일어로 사용 여부(성명, 명 모두 명으로 사용)\n사물 : 명사람 : 성명\n\n\n6\n일자\n일 -&gt; 일자로 사용(취득일, 종료일, 승인일, 발행일 -&gt; 취득일자, 종료일자) 하고 DD의 형태는 ’일’로 사용\n’일자’로 사용\n\n\n7\n-\n차대 -&gt; 차대번호 고유단어로 등록- CARBODYNO(차대번호), CAR_BODY_NO로 사용 중\n차대번호 복합어 생성\n\n\n8\n-\n상품(상품코드, 주문상품) -&gt; 제품- 별도로 사용하면 상품 영문명(Goods), 제품(Product)- 상품은 Good과 Product를 사용중- 제품은 Item을 사용중- 상품을 Commondity로 등록\n상품 : Product자산 : Asset제품 : Goods부품 : Components소모품 : Consumption Goods\n\n\n9\n-\n회사(실사용자회사명, 사용회사, 소속회사) -&gt; 업체- 회사 영문명(Corporation) 업체(Company)\n회사 : 사용업체 : 사용하지 않고 ’비즈니스파트너’로 대체\n\n\n10\n일자\n월 -&gt; 월단가, 월렌탈료, 월리스료 등으로 단어 생성- 월(Month)은 1월~12월을 표현하는 용도로만 사용\n필요시 복합어 생성ex) 월별렌탈금액 단어 생성\n\n\n11\n수\n수 -&gt; 단일어 허용할지 여부(고객수, 직원수, 회전수)\n단일어 사용하지 않고 복합어 생성\n\n\n12\n수\n건수(Count/CNT, 처리건수, 발생건수)와 개수(Count/CO, 반품갯수, 쿠폰갯수)로 분리해서 사용\n건수는 CNT약어로 생성\n\n\n13\n명\n이름(금칙어 등록), 성명 -&gt; 명으로 사용\n사물 : 명사람 : 성명\n\n\n14\n내용\n사항(특기사항, 특약사항, 참고사항) -&gt; 내용으로 사용\n내용으로 사용하며 명시적으로 사항을 사용하는 경우 복합어 생성\n\n\n15\n번호\n사업자번호(인증서 사업자번호) -&gt; 사업자등록번호 : 사업자번호가 사업자등록번호와 다른 의미이면 단어 생성\n사업자등록번호로 사용\n\n\n16\n번호\n연락처 -&gt; 전화번호로 사용 (ASIS 컬럼은 대부분 TEL로 전화번호 내용임)- 연락을 취할수 있는 모든 내용의 의미로 사용예정이면 등록\n연락처 단어 생성(연락처 Pool에서 사용)ASIS에서 연락처는 전화번호의 의미이므로 ASIS의 연락처 용어는 전화번호로 대체\n\n\n17\n번호\n핸드폰번호(가장많음), 휴대전화번호, 휴대폰 -&gt; 휴대전화번호(표준어)로 통일\n휴대전화번호로 사용\n\n\n18\n-\n대여 (대여종료일자, 대여지점, 월간대여료)-&gt; 랜탈\n렌탈로 사용하며 대여는 금칙어\n\n\n19\n-\n제작사(MAKERCODE, CONFIRMDATE)와 제조사(Maker) 차이\n제작사 : 사용제조사 : 금칙어\n\n\n20\n율\n백분율을 나타내는 ‘율’ 단어 단일어 인정 여부- 감가율, 공채율, 대비율\n복합어 생성(사전 참조)단일어 불허\n\n\n21\n세\n세금을 나타내는 ‘세’ 단어 단일어 인정 여부- 과세, 교육세\n복합어 생성(사전 참조)단일어 불허\n\n\n22\n-\n구입(구입가격, 차량구입비, 구입일자) -&gt; 구매- 구입가격, 구입일\n구매구입 : 금칙어\n\n\n23\n명\n이름을 나타내는 ‘명’ 단어 단일어 인정 여부- 파일명, 차량명\n사물 : 명사람 : 성명\n\n\n24\n-\n구입(구입가격, 차량구입비, 구입일자) -&gt; 구매- 구입가격, 구입일\n구매구입 : 금칙어\n\n\n25\n-\n제원(specification) (제원), 스팩(SPEC) (제품스팩, 제품스팩아이디) -&gt; 스펙\n스펙제원 : 금칙어\n\n\n26\n-\n제휴협력사 vs 협력사 vs 파트너\n비즈니스파트너\n\n\n27\n-\n제휴사 -&gt; ex) 대한항공\n-\n\n\n28\n-\n협력사 -&gt; 정비업체\n-\n\n\n29\n-\n이해당사자 vs 이해관계자\n이해관계자\n\n\n30\n량\n양을 나타내는 의미로 ‘량’ 단어 단일어 인정 여부- 월평균소요량, 연료량 등\n’량’으로 복합어 생성(사전 참조)단일어 불허\n\n\n31\n값\n값을 나타내는 의미로 ‘값’ 단어 단일어 인정 여부- 변경값, 관리키값, 상태값 등\n복합어 생성(사전 참조)단일어 불허\n\n\n32\n번호\n번호 그룹 도메인에 속하는 용어들의 데이터 타입 및 길이가 상이하므로, 용어별 도메인 별도 생성\n번호용어 별 도메인 생성\n\n\n33\n-\n매각제품 별 단가, 회차 별 산출금액에서 ‘별’ 단어 허용 여부\n별, 및 : 금칙어필요시 복합어 생성\n\n\n34\n명\n‘처’ (유지보수 처, 송금 처) 단어 허용 여부- 처의 의미\n처 : 금칙어-&gt; 대상 등으로 사용\n\n\n35\n번호\n순번과 일련번호 중 일련번호 사용 (순번 -&gt; 금칙어)- 일련번호를 Serial Number로 사용하고 순번을 단순한 순차적인 번호(Sequence)로 사용할지 에 대한 의사결정 필요\n순번 : 금칙어일련번호 : 순차적인 번호 (KEY)고유번호(시리얼번호) : 제품고유번호"
  }
]