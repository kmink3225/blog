[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\nBrief Introduction\n저는 R과 Python같은 오픈소스 도구를 사용하여 통계, 머신 러닝 및 딥러닝을 독학한 열정 가득한 데이터 과학자입니다. 약 7년 여 동안 데이터 모델링, 통계적 모델링, 머신 러닝 모델링 및 시각화를 통하여 데이터 관련 업무 경험을 쌓았습니다.\n\n\nExperience\n저는 한국에서 생화학 학사 학위를, 미국에서는 수학 학사 학위와 생물통계학 석사 학위를 취득했습니다. 저의 생화학 전공을 바탕으로 바이오와 의료분야에서 커리어를 시작했으며, 그 과정에서 많은 비전문가들과 협업하면서 통계 및 데이터 사이언스에 대해 그들과 소통하는 법을 익혔습니다.\n비전문가들과 SW 개발자들과의 협업을 통해서 효율적이고 효과적인 소통의 중요성을 깨달았습니다. 그 효과적인 소통은 수학, 통계, 데이터 엔지니어링 및 IT에 대한 탄탄한 지식에서 비롯된다고 생각하여, 기술에 대한 세부적이고 체계적인 지식축적을 지향합니다.\n\n\n\n\nBrief Introduction\nI am a passionate data scientist who has self-taught statistics, machine learning, and deep learning using open-source tools like R and Python. Over approximately 7 years, I have accumulated experience in data-related work through data modeling, statistical modeling, machine learning modeling, and visualization.\n\n\nExperience\nI earned my bachelor’s degree in Biochemistry in South Korea and in Mathematics in the USA, followed by a master’s degree in Biostatistics. Leveraging my background in Biochemistry, I began my career in the bio and medical fields. Throughout this journey, I collaborated with many non-experts in data science, learning how to communicate effectively about statistics and data science with them.\nThrough collaboration with non-experts and software developers, I realized the importance of efficient and effective communication. I believe that such effective communication stems from a solid understanding of mathematics, statistics, data engineering, and IT, which is why I aim to accumulate detailed and systematic knowledge in these technical areas."
  },
  {
    "objectID": "docs/blog/index.html",
    "href": "docs/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Blog Content List\n\n\nGuide Map of Blog Contents\n\n\n\nAll List\n\n\n\nAs the number of blog topics has increased, organizing the content has become more challenging. To enhance content accessibility and facilitate an understanding of the blog’s relevance, we have provided links to a list of contents categorized by topic. \n\n\n\n\n\nJan 1, 3000\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Engineering\n\n\nEngineering\n\n\n\nEngineering\n\n\n\nEngineering for Data Science \n\n\n\n\n\nJan 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Validation\n\n\nVerification & Validation on Software\n\n\n\nSurveilance\n\n\n\nAny business that directly or indirectly affects human health or life must comply with regulations regarding inspection, testing, verification and validation. It is necessary to systematically manage and document risks by arranging regulatory policy data for the medical and IT industry. These materials are rigorous and conservative, so there are various documents for each case, but the underlying principles have the same root. This blog section summarises and organizes documents with fundamental explanations of regulation for each area. 사람의 건강이나 생명에 직 간접적으로 영향을 미치는 어떠한 비즈니스는 검사, 테스트, 검증 및 인증에 관한 규정을 준수해야한다. 의료분야와 IT 분야에 대한 규정 방침 자료를 정리하여 체계적인 위험 관리를 해야한다. 이러한 자료들은 엄격하고 보수적이어서 각 사례마다 다양한 문서들이 존재하지만 그 근본 원리는 같다. 이 블로그에서는 각 영역마다 근본이 되는 문서들을 요약 및 정리한다. \n\n\n\n\n\nJan 1, 2090\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPublic Data\n\n\nTemplate\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nJan 1, 2024\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPackage Management - 1\n\n\nrequirements.text 사용\n\n\n\nEngineering\n\n\n\nEngineering for Data Science \n\n\n\n\n\nJun 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nAirflow Introduction\n\n\nTemplate\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nEnvironment Setting for Airflow\n\n\nWSL, Docker Installation, Airflow Installation, Development Environment Setting, Python Interpreter Installation, VScode Installation, Git Evnvironment Setting, Airflow Library Installation\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nOperator Baisc (Bash Operator)\n\n\nBasic Operator(Bash Operator), Cron Scheduling, Task Dependencies(Connection), External Script File Operation, Email Operator\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPython Operator\n\n\nBasic Python Operator, Importing External Python Scripts, Usage of Decorator, Understanding of Python Parameters, op_args (airflow), op_kwargs (airflow)\n\n\n\nEngineering\n\n\n\nAirflow의 Python Operator에 op_args로 변수를 할당하는 방법 \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nJinja Template, Bash Operator with Jinja Template, Airflow Datetime Count, Python Operator with Jinja Template, Bash Operator with Macros, Python Operator with Macro\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Share\n\n\nPython Operator with Xcom, Bash Operator with Xcom, Xcom between Python Operator and Bash Operator, Xcom between Python Operator and Email Operator, Global Share Variable\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTask Handling Techniques\n\n\nBranchPython Operator (Branch Processing), @task.brancch (Branch Processing), BaseBranchOperator (Branch Processing), Trigger Rule Setting, Task Groups, Edge Labels\n\n\n\nEngineering\n\n\n\nAdvanced Techniques to handle tasks (Branch Processing) \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nMore Operators\n\n\nMore Operators Provided by Airflow, Trigger Dag Run Operators, Obtaining Seoul Public Data API Key, Retrieve Seoul Public Data API Using SimpleHttp Operator, Custom Operator Development\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nConnection & Hook\n\n\ndocker_compose.yaml 해석, Postgres Container 올리기, Connection, Hook, bulk_load(), Custom Hook, Providers, Connection Type\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nAirflow Additional Function\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate Variabler\n\n\nDAG Creation, Bash Operator, Task Performance Subject,\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nConda Introduction\n\n\nConda Introduction, Conda Installation\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nDocker Introduction, Docker Installation\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nGit Introduction\n\n\nGit Introduction & Installation\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nLinux Commands\n\n\nWSL\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nVS code Introduction\n\n\nVS code Introduction and Installation\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nGET, POST, PUT, DELETE, HEAD, PATCH, OPTIONS\n\n\n\nEngineering\n\n\n\nBasic HTTP Methods \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nWSL Install\n\n\nWSL\n\n\n\nEngineering\n\n\n\ntemplate \n\n\n\n\n\nMay 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nInfrastructure Security\n\n\nWeek2\n\n\n\nEngineering\n\n\n\nAWS \n\n\n\n\n\nApr 5, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nStorage and Database\n\n\nWeek3\n\n\n\nEngineering\n\n\n\nAWS \n\n\n\n\n\nMar 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nStorage and Database\n\n\nWeek3\n\n\n\nEngineering\n\n\n\nAWS \n\n\n\n\n\nMar 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nComputing and Networking\n\n\nWeek2\n\n\n\nEngineering\n\n\n\nAWS \n\n\n\n\n\nMar 9, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (9) Priority Queue\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nFeb 3, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (8) Binary Search Tree\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (7) Deque\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 26, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (10) Graph\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 20, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (6) Queue\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 19, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (5) Stack\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 19, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (2) Array\n\n\nArray\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (3) Linked List\n\n\nLinked List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (1) Overview\n\n\nOverview\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 17, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (4) Python List\n\n\nPython List\n\n\n\nEngineering\n\n\n\nData Structure for Data Science \n\n\n\n\n\nJan 17, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nFDA Software Validation Guidance Presentation\n\n\nSource: General Principles of Software Validation\n\n\n\nSurveilance\n\n\n\nThe purpose of this article is to help understand the summary of the ‘General Principles of the ’Software Validation; Final Guidance for Industry and FDA Staff’ document issued on 2002-01-11. This article provides short sentences with many diagrams for intuitive understanding. \n\n\n\n\n\nDec 28, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nFDA Software Validation Guidance Summary\n\n\nDcoument: General Principles of Software Validation\n\n\n\nSurveilance\n\n\n\nThe purpose of this blog is to get a rough concept of the FDA approval process by making a summary of the ‘General Principles of the ’Software Validation; Final Guidance for Industry and FDA Staff’ document issued on 2002-01-11. So far, the document seems to be still valid taking into account that its guidance for the FDA approval are broad, general, and comprehensive, and that many recent FDA documents supplement it. \n\n\n\n\n\nDec 15, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/blog/posts/Language/mosaic.html",
    "href": "docs/blog/posts/Language/mosaic.html",
    "title": "Mosaic Package",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n수식 base로 함수를 그릴 수 있는 package\n\n\nCode\nlibrary(tidyverse)\nlibrary(mosaic)\n\nrm(list=ls())\n\n\n\n\nCode\nplotFun(x1^2~x1,\nx1.lim=range(-10,10),\nx2.lim=range(-10,10),\nsurface=TRUE,\n# surface=FALSE, # contour 를 보여줌\nxlab=expression(x[1]),\nylab=expression(f(x)))\n\nplotFun(5*x1^2~x1 & x2,\nx1.lim=range(-10,10),\nx2.lim=range(-10,10),\nsurface=TRUE,\nxlab=expression(x[1]),\nylab=expression(x[2]),\nzlab=expression(f(x[1],x[2])))\n\n\n그려진 그래프를 R studio에선 plot의 톱니바퀴를 누르면 축별로 회전이 가능하지만 vs code에서는 지원이 안되는 것 같음\n\n\n\n\n\n\n\n\n\n\nProject Content List\n\n\n\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/Language/mosaic.html#plotfun",
    "href": "docs/blog/posts/Language/mosaic.html#plotfun",
    "title": "Mosaic Package",
    "section": "",
    "text": "수식 base로 함수를 그릴 수 있는 package\n\n\nCode\nlibrary(tidyverse)\nlibrary(mosaic)\n\nrm(list=ls())\n\n\n\n\nCode\nplotFun(x1^2~x1,\nx1.lim=range(-10,10),\nx2.lim=range(-10,10),\nsurface=TRUE,\n# surface=FALSE, # contour 를 보여줌\nxlab=expression(x[1]),\nylab=expression(f(x)))\n\nplotFun(5*x1^2~x1 & x2,\nx1.lim=range(-10,10),\nx2.lim=range(-10,10),\nsurface=TRUE,\nxlab=expression(x[1]),\nylab=expression(x[2]),\nzlab=expression(f(x[1],x[2])))\n\n\n그려진 그래프를 R studio에선 plot의 톱니바퀴를 누르면 축별로 회전이 가능하지만 vs code에서는 지원이 안되는 것 같음"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html",
    "title": "ANCOVA",
    "section": "",
    "text": "(Draft, 바쁘니까 일단 대충이라도 적어놓음 ㅠ)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#description",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#description",
    "title": "ANCOVA",
    "section": "1 Description",
    "text": "1 Description\nANCOVA (Analysis of Covariance, ANCOVA)\n\nANOVA에 공변량 (covariate)을 추가하여 분석 수행\n공변량을 조정하여 독립변수의 순수한 영향을 검정\n공변량: 연속형 변수로 한정"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#example",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#example",
    "title": "ANCOVA",
    "section": "2 Example",
    "text": "2 Example\n\n2.1 Load Libraries and Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(faraway)\nlibrary(markdown)\nlibrary(effects)\nlibrary(HH)\nlibrary(psych)\n\n\n\n\n2.2 Data Description\n\n\nCode\nstr(sexab)\n\n\n'data.frame':   76 obs. of  3 variables:\n $ cpa : num  2.048 0.839 -0.241 -1.115 2.015 ...\n $ ptsd: num  9.71 6.17 15.16 11.31 9.95 ...\n $ csa : Factor w/ 2 levels \"Abused\",\"NotAbused\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nR console에 ?sexab를 입력하면 다음과 같은 설명이 나온다.\nPost traumatic stress disorder in abused adult females\nThe data for this example come from a study of the effects of childhood sexual abuse on adult females. 45 women being treated at a clinic, who reported childhood sexual abuse, were measured for post traumatic stress disorder and childhood physical abuse both on standardized scales. 31 women also being treated at the same clinic, who did not report childhood sexual abuse were also measured. The full study was more complex than reported here and so readers interested in the subject matter should refer to the original article.\n즉, 요약하면 아동기에 성폭력을 겸험한 성인들의 정신 건강을 측정한 데이터로서, 아동기의 성폭력 경험과 학대 경험이 성인기의 정신건강에 유의한 영향을 미치는지에 대한 실험을 한 것이다.\n이 data는 3개의 변수와 76개의 samples을 포함한다.\n\ncpa : Childhood physical abuse on standard scale, covariate\nptsd : post-traumatic stress disorder on standard scale, response variable\ncsa : Childhood sexual abuse - abused or not abused, independent variable\n\n친절하게 response variable, independent variable 및 covariate을 규명해놓았다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#eda",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#eda",
    "title": "ANCOVA",
    "section": "3 EDA",
    "text": "3 EDA\n\n3.1 Descriptive Statistics\n\n\nCode\ntemp&lt;-describeBy(ptsd~csa,data=sexab)\ntemp&lt;-rbind('abused'=temp$Abused,'notAbused'=temp$NotAbused)%&gt;%\nas.data.frame()\ntemp%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n\nabused\n1\n45\n11.941093\n3.440151\n11.31277\n11.883422\n3.857355\n5.98491\n18.99251\n13.00760\n0.1556159\n-0.9124483\n0.5128275\n\n\nnotAbused\n1\n31\n4.695874\n3.519743\n5.79447\n4.903441\n1.978841\n-3.34921\n10.91447\n14.26368\n-0.6589170\n-0.2008051\n0.6321645\n\n\n\n\n\n위의 요약된 기술 통계량들 중 표준 편차는 유사하지만 평균 ptsd가 약 7.245219의 차이를 보여준다. 아래의 histogram역시 성폭력을 경험한 그룹과 경험하지 않은 그룹간의 PTSD 수치가 다른것을 볼 수 있다.\n\n\nCode\nggplot(data=sexab,aes(x=ptsd,color=csa,fill=csa))+\ngeom_histogram(aes(y=..density..),position=\"identity\",fill='white')+\ngeom_density(alpha=0.5)+\nlabs(title=\"Histogram, PTSD Grouped by Childhood Sexual Abuse Experience\", x=\"PTSD\", y=\"Desnsity\")\n\n\n\n\n\n\n\n3.2 One-Way ANOVA\n성폭력 경험 유무에 따른 PTSD 평균 차이가 통계적으로 유의한지 확인하기 위해 ANOVA를 수행한다.\n\n\nCode\nsexab_aov&lt;-aov(ptsd~csa, data=sexab)\nsummary(sexab_aov)\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncsa          1  963.5   963.5    79.9 2.17e-13 ***\nResiduals   74  892.4    12.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n집단간 변수 csa p value가 &lt;0.05 인 것을 확인할 수 있다. csa는 5% 유의수준에서 유의하다.\n하지만 PTSD의 변동량은 아동 학대에 의해 설명될 수도 있기 때문에 ptsd의 평균은 csa뿐만 아니라 cpa에 또한 고려되어야한다.\n\n\nCode\nggplot(data=sexab,aes(x=cpa,y=ptsd))+geom_point()+geom_smooth(method=\"lm\")+\nlabs(title=\"Scatter Plot, PTSD vs CPA\", x=\"CPA\", y=\"PTSD\")\n\n\n\n\n\nCode\ncorrelation&lt;-cor.test(sexab$cpa,sexab$ptsd, method='pearson')\n\n\n그림과 같이 CPA가 증가하면서 PTSD또한 선형적으로 증가하는 패턴을 관찰할 수 있다. 두 변수간의 상관계수 = 0.49이고 p value= 6.2715909^{-6}으로 보아 두 변수 사이에 선형적인 상관관계가 있는 것으로 보인다.\n\n\nCode\nggplot(data=sexab,aes(x=cpa,y=ptsd))+geom_point()+geom_smooth(method=\"lm\")+\nfacet_wrap(.~csa)+\nlabs(title=\"Scatter Plot, PTSD vs CPA Grouped By CSA\", x=\"CPA\", y=\"PTSD\")\n\n\n\n\n\n아동기 성폭력 경험 유/무에도 PTSD와 CPA와 선형적인 관계가 있는 것으로 보이기 때문에 CSA의 PTSD로의 효과를 검정하기 위해선 CPA를 조정할 필요가 있는것으로 보인다.\n\n\nCode\n# ptsd로의 순수한 성폭력 경험의 영향도를 얻기 위해서는 아동기 신체적 학대(공변량)에 대해서 고려를 해줘야함\n\nsexab_aov&lt;-aov(ptsd~cpa+csa, data=sexab) \nsummary(sexab_aov)\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncpa          1  449.8   449.8   41.98 9.46e-09 ***\ncsa          1  624.0   624.0   58.25 6.91e-11 ***\nResiduals   73  782.1    10.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n아동기의 신체적 학대가 일정하다는 가정하에서 PTSD와 성폭력의 순수한 관계는 5% 유의수준에서 유의하고 공변량, CPA를 조정하기전과 그 유의성이 차이가 있음을 관찰할 수 있다.\n\n\nCode\n# CPA가 제거 된 후에 CSA의 순수한 효과를 알아보기\n\nancova(ptsd~cpa+csa, data=sexab) \n\n\nAnalysis of Variance Table\n\nResponse: ptsd\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ncpa        1 449.80  449.80  41.984 9.462e-09 ***\ncsa        1 624.03  624.03  58.247 6.907e-11 ***\nResiduals 73 782.08   10.71                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n두 csa집단에서 두 회귀선의 기울기 같고 절편이 다르게 나타나는 것을 관찰 할 수있다. 기울기가 같은 이유는 cpa가 ptsd에 영향을 미치는 정도가 두집단에서 일정하도록 공변량으로서 통제 했기 때문이다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#blog-guide-map-link",
    "title": "ANCOVA",
    "section": "4 Blog Guide Map Link",
    "text": "4 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "template.html",
    "href": "template.html",
    "title": "Template",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe radius of the circle is 10.\n\n\n\n1 Go to Project Content List\nProject Content List\n\n\n2 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html",
    "href": "docs/projects/qc_platform/index.html",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n진단 장비의 품질 관리는 의료 장비와 연관된 제품의 특성상 Global Market 진출시 각 나라의 정부에서 요구하는 규제사항 중 하나이다.\n\n시약의 안정성 검증 요구\n장비의 안정성 검증 요구\nSoftware의 안정성 검증 요구\nDiagnostic Algorithm의 안정성 검증 요구\n\n현재 Seegene이 사용하고 있는 진단 장비는 자사 제품이 아니기 때문에 진단 장비의 품질 관리 방식에 어려움이 있다.\nPCR 기기의 noise test는 의료 장비의 QC process 중 하나로서, Seegene의 시약 제품의 성능 안정성과 직접적으로 영향을 주는 품질 검증 시스템이다.\n회사의 규모가 커지고 잦은 조직 개편으로 수동 방식의 noise tests가 여러 부서로 이관됨에 따라 noise test 수행자의 이해도와 숙련도가 떨어져 noise test가 올바르게 수행되지 않고 있다.\n씨젠의 장기 목표인 전사 자동화를 위해 noise test를 자동화하는 프로젝트가 발탁됐다.\n\n\n\n\n\n2020.12.19~2021.03.06에 입고된 PCR기기 2201대를 2552번의 실험에서 만들어진 61,248개의 신호에서 얻은 data-driven parameters로 장비의 성능을 평가하여 합격/불합격 뿐만 아니라 장비에 등급을 차등 부여하여 시간에 따른 장비의 성능을 지속적으로 분석 가능하게 한다.\n다음의 주요 문제점을 개선한다.\n\n신호의 증폭 크기에 따라 noise test 결과에 영향을 크게 주어 잘못된 결과를 산출해주는 metric 개선\n단순한 휴먼 에러 신호에 무조건적으로 장비의 불합격처리가 결정 되는 문제 보완하여 robust한 평가체계로 현업부서의 부담을 덜어준다.\n장비 고유에서 발생하는 pattern을 찾아 장비 error 신호를 labeling 한다.\n\n20번의 test를 수동으로 계산하는 과정에 30분이 소요되는 것을 웹 기반의 자동화로 약 2~3분내로 단축시킬 수 있다.\n씨젠의 full automation을 위한 best practice example로 만들어 IT 부분과 제조 부문 및 BT부문과의 협력체계 구축 및 활성화 한다.\n시각화와 noise test result history를 제공하여 추적 및 VOC 대응 system을 구축한다.\n\n\n\n\n\n\n\n현업 부서와의 긴밀한 소통으로 QC process를 세분화하여 앞 단계 QC에서 발생하는 data를 활용하여 뒷 단계 QC인 noise test의 결과를 예측한다.\nnoise가 적다고 확실시 되는 기기에 한해서 noise test 생략\n\n\n\n\n\n전체 QC 프로세스를 자동화 또는 반 자동화\n기존의 noise 측정 metric 분석 및 새로운 metric 생성하여 검사 결과의 정확도를 향상\n시각화와 noise test result history를 제공하여 실무자의 이해도를 높이고 관리가능하게 한다.\n\n\n\n\n\n\n현업 업무 기술서 부재\n실무자의 백업 실수\n부서마다 산재된 데이터\n높은 난이도의 data cleansing\n\n실무자의 데이터로부터 분석 가능한 데이터 선별\n실무자의 데이터의 오입력\ndata 및 문서의 DRM 수동해제\n\nreverse engineering 필요\n잦은 조직 개편으로 관련 인원 및 부서 연락체계 부재\n\n\n\n\n\nBack-end Engineering for DevOps Pipeline Construction\nData Engineering for data cleansing and reverse engineering\nData Modeling for a RDB system construction\nStatistical Analysis for the noise test performance verification\nMachine Learning for pattern analysis\nFront-end Engineering for UI/UX construction\nBiologics, Biophysics, Physics Knowledge\n\n\n\n\n\n1 data scientist (me) - project owner\n5 mechanical engineers\n4 biologists\n2 patent attorneys\n2 data engineers\n3 full stack developers\n1 advisor (a professor of Computer Science at Seoul National University)\n\n\n\n\n\n\n\n\n\n\n\n\nNoise Test\nAs-Is\nTo-Be\n\n\n\n\nQC 알고리즘 개발에 사용되는 샘플 크기\nn=100\nSignals from 2552 experiments, n=61,248\n\n\nQC 알고리즘 성능 비교에 사용되는 샘플 크기\nn=61,248\nn=61,248\n\n\nEvaluation Metrics\n2 metrics\n10 metrics ( the exsting 2 metrics + new 8 metrics)\n\n\nInput Process\n특정 프로그램에서 추출한 엑셀 파일의 데이터를 수동으로 복사하여 붙여넣기\n웹 기반 자동화, 다수의 실험 파일 업로드\n\n\nOutput Process\nBatch Evaluation method,\n\n\n\n장비의 신호 중 하나라도 부적합 판정되면 장비 자체가 QC 부적합 판정 (맹점: 휴먼에러 신호가 1개라도 있으면 장비는 무조건 실격 처리)\nDifferential Evaluation method, 장비의 신호에 점수를 계산 후 평균값을 구하고 장비 등급을 A+, A, B, F로 지정. F인 경우 부적합. 오류 신호를 평가에서 제외하므로 오류 신호에 robust\n\n\n\nOutput 1\npass: 92.58%, fail: 7.42% (after excluding many human errors)\nA+ (pass): 7.01%, A (pass): 12.91%, B (pass): 75.72%, F (fail): 4.36%\n\n\nOutput 2\nNA\nVisualized Plots and Tables.\n\n\nOutput 3\nNA\nClassfication Results: Normal Signals, Human Errors, Device Errors, Manufacturing Errors\n\n\nTime Consumed\nAbout 30 minutes per 20 experiments\nAbout 25 minutes per 2552 experiments\n\n\nData Management\nNon-standard management method (작업자마다 다른 방식으로 Excel 파일로 다른 형태로 NAS 디렉토리에 저장)\nRDB uploaded by a scheduler (장비 고장 추적 분석이 가능)\n\n\n\n\n\n\n1% 유의수준에서 노이즈 테스트 결과가 전체 진단 과정에서 최종 결과의 평균 차이에 큰 영향을 미치지 않는다는 것을 통계적으로 증명했기 때문에 전체 QC 프로세스에서 노이즈 테스트를 폐지.\n데이터 사이언스 부서에서 실험과 장비에서 생성된 데이터에 대한 이해도를 높일 수 있는 기회가 됐음.\n전사 DB구축 및 플랫폼 아키텍처 구축의 기반이 됨.\n\n\n\n\n\n\n\n\nQuality Control of diagnostic equipment is one of the necessities for the regulations required by the government of each country when entering the global market due to the nature of products related to medical equipment.\n\nReagent stability verification & validation required\nEquipment stability verification & validation request\nSoftware stability verification & validation request\nStability Verification & validation Request of Diagnostic Algorithm\n\nSince the diagnostic equipment currently being used by Seegene is not its own product, it is difficult to manage the quality of the diagnostic equipment.\nThe noise test of PCR equipment is one of the QC processes of medical equipments, and it is a quality verification system that directly affects the performance stability of Seegene’s reagent products.\nAs the size of the company grows and frequent organizational reshuffles result in manual noise tests being transferred to various departments, the noise test performers’ understanding and skills are low, resulting in noise tests not being performed correctly.\nA project to automate the noise test was selected for Seegene’s long-term goal of enterprise automation.\n\n\n\n\n\nData-driven parameters obtained from 61,248 signals from 2552 experiments were used to evaluate 2201 PCR devices, which were stocked between 2020.12.19 and 2021.03.06. yield Enables continuous analysis of equipment performance over time.\nImprove the following major problems:\n\nImproved metrics that produces erroneous results by greatly affecting the noise test result depending on the size of the signal amplification.\nIt relieves the burden on the field department with a robust evaluation system by supplementing the problem of unconditionally determining equipment rejection in response to a simple human error signal.\nEquipment error signals are labeled by finding patterns that occur in equipment.\n\nThe time required for the manual calculation process can be reduced from 30 minutes per 20 tests to about 2 to 3 minutes with web-based automation.\nBy making this project the best practice example of Seegene’s full automation, establish and vitalize the cooperation system between the IT, manufacturing and BT sectors.\nVisualization and noise test result history are provided to build a tracking and VOC response system.\n\n\n\n\n\n\n\nThe QC process is subdivided through close communication with the field departments, and the results of the noise test, which is the next stage QC, are predicted by utilizing the data generated in the previous stage QC.\nNoise test omitted only for devices that are certain to have low noise.\n\n\n\n\n\nAutomate or semi-automate the entire QC process\nImprove the accuracy of inspection results by analyzing the existing noise measurement metric and creating a new metric\nVisualization and noise test result history are provided to increase the understanding of practitioners and enable management\n\n\n\n\n\n\nAbsence of job description\nBackup Mistakes by Practitioners\nData scattered across departments\nHigh level of data cleansing\n\nSelect data that can be analyzed from practitioner data\nIncorrect input of practitioner data\nManual release of DRM for data and documents\n\nreverse engineering required\nAbsence of contact system for related personnel and departments due to frequent organizational reshuffle\n\n\n\n\n\nBack-end Engineering for DevOps Pipeline Construction\nData Engineering for data cleansing and reverse engineering\nData Modeling for a RDB system construction\nStatistical Analysis for the noise test performance verification\nMachine Learning for pattern analysis\nFront-end Engineering for UI/UX construction\nBiologics, Biophysics, Physics Knowledge\n\n\n\n\n\n1 data scientist (me) - project owner\n5 mechanical engineers\n4 biologists\n2 patent attorneys\n2 data engineers\n3 full stack developers\n1 advisor (a professor of Computer Science at Seoul National University)\n\n\n\n\n\n\n\n\n\n\n\n\nNoise Test\nAs-Is\nTo-Be\n\n\n\n\nSample Size Used for QC Algorithm Development\nn=100\nSignals from 2552 experiments, n=61,248\n\n\nSample Size Used for QC Algorithm Performance Comparison\nn=61,248\nn=61,248\n\n\nEvaluation Metrics\n2 metrics\n10 metrics ( the exsting 2 metrics + new 8 metrics)\n\n\nInput Process\nmanually copy & paste data of excel files extracted from a certain program\nWeb-Based Automation, upload multiple experiment files\n\n\nOutput Process\nBatch Evaluation method, if even one of the signals from the equipment fails, the equipment fails. (Blind Spot: If there is one human error signal, the equipment is unconditionally disqualified.)\nDifferential Evaluation method, The signals from the equipment are scored, the average value is obtained, and the equipment is graded A+, A, B, and F. Failed if F. Robust on error signals as it excludes the error signal from evaluation.\n\n\nOutput 1\npass: 92.58%, fail: 7.42% (after excluding many human errors)\nA+ (pass): 7.01%, A (pass): 12.91%, B (pass): 75.72%, F (fail): 4.36%\n\n\nOutput 2\nNA\nVisualized Plots and Tables.\n\n\nOutput 3\nNA\nClassfication Results: Normal Signals, Human Errors, Device Errors, Manufacturing Errors\n\n\nTime Consumed\nAbout 30 minutes per 20 experiments\nAbout 25 minutes per 2552 experiments\n\n\nData Management\nNon-standard management method (stored in NAS directory in a different form as an Excel file in a different way for each worker)\nAutomatic loading in DB in standardized form by a scheduler (it is possible to conduct an equipment failure tracing analysis)\n\n\n\n\n\n\nThe noise test was abolished in the whole QC Process because I statistically proved that the noise test result does not have a significant impact on the difference of the mean of Ct, the final result in the whole diagnostic process at the 1% significance level.\nIt turned out to be an opportunity for the Data Science department to increase their understanding of experiments and data generated from equipment.\nIt served as the basis for building a company-wide DB and building a platform architecture."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#background",
    "href": "docs/projects/qc_platform/index.html#background",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "진단 장비의 품질 관리는 의료 장비와 연관된 제품의 특성상 Global Market 진출시 각 나라의 정부에서 요구하는 규제사항 중 하나이다.\n\n시약의 안정성 검증 요구\n장비의 안정성 검증 요구\nSoftware의 안정성 검증 요구\nDiagnostic Algorithm의 안정성 검증 요구\n\n현재 Seegene이 사용하고 있는 진단 장비는 자사 제품이 아니기 때문에 진단 장비의 품질 관리 방식에 어려움이 있다.\nPCR 기기의 noise test는 의료 장비의 QC process 중 하나로서, Seegene의 시약 제품의 성능 안정성과 직접적으로 영향을 주는 품질 검증 시스템이다.\n회사의 규모가 커지고 잦은 조직 개편으로 수동 방식의 noise tests가 여러 부서로 이관됨에 따라 noise test 수행자의 이해도와 숙련도가 떨어져 noise test가 올바르게 수행되지 않고 있다.\n씨젠의 장기 목표인 전사 자동화를 위해 noise test를 자동화하는 프로젝트가 발탁됐다."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#objective",
    "href": "docs/projects/qc_platform/index.html#objective",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "2020.12.19~2021.03.06에 입고된 PCR기기 2201대를 2552번의 실험에서 만들어진 61,248개의 신호에서 얻은 data-driven parameters로 장비의 성능을 평가하여 합격/불합격 뿐만 아니라 장비에 등급을 차등 부여하여 시간에 따른 장비의 성능을 지속적으로 분석 가능하게 한다.\n다음의 주요 문제점을 개선한다.\n\n신호의 증폭 크기에 따라 noise test 결과에 영향을 크게 주어 잘못된 결과를 산출해주는 metric 개선\n단순한 휴먼 에러 신호에 무조건적으로 장비의 불합격처리가 결정 되는 문제 보완하여 robust한 평가체계로 현업부서의 부담을 덜어준다.\n장비 고유에서 발생하는 pattern을 찾아 장비 error 신호를 labeling 한다.\n\n20번의 test를 수동으로 계산하는 과정에 30분이 소요되는 것을 웹 기반의 자동화로 약 2~3분내로 단축시킬 수 있다.\n씨젠의 full automation을 위한 best practice example로 만들어 IT 부분과 제조 부문 및 BT부문과의 협력체계 구축 및 활성화 한다.\n시각화와 noise test result history를 제공하여 추적 및 VOC 대응 system을 구축한다."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#strategies",
    "href": "docs/projects/qc_platform/index.html#strategies",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "현업 부서와의 긴밀한 소통으로 QC process를 세분화하여 앞 단계 QC에서 발생하는 data를 활용하여 뒷 단계 QC인 noise test의 결과를 예측한다.\nnoise가 적다고 확실시 되는 기기에 한해서 noise test 생략\n\n\n\n\n\n전체 QC 프로세스를 자동화 또는 반 자동화\n기존의 noise 측정 metric 분석 및 새로운 metric 생성하여 검사 결과의 정확도를 향상\n시각화와 noise test result history를 제공하여 실무자의 이해도를 높이고 관리가능하게 한다."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#issues",
    "href": "docs/projects/qc_platform/index.html#issues",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "현업 업무 기술서 부재\n실무자의 백업 실수\n부서마다 산재된 데이터\n높은 난이도의 data cleansing\n\n실무자의 데이터로부터 분석 가능한 데이터 선별\n실무자의 데이터의 오입력\ndata 및 문서의 DRM 수동해제\n\nreverse engineering 필요\n잦은 조직 개편으로 관련 인원 및 부서 연락체계 부재"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#required-skills",
    "href": "docs/projects/qc_platform/index.html#required-skills",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Back-end Engineering for DevOps Pipeline Construction\nData Engineering for data cleansing and reverse engineering\nData Modeling for a RDB system construction\nStatistical Analysis for the noise test performance verification\nMachine Learning for pattern analysis\nFront-end Engineering for UI/UX construction\nBiologics, Biophysics, Physics Knowledge"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#colaborators",
    "href": "docs/projects/qc_platform/index.html#colaborators",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "1 data scientist (me) - project owner\n5 mechanical engineers\n4 biologists\n2 patent attorneys\n2 data engineers\n3 full stack developers\n1 advisor (a professor of Computer Science at Seoul National University)"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#acheivements",
    "href": "docs/projects/qc_platform/index.html#acheivements",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Noise Test\nAs-Is\nTo-Be\n\n\n\n\nQC 알고리즘 개발에 사용되는 샘플 크기\nn=100\nSignals from 2552 experiments, n=61,248\n\n\nQC 알고리즘 성능 비교에 사용되는 샘플 크기\nn=61,248\nn=61,248\n\n\nEvaluation Metrics\n2 metrics\n10 metrics ( the exsting 2 metrics + new 8 metrics)\n\n\nInput Process\n특정 프로그램에서 추출한 엑셀 파일의 데이터를 수동으로 복사하여 붙여넣기\n웹 기반 자동화, 다수의 실험 파일 업로드\n\n\nOutput Process\nBatch Evaluation method,\n\n\n\n장비의 신호 중 하나라도 부적합 판정되면 장비 자체가 QC 부적합 판정 (맹점: 휴먼에러 신호가 1개라도 있으면 장비는 무조건 실격 처리)\nDifferential Evaluation method, 장비의 신호에 점수를 계산 후 평균값을 구하고 장비 등급을 A+, A, B, F로 지정. F인 경우 부적합. 오류 신호를 평가에서 제외하므로 오류 신호에 robust\n\n\n\nOutput 1\npass: 92.58%, fail: 7.42% (after excluding many human errors)\nA+ (pass): 7.01%, A (pass): 12.91%, B (pass): 75.72%, F (fail): 4.36%\n\n\nOutput 2\nNA\nVisualized Plots and Tables.\n\n\nOutput 3\nNA\nClassfication Results: Normal Signals, Human Errors, Device Errors, Manufacturing Errors\n\n\nTime Consumed\nAbout 30 minutes per 20 experiments\nAbout 25 minutes per 2552 experiments\n\n\nData Management\nNon-standard management method (작업자마다 다른 방식으로 Excel 파일로 다른 형태로 NAS 디렉토리에 저장)\nRDB uploaded by a scheduler (장비 고장 추적 분석이 가능)\n\n\n\n\n\n\n1% 유의수준에서 노이즈 테스트 결과가 전체 진단 과정에서 최종 결과의 평균 차이에 큰 영향을 미치지 않는다는 것을 통계적으로 증명했기 때문에 전체 QC 프로세스에서 노이즈 테스트를 폐지.\n데이터 사이언스 부서에서 실험과 장비에서 생성된 데이터에 대한 이해도를 높일 수 있는 기회가 됐음.\n전사 DB구축 및 플랫폼 아키텍처 구축의 기반이 됨."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#background-1",
    "href": "docs/projects/qc_platform/index.html#background-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Quality Control of diagnostic equipment is one of the necessities for the regulations required by the government of each country when entering the global market due to the nature of products related to medical equipment.\n\nReagent stability verification & validation required\nEquipment stability verification & validation request\nSoftware stability verification & validation request\nStability Verification & validation Request of Diagnostic Algorithm\n\nSince the diagnostic equipment currently being used by Seegene is not its own product, it is difficult to manage the quality of the diagnostic equipment.\nThe noise test of PCR equipment is one of the QC processes of medical equipments, and it is a quality verification system that directly affects the performance stability of Seegene’s reagent products.\nAs the size of the company grows and frequent organizational reshuffles result in manual noise tests being transferred to various departments, the noise test performers’ understanding and skills are low, resulting in noise tests not being performed correctly.\nA project to automate the noise test was selected for Seegene’s long-term goal of enterprise automation."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#objective-1",
    "href": "docs/projects/qc_platform/index.html#objective-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Data-driven parameters obtained from 61,248 signals from 2552 experiments were used to evaluate 2201 PCR devices, which were stocked between 2020.12.19 and 2021.03.06. yield Enables continuous analysis of equipment performance over time.\nImprove the following major problems:\n\nImproved metrics that produces erroneous results by greatly affecting the noise test result depending on the size of the signal amplification.\nIt relieves the burden on the field department with a robust evaluation system by supplementing the problem of unconditionally determining equipment rejection in response to a simple human error signal.\nEquipment error signals are labeled by finding patterns that occur in equipment.\n\nThe time required for the manual calculation process can be reduced from 30 minutes per 20 tests to about 2 to 3 minutes with web-based automation.\nBy making this project the best practice example of Seegene’s full automation, establish and vitalize the cooperation system between the IT, manufacturing and BT sectors.\nVisualization and noise test result history are provided to build a tracking and VOC response system."
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#strategies-1",
    "href": "docs/projects/qc_platform/index.html#strategies-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "The QC process is subdivided through close communication with the field departments, and the results of the noise test, which is the next stage QC, are predicted by utilizing the data generated in the previous stage QC.\nNoise test omitted only for devices that are certain to have low noise.\n\n\n\n\n\nAutomate or semi-automate the entire QC process\nImprove the accuracy of inspection results by analyzing the existing noise measurement metric and creating a new metric\nVisualization and noise test result history are provided to increase the understanding of practitioners and enable management"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#issues-1",
    "href": "docs/projects/qc_platform/index.html#issues-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Absence of job description\nBackup Mistakes by Practitioners\nData scattered across departments\nHigh level of data cleansing\n\nSelect data that can be analyzed from practitioner data\nIncorrect input of practitioner data\nManual release of DRM for data and documents\n\nreverse engineering required\nAbsence of contact system for related personnel and departments due to frequent organizational reshuffle"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#required-skills-1",
    "href": "docs/projects/qc_platform/index.html#required-skills-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Back-end Engineering for DevOps Pipeline Construction\nData Engineering for data cleansing and reverse engineering\nData Modeling for a RDB system construction\nStatistical Analysis for the noise test performance verification\nMachine Learning for pattern analysis\nFront-end Engineering for UI/UX construction\nBiologics, Biophysics, Physics Knowledge"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#colaborators-1",
    "href": "docs/projects/qc_platform/index.html#colaborators-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "1 data scientist (me) - project owner\n5 mechanical engineers\n4 biologists\n2 patent attorneys\n2 data engineers\n3 full stack developers\n1 advisor (a professor of Computer Science at Seoul National University)"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html#acheivements-1",
    "href": "docs/projects/qc_platform/index.html#acheivements-1",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Noise Test\nAs-Is\nTo-Be\n\n\n\n\nSample Size Used for QC Algorithm Development\nn=100\nSignals from 2552 experiments, n=61,248\n\n\nSample Size Used for QC Algorithm Performance Comparison\nn=61,248\nn=61,248\n\n\nEvaluation Metrics\n2 metrics\n10 metrics ( the exsting 2 metrics + new 8 metrics)\n\n\nInput Process\nmanually copy & paste data of excel files extracted from a certain program\nWeb-Based Automation, upload multiple experiment files\n\n\nOutput Process\nBatch Evaluation method, if even one of the signals from the equipment fails, the equipment fails. (Blind Spot: If there is one human error signal, the equipment is unconditionally disqualified.)\nDifferential Evaluation method, The signals from the equipment are scored, the average value is obtained, and the equipment is graded A+, A, B, and F. Failed if F. Robust on error signals as it excludes the error signal from evaluation.\n\n\nOutput 1\npass: 92.58%, fail: 7.42% (after excluding many human errors)\nA+ (pass): 7.01%, A (pass): 12.91%, B (pass): 75.72%, F (fail): 4.36%\n\n\nOutput 2\nNA\nVisualized Plots and Tables.\n\n\nOutput 3\nNA\nClassfication Results: Normal Signals, Human Errors, Device Errors, Manufacturing Errors\n\n\nTime Consumed\nAbout 30 minutes per 20 experiments\nAbout 25 minutes per 2552 experiments\n\n\nData Management\nNon-standard management method (stored in NAS directory in a different form as an Excel file in a different way for each worker)\nAutomatic loading in DB in standardized form by a scheduler (it is possible to conduct an equipment failure tracing analysis)\n\n\n\n\n\n\nThe noise test was abolished in the whole QC Process because I statistically proved that the noise test result does not have a significant impact on the difference of the mean of Ct, the final result in the whole diagnostic process at the 1% significance level.\nIt turned out to be an opportunity for the Data Science department to increase their understanding of experiments and data generated from equipment.\nIt served as the basis for building a company-wide DB and building a platform architecture."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html",
    "href": "docs/projects/phellinus_linteus/index.html",
    "title": "Project Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n현재까지 알려진 알레르기의 치료는, 증상이 유발된 경우 이에 대 한 즉시적인 약물치료가 전부임. 증상 치료를 위한 약물로서 가장 널리 사용되는 치료약으로는 항히스타민 약품 과 corticosteroid 계통의 약품이며, 지난 수년간 다수의 약물이 개발되었으나 일시적으로 증상을 완화하는 데 그칠 뿐 실제로 알레르기의 병태생리를 기본으로 한 치료법은 현재까지 없다. 따라서 새로운 개념의 알레르기 치료제 개발이 필요한 시점이며, 다음과 같은 몇몇 새로운 알레르기 치료법 개발이 구미와 일본을 중심으로 이루어지고 있다.\n\n\n\n\n\n확보된 알레르기 치료제 시제품이 있으며 이 시제품은 아토피환자를 대상으로 하는 소규모 임상에서 효능을 검증하였으나 이 시제품에 대한 특허 출원이 있지 않으므로 본 과제를 수행하여 특허출원을 이룸으로써 제품의 부가가치를 높일 수 있다.\n\n\n\n\n\n알레르기 동물 모델에서 효능 검증 및 림프구 혈관 생성 모델에서 효능 검증\n\nACA (Active Cutaneous Anaphlaxis) Assay\nPMA (Phorbol 12-myristate 13-acetate)-Induced Dermatitis\n\n림프구 조직에서 혈관생성 효과 검증 및 림프구 조직의 절편 및 신호 분자 분석\n\nLymphangiogenesis에 미치는 영향 분석 using western blot and immunohistochemistry\n\n\n\n\n\n\nAnyderm은 염증반응에 의해 유도되는 신혈관생성 과정을 억제할 수 있다 \nAnyderm은 TpCR 염증 동물 실험 모델에서 염증억제 효과 및 혈관생성과정의 표지단백질의 발현을 억제할 수 있다 \nAnyderm은 염증 동물 모델에서 증가하는 림프의 활성을 억제 하였으며, 이때 증가하는 LYVE-1 단백질의 발현을 저해 하였다 \n\n\n\n\n\n협력과제 책임자의 연구실에서는 지난 수년간 알레르기 반응에서의 신호전달, IgE 의존적, IgE 비의존적 알레르기 유발 동물모델을 이용한 항알레르기성 물질의 발굴등에 관한 많은 연구를 수행한 바 있다. 따라서 본 과제의 수행에 적합한 동물모델과 입증된 기술력을 갖고 있으므로 본 과제의 성공 가능성이 높다.\n천연물 발효추출물을 알레르기 치료제 시제품은 독성이 없으므로 chemical 기반의 치료제에 비해 유리한 면이 있다.\n천연물 발효추출물은 발효기법에 따라 다양한 발효추출물을 얻을 수 있다.\n\n\n\n\n\n\n\nThe treatment of allergies known so far is all about immediate drug treatment when the symptoms are triggered. Antihistamine drugs and corticosteroid drugs are the most widely used drugs for the treatment of symptoms. A number of drugs have been developed over the past few years, but only temporarily alleviate symptoms. Therefore, it is time to develop a new concept of allergy treatment, and several new allergy treatments are being developed in Europe and Japan as follows.\n\n\n\n\n\nThere is a secured allergy treatment prototype, and the efficacy of this prototype has been verified in small-scale clinical trials for atopic patients, but there is no patent application for this prototype.\n\n\n\n\n\nEfficacy validation in allergic animal models and efficacy validation in lymphocyte angiogenesis models\n\nACA (Active Cutaneous Anaphlaxis) Assay\nPMA (Phorbol 12-myristate 13-acetate)-Induced Dermatitis\n\nVerification of angiogenic effect in lymphoid tissue and analysis of slices and signal molecules in lymphoid tissue\n\nAnalysis of the effect on lymphangiogenesis using western blot and immunohistochemistry\n\n\n\n\n\n\nThis reagent can inhibit the angiogenesis process induced by the inflammatory response. \nThis reagent can suppress inflammation inhibitory effect and expression of marker protein of angiogenesis process in TpCR inflammatory animal model. \nThis reagent suppressed the increased lymphatic activity in an inflammatory animal model, and inhibited the increased LYVE-1 protein expression. \n\n\n\n\n\nOver the past few years, the laboratory of the person in charge of the cooperative project has conducted many studies on signal transduction in allergic reactions, discovery of anti-allergic substances using IgE-dependent and IgE-independent allergy-inducing animal models. Therefore, since we have an animal model suitable for the performance of this task and proven technology, the possibility of success of this task is high.\nAllergic treatment prototypes made from fermented extracts of natural substances are non-toxic, so they have an advantage over chemical-based treatments.\nVarious fermented extracts can be obtained from natural product fermentation extracts according to fermentation techniques."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#introduction",
    "href": "docs/projects/phellinus_linteus/index.html#introduction",
    "title": "Project Description",
    "section": "",
    "text": "현재까지 알려진 알레르기의 치료는, 증상이 유발된 경우 이에 대 한 즉시적인 약물치료가 전부임. 증상 치료를 위한 약물로서 가장 널리 사용되는 치료약으로는 항히스타민 약품 과 corticosteroid 계통의 약품이며, 지난 수년간 다수의 약물이 개발되었으나 일시적으로 증상을 완화하는 데 그칠 뿐 실제로 알레르기의 병태생리를 기본으로 한 치료법은 현재까지 없다. 따라서 새로운 개념의 알레르기 치료제 개발이 필요한 시점이며, 다음과 같은 몇몇 새로운 알레르기 치료법 개발이 구미와 일본을 중심으로 이루어지고 있다."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#objective",
    "href": "docs/projects/phellinus_linteus/index.html#objective",
    "title": "Project Description",
    "section": "",
    "text": "확보된 알레르기 치료제 시제품이 있으며 이 시제품은 아토피환자를 대상으로 하는 소규모 임상에서 효능을 검증하였으나 이 시제품에 대한 특허 출원이 있지 않으므로 본 과제를 수행하여 특허출원을 이룸으로써 제품의 부가가치를 높일 수 있다."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#methodology",
    "href": "docs/projects/phellinus_linteus/index.html#methodology",
    "title": "Project Description",
    "section": "",
    "text": "알레르기 동물 모델에서 효능 검증 및 림프구 혈관 생성 모델에서 효능 검증\n\nACA (Active Cutaneous Anaphlaxis) Assay\nPMA (Phorbol 12-myristate 13-acetate)-Induced Dermatitis\n\n림프구 조직에서 혈관생성 효과 검증 및 림프구 조직의 절편 및 신호 분자 분석\n\nLymphangiogenesis에 미치는 영향 분석 using western blot and immunohistochemistry"
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#result",
    "href": "docs/projects/phellinus_linteus/index.html#result",
    "title": "Project Description",
    "section": "",
    "text": "Anyderm은 염증반응에 의해 유도되는 신혈관생성 과정을 억제할 수 있다 \nAnyderm은 TpCR 염증 동물 실험 모델에서 염증억제 효과 및 혈관생성과정의 표지단백질의 발현을 억제할 수 있다 \nAnyderm은 염증 동물 모델에서 증가하는 림프의 활성을 억제 하였으며, 이때 증가하는 LYVE-1 단백질의 발현을 저해 하였다"
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#expected-effect",
    "href": "docs/projects/phellinus_linteus/index.html#expected-effect",
    "title": "Project Description",
    "section": "",
    "text": "협력과제 책임자의 연구실에서는 지난 수년간 알레르기 반응에서의 신호전달, IgE 의존적, IgE 비의존적 알레르기 유발 동물모델을 이용한 항알레르기성 물질의 발굴등에 관한 많은 연구를 수행한 바 있다. 따라서 본 과제의 수행에 적합한 동물모델과 입증된 기술력을 갖고 있으므로 본 과제의 성공 가능성이 높다.\n천연물 발효추출물을 알레르기 치료제 시제품은 독성이 없으므로 chemical 기반의 치료제에 비해 유리한 면이 있다.\n천연물 발효추출물은 발효기법에 따라 다양한 발효추출물을 얻을 수 있다."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#introduction-1",
    "href": "docs/projects/phellinus_linteus/index.html#introduction-1",
    "title": "Project Description",
    "section": "",
    "text": "The treatment of allergies known so far is all about immediate drug treatment when the symptoms are triggered. Antihistamine drugs and corticosteroid drugs are the most widely used drugs for the treatment of symptoms. A number of drugs have been developed over the past few years, but only temporarily alleviate symptoms. Therefore, it is time to develop a new concept of allergy treatment, and several new allergy treatments are being developed in Europe and Japan as follows."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#objective-1",
    "href": "docs/projects/phellinus_linteus/index.html#objective-1",
    "title": "Project Description",
    "section": "",
    "text": "There is a secured allergy treatment prototype, and the efficacy of this prototype has been verified in small-scale clinical trials for atopic patients, but there is no patent application for this prototype."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#methodology-1",
    "href": "docs/projects/phellinus_linteus/index.html#methodology-1",
    "title": "Project Description",
    "section": "",
    "text": "Efficacy validation in allergic animal models and efficacy validation in lymphocyte angiogenesis models\n\nACA (Active Cutaneous Anaphlaxis) Assay\nPMA (Phorbol 12-myristate 13-acetate)-Induced Dermatitis\n\nVerification of angiogenic effect in lymphoid tissue and analysis of slices and signal molecules in lymphoid tissue\n\nAnalysis of the effect on lymphangiogenesis using western blot and immunohistochemistry"
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#result-1",
    "href": "docs/projects/phellinus_linteus/index.html#result-1",
    "title": "Project Description",
    "section": "",
    "text": "This reagent can inhibit the angiogenesis process induced by the inflammatory response. \nThis reagent can suppress inflammation inhibitory effect and expression of marker protein of angiogenesis process in TpCR inflammatory animal model. \nThis reagent suppressed the increased lymphatic activity in an inflammatory animal model, and inhibited the increased LYVE-1 protein expression."
  },
  {
    "objectID": "docs/projects/phellinus_linteus/index.html#expected-effect-1",
    "href": "docs/projects/phellinus_linteus/index.html#expected-effect-1",
    "title": "Project Description",
    "section": "",
    "text": "Over the past few years, the laboratory of the person in charge of the cooperative project has conducted many studies on signal transduction in allergic reactions, discovery of anti-allergic substances using IgE-dependent and IgE-independent allergy-inducing animal models. Therefore, since we have an animal model suitable for the performance of this task and proven technology, the possibility of success of this task is high.\nAllergic treatment prototypes made from fermented extracts of natural substances are non-toxic, so they have an advantage over chemical-based treatments.\nVarious fermented extracts can be obtained from natural product fermentation extracts according to fermentation techniques."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html",
    "href": "docs/projects/LLFS/self_description.html",
    "title": "Project Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n보안상의 이유로 프로젝트에서 사용됐던 실제 data를 사용하지 못하기 때문에 분석을 위해 사용됐던 방법론을 구체적으로 보여주기 어렵다. 이에 따라 대략적인 분석 방식을 고차원의 서로 상관 관계가 있는 data를 simulation을 통해 만들어 보여주려고 한다.\n\n\n\n이 시뮬레이션 연구의 목표는 AD와 None AD와 관련된 biomarkers를 구별할 수 있는 일련의 예측인자(또는 대사물질 또는 생화학물질)를 식별하는데 사용됐던 방법론을 소개하는 것이다.\n\n\n\n\n보안 문제로 인해 이 프로젝트에 사용된 실제 데이터와 전체 분석 파이프라인을 보여주기 어렵다.\n이 시뮬레이션 연구에서는 다변량 정규분포 하에서 대사 물질 데이터를 생성하여 대사 단계에서 가상의 데이터를 생성하고 분석 방법론을 기술하는 데에만 집중할 것이다.\n시뮬레이션 경험이 많지 않아 시뮬레이션이 수학적으로 통계적으로 틀린 부분이 있을 수 있다.\n시뮬레이션은 내가 수행했던 분석 방법론을 간단히 재현하는 용도로 사용하는 것이기 때문에 시뮬레이션 자체에 많은 시간을 할애하진 않았다.\n시뮬레이션 데이터는 실제 연구를 위해 표본으로 쓰인 sample 데이터의 분포를 전혀 반영하지 않았다.\n이 시뮬레이션 연구에서, 실제 데이터의 분포를 반영하지 않았고 범주형 변수 및 연속형 변수와 종속 변수를 통계적으로 잘 연관시키지 못했기 때문에 분석 결과가 생물학적인 사실과 많이 다를 수 있다.\n\n\n\n\n\nOperating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section\n\n\n\n\n\n\n\nIt is difficult to show the methodology used for analysis in detail because the actual data used in the project cannot be used for security reasons. Accordingly, I am going to show a rough analysis method through simulation of high-dimensional, mutually correlated data.\n\n\n\nThe aim of this simulation study is to identify a set of predictors (or metabolites or bio-chemicals) that will enable to differentiate bio-markers that are associated with AD vs. non-AD.\n\n\n\n\nIn this article, due to security concerns, it is difficult to display the real data and the entire analysis pipeline used in this project.\nIn this simulation study, I will focus only on generating fake data at the metabolomic stage by generating data under multivariate normal distributions.\nSince I don’t have much experience in simulation, there may be mathematically and statistically incorrect parts in the simulation.\nI did not put a lot of effort into the simulation itself because the simulation was used to simply reproduce the analysis methodology I had performed.\nThe simulated data does not reflect the distribution of the truely sampled data used in the LLFS at all.\nIn this simulation, since the categorical and continuous variables and the dependent variable could not be statistically associated properly, the analysis result for the discrete variables could be very different from the biological or medical fact.\n\n\n\n\n\nOperating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section"
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#introduction",
    "href": "docs/projects/LLFS/self_description.html#introduction",
    "title": "Project Description",
    "section": "",
    "text": "보안상의 이유로 프로젝트에서 사용됐던 실제 data를 사용하지 못하기 때문에 분석을 위해 사용됐던 방법론을 구체적으로 보여주기 어렵다. 이에 따라 대략적인 분석 방식을 고차원의 서로 상관 관계가 있는 data를 simulation을 통해 만들어 보여주려고 한다."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#goal",
    "href": "docs/projects/LLFS/self_description.html#goal",
    "title": "Project Description",
    "section": "",
    "text": "이 시뮬레이션 연구의 목표는 AD와 None AD와 관련된 biomarkers를 구별할 수 있는 일련의 예측인자(또는 대사물질 또는 생화학물질)를 식별하는데 사용됐던 방법론을 소개하는 것이다."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#feature",
    "href": "docs/projects/LLFS/self_description.html#feature",
    "title": "Project Description",
    "section": "",
    "text": "보안 문제로 인해 이 프로젝트에 사용된 실제 데이터와 전체 분석 파이프라인을 보여주기 어렵다.\n이 시뮬레이션 연구에서는 다변량 정규분포 하에서 대사 물질 데이터를 생성하여 대사 단계에서 가상의 데이터를 생성하고 분석 방법론을 기술하는 데에만 집중할 것이다.\n시뮬레이션 경험이 많지 않아 시뮬레이션이 수학적으로 통계적으로 틀린 부분이 있을 수 있다.\n시뮬레이션은 내가 수행했던 분석 방법론을 간단히 재현하는 용도로 사용하는 것이기 때문에 시뮬레이션 자체에 많은 시간을 할애하진 않았다.\n시뮬레이션 데이터는 실제 연구를 위해 표본으로 쓰인 sample 데이터의 분포를 전혀 반영하지 않았다.\n이 시뮬레이션 연구에서, 실제 데이터의 분포를 반영하지 않았고 범주형 변수 및 연속형 변수와 종속 변수를 통계적으로 잘 연관시키지 못했기 때문에 분석 결과가 생물학적인 사실과 많이 다를 수 있다."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#development-environment",
    "href": "docs/projects/LLFS/self_description.html#development-environment",
    "title": "Project Description",
    "section": "",
    "text": "Operating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section"
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#introduction-1",
    "href": "docs/projects/LLFS/self_description.html#introduction-1",
    "title": "Project Description",
    "section": "",
    "text": "It is difficult to show the methodology used for analysis in detail because the actual data used in the project cannot be used for security reasons. Accordingly, I am going to show a rough analysis method through simulation of high-dimensional, mutually correlated data."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#goal-1",
    "href": "docs/projects/LLFS/self_description.html#goal-1",
    "title": "Project Description",
    "section": "",
    "text": "The aim of this simulation study is to identify a set of predictors (or metabolites or bio-chemicals) that will enable to differentiate bio-markers that are associated with AD vs. non-AD."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#feature-1",
    "href": "docs/projects/LLFS/self_description.html#feature-1",
    "title": "Project Description",
    "section": "",
    "text": "In this article, due to security concerns, it is difficult to display the real data and the entire analysis pipeline used in this project.\nIn this simulation study, I will focus only on generating fake data at the metabolomic stage by generating data under multivariate normal distributions.\nSince I don’t have much experience in simulation, there may be mathematically and statistically incorrect parts in the simulation.\nI did not put a lot of effort into the simulation itself because the simulation was used to simply reproduce the analysis methodology I had performed.\nThe simulated data does not reflect the distribution of the truely sampled data used in the LLFS at all.\nIn this simulation, since the categorical and continuous variables and the dependent variable could not be statistically associated properly, the analysis result for the discrete variables could be very different from the biological or medical fact."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#development-environment-1",
    "href": "docs/projects/LLFS/self_description.html#development-environment-1",
    "title": "Project Description",
    "section": "",
    "text": "Operating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html",
    "href": "docs/projects/LLFS/project_description.html",
    "title": "Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n알츠하이머병(Alzheimer’s Disease, AD)은 수백만 명의 미국인에게 영향을 미치는 가장 흔한 형태의 치매이다. 알츠하이머병은 기억력, 사고력 및 행동에 영향을 주지만 증상이 나타나기까지 거의 20년에 걸쳐 진행이 된다. 따라서 전 임상 단계에서 생리학을 이해하는 것이 필수적이다. 유전적 요인이 AD에 거의 50% 기여하는 것으로 추정된다. 유전자가 세포 환경을 변경하여 알츠하이머병 위험에 어떻게 기여하는지 더 잘 이해하기 위해 AD와 연관이 있는 유전자인 APOE를 보유한 사람들의 대사체(Metabolome)를 조사했다. 대사체는 유전체(Genome)과 단백질체(Proteome)에서 생성된 산물을 의미한다. 이러한 생화학 부산물은 유전적 요인과 환경적 요인 모두의 영향을 받는다. 모집단은 장수마을에 사는 Caucasian (백인) 참여자들이다.\n\n\n\nLLFS(Long Life Family Study) 프로젝트의 목적은 유전체, 전사체, 단백질체 및 대사체 단계를 통해 유전체에서 대사체 단계에 이르는 여러 단계에서 통계 및 기계 학습을 사용하여 분석 파이프라인을 구축하고 알츠하이머병에 대한 중요한 바이오마커를 식별하는 것이다.\n\n\n\n읽기의 편의성을 위해 LLFS project에 대한 설명을 project와 self project 와 같이 2개의 section으로 나누었다. project는 내가 실제로 프로젝트를 수행했던 과정을 기술했고 self project는 그 방법론을 대략적으로 간소화된 형태로 기술했다.\n\nProject Description (Current)\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    subgraph Data_Collection\n        direction TB\n        Multi_Centerd_Blood_Sampling---\n        Mass_Spectrometry---\n        Data_Transfer\n    end\n    subgraph Quality_Control\n        direction TB\n        Identify_Anomaly_Data---\n        Identify_Missing_Values\n    end\n    subgraph Analytics\n        direction TB \n        EDA---\n        Data_Mining---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n    subgraph Reporting_and_Conclusion\n        direction TB \n        Share_with_Faculty\n    end\n\nData_Collection--&gt;Quality_Control--&gt;Analytics--&gt;Reporting_and_Conclusion\n\n\n\n\n\nData는 장수 마을에 거주하는 백인을 대상으로 New York, Bonston, Pittsburgh 및 Denmark에 있는 여러 medical centers에서 sampled blood를 MS Spectromtetry로 Digitalization을 했다. 여러 과정을 통해 data를 csv형태로 받아 data의 QC(Quality Control)를 진행한뒤 Data 분석 업무를 수행했다. EDA (Exploratory Data Analysis) 와 Data Mining을 통해 data에 대한 이해도를 높였고 이를 토대로 통계 분석과 machine learning을 이용하여 이 data에 적합한 모형을 찾았다. 모든 결과물은 The Taub Institute for Research on Alzheimer’s Disease and the Aging Brain 의 biostaticians, medical doctors, biologists, neurologists, bioinformaticians 및 epidemiologists와 공유를 했다.\n\n\n\n\n\n\n\nflowchart LR\n    subgraph Data_Quality_Control\n        direction TB\n        identify_anomaly_data---\n        identify_missing_values\n        subgraph Missing_Value_Analysis\n            direction LR\n            MCAR---\n            MAR---\n            MNAR\n        end\n        either_imputation_or_omission---\n        communication_with_labs---\n        set_data_inclusion/exclusion_criteria\n    end\n    identify_missing_values---Missing_Value_Analysis---either_imputation_or_omission\n    subgraph Data_Preprocessing\n        direction TB\n        data_transformation---\n        log_transformation---\n        standardization\n    end\n\nData_Quality_Control--&gt;Data_Preprocessing\n\n\n\n\n\n\nData의 품질 관리를 위해 data를 생성한 biochemists와 소통하여 실험실 기준에 따라 결측치와 이상치를 구분하여 labeling을 수행했고 missing value analysis를 통해 결과에 따라 medical doctors를 포함한 다른 faculty members와 상의하여 결측치 처리를 했다. data QC criteria는 rowwise 와 columwise sum의 합이 sample size에 대하여 missing values의 비율이 5%가 넘는 환자와 변수는 분석 대상에서 제외 됐다. 모든 metabolites data는 log transoformation 과 standardization을 통해 data의 단위를 표준화 했다.\n\n\n\n\n\n\n\nflowchart TB\n    subgraph Data_Analytics\n        direction TB\n        Exploratory_Data_Analysis---\n        Data_Minig---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n\n\n\n\n\nData 분석은 크게 EDA (Exploratory Data Analysis), Statistical Analysis 및 Machine Learning과 같이 3 단계로 수행했다. 각 단계에서 나온 결과가 각 각의 단계에서 일관되게 나오는 metabolites를 선별했다.\n\n\nstudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests 및 regression analysis이 수행됐고 visualization을 통해 검정 결과를 재확인하는 작업을 수행했다. 고차원 데이터를 시각화하여 data의 pattern을 관찰하기 위해 KNN, PCA, K means clustering 및 DB Scan을 이용했다.\n\n\n\nmultivariable linear regression, logistic regression 및 Cox PH(Proportional Hazards) regression anayses 가 수행됐고 질병과 유의한 metabolites를 선별했다. multiple testing으로 인한 1 종 오류를 범하는 것을 줄이기 위해 permuted p-values를 계산하여 유의성을 한번 더 확인했다.\n\n\n\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, SVM (support vector machine), partial least square 및 sparse partial least square가 사용됐다. 질병을 가장 잘 예측하는 classifier를 평가하여 최적의 classifier를 선택했다.\n\n\n\n\n146개의 관측치와 약 3,000여개의 변수로 구성된 data에서 약 60개 내외의 대사물질이 질병과 5% 유의수준으로 유의한 관계가 있는 것으로 관찰됐고 partial least suare 가 가장 성능이 좋은 것으로 관찰됐다.\n\n\n\n\n\n\nAlzheimer Disease (AD) is the most common form of dementia that affects millions of Americans. AD affects memory, thinking and behavior, but its progression is slow, spanning nearly two decades before the symptoms appear. Thus, it is imperative to understand the physiology at the pre-clinical stage. It is estimated that genetic factors contribute nearly 50% to AD. To better understand how genes contribute to the risk of AD by altering cellular milieu, I have examined the metabolome of individuals with the AD-related genotype, APOE. The metabolome represents the products that were generated from the genome and proteome. These biochemical products represent influences of both genetic and environmental factors. The population is Caucasian participants living in longevity village.\n\n\n\nThe objective of the Long Life Family Study (LLFS) project was to build an analysis pipeline of identifying significant biomarkers for AD using statistics and machine learning at the multi-stages from the genomic to the metabolomic stage through the transcriptomic and proteomic stage.\n\n\n\nFor the convenience of reading, the LLFS project is divided into the two sections: project and self-project. The project section roughly described the process of the project I actually carried out, and the self project one described the methodology in a roughly simplified form.\n\nProject Description (Current)\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster0\n\n Data Collection  \n\ncluster1\n\n Quality Control  \n\ncluster2\n\n Analytics  \n\ncluster3\n\n Reporting and Conclusion   \n\nMulti_Centered_Blood_Sampling\n\n Multi_Centered_Blood_Sampling   \n\nMass_Spectrometry\n\n Mass_Spectrometry   \n\nData_Transfer\n\n Data_Transfer   \n\nIdentify_Anomaly_Data\n\n Identify_Anomaly_Data   \n\nData_Transfer-&gt;Identify_Anomaly_Data\n\n    \n\nIdentify_Missing_Values\n\n Identify_Missing_Values   \n\nEDA\n\n EDA   \n\nIdentify_Missing_Values-&gt;EDA\n\n    \n\nData_Mining\n\n Data_Mining   \n\nStatistical_Analysis\n\n Statistical_Analysis   \n\nMachine_Learning\n\n Machine_Learning   \n\nShare_with_Faculty\n\n Share_with_Faculty   \n\nMachine_Learning-&gt;Share_with_Faculty\n\n   \n\n\n\n\n\nData were obtained by digitization through MS Spectromtetry of blood samples from multiple medical centers in New York, Bonston, Pittsburgh, and Denmark for Caucasians residing in longevity villages. After receiving the data in a csv format through various processes, QC (Quality Control) of the data and data analysis were performed. To better understand data, exploratory data analysis (EDA) and data mining were conducted. Based on the analysis findings on data, the machine learning model to explain the data most was selcted. All findings were shared with biostatisticians, medical doctors, biologists, neurologists and epidemiologists at the neurology department and the Taub Institute for Research on Alzheimer’s Disease and the Aging Brain in the Columbia University Irving Medical Center.\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster2\n\n Data Preprocessing  \n\ncluster0\n\n Data Quality Control  \n\ncluster1\n\n Missing Value Analysis   \n\nidentify_anomaly_data\n\n identify_anomaly_data   \n\nidentify_missing_values\n\n identify_missing_values   \n\nidentify_anomaly_data-&gt;identify_missing_values\n\n    \n\nMissing_Completely_At_Random\n\n Missing_Completely_At_Random   \n\nidentify_missing_values-&gt;Missing_Completely_At_Random\n\n    \n\nMissing_At_Random\n\n Missing_At_Random   \n\nMissing_Completely_At_Random-&gt;Missing_At_Random\n\n    \n\nMissing_Not_at_Random\n\n Missing_Not_at_Random   \n\nMissing_At_Random-&gt;Missing_Not_at_Random\n\n    \n\neither_imputation_or_omission\n\n either_imputation_or_omission   \n\nMissing_Not_at_Random-&gt;either_imputation_or_omission\n\n    \n\ncommunication_with_labs\n\n communication_with_labs   \n\neither_imputation_or_omission-&gt;communication_with_labs\n\n    \n\nset_data_inclusion_exclusion_criteria\n\n set_data_inclusion_exclusion_criteria   \n\ncommunication_with_labs-&gt;set_data_inclusion_exclusion_criteria\n\n    \n\nData_Transformation\n\n Data_Transformation   \n\nset_data_inclusion_exclusion_criteria-&gt;Data_Transformation\n\n    \n\nLog_Transformation\n\n Log_Transformation   \n\nData_Transformation-&gt;Log_Transformation\n\n    \n\nStandardization\n\n Standardization   \n\nLog_Transformation-&gt;Standardization\n\n   \n\n\n\n\n\nFor data quality control, I communicated with whom generated the data, classified missing values ​​and outliers according to laboratory standards, and labeled them. Based on the results through missing value analysis, I processed the missing values through consultation with the faculty members several times. For the data QC criteria, patients and variables whose ratio of missing values ​​for the sum of the rowwise and columnwise sums exceeded 5% for the sample size were excluded from the analysis. All metabolites data were standardized through log transformation and standardization.\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster2\n\n Data Analytics   \n\nExploratory_Data_Analysis\n\n Exploratory_Data_Analysis   \n\nData_Minig\n\n Data_Minig   \n\nExploratory_Data_Analysis-&gt;Data_Minig\n\n    \n\nStatistical_Analysis\n\n Statistical_Analysis   \n\nData_Minig-&gt;Statistical_Analysis\n\n    \n\nMachine_Learning\n\n Machine_Learning   \n\nStatistical_Analysis-&gt;Machine_Learning\n\n   \n\n\n\n\n\nData analysis was performed in three stages: Exploratory Data Analysis (EDA), Statistical Analysis, and Machine Learning. In each stage, metabolites commonly associated with diseases were selected.\n\n\nStudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests, and regression testing were performed, and I visualizaed data to reconfirm the test results. To visualize high-dimensional data and observe data patterns, KNN, PCA, K means, Clustering, and DB Scan were used.\n\n\n\nMultivariable linear regression, logistic regression, and Cox PH (Proportional Hazards) regression analyses were conducted and the metabolites that are signficantly associated with the disease status were selected. In order to reduce the possibility of making a type 1 error due to multiple testing, the significance was checked once more by calculating permuted p-values.\n\n\n\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, support vector machine (SVM), partial least square, and sparse partial least square were used. The optimal classifier was selected by evaluating the classifier that best predicted the disease status.\n\n\n\n\nIn the data consisting of 146 observations and about 3,000 variables, about 60 metabolites were observed to have a significant relationship with the disease at the 5% significance level, and partial least suare was observed to perform the best."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#background",
    "href": "docs/projects/LLFS/project_description.html#background",
    "title": "Description",
    "section": "",
    "text": "알츠하이머병(Alzheimer’s Disease, AD)은 수백만 명의 미국인에게 영향을 미치는 가장 흔한 형태의 치매이다. 알츠하이머병은 기억력, 사고력 및 행동에 영향을 주지만 증상이 나타나기까지 거의 20년에 걸쳐 진행이 된다. 따라서 전 임상 단계에서 생리학을 이해하는 것이 필수적이다. 유전적 요인이 AD에 거의 50% 기여하는 것으로 추정된다. 유전자가 세포 환경을 변경하여 알츠하이머병 위험에 어떻게 기여하는지 더 잘 이해하기 위해 AD와 연관이 있는 유전자인 APOE를 보유한 사람들의 대사체(Metabolome)를 조사했다. 대사체는 유전체(Genome)과 단백질체(Proteome)에서 생성된 산물을 의미한다. 이러한 생화학 부산물은 유전적 요인과 환경적 요인 모두의 영향을 받는다. 모집단은 장수마을에 사는 Caucasian (백인) 참여자들이다."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#objective",
    "href": "docs/projects/LLFS/project_description.html#objective",
    "title": "Description",
    "section": "",
    "text": "LLFS(Long Life Family Study) 프로젝트의 목적은 유전체, 전사체, 단백질체 및 대사체 단계를 통해 유전체에서 대사체 단계에 이르는 여러 단계에서 통계 및 기계 학습을 사용하여 분석 파이프라인을 구축하고 알츠하이머병에 대한 중요한 바이오마커를 식별하는 것이다."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#how-to-read-the-contents",
    "href": "docs/projects/LLFS/project_description.html#how-to-read-the-contents",
    "title": "Description",
    "section": "",
    "text": "읽기의 편의성을 위해 LLFS project에 대한 설명을 project와 self project 와 같이 2개의 section으로 나누었다. project는 내가 실제로 프로젝트를 수행했던 과정을 기술했고 self project는 그 방법론을 대략적으로 간소화된 형태로 기술했다.\n\nProject Description (Current)"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#analysis-pipeline-architecture",
    "href": "docs/projects/LLFS/project_description.html#analysis-pipeline-architecture",
    "title": "Description",
    "section": "",
    "text": "flowchart LR\n    subgraph Data_Collection\n        direction TB\n        Multi_Centerd_Blood_Sampling---\n        Mass_Spectrometry---\n        Data_Transfer\n    end\n    subgraph Quality_Control\n        direction TB\n        Identify_Anomaly_Data---\n        Identify_Missing_Values\n    end\n    subgraph Analytics\n        direction TB \n        EDA---\n        Data_Mining---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n    subgraph Reporting_and_Conclusion\n        direction TB \n        Share_with_Faculty\n    end\n\nData_Collection--&gt;Quality_Control--&gt;Analytics--&gt;Reporting_and_Conclusion\n\n\n\n\n\nData는 장수 마을에 거주하는 백인을 대상으로 New York, Bonston, Pittsburgh 및 Denmark에 있는 여러 medical centers에서 sampled blood를 MS Spectromtetry로 Digitalization을 했다. 여러 과정을 통해 data를 csv형태로 받아 data의 QC(Quality Control)를 진행한뒤 Data 분석 업무를 수행했다. EDA (Exploratory Data Analysis) 와 Data Mining을 통해 data에 대한 이해도를 높였고 이를 토대로 통계 분석과 machine learning을 이용하여 이 data에 적합한 모형을 찾았다. 모든 결과물은 The Taub Institute for Research on Alzheimer’s Disease and the Aging Brain 의 biostaticians, medical doctors, biologists, neurologists, bioinformaticians 및 epidemiologists와 공유를 했다.\n\n\n\n\n\n\n\nflowchart LR\n    subgraph Data_Quality_Control\n        direction TB\n        identify_anomaly_data---\n        identify_missing_values\n        subgraph Missing_Value_Analysis\n            direction LR\n            MCAR---\n            MAR---\n            MNAR\n        end\n        either_imputation_or_omission---\n        communication_with_labs---\n        set_data_inclusion/exclusion_criteria\n    end\n    identify_missing_values---Missing_Value_Analysis---either_imputation_or_omission\n    subgraph Data_Preprocessing\n        direction TB\n        data_transformation---\n        log_transformation---\n        standardization\n    end\n\nData_Quality_Control--&gt;Data_Preprocessing\n\n\n\n\n\n\nData의 품질 관리를 위해 data를 생성한 biochemists와 소통하여 실험실 기준에 따라 결측치와 이상치를 구분하여 labeling을 수행했고 missing value analysis를 통해 결과에 따라 medical doctors를 포함한 다른 faculty members와 상의하여 결측치 처리를 했다. data QC criteria는 rowwise 와 columwise sum의 합이 sample size에 대하여 missing values의 비율이 5%가 넘는 환자와 변수는 분석 대상에서 제외 됐다. 모든 metabolites data는 log transoformation 과 standardization을 통해 data의 단위를 표준화 했다.\n\n\n\n\n\n\n\nflowchart TB\n    subgraph Data_Analytics\n        direction TB\n        Exploratory_Data_Analysis---\n        Data_Minig---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n\n\n\n\n\nData 분석은 크게 EDA (Exploratory Data Analysis), Statistical Analysis 및 Machine Learning과 같이 3 단계로 수행했다. 각 단계에서 나온 결과가 각 각의 단계에서 일관되게 나오는 metabolites를 선별했다.\n\n\nstudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests 및 regression analysis이 수행됐고 visualization을 통해 검정 결과를 재확인하는 작업을 수행했다. 고차원 데이터를 시각화하여 data의 pattern을 관찰하기 위해 KNN, PCA, K means clustering 및 DB Scan을 이용했다.\n\n\n\nmultivariable linear regression, logistic regression 및 Cox PH(Proportional Hazards) regression anayses 가 수행됐고 질병과 유의한 metabolites를 선별했다. multiple testing으로 인한 1 종 오류를 범하는 것을 줄이기 위해 permuted p-values를 계산하여 유의성을 한번 더 확인했다.\n\n\n\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, SVM (support vector machine), partial least square 및 sparse partial least square가 사용됐다. 질병을 가장 잘 예측하는 classifier를 평가하여 최적의 classifier를 선택했다.\n\n\n\n\n146개의 관측치와 약 3,000여개의 변수로 구성된 data에서 약 60개 내외의 대사물질이 질병과 5% 유의수준으로 유의한 관계가 있는 것으로 관찰됐고 partial least suare 가 가장 성능이 좋은 것으로 관찰됐다."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#background-1",
    "href": "docs/projects/LLFS/project_description.html#background-1",
    "title": "Description",
    "section": "",
    "text": "Alzheimer Disease (AD) is the most common form of dementia that affects millions of Americans. AD affects memory, thinking and behavior, but its progression is slow, spanning nearly two decades before the symptoms appear. Thus, it is imperative to understand the physiology at the pre-clinical stage. It is estimated that genetic factors contribute nearly 50% to AD. To better understand how genes contribute to the risk of AD by altering cellular milieu, I have examined the metabolome of individuals with the AD-related genotype, APOE. The metabolome represents the products that were generated from the genome and proteome. These biochemical products represent influences of both genetic and environmental factors. The population is Caucasian participants living in longevity village."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#objective-1",
    "href": "docs/projects/LLFS/project_description.html#objective-1",
    "title": "Description",
    "section": "",
    "text": "The objective of the Long Life Family Study (LLFS) project was to build an analysis pipeline of identifying significant biomarkers for AD using statistics and machine learning at the multi-stages from the genomic to the metabolomic stage through the transcriptomic and proteomic stage."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#how-to-read-the-contents-1",
    "href": "docs/projects/LLFS/project_description.html#how-to-read-the-contents-1",
    "title": "Description",
    "section": "",
    "text": "For the convenience of reading, the LLFS project is divided into the two sections: project and self-project. The project section roughly described the process of the project I actually carried out, and the self project one described the methodology in a roughly simplified form.\n\nProject Description (Current)"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#analysis-pipeline-architecture-1",
    "href": "docs/projects/LLFS/project_description.html#analysis-pipeline-architecture-1",
    "title": "Description",
    "section": "",
    "text": "G\n\n \n\ncluster0\n\n Data Collection  \n\ncluster1\n\n Quality Control  \n\ncluster2\n\n Analytics  \n\ncluster3\n\n Reporting and Conclusion   \n\nMulti_Centered_Blood_Sampling\n\n Multi_Centered_Blood_Sampling   \n\nMass_Spectrometry\n\n Mass_Spectrometry   \n\nData_Transfer\n\n Data_Transfer   \n\nIdentify_Anomaly_Data\n\n Identify_Anomaly_Data   \n\nData_Transfer-&gt;Identify_Anomaly_Data\n\n    \n\nIdentify_Missing_Values\n\n Identify_Missing_Values   \n\nEDA\n\n EDA   \n\nIdentify_Missing_Values-&gt;EDA\n\n    \n\nData_Mining\n\n Data_Mining   \n\nStatistical_Analysis\n\n Statistical_Analysis   \n\nMachine_Learning\n\n Machine_Learning   \n\nShare_with_Faculty\n\n Share_with_Faculty   \n\nMachine_Learning-&gt;Share_with_Faculty\n\n   \n\n\n\n\n\nData were obtained by digitization through MS Spectromtetry of blood samples from multiple medical centers in New York, Bonston, Pittsburgh, and Denmark for Caucasians residing in longevity villages. After receiving the data in a csv format through various processes, QC (Quality Control) of the data and data analysis were performed. To better understand data, exploratory data analysis (EDA) and data mining were conducted. Based on the analysis findings on data, the machine learning model to explain the data most was selcted. All findings were shared with biostatisticians, medical doctors, biologists, neurologists and epidemiologists at the neurology department and the Taub Institute for Research on Alzheimer’s Disease and the Aging Brain in the Columbia University Irving Medical Center.\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster2\n\n Data Preprocessing  \n\ncluster0\n\n Data Quality Control  \n\ncluster1\n\n Missing Value Analysis   \n\nidentify_anomaly_data\n\n identify_anomaly_data   \n\nidentify_missing_values\n\n identify_missing_values   \n\nidentify_anomaly_data-&gt;identify_missing_values\n\n    \n\nMissing_Completely_At_Random\n\n Missing_Completely_At_Random   \n\nidentify_missing_values-&gt;Missing_Completely_At_Random\n\n    \n\nMissing_At_Random\n\n Missing_At_Random   \n\nMissing_Completely_At_Random-&gt;Missing_At_Random\n\n    \n\nMissing_Not_at_Random\n\n Missing_Not_at_Random   \n\nMissing_At_Random-&gt;Missing_Not_at_Random\n\n    \n\neither_imputation_or_omission\n\n either_imputation_or_omission   \n\nMissing_Not_at_Random-&gt;either_imputation_or_omission\n\n    \n\ncommunication_with_labs\n\n communication_with_labs   \n\neither_imputation_or_omission-&gt;communication_with_labs\n\n    \n\nset_data_inclusion_exclusion_criteria\n\n set_data_inclusion_exclusion_criteria   \n\ncommunication_with_labs-&gt;set_data_inclusion_exclusion_criteria\n\n    \n\nData_Transformation\n\n Data_Transformation   \n\nset_data_inclusion_exclusion_criteria-&gt;Data_Transformation\n\n    \n\nLog_Transformation\n\n Log_Transformation   \n\nData_Transformation-&gt;Log_Transformation\n\n    \n\nStandardization\n\n Standardization   \n\nLog_Transformation-&gt;Standardization\n\n   \n\n\n\n\n\nFor data quality control, I communicated with whom generated the data, classified missing values ​​and outliers according to laboratory standards, and labeled them. Based on the results through missing value analysis, I processed the missing values through consultation with the faculty members several times. For the data QC criteria, patients and variables whose ratio of missing values ​​for the sum of the rowwise and columnwise sums exceeded 5% for the sample size were excluded from the analysis. All metabolites data were standardized through log transformation and standardization.\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster2\n\n Data Analytics   \n\nExploratory_Data_Analysis\n\n Exploratory_Data_Analysis   \n\nData_Minig\n\n Data_Minig   \n\nExploratory_Data_Analysis-&gt;Data_Minig\n\n    \n\nStatistical_Analysis\n\n Statistical_Analysis   \n\nData_Minig-&gt;Statistical_Analysis\n\n    \n\nMachine_Learning\n\n Machine_Learning   \n\nStatistical_Analysis-&gt;Machine_Learning\n\n   \n\n\n\n\n\nData analysis was performed in three stages: Exploratory Data Analysis (EDA), Statistical Analysis, and Machine Learning. In each stage, metabolites commonly associated with diseases were selected.\n\n\nStudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests, and regression testing were performed, and I visualizaed data to reconfirm the test results. To visualize high-dimensional data and observe data patterns, KNN, PCA, K means, Clustering, and DB Scan were used.\n\n\n\nMultivariable linear regression, logistic regression, and Cox PH (Proportional Hazards) regression analyses were conducted and the metabolites that are signficantly associated with the disease status were selected. In order to reduce the possibility of making a type 1 error due to multiple testing, the significance was checked once more by calculating permuted p-values.\n\n\n\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, support vector machine (SVM), partial least square, and sparse partial least square were used. The optimal classifier was selected by evaluating the classifier that best predicted the disease status.\n\n\n\n\nIn the data consisting of 146 observations and about 3,000 variables, about 60 metabolites were observed to have a significant relationship with the disease at the 5% significance level, and partial least suare was observed to perform the best."
  },
  {
    "objectID": "docs/projects/LLFS/mining.html",
    "href": "docs/projects/LLFS/mining.html",
    "title": "Data Mining",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmetabolite_data &lt;- all_data[, -c(1:5)]\noutcome_data &lt;- all_data[, 2]\n\n# normalize the metaoblites\nnormalized_metabolite_data &lt;-\n    as.data.frame(lapply(metabolite_data, function(x) scale_function(vector = x, method = \"min-max\")))\nnormalized_significant_metabolite_data &lt;-\n    normalized_metabolite_data %&gt;%\n    dplyr::select(all_of(significant_metabolites))\n\n# extract the latent variables (PCs: Principal Components)\npc_metabolites &lt;-\n    prcomp(normalized_metabolite_data)\npc_significant_metabolites &lt;-\n    prcomp(normalized_significant_metabolite_data)\n\n# calculate scores\nscores &lt;-\n    as.data.frame(pc_metabolites$x) %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(row_names = 1:n())\nsignificant_scores &lt;-\n    as.data.frame(pc_significant_metabolites$x) %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(row_names = 1:n())\n\ntemp &lt;-\n    as.data.frame(pc_metabolites$rotation) %&gt;%\n    janitor::clean_names()\nloadings &lt;- temp %&gt;%\n    mutate(\n        metabolites = rownames(.),\n        arrow_size_normalization = min(\n            (max(scores[, \"pc1\"]) - min(scores[, \"pc1\"]) /\n                (max(temp[, \"pc1\"]) - min(temp[, \"pc1\"]))),\n            (max(scores[, \"pc2\"]) - min(scores[, \"pc2\"]) /\n                (max(temp[, \"pc2\"]) - min(temp[, \"pc2\"]))),\n            (max(scores[, \"pc3\"]) - min(scores[, \"pc3\"]) /\n                (max(temp[, \"pc3\"]) - min(temp[, \"pc3\"])))\n        ),\n        arrow_pc1 = arrow_size_normalization * pc1,\n        arrow_pc2 = arrow_size_normalization * pc2,\n        arrow_pc3 = arrow_size_normalization * pc3\n    )\n\ntemp &lt;-\n    as.data.frame(pc_significant_metabolites$rotation) %&gt;%\n    janitor::clean_names()\nsignificant_loadings &lt;- temp %&gt;%\n    mutate(\n        metabolites = rownames(.),\n        arrow_size_normalization = min(\n            (max(scores[, \"pc1\"]) - min(scores[, \"pc1\"]) /\n                (max(temp[, \"pc1\"]) - min(temp[, \"pc1\"]))),\n            (max(scores[, \"pc2\"]) - min(scores[, \"pc2\"]) /\n                (max(temp[, \"pc2\"]) - min(temp[, \"pc2\"]))),\n            (max(scores[, \"pc3\"]) - min(scores[, \"pc3\"]) /\n                (max(temp[, \"pc3\"]) - min(temp[, \"pc3\"])))\n        ),\n        arrow_pc1 = arrow_size_normalization * pc1,\n        arrow_pc2 = arrow_size_normalization * pc2,\n        arrow_pc3 = arrow_size_normalization * pc3\n    )\n# arrow_size_normalization is a normalization factor that\n# ensures the variable loading arrows are scaled appropriately relative to the data points.\n# The min() function to find the smallest ratio between the range of the data points and\n# the range of the variable loadings along each principal component axis (pc1, pc2, and pc3).\n# The reason why I select the first 3 components is that\n# '3' is the maximum dimension that can visualize the PCA results in 3d.\n\noutcome_scores &lt;-\n    scores %&gt;%\n    mutate(\n        outcome = outcome_data,\n        row_names = 1:n()\n    )\noutcome_significant_scores &lt;-\n    significant_scores %&gt;%\n    mutate(\n        outcome = outcome_data,\n        row_names = 1:n()\n    )\n\n\n# total variance\ntotal_variance &lt;-\n    data.frame(\n        pc = 1:length(pc_metabolites$sdev),\n        pc_variance_proportion = summary(pc_metabolites)[[\"importance\"]][\"Proportion of Variance\", ],\n        cumulative_proportion = summary(pc_metabolites)[[\"importance\"]][\"Cumulative Proportion\", ] * 100\n    )\ntotal_variance_significance &lt;-\n    data.frame(\n        pc = 1:length(pc_significant_metabolites$sdev),\n        pc_variance_proportion = summary(pc_significant_metabolites)[[\"importance\"]][\"Proportion of Variance\", ],\n        cumulative_proportion = summary(pc_significant_metabolites)[[\"importance\"]][\"Cumulative Proportion\", ] * 100\n    )\n\nscree_plot &lt;- function(indata) {\n    scree_plot1 &lt;- ggplot(\n        data = indata,\n        aes(x = pc, y = pc_variance_proportion, group = 1)\n    ) +\n        geom_point() +\n        geom_line() +\n        labs(title = \"\", subtitle = paste0(\n            \"Scree Plot, Total Variance(\",\n            round(tail(indata, 1)[\"cumulative_proportion\"], 3),\n            \"%)Explained by \", nrow(indata), \" PCs\"\n        )) +\n        ylab(\"Total Variance Explained\") +\n        xlab(\"Principal Components\")\n    scree_plot2 &lt;- ggplot(\n        data = indata %&gt;% filter(pc &lt; 13),\n        aes(x = pc, y = pc_variance_proportion, group = 1)\n    ) +\n        geom_point() +\n        geom_line() +\n        labs(title = \"\", subtitle = paste0(\n            \"Scree Plot, Part of Variance(\",\n            round(tail(indata %&gt;% filter(pc &lt; 13), 1)[\"cumulative_proportion\"], 3),\n            \"%)Explained by \", indata %&gt;% filter(pc &lt; 13) %&gt;% nrow(), \" PCs\"\n        )) +\n        ylab(\"Total Variance Explained\") +\n        xlab(\"Principal Components\")\n    return(ggarrange(scree_plot1, scree_plot2, nrow = 1))\n}\n\nggarrange(scree_plot(total_variance), scree_plot(total_variance_significance),\n    labels = c(\n        paste0(\"All \", ncol(metabolite_data), \" Metabolites\"),\n        paste0(length(significant_metabolites), \" Significant Metabolites\")\n    ), nrow = 2\n)\n\n\n\n\n\nCode\n# 2D PCA Scatter Plots with PC1 and PC2\n\nscatter_plot &lt;- function(in_data) {\n    p &lt;- ggplot(\n        data = in_data,\n        aes(x = pc1, y = pc2, color = outcome)\n    ) +\n        geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n        stat_ellipse(type = \"norm\", level = .99) +\n        geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n        geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n        scale_color_manual(values = color_function(length(unique(in_data$outcome)))) +\n        labs(\n            title = \"2D Scatter Plot of the First 2 PCs Grouped by AD status\",\n            subtitle = paste0(ncol(in_data) - 2, \" Metabolites\")\n        )\n    return(p)\n}\n\nggarrange(scatter_plot(outcome_scores),\n    scatter_plot(outcome_significant_scores),\n    nrow = 2\n)\n\n\n\n\n\nCode\n# biplot\nbi_plot &lt;- function(in_data) {\n    p &lt;-\n        ggplot(data = in_data, aes(x = pc1, y = pc2, color = outcome)) +\n        geom_text(alpha = .75, size = 3, aes(label = row_names)) +\n        geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n        geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n        coord_equal() +\n        scale_color_manual(values = color_function(length(unique(in_data$outcome)))) +\n        stat_ellipse(type = \"norm\", level = .99) +\n        geom_text(\n            data = loadings, aes(x = arrow_pc1, y = arrow_pc2, label = metabolites),\n            alpha = 0.5, size = 5, vjust = 1, color = \"red\"\n        ) +\n        geom_segment(\n            data = loadings, aes(x = 0, y = 0, xend = arrow_pc1, yend = arrow_pc2),\n            arrow = arrow(length = unit(0.5, \"cm\")), alpha = 0.5, color = \"red\"\n        ) +\n        labs(\n            title = \"Biplot, the Effect of Metabolites on Samples with Disease Status\",\n            subtitle = paste0(ncol(in_data) - 2, \" Metabolites\")\n        ) +\n        ylab(\"PC2\") +\n        xlab(\"PC1\")\n    return(p)\n}\nggarrange(bi_plot(outcome_scores), bi_plot(outcome_significant_scores), nrow = 2)\n\n\n\n\n\nCode\nplot_ly(\n    data = outcome_scores,\n    x = ~pc1, y = ~pc2, z = ~pc3,\n    type = \"scatter3d\", mode = \"markers\", color = ~ outcome_scores$outcome,\n    colors = color_function(2),\n    size = 2\n) %&gt;%\n    layout(\n        title = \"Effect of 500 Metabolites on Samples with Disease Status in 3d\",\n        scene = list(\n            bgcolor = \"#e5ecf6\",\n            xaxis = list(title = \"PC1\"),\n            yaxis = list(title = \"PC2\"),\n            zaxis = list(title = \"PC3\")\n        ),\n        legend = list(title = list(text = \"Disease(AD) Status\"))\n    )\n\n\n\n\n\n\nCode\nplot_ly(\n    data = outcome_significant_scores,\n    x = ~pc1, y = ~pc2, z = ~pc3,\n    type = \"scatter3d\", mode = \"markers\", color = ~ outcome_scores$outcome,\n    colors = color_function(2),\n    size = 2\n) %&gt;%\n    layout(\n        title = \"Effect of 201 Metabolites on Samples with Disease Status in 3d\",\n        scene = list(\n            bgcolor = \"#e5ecf6\",\n            xaxis = list(title = \"PC1\"),\n            yaxis = list(title = \"PC2\"),\n            zaxis = list(title = \"PC3\")\n        ),\n        legend = list(title = list(text = \"Disease(AD) Status\"))\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nkmean_result_list &lt;- list(\n    \"mse_list\" = list(\n        \"all_metabolites\" = matrix(nrow = 20),\n        \"significant_metabolites\" = matrix(nrow = 20)\n    ),\n    \"cluster_sse_list\" = list(\n        \"all_metabolites\" = matrix(nrow = 20, ncol = 2),\n        \"significant_metabolites\" = matrix(nrow = 20, ncol = 2)\n    ),\n    \"Variance_Explained\" = list(\n        \"all_metabolites\" = matrix(nrow = 20),\n        \"significant_metabolites\" = matrix(nrow = 20)\n    )\n)\n\nfor (j in c(\"all_metabolites\", \"significant_metabolites\")) {\n    for (i in (1:20)) {\n        if (j == \"all_metabolites\") {\n            kmean_fit &lt;- kmeans(normalized_metabolite_data, centers = i, iter.max = 300)\n        } else {\n            kmean_fit &lt;- kmeans(normalized_significant_metabolite_data, centers = i, iter.max = 300)\n        }\n        kmean_result_list[[\"mse_list\"]][[j]][i] &lt;- mean(kmean_fit$withinss) %&gt;% round(3)\n        kmean_result_list[[\"cluster_sse_list\"]][[j]][i, 1] &lt;- i\n        kmean_result_list[[\"cluster_sse_list\"]][[j]][i, 2] &lt;- kmean_result_list[[\"mse_list\"]][[j]][i]\n        kmean_result_list[[\"Variance_Explained\"]][[j]][i] &lt;- kmean_fit$betweenss / kmean_fit$totss\n        cat(\n            \"For \", j, \", K: \", i,\n            \"within-cluster MSE: \", kmean_result_list[[\"mse_list\"]][[j]][i],\n            \"Variance_Explained: \", kmean_result_list[[\"Variance_Explained\"]][[j]][i], \"\\n\"\n        )\n    }\n}\n\n\nFor  all_metabolites , K:  1 within-cluster MSE:  13619.32 Variance_Explained:  4.941702e-15 \nFor  all_metabolites , K:  2 within-cluster MSE:  6487.104 Variance_Explained:  0.04736709 \nFor  all_metabolites , K:  3 within-cluster MSE:  4205.437 Variance_Explained:  0.0736459 \nFor  all_metabolites , K:  4 within-cluster MSE:  3107.38 Variance_Explained:  0.08736085 \nFor  all_metabolites , K:  5 within-cluster MSE:  2443.924 Variance_Explained:  0.1027728 \nFor  all_metabolites , K:  6 within-cluster MSE:  2010.74 Variance_Explained:  0.114167 \nFor  all_metabolites , K:  7 within-cluster MSE:  1705.235 Variance_Explained:  0.1235504 \nFor  all_metabolites , K:  8 within-cluster MSE:  1481.469 Variance_Explained:  0.1297835 \nFor  all_metabolites , K:  9 within-cluster MSE:  1306.354 Variance_Explained:  0.1367274 \nFor  all_metabolites , K:  10 within-cluster MSE:  1166.08 Variance_Explained:  0.1438044 \nFor  all_metabolites , K:  11 within-cluster MSE:  1051.345 Variance_Explained:  0.1508531 \nFor  all_metabolites , K:  12 within-cluster MSE:  957.817 Variance_Explained:  0.1560656 \nFor  all_metabolites , K:  13 within-cluster MSE:  880.831 Variance_Explained:  0.1592231 \nFor  all_metabolites , K:  14 within-cluster MSE:  812.71 Variance_Explained:  0.1645731 \nFor  all_metabolites , K:  15 within-cluster MSE:  755.508 Variance_Explained:  0.1679013 \nFor  all_metabolites , K:  16 within-cluster MSE:  702.891 Variance_Explained:  0.1742422 \nFor  all_metabolites , K:  17 within-cluster MSE:  657.791 Variance_Explained:  0.1789275 \nFor  all_metabolites , K:  18 within-cluster MSE:  617.96 Variance_Explained:  0.1832713 \nFor  all_metabolites , K:  19 within-cluster MSE:  582.219 Variance_Explained:  0.1877591 \nFor  all_metabolites , K:  20 within-cluster MSE:  550.074 Variance_Explained:  0.1922144 \nFor  significant_metabolites , K:  1 within-cluster MSE:  2660.227 Variance_Explained:  -8.034324e-15 \nFor  significant_metabolites , K:  2 within-cluster MSE:  1022.505 Variance_Explained:  0.2312645 \nFor  significant_metabolites , K:  3 within-cluster MSE:  632.194 Variance_Explained:  0.28706 \nFor  significant_metabolites , K:  4 within-cluster MSE:  452.85 Variance_Explained:  0.3190806 \nFor  significant_metabolites , K:  5 within-cluster MSE:  354.801 Variance_Explained:  0.3331372 \nFor  significant_metabolites , K:  6 within-cluster MSE:  290.621 Variance_Explained:  0.3445191 \nFor  significant_metabolites , K:  7 within-cluster MSE:  245.829 Variance_Explained:  0.3531377 \nFor  significant_metabolites , K:  8 within-cluster MSE:  212.9 Variance_Explained:  0.3597537 \nFor  significant_metabolites , K:  9 within-cluster MSE:  187.706 Variance_Explained:  0.3649574 \nFor  significant_metabolites , K:  10 within-cluster MSE:  167.108 Variance_Explained:  0.3718263 \nFor  significant_metabolites , K:  11 within-cluster MSE:  150.928 Variance_Explained:  0.3759169 \nFor  significant_metabolites , K:  12 within-cluster MSE:  137.487 Variance_Explained:  0.3798103 \nFor  significant_metabolites , K:  13 within-cluster MSE:  126.018 Variance_Explained:  0.3841728 \nFor  significant_metabolites , K:  14 within-cluster MSE:  116.658 Variance_Explained:  0.3860626 \nFor  significant_metabolites , K:  15 within-cluster MSE:  107.843 Variance_Explained:  0.3919131 \nFor  significant_metabolites , K:  16 within-cluster MSE:  100.885 Variance_Explained:  0.393227 \nFor  significant_metabolites , K:  17 within-cluster MSE:  94.421 Variance_Explained:  0.3966092 \nFor  significant_metabolites , K:  18 within-cluster MSE:  88.62 Variance_Explained:  0.4003684 \nFor  significant_metabolites , K:  19 within-cluster MSE:  83.8 Variance_Explained:  0.4014796 \nFor  significant_metabolites , K:  20 within-cluster MSE:  79.155 Variance_Explained:  0.4049023 \n\n\nCode\nkmean_mse_data &lt;-\n    rbind(\n        data.frame(\n            metabolites = \"all_metabolites\",\n            k = kmean_result_list[[\"cluster_sse_list\"]][[\"all_metabolites\"]][, 1],\n            mse = kmean_result_list[[\"cluster_sse_list\"]][[\"all_metabolites\"]][, 2],\n            variance_exaplained = kmean_result_list[[\"Variance_Explained\"]][[\"all_metabolites\"]]\n        ),\n        data.frame(\n            metabolites = \"significant_metabolites\",\n            k = kmean_result_list[[\"cluster_sse_list\"]][[\"significant_metabolites\"]][, 1],\n            mse = kmean_result_list[[\"cluster_sse_list\"]][[\"significant_metabolites\"]][, 2],\n            variance_exaplained = kmean_result_list[[\"Variance_Explained\"]][[\"significant_metabolites\"]]\n        )\n    )\n\nggarrange(\n    ggplot(\n        data = kmean_mse_data,\n        aes(x = k, y = mse, group = metabolites, color = metabolites)\n    ) +\n        geom_line() +\n        geom_point() +\n        scale_color_manual(values = color_function(2)) +\n        labs(title = \"K Mean Clustering Result: MSE for All Metabolites vs Significant Ones\") +\n        xlab(\"Number of Clusters\") +\n        ylab(\"Mean Squared Error\"),\n    ggplot(\n        data = kmean_mse_data,\n        aes(x = k, y = variance_exaplained, group = metabolites, color = metabolites)\n    ) +\n        geom_line() +\n        geom_point() +\n        scale_color_manual(values = color_function(2)) +\n        labs(title = \"K Mean Clustering Result: Variance Explained for All Metabolites vs Significant Ones\") +\n        xlab(\"Number of Clusters\") +\n        ylab(\"Variance Exaplained\"),\n    ncol = 1\n)\n\n\n\n\n\nCode\n# K means\n\n\nkm_clustering &lt;- kmeans(normalized_metabolite_data, centers = 2, iter.max = 300)\nkm_significant_clustering &lt;- kmeans(normalized_significant_metabolite_data, centers = 2, iter.max = 300)\n\nconfusionMatrix(table(all_data[, 2], ifelse(km_clustering$cluster == 1, \"negative\", \"positive\")))\n\n\nConfusion Matrix and Statistics\n\n          \n           negative positive\n  negative       96      214\n  positive      147       43\n                                          \n               Accuracy : 0.278           \n                 95% CI : (0.2391, 0.3195)\n    No Information Rate : 0.514           \n    P-Value [Acc &gt; NIR] : 1.0000000       \n                                          \n                  Kappa : -0.4344         \n                                          \n Mcnemar's Test P-Value : 0.0005134       \n                                          \n            Sensitivity : 0.3951          \n            Specificity : 0.1673          \n         Pos Pred Value : 0.3097          \n         Neg Pred Value : 0.2263          \n             Prevalence : 0.4860          \n         Detection Rate : 0.1920          \n   Detection Prevalence : 0.6200          \n      Balanced Accuracy : 0.2812          \n                                          \n       'Positive' Class : negative        \n                                          \n\n\nCode\nconfusionMatrix(table(all_data[, 2], ifelse(km_significant_clustering$cluster == 1, \"negative\", \"positive\")))\n\n\nConfusion Matrix and Statistics\n\n          \n           negative positive\n  negative      101      209\n  positive      143       47\n                                          \n               Accuracy : 0.296           \n                 95% CI : (0.2563, 0.3381)\n    No Information Rate : 0.512           \n    P-Value [Acc &gt; NIR] : 1.0000000       \n                                          \n                  Kappa : -0.3999         \n                                          \n Mcnemar's Test P-Value : 0.0005312       \n                                          \n            Sensitivity : 0.4139          \n            Specificity : 0.1836          \n         Pos Pred Value : 0.3258          \n         Neg Pred Value : 0.2474          \n             Prevalence : 0.4880          \n         Detection Rate : 0.2020          \n   Detection Prevalence : 0.6200          \n      Balanced Accuracy : 0.2988          \n                                          \n       'Positive' Class : negative        \n                                          \n\n\nCode\noutcome_pca_km &lt;- outcome_scores %&gt;%\n    mutate(\n        km_clusters = km_clustering$cluster,\n        km_clusters = factor(km_clusters, levels = c(1, 2)),\n        km_significant_clusters = km_significant_clustering$cluster,\n        km_significant_clusters = factor(km_significant_clusters, levels = c(1, 2))\n    )\n\n\nggplot(\n    data = outcome_pca_km,\n    aes(x = pc1, y = pc2, color = km_clusters)\n) +\n    geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n    stat_ellipse(type = \"norm\", level = .99) +\n    geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n    geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n    scale_color_manual(values = color_function(2)) +\n    labs(\n        title = \"2D Scatter Plot of the First 2 PCs Grouped by K Mean Clusters, AD Status\",\n        subtitle = paste0(ncol(outcome_scores) - 2, \" Metabolites\")\n    )\n\n\n\n\n\nCode\nggplot(\n    data = outcome_pca_km,\n    aes(x = pc1, y = pc2, color = km_significant_clusters)\n) +\n    geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n    stat_ellipse(type = \"norm\", level = .99) +\n    geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n    geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n    scale_color_manual(values = color_function(2)) +\n    labs(\n        title = \"2D Scatter Plot of the First 2 PCs Grouped by K Mean Clusters, AD Status\",\n        subtitle = paste0(ncol(outcome_significant_scores) - 2, \" Metabolites\")\n    )\n\n\n\n\n\n비지도 학습 방법인 PCA와 K-means clustering를 이용하여 차원 축소와 군집화를 시도하였으나, 이 방법을 사용하는 모든 대사체에 대해 AD 상태가 명확하게 분류되지 않는 것으로 보인다. PCA와 K-means는 EDA에서 선별된 대사산물로 군집화를 수행했을 때 전체 metabolites 보다 선별된 metabolites 에서 AD 상태에 대한 정보가 조금 더 많이 설명되는 것을 PCA를 통해 관찰할 수 있었다. K means clustering도 선별된 metabolites에 대해서 성능 향상을 보여준다. 그러나 전반적인 정확도가 매우 낮기 때문에 지도 학습을 통해 AD 상태를 잘 설명하는 대사체를 선택할 것이다.\nDimensionality reduction and clustering were attempted using PCA and K-means clustering, which are unsupervised learning methods, but AD status seems to not be clearly classified for all metabolites using the methods. When PCA and K means clustering were performed with the metabolites selected from EDA, it was observed through PCA that a little more information about AD status was explained with the selected metabolites than with the entire set of metaboliotes. K means clustering also showed an improvement in performance with the selected metabolites. However, the overall accuracy is very low, so we will select metabolites that explain AD status well through supervised learning."
  },
  {
    "objectID": "docs/projects/LLFS/mining.html#data-mining",
    "href": "docs/projects/LLFS/mining.html#data-mining",
    "title": "Data Mining",
    "section": "",
    "text": "Code\nmetabolite_data &lt;- all_data[, -c(1:5)]\noutcome_data &lt;- all_data[, 2]\n\n# normalize the metaoblites\nnormalized_metabolite_data &lt;-\n    as.data.frame(lapply(metabolite_data, function(x) scale_function(vector = x, method = \"min-max\")))\nnormalized_significant_metabolite_data &lt;-\n    normalized_metabolite_data %&gt;%\n    dplyr::select(all_of(significant_metabolites))\n\n# extract the latent variables (PCs: Principal Components)\npc_metabolites &lt;-\n    prcomp(normalized_metabolite_data)\npc_significant_metabolites &lt;-\n    prcomp(normalized_significant_metabolite_data)\n\n# calculate scores\nscores &lt;-\n    as.data.frame(pc_metabolites$x) %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(row_names = 1:n())\nsignificant_scores &lt;-\n    as.data.frame(pc_significant_metabolites$x) %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(row_names = 1:n())\n\ntemp &lt;-\n    as.data.frame(pc_metabolites$rotation) %&gt;%\n    janitor::clean_names()\nloadings &lt;- temp %&gt;%\n    mutate(\n        metabolites = rownames(.),\n        arrow_size_normalization = min(\n            (max(scores[, \"pc1\"]) - min(scores[, \"pc1\"]) /\n                (max(temp[, \"pc1\"]) - min(temp[, \"pc1\"]))),\n            (max(scores[, \"pc2\"]) - min(scores[, \"pc2\"]) /\n                (max(temp[, \"pc2\"]) - min(temp[, \"pc2\"]))),\n            (max(scores[, \"pc3\"]) - min(scores[, \"pc3\"]) /\n                (max(temp[, \"pc3\"]) - min(temp[, \"pc3\"])))\n        ),\n        arrow_pc1 = arrow_size_normalization * pc1,\n        arrow_pc2 = arrow_size_normalization * pc2,\n        arrow_pc3 = arrow_size_normalization * pc3\n    )\n\ntemp &lt;-\n    as.data.frame(pc_significant_metabolites$rotation) %&gt;%\n    janitor::clean_names()\nsignificant_loadings &lt;- temp %&gt;%\n    mutate(\n        metabolites = rownames(.),\n        arrow_size_normalization = min(\n            (max(scores[, \"pc1\"]) - min(scores[, \"pc1\"]) /\n                (max(temp[, \"pc1\"]) - min(temp[, \"pc1\"]))),\n            (max(scores[, \"pc2\"]) - min(scores[, \"pc2\"]) /\n                (max(temp[, \"pc2\"]) - min(temp[, \"pc2\"]))),\n            (max(scores[, \"pc3\"]) - min(scores[, \"pc3\"]) /\n                (max(temp[, \"pc3\"]) - min(temp[, \"pc3\"])))\n        ),\n        arrow_pc1 = arrow_size_normalization * pc1,\n        arrow_pc2 = arrow_size_normalization * pc2,\n        arrow_pc3 = arrow_size_normalization * pc3\n    )\n# arrow_size_normalization is a normalization factor that\n# ensures the variable loading arrows are scaled appropriately relative to the data points.\n# The min() function to find the smallest ratio between the range of the data points and\n# the range of the variable loadings along each principal component axis (pc1, pc2, and pc3).\n# The reason why I select the first 3 components is that\n# '3' is the maximum dimension that can visualize the PCA results in 3d.\n\noutcome_scores &lt;-\n    scores %&gt;%\n    mutate(\n        outcome = outcome_data,\n        row_names = 1:n()\n    )\noutcome_significant_scores &lt;-\n    significant_scores %&gt;%\n    mutate(\n        outcome = outcome_data,\n        row_names = 1:n()\n    )\n\n\n# total variance\ntotal_variance &lt;-\n    data.frame(\n        pc = 1:length(pc_metabolites$sdev),\n        pc_variance_proportion = summary(pc_metabolites)[[\"importance\"]][\"Proportion of Variance\", ],\n        cumulative_proportion = summary(pc_metabolites)[[\"importance\"]][\"Cumulative Proportion\", ] * 100\n    )\ntotal_variance_significance &lt;-\n    data.frame(\n        pc = 1:length(pc_significant_metabolites$sdev),\n        pc_variance_proportion = summary(pc_significant_metabolites)[[\"importance\"]][\"Proportion of Variance\", ],\n        cumulative_proportion = summary(pc_significant_metabolites)[[\"importance\"]][\"Cumulative Proportion\", ] * 100\n    )\n\nscree_plot &lt;- function(indata) {\n    scree_plot1 &lt;- ggplot(\n        data = indata,\n        aes(x = pc, y = pc_variance_proportion, group = 1)\n    ) +\n        geom_point() +\n        geom_line() +\n        labs(title = \"\", subtitle = paste0(\n            \"Scree Plot, Total Variance(\",\n            round(tail(indata, 1)[\"cumulative_proportion\"], 3),\n            \"%)Explained by \", nrow(indata), \" PCs\"\n        )) +\n        ylab(\"Total Variance Explained\") +\n        xlab(\"Principal Components\")\n    scree_plot2 &lt;- ggplot(\n        data = indata %&gt;% filter(pc &lt; 13),\n        aes(x = pc, y = pc_variance_proportion, group = 1)\n    ) +\n        geom_point() +\n        geom_line() +\n        labs(title = \"\", subtitle = paste0(\n            \"Scree Plot, Part of Variance(\",\n            round(tail(indata %&gt;% filter(pc &lt; 13), 1)[\"cumulative_proportion\"], 3),\n            \"%)Explained by \", indata %&gt;% filter(pc &lt; 13) %&gt;% nrow(), \" PCs\"\n        )) +\n        ylab(\"Total Variance Explained\") +\n        xlab(\"Principal Components\")\n    return(ggarrange(scree_plot1, scree_plot2, nrow = 1))\n}\n\nggarrange(scree_plot(total_variance), scree_plot(total_variance_significance),\n    labels = c(\n        paste0(\"All \", ncol(metabolite_data), \" Metabolites\"),\n        paste0(length(significant_metabolites), \" Significant Metabolites\")\n    ), nrow = 2\n)\n\n\n\n\n\nCode\n# 2D PCA Scatter Plots with PC1 and PC2\n\nscatter_plot &lt;- function(in_data) {\n    p &lt;- ggplot(\n        data = in_data,\n        aes(x = pc1, y = pc2, color = outcome)\n    ) +\n        geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n        stat_ellipse(type = \"norm\", level = .99) +\n        geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n        geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n        scale_color_manual(values = color_function(length(unique(in_data$outcome)))) +\n        labs(\n            title = \"2D Scatter Plot of the First 2 PCs Grouped by AD status\",\n            subtitle = paste0(ncol(in_data) - 2, \" Metabolites\")\n        )\n    return(p)\n}\n\nggarrange(scatter_plot(outcome_scores),\n    scatter_plot(outcome_significant_scores),\n    nrow = 2\n)\n\n\n\n\n\nCode\n# biplot\nbi_plot &lt;- function(in_data) {\n    p &lt;-\n        ggplot(data = in_data, aes(x = pc1, y = pc2, color = outcome)) +\n        geom_text(alpha = .75, size = 3, aes(label = row_names)) +\n        geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n        geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n        coord_equal() +\n        scale_color_manual(values = color_function(length(unique(in_data$outcome)))) +\n        stat_ellipse(type = \"norm\", level = .99) +\n        geom_text(\n            data = loadings, aes(x = arrow_pc1, y = arrow_pc2, label = metabolites),\n            alpha = 0.5, size = 5, vjust = 1, color = \"red\"\n        ) +\n        geom_segment(\n            data = loadings, aes(x = 0, y = 0, xend = arrow_pc1, yend = arrow_pc2),\n            arrow = arrow(length = unit(0.5, \"cm\")), alpha = 0.5, color = \"red\"\n        ) +\n        labs(\n            title = \"Biplot, the Effect of Metabolites on Samples with Disease Status\",\n            subtitle = paste0(ncol(in_data) - 2, \" Metabolites\")\n        ) +\n        ylab(\"PC2\") +\n        xlab(\"PC1\")\n    return(p)\n}\nggarrange(bi_plot(outcome_scores), bi_plot(outcome_significant_scores), nrow = 2)\n\n\n\n\n\nCode\nplot_ly(\n    data = outcome_scores,\n    x = ~pc1, y = ~pc2, z = ~pc3,\n    type = \"scatter3d\", mode = \"markers\", color = ~ outcome_scores$outcome,\n    colors = color_function(2),\n    size = 2\n) %&gt;%\n    layout(\n        title = \"Effect of 500 Metabolites on Samples with Disease Status in 3d\",\n        scene = list(\n            bgcolor = \"#e5ecf6\",\n            xaxis = list(title = \"PC1\"),\n            yaxis = list(title = \"PC2\"),\n            zaxis = list(title = \"PC3\")\n        ),\n        legend = list(title = list(text = \"Disease(AD) Status\"))\n    )\n\n\n\n\n\n\nCode\nplot_ly(\n    data = outcome_significant_scores,\n    x = ~pc1, y = ~pc2, z = ~pc3,\n    type = \"scatter3d\", mode = \"markers\", color = ~ outcome_scores$outcome,\n    colors = color_function(2),\n    size = 2\n) %&gt;%\n    layout(\n        title = \"Effect of 201 Metabolites on Samples with Disease Status in 3d\",\n        scene = list(\n            bgcolor = \"#e5ecf6\",\n            xaxis = list(title = \"PC1\"),\n            yaxis = list(title = \"PC2\"),\n            zaxis = list(title = \"PC3\")\n        ),\n        legend = list(title = list(text = \"Disease(AD) Status\"))\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nkmean_result_list &lt;- list(\n    \"mse_list\" = list(\n        \"all_metabolites\" = matrix(nrow = 20),\n        \"significant_metabolites\" = matrix(nrow = 20)\n    ),\n    \"cluster_sse_list\" = list(\n        \"all_metabolites\" = matrix(nrow = 20, ncol = 2),\n        \"significant_metabolites\" = matrix(nrow = 20, ncol = 2)\n    ),\n    \"Variance_Explained\" = list(\n        \"all_metabolites\" = matrix(nrow = 20),\n        \"significant_metabolites\" = matrix(nrow = 20)\n    )\n)\n\nfor (j in c(\"all_metabolites\", \"significant_metabolites\")) {\n    for (i in (1:20)) {\n        if (j == \"all_metabolites\") {\n            kmean_fit &lt;- kmeans(normalized_metabolite_data, centers = i, iter.max = 300)\n        } else {\n            kmean_fit &lt;- kmeans(normalized_significant_metabolite_data, centers = i, iter.max = 300)\n        }\n        kmean_result_list[[\"mse_list\"]][[j]][i] &lt;- mean(kmean_fit$withinss) %&gt;% round(3)\n        kmean_result_list[[\"cluster_sse_list\"]][[j]][i, 1] &lt;- i\n        kmean_result_list[[\"cluster_sse_list\"]][[j]][i, 2] &lt;- kmean_result_list[[\"mse_list\"]][[j]][i]\n        kmean_result_list[[\"Variance_Explained\"]][[j]][i] &lt;- kmean_fit$betweenss / kmean_fit$totss\n        cat(\n            \"For \", j, \", K: \", i,\n            \"within-cluster MSE: \", kmean_result_list[[\"mse_list\"]][[j]][i],\n            \"Variance_Explained: \", kmean_result_list[[\"Variance_Explained\"]][[j]][i], \"\\n\"\n        )\n    }\n}\n\n\nFor  all_metabolites , K:  1 within-cluster MSE:  13619.32 Variance_Explained:  4.941702e-15 \nFor  all_metabolites , K:  2 within-cluster MSE:  6487.104 Variance_Explained:  0.04736709 \nFor  all_metabolites , K:  3 within-cluster MSE:  4205.437 Variance_Explained:  0.0736459 \nFor  all_metabolites , K:  4 within-cluster MSE:  3107.38 Variance_Explained:  0.08736085 \nFor  all_metabolites , K:  5 within-cluster MSE:  2443.924 Variance_Explained:  0.1027728 \nFor  all_metabolites , K:  6 within-cluster MSE:  2010.74 Variance_Explained:  0.114167 \nFor  all_metabolites , K:  7 within-cluster MSE:  1705.235 Variance_Explained:  0.1235504 \nFor  all_metabolites , K:  8 within-cluster MSE:  1481.469 Variance_Explained:  0.1297835 \nFor  all_metabolites , K:  9 within-cluster MSE:  1306.354 Variance_Explained:  0.1367274 \nFor  all_metabolites , K:  10 within-cluster MSE:  1166.08 Variance_Explained:  0.1438044 \nFor  all_metabolites , K:  11 within-cluster MSE:  1051.345 Variance_Explained:  0.1508531 \nFor  all_metabolites , K:  12 within-cluster MSE:  957.817 Variance_Explained:  0.1560656 \nFor  all_metabolites , K:  13 within-cluster MSE:  880.831 Variance_Explained:  0.1592231 \nFor  all_metabolites , K:  14 within-cluster MSE:  812.71 Variance_Explained:  0.1645731 \nFor  all_metabolites , K:  15 within-cluster MSE:  755.508 Variance_Explained:  0.1679013 \nFor  all_metabolites , K:  16 within-cluster MSE:  702.891 Variance_Explained:  0.1742422 \nFor  all_metabolites , K:  17 within-cluster MSE:  657.791 Variance_Explained:  0.1789275 \nFor  all_metabolites , K:  18 within-cluster MSE:  617.96 Variance_Explained:  0.1832713 \nFor  all_metabolites , K:  19 within-cluster MSE:  582.219 Variance_Explained:  0.1877591 \nFor  all_metabolites , K:  20 within-cluster MSE:  550.074 Variance_Explained:  0.1922144 \nFor  significant_metabolites , K:  1 within-cluster MSE:  2660.227 Variance_Explained:  -8.034324e-15 \nFor  significant_metabolites , K:  2 within-cluster MSE:  1022.505 Variance_Explained:  0.2312645 \nFor  significant_metabolites , K:  3 within-cluster MSE:  632.194 Variance_Explained:  0.28706 \nFor  significant_metabolites , K:  4 within-cluster MSE:  452.85 Variance_Explained:  0.3190806 \nFor  significant_metabolites , K:  5 within-cluster MSE:  354.801 Variance_Explained:  0.3331372 \nFor  significant_metabolites , K:  6 within-cluster MSE:  290.621 Variance_Explained:  0.3445191 \nFor  significant_metabolites , K:  7 within-cluster MSE:  245.829 Variance_Explained:  0.3531377 \nFor  significant_metabolites , K:  8 within-cluster MSE:  212.9 Variance_Explained:  0.3597537 \nFor  significant_metabolites , K:  9 within-cluster MSE:  187.706 Variance_Explained:  0.3649574 \nFor  significant_metabolites , K:  10 within-cluster MSE:  167.108 Variance_Explained:  0.3718263 \nFor  significant_metabolites , K:  11 within-cluster MSE:  150.928 Variance_Explained:  0.3759169 \nFor  significant_metabolites , K:  12 within-cluster MSE:  137.487 Variance_Explained:  0.3798103 \nFor  significant_metabolites , K:  13 within-cluster MSE:  126.018 Variance_Explained:  0.3841728 \nFor  significant_metabolites , K:  14 within-cluster MSE:  116.658 Variance_Explained:  0.3860626 \nFor  significant_metabolites , K:  15 within-cluster MSE:  107.843 Variance_Explained:  0.3919131 \nFor  significant_metabolites , K:  16 within-cluster MSE:  100.885 Variance_Explained:  0.393227 \nFor  significant_metabolites , K:  17 within-cluster MSE:  94.421 Variance_Explained:  0.3966092 \nFor  significant_metabolites , K:  18 within-cluster MSE:  88.62 Variance_Explained:  0.4003684 \nFor  significant_metabolites , K:  19 within-cluster MSE:  83.8 Variance_Explained:  0.4014796 \nFor  significant_metabolites , K:  20 within-cluster MSE:  79.155 Variance_Explained:  0.4049023 \n\n\nCode\nkmean_mse_data &lt;-\n    rbind(\n        data.frame(\n            metabolites = \"all_metabolites\",\n            k = kmean_result_list[[\"cluster_sse_list\"]][[\"all_metabolites\"]][, 1],\n            mse = kmean_result_list[[\"cluster_sse_list\"]][[\"all_metabolites\"]][, 2],\n            variance_exaplained = kmean_result_list[[\"Variance_Explained\"]][[\"all_metabolites\"]]\n        ),\n        data.frame(\n            metabolites = \"significant_metabolites\",\n            k = kmean_result_list[[\"cluster_sse_list\"]][[\"significant_metabolites\"]][, 1],\n            mse = kmean_result_list[[\"cluster_sse_list\"]][[\"significant_metabolites\"]][, 2],\n            variance_exaplained = kmean_result_list[[\"Variance_Explained\"]][[\"significant_metabolites\"]]\n        )\n    )\n\nggarrange(\n    ggplot(\n        data = kmean_mse_data,\n        aes(x = k, y = mse, group = metabolites, color = metabolites)\n    ) +\n        geom_line() +\n        geom_point() +\n        scale_color_manual(values = color_function(2)) +\n        labs(title = \"K Mean Clustering Result: MSE for All Metabolites vs Significant Ones\") +\n        xlab(\"Number of Clusters\") +\n        ylab(\"Mean Squared Error\"),\n    ggplot(\n        data = kmean_mse_data,\n        aes(x = k, y = variance_exaplained, group = metabolites, color = metabolites)\n    ) +\n        geom_line() +\n        geom_point() +\n        scale_color_manual(values = color_function(2)) +\n        labs(title = \"K Mean Clustering Result: Variance Explained for All Metabolites vs Significant Ones\") +\n        xlab(\"Number of Clusters\") +\n        ylab(\"Variance Exaplained\"),\n    ncol = 1\n)\n\n\n\n\n\nCode\n# K means\n\n\nkm_clustering &lt;- kmeans(normalized_metabolite_data, centers = 2, iter.max = 300)\nkm_significant_clustering &lt;- kmeans(normalized_significant_metabolite_data, centers = 2, iter.max = 300)\n\nconfusionMatrix(table(all_data[, 2], ifelse(km_clustering$cluster == 1, \"negative\", \"positive\")))\n\n\nConfusion Matrix and Statistics\n\n          \n           negative positive\n  negative       96      214\n  positive      147       43\n                                          \n               Accuracy : 0.278           \n                 95% CI : (0.2391, 0.3195)\n    No Information Rate : 0.514           \n    P-Value [Acc &gt; NIR] : 1.0000000       \n                                          \n                  Kappa : -0.4344         \n                                          \n Mcnemar's Test P-Value : 0.0005134       \n                                          \n            Sensitivity : 0.3951          \n            Specificity : 0.1673          \n         Pos Pred Value : 0.3097          \n         Neg Pred Value : 0.2263          \n             Prevalence : 0.4860          \n         Detection Rate : 0.1920          \n   Detection Prevalence : 0.6200          \n      Balanced Accuracy : 0.2812          \n                                          \n       'Positive' Class : negative        \n                                          \n\n\nCode\nconfusionMatrix(table(all_data[, 2], ifelse(km_significant_clustering$cluster == 1, \"negative\", \"positive\")))\n\n\nConfusion Matrix and Statistics\n\n          \n           negative positive\n  negative      101      209\n  positive      143       47\n                                          \n               Accuracy : 0.296           \n                 95% CI : (0.2563, 0.3381)\n    No Information Rate : 0.512           \n    P-Value [Acc &gt; NIR] : 1.0000000       \n                                          \n                  Kappa : -0.3999         \n                                          \n Mcnemar's Test P-Value : 0.0005312       \n                                          \n            Sensitivity : 0.4139          \n            Specificity : 0.1836          \n         Pos Pred Value : 0.3258          \n         Neg Pred Value : 0.2474          \n             Prevalence : 0.4880          \n         Detection Rate : 0.2020          \n   Detection Prevalence : 0.6200          \n      Balanced Accuracy : 0.2988          \n                                          \n       'Positive' Class : negative        \n                                          \n\n\nCode\noutcome_pca_km &lt;- outcome_scores %&gt;%\n    mutate(\n        km_clusters = km_clustering$cluster,\n        km_clusters = factor(km_clusters, levels = c(1, 2)),\n        km_significant_clusters = km_significant_clustering$cluster,\n        km_significant_clusters = factor(km_significant_clusters, levels = c(1, 2))\n    )\n\n\nggplot(\n    data = outcome_pca_km,\n    aes(x = pc1, y = pc2, color = km_clusters)\n) +\n    geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n    stat_ellipse(type = \"norm\", level = .99) +\n    geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n    geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n    scale_color_manual(values = color_function(2)) +\n    labs(\n        title = \"2D Scatter Plot of the First 2 PCs Grouped by K Mean Clusters, AD Status\",\n        subtitle = paste0(ncol(outcome_scores) - 2, \" Metabolites\")\n    )\n\n\n\n\n\nCode\nggplot(\n    data = outcome_pca_km,\n    aes(x = pc1, y = pc2, color = km_significant_clusters)\n) +\n    geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n    stat_ellipse(type = \"norm\", level = .99) +\n    geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n    geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n    scale_color_manual(values = color_function(2)) +\n    labs(\n        title = \"2D Scatter Plot of the First 2 PCs Grouped by K Mean Clusters, AD Status\",\n        subtitle = paste0(ncol(outcome_significant_scores) - 2, \" Metabolites\")\n    )\n\n\n\n\n\n비지도 학습 방법인 PCA와 K-means clustering를 이용하여 차원 축소와 군집화를 시도하였으나, 이 방법을 사용하는 모든 대사체에 대해 AD 상태가 명확하게 분류되지 않는 것으로 보인다. PCA와 K-means는 EDA에서 선별된 대사산물로 군집화를 수행했을 때 전체 metabolites 보다 선별된 metabolites 에서 AD 상태에 대한 정보가 조금 더 많이 설명되는 것을 PCA를 통해 관찰할 수 있었다. K means clustering도 선별된 metabolites에 대해서 성능 향상을 보여준다. 그러나 전반적인 정확도가 매우 낮기 때문에 지도 학습을 통해 AD 상태를 잘 설명하는 대사체를 선택할 것이다.\nDimensionality reduction and clustering were attempted using PCA and K-means clustering, which are unsupervised learning methods, but AD status seems to not be clearly classified for all metabolites using the methods. When PCA and K means clustering were performed with the metabolites selected from EDA, it was observed through PCA that a little more information about AD status was explained with the selected metabolites than with the entire set of metaboliotes. K means clustering also showed an improvement in performance with the selected metabolites. However, the overall accuracy is very low, so we will select metabolites that explain AD status well through supervised learning."
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html",
    "href": "docs/projects/LLFS/data_preparation.html",
    "title": "Data Preparation",
    "section": "",
    "text": "flowchart TB\n    subgraph Simulation\n        direction TB\n        subgraph Assign_Setting_Values\n            direction LR\n            Assign_Sample_Size---\n            Assign_Dimension_Size---\n            Assign_Covariance_Correlation---\n            Assign_Several_Proportions---\n            Assign_Noise_Intensity\n        end\n        subgraph Generate_Metabolite_Variables\n            direction LR\n            Generate_Covariance_Matrix---\n            Apply_Noise_to_Covariance---\n            Generate_Weights_Matrix---\n            Use_MVN_Distribution---\n            Generate_Metabolite_Data\n        end\n        subgraph Generate_Outcome_Variable\n            direction LR\n            Calculate_Score_Matrix---\n            Use_Logit_Link---\n            Calculate_Outcome_Probabilities---\n            Use_Binomial_Distribution1---\n            Generate_Binary_Outcome_Data\n        end\n        subgraph Generate_Sex_Variable\n            direction LR\n            Use_Binomial_Distribution2---\n            Generate_Sex_Data\n        end\n        subgraph Generate_Age_Variable\n            direction LR\n            Search_the_Strongest_Metabolite---\n            Rescale_It_to_Age---\n            Generate_Age_Data\n        end\n        subgraph Generate_Genotype_Variable\n            direction LR       \n            Calculate_Marginal_Proportions---\n            Calcualte_Joint_Proportions---\n            Generate_Genotype_data\n        end\n        subgraph Merge_All_Data\n            direction LR\n            Outcome_Variable---\n            Sex_Variable---\n            Age_Variable---\n            Genotype_Variable---\n            Metabolite_Data\n        end\n        Assign_Setting_Values--&gt;Generate_Metabolite_Variables--&gt;Generate_Outcome_Variable--&gt;\n        Generate_Sex_Variable--&gt;Generate_Age_Variable--&gt;Generate_Genotype_Variable--&gt;\n        Merge_All_Data\n    end\n    subgraph Data_Analytics\n        direction LR\n        exploratory_data_analysis---\n        statistical_analysis---\n        machine_learning\n    end\n    subgraph Conclusion\n        direction LR\n    end\n    Simulation--&gt;Data_Analytics--&gt;Conclusion\n\n\n\n\n\n\nMVN: Multivariate Normal Distirubtion\n\n\n\n\nKorean\n\n\n\n\nEnglish\n\n\n\n\n\n대략적인 분석 방법론을 간단히 보여주기 위해 Simulation을 수행했다. 이해를 돕기위해 Simulation 순서도를 간략히 설명하자면 크게 7 단계로 Simulation을 수행했다.\n\n\nData Set Size Setting\nSimulation에 필요한 몇 가지 설정값들을 Global Variables로 설정하여 후차적으로 작성된 스크립트에서 호출이 자유롭도록 작성했다. 변수들은 아래의 Simulation section에 있는 Global Variables (see Section 2.2) 에서 확인 가능하다.\nCategorial Data Setting\n먼저, 고차원 데이터의 차원을 설정하기 위해 Sample Size와 변수의 수를 설정한 후 Categorical predictors를 만들기 위해 잘 알려진 분포, 내가 정한 분포, 혹은 임의로 발생하게 만든 분포를 설정하였다. (see Section 2.2, Section 2.3, and Section 2.4)\nSex Variable Setting\nSex 변수는 \\(X \\sim \\text{Bernoulli}(0.5)\\) 을 통해 data를 생성했다. (see Section 2.4)\nGenotype Variable Setting\nGenotype 변수의 data는 아직도 어떻게 통계적으로 생성해야하는지 감을 못잡은 상태이기 때문에 더 연구가 필요하다. 하지만, 질병에 대한 유전적 영향도는 반영해야하기 때문에 outcome variable과 이미 잘 알려진 genotype의 분포를 반영하려고 노력했다. 두 변수에 연관성을 갖게하기 위해 각 변수의 proportion을 marginal distirubtion으로 설정하여 두 변수의 joint proportion을 계산하여 Genotype data를 생성했다. (see Section 2.3 and Section 2.4)\nMetabolite Data Setting\n고차원의 metabolite data를 만드는 설정으로, 고차원이면서 그룹내 서로 상관 관계가 있는 변수들을 생성하기 위해 난수에 의해 발생되는 임의의 Covariance를 생성하여 MVN (Multivariate Normal Distribution)에 반영되게 했고 각 그룹의 반응 변수로의 영향(또는 가중치)도 또한 난수로 임의적으로 발생되게 설정했다. 이때, 난수에 의해 임의적으로 발생하는 수치는 내가 임의적으로 범위를 한정했다. 재현성을 위해 seed number를 고정했다. (see Section 2.4)\nOutcome Variable Setting\nMVN에 의해 만들어진 Data와 미리 만들어 놓은 가중치 Matrix의 곱을 통해 Score Matrix를 만들고 Logit Link를 이용하여 각 Sample의 확률값을 만들었다. 각 Sample의 확률값을 기반으로 \\(X \\sim \\text{Bernoulli}(p)\\), (여기서 \\(p\\)는 각 sample이 갖는 확률값을 뜻한다), 을 통해 disease status의 정보를 담은 binary outcome variable를 만들었다. (see Section 2.4)\nAge Variable Setting\nAge 변수는 생물학적, 의학적으로 치매와 연관성이 높은 요인으로 Outcome 변수로 가장 설명이 잘되는 metabolite를 탐색해 선별하여 Age 형태로 변환을 했다. 제일 어린 사람을 65세 그리고 제일 연장자를 105세로 설정하여 min max normalization을 적용했다. (see (see Section 2.3 and Section 2.4)\nMerge All Data\nSimulation을 통해 만들어진 각 변수들을 index를 만들어 병합시켜 data frame의 형태로 만들었다. (see Section 2.4)\nAnalytics & Conclusion\n분석 부분은 이 data preparation section에서는 자세히 기술하지 않고 EDA, Statistical Approach 및 ML Approach Section에서 자세히 다룰예정이다. 간략히 말하면, outcome 변수와 통계적으로 유의한 관계를 갖는 metabolite를 선별하고 그 결과가 machine learning을 이용하여 얻은 결과와 얼마나 같은지 비교 분석을 하여 Outcome variable에 가장 연관성이 있는 변수들을 규명하는 방법을 기술할 예정이다.\n\n\n\n\n\n\n\nCode\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nCode\n# the number of samples\nsample_size &lt;- 500 #1000\n# the number of predictors\npredictor_size &lt;- 1000 #5000\n# the number of groups\ngroup_size &lt;- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors &lt;- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors&lt;-floor(significant_predictors*0.4) \nnegatively_associated_predictors&lt;-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list&lt;-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%&gt;%round(3) \nnames(group_proportion_list)&lt;-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix &lt;- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data&lt;-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%&gt;%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%&gt;%round(0),\n            # the 1st index of predictors in each group\n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE),\n            # effect of each group on an outcome variable \n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]&lt;-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]&lt;-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata&lt;-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix&lt;-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients &lt;- rnorm(predictor_size,0,0.05)\n\nanswer_list&lt;-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nCode\n## Function List\n\nscale_function=function(vector=x,min=NULL,max=NULL,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data \n    # that are statistically associated with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)&lt;-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite&lt;-\n        temp_df%&gt;%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%&gt;%\n        filter(abs_mean_diff==max(abs_mean_diff))%&gt;%\n        dplyr::select(metabolite)%&gt;%pull\n    \n    ## generate age data with min max normalization\n    age_data&lt;-\n        data%&gt;%\n        dplyr::select(strong_metabolite)%&gt;%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%&gt;%\n        rename(age=1)%&gt;%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated with \n    # some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n    binary_proportion&lt;-as.numeric(table(in_response)/sample_size) #the simulated proportion for the disease vs non-disease cases\n    genotype_proportion&lt;-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion&lt;-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nCode\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range &lt;- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] &lt;- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) &lt;- 1\n    data[, group_range] &lt;- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names&lt;-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]&lt;-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities &lt;- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%&gt;%\n    ifelse(.&gt;1,1,.)%&gt;%\n    ifelse(.&lt;0,0,.)\n\nresponse &lt;-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%&gt;%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data &lt;- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data &lt;- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data &lt;- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data&lt;-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%&gt;%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nCode\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\nThis data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers.\n\n\n\n\nSimulations were performed to outline the approximate analysis methodology. For your better understanding, I will briefly describe the simulation flow diagram. The simulation was conducted in 9 steps.\n\n\nData Set Size Setting\nSome of the setting values ​​required for simulation are set as global variables so that they can be freely called in the later scripts. (see Section 2.2)\nCategorial Data Setting\nI first set the dimensions of my high-dimensional data by setting the sample size and number of variables, then I created categorical data by choosing a well-known distribution, a distribution I determined, or a distribution that occurred randomly set. (see Section 2.2, Section 2.3, and Section 2.4)\nSex Variable Setting\nThe data of the sex variable were generated through \\(X \\sim \\text{Bernoulli}(0.5)\\). (Section 2.4)\nGenotype Variable Setting\nPersonally, I have not yet figured out how to generate data for genotype (categorical) variables statistically, so further research is needed. However, since the genetic influence on the disease should be reflected, I tried to reflect the distribution of outcome variables and well-known distribution of the genotypes, APOE (from Wiki). To make an association between the two variables, the proportions of each variable were set as marginal distirubtion and the joint distribution of the two variables was calculated to generate genotype data. (Section 2.3 and Section 2.4)\nMetabolite Data Setting\nAs a setting for generating high-dimensional metabolomic data, a covariance matrix generated by random numbers is generated to create high-dimensional and mutually correlated metabolites within a group, which is used as input in the MVN (Multivariate Normal Distribution) function, and for each group, the metabolites’ effect (or weight) toward the outcome variable is also set to be randomly generated with a random number. At this time, the range of numbers randomly generated by random numbers was arbitrarily limited by myself. A seed number was fixed for reproducibility. (Section 2.4)\nOutcome Variable Setting\nA score matrix was created through the matrix multiplication of the data created by MVN and a pre-made weight matrix with the probability values of samples that were created using the Logit Link. Based on the probability value of each sample, a binary outcome variable representing disease status information was created through \\(X \\sim \\text{Bernoulli}(p)\\), (where \\(p\\) means the probability value of each sample). (Section 2.4)\nAge Variable Setting\nSince the Age variable is a important factor related to dementia biologically and medically, the metabolite best explained as an Outcome variable was selected and converted into an age scale using min-max normalization by setting the youngest to 65 and the oldest to 105. (Section 2.3 and Section 2.4)\nMerge All Data\nEach variable created through simulation was merged into a data frame. (Section 2.4)\nAnalytics & Conclusion\nThe analysis part will not be discussed in detail in this data preparation section, but will be covered in detail in the EDA, Statistical Approaches and ML Approaches section. Briefly, I describe a method to identify the variables most associated with the outcome variable by selecting metabolites that have a statistically significant relationship with the outcome variable and comparing how similar the results are to those obtained through machine learning.\n\n\n\n\n\n\n\nCode\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nCode\n# the number of samples\nsample_size &lt;- 500 #1000\n# the number of predictors\npredictor_size &lt;- 1000 #5000\n# the number of groups\ngroup_size &lt;- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors &lt;- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors&lt;-floor(significant_predictors*0.4) \nnegatively_associated_predictors&lt;-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list&lt;-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%&gt;%round(3) \nnames(group_proportion_list)&lt;-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix &lt;- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data&lt;-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%&gt;%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%&gt;%round(0),\n            # the 1st index of predictors in each group \n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE), \n            # effect of each group on an outcome variable\n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]&lt;-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]&lt;-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata&lt;-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix&lt;-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients &lt;- rnorm(predictor_size,0,0.05)\n\nanswer_list&lt;-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nCode\n## Function List\n\nscale_function=function(vector=x,min=a,max=b,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))/(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data that are statistically associated \n    # with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)&lt;-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite&lt;-\n        temp_df%&gt;%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%&gt;%\n        filter(abs_mean_diff==max(abs_mean_diff))%&gt;%\n        dplyr::select(metabolite)%&gt;%pull\n    \n    ## generate age data with min max normalization\n    age_data&lt;-\n        data%&gt;%\n        dplyr::select(strong_metabolite)%&gt;%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%&gt;%\n        rename(age=1)%&gt;%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated \n    # with some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n\n    ##the simulated proportion for the disease vs non-disease cases\n    binary_proportion&lt;-as.numeric(table(in_response)/sample_size) \n    genotype_proportion&lt;-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion&lt;-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nCode\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range &lt;- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] &lt;- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) &lt;- 1\n    data[, group_range] &lt;- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names&lt;-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]&lt;-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities &lt;- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%&gt;%\n    ifelse(.&gt;1,1,.)%&gt;%\n    ifelse(.&lt;0,0,.)\n\nresponse &lt;-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%&gt;%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data &lt;- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data &lt;- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data &lt;- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data&lt;-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%&gt;%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nCode\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\nThis data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#simulation-flowchart",
    "href": "docs/projects/LLFS/data_preparation.html#simulation-flowchart",
    "title": "Data Preparation",
    "section": "",
    "text": "flowchart TB\n    subgraph Simulation\n        direction TB\n        subgraph Assign_Setting_Values\n            direction LR\n            Assign_Sample_Size---\n            Assign_Dimension_Size---\n            Assign_Covariance_Correlation---\n            Assign_Several_Proportions---\n            Assign_Noise_Intensity\n        end\n        subgraph Generate_Metabolite_Variables\n            direction LR\n            Generate_Covariance_Matrix---\n            Apply_Noise_to_Covariance---\n            Generate_Weights_Matrix---\n            Use_MVN_Distribution---\n            Generate_Metabolite_Data\n        end\n        subgraph Generate_Outcome_Variable\n            direction LR\n            Calculate_Score_Matrix---\n            Use_Logit_Link---\n            Calculate_Outcome_Probabilities---\n            Use_Binomial_Distribution1---\n            Generate_Binary_Outcome_Data\n        end\n        subgraph Generate_Sex_Variable\n            direction LR\n            Use_Binomial_Distribution2---\n            Generate_Sex_Data\n        end\n        subgraph Generate_Age_Variable\n            direction LR\n            Search_the_Strongest_Metabolite---\n            Rescale_It_to_Age---\n            Generate_Age_Data\n        end\n        subgraph Generate_Genotype_Variable\n            direction LR       \n            Calculate_Marginal_Proportions---\n            Calcualte_Joint_Proportions---\n            Generate_Genotype_data\n        end\n        subgraph Merge_All_Data\n            direction LR\n            Outcome_Variable---\n            Sex_Variable---\n            Age_Variable---\n            Genotype_Variable---\n            Metabolite_Data\n        end\n        Assign_Setting_Values--&gt;Generate_Metabolite_Variables--&gt;Generate_Outcome_Variable--&gt;\n        Generate_Sex_Variable--&gt;Generate_Age_Variable--&gt;Generate_Genotype_Variable--&gt;\n        Merge_All_Data\n    end\n    subgraph Data_Analytics\n        direction LR\n        exploratory_data_analysis---\n        statistical_analysis---\n        machine_learning\n    end\n    subgraph Conclusion\n        direction LR\n    end\n    Simulation--&gt;Data_Analytics--&gt;Conclusion\n\n\n\n\n\n\nMVN: Multivariate Normal Distirubtion\n\n\n\n\nKorean\n\n\n\n\nEnglish\n\n\n\n\n\n대략적인 분석 방법론을 간단히 보여주기 위해 Simulation을 수행했다. 이해를 돕기위해 Simulation 순서도를 간략히 설명하자면 크게 7 단계로 Simulation을 수행했다.\n\n\nData Set Size Setting\nSimulation에 필요한 몇 가지 설정값들을 Global Variables로 설정하여 후차적으로 작성된 스크립트에서 호출이 자유롭도록 작성했다. 변수들은 아래의 Simulation section에 있는 Global Variables (see Section 2.2) 에서 확인 가능하다.\nCategorial Data Setting\n먼저, 고차원 데이터의 차원을 설정하기 위해 Sample Size와 변수의 수를 설정한 후 Categorical predictors를 만들기 위해 잘 알려진 분포, 내가 정한 분포, 혹은 임의로 발생하게 만든 분포를 설정하였다. (see Section 2.2, Section 2.3, and Section 2.4)\nSex Variable Setting\nSex 변수는 \\(X \\sim \\text{Bernoulli}(0.5)\\) 을 통해 data를 생성했다. (see Section 2.4)\nGenotype Variable Setting\nGenotype 변수의 data는 아직도 어떻게 통계적으로 생성해야하는지 감을 못잡은 상태이기 때문에 더 연구가 필요하다. 하지만, 질병에 대한 유전적 영향도는 반영해야하기 때문에 outcome variable과 이미 잘 알려진 genotype의 분포를 반영하려고 노력했다. 두 변수에 연관성을 갖게하기 위해 각 변수의 proportion을 marginal distirubtion으로 설정하여 두 변수의 joint proportion을 계산하여 Genotype data를 생성했다. (see Section 2.3 and Section 2.4)\nMetabolite Data Setting\n고차원의 metabolite data를 만드는 설정으로, 고차원이면서 그룹내 서로 상관 관계가 있는 변수들을 생성하기 위해 난수에 의해 발생되는 임의의 Covariance를 생성하여 MVN (Multivariate Normal Distribution)에 반영되게 했고 각 그룹의 반응 변수로의 영향(또는 가중치)도 또한 난수로 임의적으로 발생되게 설정했다. 이때, 난수에 의해 임의적으로 발생하는 수치는 내가 임의적으로 범위를 한정했다. 재현성을 위해 seed number를 고정했다. (see Section 2.4)\nOutcome Variable Setting\nMVN에 의해 만들어진 Data와 미리 만들어 놓은 가중치 Matrix의 곱을 통해 Score Matrix를 만들고 Logit Link를 이용하여 각 Sample의 확률값을 만들었다. 각 Sample의 확률값을 기반으로 \\(X \\sim \\text{Bernoulli}(p)\\), (여기서 \\(p\\)는 각 sample이 갖는 확률값을 뜻한다), 을 통해 disease status의 정보를 담은 binary outcome variable를 만들었다. (see Section 2.4)\nAge Variable Setting\nAge 변수는 생물학적, 의학적으로 치매와 연관성이 높은 요인으로 Outcome 변수로 가장 설명이 잘되는 metabolite를 탐색해 선별하여 Age 형태로 변환을 했다. 제일 어린 사람을 65세 그리고 제일 연장자를 105세로 설정하여 min max normalization을 적용했다. (see (see Section 2.3 and Section 2.4)\nMerge All Data\nSimulation을 통해 만들어진 각 변수들을 index를 만들어 병합시켜 data frame의 형태로 만들었다. (see Section 2.4)\nAnalytics & Conclusion\n분석 부분은 이 data preparation section에서는 자세히 기술하지 않고 EDA, Statistical Approach 및 ML Approach Section에서 자세히 다룰예정이다. 간략히 말하면, outcome 변수와 통계적으로 유의한 관계를 갖는 metabolite를 선별하고 그 결과가 machine learning을 이용하여 얻은 결과와 얼마나 같은지 비교 분석을 하여 Outcome variable에 가장 연관성이 있는 변수들을 규명하는 방법을 기술할 예정이다.\n\n\n\n\n\n\n\nCode\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nCode\n# the number of samples\nsample_size &lt;- 500 #1000\n# the number of predictors\npredictor_size &lt;- 1000 #5000\n# the number of groups\ngroup_size &lt;- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors &lt;- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors&lt;-floor(significant_predictors*0.4) \nnegatively_associated_predictors&lt;-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list&lt;-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%&gt;%round(3) \nnames(group_proportion_list)&lt;-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix &lt;- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data&lt;-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%&gt;%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%&gt;%round(0),\n            # the 1st index of predictors in each group\n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE),\n            # effect of each group on an outcome variable \n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]&lt;-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]&lt;-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata&lt;-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix&lt;-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients &lt;- rnorm(predictor_size,0,0.05)\n\nanswer_list&lt;-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nCode\n## Function List\n\nscale_function=function(vector=x,min=NULL,max=NULL,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data \n    # that are statistically associated with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)&lt;-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite&lt;-\n        temp_df%&gt;%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%&gt;%\n        filter(abs_mean_diff==max(abs_mean_diff))%&gt;%\n        dplyr::select(metabolite)%&gt;%pull\n    \n    ## generate age data with min max normalization\n    age_data&lt;-\n        data%&gt;%\n        dplyr::select(strong_metabolite)%&gt;%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%&gt;%\n        rename(age=1)%&gt;%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated with \n    # some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n    binary_proportion&lt;-as.numeric(table(in_response)/sample_size) #the simulated proportion for the disease vs non-disease cases\n    genotype_proportion&lt;-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion&lt;-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nCode\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range &lt;- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] &lt;- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) &lt;- 1\n    data[, group_range] &lt;- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names&lt;-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]&lt;-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities &lt;- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%&gt;%\n    ifelse(.&gt;1,1,.)%&gt;%\n    ifelse(.&lt;0,0,.)\n\nresponse &lt;-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%&gt;%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data &lt;- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data &lt;- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data &lt;- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data&lt;-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%&gt;%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nCode\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\nThis data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers.\n\n\n\n\nSimulations were performed to outline the approximate analysis methodology. For your better understanding, I will briefly describe the simulation flow diagram. The simulation was conducted in 9 steps.\n\n\nData Set Size Setting\nSome of the setting values ​​required for simulation are set as global variables so that they can be freely called in the later scripts. (see Section 2.2)\nCategorial Data Setting\nI first set the dimensions of my high-dimensional data by setting the sample size and number of variables, then I created categorical data by choosing a well-known distribution, a distribution I determined, or a distribution that occurred randomly set. (see Section 2.2, Section 2.3, and Section 2.4)\nSex Variable Setting\nThe data of the sex variable were generated through \\(X \\sim \\text{Bernoulli}(0.5)\\). (Section 2.4)\nGenotype Variable Setting\nPersonally, I have not yet figured out how to generate data for genotype (categorical) variables statistically, so further research is needed. However, since the genetic influence on the disease should be reflected, I tried to reflect the distribution of outcome variables and well-known distribution of the genotypes, APOE (from Wiki). To make an association between the two variables, the proportions of each variable were set as marginal distirubtion and the joint distribution of the two variables was calculated to generate genotype data. (Section 2.3 and Section 2.4)\nMetabolite Data Setting\nAs a setting for generating high-dimensional metabolomic data, a covariance matrix generated by random numbers is generated to create high-dimensional and mutually correlated metabolites within a group, which is used as input in the MVN (Multivariate Normal Distribution) function, and for each group, the metabolites’ effect (or weight) toward the outcome variable is also set to be randomly generated with a random number. At this time, the range of numbers randomly generated by random numbers was arbitrarily limited by myself. A seed number was fixed for reproducibility. (Section 2.4)\nOutcome Variable Setting\nA score matrix was created through the matrix multiplication of the data created by MVN and a pre-made weight matrix with the probability values of samples that were created using the Logit Link. Based on the probability value of each sample, a binary outcome variable representing disease status information was created through \\(X \\sim \\text{Bernoulli}(p)\\), (where \\(p\\) means the probability value of each sample). (Section 2.4)\nAge Variable Setting\nSince the Age variable is a important factor related to dementia biologically and medically, the metabolite best explained as an Outcome variable was selected and converted into an age scale using min-max normalization by setting the youngest to 65 and the oldest to 105. (Section 2.3 and Section 2.4)\nMerge All Data\nEach variable created through simulation was merged into a data frame. (Section 2.4)\nAnalytics & Conclusion\nThe analysis part will not be discussed in detail in this data preparation section, but will be covered in detail in the EDA, Statistical Approaches and ML Approaches section. Briefly, I describe a method to identify the variables most associated with the outcome variable by selecting metabolites that have a statistically significant relationship with the outcome variable and comparing how similar the results are to those obtained through machine learning.\n\n\n\n\n\n\n\nCode\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nCode\n# the number of samples\nsample_size &lt;- 500 #1000\n# the number of predictors\npredictor_size &lt;- 1000 #5000\n# the number of groups\ngroup_size &lt;- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors &lt;- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors&lt;-floor(significant_predictors*0.4) \nnegatively_associated_predictors&lt;-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list&lt;-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%&gt;%round(3) \nnames(group_proportion_list)&lt;-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix &lt;- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data&lt;-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%&gt;%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%&gt;%round(0),\n            # the 1st index of predictors in each group \n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE), \n            # effect of each group on an outcome variable\n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]&lt;-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]&lt;-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata&lt;-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix&lt;-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients &lt;- rnorm(predictor_size,0,0.05)\n\nanswer_list&lt;-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nCode\n## Function List\n\nscale_function=function(vector=x,min=a,max=b,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))/(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data that are statistically associated \n    # with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)&lt;-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite&lt;-\n        temp_df%&gt;%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%&gt;%\n        filter(abs_mean_diff==max(abs_mean_diff))%&gt;%\n        dplyr::select(metabolite)%&gt;%pull\n    \n    ## generate age data with min max normalization\n    age_data&lt;-\n        data%&gt;%\n        dplyr::select(strong_metabolite)%&gt;%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%&gt;%\n        rename(age=1)%&gt;%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated \n    # with some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n\n    ##the simulated proportion for the disease vs non-disease cases\n    binary_proportion&lt;-as.numeric(table(in_response)/sample_size) \n    genotype_proportion&lt;-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion&lt;-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nCode\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range &lt;- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] &lt;- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) &lt;- 1\n    data[, group_range] &lt;- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names&lt;-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]&lt;-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities &lt;- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%&gt;%\n    ifelse(.&gt;1,1,.)%&gt;%\n    ifelse(.&lt;0,0,.)\n\nresponse &lt;-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%&gt;%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data &lt;- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data &lt;- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data &lt;- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data&lt;-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%&gt;%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nCode\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\nThis data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#simulation",
    "href": "docs/projects/LLFS/data_preparation.html#simulation",
    "title": "Data Preparation",
    "section": "",
    "text": "Code\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nCode\n# the number of samples\nsample_size &lt;- 500 #1000\n# the number of predictors\npredictor_size &lt;- 1000 #5000\n# the number of groups\ngroup_size &lt;- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors &lt;- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors&lt;-floor(significant_predictors*0.4) \nnegatively_associated_predictors&lt;-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list&lt;-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%&gt;%round(3) \nnames(group_proportion_list)&lt;-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix &lt;- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data&lt;-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%&gt;%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%&gt;%round(0),\n            # the 1st index of predictors in each group\n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE),\n            # effect of each group on an outcome variable \n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]&lt;-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]&lt;-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata&lt;-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix&lt;-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients &lt;- rnorm(predictor_size,0,0.05)\n\nanswer_list&lt;-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nCode\n## Function List\n\nscale_function=function(vector=x,min=NULL,max=NULL,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data \n    # that are statistically associated with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)&lt;-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite&lt;-\n        temp_df%&gt;%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%&gt;%\n        filter(abs_mean_diff==max(abs_mean_diff))%&gt;%\n        dplyr::select(metabolite)%&gt;%pull\n    \n    ## generate age data with min max normalization\n    age_data&lt;-\n        data%&gt;%\n        dplyr::select(strong_metabolite)%&gt;%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%&gt;%\n        rename(age=1)%&gt;%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated with \n    # some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n    binary_proportion&lt;-as.numeric(table(in_response)/sample_size) #the simulated proportion for the disease vs non-disease cases\n    genotype_proportion&lt;-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion&lt;-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nCode\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range &lt;- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] &lt;- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) &lt;- 1\n    data[, group_range] &lt;- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names&lt;-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]&lt;-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities &lt;- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%&gt;%\n    ifelse(.&gt;1,1,.)%&gt;%\n    ifelse(.&lt;0,0,.)\n\nresponse &lt;-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%&gt;%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data &lt;- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data &lt;- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data &lt;- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data&lt;-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%&gt;%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nCode\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)"
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#data-description",
    "href": "docs/projects/LLFS/data_preparation.html#data-description",
    "title": "Data Preparation",
    "section": "",
    "text": "This data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#simulation-1",
    "href": "docs/projects/LLFS/data_preparation.html#simulation-1",
    "title": "Data Preparation",
    "section": "",
    "text": "Code\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nCode\n# the number of samples\nsample_size &lt;- 500 #1000\n# the number of predictors\npredictor_size &lt;- 1000 #5000\n# the number of groups\ngroup_size &lt;- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors &lt;- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors&lt;-floor(significant_predictors*0.4) \nnegatively_associated_predictors&lt;-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list&lt;-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%&gt;%round(3) \nnames(group_proportion_list)&lt;-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix &lt;- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data&lt;-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%&gt;%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%&gt;%round(0),\n            # the 1st index of predictors in each group \n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE), \n            # effect of each group on an outcome variable\n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]&lt;-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]&lt;-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata&lt;-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix&lt;-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients &lt;- rnorm(predictor_size,0,0.05)\n\nanswer_list&lt;-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nCode\n## Function List\n\nscale_function=function(vector=x,min=a,max=b,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))/(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data that are statistically associated \n    # with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)&lt;-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite&lt;-\n        temp_df%&gt;%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%&gt;%\n        filter(abs_mean_diff==max(abs_mean_diff))%&gt;%\n        dplyr::select(metabolite)%&gt;%pull\n    \n    ## generate age data with min max normalization\n    age_data&lt;-\n        data%&gt;%\n        dplyr::select(strong_metabolite)%&gt;%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%&gt;%\n        rename(age=1)%&gt;%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated \n    # with some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n\n    ##the simulated proportion for the disease vs non-disease cases\n    binary_proportion&lt;-as.numeric(table(in_response)/sample_size) \n    genotype_proportion&lt;-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion&lt;-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nCode\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range &lt;- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] &lt;- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) &lt;- 1\n    data[, group_range] &lt;- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names&lt;-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] &lt;- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]&lt;-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities &lt;- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%&gt;%\n    ifelse(.&gt;1,1,.)%&gt;%\n    ifelse(.&lt;0,0,.)\n\nresponse &lt;-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%&gt;%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data &lt;- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data &lt;- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data &lt;- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data&lt;-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%&gt;%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nCode\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)"
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#data-description-1",
    "href": "docs/projects/LLFS/data_preparation.html#data-description-1",
    "title": "Data Preparation",
    "section": "",
    "text": "This data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html",
    "href": "docs/projects/heavy_metal/index.html",
    "title": "Project Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n중금속(예: 구리(Cu), 아연(Zn) 및 코발트(Co))은 silent 오염 물질이다. 물 속에서 그들의 존재는 쉽게 감지되지 않는다. 이러한 오염 물질은 다양한 방식으로 우리 몸에 들어갈 수 있는데 오염된 물을 마시거나 오염된 음식을 섭취함으로써 이러한 오염 물질은 인간과 다른 살아있는 유기체에 독성 영향을 미치게 된다. 사용한 찻잎은 오염된 폐수에서 발견되는 2가 Cu, Zn 및 Co 이온에 대한 흡착 기질로 사용할 수 있다.\n오염된 물에서 중금속을 제거하기 위해 찻잎을 사용할 수 있는 비율을 모델링하기 위해 미분 방정식이 사용되었다. 이 모델은 실험의 시계열 데이터와 비선형 최소 제곱 회귀를 사용하여 테스트되었다.\n\n\n\n\n찻잎에 대한 중금속의 흡착 과정은 미분 방정식을 사용하여 mechnistically 모델링할 수 있다.\nmechanistic model parameter는 시계열 데이터에 맞게 추정할 수 있다.\n\n\n\n\n\n\n\n\n\n\\(S(t)\\) is the number of heavy metal molecules adsorbed to the tea leaves at time \\(t\\).\n\\(W(t)\\) is the number of heavy metal molecules in the water (and not adsorbed) at time \\(t\\).\n\\(W_0=W(0)=S(t)+W(t)\\)\n\\(S_e\\) is the number of heavy metal molecules adsorbed at the equilibrium state = the “relevant” number of adsorption sites. \\(S_e\\) is system dependent: e.g., a different initial concentration will have a different \\(S_e\\)\n\n\\(S_e=W_0 - W_e\\)\n\\(q(t)\\) is the fraction of heavy metal molecules adsorbed out of the waste water at time \\(t\\)\n\\(q(t)=\\frac{S(t)}{W_0}\\)\n\n\n\n\n\\[\n\\begin{align*}\n\\frac{dS}{dt} &=\\frac{W_0-S(t)}{V}fb\\frac{S_e-S(t)}{S_e}p\\\\\n&=k_s(W_0-S(t))(S_e-S(t)) \\text{ where } k_s=\\frac{fbp}{VS_e}\n\\end{align*}\n\\]\nwhere\n\n\\(\\frac{W_0-S(t)}{V}fb\\) is a fraction of heavy metal molecules per \\(mL\\) of water,\n\\(f\\) is \\(mL\\)’s of water that contact leaves per minute,\n\\(b\\) is the probability that a leaf hitting molecule hits a binding site,\n\\(\\frac{S_e-S(t)}{S_e}\\) is a fraction of biniding sites that are occupied, and\n\\(p\\) is the probability that hitting an unoccupied binding site results in binding.\n\nSince \\(q(t)=\\frac{S(t)}{W_0}\\),\n\\[\n\\begin{align*}\n\\frac{dq}{dt} &=\\frac{S'(t)}{W_0}\\\\\n&=\\frac{k_S}{W_0}(W_0-S(t))(S_e-S(t))\\\\\n&=W_0k_S\\frac{(W_0-S(t))(S_e-S(t))}{W_0^2}\\\\\n&=k_S(1-q(t))(q_e-q(t)) \\text{ where } k_q=W_0k_S=\\frac{W_0fbp}{VS_e}=\\frac{fbp}{Vq_e} \\\\\n&=\\frac{dq}{dt} \\\\\n&=k_q(1-q)(q_e-q) \\\\\nq(0) &=0\n\\end{align*}\n\\]\nSolving the above initial value problem: \\[\n\\begin{align*}\n\\frac{dq}{dt}&=k_q(1-q)(q_e-q) \\\\\n\\frac{dq}{(1-q)(q_e-q)}&=k_qdt \\\\\n\\int \\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\frac{1}{(1-q)(q_e-q)}&=\\frac{A}{1-q}+\\frac{B}{q_e-q} \\\\\n1&=A(q_e-q)+B(1-q)\\\\\n1&=A(q_e-1) \\text{ by letting } q=1\\\\\n(q_e-1)^{-1}&=A\\\\\n1&=B(1-q_e) \\text{ by letting } q=q_e\\\\\n(1-q_e)^{-1}&=B\\\\\n\\int\\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\int \\left \\{\\frac{(q_e-1)^{-1}}{1-q}+\\frac{(1-q_e)^{-1}}{q_e-q}\\right\\}dq&=\\int k_qdt \\\\\nq(t)&= \\frac{q_e-q_ee^{-kt}}{1-q_ee^{-kt}}=q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}} \\text{ where } k=(1-q_e)k_q\\\\\n\\arg\\min_k&\\sum_{i=1}^{n}(q(t_i,k)-\\hat{q}(t_i))^2\n\\end{align*}\n\\] where\n\n\\((t_i,\\hat{q}(t_i))\\) is the observed values,\n\\(q(t,k) =q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}}\\)\n\n\n\n\n\n\n\nMathematics: differential equation\nStatistics: non-linear least square with the Levenberg-Marquardt algorithm\nBiology\n\n\n\n\n\n1 mathematics professor (advisor)\n1 biology professor (advisor)\n2 mathematicians\n1 biologist\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetal\n\\(q_e\\)\nparameter\nEstimate\nStd. Error\n\\(t\\) value\n\\(Pr(&gt;|t|)\\)\ndf\nn\n\n\n\n\ncopper\n.46\n\\(k\\)\n0.01281219\n0.0008855\n20.47\n\\(&lt;2e-16\\)\n34\n35\n\n\ncobalt\n.235\n\\(k\\)\n0.069941\n0.003279\n21.33\n\\(&lt;2e-16\\)\n34\n35\n\n\nzinc\n.24\n\\(k\\)\n0.150255\n0.009133\n16.45\n\\(&lt;2e-16\\)\n32\n33\n\n\n\n\n\n\n\n\n\nHeavy metals (such as Copper (Cu), Zinc (Zn), and Cobalt (Co)) are silent pollutants; their presence in water is not easily detected. These pollutants can enter our bodies in different ways, e.g. through drinking contaminated water or eating contaminated food. These pollutants have toxic effects on humans and other living organisms. Spent tea leaves can be used as adsorbent substrates for divalent Cu, Zn and Co ions found in polluted waste water.\nDifferential equations were used to model the rate at which tea leaves can be used to clear heavy metals from polluted water. The model was tested using time series data from experiments and non-linear least squares regression.\n\n\n\n\n\nthe adsorption process of heavy metals to tea leaves can be mechanistically modeld using a differential equation.\nthe parameter of the mechanistic model can be estimated to fit the time series data.\n\n\n\n\n\n\n\n\n\n\\(S(t)\\) is the number of heavy metal molecules adsorbed to the tea leaves at time \\(t\\).\n\\(W(t)\\) is the number of heavy metal molecules in the water (and not adsorbed) at time \\(t\\).\n\\(W_0=W(0)=S(t)+W(t)\\)\n\\(S_e\\) is the number of heavy metal molecules adsorbed at the equilibrium state = the “relevant” number of adsorption sites. \\(S_e\\) is system dependent: e.g., a different initial concentration will have a different \\(S_e\\)\n\n\\(S_e=W_0 - W_e\\)\n\\(q(t)\\) is the fraction of heavy metal molecules adsorbed out of the waste water at time \\(t\\)\n\\(q(t)=\\frac{S(t)}{W_0}\\)\n\n\n\n\n\\[\n\\begin{align*}\n\\frac{dS}{dt} &=\\frac{W_0-S(t)}{V}fb\\frac{S_e-S(t)}{S_e}p\\\\\n&=k_s(W_0-S(t))(S_e-S(t)) \\text{ where } k_s=\\frac{fbp}{VS_e}\n\\end{align*}\n\\]\nwhere\n\n\\(\\frac{W_0-S(t)}{V}fb\\) is a fraction of heavy metal molecules per \\(mL\\) of water,\n\\(f\\) is \\(mL\\)’s of water that contact leaves per minute,\n\\(b\\) is the probability that a leaf hitting molecule hits a binding site,\n\\(\\frac{S_e-S(t)}{S_e}\\) is a fraction of biniding sites that are occupied, and\n\\(p\\) is the probability that hitting an unoccupied binding site results in binding.\n\nSince \\(q(t)=\\frac{S(t)}{W_0}\\),\n\\[\n\\begin{align*}\n\\frac{dq}{dt} &=\\frac{S'(t)}{W_0}\\\\\n&=\\frac{k_S}{W_0}(W_0-S(t))(S_e-S(t))\\\\\n&=W_0k_S\\frac{(W_0-S(t))(S_e-S(t))}{W_0^2}\\\\\n&=k_S(1-q(t))(q_e-q(t)) \\text{ where } k_q=W_0k_S=\\frac{W_0fbp}{VS_e}=\\frac{fbp}{Vq_e} \\\\\n&=\\frac{dq}{dt} \\\\\n&=k_q(1-q)(q_e-q) \\\\\nq(0) &=0\n\\end{align*}\n\\]\nSolving the above initial value problem: \\[\n\\begin{align*}\n\\frac{dq}{dt}&=k_q(1-q)(q_e-q) \\\\\n\\frac{dq}{(1-q)(q_e-q)}&=k_qdt \\\\\n\\int \\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\frac{1}{(1-q)(q_e-q)}&=\\frac{A}{1-q}+\\frac{B}{q_e-q} \\\\\n1&=A(q_e-q)+B(1-q)\\\\\n1&=A(q_e-1) \\text{ by letting } q=1\\\\\n(q_e-1)^{-1}&=A\\\\\n1&=B(1-q_e) \\text{ by letting } q=q_e\\\\\n(1-q_e)^{-1}&=B\\\\\n\\int\\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\int \\left \\{\\frac{(q_e-1)^{-1}}{1-q}+\\frac{(1-q_e)^{-1}}{q_e-q}\\right\\}dq&=\\int k_qdt \\\\\nq(t)&= \\frac{q_e-q_ee^{-kt}}{1-q_ee^{-kt}}=q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}} \\text{ where } k=(1-q_e)k_q\\\\\n\\arg\\min_k&\\sum_{i=1}^{n}(q(t_i,k)-\\hat{q}(t_i))^2\n\\end{align*}\n\\] where\n\n\\((t_i,\\hat{q}(t_i))\\) is the observed values,\n\\(q(t,k) =q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}}\\)\n\n\n\n\n\n\n\nMathematics: differential equation\nStatistics: non-linear least square with the Levenberg-Marquardt algorithm\nBiology\n\n\n\n\n\n1 mathematics professor (advisor)\n1 biology professor (advisor)\n2 mathematicians\n1 biologist\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetal\n\\(q_e\\)\nparameter\nEstimate\nStd. Error\n\\(t\\) value\n\\(Pr(&gt;|t|)\\)\ndf\nn\n\n\n\n\ncopper\n.46\n\\(k\\)\n0.01281219\n0.0008855\n20.47\n\\(&lt;2e-16\\)\n34\n35\n\n\ncobalt\n.235\n\\(k\\)\n0.069941\n0.003279\n21.33\n\\(&lt;2e-16\\)\n34\n35\n\n\nzinc\n.24\n\\(k\\)\n0.150255\n0.009133\n16.45\n\\(&lt;2e-16\\)\n32\n33"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#objective",
    "href": "docs/projects/heavy_metal/index.html#objective",
    "title": "Project Description",
    "section": "",
    "text": "찻잎에 대한 중금속의 흡착 과정은 미분 방정식을 사용하여 mechnistically 모델링할 수 있다.\nmechanistic model parameter는 시계열 데이터에 맞게 추정할 수 있다."
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#methodology",
    "href": "docs/projects/heavy_metal/index.html#methodology",
    "title": "Project Description",
    "section": "",
    "text": "\\(S(t)\\) is the number of heavy metal molecules adsorbed to the tea leaves at time \\(t\\).\n\\(W(t)\\) is the number of heavy metal molecules in the water (and not adsorbed) at time \\(t\\).\n\\(W_0=W(0)=S(t)+W(t)\\)\n\\(S_e\\) is the number of heavy metal molecules adsorbed at the equilibrium state = the “relevant” number of adsorption sites. \\(S_e\\) is system dependent: e.g., a different initial concentration will have a different \\(S_e\\)\n\n\\(S_e=W_0 - W_e\\)\n\\(q(t)\\) is the fraction of heavy metal molecules adsorbed out of the waste water at time \\(t\\)\n\\(q(t)=\\frac{S(t)}{W_0}\\)\n\n\n\n\n\\[\n\\begin{align*}\n\\frac{dS}{dt} &=\\frac{W_0-S(t)}{V}fb\\frac{S_e-S(t)}{S_e}p\\\\\n&=k_s(W_0-S(t))(S_e-S(t)) \\text{ where } k_s=\\frac{fbp}{VS_e}\n\\end{align*}\n\\]\nwhere\n\n\\(\\frac{W_0-S(t)}{V}fb\\) is a fraction of heavy metal molecules per \\(mL\\) of water,\n\\(f\\) is \\(mL\\)’s of water that contact leaves per minute,\n\\(b\\) is the probability that a leaf hitting molecule hits a binding site,\n\\(\\frac{S_e-S(t)}{S_e}\\) is a fraction of biniding sites that are occupied, and\n\\(p\\) is the probability that hitting an unoccupied binding site results in binding.\n\nSince \\(q(t)=\\frac{S(t)}{W_0}\\),\n\\[\n\\begin{align*}\n\\frac{dq}{dt} &=\\frac{S'(t)}{W_0}\\\\\n&=\\frac{k_S}{W_0}(W_0-S(t))(S_e-S(t))\\\\\n&=W_0k_S\\frac{(W_0-S(t))(S_e-S(t))}{W_0^2}\\\\\n&=k_S(1-q(t))(q_e-q(t)) \\text{ where } k_q=W_0k_S=\\frac{W_0fbp}{VS_e}=\\frac{fbp}{Vq_e} \\\\\n&=\\frac{dq}{dt} \\\\\n&=k_q(1-q)(q_e-q) \\\\\nq(0) &=0\n\\end{align*}\n\\]\nSolving the above initial value problem: \\[\n\\begin{align*}\n\\frac{dq}{dt}&=k_q(1-q)(q_e-q) \\\\\n\\frac{dq}{(1-q)(q_e-q)}&=k_qdt \\\\\n\\int \\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\frac{1}{(1-q)(q_e-q)}&=\\frac{A}{1-q}+\\frac{B}{q_e-q} \\\\\n1&=A(q_e-q)+B(1-q)\\\\\n1&=A(q_e-1) \\text{ by letting } q=1\\\\\n(q_e-1)^{-1}&=A\\\\\n1&=B(1-q_e) \\text{ by letting } q=q_e\\\\\n(1-q_e)^{-1}&=B\\\\\n\\int\\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\int \\left \\{\\frac{(q_e-1)^{-1}}{1-q}+\\frac{(1-q_e)^{-1}}{q_e-q}\\right\\}dq&=\\int k_qdt \\\\\nq(t)&= \\frac{q_e-q_ee^{-kt}}{1-q_ee^{-kt}}=q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}} \\text{ where } k=(1-q_e)k_q\\\\\n\\arg\\min_k&\\sum_{i=1}^{n}(q(t_i,k)-\\hat{q}(t_i))^2\n\\end{align*}\n\\] where\n\n\\((t_i,\\hat{q}(t_i))\\) is the observed values,\n\\(q(t,k) =q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}}\\)"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#required-skills",
    "href": "docs/projects/heavy_metal/index.html#required-skills",
    "title": "Project Description",
    "section": "",
    "text": "Mathematics: differential equation\nStatistics: non-linear least square with the Levenberg-Marquardt algorithm\nBiology"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#colaborators",
    "href": "docs/projects/heavy_metal/index.html#colaborators",
    "title": "Project Description",
    "section": "",
    "text": "1 mathematics professor (advisor)\n1 biology professor (advisor)\n2 mathematicians\n1 biologist"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#result",
    "href": "docs/projects/heavy_metal/index.html#result",
    "title": "Project Description",
    "section": "",
    "text": "Metal\n\\(q_e\\)\nparameter\nEstimate\nStd. Error\n\\(t\\) value\n\\(Pr(&gt;|t|)\\)\ndf\nn\n\n\n\n\ncopper\n.46\n\\(k\\)\n0.01281219\n0.0008855\n20.47\n\\(&lt;2e-16\\)\n34\n35\n\n\ncobalt\n.235\n\\(k\\)\n0.069941\n0.003279\n21.33\n\\(&lt;2e-16\\)\n34\n35\n\n\nzinc\n.24\n\\(k\\)\n0.150255\n0.009133\n16.45\n\\(&lt;2e-16\\)\n32\n33"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#background",
    "href": "docs/projects/heavy_metal/index.html#background",
    "title": "Project Description",
    "section": "",
    "text": "Heavy metals (such as Copper (Cu), Zinc (Zn), and Cobalt (Co)) are silent pollutants; their presence in water is not easily detected. These pollutants can enter our bodies in different ways, e.g. through drinking contaminated water or eating contaminated food. These pollutants have toxic effects on humans and other living organisms. Spent tea leaves can be used as adsorbent substrates for divalent Cu, Zn and Co ions found in polluted waste water.\nDifferential equations were used to model the rate at which tea leaves can be used to clear heavy metals from polluted water. The model was tested using time series data from experiments and non-linear least squares regression."
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#objective-1",
    "href": "docs/projects/heavy_metal/index.html#objective-1",
    "title": "Project Description",
    "section": "",
    "text": "the adsorption process of heavy metals to tea leaves can be mechanistically modeld using a differential equation.\nthe parameter of the mechanistic model can be estimated to fit the time series data."
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#methodology-1",
    "href": "docs/projects/heavy_metal/index.html#methodology-1",
    "title": "Project Description",
    "section": "",
    "text": "\\(S(t)\\) is the number of heavy metal molecules adsorbed to the tea leaves at time \\(t\\).\n\\(W(t)\\) is the number of heavy metal molecules in the water (and not adsorbed) at time \\(t\\).\n\\(W_0=W(0)=S(t)+W(t)\\)\n\\(S_e\\) is the number of heavy metal molecules adsorbed at the equilibrium state = the “relevant” number of adsorption sites. \\(S_e\\) is system dependent: e.g., a different initial concentration will have a different \\(S_e\\)\n\n\\(S_e=W_0 - W_e\\)\n\\(q(t)\\) is the fraction of heavy metal molecules adsorbed out of the waste water at time \\(t\\)\n\\(q(t)=\\frac{S(t)}{W_0}\\)\n\n\n\n\n\\[\n\\begin{align*}\n\\frac{dS}{dt} &=\\frac{W_0-S(t)}{V}fb\\frac{S_e-S(t)}{S_e}p\\\\\n&=k_s(W_0-S(t))(S_e-S(t)) \\text{ where } k_s=\\frac{fbp}{VS_e}\n\\end{align*}\n\\]\nwhere\n\n\\(\\frac{W_0-S(t)}{V}fb\\) is a fraction of heavy metal molecules per \\(mL\\) of water,\n\\(f\\) is \\(mL\\)’s of water that contact leaves per minute,\n\\(b\\) is the probability that a leaf hitting molecule hits a binding site,\n\\(\\frac{S_e-S(t)}{S_e}\\) is a fraction of biniding sites that are occupied, and\n\\(p\\) is the probability that hitting an unoccupied binding site results in binding.\n\nSince \\(q(t)=\\frac{S(t)}{W_0}\\),\n\\[\n\\begin{align*}\n\\frac{dq}{dt} &=\\frac{S'(t)}{W_0}\\\\\n&=\\frac{k_S}{W_0}(W_0-S(t))(S_e-S(t))\\\\\n&=W_0k_S\\frac{(W_0-S(t))(S_e-S(t))}{W_0^2}\\\\\n&=k_S(1-q(t))(q_e-q(t)) \\text{ where } k_q=W_0k_S=\\frac{W_0fbp}{VS_e}=\\frac{fbp}{Vq_e} \\\\\n&=\\frac{dq}{dt} \\\\\n&=k_q(1-q)(q_e-q) \\\\\nq(0) &=0\n\\end{align*}\n\\]\nSolving the above initial value problem: \\[\n\\begin{align*}\n\\frac{dq}{dt}&=k_q(1-q)(q_e-q) \\\\\n\\frac{dq}{(1-q)(q_e-q)}&=k_qdt \\\\\n\\int \\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\frac{1}{(1-q)(q_e-q)}&=\\frac{A}{1-q}+\\frac{B}{q_e-q} \\\\\n1&=A(q_e-q)+B(1-q)\\\\\n1&=A(q_e-1) \\text{ by letting } q=1\\\\\n(q_e-1)^{-1}&=A\\\\\n1&=B(1-q_e) \\text{ by letting } q=q_e\\\\\n(1-q_e)^{-1}&=B\\\\\n\\int\\frac{dq}{(1-q)(q_e-q)}&=\\int k_qdt \\\\\n\\int \\left \\{\\frac{(q_e-1)^{-1}}{1-q}+\\frac{(1-q_e)^{-1}}{q_e-q}\\right\\}dq&=\\int k_qdt \\\\\nq(t)&= \\frac{q_e-q_ee^{-kt}}{1-q_ee^{-kt}}=q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}} \\text{ where } k=(1-q_e)k_q\\\\\n\\arg\\min_k&\\sum_{i=1}^{n}(q(t_i,k)-\\hat{q}(t_i))^2\n\\end{align*}\n\\] where\n\n\\((t_i,\\hat{q}(t_i))\\) is the observed values,\n\\(q(t,k) =q_e\\frac{1-e^{-kt}}{1-q_ee^{-kt}}\\)"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#required-skills-1",
    "href": "docs/projects/heavy_metal/index.html#required-skills-1",
    "title": "Project Description",
    "section": "",
    "text": "Mathematics: differential equation\nStatistics: non-linear least square with the Levenberg-Marquardt algorithm\nBiology"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#colaborators-1",
    "href": "docs/projects/heavy_metal/index.html#colaborators-1",
    "title": "Project Description",
    "section": "",
    "text": "1 mathematics professor (advisor)\n1 biology professor (advisor)\n2 mathematicians\n1 biologist"
  },
  {
    "objectID": "docs/projects/heavy_metal/index.html#result-1",
    "href": "docs/projects/heavy_metal/index.html#result-1",
    "title": "Project Description",
    "section": "",
    "text": "Metal\n\\(q_e\\)\nparameter\nEstimate\nStd. Error\n\\(t\\) value\n\\(Pr(&gt;|t|)\\)\ndf\nn\n\n\n\n\ncopper\n.46\n\\(k\\)\n0.01281219\n0.0008855\n20.47\n\\(&lt;2e-16\\)\n34\n35\n\n\ncobalt\n.235\n\\(k\\)\n0.069941\n0.003279\n21.33\n\\(&lt;2e-16\\)\n34\n35\n\n\nzinc\n.24\n\\(k\\)\n0.150255\n0.009133\n16.45\n\\(&lt;2e-16\\)\n32\n33"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "",
    "text": "Notice\nLast Update\nIntroduction\n\nDefinition of SW Validation\nSome Terminology\nRationale\nObjective of SW Validation\nWhat to validate\nMain Institutions\n\nQuality System Regulation\nVerification\nValidation\nBenefits and Difficulty in SW V&V\nSW Development as Part of System Design\n\nOverview\nDesign Reveiw\n\n\n\n\nValidation Pinciples\n\nOverview\nConditions\nPlanning\nAfter SW Change\nSW Lifecycle\n\nSW Lifecycle Tasks\n\nOverview\nQuality Planning\nConfiguration Management\nTask Requirements\nDesign Overview\n\nDesign Consideration\nDesign Specification\nDesign Activity and Task\n\n\n\n\n\nTesting Tasks\n\nOverview\nConsideration Before Testing Tasks\nCode Based Testing\nSolution to White Box Testing\nDevelopment Testing\nUser Site Testing\n\nOverview\nTesting\n\n\nMaintenance and SW Changes\nValidation of Quality System SW\n\nOverview\nFactors in Validation"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#table-of-contents",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#table-of-contents",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "",
    "text": "Notice\nLast Update\nIntroduction\n\nDefinition of SW Validation\nSome Terminology\nRationale\nObjective of SW Validation\nWhat to validate\nMain Institutions\n\nQuality System Regulation\nVerification\nValidation\nBenefits and Difficulty in SW V&V\nSW Development as Part of System Design\n\nOverview\nDesign Reveiw\n\n\n\n\nValidation Pinciples\n\nOverview\nConditions\nPlanning\nAfter SW Change\nSW Lifecycle\n\nSW Lifecycle Tasks\n\nOverview\nQuality Planning\nConfiguration Management\nTask Requirements\nDesign Overview\n\nDesign Consideration\nDesign Specification\nDesign Activity and Task\n\n\n\n\n\nTesting Tasks\n\nOverview\nConsideration Before Testing Tasks\nCode Based Testing\nSolution to White Box Testing\nDevelopment Testing\nUser Site Testing\n\nOverview\nTesting\n\n\nMaintenance and SW Changes\nValidation of Quality System SW\n\nOverview\nFactors in Validation"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#notice",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#notice",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Notice",
    "text": "Notice\n\nI am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto. (it seems that Quarto system has some issues on mermaid diagrams.)\nThe FDA validation guidance document is a bit difficult to understand because its explanations provide abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\nLast Update\n\n2022-12-28, Summary of Document"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#introduction",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#introduction",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Introduction",
    "text": "Introduction\n\nDefinition of Software Validation\nSoftware Validation is a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997.\n(See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\n\n\nSome Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology: requirements, specification, verification, and validation.\n\n\n\nRationale\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\nObjective of SW validation is to ensure\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\nWhat to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\nMain Institutions\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#quality-system-regulation",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#quality-system-regulation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Quality System Regulation",
    "text": "Quality System Regulation\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n\n\nflowchart TB\n    subgraph Quality_System_Regulation\n        direction LR\n        subgraph Requirement\n            direction TB\n            user_requirements\n        end\n        subgraph Specification\n           direction TB\n           document_user_requirements \n        end \n        subgraph Verification\n           direction TB\n           verify_spacified_requirements\n        end\n        subgraph Validation\n           direction TB\n           Confirmation_by_Examinations\n           Provision_of_objective_3evidences\n        end\n        Requirement--&gt; Specification --&gt; Verification --&gt; Validation                    \n    end\n    subgraph First_Detail\n        direction TB\n        subgraph User_Requirement\n            direction TB\n            any_need_for_customer---\n            any_need_for_system---\n            any_need_for_software\n        end\n            subgraph Document_User_Requirement\n            direction TB\n            define_means_for_requirements---\n          define_criteria_for_requirements\n        end         \n        subgraph Verify_Spacified_Requirement\n            direction TB\n            Objective_Evidence---&gt;|needs|Software_Testing\n        end\n        subgraph SW_Validation\n            direction TB\n            subgraph Confirmation_by_Examination\n            direction TB\n                subgraph Examination_List_of_SW_LifeCycle\n                    direction TB\n                    comprehensiveness_of_software_testing---\n                    inspection_verification_test---\n                    analysis_verification_test---\n                    other_varification_tests    \n                end \n            end             \n            subgraph Provision_of_Objective_3evidences\n                direction TB\n                Software_specifications_conformity---\n                Consistent_SW_Implementation---\n                Correctness_Completeness_Traceability\n            end\n        end\n        Requirement---User_Requirement\n        Specification---Document_User_Requirement\n        Verification---Verify_Spacified_Requirement\n        Confirmation_by_Examinations---Confirmation_by_Examination\n        Provision_of_objective_3evidences---Provision_of_Objective_3evidences             \n    end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#verification",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#verification",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Verification",
    "text": "Verification\n\n\n\n\nflowchart LR\n    subgraph Objective_Evidence\n        direction LR\n        subgraph Design_Outputs_of_SW_life_cycle_for_Specified_Requirements\n            direction TB\n            Consistency---\n            Completeness---\n            Correctness---\n            Documentation\n        end       \n        subgraph Software_Testing\n            direction LR\n            subgraph Testing_Environments\n                direction TB\n                satisfaction_for_input_requirements\n                satisfaction_for_input_requirements---Simulated_Use_Environment\n                subgraph User_Site_Testing\n                    direction TB                            \n                    Installation_Qualification---\n                    Operational_Qualification---\n                    Performance_Qualification\n                end\n            end\n            satisfaction_for_input_requirements---User_Site_Testing\n            subgraph Testing_Activities\n                direction TB\n                static_analyses---\n                dynamic_analyses---\n                code_and_document_inspections---\n                walkthroughs\n            end \n        Testing_Environments--&gt;Testing_Activities\n        end\n    Design_Outputs_of_SW_life_cycle_for_Specified_Requirements--&gt;Software_Testing--&gt;Testing_Activities\nend    \n\n\n\n\n\n\n\nInstallation_Qualification (IQ): documentation of correct installations according to requirements, specifications, vendor’s recommendations, and the FDA’s guidance for all hardware, software, equipment and systems.\nOperational_Qualification (OQ): establishment of confidence that the software shows constant performances according to specified requirements.\nPerformance_Qualification (PQ): confirmation of the performance in the intended use according to the specified requirements for functionality and safety throughout the SW life cycle."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation",
    "text": "Validation\n\n\n\n\nflowchart LR\n    subgraph Validation\n    direction LR\n        subgraph Confirmation_by_Examination\n            direction TB\n            subgraph Examination_List_at_each_stage_of_SW_Life_Cycle\n                direction TB\n                comprehensiveness_of_software_testing---\n                inspection_verification_test---\n                analysis_verification_test---\n                other_varification_tests    \n            end \n        end\n        subgraph Provision_of_objective_3evidences\n            direction TB\n            subgraph Software_specifications_conform_to\n                direction TB\n                user_needs \n                intended_uses\n            end\n            subgraph Consistent_SW_Implementation\n                direction TB\n                particular_requirements\n            end\n            subgraph Correctness_Completeness_Traceability\n                direction TB\n                correct_complete_implementation_by_all_SW_requirements---\n                traceable_to_system_requirements\n            end\n            Software_specifications_conform_to---\n            Consistent_SW_Implementation---\n            Correctness_Completeness_Traceability\n        end\n        Confirmation_by_Examination--&gt;\n        Provision_of_objective_3evidences\n    end\n\n\n\n\n\n\n\n\n\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#benefits-and-difficulty-of-sw-vv",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#benefits-and-difficulty-of-sw-vv",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Benefits and Difficulty of SW V&V",
    "text": "Benefits and Difficulty of SW V&V\n\nBenefits of SW V&V\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nReduce long term costs by making V&V easier and less costly to reliably modify software and revalidate software changes.\n\n\n\nDifficulty in SW V&V\n\na developer cannot test forever, and\n\nit is difficult to know how much evidence is enough.\na matter of developing a level of confidence that the device meets all requirements\n\nConsiderations for an acceptable level of confidence\nmeasures and estimates such as defects found in specifications documents\ntesting coverage, and other techniques are all used before shipping the product.\na level of confidence varies depending upon the safety risk (hazard) of a SW or device"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-development-as-part-of-system-design",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-development-as-part-of-system-design",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Development as Part of System Design",
    "text": "SW Development as Part of System Design\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        purpose_design_review---\n        design_review_types---\n        design_review_requirements---\n        design_review_outputs\n    end\n\n\n\n\n\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nuser’s needs\nintended uses from which the product is developed.\n\nA primary goal of SW validation is to demonstrate that all completed SW products comply with all documented requirements.\n\n\nDesign Review\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        subgraph Purpose_Design_Review\n            direction TB\n            documented_structured_comprehensive_systematic_examinations---\n            adequacy_of_design_requirements---\n            capability_of_design_for_requirements---\n            identification_of_problem   \n        end\n        subgraph Design_Reivew_Types\n            direction TB\n            subgraph Formal_Design_Review\n                direction TB\n                3rd_parties_outside_development_team\n            end\n            subgraph Informal_Design_Review\n                direction TB\n                within_development_team\n            end\n        Formal_Design_Review---Informal_Design_Review    \n        end\n        subgraph Design_Review_Requirements\n            direction TB\n               necessary_at_least_one_formal_design_review---\n               optinal_informal_design_review---\n               recommended_multiple_design_reviews\n        end\n        subgraph Formal_Design_Review_Outputs\n            direction TB\n            more_than_10_outputs\n        end\n        Purpose_Design_Review--&gt; Design_Reivew_Types--&gt; Design_Review_Requirements\n        Design_Review_Requirements--&gt;Formal_Design_Review_Outputs\n    end\n\n\n\n\n\n\n\nDesign review is a primary tool for managing and evaluating development projects.\nAt least one formal design review must be conducted during the device design process.\nIt is recommended that multiple design reviews be conducted.\nProblems found at this point can\n\nbe resolved more easily,\nsave time and money, and\nreduce the likelihood of missing a critical issue."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-principles",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-principles",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation Principles",
    "text": "Validation Principles\n\nOverview\n\n\n\n\nflowchart LR\n  subgraph Validation_Principles\n        direction LR\n        subgraph Validation_Starting_Point\n            direction TB\n            during_design_planning---\n            during_development_planning---\n            all_results_should_be_supported_by_evidence_collected_from_planning_SW_lifecylce\n        end\n        subgraph Validation_Conditions\n            direction TB\n            Requirements---Estabilishment_Confidence---SW_Lifecycle\n        end\n\n        subgraph Validation_Planning\n            direction TB\n            Specify_Areas\n            subgraph Validation_Coverage\n                direction TB\n            end\n            subgraph Validation_Process_Establishment\n                direction TB\n            end\n        Specify_Areas---Validation_Coverage---Validation_Process_Establishment\n        end\n\n        subgraph After_Self_Validation\n            direction TB\n            subgraph Validation_After_SW_Change\n        direction TB\n        end\n\n        subgraph Independence_of_Review\n        direction TB\n\n        end\n        Validation_After_SW_Change---Independence_of_Review\n        end\n            Validation_Starting_Point--&gt;Validation_Conditions--&gt;Validation_Planning--&gt;\nAfter_Self_Validation\n    end\n\n\n\n\n\n\nPreparation for software validation should begin as early as possible because the final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n\nConditions\n\n\n\n\nflowchart LR\n\nsubgraph Validation_Conditions\n    direction LR\n    subgraph SW_Requirments\n        direction TB\n        subgraph Documented_SW_Requirments_Specification\n            direction TB\n            Baseline_Provision_for_V&V---\n            establishment_of_software_requirements_specification\n        end\n    end\n    subgraph Estabilishment_Confidence\n        direction TB\n            mixture_of_methods_techinques---\n            preventing_SW_errors---\n            detecting_SW_errors                 \n    end\n    subgraph SW_Lifecycle\n        direction TB\n        validation_must_be_conducted_within_established_environment_across_lifecycle---\n        lifecycle_contains_SW_engineering_tasks_and_documentation---\n        V&V_tasks_must_reflect_intended_use\n    end\nend\nSW_Requirments---Estabilishment_Confidence---SW_Lifecycle\n\n\n\n\n\n\n\nPlanning\n\n\n\n\nflowchart LR\n    subgraph Validation_Planning\n        direction LR\n        define_what_to_accomplish\n        subgraph Specify_Areas\n            direction TB\n            scope---\n            approach---\n            resources---\n            schedules_activities---\n            types_activitieis---\n            extent_of_activities---\n            tasks---\n            work_items\n        end\n            define_what_to_accomplish--&gt;Specify_Areas\n        subgraph Validation_Coverage\n               direction TB\n            depending_on_SW_complexity_of_SW_design---\n            depending_on_safety_risk_for_specified_intended_use---\n            select_activities_tasks_work_items_for_complexity_safety_risk\n        end\n        subgraph Validation_Process_Establishment\n            direction TB\n            establish_how_to_conduct--&gt;\n            identify_sequence_of_specific_actions--&gt;\n            identify_specific_activitieis--&gt;\n            identify_specific_tasks--&gt;\n            identify_specific_work_items\n        end\n    Specify_Areas--&gt;Validation_Coverage--&gt;Validation_Process_Establishment\n    end\n\n\n\n\n\n\n\nAfter SW Change\n\n\n\n\nflowchart LR\n\nsubgraph After_Self_Validation\n    direction LR\n    subgraph Validation_After_SW_Change\n        direction TB\n        determine_extent_of_change_on_entire_SW_system--&gt;\n        determine_impact_of_change_on_entire_SW_system--&gt;\n        conduct_SW_regression_testing_on_unchanged_but_vulnerable_modules\n    end\n    subgraph Independence_of_Review\n        direction TB\n        follow_basic_quality_assurance_precept_of_independence_of_review---\n        avoid_self_validation---\n        should_conduct_contracted_3rd_party_independent_V&V---\n        or_conduct_blind_test_with_internal_staff\n    end\n    Validation_After_SW_Change---Independence_of_Review\nend\n    \n\n\n\n\n\n\n\nSW Lifecycle\n\n\n\n\nflowchart LR\nsubgraph SW_Lifecycle\n    direction TB\n    validation_must_be_conducted_within_the_established_environment_across_lifecycle---\n    lifecycle_contains_SW_engineering_tasks_and_documentation---\n    V&V_tasks_must_reflect_intended_use\nend\n\nsubgraph SW_Lifecycle_Activities\n    direction TB\n    subgraph should_establish_lifecycle_model\n        direction TB\n        subgraph SW_Lifecycle_Model_List_Defined_in_FDA\n            direction TB\n            waterfall---\n            spiral---\n            rapid_prototyping---\n            incremental_development---\n            etc\n        end     \n    end\n    subgraph should_cover_SW_birth_to_retirement\n        direction TB\n        subgraph Lifecycle_Activities\n            direction TB\n            Quality_Plan--&gt;\n            System_Requirements_Definition--&gt;\n            Detailed_Software_Requirements_Specification--&gt;\n            Software_Design_Specification--&gt;\n            Construction_or_Coding--&gt;\n            Testing--&gt;\n            Installation--&gt;\n            Operation_and_Support--&gt;\n            Maintenance--&gt;\n            Retirement\n        end\n    end\n    should_establish_lifecycle_model--&gt;should_cover_SW_birth_to_retirement\n    should_cover_SW_birth_to_retirement--&gt;Lifecycle_Activities\nend\nsubgraph SW_Lifecycle_Tasks\n    direction TB\n    should_define_and_document_risk_related_tasks---\n    should_define_and_document_which_tasks_are_appropriate_in_vice_versa---\n    Quality_Planning---\n    Quality_Planning_Tasks---\n    Inclusion_Task_List_for_Plan---\n    Identification_Task_List_for_Plan---\n    Configuration_Management---\n    Control---\n    Management---\n    Procedures---\n    ensure_proper_communications_and_documentation---\n    Task_Requirements\nend\nSW_Lifecycle--&gt;SW_Lifecycle_Activities--&gt;SW_Lifecycle_Tasks"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-lifecycle-tasks",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-lifecycle-tasks",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Lifecycle Tasks",
    "text": "SW Lifecycle Tasks\n\nOverview\n\n\n\n\n \nflowchart TB\n\nsubgraph SW_Lifecycle_Tasks\n    direction LR\n    subgraph Define_and_Document_List\n        direction TB\n        risk_related_tasks---\n        whether_or_not_tasks_are_appropriate\n    end\n    \n    subgraph Quality_Planning\n        direction TB\n        subgraph Quality_Planning_Tasks\n            direction TB\n        \n        end\n        subgraph Inclusion_List_for_Plan\n            direction TB\n            \n        end\n        subgraph Identification_List_for_Plan\n            direction TB\n            \n        end\n    Quality_Planning_Tasks--&gt;Inclusion_List_for_Plan--&gt;Identification_List_for_Plan\n    end\n    \n    subgraph Configuration_Management\n        direction TB\n        subgraph Control\n            direction TB\n            \n        end\n        subgraph Management\n            direction TB\n        end\n        subgraph Procedures\n            direction TB\n        end\n        ensure_proper_communications_and_documentation\n        Control--&gt;Management--&gt;Procedures--&gt;ensure_proper_communications_and_documentation \n    end\n    subgraph Task_Requirements\n        direction TB\n        identification---\n        analysis---\n        predetermined_documentation_about_device_its_intended_use---\n        Requirements_Specification_List---\n        Verfification_List_by_Evaluation---\n        Requirements_Tasks    \n    end\nDefine_and_Document_List--&gt;Quality_Planning--&gt;Configuration_Management--&gt;Task_Requirements\nend     \n\n\n\n\n\n\n\nQuality Planning\n\n\n\n\nflowchart TB\nsubgraph Quality_Planning\n    direction LR\n    subgraph Quality_Planning_Tasks\n        direction TB\n        Risk_Hazard_Management_Plan---\n        Configuration_Management_Plan---\n        Software_Quality_Assurance_Plan---\n        Software_Verification_and_Validation_Plan---\n        Verification_and_Validation_Tasks---\n        Acceptance_Criteria---\n        Schedule_and_Resource_Allocation_for_V&V_activities---\n        Reporting_Requirements---\n        Formal_Design_Review_Requirements---\n        Other_Technical_Review_Requirements---\n        Problem_Reporting_and_Resolution_Procedures---\n        Other_Support_Activities\n    end\n    subgraph Inclusion_List_for_Plan\n        direction TB\n        specific_tasks_for_each_life_cycle_activity---\n        Enumeration_of_important_quality_factors--- \n        like_reliability_maintainability_usability---\n        Methods_and_procedures_for_each_task---\n        Task_acceptance_criteria---\n        Criteria_for_defining_and_documenting_outputs_for_input_requirements---\n        Inputs_for_each_task---\n        Outputs_from_each_task---\n        Roles_resources_and_responsibilities_for_each_task---\n        Risks_and_assumptions---\n        Documentation_of_user_needs    \n    end\n    subgraph Identification_List_for_Plan\n        direction TB\n        personnel---\n        facility_and_equipment_resources_for_each_task---\n        role_that_risk_hazard_management        \n    end\nQuality_Planning_Tasks--&gt;Inclusion_List_for_Plan--&gt;Identification_List_for_Plan\nend\n\n\n\n\n\n\n\nConfiguration Management\n\n\n\n\nflowchart LR\nsubgraph Configuration_Management\n    direction LR\n    subgraph Control\n        direction TB\n        control_multiple_parallel_development_activities---\n        ensure_positive_and_correct_correspondence_of---\n        specifications_documents---\n        source_code---\n        object_code---\n        test_suites---\n        ensure_accurate_identification_of_approved_versions---\n        ensure_access_to_approved_versions---\n        create_procedures_for_reporting---\n        create_procedures_for_resolving_SW_anomalies                            \n    end\n    subgraph Management\n        direction TB\n        identify_reports---\n        specify_contents---\n        specify_format---\n        specify_responsible_organizational_elements_for_each_report\n    end\n    subgraph Procedures\n        direction TB\n        necessary_for_review_of_SW_development_results---\n        necessary_for_approval_of_SW_development_results\n    end\n    ensure_proper_communications_and_documentation\n    Control--&gt;Management--&gt;Procedures--&gt;ensure_proper_communications_and_documentation \nend\n\n\n\n\n\n\n\nTask Requirements\n\n\n\n\n\nflowchart TB\n    subgraph Task_Requirements\n        direction LR\n        subgraph group\n            direction TB\n            identification---\n            analysis---\n            predetermined_documentation_about_device_its_intended_use\n        end\n        \n        subgraph Requirements_Specification_List\n            direction TB\n            All_software_system_inputs---\n            All_software_system_outputs---\n            All_functions_that_software_system_will_perform---\n            All_performance_requirements_that_software_will_meet---\n            requirement_example_data_throughput_reliability_timing---\n            definition_of_all_external_and_user_interfaces---\n            any_internal_software_to_system_interfaces---\n            How_users_will_interact_with_system---\n            What_constitutes_error---\n            how_errors_should_be_handled---\n            Required_response_times---\n            Intended_operating_environment_for_software---\n            All_acceptable_ranges_limits_defaults_specific_values---\n            All_safety_related_requirements_that_will_be_implemented_in_SW---\n            All_safety_related_specifications_that_will_be_implemented_in_SW---\n            All_safety_related_features_that_will_be_implemented_in_SW---\n            All_safety_related_functions_that_will_be_implemented_in_SW---\n            clearly_identify_potential_hazards---\n            risk_evaluation_for_accuracy---\n            risk_evaluation_for_completeness---\n            risk_evaluation_for_consistency---\n            risk_evaluation_for_testability---\n            risk_evaluation_for_correctness---\n            risk_evaluation_for_clarity\n        end\n        subgraph Verfification_List_by_Evaluation\n            direction TB\n            no_internal_inconsistencies_among_requirements---\n            All_of_performance_requirements_for_system---\n            Complete_correct_Fault_tolerance_safety_security_requirements---\n            Accurate_Complete_Allocation_of_software_functions---\n            Appropriate_Software_requirements_for_system_hazards---\n            mesurable_requirements---\n            objectively_verifiable_requirements---\n            traceable_requirements\n        end\n        subgraph Requirements_Tasks\n            direction TB\n            Preliminary_Risk_Analysis---\n            Traceability_Analysis---\n            ex_Software_Requirements_to_System_Requirements_vice_versa---\n            ex_Software_Requirements_to_Risk_Analysis---\n            Description_of_User_Characteristics---\n            Listing_of_Characteristics_and_Limitations_of_Memory---\n            Software_Requirements_Evaluation---\n            Software_User_Interface_Requirements_Analysis---\n            System_Test_Plan_Generation---\n            Acceptance_Test_Plan_Generation---\n            Ambiguity_Review_or_Analysis\n        end\n    group--&gt;Requirements_Specification_List \n    Verfification_List_by_Evaluation--&gt;Requirements_Tasks\n    end\n\n\n\n\n\n\n\nDesign Overview\n\n\n\n\nflowchart TB\n    subgraph Deign_Task\n        direction LR\n    subgraph Design_Consideration_List\n        direction TB\n        subgraph Description\n                    direction TB\n                end\n        subgraph Human_Factors_Engineering\n          direction TB\n    \n        end\n        subgraph Safety_Usability_Issues_Conisderation\n            direction TB\n\n            end\n        Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n    subgraph Design_Specificiation\n        direction TB\n        subgraph Performing_List\n            direction TB\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n        end\n    Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design \n    end\n    subgraph Design_Activity_and_Task_List\n        direction TB\n        subgraph Final_Design_activity\n            direction TB\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n            end\n            subgraph Coding_Tasks\n                direction TB\n            end\n        traceability_analysis--&gt;Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end\n    Design_Consideration_List---Design_Specificiation---Design_Activity_and_Task_List\n\n    end\n\n\n\n\n\n\nDesign Consideration\n\n\n\n\nflowchart TB\nsubgraph Design_Consideration_List\n    direction LR\n        subgraph Requirement_Specification\n            direction TB\n            logical_representation---\n            physical_representation\n        end\n    subgraph Description\n            direction TB\n            what_to_do---\n            how_to_do                   \n        end\n    subgraph Human_Factors_Engineering\n      direction TB\n            entire_design_and_development_process---\n            device_design_requirements---\n            analyses---\n            tests\n    end\n    subgraph Safety_Usability_Issues_Conisderation\n        direction TB\n                flowcharts--- \n                state_diagrams--- \n                prototyping_tools---\n                test_plans\n        end\n        Requirement_Specification---Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n\n\n\n\n\n\n\nDesign Specification\n\n\n\n\nflowchart TB\nsubgraph Design_Specificiation\n        direction LR\n        subgraph Conceptual_Specification\n            direction TB\n            requirements_specification---\n            predetermined_criteria---\n            Software_risk_analysis---\n            Development_procedures---\n            coding_guidelines\n        end\n        subgraph Performing_List\n            direction TB\n            task---\n            function_analyses---\n            risk_analyses---\n            prototype_tests_and_reviews---\n            full_usability_tests\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n            SW_requirements_specification---\n            predetermined_criteria_for_SW_acceptance---\n            SW_risk_analysis---\n            Development_procedure_list---\n            coding_guidance---\n            Systems_documentation---\n            Hardware_to_be_used---\n            Parameters_to_be_measured---\n            Logical_structure---\n            Control_logic---\n            logical_processing_steps_aka_algorithms---\n            Data_structures_diagram---\n            data_flow_diagrams---\n            Definitions_of_variables---\n            description_of_where_they_are_used---\n            Error_alarm_and_warning_messages---\n            Supporting_software---\n            internal_modules_Communication_links---\n            supporting_sw_links---\n            link_with_hardware---\n            link_with_user---\n            physical_Security_measures---\n            logical_security_measures\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n            complete--- \n            correct---\n            consistent--- \n            unambiguous--- \n            feasible---\n            maintainable---\n            analyses_of_control_flow---\n            data_flow--- \n            complexity--- \n            timing--- \n            sizing--- \n            memory_allocation---\n            module_architecture---\n            traceability_analysis_of_modules--- \n            criticality_analysis\n        end\n    Conceptual_Specification---Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design  \n    end\n\n\n\n\n\n\n\nDesign Activity and Task\n\n\n\n\n\nflowchart TB\nsubgraph Design_Activity_and_Task_List\n        direction LR\n        subgraph Final_Design_activity\n            direction TB\n            Formal_Design_Review_Before_Design_Implementation---\n            correct_consistent_complete_accurate_testable\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n            Updated_Software_Risk_Analysis---\n            Traceability_Analysis---\n            Software_Design_Evaluation---\n            Design_Communication_Link_Analysis---\n            Module_Test_Plan_Generation---\n            Integration_Test_Plan_Generation---\n            module_Test_Design_Generation---\n            integration_Test_Design_Generation---\n            system_Test_Design_Generation---\n            acceptance_Test_Design_Generation   \n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n                each_element_implementation---\n                each_module_implementation_to_element_and_risk_analysis---\n                each_functions_implemented_to_element_and_risk_analysis---\n                Tests_for_modules_to_element_and_risk_analysis--- \n                Tests_for_functions_to_element_and_risk_analysis---\n                Tests_for_modules_to_source_code---\n                Tests_for_functions_to_source_code\n            end\n            subgraph Coding_Tasks\n                direction TB\n                Traceability_Analyses---\n                Source_Code_to_Design_Specification_and_vice_versa---\n                Test_Cases_to_Source_Code_and_to_Design_Specification---\n                Source_Code_and_Source_Code_Documentation_Evaluation---\n                Source_Code_Interface_Analysis---\n                Test_Procedure_and_Test_Case_Generation \n            end\n        traceability_analysis--&gt;Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#testing-task",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#testing-task",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Testing Task",
    "text": "Testing Task\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction TB\n            subgraph Test_Plans\n                direction TB\n            end\n            subgraph Conditions\n                direction TB\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n        subgraph Code_Based_Testing\n            direction TB\n            subgraph white_box_testing\n                direction TB\n            end\n            subgraph Evaluation_of_level_of_white_box_testing\n                direction TB\n            end\n            subgraph Coverage_Metrics_of_White_Box_Testing\n                direction TB\n            end\n        white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n        end\n        subgraph Alternatives_to_White_Box_Testing\n            direction TB\n            subgraph Types_of_Functional_Software_Testing_Increasing_Cost\n                direction TB\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n            end\n            subgraph Change_in_SW\n                direction TB    \n            end\n        Types_of_Functional_Software_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW\n        end\n        \n\n        subgraph Development_Testing\n            direction TB\n            subgraph unit_level_testing\n                direction TB    \n            end\n            subgraph integration_level_testing\n                direction TB\n            end\n            subgraph system_level_testing\n                direction TB\n            end\n            subgraph Error_Detected\n                direction TB        \n            end\n        unit_level_testing--&gt;integration_level_testing--&gt;system_level_testing--&gt;Error_Detected\n        end\n\n        subgraph Testing_Tasks\n            direction TB\n        end\n        subgraph User_Site_Testing\n            direction TB\n            subgraph Quality_System_Rregulation\n                direction TB\n            end\n            subgraph Understand_Terminology\n                direction TB\n            end\n            subgraph Testing\n                direction TB\n            end\n            Quality_System_Rregulation---Understand_Terminology---Testing\n        end\nConsideration_Before_Testing_Tasks---Code_Based_Testing---Alternatives_to_White_Box_Testing\nDevelopment_Testing---Testing_Tasks---User_Site_Testing\n    end\n\n\n\n\n\n\n\nConsideration Before Testing Tasks\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction LR\n            subgraph Test_Plans\n                direction TB\n                should_identify_control_measures_like_traceability_analysis---\n                ensure_that_intended_coverage_is_achieved---\n                ensure_that_proper_documentation_is_prepared---\n                conduct_tests_not_by_SW_developers_but_in_other_sites\n            end\n            subgraph Conditions\n                direction TB\n                use_defined_inputs---\n                documented_outcomes---\n                gonnabe_time_consuming_activity---\n                gonnabe_difficult_activity---\n                gonnabe_imperfect_activity---\n                testing_all_program_functionality---\n                does_not_mean_100_prcnt_correction_perfection---\n                make_detailed_objective_evaluation---\n                requires_sophisticated_definition_specificiation---\n                all_test_procedures_data_results_are_documented---\n                all_test_procedures_data_results_are_suitable_for_review---\n                all_test_procedures_data_results_are_suitable_for_objective_decision_making---\n                all_test_procedures_data_results_are_suitable_for_subsequent_regression_testing\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n                make_test_plans---\n                make_test_cases---\n                plan_schedules---\n                plan_environments---\n                plan_resources_of_personnel_tools---\n                plan_methodologies---\n                plan_inputs_procedures_outputs_expected_results---\n                plan_documentation---\n                plan_reporting_criteria\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n                expected_test_outcome_is_predefined---\n                good_test_case_has_high_probability_of_exposing_errors---\n                successful_test_is_one_that_finds_errors---\n                There_is_independence_from_coding---\n                Both_application_for_user_and_SW_for_programming_expertise_are_employed---\n                Testers_use_different_tools_from_coders---\n                Examining_only_the_usual_case_is_insufficient---\n                Test_documentation_permits_its_reuse---\n                Test_documentation_permits_independent_confirmation_---\n                of_pass/fail_test_outcome_during_subsequent_review\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n\nend\n\n\n\n\n\n\n\nCode Based Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n            subgraph Code_Based_Testing\n                direction LR\n                subgraph white_box_testing\n                    direction TB\n                    identify_dead_code_never_executed---\n                    conduct_unit_test---\n                    conduct_other_level_tests\n                end\n                subgraph Evaluation_of_level_of_white_box_testing\n                    direction TB\n                    use_coverage_metrics---\n                    metrics_of_completeness_of_test_selection_criteria---\n                    coverage_should_be_commensurate_with_level_of_SW_risk---\n                    coverage_means_100_prcnt_coverage\n                end\n                subgraph Coverage_Metrics_of_White_Box_Testing\n                    direction TB\n                    Statement_Coverage---\n                    Decision_or_Branch_Coverage---\n                    Condition_Coverage---\n                    Multi_Condition_Coverage\n                    Loop_Coverage---\n                    Path_Coverage---\n                    Data_Flow_Coverage\n                end\n            white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n            end\nend\n\n\n\n\n\n\n\nSolution to White Box Testing\n\n\n\n\n\n \nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Alternatives_to_White_Box_Testing\n            direction LR\n            subgraph Types_of_Testing_Increasing_Cost\n                direction TB\n                    Normal_Case---\n                    Output_Forcing---\n                    Robustness---\n                    Combinations_of_Inputs\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n                difficulty_in_linking_---\n                tests_completion_criteria_to_SW_reliability\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n                statistical_testing---\n                provide_further_assurance_of_reliability---\n                generate_randomly_test_data_from_defined_distributions---\n                distribution_defined_by_expected_use---\n                distribution_defined_by_hazardous_use---\n                distribution_defined_by_malicious_use---\n                large_test_data_cover_particular_areas_or_concerns---\n                statistical_testing_provides_high_structural_coverage---\n                statistical_testing_requires_stable_system---\n                structural_and_functional_testing_are_prerequisites_for_statistical_testing\n            end\n            subgraph Change_in_SW\n                direction TB\n                conduct_regression_analysis_and_testing---\n                should_demonstrate_correct_implementation---\n                should_demonstrate_no_adverse_impact_on_other_modules   \n            end\n            subgraph Testing_Tasks\n                direction TB\n                Test_Planning---\n                Structural_Test_Case_Identification---\n                Functional_Test_Case_Identification---\n                Traceability_Analysis_Testing---\n                Unit_Tests_to_Detailed_Design---\n                Integration_Tests_to_High_Level_Design---\n                System_Tests_to_Software_Requirements---\n                Unit_Test_Execution---\n                Integration_Test_Execution---\n                Functional_Test_Execution---\n                System_Test_Execution---\n                Acceptance_Test_Execution---\n                Test_Results_Evaluation---\n                Error_Evaluation_Resolution---\n                Final_Test_Report\n            end\n        Types_of_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW---Testing_Tasks\n        end\nend\n\n\n\n\n\n\n\nDevelopment Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Development_Testing\n            direction LR\n            subgraph unit_level_testing\n                direction TB    \n                focus_on_early_examination_of_sub_program_functionality---\n                ensure_functionality_invisible_at_system_level_examined---\n                ensure_quality_software_units_furnished_for_integration\n            end\n            subgraph integration_level_testing\n                direction TB\n                focuses_on_transfer_of_data---\n                focuses_on_control_across_program's_internal_and_external_interfaces\n            end\n            subgraph system_level_testing\n                direction TB\n                demonstrate_all_specified_functionality_exists---\n                demonstrate_SW_is_trustworthy---\n                verifies_as_built_program's_functionality_and_performance_on_requirements---\n                addresses_functional_concerns_and_intended_uses---\n                like_Performance_issues---\n                like_Responses_to_stress_conditions---\n                like_Operation_of_internal_and_external_security_features---\n                like_Effectiveness_of_recovery_procedures---\n                like_disaster_recovery---\n                like_Usability---\n                like_Compatibility_with_other_SW---\n                like_Behavior_in_each_of_the_defined_hardware_configurations---\n                like_Accuracy_of_documentation\n            end\n            subgraph Error_Detected\n                direction TB        \n                should_be_logged---\n                should_be_classified---\n                should_be_reviewed---\n                should_be_resolved_before_SW_release\n            end\n        unit_level_testing--&gt;integration_level_testing--&gt;system_level_testing--&gt;Error_Detected\n        end\nend\n\n\n\n\n\n\n\nUser Site Testing\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph User_Site_Testing\n            direction LR\n            subgraph Quality_System_Rregulation\n                direction TB\n                installation---\n                inspection_procedures---\n                testing_appropriateness---\n                documentation_of_inspection---\n                testing_to_demonstrate_proper_installation\n            end\n            subgraph Understand_Terminology\n                direction TB\n                beta_test---\n                site_validation---\n                user_acceptance_test---\n                installation_verification---\n                installation_testing\n            end\n            subgraph Testing\n                direction TB\n                subgraph Requirements\n                    direction TB\n                    either_actual_or_simulated_use---\n                    verification_of_intended_functionality---\n                    constant_contact_FDA_center\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n    \n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_System_Ability\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_User_Ability\n                        direction TB\n        \n                    end \n                    subgraph Evaluation_of_Operator_Ability\n                        direction TB\n        \n                    end\n                constant_contact_FDA_center--&gt;Follow_Predefiened_Plan--&gt;Documented_Evidence--&gt;\n            Evaluation_of_System_Ability--&gt;Evaluation_of_User_Ability--&gt;Evaluation_of_Operator_Ability    \n                end \n                        \n            \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements--&gt;User_Site_Testing_Task\n        end\n        Quality_System_Rregulation--&gt;    Understand_Terminology--&gt;Testing--&gt;User_Site_Testing_Task\n        end\nend\n\n\n\n\n\n\n\nTesting\n\n\n\n\nflowchart TB\n            subgraph Testing\n                direction LR\n                subgraph Requirements\n                    direction LR\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n                        either_actual_or_simulated_use---\n                        verification_of_intended_functionality---\n                        constant_contact_FDA_center---\n                        formal_summary_of_testing---\n                        record_of_formal_acceptance\n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n                        testing_plan_of_full_range_of_operating_conditions---\n                        testing_plan_to_detect_any_latent_faults---\n                        all_testing_procedures---\n                        test_input_data---\n                        test_results---\n                        hardware_installation_and_configuration---\n                        software_installation_and_configuration---\n                        exercising_measure_of_all_system_components---\n                        versions_of_all_system_components           \n                    end\n                    subgraph Evaluation\n                        direction TB\n                      subgraph Evaluation_of_System_Ability\n                            direction TB\n                            high_volume_of_data---\n                            heavy_loads_or_stresses---\n                            security\n                            subgraph fault_testing\n                                direction TB\n                                avoidance---\n                                detection---\n                                tolerance---\n                                recovery\n                            end\n                        security---fault_testing---\n                        error_message---\n                        implementation_of_safety_requirements\n                        end\n                      subgraph Evaluation_of_User_Ability\n                            direction TB\n                            ability_to_understand_system---\n                            ability_to_interface_with_system\n                        end \n                        subgraph Evaluation_of_Operator_Ability\n                            direction TB\n                            ability_to_perform_intended_functions---\n                            ability_to_respond_in_alarms---\n                            ability_to_respond_in_warnings---\n                            ability_to_respond_in_error_messages\n                        end\n\n                    end\n            Follow_Predefiened_Plan--&gt;Documented_Evidence--&gt;\n            Evaluation_of_System_Ability--&gt;Evaluation_of_User_Ability--&gt;Evaluation_of_Operator_Ability    \n            end     \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements--&gt;User_Site_Testing_Task\n        end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#maintenance-and-software-changes",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#maintenance-and-software-changes",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Maintenance and Software Changes",
    "text": "Maintenance and Software Changes\n\n\n\n\nflowchart LR\n    subgraph Hardware_VS_Software\n        direction LR\n        subgraph HW_maintenance_Inclusion\n            direction TB\n            preventive_hardware_maintenance_actions--- \n            component_replacement---\n            corrective_changes\n        end\n        subgraph SW_maintenance_Inclusion\n            direction TB\n            corrective---\n            perfective---\n            adaptive_maintenance---\n            not_include_preventive_maintenance_actions---\n            not_include_software_component_replacement\n        end\n    end\n    subgraph Maintenance_Type\n        direction TB\n        Corrective_maintenance---\n        Perfective_maintenance---\n        Adaptive_maintenance---\n        Sufficient_regression_analysis---\n        Sufficient_regression_testing\n    end\n    subgraph Factors_of_Validation_for_SW_change\n        direction TB\n        type_of_change---\n        development_products_affected---\n        impact_of_those_products_on_operation\n    end\n    subgraph Factors_of_Limitting_Validation_Effort\n        direction TB\n        documentation_of_design_structure---\n        documentation_of_interrelationships_of_modules---\n        documentation_of_interrelationships_of_interfaces---\n        test_documentation---\n    test_cases---\n        results_of_previous_verification_and_validation_testing\n    end\n    subgraph Maintenance_tasks\n        direction TB\n        Software_Validation_Plan_Revision---\n        Anomaly_Evaluation---\n        Problem_Identification_and_Resolution_Tracking---\n        Proposed_Change_Assessment---\n        Task_Iteration---\n        Documentation_Updating\n    end\nHardware_VS_Software--&gt;Maintenance_Type--&gt;Factors_of_Validation_for_SW_change--&gt;\nFactors_of_Limitting_Validation_Effort--&gt;Maintenance_tasks"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-of-quality-system-software",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-of-quality-system-software",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation of Quality System Software",
    "text": "Validation of Quality System Software\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Use_of_Computers_and_automated_equipment\n        direction TB\n        medical_device_design---\n        laboratory_testing_and_analysis---\n        product_inspection_and_acceptance---\n        production_and_process_control---\n        environmental_controls---\n        packaging---\n        labeling---\n        traceability---\n        document_control---\n        complaint_management---\n        programmable_logic_controllers---\n        digital_function_controllers---\n        statistical_process_control---\n        supervisory_control_and_data_acquisition---\n        robotics---\n        human_machine_interfaces---\n        input_output_devices---\n        computer_operating_systems\n    end\n    subgraph Factors_in_Validation\n        direction TB\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System--&gt;Validation_Supporting_Factors--&gt;Factors_of_Validation_Evidence_Level--&gt;\nFactors_of_Easing_Validation_Effort\n    end\n    subgraph Documented_User_Requirements\n        direction TB\n        intended_use_of_software_or_automated_equipment---\n      level_of_dependency_on_software_or_equipment\n    end\n    subgraph List_That_Must_Be_Defined_by_User\n        direction TB\n        \n    end\n    subgraph Documentation_List\n        direction TB\n        documented_protocol---\n        documented_validation_results\n        subgraph Documented_Test_Cases\n            direction TB\n        \n        end\n        documented_validation_results---Documented_Test_Cases\n    end\n\n    subgraph Manufaturer's_Responsbility\n        direction TB\n        \n    end\nUse_of_Computers_and_automated_equipment---Factors_in_Validation---Documented_User_Requirements---\nList_That_Must_Be_Defined_by_User---Documentation_List---Manufaturer's_Responsbility\n\n\n\n\n\n\n\nFactors in Validation\n\n\n\n\nflowchart LR\n    subgraph Factors_in_Validation\n        direction LR\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n                electronic_records_regulation---\n                electronic_signatures_regulation---\n                regulations_establishment---\n                security---\n                data_integrity---\n                validation_requirements \n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n            verifications_of_outputs_from_each_stage--- \n            verifications_of_outputs_throught_SW_life_cycle---\n            checking_for_proper_operation_in_intended_use_environment\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n            risk_posed_by_automated_operation---\n            complexity_of_process_software---\n            degree_of_dependence_on_automated_process\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System--&gt;Validation_Supporting_Factors--&gt;Factors_of_Validation_Evidence_Level--&gt;\nFactors_of_Easing_Validation_Effort\n    end"
  },
  {
    "objectID": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html",
    "href": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html",
    "title": "Categorical Data Analysis",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nVisualization Methods\n\nfor EDA: visualize patterns, trends, anomalies in data\nfor model diagnostic methods: visualize to assess violations of assumptions\nfor summary methods: visualize to provide an interpretable summary of data\n\napply theory to practice\n\nconert research questions into statistical hypotheses and models\nlook into the difference between non-parametric (ex. fisher exact test) vs parametric (ex. \\(\\chi^2 test for independence\\)) vs model-based methods (ex. logistic regression)\nfor summary methods: visualize to provide an interpretable summary of data\n\n\n\n\n\n\ncategorical (or frequency) data consist of a discrete set of categories, which may be ordered or unordered.\n\nunordered\n\ngener: {male, female, transgender}\nmarital status: {never married, married, separated, divorced, widowed}\nparty preference: {NDP, liberal, conservative, green}\ntreatment improvement: {none, some, marked}\n\nordered\n\nage group: {0s,10s,20s,30s, …}\nnumber of children: {0, 1 , 2 ,3, …} ## Structures\n\n\n\nCategorical data appears in various forms like:\n\ntables\n\none way\ntwo way\nthree way\n\nmatrices\narray\ndata frames\n\ncase form\nfrequency form"
  },
  {
    "objectID": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html#goal",
    "href": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html#goal",
    "title": "Categorical Data Analysis",
    "section": "",
    "text": "Visualization Methods\n\nfor EDA: visualize patterns, trends, anomalies in data\nfor model diagnostic methods: visualize to assess violations of assumptions\nfor summary methods: visualize to provide an interpretable summary of data\n\napply theory to practice\n\nconert research questions into statistical hypotheses and models\nlook into the difference between non-parametric (ex. fisher exact test) vs parametric (ex. \\(\\chi^2 test for independence\\)) vs model-based methods (ex. logistic regression)\nfor summary methods: visualize to provide an interpretable summary of data"
  },
  {
    "objectID": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html#definition-of-categorical-data",
    "href": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html#definition-of-categorical-data",
    "title": "Categorical Data Analysis",
    "section": "",
    "text": "categorical (or frequency) data consist of a discrete set of categories, which may be ordered or unordered.\n\nunordered\n\ngener: {male, female, transgender}\nmarital status: {never married, married, separated, divorced, widowed}\nparty preference: {NDP, liberal, conservative, green}\ntreatment improvement: {none, some, marked}\n\nordered\n\nage group: {0s,10s,20s,30s, …}\nnumber of children: {0, 1 , 2 ,3, …} ## Structures\n\n\n\nCategorical data appears in various forms like:\n\ntables\n\none way\ntwo way\nthree way\n\nmatrices\narray\ndata frames\n\ncase form\nfrequency form"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-03-21_mixed_model/index.html",
    "href": "docs/blog/posts/statistics/2023-03-21_mixed_model/index.html",
    "title": "template",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe radius of the circle is 10.\n\n\n\n1 Go to Project Content List\nProject Content List\n\n\n2 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html",
    "title": "Poisson Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\nDefinition 1 모수 (parameter)가 단위 시간 또는 공간 당 평균 발생 횟수 \\(\\lambda\\) 일 때 주어진 단위 시간 또는 공간 내에 발생하는 사건의 횟수를 확률 변수 \\(X\\) 로 하는 분포를 Poisson Distribution이라 한다. \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;\\lambda)&=\\frac{e^{-\\lambda}\\lambda^{x}}{x!} &(x=0,1,2, ..)\n\\end{aligned}\n\\] (\\(\\lambda\\) 는 단위 시간 또는 공간 당 평균 발생 횟수) 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=0}^{\\infty}xf(x)\\\\\n             &=\\sum_{x=0}^{\\infty}x\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n             &=\\sum_{x=1}^{\\infty}x\\frac{e^{-\\lambda}\\lambda^{x}}{x!} &\\because x=0 \\rightarrow \\text{equation}=0\\\\\n             &=\\sum_{x=1}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x}}{(x-1)!}\\\\\n             &=\\lambda\\sum_{x-1=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x-1}}{(x-1)!}\\\\\n             &=\\lambda\\sum_{y=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{y}}{(y)!} &\\because x-1=y \\text{  }(y=0,1,2, ...)\\\\\n             &=\\lambda\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n    \\text{E}(X(X-1))&=\\sum_{x=0}^{\\infty}x(x-1)f(x)\\\\\n             &=\\sum_{x=0}^{\\infty}x(x-1)\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n             &=\\lambda^2\\sum_{x=2}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x-2}}{(x-2)!}\\\\\n             &=\\lambda^2\\sum_{y=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{y}}{(y)!} &\\because x-2=y \\text{  }(y=0,1,2, ...)\\\\\n             &=\\lambda^2\\\\\n    \\text{E}(X^2)&=\\lambda^2+\\lambda\\\\\n    \\text{Var}(X)&=\\lambda^2+\\lambda-\\lambda^2=\\lambda\\\\\n\\end{aligned}\n\\]\n\n\n\n어느 의료 장비 제조 업체의 의료 장비 불량률이 2% 라고 가정했을 때 임의로 100대의 의료 장비를 구매하여 제조 업체의 Quality Control (QC) guide line을 따라 Quality Control (QC)를 진행 했을 때 불량품이 하나도 발생하지 않을 확률은 다음과 같다.\n\\[\n\\begin{aligned}\n  \\lambda &= 100*0.02=2\\\\\n  f(x)&=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}=\\frac{2^{x}e^{-2}}{x!}\\\\\n  f(0)&=\\frac{2^{0}e^{-2}}{0!}=e^{-2}\n\\end{aligned}\n\\]\n\n\nDefinition 2 \\(X\\sim B(n,p)\\) 일 때 \\(p\\) 가 충분히 작고 \\(n \\rightarrow \\infty\\) 고 \\(np=\\lambda\\) 한다면 \\(x=0,1,2, ...\\) 에 대하여\n\\[\n\\begin{aligned}\n  \\lim_{n \\to \\infty}\\binom{n}{x}p^{x}(1-p)^{n-x}=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}\n\\end{aligned}\n\\] (\\(\\lambda\\) 는 단위 시간 또는 공간 당 평균 발생 횟수) 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html#poisson-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html#poisson-distribution",
    "title": "Poisson Distribution",
    "section": "",
    "text": "Definition 1 모수 (parameter)가 단위 시간 또는 공간 당 평균 발생 횟수 \\(\\lambda\\) 일 때 주어진 단위 시간 또는 공간 내에 발생하는 사건의 횟수를 확률 변수 \\(X\\) 로 하는 분포를 Poisson Distribution이라 한다. \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;\\lambda)&=\\frac{e^{-\\lambda}\\lambda^{x}}{x!} &(x=0,1,2, ..)\n\\end{aligned}\n\\] (\\(\\lambda\\) 는 단위 시간 또는 공간 당 평균 발생 횟수) 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=0}^{\\infty}xf(x)\\\\\n             &=\\sum_{x=0}^{\\infty}x\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n             &=\\sum_{x=1}^{\\infty}x\\frac{e^{-\\lambda}\\lambda^{x}}{x!} &\\because x=0 \\rightarrow \\text{equation}=0\\\\\n             &=\\sum_{x=1}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x}}{(x-1)!}\\\\\n             &=\\lambda\\sum_{x-1=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x-1}}{(x-1)!}\\\\\n             &=\\lambda\\sum_{y=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{y}}{(y)!} &\\because x-1=y \\text{  }(y=0,1,2, ...)\\\\\n             &=\\lambda\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n    \\text{E}(X(X-1))&=\\sum_{x=0}^{\\infty}x(x-1)f(x)\\\\\n             &=\\sum_{x=0}^{\\infty}x(x-1)\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n             &=\\lambda^2\\sum_{x=2}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x-2}}{(x-2)!}\\\\\n             &=\\lambda^2\\sum_{y=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{y}}{(y)!} &\\because x-2=y \\text{  }(y=0,1,2, ...)\\\\\n             &=\\lambda^2\\\\\n    \\text{E}(X^2)&=\\lambda^2+\\lambda\\\\\n    \\text{Var}(X)&=\\lambda^2+\\lambda-\\lambda^2=\\lambda\\\\\n\\end{aligned}\n\\]\n\n\n\n어느 의료 장비 제조 업체의 의료 장비 불량률이 2% 라고 가정했을 때 임의로 100대의 의료 장비를 구매하여 제조 업체의 Quality Control (QC) guide line을 따라 Quality Control (QC)를 진행 했을 때 불량품이 하나도 발생하지 않을 확률은 다음과 같다.\n\\[\n\\begin{aligned}\n  \\lambda &= 100*0.02=2\\\\\n  f(x)&=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}=\\frac{2^{x}e^{-2}}{x!}\\\\\n  f(0)&=\\frac{2^{0}e^{-2}}{0!}=e^{-2}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html#blog-guide-map-link",
    "title": "Poisson Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html",
    "title": "Binomial Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 성공 확률이 p인 bernoulli distribution을 n 번 시행했을 때 성공횟수를 확률 변수 X로 갖는 probability distribution을 binomial distribution이라 한다. probability mass function은\n즉, \\[\n\\begin{aligned}\n  f_X(x;n,p)&=\\binom{n}{x}p^{x}(1-p)^{n-x} \\text{ } (y=0,1,2, ..., n)\\\\\n\\end{aligned}\n\\] 이고 Notation은 보통 \\(X \\sim Bin(n,p) \\text{ or } X \\sim B(n,p) \\text{ or } X \\sim Binomial(n,p)\\) 와 같이 쓰인다 (binomial distribution은 bernoulli distribution을 전제로 한다).\n\n\n\n\\[\n\\begin{aligned}\n    \\text{Let } &I_i \\text{ be } 1\\{x_i=1\\} \\\\\n    X&=\\sum_{i=1}^{n}I_i=I_1+I_2+ ... +I_n\\\\\n  \\text{E}(X)&=\\text{E}(\\sum_{i=1}^{n}I_i)\\\\\n             &=\\text{E}(I_1+I_2+ ... +I_n)\\\\\n             &=\\text{E}(I_1)+\\text{E}(I_2)+ ... +\\text{E}(I_n)\\\\\n             &=p+p+...+p\\\\\n             &=np\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{Var}(X)&=\\text{Var}(I_1+I_2+ ... +I_n)\\\\\n             &=\\text{Var}(I_1)+\\text{Var}(I_2)+ ... +\\text{Var}(I_n)\\\\\n             &=p(1-p)+p(1-p)+...+p(1-p)\\\\\n             &=np(1-p)\n\\end{aligned}\n\\]\n\n\n\n쌍란이 나올 확률이 0.05라고 가정했을 때 Super Market에서 1 pack of 12 eggs을 구매했을 때\n\n3개의 eggs에서 쌍란이 나올 확률은 \\[\n\\begin{aligned}\nX&\\sim Bin(12,0.05)\\\\\nf(X=3)&=\\binom{12}{3}0.05^3 0.95^9\n\\end{aligned}\n\\] 이다.\n적어도 3개의 eggs에서 쌍란이 나올 확률은 \\[\n\\begin{aligned}\nP(X\\ge3)&=1-F_X(3)\\\\\n         &=1-(f(X=3)+f(X=2)+f(X=1)+f(X=0))\\\\\n         &=1-(\\binom{12}{3}0.05^3 0.95^9+\\binom{12}{2}0.05^2 0.95^{10}+\\\\\n         &\\binom{12}{1}0.05^1 0.95^{11}+\\binom{12}{0}0.05^0 0.95^{12})\n\\end{aligned}\n\\] 이다.\n\n다른 예시로는, 분자 진단 시장에서 golden standard라고 평가받는 PCR (Polynomial Chain Reaction)에 사용되는 medical device가 2000 대 중 5대 꼴로 기계적 결함이 발견된다고 가정할 때, 1년에 평균 100대의 분잔 진단 장비를 공급받는 구매자 입장에서 장비의 결함이 발생할 연간 평균과 분산의 추정은 다음과 같다.\n\\[\n\\begin{aligned}\n    X&\\sim Bin(100,\\frac{5}{2000})\\\\\n    f(X=x)&=\\binom{100}{x}\\frac{5}{2000}^x (1-\\frac{5}{2000})^{100-x}\\\\\n    \\text{E}(X)&=100(\\frac{5}{2000})\\\\\n    \\text{Var}(X)&=100(\\frac{5}{2000})(1-\\frac{5}{2000})\n\\end{aligned}\n\\]\n\n\n\n\n\nDefinition 2 n번의 독립 시행에서 각 각 p_1, p_2, …, p_n 의 성공 확률로 E_1, E_2, …, E_n 중 어느 하나를 발생시킬 때 각 event E_i에 대응되는 발생 횟수를 확률 변수 X_1, X_2, …, X_n 로 갖는 joint probability mass function은\n\\[\n\\begin{aligned}\n  f_X(\\mathbf X = x_1,x_2, ...,x_n)&=\\binom{n}{x_1,x_2, ..., x_n}p_1^{x_1}(p_2)^{x_2}\\dots (p_n)^{x_n} \\\\\n\\end{aligned}\n\\] 이다. (단, \\(\\sum_{i=1}^{n}x_i=n, \\sum_{i=1}^{n}p_i=1\\))\n\n\n\n주사위를 5 번 던질 때 1 또는 6의 눈이 1번, 3, 4 또는 5의 눈이 2번 , 2의 눈이 2번 나올 확률은\n\\[\n\\begin{aligned}\n    &x_1= 1, x_2=2, x_3=2\\\\\n    f(X=(1,2,2))&=\\binom{5}{1,2,3}\\frac{1}{3}^1\\frac{1}{2}^2\\frac{1}{6}^2\n\\end{aligned}\n\\] 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html#binomial-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html#binomial-distribution",
    "title": "Binomial Distribution",
    "section": "",
    "text": "Definition 1 성공 확률이 p인 bernoulli distribution을 n 번 시행했을 때 성공횟수를 확률 변수 X로 갖는 probability distribution을 binomial distribution이라 한다. probability mass function은\n즉, \\[\n\\begin{aligned}\n  f_X(x;n,p)&=\\binom{n}{x}p^{x}(1-p)^{n-x} \\text{ } (y=0,1,2, ..., n)\\\\\n\\end{aligned}\n\\] 이고 Notation은 보통 \\(X \\sim Bin(n,p) \\text{ or } X \\sim B(n,p) \\text{ or } X \\sim Binomial(n,p)\\) 와 같이 쓰인다 (binomial distribution은 bernoulli distribution을 전제로 한다).\n\n\n\n\\[\n\\begin{aligned}\n    \\text{Let } &I_i \\text{ be } 1\\{x_i=1\\} \\\\\n    X&=\\sum_{i=1}^{n}I_i=I_1+I_2+ ... +I_n\\\\\n  \\text{E}(X)&=\\text{E}(\\sum_{i=1}^{n}I_i)\\\\\n             &=\\text{E}(I_1+I_2+ ... +I_n)\\\\\n             &=\\text{E}(I_1)+\\text{E}(I_2)+ ... +\\text{E}(I_n)\\\\\n             &=p+p+...+p\\\\\n             &=np\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{Var}(X)&=\\text{Var}(I_1+I_2+ ... +I_n)\\\\\n             &=\\text{Var}(I_1)+\\text{Var}(I_2)+ ... +\\text{Var}(I_n)\\\\\n             &=p(1-p)+p(1-p)+...+p(1-p)\\\\\n             &=np(1-p)\n\\end{aligned}\n\\]\n\n\n\n쌍란이 나올 확률이 0.05라고 가정했을 때 Super Market에서 1 pack of 12 eggs을 구매했을 때\n\n3개의 eggs에서 쌍란이 나올 확률은 \\[\n\\begin{aligned}\nX&\\sim Bin(12,0.05)\\\\\nf(X=3)&=\\binom{12}{3}0.05^3 0.95^9\n\\end{aligned}\n\\] 이다.\n적어도 3개의 eggs에서 쌍란이 나올 확률은 \\[\n\\begin{aligned}\nP(X\\ge3)&=1-F_X(3)\\\\\n         &=1-(f(X=3)+f(X=2)+f(X=1)+f(X=0))\\\\\n         &=1-(\\binom{12}{3}0.05^3 0.95^9+\\binom{12}{2}0.05^2 0.95^{10}+\\\\\n         &\\binom{12}{1}0.05^1 0.95^{11}+\\binom{12}{0}0.05^0 0.95^{12})\n\\end{aligned}\n\\] 이다.\n\n다른 예시로는, 분자 진단 시장에서 golden standard라고 평가받는 PCR (Polynomial Chain Reaction)에 사용되는 medical device가 2000 대 중 5대 꼴로 기계적 결함이 발견된다고 가정할 때, 1년에 평균 100대의 분잔 진단 장비를 공급받는 구매자 입장에서 장비의 결함이 발생할 연간 평균과 분산의 추정은 다음과 같다.\n\\[\n\\begin{aligned}\n    X&\\sim Bin(100,\\frac{5}{2000})\\\\\n    f(X=x)&=\\binom{100}{x}\\frac{5}{2000}^x (1-\\frac{5}{2000})^{100-x}\\\\\n    \\text{E}(X)&=100(\\frac{5}{2000})\\\\\n    \\text{Var}(X)&=100(\\frac{5}{2000})(1-\\frac{5}{2000})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html#multinomial-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html#multinomial-distribution",
    "title": "Binomial Distribution",
    "section": "",
    "text": "Definition 2 n번의 독립 시행에서 각 각 p_1, p_2, …, p_n 의 성공 확률로 E_1, E_2, …, E_n 중 어느 하나를 발생시킬 때 각 event E_i에 대응되는 발생 횟수를 확률 변수 X_1, X_2, …, X_n 로 갖는 joint probability mass function은\n\\[\n\\begin{aligned}\n  f_X(\\mathbf X = x_1,x_2, ...,x_n)&=\\binom{n}{x_1,x_2, ..., x_n}p_1^{x_1}(p_2)^{x_2}\\dots (p_n)^{x_n} \\\\\n\\end{aligned}\n\\] 이다. (단, \\(\\sum_{i=1}^{n}x_i=n, \\sum_{i=1}^{n}p_i=1\\))\n\n\n\n주사위를 5 번 던질 때 1 또는 6의 눈이 1번, 3, 4 또는 5의 눈이 2번 , 2의 눈이 2번 나올 확률은\n\\[\n\\begin{aligned}\n    &x_1= 1, x_2=2, x_3=2\\\\\n    f(X=(1,2,2))&=\\binom{5}{1,2,3}\\frac{1}{3}^1\\frac{1}{2}^2\\frac{1}{6}^2\n\\end{aligned}\n\\] 이다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html#blog-guide-map-link",
    "title": "Binomial Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html#bernoulli-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html#bernoulli-distribution",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Definition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html#blog-guide-map-link",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html#bernoulli-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html#bernoulli-distribution",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Definition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html#blog-guide-map-link",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html",
    "title": "Probability",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nexperiment(실험): 연구 수행 방식.\ntrial (시행): 연구 실험 시행.\nsample space(표본 공간, (\\(\\text{S or } \\Omega\\)): 실험의 가능한 모든 결과의 모음 또는 근원 사상의 집합.\nelement (근원 사상, \\(\\omega\\)): 표본 공간의 원소.\nevent (사건, \\(E\\)): 근원 사상의 집합 또는 표본 공간의 부분 집합.\n\n\n\n\n\n\\(\\omega \\in A\\) : \\(\\omega\\) is an element of a set A\n\\(\\omega \\not\\in A\\) : \\(\\omega\\) is not an element of a set A\n\\(B \\subset A\\) : B is a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\in A\\)\n\\(A = B\\) : \\(B \\subset A\\), \\(A \\subset B\\)\n\\(B \\not\\subset A\\) : B is not a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\not\\in A\\)\n\n\n\n\n확률은 우연 (또는 가능성)과 불확실성에 대한 연구이다. 그 이론은 집합 이론을 기반으로 한다. 고전 확률은 가능성 조합 게임, 이론 오류와 같은 도박과 같다. 확률은 통계학, 경제학, 연산연구, 심리학, 생물학, 역학, 의학 등에 사용된다. 확률을 이해하기 위해 미적분학과 집합론의 지식이 요구되며 관련 학문은 ​​해석학, 측도론, 확률과정 등이 있다. 확률을 정의하려면 사건 수집에 대한 규칙성이 필요하다.\n\n\n\n\n확률은 사건이 발생할 가능성을 나타낸다.\n고전적 정의\n\n\nDefinition 1 The probability of event A is the sum of the probabilities assigned to all sample points in event A. Therefore, \\(0 \\le P(A) \\le 1, P(\\emptyset)=0, P(\\Omega)=1\\). In addition, if \\(A_1, A_2, A_3, ...\\) are mutually exclusive,\n\\[\nP(A_1 \\cup A_2 \\cup A_3 \\dots)=P(A_1) + P(A_2) + P(A_3) + \\dots =\\sum_{i=1}^{\\infty}P(A_i)\n\\]\n\n위의 정의에서 the probabilities assigned to all sample points in event A 의 표현은 사건 A안에 있는 element의 가중치로 해석할 수 있다. 즉, 쉽게 말하면, 확률은 표본 공간, sample space \\(\\Omega\\) 의 원소 (element) \\(\\omega\\) 에 할당된 가중치를 더한 것이다.\n예를 들어, 주사위 모양을 어떤식으로든 조작해 홀수가 짝수보다 2배 더 많이 발생하게끔 만들어 1 번 던질 때 3 보다 작은 수가 나올 사건을 A라고 하면 \\(\\Omega=\\{1,2,3,4,5,6\\}\\) 이고 각 홀수 원소에 가중치가 2배씩 붙기 때문에 홀수 눈이 발생할 확률은 \\(\\frac{2x}{2x+x+2x+x+2x+x}=\\frac{2}{9}\\), 반면에, 짝수의 눈이 나올 확률은 \\(\\frac{x}{2x+x+2x+x+2x+x}=\\frac{1}{9}\\) 이다. 이 때 확률은 위의 정의를 따라야 하므로\n\n\\(0\\le P(evenNumber)=\\frac{2}{9}, P(oddNumber)=\\frac{1}{9}\\le 1\\) 이고\n\\(P(\\Omega=\\{1,2,3,4,5,6\\})=P(evenNumbers)+P(oddNumbers)=1\\)\n\n이므로 확률이라고 할 수있다.\n\n그러므로 \\(P(A={1,2})=P(1)+P(2)=\\frac{2}{9}+\\frac{1}{9}=\\frac{3}{9}\\) 이다.\n\n\nTheorem 1 Countable sample space \\(\\Omega\\) consists of \\(N\\) distinctive equally likely elements (i.e. \\(n(S)=N\\)), and an event \\(A\\) is a subset of the sample space. The event A consists of \\(n\\) distinctive equally likely elements (i.e. \\(n(A)=n\\)). Then \\[\nP(A)=\\frac{n}{N}\n\\]\n\nelement가 오직 동일한 확률로 발생할 때에만 (equally likely), 확률은 \\(\\frac{n(A)}{n(\\Omega \\space or \\space S)}\\) 의 비율(proportion)로 표현될 수 있다.\n예를 들어, 주사위의 눈이 3 보다 작은 수가 나올 사건을 A라고 하면 \\(P(A)= \\frac{n(A)}{n(S)}=\\frac{n(\\{1,2\\})}{n(\\{1,2,3,4,5,6\\})}=\\frac{2}{6}\\) 가 된다.\n\nTheorem 2 If in \\(N\\) identical and independent repeated experiments, an event \\(A\\) happens \\(n\\) times, the the probability of \\(A\\) is defined by \\[\nP(A)=\\lim_{N\\to\\infty}\\frac{n}{N}\n\\]\n\n\nTheorem 3 Basic probability theorem: the complement and additive rule. \\[\n\\begin{aligned}\nP(E^c)&=1-P(E) \\\\\nP(E_1 \\cup E_2)&= P(E_1) + P(E_2) - P(E_1 \\cap E_2) \\\\\n\\end{aligned}\n\\]\n\\(E_1\\) and \\(E_2\\) are mutually exclusive.\n\n\nTheorem 4  \nGeneralized additive rule $$\n\\[\\begin{aligned}\n\n\\end{aligned}\\]\n$$\n\n나머지는 확률 이론 서적을 살펴 보길 바란다.\n\n\n\n\n\n\n\n\nexperiment: the way carry out a study, study design.\ntrial: study experiment trial.\nsample space (\\(\\text{S or } \\Omega\\)): the set of all possible elements (i.e. the collection of all possible outcomes of an experiment).\nelement (\\(\\omega\\)): each outcome of sample space, it is also called ‘sample point’.\nevent (\\(E\\)): a set of sample points or outcomes or a subset of sample space.\n\n\n\n\n\n\\(\\omega \\in A\\) : \\(\\omega\\) is an element of a set A\n\\(\\omega \\not\\in A\\) : \\(\\omega\\) is not an element of a set A\n\\(B \\subset A\\) : B is a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\in A\\)\n\\(A = B\\) : \\(B \\subset A\\), \\(A \\subset B\\)\n\\(B \\not\\subset A\\) : B is not a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\not\\in A\\)\n\n\n\n\nProbability is on study of chance and uncertainty. Its theory builds on set theory. Classic probability is like a gambling of combinatorial games of chance, theory errors. Probability is used in statistics, economics, operation research, psychology, biology, epidemiology, medicine, etc. The prerequisite for probability is calculus and set theory, and the related study is real analysis, measure theory, and stochastic process. To define probability, some regularity on the collection of events is required.\n\n\n\n\nprobability shows the possibility of the occurrence of an event.\nclassic definition ::: {#def-classic}\n\nCountable sample space \\(\\Omgega\\) consists of \\(N\\) distinctive equally likely elements (i.e. \\(n(S)=N\\)), and an event \\(A\\) is a subset of the sample space. The event A consists of \\(n\\) distinctive equally likely elements (i.e. \\(n(A)=n\\)). Then \\[\nP(A)=\\frac{n}{N}\n\\]\n\n\n\nTheorem 5 If in \\(N\\) identical and independent repeated experiments, an event \\(A\\) happens \\(n\\) times, the the probability of \\(A\\) is defined by \\[\nP(A)=\\lim_{N\\to\\infty}\\frac{n}{N}\n\\]\n\n\nThe case of the sample space consisting of \\(N\\) distinctive not equally likely elements,\nThe case of the uncountable sample space\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#terms",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#terms",
    "title": "Probability",
    "section": "",
    "text": "experiment(실험): 연구 수행 방식.\ntrial (시행): 연구 실험 시행.\nsample space(표본 공간, (\\(\\text{S or } \\Omega\\)): 실험의 가능한 모든 결과의 모음 또는 근원 사상의 집합.\nelement (근원 사상, \\(\\omega\\)): 표본 공간의 원소.\nevent (사건, \\(E\\)): 근원 사상의 집합 또는 표본 공간의 부분 집합."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#notations",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#notations",
    "title": "Probability",
    "section": "",
    "text": "\\(\\omega \\in A\\) : \\(\\omega\\) is an element of a set A\n\\(\\omega \\not\\in A\\) : \\(\\omega\\) is not an element of a set A\n\\(B \\subset A\\) : B is a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\in A\\)\n\\(A = B\\) : \\(B \\subset A\\), \\(A \\subset B\\)\n\\(B \\not\\subset A\\) : B is not a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\not\\in A\\)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#overview",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#overview",
    "title": "Probability",
    "section": "",
    "text": "확률은 우연 (또는 가능성)과 불확실성에 대한 연구이다. 그 이론은 집합 이론을 기반으로 한다. 고전 확률은 가능성 조합 게임, 이론 오류와 같은 도박과 같다. 확률은 통계학, 경제학, 연산연구, 심리학, 생물학, 역학, 의학 등에 사용된다. 확률을 이해하기 위해 미적분학과 집합론의 지식이 요구되며 관련 학문은 ​​해석학, 측도론, 확률과정 등이 있다. 확률을 정의하려면 사건 수집에 대한 규칙성이 필요하다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#probability",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#probability",
    "title": "Probability",
    "section": "",
    "text": "확률은 사건이 발생할 가능성을 나타낸다.\n고전적 정의\n\n\nDefinition 1 The probability of event A is the sum of the probabilities assigned to all sample points in event A. Therefore, \\(0 \\le P(A) \\le 1, P(\\emptyset)=0, P(\\Omega)=1\\). In addition, if \\(A_1, A_2, A_3, ...\\) are mutually exclusive,\n\\[\nP(A_1 \\cup A_2 \\cup A_3 \\dots)=P(A_1) + P(A_2) + P(A_3) + \\dots =\\sum_{i=1}^{\\infty}P(A_i)\n\\]\n\n위의 정의에서 the probabilities assigned to all sample points in event A 의 표현은 사건 A안에 있는 element의 가중치로 해석할 수 있다. 즉, 쉽게 말하면, 확률은 표본 공간, sample space \\(\\Omega\\) 의 원소 (element) \\(\\omega\\) 에 할당된 가중치를 더한 것이다.\n예를 들어, 주사위 모양을 어떤식으로든 조작해 홀수가 짝수보다 2배 더 많이 발생하게끔 만들어 1 번 던질 때 3 보다 작은 수가 나올 사건을 A라고 하면 \\(\\Omega=\\{1,2,3,4,5,6\\}\\) 이고 각 홀수 원소에 가중치가 2배씩 붙기 때문에 홀수 눈이 발생할 확률은 \\(\\frac{2x}{2x+x+2x+x+2x+x}=\\frac{2}{9}\\), 반면에, 짝수의 눈이 나올 확률은 \\(\\frac{x}{2x+x+2x+x+2x+x}=\\frac{1}{9}\\) 이다. 이 때 확률은 위의 정의를 따라야 하므로\n\n\\(0\\le P(evenNumber)=\\frac{2}{9}, P(oddNumber)=\\frac{1}{9}\\le 1\\) 이고\n\\(P(\\Omega=\\{1,2,3,4,5,6\\})=P(evenNumbers)+P(oddNumbers)=1\\)\n\n이므로 확률이라고 할 수있다.\n\n그러므로 \\(P(A={1,2})=P(1)+P(2)=\\frac{2}{9}+\\frac{1}{9}=\\frac{3}{9}\\) 이다.\n\n\nTheorem 1 Countable sample space \\(\\Omega\\) consists of \\(N\\) distinctive equally likely elements (i.e. \\(n(S)=N\\)), and an event \\(A\\) is a subset of the sample space. The event A consists of \\(n\\) distinctive equally likely elements (i.e. \\(n(A)=n\\)). Then \\[\nP(A)=\\frac{n}{N}\n\\]\n\nelement가 오직 동일한 확률로 발생할 때에만 (equally likely), 확률은 \\(\\frac{n(A)}{n(\\Omega \\space or \\space S)}\\) 의 비율(proportion)로 표현될 수 있다.\n예를 들어, 주사위의 눈이 3 보다 작은 수가 나올 사건을 A라고 하면 \\(P(A)= \\frac{n(A)}{n(S)}=\\frac{n(\\{1,2\\})}{n(\\{1,2,3,4,5,6\\})}=\\frac{2}{6}\\) 가 된다.\n\nTheorem 2 If in \\(N\\) identical and independent repeated experiments, an event \\(A\\) happens \\(n\\) times, the the probability of \\(A\\) is defined by \\[\nP(A)=\\lim_{N\\to\\infty}\\frac{n}{N}\n\\]\n\n\nTheorem 3 Basic probability theorem: the complement and additive rule. \\[\n\\begin{aligned}\nP(E^c)&=1-P(E) \\\\\nP(E_1 \\cup E_2)&= P(E_1) + P(E_2) - P(E_1 \\cap E_2) \\\\\n\\end{aligned}\n\\]\n\\(E_1\\) and \\(E_2\\) are mutually exclusive.\n\n\nTheorem 4  \nGeneralized additive rule $$\n\\[\\begin{aligned}\n\n\\end{aligned}\\]\n$$\n\n나머지는 확률 이론 서적을 살펴 보길 바란다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#terms-1",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#terms-1",
    "title": "Probability",
    "section": "",
    "text": "experiment: the way carry out a study, study design.\ntrial: study experiment trial.\nsample space (\\(\\text{S or } \\Omega\\)): the set of all possible elements (i.e. the collection of all possible outcomes of an experiment).\nelement (\\(\\omega\\)): each outcome of sample space, it is also called ‘sample point’.\nevent (\\(E\\)): a set of sample points or outcomes or a subset of sample space."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#notations-1",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#notations-1",
    "title": "Probability",
    "section": "",
    "text": "\\(\\omega \\in A\\) : \\(\\omega\\) is an element of a set A\n\\(\\omega \\not\\in A\\) : \\(\\omega\\) is not an element of a set A\n\\(B \\subset A\\) : B is a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\in A\\)\n\\(A = B\\) : \\(B \\subset A\\), \\(A \\subset B\\)\n\\(B \\not\\subset A\\) : B is not a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\not\\in A\\)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#overview-1",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#overview-1",
    "title": "Probability",
    "section": "",
    "text": "Probability is on study of chance and uncertainty. Its theory builds on set theory. Classic probability is like a gambling of combinatorial games of chance, theory errors. Probability is used in statistics, economics, operation research, psychology, biology, epidemiology, medicine, etc. The prerequisite for probability is calculus and set theory, and the related study is real analysis, measure theory, and stochastic process. To define probability, some regularity on the collection of events is required."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#probability-1",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#probability-1",
    "title": "Probability",
    "section": "",
    "text": "probability shows the possibility of the occurrence of an event.\nclassic definition ::: {#def-classic}\n\nCountable sample space \\(\\Omgega\\) consists of \\(N\\) distinctive equally likely elements (i.e. \\(n(S)=N\\)), and an event \\(A\\) is a subset of the sample space. The event A consists of \\(n\\) distinctive equally likely elements (i.e. \\(n(A)=n\\)). Then \\[\nP(A)=\\frac{n}{N}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html#blog-guide-map-link",
    "title": "Probability",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nBayes’ rule (베이즈 정리)는 prior probability(사전 확률)과 posterior probability(사후 확률)의 관계를 조건부 확률을 이용하여 확립한 것이다.\n\nprior probability(사전 확률)는 데이터를 얻기 전 연구자의 가설이 들어간 일종의 사건 발생의 신뢰도로 해석하기도 하고 prior probability density function (사전 확률 밀도 함수) 라고도 표현된다.\nposterior probability(사후 확률)는 데이터가 주어진 후 연구자의 가설이 들어간 사건 발생의 신뢰도로 해석하기도 하고 posterior probability desnsity function(사후 확률 밀도 함수) 라고도 표현된다.\n\n좀 더 구체적으로, 2개의 사건 A와 B로 한정시켜 생각해봤을 때, 조건부 확률 \\(P(A|B)\\) 는 각 사건의 확률 \\(P(A), P(B), P(B|A)\\) 를 사용하여 게산될 수 있다. 그래서 베이즈 정리는 \\(P(B|A)\\), \\(P(B|\\overline{A})\\), \\(P(A)\\) 의 정보를 알고있거나 계산 가능할 때 아래와 같은 \\(P(A|B)\\) 의 확률을 구할 수 있는 공식을 제공한다(Equation 1).\n\n\nBayes’ rule을 좀 더 직관적으로 이해하기 위해선 Bayes’ rule와 연관된 친숙한 개념들을 상기시킬 필요가 있다. 우리에게 친숙한 개념인 연역법과 귀납법에 대해서 간단이 살펴본다.\n\n\n\n\n연역법 (deduction method or deductive reasoning)는 하나 (=대전제) 또는 둘 이상의 명제(=대전제+소전제들)를 전제로 하여 명확한 논리에 근거해 새로운 명제(결론)를 도출하는 방법이다. 보통 일반적인 명제에서 구체적인 명제로 도출해내는 방식으로 연역법을 설명하기도 한다. 연역법은 전제와 결론의 타당성보다는 결론을 이끌어내는 논리 전개에 엄격함을 요구한다. 그래서 명쾌한 논리가 보장된다면 연역적 추론의 결론은 그 전제들이 결정적 근거가 되어 전제와 결론이 필연성을 갖게 된다. 따라서, 전제가 진리(=참)이면 결론도 항상 진리(=참)이고 전제가 거짓이면 결론도 거짓으로 도출된다. 하지만, 모든 연역적 추론에서 출발되는 최초의 명제가 결코 연역에 의해 도출될 수 없다는 약점을 갖고있다. 즉, 반드시 검증된 명제를 대전제로 하여 연역적 추리를 시작해야한다. Source: naver encyclopedia -deductive method (cf. 귀류법)\n예를 들어, 아리스토텔레스의 삼단논법의 논리 형식이 가장 많이 인용이 된다. 대전제와 소전제가 하나씩있는 둘 이상의 명제로부터 결론이 도출되는 예를 살펴보자.\n\n대전제: 모든 사람은 죽는다.\n소전제: 소크라테스는 사람이다.\n결론: 그러므로 소크라테스도 죽는다.\n\n\n\n\n귀납법 (Induction method or Inductive reasoning)은 전제와 결론을 뒷받침하는 논리에 의해 그 타당성이 평가된다. 귀납적 추론은 관찰과 실험에서 얻은 특수한 사례 (= data)를 근거로 전체에 적용시키는 귀납적 비약을 통해 이루어진다. 이와 같이 귀납에서 얻어진 결론은 일정한 개연성을 지닐 뿐이며, 특정 data에 따라 귀납적 추론의 타당성에 영향을 미친다. 그러므로, 검증된 data가 많을 수록 신뢰도와 타당성이 증가한다는 특징이 있다.하지만, 귀납적 추론의 결론이 진리인 것은 아니다. Source: naver encyclopedia - inductive method\n\n특수한 사례 (or data): 소크라테스는 죽었다, 플라톤도 죽었다, 아리스토텔레스도 죽었다.\n소전제: 소크라테스는 사람이다.\n결론: 그러므로 소크라테스도 죽는다.\n\n위와 같이 연역적 추론과 귀납적 추론은 서로 반대되는 개념으로 각 각 강점과 약점이 있으며 현실에서는 서로 상호 보완적으로 쓰이고 있다. 따라서, 전제로 삼는 대전제 역시 검증 과정이 필요하고 그 가설에서 몇 개의 명제를 연역해 실험과 관찰 등을 수행하는 가설연역법(hypothetical deductive method)이 널리 쓰이고 있다.\n\n\n\n\n통계학에선 모수(parameter)를 추정하는 여러 방법론들이 있는데 이번 블로그에서는 Frequentism와 Bayeseanism, 이 2가지 방법론에 초점을 둔다.\n\n\n통계학에서 가장 널리쓰이고 있는 방법론으로, 연역법에 근거한 결론 도출 방식을 이용한다. 간단히 말하면, 이미 알려진 분포에서 연구자의 관측치가 발생할 확률을 관찰하여 결론을 유도 하는 방법이다. p-value에 의한 결론 도출방식이 그 대표적인 예이다. 연구자의 데이터가 여러 수학자와 통계학자들이 증명해 놓은 분포하에서 발생한 사실이 입증이 됐을 때 연구자의 관측치가 그 named distribution(like normal distribution)에서 발생할 확률이 낮을 수록 p-value가 작아지고 일정 유의수준에 따라 연구자는 귀무가설을 기각하는 논리방식을 따른다.\n\n\n\n통계학에서 역시 많이 쓰이는 방법론으로, 귀납법에 근거한 결론 도출 방식을 이용한다. 간단히 말하면, 확률을 확률변수가 갖는 sample space에 대한 특정 사건이 발생한 사건의 비로 보는 것 (equally likely라고 가정)이 아니라 내가 설정한 가설에 대한 신뢰도로 바라보는 것이다. 따라서, 사전에 이미 알고있는 데이터가 있어 사전 확률 (prior probability)을 알고있고 이 사전 확률이 추가적인 data에 의해 조정되는 사후 확률 (posterior probability)이 계산된다. 이때 사전 확률자체보다는 추가적인 data와 사후 확률을 계산하는데 사용되는 likelihood의 타당성이 더 중요하다. 더 구체적인 내용은 Bayesean statistics에 기본이 되는 Bayes’ rule에서 살펴보기로 한다.\nsource: Frequentism vs Bayeseanism\n\n\n\n\n\n\nTheorem 1 \\[\n\\begin{aligned}\nP(A|B)&=\\frac{P(B|A)P(A)}{P(B)}\\\\\n      &=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\n\\end{aligned}\n\\tag{1}\\]\n\n\n\\(P(A|B)\\): posterior probability, B(data)가 주어졌을때 가설 A에 대한 신뢰도\n\\(P(A)\\): prior probability, 가설 A에대한 신뢰도\n\\(P(B|A)\\): likelihood, 가설 A가 주어졌을때 B(Data)가 발생할 신뢰도\n\\(P(B)\\): marginal probability, Data의 신뢰도\n\nEquation 1 의 두 분째 등식을 이해하기 위해선, Law of Total Probability (전 확률 법칙) 또는 Total Probability Rule (전 확률 정리)을 이해해야한다.\n\n\n\n\nTheorem 2 Let \\(A_1, A_2, ..., A_k\\) be a set of mutually exclusive and exhaustive events. Let \\(A\\) be a event and a partition of sample space \\(\\Omega\\), then\n\\[\nP(B)=\\sum_{i=1}^{n}P(B|A_i)P(A_i)\n\\]\n\n\n\n\n\n\nFigure 1: Law of Total Probability Example - Two Events\n\n\nSource: Law of ToTal Probability with Proof\n\n\n\n\nFigure 2: Law of Total Probability Example - Multiple Events\n\n\nSource: MIT RES.6-012 Introduction to Probability, Spring 2018 - Youtube\n\n\n\\[\n\\begin{aligned}\nP(B)&=P(B\\cap A) + P(B\\cap \\overline A)\\\\\n    &=P(B\\cap A) + P(B\\cap \\overline A)\\\\\n    &=P(B|A)P(A)+P(B|\\overline A)P(\\overline A)\\\\\nP(A\\cap B)&=P(B|A)P(A)=P(A|B)P(B)\\\\\n\\therefore\nP(A|B)&=\\frac{P(A \\cap B)}{P(B)}\\\\\n      &=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\n\\end{aligned}\n\\tag{2}\\]\nLaw of total probability 를 이용하여 Bayes’ rule이 Equation 2 와 같이 변형되었다. 최종식을 보면 좀 더 직관적인 해석이 가능해지는데 P(B) 가 A와의 교집합 확률의 총합이 되면서 분자가 그 일부가 되는 비율의 개념으로 해석될 수 있다. Figure 1 를 보면 \\(P(A|B)=\\frac{P(B \\cap A)}{P(B)}=\\frac{P(B \\cap A)}{P(B \\cap A)+P(B \\cap \\overline A)}\\) 로 표현되는 것을 볼 수 있다. 그 것을 조금 더 일반화 한 경우는 Figure 2 를 참고하여 유추할 수 있다.\n\n\n\n\\[\n\\begin{aligned}\nP(A|B)&=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\\\\\nP(\\theta|x)&=\\frac{P(x|\\theta)P(\\theta)}{P(x|\\theta)P(\\theta)+P(x|\\overline \\theta)P(\\overline \\theta)}\n\\end{aligned}\n\\]\n많은 참고 문헌에서 사건 A, B를 모수, \\(\\theta\\) 와 data, \\(x\\) 로 표현하기도 한다. 즉, data \\(x\\) 가 주어졌을 때 모수 \\(\\theta\\) 가 발생할 확률이 data에 의해서 update된다.\n\n\\(P(\\theta)\\)\n\nprior probability density function\n데이터없이 초기에 임시로 부여된 모델 또는 모수의 확률\n\n\\(P(x|\\theta)\\)\n\nlikelihood\n초기에 임시로 부여된 모델 또는 모수가 주어졌을 때 data x가 발생할 우도\n좀 더 파격적으로 해석하면, 초기에 임시로 부여된 모델 또는 모수가 data x에 들어맞을(or fittng) 확률\n\n\\(P(x)\\)\n\nmarginal proability\n데이터가 발생할 확률로 \\(\\theta\\) 와 상관없기 때문에 상수로 취급한다.\n\n\\(P(\\theta|x)\\)\n\nposterior probability density function\ndata가 주어졌을 때 모델 또는 모수의 확률\nBayes’ Rule에 의한 최적화에서 다음 최적화 iteration에서 Prior로 쓰인다.\n\n\n\\(P(x)\\) 는 상수이기 때문에 생략가능 하여 아래의 식과 같이 정리 할 수 있다. \\[\nP(\\theta|x)\\propto P(x|\\theta)P(\\theta)\n\\]\n\\(P(\\theta|x)\\) 는 \\(P(x|\\theta)P(\\theta)\\) 에만 영향을 받는 것을 볼 수 있다.\n\n\n\n펭수는 평소 관심이 있던 코니에게서 초콜릿을 선물받았다. 펭수는 초콜릿을 준 코니가 나를 좋아하는지가 궁금하기 때문에 이것을 통계적으로 계산해본다.\n펭수는 먼저 다음 두 상황을 가정한다.\n\n\\(P(like)=0.5\\)\n\n코니가 펭수를 좋아한다는 가설의 신뢰도는 반 반이다. 즉, 정보없는 상태에서의 펭수의 prior probability.\n0.5로 설정한 이유는 다음의 원리를 따랐다. The Principle of Insufficient Reason(이유불충분의 원리- 하나의 사건을 기대할만한 어떤 이유가 없는 경우에는 가능한 모든 사건에 동일한 확률을 할당해야 한다는 원칙).\n\n\\(P(choco)\\)\n\n초콜릿을 받았다라는 data가 발생할 신뢰도\n\n\n펭수는 코니에게 자신을 좋아하는지 알 길이 없으니 사람이 호감이 있을 때에 대한 초콜릿 선물 데이터를 조사하기 시작한다. 즉, 호감의 근거는 초콜릿으로 한정했고 초콜릿 선물 방식의 불확실성을 호감으로 설명하는 문헌을 찾기 시작했다. 그리고 펭수는 도서관에 있는 일반인 100명을 대상으로 초콜릿과 호감과의 관계를 연구한 초콜릿과 호감 논문을 통해 두 가지 정보를 알게된다.\n\n일반적으로, 어떤 사람이 상대방에게 호감이 있어서 초콜릿을 줄 확률은 \\(40%\\) 이다. 즉, \\(P(choco|like)=0.4\\)\n일반적으로, 어떤 사람이 상대방에게 호감이 없지만 예의상 초콜릿을 줄 확률은 \\(30%\\) 이다. 즉, \\(P(choco|\\overline{like})=0.3\\)\n위의 2가지 정보로 유추 가능한 정보\n\n\\(P(\\overline{choco}|like)=0.6\\)\n\\(P(\\overline{choco}|\\overline{like})=0.7\\)\n\n초콜릿에 관한 조사를 토대로 얻은 4가지 정보로 유추할 수 있는 정보\n\n\\(P(choco|like)=0.4\\): like를 받고 있는 50명 중 \\(40%\\) 인 20명은 초콜릿을 받는다.\n\\(P(\\overline{choco}|like)=0.6\\): like를 받고 있는 50명 중 \\(60%\\) 인 30명은 초콜릿을 받지 못한다.\n\\(P(choco|\\overline{like})=0.3\\): like를 받지 않는 50명 중 \\(30%\\) 인 15명은 예의상 준 초콜릿을 받는다.\n\\(P(\\overline{choco}|\\overline{like})=0.7\\): like를 받지 않는 50명 중 \\(70%\\) 인 35명은 초콜릿을 받지 못한다.\n\n\n펭수의 관점으로 정보를 재분류\n\n펭수가 궁금한 정보\n\n\\(P(like|choco)=?\\), posterior probability\n\n펭수가 가정한 정보\n\n\\(P(like)=0.5\\), prior probability by The Principle of Insufficient Reason\n\n펭수가 조사한 정보\n\n\\(P(choco|like)=0.4\\), likelihood\n\\(P(choco)\\): marginal probability\n\n\\(P(choco)=P(choco|like)+P(choco|\\overline{like})=\\frac{20+15}{100}=0.35\\)\n\n\n\n위의 정리한 정보를 Bayes’ rule에 대입하면,\n\\[\nP(like|choco)=\\frac{P(choco|like)\\times P(like)}{P(choco)}=\\frac{0.4\\times 0.5}{0.35}=0.57\n\\]\n펭수의 prior probability(\\(P(A)=0.5\\))가 posterior probability(\\(P(A|B)=0.57\\))로 업데이트 될 수 있다. 초콜릿과 호감 논문을 읽고 코니가 자신을 좋아할 확률이 높아진 것에 대해 기대감을 얻은 용기가 없는 펭수는 100명 보다 더 많은 독립적인 사람들로 실험한 논문을 찾아 다시 자신의 업데이트 된 사전 확률을 계속해서 업데이트할 생각이다. 그리고 자신의 사전 확률을 추가적인 데이터를 갖고 사후 확률로 계속해서 업데이트시켜 정확한 확률을 구한다.\n위의 예시는 영상 자료: 초콜릿을 준 코니의 마음을 시청하고 영감을 얻은 슬기로운 통계생활 tistory에 있는 Source: 베이즈 정리(Bayes’ rule) 완벽히 정리하기 슬기로운 통계생활 블로그를 요약 및 약간의 각색을 한 것이다.\n\n\n\n\n\nTheorem 3 Let \\(A_1, A_2, ..., A_k\\) be a set of mutually exclusive and exhaustive events. Let \\(A\\) be a event, then\n\\[\nP(A_i|B)=\\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^{k}P(B|A_i)P(A_i)}\n\\]\n\n\n\n\nMaximum a posterior estimation는 statistical estimation methods의 큰 기둥 중 하나인 maximum likelihood estimation과 더불어 parameter \\(\\theta\\) 를 추정하는데 많이 사용되는 방법론이다. 사후 확률 밀도 함수 \\(f(x|\\theta)\\) 또는 \\(P(x|\\theta)\\) 를 최대화하는 \\(\\theta\\) 의 추정치를 구하는 방법이며 아래와 같은 argument로 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\hat{\\theta}&=\\arg \\max_{\\theta}\\frac{f(x|\\theta)f(\\theta)}{\\int f(x|\\theta)f(\\theta)}\\\\\n            &\\propto\\arg \\max_{\\theta}f(x|\\theta)f(\\theta)\n\\end{aligned}\n\\]\n최대 우도 추정량과 달리 최대 사후 추정량에는 최대화하는 식에 사전 확률이 추가되어 있는 것을 볼 수 있다. 그러므로 분자 부분인 \\(f(x|\\theta)f(\\theta)\\) 만을 최대화 한다. 분모 부분인 \\(\\int f(x|\\theta)f(\\theta)\\) nomarlizing penalty 또는 constant로 간주한다. 여기서 \\(P(\\theta)\\) 초기 가정치 인데 아무렇게나 설정하기 보다는 good estimate로 설정해야 통계학자들로부터의 공격을 최소화시킬 수 있다. MAP는 나이브 베이즈의 알고리즘의 핵심이다.\n[참고] 최대 우도 추정량 \\[\n\\begin{aligned}\n\\hat{\\theta}=\\arg \\max_{\\theta}L(x|\\theta)=\\arg \\max_{\\theta}\\Pi_{i=1}^{n}f(x|\\theta)\n\\end{aligned}\n\\]\n\n\n\nNaive Bayes에 대한 구체적인 글은 다른 블로그에 소개한다. Naive Bayes는 Bayes’ Rule을 이용해 \\(\\theta\\) 를 최적화 시킨다. Naive Bayes의 Naive는 features 또는 explanotry variables이 서로 conditionally indepdent라고 가정한 것에서 이름 붙여졌다.\n\n\n\n\n\nBayes’ rule provides a formula how to calculate \\(P(A|B)\\) if \\(P(B|A)\\), \\(P(B|\\overline{A})\\), \\(P(A)\\) are available\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#bayes-rule",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#bayes-rule",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Bayes’ rule (베이즈 정리)는 prior probability(사전 확률)과 posterior probability(사후 확률)의 관계를 조건부 확률을 이용하여 확립한 것이다.\n\nprior probability(사전 확률)는 데이터를 얻기 전 연구자의 가설이 들어간 일종의 사건 발생의 신뢰도로 해석하기도 하고 prior probability density function (사전 확률 밀도 함수) 라고도 표현된다.\nposterior probability(사후 확률)는 데이터가 주어진 후 연구자의 가설이 들어간 사건 발생의 신뢰도로 해석하기도 하고 posterior probability desnsity function(사후 확률 밀도 함수) 라고도 표현된다.\n\n좀 더 구체적으로, 2개의 사건 A와 B로 한정시켜 생각해봤을 때, 조건부 확률 \\(P(A|B)\\) 는 각 사건의 확률 \\(P(A), P(B), P(B|A)\\) 를 사용하여 게산될 수 있다. 그래서 베이즈 정리는 \\(P(B|A)\\), \\(P(B|\\overline{A})\\), \\(P(A)\\) 의 정보를 알고있거나 계산 가능할 때 아래와 같은 \\(P(A|B)\\) 의 확률을 구할 수 있는 공식을 제공한다(Equation 1).\n\n\nBayes’ rule을 좀 더 직관적으로 이해하기 위해선 Bayes’ rule와 연관된 친숙한 개념들을 상기시킬 필요가 있다. 우리에게 친숙한 개념인 연역법과 귀납법에 대해서 간단이 살펴본다.\n\n\n\n\n연역법 (deduction method or deductive reasoning)는 하나 (=대전제) 또는 둘 이상의 명제(=대전제+소전제들)를 전제로 하여 명확한 논리에 근거해 새로운 명제(결론)를 도출하는 방법이다. 보통 일반적인 명제에서 구체적인 명제로 도출해내는 방식으로 연역법을 설명하기도 한다. 연역법은 전제와 결론의 타당성보다는 결론을 이끌어내는 논리 전개에 엄격함을 요구한다. 그래서 명쾌한 논리가 보장된다면 연역적 추론의 결론은 그 전제들이 결정적 근거가 되어 전제와 결론이 필연성을 갖게 된다. 따라서, 전제가 진리(=참)이면 결론도 항상 진리(=참)이고 전제가 거짓이면 결론도 거짓으로 도출된다. 하지만, 모든 연역적 추론에서 출발되는 최초의 명제가 결코 연역에 의해 도출될 수 없다는 약점을 갖고있다. 즉, 반드시 검증된 명제를 대전제로 하여 연역적 추리를 시작해야한다. Source: naver encyclopedia -deductive method (cf. 귀류법)\n예를 들어, 아리스토텔레스의 삼단논법의 논리 형식이 가장 많이 인용이 된다. 대전제와 소전제가 하나씩있는 둘 이상의 명제로부터 결론이 도출되는 예를 살펴보자.\n\n대전제: 모든 사람은 죽는다.\n소전제: 소크라테스는 사람이다.\n결론: 그러므로 소크라테스도 죽는다.\n\n\n\n\n귀납법 (Induction method or Inductive reasoning)은 전제와 결론을 뒷받침하는 논리에 의해 그 타당성이 평가된다. 귀납적 추론은 관찰과 실험에서 얻은 특수한 사례 (= data)를 근거로 전체에 적용시키는 귀납적 비약을 통해 이루어진다. 이와 같이 귀납에서 얻어진 결론은 일정한 개연성을 지닐 뿐이며, 특정 data에 따라 귀납적 추론의 타당성에 영향을 미친다. 그러므로, 검증된 data가 많을 수록 신뢰도와 타당성이 증가한다는 특징이 있다.하지만, 귀납적 추론의 결론이 진리인 것은 아니다. Source: naver encyclopedia - inductive method\n\n특수한 사례 (or data): 소크라테스는 죽었다, 플라톤도 죽었다, 아리스토텔레스도 죽었다.\n소전제: 소크라테스는 사람이다.\n결론: 그러므로 소크라테스도 죽는다.\n\n위와 같이 연역적 추론과 귀납적 추론은 서로 반대되는 개념으로 각 각 강점과 약점이 있으며 현실에서는 서로 상호 보완적으로 쓰이고 있다. 따라서, 전제로 삼는 대전제 역시 검증 과정이 필요하고 그 가설에서 몇 개의 명제를 연역해 실험과 관찰 등을 수행하는 가설연역법(hypothetical deductive method)이 널리 쓰이고 있다.\n\n\n\n\n통계학에선 모수(parameter)를 추정하는 여러 방법론들이 있는데 이번 블로그에서는 Frequentism와 Bayeseanism, 이 2가지 방법론에 초점을 둔다.\n\n\n통계학에서 가장 널리쓰이고 있는 방법론으로, 연역법에 근거한 결론 도출 방식을 이용한다. 간단히 말하면, 이미 알려진 분포에서 연구자의 관측치가 발생할 확률을 관찰하여 결론을 유도 하는 방법이다. p-value에 의한 결론 도출방식이 그 대표적인 예이다. 연구자의 데이터가 여러 수학자와 통계학자들이 증명해 놓은 분포하에서 발생한 사실이 입증이 됐을 때 연구자의 관측치가 그 named distribution(like normal distribution)에서 발생할 확률이 낮을 수록 p-value가 작아지고 일정 유의수준에 따라 연구자는 귀무가설을 기각하는 논리방식을 따른다.\n\n\n\n통계학에서 역시 많이 쓰이는 방법론으로, 귀납법에 근거한 결론 도출 방식을 이용한다. 간단히 말하면, 확률을 확률변수가 갖는 sample space에 대한 특정 사건이 발생한 사건의 비로 보는 것 (equally likely라고 가정)이 아니라 내가 설정한 가설에 대한 신뢰도로 바라보는 것이다. 따라서, 사전에 이미 알고있는 데이터가 있어 사전 확률 (prior probability)을 알고있고 이 사전 확률이 추가적인 data에 의해 조정되는 사후 확률 (posterior probability)이 계산된다. 이때 사전 확률자체보다는 추가적인 data와 사후 확률을 계산하는데 사용되는 likelihood의 타당성이 더 중요하다. 더 구체적인 내용은 Bayesean statistics에 기본이 되는 Bayes’ rule에서 살펴보기로 한다.\nsource: Frequentism vs Bayeseanism\n\n\n\n\n\n\nTheorem 1 \\[\n\\begin{aligned}\nP(A|B)&=\\frac{P(B|A)P(A)}{P(B)}\\\\\n      &=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\n\\end{aligned}\n\\tag{1}\\]\n\n\n\\(P(A|B)\\): posterior probability, B(data)가 주어졌을때 가설 A에 대한 신뢰도\n\\(P(A)\\): prior probability, 가설 A에대한 신뢰도\n\\(P(B|A)\\): likelihood, 가설 A가 주어졌을때 B(Data)가 발생할 신뢰도\n\\(P(B)\\): marginal probability, Data의 신뢰도\n\nEquation 1 의 두 분째 등식을 이해하기 위해선, Law of Total Probability (전 확률 법칙) 또는 Total Probability Rule (전 확률 정리)을 이해해야한다.\n\n\n\n\nTheorem 2 Let \\(A_1, A_2, ..., A_k\\) be a set of mutually exclusive and exhaustive events. Let \\(A\\) be a event and a partition of sample space \\(\\Omega\\), then\n\\[\nP(B)=\\sum_{i=1}^{n}P(B|A_i)P(A_i)\n\\]\n\n\n\n\n\n\nFigure 1: Law of Total Probability Example - Two Events\n\n\nSource: Law of ToTal Probability with Proof\n\n\n\n\nFigure 2: Law of Total Probability Example - Multiple Events\n\n\nSource: MIT RES.6-012 Introduction to Probability, Spring 2018 - Youtube\n\n\n\\[\n\\begin{aligned}\nP(B)&=P(B\\cap A) + P(B\\cap \\overline A)\\\\\n    &=P(B\\cap A) + P(B\\cap \\overline A)\\\\\n    &=P(B|A)P(A)+P(B|\\overline A)P(\\overline A)\\\\\nP(A\\cap B)&=P(B|A)P(A)=P(A|B)P(B)\\\\\n\\therefore\nP(A|B)&=\\frac{P(A \\cap B)}{P(B)}\\\\\n      &=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\n\\end{aligned}\n\\tag{2}\\]\nLaw of total probability 를 이용하여 Bayes’ rule이 Equation 2 와 같이 변형되었다. 최종식을 보면 좀 더 직관적인 해석이 가능해지는데 P(B) 가 A와의 교집합 확률의 총합이 되면서 분자가 그 일부가 되는 비율의 개념으로 해석될 수 있다. Figure 1 를 보면 \\(P(A|B)=\\frac{P(B \\cap A)}{P(B)}=\\frac{P(B \\cap A)}{P(B \\cap A)+P(B \\cap \\overline A)}\\) 로 표현되는 것을 볼 수 있다. 그 것을 조금 더 일반화 한 경우는 Figure 2 를 참고하여 유추할 수 있다.\n\n\n\n\\[\n\\begin{aligned}\nP(A|B)&=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\\\\\nP(\\theta|x)&=\\frac{P(x|\\theta)P(\\theta)}{P(x|\\theta)P(\\theta)+P(x|\\overline \\theta)P(\\overline \\theta)}\n\\end{aligned}\n\\]\n많은 참고 문헌에서 사건 A, B를 모수, \\(\\theta\\) 와 data, \\(x\\) 로 표현하기도 한다. 즉, data \\(x\\) 가 주어졌을 때 모수 \\(\\theta\\) 가 발생할 확률이 data에 의해서 update된다.\n\n\\(P(\\theta)\\)\n\nprior probability density function\n데이터없이 초기에 임시로 부여된 모델 또는 모수의 확률\n\n\\(P(x|\\theta)\\)\n\nlikelihood\n초기에 임시로 부여된 모델 또는 모수가 주어졌을 때 data x가 발생할 우도\n좀 더 파격적으로 해석하면, 초기에 임시로 부여된 모델 또는 모수가 data x에 들어맞을(or fittng) 확률\n\n\\(P(x)\\)\n\nmarginal proability\n데이터가 발생할 확률로 \\(\\theta\\) 와 상관없기 때문에 상수로 취급한다.\n\n\\(P(\\theta|x)\\)\n\nposterior probability density function\ndata가 주어졌을 때 모델 또는 모수의 확률\nBayes’ Rule에 의한 최적화에서 다음 최적화 iteration에서 Prior로 쓰인다.\n\n\n\\(P(x)\\) 는 상수이기 때문에 생략가능 하여 아래의 식과 같이 정리 할 수 있다. \\[\nP(\\theta|x)\\propto P(x|\\theta)P(\\theta)\n\\]\n\\(P(\\theta|x)\\) 는 \\(P(x|\\theta)P(\\theta)\\) 에만 영향을 받는 것을 볼 수 있다.\n\n\n\n펭수는 평소 관심이 있던 코니에게서 초콜릿을 선물받았다. 펭수는 초콜릿을 준 코니가 나를 좋아하는지가 궁금하기 때문에 이것을 통계적으로 계산해본다.\n펭수는 먼저 다음 두 상황을 가정한다.\n\n\\(P(like)=0.5\\)\n\n코니가 펭수를 좋아한다는 가설의 신뢰도는 반 반이다. 즉, 정보없는 상태에서의 펭수의 prior probability.\n0.5로 설정한 이유는 다음의 원리를 따랐다. The Principle of Insufficient Reason(이유불충분의 원리- 하나의 사건을 기대할만한 어떤 이유가 없는 경우에는 가능한 모든 사건에 동일한 확률을 할당해야 한다는 원칙).\n\n\\(P(choco)\\)\n\n초콜릿을 받았다라는 data가 발생할 신뢰도\n\n\n펭수는 코니에게 자신을 좋아하는지 알 길이 없으니 사람이 호감이 있을 때에 대한 초콜릿 선물 데이터를 조사하기 시작한다. 즉, 호감의 근거는 초콜릿으로 한정했고 초콜릿 선물 방식의 불확실성을 호감으로 설명하는 문헌을 찾기 시작했다. 그리고 펭수는 도서관에 있는 일반인 100명을 대상으로 초콜릿과 호감과의 관계를 연구한 초콜릿과 호감 논문을 통해 두 가지 정보를 알게된다.\n\n일반적으로, 어떤 사람이 상대방에게 호감이 있어서 초콜릿을 줄 확률은 \\(40%\\) 이다. 즉, \\(P(choco|like)=0.4\\)\n일반적으로, 어떤 사람이 상대방에게 호감이 없지만 예의상 초콜릿을 줄 확률은 \\(30%\\) 이다. 즉, \\(P(choco|\\overline{like})=0.3\\)\n위의 2가지 정보로 유추 가능한 정보\n\n\\(P(\\overline{choco}|like)=0.6\\)\n\\(P(\\overline{choco}|\\overline{like})=0.7\\)\n\n초콜릿에 관한 조사를 토대로 얻은 4가지 정보로 유추할 수 있는 정보\n\n\\(P(choco|like)=0.4\\): like를 받고 있는 50명 중 \\(40%\\) 인 20명은 초콜릿을 받는다.\n\\(P(\\overline{choco}|like)=0.6\\): like를 받고 있는 50명 중 \\(60%\\) 인 30명은 초콜릿을 받지 못한다.\n\\(P(choco|\\overline{like})=0.3\\): like를 받지 않는 50명 중 \\(30%\\) 인 15명은 예의상 준 초콜릿을 받는다.\n\\(P(\\overline{choco}|\\overline{like})=0.7\\): like를 받지 않는 50명 중 \\(70%\\) 인 35명은 초콜릿을 받지 못한다.\n\n\n펭수의 관점으로 정보를 재분류\n\n펭수가 궁금한 정보\n\n\\(P(like|choco)=?\\), posterior probability\n\n펭수가 가정한 정보\n\n\\(P(like)=0.5\\), prior probability by The Principle of Insufficient Reason\n\n펭수가 조사한 정보\n\n\\(P(choco|like)=0.4\\), likelihood\n\\(P(choco)\\): marginal probability\n\n\\(P(choco)=P(choco|like)+P(choco|\\overline{like})=\\frac{20+15}{100}=0.35\\)\n\n\n\n위의 정리한 정보를 Bayes’ rule에 대입하면,\n\\[\nP(like|choco)=\\frac{P(choco|like)\\times P(like)}{P(choco)}=\\frac{0.4\\times 0.5}{0.35}=0.57\n\\]\n펭수의 prior probability(\\(P(A)=0.5\\))가 posterior probability(\\(P(A|B)=0.57\\))로 업데이트 될 수 있다. 초콜릿과 호감 논문을 읽고 코니가 자신을 좋아할 확률이 높아진 것에 대해 기대감을 얻은 용기가 없는 펭수는 100명 보다 더 많은 독립적인 사람들로 실험한 논문을 찾아 다시 자신의 업데이트 된 사전 확률을 계속해서 업데이트할 생각이다. 그리고 자신의 사전 확률을 추가적인 데이터를 갖고 사후 확률로 계속해서 업데이트시켜 정확한 확률을 구한다.\n위의 예시는 영상 자료: 초콜릿을 준 코니의 마음을 시청하고 영감을 얻은 슬기로운 통계생활 tistory에 있는 Source: 베이즈 정리(Bayes’ rule) 완벽히 정리하기 슬기로운 통계생활 블로그를 요약 및 약간의 각색을 한 것이다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#generalized-bayes-rule",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#generalized-bayes-rule",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Theorem 3 Let \\(A_1, A_2, ..., A_k\\) be a set of mutually exclusive and exhaustive events. Let \\(A\\) be a event, then\n\\[\nP(A_i|B)=\\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^{k}P(B|A_i)P(A_i)}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#maximum-a-posterior-estimation-map",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#maximum-a-posterior-estimation-map",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Maximum a posterior estimation는 statistical estimation methods의 큰 기둥 중 하나인 maximum likelihood estimation과 더불어 parameter \\(\\theta\\) 를 추정하는데 많이 사용되는 방법론이다. 사후 확률 밀도 함수 \\(f(x|\\theta)\\) 또는 \\(P(x|\\theta)\\) 를 최대화하는 \\(\\theta\\) 의 추정치를 구하는 방법이며 아래와 같은 argument로 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\hat{\\theta}&=\\arg \\max_{\\theta}\\frac{f(x|\\theta)f(\\theta)}{\\int f(x|\\theta)f(\\theta)}\\\\\n            &\\propto\\arg \\max_{\\theta}f(x|\\theta)f(\\theta)\n\\end{aligned}\n\\]\n최대 우도 추정량과 달리 최대 사후 추정량에는 최대화하는 식에 사전 확률이 추가되어 있는 것을 볼 수 있다. 그러므로 분자 부분인 \\(f(x|\\theta)f(\\theta)\\) 만을 최대화 한다. 분모 부분인 \\(\\int f(x|\\theta)f(\\theta)\\) nomarlizing penalty 또는 constant로 간주한다. 여기서 \\(P(\\theta)\\) 초기 가정치 인데 아무렇게나 설정하기 보다는 good estimate로 설정해야 통계학자들로부터의 공격을 최소화시킬 수 있다. MAP는 나이브 베이즈의 알고리즘의 핵심이다.\n[참고] 최대 우도 추정량 \\[\n\\begin{aligned}\n\\hat{\\theta}=\\arg \\max_{\\theta}L(x|\\theta)=\\arg \\max_{\\theta}\\Pi_{i=1}^{n}f(x|\\theta)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#naive-bayes-classifier",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#naive-bayes-classifier",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Naive Bayes에 대한 구체적인 글은 다른 블로그에 소개한다. Naive Bayes는 Bayes’ Rule을 이용해 \\(\\theta\\) 를 최적화 시킨다. Naive Bayes의 Naive는 features 또는 explanotry variables이 서로 conditionally indepdent라고 가정한 것에서 이름 붙여졌다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html#blog-guide-map-link",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html",
    "title": "MANOVA",
    "section": "",
    "text": "다변량 분산분석(Multivariate Analysis of Variance, MANOVA)\n\n2개 이상의 종속변수가 있을 경우 집단별 차이를 동시에 검정\n연구의 타당성 증가"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#description",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#description",
    "title": "MANOVA",
    "section": "",
    "text": "다변량 분산분석(Multivariate Analysis of Variance, MANOVA)\n\n2개 이상의 종속변수가 있을 경우 집단별 차이를 동시에 검정\n연구의 타당성 증가"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#example",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#example",
    "title": "MANOVA",
    "section": "2 Example",
    "text": "2 Example\n\n2.1 Load Libraries and Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(faraway)\nlibrary(markdown)\nlibrary(heplots)\nlibrary(HH)\nlibrary(psych)\n\n\n\n\n2.2 Data Description\n\n\nCode\nstr(Skulls)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ epoch: Ord.factor w/ 5 levels \"c4000BC\"&lt;\"c3300BC\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ mb   : num  131 125 131 119 136 138 139 125 131 134 ...\n $ bh   : num  138 131 132 132 143 137 130 136 134 134 ...\n $ bl   : num  89 92 99 96 100 89 108 93 102 99 ...\n $ nh   : num  49 48 50 44 54 56 48 48 51 51 ...\n\n\nR console에 ?Skulls를 입력하면 다음과 같은 설명이 나온다.\nMeasurements made on Egyptian skulls from five epochs.\n\nThe epochs correspond to the following periods of Egyptian history:\n\nthe early predynstic period (circa 4000 BC);\nthe late predynatic period (circa 3300 BC);\nthe 12th and 13t dynasties (circa 1850 BC);\nthe Ptolemiac peiod (circa 200 BC);\nthe Roman period(circa 150 AD).\n\n\nThe question is hether the measurements change over time. Non-constant measurements of the skulls over time would indicate interbreeding with immigrant populations. Note that using polynomial contrasts for epoch essentially treats the time points as equally spaced\n즉, skulls 고대 이집트 왕조 부터 로마시대까지 이집트 지역에서 발군된 두개골의 크기를 측정한 데이터 이집트 역사를 5개의 시대로 구분하고 각 시대별로 30개씩의 두개골을 4개의 지표로 측정\n이 data는 5개의 변수와 150개의 samples을 포함한다.\n\nepoch :\nmb :\nbh :\nbl :\nnh :"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#eda",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#eda",
    "title": "MANOVA",
    "section": "3 EDA",
    "text": "3 EDA\n\n3.1 Descriptive Statistics\n\n\nCode\nlibrary(heplots)\n#skulls 고대 이집트 왕조 부터 로마시대까지 이집트 지역에서 발군된 두개골의 크기를 측정한 데이터\n# 이집트 역사를 5개의 시대로 구분하고 각 시대별로 30개씩의 두개골을 4개의 지표로 측정\n# epoch: 이집트의 시대를 5개로 구분, 독립변수\n# mb : 두개골의 폭, 종속 변수\n# bh : 두개골의 높이, 종속 변수\n# bl : 두개골의 길이, 종속 변수\n# nh : 코의 높이, 종속 변수\n\nlibrary(dplyr)\nsample_n(Skulls,10)\n\n\n      epoch  mb  bh  bl nh\n19  c4000BC 139 136  96 50\n18  c4000BC 132 133  93 53\n111  c200BC 139 130  94 53\n36  c3300BC 135 136  98 52\n132  cAD150 141 136 101 54\n80  c1850BC 136 135  94 53\n138  cAD150 137 135  96 54\n133  cAD150 135 135  95 56\n22  c4000BC 135 135 103 47\n54  c3300BC 129 126  91 50\n\n\nCode\nattach(Skulls)# Skulls를 작업 경로에 포함시키기\nsearch() # 작업 경로 확인인\n\n\n [1] \".GlobalEnv\"           \"Skulls\"               \"package:psych\"       \n [4] \"package:HH\"           \"package:gridExtra\"    \"package:multcomp\"    \n [7] \"package:TH.data\"      \"package:MASS\"         \"package:survival\"    \n[10] \"package:mvtnorm\"      \"package:latticeExtra\" \"package:grid\"        \n[13] \"package:lattice\"      \"package:heplots\"      \"package:broom\"       \n[16] \"package:markdown\"     \"package:faraway\"      \"package:lubridate\"   \n[19] \"package:forcats\"      \"package:stringr\"      \"package:dplyr\"       \n[22] \"package:purrr\"        \"package:readr\"        \"package:tidyr\"       \n[25] \"package:tibble\"       \"package:ggplot2\"      \"package:tidyverse\"   \n[28] \"package:stats\"        \"package:graphics\"     \"package:grDevices\"   \n[31] \"package:utils\"        \"package:datasets\"     \"package:methods\"     \n[34] \"Autoloads\"            \"package:base\"        \n\n\nCode\n# 종속 변수를 결합시켜 하나의 행렬로 만들기\ny&lt;-cbind(mb,bh,bl,nh)\n# 시대별 두개골  길이의 평균 보기\naggregate(y,by=list(epoch),mean) # 언뜻 보기에 차이가 있는 것 처럼 보임\n\n\n  Group.1       mb       bh       bl       nh\n1 c4000BC 131.3667 133.6000 99.16667 50.53333\n2 c3300BC 132.3667 132.7000 99.06667 50.23333\n3 c1850BC 134.4667 133.8000 96.03333 50.56667\n4  c200BC 135.5000 132.3000 94.53333 51.96667\n5  cAD150 136.1667 130.3333 93.50000 51.36667\n\n\nCode\n# 모집단으로 일반화하기 위해 통계적 검정 시행\nskulls_manova&lt;-manova(y~epoch)\nsummary(skulls_manova)\n\n\n           Df  Pillai approx F num Df den Df    Pr(&gt;F)    \nepoch       4 0.35331    3.512     16    580 4.675e-06 ***\nResiduals 145                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# 시대별 두개골 측정값이 차이가 있는 것으로 보임\n\n# 구체적으로 어느 두개 골 측정값에서 차이가 나는지 확인\nsummary.aov(skulls_manova)\n\n\n Response mb :\n             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nepoch         4  502.83 125.707  5.9546 0.0001826 ***\nResiduals   145 3061.07  21.111                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response bh :\n             Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nepoch         4  229.9  57.477  2.4474 0.04897 *\nResiduals   145 3405.3  23.485                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response bl :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nepoch         4  803.3 200.823  8.3057 4.636e-06 ***\nResiduals   145 3506.0  24.179                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response nh :\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nepoch         4   61.2  15.300   1.507 0.2032\nResiduals   145 1472.1  10.153               \n\n\nCode\n# nh는 차이가 없는 것으로 보임\n\n## 시간에 따라 두개골 측정이 다르다는 것은 이민족 유입의 혼혈 가능성이 있음\n\ndetach(Skulls)# 작업경로에서 삭제제\n\n\n\n\n3.2 One-Way ANOVA"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#blog-guide-map-link",
    "title": "MANOVA",
    "section": "4 Blog Guide Map Link",
    "text": "4 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html",
    "href": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html",
    "title": "Conditional Probability",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n조건부 확률은 조건이 주어졌을 때 한 사건이 발생할 확률이며 하나의 사건이 다른 사건에 영향을 미쳐 그 확률값이 달라지는 것을 의미한다. 즉, 한 사건, B가 조건으로 주어지고 다른 사건A가 발생할 확률 \\(P(A|B)\\) 가 사건 A가 발생할 확률 \\(P(A)\\) 와 다르다는 것을 의미한다 (\\(P(A)\\not=P(A|B)\\)).\n예를 들어, 주사위를 던질 때 특정 주사위의 눈 (1~6)이 나올 확률은 \\(\\frac{1}{6}\\) 으로 같다 (eqaully likely)라고 가정할 때 주사위의 눈이 나올 수 있는 모든 집합 표본 공간 \\(S\\) 에 대한 특정 주사위의 눈이 나오는 사건 \\(A\\) 가 발생할 확률은 \\(\\frac{n(A)}{n(S)}\\) 와 같다. 다시 말해서, 조건부 확률은 2개 이상의 사건에 대해서 하나의 사건이 다른 사건이 발생할 확률에 영향을 미치는 개념을 말한다. 가장 간단한 2개의 사건 \\(A, B\\) 에 대해서 살펴볼 때 조건부 확률은 다음과 (Equation 1)과 같다.\n\nDefinition 1 If \\(A\\) and \\(B\\) are events in sample space \\(S\\), and \\(P(B)&gt;0\\), then the conditional probability of \\(A\\) given \\(B\\), written \\(P(A|B)\\), is \\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n\\tag{1}\\]\nwhere \\(0 &lt; P(B) \\le 1\\)\n\n위의 정의에서 볼 수 있듯이, sample space \\(S\\) 가 B로 update 되어 P(B|B)=1이 되고 사건 A의 outcome이 B에 관해서 조정된다.\n예를 들어, 사건 \\(A\\) 는 주사위의 눈이 1이 나오는 사건, 사건 \\(B\\) 는 주사위의 눈이 3 이하가 나오는 사건이라고 했을 때 사건 \\(A\\) 가 사건 \\(B\\) 의 부분 집합이므로 두 사건이 서로 독립이 아니다. 즉, $ AB $ 사건에서 주사위의 눈이 1 나오는 경우 밖에 없다. 이렇게 사건 \\(B\\) 가 주어졌을 때 혹은 \\(B\\) 가 먼저 일어났을 때 1이 나올 확률은 달라지게 된다. 즉, \\(A\\) 의 sample space = \\(\\{1,2,3,4,5,6\\}\\) 이고 \\(A|B\\) 의 sample space = \\(\\{1,2,3\\}\\) 이 되기 때문에 \\(P(A) \\not= P(A|B)\\) 가 된다.\n좀 더 구체적으로 계산을 하게 되면, \\(P(A)=\\frac{1}{6}, P(B)=\\frac{3}{6}, P(A\\cap B)=\\frac{1}{6}\\) 일 때,\n\\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}=\\frac{\\frac{1}{6}}{\\frac{3}{6}}=\\frac{1}{3}\n\\]\n인 것을 알 수 있다.\n\nDefinition 2 Independence \\[\n\\begin{aligned}\nP(A)&=P(A|B)\\\\\nP(B)&=P(B|A)\\\\\n\\end{aligned}\n\\tag{2}\\]\nA and B are independent.\n\n사건 A, B가 독립일 때 두 두사건이 동시에 발생할 확률은 \\(P(A \\cap B)=P(A)P(B)=P(B)P(A)\\) 이다. 반면에, 두 사건이 독립이 아니라면 Equation 1 을 이용하여 \\(P(A \\cap B)\\) 는 \\(P(B|A)P(A)\\) 또는 \\(P(A|B)P(B)\\) 로 표현될 수 있다.독립일 때 동시에 발생할 확률에서 \\(P(A)\\) 가 B를 조건으로 봤을 때 동시에 발생할 확률 \\(P(A|B)\\) 로 바뀐 것을 볼 수 있다.\n\nTheorem 1 Multiplicative Rule A,B dependent \\[\n\\begin{aligned}\nP(A\\cap B)&=P(A)P(B|A)\n\\end{aligned}\n\\]\nA,B independent \\[\n\\begin{aligned}\nP(A\\cap B)&=P(A)P(B)\n\\end{aligned}\n\\]\n\n\nTheorem 2 Generalized multiplicative rule $$\n$$\n\n\nTheorem 3 Total Probability Rule $$\n$$\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html#conditional-probability",
    "href": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html#conditional-probability",
    "title": "Conditional Probability",
    "section": "",
    "text": "조건부 확률은 조건이 주어졌을 때 한 사건이 발생할 확률이며 하나의 사건이 다른 사건에 영향을 미쳐 그 확률값이 달라지는 것을 의미한다. 즉, 한 사건, B가 조건으로 주어지고 다른 사건A가 발생할 확률 \\(P(A|B)\\) 가 사건 A가 발생할 확률 \\(P(A)\\) 와 다르다는 것을 의미한다 (\\(P(A)\\not=P(A|B)\\)).\n예를 들어, 주사위를 던질 때 특정 주사위의 눈 (1~6)이 나올 확률은 \\(\\frac{1}{6}\\) 으로 같다 (eqaully likely)라고 가정할 때 주사위의 눈이 나올 수 있는 모든 집합 표본 공간 \\(S\\) 에 대한 특정 주사위의 눈이 나오는 사건 \\(A\\) 가 발생할 확률은 \\(\\frac{n(A)}{n(S)}\\) 와 같다. 다시 말해서, 조건부 확률은 2개 이상의 사건에 대해서 하나의 사건이 다른 사건이 발생할 확률에 영향을 미치는 개념을 말한다. 가장 간단한 2개의 사건 \\(A, B\\) 에 대해서 살펴볼 때 조건부 확률은 다음과 (Equation 1)과 같다.\n\nDefinition 1 If \\(A\\) and \\(B\\) are events in sample space \\(S\\), and \\(P(B)&gt;0\\), then the conditional probability of \\(A\\) given \\(B\\), written \\(P(A|B)\\), is \\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n\\tag{1}\\]\nwhere \\(0 &lt; P(B) \\le 1\\)\n\n위의 정의에서 볼 수 있듯이, sample space \\(S\\) 가 B로 update 되어 P(B|B)=1이 되고 사건 A의 outcome이 B에 관해서 조정된다.\n예를 들어, 사건 \\(A\\) 는 주사위의 눈이 1이 나오는 사건, 사건 \\(B\\) 는 주사위의 눈이 3 이하가 나오는 사건이라고 했을 때 사건 \\(A\\) 가 사건 \\(B\\) 의 부분 집합이므로 두 사건이 서로 독립이 아니다. 즉, $ AB $ 사건에서 주사위의 눈이 1 나오는 경우 밖에 없다. 이렇게 사건 \\(B\\) 가 주어졌을 때 혹은 \\(B\\) 가 먼저 일어났을 때 1이 나올 확률은 달라지게 된다. 즉, \\(A\\) 의 sample space = \\(\\{1,2,3,4,5,6\\}\\) 이고 \\(A|B\\) 의 sample space = \\(\\{1,2,3\\}\\) 이 되기 때문에 \\(P(A) \\not= P(A|B)\\) 가 된다.\n좀 더 구체적으로 계산을 하게 되면, \\(P(A)=\\frac{1}{6}, P(B)=\\frac{3}{6}, P(A\\cap B)=\\frac{1}{6}\\) 일 때,\n\\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}=\\frac{\\frac{1}{6}}{\\frac{3}{6}}=\\frac{1}{3}\n\\]\n인 것을 알 수 있다.\n\nDefinition 2 Independence \\[\n\\begin{aligned}\nP(A)&=P(A|B)\\\\\nP(B)&=P(B|A)\\\\\n\\end{aligned}\n\\tag{2}\\]\nA and B are independent.\n\n사건 A, B가 독립일 때 두 두사건이 동시에 발생할 확률은 \\(P(A \\cap B)=P(A)P(B)=P(B)P(A)\\) 이다. 반면에, 두 사건이 독립이 아니라면 Equation 1 을 이용하여 \\(P(A \\cap B)\\) 는 \\(P(B|A)P(A)\\) 또는 \\(P(A|B)P(B)\\) 로 표현될 수 있다.독립일 때 동시에 발생할 확률에서 \\(P(A)\\) 가 B를 조건으로 봤을 때 동시에 발생할 확률 \\(P(A|B)\\) 로 바뀐 것을 볼 수 있다.\n\nTheorem 1 Multiplicative Rule A,B dependent \\[\n\\begin{aligned}\nP(A\\cap B)&=P(A)P(B|A)\n\\end{aligned}\n\\]\nA,B independent \\[\n\\begin{aligned}\nP(A\\cap B)&=P(A)P(B)\n\\end{aligned}\n\\]\n\n\nTheorem 2 Generalized multiplicative rule $$\n$$\n\n\nTheorem 3 Total Probability Rule $$\n$$"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html#blog-guide-map-link",
    "title": "Conditional Probability",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-21_transformation/index.html",
    "href": "docs/blog/posts/statistics/2023-02-21_transformation/index.html",
    "title": "Transformation of Random Variables",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n확률 변수 2개 이상에 대한 확률 분포를 joint probability distribution (결합확률분포)라고 하는데 두 확률 변수 \\(X\\) 와 \\(Y\\) 의 관계에 대해서 규명해야 할 때가 있다. 예를 들어, X 와 Y의 높은 상관계수라든지 또는 비선형적인 관계가 관찰될 때 그 관계가 수리적으로 모델링이 가능하고 한 확률 변수의 분포에 대한 정보를 알고있다면 미지의 다른 확률 변수의 분포가 추정가능해진다. 이 때 두 변수에 대한 관계 정도가 높으면 높을수록 추정이 쉬워진다.\n이번 블로그에서는 주어진 확률 변수 \\(X\\) 에 대해서 \\(X\\) 의 pmf (probability mass function) 또는 pdf (probability density function) \\(f_x(x)\\) 를 알고있을 때 확률 변수에 \\(X\\) 에 적절한 함수의 변환을 적용해 확률 변수 \\(Y\\) 를 \\(Y=u(X)\\) 라는 관계식이 정의 가능할 때 \\(Y\\) 의 pmf 또는 pdf를 구하는 방법에 집중한다. 후에 MGF (Momment Generating Function) 학습에 응용될 수 있는 개념으로 잘 정리할 필요가 있다.\n\n\n\n\nTheorem 1 Discrete random variable \\(X\\) 의 probability distribution이 \\(f_X(x)\\) 이고 \\(X\\) 와 \\(Y\\) 사이에는 \\(Y=u(X)\\) 라는 one-to-one relation이 성립될 때 \\(y=u(x)\\) 를 유일한 \\(x\\) 를 \\(y\\) 에 대한 함수인 \\(x=w(y)\\) 로 표현 가능하다면 \\(Y\\) 의 probability distribution는 \\[\nf_Y(y)=f_X(w(y))\n\\] 이다.\n\n\n\n\nIn the case of a One-to-One relation\n\n동전을 독립적으로 2번 던질 때, 확률 변수 \\(X\\) 가 앞면이 나오는 수라고 정의했을 때, 확률 분포 아래 표 (a)와 같다. \\(Y=2X+1\\) 라는 관계가 성립할 때 \\(Y\\) 의 분포는 아래 표 (b)와 같다.\n\n\nTable 1: Exmaple: Transformation of Discrete Random Variable (One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\\(X\\)\n0\n1\n2\n\n\n\n\n\\(P_X(X=x)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y=2X+1\\)\n\n\n\\(Y=2X+1\\)\n1\n3\n5\n\n\n\n\n\\(P_Y(Y=y)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n\nIn the case of not a One-to-One relation\n\n동전을 독립적으로 2번 던질 때, 확률 변수 \\(X\\) 가 앞면이 나오는 수의 합이라고 정의 했을때, \\(X\\) 의 확률 분포는 아래 표 (a) 와 같다. 이 때 \\(Y=mod(X,2)\\) 라는 관계가 성립할 때 \\(Y\\) 의 분포는 아래 표 (b) 같다.\n\n\nTable 2: Exmaple: Transformation of Discrete Random Variable (Not One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\\(X\\)\n0\n1\n2\n\n\n\n\n\\(P_X(X=x)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y=mod(x,2)\\)\n\n\n\\(Y=mod(x,2)\\)\n0\n1\n\n\n\n\n\\(P_Y(Y=y)\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{2}{4}\\)\n\n\n\n\n\n\n위의 예시와 같이 두 확률 변수가 one to one 관계일 때는 확률분포가 그대로 유지되어서 쉽게 변환된 확률 변수의 분포가 추정가능하지만 one to one 관계가 아닐 경우 확률 분포가 바뀌게 된다.\n\nanother example (In the case of not a One-to-One relation)\n\n기하분포 (Geometric Distribution)란 동일한 베르누이 (Bernoulli) 분포의 시행을 독립적으로 반복할 때 첫 성공까지의 시행 횟수를 확률변수 \\(X\\) 로 하는 분포이다. 즉, \\(x-1\\) 번째까지 베르누이 시행이 실패하고 \\(x\\) 번째 시행에서 성공할 확률 분포를 말한다.\nNotation은 \\(X \\sim Geometric(p)\\) 또는 \\(X \\sim Geo(p)\\) 라고 표현하고, \\(p\\) 는 독립 시행에서 성공할 확률이다. (참고: \\(text{E}(X)=\\frac{1}{p}\\), \\(\\text{Var}(X)=\\frac{1-p}{p^2}\\))\n\\(X \\sim Geometric(\\frac{4}{5})\\) 일 때 \\(X\\) 의 확률분포 \\(f(x)=\\frac{4}{5}(\\frac{1}{5})^{(x-1)}\\), \\(x=1,2,3 ...\\) 가 기하분포일 때 \\(Y=X^2\\) 의 확률분포는\n\\[\n\\begin{aligned}\n  y&=u(x)=x^2 \\\\\n  x&=w(y)=\\sqrt{y} \\text{ } (x&gt;0)\\\\\n  f_Y(y)&=f_X(w(y))=f_X(\\sqrt{y})=\\frac{4}{5}(\\frac{1}{5}^{(\\sqrt{y}-1)})\\\\\n  \\therefore f_Y(y)&=\n  \\begin{cases}\n    \\frac{4}{5}(\\frac{1}{5}^{(\\sqrt{y}-1)}) &\\text{if} \\text{  } y =1, 4, 9, ...\\\\\n     0 \\text{  } & \\text{otherwise}\n    \\end{cases}\n\\end{aligned}\n\\]\n이다.\n\n\n\n\n\nTheorem 2 Two discrete random variables \\(X_1\\) 과 \\(X_2\\) 의 joint probability distribution이 \\(f(x_1,x_2)\\) 이고 \\((x_1,x_2)\\) 와 \\((y_1,y_2)\\) 가 one-to-one relation이 성립되어 \\(y_1=u_1(x_1,x_2)\\) 와 \\(y_2=u_2(x_1,x_2)\\) 를 유일한 \\(x_1=w_1(y_1,y_2)\\) 와 \\(x_2=w_2(y_1,y_2)\\) 의 함수로 표현 가능하다면 새로운 확률변수 \\(\\mathbf{Y}\\), 즉, \\(Y_1\\) 과 \\(Y_2\\) 의 joint probability distribution는 \\[\nf_Y(y_1,y_2)=f_X(w_1(y_1,y_2),w_2(y_1,y_2))\n\\] 이다.\n\n\n\n\nexmaple 1\n\n다항 분포(multinomial distribution)란 2개 이상의 독립적인 확률 변수 \\(\\mathbf{X}=X_1, X_2, ...\\) 들에 대한 확률분포로, 여러 독립 시행에서 각 각의 값이 특정 횟수가 나타날 확률을 정의하고 독립 변수가 2개인 경우 다항 분포의 특별한 case로 이항 분포 (binomial distribution)가 된다.\n참고( Source: wiki) :\n\\[\n\\begin{aligned}\n  f_(x) & =\\frac{n!}{x_1!x_2!\\dots x_k!}p_1^{x_1}p_2^{x_2}\\dots p_k^{x_k}\\\\\n  \\text{E}(x)&=np_i\\\\\n  \\text{Var}(x)&=np_i(1-p_i)\n\\end{aligned}\n\\]\n\\(X_1\\) and \\(X_2\\) 가 다음과 같은 multinomial distribution을 따를 때 \\[\nf(x_1,x_2) = \\binom{2}{x_1,x_2,2-x_1-x_2}\\frac{1}{4}^{x_1}\\frac{1}{3}^{x_2}\\frac{5}{12}^{2-x_1-x_2} x_1=0,1,2, \\space x_2=0,1,2, \\space x_1+x_2\\le 2\n\\]\n\\(Y_1=X_1+X_2\\) 와 \\(Y_2=X_1-X_2\\) 의 결합 확률 분포는\n\\[\n\\begin{aligned}\n  y_1&=u_1(x_1,x_2)=x_1+x_2\\\\\n  y_2&=u_2(x_1,x_2)=x_1-x_2\\\\\n  x_1&=w_1(y_1,y_2)=\\frac{y_1+y_2}{2}\\\\\n  x_2&=w_2(y_1,y_2)=\\frac{y_1-y_2}{2}\\\\\n  f_Y(y_1,y_2)&=\\binom{2}{\\frac{y_1+y_2}{2},\\frac{y_1-y_2}{2},2-y_1 }\\frac{1}{4}^{\\frac{y_1+y_2}{2}}\\frac{5}{12}^{2-y_1} y_1=0,1,2, \\space y_2=-2,-1,0,1,2\n\\end{aligned}\n\\] 이다.\n\nexmaple 2\n\n\\(X_1\\) 과 \\(X_2\\) 의 결합확률분포가 다음과 같을 때 변환된 \\(Y_1=X_1+X_2\\) 와 \\(Y_2=X_1X_2\\) 의 결합확률 분포는?\n\n\nTable 3: Exmaple: Transformation of Joint Discrete Random Variable (Not One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\n\n\n\n\n\n\n\\((x_1,x_2)\\)\n(0, 0)\n(0, 1)\n(1, 0)\n(1, 1)\n\n\n\n\n\\(f_X(x_1,x_2)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y\\)\n\n\n\\((y_1,y_2)\\)\n(0, 0)\n(1, 0)\n(2, 1)\n\n\n\n\n\n\\(f_Y(y_1,y_2)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 3 continuous random variable \\(X\\) 의 probability distribution이 \\(f_X(x)\\) 이고 \\(X\\) 와 \\(Y\\) 사이에는 \\(Y=u(X)\\) 라는 one-to-one relation이 성립될 때 \\(y=u(x)\\) 를 유일한 \\(x\\) 를 \\(y\\) 에 대한 함수인 \\(x=w(y)\\) 로 표현 가능할 때 Y의 확률 분포는 \\[\nf_Y(y)=f_X(w(y))|J|\n\\] 이다. (\\(J=w'(y)\\), called ‘Jacobian’)\n\n\n\n\nIn the case of a One-to-One relation\n\n지수분포(exponential distribution)는 연속 확률 분포 중 하나로, 사건이 서로 독립적일 때, 일정 시간동안 발생하는 사건의 횟수가 푸아송 분포를 따른다면, 다음 사건이 발생할 때까지의 대기 시간을 확률 변수 \\(X \\in [0, \\infty)\\) 로 하는 분포이다. (Source: Wiki).\n참고: \\[\n\\begin{aligned}\n  X&\\sim Exp(\\lambda) \\\\\n  \\text E[X] &= \\frac{1}{\\lambda}\\\\\n  \\text{Var}[X] &= \\frac{1}{\\lambda^2}\\\\\n  f(x;\\lambda)&=\n  \\begin{cases}\n    \\lambda e^{-\\lambda x} & x \\ge 0\\\\\n    0 & x&lt;0\n  \\end{cases}\\\\\n\\end{aligned}\n\\] \\(\\lambda&gt;0\\) is the parameter of the distribution, called ‘rate parameter’.\nTransformation of Linear Function. When two random variables \\(X \\sim \\text{exp}(\\lambda)\\) and \\(Y\\) has the relation, \\(Y=aX+b\\) \\((a\\ne 0)\\). When \\(f_X(x)=\\lambda e^{-\\lambda x}\\), \\(f_Y(y)\\) is\nBy Theorem\n\\[\n\\begin{aligned}\n  Y&=u(X)=aX+b\\\\\n  y&=u(x)=ax+b\\\\\n  x&=w(y)=\\frac{y-b}{a}\\\\\n  f_Y(y)&=f_X(w(y))|J|=f_X(\\frac{y-b}{a})|J|\\\\\n        &=f_X(\\frac{y-b}{a})|w'(y)|\\\\\n        &=f_X(\\frac{y-b}{a})|\\frac{1}{a}|\\\\\n        &=\\frac{f_X(\\frac{y-b}{a})}{|a|}\\\\\n\\end{aligned}\n\\]\nBy Definition\n\n\n\n\\(a&gt;0\\) \\[\n\\begin{aligned}\nF_Y(Y)&=P_Y(Y\\le y)\\\\\n      &=P_Y(aX+b\\le y)\\\\\n      &=P_Y(X\\le \\frac{y-b}{a})\\\\\n      &=F_X(\\frac{y-b}{a})\\\\\nF'_Y(Y)&=\\frac{d}{dy}F_X(\\frac{y-b}{a})\\\\\n       &=f_X(\\frac{y-b}{a})\\frac{1}{a}\\\\\n\\end{aligned}\n\\]\n\n\n\n\\(a\\le 0\\) \\[\n\\begin{aligned}\nF_Y(Y)&=P_Y(Y\\le y)\\\\\n      &=P_Y(aX+b\\le y)\\\\\n      &=P_Y(X&gt; \\frac{y-b}{a})\\\\\n      &=1-F_X(\\frac{y-b}{a})\\\\\nF'_Y(Y)&=\\frac{d}{dy}(1-F_X(\\frac{y-b}{a}))\\\\\n       &=-f_X(\\frac{y-b}{a})\\frac{1}{a}\\\\  \n\\end{aligned}\n\\]\n\n\\[\n\\therefore f_Y(y)=\\frac{f_X(\\frac{y-b}{a})}{|a|}=\\frac{\\lambda exp(-\\lambda (\\frac{y-b}{a}))}{|a|}\n\\]\n\n위의 정리를 유도를 하진 않았지만 증명을 해보면 사실, \\(|J|\\) 는 역함수의 미분의 계수인 것을 할 수 있다. 위의 예시를 By definition 으로 푼 것을 보면 \\(y=u(x)\\) 와 \\(x=w(y)\\) 의 관계인 것을 알 수 있고 \\(|J|\\) 에 해당되는 \\(\\frac{1}{a}\\) 는 CDF \\(F(Y)\\) 의 derivative를 구하는 과정에서 발생하는 chain rule에 의해 생긴 것을 알 수 있다.\n\\[\n\\begin{aligned}\n  y&=u(x)\\\\\n  \\frac{dy}{dx}&=u'(x)\\\\\n  x&=w(y)=u^{-1}(y)\\\\\n  \\frac{dx}{dy}&=w'(y) \\\\\n  w'(y)&=\\frac{dx}{dy}=(\\frac{dy}{dx})^{-1}=\\frac{1}{u'(x)}\n\\end{aligned}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 20, 1000)\ny = 2*x+1\ny2 = x/2+1\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,x,color='black',label=r'$y=x$', linestyle='dashed')\nax.plot(x,2*x-2,color='black',label=r'$x=w(y) \\rightarrow y=2x-2 $')\nax.plot(x,y2,color='red',label=r'$y=u(x)=\\frac{1}{2}x+1$')\nax.set_aspect(1)\nax.set_title(r\"Example of Relation of X and Y, $y=u(x)$ vs $x=w(y)$\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nax.set_xlim(-20, 40)\n\nplt.legend(shadow=True, loc=(-0.5, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 20, 1000)\ny = 2*x+1\ny2 = x/2+1\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,y,color='black',label=r'$y=2x+1$')\nax.plot(x,y2,color='red',label=r'$y=\\frac{1}{2}x+1$')\nax.set_aspect(1)\nax.set_title(r\"Transformation of Linear Function, $f_Y(y)=\\frac{f_X(\\frac{y-b}{a})}{|a|}$\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y=u(X)\")\nax.set_xlim(-20, 40)\nax.text(10, 0,'|', color='black', horizontalalignment='center', verticalalignment='center')\nax.text(10, -5, r'$\\frac{y-b}{a}$', horizontalalignment='center',color='black')\nax.text(0, 5,'--', color='black', horizontalalignment='center', verticalalignment='center')\nax.text(-9, 5, r'$ax+b$', verticalalignment='center',color='black')\n\nplt.legend(shadow=True, loc=(-0.5, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\\(P(Y\\le y)=P(X\\le \\frac{y-b}{a})\\) 이기 때문에 \\(X\\) 의 특정 구간의 확률과 대응되는 \\(Y\\) 의 구간의 확률이 같다고 했을 때 \\(Y \\le y\\) 의 범위는 \\(X\\) 를 \\(b\\) 만큼 이동한 후 \\(a\\) 배한 \\(x\\) 좌표 이하 구간에 대응 된다 즉, \\(X\\le \\frac{y-b}{a}\\). 이 때, 두 구간에서의 확률 밀도가 같아야 하기 때문에 \\(f_x(x)\\) 가 \\(f_Y(y)\\) 의 \\(\\frac{1}{a}\\) 배 인 것을 알 수 있다.\n위의 그림에서 처럼, 두 확률 변수 \\(X\\) 와 \\(Y\\) 의 관계를 \\(y = 2x+1\\) \\((a&gt;1)\\) 와 \\(y = \\frac{1}{2}x+1\\) \\((0 \\le a\\le 1)\\) 로 두 가지의 예로 들면, 위의 그래프의 경우, \\(y = 2x+1\\) \\((a&gt;1)\\) 일 때 \\(0&lt;X&lt;10\\) 는 \\(1&lt;Y&lt;21\\) 의 구간이 2배가 되는 것을 관찰 할 수 있다. 반대로, \\(y = \\frac{1}{2}x+1\\) \\((0 \\le a\\le 1)\\) 일 때 \\(0&lt;X&lt;10\\) 는 \\(1&lt;Y&lt;6\\) 의 구간이 \\(\\frac{1}{2}\\) 배가 된다. 이 때 각 예시의 경우에 \\(X\\) 와 \\(Y\\) 의 확률 값이 같기 때문에 (\\(P(Y\\le y)=P(X\\le \\frac{y-1}{2})\\) 와 \\(P(Y\\le y)=P(X\\le \\frac{y-1}{\\frac{1}{2}})\\)). \\(f_x(x)\\) 가 \\(f_Y(y)\\) 의 각 각 \\(2\\) 배와 \\(\\frac{1}{2}\\) 배 인 것을 추정 할 수 있다.\n\nIn the case of not a One-to-One relation\n\n\\(Y=X^2\\) 일 때,\n\\[\n\\begin{aligned}\n  F_Y(Y)&=P_Y(Y\\le y)\\\\\n        &=P_Y(X^2\\le y)\\\\\n        &=P_Y(-\\sqrt{y}\\le X \\le \\sqrt y)\\\\\n        &=F_X(\\sqrt{y})-F_X(-\\sqrt{y})\\\\\n  F'_Y(Y)&=\\frac{d}{dy}(F_X(\\sqrt{y})-F_X(-\\sqrt{y}))\\\\\n        &=f_X(\\sqrt{y})\\frac{1}{2\\sqrt{y}}+f_X(-\\sqrt{y})\\frac{1}{2\\sqrt{y}}\\\\\n        &=\\frac{f_X(\\sqrt{y})}{2\\sqrt{y}}+\\frac{f_X(-\\sqrt{y})}{2\\sqrt{y}}\\\\\n\\end{aligned}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 1000)\ny = x*x\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,y,color='black',label=r'$y=x^2$')\n#ax.text(3.0,0.5,r'$\\sqrt{y}$')\n#ax.text(-4,0.5,r'$-\\sqrt{y}$')\nax.set_xlim([-10, 10])\nax.set_ylim([0, 10])\n\nax.set_aspect(1)\nax.set_title(r\"Transformation of Random Variables, $y=x^2$\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\n\nplt.legend(shadow=True, loc=(-0.2, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\\(X\\) 의 분포를 알고 \\(X\\) 와 \\(Y\\) 의 relation을 알고있을 때 \\(X\\) 의 분포로부터 \\(Y\\) 의 분포를 추정 가능한 것의 이점 중 하나는 \\(Y\\) 의 통계량을 \\(Y\\) 의 분포를 사용하지 않고 계산해낼 수 있다는 점이다.\n예를 들어, \\(X \\sim N(0,1)\\) 이고 \\(Y=u(X)\\) 라는 관계를 알고있을 때, \\(Y\\) 의 통계량 \\(E(Y)\\), \\(Var(Y)\\) 를 알고싶다면 굳이 힘들게 \\(Y\\) 의 분포를 계산할 필요가 없다. 이미 \\(X\\) 의 확률 분포 \\(f_X(x)\\) 를 알고있기 때문에 아래와 같이 \\(Y\\) 의 통계량을 계산해낼 수 있다.\n\\[\n\\begin{aligned}\n  \\text{E}(Y)&=\\int Y f_Y(y) dy\\\\\n            &=\\int u(X) f_X(x) dx\\\\\n  \\text{E}(Y^2)&=\\int Y^2 f_Y(y) dy\\\\\n            &=\\int u(X)^2 f_X(x) dx\\\\\n  \\text{Var}(Y)&=\\text{E}(Y^2)-\\text{E}(Y)^2\\\\\n  &=\\text{E}(g(X)^2)-\\text{E}(g(X))^2\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\nTheorem 4 Two continuous random variable \\(X_1\\) 과 \\(X_2\\) 의 joint probability distribution이 \\(f(x_1,x_2)\\) 이고 \\((x_1,x_2)\\) 와 \\((y_1,y_2)\\) 가 one-to-one relation이 성립될 때 \\(y_1=u_1(x_1,x_2)\\) 와 \\(y_2=u_2(x_1,x_2)\\) 를 유일한 \\(x_1=w_1(y_1,y_2)\\) 와 \\(x_2=w_2(y_1,y_2)\\) 의 함수로 표현 가능할 때 \\(Y_1=u_1(X_1,X_2)\\) 와 \\(Y_2=u_2(X_1,X_2)\\) 로 정의되는 새로운 확률변수 \\(Y_1\\) 과 \\(Y_2\\) 의 joint probability distribution는 \\[\ng(y_1.y_2)=f(w_1(y_1,y_2),w_2(y_1,y_2)) |J|\n\\] 이다.\n여기서, \\[\n\\begin{aligned}\n  |J|=|\n  \\begin{bmatrix}\n  \\frac{\\partial x_1}{\\partial y_1}&\\frac{\\partial x_1}{\\partial y_2}\\\\\n  \\frac{\\partial x_2}{\\partial y_1}&\\frac{\\partial x_2}{\\partial y_2}\\\\\n  \\end{bmatrix}| = | \\begin{bmatrix}\n  \\frac{\\partial w_1(y_1,y_2)}{\\partial y_1}&\\frac{\\partial w_1(y_1,y_2)}{\\partial y_2}\\\\\n  \\frac{\\partial w_2(y_1,y_2)}{\\partial y_1}&\\frac{\\partial w_2(y_1,y_2)}{\\partial y_2}\\\\\n  \\end{bmatrix} |\n\\end{aligned}\n\\] called ‘the determinant of jacobian matrix’\n\n\n\n\n예제\n\n연속확률변수 \\(X_1\\) 과 \\(X_2\\) 는 결합확률분포 \\[\nf(x_1,x_2)=\n  \\begin{cases}\n    4x_1x_2, & \\text{if  } 0&lt;x_1&lt;1,& 0&lt; x_2 &lt;1 \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\] 일 때, \\(Y_1=X_1^2\\) 과 \\(Y_2=X_1X_2\\) 의 결합확률 분포는\n\\[\n\\begin{aligned}\ny_1&=u_1(x_1,x_2)= x_1^2\\\\\ny_2&=u_2(x_1,x_2)= x_1x_2\\\\\nx_1&=w_1(y_1,y_2)= \\sqrt{y_1} & (y_1&gt;0)\\\\\nx_2&=w_2(y_1,y_2)= \\frac{y_2}{x_1} = \\frac{y_2}{\\sqrt{y_1}}  & (y_1&gt;0)\\\\\ng(y_1.y_2)&=f(w_1(y_1,y_2),w_2(y_1,y_2))|J|\\\\\ng(y_1.y_2)&=f(w_1(y_1,y_2),w_2(y_1,y_2))|J|\\\\\n          &=4\\sqrt{y_1}\\frac{y_2}{\\sqrt{y_1}}\\frac{1}{2y_1}\\\\\n          &=\\begin{cases}\n          \\frac{2y_2}{y_1} & \\text{if  } y_2^2&lt;y_1&lt;1, \\text{  } 0&lt;y_2&lt;1\\\\\n          0 & \\text{otherwise}\n          \\end{cases}\\\\\n          |J|&=|\n          \\begin{bmatrix}\n          \\frac{\\partial x_1}{\\partial y_1}&\\frac{\\partial x_1}{\\partial y_2}\\\\\n          \\frac{\\partial x_2}{\\partial y_1}&\\frac{\\partial x_2}{\\partial y_2}\\\\\n          \\end{bmatrix}|\\\\\n           &=|\n          \\begin{bmatrix}\n          \\frac{1}{2\\sqrt{y_1}}&0\\\\\n          y_2(-\\frac{1}{2}y_1^{\\frac{-3}{2}})&\\frac{1}{\\sqrt{y_1}}\\\\\n          \\end{bmatrix}|\\\\\n          &=\\frac{1}{2y_1}\n\\end{aligned}\n\\]\n이다.\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-21_transformation/index.html#trnasformation-of-random-variables",
    "href": "docs/blog/posts/statistics/2023-02-21_transformation/index.html#trnasformation-of-random-variables",
    "title": "Transformation of Random Variables",
    "section": "",
    "text": "확률 변수 2개 이상에 대한 확률 분포를 joint probability distribution (결합확률분포)라고 하는데 두 확률 변수 \\(X\\) 와 \\(Y\\) 의 관계에 대해서 규명해야 할 때가 있다. 예를 들어, X 와 Y의 높은 상관계수라든지 또는 비선형적인 관계가 관찰될 때 그 관계가 수리적으로 모델링이 가능하고 한 확률 변수의 분포에 대한 정보를 알고있다면 미지의 다른 확률 변수의 분포가 추정가능해진다. 이 때 두 변수에 대한 관계 정도가 높으면 높을수록 추정이 쉬워진다.\n이번 블로그에서는 주어진 확률 변수 \\(X\\) 에 대해서 \\(X\\) 의 pmf (probability mass function) 또는 pdf (probability density function) \\(f_x(x)\\) 를 알고있을 때 확률 변수에 \\(X\\) 에 적절한 함수의 변환을 적용해 확률 변수 \\(Y\\) 를 \\(Y=u(X)\\) 라는 관계식이 정의 가능할 때 \\(Y\\) 의 pmf 또는 pdf를 구하는 방법에 집중한다. 후에 MGF (Momment Generating Function) 학습에 응용될 수 있는 개념으로 잘 정리할 필요가 있다.\n\n\n\n\nTheorem 1 Discrete random variable \\(X\\) 의 probability distribution이 \\(f_X(x)\\) 이고 \\(X\\) 와 \\(Y\\) 사이에는 \\(Y=u(X)\\) 라는 one-to-one relation이 성립될 때 \\(y=u(x)\\) 를 유일한 \\(x\\) 를 \\(y\\) 에 대한 함수인 \\(x=w(y)\\) 로 표현 가능하다면 \\(Y\\) 의 probability distribution는 \\[\nf_Y(y)=f_X(w(y))\n\\] 이다.\n\n\n\n\nIn the case of a One-to-One relation\n\n동전을 독립적으로 2번 던질 때, 확률 변수 \\(X\\) 가 앞면이 나오는 수라고 정의했을 때, 확률 분포 아래 표 (a)와 같다. \\(Y=2X+1\\) 라는 관계가 성립할 때 \\(Y\\) 의 분포는 아래 표 (b)와 같다.\n\n\nTable 1: Exmaple: Transformation of Discrete Random Variable (One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\\(X\\)\n0\n1\n2\n\n\n\n\n\\(P_X(X=x)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y=2X+1\\)\n\n\n\\(Y=2X+1\\)\n1\n3\n5\n\n\n\n\n\\(P_Y(Y=y)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n\nIn the case of not a One-to-One relation\n\n동전을 독립적으로 2번 던질 때, 확률 변수 \\(X\\) 가 앞면이 나오는 수의 합이라고 정의 했을때, \\(X\\) 의 확률 분포는 아래 표 (a) 와 같다. 이 때 \\(Y=mod(X,2)\\) 라는 관계가 성립할 때 \\(Y\\) 의 분포는 아래 표 (b) 같다.\n\n\nTable 2: Exmaple: Transformation of Discrete Random Variable (Not One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\\(X\\)\n0\n1\n2\n\n\n\n\n\\(P_X(X=x)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y=mod(x,2)\\)\n\n\n\\(Y=mod(x,2)\\)\n0\n1\n\n\n\n\n\\(P_Y(Y=y)\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{2}{4}\\)\n\n\n\n\n\n\n위의 예시와 같이 두 확률 변수가 one to one 관계일 때는 확률분포가 그대로 유지되어서 쉽게 변환된 확률 변수의 분포가 추정가능하지만 one to one 관계가 아닐 경우 확률 분포가 바뀌게 된다.\n\nanother example (In the case of not a One-to-One relation)\n\n기하분포 (Geometric Distribution)란 동일한 베르누이 (Bernoulli) 분포의 시행을 독립적으로 반복할 때 첫 성공까지의 시행 횟수를 확률변수 \\(X\\) 로 하는 분포이다. 즉, \\(x-1\\) 번째까지 베르누이 시행이 실패하고 \\(x\\) 번째 시행에서 성공할 확률 분포를 말한다.\nNotation은 \\(X \\sim Geometric(p)\\) 또는 \\(X \\sim Geo(p)\\) 라고 표현하고, \\(p\\) 는 독립 시행에서 성공할 확률이다. (참고: \\(text{E}(X)=\\frac{1}{p}\\), \\(\\text{Var}(X)=\\frac{1-p}{p^2}\\))\n\\(X \\sim Geometric(\\frac{4}{5})\\) 일 때 \\(X\\) 의 확률분포 \\(f(x)=\\frac{4}{5}(\\frac{1}{5})^{(x-1)}\\), \\(x=1,2,3 ...\\) 가 기하분포일 때 \\(Y=X^2\\) 의 확률분포는\n\\[\n\\begin{aligned}\n  y&=u(x)=x^2 \\\\\n  x&=w(y)=\\sqrt{y} \\text{ } (x&gt;0)\\\\\n  f_Y(y)&=f_X(w(y))=f_X(\\sqrt{y})=\\frac{4}{5}(\\frac{1}{5}^{(\\sqrt{y}-1)})\\\\\n  \\therefore f_Y(y)&=\n  \\begin{cases}\n    \\frac{4}{5}(\\frac{1}{5}^{(\\sqrt{y}-1)}) &\\text{if} \\text{  } y =1, 4, 9, ...\\\\\n     0 \\text{  } & \\text{otherwise}\n    \\end{cases}\n\\end{aligned}\n\\]\n이다.\n\n\n\n\n\nTheorem 2 Two discrete random variables \\(X_1\\) 과 \\(X_2\\) 의 joint probability distribution이 \\(f(x_1,x_2)\\) 이고 \\((x_1,x_2)\\) 와 \\((y_1,y_2)\\) 가 one-to-one relation이 성립되어 \\(y_1=u_1(x_1,x_2)\\) 와 \\(y_2=u_2(x_1,x_2)\\) 를 유일한 \\(x_1=w_1(y_1,y_2)\\) 와 \\(x_2=w_2(y_1,y_2)\\) 의 함수로 표현 가능하다면 새로운 확률변수 \\(\\mathbf{Y}\\), 즉, \\(Y_1\\) 과 \\(Y_2\\) 의 joint probability distribution는 \\[\nf_Y(y_1,y_2)=f_X(w_1(y_1,y_2),w_2(y_1,y_2))\n\\] 이다.\n\n\n\n\nexmaple 1\n\n다항 분포(multinomial distribution)란 2개 이상의 독립적인 확률 변수 \\(\\mathbf{X}=X_1, X_2, ...\\) 들에 대한 확률분포로, 여러 독립 시행에서 각 각의 값이 특정 횟수가 나타날 확률을 정의하고 독립 변수가 2개인 경우 다항 분포의 특별한 case로 이항 분포 (binomial distribution)가 된다.\n참고( Source: wiki) :\n\\[\n\\begin{aligned}\n  f_(x) & =\\frac{n!}{x_1!x_2!\\dots x_k!}p_1^{x_1}p_2^{x_2}\\dots p_k^{x_k}\\\\\n  \\text{E}(x)&=np_i\\\\\n  \\text{Var}(x)&=np_i(1-p_i)\n\\end{aligned}\n\\]\n\\(X_1\\) and \\(X_2\\) 가 다음과 같은 multinomial distribution을 따를 때 \\[\nf(x_1,x_2) = \\binom{2}{x_1,x_2,2-x_1-x_2}\\frac{1}{4}^{x_1}\\frac{1}{3}^{x_2}\\frac{5}{12}^{2-x_1-x_2} x_1=0,1,2, \\space x_2=0,1,2, \\space x_1+x_2\\le 2\n\\]\n\\(Y_1=X_1+X_2\\) 와 \\(Y_2=X_1-X_2\\) 의 결합 확률 분포는\n\\[\n\\begin{aligned}\n  y_1&=u_1(x_1,x_2)=x_1+x_2\\\\\n  y_2&=u_2(x_1,x_2)=x_1-x_2\\\\\n  x_1&=w_1(y_1,y_2)=\\frac{y_1+y_2}{2}\\\\\n  x_2&=w_2(y_1,y_2)=\\frac{y_1-y_2}{2}\\\\\n  f_Y(y_1,y_2)&=\\binom{2}{\\frac{y_1+y_2}{2},\\frac{y_1-y_2}{2},2-y_1 }\\frac{1}{4}^{\\frac{y_1+y_2}{2}}\\frac{5}{12}^{2-y_1} y_1=0,1,2, \\space y_2=-2,-1,0,1,2\n\\end{aligned}\n\\] 이다.\n\nexmaple 2\n\n\\(X_1\\) 과 \\(X_2\\) 의 결합확률분포가 다음과 같을 때 변환된 \\(Y_1=X_1+X_2\\) 와 \\(Y_2=X_1X_2\\) 의 결합확률 분포는?\n\n\nTable 3: Exmaple: Transformation of Joint Discrete Random Variable (Not One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\n\n\n\n\n\n\n\\((x_1,x_2)\\)\n(0, 0)\n(0, 1)\n(1, 0)\n(1, 1)\n\n\n\n\n\\(f_X(x_1,x_2)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y\\)\n\n\n\\((y_1,y_2)\\)\n(0, 0)\n(1, 0)\n(2, 1)\n\n\n\n\n\n\\(f_Y(y_1,y_2)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 3 continuous random variable \\(X\\) 의 probability distribution이 \\(f_X(x)\\) 이고 \\(X\\) 와 \\(Y\\) 사이에는 \\(Y=u(X)\\) 라는 one-to-one relation이 성립될 때 \\(y=u(x)\\) 를 유일한 \\(x\\) 를 \\(y\\) 에 대한 함수인 \\(x=w(y)\\) 로 표현 가능할 때 Y의 확률 분포는 \\[\nf_Y(y)=f_X(w(y))|J|\n\\] 이다. (\\(J=w'(y)\\), called ‘Jacobian’)\n\n\n\n\nIn the case of a One-to-One relation\n\n지수분포(exponential distribution)는 연속 확률 분포 중 하나로, 사건이 서로 독립적일 때, 일정 시간동안 발생하는 사건의 횟수가 푸아송 분포를 따른다면, 다음 사건이 발생할 때까지의 대기 시간을 확률 변수 \\(X \\in [0, \\infty)\\) 로 하는 분포이다. (Source: Wiki).\n참고: \\[\n\\begin{aligned}\n  X&\\sim Exp(\\lambda) \\\\\n  \\text E[X] &= \\frac{1}{\\lambda}\\\\\n  \\text{Var}[X] &= \\frac{1}{\\lambda^2}\\\\\n  f(x;\\lambda)&=\n  \\begin{cases}\n    \\lambda e^{-\\lambda x} & x \\ge 0\\\\\n    0 & x&lt;0\n  \\end{cases}\\\\\n\\end{aligned}\n\\] \\(\\lambda&gt;0\\) is the parameter of the distribution, called ‘rate parameter’.\nTransformation of Linear Function. When two random variables \\(X \\sim \\text{exp}(\\lambda)\\) and \\(Y\\) has the relation, \\(Y=aX+b\\) \\((a\\ne 0)\\). When \\(f_X(x)=\\lambda e^{-\\lambda x}\\), \\(f_Y(y)\\) is\nBy Theorem\n\\[\n\\begin{aligned}\n  Y&=u(X)=aX+b\\\\\n  y&=u(x)=ax+b\\\\\n  x&=w(y)=\\frac{y-b}{a}\\\\\n  f_Y(y)&=f_X(w(y))|J|=f_X(\\frac{y-b}{a})|J|\\\\\n        &=f_X(\\frac{y-b}{a})|w'(y)|\\\\\n        &=f_X(\\frac{y-b}{a})|\\frac{1}{a}|\\\\\n        &=\\frac{f_X(\\frac{y-b}{a})}{|a|}\\\\\n\\end{aligned}\n\\]\nBy Definition\n\n\n\n\\(a&gt;0\\) \\[\n\\begin{aligned}\nF_Y(Y)&=P_Y(Y\\le y)\\\\\n      &=P_Y(aX+b\\le y)\\\\\n      &=P_Y(X\\le \\frac{y-b}{a})\\\\\n      &=F_X(\\frac{y-b}{a})\\\\\nF'_Y(Y)&=\\frac{d}{dy}F_X(\\frac{y-b}{a})\\\\\n       &=f_X(\\frac{y-b}{a})\\frac{1}{a}\\\\\n\\end{aligned}\n\\]\n\n\n\n\\(a\\le 0\\) \\[\n\\begin{aligned}\nF_Y(Y)&=P_Y(Y\\le y)\\\\\n      &=P_Y(aX+b\\le y)\\\\\n      &=P_Y(X&gt; \\frac{y-b}{a})\\\\\n      &=1-F_X(\\frac{y-b}{a})\\\\\nF'_Y(Y)&=\\frac{d}{dy}(1-F_X(\\frac{y-b}{a}))\\\\\n       &=-f_X(\\frac{y-b}{a})\\frac{1}{a}\\\\  \n\\end{aligned}\n\\]\n\n\\[\n\\therefore f_Y(y)=\\frac{f_X(\\frac{y-b}{a})}{|a|}=\\frac{\\lambda exp(-\\lambda (\\frac{y-b}{a}))}{|a|}\n\\]\n\n위의 정리를 유도를 하진 않았지만 증명을 해보면 사실, \\(|J|\\) 는 역함수의 미분의 계수인 것을 할 수 있다. 위의 예시를 By definition 으로 푼 것을 보면 \\(y=u(x)\\) 와 \\(x=w(y)\\) 의 관계인 것을 알 수 있고 \\(|J|\\) 에 해당되는 \\(\\frac{1}{a}\\) 는 CDF \\(F(Y)\\) 의 derivative를 구하는 과정에서 발생하는 chain rule에 의해 생긴 것을 알 수 있다.\n\\[\n\\begin{aligned}\n  y&=u(x)\\\\\n  \\frac{dy}{dx}&=u'(x)\\\\\n  x&=w(y)=u^{-1}(y)\\\\\n  \\frac{dx}{dy}&=w'(y) \\\\\n  w'(y)&=\\frac{dx}{dy}=(\\frac{dy}{dx})^{-1}=\\frac{1}{u'(x)}\n\\end{aligned}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 20, 1000)\ny = 2*x+1\ny2 = x/2+1\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,x,color='black',label=r'$y=x$', linestyle='dashed')\nax.plot(x,2*x-2,color='black',label=r'$x=w(y) \\rightarrow y=2x-2 $')\nax.plot(x,y2,color='red',label=r'$y=u(x)=\\frac{1}{2}x+1$')\nax.set_aspect(1)\nax.set_title(r\"Example of Relation of X and Y, $y=u(x)$ vs $x=w(y)$\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nax.set_xlim(-20, 40)\n\nplt.legend(shadow=True, loc=(-0.5, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 20, 1000)\ny = 2*x+1\ny2 = x/2+1\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,y,color='black',label=r'$y=2x+1$')\nax.plot(x,y2,color='red',label=r'$y=\\frac{1}{2}x+1$')\nax.set_aspect(1)\nax.set_title(r\"Transformation of Linear Function, $f_Y(y)=\\frac{f_X(\\frac{y-b}{a})}{|a|}$\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y=u(X)\")\nax.set_xlim(-20, 40)\nax.text(10, 0,'|', color='black', horizontalalignment='center', verticalalignment='center')\nax.text(10, -5, r'$\\frac{y-b}{a}$', horizontalalignment='center',color='black')\nax.text(0, 5,'--', color='black', horizontalalignment='center', verticalalignment='center')\nax.text(-9, 5, r'$ax+b$', verticalalignment='center',color='black')\n\nplt.legend(shadow=True, loc=(-0.5, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\\(P(Y\\le y)=P(X\\le \\frac{y-b}{a})\\) 이기 때문에 \\(X\\) 의 특정 구간의 확률과 대응되는 \\(Y\\) 의 구간의 확률이 같다고 했을 때 \\(Y \\le y\\) 의 범위는 \\(X\\) 를 \\(b\\) 만큼 이동한 후 \\(a\\) 배한 \\(x\\) 좌표 이하 구간에 대응 된다 즉, \\(X\\le \\frac{y-b}{a}\\). 이 때, 두 구간에서의 확률 밀도가 같아야 하기 때문에 \\(f_x(x)\\) 가 \\(f_Y(y)\\) 의 \\(\\frac{1}{a}\\) 배 인 것을 알 수 있다.\n위의 그림에서 처럼, 두 확률 변수 \\(X\\) 와 \\(Y\\) 의 관계를 \\(y = 2x+1\\) \\((a&gt;1)\\) 와 \\(y = \\frac{1}{2}x+1\\) \\((0 \\le a\\le 1)\\) 로 두 가지의 예로 들면, 위의 그래프의 경우, \\(y = 2x+1\\) \\((a&gt;1)\\) 일 때 \\(0&lt;X&lt;10\\) 는 \\(1&lt;Y&lt;21\\) 의 구간이 2배가 되는 것을 관찰 할 수 있다. 반대로, \\(y = \\frac{1}{2}x+1\\) \\((0 \\le a\\le 1)\\) 일 때 \\(0&lt;X&lt;10\\) 는 \\(1&lt;Y&lt;6\\) 의 구간이 \\(\\frac{1}{2}\\) 배가 된다. 이 때 각 예시의 경우에 \\(X\\) 와 \\(Y\\) 의 확률 값이 같기 때문에 (\\(P(Y\\le y)=P(X\\le \\frac{y-1}{2})\\) 와 \\(P(Y\\le y)=P(X\\le \\frac{y-1}{\\frac{1}{2}})\\)). \\(f_x(x)\\) 가 \\(f_Y(y)\\) 의 각 각 \\(2\\) 배와 \\(\\frac{1}{2}\\) 배 인 것을 추정 할 수 있다.\n\nIn the case of not a One-to-One relation\n\n\\(Y=X^2\\) 일 때,\n\\[\n\\begin{aligned}\n  F_Y(Y)&=P_Y(Y\\le y)\\\\\n        &=P_Y(X^2\\le y)\\\\\n        &=P_Y(-\\sqrt{y}\\le X \\le \\sqrt y)\\\\\n        &=F_X(\\sqrt{y})-F_X(-\\sqrt{y})\\\\\n  F'_Y(Y)&=\\frac{d}{dy}(F_X(\\sqrt{y})-F_X(-\\sqrt{y}))\\\\\n        &=f_X(\\sqrt{y})\\frac{1}{2\\sqrt{y}}+f_X(-\\sqrt{y})\\frac{1}{2\\sqrt{y}}\\\\\n        &=\\frac{f_X(\\sqrt{y})}{2\\sqrt{y}}+\\frac{f_X(-\\sqrt{y})}{2\\sqrt{y}}\\\\\n\\end{aligned}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 1000)\ny = x*x\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,y,color='black',label=r'$y=x^2$')\n#ax.text(3.0,0.5,r'$\\sqrt{y}$')\n#ax.text(-4,0.5,r'$-\\sqrt{y}$')\nax.set_xlim([-10, 10])\nax.set_ylim([0, 10])\n\nax.set_aspect(1)\nax.set_title(r\"Transformation of Random Variables, $y=x^2$\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\n\nplt.legend(shadow=True, loc=(-0.2, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\\(X\\) 의 분포를 알고 \\(X\\) 와 \\(Y\\) 의 relation을 알고있을 때 \\(X\\) 의 분포로부터 \\(Y\\) 의 분포를 추정 가능한 것의 이점 중 하나는 \\(Y\\) 의 통계량을 \\(Y\\) 의 분포를 사용하지 않고 계산해낼 수 있다는 점이다.\n예를 들어, \\(X \\sim N(0,1)\\) 이고 \\(Y=u(X)\\) 라는 관계를 알고있을 때, \\(Y\\) 의 통계량 \\(E(Y)\\), \\(Var(Y)\\) 를 알고싶다면 굳이 힘들게 \\(Y\\) 의 분포를 계산할 필요가 없다. 이미 \\(X\\) 의 확률 분포 \\(f_X(x)\\) 를 알고있기 때문에 아래와 같이 \\(Y\\) 의 통계량을 계산해낼 수 있다.\n\\[\n\\begin{aligned}\n  \\text{E}(Y)&=\\int Y f_Y(y) dy\\\\\n            &=\\int u(X) f_X(x) dx\\\\\n  \\text{E}(Y^2)&=\\int Y^2 f_Y(y) dy\\\\\n            &=\\int u(X)^2 f_X(x) dx\\\\\n  \\text{Var}(Y)&=\\text{E}(Y^2)-\\text{E}(Y)^2\\\\\n  &=\\text{E}(g(X)^2)-\\text{E}(g(X))^2\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\nTheorem 4 Two continuous random variable \\(X_1\\) 과 \\(X_2\\) 의 joint probability distribution이 \\(f(x_1,x_2)\\) 이고 \\((x_1,x_2)\\) 와 \\((y_1,y_2)\\) 가 one-to-one relation이 성립될 때 \\(y_1=u_1(x_1,x_2)\\) 와 \\(y_2=u_2(x_1,x_2)\\) 를 유일한 \\(x_1=w_1(y_1,y_2)\\) 와 \\(x_2=w_2(y_1,y_2)\\) 의 함수로 표현 가능할 때 \\(Y_1=u_1(X_1,X_2)\\) 와 \\(Y_2=u_2(X_1,X_2)\\) 로 정의되는 새로운 확률변수 \\(Y_1\\) 과 \\(Y_2\\) 의 joint probability distribution는 \\[\ng(y_1.y_2)=f(w_1(y_1,y_2),w_2(y_1,y_2)) |J|\n\\] 이다.\n여기서, \\[\n\\begin{aligned}\n  |J|=|\n  \\begin{bmatrix}\n  \\frac{\\partial x_1}{\\partial y_1}&\\frac{\\partial x_1}{\\partial y_2}\\\\\n  \\frac{\\partial x_2}{\\partial y_1}&\\frac{\\partial x_2}{\\partial y_2}\\\\\n  \\end{bmatrix}| = | \\begin{bmatrix}\n  \\frac{\\partial w_1(y_1,y_2)}{\\partial y_1}&\\frac{\\partial w_1(y_1,y_2)}{\\partial y_2}\\\\\n  \\frac{\\partial w_2(y_1,y_2)}{\\partial y_1}&\\frac{\\partial w_2(y_1,y_2)}{\\partial y_2}\\\\\n  \\end{bmatrix} |\n\\end{aligned}\n\\] called ‘the determinant of jacobian matrix’\n\n\n\n\n예제\n\n연속확률변수 \\(X_1\\) 과 \\(X_2\\) 는 결합확률분포 \\[\nf(x_1,x_2)=\n  \\begin{cases}\n    4x_1x_2, & \\text{if  } 0&lt;x_1&lt;1,& 0&lt; x_2 &lt;1 \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\] 일 때, \\(Y_1=X_1^2\\) 과 \\(Y_2=X_1X_2\\) 의 결합확률 분포는\n\\[\n\\begin{aligned}\ny_1&=u_1(x_1,x_2)= x_1^2\\\\\ny_2&=u_2(x_1,x_2)= x_1x_2\\\\\nx_1&=w_1(y_1,y_2)= \\sqrt{y_1} & (y_1&gt;0)\\\\\nx_2&=w_2(y_1,y_2)= \\frac{y_2}{x_1} = \\frac{y_2}{\\sqrt{y_1}}  & (y_1&gt;0)\\\\\ng(y_1.y_2)&=f(w_1(y_1,y_2),w_2(y_1,y_2))|J|\\\\\ng(y_1.y_2)&=f(w_1(y_1,y_2),w_2(y_1,y_2))|J|\\\\\n          &=4\\sqrt{y_1}\\frac{y_2}{\\sqrt{y_1}}\\frac{1}{2y_1}\\\\\n          &=\\begin{cases}\n          \\frac{2y_2}{y_1} & \\text{if  } y_2^2&lt;y_1&lt;1, \\text{  } 0&lt;y_2&lt;1\\\\\n          0 & \\text{otherwise}\n          \\end{cases}\\\\\n          |J|&=|\n          \\begin{bmatrix}\n          \\frac{\\partial x_1}{\\partial y_1}&\\frac{\\partial x_1}{\\partial y_2}\\\\\n          \\frac{\\partial x_2}{\\partial y_1}&\\frac{\\partial x_2}{\\partial y_2}\\\\\n          \\end{bmatrix}|\\\\\n           &=|\n          \\begin{bmatrix}\n          \\frac{1}{2\\sqrt{y_1}}&0\\\\\n          y_2(-\\frac{1}{2}y_1^{\\frac{-3}{2}})&\\frac{1}{\\sqrt{y_1}}\\\\\n          \\end{bmatrix}|\\\\\n          &=\\frac{1}{2y_1}\n\\end{aligned}\n\\]\n이다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-21_transformation/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-21_transformation/index.html#blog-guide-map-link",
    "title": "Transformation of Random Variables",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html#bernoulli-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html#bernoulli-distribution",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Definition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html#blog-guide-map-link",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html#bernoulli-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html#bernoulli-distribution",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Definition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html#blog-guide-map-link",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x;p)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}x^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html#bernoulli-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html#bernoulli-distribution",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Definition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x;p)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}x^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html#blog-guide-map-link",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html",
    "title": "Geometric Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 성공 확률이 \\(p\\) 인 independent Bernoulli trials을 시행할 때 첫 성공할때까지의 시행 횟수를 확률 변수 \\(X\\) 로 갖는 분포를 geometric distribution이라고 하고 \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;p)&=p(1-p)^{x-1}\n\\end{aligned}\n\\] 이다. (단, \\(x=1,2,...\\))\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=1}^{\\infty} x f(x)\\\\\n             &=\\sum_{x=1}^{\\infty} x pq^{x-1} &\\text{ }(q=1-p)\\\\\n             &=p\\sum_{x=1}^{\\infty} x q^{x-1} \\\\\n             &=p\\frac{d}{dq}\\sum_{x=1}^{\\infty} q^{x} \\\\\n             &=p\\frac{d}{dq}\\frac{q}{1-q} \\\\\n             &=p\\frac{(1-q)+q}{(1-q)^2} \\\\\n             &=p\\frac{(1-q)+q}{p^2} \\\\\n             &=p\\frac{1}{p^2}\\\\\n             &=\\frac{1}{p}\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X(X-1))&=\\sum_{x=1}^{\\infty}x(x-1)pq^{x-1}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}x(x-1)q^{x-2}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}\\frac{d^2}{dq^2}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}\\sum_{x=1}^{\\infty}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}(\\frac{q}{1-q})\\\\\n                  &=pq\\frac{d}{dq}(\\frac{1}{(1-q)^2})\\\\\n                  &=pq(\\frac{2(1-q)}{(1-q)^4})\\\\\n                  &=\\frac{2q}{p^2}\\\\\n  \\text{E}(X^2)&=\\frac{2q}{p^2}+\\text{E}(X)=\\frac{2q}{p^2}+\\frac{1}{p}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n               &=\\frac{2q}{p^2}+\\frac{1}{p}-\\frac{1}{p^2}\\\\\n               &=\\frac{1-p}{p^2}\\\\\n\\end{aligned}\n\\]\n\n\n\n한 장비 제조 업체에서 사람의 건강과 생명을 진단하는데 사용되는 medical device가 1000 개마다 2개 꼴로 불량이 발생한다고 가정할 때 그 장비를 공급받는 구매자가 medical device의 공급 업체의 불량 평가 기준으로 quality control을 1대 씩 진행할 때 불량 장비가 3번째 검사에서 발생할 확률은\n\\[\n\\begin{aligned}\n   \\text{f}(x=3;p=\\frac{2}{1000})&=(\\frac{998}{1000})^2\\frac{2}{1000}\\\\\n\\end{aligned}\n\\] 이다. 평균과 분산 각 각 \\((\\frac{2}{1000})^{-1}\\), \\(\\frac{1-\\frac{2}{1000}}{(\\frac{2}{1000})^2}\\) 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html#geometric-distribution",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html#geometric-distribution",
    "title": "Geometric Distribution",
    "section": "",
    "text": "Definition 1 성공 확률이 \\(p\\) 인 independent Bernoulli trials을 시행할 때 첫 성공할때까지의 시행 횟수를 확률 변수 \\(X\\) 로 갖는 분포를 geometric distribution이라고 하고 \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;p)&=p(1-p)^{x-1}\n\\end{aligned}\n\\] 이다. (단, \\(x=1,2,...\\))\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=1}^{\\infty} x f(x)\\\\\n             &=\\sum_{x=1}^{\\infty} x pq^{x-1} &\\text{ }(q=1-p)\\\\\n             &=p\\sum_{x=1}^{\\infty} x q^{x-1} \\\\\n             &=p\\frac{d}{dq}\\sum_{x=1}^{\\infty} q^{x} \\\\\n             &=p\\frac{d}{dq}\\frac{q}{1-q} \\\\\n             &=p\\frac{(1-q)+q}{(1-q)^2} \\\\\n             &=p\\frac{(1-q)+q}{p^2} \\\\\n             &=p\\frac{1}{p^2}\\\\\n             &=\\frac{1}{p}\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X(X-1))&=\\sum_{x=1}^{\\infty}x(x-1)pq^{x-1}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}x(x-1)q^{x-2}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}\\frac{d^2}{dq^2}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}\\sum_{x=1}^{\\infty}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}(\\frac{q}{1-q})\\\\\n                  &=pq\\frac{d}{dq}(\\frac{1}{(1-q)^2})\\\\\n                  &=pq(\\frac{2(1-q)}{(1-q)^4})\\\\\n                  &=\\frac{2q}{p^2}\\\\\n  \\text{E}(X^2)&=\\frac{2q}{p^2}+\\text{E}(X)=\\frac{2q}{p^2}+\\frac{1}{p}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n               &=\\frac{2q}{p^2}+\\frac{1}{p}-\\frac{1}{p^2}\\\\\n               &=\\frac{1-p}{p^2}\\\\\n\\end{aligned}\n\\]\n\n\n\n한 장비 제조 업체에서 사람의 건강과 생명을 진단하는데 사용되는 medical device가 1000 개마다 2개 꼴로 불량이 발생한다고 가정할 때 그 장비를 공급받는 구매자가 medical device의 공급 업체의 불량 평가 기준으로 quality control을 1대 씩 진행할 때 불량 장비가 3번째 검사에서 발생할 확률은\n\\[\n\\begin{aligned}\n   \\text{f}(x=3;p=\\frac{2}{1000})&=(\\frac{998}{1000})^2\\frac{2}{1000}\\\\\n\\end{aligned}\n\\] 이다. 평균과 분산 각 각 \\((\\frac{2}{1000})^{-1}\\), \\(\\frac{1-\\frac{2}{1000}}{(\\frac{2}{1000})^2}\\) 이다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html#blog-guide-map-link",
    "title": "Geometric Distribution",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-28_mgf/index.html",
    "href": "docs/blog/posts/statistics/2023-02-28_mgf/index.html",
    "title": "Momment Generating Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 확률 변수 \\(X^r\\) 의 expectation, \\(\\text{E}((X-\\mu)^r)\\) 를 확률 변수 \\(X\\) 의 평균 \\(\\mu\\) 에 대한 \\(r\\) 차 중심 적률(moment) 이라하고 그 notation은 \\(\\mu_r'=\\text{E}((X-\\mu)^r)\\) 로 한다. \\(\\mu_r'=\\text{E}(X^r)\\) 은 원점에 대한 \\(r\\) 차 중심 적률이라 한다.\n즉,\n\\[\n\\begin{aligned}\n  \\mu_{r}'&=\n    \\begin{cases}\n      \\text{E}((X-\\mu)^r)&=\n        \\begin{cases}\n          \\sum_{x}(x-\\mu)^rf(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}(x-\\mu)^rf(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases} & \\text{moment about }\\mu\\\\\n      \\text{E}(X^r)&=\n        \\begin{cases}\n          \\sum_{x}x^rf(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}x^rf(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases} & \\text{moment about origin}\\\\\\\\\n    \\end{cases}  \n\\end{aligned}\n\\]\n이다.\n\n분포의 특징을 묘사하는 parameters 중 많은 종류가 확률 변수의 적률을 이용해 계산될 수 있다. 그 대표적인 예가\n\n평균 (mean): 분포의 위치를 나타내는 척도, 1차 중심 적률로 계산\n분산 (variance): 분포가 평균으로부터 퍼진 정도를 나타내는 척도, 2차 중심 적률로 계산\n왜도 (skewedness): 분포가 기울어진 방향과 정도를 나타내는 척도, 3차 중심 적률로 계산\n첨도 (kurtosis): 분포가 위로 뾰족한 정도를 나타내는 척도, 4차 중심 적률로 계산\n\n\nDefinition 2 확률 변수 \\(X\\) 의 적률 생성 함수 (Moment Generating Function, mgf), \\(\\text{M}_X(t) =\\text{E}(e^{tX})\\) 로 정의한다.\n즉,\n\\[\n\\begin{aligned}\n  M_X(t)=\\text{E}(e^{tX})&=\n        \\begin{cases}\n          \\sum_{x}e^{tx}f(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}e^{tx}f(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases}\n\\end{aligned}\n\\]\n이다.\n\n\nTheorem 1 확률 변수 \\(X\\) 의 적률 생성 함수 (Moment Generating Function, mgf), \\(\\text{M}_X(t) =\\text{E}(e^{tX})\\) 로 r차 적률 계산은 다음과 같이 할 수 있다.\n\\[\n\\begin{aligned}\n  \\frac{d^r}{dt^r}M_X(t) \\bigg|_{t=0}=M_X^r(0)=\\text{E}(X^r)=\\mu_r'\n\\end{aligned}\n\\]\n이다. 즉, 적률 생성 함수 (mgf) \\(M_X(t)\\) 를 구하고 r 번 미번한 후에 \\(t=0\\) 대입하면 r차 중심 적률을 구할 수 있다.\n\nproof (for the only continuous case)\n\\[\n\\begin{aligned}\n  \\text{First Order Moment}\\\\\n  \\frac{d}{dt}M_X(t)&= \\frac{d}{dt}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d}{dt}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}\\frac{d}{dt}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}xe^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}xf(x)dx\\\\\n    &=  \\text{E}(X)\\\\\n    &=  \\mu_1'\\\\\n  \\text{Second Order Moment}\\\\\n  \\frac{d^2}{dt^2}M_X(t)&= \\frac{d^2}{dt^2}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d^2}{dt^2}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}\\frac{d}{dt}xe^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^2e^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^2f(x)dx\\\\\n    &=  \\text{E}(X^2)\\\\\n    &=  \\mu_2'\\\\\n  \\vdots\\\\\n  \\text{r th Order Moment}\\\\\n  \\frac{d^r}{dt^r}M_X(t)&= \\frac{d^r}{dt^r}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d^r}{dt^r}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^re^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^rf(x)dx\\\\\n    &=  \\text{E}(X^r)\\\\\n    &=  \\mu_r'\n\\end{aligned}\n\\]\n\n\n\n\n\nMGF of \\(X\\sim B(n,p)\\)\n\n확률 변수 \\(X \\sim B(n,p)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n\\[\n\\begin{aligned}\n  f(x)&=\\binom{n}{x}p^xq^{n-x}\\\\\n  \\sum_{x=0}^{n}f(x)&=\\sum_{x=0}^{n}\\binom{n}{x}p^xq^{n-x}=(p+q)^n\\\\\n  M_X(t)&=\\sum_{x=0}^{n}e^{tx}f(x)=\\sum_{x=0}^{n}e^{tx}\\binom{n}{x}p^xq^{n-x}\\\\\n        &=\\sum_{x=0}^{n}\\binom{n}{x}(pe^t)^xq^{n-x}=(pe^t+q)^n\\\\\n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}(pe^t+q)^n\\\\\n    &=n(pe^t+q)^{n-1}pe^t\\bigg|_{t=0}\\\\\n    &=n(p+q)^{n-1}p=np\\\\\n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}(n(pe^t+q)^{n-1}pe^t)\\bigg|_{t=0}\\\\\n    &=(n(n-1)(pe^t+q)^{n-2}pe^tpe^t+n(pe^t+q)^{n-1}pe^t)\\bigg|_{t=0}\\\\\n    &=n(n-1)(p+q)^{n-2}p^2+n(p+q)^{n-1}p\\\\\n    &=n(n-1)p^2+np\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=n(n-1)p^2+np-(np)^2\\\\\n    &=n^2p^2-np^2+np-n^2p^2\\\\\n    &=np(1-p)=npq\n\\end{aligned}\n\\]\n\nMGF of \\(X\\sim Poisson(\\lambda)\\)\n\n확률 변수 \\(X \\sim Poisson(\\lambda)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n  M_X(t)&=\\sum_{x=0}^{\\infty}e^{tx}f(x)=\\sum_{x=0}^{\\infty}e^{tx}\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n        &=e^{-\\lambda}\\sum_{x=0}^{\\infty}\\frac{{e^{t}\\lambda^{x}}^x}{x!}\\\\\n        &=e^{-\\lambda}e^{\\lambda e^t} \\because \\text{(Maclaurin's Series)}\\\\\n        &=e^{\\lambda(e^t-1)} \\\\\n        (&\\text{Maclaurin's Series: } \\sum_{n=0}^{\\infty}\\frac{x^n}{n!}=1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+ ... =e^x)\\\\     \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda e^{e^t}e^{\\lambda(e^t-1)}\\bigg|_{t=0}=\\lambda\\\\\n  \n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}\\lambda e^{e^t}e^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda e^te^{\\lambda (e^t-1)}+\\lambda e^t\\lambda e^te^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda + \\lambda^2\\\\\n\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=\\lambda + \\lambda^2 -\\lambda^2\\\\\n    &=\\lambda\n\\end{aligned}\\]\n$$\n\n\n\n\nMGF of \\(X\\sim N(0,1)\\)\n\n확률 변수 \\(X \\sim N(0,1)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}(-\\infty &lt; x &lt; \\infty)\\\\\n  M_X(t)&=\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx=\\int_{-\\infty}^{\\infty}e^{tx}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{x^2}{2}+tx}dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(x-t)^2+\\frac{1}{2}t^2}dx\\\\\n        &=e^{\\frac{t^2}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(x-t)^2}dx\\\\\n        &=e^{\\frac{t^2}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}u^2}du \\text{  }(\\because x-t=u \\rightarrow dx=du)\\\\\n        &=e^{\\frac{t^2}{2}} \\\\     \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=te^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=0\\\\\n  \n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}te^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=e^{\\frac{t^2}{2}}+t(te^{\\frac{t^2}{2}})\\bigg|_{t=0}\\\\\n    &=1\\\\\n\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=1\n    \n\\end{aligned}\\]\n$$\n\nMGF of \\(X\\sim N(\\mu,\\sigma^2)\\)\n\n확률 변수 \\(X\\sim N(\\mu,\\sigma^2)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}(-\\infty &lt; x &lt; \\infty)\\\\\n  M_X(t)&=\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{\\infty}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}+tx}dx\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}(\\frac{(x-(t\\sigma^2+\\mu))}{\\sigma})^2}dx\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{u^2}{2}}du\\\\\n        (&\\because \\frac{x-(t\\sigma^2+\\mu)}{\\sigma}=u \\rightarrow \\frac{dx}{\\sigma}=du) \\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^2}{2}}du\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\\\\n  \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\bigg|_{t=0}\\\\\n    &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}(\\sigma^2t+\\mu)\\bigg|_{t=0}\\\\\n    &=\\mu\\\\\n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}e^{\\frac{t^2\\sigma^2}{2}+t\\mu}(\\sigma^2t+\\mu)\\bigg|_{t=0}\\\\\n    &=\\sigma^2+\\mu^2 \\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=\\sigma^2\n\\end{aligned}\\]\n$$\n\n\n\n\n\nTheorem 2 확률 변수 \\(X\\) 와 \\(Y\\) 가 유한한 같은 적률 생성 함수를 가지면 두 확률 변수는 같은 확률 분포를 갖는다. (단, \\(t\\in[-c,c]\\) where \\(c\\) is a positive constant) \\[\n\\text{M}_X(t) =\\text{M}_Y(t) \\rightarrow F_X(a)=F_Y(a) \\text{ for } a \\in \\mathbb{R}\n\\] 다시 말해서, 확률 변수의 분포의 특징이 적률 생성 함수에 의하여 유일하게 결정된다.\n\nProof Reference-Washington University\nMGF reference\n\nTheorem 3 \\[\n\\text{M}_{X+a}(t) =e^{at}\\text{M}_X(t)\n\\]\n\nProof) \\(\\text{E}(e^{t(x+a)})=\\text{E}(e^{at}e^{tx})=e^{at}\\text{E}(e^{tx})=e^{at}\\text{M}_X(t)\\)\n\nTheorem 4 \\[\n\\text{M}_{aX}(t) =\\text{M}_X(at)\n\\]\n\nProof) \\(\\text{E}(e^{atx})=\\text{E}(e^{at(x)})=\\text{M}_X(at)\\)\n\nExample) when \\(X \\sim N(0,1)\\), the mgf of \\(Y=aX+b\\) is ?\n\n\\[\n\\begin{aligned}\n  \\text{M}_Y(t)&=\\text{M}_{aX+b}(t)\\\\\n               &=e^{bt}\\text{M}_{X}(at)\\\\\n               &=e^{bt}e^{\\frac{a^2t^2}{2}}\\\\\n               &=e^{bt+\\frac{a^2t^2}{2}}\n\\end{aligned}\n\\]\n\nTheorem 5 서로 독립인 확률 변수 \\(X_1,X_2, ..., X_n\\) 의 적률 생성 함수가 각 각 \\(\\text{M}_{X_1}(t), \\text{M}_{X_2}(t), ..., \\text{M}_{X_n}(t)\\) 일 때, 확률 변수 \\(Y=X_1+X_2+...+X_n\\) 의 적률 생성함수 \\(\\text{M}_Y(t)\\) 는 \\(\\text{M}_{X_1}(t)\\text{M}_{X_2}(t) \\dots \\text{M}_{X_n}(t)\\) 이다.\n\\[\n\\begin{aligned}\n  \\text{M}_Y(t) &= E(e^{Yt})\\\\\n                &= E(e^{t(X_1+X_2+ ...+ X_n)})\\\\\n                &= E(e^{tX_1}e^{tX_2}\\dots e^{tX_n})\\\\\n                &= E(e^{tX_1})E(e^{tX_2})\\dots E(e^{tX_n}) \\because X_i \\text{ are independent}\\\\\n                &= \\text{M}_{X_1}(t)\\text{M}_{X_2}(t) \\dots \\text{M}_{X_n}(t)\n\\end{aligned}\n\\]\n\n\nExample) \\(X_1, X_2, ..., X_n\\) 가 서로 독립이고 parameter 가 각 각 \\(\\lambda_1, \\lambda_2, ..., \\lambda_n\\) 인 poisson 분포를 따른다면 \\(Y=X_1+X_2+...+X_n\\) 의 mgf는\n\n$$\n\\[\\begin{aligned}\n  \\text{M}_X(t) &= e^{\\lambda(e^t-1)}\\\\\n  \\text{M}_Y(t) &= E(e^{Yt})\\\\\n                &= E(e^{t(X_1+X_2+ ...+ X_n)})\\\\\n                &= E(e^{tX_1}e^{tX_2}\\dots e^{tX_n})\\\\\n                &= E(e^{tX_1})E(e^{tX_2})\\dots E(e^{tX_n}) \\because X_i \\text{ are independent}\\\\\n                &= \\text{M}_{X_1}(t)\\text{M}_{X_2}(t) \\dots \\text{M}_{X_n}(t)\\\\\n                &= e^{\\lambda_1(e^t-1)}+e^{\\lambda_2(e^t-1)}+\\dots+e^{\\lambda_n(e^t-1)}\\\\\n                &= e^{(\\lambda_1+\\lambda_2+\\dots+\\lambda_n)(e^t-1)}\\\\\n                &= e^{\\sum_{i=1}^{n}\\lambda_i(e^t-1)}\\\\\n\\end{aligned}\\]\n$$\n이다. 즉, Y는 parameter가 \\(\\sum_{i=1}^{n}\\lambda_i\\) 인 poisson 분포를 따른다. 뿐만 아니라, 정규 분포, poisson 분포, 카이제곱 분포의 경우 독립 변수들의 합으로 만든 분포도 같은 종류의 분포를 따르게 된다.\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-28_mgf/index.html#momment-generating-function",
    "href": "docs/blog/posts/statistics/2023-02-28_mgf/index.html#momment-generating-function",
    "title": "Momment Generating Function",
    "section": "",
    "text": "Definition 1 확률 변수 \\(X^r\\) 의 expectation, \\(\\text{E}((X-\\mu)^r)\\) 를 확률 변수 \\(X\\) 의 평균 \\(\\mu\\) 에 대한 \\(r\\) 차 중심 적률(moment) 이라하고 그 notation은 \\(\\mu_r'=\\text{E}((X-\\mu)^r)\\) 로 한다. \\(\\mu_r'=\\text{E}(X^r)\\) 은 원점에 대한 \\(r\\) 차 중심 적률이라 한다.\n즉,\n\\[\n\\begin{aligned}\n  \\mu_{r}'&=\n    \\begin{cases}\n      \\text{E}((X-\\mu)^r)&=\n        \\begin{cases}\n          \\sum_{x}(x-\\mu)^rf(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}(x-\\mu)^rf(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases} & \\text{moment about }\\mu\\\\\n      \\text{E}(X^r)&=\n        \\begin{cases}\n          \\sum_{x}x^rf(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}x^rf(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases} & \\text{moment about origin}\\\\\\\\\n    \\end{cases}  \n\\end{aligned}\n\\]\n이다.\n\n분포의 특징을 묘사하는 parameters 중 많은 종류가 확률 변수의 적률을 이용해 계산될 수 있다. 그 대표적인 예가\n\n평균 (mean): 분포의 위치를 나타내는 척도, 1차 중심 적률로 계산\n분산 (variance): 분포가 평균으로부터 퍼진 정도를 나타내는 척도, 2차 중심 적률로 계산\n왜도 (skewedness): 분포가 기울어진 방향과 정도를 나타내는 척도, 3차 중심 적률로 계산\n첨도 (kurtosis): 분포가 위로 뾰족한 정도를 나타내는 척도, 4차 중심 적률로 계산\n\n\nDefinition 2 확률 변수 \\(X\\) 의 적률 생성 함수 (Moment Generating Function, mgf), \\(\\text{M}_X(t) =\\text{E}(e^{tX})\\) 로 정의한다.\n즉,\n\\[\n\\begin{aligned}\n  M_X(t)=\\text{E}(e^{tX})&=\n        \\begin{cases}\n          \\sum_{x}e^{tx}f(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}e^{tx}f(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases}\n\\end{aligned}\n\\]\n이다.\n\n\nTheorem 1 확률 변수 \\(X\\) 의 적률 생성 함수 (Moment Generating Function, mgf), \\(\\text{M}_X(t) =\\text{E}(e^{tX})\\) 로 r차 적률 계산은 다음과 같이 할 수 있다.\n\\[\n\\begin{aligned}\n  \\frac{d^r}{dt^r}M_X(t) \\bigg|_{t=0}=M_X^r(0)=\\text{E}(X^r)=\\mu_r'\n\\end{aligned}\n\\]\n이다. 즉, 적률 생성 함수 (mgf) \\(M_X(t)\\) 를 구하고 r 번 미번한 후에 \\(t=0\\) 대입하면 r차 중심 적률을 구할 수 있다.\n\nproof (for the only continuous case)\n\\[\n\\begin{aligned}\n  \\text{First Order Moment}\\\\\n  \\frac{d}{dt}M_X(t)&= \\frac{d}{dt}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d}{dt}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}\\frac{d}{dt}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}xe^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}xf(x)dx\\\\\n    &=  \\text{E}(X)\\\\\n    &=  \\mu_1'\\\\\n  \\text{Second Order Moment}\\\\\n  \\frac{d^2}{dt^2}M_X(t)&= \\frac{d^2}{dt^2}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d^2}{dt^2}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}\\frac{d}{dt}xe^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^2e^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^2f(x)dx\\\\\n    &=  \\text{E}(X^2)\\\\\n    &=  \\mu_2'\\\\\n  \\vdots\\\\\n  \\text{r th Order Moment}\\\\\n  \\frac{d^r}{dt^r}M_X(t)&= \\frac{d^r}{dt^r}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d^r}{dt^r}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^re^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^rf(x)dx\\\\\n    &=  \\text{E}(X^r)\\\\\n    &=  \\mu_r'\n\\end{aligned}\n\\]\n\n\n\n\n\nMGF of \\(X\\sim B(n,p)\\)\n\n확률 변수 \\(X \\sim B(n,p)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n\\[\n\\begin{aligned}\n  f(x)&=\\binom{n}{x}p^xq^{n-x}\\\\\n  \\sum_{x=0}^{n}f(x)&=\\sum_{x=0}^{n}\\binom{n}{x}p^xq^{n-x}=(p+q)^n\\\\\n  M_X(t)&=\\sum_{x=0}^{n}e^{tx}f(x)=\\sum_{x=0}^{n}e^{tx}\\binom{n}{x}p^xq^{n-x}\\\\\n        &=\\sum_{x=0}^{n}\\binom{n}{x}(pe^t)^xq^{n-x}=(pe^t+q)^n\\\\\n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}(pe^t+q)^n\\\\\n    &=n(pe^t+q)^{n-1}pe^t\\bigg|_{t=0}\\\\\n    &=n(p+q)^{n-1}p=np\\\\\n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}(n(pe^t+q)^{n-1}pe^t)\\bigg|_{t=0}\\\\\n    &=(n(n-1)(pe^t+q)^{n-2}pe^tpe^t+n(pe^t+q)^{n-1}pe^t)\\bigg|_{t=0}\\\\\n    &=n(n-1)(p+q)^{n-2}p^2+n(p+q)^{n-1}p\\\\\n    &=n(n-1)p^2+np\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=n(n-1)p^2+np-(np)^2\\\\\n    &=n^2p^2-np^2+np-n^2p^2\\\\\n    &=np(1-p)=npq\n\\end{aligned}\n\\]\n\nMGF of \\(X\\sim Poisson(\\lambda)\\)\n\n확률 변수 \\(X \\sim Poisson(\\lambda)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n  M_X(t)&=\\sum_{x=0}^{\\infty}e^{tx}f(x)=\\sum_{x=0}^{\\infty}e^{tx}\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n        &=e^{-\\lambda}\\sum_{x=0}^{\\infty}\\frac{{e^{t}\\lambda^{x}}^x}{x!}\\\\\n        &=e^{-\\lambda}e^{\\lambda e^t} \\because \\text{(Maclaurin's Series)}\\\\\n        &=e^{\\lambda(e^t-1)} \\\\\n        (&\\text{Maclaurin's Series: } \\sum_{n=0}^{\\infty}\\frac{x^n}{n!}=1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+ ... =e^x)\\\\     \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda e^{e^t}e^{\\lambda(e^t-1)}\\bigg|_{t=0}=\\lambda\\\\\n  \n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}\\lambda e^{e^t}e^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda e^te^{\\lambda (e^t-1)}+\\lambda e^t\\lambda e^te^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda + \\lambda^2\\\\\n\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=\\lambda + \\lambda^2 -\\lambda^2\\\\\n    &=\\lambda\n\\end{aligned}\\]\n$$\n\n\n\n\nMGF of \\(X\\sim N(0,1)\\)\n\n확률 변수 \\(X \\sim N(0,1)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}(-\\infty &lt; x &lt; \\infty)\\\\\n  M_X(t)&=\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx=\\int_{-\\infty}^{\\infty}e^{tx}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{x^2}{2}+tx}dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(x-t)^2+\\frac{1}{2}t^2}dx\\\\\n        &=e^{\\frac{t^2}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(x-t)^2}dx\\\\\n        &=e^{\\frac{t^2}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}u^2}du \\text{  }(\\because x-t=u \\rightarrow dx=du)\\\\\n        &=e^{\\frac{t^2}{2}} \\\\     \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=te^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=0\\\\\n  \n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}te^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=e^{\\frac{t^2}{2}}+t(te^{\\frac{t^2}{2}})\\bigg|_{t=0}\\\\\n    &=1\\\\\n\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=1\n    \n\\end{aligned}\\]\n$$\n\nMGF of \\(X\\sim N(\\mu,\\sigma^2)\\)\n\n확률 변수 \\(X\\sim N(\\mu,\\sigma^2)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}(-\\infty &lt; x &lt; \\infty)\\\\\n  M_X(t)&=\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{\\infty}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}+tx}dx\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}(\\frac{(x-(t\\sigma^2+\\mu))}{\\sigma})^2}dx\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{u^2}{2}}du\\\\\n        (&\\because \\frac{x-(t\\sigma^2+\\mu)}{\\sigma}=u \\rightarrow \\frac{dx}{\\sigma}=du) \\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^2}{2}}du\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\\\\n  \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\bigg|_{t=0}\\\\\n    &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}(\\sigma^2t+\\mu)\\bigg|_{t=0}\\\\\n    &=\\mu\\\\\n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}e^{\\frac{t^2\\sigma^2}{2}+t\\mu}(\\sigma^2t+\\mu)\\bigg|_{t=0}\\\\\n    &=\\sigma^2+\\mu^2 \\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=\\sigma^2\n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-28_mgf/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-02-28_mgf/index.html#blog-guide-map-link",
    "title": "Momment Generating Function",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-03-25_MLE/index.html",
    "href": "docs/blog/posts/statistics/2023-03-25_MLE/index.html",
    "title": "Maximum Likelihood Estimation, Statistical Bias, and Point Estimation",
    "section": "",
    "text": "0.1 Definition\nTo talk about MLE (Maximum Likelihood Estimation), we need to recap the concepts and definitions of probability and likelihood. They are related but distinct concepts.\n\nprobability is a measure of the chance that an event will occur, given some prior knowledge or assumptions.\nlikelihood is a measure of the plausibility or compatibility of a particular set of model parameters, given the observed data.\n\n\n\n\n\n\n\nChance vs Plausibility (personal opinion)\n\n\n\n\nchance is a general term but closer term to statistics, which is used in the situation of the probability or likelihood of an event occurring based on randomness or uncertainty.\n\nplausibility chance is a term more often used in everyday life, which refers to the degree to which something is believable based on the available evidence or information.\n\n\n\n\nDefinition 1 Let \\(X_1, X_2, ..., X_n\\) be a set of iid random variables with pdf or pmf \\(f(x_i | \\theta)\\), where \\(\\theta\\) is a vector of unknown parameters. Then, the likelihood function is defined as the joint pdf or pmf of the observed data, given the values of the parameters:\n\\[\nL(\\theta | x_1, x_2, ..., x_n) = L(\\theta | \\mathbf x) = \\prod_{i=1}^n f(x_i | \\theta)\n\\]\n\nThe likelihood function is a function of the parameters \\(\\theta\\). The function measures the probability of observing a set of data, given the values of the parameters of a statistical model in order to estimate the values of the parameters by finding the values that maximize the likelihood function. The likelihood function is often used in the maximum likelihood estimation (MLE) method, where the MLE estimator is the set of parameter values that maximize the likelihood function.\nThe likelihood function is related to the concept of conditional probability. Given a set of observed data \\(x_1, x_2, ..., x_n\\), the likelihood function measures the probability of observing these data, assuming a particular set of parameter values. The likelihood function is not a probability distribution, but it can be used to derive a probability distribution for the parameters, known as the posterior distribution, using Bayes’ theorem.\n\n\n\n\n\n\nProbability\n\n\n\nProbability is a measure of the likelihood or chance that an event will occur, which is used to quantify uncertainty and randomness.\nprobability is a function that maps a real number mapped from a random variable into \\([0,1]\\). The probability function, denoted by \\(P\\), satisfies the following axioms:\n\nNon-negativity: For any event \\(A \\in \\Omega\\), \\(P(A) \\geq 0\\).\nNormalization: The probability of the entire sample space is 1, i.e., \\(P(\\Omega) = 1\\).\nAdditivity: For any two disjoint events \\(A, B \\in \\Omega\\), or \\(A \\cap B = \\emptyset\\), the probability of their union is equal to the sum of their individual probabilities, i.e., \\(P(A \\cup B) = P(A) + P(B)\\).\n\n\n\n\n0.1.1 Probability vs Likelihood\nProbability and likelihood are related but distinct concepts in statistics.\n\nProbability refers to the measure of the likelihood that a particular event will occur, scaled on \\([0,1]\\). It is calculated based on a known probability distribution (= some prior knowledge or assumptions) before the data is observed.\nOn the other hand, likelihood refers to the probability of observing a set of data given a particular set of parameter values in a statistical model. It is calculated based on the unknown parameters after the data is observed.\n\nThe likelihood function is used to estimate the values of the parameters by finding the parameter values that maximize the likelihood function.\nFor instance of a coin flip, the probability of getting heads on a coin flip is 0.5, regardless of whether the coin has been flipped or not (i.e., without data). In contrast, the likelihood of observing heads after a coin has been flipped depends on the parameter of interest. We need to find the parameter given data, the results of multiple coin flips.\nIf we want to estimate the probability of heads, we can use the maximum likelihood estimation (MLE) approach, which involves finding the value of the coin bias that maximizes the likelihood of observing the observed sequence of heads and tails. The likelihood of the data is calculated using the binomial distribution, which gives the probability of observing a certain number of heads, given the number of tosses and the coin bias. In this case, the likelihood function is a function of the coin bias, and the probability of heads is the value of the coin bias that maximizes the likelihood function.\n\n\n0.1.2 Relation between Likelihood and PMF or PDF\nThe likelihood function is closely related to pmf or pdf of the data, which is a function that describes the probability of observing a particular value or range of values for the data, given the model parameters. The pmf or pdf is a function of the data, not the parameters, and is often written as \\(f(x|\\theta)\\). The likelihood function is proportional to the pdf because is a product of pdfs when \\(X_i\\) is independent, but with the data fixed and the parameter values treated as variables.\n\n\nCode\nlibrary(tidyverse)\n\n# Probability of getting heads on a fair coin flip\nprob &lt;- 0.5\n\n# likelihood\n## Simulate a coin flip with a biased coin\nset.seed(123) # Set random seed for reproducibility\nn &lt;- 100 # flipping numbers\np &lt;- 0.2 # Probability of getting heads\nx &lt;- rbinom(n, size = 1, prob = p) # Simulate n coin flips\nx%&gt;%head(10)\n\n\n [1] 0 0 0 1 1 0 0 1 0 0\n\n\nCode\n## Calculate the likelihood of observing the data given the parameter value p\nlikelihood &lt;- prod(dbinom(x, size = 1, prob = p))\nlikelihood%&gt;%round()\n\n\n[1] 0\n\n\nIn this example, prob is the assumed probability of getting heads on a fair coin flip. The probability of getting heads on a fair coin flip is 0.5, which is a fixed value that does not depend on any specific data. On the other hand, p is the probability of getting heads for the biased coin that we are simulating. The likelihood of observing a set of coin flips depends on the parameter value, which is unknown (actually, we know that it was \\(p=0.2\\)), and the observed data. We simulate a set of 100 coin flips with a biased coin that has a probability of 0.2 of getting heads. We then calculate the likelihood of observing this data given the parameter value of 0.2, which is the product of the probability mass function for each flip.\nHowever, in a real-world scenario, we would not know the true value of p and we would need to estimate it based on the observed data. By finding the Maximum Likelihood Estimation (MLE) of p, we are estimating the value of p that is most likely to have generated the observed data. To find MLE of p that maximizes the likelihood function, we can use numerical optimization methods.\nAs n increases, the product term in the likelihood function \\(\\prod_{i=1}^{n} f(x_i; \\theta)\\), where \\(f\\) is the pdf or pmf of the distribution being used, can become very small (since it is a product of values less than 1) and may result in numerical underflow (i.e., the product becomes so small that it rounds down to 0 in computer calculations). In practice, we typically take the logarithm of the likelihood function, called the log-likelihood, to avoid this issue:\n\\[\n\\log L(\\theta|x_1, x_2, ..., x_n) = \\sum_{i=1}^{n} \\log f(x_i; \\theta)\n\\]\nUsing the logarithm allows us to convert the product of small probabilities into a sum of log-probabilities, which are typically easier to work with numerically and mathematically. In this case, as n increases, the sum term in the log-likelihood can decrease (since it is a sum of negative values), but the decrease may not be as severe as in the product term of the likelihood function because of the log scale converting very small or large values into larger or smaller values .\n\nDefinition 2 The MLE estimator \\(\\hat{\\theta}\\) is the value of the parameter vector that maximizes the likelihood function:\n\\[\n\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} L(\\theta | x_1, x_2, ..., x_n)\n\\]\nor equivalently, maximizes the log-likelihood function:\n\\[\n\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\log L(\\theta | x_1, x_2, ..., x_n)\n\\]\n\nThe Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a statistical model by finding the values of the parameters that maximize the likelihood function. The likelihood function is the probability of observing the data, given the parameters of the model. The MLE estimator is the set of parameter values that maximize the likelihood function. In other words, the MLE is the set of parameter values that make the observed data most probable, given the assumed probability distribution. The likelihood function is typically the product or the sum (depending on whether the observations are assumed to be independent or not) of the probabilities or probability densities of the observations, evaluated at the values of the parameters.\nThe MLE estimator has desirable statistical properties, such as consistency, efficiency, and asymptotic normality, under certain regularity conditions on the likelihood function and the parameter space. However, it is important to note that the MLE is not always the best estimator for a given problem, and other estimation methods may be more appropriate depending on the specific characteristics of the data and the model.\nThe likelihood function is the joint probability density (or mass) function of the data, viewed as a function of the parameters, and we find the maximum of this function by differentiating it with respect to the parameters and setting the derivative to zero.\n\n\n\n0.2 MLE of OLS\nIn a linear regression, the maximum likelihood estimate of the ordinary least squares (OLS) coefficients is equivalent to the least squares estimate. To derive this, we assume that \\(\\epsilon_i \\sim N(0,\\sigma^2)\\) and that the observations are independent. Then, the likelihood function for the data \\(Y = (Y_1, Y_2, \\dots, Y_n)\\) is:\n\\[\nL(Y|\\theta) =L(Y|\\beta,\\sigma^2) = (2\\pi\\sigma^2)^{-\\frac{n}{2}} e^{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\beta)^2}\n\\]\nwhere \\(X_i\\) is the \\(i\\) th row of the design matrix \\(X\\) and \\(\\beta\\) is the vector of regression coefficients.\nTo find the maximum likelihood estimates of \\(\\beta\\) and \\(\\sigma^2\\), we maximize the likelihood function with respect to these parameters. Taking the log of the likelihood function and simplifying, we obtain:\n\\[\n\\log L(Y|\\theta) = \\log L(Y|\\beta,\\sigma^2) = -\\frac{n}{2} \\log (2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\beta)^2\n\\]\nTo maximize this function with respect to \\(\\beta\\), we differentiate with respect to \\(\\beta\\) and set the derivative to zero, \\(\\frac{\\partial}{\\partial \\theta} \\log L(Y|\\theta) = 0\\):\nSolving for \\(\\beta\\), we obtain:\n\\[\n\\frac{\\partial}{\\partial \\theta} \\log L(Y|\\beta,\\sigma^2) = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n 2X_i(Y_i - X_i\\beta) = 0\n\\]\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\] , which is the OLS estimate of \\(\\beta\\).\nSolving for \\(\\sigma^2\\), we differentiate with respect to \\(sigma^2\\) and set the derivative to zero:\n\\[\n\\frac{\\partial}{\\partial \\sigma^2} log L(Y|\\beta,\\sigma^2) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^n (Y_i - X_i\\beta)^2 = 0\n\\] \\[\n\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (Y_i - X_i\\beta)^2}{n}\n\\]\nwhich is the OLS estimate of \\(\\sigma^2\\).\nTherefore, we see that the maximum likelihood estimates of \\(\\beta\\) and \\(\\sigma^2\\) in linear regression with normally distributed errors are equivalent to the OLS estimates of these parameters.\n\n\n0.3 Statistical Bias\nStatistical bias refers to a systematic error or deviation in the results of a statistical analysis that is caused by factors other than chance. A biased estimator is one that consistently produces estimates that are systematically different from the true value of the parameter being estimated.\n\n\n\n\n\n\nThere are 5 Types of bias\n\n\n\nThe above article discusses five types of statistical bias that analysts, data scientists, and other business professionals should be aware of to minimize their effects on the final results.\n\nselection bias: data selection methods are not truly random, leading to unequal representation of the population.\nbias in assignment: pre-existing differences between groups in an experiment can affect the outcome, a.k.a allocation bias, treatment assignment bias, or exposure assignment bias.\nconfounders: additional variables not accounted for in the experimental design can impact the results.\nself-serving bias: individuals tend to downplay undesirable qualities and overemphasize desirable ones a.k.a cognitive bias. In other words, people tend to take credit for their successes and blame outside factors for their failures.\nexperimenter expectations: researchers can unconsciously influence the data through verbal or non-verbal cues.\n\nBeing aware of these biases can lead to better models and more reliable insights for data-backed business decisions.\nSource: Article Written by Jenny Gutbezahl\n\n\nIt is important to detect and correct for bias in statistical analyses, as biased estimates can lead to incorrect conclusions and decisions. One way to correct for bias is to use an unbiased estimator, which is one that has a zero bias, i.e., its expected value is equal to the true value of the parameter being estimated.\n\nDefinition 3 An estimator \\(\\hat{\\theta}\\) is said to be biased if\n\\[\n\\operatorname{E}(\\hat{\\theta})\\ne \\theta\n\\]\nwhere \\(\\operatorname{E}(\\hat{\\theta})\\) is the expected value of the estimator \\(\\hat{\\theta}\\), and \\(\\theta\\) is the true value of the parameter being estimated.\n\nAn estimator is said to be unbiased if its expected value is equal to the true value of the parameter being estimated. In other words, an estimator is unbiased if, on average, it gives an estimate that is equal to the true value of the parameter."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html",
    "title": "Variables",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n어떤 현상이나 사물의 의미를 추상적인 용어를 사용하여 관념적으로 구성한 것\nex) 성 (gender)\n\n\n\n\n\n한 연속선상에서 둘 이상의 값을 가지는 개념\nex) 성별(sex)\n\n\n\n\n\n결코 변하지 않는 단 하나의 값\nex) 남자, 여자\n\n\n\n\n\n\n\n\n독립변수 (independent variable) 는 조사하고자 하는 사건이나 상황을 독립적으로 발생시키는 원인이 되는 변수로서 원인변수, 설명변수, 예측변수 라고도 부름\nif s + v, s + v 에서 if s + v 에서 독립 변수의 정보를 추출해낼 수 있다.\n예를 들어, 사람이 운동을 하면 근육량이 늘어 난다.\n여기서 사람이 운동을 하면 부분에서 독립 변수를 뽑아낸다면 운동의 유무가 독립변수가 될 수 있다. 근육량의 변화는 운동의 유무에 따라 결정되는 원인 부분이기 때문이다.\n즉, 운동 (독립변수) \\(\\rightarrow\\) 근육량 (종속변수)\n\n\n\n\n\n종속변수 (dependent variable) 는 다른 변수에 영향을 받는 변수\n다른 변수에 영향을 미칠 수 없는 변수\n인과 관계에서 결과(effect)를 나타냄\n결과변수, 피설명변서, 피예측변수, 반응변수, 가설적 변수라고도 부른다.\nif s + v, s + v 에서 s + v 에서 독립 변수의 정보를 추출해낼 수 있다.\n예를 들어, 사람이 운동을 하면 근육량이 늘어난다.\n여기서 근육량이 늘어난다. 부분에서 종속 변수가 뽑힌다. 근육량이 종속변수가 될 수 있다. 근육량은 운동의 유무에 따라 그 효과가 영향을 받는 결과 부분이기 때문이다.\n즉, 운동 (독립변수) \\(\\rightarrow\\) 근육량 (종속변수)\n\n\n\n\n\n매개변수 (intervening or mediating variable)는 두 변수는 서로 직접적인 관계가 약하거나 없는데 두 변수가 간접적으로 관계를 갖는 것처럼 보이도록 하는 변수\n독립변수와 종속변수 사이에 개입하여 두 변수를 연결하는 변수\n매개변수는 독립변수의 결과변수인 동시에 종속변수의 원인이 되는 변수\n독립변수 \\(\\rightarrow\\) 매개변수 \\(\\rightarrow\\) 종속변수\n\n여기서 독립변수와 종속변수는 직접적인 관계가 없지만, 매개 변수에 의해 마치 관계를 갖는 것처럼 보임\n\n예를 들어, 매개 변수의 효과가 선행연구가 뒷받침 되거나 자명한 관계가 보여야 한다.\n\n여가 문화 요인의 매개효과를 고려한 장애인의 경제상태가 삶의 만족도에 미치는 영향\n독립변수: 경제 상태\n종속변수: 삶의 만족도\n매개변수: 여가 문화\n경제 상태는 여가 문화의 독립 변수로서 돈이 많으면 여가 문화를 즐길 수 있는 비용을 지불할 수 있는 원인이 된다.\n여가 문화는 삶의 만족도의 독립 변수로서 여가 문화를 즐김으로 인해 본인이 원하는 취미 활동을 향유함으로써 삶의 만족도가 증가하는 원인이 된다.\n\n예를 들어, 학업 집중도의 매개 효과를 고려한 수면의 질이 학업 성취도에 미치는 영향\n\n독립변수: 수면의 질\n종속변수: 학업 성취도\n매개변수: 학업 집중도\n수면의 질은 학업 집중도에 대한 독립 변수로서 수면의 질이 높으면 졸지 않고 집중도가 올라가는 경향의 원인이 된다.\n학업 집중도는 학업성취도의 독립 변수로서 학업 집중도가 올라가면 학업 성취도가 올라감으로써 학업 성취도 상승에 대한 원인된다.\n하지만, 수면의 질이 학업 성취도에 대한 연관성은 반드시 보장되진 않는다.\n\n데이터 분석에서 다음과 같은 증거가 있어야 한다.\n\n데이터 분석에서 경제 상태 (독립 변수)와 삶의 만족도 (종속 변수)가 강한 상관관계가 나타나야한다\n경제 상태 (독립 변수)와 여가 문화 (매개 변수)가 강한 상관관계가 나타나야한다.\n여가 문화 (매개 변수)와 삶의 만족도 (종속 변수)와 강한 상관관계가 나타나야한다.\n완전 매개의 경우: 회귀 모형에 경제 상태 (독립 변수)와 여가 문화(매개 변수)를 넣었을 때 사람의 만족도 (종속 변수)에 대한 경제 상태 (독립 변수)의 유의성이 낮아야 하고 여가 문화 (매개 변수)의 유의성이 높으면 여가 문화는 완전 매개 변수로 간주될 수 있다.\n부분 매개의 경우: 경제 상태 (매개 변수)의 유의성이 기준치 이상이지만 낮아졌을 경우 여가 문화는 부분 매개 변수로 간주될 수 있다.\n\n한계점\n\n매개 변수는 종종 성격, 지능, 태도, 문화와 같은 가상의 구성물일 수 있다.\n그래서, 정량화 시키기 힘들 수 있다.\n\n\n\n\n\n\n조절변수 (moderating variable) 는 독립변수가 종속변수에 원인이 되며 영향을 미치는 영향력의 강도, 세기 , 방향을 조절하는 변수\n독립변수 \\(\\xrightarrow{\\text{조절변수}}\\) 종속변수\n예를 들어, 따돌림 피해자 학생에 대한 교사의 지지도에 따라 집단따돌림이 자존감에 미치는 영향\n\n독립변수: 집단 따돌림\n종속변수: 학생의 자존감\n매개변수: 교사의 지지도\n집단 따돌림은 학생의 자존감에 악영향을 미친다는 선행 연구나 자명한 관계가 있다고 가정\n교사가 피해자 학생에게 무관심하다면 피해자 학생에 대한 학교폭력이 가중되면서 자존감이 더 하락할 것.\n교사가 피해자 학생에게 칭찬과 응원을 한다면 피해자 학생은 자존감이 올라갈 것.\n\n예를 들어, 자녀유무에 따라 부부간 의사소통이 이혼에 미치는 영향\n\n독립변수: 부부간 의사소통\n종속변수: 이혼률\n매개변수: 자녀 유무\n부부간 의사소통이 없다면 이혼률이 상승한다는 선행연구나 자명한 관계가 있다고 가정\n부부간 의사소통이 없다면 이혼가능성이 올라간다는 전제가 있고 자녀가 있다면 이혼 가능성이 낮아 질 수 있음\n\n데이터 분석에서 다음과 같은 증거가 있어야 한다.\n\n데이터 분석에서 독립 변수와 종속 변수가 강한 상관관계가 나타나야 한다\n회귀 모형에 독립 변수와 조절 변수의 interaction term을 넣었을 때 독립변수, 조절변수, interaction term의 유의성이 있어야한다.\n\n\n\n\n\n\n외생변수 (extraneous variable)는 독립변수 외에 종속변수에 영향을 주는 변수로서 원래 관계가 없는 변수를 관계가 있는 것처럼 만들어 종속 변수에 대한 가짜 독립변수가 만들어진다 (가식적 관계 또는 허위관계)\n외생변수를 통제하면 관계가 있는 것으로 나타났던 독립변수와 종속변수의 관계가 사라짐\n외새변수는 반드시 통제해줘야 함\n구조 방정식에서 독립 변수의 개념, 다른 변수에 영향을 주는 변수. 즉 기존에 원인인 것처럼 보이던 가짜 독립 변수가 아니라 실제 원인인 외생변수가 종속변수에 대한 진짜 독립변수.\n독립변수 \\(\\xleftarrow{\\text{+}}\\) 외생변수 \\(\\xrightarrow{\\text{+}}\\) 종속변수\n예를 들어, 무릎이 쑤시면 비가 온다.\n\n기압이 낮으면 무릎의 통증을 유발하고 동시에 비가 내리는 원인이 된다. 기압의 존재를 찾아내지 못하면 마치 무릎의 통증이 비를 내리게 하는 원인인 것 처럼 인식한다.\n대기압이 낮으면 비구름이 고기압에서 저기압으로 이동하면서 비를 내리게 하고 동시에 대기압이 낮을때 상대적으로 체내의 내부 기압이 높아져 팽창하여 관절이 부딪히게 된다.\n\n외생 변수를 간과할 때 발생하는 문제\n\nselection bias 발생\n\nsampling bias or ascertainment biase: 특정 samples이 포함되거나 포함되지 않음\nattrition bias: study에 중도하차한 samples이 study에 남은 samples과 systematic difference 발생\nsurvivorship bias: 결론 도출시 연구자 또는 분석가가 전체 cases를 고려하지 않고 성공 cases에만 집중하여 결론도출\nNonresponse bias: 설문에 응답하지 않은 samples이 설문에 응답한 samples과 다르게 선정됨\nundercoverage biase: 모집단의 일부가 sample에 대표성이 결여\n\n\n\n\n\n\n\n내생변수 (intraneous variable)는 구조 방정식에서 사용되는 용어로 다른 변수로부터 영향을 받는 변수로서 모형 안에서 그 값이 결정되는 변수\n구조 방정식에서 종속변수에 해당.\n구조 방정식은 수많은 변수들을 한번에 고려하는 모형으로 네트 워크 분석이 요구된다. 구조 방정식 안에서는\n\n외생변수 (원인) \\(\\rightarrow\\) 내생변수 (결과) 의 관계를 갖는다.\n이때, 외생 변수와 내생 변수에 많은 변수들이 관계를 맺는다.\n\n\n\n\n\n\n억압변수 (suppressor variable)은 원래 관계가 있는데 관계가 없는 것처럼 보이게 하는 변수\n독립변수와 종속변수 중 하나의 변수와는 양의 상관관계가 있고 다른 하나의 변수와는 음의 상관관계가 있어 독립변수와 종속변수 간에 마치 아무런 관계가 없는 것처럼 보이게 만드는 변수\n두 변수 간에 실제 존재하는 관계를 드러나지 못하게 억누른다는 의미에서 억밥 변수라고 부름\n억압 변수를 통제하면 독립변수와 종속 변수의 참된 관계가 드러남\n외생 변수와 마찬가지로 억압변수는 반드시 통제되어야한다.\n독립변수 (+) \\(\\leftarrow\\) 억압변수 \\(\\rightarrow\\) (-) 종속변수\n예를 들어, 교육 수준이 높으면 소득수준이 낮을 것이다.\n\n위의 명제는 마치 거짓인 것처럼 보이지만 고령자란 변수를 억압변수로 설정하면 교육 수준이 높은 고령자의 경우 젊었을 때 돈을 많이 벌어놨기 때문에 은퇴후 돈을 벌지 않아도 된다. 그러므로 소득 수준이 낮기 때문에 위의 명제가 참이 될 수 있다.\n\n\n\n\n\n\n통제변수 (control variable)는 독립변수와 종속변수의 관계에 영향을 미칠만한 제 3의 변수로서 외생변수, 매개변수, 조절변수, 억제변수들 중 연구자가 중점적으로 보고자 하는 변수들의 실제적 관계를 검증하기 위해 조사과정에 영향을 미치지 않도록 실제로 통제하는 변수\n즉, 통제변수란 독립변수와 종속변수의 인과관계에 영향을 주는 제 3의 변수 중 조사설계에서 조사자가 통제하려는 변수\n일반적으로 하나의 종속변수에 수많은 독립변수가 있지만 분석가가 관심있어하는 독립 변수의 종속변수에 대한 순수한 효과를 관찰하기 위해 제 3의 변수들 (통제 변수)을 통제하거나 일정하게 유지해줘야 관찰이 가능하다.\n\n\n\n\n\n\n\n\n이산변수는 명목척도 (nominal scale), 서열척도(ordinal scale)로 측정되는 변수\n이산이란 각 값의 사이가 떨어져 있어 그 사이의 값은 아무런 의미가 없다는 뜻\n성별, 종교 등\n예를 들어, 남성을 1, 여성을 2라고 값을 부여했을 때 1과 2사이의 값은 존재할 수 없음\n\n\n\n\n\n연속변수는 등간척도 (interval scale)와 비율척도 (ratio scale)로 측정된 변수\n키, 연령, 무게 등\n\n\n\n\n\n양이 아닌 해석 또는 의미가 중요한 변수. 성별 종교 등 (성별 1,2가 양을 의미하는 것이 아님)\n\n\n\n\n\n키 연령 무게 등 - 측정값이 양을 의미함\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#concept",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#concept",
    "title": "Variables",
    "section": "",
    "text": "어떤 현상이나 사물의 의미를 추상적인 용어를 사용하여 관념적으로 구성한 것\nex) 성 (gender)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#concept-of-variable",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#concept-of-variable",
    "title": "Variables",
    "section": "",
    "text": "한 연속선상에서 둘 이상의 값을 가지는 개념\nex) 성별(sex)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#constant",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#constant",
    "title": "Variables",
    "section": "",
    "text": "결코 변하지 않는 단 하나의 값\nex) 남자, 여자"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#독립변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#독립변수",
    "title": "Variables",
    "section": "",
    "text": "독립변수 (independent variable) 는 조사하고자 하는 사건이나 상황을 독립적으로 발생시키는 원인이 되는 변수로서 원인변수, 설명변수, 예측변수 라고도 부름\nif s + v, s + v 에서 if s + v 에서 독립 변수의 정보를 추출해낼 수 있다.\n예를 들어, 사람이 운동을 하면 근육량이 늘어 난다.\n여기서 사람이 운동을 하면 부분에서 독립 변수를 뽑아낸다면 운동의 유무가 독립변수가 될 수 있다. 근육량의 변화는 운동의 유무에 따라 결정되는 원인 부분이기 때문이다.\n즉, 운동 (독립변수) \\(\\rightarrow\\) 근육량 (종속변수)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#종속변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#종속변수",
    "title": "Variables",
    "section": "",
    "text": "종속변수 (dependent variable) 는 다른 변수에 영향을 받는 변수\n다른 변수에 영향을 미칠 수 없는 변수\n인과 관계에서 결과(effect)를 나타냄\n결과변수, 피설명변서, 피예측변수, 반응변수, 가설적 변수라고도 부른다.\nif s + v, s + v 에서 s + v 에서 독립 변수의 정보를 추출해낼 수 있다.\n예를 들어, 사람이 운동을 하면 근육량이 늘어난다.\n여기서 근육량이 늘어난다. 부분에서 종속 변수가 뽑힌다. 근육량이 종속변수가 될 수 있다. 근육량은 운동의 유무에 따라 그 효과가 영향을 받는 결과 부분이기 때문이다.\n즉, 운동 (독립변수) \\(\\rightarrow\\) 근육량 (종속변수)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#매개변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#매개변수",
    "title": "Variables",
    "section": "",
    "text": "매개변수 (intervening or mediating variable)는 두 변수는 서로 직접적인 관계가 약하거나 없는데 두 변수가 간접적으로 관계를 갖는 것처럼 보이도록 하는 변수\n독립변수와 종속변수 사이에 개입하여 두 변수를 연결하는 변수\n매개변수는 독립변수의 결과변수인 동시에 종속변수의 원인이 되는 변수\n독립변수 \\(\\rightarrow\\) 매개변수 \\(\\rightarrow\\) 종속변수\n\n여기서 독립변수와 종속변수는 직접적인 관계가 없지만, 매개 변수에 의해 마치 관계를 갖는 것처럼 보임\n\n예를 들어, 매개 변수의 효과가 선행연구가 뒷받침 되거나 자명한 관계가 보여야 한다.\n\n여가 문화 요인의 매개효과를 고려한 장애인의 경제상태가 삶의 만족도에 미치는 영향\n독립변수: 경제 상태\n종속변수: 삶의 만족도\n매개변수: 여가 문화\n경제 상태는 여가 문화의 독립 변수로서 돈이 많으면 여가 문화를 즐길 수 있는 비용을 지불할 수 있는 원인이 된다.\n여가 문화는 삶의 만족도의 독립 변수로서 여가 문화를 즐김으로 인해 본인이 원하는 취미 활동을 향유함으로써 삶의 만족도가 증가하는 원인이 된다.\n\n예를 들어, 학업 집중도의 매개 효과를 고려한 수면의 질이 학업 성취도에 미치는 영향\n\n독립변수: 수면의 질\n종속변수: 학업 성취도\n매개변수: 학업 집중도\n수면의 질은 학업 집중도에 대한 독립 변수로서 수면의 질이 높으면 졸지 않고 집중도가 올라가는 경향의 원인이 된다.\n학업 집중도는 학업성취도의 독립 변수로서 학업 집중도가 올라가면 학업 성취도가 올라감으로써 학업 성취도 상승에 대한 원인된다.\n하지만, 수면의 질이 학업 성취도에 대한 연관성은 반드시 보장되진 않는다.\n\n데이터 분석에서 다음과 같은 증거가 있어야 한다.\n\n데이터 분석에서 경제 상태 (독립 변수)와 삶의 만족도 (종속 변수)가 강한 상관관계가 나타나야한다\n경제 상태 (독립 변수)와 여가 문화 (매개 변수)가 강한 상관관계가 나타나야한다.\n여가 문화 (매개 변수)와 삶의 만족도 (종속 변수)와 강한 상관관계가 나타나야한다.\n완전 매개의 경우: 회귀 모형에 경제 상태 (독립 변수)와 여가 문화(매개 변수)를 넣었을 때 사람의 만족도 (종속 변수)에 대한 경제 상태 (독립 변수)의 유의성이 낮아야 하고 여가 문화 (매개 변수)의 유의성이 높으면 여가 문화는 완전 매개 변수로 간주될 수 있다.\n부분 매개의 경우: 경제 상태 (매개 변수)의 유의성이 기준치 이상이지만 낮아졌을 경우 여가 문화는 부분 매개 변수로 간주될 수 있다.\n\n한계점\n\n매개 변수는 종종 성격, 지능, 태도, 문화와 같은 가상의 구성물일 수 있다.\n그래서, 정량화 시키기 힘들 수 있다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#조절변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#조절변수",
    "title": "Variables",
    "section": "",
    "text": "조절변수 (moderating variable) 는 독립변수가 종속변수에 원인이 되며 영향을 미치는 영향력의 강도, 세기 , 방향을 조절하는 변수\n독립변수 \\(\\xrightarrow{\\text{조절변수}}\\) 종속변수\n예를 들어, 따돌림 피해자 학생에 대한 교사의 지지도에 따라 집단따돌림이 자존감에 미치는 영향\n\n독립변수: 집단 따돌림\n종속변수: 학생의 자존감\n매개변수: 교사의 지지도\n집단 따돌림은 학생의 자존감에 악영향을 미친다는 선행 연구나 자명한 관계가 있다고 가정\n교사가 피해자 학생에게 무관심하다면 피해자 학생에 대한 학교폭력이 가중되면서 자존감이 더 하락할 것.\n교사가 피해자 학생에게 칭찬과 응원을 한다면 피해자 학생은 자존감이 올라갈 것.\n\n예를 들어, 자녀유무에 따라 부부간 의사소통이 이혼에 미치는 영향\n\n독립변수: 부부간 의사소통\n종속변수: 이혼률\n매개변수: 자녀 유무\n부부간 의사소통이 없다면 이혼률이 상승한다는 선행연구나 자명한 관계가 있다고 가정\n부부간 의사소통이 없다면 이혼가능성이 올라간다는 전제가 있고 자녀가 있다면 이혼 가능성이 낮아 질 수 있음\n\n데이터 분석에서 다음과 같은 증거가 있어야 한다.\n\n데이터 분석에서 독립 변수와 종속 변수가 강한 상관관계가 나타나야 한다\n회귀 모형에 독립 변수와 조절 변수의 interaction term을 넣었을 때 독립변수, 조절변수, interaction term의 유의성이 있어야한다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#외생변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#외생변수",
    "title": "Variables",
    "section": "",
    "text": "외생변수 (extraneous variable)는 독립변수 외에 종속변수에 영향을 주는 변수로서 원래 관계가 없는 변수를 관계가 있는 것처럼 만들어 종속 변수에 대한 가짜 독립변수가 만들어진다 (가식적 관계 또는 허위관계)\n외생변수를 통제하면 관계가 있는 것으로 나타났던 독립변수와 종속변수의 관계가 사라짐\n외새변수는 반드시 통제해줘야 함\n구조 방정식에서 독립 변수의 개념, 다른 변수에 영향을 주는 변수. 즉 기존에 원인인 것처럼 보이던 가짜 독립 변수가 아니라 실제 원인인 외생변수가 종속변수에 대한 진짜 독립변수.\n독립변수 \\(\\xleftarrow{\\text{+}}\\) 외생변수 \\(\\xrightarrow{\\text{+}}\\) 종속변수\n예를 들어, 무릎이 쑤시면 비가 온다.\n\n기압이 낮으면 무릎의 통증을 유발하고 동시에 비가 내리는 원인이 된다. 기압의 존재를 찾아내지 못하면 마치 무릎의 통증이 비를 내리게 하는 원인인 것 처럼 인식한다.\n대기압이 낮으면 비구름이 고기압에서 저기압으로 이동하면서 비를 내리게 하고 동시에 대기압이 낮을때 상대적으로 체내의 내부 기압이 높아져 팽창하여 관절이 부딪히게 된다.\n\n외생 변수를 간과할 때 발생하는 문제\n\nselection bias 발생\n\nsampling bias or ascertainment biase: 특정 samples이 포함되거나 포함되지 않음\nattrition bias: study에 중도하차한 samples이 study에 남은 samples과 systematic difference 발생\nsurvivorship bias: 결론 도출시 연구자 또는 분석가가 전체 cases를 고려하지 않고 성공 cases에만 집중하여 결론도출\nNonresponse bias: 설문에 응답하지 않은 samples이 설문에 응답한 samples과 다르게 선정됨\nundercoverage biase: 모집단의 일부가 sample에 대표성이 결여"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#내생변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#내생변수",
    "title": "Variables",
    "section": "",
    "text": "내생변수 (intraneous variable)는 구조 방정식에서 사용되는 용어로 다른 변수로부터 영향을 받는 변수로서 모형 안에서 그 값이 결정되는 변수\n구조 방정식에서 종속변수에 해당.\n구조 방정식은 수많은 변수들을 한번에 고려하는 모형으로 네트 워크 분석이 요구된다. 구조 방정식 안에서는\n\n외생변수 (원인) \\(\\rightarrow\\) 내생변수 (결과) 의 관계를 갖는다.\n이때, 외생 변수와 내생 변수에 많은 변수들이 관계를 맺는다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#억압변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#억압변수",
    "title": "Variables",
    "section": "",
    "text": "억압변수 (suppressor variable)은 원래 관계가 있는데 관계가 없는 것처럼 보이게 하는 변수\n독립변수와 종속변수 중 하나의 변수와는 양의 상관관계가 있고 다른 하나의 변수와는 음의 상관관계가 있어 독립변수와 종속변수 간에 마치 아무런 관계가 없는 것처럼 보이게 만드는 변수\n두 변수 간에 실제 존재하는 관계를 드러나지 못하게 억누른다는 의미에서 억밥 변수라고 부름\n억압 변수를 통제하면 독립변수와 종속 변수의 참된 관계가 드러남\n외생 변수와 마찬가지로 억압변수는 반드시 통제되어야한다.\n독립변수 (+) \\(\\leftarrow\\) 억압변수 \\(\\rightarrow\\) (-) 종속변수\n예를 들어, 교육 수준이 높으면 소득수준이 낮을 것이다.\n\n위의 명제는 마치 거짓인 것처럼 보이지만 고령자란 변수를 억압변수로 설정하면 교육 수준이 높은 고령자의 경우 젊었을 때 돈을 많이 벌어놨기 때문에 은퇴후 돈을 벌지 않아도 된다. 그러므로 소득 수준이 낮기 때문에 위의 명제가 참이 될 수 있다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#통제변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#통제변수",
    "title": "Variables",
    "section": "",
    "text": "통제변수 (control variable)는 독립변수와 종속변수의 관계에 영향을 미칠만한 제 3의 변수로서 외생변수, 매개변수, 조절변수, 억제변수들 중 연구자가 중점적으로 보고자 하는 변수들의 실제적 관계를 검증하기 위해 조사과정에 영향을 미치지 않도록 실제로 통제하는 변수\n즉, 통제변수란 독립변수와 종속변수의 인과관계에 영향을 주는 제 3의 변수 중 조사설계에서 조사자가 통제하려는 변수\n일반적으로 하나의 종속변수에 수많은 독립변수가 있지만 분석가가 관심있어하는 독립 변수의 종속변수에 대한 순수한 효과를 관찰하기 위해 제 3의 변수들 (통제 변수)을 통제하거나 일정하게 유지해줘야 관찰이 가능하다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#이산변수-비연속변수-불연속변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#이산변수-비연속변수-불연속변수",
    "title": "Variables",
    "section": "",
    "text": "이산변수는 명목척도 (nominal scale), 서열척도(ordinal scale)로 측정되는 변수\n이산이란 각 값의 사이가 떨어져 있어 그 사이의 값은 아무런 의미가 없다는 뜻\n성별, 종교 등\n예를 들어, 남성을 1, 여성을 2라고 값을 부여했을 때 1과 2사이의 값은 존재할 수 없음"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#연속-변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#연속-변수",
    "title": "Variables",
    "section": "",
    "text": "연속변수는 등간척도 (interval scale)와 비율척도 (ratio scale)로 측정된 변수\n키, 연령, 무게 등"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#질적-변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#질적-변수",
    "title": "Variables",
    "section": "",
    "text": "양이 아닌 해석 또는 의미가 중요한 변수. 성별 종교 등 (성별 1,2가 양을 의미하는 것이 아님)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#양적변수",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#양적변수",
    "title": "Variables",
    "section": "",
    "text": "키 연령 무게 등 - 측정값이 양을 의미함"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-11-07-variable/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-11-07-variable/index.html#blog-guide-map-link",
    "title": "Variables",
    "section": "",
    "text": "Statistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html",
    "href": "docs/blog/posts/statistics/guide_map/index.html",
    "title": "Content List, Statistics",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#basic",
    "href": "docs/blog/posts/statistics/guide_map/index.html#basic",
    "title": "Content List, Statistics",
    "section": "Basic",
    "text": "Basic\n\nProbability Theory\n\n2023-02-05, Set Theory\n2023-02-05, [Basics of Probability Theory - Axiomatic Foundations]\n2023-02-05, [Basics of Probability Theory - Calculus of Probabilities]\n2023-02-05, Basics of Probability Theory - Probability\n2023-02-05, Conditional Probability\n2023-02-05, [Independence]\n2023-02-05, Bayes’ Rule\n2023-02-05, Random Variable\n1111-11-11, Probability Distribution\n\n\n\nTransformations and Expectations\n\n2023-02-21, Transformation of Random Variables\n1111-11-11, Expected Value vs Realizaed Value\n1111-11-11, Variance\n1111-11-11, Covariance and Correlation\n2023-02-28, Moment Generating Function, MGF\n\n\n\nExponential Family Distributions\n\nDiscrete Random Variable\n\n2023-02-27,Bernoulli Distribution\n2023-02-28,Binomial Distribution\n2023-03-01,Poisson Distribution\n2023-03-01,Geometric Distribution\n1111-11-11, Hypergeometric Distribution\n\nContinuous Random Variable\n\n1111-11-11, Normal Distribution\n1111-11-11, Exponential Distribution\n1111-11-11, Beta Distribution\n1111-11-11, Chi-squared Distribution\n\n1111-11-11,\n\n\n\nMultiple Random Variables\n\n1111-11-11, Joint Distribution and Marginal Distribution\n\n\n\nPoint Estimation\n\n1111-11-11, Estimation Methods - Method of Moments\n2023-03-29, Estimation Methods - Maximum Likelihood Estimation & Statistical Bias\n1111-11-11, Estimation Methods - Bayesian Estimation\n1111-11-11, Estimation Methods - The EM Algorithm\n1111-11-11, Evaluation Methods of Estimators - Mean Squared Error\n1111-11-11, Evaluation Methods of Estimators - Best Unbiased Estimators\n1111-11-11, Evaluation Methods of Estimators - Sufficiency and Unbiasedness\n1111-11-11, Evaluation Methods of Estimators - Loss Function Optimality"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#hypothesis-testing",
    "href": "docs/blog/posts/statistics/guide_map/index.html#hypothesis-testing",
    "title": "Content List, Statistics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n1111-11-11, Hypothesis Testing\n1111-11-11, Permutation Test\n\n\nMethods of Finding Tests\n\n1111-11-11, Likelihood Ratio Tests\n1111-11-11, Bayesian Tests\n1111-11-11, Union-Intersection and Intersection-Union Tets\n\n\n\nMethods of Evaluating Tests\n\n1111-11-11, Power\n1111-11-11, Error Proabilities and the Power Function\n1111-11-11, Most Powerful Tests\n2022-12-28, p-values\n1111-11-11, Loss Function Optimality\n1111-11-11, Multiple Testing\n1111-11-11, Sample Size Calculation\n1111-11-11, A/B Testing\n2023-01-07, ANOVA\n\n2023-01-27, ANCOVA\n2023-01-27, repeated measures ANOVA\n2023-01-28, MANOVA"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#categorical-data-analysis",
    "href": "docs/blog/posts/statistics/guide_map/index.html#categorical-data-analysis",
    "title": "Content List, Statistics",
    "section": "Categorical Data Analysis",
    "text": "Categorical Data Analysis\n\n1111-11-11, Introduction\n1111-11-11,\n1111-11-11,\n2022-12-28,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n2023-01-07,\n2023-01-27,\n2023-01-27,\n2023-01-28,"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#regression",
    "href": "docs/blog/posts/statistics/guide_map/index.html#regression",
    "title": "Content List, Statistics",
    "section": "Regression",
    "text": "Regression\n\n1111-11-11, Least Square and Simple Linear Regression\n1111-11-11, Multiple Linear Regression\n\n\nGeneralized Linear Models\n\n1111-11-11, Logistic Regression\n1111-11-11, Multinomial Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#longitudinal-data-analysis",
    "href": "docs/blog/posts/statistics/guide_map/index.html#longitudinal-data-analysis",
    "title": "Content List, Statistics",
    "section": "Longitudinal Data Analysis",
    "text": "Longitudinal Data Analysis\n\n2023-03-23, LDA (1) - Intro\n2023-03-23, LDA (2) - Concepts & Covariance Models\n2023-03-25, LDA (3) - WLS & REML\n2023-03-25, LDA (4) - Respiratory Infection Data Example\n2023-03-28, LDA (5) - Epileptic Seizures Data Example\n\n\nMixed Models\n\n1111-11-11, Linear Mixed Models"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#generalized-additive-models",
    "href": "docs/blog/posts/statistics/guide_map/index.html#generalized-additive-models",
    "title": "Content List, Statistics",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#survival-analysis",
    "href": "docs/blog/posts/statistics/guide_map/index.html#survival-analysis",
    "title": "Content List, Statistics",
    "section": "Survival Analysis",
    "text": "Survival Analysis\n\n1111-11-11, Cox-Hazard Model"
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/2_covariance_model.html",
    "href": "docs/blog/posts/statistics/LDA/2_covariance_model.html",
    "title": "LDA (2) - Concept & Covariance Models",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\\(y_{ij}\\) : the univariate response (i.e. scalar) for the \\(i\\) th subject at the \\(j\\) th occasion or measurement\n\nlater when I use the vector case, I will re-define this notation, but focus on the scalar case for now.\n\n\\(x_{ij}\\) : the predictor at time \\(t_{ij}\\), which is either a scalr or vector.\n\na scalar case: \\(x_{ij}\\) where \\(i\\) is the \\(i\\) th subject, and \\(j\\) is the \\(j\\) th measurement.\na vector case: \\(x_{ijk}\\) where \\(i\\) is the \\(i\\) th subject, \\(j\\) is the \\(j\\) th measurement, and \\(k \\in [1,p]\\) is the \\(k\\) th predictor.\nsometimes, covariate for different measurements could be the same. In this case, the notation could be written in \\(x_{i}\\)\n\nex) a gender does not change over time in the most cases.\n\n\n\\(i=1, \\dots, m\\) : i is the index for the \\(i\\) th subject\n\\(j=1, \\dots, n_i\\) : j is the index for the \\(j\\) th measurement of the \\(i\\) th subject\n\n\\({n_i}\\) is the number of measurements of the \\(i\\) th subject, each \\({n_i}\\) does not have to the same.\nbalanced desgin: \\({n_i}\\) is the same.\nunbalanced desgin: \\({n_i}\\) is different.\n\n\\(\\mathbf y_i\\) : a vector (not a matrix), \\((y_{i1},y_{i2},\\dots ,y_{in_i})\\) of the \\(i\\) th subject\n\\(\\mathbf Y\\) : the reponse matrix\n\\(\\mathbf X\\) : the predictor matrix\n\\(\\text{E}(y_{ij})\\) : \\(\\mu_{ij}\\)\n\\(\\text{E}(\\mathbf y_i)\\) : \\(\\mathbf \\mu_{i}\\)\n\\(\\text{Var}(\\mathbf y_i)\\) : \\(\\text{Var}(\\mathbf y_i)\\) is a variance-covariance matrix of the different measurement for the \\(i\\) th subject\n\nfor now, we do not care of the variance covariance of the different subjects because we assume that the measurements of different subjects are indpendent. \\[\n\\begin{bmatrix}\n\\text{Var}(y_{i1}) & \\text{Cov}( y_{i1}, y_{i2}) & \\dots & \\text{Cov}( y_{i1}, y_{in_i}) \\\\\n                           & \\text{Var}( y_{i2}) & \\dots & \\text{Cov}( y_{i2}, y_{in_i}) \\\\\n                             &                           & \\ddots & \\vdots \\\\\n                             &&                            \\dots & \\text{Var}( y_{in_i})\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nthe measurements for the same subject are not independent.\nthe measurements for the different subject are independent.\nsome correlation structures of the different measurements.\n\n\n\n\n\nMarginal Models\n\n\\(\\text{E}(y_{ij}) = \\mathbf x_{ij} \\mathbf \\beta\\)\n\\(\\text{Var}(\\mathbf y_i)= \\mathbf V_i\\)\nto build a marginal model, we just need info on the 3 things\n\nthe distribution : a multivariate normal distribution\nmean and variance-covariance\n\n\\(\\beta\\) is fixed. That’s why we call this marginal models ‘fixed effect’\n\n\n\n\n\n\n\n\nRecall\n\n\n\nWe find MLE for the linear regression with the 3 things: the normal distribution (iid), \\(\\mu\\) and \\(\\sigma^2\\)\n\n\n\nMixed Effects Models\n\n\\(\\text{E}(y_{ij}|\\mathbf \\beta_i) = \\mathbf x_{ij} \\mathbf \\beta_i\\)\n\\(\\mathbf \\beta_i = \\mathbf \\beta (\\text{fixed effect}) + \\mathbf u_i (\\text{subject-specific random effect})\\)\n\\(\\mathbf \\beta_i\\) is a random coefficient specific for the \\(i\\) th subject, That’s why we call this mixed effect models ‘random effect’\nsubject-specific random effect: differenct subjects have different \\(\\mathbf \\beta_i\\)\n\nTransition Models\n\n\\(\\text{E}(y_{ij}|y_{i,j-1},\\dots,y_{i,1},\\mathbf x_{ij})\\)\nMarkov Process: the response variable in the previous time point will affect the measurement in the current time point.\n\n\n\n\nConsider an example of a simple linear model (i.e., a univaiable linear model) \\[\ny_{ij}=\\beta_0+\\beta_1t_{ij} + \\epsilon_{ij}\n\\]\n\nmean part: \\(\\text{E}(y_{ij})\\)\nvariance part: \\(\\text{Var}(\\mathbf y_{i})=\\text{Var}(\\mathbf \\epsilon_{i})\\)\n\nmore often, a correlation matrix is used in LDA because correlation is more interpretable.\n\n\n\\[\n\\text{Corr}(\\mathbf y_i) =\n\\begin{bmatrix}\n1 & \\rho_{12}& \\dots & \\rho_{1n_i} \\\\\n\\rho_{21} & 1 & \\dots & \\rho_{2n_i} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\rho_{n_i1} & \\rho_{n_i2}& \\dots & 1\n\\end{bmatrix}\n\\]\n\nin this correlation matrix, there are \\(\\frac{n(n-1)}{2}\\) parameters to estimate\nin the mean part, there are 2 parameters, \\(\\mathbf \\beta\\) to estimate Likewise, the number of the estimators depends on the number of the measurements and the covriates.\n\nIn LDA, since the responses are multiple, we need to look into the correlation characteristics.\n\n\n\nIn empirical observations about the nature of the correlation among repeated measures,\n\ncorrelations among the repeated measures are usually positive\ncorrelations tend to decrease with increasing time separation\ncorrelations among repeated measures rarely approach zero\ncorrelations between any pair of repeated meausres regardless of distance in time is constrained by the reliability of the measurement process.\n\nif the measurement process is not very reliable or consistent, then even if two measurements are taken close together in time, their correlation will not be very strong. Similarly, if the measurement process is highly reliable or consistent, then two measurements taken far apart in time may still be highly correlated. Reliability refers to the degree to which a measurement process produces consistent and accurate results over time.\n\n\n\n\n\nThere are 2 types of covariance structure: unbalanced design and balanced design. For now, let’s focus on the balanced design.\n\n\n\nobservations for each subject are not made on the same grid\nthese observations can be made at different time points and different numbers of observations may be made for each subject.\nMissing observations falls into this category.\n\n\n\n\n\nobservations for each subject are made on the same grid and there is no missing data.\n\nnumber and timing of the repeated measurements are the same for all individuals.\n\nThen, \\(t_{ij}\\) can be denoted as \\(t_j\\) where \\(j \\in 1, \\dots, n\\) because the size of the measurements is the same (\\(n_i\\) is the same)\nThe covariance of the response variable \\(\\mathbf Y_{m\\times n}\\) :\n\n$$\n\\[\\begin{aligned}\n  \\text{Cov}(\\mathbf Y)\n  &=\\text{Cov}(\\mathbf y_1,\\dots,y_m) \\\\\n  &=\n  \\begin{bmatrix}\n    \\text{Var}(\\mathbf y_1) & 0 & \\dots & 0 \\\\\n    0 & \\text{Var}(\\mathbf y_2) & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\text{Var}(\\mathbf y_m)\n  \\end{bmatrix}\\\\\n  &=\n  \\begin{bmatrix}\n    \\begin{bmatrix}\n    \\text{Var}(y_{11}) & \\text{Cov}( y_{11}, y_{12}) & \\dots & \\text{Cov}( y_{11}, y_{1n_1}) \\\\\n                               & \\text{Var}( y_{12}) & \\dots & \\text{Cov}( y_{12}, y_{1n_1}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{1n_1})\n\\end{bmatrix} & 0 & \\dots & 0 \\\\\n    0 & \\begin{bmatrix}\n    \\text{Var}(y_{21}) & \\text{Cov}( y_{21}, y_{22}) & \\dots & \\text{Cov}( y_{21}, y_{in_2}) \\\\\n                               & \\text{Var}( y_{22}) & \\dots & \\text{Cov}( y_{22}, y_{in_2}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{2n_2})\n\\end{bmatrix} & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\begin{bmatrix}\n    \\text{Var}(y_{m1}) & \\text{Cov}( y_{m1}, y_{m2}) & \\dots & \\text{Cov}( y_{m1}, y_{mn_m}) \\\\\n                               & \\text{Var}( y_{m2}) & \\dots & \\text{Cov}( y_{m2}, y_{mn_m}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{mn_m})\n\\end{bmatrix}\n\n  \\end{bmatrix} \\\\\n  &=\n  \\begin{bmatrix}\n    \\Sigma_1 & 0 & \\dots & 0 \\\\\n    0 & \\Sigma_1 & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\Sigma_m\n  \\end{bmatrix}\n\\end{aligned}\\]\n$$\nIf we assume the covariance matrices for different subjects are the same, we can denote \\(\\text{Cov}(\\mathbf Y)=\\Sigma\\).\n\n\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho & \\rho & \\dots & \\rho \\\\\n    \\rho & 1 & \\rho & \\dots & \\rho \\\\\n    \\rho & \\rho & 1 & \\dots & \\rho \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho & \\rho & \\rho & \\dots & 1\n  \\end{bmatrix}\n\\]\n\ncompound symmetry is a.k.a Exchangeable\nAssume variance is constant across visits (say \\(\\sigma^2\\))\nAssume correlation between any two visits are constant (say \\(\\rho\\)).\nParsimonious: there are two parameters in the covariance, \\(\\sigma^2\\) and \\(\\rho\\) (computational benefit)\nWithout any contraint on \\(\\sigma^2\\), you will get closed form estimate.\nCovariance variance matrix is plugged into likelihood function to estimate 3 kinds of parameters \\(\\sigma^2\\), \\(\\rho\\), and \\(\\beta\\)\nThis structure is so parsimonuous that it could be unrealistic: not commonly used\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho_1 & \\rho_2 & \\dots & \\rho_{n-1} \\\\\n    \\rho_1 & 1 & \\rho_1 & \\dots & \\rho_{n-2} \\\\\n    \\rho_2 & \\rho_1 & 1 & \\dots & \\rho_{n-3} \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho_{n-1} & \\rho_{n-2} & \\rho_{n-3} & \\dots & 1\n  \\end{bmatrix}\n\\]\n\nToeplitz structure is more flexible than compound symmetry\nAssume variance is constant across visits and \\(\\text{Corr}(y_{ij}, y_{i,j+k}) = \\rho_k\\).\nAssume correlation among responses at adjacent measurements is constant.\nOnly suitable for measurements made at equal intervals of time between different measurement.\nWithout any contraint on \\(\\sigma^2\\), you will get closed form estimate.\nToeplitz covariance has free \\(n\\) parameters to estimate (\\(1\\) for variance and \\(n-1\\) correlation parameters)\nThe larger time differences, the smaller its correlations\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho^1 & \\rho^2 & \\dots & \\rho^{n-1} \\\\\n    \\rho^1 & 1 & \\rho^1 & \\dots & \\rho^{n-2} \\\\\n    \\rho^2 & \\rho^1 & 1 & \\dots & \\rho^{n-3} \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho^{n-1} & \\rho^{n-2} & \\rho^{n-3} & \\dots & 1\n  \\end{bmatrix}\n\\]\n\nA special case of toeplitz structure with \\(\\text{Corr}(y_{ij},y_{i,j+k})=\\rho^k\\)\nsimpler than toeplitz, only 2 parameters\nOnly suitable for measurements made at equal intervals of time between different measurement.\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho^1 & 0 & \\dots & 0 \\\\\n    \\rho^1 & 1 & \\rho^1 & \\dots & 0 \\\\\n    0 & \\rho^1 & 1 & \\dots & 0 \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & 0 & \\dots & 1\n  \\end{bmatrix}\n\\]\nLook at the more general case of the banded structure in Wiki.\n\nAssume correlation is 0 beyond some specified interval.\nCan be combined with the previous patterns.\nVery strong assumption about how quickly the correlation decays to 0 with increasing time separation.\n\n\n\n\n\nA generalization of autoregressive pattern\nThe most general and reasonable structure\nSuitable for unevenly spaced measurements, take actual time points (time difference), the larger time difference the smaller correlation\nAssumption that the variance of different measurements over time is the same, which can be easily generalized. You can put different variance on the diagonal.\nLet \\(\\{t_{i1},\\dots,t_{in_i}\\}\\) denote the observation times for the \\(i\\) th individual. Then, the correlation is \\(\\text{Corr}(Y_{ij} ,Y_{ik}) = \\rho^{|t_{ij}-t_{ik}|}\\)\nCorrelation decreases exponentially with the time separations between them.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/2_covariance_model.html#notations",
    "href": "docs/blog/posts/statistics/LDA/2_covariance_model.html#notations",
    "title": "LDA (2) - Concept & Covariance Models",
    "section": "",
    "text": "\\(y_{ij}\\) : the univariate response (i.e. scalar) for the \\(i\\) th subject at the \\(j\\) th occasion or measurement\n\nlater when I use the vector case, I will re-define this notation, but focus on the scalar case for now.\n\n\\(x_{ij}\\) : the predictor at time \\(t_{ij}\\), which is either a scalr or vector.\n\na scalar case: \\(x_{ij}\\) where \\(i\\) is the \\(i\\) th subject, and \\(j\\) is the \\(j\\) th measurement.\na vector case: \\(x_{ijk}\\) where \\(i\\) is the \\(i\\) th subject, \\(j\\) is the \\(j\\) th measurement, and \\(k \\in [1,p]\\) is the \\(k\\) th predictor.\nsometimes, covariate for different measurements could be the same. In this case, the notation could be written in \\(x_{i}\\)\n\nex) a gender does not change over time in the most cases.\n\n\n\\(i=1, \\dots, m\\) : i is the index for the \\(i\\) th subject\n\\(j=1, \\dots, n_i\\) : j is the index for the \\(j\\) th measurement of the \\(i\\) th subject\n\n\\({n_i}\\) is the number of measurements of the \\(i\\) th subject, each \\({n_i}\\) does not have to the same.\nbalanced desgin: \\({n_i}\\) is the same.\nunbalanced desgin: \\({n_i}\\) is different.\n\n\\(\\mathbf y_i\\) : a vector (not a matrix), \\((y_{i1},y_{i2},\\dots ,y_{in_i})\\) of the \\(i\\) th subject\n\\(\\mathbf Y\\) : the reponse matrix\n\\(\\mathbf X\\) : the predictor matrix\n\\(\\text{E}(y_{ij})\\) : \\(\\mu_{ij}\\)\n\\(\\text{E}(\\mathbf y_i)\\) : \\(\\mathbf \\mu_{i}\\)\n\\(\\text{Var}(\\mathbf y_i)\\) : \\(\\text{Var}(\\mathbf y_i)\\) is a variance-covariance matrix of the different measurement for the \\(i\\) th subject\n\nfor now, we do not care of the variance covariance of the different subjects because we assume that the measurements of different subjects are indpendent. \\[\n\\begin{bmatrix}\n\\text{Var}(y_{i1}) & \\text{Cov}( y_{i1}, y_{i2}) & \\dots & \\text{Cov}( y_{i1}, y_{in_i}) \\\\\n                           & \\text{Var}( y_{i2}) & \\dots & \\text{Cov}( y_{i2}, y_{in_i}) \\\\\n                             &                           & \\ddots & \\vdots \\\\\n                             &&                            \\dots & \\text{Var}( y_{in_i})\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/2_covariance_model.html#assumptions",
    "href": "docs/blog/posts/statistics/LDA/2_covariance_model.html#assumptions",
    "title": "LDA (2) - Concept & Covariance Models",
    "section": "",
    "text": "the measurements for the same subject are not independent.\nthe measurements for the different subject are independent.\nsome correlation structures of the different measurements."
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/2_covariance_model.html#for-continuous-responses",
    "href": "docs/blog/posts/statistics/LDA/2_covariance_model.html#for-continuous-responses",
    "title": "LDA (2) - Concept & Covariance Models",
    "section": "",
    "text": "Marginal Models\n\n\\(\\text{E}(y_{ij}) = \\mathbf x_{ij} \\mathbf \\beta\\)\n\\(\\text{Var}(\\mathbf y_i)= \\mathbf V_i\\)\nto build a marginal model, we just need info on the 3 things\n\nthe distribution : a multivariate normal distribution\nmean and variance-covariance\n\n\\(\\beta\\) is fixed. That’s why we call this marginal models ‘fixed effect’\n\n\n\n\n\n\n\n\nRecall\n\n\n\nWe find MLE for the linear regression with the 3 things: the normal distribution (iid), \\(\\mu\\) and \\(\\sigma^2\\)\n\n\n\nMixed Effects Models\n\n\\(\\text{E}(y_{ij}|\\mathbf \\beta_i) = \\mathbf x_{ij} \\mathbf \\beta_i\\)\n\\(\\mathbf \\beta_i = \\mathbf \\beta (\\text{fixed effect}) + \\mathbf u_i (\\text{subject-specific random effect})\\)\n\\(\\mathbf \\beta_i\\) is a random coefficient specific for the \\(i\\) th subject, That’s why we call this mixed effect models ‘random effect’\nsubject-specific random effect: differenct subjects have different \\(\\mathbf \\beta_i\\)\n\nTransition Models\n\n\\(\\text{E}(y_{ij}|y_{i,j-1},\\dots,y_{i,1},\\mathbf x_{ij})\\)\nMarkov Process: the response variable in the previous time point will affect the measurement in the current time point.\n\n\n\n\nConsider an example of a simple linear model (i.e., a univaiable linear model) \\[\ny_{ij}=\\beta_0+\\beta_1t_{ij} + \\epsilon_{ij}\n\\]\n\nmean part: \\(\\text{E}(y_{ij})\\)\nvariance part: \\(\\text{Var}(\\mathbf y_{i})=\\text{Var}(\\mathbf \\epsilon_{i})\\)\n\nmore often, a correlation matrix is used in LDA because correlation is more interpretable.\n\n\n\\[\n\\text{Corr}(\\mathbf y_i) =\n\\begin{bmatrix}\n1 & \\rho_{12}& \\dots & \\rho_{1n_i} \\\\\n\\rho_{21} & 1 & \\dots & \\rho_{2n_i} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\rho_{n_i1} & \\rho_{n_i2}& \\dots & 1\n\\end{bmatrix}\n\\]\n\nin this correlation matrix, there are \\(\\frac{n(n-1)}{2}\\) parameters to estimate\nin the mean part, there are 2 parameters, \\(\\mathbf \\beta\\) to estimate Likewise, the number of the estimators depends on the number of the measurements and the covriates.\n\nIn LDA, since the responses are multiple, we need to look into the correlation characteristics.\n\n\n\nIn empirical observations about the nature of the correlation among repeated measures,\n\ncorrelations among the repeated measures are usually positive\ncorrelations tend to decrease with increasing time separation\ncorrelations among repeated measures rarely approach zero\ncorrelations between any pair of repeated meausres regardless of distance in time is constrained by the reliability of the measurement process.\n\nif the measurement process is not very reliable or consistent, then even if two measurements are taken close together in time, their correlation will not be very strong. Similarly, if the measurement process is highly reliable or consistent, then two measurements taken far apart in time may still be highly correlated. Reliability refers to the degree to which a measurement process produces consistent and accurate results over time.\n\n\n\n\n\nThere are 2 types of covariance structure: unbalanced design and balanced design. For now, let’s focus on the balanced design.\n\n\n\nobservations for each subject are not made on the same grid\nthese observations can be made at different time points and different numbers of observations may be made for each subject.\nMissing observations falls into this category.\n\n\n\n\n\nobservations for each subject are made on the same grid and there is no missing data.\n\nnumber and timing of the repeated measurements are the same for all individuals.\n\nThen, \\(t_{ij}\\) can be denoted as \\(t_j\\) where \\(j \\in 1, \\dots, n\\) because the size of the measurements is the same (\\(n_i\\) is the same)\nThe covariance of the response variable \\(\\mathbf Y_{m\\times n}\\) :\n\n$$\n\\[\\begin{aligned}\n  \\text{Cov}(\\mathbf Y)\n  &=\\text{Cov}(\\mathbf y_1,\\dots,y_m) \\\\\n  &=\n  \\begin{bmatrix}\n    \\text{Var}(\\mathbf y_1) & 0 & \\dots & 0 \\\\\n    0 & \\text{Var}(\\mathbf y_2) & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\text{Var}(\\mathbf y_m)\n  \\end{bmatrix}\\\\\n  &=\n  \\begin{bmatrix}\n    \\begin{bmatrix}\n    \\text{Var}(y_{11}) & \\text{Cov}( y_{11}, y_{12}) & \\dots & \\text{Cov}( y_{11}, y_{1n_1}) \\\\\n                               & \\text{Var}( y_{12}) & \\dots & \\text{Cov}( y_{12}, y_{1n_1}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{1n_1})\n\\end{bmatrix} & 0 & \\dots & 0 \\\\\n    0 & \\begin{bmatrix}\n    \\text{Var}(y_{21}) & \\text{Cov}( y_{21}, y_{22}) & \\dots & \\text{Cov}( y_{21}, y_{in_2}) \\\\\n                               & \\text{Var}( y_{22}) & \\dots & \\text{Cov}( y_{22}, y_{in_2}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{2n_2})\n\\end{bmatrix} & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\begin{bmatrix}\n    \\text{Var}(y_{m1}) & \\text{Cov}( y_{m1}, y_{m2}) & \\dots & \\text{Cov}( y_{m1}, y_{mn_m}) \\\\\n                               & \\text{Var}( y_{m2}) & \\dots & \\text{Cov}( y_{m2}, y_{mn_m}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{mn_m})\n\\end{bmatrix}\n\n  \\end{bmatrix} \\\\\n  &=\n  \\begin{bmatrix}\n    \\Sigma_1 & 0 & \\dots & 0 \\\\\n    0 & \\Sigma_1 & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\Sigma_m\n  \\end{bmatrix}\n\\end{aligned}\\]\n$$\nIf we assume the covariance matrices for different subjects are the same, we can denote \\(\\text{Cov}(\\mathbf Y)=\\Sigma\\).\n\n\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho & \\rho & \\dots & \\rho \\\\\n    \\rho & 1 & \\rho & \\dots & \\rho \\\\\n    \\rho & \\rho & 1 & \\dots & \\rho \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho & \\rho & \\rho & \\dots & 1\n  \\end{bmatrix}\n\\]\n\ncompound symmetry is a.k.a Exchangeable\nAssume variance is constant across visits (say \\(\\sigma^2\\))\nAssume correlation between any two visits are constant (say \\(\\rho\\)).\nParsimonious: there are two parameters in the covariance, \\(\\sigma^2\\) and \\(\\rho\\) (computational benefit)\nWithout any contraint on \\(\\sigma^2\\), you will get closed form estimate.\nCovariance variance matrix is plugged into likelihood function to estimate 3 kinds of parameters \\(\\sigma^2\\), \\(\\rho\\), and \\(\\beta\\)\nThis structure is so parsimonuous that it could be unrealistic: not commonly used\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho_1 & \\rho_2 & \\dots & \\rho_{n-1} \\\\\n    \\rho_1 & 1 & \\rho_1 & \\dots & \\rho_{n-2} \\\\\n    \\rho_2 & \\rho_1 & 1 & \\dots & \\rho_{n-3} \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho_{n-1} & \\rho_{n-2} & \\rho_{n-3} & \\dots & 1\n  \\end{bmatrix}\n\\]\n\nToeplitz structure is more flexible than compound symmetry\nAssume variance is constant across visits and \\(\\text{Corr}(y_{ij}, y_{i,j+k}) = \\rho_k\\).\nAssume correlation among responses at adjacent measurements is constant.\nOnly suitable for measurements made at equal intervals of time between different measurement.\nWithout any contraint on \\(\\sigma^2\\), you will get closed form estimate.\nToeplitz covariance has free \\(n\\) parameters to estimate (\\(1\\) for variance and \\(n-1\\) correlation parameters)\nThe larger time differences, the smaller its correlations\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho^1 & \\rho^2 & \\dots & \\rho^{n-1} \\\\\n    \\rho^1 & 1 & \\rho^1 & \\dots & \\rho^{n-2} \\\\\n    \\rho^2 & \\rho^1 & 1 & \\dots & \\rho^{n-3} \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho^{n-1} & \\rho^{n-2} & \\rho^{n-3} & \\dots & 1\n  \\end{bmatrix}\n\\]\n\nA special case of toeplitz structure with \\(\\text{Corr}(y_{ij},y_{i,j+k})=\\rho^k\\)\nsimpler than toeplitz, only 2 parameters\nOnly suitable for measurements made at equal intervals of time between different measurement.\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho^1 & 0 & \\dots & 0 \\\\\n    \\rho^1 & 1 & \\rho^1 & \\dots & 0 \\\\\n    0 & \\rho^1 & 1 & \\dots & 0 \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & 0 & \\dots & 1\n  \\end{bmatrix}\n\\]\nLook at the more general case of the banded structure in Wiki.\n\nAssume correlation is 0 beyond some specified interval.\nCan be combined with the previous patterns.\nVery strong assumption about how quickly the correlation decays to 0 with increasing time separation.\n\n\n\n\n\nA generalization of autoregressive pattern\nThe most general and reasonable structure\nSuitable for unevenly spaced measurements, take actual time points (time difference), the larger time difference the smaller correlation\nAssumption that the variance of different measurements over time is the same, which can be easily generalized. You can put different variance on the diagonal.\nLet \\(\\{t_{i1},\\dots,t_{in_i}\\}\\) denote the observation times for the \\(i\\) th individual. Then, the correlation is \\(\\text{Corr}(Y_{ij} ,Y_{ik}) = \\rho^{|t_{ij}-t_{ik}|}\\)\nCorrelation decreases exponentially with the time separations between them."
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/2_covariance_model.html#go-to-blog-content-list",
    "href": "docs/blog/posts/statistics/LDA/2_covariance_model.html#go-to-blog-content-list",
    "title": "LDA (2) - Concept & Covariance Models",
    "section": "",
    "text": "Blog Content List"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html",
    "title": "FDA Software Validation Guidance Summary",
    "section": "",
    "text": "I am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto.\nThe FDA validation guidance document is a bit difficult to understand because its explanations provides abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\n\n\n2022-12-28, download this article as PDF\n2022-12-28, summary with diagrams\n\n\n\n\nFDA: General Principles of Software Validation\n\n\n\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\n\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#notice",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#notice",
    "title": "FDA Software Validation Guidance Summary",
    "section": "",
    "text": "I am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto.\nThe FDA validation guidance document is a bit difficult to understand because its explanations provides abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\n\n\n2022-12-28, download this article as PDF\n2022-12-28, summary with diagrams\n\n\n\n\nFDA: General Principles of Software Validation\n\n\n\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\n\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#purpose",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#purpose",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.1 Purpose",
    "text": "2.1 Purpose\nThe purpose is to make a sketch of general validation principle of the validation of medical device software or software used to design or develop."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#scope",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#scope",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.2 Scope",
    "text": "2.2 Scope\nThe scope of this guidance is broad. The important activities for the software validation include at least:\n\nplanning,\nverfication,\ntesting,\ntraceability, and\nconfiguration management.\n\nAll of the activities above should be\n\nintegrated\nbe able to describe software life cycle management and\nbe able to describe software risk management.\n\nThe software validation and verification activities should be focused into the entire software life cycle. (It does not necessarily mean that the activies must follow any technical models.)\nThe guidance is applicable to any software related to a regulated medical device and anyone who is employed in a bio or medical industry.\n\n2.2.1 The Least Burdensome Approach\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n2.2.2 Regulatory Requirements for Software Validation\n\nSoftware validation: a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997. (See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\nSpecific requirements for validation of device software are found in 21 CFR §820.30(g). Other design controls, such as planning, input, verification, and reviews, are required for medical device software. (See 21 CFR §820.30.)\ncomputer systems used to create, modify, and maintain electronic records and to manage electronic signatures are also subject to the validation requirements. (See 21 CFR §11.10(a).)\n\n\n2.2.2.1 Objective\nThe objective of software validation is to ensure:\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\n2.2.2.2 What to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\n\n2.2.3 Quality System Regulation vs Pre-market Submissions\nThis document does not address any specific requirements but general ones. Specific issues should be addressed to\n\nthe Office of Device Evaluation (ODE),\nCenter for Devices and Radiological Health (CDRH)\nthe Office of Blood Research and Review,\nCenter for Biologics Evaluation and Research (CBER). See the references in Appendix A for applicable FDA guidance documents for pre-market submissions."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#context-for-software-validation",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#context-for-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.3 Context for Software Validation",
    "text": "2.3 Context for Software Validation\n\nValidation elements that FDA expects to do for the Quality System regulation, using the principles and tasks are listed in Sections 4 and 5.\nAdditional specific information is available from many of the references listed in Appendix A\n\n\n2.3.1 Definition and Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology between the medical device Quality System regulation and the software industry:\n\nrequirements,\nspecification,\nverification, and\nvalidation.\n\n\n\n2.3.1.1 Requirements and Specifications\nThe Quality System regulation states\n\nthat design input requirements must be documented and\nthat specified requirements must be verified\n\nBut, the regulation does not further clarify the distinction between the terms “requirement” and “specification.”\n\nRequirement\n\ncan be any need or expectation for a system or for its software.\nreflects the stated or implied needs of the customer: requirements may be\n\nmarket-based,\ncontractual,\nstatutory, or\nan organization’s internal requirements.\n\nvarious examples of requirements\n\ndesign, functional, implementation, interface, performance, or physical requirements\n\nSoftware requirements derived from the system requirements for those aspects of system functionality\nSoftware requirements are typically stated in functional terms and are defined, refined, and updated as a development project progresses.\nSuccess in accurately and completely documenting software requirements is a crucial factor in successful validation of the resulting software.\n\nSpecification\n\ndefined as “a document that states requirements.” (See 21 CFR §820.3(y).)\nIt may refer to or include drawings, patterns, or other relevant documents\nIt usually indicates the means and the criteria whereby conformity with the requirement can be checked.\nVarious examples of written specifications\n\nsystem requirements specification,\nsoftware requirements specification,\nsoftware design specification,\nsoftware test specification,\nsoftware integration specification, etc.\n\nAll of these documents are design outputs for which various forms of verification are necessary.\n\n\n\n\n2.3.1.2 Verifiaction and Validation\nThe Quality System regulation is harmonized with ISO 8402:1994, which treats “verification” and “validation” as separate and distinct terms.\n\nSoftware verification\n\nIt provides objective evidence that the design outputs of a particular phase of the software development life cycle meet all of the specified requirements for that phase.\nIt looks for\n\nconsistency,\ncompleteness, and\ncorrectness of the software and its supporting documentation\n\nSoftware testing\n\nverification activities intended to confirm that software development output meets its input requirements.\n\nTypes of verification activities include\n\nvarious static and dynamic analyses,\ncode and document inspections,\nwalkthroughs, and other techniques.\n\n\nSoftware Validation\n\nConfirmation by examination and provision of the following objective evidence:\nEvidence 1: software specifications conform to user needs and intended uses, and\nEvidnece 2: the particular requirements implemented through software can be consistently fulfilled.\nEvidnece 3: all software requirements have been implemented correctly and completely and are traceable to system requirements.\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device.\n\nDifficulty in Software verification and validation\n\na developer cannot test forever, and\nit is difficult to know how much evidence is enough.\nIn large measure, software validation is a matter of developing a “level of confidence” that the device meets all requirements and user expectations for the software automated functions and features of the device.\nConsiderations for an acceptable level of confidence\n\nmeasures such as defects found in specifications documents,\nestimates of defects remaining,\ntesting coverage, and other techniques are all used to develop before shipping the product.\nHowever, a level of confidence varies depending upon the safety risk (hazard) posed by the automated functions of the device. (Info on safety risk is found in Section 4 and in the international standards ISO/IEC 14971-1 and IEC 60601-1-4 referenced in Appendix A).\n\n\n\n\n\n2.3.1.3 IQ/OQ/PQ\nIQ/OQ/PQ are the terminology related to user site software validation\n\nInstallation qualification (IQ)\nOperational qualification (OQ)\nPerformance qualification (PQ).\n\nDefinitions of these terms may be found in FDA’s Guideline on General Principles of Process Validation, dated May 11, 1987, and in FDA’s Glossary of Computerized System and Software Development Terminology, dated August 1995. Both FDA personnel and device manufacturers need to be aware of these differences in terminology as they ask for and provide information regarding software validation.\n\n\n\n2.3.2 Software Development as Part of System Design\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nthe user’s needs\nintended uses from which the product is developed.\n\nA primary goal of software validation is to then demonstrate that all completed software products comply with all documented software and system requirements.\n\n\n2.3.3 Software Is Different from Hardware\nSoftware engineering needs an even greater level of managerial scrutiny and control than does hardware engineering.\n\n\n2.3.4 Benefits of Software Validation\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nSoftware validation can also reduce long term costs by making it easier and less costly to reliably modify software and revalidate software changes.\n\n\n\n2.3.5 Design Review\nDesign reviews are documented, comprehensive, and systematic examinations of a design to evaluate\n\nthe adequacy of the design requirements,\nthe capability of the design to meet these requirements, and\nto identify problems.\n\nDesign review is a primary tool for managing and evaluating development projects.\n\nIt is strongly recommended that it should be formal design because it is more structured than the informal one.\nIt includes participation from others outside the development team.\nIt may review reference or include results from other formal and informal reviews.\nDesign reviews should include\n\nexamination of development plans,\nrequirements specifications,\ndesign specifications,\ntesting plans and procedures,\nall other documents and activities associated with the project,\nverification results from each stage of the defined life cycle, and\nvalidation results for the overall device.\n\nThe Quality System regulation requires that at least one formal design review be conducted during the device design process. However, it is recommended that multiple design reviews be conducted\n\n(e.g., at the end of each software life cycle activity, in preparation for proceeding to the next activity).\n\nFormal design reviews documented should include:\n\nthe appropriate tasks and expected results, outputs, or products been established for each software life cycle activity\ncorrectness, completeness, consistency, and accuracy\nsatisfaction for the standards, practices, and conventions of that activity\nestablishment of a proper basis for initiating tasks for the next software life cycle activity"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#principles-of-software-validation",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#principles-of-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.4 Principles of Software Validation",
    "text": "2.4 Principles of Software Validation\n\n2.4.1 Requirements\nA documented software requirements specification provides a baseline for both validation and verification. The software validation process must include an established software requirements specification (Ref: 21 CFR 820.3(z) and (aa) and 820.30(f) and (g)).\n\n\n2.4.2 Defect Prevention\nIn order to establish that confidence, software developers should use a mixture of methods and techniques to prevent software errors and to detect software errors that do occur.\n\n\n2.4.3 Time and Effort\nPreparation for software validation should begin early, i.e., during design and development planning and design input. The final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n2.4.4 Software Life Cycle\n\nSoftware validation takes place within the environment of an established software life cycle.\nThe software life cycle contains software engineering tasks and documentation necessary to support the software validation effort.\nspecific verification and validation tasks need to be appropriate for the intended use of the software\n\n\n\n2.4.5 Plans\n\nThe software validation process is defined and controlled through the use of a plan.\nThe software validation plan defines “what” is to be accomplished through the software validation effort.\nSoftware validation plans specify areas such as\n\nscope,\napproach,\nresources,\nschedules and the types and extent of activities,\ntasks, and\nwork items.\n\n\n\n\n2.4.6 Procedures\nThe software validation process is executed through the use of procedures. These procedures establish “how” to conduct the software validation effort. The procedures should identify the specific actions or sequence of actions that must be taken to complete individual validation activities, tasks, and work items.\n\n\n2.4.7 Software Validation After a Change\n\nDue to the complexity of software, a small local change may have a significant global system impact.\nIf a change exists in the software, the whole validation status of the software needs to be re-established.\nneed to determine the extent and impact of that change on the entire software system.\nthe software developer should then conduct an appropriate level of software regression testing to show that unchanged but vulnerable portions of the system have not been adversely affected.\n\n\n\n2.4.8 Validation Coverage\n\nValidation coverage should be based on the software’s complexity and safety risk.\nThe selection of validation activities, tasks, and work items should be commensurate with the complexity of the software design and the risk associated with the use of the software for the specified intended use.\n\n\n\n2.4.9 Independence of Review\n\nValidation activities should be based on the basic quality assurance precept of “independence of review.”\nSelf-validation is extremely difficult.\nWhen possible, an independent evaluation is always better (like a contracted third-party independent verification and validation)\nAnother approach is to assign internal staff members that are not involved in a particular design or its implementation, but who have sufficient knowledge to evaluate the project and conduct the verification and validation activities.\n\n\n\n2.4.10 Flexibility and Responsibility\nThe device manufacturer has flexibility in choosing how to apply these validation principles, but retains ultimate responsibility for demonstrating that the software has been validated. FDA regulated medical device applications include software that:\n\nIs a component, part, or accessory of a medical device;\n\ncomponents: e.g., application software, operating systems, compilers, debuggers, configuration management tools, and many more\n\nIs itself a medical device; or\nIs used in manufacturing, design and development, or other parts of the quality system.\nNo matter how complex and disperse the software is, the manufacturer is in charge of responsibility for software validation."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#activities-and-tasks",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#activities-and-tasks",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.5 Activities and Tasks",
    "text": "2.5 Activities and Tasks\nSoftware validation is accomplished through a series of activities and tasks that are planned and executed at various stages of the software development life cycle. These tasks may be\n\none time occurrences\niterated many times\n\n\n2.5.1 Software Life Cycle Activities\n\nSoftware developers should establish a software life cycle model that is appropriate for their product and organization.\nThe selected software life cycle model should cover the software from its birth to its retirement.\nActivities in a typical software life cycle model:\n\nQuality Planning\nSystem Requirements Definition\nDetailed Software Requirements Specification\nSoftware Design Specification\nConstruction or Coding\nTesting\nInstallation\nOperation and Support\nMaintenance\nRetirement\n\nVerification, testing, and other tasks that support software validation occur during each of these activities.\nSeveral software life cycle models defined in FDA’s Glossary of Computerized System and Software Development\n\nTerminology dated August 1995:\n\nwaterfall\nspiral\nrapid prototyping\nincremental development, etc.\n\n\n\n2.5.2 Typical Tasks Supporting Validation\nthe software developer should at least consider each of the risk-related tasks and should define and document which tasks are or are not appropriate for their specific application.\n\n2.5.2.1 Quality Planning\nDesign and development planning should culminate in a plan that identifies\n\nnecessary tasks,\nprocedures for anomaly reporting and resolution,\nnecessary resources, and\nmanagement review requirements including formal design reviews.\n\nThe plan should include:\n\nThe specific tasks for each life cycle activity;\nEnumeration of important quality factors (e.g., reliability, maintainability, and usability);\nMethods and procedures for each task;\nTask acceptance criteria;\nCriteria for defining and documenting outputs in terms that will allow evaluation of their conformance to input requirements;\nInputs for each task;\nOutputs from each task;\nRoles, resources, and responsibilities for each task;\nRisks and assumptions; and\nDocumentation of user needs.\n\nThe plan should identify\n\nthe personnel,\nthe facility and equipment resources for each task, and\nthe role that risk (hazard) management will play.\n\nA configuration management plan should be developed that will guide and control multiple parallel development activities and ensure proper communications and documentation.\nControls are necessary to ensure positive and correct correspondence among all approved versions of the specifications documents, source code, object code, and test suites that comprise a software system. The controls also should ensure accurate identification of, and access to, the currently approved versions.\nProcedures should be created for reporting and resolving software anomalies found through validation or other activities.\nManagement should identify the reports and specify the contents, format, and responsible organizational elements for each report. Procedures also are necessary for the review and approval of software development results, including the responsible organizational elements for such reviews and approvals.\nTypical Tasks – Quality Planning\n\nRisk (Hazard) Management Plan\nConfiguration Management Plan\nSoftware Quality Assurance Plan\n\nSoftware Verification and Validation Plan\n\nVerification and Validation Tasks, and Acceptance Criteria\nSchedule and Resource Allocation (for software verification and validation activities)\nReporting Requirements\n\nFormal Design Review Requirements\nOther Technical Review Requirements\n\nProblem Reporting and Resolution Procedures\nOther Support Activities\n\n\n\n2.5.2.2 Requirements\nRequirements development includes the\n\nidentification,\nanalysis, and\ndocumentation of information about the device and its intended use.\n\nAreas of special importance include allocation of system functions to\n\nhardware/software,\noperating conditions,\nuser characteristics,\npotential hazards, and\nanticipated tasks.\n\nIn addition, the requirements should state clearly the intended use of the software. It is not possible to validate software without predetermined and documented software requirements. Typical software requirements specify the following:\n\nAll software system inputs;\nAll software system outputs;\nAll functions that the software system will perform;\nAll performance requirements that the software will meet, (e.g., data throughput, reliability, and timing);\nThe definition of all external and user interfaces, as well as any internal software-to-system interfaces;\nHow users will interact with the system;\nWhat constitutes an error and how errors should be handled;\nRequired response times;\nThe intended operating environment for the software, if this is a design constraint (e.g., hardware platform, operating system);\nAll ranges, limits, defaults, and specific values that the software will accept; and\nAll safety related requirements, specifications, features, or functions that will be implemented in software.\n\nSoftware requirement specifications should identify clearly the potential hazards that can result from a software failure in the system as well as any safety requirements to be implemented in software.\nThe consequences of software failure should be evaluated, along with means of mitigating such failures (e.g., hardware mitigation, defensive programming, etc.).\nThe Quality System regulation requires a mechanism for addressing incomplete, ambiguous, or conflicting requirements. (See 21 CFR 820.30(c).) Each requirement (e.g., hardware, software, user, operator interface, and safety) identified in the software requirements specification should be evaluated for accuracy, completeness, consistency, testability, correctness, and clarity.\nFor example, software requirements should be evaluated to verify that:\n\nThere are no internal inconsistencies among requirements;\nAll of the performance requirements for the system have been spelled out;\nFault tolerance, safety, and security requirements are complete and correct;\nAllocation of software functions is accurate and complete;\nSoftware requirements are appropriate for the system hazards; and\nAll requirements are expressed in terms that are measurable or objectively verifiable.\n\nA software requirements traceability analysis should be conducted to trace software requirements to (and from) system requirements and to risk analysis results. In addition to any other analyses and documentation used to verify software requirements, a formal design review is recommended to confirm that requirements are fully specified and appropriate before extensive software design efforts begin. Requirements can be approved and released incrementally, but care should be taken that interactions and interfaces among software (and hardware) requirements are properly reviewed, analyzed, and controlled.\nTypical Tasks – Requirements\n\nPreliminary Risk Analysis\nTraceability Analysis\n\nSoftware Requirements to System Requirements (and vice versa)\nSoftware Requirements to Risk Analysis\n\nDescription of User Characteristics\nListing of Characteristics and Limitations of Primary and Secondary Memory\nSoftware Requirements Evaluation\nSoftware User Interface Requirements Analysis\nSystem Test Plan Generation\nAcceptance Test Plan Generation\nAmbiguity Review or Analysis\n\n\n\n2.5.2.3 Design\nIn the design process, the software requirements specification is translated into a logical and physical representation of the software to be implemented. The software design specification is a description of what the software should do and how it should do it. The design specification may contain both a high level summary of the design and detailed design information. Human factors engineering should be woven into\n\nthe entire design and development process,\nthe device design requirements,\nanalyses, and\ntests.\n\nDevice safety and usability issues should be considered when developing\n\nflowcharts,\nstate diagrams,\nprototyping tools, and\ntest plans.\n\nAlso, task and function analyses, risk analyses, prototype tests and reviews, and full usability tests should be performed. Participants from the user population should be included when applying these methodologies.\nThe software design specification should include:\n\nSoftware requirements specification, including predetermined criteria for acceptance of the software;\nSoftware risk analysis;\nDevelopment procedures and coding guidelines (or other programming procedures);\nSystems documentation (e.g., a narrative or a context diagram) that describes the systems context in which the program is intended to function, including the relationship of hardware, software, and the physical environment;\nHardware to be used;\nParameters to be measured or recorded;\nLogical structure (including control logic) and logical processing steps (e.g., algorithms);\nData structures and data flow diagrams;\nDefinitions of variables (control and data) and description of where they are used;\nError, alarm, and warning messages;\nSupporting software (e.g., operating systems, drivers, other application software);\nCommunication links (links among internal modules of the software, links with the supporting software, links with the hardware, and links with the user);\nSecurity measures (both physical and logical security); and\nAny additional constraints not identified in the above elements.\n\nThe first four of the elements noted above usually are separate pre-existing documents that are included by reference in the software design specification. Software requirements specification was discussed in the preceding section, as was software risk analysis.\nSoftware design evaluations criteria:\n\ncomplete,\ncorrect,\nconsistent,\nunambiguous,\nfeasible,\nmaintainable,\nanalyses of control flow,\ndata flow,\ncomplexity,\ntiming,\nsizing,\nmemory allocation,\ncriticality analysis, and many other aspects of the design\n\nAppropriate consideration of software architecture (e.g., modular structure) during design can reduce the magnitude of future validation efforts when software changes are needed.\nA traceability analysis should be conducted to verify that the software design implements all of the software requirements. As a technique for identifying where requirements are not sufficient, the traceability analysis should also verify that all aspects of the design are traceable to software requirements.\nAn analysis of communication links should be conducted to evaluate the proposed design with respect to hardware, user, and related software requirements. At the end of the software design activity, a Formal Design Review should be conducted to verify that the design is correct, consistent, complete, accurate, and testable, before moving to implement the design.\nSeveral versions of both the software requirement specification and the software design specification should be maintained. All approved versions should be archived and controlled in accordance with established configuration management procedures.\nTypical Tasks – Design\n\nUpdated Software Risk Analysis\nTraceability Analysis - Design Specification to Software Requirements (and vice versa)\nSoftware Design Evaluation\nDesign Communication Link Analysis\nModule Test Plan Generation\nIntegration Test Plan Generation\nTest Design Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.4 Construction or Coding\nSoftware may be constructed either by coding. Coding is the software activity where the detailed design specification is implemented as source code. It is the last stage in decomposition of the software requirements where module specifications are translated into a programming language.\nCoding usually involves the use of a high-level programming language, but may also entail the use of assembly language (or microcode) for time-critical operations.\nA source code traceability analysis is an important tool to verify that all code is linked to established specifications and established test procedures. A source code traceability analysis should be conducted and documented to verify that:\n\nEach element of the software design specification has been implemented in code;\nModules and functions implemented in code can be traced back to an element in the software design specification and to the risk analysis;\nTests for modules and functions can be traced back to an element in the software design specification and to the risk analysis; and\nTests for modules and functions can be traced to source code for the same modules and functions.\n\nTypical Tasks – Construction or Coding\n\nTraceability Analyses\n\nSource Code to Design Specification (and vice versa)\nTest Cases to Source Code and to Design Specification\n\nSource Code and Source Code Documentation Evaluation\nSource Code Interface Analysis\nTest Procedure and Test Case Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.5 Testing by the Software Developer\nSoftware testing entails running software products under known conditions with defined inputs and documented outcomes that can be compared to their predefined expectations. It is a time consuming, difficult, and imperfect activity.\nAs such, it requires early planning in order to be effective and efficient. Test plans and test cases should be created as early in the software development process as feasible.\nThey should identify\n\nthe schedules,\nenvironments,\nresources (personnel, tools, etc.),\nmethodologies,\ncases (inputs, procedures, outputs, expected results),\ndocumentation, and\nreporting criteria.\n\nDescriptions of categories of software and software testing effort appear in the literature\n\nNIST Special Publication 500-235, Structured Testing: A Testing Methodology Using the Cyclomatic Complexity Metric;\nNUREG/CR-6293, Verification and Validation Guidelines for High Integrity Systems; and\nIEEE Computer Society Press, Handbook of Software Reliability Engineering.\n\nTesting of all program functionality does not mean all of the program has been tested. Testing of all of a program’s code does not mean all necessary functionality is present in the program. Testing of all program functionality and all program code does not mean the program is 100% correct! Software testing that finds no errors should not be interpreted to mean that errors do not exist in the software product; it may mean the testing was superficial.\nAn essential element of a software test case is the expected result. It is the key detail that permits objective evaluation of the actual test result. This necessary testing information is obtained from the corresponding, predefined definition or specification.\nA software testing process should be based on principles that foster effective examinations of a software product. Applicable software testing tenets include:\n\nThe expected test outcome is predefined;\nA good test case has a high probability of exposing an error;\nA successful test is one that finds an error;\nThere is independence from coding;\nBoth application (user) and software (programming) expertise are employed;\nTesters use different tools from coders;\nExamining only the usual case is insufficient;\nTest documentation permits its reuse and an independent confirmation of the pass/fail status of a test outcome during subsequent review.\n\nCode-based testing is also known as structural testing or “white-box” testing. It identifies test cases based on knowledge obtained from the source code, detailed design specification, and other development documents. Structural testing can identify “dead” code that is never executed when the program is run. Structural testing is accomplished primarily with unit (module) level testing, but can be extended to other levels of software testing.\nThe level of structural testing can be evaluated using metrics that are designed to show what percentage of the software structure has been evaluated during structural testing. These metrics are typically referred to as “coverage” and are a measure of completeness with respect to test selection criteria. The amount of structural coverage should be commensurate with the level of risk posed by the software. Use of the term “coverage” usually means 100% coverage. Common structural coverage metrics include:\n\nStatement Coverage – This criteria requires sufficient test cases for each program statement to be executed at least once; however, its achievement is insufficient to provide confidence in a software product’s behavior.\nDecision (Branch) Coverage – This criteria requires sufficient test cases for each program decision or branch to be executed so that each possible outcome occurs at least once. It is considered to be a minimum level of coverage for most software products, but decision coverage alone is insufficient for high-integrity applications.\nCondition Coverage – This criteria requires sufficient test cases for each condition in a program decision to take on all possible outcomes at least once. It differs from branch coverage only when multiple conditions must be evaluated to reach a decision.\nMulti-Condition Coverage – This criteria requires sufficient test cases to exercise all possible combinations of conditions in a program decision.\nLoop Coverage – This criteria requires sufficient test cases for all program loops to be executed for zero, one, two, and many iterations covering initialization, typical running and termination (boundary) conditions.\nPath Coverage – This criteria requires sufficient test cases for each feasible path, basis path, etc., from start to exit of a defined program segment, to be executed at least once. Because of the very large number of possible paths through a software program, path coverage is generally not achievable. The amount of path coverage is normally established based on the risk or criticality of the software under test.\nData Flow Coverage – This criteria requires sufficient test cases for each feasible data flow to be executed at least once. A number of data flow testing strategies are available.\n\nThe following types of functional software testing involve generally increasing levels of effort:\n\nNormal Case – Testing with usual inputs is necessary. However, testing a software product only with expected, valid inputs does not thoroughly test that software product. By itself, normal case testing cannot provide sufficient confidence in the dependability of the software product.\nOutput Forcing – Choosing test inputs to ensure that selected (or all) software outputs are generated by testing.\nRobustness – Software testing should demonstrate that a software product behaves correctly when given unexpected, invalid inputs. Methods for identifying a sufficient set of such test cases include Equivalence Class Partitioning, Boundary Value Analysis, and Special Case Identification (Error Guessing). While important and necessary, these techniques do not ensure that all of the most appropriate challenges to a software product have been identified for testing.\nCombinations of Inputs – The functional testing methods identified above all emphasize individual or single test inputs. Most software products operate with multiple inputs under their conditions of use. Thorough software product testing should consider the combinations of inputs a software unit or system may encounter during operation. Error guessing can be extended to identify combinations of inputs, but it is an ad hoc technique. Cause-effect graphing is one functional software testing technique that systematically identifies combinations of inputs to a software product for inclusion in test cases.\n\nFunctional and structural software test case identification techniques provide specific inputs for testing, rather than random test inputs. One weakness of these techniques is the difficulty in linking structural and functional test completion criteria to a software product’s reliability.\nAdvanced software testing methods, such as statistical testing, can be employed to provide further assurance that a software product is dependable. Statistical testing uses randomly generated test data from defined distributions based on an operational profile (e.g., expected use, hazardous use, or malicious use of the software product). Large amounts of test data are generated and can be targeted to cover particular areas or concerns, providing an increased possibility of identifying individual and multiple rare operating conditions that were not anticipated by either the software product’s designers or its testers. Statistical testing also provides high structural coverage. It does require a stable software product. Thus, structural and functional testing are prerequisites for statistical testing of a software product.\nAnother aspect of software testing is the testing of software changes. Changes occur frequently during software development. These changes are the result of\n\ndebugging that finds an error and it is corrected,\nnew or changed requirements (“requirements creep”), and\nmodified designs as more effective or efficient implementations are found.\n\nOnce a software product has been baselined (approved), any change to that product should have its own “mini life cycle,” including testing. Testing of a changed software product requires additional effort. It should demonstrate\n\nthat the change was implemented correctly, and\nthat the change did not adversely impact other parts of the software product.\n\nRegression analysis is the determination of the impact of a change based on review of the relevant documentation in order to identify the necessary regression tests to be run. Regression testing is the rerunning of test cases that a program has previously executed correctly and comparing the current result to the previous result in order to detect unintended effects of a software change. Regression analysis and regression testing should also be employed when using integration methods to build a software product to ensure that newly integrated modules do not adversely impact the operation of previously integrated modules.\nIn order to provide a thorough and rigorous examination of a software product, development testing is typically organized into levels: unit, integration, and system levels of testing.\n\nUnit (module or component) level testing focuses on the early examination of sub-program functionality and ensures that functionality not visible at the system level is examined by testing. Unit testing ensures that quality software units are furnished for integration into the finished software product.\nIntegration level testing focuses on the transfer of data and control across a program’s internal and external interfaces. External interfaces are those with\n\nother software (including operating system software),\nsystem hardware, and\nthe users and can be described as communications links.\n\nSystem level testing demonstrates that all specified functionality exists and that the software product is trustworthy. This testing verifies the as-built program’s functionality and performance with respect to the requirements for the software product as exhibited on the specified operating platform(s). System level software testing addresses functional concerns and the following elements of a device’s software that are related to the intended use(s):\n\nPerformance issues (e.g., response times, reliability measurements);\nResponses to stress conditions, e.g., behavior under maximum load, continuous use;\nOperation of internal and external security features;\nEffectiveness of recovery procedures, including disaster recovery;\nUsability; (Usability vs Utility??)\nCompatibility with other software products;\nBehavior in each of the defined hardware configurations; and\nAccuracy of documentation.\n\n\nControl measures (e.g., a traceability analysis) should be used to ensure that the intended coverage is achieved.\nSystem level testing also exhibits the software product’s behavior in the intended operating environment. The location of such testing is dependent upon the software developer’s ability to produce the target operating environment(s). Depending upon the circumstances, simulation and/or testing at (potential) customer locations may be utilized.\nTest plans should identify the controls needed to ensure\n\nthat the intended coverage is achieved and\nthat proper documentation is prepared when planned system level testing is conducted at sites not directly controlled by the software developer.\n\nTest procedures, test data, and test results\n\nshould be documented in a manner permitting objective pass/fail decisions to be reached.\nshould also be suitable for review and objective decision making subsequent to running the test,\nshould be suitable for use in any subsequent regression testing.\n\nErrors detected during testing should be\n\nlogged,\nclassified,\nreviewed, and\nresolved prior to release of the software.\n\nSoftware error data that is collected and analyzed during a development life cycle may be used to determine the suitability of the software product for release for commercial distribution. Test reports should comply with the requirements of the corresponding test plans.\nSoftware testing tools are frequently used to ensure consistency, thoroughness, and efficiency in the testing of such software products and to fulfill the requirements of the planned testing activities.\nAppropriate documentation providing evidence of the validation of these software tools for their intended use should be maintained (see section 6 of this guidance).\nTypical Tasks – Testing by the Software Developer\n\nTest Planning\nStructural Test Case Identification\nFunctional Test Case Identification\nTraceability Analysis - Testing\nUnit (Module) Tests to Detailed Design\nIntegration Tests to High Level Design\nSystem Tests to Software Requirements\nUnit (Module) Test Execution\nIntegration Test Execution\nFunctional Test Execution\nSystem Test Execution\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.6 User Site Testing\nTesting at the user site is an essential part of software validation. The Quality System regulation requires\n\ninstallation and\ninspection procedures (including testing where appropriate) as well as\ndocumentation of inspection and\ntesting to demonstrate proper installation. (See 21 CFR §820.170.)\n\nLikewise, manufacturing equipment must meet specified requirements, and automated systems must be validated for their intended use. (See 21 CFR §820.70(g) and 21 CFR §820.70(i) respectively.)\nTerminology regarding user site testing can be confusing. Terms such as\n\nbeta test,\nsite validation,\nuser acceptance test,\ninstallation verification, and\ninstallation testing have all been used to describe user site testing.\n\nFor the purposes of this guidance, the term “user site testing” encompasses all of these and any other testing that takes place outside of the developer’s controlled environment.\nThis testing should take place at a user’s site with the actual hardware and software that will be part of the installed system configuration. The testing is accomplished through either actual or simulated use of the software being tested within the context in which it is intended to function.\nTest planners should check with the FDA Center(s) with the corresponding product jurisdiction to determine whether there are any additional regulatory requirements for user site testing.\nUser site testing should follow a pre-defined written plan with\n\na formal summary of testing and\na record of formal acceptance.\n\nThe following documented evidence should be retained:\n\nall testing procedures,\ntest input data, and\ntest results\n\nThere should be evidence that hardware and software are installed and configured as specified. Measures should ensure that all system components are exercised during the testing and that the versions of these components are those specified. The testing plan should specify testing throughout the full range of operating conditions and should specify continuation for a sufficient time to allow the system to encounter a wide spectrum of conditions and events in an effort to detect any latent faults that are not apparent during more normal activities.\nSome of the evaluations of the system’s ability that have been performed earlier by the software developer at the developer’s site should be repeated at the site of actual use. These may include tests for:\n\na high volume of data,\nheavy loads or stresses,\nsecurity,\nfault testing (avoidance, detection, tolerance, and recovery),\nerror messages, and\nimplementation of safety requirements.\n\nThere should be an evaluation of the ability of the users of the system to understand and correctly interface with it.\nOperators should be able to perform the intended functions and respond in an appropriate and timely manner to all alarms, warnings, and error messages.\nRecords should be maintained of both proper system performance and any system failures that are encountered.\nThe revision of the system to compensate for faults detected during this user site testing should follow the same procedures and controls as for any other software change.\nThe developers of the software may or may not be involved in the user site testing.\n\nIf the developers are involved, they may seamlessly carry over to the user’s site the last portions of design-level systems testing.\nIf the developers are not involved, it is all the more important that the user have persons who understand the importance of careful test planning, the definition of expected test results, and the recording of all test outputs.\n\nTypical Tasks – User Site Testing\n\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.7 Maintenance and Software Changes\n\n2.5.2.7.1 Hardware vs Software\nHardware maintenance typically includes\n\npreventive hardware maintenance actions,\ncomponent replacement, and\ncorrective changes.\n\nSoftware maintenance includes\n\ncorrective,\nperfective, and\nadaptive maintenance\nbut does not include preventive maintenance actions or software component replacement.\n\n\n\n2.5.2.7.2 Maintenance Types\n\nCorrective maintenance: Changes made to correct errors and faults in the software.\nPerfective maintenance: Changes made to the software to improve the performance, maintainability, or other attributes of the software system .\nAdaptive maintenance: Changes to make the software system usable in a changed environment.\n\nSufficient regression analysis and testing should be conducted to demonstrate that portions of the software not involved in the change were not adversely impacted. When changes are made to a software system,\n\neither during initial development or\nduring post release maintenance,\n\nThis is in addition to testing that evaluates the correctness of the implemented change(s). The specific validation effort necessary for each software change is determined by\n\nthe type of change,\nthe development products affected, and the\nimpact of those products on the operation of the software.\n\n\n\n2.5.2.7.3 Factors of Limitting Validation Effort Needed When a Change Is Made\n\ncareful and complete documentation of the design structure and\ncareful and complete documentation of interrelationships of various modules,\ninterfaces, etc.\nFor example,\n\ntest documentation,\ntest cases, and\nresults of previous verification and validation testing All of them need to be archived if they are to be available for performing subsequent regression testing.\n\n\nThe following additional maintenance tasks should be addressed:\n\nSoftware Validation Plan Revision - For software that was previously validated, the existing software validation plan should be revised to support the validation of the revised software. If no previous software validation plan exists, such a plan should be established to support the validation of the revised software.\nAnomaly Evaluation – Software organizations frequently maintain documentation, such as software problem reports that describe software anomalies discovered and the specific corrective action taken to fix each anomaly.\n\nToo often, however, mistakes are repeated because software developers do not take the next step to determine the root causes of problems and make the process and procedural changes needed to avoid recurrence of the problem.\nSoftware anomalies should be evaluated in terms of their severity and their effects on system operation and safety,\nbut they should also be treated as symptoms of process deficiencies in the quality system.\nA root cause analysis of anomalies can identify specific quality system deficiencies.\nWhere trends are identified (e.g., recurrence of similar software anomalies), appropriate corrective and preventive actions must be implemented and documented to avoid further recurrence of similar quality problems. (See 21 CFR 820.100.)\n\nProblem Identification and Resolution Tracking - All problems discovered during maintenance of the software should be documented. The resolution of each problem should be tracked to ensure it is fixed, for historical reference, and for trending.\nProposed Change Assessment - All proposed modifications, enhancements, or additions should be assessed to determine the effect each change would have on the system. This information should determine the extent to which verification and/or validation tasks need to be iterated.\nTask Iteration - For approved software changes, all necessary verification and validation tasks should be performed to ensure that planned changes are implemented correctly, all documentation is complete and up to date, and no unacceptable changes have occurred in software performance.\nDocumentation Updating – Documentation should be carefully reviewed to determine which documents have been impacted by a change. All approved documents (e.g., specifications, test procedures, user manuals, etc.) that have been affected should be updated in accordance with configuration management procedures. Specifications should be updated before any maintenance and software changes are made."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.6 Validation of Automated Process Equipment and Quality System Software",
    "text": "2.6 Validation of Automated Process Equipment and Quality System Software\nThe Quality System regulation requires that “when computers or automated data processing systems are used as part of production or the quality system, the [device] manufacturer shall validate computer software for its intended use according to an established protocol.” (See 21 CFR §820.70(i)). This has been a regulatory requirement of FDA’s medical device Good Manufacturing Practice (GMP) regulations since 1978.\nComputer systems that implement part of a device manufacturer’s production processes or quality system (or that are used to create and maintain records required by any other FDA regulation) are subject to the Electronic Records; Electronic Signatures regulation. (See 21 CFR Part 11.) This regulation establishes additional security, data integrity, and validation requirements when records are created or maintained electronically. These additional Part 11 requirements should be carefully considered and included in system requirements and software requirements for any automated record keeping systems. System validation and software validation should demonstrate that all Part 11 requirements have been met.\nComputers and automated equipment are used extensively throughout all aspects of\n\nmedical device design,\nlaboratory testing and analysis,\nproduct inspection and acceptance,\nproduction and process control,\nenvironmental controls,\npackaging,\nlabeling,\ntraceability,\ndocument control,\ncomplaint management, and many other aspects of the quality system.\n\nIncreasingly, automated plant floor operations can involve extensive use of embedded systems in:\n\nprogrammable logic controllers;\ndigital function controllers;\nstatistical process control;\nsupervisory control and data acquisition;\nrobotics;\nhuman-machine interfaces;\ninput/output devices; and\ncomputer operating systems.\n\nAll software tools used for software design are subject to the requirement for software validation, but the validation approach used for each application can vary widely.\nValidation is typically supported by:\n\nverifications of the outputs from each stage of that software development life cycle; and\nchecking for proper operation of the finished software in the device manufacturer’s intended use environment.\n\n\n2.6.1 How Much Validation Evidence Is Needed?\nThe level of validation effort should be commensurate with\n\nthe risk posed by the automated operation,\nthe complexity of the process software,\nthe degree to which the device manufacturer is dependent upon that automated process to produce a safe and effective device\n\nDocumented requirements and risk analysis of the automated process help to define the scope of the evidence needed to show that the software is validated for its intended use. Without a plan, extensive testing may be needed for:\n\na plant-wide electronic record and electronic signature system;\nan automated controller for a sterilization cycle; or\nautomated test equipment used for inspection and acceptance of finished circuit boards in a lifesustaining / life-supporting device.\n\nHigh risk applications should not be running in the same operating environment with non-validated software functions, even if those software functions are not used. Risk mitigation techniques such as memory partitioning or other approaches to resource protection may need to be considered when high risk applications and lower risk applications are to be used in the same operating environment.\nWhen software is upgraded or any changes are made to the software, the device manufacturer should consider how those changes may impact the “used portions” of the software and must reconfirm the validation of those portions of the software that are used. (See 21 CFR §820.70(i).)\n\n\n2.6.2 Defined User Equipment\nA very important key to software validation is a documented user requirements specification that defines:\n\nthe “intended use” of the software or automated equipment; and\nthe extent to which the device manufacturer is dependent upon that software or equipment for production of a quality medical device.\n\nThe device manufacturer (user) needs to define the expected operating environment including any required hardware and software configurations, software versions, utilities, etc. The user also needs to:\n\ndocument requirements for system performance, quality, error handling, startup, shutdown, security, etc.;\nidentify any safety related functions or features, such as sensors, alarms, interlocks, logical processing steps, or command sequences; and\ndefine objective criteria for determining acceptable performance.\n\nThe validation must be conducted in accordance with a documented protocol, and the validation results must also be documented. (See 21 CFR §820.70(i).) Test cases should be documented that will exercise the system to challenge its performance against the pre-determined criteria, especially for its most critical parameters.\nTest cases should address\n\nerror and alarm conditions,\nstartup, shutdown,\nall applicable user functions and operator controls,\npotential operator errors,\nmaximum and minimum ranges of allowed values, and\nstress conditions applicable to the intended use of the equipment.\n\nThe test cases should be executed and the results should be recorded and evaluated to determine whether the results support a conclusion that the software is validated for its intended use.\nA device manufacturer may conduct a validation using their own personnel or may depend on a third party such as the equipment/software vendor or a consultant. In any case, the device manufacturer retains the ultimate responsibility for ensuring that the production and quality system software:\n\nis validated according to a written procedure for the particular intended use; and\nwill perform as intended in the chosen application.\n\nThe device manufacturer should have documentation including:\n\ndefined user requirements;\nvalidation protocol used;\nacceptance criteria;\ntest cases and results; and\na validation summary that objectively confirms that the software is validated for its intended use.\n\n\n\n2.6.3 Validation of Off-The-Shelf Software and Automated Equipment\nMost of the automated equipment and systems used by device manufacturers are supplied by thirdparty vendors and are purchased off-the-shelf (OTS). The device manufacturer is responsible for ensuring that the product development methodologies used by the OTS software developer are appropriate and sufficient for the device manufacturer’s intended use of that OTS software.\nWhere possible and depending upon the device risk involved, the device manufacturer should consider auditing the vendor’s design and development methodologies used in the construction of the OTS software and should assess the development and validation documentation generated for the OTS software. Such audits can be conducted by the device manufacturer or by a qualified third party.\nThe audit should demonstrate that the vendor’s procedures for and results of the verification and validation activities performed the OTS software are appropriate and sufficient for the safety and effectiveness requirements of the medical device to be produced using that software."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html",
    "title": "Content List, Validation",
    "section": "",
    "text": "0000-00-00, EN62304"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#sgs",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#sgs",
    "title": "Content List, Validation",
    "section": "",
    "text": "0000-00-00, EN62304"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#fda",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#fda",
    "title": "Content List, Validation",
    "section": "2 FDA",
    "text": "2 FDA\n\n2023-01-27, General Principles of SW Validation\n2023-01-27, General Principles of SW Validation - Diagram Summary\n1111-11-11, Guidance for the Content of Premarket Submissions for Software Contained in Medical Devices"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#dhf",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#dhf",
    "title": "Content List, Validation",
    "section": "3 DHF",
    "text": "3 DHF"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#public-health",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#public-health",
    "title": "Content List, Validation",
    "section": "4 Public Health",
    "text": "4 Public Health"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#wet-lab",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#wet-lab",
    "title": "Content List, Validation",
    "section": "5 Wet Lab",
    "text": "5 Wet Lab\n\n0000-00-00, PCR (Polymerase Chain Reaction) Experiment"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html",
    "href": "docs/projects/dsp_validation/index.html",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n의료 장비와 연관된 시약 제품의 특성상 Global Market 진출시 각 국가의 정부에서 자국민의 건강 및 생명의 안전을 위해 요구하는 규제사항들이 있다.\n\n시약의 안정성 검증 요구\n장비의 안정성 검증 요구\nSoftware의 안정성 검증 요구\nDiagnostic Algorithm의 안정성 검증 요구\n\nCOVID19 특수 시기 해제 후 Global market으로의 진입 및 관리를 위해 각 국의 정부가 요구하는 제품의 안정성 검증 및 규제 사항들을 충족시켜야한다.\nEU(European Union)의 경우 IVDR (In Vitro Diagnostics Regulation) 을 요구한다.\n북미시장에 진출하기 위해 세계에서 가장 엄격한 기준을 요구하는 미국의 FDA와 캐나다의 Health Canada의 surveilance 기준으로 진단 알고리즘의 안정성 검증 문서를 기획하고 작성해야한다.\n시간이 지날 수록 각 국에서 software 및 algorithms에 대한 규제가 강화되고 있기때문에 기존의 Software Engineering에 의한 안전성 검증 방식보다 더 엄격한 Advanced Testing이 요구되고 있다.\nSeegene의 Diagnostic Signal Process (DSP) Algorithm의 안정성 검증 방식은 기업의 사업성과 직결되는 만큼 회사내에서 1급 보안사항으로 분류되어 구체적이고 자세한 기획 및 구현 내용을 공유할 수는 없다.\n\n\n\n\n\nalgorithm이 안전한 성능을 보여준다는 것을 통계적으로 증명하기 위한 system을 기획한다.\nalgorithm이 안전한 성능을 보여준다는 것을 Statistical Validation Sytem을 확립하여 통계적 분석으로 증명한다.\n\n여기서, 확립 (Establishment)은 정의(definition), 문서화(documentation) 및 구현 (Implement)으로 정의된다.\n\nalgorithm의 risk를 구체적으로 정의하고 risk가 algorithm에 미치는 영향도를 정량 분석한다.\nalgorithm이 risk 관리가 가능하다는 것을 statistical simulation을 통해 증명한다.\nalgorithm 구현 및 운영에 있어서 code의 변화가 있을 경우 새로운 validation report를 제출해야하므로 자동화 시스템을 구축한다.\n\n\n\n\n\n세계에서 가장 엄격한 검열 인증서를 발급 및 교육을 제공하는 기업인 SGS의 guidance를 참고한다.\nSGS는 FDA를 target으로 guidance를 제공한다.\nSoftware의 안전성 검증을 위해 FDA에서 제공하는 General Principles of Software Validation 문서를 정독 후 이 문서를 기반으로 validation system을 확립한다.\n\nCopy: General Principles of Software Validation\nSummary: General Principles of Software Validation\nDiagram: General Principles of Software Validation\n\nSoftware engineering은 General Principles of Software Validation 문서를 기반하여 수행한다.\nDiagnostic Algorithm의 안정성 검증은 Structural Testing와 Advanced Testing 모두 포함한다. Structural Testing은 code 기반의 Software Engineering Testing을 의미하고 Advanced Testing은 Statiscal Analysis에 기반한 Statistical Testing을 의미한다. Advanced Testing은 안정적인 Software Engineering System 구축이 그 전제가 된다.\nalgorithm 안전성에 대한 정의와 논리를 확립한다.\nalgorithm 안전성에 대한 지표를 확립한다.\nAdvanced Testing인 Statistical Testing은 data scientist의 창의성이 요구되는 작업으로 Testing Model을 기획하여 statistical analysis design을 구체화 및 문서화한다.\nBT (Biotechnology) 부문과 IT (Information technology) 부문의 협력이 전제가 되어야하며 BT 부서의 experiment design 및 limitation factors at a experimental level을 고려한 engineering design과 statistical design을 확립한다.\n기획된 Testing model에 적합한 statistical model을 찾고 minimum reuirement sample size를 계산한다.\n위의 전략대로, BT 부서에서의 실험과 IT 부서 (Data Science 팀)에서 분석을 수행한다.\nalgorithm 구현 및 운영에 있어서 code의 변화가 있을 경우와 새로운 제품에 대한 새로운 validation report를 제출해야하는 의무사항에 대비한 문서 자동화 시스템을 구축한다.\n\n\n\n\n\n\n\nBT에서 생성된 data를 입력할 수 있는 시스템 부재\nBT부서의 업무기술서 부재로 인한 소통의 어려움\n입력 데이터를 전처리하는 시스템 부재\nData Science 팀내 업무 기술서 부재로 인한 팀내 소통의 어려움\nValidation report에 대한 선례 및 template를 찾을 수 없을 정도로 매우 드물다.\n\n\n\n\n\nBT에서 생성된 data를 입력할 수 있는 시스템 구축\n\ndigitalization: experimental design file, raw data generated from medical device, data extracted from medical device\n\nBT부서와의 소통으로 업무 문서화를 진행하여 실험 결과의 기대 정답 기준 확립, 독립 변수 및 종속 변수 확립\n입력 데이터를 전처리하여 diagnostic algorithm의 결과물을 병합하는 engineering 시스템 구축\nData Quality Control Process 강화\n\n1단계 오타 교정\n2단계 결측치 처리\n3단계 anomaly data 처리\n4단계 algorithm data 정합성 1차 검정: FDA validtion을 위한 전처리된 algorithm vs Original algorithm\n5단계 algorithm data 정합성 2차 검정: Data Science팀의 FDA validtion을 위한 전처리된 algorithm vs BT 부서에 published algorithm\n\nData Science 팀내 업무 기술서 작성으로 코드 중앙화, 데이터 중앙화, 특이사항 문서화 실현\nSeegene 고유의 software testing & advanced testing model 기획 및 확립 후 statistical analysis 기획 및 수행\n\n\n\n\n\n\nFDA software validation knowledge\nStatistics\nDynamic documentation\nBiology\nClinical study design\n\n\n\n\n\n5 data scientists (I am a project owner.)\n3 data enineers\n27 biologists\n2 patent attorneys\n\n\n\n\n\n\n\n\n\n\n\nDSP Algorithm Output\nDescription\n\n\n\n\nFDA Validation 1st Draft\nFDA 제출용 verification & validation report 1차 초안\n\n\nData Input System\n임시적인 데이터 입력 시스템으로 대량의 data를 대량으로 연산하는 플랫폼 구축으로 발전\n\n\nDocumentation System\n기존에 부재했던 문서화 및 문서 자동화 시스템 구축 \\(\\rightarrow\\) 업무 소통과 Relational Database System 구축에 필요\n\n\nData Management System\ndata quality control system\n\n\nFDA Validation Model\nDSP algorithm을 위한 Validation Model 확립\n\n\nPatent Invention\nFDA Validation Model 발명\n\n\nIn-house first Performance evaluation of algorithms and reagent products\n사내에 기존에 존재하지 않았던 알고리즘 및 시약 제품의 종합 성능 평가\n\n\nStatistical analysis related to algorithmic risk management\n시약과 장비 고유의 random effect와 다른 교란자로 인해 발생할 수 있는 noise 및 anomaly data에 위험 관리 관련 통계 분석을 수행\n\n\n\n\n\n\n\nBT의 업무 기술서를 협업으로 공동작성하고 RDB system 구축\n시약, 장비, software 및 algorithm Validation용 DevOps Platform 구축\n\n\n\n\n\n\n\nDue to the nature of reagent products related to medical device, there are regulations required by each country’s government for the health and life safety of its citizens when entering the global market.\n\nReagent stability verification and validation required\nEquipment stability verification and validation request\nSoftware stability verification and validation request\nStability verification and validation Request of Diagnostic Algorithm\n\nIn order to enter and manage the global market after the COVID19 special period is lifted, product safety verification and regulatory requirements required by each country’s government must be met.\nIn the case of the EU (European Union), IVDR (In Vitro Diagnostics Regulation) is required\nIn order to enter the North American market, it is necessary to plan and write a document verifying the stability of the diagnostic algorithm based on the surveilance standards of the US FDA and Canada’s Health Canada, which require the world’s most stringent standards.\nAs time goes by, regulations on software and algorithms are being strengthened in each country, so advanced testing that is more stringent than the existing safety verification method by software engineering is required.\nTherefore, the stability verification and validation of the diagnostic algorithm includes software engineering testing and advanced testing. Here, advanced testing means statistical testing based on statistical analysis, and building a stable software engineering system is the prerequisite.\nSince the stability verification method of Seegene’s Diagnostic Signal Process (DSP) Algorithm is directly related to the business performance of the company, it is classified as a first-class security matter within the company, so specific and detailed planning and implementation details cannot be shared.\n\n\n\n\n\nDesign a system to statistically prove that the algorithm shows safe performance.\nEstablish a Statistical Validation System to prove that the algorithm shows safe performance through statistical analysis.\n\nHere, Establishment is defined as Definition, Documentation, and Implementation.\n\nDefine the risk of the algorithm in detail and quantitatively analyze the effect of the risk on the algorithm.\nIt is proved through statistical simulation that the algorithm is capable of risk management.\nIn the case of code changes according to algorithm implementations and operations, a new validation report must be submitted, so an automation system is built.\n\n\n\n\n\nRefer to the guidance of SGS, a company that issues and provides training for the world’s most stringent inspection certificates.\nSGS provides guidance to the FDA as a target.\nAfter thoroughly reading the General Principles of Software Validation document provided by the FDA for software safety verification, establish a validation system based on this document.\n\nCopy: General Principles of Software Validation\nSummary: General Principles of Software Validation\nDiagram: General Principles of Software Validation\n\nSoftware engineering is performed based on the General Principles of Software Validation document.\nThe stability verification of Diagnostic Algorithm includes both Structural Testing and Advanced Testing. Structural Testing means code-based Software Engineering Testing and Advanced Testing means Statistical Testing based on Statistical Analysis. Advanced Testing is based on the establishment of a stable Software Engineering System.\nEstablish a definition and logic for algorithm safety.\nEstablish metrics or indicators for algorithm safety.\nStatistical Testing, which is Advanced Testing, is a task that requires the creativity of a data scientist, and a testing model is planned to materialize and document statistical analysis design.\nCooperation between the BT (Biotechnology) sector and the IT (Information technology) sector must be a premise, and engineering design and statistical design should be established considering the BT department’s experimental design and limitation factors at a experimental level.\nFind a statistical model suitable for the planned testing model and calculate the minimum reuirement sample size.\nAs per the above strategy, the BT department conducts experiments and the IT department (Data Science team) conducts analysis.\nEstablish a document automation system in case of code changes in algorithm implementation and operation and the obligation to submit a new validation report for new products.\n\n\n\n\n\n\n\nAbsence of a system that can input data generated by BT departments\nDifficulties in communication due to lack of job description in BT departments.\nAbsence of a system that preprocesses input data.\nDifficulties in communication within the team due to lack of job description within the Data Science team.\nIt is so rare that no precedent or template for validation report can be found.\n\n\n\n\n\nBuilding a system that can input data generated by BT departments\n\ndigitalization: experimental design file, raw data generated from medical device, data extracted from medical device\n\nWork documented through communication with the BT department to establish the standard for the expected correct answer of the experiment results, and to establish independent and dependent variables\nBuilding an engineering system that preprocesses input data and merges the results of diagnostic algorithms\nStrengthen Data Quality Control Process\n\nStep 1 typo correction\nStep 2 missing value processing\nStep 3 anomaly data processing\nStep 4 algorithm data conformity 1st Test: Preprocessed algorithm for FDA validation vs Original algorithm\nStep 5 algorithm data conformity 2nd test: Data Science team’s preprocessed algorithm for FDA validation vs algorithm published by BT department\n\nRealization of code centralization, data centralization, and documentation of specific matters by writing job descriptions within the Data Science team\nPlan and conduct statistical analysis after planning and establishing Seegene’s own software testing & advanced testing model\n\n\n\n\n\n\nFDA software validation knowledge\nStatistics\nDynamic documentation\nBiology\nClinical study design\n\n\n\n\n\n5 data scientists (I am a project owner.)\n3 data enineers\n27 biologists\n2 patent attorneys\n\n\n\n\n\n\n\n\n\n\n\nDSP Algorithm Output\nDescription\n\n\n\n\nFDA Validation 1st Draft\nthe 1st draft of verification & validation report for FDA submission\n\n\nData Input System\nIt is a temporary data input system that develops into a platform that calculates a large amount of data in large quantities.\n\n\nDocumentation System\nEstablishment of previously absent documentation and document automation systems \\(\\rightarrow\\) Necessary for business communication and establishment of Relational Database System\n\n\nData Management System\ndata quality control system\n\n\nFDA Validation Model\nEstablishment of validation model for DSP algorithm\n\n\nPatent Invention\nInventing the FDA Validation Model\n\n\nIn-house first Performance evaluation of algorithms and reagent products\nComprehensive performance evaluation of algorithms and reagent products that did not previously exist in-house\n\n\nStatistical analysis related to algorithmic risk management\nRisk management-related statistical analysis is performed on noise and anomaly data that may occur due to reagent and equipment-specific random effects and other confounders.\n\n\n\n\n\n\n\nCollaboratively write BT’s job description and establish RDB system\nBuilding a DevOps Platform for reagents, equipment, software and algorithm validation"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#background",
    "href": "docs/projects/dsp_validation/index.html#background",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "의료 장비와 연관된 시약 제품의 특성상 Global Market 진출시 각 국가의 정부에서 자국민의 건강 및 생명의 안전을 위해 요구하는 규제사항들이 있다.\n\n시약의 안정성 검증 요구\n장비의 안정성 검증 요구\nSoftware의 안정성 검증 요구\nDiagnostic Algorithm의 안정성 검증 요구\n\nCOVID19 특수 시기 해제 후 Global market으로의 진입 및 관리를 위해 각 국의 정부가 요구하는 제품의 안정성 검증 및 규제 사항들을 충족시켜야한다.\nEU(European Union)의 경우 IVDR (In Vitro Diagnostics Regulation) 을 요구한다.\n북미시장에 진출하기 위해 세계에서 가장 엄격한 기준을 요구하는 미국의 FDA와 캐나다의 Health Canada의 surveilance 기준으로 진단 알고리즘의 안정성 검증 문서를 기획하고 작성해야한다.\n시간이 지날 수록 각 국에서 software 및 algorithms에 대한 규제가 강화되고 있기때문에 기존의 Software Engineering에 의한 안전성 검증 방식보다 더 엄격한 Advanced Testing이 요구되고 있다.\nSeegene의 Diagnostic Signal Process (DSP) Algorithm의 안정성 검증 방식은 기업의 사업성과 직결되는 만큼 회사내에서 1급 보안사항으로 분류되어 구체적이고 자세한 기획 및 구현 내용을 공유할 수는 없다."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#objective",
    "href": "docs/projects/dsp_validation/index.html#objective",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "algorithm이 안전한 성능을 보여준다는 것을 통계적으로 증명하기 위한 system을 기획한다.\nalgorithm이 안전한 성능을 보여준다는 것을 Statistical Validation Sytem을 확립하여 통계적 분석으로 증명한다.\n\n여기서, 확립 (Establishment)은 정의(definition), 문서화(documentation) 및 구현 (Implement)으로 정의된다.\n\nalgorithm의 risk를 구체적으로 정의하고 risk가 algorithm에 미치는 영향도를 정량 분석한다.\nalgorithm이 risk 관리가 가능하다는 것을 statistical simulation을 통해 증명한다.\nalgorithm 구현 및 운영에 있어서 code의 변화가 있을 경우 새로운 validation report를 제출해야하므로 자동화 시스템을 구축한다."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#methodology",
    "href": "docs/projects/dsp_validation/index.html#methodology",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "세계에서 가장 엄격한 검열 인증서를 발급 및 교육을 제공하는 기업인 SGS의 guidance를 참고한다.\nSGS는 FDA를 target으로 guidance를 제공한다.\nSoftware의 안전성 검증을 위해 FDA에서 제공하는 General Principles of Software Validation 문서를 정독 후 이 문서를 기반으로 validation system을 확립한다.\n\nCopy: General Principles of Software Validation\nSummary: General Principles of Software Validation\nDiagram: General Principles of Software Validation\n\nSoftware engineering은 General Principles of Software Validation 문서를 기반하여 수행한다.\nDiagnostic Algorithm의 안정성 검증은 Structural Testing와 Advanced Testing 모두 포함한다. Structural Testing은 code 기반의 Software Engineering Testing을 의미하고 Advanced Testing은 Statiscal Analysis에 기반한 Statistical Testing을 의미한다. Advanced Testing은 안정적인 Software Engineering System 구축이 그 전제가 된다.\nalgorithm 안전성에 대한 정의와 논리를 확립한다.\nalgorithm 안전성에 대한 지표를 확립한다.\nAdvanced Testing인 Statistical Testing은 data scientist의 창의성이 요구되는 작업으로 Testing Model을 기획하여 statistical analysis design을 구체화 및 문서화한다.\nBT (Biotechnology) 부문과 IT (Information technology) 부문의 협력이 전제가 되어야하며 BT 부서의 experiment design 및 limitation factors at a experimental level을 고려한 engineering design과 statistical design을 확립한다.\n기획된 Testing model에 적합한 statistical model을 찾고 minimum reuirement sample size를 계산한다.\n위의 전략대로, BT 부서에서의 실험과 IT 부서 (Data Science 팀)에서 분석을 수행한다.\nalgorithm 구현 및 운영에 있어서 code의 변화가 있을 경우와 새로운 제품에 대한 새로운 validation report를 제출해야하는 의무사항에 대비한 문서 자동화 시스템을 구축한다."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#issues-및-solutions",
    "href": "docs/projects/dsp_validation/index.html#issues-및-solutions",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "BT에서 생성된 data를 입력할 수 있는 시스템 부재\nBT부서의 업무기술서 부재로 인한 소통의 어려움\n입력 데이터를 전처리하는 시스템 부재\nData Science 팀내 업무 기술서 부재로 인한 팀내 소통의 어려움\nValidation report에 대한 선례 및 template를 찾을 수 없을 정도로 매우 드물다.\n\n\n\n\n\nBT에서 생성된 data를 입력할 수 있는 시스템 구축\n\ndigitalization: experimental design file, raw data generated from medical device, data extracted from medical device\n\nBT부서와의 소통으로 업무 문서화를 진행하여 실험 결과의 기대 정답 기준 확립, 독립 변수 및 종속 변수 확립\n입력 데이터를 전처리하여 diagnostic algorithm의 결과물을 병합하는 engineering 시스템 구축\nData Quality Control Process 강화\n\n1단계 오타 교정\n2단계 결측치 처리\n3단계 anomaly data 처리\n4단계 algorithm data 정합성 1차 검정: FDA validtion을 위한 전처리된 algorithm vs Original algorithm\n5단계 algorithm data 정합성 2차 검정: Data Science팀의 FDA validtion을 위한 전처리된 algorithm vs BT 부서에 published algorithm\n\nData Science 팀내 업무 기술서 작성으로 코드 중앙화, 데이터 중앙화, 특이사항 문서화 실현\nSeegene 고유의 software testing & advanced testing model 기획 및 확립 후 statistical analysis 기획 및 수행"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#required-skills",
    "href": "docs/projects/dsp_validation/index.html#required-skills",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "FDA software validation knowledge\nStatistics\nDynamic documentation\nBiology\nClinical study design"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#colaborators",
    "href": "docs/projects/dsp_validation/index.html#colaborators",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "5 data scientists (I am a project owner.)\n3 data enineers\n27 biologists\n2 patent attorneys"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#acheivements",
    "href": "docs/projects/dsp_validation/index.html#acheivements",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "DSP Algorithm Output\nDescription\n\n\n\n\nFDA Validation 1st Draft\nFDA 제출용 verification & validation report 1차 초안\n\n\nData Input System\n임시적인 데이터 입력 시스템으로 대량의 data를 대량으로 연산하는 플랫폼 구축으로 발전\n\n\nDocumentation System\n기존에 부재했던 문서화 및 문서 자동화 시스템 구축 \\(\\rightarrow\\) 업무 소통과 Relational Database System 구축에 필요\n\n\nData Management System\ndata quality control system\n\n\nFDA Validation Model\nDSP algorithm을 위한 Validation Model 확립\n\n\nPatent Invention\nFDA Validation Model 발명\n\n\nIn-house first Performance evaluation of algorithms and reagent products\n사내에 기존에 존재하지 않았던 알고리즘 및 시약 제품의 종합 성능 평가\n\n\nStatistical analysis related to algorithmic risk management\n시약과 장비 고유의 random effect와 다른 교란자로 인해 발생할 수 있는 noise 및 anomaly data에 위험 관리 관련 통계 분석을 수행"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#long-term-project",
    "href": "docs/projects/dsp_validation/index.html#long-term-project",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "BT의 업무 기술서를 협업으로 공동작성하고 RDB system 구축\n시약, 장비, software 및 algorithm Validation용 DevOps Platform 구축"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#background-1",
    "href": "docs/projects/dsp_validation/index.html#background-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "Due to the nature of reagent products related to medical device, there are regulations required by each country’s government for the health and life safety of its citizens when entering the global market.\n\nReagent stability verification and validation required\nEquipment stability verification and validation request\nSoftware stability verification and validation request\nStability verification and validation Request of Diagnostic Algorithm\n\nIn order to enter and manage the global market after the COVID19 special period is lifted, product safety verification and regulatory requirements required by each country’s government must be met.\nIn the case of the EU (European Union), IVDR (In Vitro Diagnostics Regulation) is required\nIn order to enter the North American market, it is necessary to plan and write a document verifying the stability of the diagnostic algorithm based on the surveilance standards of the US FDA and Canada’s Health Canada, which require the world’s most stringent standards.\nAs time goes by, regulations on software and algorithms are being strengthened in each country, so advanced testing that is more stringent than the existing safety verification method by software engineering is required.\nTherefore, the stability verification and validation of the diagnostic algorithm includes software engineering testing and advanced testing. Here, advanced testing means statistical testing based on statistical analysis, and building a stable software engineering system is the prerequisite.\nSince the stability verification method of Seegene’s Diagnostic Signal Process (DSP) Algorithm is directly related to the business performance of the company, it is classified as a first-class security matter within the company, so specific and detailed planning and implementation details cannot be shared."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#objective-1",
    "href": "docs/projects/dsp_validation/index.html#objective-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "Design a system to statistically prove that the algorithm shows safe performance.\nEstablish a Statistical Validation System to prove that the algorithm shows safe performance through statistical analysis.\n\nHere, Establishment is defined as Definition, Documentation, and Implementation.\n\nDefine the risk of the algorithm in detail and quantitatively analyze the effect of the risk on the algorithm.\nIt is proved through statistical simulation that the algorithm is capable of risk management.\nIn the case of code changes according to algorithm implementations and operations, a new validation report must be submitted, so an automation system is built."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#methodology-1",
    "href": "docs/projects/dsp_validation/index.html#methodology-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "Refer to the guidance of SGS, a company that issues and provides training for the world’s most stringent inspection certificates.\nSGS provides guidance to the FDA as a target.\nAfter thoroughly reading the General Principles of Software Validation document provided by the FDA for software safety verification, establish a validation system based on this document.\n\nCopy: General Principles of Software Validation\nSummary: General Principles of Software Validation\nDiagram: General Principles of Software Validation\n\nSoftware engineering is performed based on the General Principles of Software Validation document.\nThe stability verification of Diagnostic Algorithm includes both Structural Testing and Advanced Testing. Structural Testing means code-based Software Engineering Testing and Advanced Testing means Statistical Testing based on Statistical Analysis. Advanced Testing is based on the establishment of a stable Software Engineering System.\nEstablish a definition and logic for algorithm safety.\nEstablish metrics or indicators for algorithm safety.\nStatistical Testing, which is Advanced Testing, is a task that requires the creativity of a data scientist, and a testing model is planned to materialize and document statistical analysis design.\nCooperation between the BT (Biotechnology) sector and the IT (Information technology) sector must be a premise, and engineering design and statistical design should be established considering the BT department’s experimental design and limitation factors at a experimental level.\nFind a statistical model suitable for the planned testing model and calculate the minimum reuirement sample size.\nAs per the above strategy, the BT department conducts experiments and the IT department (Data Science team) conducts analysis.\nEstablish a document automation system in case of code changes in algorithm implementation and operation and the obligation to submit a new validation report for new products."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#issues-solutions",
    "href": "docs/projects/dsp_validation/index.html#issues-solutions",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "Absence of a system that can input data generated by BT departments\nDifficulties in communication due to lack of job description in BT departments.\nAbsence of a system that preprocesses input data.\nDifficulties in communication within the team due to lack of job description within the Data Science team.\nIt is so rare that no precedent or template for validation report can be found.\n\n\n\n\n\nBuilding a system that can input data generated by BT departments\n\ndigitalization: experimental design file, raw data generated from medical device, data extracted from medical device\n\nWork documented through communication with the BT department to establish the standard for the expected correct answer of the experiment results, and to establish independent and dependent variables\nBuilding an engineering system that preprocesses input data and merges the results of diagnostic algorithms\nStrengthen Data Quality Control Process\n\nStep 1 typo correction\nStep 2 missing value processing\nStep 3 anomaly data processing\nStep 4 algorithm data conformity 1st Test: Preprocessed algorithm for FDA validation vs Original algorithm\nStep 5 algorithm data conformity 2nd test: Data Science team’s preprocessed algorithm for FDA validation vs algorithm published by BT department\n\nRealization of code centralization, data centralization, and documentation of specific matters by writing job descriptions within the Data Science team\nPlan and conduct statistical analysis after planning and establishing Seegene’s own software testing & advanced testing model"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#required-skills-1",
    "href": "docs/projects/dsp_validation/index.html#required-skills-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "FDA software validation knowledge\nStatistics\nDynamic documentation\nBiology\nClinical study design"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#colaborators-1",
    "href": "docs/projects/dsp_validation/index.html#colaborators-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "5 data scientists (I am a project owner.)\n3 data enineers\n27 biologists\n2 patent attorneys"
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#acheivements-1",
    "href": "docs/projects/dsp_validation/index.html#acheivements-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "DSP Algorithm Output\nDescription\n\n\n\n\nFDA Validation 1st Draft\nthe 1st draft of verification & validation report for FDA submission\n\n\nData Input System\nIt is a temporary data input system that develops into a platform that calculates a large amount of data in large quantities.\n\n\nDocumentation System\nEstablishment of previously absent documentation and document automation systems \\(\\rightarrow\\) Necessary for business communication and establishment of Relational Database System\n\n\nData Management System\ndata quality control system\n\n\nFDA Validation Model\nEstablishment of validation model for DSP algorithm\n\n\nPatent Invention\nInventing the FDA Validation Model\n\n\nIn-house first Performance evaluation of algorithms and reagent products\nComprehensive performance evaluation of algorithms and reagent products that did not previously exist in-house\n\n\nStatistical analysis related to algorithmic risk management\nRisk management-related statistical analysis is performed on noise and anomaly data that may occur due to reagent and equipment-specific random effects and other confounders."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html#long-term-project-1",
    "href": "docs/projects/dsp_validation/index.html#long-term-project-1",
    "title": "FDA Algorithms Validation",
    "section": "",
    "text": "Collaboratively write BT’s job description and establish RDB system\nBuilding a DevOps Platform for reagents, equipment, software and algorithm validation"
  },
  {
    "objectID": "docs/projects/index.html",
    "href": "docs/projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\nPLAN, MATH, SP, STAT, ML, and DL stands for 'Intellectual Property Planning', 'Mathematics', 'Signal Processing', 'Statistics', 'Machine Learning', and \"Deep Learning\", respectively.\n\n\n\n[STAT][Surveilance] Diagnostic Algorithm Validation for FDA (Current Project)\n[STAT][ML] LLFS (Long Life Family Study)\n[STAT][ML] Diagnostic Device QC Platform\n[PLAN] Platform IP Planning (To be written)\n[ML] Data-Driven Diagnostic Algorithm (To be written)\n[STAT][SP] Clinical Data Analysis for QC (To be written)\n[ML][MATH][Biology] Heavy Metal Removal Algorihtm using Tea Leaves\n[ML] Diffusion Model of Social Networks using Genetic Algorithm (To be written)\n[Biochemistry] Effects of Phellinus Linteus toward Formation of Lymphatic Vessel\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html",
    "href": "docs/projects/LLFS/eda.html",
    "title": "EDA",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n(한글 준비중)\nplease, read the English section first.\n\n\nCode\n## Function List\n\ncolor_function&lt;-function(category_number){\nreturn(\n    if(category_number==2){\n        c(\"darkblue\",\"darkred\")\n    }else if(category_number==3){\n        c(\"darkblue\",\"darkred\",\"yellow4\")\n    }else if(category_number==4){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\")\n    }else if(category_number==5){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\")\n    }else{\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\",\"darkgreen\")\n    }\n    )\n}\n\nscale_function=function(vector=x,min=NULL,max=NULL,method){\n    scaling_methods&lt;-c('min_max normalization','customized normalization','standardization')\n\n    if(method==\"min-max\"){\n        output=(vector-min(vector))/(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        output=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else if(method==\"standarized\"){\n        output=(vector-mean(vector))/sd(vector)\n    }else{\n        output=paste0(\"Error!, no such a scaling method in this module. \\n Please, put the first word of each method you want to use in the 'method' argument among the following tests: \", paste(scaling_methods,collapse=\", \"))\n    }\n  return(output)\n}\n\nmultiple_shapiro_test&lt;-function(in_data){\n        normality_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                            function(x)shapiro.test(x))\n        temp&lt;-data.frame(matrix(nrow=length(normality_test),ncol=4))\n        for (i in 1:length(normality_test)){\n            temp[i,]&lt;-c(\n                coloumn_name=names(normality_test)[i],\n                statistic=normality_test[[i]]$statistic,\n                p_value=normality_test[[i]]$p.value,\n                method=normality_test[[i]]$method)\n        }\n        names(temp)&lt;-c('column_name','statistic','p_value','method')\n        output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'not_normal','normal'))%&gt;%\n            dplyr::select('column_name','statistic','p_value','p_adjusted','type','method')\n        return(output)\n}    \n\nmultiple_levene_test&lt;-function(in_data,categorical_variable){\n        homoscedasticity_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                                    function(x)leveneTest(x~in_data[,categorical_variable]))\n        temp&lt;-data.frame(matrix(nrow=length(homoscedasticity_test),ncol=6))\n            for (i in 1:length(homoscedasticity_test)){\n                temp[i,]&lt;-c(\n                    coloumn_name=names(homoscedasticity_test)[i],\n                    group_df=homoscedasticity_test[[i]]$Df[1],\n                    residual_df=homoscedasticity_test[[i]]$Df[2],\n                    statistic=homoscedasticity_test[[i]]$`F value`[1],\n                    p_value=homoscedasticity_test[[i]]$`Pr(&gt;F)`[1],\n                    method=\"levene's test\")\n            }\n            names(temp)&lt;-c('column_name','group_df','residual_df','statistic','p_value','method')\n            output&lt;-temp%&gt;%\n                mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n                type=ifelse(p_adjusted&lt;0.05,'heteroscedasticity','homoscedasticity'))%&gt;%\n                dplyr::select('column_name','group_df','residual_df','statistic','p_value','p_adjusted','type','method')\n        return(output)} \n\nmultiple_unpaired_t_test&lt;-function(in_data,categorical_variable,homo_variables,hetero_variables){\n    homo_unpaired_t_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))][,homo_variables],2,\n                                    function(x)t.test(x~in_data[,categorical_variable],var.equal=TRUE))\n    hetero_unpaired_t_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))][,hetero_variables],2,\n                                    function(x)t.test(x~in_data[,categorical_variable],var.equal=FALSE)) \n    unpaired_t_test&lt;-c(homo_unpaired_t_test,hetero_unpaired_t_test)\n\n    temp&lt;-data.frame(matrix(nrow=length(unpaired_t_test),ncol=7))\n        for (i in 1:length(unpaired_t_test)){\n            temp[i,]&lt;-c(names(unpaired_t_test)[i], \n                        unpaired_t_test[[i]]$estimate,\n                        unpaired_t_test[[i]]$parameter,\n                        unpaired_t_test[[i]]$statistic,\n                        unpaired_t_test[[i]]$p.value,\n                        unpaired_t_test[[i]]$method)\n        }\n        names(temp)&lt;-c('column_name',names(unpaired_t_test[[1]]$estimate),'df','statistic','p_value','method')\n        output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'significant','insignificant'))%&gt;%\n            dplyr::select('column_name',names(unpaired_t_test[[1]]$estimate),'df','statistic','p_value','p_adjusted','type','method')\n    return(output)} \n\n\nmultiple_correlation_test&lt;-function(in_data,in_numeric_variable){\n    correlation_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                                    function(x)cor.test(x,in_data[,in_numeric_variable],method='pearson'))\n    temp&lt;-data.frame(matrix(nrow=length(correlation_test),ncol=6))\n        for (i in 1:length(correlation_test)){\n            temp[i,]&lt;-c(names(correlation_test)[i], \n                        correlation_test[[i]]$estimate,\n                        correlation_test[[i]]$parameter,\n                        correlation_test[[i]]$statistic,\n                        correlation_test[[i]]$p.value,\n                        correlation_test[[i]]$method)\n        }\n        names(temp)&lt;-c('column_name',names(correlation_test[[1]]$estimate),'df','statistic','p_value','method')\n        output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'significant','insignificant'))%&gt;%\n            dplyr::select('column_name',names(correlation_test[[1]]$estimate),'df','statistic','p_value','p_adjusted','type','method')\n    return(output)} \n\nmultiple_anova_test&lt;-function(in_data, in_categorical_variable){\n    aov_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                function(x)aov(x~get(in_categorical_variable),data=in_data)%&gt;%summary)\n\n    temp&lt;-data.frame(matrix(nrow=length(aov_test),ncol=10))\n    for (i in 1:length(aov_test)){\n        temp[i,]&lt;-c(names(aov_test)[i], \n                    aov_test[[i]][[1]]$`Df`[1],\n                    aov_test[[i]][[1]]$`Df`[2],\n                    aov_test[[i]][[1]]$`Sum Sq`[1],\n                    aov_test[[i]][[1]]$`Sum Sq`[2],\n                    aov_test[[i]][[1]]$`Mean Sq`[1],\n                    aov_test[[i]][[1]]$`Mean Sq`[2],\n                    aov_test[[i]][[1]]$`F value`[1],\n                    aov_test[[i]][[1]]$`Pr(&gt;F)`[1],\n                    'one_way_anova')\n    }\n    names(temp)&lt;-c('column_name','group_df','residual_df','group_ssq','residual_ssq',\n                    'group_msq','residual_msq','F_value','p_value','method')\n    output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'significant','insignificant'))%&gt;%\n            dplyr::select('column_name','group_df','residual_df','group_ssq','residual_ssq',\n                    'group_msq','residual_msq','F_value','p_value','method',\n                    'p_adjusted','type','method')\n    return(output)} \n\nmain_statistical_test&lt;-function(\n    in_data,method,categorical_variable,in_numeric_variable,\n    homo_variables=NULL,hetero_variables=NULL,\n    fun1=multiple_shapiro_test,\n    fun2=multiple_levene_test,\n    fun3=multiple_unpaired_t_test){\n    test_list&lt;-c(\"shapiro wilks test\",\"levene's test\",\"student t test\",\"anova\",\"correlation test\")#,\"ANCOVA\",\"MANOVA\",\"wilcoxon manwhitney\",\"kruskal wallis test\",\"fisher exact test\",\"anderson darling\")\n    error_massage&lt;-paste0(\"Error!, no such a test in this module. \\n Please, put the first word of each method you want to use in the 'method' argument among the following tests: \", paste(test_list,collapse=\", \"))\n    if(grepl('shapiro',method)){\n        output=multiple_shapiro_test(in_data)\n    }else if(grepl('levene',method)){\n        output=multiple_levene_test(in_data,categorical_variable)\n        # var.test()\n    }else if(grepl('student',method)){\n        # code unpaired vs paired t test in the future\n        output=multiple_unpaired_t_test(in_data,categorical_variable,homo_variables,hetero_variables)\n    }else if(grepl('kruskal',method)){\n        return(error_massage)\n    }else if(grepl('wilcoxon|manwhitney',method)){\n        return(error_massage)\n    }else if(grepl('anova|aov',method)){\n        output=multiple_anova_test(in_data,categorical_variable)\n    }else if(grepl('cor',method)){\n        output=multiple_correlation_test(in_data,in_numeric_variable)\n    }else{\n        return(error_massage)\n    }\n    return(output)\n}\n\n\ngetNumericSummaryTable=function(in_data,group_variable,summary_variable,set_color=color_function,...){\n    # table\n    temp&lt;-in_data %&gt;% \n    #group_by_at(vars(...)) %&gt;% \n    group_by_at(vars(group_variable)) %&gt;% \n    mutate(count=n())%&gt;%\n    summarise_at(vars(summary_variable,count),\n                 list(mean=mean,\n                 sd=sd,\n                 min=min,\n                 Q1=~quantile(., probs = 0.25),\n                 median=median, \n                 Q3=~quantile(., probs = 0.75),\n                 max=max))%&gt;%\n                 as.data.frame()%&gt;%\n                 rename(\n                 n=count_mean)%&gt;%\n                 dplyr::select(-contains('count'))%&gt;%\n                 as.data.frame()\n    names(temp)&lt;-c(\"group\",\n    sapply(names(temp)[-1],function(x)str_replace(x,paste0(summary_variable,\"_\"),\"\")))\n    output&lt;-temp%&gt;%\n    mutate(\n        variable=group_variable,\n        summary=summary_variable,\n        mean=mean%&gt;%round(2),\n        sd=sd%&gt;%round(2),\n        min=min%&gt;%round(2),\n        Q1=Q1%&gt;%round(2),\n        Q4=Q3%&gt;%round(2),\n        max=max%&gt;%round(2),\n        IQR_min=Q1-(Q3-Q1)*1.5%&gt;%round(2),\n    IQR_max=Q3+(Q3-Q1)*1.5%&gt;%round(2),\n    proportion=paste0(round(n/nrow(all_data)*100,2),\"%\"))%&gt;%\n    dplyr::select(variable,group,summary,n,proportion,mean,sd,min,IQR_min,Q1,median,Q3,IQR_max,max)\n    return(output)\n}\n\ngetNumericSummaryPlot=function(\n    in_data=all_data,group_variable,summary_variable,\n    set_color=color_function,\n    summary_function=getNumericSummaryTable,...){\n    # plot\n    temp=getNumericSummaryTable(in_data,group_variable,summary_variable)\n    temp2=temp\n    names(temp2)[2]=group_variable\n    plot&lt;-\n    in_data%&gt;%\n    dplyr::select(group_variable,summary_variable)%&gt;%\n    inner_join(.,temp2,by=group_variable)%&gt;%\n    ggplot(aes(x=age,fill=get(group_variable),color=get(group_variable)))+\n    geom_histogram(aes(y=..density..),binwidth=1,alpha=0.5, position=\"identity\")+\n    geom_vline(aes(xintercept=mean,color=get(group_variable)), linetype=\"dashed\", size=1.5) + \n    geom_density(aes(y=..density..),alpha=0.3) +\n    scale_color_manual(values=set_color(nrow(temp2)))+\n    scale_fill_manual(values=set_color(nrow(temp2)))+\n    theme_bw()+\n    theme(legend.position = c(.95, .95),\n    legend.justification = c(\"right\", \"top\"),\n    legend.margin = margin(6, 6, 6, 6),\n    legend.text = element_text(size = 10))+\n    guides(fill=guide_legend(title=group_variable),\n    color=FALSE)+\n    geom_text(aes(label=round(mean,1),y=0,x=mean),\n                vjust=-1,col='yellow',size=5)+\n    ggtitle(paste0(\"Histogram & Density, \", summary_variable, \" Grouped by \", group_variable))+\n        labs(x=summary_variable, y = \"Density\")\n\n    result&lt;-plot\n    return(result)\n}\n\n\n\n\nCode\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# raw data\nnormality_test_result&lt;-multiple_shapiro_test(all_data)%&gt;%\n    filter(column_name!='id')%&gt;%\n    group_by(type)%&gt;%\n    summarise(count=n())%&gt;%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%&gt;%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%&gt;%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n1001\n1001\n1\n\n\n\n\n\nOut of 1001 numeric variables, the variables following a normal distribution are 1001 (100%) and the ones that do not are NA (NA%).\n\n\n\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables&lt;-\n    multiple_shapiro_test(all_data)%&gt;%\n        filter(p_value&gt;0.05,column_name!='id')%&gt;%\n            dplyr::select(column_name)%&gt;%\n            pull%&gt;%sample(16)\n\nnormal_data&lt;-\n    all_data%&gt;%\n        dplyr::select(outcome,normal_variables)%&gt;%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%&gt;%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',linewidth=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution Grouped by Disease Status\")\n\n\n\n\n\n\n\n\nThere is no variable that do not follow a normal distribution.\n\n\n\n\n\n\nThrough the exploratory data analysis above, it was confirmed that all variables follow a normal distribution, and t tests were conducted to select metabolites that have significant relationships with the disease status, AD. To minimize type 1 error due to multiple testings, bonferroni correction was used in the EDA\n\nHomoscedasticity Test\n\nLeven’s test is performed to confirm that each variable has equality of variance, one of the assumptions of the t test and ANOVA.\n\n\nCode\nleven_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")%&gt;%\n    filter(column_name!='id')\n\nhomo_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&gt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\nhetero_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&lt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\n\n\n10 variables randomly selected out of 1001 variables with equal variance: meta408, meta959, meta796, meta957, meta740, meta212, meta171, meta769, meta986, meta178\n0 variables randomly selected with equal variance:\n\n\n\nCode\nhomo_variable_sample&lt;-homo_variable%&gt;%sample(10)\nhetero_variable_sample&lt;-hetero_variable\n\nstratified_levene_data&lt;-all_data%&gt;%\n    dplyr::select(outcome,homo_variable_sample,hetero_variable_sample)%&gt;%\n    gather(key=metabolite,value=value,c(homo_variable_sample,hetero_variable_sample))%&gt;%\n    mutate(levene_test=ifelse(metabolite%in%(homo_variable_sample),\"homoscedasticity\",\"heteroscedasticity\"))\n\nstratified_levene_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(stratified_levene_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    facet_wrap(.~levene_test)+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity vs Homoscedasticity\")\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\n\nCode\nt_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"student\",\n                        categorical_variable=\"outcome\",\n                        homo_variables=homo_variable,\n                        hetero_variables=hetero_variable)\n\nmetabolites_associated_AD&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nmetabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%&gt;%\n    group_by(outcome)%&gt;%\n    summarise(mean=mean(value),sd=sd(value))%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nmean\nsd\n\n\n\n\nnegative\n0.2184112\n0.9869302\n\n\npositive\n-0.4186542\n0.9830175\n\n\n\n\n\nCode\ntop_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%head(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\ntop_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,top_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\nbottom_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%tail(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nbottom_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,bottom_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,bottom_metabolites_associated_AD)        \n\na1&lt;-top_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(top_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Strongly Associated with AD\")\n\na2&lt;-bottom_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(bottom_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Weakly Associated with AD\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\nCode\nsignificant_metabolites&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\n\nAs a result of the t tests, there are 201 metabolites that are significantly associated with AD status at the 5% significance level. Metabolites with the highest significance were designated as strong metabolites and metabolites with the lowest significance among metabolites significantly related to AD status were designated as weak metabolites, and the expression level of metabolites between the disease status was confirmed through visualization. As a result, a greater difference was observed in strong metabolites than in weak metabolites, but both groups were not clearly separated in terms of AD status.\n\n\n\n\n\nCode\nad_sex_summary&lt;-all_data%&gt;%\n    group_by(outcome,sex)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, sex,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$sex)$p.value,\n    method=\"chisquare_test\")\n\nad_sex_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\nsex\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\nman\n150\n30%\n85.79333\n5.146572\n0.1049983\nchisquare_test\n\n\nnegative\nwoman\n160\n32%\n84.96875\n5.284868\n0.1049983\nchisquare_test\n\n\npositive\nman\n77\n15.4%\n79.85714\n5.167495\n0.1049983\nchisquare_test\n\n\npositive\nwoman\n113\n22.6%\n81.29204\n4.896576\n0.1049983\nchisquare_test\n\n\n\n\n\n\n\n\n\n\nCode\nad_genotype_summary&lt;-all_data%&gt;%\n    group_by(outcome,genotype)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, genotype,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\")\n\ngenotype_ad_summary&lt;-all_data%&gt;%\n    group_by(genotype,outcome)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(genotype,outcome,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\") \n\nad_genotype_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\ngenotype\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\ne3\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\nnegative\ne2\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\nnegative\ne4\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\npositive\ne3\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\npositive\ne2\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\npositive\ne4\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nCode\ngenotype_ad_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenotype\noutcome\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\ne3\nnegative\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\ne3\npositive\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\ne2\nnegative\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\ne2\npositive\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\ne4\nnegative\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\ne4\npositive\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nAs you see the tables above, the count of AD status and that of genotype status are proportionately the same. Thus, there is no relation between AD and genotype in this data at the significant level 5%.\n\n\n\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually. For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nad_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\n\n\nCode\nage_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n310\n62%\n85.37\n5.23\n72\n69.625\n81.25\n85\n89\n100.625\n105\n\n\noutcome\npositive\nage\n190\n38%\n80.71\n5.04\n65\n66.500\n77.00\n81\n84\n94.500\n93\n\n\nsex\nman\nage\n227\n45.4%\n83.78\n5.86\n65\n68.000\n80.00\n84\n88\n100.000\n105\n\n\nsex\nwoman\nage\n273\n54.6%\n83.45\n5.43\n67\n69.500\n80.00\n83\n87\n97.500\n100\n\n\ngenotype\ne3\nage\n387\n77.4%\n83.61\n5.64\n65\n69.500\n80.00\n84\n87\n97.500\n102\n\n\ngenotype\ne2\nage\n40\n8%\n85.83\n5.62\n78\n70.875\n81.75\n86\n89\n99.875\n105\n\n\ngenotype\ne4\nage\n73\n14.6%\n82.30\n5.25\n67\n68.500\n79.00\n82\n86\n96.500\n94\n\n\n\n\n\n\n\nCode\nplot&lt;- ggarrange(\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\"),\n    ncol=2, nrow=2,legend=\"bottom\")\nplot\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -4.66, but their standard deviations are 5.04 and 5.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status 4.66 years younger than those with a positive one.\n\n\nCode\na1&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=age,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(legend.position=\"top\")+\n    labs(title=\"Histogram, Age Distribution Grouped by Disease Status\")   \na2&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=outcome,y=age,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Age Distribution Grouped by Disease Status\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\n\n\nCode\nage_correlation_data&lt;-\n    main_statistical_test(in_data=all_data,in_numeric_variable = 'age',method = \"cor\")%&gt;%\n    filter(p_adjusted&lt;0.05,column_name!='id')\n\n\n151 metabolites are significantly associated with age.\n\n\n\n\n\nCode\ngenotype_aov&lt;-main_statistical_test(in_data=all_data,categorical_variable=\"genotype\",method='aov')\ngenotype_aov%&gt;%\nfilter(column_name!='id',p_adjusted&lt;0.05)%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ngroup_df\nresidual_df\ngroup_ssq\nresidual_ssq\ngroup_msq\nresidual_msq\nF_value\np_value\nmethod\np_adjusted\ntype\n\n\n\n\n\n\n\n0 metabolites are significantly associated with the genotype variable.\n\n\n\n\n\n\n\n\nCode\nall_data%&gt;%\n    group_by(outcome,sex,genotype)%&gt;%\n    summarise(count=n(),\n    mean_age=mean(age),\n    sd_age=sd(age))%&gt;%ungroup%&gt;%\n    mutate(sum_count=sum(count),\n    proportion=paste0(round(count/sum_count*100,3),'%'))%&gt;%\n    dplyr::select(outcome,sex,genotype,count,proportion, mean_age,sd_age)%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\ngenotype\ncount\nproportion\nmean_age\nsd_age\n\n\n\n\nnegative\nman\ne3\n113\n22.6%\n85.56637\n5.091793\n\n\nnegative\nman\ne2\n14\n2.8%\n88.57143\n6.548215\n\n\nnegative\nman\ne4\n23\n4.6%\n85.21739\n4.067125\n\n\nnegative\nwoman\ne3\n129\n25.8%\n85.07752\n5.370074\n\n\nnegative\nwoman\ne2\n12\n2.4%\n85.75000\n5.446016\n\n\nnegative\nwoman\ne4\n19\n3.8%\n83.73684\n4.628920\n\n\npositive\nman\ne3\n66\n13.2%\n79.74242\n5.384624\n\n\npositive\nman\ne2\n5\n1%\n83.00000\n3.000000\n\n\npositive\nman\ne4\n6\n1.2%\n78.50000\n3.082207\n\n\npositive\nwoman\ne3\n79\n15.8%\n81.65823\n4.784821\n\n\npositive\nwoman\ne2\n9\n1.8%\n83.22222\n3.632416\n\n\npositive\nwoman\ne4\n25\n5%\n79.44000\n5.260545\n\n\n\n\n\nExcept for the disease status variable, it looks like there is no difference of age in average and count in the other categorical variables. Taking into account age varialble is significantly associated with the metabolites that are significantly associated with the disease status, outcome (or AD). There is no need to conduct futher exploratory data analysis in the multivariable analysis senction.\n\n\n\n\n\n\nplease, read the English section first.\n\n\nCode\n## Function List\n\ncolor_function&lt;-function(category_number){\nreturn(\n    if(category_number==2){\n        c(\"darkblue\",\"darkred\")\n    }else if(category_number==3){\n        c(\"darkblue\",\"darkred\",\"yellow4\")\n    }else if(category_number==4){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\")\n    }else if(category_number==5){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\")\n    }else{\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\",\"darkgreen\")\n    }\n    )\n}\n\nscale_function=function(vector=x,min=NULL,max=NULL,method){\n    scaling_methods&lt;-c('min_max normalization','customized normalization','standardization')\n\n    if(method==\"min-max\"){\n        output=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        output=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else if(method==\"standarized\"){\n        output=(vector-mean(vector))/sd(vector)\n    }else{\n        output=paste0(\"Error!, no such a scaling method in this module. \\n Please, put the first word of each method you want to use in the 'method' argument among the following tests: \", paste(scaling_methods,collapse=\", \"))\n    }\n  return(output)\n}\n\nmultiple_shapiro_test&lt;-function(in_data){\n        normality_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                            function(x)shapiro.test(x))\n        temp&lt;-data.frame(matrix(nrow=length(normality_test),ncol=4))\n        for (i in 1:length(normality_test)){\n            temp[i,]&lt;-c(\n                coloumn_name=names(normality_test)[i],\n                statistic=normality_test[[i]]$statistic,\n                p_value=normality_test[[i]]$p.value,\n                method=normality_test[[i]]$method)\n        }\n        names(temp)&lt;-c('column_name','statistic','p_value','method')\n        output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'not_normal','normal'))%&gt;%\n            dplyr::select('column_name','statistic','p_value','p_adjusted','type','method')\n        return(output)\n}    \n\nmultiple_levene_test&lt;-function(in_data,categorical_variable){\n        homoscedasticity_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                                    function(x)leveneTest(x~in_data[,categorical_variable]))\n        temp&lt;-data.frame(matrix(nrow=length(homoscedasticity_test),ncol=6))\n            for (i in 1:length(homoscedasticity_test)){\n                temp[i,]&lt;-c(\n                    coloumn_name=names(homoscedasticity_test)[i],\n                    group_df=homoscedasticity_test[[i]]$Df[1],\n                    residual_df=homoscedasticity_test[[i]]$Df[2],\n                    statistic=homoscedasticity_test[[i]]$`F value`[1],\n                    p_value=homoscedasticity_test[[i]]$`Pr(&gt;F)`[1],\n                    method=\"levene's test\")\n            }\n            names(temp)&lt;-c('column_name','group_df','residual_df','statistic','p_value','method')\n            output&lt;-temp%&gt;%\n                mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n                type=ifelse(p_adjusted&lt;0.05,'heteroscedasticity','homoscedasticity'))%&gt;%\n                dplyr::select('column_name','group_df','residual_df','statistic','p_value','p_adjusted','type','method')\n        return(output)} \n\nmultiple_unpaired_t_test&lt;-function(in_data,categorical_variable,homo_variables,hetero_variables){\n    homo_unpaired_t_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))][,homo_variables],2,\n                                    function(x)t.test(x~in_data[,categorical_variable],var.equal=TRUE))\n    hetero_unpaired_t_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))][,hetero_variables],2,\n                                    function(x)t.test(x~in_data[,categorical_variable],var.equal=FALSE)) \n    unpaired_t_test&lt;-c(homo_unpaired_t_test,hetero_unpaired_t_test)\n\n    temp&lt;-data.frame(matrix(nrow=length(unpaired_t_test),ncol=7))\n        for (i in 1:length(unpaired_t_test)){\n            temp[i,]&lt;-c(names(unpaired_t_test)[i], \n                        unpaired_t_test[[i]]$estimate,\n                        unpaired_t_test[[i]]$parameter,\n                        unpaired_t_test[[i]]$statistic,\n                        unpaired_t_test[[i]]$p.value,\n                        unpaired_t_test[[i]]$method)\n        }\n        names(temp)&lt;-c('column_name',names(unpaired_t_test[[1]]$estimate),'df','statistic','p_value','method')\n        output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'significant','insignificant'))%&gt;%\n            dplyr::select('column_name',names(unpaired_t_test[[1]]$estimate),'df','statistic','p_value','p_adjusted','type','method')\n    return(output)} \n\n\nmultiple_correlation_test&lt;-function(in_data,in_numeric_variable){\n    correlation_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                                    function(x)cor.test(x,in_data[,in_numeric_variable],method='pearson'))\n    temp&lt;-data.frame(matrix(nrow=length(correlation_test),ncol=6))\n        for (i in 1:length(correlation_test)){\n            temp[i,]&lt;-c(names(correlation_test)[i], \n                        correlation_test[[i]]$estimate,\n                        correlation_test[[i]]$parameter,\n                        correlation_test[[i]]$statistic,\n                        correlation_test[[i]]$p.value,\n                        correlation_test[[i]]$method)\n        }\n        names(temp)&lt;-c('column_name',names(correlation_test[[1]]$estimate),'df','statistic','p_value','method')\n        output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'significant','insignificant'))%&gt;%\n            dplyr::select('column_name',names(correlation_test[[1]]$estimate),'df','statistic','p_value','p_adjusted','type','method')\n    return(output)} \n\nmultiple_anova_test&lt;-function(in_data, in_categorical_variable){\n    aov_test&lt;-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                function(x)aov(x~get(in_categorical_variable),data=in_data)%&gt;%summary)\n\n    temp&lt;-data.frame(matrix(nrow=length(aov_test),ncol=10))\n    for (i in 1:length(aov_test)){\n        temp[i,]&lt;-c(names(aov_test)[i], \n                    aov_test[[i]][[1]]$`Df`[1],\n                    aov_test[[i]][[1]]$`Df`[2],\n                    aov_test[[i]][[1]]$`Sum Sq`[1],\n                    aov_test[[i]][[1]]$`Sum Sq`[2],\n                    aov_test[[i]][[1]]$`Mean Sq`[1],\n                    aov_test[[i]][[1]]$`Mean Sq`[2],\n                    aov_test[[i]][[1]]$`F value`[1],\n                    aov_test[[i]][[1]]$`Pr(&gt;F)`[1],\n                    'one_way_anova')\n    }\n    names(temp)&lt;-c('column_name','group_df','residual_df','group_ssq','residual_ssq',\n                    'group_msq','residual_msq','F_value','p_value','method')\n    output&lt;-temp%&gt;%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted&lt;0.05,'significant','insignificant'))%&gt;%\n            dplyr::select('column_name','group_df','residual_df','group_ssq','residual_ssq',\n                    'group_msq','residual_msq','F_value','p_value','method',\n                    'p_adjusted','type','method')\n    return(output)} \n\nmain_statistical_test&lt;-function(\n    in_data,method,categorical_variable,in_numeric_variable,\n    homo_variables=NULL,hetero_variables=NULL,\n    fun1=multiple_shapiro_test,\n    fun2=multiple_levene_test,\n    fun3=multiple_unpaired_t_test){\n    test_list&lt;-c(\"shapiro wilks test\",\"levene's test\",\"student t test\",\"anova\",\"correlation test\")#,\"ANCOVA\",\"MANOVA\",\"wilcoxon manwhitney\",\"kruskal wallis test\",\"fisher exact test\",\"anderson darling\")\n    error_massage&lt;-paste0(\"Error!, no such a test in this module. \\n Please, put the first word of each method you want to use in the 'method' argument among the following tests: \", paste(test_list,collapse=\", \"))\n    if(grepl('shapiro',method)){\n        output=multiple_shapiro_test(in_data)\n    }else if(grepl('levene',method)){\n        output=multiple_levene_test(in_data,categorical_variable)\n        # var.test()\n    }else if(grepl('student',method)){\n        # code unpaired vs paired t test in the future\n        output=multiple_unpaired_t_test(in_data,categorical_variable,homo_variables,hetero_variables)\n    }else if(grepl('kruskal',method)){\n        return(error_massage)\n    }else if(grepl('wilcoxon|manwhitney',method)){\n        return(error_massage)\n    }else if(grepl('anova|aov',method)){\n        output=multiple_anova_test(in_data,categorical_variable)\n    }else if(grepl('cor',method)){\n        output=multiple_correlation_test(in_data,in_numeric_variable)\n    }else{\n        return(error_massage)\n    }\n    return(output)\n}\n\n\ngetNumericSummaryTable=function(in_data,group_variable,summary_variable,set_color=color_function,...){\n    # table\n    temp&lt;-in_data %&gt;% \n    #group_by_at(vars(...)) %&gt;% \n    group_by_at(vars(group_variable)) %&gt;% \n    mutate(count=n())%&gt;%\n    summarise_at(vars(summary_variable,count),\n                 list(mean=mean,\n                 sd=sd,\n                 min=min,\n                 Q1=~quantile(., probs = 0.25),\n                 median=median, \n                 Q3=~quantile(., probs = 0.75),\n                 max=max))%&gt;%\n                 as.data.frame()%&gt;%\n                 rename(\n                 n=count_mean)%&gt;%\n                 dplyr::select(-contains('count'))%&gt;%\n                 as.data.frame()\n    names(temp)&lt;-c(\"group\",\n    sapply(names(temp)[-1],function(x)str_replace(x,paste0(summary_variable,\"_\"),\"\")))\n    output&lt;-temp%&gt;%\n    mutate(\n        variable=group_variable,\n        summary=summary_variable,\n        mean=mean%&gt;%round(2),\n        sd=sd%&gt;%round(2),\n        min=min%&gt;%round(2),\n        Q1=Q1%&gt;%round(2),\n        Q4=Q3%&gt;%round(2),\n        max=max%&gt;%round(2),\n        IQR_min=Q1-(Q3-Q1)*1.5%&gt;%round(2),\n    IQR_max=Q3+(Q3-Q1)*1.5%&gt;%round(2),\n    proportion=paste0(round(n/nrow(all_data)*100,2),\"%\"))%&gt;%\n    dplyr::select(variable,group,summary,n,proportion,mean,sd,min,IQR_min,Q1,median,Q3,IQR_max,max)\n    return(output)\n}\n\ngetNumericSummaryPlot=function(\n    in_data=all_data,group_variable,summary_variable,\n    set_color=color_function,\n    summary_function=getNumericSummaryTable,...){\n    # plot\n    temp=getNumericSummaryTable(in_data,group_variable,summary_variable)\n    temp2=temp\n    names(temp2)[2]=group_variable\n    plot&lt;-\n    in_data%&gt;%\n    dplyr::select(group_variable,summary_variable)%&gt;%\n    inner_join(.,temp2,by=group_variable)%&gt;%\n    ggplot(aes(x=age,fill=get(group_variable),color=get(group_variable)))+\n    geom_histogram(aes(y=..density..),binwidth=1,alpha=0.5, position=\"identity\")+\n    geom_vline(aes(xintercept=mean,color=get(group_variable)), linetype=\"dashed\", size=1.5) + \n    geom_density(aes(y=..density..),alpha=0.3) +\n    scale_color_manual(values=set_color(nrow(temp2)))+\n    scale_fill_manual(values=set_color(nrow(temp2)))+\n    theme_bw()+\n    theme(legend.position = c(.95, .95),\n    legend.justification = c(\"right\", \"top\"),\n    legend.margin = margin(6, 6, 6, 6),\n    legend.text = element_text(size = 10))+\n    guides(fill=guide_legend(title=group_variable),\n    color=FALSE)+\n    geom_text(aes(label=round(mean,1),y=0,x=mean),\n                vjust=-1,col='yellow',size=5)+\n    ggtitle(paste0(\"Histogram & Density, \", summary_variable, \" Grouped by \", group_variable))+\n        labs(x=summary_variable, y = \"Density\")\n\n    result&lt;-plot\n    return(result)\n}\n\n\n\n\nCode\n# load simulation data\nsimulated_data&lt;-read_rds(datapath)\n\n# simple data pre-processing\nall_data&lt;-\n    simulated_data%&gt;%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]&lt;-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# raw data\nnormality_test_result&lt;-multiple_shapiro_test(all_data)%&gt;%\n    filter(column_name!='id')%&gt;%\n    group_by(type)%&gt;%\n    summarise(count=n())%&gt;%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%&gt;%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%&gt;%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n1001\n1001\n1\n\n\n\n\n\nOut of 1001 numeric variables, the variables following a normal distribution are 1001 (100%) and the ones that do not are NA (NA%).\n\n\n\n\n\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables&lt;-\n    multiple_shapiro_test(all_data)%&gt;%\n        filter(p_value&gt;0.05,column_name!='id')%&gt;%\n            dplyr::select(column_name)%&gt;%\n            pull%&gt;%sample(16)\n\nnormal_data&lt;-\n    all_data%&gt;%\n        dplyr::select(outcome,normal_variables)%&gt;%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%&gt;%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',linewidth=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution Grouped by Disease Status\")\n\n\n\n\n\n\n\n\nThere is no variable that do not follow a normal distribution.\n\n\n\n\n\n\nThrough the exploratory data analysis above, it was confirmed that all variables follow a normal distribution, and t tests were conducted to select metabolites that have significant relationships with the disease status, AD. To minimize type 1 error due to multiple testings, bonferroni correction was used in the EDA\n\nHomoscedasticity Test\n\nLeven’s test is performed to confirm that each variable has equality of variance, one of the assumptions of the t test and ANOVA.\n\n\nCode\nleven_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")%&gt;%\n    filter(column_name!='id')\n\nhomo_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&gt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\nhetero_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&lt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\n\n\n10 variables randomly selected out of 1001 variables with equal variance: meta408, meta959, meta796, meta957, meta740, meta212, meta171, meta769, meta986, meta178\n0 variables randomly selected with equal variance:\n\n\n\nCode\nhomo_variable_sample&lt;-homo_variable%&gt;%sample(10)\nhetero_variable_sample&lt;-hetero_variable\n\nstratified_levene_data&lt;-all_data%&gt;%\n    dplyr::select(outcome,homo_variable_sample,hetero_variable_sample)%&gt;%\n    gather(key=metabolite,value=value,c(homo_variable_sample,hetero_variable_sample))%&gt;%\n    mutate(levene_test=ifelse(metabolite%in%(homo_variable_sample),\"homoscedasticity\",\"heteroscedasticity\"))\n\nstratified_levene_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(stratified_levene_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    facet_wrap(.~levene_test)+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity vs Homoscedasticity\")\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\n\nCode\nt_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"student\",\n                        categorical_variable=\"outcome\",\n                        homo_variables=homo_variable,\n                        hetero_variables=hetero_variable)\n\nmetabolites_associated_AD&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nmetabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%&gt;%\n    group_by(outcome)%&gt;%\n    summarise(mean=mean(value),sd=sd(value))%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nmean\nsd\n\n\n\n\nnegative\n0.2184112\n0.9869302\n\n\npositive\n-0.4186542\n0.9830175\n\n\n\n\n\nCode\ntop_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%head(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\ntop_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,top_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\nbottom_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%tail(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nbottom_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,bottom_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,bottom_metabolites_associated_AD)        \n\na1&lt;-top_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(top_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Strongly Associated with AD\")\n\na2&lt;-bottom_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(bottom_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Weakly Associated with AD\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\nCode\nsignificant_metabolites&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n# significant_metabolites%&gt;%write_rds(.,file='./docs/data/llfs_fake_significant_metabolites.rds')\n\n\nAs a result of the t tests, there are 201 metabolites that are significantly associated with AD status. Metabolites with the highest significance were designated as strong metabolites and metabolites with the lowest significance among metabolites significantly related to AD status were designated as weak metabolites, and the expression level of metabolites between the disease status was confirmed through visualization. As a result, a greater difference was observed in strong metabolites than in weak metabolites, but both groups were not clearly separated in terms of AD status.\n\n\n\n\n\nCode\nad_sex_summary&lt;-all_data%&gt;%\n    group_by(outcome,sex)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, sex,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$sex)$p.value,\n    method=\"chisquare_test\")\n\nad_sex_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\nsex\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\nman\n150\n30%\n85.79333\n5.146572\n0.1049983\nchisquare_test\n\n\nnegative\nwoman\n160\n32%\n84.96875\n5.284868\n0.1049983\nchisquare_test\n\n\npositive\nman\n77\n15.4%\n79.85714\n5.167495\n0.1049983\nchisquare_test\n\n\npositive\nwoman\n113\n22.6%\n81.29204\n4.896576\n0.1049983\nchisquare_test\n\n\n\n\n\n\n\n\n\n\nCode\nad_genotype_summary&lt;-all_data%&gt;%\n    group_by(outcome,genotype)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, genotype,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\")\n\ngenotype_ad_summary&lt;-all_data%&gt;%\n    group_by(genotype,outcome)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(genotype,outcome,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\") \n\nad_genotype_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\ngenotype\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\ne3\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\nnegative\ne2\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\nnegative\ne4\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\npositive\ne3\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\npositive\ne2\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\npositive\ne4\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nCode\ngenotype_ad_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenotype\noutcome\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\ne3\nnegative\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\ne3\npositive\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\ne2\nnegative\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\ne2\npositive\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\ne4\nnegative\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\ne4\npositive\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nAs you see the tables above, the count of AD status and that of genotype status are proportionately the same. Thus, there is no relation between AD and genotype in this data at the significant level 5%.\n\n\n\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually. For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nad_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\n\n\nCode\nage_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n310\n62%\n85.37\n5.23\n72\n69.625\n81.25\n85\n89\n100.625\n105\n\n\noutcome\npositive\nage\n190\n38%\n80.71\n5.04\n65\n66.500\n77.00\n81\n84\n94.500\n93\n\n\nsex\nman\nage\n227\n45.4%\n83.78\n5.86\n65\n68.000\n80.00\n84\n88\n100.000\n105\n\n\nsex\nwoman\nage\n273\n54.6%\n83.45\n5.43\n67\n69.500\n80.00\n83\n87\n97.500\n100\n\n\ngenotype\ne3\nage\n387\n77.4%\n83.61\n5.64\n65\n69.500\n80.00\n84\n87\n97.500\n102\n\n\ngenotype\ne2\nage\n40\n8%\n85.83\n5.62\n78\n70.875\n81.75\n86\n89\n99.875\n105\n\n\ngenotype\ne4\nage\n73\n14.6%\n82.30\n5.25\n67\n68.500\n79.00\n82\n86\n96.500\n94\n\n\n\n\n\n\n\nCode\nplot&lt;- ggarrange(\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\"),\n    ncol=2, nrow=2,legend=\"bottom\")\nplot\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -4.66, but their standard deviations are 5.04 and 5.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status 4.66 years younger than those with a positive one.\n\n\nCode\na1&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=age,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(legend.position=\"top\")+\n    labs(title=\"Histogram, Age Distribution Grouped by Disease Status\")   \na2&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=outcome,y=age,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Age Distribution Grouped by Disease Status\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\n\n\nCode\nage_correlation_data&lt;-\n    main_statistical_test(in_data=all_data,in_numeric_variable = 'age',method = \"cor\")%&gt;%\n    filter(p_adjusted&lt;0.05,column_name!='id')\n\n\n151 metabolites are significantly associated with age.\n\n\n\n\n\nCode\ngenotype_aov&lt;-main_statistical_test(in_data=all_data,categorical_variable=\"genotype\",method='aov')\ngenotype_aov%&gt;%\nfilter(column_name!='id',p_adjusted&lt;0.05)%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ngroup_df\nresidual_df\ngroup_ssq\nresidual_ssq\ngroup_msq\nresidual_msq\nF_value\np_value\nmethod\np_adjusted\ntype\n\n\n\n\n\n\n\n0 metabolites are significantly associated with the genotype variable.\n\n\n\n\n\n\n\n\nCode\nall_data%&gt;%\n    group_by(outcome,sex,genotype)%&gt;%\n    summarise(count=n(),\n    mean_age=mean(age),\n    sd_age=sd(age))%&gt;%ungroup%&gt;%\n    mutate(sum_count=sum(count),\n    proportion=paste0(round(count/sum_count*100,3),'%'))%&gt;%\n    dplyr::select(outcome,sex,genotype,count,proportion, mean_age,sd_age)%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\ngenotype\ncount\nproportion\nmean_age\nsd_age\n\n\n\n\nnegative\nman\ne3\n113\n22.6%\n85.56637\n5.091793\n\n\nnegative\nman\ne2\n14\n2.8%\n88.57143\n6.548215\n\n\nnegative\nman\ne4\n23\n4.6%\n85.21739\n4.067125\n\n\nnegative\nwoman\ne3\n129\n25.8%\n85.07752\n5.370074\n\n\nnegative\nwoman\ne2\n12\n2.4%\n85.75000\n5.446016\n\n\nnegative\nwoman\ne4\n19\n3.8%\n83.73684\n4.628920\n\n\npositive\nman\ne3\n66\n13.2%\n79.74242\n5.384624\n\n\npositive\nman\ne2\n5\n1%\n83.00000\n3.000000\n\n\npositive\nman\ne4\n6\n1.2%\n78.50000\n3.082207\n\n\npositive\nwoman\ne3\n79\n15.8%\n81.65823\n4.784821\n\n\npositive\nwoman\ne2\n9\n1.8%\n83.22222\n3.632416\n\n\npositive\nwoman\ne4\n25\n5%\n79.44000\n5.260545\n\n\n\n\n\nExcept for the disease status variable, it looks like there is no difference of age in average and count in the other categorical variables. Taking into account age varialble is significantly associated with the metabolites that are significantly associated with the disease status, outcome (or AD). There is no need to conduct futher exploratory data analysis in the multivariable analysis senction."
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#eda",
    "href": "docs/projects/LLFS/eda.html#eda",
    "title": "EDA",
    "section": "",
    "text": "Code\n# raw data\nnormality_test_result&lt;-multiple_shapiro_test(all_data)%&gt;%\n    filter(column_name!='id')%&gt;%\n    group_by(type)%&gt;%\n    summarise(count=n())%&gt;%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%&gt;%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%&gt;%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n1001\n1001\n1\n\n\n\n\n\nOut of 1001 numeric variables, the variables following a normal distribution are 1001 (100%) and the ones that do not are NA (NA%).\n\n\n\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables&lt;-\n    multiple_shapiro_test(all_data)%&gt;%\n        filter(p_value&gt;0.05,column_name!='id')%&gt;%\n            dplyr::select(column_name)%&gt;%\n            pull%&gt;%sample(16)\n\nnormal_data&lt;-\n    all_data%&gt;%\n        dplyr::select(outcome,normal_variables)%&gt;%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%&gt;%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',linewidth=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution Grouped by Disease Status\")\n\n\n\n\n\n\n\n\nThere is no variable that do not follow a normal distribution.\n\n\n\n\n\n\nThrough the exploratory data analysis above, it was confirmed that all variables follow a normal distribution, and t tests were conducted to select metabolites that have significant relationships with the disease status, AD. To minimize type 1 error due to multiple testings, bonferroni correction was used in the EDA\n\nHomoscedasticity Test\n\nLeven’s test is performed to confirm that each variable has equality of variance, one of the assumptions of the t test and ANOVA.\n\n\nCode\nleven_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")%&gt;%\n    filter(column_name!='id')\n\nhomo_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&gt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\nhetero_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&lt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\n\n\n10 variables randomly selected out of 1001 variables with equal variance: meta408, meta959, meta796, meta957, meta740, meta212, meta171, meta769, meta986, meta178\n0 variables randomly selected with equal variance:\n\n\n\nCode\nhomo_variable_sample&lt;-homo_variable%&gt;%sample(10)\nhetero_variable_sample&lt;-hetero_variable\n\nstratified_levene_data&lt;-all_data%&gt;%\n    dplyr::select(outcome,homo_variable_sample,hetero_variable_sample)%&gt;%\n    gather(key=metabolite,value=value,c(homo_variable_sample,hetero_variable_sample))%&gt;%\n    mutate(levene_test=ifelse(metabolite%in%(homo_variable_sample),\"homoscedasticity\",\"heteroscedasticity\"))\n\nstratified_levene_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(stratified_levene_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    facet_wrap(.~levene_test)+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity vs Homoscedasticity\")\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\n\nCode\nt_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"student\",\n                        categorical_variable=\"outcome\",\n                        homo_variables=homo_variable,\n                        hetero_variables=hetero_variable)\n\nmetabolites_associated_AD&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nmetabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%&gt;%\n    group_by(outcome)%&gt;%\n    summarise(mean=mean(value),sd=sd(value))%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nmean\nsd\n\n\n\n\nnegative\n0.2184112\n0.9869302\n\n\npositive\n-0.4186542\n0.9830175\n\n\n\n\n\nCode\ntop_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%head(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\ntop_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,top_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\nbottom_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%tail(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nbottom_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,bottom_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,bottom_metabolites_associated_AD)        \n\na1&lt;-top_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(top_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Strongly Associated with AD\")\n\na2&lt;-bottom_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(bottom_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Weakly Associated with AD\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\nCode\nsignificant_metabolites&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\n\nAs a result of the t tests, there are 201 metabolites that are significantly associated with AD status at the 5% significance level. Metabolites with the highest significance were designated as strong metabolites and metabolites with the lowest significance among metabolites significantly related to AD status were designated as weak metabolites, and the expression level of metabolites between the disease status was confirmed through visualization. As a result, a greater difference was observed in strong metabolites than in weak metabolites, but both groups were not clearly separated in terms of AD status.\n\n\n\n\n\nCode\nad_sex_summary&lt;-all_data%&gt;%\n    group_by(outcome,sex)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, sex,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$sex)$p.value,\n    method=\"chisquare_test\")\n\nad_sex_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\nsex\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\nman\n150\n30%\n85.79333\n5.146572\n0.1049983\nchisquare_test\n\n\nnegative\nwoman\n160\n32%\n84.96875\n5.284868\n0.1049983\nchisquare_test\n\n\npositive\nman\n77\n15.4%\n79.85714\n5.167495\n0.1049983\nchisquare_test\n\n\npositive\nwoman\n113\n22.6%\n81.29204\n4.896576\n0.1049983\nchisquare_test\n\n\n\n\n\n\n\n\n\n\nCode\nad_genotype_summary&lt;-all_data%&gt;%\n    group_by(outcome,genotype)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, genotype,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\")\n\ngenotype_ad_summary&lt;-all_data%&gt;%\n    group_by(genotype,outcome)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(genotype,outcome,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\") \n\nad_genotype_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\ngenotype\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\ne3\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\nnegative\ne2\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\nnegative\ne4\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\npositive\ne3\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\npositive\ne2\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\npositive\ne4\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nCode\ngenotype_ad_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenotype\noutcome\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\ne3\nnegative\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\ne3\npositive\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\ne2\nnegative\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\ne2\npositive\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\ne4\nnegative\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\ne4\npositive\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nAs you see the tables above, the count of AD status and that of genotype status are proportionately the same. Thus, there is no relation between AD and genotype in this data at the significant level 5%.\n\n\n\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually. For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nad_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\n\n\nCode\nage_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n310\n62%\n85.37\n5.23\n72\n69.625\n81.25\n85\n89\n100.625\n105\n\n\noutcome\npositive\nage\n190\n38%\n80.71\n5.04\n65\n66.500\n77.00\n81\n84\n94.500\n93\n\n\nsex\nman\nage\n227\n45.4%\n83.78\n5.86\n65\n68.000\n80.00\n84\n88\n100.000\n105\n\n\nsex\nwoman\nage\n273\n54.6%\n83.45\n5.43\n67\n69.500\n80.00\n83\n87\n97.500\n100\n\n\ngenotype\ne3\nage\n387\n77.4%\n83.61\n5.64\n65\n69.500\n80.00\n84\n87\n97.500\n102\n\n\ngenotype\ne2\nage\n40\n8%\n85.83\n5.62\n78\n70.875\n81.75\n86\n89\n99.875\n105\n\n\ngenotype\ne4\nage\n73\n14.6%\n82.30\n5.25\n67\n68.500\n79.00\n82\n86\n96.500\n94\n\n\n\n\n\n\n\nCode\nplot&lt;- ggarrange(\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\"),\n    ncol=2, nrow=2,legend=\"bottom\")\nplot\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -4.66, but their standard deviations are 5.04 and 5.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status 4.66 years younger than those with a positive one.\n\n\nCode\na1&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=age,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(legend.position=\"top\")+\n    labs(title=\"Histogram, Age Distribution Grouped by Disease Status\")   \na2&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=outcome,y=age,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Age Distribution Grouped by Disease Status\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\n\n\nCode\nage_correlation_data&lt;-\n    main_statistical_test(in_data=all_data,in_numeric_variable = 'age',method = \"cor\")%&gt;%\n    filter(p_adjusted&lt;0.05,column_name!='id')\n\n\n151 metabolites are significantly associated with age.\n\n\n\n\n\nCode\ngenotype_aov&lt;-main_statistical_test(in_data=all_data,categorical_variable=\"genotype\",method='aov')\ngenotype_aov%&gt;%\nfilter(column_name!='id',p_adjusted&lt;0.05)%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ngroup_df\nresidual_df\ngroup_ssq\nresidual_ssq\ngroup_msq\nresidual_msq\nF_value\np_value\nmethod\np_adjusted\ntype\n\n\n\n\n\n\n\n0 metabolites are significantly associated with the genotype variable.\n\n\n\n\n\n\n\n\nCode\nall_data%&gt;%\n    group_by(outcome,sex,genotype)%&gt;%\n    summarise(count=n(),\n    mean_age=mean(age),\n    sd_age=sd(age))%&gt;%ungroup%&gt;%\n    mutate(sum_count=sum(count),\n    proportion=paste0(round(count/sum_count*100,3),'%'))%&gt;%\n    dplyr::select(outcome,sex,genotype,count,proportion, mean_age,sd_age)%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\ngenotype\ncount\nproportion\nmean_age\nsd_age\n\n\n\n\nnegative\nman\ne3\n113\n22.6%\n85.56637\n5.091793\n\n\nnegative\nman\ne2\n14\n2.8%\n88.57143\n6.548215\n\n\nnegative\nman\ne4\n23\n4.6%\n85.21739\n4.067125\n\n\nnegative\nwoman\ne3\n129\n25.8%\n85.07752\n5.370074\n\n\nnegative\nwoman\ne2\n12\n2.4%\n85.75000\n5.446016\n\n\nnegative\nwoman\ne4\n19\n3.8%\n83.73684\n4.628920\n\n\npositive\nman\ne3\n66\n13.2%\n79.74242\n5.384624\n\n\npositive\nman\ne2\n5\n1%\n83.00000\n3.000000\n\n\npositive\nman\ne4\n6\n1.2%\n78.50000\n3.082207\n\n\npositive\nwoman\ne3\n79\n15.8%\n81.65823\n4.784821\n\n\npositive\nwoman\ne2\n9\n1.8%\n83.22222\n3.632416\n\n\npositive\nwoman\ne4\n25\n5%\n79.44000\n5.260545\n\n\n\n\n\nExcept for the disease status variable, it looks like there is no difference of age in average and count in the other categorical variables. Taking into account age varialble is significantly associated with the metabolites that are significantly associated with the disease status, outcome (or AD). There is no need to conduct futher exploratory data analysis in the multivariable analysis senction."
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#eda-1",
    "href": "docs/projects/LLFS/eda.html#eda-1",
    "title": "EDA",
    "section": "",
    "text": "Code\n# raw data\nnormality_test_result&lt;-multiple_shapiro_test(all_data)%&gt;%\n    filter(column_name!='id')%&gt;%\n    group_by(type)%&gt;%\n    summarise(count=n())%&gt;%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%&gt;%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%&gt;%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n1001\n1001\n1\n\n\n\n\n\nOut of 1001 numeric variables, the variables following a normal distribution are 1001 (100%) and the ones that do not are NA (NA%).\n\n\n\n\n\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables&lt;-\n    multiple_shapiro_test(all_data)%&gt;%\n        filter(p_value&gt;0.05,column_name!='id')%&gt;%\n            dplyr::select(column_name)%&gt;%\n            pull%&gt;%sample(16)\n\nnormal_data&lt;-\n    all_data%&gt;%\n        dplyr::select(outcome,normal_variables)%&gt;%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%&gt;%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',linewidth=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\nCode\nnormal_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution Grouped by Disease Status\")\n\n\n\n\n\n\n\n\nThere is no variable that do not follow a normal distribution.\n\n\n\n\n\n\nThrough the exploratory data analysis above, it was confirmed that all variables follow a normal distribution, and t tests were conducted to select metabolites that have significant relationships with the disease status, AD. To minimize type 1 error due to multiple testings, bonferroni correction was used in the EDA\n\nHomoscedasticity Test\n\nLeven’s test is performed to confirm that each variable has equality of variance, one of the assumptions of the t test and ANOVA.\n\n\nCode\nleven_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")%&gt;%\n    filter(column_name!='id')\n\nhomo_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&gt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\nhetero_variable&lt;-leven_test_result%&gt;%\nfilter(p_adjusted&lt;0.05)%&gt;%\ndplyr::select(column_name)%&gt;%\npull()%&gt;%\nunique()\n\n\n\n10 variables randomly selected out of 1001 variables with equal variance: meta408, meta959, meta796, meta957, meta740, meta212, meta171, meta769, meta986, meta178\n0 variables randomly selected with equal variance:\n\n\n\nCode\nhomo_variable_sample&lt;-homo_variable%&gt;%sample(10)\nhetero_variable_sample&lt;-hetero_variable\n\nstratified_levene_data&lt;-all_data%&gt;%\n    dplyr::select(outcome,homo_variable_sample,hetero_variable_sample)%&gt;%\n    gather(key=metabolite,value=value,c(homo_variable_sample,hetero_variable_sample))%&gt;%\n    mutate(levene_test=ifelse(metabolite%in%(homo_variable_sample),\"homoscedasticity\",\"heteroscedasticity\"))\n\nstratified_levene_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(stratified_levene_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    facet_wrap(.~levene_test)+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity vs Homoscedasticity\")\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\n\nCode\nt_test_result&lt;-\n    main_statistical_test(in_data=all_data,method=\"student\",\n                        categorical_variable=\"outcome\",\n                        homo_variables=homo_variable,\n                        hetero_variables=hetero_variable)\n\nmetabolites_associated_AD&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nmetabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%&gt;%\n    group_by(outcome)%&gt;%\n    summarise(mean=mean(value),sd=sd(value))%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nmean\nsd\n\n\n\n\nnegative\n0.2184112\n0.9869302\n\n\npositive\n-0.4186542\n0.9830175\n\n\n\n\n\nCode\ntop_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%head(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\ntop_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,top_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\nbottom_metabolites_associated_AD&lt;-t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    arrange(p_adjusted)%&gt;%tail(10)%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n\nbottom_metabolites_associated_AD_data&lt;-\n    all_data%&gt;%\n    dplyr::select(outcome,bottom_metabolites_associated_AD)%&gt;%\n    gather(key=metabolite,value=value,bottom_metabolites_associated_AD)        \n\na1&lt;-top_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(top_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Strongly Associated with AD\")\n\na2&lt;-bottom_metabolites_associated_AD_data%&gt;%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(bottom_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Weakly Associated with AD\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\nCode\nsignificant_metabolites&lt;-\n    t_test_result%&gt;%\n    filter(type==\"significant\"&column_name!='age')%&gt;%\n    dplyr::select(column_name)%&gt;%pull\n# significant_metabolites%&gt;%write_rds(.,file='./docs/data/llfs_fake_significant_metabolites.rds')\n\n\nAs a result of the t tests, there are 201 metabolites that are significantly associated with AD status. Metabolites with the highest significance were designated as strong metabolites and metabolites with the lowest significance among metabolites significantly related to AD status were designated as weak metabolites, and the expression level of metabolites between the disease status was confirmed through visualization. As a result, a greater difference was observed in strong metabolites than in weak metabolites, but both groups were not clearly separated in terms of AD status.\n\n\n\n\n\nCode\nad_sex_summary&lt;-all_data%&gt;%\n    group_by(outcome,sex)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, sex,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$sex)$p.value,\n    method=\"chisquare_test\")\n\nad_sex_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\nsex\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\nman\n150\n30%\n85.79333\n5.146572\n0.1049983\nchisquare_test\n\n\nnegative\nwoman\n160\n32%\n84.96875\n5.284868\n0.1049983\nchisquare_test\n\n\npositive\nman\n77\n15.4%\n79.85714\n5.167495\n0.1049983\nchisquare_test\n\n\npositive\nwoman\n113\n22.6%\n81.29204\n4.896576\n0.1049983\nchisquare_test\n\n\n\n\n\n\n\n\n\n\nCode\nad_genotype_summary&lt;-all_data%&gt;%\n    group_by(outcome,genotype)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(outcome, genotype,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\")\n\ngenotype_ad_summary&lt;-all_data%&gt;%\n    group_by(genotype,outcome)%&gt;%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%&gt;%\n            ungroup%&gt;%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%&gt;%\n    dplyr::select(genotype,outcome,count,proportion,mean_age,sd_age)%&gt;%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\") \n\nad_genotype_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\ngenotype\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\ne3\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\nnegative\ne2\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\nnegative\ne4\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\npositive\ne3\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\npositive\ne2\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\npositive\ne4\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nCode\ngenotype_ad_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenotype\noutcome\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\ne3\nnegative\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\ne3\npositive\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\ne2\nnegative\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\ne2\npositive\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\ne4\nnegative\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\ne4\npositive\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nAs you see the tables above, the count of AD status and that of genotype status are proportionately the same. Thus, there is no relation between AD and genotype in this data at the significant level 5%.\n\n\n\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually. For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nad_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\n\n\nCode\nage_summary%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n310\n62%\n85.37\n5.23\n72\n69.625\n81.25\n85\n89\n100.625\n105\n\n\noutcome\npositive\nage\n190\n38%\n80.71\n5.04\n65\n66.500\n77.00\n81\n84\n94.500\n93\n\n\nsex\nman\nage\n227\n45.4%\n83.78\n5.86\n65\n68.000\n80.00\n84\n88\n100.000\n105\n\n\nsex\nwoman\nage\n273\n54.6%\n83.45\n5.43\n67\n69.500\n80.00\n83\n87\n97.500\n100\n\n\ngenotype\ne3\nage\n387\n77.4%\n83.61\n5.64\n65\n69.500\n80.00\n84\n87\n97.500\n102\n\n\ngenotype\ne2\nage\n40\n8%\n85.83\n5.62\n78\n70.875\n81.75\n86\n89\n99.875\n105\n\n\ngenotype\ne4\nage\n73\n14.6%\n82.30\n5.25\n67\n68.500\n79.00\n82\n86\n96.500\n94\n\n\n\n\n\n\n\nCode\nplot&lt;- ggarrange(\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\"),\n    ncol=2, nrow=2,legend=\"bottom\")\nplot\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -4.66, but their standard deviations are 5.04 and 5.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status 4.66 years younger than those with a positive one.\n\n\nCode\na1&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=age,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(legend.position=\"top\")+\n    labs(title=\"Histogram, Age Distribution Grouped by Disease Status\")   \na2&lt;-all_data%&gt;%\n    dplyr::select(outcome,age)%&gt;%\n    ggplot(aes(x=outcome,y=age,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Age Distribution Grouped by Disease Status\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\n\n\nCode\nage_correlation_data&lt;-\n    main_statistical_test(in_data=all_data,in_numeric_variable = 'age',method = \"cor\")%&gt;%\n    filter(p_adjusted&lt;0.05,column_name!='id')\n\n\n151 metabolites are significantly associated with age.\n\n\n\n\n\nCode\ngenotype_aov&lt;-main_statistical_test(in_data=all_data,categorical_variable=\"genotype\",method='aov')\ngenotype_aov%&gt;%\nfilter(column_name!='id',p_adjusted&lt;0.05)%&gt;%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ngroup_df\nresidual_df\ngroup_ssq\nresidual_ssq\ngroup_msq\nresidual_msq\nF_value\np_value\nmethod\np_adjusted\ntype\n\n\n\n\n\n\n\n0 metabolites are significantly associated with the genotype variable.\n\n\n\n\n\n\n\n\nCode\nall_data%&gt;%\n    group_by(outcome,sex,genotype)%&gt;%\n    summarise(count=n(),\n    mean_age=mean(age),\n    sd_age=sd(age))%&gt;%ungroup%&gt;%\n    mutate(sum_count=sum(count),\n    proportion=paste0(round(count/sum_count*100,3),'%'))%&gt;%\n    dplyr::select(outcome,sex,genotype,count,proportion, mean_age,sd_age)%&gt;%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\ngenotype\ncount\nproportion\nmean_age\nsd_age\n\n\n\n\nnegative\nman\ne3\n113\n22.6%\n85.56637\n5.091793\n\n\nnegative\nman\ne2\n14\n2.8%\n88.57143\n6.548215\n\n\nnegative\nman\ne4\n23\n4.6%\n85.21739\n4.067125\n\n\nnegative\nwoman\ne3\n129\n25.8%\n85.07752\n5.370074\n\n\nnegative\nwoman\ne2\n12\n2.4%\n85.75000\n5.446016\n\n\nnegative\nwoman\ne4\n19\n3.8%\n83.73684\n4.628920\n\n\npositive\nman\ne3\n66\n13.2%\n79.74242\n5.384624\n\n\npositive\nman\ne2\n5\n1%\n83.00000\n3.000000\n\n\npositive\nman\ne4\n6\n1.2%\n78.50000\n3.082207\n\n\npositive\nwoman\ne3\n79\n15.8%\n81.65823\n4.784821\n\n\npositive\nwoman\ne2\n9\n1.8%\n83.22222\n3.632416\n\n\npositive\nwoman\ne4\n25\n5%\n79.44000\n5.260545\n\n\n\n\n\nExcept for the disease status variable, it looks like there is no difference of age in average and count in the other categorical variables. Taking into account age varialble is significantly associated with the metabolites that are significantly associated with the disease status, outcome (or AD). There is no need to conduct futher exploratory data analysis in the multivariable analysis senction."
  },
  {
    "objectID": "docs/projects/LLFS/ml_approach.html",
    "href": "docs/projects/LLFS/ml_approach.html",
    "title": "ML Approach",
    "section": "",
    "text": "alpha      mse fit.name\n1    0.0 2217.331   alpha0\n2    0.1 2217.331 alpha0.1\n3    0.2 2217.331 alpha0.2\n4    0.3 2217.331 alpha0.3\n5    0.4 2217.331 alpha0.4\n6    0.5 2217.331 alpha0.5\n7    0.6 2217.331 alpha0.6\n8    0.7 2217.331 alpha0.7\n9    0.8 2217.331 alpha0.8\n10   0.9 2217.331 alpha0.9\n11   1.0 2217.331   alpha1"
  },
  {
    "objectID": "docs/projects/LLFS/statistical_approach.html",
    "href": "docs/projects/LLFS/statistical_approach.html",
    "title": "Statistical Approach",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 Go to Project Content List\nProject Content List\n\n\n2 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "README copy.html",
    "href": "README copy.html",
    "title": "website",
    "section": "",
    "text": "website"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\nFigure 1"
  },
  {
    "objectID": "about.html#enthusiastic-data-scientist",
    "href": "about.html#enthusiastic-data-scientist",
    "title": "Kwangmin Kim",
    "section": "Enthusiastic Data Scientist",
    "text": "Enthusiastic Data Scientist\n\nInterests\nData Modeling, Statistics, Machine Learning, Deep Learning, Optimization"
  },
  {
    "objectID": "docs/blog/posts/content_list.html",
    "href": "docs/blog/posts/content_list.html",
    "title": "Blog Content List",
    "section": "",
    "text": "Note\n\n\n\n\nScalars are denoted with a lower-case letter (ex a ) or a non-bolded lower-case Greek letter (ex \\(\\alpha\\) ).\nVectors are denoted using a bold-faced lower-case letter (ex \\(\\mathbf a\\)).\nMatrices are denoted using a bold-faced upper-case letter (ex \\(\\mathbf A\\), \\(\\mathbf \\phi\\)) or a bold-faced upper-case Greek letter (ex \\(\\mathbf \\Phi\\)).\nTensors are denoted using a bold-faced upper-case letter with multiple subscripts or superscripts, indicating the number of indices and the dimensions of the tensor along each axis.\n\nA second-order tensor (also known as a matrix) \\(\\mathbf A\\) with dimensions \\(n \\times m\\) can be represented as: \\(\\mathbf A_{ij}\\) where \\(i = 1,\\dots,m\\) and \\(j = 1,\\dots,n\\), which are the indices that run over the rows and columns of the matrix, respectively.\nA third-order tensor \\(T\\) with dimensions \\(n \\times m \\times p\\) can be represented as: \\(\\mathbf A_{ijk}\\) where \\(i = 1,\\dots,m\\), \\(j = 1,\\dots,n\\), which are \\(i\\), and \\(k = 1,\\dots,p\\) \\(j\\), and \\(k\\), which are the indices that run over the three dimensions of the tensor.\n\n\n\n\n\nContents\n\nEngineering\nSurveilance\n\n\n\nReference\n\nStatistics\n\nGeorge Casella & Rogeer L. Berger - Statistcal Inference, 2nd Edition\nDobson and Barnett (2008) An Introduction to Generalized Linear Model. 3rd Ed. Chapman & Hall.\nFitzmaurice, Laird and Ware (2011) Applied Longitudinal Analysis. 2nd Ed. Wiley.\nHosmer, Lemeshow and May (2008) Applied Survival Analysis. 2nd Ed. Wiley.\n슬기로운 통계생활 - https://www.youtube.com/@statisticsplaybook\n슬기로운 통계생활 - https://github.com/statisticsplaybook\nFast Campus, Coursera, Inflearn\n그 외 다수의 Youtube, and Documents from Googling\n\nMathematics\n\nJames Stewart - Calculus Early Transcedentals, 7th Eidition & any James Stewart series\nGILBERT STRANG - Introduction to Linear Algebra, 4th Edition.\n임장환 - 머신러닝, 인공지능, 컴퓨터 비전 전공자를 위한 최적화 이론\nFast Campus, Coursera, Inflearn\n8일간의 선형대수학 기초(이상준 경희대 교수)\nLinear Algebra(Prof. Gilbert Strang, MIT Open Courseware)\nComputational Linear Algebra for Coders\nImmersive linear Algebra\n3blue1brown\n그 외 다수의 Youtube, and Documents from Googling\n\nMachine Learning\n\nGareth M. James, Daniela Witten, Trevor Hastie, Robert Tibshirani - An Introduction to Statistical Learning: With Applications in R 2nd Edition\nTrevor Hastie, Robert Tibshirani, Jerome H. Friedman - The Elements of Statistical Learning 2nd Edition\nFast Campus, Coursera, Inflearn\n그 외 다수의 Youtube, and Documents from Googling\n\nDeep Learning\n\nSaito Koki - Deep Learning from Scratch 1,2,3 (밑바닥부터 시작하는 딥러닝 1,2,3)\n조준우 - 머신러닝·딥러닝에 필요한 기초 수학 with 파이썬\n조준우 - https://github.com/metamath1/noviceml\n동빈나 - https://www.youtube.com/c/dongbinna\n혁펜하임 - https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag\nFast Campus, Coursera, Inflearn\n다수의 Youtube, and Documents from Googling\n\nEngineering\n\nFast Campus, Coursera, Inflearn\n그 외 다수의 Youtube, and Documents from Googling"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html",
    "title": "Content List, Mathematics",
    "section": "",
    "text": "2023-03-24, Variable types\n1111-11-11, Function\n\n2023-01-31, Function (1) - Univariable Scalar Function (One to One)\n2023-01-31, Function (2) - Multi-variable Scalar Function (Many to One)\n2023-01-31, Function (3) - Univariable Vector Function (One to Many)\n2023-01-31, Function (4) - Multi-variable Vector Function (Many to Many)\n2023-02-18, Function (5) - Composite Function\n\n2023-02-18, Transformations of Functions\n1111-11-11, Vector & Matrix\n2023-03-15, Limit, \\(\\epsilon-\\delta\\) Method\nDifferentiation\n\n2023-02-04, Derivative (1) - Univariable Scalar Funtion\n1111-11-11, Derivative (2) - Chain Rule & Partial Derivative\n1111-11-11, Derivative (3) - Higher Order Derivative\n1111-11-11, Derivative (4) - Mean Value Theorem\n1111-11-11, Derivative (5) - Gradient\n\n2023-03-15, Talyer’s Series\n1111-11-11, Gradient Direction\n1111-11-11, Random Variable\n1111-11-11, Probability Distribution\n1111-11-11, Information Theory - Entropy\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n\n\n\n\n\n2023-03-30, Basic Vector(1) - Vector Operations\n2023-03-30, Basic Vector(2) - Vector Norm and Dot Product”\n2023-03-30, Basic Vector(3) - Vector Equation\n2023-03-30, Basic Matrix(1) - Matrix Operation\n2023-03-30, Basic Matrix(2) - Matrix Multiplication\n2023-03-30, Basic Matrix(3) - System of Linear Equations\n2023-03-30, Basic Matrix(4) - Special Matrix\n2023-04-14, [Lineqr Equations]\n2023-04-14, [Vector Space and Subspaces]\n2023-04-21, [Orthogonality]\n1111-11-11, [Determinants]\n1111-11-11, [Eigen Value & Eigen Vector]\n1111-11-11, [Linear Transformations]\n1111-11-11, Basis, Dimension, & Rank\n1111-11-11,\n1111-11-11, Eigen Decomposition\n1111-11-11, Singular Value Decomposition (SVD)\n1111-11-11, Group\n1111-11-11, Rotation & Group\n2023-04-02, Matrix Transformation (5) - Quadratic Form\n2023-04-02, Matrix Calculus (1) - Quadratic Form\n1111-11-11,\n1111-11-11,\n1111-11-11,\n\n\n\n\n\n2023-03-23, Minimizer & Minimum\n1111-11-11, Convex Set\n1111-11-11, Convex Function\n1111-11-11, Unconstrained Optimization\n1111-11-11, Non-linear Least Square\n1111-11-11, Largrange Multiplier Method\n\n1111-11-11, Largrange Primal Function\n1111-11-11, Largrange Dual Function\n1111-11-11, KKT conditions\n\n1111-11-11, Gradient Descent Optimizers\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#basic",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#basic",
    "title": "Content List, Mathematics",
    "section": "",
    "text": "2023-03-24, Variable types\n1111-11-11, Function\n\n2023-01-31, Function (1) - Univariable Scalar Function (One to One)\n2023-01-31, Function (2) - Multi-variable Scalar Function (Many to One)\n2023-01-31, Function (3) - Univariable Vector Function (One to Many)\n2023-01-31, Function (4) - Multi-variable Vector Function (Many to Many)\n2023-02-18, Function (5) - Composite Function\n\n2023-02-18, Transformations of Functions\n1111-11-11, Vector & Matrix\n2023-03-15, Limit, \\(\\epsilon-\\delta\\) Method\nDifferentiation\n\n2023-02-04, Derivative (1) - Univariable Scalar Funtion\n1111-11-11, Derivative (2) - Chain Rule & Partial Derivative\n1111-11-11, Derivative (3) - Higher Order Derivative\n1111-11-11, Derivative (4) - Mean Value Theorem\n1111-11-11, Derivative (5) - Gradient\n\n2023-03-15, Talyer’s Series\n1111-11-11, Gradient Direction\n1111-11-11, Random Variable\n1111-11-11, Probability Distribution\n1111-11-11, Information Theory - Entropy\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#linear-algebra",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#linear-algebra",
    "title": "Content List, Mathematics",
    "section": "",
    "text": "2023-03-30, Basic Vector(1) - Vector Operations\n2023-03-30, Basic Vector(2) - Vector Norm and Dot Product”\n2023-03-30, Basic Vector(3) - Vector Equation\n2023-03-30, Basic Matrix(1) - Matrix Operation\n2023-03-30, Basic Matrix(2) - Matrix Multiplication\n2023-03-30, Basic Matrix(3) - System of Linear Equations\n2023-03-30, Basic Matrix(4) - Special Matrix\n2023-04-14, [Lineqr Equations]\n2023-04-14, [Vector Space and Subspaces]\n2023-04-21, [Orthogonality]\n1111-11-11, [Determinants]\n1111-11-11, [Eigen Value & Eigen Vector]\n1111-11-11, [Linear Transformations]\n1111-11-11, Basis, Dimension, & Rank\n1111-11-11,\n1111-11-11, Eigen Decomposition\n1111-11-11, Singular Value Decomposition (SVD)\n1111-11-11, Group\n1111-11-11, Rotation & Group\n2023-04-02, Matrix Transformation (5) - Quadratic Form\n2023-04-02, Matrix Calculus (1) - Quadratic Form\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#optimization",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#optimization",
    "title": "Content List, Mathematics",
    "section": "",
    "text": "2023-03-23, Minimizer & Minimum\n1111-11-11, Convex Set\n1111-11-11, Convex Function\n1111-11-11, Unconstrained Optimization\n1111-11-11, Non-linear Least Square\n1111-11-11, Largrange Multiplier Method\n\n1111-11-11, Largrange Primal Function\n1111-11-11, Largrange Dual Function\n1111-11-11, KKT conditions\n\n1111-11-11, Gradient Descent Optimizers\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html",
    "href": "docs/blog/posts/Engineering/guide_map/index.html",
    "title": "Content List, Engineering",
    "section": "",
    "text": "0000-00-00, Terminology"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#it-terminology",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#it-terminology",
    "title": "Content List, Engineering",
    "section": "",
    "text": "0000-00-00, Terminology"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#data-structure",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#data-structure",
    "title": "Content List, Engineering",
    "section": "2 Data Structure",
    "text": "2 Data Structure\n\n2023-01-17, Overview\n2023-01-18, Array\n2023-01-18, Linked List\n2023-01-18, Python List\n2023-01-19, Stack\n2023-01-19, Queue\n2023-01-26, Deque\n2023-01-26, Binary Search Tree\n2023-01-20, Priority Queue\n2023-01-20, Graph"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#development-environment-setting",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#development-environment-setting",
    "title": "Content List, Engineering",
    "section": "3 Development Environment Setting",
    "text": "3 Development Environment Setting\n\n3.1 WSL2\n\n2023-05-01, Introduction & Installation\n2023-05-01, Frequently Used Linux Command\n\n\n\n3.2 Docker\n\n2023-05-01, Introduction & Installation\n\n\n\n3.3 VS code\n\n2023-05-01, Introduction & Installation\n\n\n\n3.4 Conda\n\n2023-05-01, Introduction & Installation\n\n\n\n3.5 Git\n\n2023-05-01, Introduction & Installation"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#documentation",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#documentation",
    "title": "Content List, Engineering",
    "section": "4 Documentation",
    "text": "4 Documentation\n\n4.1 Dynamic Documentation\n\n2023-01-19, Quarto\n2023-01-19, xaringan[R]\n2023-01-19, Bookdown[R]\n2023-01-19, DISTL\n2023-01-26, Sphinx[Python]\n\n\n\n4.2 Diagrams\n\n2023-05-01, Quarto & Diagrams\n2023-05-01, Graphiz Gallery\n2023-05-01, Mermaid Gallery"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#aws-cloud",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#aws-cloud",
    "title": "Content List, Engineering",
    "section": "5 AWS Cloud",
    "text": "5 AWS Cloud\nCoursera Course: AWS Fundamentals\n\n2023-03-09, Computing and Networking\n2023-03-12, Storage and Database\n2023-03-26, Monitoring and SharedResponsibility\n2023-04-05, Infrastructure Security"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#azure-cloud",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#azure-cloud",
    "title": "Content List, Engineering",
    "section": "6 Azure Cloud",
    "text": "6 Azure Cloud"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#gcp",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#gcp",
    "title": "Content List, Engineering",
    "section": "7 GCP",
    "text": "7 GCP"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#apache-airflow",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#apache-airflow",
    "title": "Content List, Engineering",
    "section": "10 Apache Airflow",
    "text": "10 Apache Airflow\n\n2023-05-01, Introduction\n2023-05-01, Airflow Environment Setting\n2023-05-01, Operator Basics\n2023-05-01, Python Operators\n2023-05-01, Template Variable\n2023-05-01, Data Share\n2023-05-01, Task Handling - Advanced\n2023-05-01, More Operators\n2023-05-10, Connection & Hook\n2023-05-10, Sensor\n2023-05-10, More Airflow Functions\n2023-05-10, Operate WebApp Using Rshiny"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#apache-spark",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#apache-spark",
    "title": "Content List, Engineering",
    "section": "11 Apache Spark",
    "text": "11 Apache Spark"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#data-modeling",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#data-modeling",
    "title": "Content List, Engineering",
    "section": "12 Data Modeling",
    "text": "12 Data Modeling"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#front-end",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#front-end",
    "title": "Content List, Engineering",
    "section": "13 Front End",
    "text": "13 Front End"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#back-end",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#back-end",
    "title": "Content List, Engineering",
    "section": "14 Back End",
    "text": "14 Back End"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/01.intro.html",
    "href": "docs/blog/posts/Engineering/airflow/01.intro.html",
    "title": "Airflow Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n파이썬을 이용해 워크플로우를 만들고 관리할 수 있는 오픈소스 기반 워크플로우 관리 도구\n2014년 에어비앤비에서 만든 워크플로우 관리 솔루션으로 현재는 Apache Open Source 재단에서 관리되고 있는 프로젝트\nAirflow는 워크플로우를 DAG을 사용하여 정의하고, 관리하는 프로그램\n\n자유도가 크고, 확장성이 좋은 워크플로우 관리 프로그램\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nREST API를 사용한 데이터를 받아 전처리\n\n REST API를 사용한 데이터를 받아 전처리   \n\n기존 DB의 데이터 삭제 (중복 제거)\n\n 기존 DB의 데이터 삭제 (중복 제거)   \n\nREST API를 사용한 데이터를 받아 전처리–기존 DB의 데이터 삭제 (중복 제거)\n\n   \n\n전처리한 데이터를 DB에 삽입\n\n 전처리한 데이터를 DB에 삽입   \n\n기존 DB의 데이터 삭제 (중복 제거)–전처리한 데이터를 DB에 삽입\n\n  \n\n\nFigure 1: Airflow Workflow Simple Example\n\n\n\n\n\n파이썬으로 제작된 도구이며 이용자가 워크플로우 생성시에도 파이썬으로 구현해야 함\n하나의 워크플로우는 DAG(Directed Acyclic Graph) 이라 부르며 DAG 안에는 1개 이상의 Task가 존재\n\n예를 들어, REST API로부터 데이터를 내려 받아 DB에 insert하려는 과제를 수행하기 위해 Figure 1 와 같은 단계들이 필요하다.\n각 각의 단계를 task라 하고 각 각 선/후행 단계가 있다. (1번 task \\(\\rightarrow\\) 2번 task \\(\\rightarrow\\) 3번 task)\n이 tasks의 집합을 DAG이라고 한다.\nTask간 선후행 연결이 가능하되 순환되지 않고 방향성을 가짐(=DAG)\n\nCron 기반의 스케줄링\n\nLinux에서 사용되는 스케쥴링으로 task들이 시작되어야 하는 시작 시간이나 주기를 설정\n\n모니터링 및 실패 작업에 대한 재실행 기능이 간편\n\n\n\n\nFigure 2: DAG Simple Example\n\n\n\nIn Figure 2, 초록색 테두리의 node는 성공한 task를 의미하고 분홍색 테두리의 node는 실패한 task를 의미한다. 위의 그림에는 없지만 회색 테두리는 queue (준비) 상태를 의미한다. Airflow에는 DAG이 일련의 task로 구성되어 있기 때문에 실행 상태도 성공, 실패 및 준비 상태같은 여러 종류가 있다. 나머지 상태는 뒷 부분에서 차차 다뤄보기로 한다.\n\n\n\n\nTask Status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Orchestrating Task Tools (a.k.a Data Workflows) Demand Comparison\n\n\n\n\n\n\n\n\n\n(b) Orchestrating Task Tools Computation Comparison\n\n\n\n\nFigure 3: Airflow Figure Reference: Airflow vs. Luigi vs. Argo vs. MLFlow vs. KubeFlow by Markus Schmitt\n\n\n\nMaturity: 성숙도로서 github에서 얼마나 많은 사람들이 관리하고 있는지 즉 community의 활성도를 나타내는 지표. 유료 서비스의 경우 user 가 제작사에게 패치를 요청할 수 있지만 open source에 경우 제작사가 없어 community의 활성도가 중요하다.\nPopularity: github starts의 개수\nSimplicity: workflow를 얼마나 쉽게 사용할 수 있는지에 대한 난이도로 airflow는 사용하기에 어려운 난이도를 보여준다.\nBreadth: 확장성. 즉 얼마나 customizing할 수 있는지 보여주는 척도\n\nFigure 3 을 보면, Figure 3 (a) 에서 볼수 있듯이 최근 들어 airflow의 인기가 급증하는 것을 볼 수 있다. Figure 3 (b) 에서 그 이유를 짐작할 수 있는데 airflow가 simplicity가 C 사용하기는 어렵지만 구현할 수 있는 폭 breadth가 높다 (Breadth: A). 즉, 다른 workflows에 비해 복잡한 코딩을 요구하는 만큼 그 자유도가 높다는 것을 짐작할 수 있다.\n\n\n\n\n\n파이썬에 익숙하다면 러닝 커브 빠르게 극복 가능\n대규모 워크플로우 환경에서 부하 증가시 수평적 확장 가능한 Kubenetes 등 아키텍처 지원\n파이썬에서 지원되는 라이브러리 활용하여 다양한 도구 컨트롤 가능 (GCP, AWS등 대다수 클라우드에서 제공하는 서비스)\n\nGCP: Google Cloud Platform\nAWS: Amazon Web Services\n\nAirflow에서 제공하는 파이썬 소스 기반으로 원하는 작업을 위한 커스터마이징이 가능 (오퍼레이터, Hook, 센서 등)\n\n\n\n\n\n실시간 워크플로우 관리에 적합치 않음 (최소 분 단위 실행)\n워크플로우(DAG) 개수가 많아질 경우 모니터링이 쉽지 않음\n워크플로우를 GUI환경에서 만들지 않기에 파이썬에 익숙하지 않다면 다루기 쉽지 않음 협업 환경에서 프로그래밍 표준이 없으면 유지관리가 쉽지 않음\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/01.intro.html#characteristics",
    "href": "docs/blog/posts/Engineering/airflow/01.intro.html#characteristics",
    "title": "Airflow Introduction",
    "section": "",
    "text": "G\n\n  \n\nREST API를 사용한 데이터를 받아 전처리\n\n REST API를 사용한 데이터를 받아 전처리   \n\n기존 DB의 데이터 삭제 (중복 제거)\n\n 기존 DB의 데이터 삭제 (중복 제거)   \n\nREST API를 사용한 데이터를 받아 전처리–기존 DB의 데이터 삭제 (중복 제거)\n\n   \n\n전처리한 데이터를 DB에 삽입\n\n 전처리한 데이터를 DB에 삽입   \n\n기존 DB의 데이터 삭제 (중복 제거)–전처리한 데이터를 DB에 삽입\n\n  \n\n\nFigure 1: Airflow Workflow Simple Example\n\n\n\n\n\n파이썬으로 제작된 도구이며 이용자가 워크플로우 생성시에도 파이썬으로 구현해야 함\n하나의 워크플로우는 DAG(Directed Acyclic Graph) 이라 부르며 DAG 안에는 1개 이상의 Task가 존재\n\n예를 들어, REST API로부터 데이터를 내려 받아 DB에 insert하려는 과제를 수행하기 위해 Figure 1 와 같은 단계들이 필요하다.\n각 각의 단계를 task라 하고 각 각 선/후행 단계가 있다. (1번 task \\(\\rightarrow\\) 2번 task \\(\\rightarrow\\) 3번 task)\n이 tasks의 집합을 DAG이라고 한다.\nTask간 선후행 연결이 가능하되 순환되지 않고 방향성을 가짐(=DAG)\n\nCron 기반의 스케줄링\n\nLinux에서 사용되는 스케쥴링으로 task들이 시작되어야 하는 시작 시간이나 주기를 설정\n\n모니터링 및 실패 작업에 대한 재실행 기능이 간편\n\n\n\n\nFigure 2: DAG Simple Example\n\n\n\nIn Figure 2, 초록색 테두리의 node는 성공한 task를 의미하고 분홍색 테두리의 node는 실패한 task를 의미한다. 위의 그림에는 없지만 회색 테두리는 queue (준비) 상태를 의미한다. Airflow에는 DAG이 일련의 task로 구성되어 있기 때문에 실행 상태도 성공, 실패 및 준비 상태같은 여러 종류가 있다. 나머지 상태는 뒷 부분에서 차차 다뤄보기로 한다.\n\n\n\n\nTask Status"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/01.intro.html#motivation",
    "href": "docs/blog/posts/Engineering/airflow/01.intro.html#motivation",
    "title": "Airflow Introduction",
    "section": "",
    "text": "(a) Orchestrating Task Tools (a.k.a Data Workflows) Demand Comparison\n\n\n\n\n\n\n\n\n\n(b) Orchestrating Task Tools Computation Comparison\n\n\n\n\nFigure 3: Airflow Figure Reference: Airflow vs. Luigi vs. Argo vs. MLFlow vs. KubeFlow by Markus Schmitt\n\n\n\nMaturity: 성숙도로서 github에서 얼마나 많은 사람들이 관리하고 있는지 즉 community의 활성도를 나타내는 지표. 유료 서비스의 경우 user 가 제작사에게 패치를 요청할 수 있지만 open source에 경우 제작사가 없어 community의 활성도가 중요하다.\nPopularity: github starts의 개수\nSimplicity: workflow를 얼마나 쉽게 사용할 수 있는지에 대한 난이도로 airflow는 사용하기에 어려운 난이도를 보여준다.\nBreadth: 확장성. 즉 얼마나 customizing할 수 있는지 보여주는 척도\n\nFigure 3 을 보면, Figure 3 (a) 에서 볼수 있듯이 최근 들어 airflow의 인기가 급증하는 것을 볼 수 있다. Figure 3 (b) 에서 그 이유를 짐작할 수 있는데 airflow가 simplicity가 C 사용하기는 어렵지만 구현할 수 있는 폭 breadth가 높다 (Breadth: A). 즉, 다른 workflows에 비해 복잡한 코딩을 요구하는 만큼 그 자유도가 높다는 것을 짐작할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/01.intro.html#strength",
    "href": "docs/blog/posts/Engineering/airflow/01.intro.html#strength",
    "title": "Airflow Introduction",
    "section": "",
    "text": "파이썬에 익숙하다면 러닝 커브 빠르게 극복 가능\n대규모 워크플로우 환경에서 부하 증가시 수평적 확장 가능한 Kubenetes 등 아키텍처 지원\n파이썬에서 지원되는 라이브러리 활용하여 다양한 도구 컨트롤 가능 (GCP, AWS등 대다수 클라우드에서 제공하는 서비스)\n\nGCP: Google Cloud Platform\nAWS: Amazon Web Services\n\nAirflow에서 제공하는 파이썬 소스 기반으로 원하는 작업을 위한 커스터마이징이 가능 (오퍼레이터, Hook, 센서 등)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/01.intro.html#weakness",
    "href": "docs/blog/posts/Engineering/airflow/01.intro.html#weakness",
    "title": "Airflow Introduction",
    "section": "",
    "text": "실시간 워크플로우 관리에 적합치 않음 (최소 분 단위 실행)\n워크플로우(DAG) 개수가 많아질 경우 모니터링이 쉽지 않음\n워크플로우를 GUI환경에서 만들지 않기에 파이썬에 익숙하지 않다면 다루기 쉽지 않음 협업 환경에서 프로그래밍 표준이 없으면 유지관리가 쉽지 않음"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Docker/01.docker_install.html",
    "href": "docs/blog/posts/Engineering/Docker/01.docker_install.html",
    "title": "Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n1 Docker\n\n리눅스내 가상화 관련 커널을 활용하여 어플리케이션을 독립적 환경에서 실행시키는 기술\n가상화 서버(VM) 대비 Guest OS가 없어 경량화된 가상화 서버로 볼 수 있음\n\nVM의 hypervisor요소가 다수의 VM의 Guest OS와 App을 독립적으로 운영될 수 있도록 관리해줌\n하지만 VM은 HOST OS의 자원을 할당을 해줘야하는 overhead문제가 있음\nDocker는 Hypervisor와 Guest OS가 필요없이 Apps을 독립적으로 구동 시킬 수 있음\nDocker에서는 하나의 App을 container라 부르고 container를 경량화된 가상화 서버라고 생각하면 됨\ncontainer는 VM만큼 완전히 독립적으로 운영할 수는 없지만 overhead를 최소화한 VM이라고 생각할 수 있음\n\n\n\n\n\nDocker Compoenent Architecture\n\n\n\n\n2 Docker Installation\n\nDocker 설치 링크\n\nUninstall old versions: for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg; done 실행\nInstall using the apt repository\n\nSet up the repository\n\nrepository update: sudo apt-get update\nDocker 설치에 필요한 사전 libraries: sudo apt-get install ca-certificates curl gnupg 실행\nAdd Docker’s official GPG key\n\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\nUse the following command to set up the repository:\n\necho \\ \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n\n\nInstall Docker Engine\n\nUpdate the apt package index: sudo apt-get update\nTo install the latest version of community edition (ce), run: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-compose-plugin은 docker-compose 기능을 사용할 수 있게함\n\nVerify that the Docker Engine Installation is successful by running the hello-world image.\n\nsudo docker run hello-world : hello world image 다운로드 받음\n\ndocker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?. 라고 뜨면 docker demon이 시작이 안되어 있기 때문에 발생하는 에러창. docker demon 띄우면 됨\n\ndocker demon 실행: sudo service docker start WSL2킬때마다 실행해야줘야함\n그래도 에러창 뜨면 sudo service docker status 실행 시켜 docker demon 켜져있는지 확인\n\n\n\n\n\n\n\n\n\n\n\n\n3 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n딥러닝은 다양한 알고리즘의 조합으로 수행되기 때문에 다양한 알고리즘을 정확하게 작성하기 위해서는 다수의 다양한 자료(data)를 담기 위해 사용되는 자료 구조를 이해할 필요가 있다. 즉, 자료구조는 정확한 알고리즘을 구현하기 위해 다수의 자료(data)를 담기 위한 구조이다.\n\n딥러닝 유저들간에도 자료구조를 이해하는 것에 대한 의견이 분분하지만\n올바른 자료구조를 사용하는 것은 프로그램을 조직적으로 만들 수 있는 능력을 키울 수 있다.\n데이터의 수가 많아질수록 효율적인 자료구조가 필요하다.\n예시) 학생 수가 1,000,000명 이상인 학생 관리 프로그램\n\n매일 자료 조회가 1억번 이상 발생한다면 더 빠르게 동작하는 자료 구조를 사용해야 프로그램의 효율성을 올릴 수 있다.\n\n\n\n\n\n자료구조의 필요성에 대해서 이해할 필요가 있다.\n성능 비교: 자료구조/알고리즘의 성능 측정 방법에 대해 이해할 필요가 있다.\n\nA: 적당한 속도의 삽입 & 적당한 속도의 추출 (삽입: \\(O (log N)\\) / 추출: \\(O(log N)\\))\nB: 느린 삽입 & 빠른 추출 (삽입: \\(O (N)\\) / 추출: \\(O (1)\\))\nA vs B? 상황에 따라 A를 만들지 B를 만들지 선택해야 한다. 삽입 연산이 많으면 A를, 추출 연산이 많으면 B를 택해야 한다. (속도 비교: \\(O (N) &lt; O (log N)&lt; O (1)\\))\n하지만, 실무적으로 많은 개발자들이 A를 택한다. 왜냐면 log 복잡도는 상수 복잡도와 속도가 비슷하기 때문\n\n\n\n\n\n\n\n이처럼 상황에 맞게 알고리즘의 연산 속도를 결정해야 하므로 데이터를 효과적으로 저장하고, 처리하는 방법에 대해 바르게 이해할 필요가 있다.\n자료구조를 제대로 이해해야 불필요하게 메모리와 계산을 낭비하지 않는다.\nC언어를 기준으로 정수(int) 형식의 데이터가 100만 개가량이 존재한다고 가정하자.\n해당 프로그램을 이용하면, 내부적으로 하루에 데이터 조회가 1억 번 이상 발생한다.\n이때 원하는 데이터를 가장 빠르게 찾도록 해주는 자료구조는 무엇일까?\n\n트리(tree)와 같은 자료구조를 활용할 수 있다.\n\n\n\n\n\n\n선형 자료 구조(linear data structure) 선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 하나 존재하는 자료구조이며 데이터가 일렬로 연속적으로(순차적으로) 연결되어 있다.\n\n배열(array)\n연결 리스트(linked list)\n스택(stack)\n큐(queue)\n\n비선형 자료 구조(non-linear data structure)\n비선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 여러 개 올 수 있는 자료구조이며 데이터가 일직선상으로 연결되어 있지 않아도 된다.\n\n트리(tree)\n그래프(graph)\n\n\n\n\n\n\n효율적인 자료구조 설계를 위해 알고리즘 지식이 필요하다.\n효율적인 알고리즘을 작성하기 위해서 문제 상황에 맞는 적절한 자료구조가 사용되어야 한다.\n프로그램을 작성할 때 자료구조와 알고리즘 모두 고려해야 한다.\n\n\n\n\n\n시간 복잡도(time complexity): 알고리즘에 사용되는 연산 횟수를 측정 (시간 측정)\n공간 복잡도(space complexity): 알고리즘에 사용되는 메모리의 양을 측정 (공간 측정)\n공간을 많이 사용하는 대신 시간을 단축하는 방법이 흔히 사용된다.\n프로그램의 성능 측정 방법: Big-O 표기법\n\n복잡도를 표현할 때는 Big-O 표기법을 사용한다.\n\n특정한 알고리즘이 얼마나 효율적인지 수치적으로 표현할 수 있다.\n가장 빠르게 증가하는 항만을 고려하는 표기법이다.\n\n아래의 알고리즘은 \\(O(n)\\) 의 시간 복잡도를 가진다. 왜냐면, n에 따라 summary += i의 연산 횟수가 정해지기 때문이다.\n\n\n\n\nCode\nn = 10\nsummary = 0\nfor i in range(n):\n    summary += i\nprint(summary)\n\n\n45\n\n\n\n다음 알고리즘은 \\(O (n^2)\\) 의 시간 복잡도를 가진다. 2 중 for loop은 i와 j가 n에 따라 각 각 n 번씩 연산되기때문에 \\(n \\times n\\) 회 만큼 연산된다.\n\n\n\nCode\nn = 3\nfor i in range(1, n + 1):\n    for j in range(1, n + 1):\n        print(f\"{i} X {j} = {i * j}\")\n\n\n1 X 1 = 1\n1 X 2 = 2\n1 X 3 = 3\n2 X 1 = 2\n2 X 2 = 4\n2 X 3 = 6\n3 X 1 = 3\n3 X 2 = 6\n3 X 3 = 9\n\n\n\n일반적으로 연산 횟수가 10억 (\\(1.0 \\times 10^9\\))을 넘어가면 1초 이상의 시간이 소요된다.\n[예시] n이 1,000일 때를 고려해 보자.\n\n\\(O(n)\\): 약 1,000번의 연산\n\\(O(nlogn )\\): 약 10,000번의 연산 (약 \\(log10=10\\))\n\\(O(n^2)\\): 약 1,000,000번의 연산\n\\(O(n^3)\\): 약 1,000,000,000번의 연산\n\n그러므로, 알고리즘 짤 때 코딩 레벨로 연산 횟수를 계산해서 연산 시간을 어림잡아 추정할 수 있다.\n시간 복잡도 속도 비교\n By Cmglee - Own work, CC BY-SA 4.0\nBig-O 표기법으로 시간 복잡도를 표기할 때는 가장 영향력이 큰 항만을 표시한다.\n\n\\(O(3n^2 + n) = O(n^2)\\)\n현실 세계에서는 동작 시간이 1초 이내인 알고리즘을 설계할 필요가 있다.\n실무적으로 프로그램 동작 시간이 1초 이상이면 매우 느린 것으로 간주.\n\n공간 복잡도를 나타낼 때는 MB 단위로 표기한다.\nint a[1000]: 4KB int a[1000000]: 4MB int a[2000][2000]: 16MB\n자료구조를 적절히 활용하기\n\n자료구조의 종류로는 스택, 큐, 트리 등이 있다.\n프로그램을 작성할 때는 자료구조를 적절히 활용하여 시간 복잡도를 최소화하여야 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#data-structure",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#data-structure",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "딥러닝은 다양한 알고리즘의 조합으로 수행되기 때문에 다양한 알고리즘을 정확하게 작성하기 위해서는 다수의 다양한 자료(data)를 담기 위해 사용되는 자료 구조를 이해할 필요가 있다. 즉, 자료구조는 정확한 알고리즘을 구현하기 위해 다수의 자료(data)를 담기 위한 구조이다.\n\n딥러닝 유저들간에도 자료구조를 이해하는 것에 대한 의견이 분분하지만\n올바른 자료구조를 사용하는 것은 프로그램을 조직적으로 만들 수 있는 능력을 키울 수 있다.\n데이터의 수가 많아질수록 효율적인 자료구조가 필요하다.\n예시) 학생 수가 1,000,000명 이상인 학생 관리 프로그램\n\n매일 자료 조회가 1억번 이상 발생한다면 더 빠르게 동작하는 자료 구조를 사용해야 프로그램의 효율성을 올릴 수 있다.\n\n\n\n\n\n자료구조의 필요성에 대해서 이해할 필요가 있다.\n성능 비교: 자료구조/알고리즘의 성능 측정 방법에 대해 이해할 필요가 있다.\n\nA: 적당한 속도의 삽입 & 적당한 속도의 추출 (삽입: \\(O (log N)\\) / 추출: \\(O(log N)\\))\nB: 느린 삽입 & 빠른 추출 (삽입: \\(O (N)\\) / 추출: \\(O (1)\\))\nA vs B? 상황에 따라 A를 만들지 B를 만들지 선택해야 한다. 삽입 연산이 많으면 A를, 추출 연산이 많으면 B를 택해야 한다. (속도 비교: \\(O (N) &lt; O (log N)&lt; O (1)\\))\n하지만, 실무적으로 많은 개발자들이 A를 택한다. 왜냐면 log 복잡도는 상수 복잡도와 속도가 비슷하기 때문"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료-구조의-필요성",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료-구조의-필요성",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "이처럼 상황에 맞게 알고리즘의 연산 속도를 결정해야 하므로 데이터를 효과적으로 저장하고, 처리하는 방법에 대해 바르게 이해할 필요가 있다.\n자료구조를 제대로 이해해야 불필요하게 메모리와 계산을 낭비하지 않는다.\nC언어를 기준으로 정수(int) 형식의 데이터가 100만 개가량이 존재한다고 가정하자.\n해당 프로그램을 이용하면, 내부적으로 하루에 데이터 조회가 1억 번 이상 발생한다.\n이때 원하는 데이터를 가장 빠르게 찾도록 해주는 자료구조는 무엇일까?\n\n트리(tree)와 같은 자료구조를 활용할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료-구조의-종류",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료-구조의-종류",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "선형 자료 구조(linear data structure) 선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 하나 존재하는 자료구조이며 데이터가 일렬로 연속적으로(순차적으로) 연결되어 있다.\n\n배열(array)\n연결 리스트(linked list)\n스택(stack)\n큐(queue)\n\n비선형 자료 구조(non-linear data structure)\n비선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 여러 개 올 수 있는 자료구조이며 데이터가 일직선상으로 연결되어 있지 않아도 된다.\n\n트리(tree)\n그래프(graph)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료구조와-알고리즘",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료구조와-알고리즘",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "효율적인 자료구조 설계를 위해 알고리즘 지식이 필요하다.\n효율적인 알고리즘을 작성하기 위해서 문제 상황에 맞는 적절한 자료구조가 사용되어야 한다.\n프로그램을 작성할 때 자료구조와 알고리즘 모두 고려해야 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#프로그램의-성능-측정-방법",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#프로그램의-성능-측정-방법",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "시간 복잡도(time complexity): 알고리즘에 사용되는 연산 횟수를 측정 (시간 측정)\n공간 복잡도(space complexity): 알고리즘에 사용되는 메모리의 양을 측정 (공간 측정)\n공간을 많이 사용하는 대신 시간을 단축하는 방법이 흔히 사용된다.\n프로그램의 성능 측정 방법: Big-O 표기법\n\n복잡도를 표현할 때는 Big-O 표기법을 사용한다.\n\n특정한 알고리즘이 얼마나 효율적인지 수치적으로 표현할 수 있다.\n가장 빠르게 증가하는 항만을 고려하는 표기법이다.\n\n아래의 알고리즘은 \\(O(n)\\) 의 시간 복잡도를 가진다. 왜냐면, n에 따라 summary += i의 연산 횟수가 정해지기 때문이다.\n\n\n\n\nCode\nn = 10\nsummary = 0\nfor i in range(n):\n    summary += i\nprint(summary)\n\n\n45\n\n\n\n다음 알고리즘은 \\(O (n^2)\\) 의 시간 복잡도를 가진다. 2 중 for loop은 i와 j가 n에 따라 각 각 n 번씩 연산되기때문에 \\(n \\times n\\) 회 만큼 연산된다.\n\n\n\nCode\nn = 3\nfor i in range(1, n + 1):\n    for j in range(1, n + 1):\n        print(f\"{i} X {j} = {i * j}\")\n\n\n1 X 1 = 1\n1 X 2 = 2\n1 X 3 = 3\n2 X 1 = 2\n2 X 2 = 4\n2 X 3 = 6\n3 X 1 = 3\n3 X 2 = 6\n3 X 3 = 9\n\n\n\n일반적으로 연산 횟수가 10억 (\\(1.0 \\times 10^9\\))을 넘어가면 1초 이상의 시간이 소요된다.\n[예시] n이 1,000일 때를 고려해 보자.\n\n\\(O(n)\\): 약 1,000번의 연산\n\\(O(nlogn )\\): 약 10,000번의 연산 (약 \\(log10=10\\))\n\\(O(n^2)\\): 약 1,000,000번의 연산\n\\(O(n^3)\\): 약 1,000,000,000번의 연산\n\n그러므로, 알고리즘 짤 때 코딩 레벨로 연산 횟수를 계산해서 연산 시간을 어림잡아 추정할 수 있다.\n시간 복잡도 속도 비교\n By Cmglee - Own work, CC BY-SA 4.0\nBig-O 표기법으로 시간 복잡도를 표기할 때는 가장 영향력이 큰 항만을 표시한다.\n\n\\(O(3n^2 + n) = O(n^2)\\)\n현실 세계에서는 동작 시간이 1초 이내인 알고리즘을 설계할 필요가 있다.\n실무적으로 프로그램 동작 시간이 1초 이상이면 매우 느린 것으로 간주.\n\n공간 복잡도를 나타낼 때는 MB 단위로 표기한다.\nint a[1000]: 4KB int a[1000000]: 4MB int a[2000][2000]: 16MB\n자료구조를 적절히 활용하기\n\n자료구조의 종류로는 스택, 큐, 트리 등이 있다.\n프로그램을 작성할 때는 자료구조를 적절히 활용하여 시간 복잡도를 최소화하여야 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/WSL/01.wsl-install.html",
    "href": "docs/blog/posts/Engineering/WSL/01.wsl-install.html",
    "title": "WSL Install",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\nWindows Subsystem for Linux은 Windows에서 리눅스 실행환경을 지원하는 Windows의 확장 기능\n\nWindows에서 바로 리눅스 명령어를 실행할 수 있어서, Windows와 리눅스를 함께 사용하는 개발자들에게 편리\nWSL 이전에는 가상 머신 (Virtual Machine, VM)을 많이 사용했었음\n\nVM: 컴퓨터 안에 구축된 가상 컴퓨터 개념으로 CPU, Memory, Network Interface, and Storage를 갖춘 온전한 컴퓨터 시스템으로 작동하는 가상환경\n하지만 VM은 메모리 overhead가 심한 문제점이 있었음\n\noverhead: 컴퓨터가 어떤 연산 및 처리를 하기 위해 들어가는 간접적인 처리시간 메모리등을 말함\n예를 들어, VM을 쓰려면 컴퓨터가 디스크와 메모리의 일정 이상 부분을 할당해줘야 VM을 쓸 수 있었음\n그래서 VM이 많을 수록 overhead가 심해지는 현상이 발생했는데 WSL 개발 이후로 Linux를 더 가볍게 사용할 수 있게됐음\n\n\n\n\n\n\n\nAirflow는 Windows에 직접 설치 불가\nWindows에서 리눅스 작업환경을 만들기 위해서 WSL 설치가 필수\n여유가 된다면 가상화 VM 또는 Public Cloud (AWS, GCP, Azure)의 컴퓨팅 서비스에서 Linux 및 Airflow 설치 가능\n\n\n\n\n\n설치 전 체크사항 (시작버튼 → 시스템 정보에서 확인)\n\nWindows 10 버전 2004 이상\nWindows 11\n\nPowerShell 명령어로 설치\n\nwsl –install\n\n\n\n\nWSL Install 설명 공식 홈페이지\n\n\n\nOpen PowerShell or Windows Command Prompt in administrator mode by right-clicking and selecting “Run as administrator”\nwsl --install 실행\nEnter New UNIX username/password\nturn off PowerShell\nturn on PowerShell again\nwsl -l -v 실행 반드시 version 2가 설치되어 있어야함 (WSL2)\n\nWSL1이 설치되었으면 WSL2 업그래이드 해야함 (windows update 해야함)\nWSL1은 나중에 사용할 docker가 제대로 작동하지 않음\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/WSL/01.wsl-install.html#wsl-windows-subsystem-for-linux",
    "href": "docs/blog/posts/Engineering/WSL/01.wsl-install.html#wsl-windows-subsystem-for-linux",
    "title": "WSL Install",
    "section": "",
    "text": "Windows Subsystem for Linux은 Windows에서 리눅스 실행환경을 지원하는 Windows의 확장 기능\n\nWindows에서 바로 리눅스 명령어를 실행할 수 있어서, Windows와 리눅스를 함께 사용하는 개발자들에게 편리\nWSL 이전에는 가상 머신 (Virtual Machine, VM)을 많이 사용했었음\n\nVM: 컴퓨터 안에 구축된 가상 컴퓨터 개념으로 CPU, Memory, Network Interface, and Storage를 갖춘 온전한 컴퓨터 시스템으로 작동하는 가상환경\n하지만 VM은 메모리 overhead가 심한 문제점이 있었음\n\noverhead: 컴퓨터가 어떤 연산 및 처리를 하기 위해 들어가는 간접적인 처리시간 메모리등을 말함\n예를 들어, VM을 쓰려면 컴퓨터가 디스크와 메모리의 일정 이상 부분을 할당해줘야 VM을 쓸 수 있었음\n그래서 VM이 많을 수록 overhead가 심해지는 현상이 발생했는데 WSL 개발 이후로 Linux를 더 가볍게 사용할 수 있게됐음"
  },
  {
    "objectID": "docs/blog/posts/Engineering/WSL/01.wsl-install.html#why-to-install-wsl",
    "href": "docs/blog/posts/Engineering/WSL/01.wsl-install.html#why-to-install-wsl",
    "title": "WSL Install",
    "section": "",
    "text": "Airflow는 Windows에 직접 설치 불가\nWindows에서 리눅스 작업환경을 만들기 위해서 WSL 설치가 필수\n여유가 된다면 가상화 VM 또는 Public Cloud (AWS, GCP, Azure)의 컴퓨팅 서비스에서 Linux 및 Airflow 설치 가능"
  },
  {
    "objectID": "docs/blog/posts/Engineering/WSL/01.wsl-install.html#how-to-install-wsl",
    "href": "docs/blog/posts/Engineering/WSL/01.wsl-install.html#how-to-install-wsl",
    "title": "WSL Install",
    "section": "",
    "text": "설치 전 체크사항 (시작버튼 → 시스템 정보에서 확인)\n\nWindows 10 버전 2004 이상\nWindows 11\n\nPowerShell 명령어로 설치\n\nwsl –install\n\n\n\n\nWSL Install 설명 공식 홈페이지\n\n\n\nOpen PowerShell or Windows Command Prompt in administrator mode by right-clicking and selecting “Run as administrator”\nwsl --install 실행\nEnter New UNIX username/password\nturn off PowerShell\nturn on PowerShell again\nwsl -l -v 실행 반드시 version 2가 설치되어 있어야함 (WSL2)\n\nWSL1이 설치되었으면 WSL2 업그래이드 해야함 (windows update 해야함)\nWSL1은 나중에 사용할 docker가 제대로 작동하지 않음"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nWSL Installation\nFrequently Used Linux Commands\n\n\n\n\n\nDocker Installation\ndocker를 사용하기 전에 sudo service docker start 실행해야 함\ndocker 설치 확인\n\nsudo docker run hello-world 하면 다음과 같은 메세지 떠야함\n\n먼저 docker는 local에서 실행하고자 하는 image를 찾고, 없으면 가장 최신 version의 image를 download 받는다.\n하지만, 위의 그림에 있는 메세지와 같이 이미 실행하고자 하는 image가 있어 다운로드 받을 필요가 없으면 image를 찾고 다운로드 받았다는 메세지는 생략되게 된다.\n\n\n\n\n\n\nAirflow 설치 방법은 여러가지가 존재하며 그 중 하나가 도커 설치임\n도커 컴포즈 (docker compose)를 이용하여 한번에 쉽게 설치 가능\n\nDocker Compose를 이용하여 Airflow 설치 링크\ndocker compose: 여러 개의 도커 컨테이너 설정을 한방에 관리하기 위한 도커 확장 기술로 에어플로우를 설치하기 위한 도커 컨테이너 세팅 내용이 들어있음\n\nairflow 자체도 여러개의 docker containers로 구성됨\n\nDocker Compose를 이용하여 Airflow 설치 링크의 Fetching docker-compose.yaml 부터 시작\n\ncurl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.6.1/docker-compose.yaml'\nls로 docker-compose.yaml 다운로드됐는지 확인\nvi * 로 docker-compose.yaml 열어보면 주석처리와 airflow의 설정내용들을 확인할 수 있다.\n\nSetting the right Airflow user\n\ndirectories 만들기: mkdir -p ./dags ./logs ./plugins ./config\n.env 파일 만들기: echo -e \"AIRFLOW_UID=$(id -u)\" &gt; .env\nvi .env: AIRFLOW_UID=1000 인 이유는 OS 계정의 uid가 1000이라는 뜻\n\nInitialize the database\n\nsudo docker compose up airflow-init: sudo 반드시 앞에 붙여야함. exited with code 0가 떠야 정상적으로 설치 된 것임\n\n\n\n\n\n\n\nservice 띄우기\n\nsudo docker compose up 실행. sudo 반드시 앞에 붙여야함.\n\nhttp 상태가 계속해서 업데이트 되야 airflow가 돌아가고 있는 것임. 계속해서 update되는 http command 닫으면 airflow멈춤. 두번째 터미널 열어서 작업해야함\n두 번째 터미널 열고 sudo docker ps 실행하여 container list 상태 확인. 총 6개 올라와야 정상\n\nairflow-airflow-worker-1 : scheduler로 부터 부여된 tasks 수행\nairflow-airflow-triggerer-1\nairflow-airflow-webserver-1\nairflow-airflow-scheduler-1 : tasks와 dags을 monitoring\nairflow-postgres-1\nairflow-redis-1 : for queue service, scheduler에서 worker로 message 전달\n\n웹 브라우저 창에 localhost:8080 입력하여 airflow service창에 접속\n\ndefault ID/PW: airflow/airflow\n웹 브라우저에서 local로 airflow service 접속 원리\n\n웹 브라우저는 local PC에 있음\nairflow는 WSL안에 docker container로 실행되고 있음\n이렇게 2개의 다른 공간이 연결될 수 있는 이유는 WSL은 기본적으로 local PC의 local host IP와 연결이 되어 있음\n그래서 웹 브라우저에서 local로 localhost:8080 라고 입력하면 WSL에서 8080 port를 입력하는 것과 같은 효과가 있기 때문에 local 웹브라우저에서 WSL container로 들어갈 수 있는 것임.\nsample DAGs이 만들어져 있는 것을 확인 할 수 있음\n\nAirflow Webserver (or Web Browser) Port 변경\n\nAirflow의 default port 는 8080으로 설정되어 있는데 만약 port를 변경하고 싶으면 docker-compose.yaml을 수정해야한다. 다음은 docker-compose.yaml의 airflow webserver 부분을 복붙한 것이다. &lt;변경전 with comments&gt;\n\n  airflow-webserver:\n    &lt;&lt;: *airflow-common\n    command: webserver\n    ports:\n      - \"8080(local port 변경하고자 하는 부분):8080 (airflow의 healthcheck.test의 port)\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n      interval: 10s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n    networks:\n      network_custom:\n        ipv4_address: 172.28.0.6\n&lt;변경후&gt; markdown     airflow-webserver:       &lt;&lt;: *airflow-common       command: webserver       ports:         - \"8787:8080\"       healthcheck:         test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]         interval: 10s         timeout: 10s         retries: 5         start_period: 30s       restart: always       depends_on:         &lt;&lt;: *airflow-common-depends-on         airflow-init:           condition: service_completed_successfully       networks:         network_custom:           ipv4_address: 172.28.0.6 위 처럼 변경하면 되지만 나는 그냥 기본 port인 8080을 사용한다.\n\nexample_bash_operator DAG을 들어가 보면\n\nGrid: 수행 이력을 보여주는 tab\n\nDAG 이름 example_bash_operator 옆에 pause toggle를 활성화 시키면 unpaused 됨\nAuto-regresh toggle 활성화 시키면 DAG이 한번 돌아감\n\nGraph: DAG을 구성하고 있는 tasks를 보여주는 tab. 각 각의 task가 색 별로 상태를 보여주고 있음\nCalendar: 참고할 것\nTask Duration: 참고할 것\nTask Tries: 참고할 것\nLanding Times: 참고할 것\nGantt: 참고할 것\nDetails: 참고할 것\nCode: DAG을 구성하고 있는 python code를 볼 수 있음\nAudit Log: 참고할 것\n\n\n\n\n\n\n\n\nCPU: 4Core 이상\nMemory: 16GB (권장-문제없음) / 8GB (최소-약간 버벅 거림)\nWSL에서 다수의 컨테이너 실행시 메모리 점유율 상승할 수 있음\n\nairflow service창과 WSL 창 닫고 다시 키면 어느 정도 메로리 점유율 낮아짐\n\n\n\n\n\n\nuser가 만든 DAG이 airflow까지 전달되는 workflow가 아래와 같이 묘사되어 있다.\n\n\n\n\n개발 환경 workflow\n\n\n\n위의 그림에서 보면 6 containers가 있고 airflow setting 할때 dags, logs, plugins, config directories를 만들었는데 모두 airflow containers에 연결되어 있음\n\nmount 의미: directory안에 file을 넣으면 containers가 file을 인식할 수 있음\nuser가 만든 dag을 dags directory에 넣으면 airflow container가 dags안에 있는 dag을 인식하여 서비스에 띄어줌\n\n개발환경 세팅의 목표\n\n로컬 환경에서 만든 dag을 dags directory에 배포하여 containers가 user가 만든 dag을 인식하여 airflow서비스까지 띄우는 것이 목표\n다시 말해서, 그냥 로컬 환경에서 만든 dag을 dags directory에 배포하면 됨\n\nActions\n\n로컬 컴퓨터에 python interpreter 설치\n\n아무 python version을 설치하면 안되고 airflow containers가 쓰고있는 python version과 일치시켜야 함!\n\nIDE Tool(VScode) 개발환경 설정\nGithub 레파지토리 생성\n로컬 컴퓨터에 Python Airflow Libraries 설치\nWSL에 Git 설치 및 git pull이 가능한 환경구성\n\ngit repository에 DAG을 만들어 push하여 dags directory에 pull이 되어 dag이 들어가게 하면 됨.\n\n\n\n\n\n\n\nActions\n\n컨테이너에서 사용하는 파이썬 버전 확인\n\ncontainer안에 들어가기: sudo docker exec -it {container-name or container-id} 명령어 \\(\\rightarrow\\) sudo docker exec -it airflow-airflow-worker-1 bash: -it는 session이 안 끊어지도록 유지해주는 옵션\npython -V 실행하여 python version 확인 : 현재 나의 python version은 Python 3.7.16\nctrl D로 exit\n\n파이썬 인터프리터 다운로드\n\n보안상의 업데이트 말곤 기능이 같기 때문에 Python 3.7.16대신 Python 3.7.9 설치하면 됨\n\n로컬 컴퓨터에 파이썬 설치\n\nconda에 설치하고 싶으면 conda create -n airflow python=3.7.9 or\nglobal 환경에 설치하고 싶으면 Windows x86-64 executable installer 다운로드 및 설치\n\n\n\n\n\n\n\nVScode란?\n\nMicrosoft사에서 2015년에 제작, 다양한 언어 개발을 돕는 IDE tool\nVisual Studio 라는 IDE 툴과는 엄연히 다른 툴\n\nActions\n\nVScode 다운로드\n\n설치 마법사에서 추가 작업 선택란에 code로 열기 작업을 windows탐색기 파일의 상황에 맞는 메뉴에 추가 선택할 것. programming file을 열때 VScode가 디폴트가 되도록함\n\nVScode 설치, 파이썬 확장팩 설치\n프로젝트 생성, 파이썬 가상환경 설정\n\nVScode가 file이나 directory단위로 관리하는 IDE tool이라 프로젝트 생성 개념이 없음\nwindows에 프로젝트 directory하나 만들고 VScode에서 open folder로 열면 그 folder를 최상위 folder로 인식 (project 생성됨)\n\npython interpreter 설정\n\nVScode &gt; Terminal &gt; New Terminal &gt; python version 확인\n\n\n파이썬 가상환경\n\n라이브러리 버전 충돌 방지를 위해 설치/사용되는 파이썬 인터프리터 환경을 격리시키는 기술\n파이썬은 라이브러리 설치 시점에 따라서도 설치되는 버전이 상이한 경우가 많음\n\n\n\n\n가상 환경의 필요성\n\n\n\npython을 global 환경에 설치할 경우 위의 그림처럼 C,D프로젝트가 동시에 진행될 때 둘 중하나의 library version이 차이가 나면 old version의 library 로 진행되는 프로젝트는 에러가 발생함\n\n2개의 다른 프로젝트가 같은 python interpreter를 바라보고 library를 설치하기 때문에 종속성 문제가 생김 (library 충돌 발생)\n그래서 다른 가상환경 venv안에 다른 프로젝트를 할당해서 독립적으로 프로젝트를 진행하는게 일반적임\n\npython 가상환경 만들기\n\nconda로 만들 경우 conda 설치 후 만들면 됨. 설치 링크\npython에 있는 가상환경 생성 기능으로 만들 경우 python -m airflow ./venv 실행\n\n./venv directory에 python 설치하고 version 관리하겠다는 의미\n\n\nVScode가 python 가상환경 참조하도록 설정\n\nhelp&gt;show all commands or ctrl+shift+p 누른후 interpreter 입력하여 가상환경에 있는 python 클릭\n\nterminal 에서 가상환경 잘 잡혔는지 확인\n\n\n\n\n\n\nGit Installation & Environment Setting\n\n\n\n\n\nAirflow 라이브러리 설치 대상과 설치 이유\n\n설치 대상: 로컬 컴퓨터의 파이썬 가상환경(본인의 경우: airflow)\nWhy? Airflow DAG 개발을 위해 Airflow의 python class files 및 라이브러리들이 많기 때문에 필요\n\nAirflow 라이브러리 설치 가이드\n\nconda activate airflow 가상환경으로 들어감\npip install \"apache-airflow[celery]==2.6.1\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.6.1/constraints-3.7.txt\"\n\n리눅스에서 파이썬 Airflow 라이브러리 설치시 그 자체로 Airflow 서비스 사용 가능\n\n하지만 WSL에서 pip install 명령으로 Airflow를 설치하지 않는 이유?\npip install 로 Airflow 설치시 저사양의 아키텍처로 설치되며 여러 제약이 존재함 (Task를 한번에 1개씩만 실행 가능 등)\n그러므로 docker로 설치해야 제약이 없음\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html#개발-환경-권장-사양",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html#개발-환경-권장-사양",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "CPU: 4Core 이상\nMemory: 16GB (권장-문제없음) / 8GB (최소-약간 버벅 거림)\nWSL에서 다수의 컨테이너 실행시 메모리 점유율 상승할 수 있음\n\nairflow service창과 WSL 창 닫고 다시 키면 어느 정도 메로리 점유율 낮아짐"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html#개발-환경-workflow",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html#개발-환경-workflow",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "user가 만든 DAG이 airflow까지 전달되는 workflow가 아래와 같이 묘사되어 있다.\n\n\n\n\n개발 환경 workflow\n\n\n\n위의 그림에서 보면 6 containers가 있고 airflow setting 할때 dags, logs, plugins, config directories를 만들었는데 모두 airflow containers에 연결되어 있음\n\nmount 의미: directory안에 file을 넣으면 containers가 file을 인식할 수 있음\nuser가 만든 dag을 dags directory에 넣으면 airflow container가 dags안에 있는 dag을 인식하여 서비스에 띄어줌\n\n개발환경 세팅의 목표\n\n로컬 환경에서 만든 dag을 dags directory에 배포하여 containers가 user가 만든 dag을 인식하여 airflow서비스까지 띄우는 것이 목표\n다시 말해서, 그냥 로컬 환경에서 만든 dag을 dags directory에 배포하면 됨\n\nActions\n\n로컬 컴퓨터에 python interpreter 설치\n\n아무 python version을 설치하면 안되고 airflow containers가 쓰고있는 python version과 일치시켜야 함!\n\nIDE Tool(VScode) 개발환경 설정\nGithub 레파지토리 생성\n로컬 컴퓨터에 Python Airflow Libraries 설치\nWSL에 Git 설치 및 git pull이 가능한 환경구성\n\ngit repository에 DAG을 만들어 push하여 dags directory에 pull이 되어 dag이 들어가게 하면 됨."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html#python-interpreter-installation",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html#python-interpreter-installation",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "Actions\n\n컨테이너에서 사용하는 파이썬 버전 확인\n\ncontainer안에 들어가기: sudo docker exec -it {container-name or container-id} 명령어 \\(\\rightarrow\\) sudo docker exec -it airflow-airflow-worker-1 bash: -it는 session이 안 끊어지도록 유지해주는 옵션\npython -V 실행하여 python version 확인 : 현재 나의 python version은 Python 3.7.16\nctrl D로 exit\n\n파이썬 인터프리터 다운로드\n\n보안상의 업데이트 말곤 기능이 같기 때문에 Python 3.7.16대신 Python 3.7.9 설치하면 됨\n\n로컬 컴퓨터에 파이썬 설치\n\nconda에 설치하고 싶으면 conda create -n airflow python=3.7.9 or\nglobal 환경에 설치하고 싶으면 Windows x86-64 executable installer 다운로드 및 설치"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html#vscode-installation",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html#vscode-installation",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "VScode란?\n\nMicrosoft사에서 2015년에 제작, 다양한 언어 개발을 돕는 IDE tool\nVisual Studio 라는 IDE 툴과는 엄연히 다른 툴\n\nActions\n\nVScode 다운로드\n\n설치 마법사에서 추가 작업 선택란에 code로 열기 작업을 windows탐색기 파일의 상황에 맞는 메뉴에 추가 선택할 것. programming file을 열때 VScode가 디폴트가 되도록함\n\nVScode 설치, 파이썬 확장팩 설치\n프로젝트 생성, 파이썬 가상환경 설정\n\nVScode가 file이나 directory단위로 관리하는 IDE tool이라 프로젝트 생성 개념이 없음\nwindows에 프로젝트 directory하나 만들고 VScode에서 open folder로 열면 그 folder를 최상위 folder로 인식 (project 생성됨)\n\npython interpreter 설정\n\nVScode &gt; Terminal &gt; New Terminal &gt; python version 확인\n\n\n파이썬 가상환경\n\n라이브러리 버전 충돌 방지를 위해 설치/사용되는 파이썬 인터프리터 환경을 격리시키는 기술\n파이썬은 라이브러리 설치 시점에 따라서도 설치되는 버전이 상이한 경우가 많음\n\n\n\n\n가상 환경의 필요성\n\n\n\npython을 global 환경에 설치할 경우 위의 그림처럼 C,D프로젝트가 동시에 진행될 때 둘 중하나의 library version이 차이가 나면 old version의 library 로 진행되는 프로젝트는 에러가 발생함\n\n2개의 다른 프로젝트가 같은 python interpreter를 바라보고 library를 설치하기 때문에 종속성 문제가 생김 (library 충돌 발생)\n그래서 다른 가상환경 venv안에 다른 프로젝트를 할당해서 독립적으로 프로젝트를 진행하는게 일반적임\n\npython 가상환경 만들기\n\nconda로 만들 경우 conda 설치 후 만들면 됨. 설치 링크\npython에 있는 가상환경 생성 기능으로 만들 경우 python -m airflow ./venv 실행\n\n./venv directory에 python 설치하고 version 관리하겠다는 의미\n\n\nVScode가 python 가상환경 참조하도록 설정\n\nhelp&gt;show all commands or ctrl+shift+p 누른후 interpreter 입력하여 가상환경에 있는 python 클릭\n\nterminal 에서 가상환경 잘 잡혔는지 확인"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html#git-environment-setting",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html#git-environment-setting",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "Git Installation & Environment Setting"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/02.env_setting.html#airflow-library-installation",
    "href": "docs/blog/posts/Engineering/airflow/02.env_setting.html#airflow-library-installation",
    "title": "Environment Setting for Airflow",
    "section": "",
    "text": "Airflow 라이브러리 설치 대상과 설치 이유\n\n설치 대상: 로컬 컴퓨터의 파이썬 가상환경(본인의 경우: airflow)\nWhy? Airflow DAG 개발을 위해 Airflow의 python class files 및 라이브러리들이 많기 때문에 필요\n\nAirflow 라이브러리 설치 가이드\n\nconda activate airflow 가상환경으로 들어감\npip install \"apache-airflow[celery]==2.6.1\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.6.1/constraints-3.7.txt\"\n\n리눅스에서 파이썬 Airflow 라이브러리 설치시 그 자체로 Airflow 서비스 사용 가능\n\n하지만 WSL에서 pip install 명령으로 Airflow를 설치하지 않는 이유?\npip install 로 Airflow 설치시 저사양의 아키텍처로 설치되며 여러 제약이 존재함 (Task를 한번에 1개씩만 실행 가능 등)\n그러므로 docker로 설치해야 제약이 없음"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#python",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#python",
    "title": "Content List, Engineering",
    "section": "8 python",
    "text": "8 python\n\n2023-06-01, Package Management: requirements.txt"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Python/package_management.html",
    "href": "docs/blog/posts/Engineering/Python/package_management.html",
    "title": "Package Management - 1",
    "section": "",
    "text": "여러 projects를 오랫동안 관리하다 보면 다수의 packages를 설치해야하는데 이 와중에 PC 포맷, fork, cloning과 같은 code를 옮겨야하는 일이 생길 수 있다. 이렇게 새로운 환경에서 이전에 관리하던 projects를 재현해야하는데 수 많은 packages를 하나 하나씩 재설치해야하는 것은 여간 번거로운 작업이 아니다.\n\npip list를 이용한 pip로 설치된 패키지 조회\n\n아래와 같이 package list들이 출력이 된다 (일부 스크린샷함). 이 많은 packages를 하나 하나씩 conda install package_name 실행하여 설치할 순 없다. \n\npip freeze &gt; requirements.txt 명령어를 실행하여 requirements.txt 를 만들어 준다. 이 txt 파일 안에는 packages의 이름과 version 정보까지 기록되어 있는 것을 확인할 수 있다.\npip install -r requirements.txt 명령어를 실행하여 requirements.txt 에 있는 packages를 한꺼번에 설치할 수 있게 된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/Python/package_management.html#requirements.txt-를-이용한-packages-관리",
    "href": "docs/blog/posts/Engineering/Python/package_management.html#requirements.txt-를-이용한-packages-관리",
    "title": "Package Management - 1",
    "section": "",
    "text": "여러 projects를 오랫동안 관리하다 보면 다수의 packages를 설치해야하는데 이 와중에 PC 포맷, fork, cloning과 같은 code를 옮겨야하는 일이 생길 수 있다. 이렇게 새로운 환경에서 이전에 관리하던 projects를 재현해야하는데 수 많은 packages를 하나 하나씩 재설치해야하는 것은 여간 번거로운 작업이 아니다.\n\npip list를 이용한 pip로 설치된 패키지 조회\n\n아래와 같이 package list들이 출력이 된다 (일부 스크린샷함). 이 많은 packages를 하나 하나씩 conda install package_name 실행하여 설치할 순 없다. \n\npip freeze &gt; requirements.txt 명령어를 실행하여 requirements.txt 를 만들어 준다. 이 txt 파일 안에는 packages의 이름과 version 정보까지 기록되어 있는 것을 확인할 수 있다.\npip install -r requirements.txt 명령어를 실행하여 requirements.txt 에 있는 packages를 한꺼번에 설치할 수 있게 된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nDAG\n\n\n\nBash_Operator\n\nBash_Operator\n\n\n\nTask1\n\nTask1\n\n\n\nBash_Operator-&gt;Task1\n\n\n\n\n\nPython_Operator\n\nPython_Operator\n\n\n\nTask2\n\nTask2\n\n\n\nPython_Operator-&gt;Task2\n\n\n\n\n\nS3_Operator\n\nS3_Operator\n\n\n\nTask3\n\nTask3\n\n\n\nS3_Operator-&gt;Task3\n\n\n\n\n\nGCS_Operator\n\nGCS_Operator\n\n\n\nTask4\n\nTask4\n\n\n\nGCS_Operator-&gt;Task4\n\n\n\n\n\n\n\n\n\n\n\nworkflow = DAG\nOpeartor\n\n특정 행위를 할 수 있는 기능을 모아 놓은 클래스 또는 설계도\n\nTask\n\noperator가 객체화(instantiation)되어 DAG에서 실행 가능한 object\n방향성을 갖고 순환되지 않음 (DAG)\n\nBash Operator\n\nLinux에서 shell script 명령을 수행하는 operator\n\nPython Operator\n\npython 함수를 실행하는 operator\n\nS3 Operator\n\nAWS의 S3 solution (object storage)을 control할 수 있는 operator\n\nGCS Operator\n\nGCP의 GCS solution (object storage)을 control할 수 있는 operator\n\noperators을 사용하여 dags을 작성하여 git을 통해 배포한다.\ndag 작성 및 배포\n  from __future__ import annotations\n\n  import datetime # python에는 datatime이라는 data type이 있음\n  import pendulum # datetime data type을 처리하는 library\n\n  from airflow import DAG\n  from airflow.operators.bash import BashOperator\n  from airflow.operators.empty import EmptyOperator\n\n  with DAG(\n      dag_id=\"dags_bash_operator\", \n      # airflow service web 상에서 보여지는 이름, python file명과는 무관하지만 \n      # 실무에서는 일반적으로 python 파일명과 dag_id는 일치시키는 것이 다수의 dags 관리에 편리하다.\n      schedule=\"0 0 * * *\", # \"분 시 일 월 요일\", cron schedule로서 매일 0분 0시에 실행\n      start_date=pendulum.datetime(2023, 6, 9, tz=\"Asia/Seoul\"), #dags이 언제 실행될지 설정\n      # UTC: 세계 표준시로 한국 보다 9시간이 느림. Asia/Seoul로 변경해야 지정한 날짜에 0분 0시에 실행될 수 있다.\n      catchup=False, # start_date를 현재보다 과거로 설정하게 될 경우 \n      # catchup=True면 과거 부터 현재까지 소급해서 실행. \n      # 시간 순서대로 실행하는게 아니라 병렬로 한번에 실행하기 때문에 메모리를 많이 잡아먹을 수 있음. \n      # 그래서 보통 False로 처리. catchup=False면 현재부터만 실행\n      # dagrun_timeout=datetime.timedelta(minutes=60), # dag이 60분 이상 구동시 실패가 되도록 설정\n      # tags=[\"example\", \"example2\"], #airflow service web browser상 dag의 tag를 의미. 즉 dag id 바로 밑 파란색 박스를 의미. tag를 누르면 같은 tag를 가진 dags들만 filtering돼서 선택됨 \n      ## dags 이 수 백개가 될 때 tag로 filtering 하면 용이함 \n      # params={\"example_key\": \"example_value\"}, # as dag: 이하 tasks를 정의할 때, \n      ## tasks에 공통 passing parameters가 있을 때 씀\n  ) as dag:\n      # [START how to_operator_bash]\n      bash_task1 = BashOperator(\n          task_id=\"bash_task1\", # airflow web service의 dag graph에 표시될 task명\n          # task역시 task object name (bash_task1)과 task_id(bash_task1)를 일치시키는 것이 좋음\n          bash_command=\"echo this task works well!\",\n          # bash_command 이하는 shell script를 적어주면 됨\n      )\n      # [END how to_operator_bash]\n      bash_task2 = BashOperator(\n          task_id=\"bash_task2\",  \n          bash_command=\"echo $HOSTNAME\", # $HOSTNAME: HOSTNAME 환경변수 호출\n          # WSL terminal 이름이 출력된다.\n      )\n      bash_task1 &gt;&gt; bash_task2 # 수행될 tasks의 관계 설정\n배포된 dags을 airflow containers과 연결 시키기 위해 docker-compose.yaml 실행\n\nvi docker-compose.yaml 실행 후 docker-compose.yaml 안에서 Volumns 항목이 wsl의 directory와 container directory를 연결(mount)해주는 요소\n\nVolumes\n  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n\n위와 같이 Volumns 항목이 뜨는데 :을 기준으로 왼쪽이 WSL directories(volumns), 오른쪽이 Docker container directories(volumns)\n다른 WSL창을 열어 echo ${AIRFLOW_PROJ_DIR:-.} 실행하면 AIRFLOW_PROJ_DIR에 값이 없기 때문에 . 출력됨\n\nAIRFLOW_PROJ_DIR:-. : shell script문법으로 AIRFLOW_PROJ_DIR에 값이 있으면 출력하고 없으면 .을 출력하라는 의미\necho AIRFLOW_PROJ_DIR: 아무것도 출력 안됨\n\n${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags는 ./dags를 /opt/airflow/dags에 연결시키라는 의미\n\n./: docker-compose.yaml이 위치하고있는 현재 directory를 의미\n\n배포된 dags를 자동으로 docker container에 연동시키기 위해 Volumns을 다음과 같이 편집\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n\ndirectory hierarchy에 따라 위의 volumes path를 다르게 설정해야한다. docker service web browse(i.e. localhost:8080) 껏다 켜가면서 확인하면서 설정\n\n새로운 dags 배포할 때마다 airflow service 껐다가 켜야 한다.\n\nairflow service 껐다 켜서 잘 반영됐는지 확인\n\ndocker가 설치된 wsl directory이동 먼저 할 것\nairflow service 끄기: sudo docker compose down\nairflow service 켜기: sudo docker compose up\n\nairflow web service상에서 dags이 잘 mount 되었는지 확인\n\n기본적으로 dags은 airflow web service상에 올라올 때 unpaused 상태로 올라옴\n하지만 schedule이 걸려있는 dags은 unpaused상태에서 한번 돌고 올라옴\ndag을 클릭하면 긴 녹색 막대기를 누르면 수행된 schedule내용이 나오고\n각 각의 task에 대응되는 녹색 네모 박스를 누르면 결과들을 조회할 수 있다.\n\n네모 박스를 누르고 log (audit log 아님)를 누르면 결과가 자세히 조회된다.\nbash_task2 의 bash_command=\"echo $HOSTNAME\" 의 결과값으로 조회된 값은 docker worker container id 를 의미한다.\n\n하지만 본인의 경우, airflow web service에서 794f3b56824a가 출력된 것을 확인했고\nsudo docker ps로 container ID를 확인한 결과 airflow-airflow-worker-1 의 32092b201878 로 달랐다.\n\n실제 worker container로 들어가 echo $HOSTNAME 실행하면 worker container id 출력되어야 함\n\nworker container로 들어가기: sudo docker exec -it container-name bash \\(\\rightarrow\\) 본인의 경우: sudo docker exec -it airflow-airflow-worker-1 bash 이 과정이 dag을 돌린과정과 같은 mechanism임\necho $HOSTNAME 실행 : 32092b201878 출력됨 (어쨌든 airflow web service상의 794f3b56824a와 달랐음)\nsudo docker exec -it 794f3b56824a bash 결과 Error response from daemon: No such container: 794f3b56824a 라는 에러메세지 뜸\n\n즉, worker container가 실제 task를 처리하는 것을 볼 수 있었다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Process\n\n\n\nScheduler\n\nScheduler\n\n\n\nScheduler-&gt;Scheduler\n\n\n3.check start time\n\n\n\nDAG_file\n\nDAG_file\n\n\n\nScheduler-&gt;DAG_file\n\n\n1.parsing\n\n\n\nQueue\n\nQueue\n\n\n\nScheduler-&gt;Queue\n\n\n\n\n\nMeta_DB\n\nMeta_DB\n\n\n\nScheduler-&gt;Meta_DB\n\n\n2.save information\n\n\n\nWorker\n\nWorker\n\n\n\nDAG_file-&gt;Worker\n\n\n5.Processing after reading\n\n\n\nWorker-&gt;Meta_DB\n\n\n6.Results update\n\n\n\nQueue-&gt;Worker\n\n\n4.start instruction\n\n\n\n\n\n\n\n\n\nscheduler\n\nairflow에서 brain역할\n\nparsing: a user가 만든 dag 파일을 읽어들여 문법적 오류 여부와 tasks 간의 관계를 분석\nsave information: DAG Parsing 후 DB에 정보저장 (tasks, task relations, schedule, etc.)\ncheck start time: DAG 시작 실행 시간 확인\nstart instruction: DAG 시작 실행 시간마다 worker에 실행 지시\n\n\nscheduler와 workder 사이에 queue 상태가 있을 수 있음\n\n\nworker (Worker Container)\n\nairflow 처리 주체 (subject)\n\nProcessing after reading: scheduler가 시킨 DAG 파일을 찾아 읽고 처리\nResults update: 처리가 되기 전/후를 Meta DB에 update함\n\n\n\n\n\n\n\n\n\n\ntask가 실행되어야 하는 시간(주기)을 정하기 위한 다섯개의 필드로 구성된 문자열\nCron을 이용하면 왠만한 scheduling 모두 가능\n\n{minutes} {hour} {day} {month} {weekday}\n\n\n\n\n\n\n\n\nNumber\nSpecial Characters\nDescription\n\n\n\n\n1\n*\n모든 값\n\n\n2\n-\n범위 지정\n\n\n3\n,\n여러 값 지정\n\n\n4\n/\n증가값 지정. staring-value/ending-value\n\n\n5\nL\n마지막 값 (일, 요일에만 설정 가능)  * 일에 L 입력시 해당 월의 마지막 일 의미  ※ 요일에 L 입력시 토요일 의미\n\n\n6\n#\n몇 번째 요일인지 지정\n\n\n\n\n\n\n\n\n\n\n\nCron schedule\nDescription\nNote\n\n\n\n\n15 2 * * *\n매일 02시 15분에 도는 daily batch\n\n\n\n0 * * * *\n매시 정각에 도는 시간 단위 batch\n\n\n\n0 0 1 * *\n매월 1일 0시 0분 도는 monthly batch\n\n\n\n10 1 * * 1\n매주 월요일 1시 10분에 도는 weekly batch\n0: 일요일, 1: 월요일, 2: 화요일, 3:수요일, 4: 목요일, 5: 금요일, 6: 토요일\n\n\n0 9-18 * * *\n매일 9시부터 18시까지 정각마다 도는 daily batch\n보통 이렇게 scheduling하지는 않음. 하지만 구현할 수 있음\n\n\n0 1 1,2,3 * *\n매월 1일, 2일 3일만 1시에 도는 monthly batch\n보통 이렇게 scheduling하지는 않음. 하지만 구현할 수 있음\n\n\n*/30 * * *\n삼십분마다 (0분, 30분)\n\n\n\n10-59/30 * * * *\n10분부터 삼십분마다 (10분, 40분에 도는 작업)\n\n\n\n10 1 * * 1-5\n평일만 01시 10분\n\n\n\n0 */2 * * *\n2시간 마다 (0시, 02시, 04시 …)\n1-23/2: 1시부터 2시간 마다\n\n\n0 0 */2 * *\n짝수일 0시 0분\n\n\n\n10 1 L * *\n매월 마지막 일 01시 10분에 도는 montly batch\n빈번하게 사용되는 schedule\n\n\n10 1 * * 6#3\n매월 세 번째 토요일 01시 10분 도는 montly batch\n\n\n\n\n\n\n\n\n\n\n\nTask 연결 방법 종류\n\n&gt;&gt;, &lt;&lt; 사용하기 (Airflow 공식 추천방식)\n함수 사용하기\n\n복잡한 Task 는 어떻게 연결하는가?\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Connection\n\n\n\ntask1\n\ntask1\n\n\n\ntask2\n\ntask2\n\n\n\ntask1-&gt;task2\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask1-&gt;task3\n\n\n\n\n\ntask4\n\ntask4\n\n\n\ntask2-&gt;task4\n\n\n\n\n\ntask3-&gt;task4\n\n\n\n\n\ntask6\n\ntask6\n\n\n\ntask4-&gt;task6\n\n\n\n\n\ntask5\n\ntask5\n\n\n\ntask5-&gt;task4\n\n\n\n\n\ntask8\n\ntask8\n\n\n\ntask6-&gt;task8\n\n\n\n\n\ntask7\n\ntask7\n\n\n\ntask7-&gt;task6\n\n\n\n\n\n\n\n\n\n\n\n\n\n방법1 : 모든 경우의 수에 대해서 연결 가능하지만 가독성 떨어짐\n\ntask1 &gt;&gt; task2\ntask1 &gt;&gt; task3\ntask2 &gt;&gt; task4\ntask3 &gt;&gt; task4\ntask5 &gt;&gt; task4\ntask4 &gt;&gt; task6\ntask7 &gt;&gt; task6\ntask6 &gt;&gt; task8\n\n방법2: 같은 레벨의 tasks는 list로 묶어 준다. 가독성이 높지만 구현이 안되는 경우 있을 수 있음\n\ntask1 &gt;&gt; [task2, task3] &gt;&gt; task4\ntask5 &gt;&gt; task4\n[task4, task7] &gt;&gt; task6 &gt;&gt; task8\n\n방법3: 역방향은 &lt;&lt;를 이용 (권장 하지 않음)\n\ntask1 &gt;&gt; [task2, task3] &gt;&gt; task4 &lt;&lt; task5\ntask4 &gt;&gt; task 6 &lt;&lt; task7\ntask6 &gt;&gt; task8\n\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.empty import EmptyOperator\n#EmptyOperator는 어떤 연산도 하지 않는 class\n\nwith DAG(\n    dag_id=\"dags_task_connection\",\n    schedule=None,\n    start_date=pendulum.datetime(2023,3,1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # 8개의 instances: task1~task8\n    task1=EmptyOperator(\n        task_id='task1'\n    )\n    task2=EmptyOperator(\n        task_id='task2'\n    )\n    task3=EmptyOperator(\n        task_id='task3'\n    )\n    task4=EmptyOperator(\n        task_id='task4'\n    )\n    task5=EmptyOperator(\n        task_id='task5'\n    )\n    task6=EmptyOperator(\n        task_id='task6'\n    )\n    task7=EmptyOperator(\n        task_id='task7'\n    )\n    task8=EmptyOperator(\n        task_id='task8'\n    )\n  \n  task1 &gt;&gt; [task2, task3] &gt;&gt; task4\n  task5 &gt;&gt; task4\n  [task4, task7] &gt;&gt; task6 &gt;&gt; task8\n\n\n\n\n\nReference: Airflow Official Document\n\nContent/Core Concepts/DAGs 참고\nDAGs에 대한 숙련도가 올라가면 이 링크를 참고하면 매우 유용\n\nDAG을 어떤 상황에서 어떻게 짜야하는지에 대한 guidance가 자세히 적혀 있음\n예를 들어, dag을 생성하는 방법 (dag declaration)에는 with 문을 사용하는 방법과 standard constructor (표준 생성자)를 사용하는 방법이 있음\n\nwith statement\n\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\n\nwith DAG(\n    dag_id=\"my_dag_name\",\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@daily\",\n):\nEmptyOperator(task_id=\"task\")\n\nstandard constructor (class)\n\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\n\n#class 생성\nmy_dag = DAG( \n    dag_id=\"my_dag_name\",\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@daily\",\n)\nEmptyOperator(task_id=\"task\", dag=my_dag)\n\npython의 decorator기능 활용 (dag decorator to turn a function into a DAG generator)\n\nimport datetime\n\nfrom airflow.decorators import dag\nfrom airflow.operators.empty import EmptyOperator\n\n\n@dag(start_date=datetime.datetime(2021, 1, 1), schedule=\"@daily\")\ndef generate_dag():\n    EmptyOperator(task_id=\"task\")\n\n\ngenerate_dag()\n\n\ntask dependencies 설정을 위한 emplicit methods.\n\nset_upstream and set_downstream\n\nfirst_task.set_downstream(second_task, third_task)\nthird_task.set_upstream(fourth_task)\n\ncross_downstream\n\nfrom airflow.models.baseoperator import cross_downstream\n\n#Replaces\n#[op1, op2] &gt;&gt; op3\n#[op1, op2] &gt;&gt; op4\ncross_downstream([op1, op2], [op3, op4])\n\nchain\n\nfrom airflow.models.baseoperator import chain\n\n#Replaces op1 &gt;&gt; op2 &gt;&gt; op3 &gt;&gt; op4\nchain(op1, op2, op3, op4)\n\n#You can also do it dynamically\nchain(*[EmptyOperator(task_id='op' + i) for i in range(1, 6)])\n\n#or\n\nfrom airflow.models.baseoperator import chain\n\n#Replaces\n#op1 &gt;&gt; op2 &gt;&gt; op4 &gt;&gt; op6\n#op1 &gt;&gt; op3 &gt;&gt; op5 &gt;&gt; op6\nchain(op1, [op2, op3], [op4, op5], op6)\n\n\n\n\n\n\n\n외부 script file such as *.py and *.sh 은 docker가 인식할 수 있도록 docker의 plugins directory안에 넣어줘야 실행된다.\n\n\n\n\nUnix/Linux Shell 명령어로 적혀진 파일로 인터프리터에 의해 한 줄씩 처리된다.\n\ninterpreter: CPU가 programming 언어를 처리하는데 크게 compiling 방식과 interpreting 방식 2가지 방식이 있다.\n\ncompiling\n\nprogramming language를 목적 코드인 2진수로 처리한다음 읽음\ncompile 할 때 연산 시간은 다소 소요되지만 한 번 compile 된 script는 실행 속도가 매우 빠름\nC, Java\n\ninterpreting: compiling없이 한줄씩 읽는 방식\n\ncompiling방식에 비해 실행 속도가 느림\npython, shell\n\n\n\nbashOperator를 이용하여 shell script 처리\nEcho, mkdir, cd, cp, tar, touch 등의 기본적인 쉘 명령어를 입력하여 작성하며 변수를 입력받거나 For 문, if 문 그리고 함수도 사용 가능\n확장자가 없어도 동작하지만 주로 파일명에 .sh 확장자를 붙인다.\n\n\n\n\n\nbashOperator를 이용하다면 bashOperator안에 shell 명령어들을 써서 넣어도 동작은 하지만\n쉘 명령어를 이용하여 복잡한 로직을 처리하는 경우 shell script를 이용하는 것이 좋다\n\n예를들어, sftp (source sever)를 통해 csv나 json같은 파일을 받은 후 전처리하여 DB에 Insert & tar.gz으로 압축하고 싶을때, 이렇게 복잡한 tasks를 bashOperator에 모두 기입하기 보다는 script를 짜서 bashOperator에서 호출하는 방식이 가독성이나 유지보수 측면에서 더 효율적이다.\n\n쉘 명령어 재사용을 위한 경우\n\n위의 예시를 server 100대에 대하여 반복 수행할 때 logic이 같으면 shell script를 100번 호출하는 것이 더 간편\nsftp: 접속할 때 IP, Port, account, pw 가 필요한데 이런 것을 변수화 시키고 DB전처리 로직을 shell script에 짜 놓으면 됨.\n\n\n\n\n\n\n문제점\n\n컨테이너는 외부의 파일을 인식할 수 없다. shell script를 wsl directory 어딘가에 넣으면 container가 인식을 못함.\n컨테이너 안에 파일을 만들어주면 컨테이너 재시작시 파일이 사라진다. docker에서 이미지를 띄우는 것을 container를 만들었다라고 하는데 container 재 실행시 초기화 되어 실행된다. (docker의 특징). 그래서 컨테이너 안에 shell script 파일 넣어도 재시작시 삭제가 됨.\n\n해결방법\n\n\n빨간 네모박스의 plugins에 shell script를 저장한다. airflow document에서는 customized python and shell script를 plugins에 저장하는 것을 권장\n\nexample\n\ncd github-repository/plugins/shell\nvi select_fruit.sh #i 누르면 편집가능하고 편집 후 esc+wq! 입력하고 enter치면 저장하고 나감\nchmod +x select_fruit.sh #실행 권한을 부여\n./select_fruit.sh kmkim # ./test2.sh 는 test2.sh을 실행한다는 의미 출력물: kmkim 출력됨\ngit add -A\ngit commit -m \"shell script example\"\ngit push\n# echo $1 #첫 번째 인수 출력\n\nFRUIT=$1\nif [ $FRUIT == APPLE ]; then\n  echo \"You selected Apple!\"\nelif [ $FRUIT == ORANGE ]; then\n  echo \"You selected Orange!\"\nelif [ $FRUIT == Grape ]; then\n  echo \"You selected Grape!\"\nelse \n  echo \"You selected other Fruit!\"\nfi\n\ncontainer에서 github repository에 있는 plugins/shell에 있는 shell script 인식하게 하기\n\nvi docker-compose.yaml 에서 67line 수정\nvolumes:\n  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n\nselect_fruit.sh 실행 권한 부여\n\n아래와 같이 6번의 task 수행 실패가 발생했는데 처음엔 volumne의 path 설정이 잘못 됐는지 알고 계속 docker-compose.yaml을 살펴봤다. 하지만 이상이 없는 것을 확인하고 task의 log를 확인해 봤는데 다음과 같은 error가 뜬것을 확인할 수 있었다.\n\n\n\nexecution error\n\n\n/bin/bash: line 1: /opt/***/plugins/shell/select_fruit.sh: Permission denied\n이럴 땐 다음과 같이 실행권한을 부여하게 되면 해결된다.\n(airflow) kmkim@K100230201051:~/airflow/plugins/shell$ chmod +x select_fruit.sh\n\n\n\n\n\n이메일 전송해주는 오퍼레이터\nemail_t1 = EmailOperator(\n  task_id='email_t1',\n  to='hjkim_sun@naver.com',\n  subject='Airflow 처리결과',\n  html_content='정상 처리되었습니다.'\n)\n구글 메일 서버 사용\n\n\n\n\n\n\n이메일 전송을 위해 사전 셋팅 작업 필요(Google)\n\ngoogle mail server사용\ngmail &gt;&gt; settings(설정) &gt;&gt; See all settings (모든 설정 보기) &gt;&gt; Forwarding and POP/IMAP (전달 및 POP/IMAP) &gt;&gt; IMAP access (IMAP 접근): Enable IMAP (IMAP 사용)\nManage Your Google Acccount (구글 계정 관리) &gt;&gt; Security (보안) &gt;&gt; 2-Step Verification (2단계 인증) &gt;&gt; App Passwords: 앱비밀번호 setting &gt;&gt; select app: Mail , Select device: Windows Computer &gt;&gt; Generate app pasword message window popped up\n\n\n\n\n\n\n사전 설정 작업 (airflow)\n\ndocker-compose.yaml 편집 (environment 항목에 추가)\n\n# 띄어쓰기 주의\nAIRFLOW__SMTP__SMTP_HOST: 'smtp.gmail.com'  \nAIRFLOW__SMTP__SMTP_USER: '{gmail 계정}'\nAIRFLOW__SMTP__SMTP_PASSWORD: '{앱비밀번호}'\nAIRFLOW__SMTP__SMTP_PORT: 587\nAIRFLOW__SMTP__SMTP_MAIL_FROM: '{gmail 계정}' # 이메일을 누가 보내는 것으로 할건지 정함\n\n\n\n\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.email import EmailOperator\n\nwith DAG(\n    dag_id=\"dags_email_operator\",\n    schedule=\"0 8 1 * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    sending_email_task=EmailOperator(\n        task_id='sending_email_task',\n        to='sdf@naver.com',\n        cc=['sdf2@gmail.com', 'sdf3@gmail.com'],\n        subject='Airflow Test',\n        html_content= \"\"\"\n            this is a test for airflow.&lt;br/&gt;&lt;br/&gt;\n            \n            {{ ds }}&lt;br/&gt;\n        \"\"\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#airflow-dag-생성",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#airflow-dag-생성",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "G\n\n\ncluster0\n\nDAG\n\n\n\nBash_Operator\n\nBash_Operator\n\n\n\nTask1\n\nTask1\n\n\n\nBash_Operator-&gt;Task1\n\n\n\n\n\nPython_Operator\n\nPython_Operator\n\n\n\nTask2\n\nTask2\n\n\n\nPython_Operator-&gt;Task2\n\n\n\n\n\nS3_Operator\n\nS3_Operator\n\n\n\nTask3\n\nTask3\n\n\n\nS3_Operator-&gt;Task3\n\n\n\n\n\nGCS_Operator\n\nGCS_Operator\n\n\n\nTask4\n\nTask4\n\n\n\nGCS_Operator-&gt;Task4\n\n\n\n\n\n\n\n\n\n\n\nworkflow = DAG\nOpeartor\n\n특정 행위를 할 수 있는 기능을 모아 놓은 클래스 또는 설계도\n\nTask\n\noperator가 객체화(instantiation)되어 DAG에서 실행 가능한 object\n방향성을 갖고 순환되지 않음 (DAG)\n\nBash Operator\n\nLinux에서 shell script 명령을 수행하는 operator\n\nPython Operator\n\npython 함수를 실행하는 operator\n\nS3 Operator\n\nAWS의 S3 solution (object storage)을 control할 수 있는 operator\n\nGCS Operator\n\nGCP의 GCS solution (object storage)을 control할 수 있는 operator\n\noperators을 사용하여 dags을 작성하여 git을 통해 배포한다.\ndag 작성 및 배포\n  from __future__ import annotations\n\n  import datetime # python에는 datatime이라는 data type이 있음\n  import pendulum # datetime data type을 처리하는 library\n\n  from airflow import DAG\n  from airflow.operators.bash import BashOperator\n  from airflow.operators.empty import EmptyOperator\n\n  with DAG(\n      dag_id=\"dags_bash_operator\", \n      # airflow service web 상에서 보여지는 이름, python file명과는 무관하지만 \n      # 실무에서는 일반적으로 python 파일명과 dag_id는 일치시키는 것이 다수의 dags 관리에 편리하다.\n      schedule=\"0 0 * * *\", # \"분 시 일 월 요일\", cron schedule로서 매일 0분 0시에 실행\n      start_date=pendulum.datetime(2023, 6, 9, tz=\"Asia/Seoul\"), #dags이 언제 실행될지 설정\n      # UTC: 세계 표준시로 한국 보다 9시간이 느림. Asia/Seoul로 변경해야 지정한 날짜에 0분 0시에 실행될 수 있다.\n      catchup=False, # start_date를 현재보다 과거로 설정하게 될 경우 \n      # catchup=True면 과거 부터 현재까지 소급해서 실행. \n      # 시간 순서대로 실행하는게 아니라 병렬로 한번에 실행하기 때문에 메모리를 많이 잡아먹을 수 있음. \n      # 그래서 보통 False로 처리. catchup=False면 현재부터만 실행\n      # dagrun_timeout=datetime.timedelta(minutes=60), # dag이 60분 이상 구동시 실패가 되도록 설정\n      # tags=[\"example\", \"example2\"], #airflow service web browser상 dag의 tag를 의미. 즉 dag id 바로 밑 파란색 박스를 의미. tag를 누르면 같은 tag를 가진 dags들만 filtering돼서 선택됨 \n      ## dags 이 수 백개가 될 때 tag로 filtering 하면 용이함 \n      # params={\"example_key\": \"example_value\"}, # as dag: 이하 tasks를 정의할 때, \n      ## tasks에 공통 passing parameters가 있을 때 씀\n  ) as dag:\n      # [START how to_operator_bash]\n      bash_task1 = BashOperator(\n          task_id=\"bash_task1\", # airflow web service의 dag graph에 표시될 task명\n          # task역시 task object name (bash_task1)과 task_id(bash_task1)를 일치시키는 것이 좋음\n          bash_command=\"echo this task works well!\",\n          # bash_command 이하는 shell script를 적어주면 됨\n      )\n      # [END how to_operator_bash]\n      bash_task2 = BashOperator(\n          task_id=\"bash_task2\",  \n          bash_command=\"echo $HOSTNAME\", # $HOSTNAME: HOSTNAME 환경변수 호출\n          # WSL terminal 이름이 출력된다.\n      )\n      bash_task1 &gt;&gt; bash_task2 # 수행될 tasks의 관계 설정\n배포된 dags을 airflow containers과 연결 시키기 위해 docker-compose.yaml 실행\n\nvi docker-compose.yaml 실행 후 docker-compose.yaml 안에서 Volumns 항목이 wsl의 directory와 container directory를 연결(mount)해주는 요소\n\nVolumes\n  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n\n위와 같이 Volumns 항목이 뜨는데 :을 기준으로 왼쪽이 WSL directories(volumns), 오른쪽이 Docker container directories(volumns)\n다른 WSL창을 열어 echo ${AIRFLOW_PROJ_DIR:-.} 실행하면 AIRFLOW_PROJ_DIR에 값이 없기 때문에 . 출력됨\n\nAIRFLOW_PROJ_DIR:-. : shell script문법으로 AIRFLOW_PROJ_DIR에 값이 있으면 출력하고 없으면 .을 출력하라는 의미\necho AIRFLOW_PROJ_DIR: 아무것도 출력 안됨\n\n${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags는 ./dags를 /opt/airflow/dags에 연결시키라는 의미\n\n./: docker-compose.yaml이 위치하고있는 현재 directory를 의미\n\n배포된 dags를 자동으로 docker container에 연동시키기 위해 Volumns을 다음과 같이 편집\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n\ndirectory hierarchy에 따라 위의 volumes path를 다르게 설정해야한다. docker service web browse(i.e. localhost:8080) 껏다 켜가면서 확인하면서 설정\n\n새로운 dags 배포할 때마다 airflow service 껐다가 켜야 한다.\n\nairflow service 껐다 켜서 잘 반영됐는지 확인\n\ndocker가 설치된 wsl directory이동 먼저 할 것\nairflow service 끄기: sudo docker compose down\nairflow service 켜기: sudo docker compose up\n\nairflow web service상에서 dags이 잘 mount 되었는지 확인\n\n기본적으로 dags은 airflow web service상에 올라올 때 unpaused 상태로 올라옴\n하지만 schedule이 걸려있는 dags은 unpaused상태에서 한번 돌고 올라옴\ndag을 클릭하면 긴 녹색 막대기를 누르면 수행된 schedule내용이 나오고\n각 각의 task에 대응되는 녹색 네모 박스를 누르면 결과들을 조회할 수 있다.\n\n네모 박스를 누르고 log (audit log 아님)를 누르면 결과가 자세히 조회된다.\nbash_task2 의 bash_command=\"echo $HOSTNAME\" 의 결과값으로 조회된 값은 docker worker container id 를 의미한다.\n\n하지만 본인의 경우, airflow web service에서 794f3b56824a가 출력된 것을 확인했고\nsudo docker ps로 container ID를 확인한 결과 airflow-airflow-worker-1 의 32092b201878 로 달랐다.\n\n실제 worker container로 들어가 echo $HOSTNAME 실행하면 worker container id 출력되어야 함\n\nworker container로 들어가기: sudo docker exec -it container-name bash \\(\\rightarrow\\) 본인의 경우: sudo docker exec -it airflow-airflow-worker-1 bash 이 과정이 dag을 돌린과정과 같은 mechanism임\necho $HOSTNAME 실행 : 32092b201878 출력됨 (어쨌든 airflow web service상의 794f3b56824a와 달랐음)\nsudo docker exec -it 794f3b56824a bash 결과 Error response from daemon: No such container: 794f3b56824a 라는 에러메세지 뜸\n\n즉, worker container가 실제 task를 처리하는 것을 볼 수 있었다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#subject-of-task-performance",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#subject-of-task-performance",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "G\n\n\ncluster0\n\nTask Process\n\n\n\nScheduler\n\nScheduler\n\n\n\nScheduler-&gt;Scheduler\n\n\n3.check start time\n\n\n\nDAG_file\n\nDAG_file\n\n\n\nScheduler-&gt;DAG_file\n\n\n1.parsing\n\n\n\nQueue\n\nQueue\n\n\n\nScheduler-&gt;Queue\n\n\n\n\n\nMeta_DB\n\nMeta_DB\n\n\n\nScheduler-&gt;Meta_DB\n\n\n2.save information\n\n\n\nWorker\n\nWorker\n\n\n\nDAG_file-&gt;Worker\n\n\n5.Processing after reading\n\n\n\nWorker-&gt;Meta_DB\n\n\n6.Results update\n\n\n\nQueue-&gt;Worker\n\n\n4.start instruction\n\n\n\n\n\n\n\n\n\nscheduler\n\nairflow에서 brain역할\n\nparsing: a user가 만든 dag 파일을 읽어들여 문법적 오류 여부와 tasks 간의 관계를 분석\nsave information: DAG Parsing 후 DB에 정보저장 (tasks, task relations, schedule, etc.)\ncheck start time: DAG 시작 실행 시간 확인\nstart instruction: DAG 시작 실행 시간마다 worker에 실행 지시\n\n\nscheduler와 workder 사이에 queue 상태가 있을 수 있음\n\n\nworker (Worker Container)\n\nairflow 처리 주체 (subject)\n\nProcessing after reading: scheduler가 시킨 DAG 파일을 찾아 읽고 처리\nResults update: 처리가 되기 전/후를 Meta DB에 update함"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#cron-scheduling",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#cron-scheduling",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "task가 실행되어야 하는 시간(주기)을 정하기 위한 다섯개의 필드로 구성된 문자열\nCron을 이용하면 왠만한 scheduling 모두 가능\n\n{minutes} {hour} {day} {month} {weekday}\n\n\n\n\n\n\n\n\nNumber\nSpecial Characters\nDescription\n\n\n\n\n1\n*\n모든 값\n\n\n2\n-\n범위 지정\n\n\n3\n,\n여러 값 지정\n\n\n4\n/\n증가값 지정. staring-value/ending-value\n\n\n5\nL\n마지막 값 (일, 요일에만 설정 가능)  * 일에 L 입력시 해당 월의 마지막 일 의미  ※ 요일에 L 입력시 토요일 의미\n\n\n6\n#\n몇 번째 요일인지 지정\n\n\n\n\n\n\n\n\n\n\n\nCron schedule\nDescription\nNote\n\n\n\n\n15 2 * * *\n매일 02시 15분에 도는 daily batch\n\n\n\n0 * * * *\n매시 정각에 도는 시간 단위 batch\n\n\n\n0 0 1 * *\n매월 1일 0시 0분 도는 monthly batch\n\n\n\n10 1 * * 1\n매주 월요일 1시 10분에 도는 weekly batch\n0: 일요일, 1: 월요일, 2: 화요일, 3:수요일, 4: 목요일, 5: 금요일, 6: 토요일\n\n\n0 9-18 * * *\n매일 9시부터 18시까지 정각마다 도는 daily batch\n보통 이렇게 scheduling하지는 않음. 하지만 구현할 수 있음\n\n\n0 1 1,2,3 * *\n매월 1일, 2일 3일만 1시에 도는 monthly batch\n보통 이렇게 scheduling하지는 않음. 하지만 구현할 수 있음\n\n\n*/30 * * *\n삼십분마다 (0분, 30분)\n\n\n\n10-59/30 * * * *\n10분부터 삼십분마다 (10분, 40분에 도는 작업)\n\n\n\n10 1 * * 1-5\n평일만 01시 10분\n\n\n\n0 */2 * * *\n2시간 마다 (0시, 02시, 04시 …)\n1-23/2: 1시부터 2시간 마다\n\n\n0 0 */2 * *\n짝수일 0시 0분\n\n\n\n10 1 L * *\n매월 마지막 일 01시 10분에 도는 montly batch\n빈번하게 사용되는 schedule\n\n\n10 1 * * 6#3\n매월 세 번째 토요일 01시 10분 도는 montly batch"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#task-connection-methods",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#task-connection-methods",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "Task 연결 방법 종류\n\n&gt;&gt;, &lt;&lt; 사용하기 (Airflow 공식 추천방식)\n함수 사용하기\n\n복잡한 Task 는 어떻게 연결하는가?\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Connection\n\n\n\ntask1\n\ntask1\n\n\n\ntask2\n\ntask2\n\n\n\ntask1-&gt;task2\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask1-&gt;task3\n\n\n\n\n\ntask4\n\ntask4\n\n\n\ntask2-&gt;task4\n\n\n\n\n\ntask3-&gt;task4\n\n\n\n\n\ntask6\n\ntask6\n\n\n\ntask4-&gt;task6\n\n\n\n\n\ntask5\n\ntask5\n\n\n\ntask5-&gt;task4\n\n\n\n\n\ntask8\n\ntask8\n\n\n\ntask6-&gt;task8\n\n\n\n\n\ntask7\n\ntask7\n\n\n\ntask7-&gt;task6\n\n\n\n\n\n\n\n\n\n\n\n\n\n방법1 : 모든 경우의 수에 대해서 연결 가능하지만 가독성 떨어짐\n\ntask1 &gt;&gt; task2\ntask1 &gt;&gt; task3\ntask2 &gt;&gt; task4\ntask3 &gt;&gt; task4\ntask5 &gt;&gt; task4\ntask4 &gt;&gt; task6\ntask7 &gt;&gt; task6\ntask6 &gt;&gt; task8\n\n방법2: 같은 레벨의 tasks는 list로 묶어 준다. 가독성이 높지만 구현이 안되는 경우 있을 수 있음\n\ntask1 &gt;&gt; [task2, task3] &gt;&gt; task4\ntask5 &gt;&gt; task4\n[task4, task7] &gt;&gt; task6 &gt;&gt; task8\n\n방법3: 역방향은 &lt;&lt;를 이용 (권장 하지 않음)\n\ntask1 &gt;&gt; [task2, task3] &gt;&gt; task4 &lt;&lt; task5\ntask4 &gt;&gt; task 6 &lt;&lt; task7\ntask6 &gt;&gt; task8\n\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.empty import EmptyOperator\n#EmptyOperator는 어떤 연산도 하지 않는 class\n\nwith DAG(\n    dag_id=\"dags_task_connection\",\n    schedule=None,\n    start_date=pendulum.datetime(2023,3,1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # 8개의 instances: task1~task8\n    task1=EmptyOperator(\n        task_id='task1'\n    )\n    task2=EmptyOperator(\n        task_id='task2'\n    )\n    task3=EmptyOperator(\n        task_id='task3'\n    )\n    task4=EmptyOperator(\n        task_id='task4'\n    )\n    task5=EmptyOperator(\n        task_id='task5'\n    )\n    task6=EmptyOperator(\n        task_id='task6'\n    )\n    task7=EmptyOperator(\n        task_id='task7'\n    )\n    task8=EmptyOperator(\n        task_id='task8'\n    )\n  \n  task1 &gt;&gt; [task2, task3] &gt;&gt; task4\n  task5 &gt;&gt; task4\n  [task4, task7] &gt;&gt; task6 &gt;&gt; task8\n\n\n\n\n\nReference: Airflow Official Document\n\nContent/Core Concepts/DAGs 참고\nDAGs에 대한 숙련도가 올라가면 이 링크를 참고하면 매우 유용\n\nDAG을 어떤 상황에서 어떻게 짜야하는지에 대한 guidance가 자세히 적혀 있음\n예를 들어, dag을 생성하는 방법 (dag declaration)에는 with 문을 사용하는 방법과 standard constructor (표준 생성자)를 사용하는 방법이 있음\n\nwith statement\n\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\n\nwith DAG(\n    dag_id=\"my_dag_name\",\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@daily\",\n):\nEmptyOperator(task_id=\"task\")\n\nstandard constructor (class)\n\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\n\n#class 생성\nmy_dag = DAG( \n    dag_id=\"my_dag_name\",\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@daily\",\n)\nEmptyOperator(task_id=\"task\", dag=my_dag)\n\npython의 decorator기능 활용 (dag decorator to turn a function into a DAG generator)\n\nimport datetime\n\nfrom airflow.decorators import dag\nfrom airflow.operators.empty import EmptyOperator\n\n\n@dag(start_date=datetime.datetime(2021, 1, 1), schedule=\"@daily\")\ndef generate_dag():\n    EmptyOperator(task_id=\"task\")\n\n\ngenerate_dag()\n\n\ntask dependencies 설정을 위한 emplicit methods.\n\nset_upstream and set_downstream\n\nfirst_task.set_downstream(second_task, third_task)\nthird_task.set_upstream(fourth_task)\n\ncross_downstream\n\nfrom airflow.models.baseoperator import cross_downstream\n\n#Replaces\n#[op1, op2] &gt;&gt; op3\n#[op1, op2] &gt;&gt; op4\ncross_downstream([op1, op2], [op3, op4])\n\nchain\n\nfrom airflow.models.baseoperator import chain\n\n#Replaces op1 &gt;&gt; op2 &gt;&gt; op3 &gt;&gt; op4\nchain(op1, op2, op3, op4)\n\n#You can also do it dynamically\nchain(*[EmptyOperator(task_id='op' + i) for i in range(1, 6)])\n\n#or\n\nfrom airflow.models.baseoperator import chain\n\n#Replaces\n#op1 &gt;&gt; op2 &gt;&gt; op4 &gt;&gt; op6\n#op1 &gt;&gt; op3 &gt;&gt; op5 &gt;&gt; op6\nchain(op1, [op2, op3], [op4, op5], op6)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#what-is-shell-script",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#what-is-shell-script",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "Unix/Linux Shell 명령어로 적혀진 파일로 인터프리터에 의해 한 줄씩 처리된다.\n\ninterpreter: CPU가 programming 언어를 처리하는데 크게 compiling 방식과 interpreting 방식 2가지 방식이 있다.\n\ncompiling\n\nprogramming language를 목적 코드인 2진수로 처리한다음 읽음\ncompile 할 때 연산 시간은 다소 소요되지만 한 번 compile 된 script는 실행 속도가 매우 빠름\nC, Java\n\ninterpreting: compiling없이 한줄씩 읽는 방식\n\ncompiling방식에 비해 실행 속도가 느림\npython, shell\n\n\n\nbashOperator를 이용하여 shell script 처리\nEcho, mkdir, cd, cp, tar, touch 등의 기본적인 쉘 명령어를 입력하여 작성하며 변수를 입력받거나 For 문, if 문 그리고 함수도 사용 가능\n확장자가 없어도 동작하지만 주로 파일명에 .sh 확장자를 붙인다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#why-to-need-shell-script",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#why-to-need-shell-script",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "bashOperator를 이용하다면 bashOperator안에 shell 명령어들을 써서 넣어도 동작은 하지만\n쉘 명령어를 이용하여 복잡한 로직을 처리하는 경우 shell script를 이용하는 것이 좋다\n\n예를들어, sftp (source sever)를 통해 csv나 json같은 파일을 받은 후 전처리하여 DB에 Insert & tar.gz으로 압축하고 싶을때, 이렇게 복잡한 tasks를 bashOperator에 모두 기입하기 보다는 script를 짜서 bashOperator에서 호출하는 방식이 가독성이나 유지보수 측면에서 더 효율적이다.\n\n쉘 명령어 재사용을 위한 경우\n\n위의 예시를 server 100대에 대하여 반복 수행할 때 logic이 같으면 shell script를 100번 호출하는 것이 더 간편\nsftp: 접속할 때 IP, Port, account, pw 가 필요한데 이런 것을 변수화 시키고 DB전처리 로직을 shell script에 짜 놓으면 됨."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#worker-컨테이너가-쉘-스크립트를-수행하려면",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#worker-컨테이너가-쉘-스크립트를-수행하려면",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "문제점\n\n컨테이너는 외부의 파일을 인식할 수 없다. shell script를 wsl directory 어딘가에 넣으면 container가 인식을 못함.\n컨테이너 안에 파일을 만들어주면 컨테이너 재시작시 파일이 사라진다. docker에서 이미지를 띄우면 container라 하는데 container 재 실행시 초기화 되어 실행된다.\n\n해결방법\n\n\n빨간 네모박스의 plugins에 shell script를 저장한다. airflow document에서는 customized python and shell script를 plugins에 저장하는 것을 권장\n\nexample\n\ncd github-repository/plugins/shell\nvi select_fruit.sh #i 누르면 편집가능하고 편집 후 esc+wq! 입력하고 enter치면 저장하고 나감\nchmod +x select_fruit.sh #실행 권한을 부여\n./select_fruit.sh kmkim # ./test2.sh 는 test2.sh을 실행한다는 의미 출력물: kmkim 출력됨\ngit add -A\ngit commit -m \"shell script example\"\ngit push\necho $1 #첫 번째 인수 출력\n\ncontainer에서 github repository에 있는 plugins/shell에 있는 shell script 인식하게 하기\n\nvi docker-compose.yaml 에서 67line 수정\nvolumes:\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow//plugins:/opt/airflow/plugins\n\n\n이메일 전송해주는 오퍼레이터\nemail_t1 = EmailOperator(\n  task_id='email_t1',\n  to='hjkim_sun@naver.com',\n  subject='Airflow 처리결과',\n  html_content='정상 처리되었습니다.'\n)\n구글 메일 서버 사용"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#presetting",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#presetting",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "이메일 전송을 위해 사전 셋팅 작업 필요(Google)\n\ngoogle mail server사용\ngmail &gt;&gt; settings(설정) &gt;&gt; See all settings (모든 설정 보기) &gt;&gt; Forwarding and POP/IMAP (전달 및 POP/IMAP) &gt;&gt; IMAP access (IMAP 접근): Enable IMAP (IMAP 사용)\nManage Your Google Acccount (구글 계정 관리) &gt;&gt; Security (보안) &gt;&gt; 2-Step Verification (2단계 인증) &gt;&gt; App Passwords: 앱비밀번호 setting &gt;&gt; select app: Mail , Select device: Windows Computer &gt;&gt; Generate app pasword message window popped up\n\n\n\n\n\n\n사전 설정 작업 (airflow)\n\ndocker-compose.yaml 편집 (environment 항목에 추가)\n\n# 띄어쓰기 주의\nAIRFLOW__SMTP__SMTP_HOST: 'smtp.gmail.com'  \nAIRFLOW__SMTP__SMTP_USER: '{gmail 계정}'\nAIRFLOW__SMTP__SMTP_PASSWORD: '{앱비밀번호}'\nAIRFLOW__SMTP__SMTP_PORT: 587\nAIRFLOW__SMTP__SMTP_MAIL_FROM: '{gmail 계정}' # 이메일을 누가 보내는 것으로 할건지 정함"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#emailoperator-작성",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#emailoperator-작성",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "from airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.email import EmailOperator\n\nwith DAG(\n    dag_id=\"dags_email_operator\",\n    schedule=\"0 8 1 * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    sending_email_task=EmailOperator(\n        task_id='sending_email_task',\n        to='sdf@naver.com',\n        cc=['sdf2@gmail.com', 'sdf3@gmail.com'],\n        subject='Airflow Test',\n        html_content= \"\"\"\n            this is a test for airflow.&lt;br/&gt;&lt;br/&gt;\n            \n            {{ ds }}&lt;br/&gt;\n        \"\"\"\n    )"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#worker-컨테이너가-외부-스크립트shell를-수행하려면",
    "href": "docs/blog/posts/Engineering/airflow/03.operator_basic.html#worker-컨테이너가-외부-스크립트shell를-수행하려면",
    "title": "Operator Baisc (Bash Operator)",
    "section": "",
    "text": "문제점\n\n컨테이너는 외부의 파일을 인식할 수 없다. shell script를 wsl directory 어딘가에 넣으면 container가 인식을 못함.\n컨테이너 안에 파일을 만들어주면 컨테이너 재시작시 파일이 사라진다. docker에서 이미지를 띄우는 것을 container를 만들었다라고 하는데 container 재 실행시 초기화 되어 실행된다. (docker의 특징). 그래서 컨테이너 안에 shell script 파일 넣어도 재시작시 삭제가 됨.\n\n해결방법\n\n\n빨간 네모박스의 plugins에 shell script를 저장한다. airflow document에서는 customized python and shell script를 plugins에 저장하는 것을 권장\n\nexample\n\ncd github-repository/plugins/shell\nvi select_fruit.sh #i 누르면 편집가능하고 편집 후 esc+wq! 입력하고 enter치면 저장하고 나감\nchmod +x select_fruit.sh #실행 권한을 부여\n./select_fruit.sh kmkim # ./test2.sh 는 test2.sh을 실행한다는 의미 출력물: kmkim 출력됨\ngit add -A\ngit commit -m \"shell script example\"\ngit push\n# echo $1 #첫 번째 인수 출력\n\nFRUIT=$1\nif [ $FRUIT == APPLE ]; then\n  echo \"You selected Apple!\"\nelif [ $FRUIT == ORANGE ]; then\n  echo \"You selected Orange!\"\nelif [ $FRUIT == Grape ]; then\n  echo \"You selected Grape!\"\nelse \n  echo \"You selected other Fruit!\"\nfi\n\ncontainer에서 github repository에 있는 plugins/shell에 있는 shell script 인식하게 하기\n\nvi docker-compose.yaml 에서 67line 수정\nvolumes:\n  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n\nselect_fruit.sh 실행 권한 부여\n\n아래와 같이 6번의 task 수행 실패가 발생했는데 처음엔 volumne의 path 설정이 잘못 됐는지 알고 계속 docker-compose.yaml을 살펴봤다. 하지만 이상이 없는 것을 확인하고 task의 log를 확인해 봤는데 다음과 같은 error가 뜬것을 확인할 수 있었다.\n\n\n\nexecution error\n\n\n/bin/bash: line 1: /opt/***/plugins/shell/select_fruit.sh: Permission denied\n이럴 땐 다음과 같이 실행권한을 부여하게 되면 해결된다.\n(airflow) kmkim@K100230201051:~/airflow/plugins/shell$ chmod +x select_fruit.sh"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#public-data",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#public-data",
    "title": "Content List, Engineering",
    "section": "15 Public data",
    "text": "15 Public data\n\n2024-01-01, Public Data Centers"
  },
  {
    "objectID": "docs/blog/posts/Engineering/public_data/index.html",
    "href": "docs/blog/posts/Engineering/public_data/index.html",
    "title": "Public Data",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\nGovernment and Health Data: datasets related to sociology, economic development, education, health care, and more. datasets from the Indian Government, European Union, UK Government, US Government, and US Bureau of Labor Statistics are typically well-maintained and comprehensive.\nSocioeconomic Data by World Bodies: Organizations like the United Nations, UNICEF, World Health Organization, World Bank, IMF, and the Asian Development Bank provide extensive datasets.\nFinancial Data: Datasets from the National Stock Exchange of India, Reserve Bank of India, NASDAQ, and the New York Stock Exchange offer historical trading data (time-series analysis, stock market trends, and algorithmic trading models).\nComputer Vision Datasets: datasets like ImageNet, COCO, Google’s Open Image Dataset, and LSUN.\nNatural Language Processing (NLP) Datasets: datasets for voice and speech recognition, language translation, and other linguistics-related tasks\n\n\n\n\n\nGoogle Cloud Datasets: datasets hosted on Google Cloud\nMicrosoft Azure Open Datasets: their curated datasets working within the Azure ecosystem for your machine learning projects.\nKaggle: data for its machine learning competitions\nUS Government’s Data (data.gov): the US government data.\nWorld Bank Data: economic and development datasets.\nQuandl: financial and economic dataset (time-series data)\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/public_data/index.html#topic",
    "href": "docs/blog/posts/Engineering/public_data/index.html#topic",
    "title": "Public Data",
    "section": "",
    "text": "Government and Health Data: datasets related to sociology, economic development, education, health care, and more. datasets from the Indian Government, European Union, UK Government, US Government, and US Bureau of Labor Statistics are typically well-maintained and comprehensive.\nSocioeconomic Data by World Bodies: Organizations like the United Nations, UNICEF, World Health Organization, World Bank, IMF, and the Asian Development Bank provide extensive datasets.\nFinancial Data: Datasets from the National Stock Exchange of India, Reserve Bank of India, NASDAQ, and the New York Stock Exchange offer historical trading data (time-series analysis, stock market trends, and algorithmic trading models).\nComputer Vision Datasets: datasets like ImageNet, COCO, Google’s Open Image Dataset, and LSUN.\nNatural Language Processing (NLP) Datasets: datasets for voice and speech recognition, language translation, and other linguistics-related tasks"
  },
  {
    "objectID": "docs/blog/posts/Engineering/public_data/index.html#api",
    "href": "docs/blog/posts/Engineering/public_data/index.html#api",
    "title": "Public Data",
    "section": "",
    "text": "Google Cloud Datasets: datasets hosted on Google Cloud\nMicrosoft Azure Open Datasets: their curated datasets working within the Azure ecosystem for your machine learning projects.\nKaggle: data for its machine learning competitions\nUS Government’s Data (data.gov): the US government data.\nWorld Bank Data: economic and development datasets.\nQuandl: financial and economic dataset (time-series data)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/Conda/01.conda_install.html",
    "href": "docs/blog/posts/Engineering/Conda/01.conda_install.html",
    "title": "Conda Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n1 Conda\n(To be written)\n\n파이썬 가상환경\n\n라이브러리 버전 충돌 방지를 위해 설치/사용되는 파이썬 인터프리터 환경을 격리시키는 기술\n파이썬은 라이브러리 설치 시점에 따라서도 설치되는 버전이 상이한 경우가 많음\n\n\n\n\n가상 환경의 필요성\n\n\n\npython을 global 환경에 설치할 경우 위의 그림처럼 C,D프로젝트가 동시에 진행될 때 둘 중하나의 library version이 차이가 나면 old version의 library 로 진행되는 프로젝트는 에러가 발생함\n\n2개의 다른 프로젝트가 같은 python interpreter를 바라보고 library를 설치하기 때문에 종속성 문제가 생김 (library 충돌 발생)\n그래서 다른 가상환경 venv안에 다른 프로젝트를 할당해서 독립적으로 프로젝트를 진행하는게 일반적임\n\nminiconda install link\n\nLinux: wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nMac OS M1: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\nMac OS Intel: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\n\n설치 스크립트 실행\n\n./Miniconda3-latest-Linux-x86_64.sh\n만약, permission denied error message 뜨면 실행권한 줄것\nchmod +x Miniconda3-latest-Linux-x86_64.sh\n\nLicence 동의, Miniconda 설치 경로 지정, 초기화: 그냥 다 yes 하면 됨\n.bashrc 최신화\n\nsource ~/.bashrc\n\nprompt 재시작하면됨\npython 가상환경 만들기: 보통 가상환경 이름 = 프로젝트 이름\n\nconda create -n {project_name} python={python version}\npython에 있는 가상환경 생성 기능으로 만들 경우 python -m airflow ./venv 실행\n\n./venv directory에 python 설치하고 version 관리하겠다는 의미\n\n\nVScode가 python 가상환경 참조하도록 설정\n\nhelp&gt;show all commands or ctrl+shift+p 누른후 interpreter 입력하여 가상환경에 있는 python 클릭\n\nterminal 에서 가상환경 잘 잡혔는지 확인\n생성된 가상 환경 리스트 보기\n\nconda env list\n\n가상 환경 활성화\n\nconda activate {project_name or 가상환경_name}\n\n\n가상 환경 비활성화\n\nconda deactivate\n\n가상 환경을 바꿀 시 비활성화 해주고 다른 가상환경 활성화\n\n\n가상 환경 삭제\n\nconda remove --name {가상 환경_ 이름} -all\n\n\n\n\n\n\n\n\n\n\n\n\n2 Go to Blog Content List\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html",
    "title": "Python Operator",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nimport libraries\n\nfrom {package name} import {operator or class name}\nex) from airflow.operators.python import PythonOperator\n\n정의된 파이썬 함수를 실행시키는 오퍼레이터\n가장 많이 쓰이는 오퍼레이터\n\n\n\n\n\n\n\n\n\n\n\n\nPackage\nOperator\nImportance\nDescription\n\n\n\n\nairflow.operators.python\nPythonOperator\n***\n어떤 파이썬함수를실행시키기 위한오퍼레이터\n\n\n\nBranchPythonOperator\n*\n파이썬 함수 실행 결과에 따라 task를 선택적으로 실행시킬 때 사용되는 오퍼레이터. task1 &gt;&gt; [task2, task3] 와 같은 상황에서 BranchOperator에서 task1을 실행시키고 task2와 task3 중 하나를 택하여 실행시킬 수 있는 operator. 즉, 조건적으로 task를 실행시키는 operator\n\n\n\nShortCircuitOperator\n\n파이썬 함수 실행 결과에 따라 후행 Task를 실행하지 않고 dag자체를 종료시킬 수 있는 오퍼레이터\n\n\n\nPythonVirtualenvOperator\n\n파이썬 가상환경 생성후 Job 수행하고 마무리되면 가상환경을 삭제해주는 3개의 과정을 실행시켜 주는 오퍼레이터\n\n\n\nExternalPythonOperator\n\n기존에 존재하는 파이썬가상환경에서 Job 수행하게 하는 오퍼레이터\n\n\n\n\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nimport random # for random.randint(0,3)\n\nwith DAG(\n    dag_id=\"dags_python_operator\",\n    schedule=\"30 6 * * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    def select_fruit(): # 실행 시켜줄 함수 선언\n        fruit =['Apple','Kiwi','banana','avocado']\n        rand_int = random.randint(0,3) # 0,1,2,3 4개의 정수 중 하나 return\n        print(fruit[rand_int])\n    py_t1 = PythonOperator(\n        task_id ='py_t1',\n        python_callable=select_fruit\n    )\n    py_t1\n\n\n\n\n\n\n\n예) PyhonOperator의 Import원리\n\nfrom airflow.operators.python import PythonOperator\nAirflow 폴더 아래 operators 폴더 아래 python 파일 아래에서 PythonOperator 클래스를 호출하라는 의미\n\npython이 경로 찾는 방식을 알아놔야 함\n\ndag에서 우리가 만든 외부 함수를 import 해와야 하는데\nimport 경로를 python이 찾을 수 있도록 그 문법에 맞게 작성해야함\n\n파이썬은 sys.path 변수에서 모듈의 위치를 검색\n\npip install로 설치된 libraries\n\ncontainer 안에 들어간 후 설치된 libraries 조회\n\n\n\nconda 환경에 설치된 libraries\n  (airflow) PS C:\\Users\\kmkim\\Desktop\\projects\\airflow&gt; python\n  Python 3.8.18 | packaged by conda-forge | (default, Dec 23 2023, 17:17:17) [MSC v.1929 64 bit (AMD64)] on win32\n  Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n  &gt;&gt;&gt; import sys\n  &gt;&gt;&gt; from pprint import pprint \n  &gt;&gt;&gt; pprint(sys.path) \n  ['',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\python38.zip',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\DLLs',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\lib',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\lib\\\\site-packages']\n  &gt;&gt;&gt;\n\nsys.path 에 값을 추가하는 방법\n\n명시적으로 추가 (ex: sys.path.append(‘/home/kkm’) )\nOS 환경변수 PYTHONPATH 에 값을 추가\n\n\n\n\n\n\n\n\n다행히, 파이썬에서와 같이 귀찮은 방식보다는 Airflow에서는 자동적으로 dags 폴더와 plugins 폴더를 sys.path에 추가함\n\n컨테이너에서 airflow info 명령을 수행해보면 아래 그림과 같은 정보를 확인할 수 있다. \n\nplugins 폴더 이용하기 \n\nplugins까지는 airflow에서 기본적으로 인식하고 있기 때문에 from common.common_func import get_sftp에서와 같이 common부터 경로 써주면 됨.\n\n파이썬 스크립트를 이용하면 좋은 점\n\n공통함수 작성이 가능해지고\n재활용성이 증가하고\nDAG의 가독성이 올라가고\n디버깅에도 용이하다.\n\nPythonOperator 예시\n\ndags_python_import_func.py\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom common.common_func import get_sftp # 여기서 path 가 맞지 않아 local에서 error가 발생\n    # .env 파일을 만들어서 workspace 경로 설정을 해줘야 한다.\nimport random\n\nwith DAG(\n    dag_id=\"dags_python_import_func\",\n    schedule=\"30 6 * * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    get_sftp_task = PythonOperator(\n        task_id ='get_sftp_task',\n        python_callable=get_sftp\n    )\n    get_sftp_task\n\n.env: wsl airflow directory에는 배포될 필요없는 파일이기 때문에 .gitignore에 추가해도 됨\n\nWORKSPACE_FOLDER=C:\\Users\\kmkim\\Desktop\\projects\\airflow #'airflow' directory path입력\nPYTHONPATH=${WORKSPACE_FOLDER}/plugins #'plugins' directory추가\n\n\n\n\n\n\n\n\n\npython에 있는 기능\n데커레이터(Decorator): 장식하다, 꾸미다\nWhat to decorate?: 함수를 장식하다\n기능: python 함수를 좀 더 쉽게 사용할 수 있다. 원래의 함수를 감싸서 (Wrapping) 바깥에 추가 기능을 덧붙이는 방법\n\n파이썬은 함수 안에 함수를 선언하는 것이 가능하고\n함수의 인자로 함수를 전달하는 것이 가능하며\n함수 자체를 리턴하는 것이 가능하다.\n\nwarpping (함수 감싸기 )\n\n파이썬은 함수 안에 함수를 선언하는 것이 가능\n함수의 인자로 함수를 전달하는 것이 가능\n함수 자체를 리턴하는 것이 가능\n\n함수 감싸기 예시\n\n\nget_data() 함수 안에 log를 쌓아야하는 상황\n\ndef get_data()\n    print('function starts working')\n\nlog는 간단하게 여기서 print('target 함수 실행 전 입니다.') 와 print('target 함수 실행 후 입니다.') 라고 설정\nget_data() 함수 안에 log를 쌓는 python code를 작성하는 것이 아니라 get_data() 를 인수로 받는 warpping function, outer_func(target_func)을 만든다\n\ndef outer_func(target_func):\n    def inner_func():\n        print('target 함수 실행 전 입니다.')\n        print('function starts working')\n        print('target 함수 실행 후 입니다.')\n    return inner_func\na=outer_func(get_data) \na() # get_data()을 인수로 받은 inner_func()이 출력된다.\nwrapping 함수의 장점\n\nget_data()와 같은 target function이 수 백개가 되면 수 백개의 log정보를 수 백개의 target functions에 기입해야한다. wrapping 함수를 사용하면 코드의 재사용이 가능해진다.\n\nwrapping 함수의 단점\n\nget_data()와 같은 target function이 수 백개가 되면 a=outer_func(get_data1);a(), a=outer_func(get_data2);a(), \\(\\dots\\),a=outer_func(get_data100);a() 이런식으로 만들어야 함.\n이런 단점을 보완한 것이 decorator\n\n\ndecorator\n\n\n@outer_func 을 target function위에 작성\n\n@outer_func\ndef get_data()\n    print('function starts working')\n\n이렇게, decoration을 하게 되면 get_data1();get_data2();get_data3() 만 실행하면 자동으로 outer_func()이 실행됨\n\n\ntask decorator in airflow\n\nairflow에도 비슷한 기능이 있는데 파이썬 함수 정의만으로 쉽게 Task 생성\n\n\nairflow official document에서는 PythonOperator Import해서 DAG을 만드는 것 보다 task decorator를 사용하는 것을 더 권장\n\n실제로, airflow web service에서 example_python_operator DAG을 보면 example_python_deocartor라고 이름이 안지어졌는데도 decorator 예시가 바로 나옴.\n왜냐면, 나중에 task간에 data를 공유할 때 task decorator를 사용하면 PythonOperator를 사용하면 data 공유가 한결 더 쉬워진다.\ntask decorator를 사용하면 task connection 을 작성할 필요가 없어짐\ntask decorator dag 예시\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"dags_python_decorator\",\n    schedule='0 2 * * 1', #매주 월요일 2시 실행\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False,\n) as dag:\n\n    @task(task_id=\"python_task1\")\n    def print_context(some_input):\n        print(some_input)\n\n    python_task1 = print_context('task decorator execution!') #인수(argument) 추가\n\n\n\n\n\n\n\n일반적인 함수 인자 작성 방법\n\ndef regist(name, sex):\n    print(name)\n    print(sex)\n\nregist('kkm','man')\n\nBut, 호출하는 로직에서 몇 개의 파라미터를 넘길지 모를 때 또는 파라미터의 개수를 제한하지 않으려면?\n또는 선택적으로 변수를 받을 수도 있을 때는? (가령, 주소와 전화번호)\n파이썬 함수에서 인수 형태는 크게 3가지가 있음\n\n일반 변수 형태를 명시적으로 받는 것: name, sex\n*argument 방식: *args\n**keyword_arguement 방식: **kwargs\n\n\n\n\n\ndef regist(name, sex, *args): \n    #name, sex: 필수 정보로 설정\n    #*args: 부가 정보로 설정\n    print(type(args)) #tuple로 되어 있음, tuple에서 값을 꺼낼때는 index로 접근\n    country = args[0] if len(args)&gt;=1 else None #부가 정보가 없을때는 error가 발생하기 때문에 조건문을 달아줌. 즉, args의 길이가 1이상일때만 1번째 값을 꺼냄.\n    city = args[1] if len(args)&gt;=2 else None\n\nregist('gdhong','man') #필수 정보만 입력\nregist('gdhong','man','korea','seoul') #필수 정보 + 부가 정보\n\nregist('gdhong','man') 은 name='gdhong' 과 sex='man'\nregist('gdhong','man','korea','seoul') 은 name='gdhong' 과 sex='man'. 'korea','seoul' 는 *args가 catch한다. 이 경우에, name과 sex는 필수 정보 *args는 부가 정보의 개념\nargs로 들어온 값은 tuple 저장된다.\nargs에서 값을 꺼낼 때는 인덱스를 이용한다 (ex: args[0] , args[1])\nargs라는 이름 외 다른 이름으로 받아도 됨 (ex: some_func(*kk):)\n\n\n\n\ndef some_func(**kwargs):\n    print(type(kwargs)) # dictionary type\n    print(kwargs)\n    name = kwargs.get('name') or '' #dictionary 값을 얻을 때는 get('key value') 씀\n    country = kwargs.get('country') or '' # 'country' key 없으면 none이 반환됨\n    print(f'name:{name}, country:{country}')\n\nsome_func(name=’hjkim’, country=’kr’)\n\n\n\n\n\n\n방어 로직\n\n\n\n\ndict = {'name'='kkm'} 라고 dictionary 선언 후, kkm이라는 dictionary의 value을 꺼낼때, dict['name']로 값을 호출하면 name 이라는 키가 없을 때 에러가 발생.\n이 때 dict.get('name') 으로 시도하면 name 이라는 키가 없을 때 에러나지 않고 None이 반환되어 상대적으로 안전.\ndict.get('name') or '' 의 의미는 name 이라는 키가 있으면 value를 꺼내오고 키가 없으면 빈 문자 열('')을 받는다는 의미.\n\n\n\n\n\n\ndef regist(name, sex, *args, **kwargs):\n    print(name)\n    print(sex)\n    print(args)\n    print(kwargs)\n\nregist('kkm', 'man', 'korea', 'seoul', phone=010, email='kkm@naver.com')\n\n함수의 인수에 잘 작동원리\n\n*,** 가 없는 인수는 name과 sex로 고정 되어 있기 때문에 아무리 많은 인수값이 오더라도 첫 2개는 name과 sex에 무조건 할당 됨.\n나머지 인수값 중 'key':'value' 형태가 아닌 것은 모두 *arg에 할당됨. (즉, ‘korea’, ‘seoul’)\n'key':'value' arugments는 **kwargs에 할당된다. (즉, phone=010, email=‘kkm@naver.com’)\n\n\n\n\n\n\n\ndef regist(name, sex):\n    print(f'이름은 {name}이고 성별은 {sex}입니다')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=['hjkim','man'] #list로 작성되고 regist()의 인수값으로 사용된다. \n    #'hjkim'과 'man'은 각 각 regist()의 name과 sex에 할당된다.\n)\n\n\n\n\n\ndef regist(name, sex, *args):\n    print(name) # string 형태로\n    print(sex) # string 형태로\n    print(args) # tuple 형태로  출력 됨\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=[‘hjkim’,’man’,’kr’,’seoul’] # ‘hjkim’,’man’ 는 name과 sex에 ,’kr’,’seoul’는 *args에 할당된다.\n)\n\n\n\n\ndef regist(*args):\n    print(args) # tuple 형태로  출력 됨\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=[‘hjkim’,’man’,’kr’,’seoul’] # ‘hjkim’,’man’,’kr’,’seoul’는 *args에 할당된다.\n)\n\n\n\n\n\ndef regist(*args):\n    print(args) # tuple 인 ('hjkim','man','kr','seoul')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=['hjkim','man','kr','seoul'] # 모두 *args에 할당된다.\n)\n\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom common.common_func import regist\nwith DAG(\n    dag_id=\"dags_python_with_op_args\",\n    schedule=\"30 6 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    regist_t1 = PythonOperator(\n        task_id='regist_t1',\n        python_callable=regist,\n        op_args=['hjkim','man','kr','seoul']\n    )\n\n    regist_t1\n\n\n\n\n\n\n\ndef regist(name, sex):\n    print(f'이름은 {name}이고 성별은 {sex}입니다')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_kwargs={'name':'hjkim','sex':'man'} #dictonary 형태로 작성\n    # regist(name,sex)의 argument 가 name, sex이고 op_kwargs의 key 값에 regist(name,sex)의 argument를 똑같이 작성해준다.\n)\n\n\n\ndef regist(name, sex, **kwargs):\n    print(name)\n    print(sex)\n    print(kwargs) # dictionary 형태로 출력됨\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_kwargs={'name':'hjkim','sex':'man',\\\n    'country':'kr','city':'seoul'} # op_kwargs의 key값 중 name과 sex는 regist()의 인수명과 일치하므로 자동적으로 연결되어 regist()의 name과 sex에 할당되고 country와 city는 **kwargs에 할당된다.\n)\n\n\n\ndef regist(**kwargs):\n    name=kwargs['name'] or ''\n    sex=kwargs['sex'] or ''\n    country = kwargs['country'] or ''\n    city = kwargs['city'] or ''\n    print(f'name은 {name}이고, \\\n        성별은 {sex}이고, \\\n        국가는 {country} 이고, \\\n        도시는 {city} 입니다.')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_kwargs={'name':'hjkim','sex':'man',\\\n    'country':'kr',city:'seoul'}\n)\n\n\n\ndef regist(name, sex, *args, **kwargs):\n    print(name)\n    print(sex)\n    print(args)\n    print(kwargs)\n\npython_task_2 = PythonOperator(\n    task_id='python_task_2',\n    python_callable=regist,\n    op_args=['hjkim','man','kr','seoul'], #name='hjkim', sex='man',**args=('kr','seoul')\n    op_kwargs={'phone’:010,'email':'hjkim_sun@naver.com'} #**kwargs={'phone’:010,'email':'hjkim_sun@naver.com'}\n)\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom common.common_func import regist2\nwith DAG(\n    dag_id=\"dags_python_with_op_kwargs\",\n    schedule=\"30 6 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    regist2_t1 = PythonOperator(\n        task_id='regist2_t1',\n        python_callable=regist2,\n        op_args=['kkm','man','kr','seoul'],\n        op_kwargs={'email':'kkm@naver.com','phone':'010-9999-9999'}\n    )\n\n    regist2_t1\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-types",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-types",
    "title": "Python Operator",
    "section": "",
    "text": "Package\nOperator\nImportance\nDescription\n\n\n\n\nairflow.operators.python\nPythonOperator\n***\n어떤 파이썬함수를실행시키기 위한오퍼레이터\n\n\n\nBranchPythonOperator\n*\n파이썬 함수 실행 결과에 따라 task를 선택적으로 실행시킬 때 사용되는 오퍼레이터. task1 &gt;&gt; [task2, task3] 와 같은 상황에서 BranchOperator에서 task1을 실행시키고 task2와 task3 중 하나를 택하여 실행시킬 수 있는 operator. 즉, 조건적으로 task를 실행시키는 operator\n\n\n\nShortCircuitOperator\n\n파이썬 함수 실행 결과에 따라 후행 Task를 실행하지 않고 dag자체를 종료시킬 수 있는 오퍼레이터\n\n\n\nPythonVirtualenvOperator\n\n파이썬 가상환경 생성후 Job 수행하고 마무리되면 가상환경을 삭제해주는 3개의 과정을 실행시켜 주는 오퍼레이터\n\n\n\nExternalPythonOperator\n\n기존에 존재하는 파이썬가상환경에서 Job 수행하게 하는 오퍼레이터"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-example",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-example",
    "title": "Python Operator",
    "section": "",
    "text": "from airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nimport random # for random.randint(0,3)\n\nwith DAG(\n    dag_id=\"dags_python_operator\",\n    schedule=\"30 6 * * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    def select_fruit(): # 실행 시켜줄 함수 선언\n        fruit =['Apple','Kiwi','banana','avocado']\n        rand_int = random.randint(0,3) # 0,1,2,3 4개의 정수 중 하나 return\n        print(fruit[rand_int])\n    py_t1 = PythonOperator(\n        task_id ='py_t1',\n        python_callable=select_fruit\n    )\n    py_t1"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-module-path",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-module-path",
    "title": "Python Operator",
    "section": "",
    "text": "from airflow.operators.python import PythonOperator\n\nAirflow 폴더 아래 operators 폴더 아래 python 파일 아래에서 PythonOperator 클래스를 호출하라는 의미\n\npython이 경로 찾는 방식을 알아놔야 함\n\ndag에서 우리가 만든 외부 함수를 import 해와야 하는데\nimport 경로를 python이 찾을 수 있도록 그 문법에 맞게 작성해야함\n\n파이썬은 sys.path 변수에서 모듈의 위치를 검색\n\npip install로 설치된 libraries \nconda 환경에 설치된 libraries\n\n['c:\\\\Users\\\\kmkim\\\\Desktop\\\\projects\\\\website\\\\docs\\\\blog\\\\posts\\\\Engineering\\\\airflow', \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\python39.zip',             \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\DLLs', \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\lib', \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study', \n'', # 실행하는 파이썬 파일과 동일 디렉토리에 있는 파이썬 파일, 이것에 한해서 그냥 모듈명으로만 호출 가능\n    # 예를 들어, a.py 와 b.py와 동일한 디렉토리안에 있다면 a.py안에서 `import b` 라고 해도 호출 가능\n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\lib\\\\site-packages', \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\lib\\\\site-packages\\\\win32', \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\lib\\\\site-packages\\\\win32\\\\lib', \n'c:\\\\Users\\\\kmkim\\\\.conda\\\\envs\\\\study\\\\lib\\\\site-packages\\\\Pythonwin']\nsys.path 에 값을 추가하는 방법\n\n명시적으로 추가 (ex: sys.path.append(‘/home/hjkim’) )\nOS 환경변수 PYTHONPATH 에 값을 추가"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#plugins-directory-이용",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#plugins-directory-이용",
    "title": "Python Operator",
    "section": "",
    "text": "다행히, 파이썬에서와 같이 귀찮은 방식보다는 Airflow에서는 자동적으로 dags 폴더와 plugins 폴더를 sys.path에 추가함\n\n컨테이너에서 airflow info 명령을 수행해보면 아래 그림과 같은 정보를 확인할 수 있다. \n\nplugins 폴더 이용하기 \n\nplugins까지는 airflow에서 기본적으로 인식하고 있기 때문에 from common.common_func import get_sftp에서와 같이 common부터 경로 써주면 됨.\n\n파이썬 스크립트를 이용하면 좋은 점\n\n공통함수 작성이 가능해지고\n재활용성이 증가하고\nDAG의 가독성이 올라가고\n디버깅에도 용이하다.\n\nPythonOperator 예시\n\ndags_python_import_func.py\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom common.common_func import get_sftp # 여기서 path 가 맞지 않아 local에서 error가 발생\n    # .env 파일을 만들어서 workspace 경로 설정을 해줘야 한다.\nimport random\n\nwith DAG(\n    dag_id=\"dags_python_import_func\",\n    schedule=\"30 6 * * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    get_sftp_task = PythonOperator(\n        task_id ='get_sftp_task',\n        python_callable=get_sftp\n    )\n    get_sftp_task\n\n.env: wsl airflow directory에는 배포될 필요없는 파일이기 때문에 .gitignore에 추가해도 됨\n\nWORKSPACE_FOLDER=C:\\Users\\kmkim\\Desktop\\projects\\airflow #'airflow' directory path입력\nPYTHONPATH=${WORKSPACE_FOLDER}/plugins #'plugins' directory추가"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-function-parameter",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-function-parameter",
    "title": "Python Operator",
    "section": "",
    "text": "일반적인 함수 인자 작성 방법\n\ndef regist(name, sex):\n    print(name)\n    print(sex)\n\nregist('kkm','man')\n\nBut, 호출하는 로직에서 몇 개의 파라미터를 넘길지 모를 때 또는 파라미터의 개수를 제한하지 않으려면?\n또는 선택적으로 변수를 받을 수도 있을 때는? (가령, 주소와 전화번호)\n파이썬 함수에서 인수 형태는 크게 3가지가 있음\n\n일반 변수 형태를 명시적으로 받는 것: name, sex\n*argument 방식: *args\n**keyword_arguement 방식: **kwargs"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#arguement-arg",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#arguement-arg",
    "title": "Python Operator",
    "section": "",
    "text": "def regist(name, sex, *args): \n    #name, sex: 필수 정보로 설정\n    #*args: 부가 정보로 설정\n    print(type(args)) #tuple로 되어 있음, tuple에서 값을 꺼낼때는 index로 접근\n    country = args[0] if len(args)&gt;=1 else None #부가 정보가 없을때는 error가 발생하기 때문에 조건문을 달아줌. 즉, args의 길이가 1이상일때만 1번째 값을 꺼냄.\n    city = args[1] if len(args)&gt;=2 else None\n\nregist('gdhong','man') #필수 정보만 입력\nregist('gdhong','man','korea','seoul') #필수 정보 + 부가 정보\n\nregist('gdhong','man') 은 name='gdhong' 과 sex='man'\nregist('gdhong','man','korea','seoul') 은 name='gdhong' 과 sex='man'. 'korea','seoul' 는 *args가 catch한다. 이 경우에, name과 sex는 필수 정보 *args는 부가 정보의 개념\nargs로 들어온 값은 tuple 저장된다.\nargs에서 값을 꺼낼 때는 인덱스를 이용한다 (ex: args[0] , args[1])\nargs라는 이름 외 다른 이름으로 받아도 됨 (ex: some_func(*kk):)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#argument-kwargs",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#argument-kwargs",
    "title": "Python Operator",
    "section": "",
    "text": "def some_func(**kwargs):\n    print(type(kwargs)) # dictionary type\n    print(kwargs)\n    name = kwargs.get('name') or '' #dictionary 값을 얻을 때는 get('key value') 씀\n    country = kwargs.get('country') or '' # 'country' key 없으면 none이 반환됨\n    print(f'name:{name}, country:{country}')\n\nsome_func(name=’hjkim’, country=’kr’)\n\n\n\n\n\n\n방어 로직\n\n\n\n\ndict = {'name'='kkm'} 라고 dictionary 선언 후, kkm이라는 dictionary의 value을 꺼낼때, dict['name']로 값을 호출하면 name 이라는 키가 없을 때 에러가 발생.\n이 때 dict.get('name') 으로 시도하면 name 이라는 키가 없을 때 에러나지 않고 None이 반환되어 상대적으로 안전.\ndict.get('name') or '' 의 의미는 name 이라는 키가 있으면 value를 꺼내오고 키가 없으면 빈 문자 열('')을 받는다는 의미."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#mixed-usage-arg-kwargs",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#mixed-usage-arg-kwargs",
    "title": "Python Operator",
    "section": "",
    "text": "def regist(name, sex, *args, **kwargs):\n    print(name)\n    print(sex)\n    print(args)\n    print(kwargs)\n\nregist('kkm', 'man', 'korea', 'seoul', phone=010, email='kkm@naver.com')\n\n함수의 인수에 잘 작동원리\n\n*,** 가 없는 인수는 name과 sex로 고정 되어 있기 때문에 아무리 많은 인수값이 오더라도 첫 2개는 name과 sex에 무조건 할당 됨.\n나머지 인수값 중 'key':'value' 형태가 아닌 것은 모두 *arg에 할당됨. (즉, ‘korea’, ‘seoul’)\n'key':'value' arugments는 **kwargs에 할당된다. (즉, phone=010, email=‘kkm@naver.com’)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-op_args-usage",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-op_args-usage",
    "title": "Python Operator",
    "section": "",
    "text": "def regist(name, sex):\n    print(f'이름은 {name}이고 성별은 {sex}입니다')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=['hjkim','man'] #list로 작성되고 regist()의 인수값으로 사용된다. \n    #'hjkim'과 'man'은 각 각 regist()의 name과 sex에 할당된다.\n)\n\n\n\n\n\ndef regist(name, sex, *args):\n    print(name) # string 형태로\n    print(sex) # string 형태로\n    print(args) # tuple 형태로  출력 됨\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=[‘hjkim’,’man’,’kr’,’seoul’] # ‘hjkim’,’man’ 는 name과 sex에 ,’kr’,’seoul’는 *args에 할당된다.\n)\n\n\n\n\ndef regist(*args):\n    print(args) # tuple 형태로  출력 됨\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=[‘hjkim’,’man’,’kr’,’seoul’] # ‘hjkim’,’man’,’kr’,’seoul’는 *args에 할당된다.\n)\n\n\n\n\n\ndef regist(*args):\n    print(args) # tuple 인 ('hjkim','man','kr','seoul')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_args=['hjkim','man','kr','seoul'] # 모두 *args에 할당된다.\n)\n\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom common.common_func import regist\nwith DAG(\n    dag_id=\"dags_python_with_op_args\",\n    schedule=\"30 6 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    regist_t1 = PythonOperator(\n        task_id='regist_t1',\n        python_callable=regist,\n        op_args=['hjkim','man','kr','seoul']\n    )\n\n    regist_t1"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python의-module-path-탐색하는-방식의-이해",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python의-module-path-탐색하는-방식의-이해",
    "title": "Python Operator",
    "section": "",
    "text": "예) PyhonOperator의 Import원리\n\nfrom airflow.operators.python import PythonOperator\nAirflow 폴더 아래 operators 폴더 아래 python 파일 아래에서 PythonOperator 클래스를 호출하라는 의미\n\npython이 경로 찾는 방식을 알아놔야 함\n\ndag에서 우리가 만든 외부 함수를 import 해와야 하는데\nimport 경로를 python이 찾을 수 있도록 그 문법에 맞게 작성해야함\n\n파이썬은 sys.path 변수에서 모듈의 위치를 검색\n\npip install로 설치된 libraries\n\ncontainer 안에 들어간 후 설치된 libraries 조회\n\n\n\nconda 환경에 설치된 libraries\n  (airflow) PS C:\\Users\\kmkim\\Desktop\\projects\\airflow&gt; python\n  Python 3.8.18 | packaged by conda-forge | (default, Dec 23 2023, 17:17:17) [MSC v.1929 64 bit (AMD64)] on win32\n  Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n  &gt;&gt;&gt; import sys\n  &gt;&gt;&gt; from pprint import pprint \n  &gt;&gt;&gt; pprint(sys.path) \n  ['',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\python38.zip',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\DLLs',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\lib',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow',\n   'C:\\\\Users\\\\kmkim\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\airflow\\\\lib\\\\site-packages']\n  &gt;&gt;&gt;\n\nsys.path 에 값을 추가하는 방법\n\n명시적으로 추가 (ex: sys.path.append(‘/home/kkm’) )\nOS 환경변수 PYTHONPATH 에 값을 추가"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-op_kwargs-usage",
    "href": "docs/blog/posts/Engineering/airflow/04.python_operator.html#python-operator-op_kwargs-usage",
    "title": "Python Operator",
    "section": "",
    "text": "def regist(name, sex):\n    print(f'이름은 {name}이고 성별은 {sex}입니다')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_kwargs={'name':'hjkim','sex':'man'} #dictonary 형태로 작성\n    # regist(name,sex)의 argument 가 name, sex이고 op_kwargs의 key 값에 regist(name,sex)의 argument를 똑같이 작성해준다.\n)\n\n\n\ndef regist(name, sex, **kwargs):\n    print(name)\n    print(sex)\n    print(kwargs) # dictionary 형태로 출력됨\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_kwargs={'name':'hjkim','sex':'man',\\\n    'country':'kr','city':'seoul'} # op_kwargs의 key값 중 name과 sex는 regist()의 인수명과 일치하므로 자동적으로 연결되어 regist()의 name과 sex에 할당되고 country와 city는 **kwargs에 할당된다.\n)\n\n\n\ndef regist(**kwargs):\n    name=kwargs['name'] or ''\n    sex=kwargs['sex'] or ''\n    country = kwargs['country'] or ''\n    city = kwargs['city'] or ''\n    print(f'name은 {name}이고, \\\n        성별은 {sex}이고, \\\n        국가는 {country} 이고, \\\n        도시는 {city} 입니다.')\n\npython_task = PythonOperator(\n    task_id='python_task',\n    python_callable=regist,\n    op_kwargs={'name':'hjkim','sex':'man',\\\n    'country':'kr',city:'seoul'}\n)\n\n\n\ndef regist(name, sex, *args, **kwargs):\n    print(name)\n    print(sex)\n    print(args)\n    print(kwargs)\n\npython_task_2 = PythonOperator(\n    task_id='python_task_2',\n    python_callable=regist,\n    op_args=['hjkim','man','kr','seoul'], #name='hjkim', sex='man',**args=('kr','seoul')\n    op_kwargs={'phone’:010,'email':'hjkim_sun@naver.com'} #**kwargs={'phone’:010,'email':'hjkim_sun@naver.com'}\n)\n\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom common.common_func import regist2\nwith DAG(\n    dag_id=\"dags_python_with_op_kwargs\",\n    schedule=\"30 6 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    regist2_t1 = PythonOperator(\n        task_id='regist2_t1',\n        python_callable=regist2,\n        op_args=['kkm','man','kr','seoul'],\n        op_kwargs={'email':'kkm@naver.com','phone':'010-9999-9999'}\n    )\n\n    regist2_t1"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html",
    "title": "Template Variabler",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nJinja는 Python에서 사용되는 인기 있는 템플릿 엔진으로 웹 프레임워크인 Flask와 종종 함께 사용되며, Django 템플릿 언어에 영향을 받았음.\nJinja를 사용하면 HTML 파일에 파이썬 코드를 삽입하여 동적인 웹 페이지를 쉽게 만들 수 있음\nJinja는 웹 개발을 더 효율적으로 만들어 주는 강력한 도구로 Python과 Flask 또는 Django와 같은 웹 프레임워크를 사용하는 개발자들에게 널리 사용되고 있음\ntemplate engine\n\n템플릿 엔진은 웹 개발에서 사용되는 소프트웨어 또는 라이브러리로, 프로그래머가 템플릿에 데이터를 삽입하여 동적인 웹 페이지를 생성할 수 있게 해줌\n이러한 엔진의 주요 기능은 템플릿이라고 불리는 특정한 형식의 문서에서 변수들을 실제 값으로 바꾸는 것\n쉽게 말해서 템플릿 엔진은 미리 정의된 문서 틀(템플릿)에 데이터를 채워 넣어 실제 사용자가 볼 수 있는 웹 페이지를 만드는 도구이다\n예시: HTML 템플릿 파일 (예: template.html) + Python Flask 라우트 (예: app.py)\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;{{ title }}&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;{{ heading }}&lt;/h1&gt;\n    &lt;p&gt;Welcome, {{ username }}!&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n이 HTML 파일은 Jinja 템플릿을 사용하여 동적인 데이터를 표시한다. 여기서 {{ title }}, {{ heading }}, {{ username }}은 템플릿에서 치환될 변수들이다.\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return render_template('template.html', title='Home Page', heading='Welcome to My Website', username='Alice')\n\nif __name__ == '__main__':\n    app.run(debug=True)\nFlask 앱에서 render_template 함수를 사용하여 template.html 파일을 렌더링하고, title, heading, username 변수에 값을 전달. 이 값들은 사용자가 최종적으로 보는 웹 페이지에서 해당 위치에 표시됨.\ntemplate engine은 여러 솔루션이 존재하며 그 중 Jinja 템플릿은 파이썬 언어에서 사용하는 엔진\nfrom jinja2 import Template # jinja2 library는 airflow 설치시 자동 설치됨\n\ntemplate = Template('my name is {{name}}') #Template은 class, {{name}}은 변수\nnew_template = template.render('name=kkm')\nprint(new_template)\nJinja 템플릿, 어디서 쓰이나?\n\n파이썬 기반 웹 프레임워크인 Flask, Django(장고)에서 주로 사용\n\n예를 들어, HTML 형태의 정적 template 문서를 만들어 놓고 back end server의 처리 결과에 따라 값을 바꾸어 보여줄 때 jinja template engine이 사용될 수 있다. (주로 HTML 템플릿 저장 후 화면에 보여질 때 실제 값으로 변환해서 출력)\n\nSQL작성시에도 활용 가능\n\n예를 들어, select * from tables where base_dt = {{}} 라는 SQL template을 만들어 넣고 jinja template engine을 이용해서 날짜 변수 {{}}에 runtime시 발생하는 실제 값을 할당할 수 있다. 이 예시는 tables에 있는 데이터가 매일 update될 때 base_dt라는 변수에 따라 데이터를 부분 선택할 수 있게 해준다.\n\n\n\n\n\n\n오퍼레이터 파라미터 입력시 중괄호 {} 2개({{}})를 이용하면 Airflow에서 기본적으로 제공하는 변수들(ex: 수행 날짜, DAG_ID)을 치환된 값으로 입력할 수 있음.\n\nairflow에서 제공하는 기본 variable list or google ‘airflow templates reference’\n{ data_interval_start }: schedule 구간의 시작점을 반환, pendulum.DateTime는 timestamped type (중요)\n{ data_interval_end }: schedule 구간의 끝점을 반환 (중요)\n{ ds }: { data_interval_start }의 value를 string 형태(‘YYYY-MM-DD’)로 반환 (중요)\n{ ds_nodash }: {ds}를 string 형태(‘YYYYMMDD’) 로 반환\n{ ts }: timestamped 의 약자로 { data_interval_start }를 string 형태(‘YYYY-MM-DD T00:00:00+00:00’) 로 반환\n{ ts_nodash_with_tz }: timestamped 의 약자로 { ts }를 string 형태(‘YYYYMMDDT000000+0000’) 로 반환\n{ ts_nodash  }: timestamped 의 약자로 {ts}를 string 형태(‘YYYYMMDDT000000’) 로 반환\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용 가능하지는 않음!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요 or google airflow operators\n\noperator 설명란에 parameters 란에 각 parameter의 설명 부분 맨 끝에 (templated) 라고 적혀 있는 parameter는 jinja template 사용 가능. template_fields에 요약되어 있음\n예를 들어, airflow.operators.bash 에서 Parameters를 보면\n\nbash_command (str) – The command, set of commands or reference to a bash script (must be ‘.sh’) to be executed. (templated) - jinja template 사용 가능\nenv (dict[str, str] | None) – If env is not None, it must be a dict that defines the environment variables for the new process; these are used instead of inheriting the current process environment, which is the default behavior. (templated) - jinja template 사용 가능\nappend_env (bool) – If False(default) uses the environment variables passed in env params and does not inherit the current process environment. If True, inherits the environment variables from current passes and then environment variable passed by the user will either update the existing inherited environment variables or the new variables gets appended to it - 사용 불가\noutput_encoding (str) – Output encoding of bash command - 사용 불가\nskip_on_exit_code (int | Container[int] | None) – If task exits with this exit code, leave the task in skipped state (default: 99). If set to None, any non-zero exit code will be treated as a failure. - 사용 불가\ncwd (str | None) – Working directory to execute the command in. If None (default), the command is run in a temporary directory. - 사용 불가\ntemplate_fields: Sequence[str]= (‘bash_command’, ‘env’)[source]\n\n하지만, parameter 설명란과 template_fields에 template 변수가 일치하지 않을 수 있음. 그럴 땐 template_fiedls를 기준으로 함\n예를 들어, airflow.operators.python에서 Parameters를 보면 아래와 같이 templates_dict만 사용 가능한 것 처럼 보이지만 template_fields를 보면 op_kwargs 와 op_args도 jinja template으로 사용 가능한 것을 알 수 있다.\n\npython_callable (Callable) – A reference to an object that is callable\nop_kwargs (Mapping[str, Any] | None) – a dictionary of keyword arguments that will get unpacked in your function\nop_args (Collection[Any] | None) – a list of positional arguments that will get unpacked when calling your callable\ntemplates_dict (dict[str, Any] | None) – a dictionary where the values are templates that will get templated by the Airflow engine sometime between __init__and execute takes place and are made available in your callable’s context after the template has been applied. (templated) - 사용 가능\ntemplates_exts (Sequence[str] | None) – a list of file extensions to resolve while processing templated fields, for examples [‘.sql’, ‘.hql’]\nshow_return_value_in_logs (bool) – a bool value whether to show return_value logs. Defaults to True, which allows return value log output. It can be set to False to prevent log output of return value when you return huge data such as transmission a large amount of XCom to TaskAPI.\ntemplate_fields: Sequence[str]= (‘templates_dict’, ‘op_args’, ‘op_kwargs’)[source]\n\n\n\n\n\n\n\n\n\n\n\n\nrecap) Bash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n\n\n파라미터\n\nbash_command (str) (templated)\nenv (dict[str, str] | None) (templated)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)\n\nDAG Full example ```markdown from airflow import DAG import pendulum import datetime from airflow.operators.bash import BashOperator\nwith DAG( dag_id=“dags_bash_with_template”, schedule=“10 0 * * *“, start_date=pendulum.datetime(2023, 3, 1, tz=”Asia/Seoul”), catchup=False ) as dag: bash_t1 = BashOperator( task_id=‘bash_t1’, bash_command=‘echo “data_interval_end: {{ data_interval_end }}”’ )\n  bash_t2 = BashOperator(\n      task_id='bash_t2',\n      env={\n          'START_DATE':'{{data_interval_start | ds }}', #| ds: time stamped type을 YYYY-MM-DD로 변환\n          'END_DATE':'{{data_interval_end | ds }}' #| ds: time stamped type을 YYYY-MM-DD로 변환\n      },\n      bash_command='echo $START_DATE && echo $END_DATE' #shell script syntax: statement1 && statement2\n      # statment1이 성공하면 statement2를 실행한다.\n  )\n\n  bash_t1 &gt;&gt; bash_t2\n```\n\nAirflow Web Service Result\n\n[2023-06-17, 01:00:00 UTC] {taskinstance.py:1327} INFO - Executing &lt;Task(BashOperator): bash_t1&gt; on 2023-06-15 15:10:00+00:00 에서 2023-06-15 15:10:00+00:00의 +뒤는 time zone을 의미. 00:00 이면 utc (세계 표준시로 한국 보다 9시간 느림)를 의미. 한국 시간으로 변환하려면 9시간을 더해야한다. 즉, 2023-06-16 00:10:00이 한국 서울 시간임\n\n\n[2023-06-17, 01:00:02 UTC] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'echo $START_DATE && echo $END_DATE']\n[2023-06-17, 01:00:02 UTC] {subprocess.py:86} INFO - Output:\n[2023-06-17, 01:00:02 UTC] {subprocess.py:93} INFO - 2023-06-15\n[2023-06-17, 01:00:02 UTC] {subprocess.py:93} INFO - 2023-06-16`\n\n\n\n\n\n\n\n\n\n상황\n\nDaily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행- 매일 00:00에 데이터 가져오기)\n전체 data는 너무 많기 때문에 증분된 데이터만 가져오기. 즉 오늘이 2023/02/25 라면 2023/02/24 와 2023/02/25 사이에 있는 data만 가져온다.\n\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n\n생각해볼 point: 각 관점에 따라 날짜가 다름\n\n데이터 관점의 시작일: 2023-02-24\n데이터 관점의 종료일: 2023-02-25\nDAG이 실행되는 시점: 2023-02-25\nairflow는 ETL을 위한 도구로 만들어졌기 때문에 data관점에서 전처리를 하는 사상이 담겨져 있다.\n\n\n\n\n\n\n예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start (airflow new version - from 2.5.2 version)\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (airflow old version - until 2.5.1 version)\n위와 같이 airflow의 대부분의 변수들이 논리적 기준일을 데이터 관점의 시작일을 기준으로 한다.\nexecution_date 라는 명명법이 너무 혼란스러웠음 실행 날짜란 의미는 대부분의 사람들이 dag이 실행되는 날로 인식을 하는데 data관점에서 날짜를 출력함. 그래서 data_interval_start로 변수명을 바꿈\n\nex. 2023-02-25 배치일 (DAG이 실행되는 날짜)\n\n= data_interval_end (airflow new version - from 2.5.2 version)\n=\n=\n=\n= next_execution_date (airflow old version - until 2.5.1 version)\nnext execution_date 라는 명명법은 대부분의 사람들이 dag이 실행되는 날로 인식을 하기 때문에 혼란스러워서 data_interval_end로 바꿈. 왜냐면 현재 dag 실행 날짜가 next execution_date로 표시되고 그 이전 실행 날짜를 execution_date로 표기해서 실제 실행날짜와 변수 이름이 맞지가 않음.\n그러므로, 배치가 돌고있는 현재 날짜를 출력하고 싶으면 data_interval_end에 접근해야하고 그 이전 배치의 날짜를 출력하고 싶으면 data_interval_start에 접근해야한다.\n\n\nFull Exmaple\n\nDAG\n\n# dags_bash_with_template.py\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"dags_python_show_templates\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 6, 10, tz=\"Asia/Seoul\"),\n    catchup=True #catchup 할때 task 순서를 유념해서 연결시키지 않으면 dags실행을 pause/unpuase시 task들이 꼬일 수 있다.\n) as dag:\n\n    @task(task_id='python_task')\n    def show_templates(**kwargs):\n        from pprint import pprint \n        pprint(kwargs) #pprint는 리스트나 딕셔너리를 줄넘김으로 이쁘게 출력해줌\n\n    show_templates()\n\nAirflow Web Service Result\n\n[2023-06-17, 01:40:17 UTC] {logging_mixin.py:149} INFO - {'conf': &lt;***.configuration.AirflowConfigParser object at 0x7f668aeec910&gt;,\n'conn': None,\n'dag': &lt;DAG: dags_python_show_templates&gt;,\n'dag_run': &lt;DagRun dags_python_show_templates @ 2023-06-09 00:30:00+00:00: scheduled__2023-06-09T00:30:00+00:00, state:running, queued_at: 2023-06-17 01:40:15.833772+00:00. externally triggered: False&gt;,\n**'data_interval_end': DateTime(2023, 6, 10, 0, 30, 0, tzinfo=Timezone('UTC')),**\n**'data_interval_start': DateTime(2023, 6, 9, 0, 30, 0, tzinfo=Timezone('UTC')),**\n**'ds': '2023-06-09',**\n**'ds_nodash': '20230609',**\n*'execution_date': &lt;Proxy at 0x7f665d530640 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'execution_date', DateTime(2023, 6, 9, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n'expanded_ti_count': None,\n'inlets': [],\n**'logical_date': DateTime(2023, 6, 9, 0, 30, 0, tzinfo=Timezone('UTC')),**\n'macros': &lt;module '***.macros' from '/home/***/.local/lib/python3.7/site-packages/***/macros/__init__.py'&gt;,\n*'next_ds': &lt;Proxy at 0x7f665d530690 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'next_ds', '2023-06-10')&gt;*,\n*'next_ds_nodash': &lt;Proxy at 0x7f665d5306e0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'next_ds_nodash', '20230610')&gt;*,\n*'next_execution_date': &lt;Proxy at 0x7f665d530780 with factory functools.partial*(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'next_execution_date', DateTime(2023, 6, 10, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n'outlets': [],\n'params': {},\n'prev_data_interval_end_success': DateTime(2023, 6, 6, 0, 30, 0, tzinfo=Timezone('UTC')),\n'prev_data_interval_start_success': DateTime(2023, 6, 5, 0, 30, 0, tzinfo=Timezone('UTC')),\n*'prev_ds': &lt;Proxy at 0x7f665d5307d0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_ds', '2023-06-08')&gt;*,\n*'prev_ds_nodash': &lt;Proxy at 0x7f665d530820 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_ds_nodash', '20230608')&gt;*,\n*'prev_execution_date': &lt;Proxy at 0x7f665d530870 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_execution_date', DateTime(2023, 6, 8, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n*'prev_execution_date_success': &lt;Proxy at 0x7f665d5308c0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_execution_date_success', DateTime(2023, 6, 5, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n'prev_start_date_success': DateTime(2023, 6, 17, 1, 40, 15, 103936, tzinfo=Timezone('UTC')),\n'run_id': 'scheduled__2023-06-09T00:30:00+00:00',\n'task': &lt;Task(_PythonDecoratedOperator): python_task&gt;,\n'task_instance': &lt;TaskInstance: dags_python_show_templates.python_task scheduled__2023-06-09T00:30:00+00:00 [running]&gt;,\n'task_instance_key_str': 'dags_python_show_templates__python_task__20230609',\n'templates_dict': None,\n'test_mode': False,\n'ti': &lt;TaskInstance: dags_python_show_templates.python_task scheduled__2023-06-09T00:30:00+00:00 [running]&gt;,\n*'tomorrow_ds': &lt;Proxy at 0x7f665d530910 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'tomorrow_ds', '2023-06-10')&gt;*,\n*'tomorrow_ds_nodash': &lt;Proxy at 0x7f665d530960 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'tomorrow_ds_nodash', '20230610')&gt;*,\n'triggering_dataset_events': {},\n**'ts': '2023-06-09T00:30:00+00:00',**\n**'ts_nodash': '20230609T003000',**\n**'ts_nodash_with_tz': '20230609T003000+0000',**\n'var': {'json': None, 'value': None},\n*'yesterday_ds': &lt;Proxy at 0x7f665d5309b0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'yesterday_ds', '2023-06-08')&gt;*,\n*'yesterday_ds_nodash': &lt;Proxy at 0x7f665d530a00 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'yesterday_ds_nodash', '20230608')&gt;}*\n\n위에서, 과거 혼란을 주는 변수들은 italic채로 표시를 했고 출력물을 보면 depreacted될 예정이라고 적혀져 있어 곧 안쓰일 예정이라고 적혀져 있다.\nbold채로 쓰여진 출력물이 개선된 명명법으로 이름 붙여진 변수들인데 대부분의 시간들이 data관점에서 logical date를 선정한 것을 알 수 있다. dag 배치 실행 날짜를 보기 위해선 data_interval_end를 보면 2023-06-10이 실행 날짜인 것을 알 수 있다. logical date의 2023-06-10 이전 배치 실행 날짜이다.\n실제 업무나 작업시 data_interval_end가 자주 쓰인다.\n\n\n\n\n\n\n\n\n\nPython 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs (templated)\nop_args (templated)\ntemplates_dict (templated)\ntemplates_exts\nshow_return_value_in_logs\n\nOperator Template\n\njinja template을 이용하여 runtime date를 얻을 때 2가지 방식이 있음\n\n함수를 만들어 op_kwargs에 jinja template 변수를 만들고 이 변수에 저장된 값을 꺼내 쓰는 법\n**kwargs로부터 얻음 - 2번째 방법이 더 편한것 같지만 개인 취향에 따름\n\n함수를 만들어 jinja template를 이용해 연산\n\n\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()\nFull Example\n\nDAGS\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"dags_python_template\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 3, 10, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    def python_function1(start_date, end_date, **kwargs):\n        print(start_date)\n        print(end_date)\n\n    python_t1 = PythonOperator(\n        task_id='python_t1',\n        python_callable=python_function1,\n        op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n    )\n\n    @task(task_id='python_t2')\n    def python_function2(**kwargs):\n        print(kwargs)\n        print('ds:' + kwargs['ds'])\n        print('ts:' + kwargs['ts'])\n        print('data_interval_start:' + str(kwargs['data_interval_start']))\n        print('data_interval_end:' + str(kwargs['data_interval_end']))\n        print('task_instance:' + str(kwargs['ti']))\n\n\n    python_t1 &gt;&gt; python_function2() #decorator사용시 함수를 실행주기만 해도 task가 생성되기 때문에 함수를 task로 연결할 수 있다.\n\nAirflow Web Service Result\n\n\n\n\n\n\n\n\njinja template 안에서 날짜 연산을 가능하게 해주는 기능\n\n파이썬의 datetime + dateutil library로 가능\n\nMacro 변수의 필요성\n\n가령, 어떤 DAG의 스케줄은 매일 말일에 도는 스케줄 (0 0 L * *)인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은 상황. 즉,\n\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\n날짜 구간을 hard coding 해놓는게 아니라 DAG이 도는 시점에 따라 알맞게 들어가야 함.\n예를 들어, 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되어야함 DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지 생각해봐야함 (반드시, data_interval_end 에서 하루 뺀 값이 나와야 하는데)\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN {{ data_interval_start }} AND {{ data_interval_start }} - 1day\n'''\n{ data_interval_start } - 1day 이 부분 연산을 하는데 macro 변수가 쓰임\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime, python의 datetime library 를 이용가능하게 하거나 datetime library를 template 변수내에서 날짜 연산 기능\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta, 날짜 연산 기능\n\n\nmacros.dateutil\nA reference to the dateutil package, python의 dateutil library를 이용가능하게 하거나 dateutil library를 template 변수내에서 이용가능하게 하여 날짜 연산 기능\n\n\nmacros.time\nThe standard lib’s time, 날짜 연산 기능\n\n\nmacros.uuid\nThe standard lib’s uuid, 고유 ID 부여\n\n\nmacros.random\nThe standard lib’s random, python rand() 사용가능하게 해줌\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리, 매우 빈번하게 쓰임\n예를 들어, macros.dateutil에서 relativedelta.relativedelta() 함수를 쓸수 있도록 해줌. macros.dateutil.relativedelta.relativedelta()\n\nMacro를 잘 쓰려면 python의 datetime 및 dateutil library에 익숙해져야 함.\n\n\n\n\n\n만약, jupyter notebook (대화형 입력창)이 없는 환경인데 jupyter notebook에서 python을 실행하고 싶으면 terminal에 다음 명령어를 실행해서 설치\n\n대화형 입력창: 일련의 명령어들을 한번에 실행시키는 script code 형식이 아니라 명령어 한줄마다 결과값을 볼 수 있는 창\n\n\npip install jupyter # 약 5분 소요\npython -m notebook\n\n\nCode\nfrom datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) #월을 1월로 변경하는 명령어, relativedelta library 사용\nprint(now.replace(month=1)) # 월을 1월로 변경하는 명령어, datetime library 사용, print(now+relativedelta.relativedelta(month=1)) 와 같은 명령어\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기: 먼저 month 값에서 1을 빼고 그 결과 값(month)의 가장 가까운 말일을 자동으로 선택해줌\n\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\n\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기. relativedelta library장점이 연산 연러개를 이어 붙일 수 있음\n\n\ncurrent time:2003-03-30 00:00:00\n-------------month operation-------------\n2003-01-30 00:00:00\n2003-01-30 00:00:00\n2003-02-28 00:00:00\n-------------day operation-------------\n2003-03-01 00:00:00\n2003-03-01 00:00:00\n2003-03-29 00:00:00\n-------------multiple operations-------------\n2003-02-27 00:00:00\n\n\n\n\n\n\n예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일 (6#2)에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''}, #env 변수에 template 변수를 작성\n)\n\n이 부분에 template + macro 활용\n\nDAG 예시1.\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_with_macro_eg1\",\n    schedule=\"10 0 L * *\", #매월 말일날 도는 DAG\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # START_DATE: 전월 말일, END_DATE: 1일 전\n    bash_task_1 = BashOperator(\n        task_id='bash_task_1',\n        env={'START_DATE':'{{ data_interval_start.in_timezone(\"Asia/Seoul\") | ds }}',\n                #template 변수에 꺼내쓰는 모든 날짜 변수는 default로 timezone이 UTC로 맞춰져있기 때문에 현지에 맞게 고쳐줘야한다. 한국 시간에 맞추려면 9시간을 더해야하는데, .in_timezone(\"Asia/Seoul\")로 해결 가능\n                #data_interval_start.in_timezone(\"Asia/Seoul\")는 timestamp형식으로 출력되기 때문에 yyyy-mm-dd로 출력하기위해 ds 연산 붙임\n             'END_DATE':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") - macros.dateutil.relativedelta.relativedelta(days=1)) | ds}}'\n             # 연산자가 -로 되어 있이기 때문에  days=-1로 할필요없음\n        },\n        bash_command='echo \"START_DATE: $START_DATE\" && echo \"END_DATE: $END_DATE\"'\n    )\n\n예시2. DAG full Exmaple\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_with_macro_eg2\",\n    schedule=\"10 0 * * 6#2\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # START_DATE: 2주전 월요일, END_DATE: 2주전 토요일\n    # 예를 들어, 2023-04-01 토요일은 첫째 주 토요일로 인식\n    # 2023-04-08 토요일은 둘째 주 토요일로 인식 (군대에서 순서를 세는 방식과 다름)\n    # 2023-04-08 토요일을 START_DATE(배치일)로 정하면 END_DATE는 배치일 기준으로부터 2 주를 뺀 토요일은 2023-03-25가 된다.\n    # 배치일 기준 (2023-04-08 토요일)으로 그 전 배치의 START_DATE를 구하려면 END_DATE로부터 5일을 뺀 날짜인 2023-03-20 (월요일)이 START_DATE가 된다.\n    # 이는 즉, 배치일 기준 (2023-04-08 토요일) 19일을 빼준 날짜와 같다.\n    bash_task_2 = BashOperator(\n        task_id='bash_task_2',\n        env={'START_DATE':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") - macros.dateutil.relativedelta.relativedelta(days=19)) | ds}}', #2주전 월요일\n             'END_DATE':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") - macros.dateutil.relativedelta.relativedelta(days=14)) | ds}}' #2주전 툐요일\n        },\n        bash_command='echo \"START_DATE: $START_DATE\" && echo \"END_DATE: $END_DATE\"'\n    )\n\n\n\n\n\nTemplate 변수를 지원하는 parameters\n패러미터\n\npython_callable (Callable | None)\nop_kwargs (Templated)\nop_args (Templated)\ntemplates_dict (Templated)\ntemplates_exts\nshow_return_value_in_logs\n\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\")\n    #templates 변수를 꺼내온 값들을 key:value 형태로 꺼내온 뒤\n    #get_datetime_macro(**kwargs)의 **kwargs에 전달된다. \n+ macros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}',\n# 배치일로 부터 한달을 빼고 일 1로 함. 즉, 전월 1일\n# 예를 들어, 배치일이 3월 15일이라면 2월 1일로 end_date를 설정한다.\n'end_date': '{{\n(data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\nmacros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n# end_date는 배치일이 3월 15일이라면 2월 28일로 된다.\n\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {} # kwargs.get('templates_dict')이 빈값이면 {}로 할당\n    if templates_dict:\n    start_date = templates_dict.get('start_date') or 'start_date없음'\n    end_date = templates_dict.get('end_date') or 'end_date없음'\n    print(start_date)\n    print(end_date)\n\nget_datetime_macro(kwargs)의 templates_dict에는 {‘start_date’:’{{ (data_interval_end.in_timezone(“Asia/Seoul”) #templates 변수를 꺼내온 값들을 key:value 형태로 꺼내온 뒤 #get_datetime_macro(kwargs)의 **kwargs에 전달된다.\nmacros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}‘, ’end_date’: ‘{{ (data_interval_end.in_timezone(“Asia/Seoul”).replace(day=1) + macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}’ } 전체가 들어감\n\n그러나 Python 오퍼레이터에서 굳이 macro를 사용할 필요가 있을까? 날짜 연산을 python 문법을 이용해서 DAG 안에서 직접 연산하면 macro 변수를 사용안해도 날짜를 계산할 수 있음.\n\nmacro 사용 : template 변수 내에서 macro를 이용해 날짜를 반환 후에 start_date에 할당\n\n@task(task_id='task_using_macros',\n    templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") + macros.dateutil.relativedelta.relativedelta(months=-1,day=1)) | ds }}',\n    'end_date': '{{ (data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) +\n    macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n    }\n)\n\ndef get_datetime_macro(**kwargs):\n    templates_dict = kwargs.get('templates_dict') or {}\n    if templates_dict:\n        start_date = templates_dict.get('start_date') or 'start_date없음'\n        end_date = templates_dict.get('end_date') or 'end_date없음'\n        print(start_date)\n        print(end_date)\n\npython 문법을 사용하여 직접 연산: 라이브러리를 이용해 날짜를 연산\n\n@task(task_id='task_direct_calc')\ndef get_datetime_calc(**kwargs):\n    from dateutil.relativedelta import relativedelta #relativedelta함수 직접 import\n    data_interval_end = kwargs['data_interval_end'] #data_interval_end는 datetime type\nprev_month_day_first = data_interval_end.in_timezone('Asia/Seoul') + relativedelta(months=-1, day=1) #data_interval_end는 datetime type에는 in_timezone() method가 있음\nprev_month_day_last = data_interval_end.in_timezone('Asia/Seoul').replace(day=1) + relativedelta(days=-1)\nprint(prev_month_day_first.strftime('%Y-%m-%d')) # | ds 구현\nprint(prev_month_day_last.strftime('%Y-%m-%d'))  # | ds 구현\n예시: Dags full example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.decorators import task\n\n\n\nwith DAG(\n    dag_id=\"dags_python_with_macro\",\n    schedule=\"10 0 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    # macro 이용\n    @task(task_id='task_using_macros',\n      templates_dict={'start_date':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") + macros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}',\n                      'end_date': '{{ (data_interval_end.in_timezone(\"Asia/Seoul\").replace(day=1) + macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'\n     }\n    )\n    \n    def get_datetime_macro(**kwargs):\n        \n        templates_dict = kwargs.get('templates_dict') or {}\n        if templates_dict:\n            start_date = templates_dict.get('start_date') or 'start_date없음'\n            end_date = templates_dict.get('end_date') or 'end_date없음'\n            print(start_date)\n            print(end_date)\n\n\n    # python 이용\n    @task(task_id='task_direct_calc')\n    def get_datetime_calc(**kwargs):\n        from dateutil.relativedelta import relativedelta # 스케쥴러 부하 경감을 위해 task안에다가 library호출\n        # 다시 말해서, scheduler는 dag이 실행되지 않더라도 사용자가 작성한 dag을 주기적으로 문법적인 오류가 있는지를 검사하기 위해 parsing함\n        # DAG이 시작하기 이전 code (즉, `with DAG` 이전 부분) 와 task가 시작하기 이전 code (`as dag:` 이후 부분과 task 선언 이전 부분)를 parsing 및 검사\n        # 하지만 operator 안 과 task decorator안에 있는 부분은 parsing 및 검사하지 않음.\n        # 실제로 대형 프로젝트에서 겪는 scheduluer부하 문제를 해결하는 팁이 될 수 있음\n        data_interval_end = kwargs['data_interval_end']\n        prev_month_day_first = data_interval_end.in_timezone('Asia/Seoul') + relativedelta(months=-1, day=1)\n        prev_month_day_last = data_interval_end.in_timezone('Asia/Seoul').replace(day=1) +  relativedelta(days=-1)\n        print(prev_month_day_first.strftime('%Y-%m-%d'))\n        print(prev_month_day_last.strftime('%Y-%m-%d'))\n\n    get_datetime_macro() &gt;&gt; get_datetime_calc()\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#airflow에서-사용법",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#airflow에서-사용법",
    "title": "Template Variabler",
    "section": "",
    "text": "오퍼레이터 파라미터 입력시 중괄호 {} 2개({{}})를 이용하면 Airflow에서 기본적으로 제공하는 변수들(ex: 수행 날짜, DAG_ID)을 치환된 값으로 입력할 수 있음.\n\nairflow에서 제공하는 기본 variable list or google ‘airflow templates reference’\n{ data_interval_start }: schedule 구간의 시작점을 반환, pendulum.DateTime는 timestamped type (중요)\n{ data_interval_end }: schedule 구간의 끝점을 반환 (중요)\n{ ds }: { data_interval_start }의 value를 string 형태(‘YYYY-MM-DD’)로 반환 (중요)\n{ ds_nodash }: {ds}를 string 형태(‘YYYYMMDD’) 로 반환\n{ ts }: timestamped 의 약자로 { data_interval_start }를 string 형태(‘YYYY-MM-DD T00:00:00+00:00’) 로 반환\n{ ts_nodash_with_tz }: timestamped 의 약자로 { ts }를 string 형태(‘YYYYMMDDT000000+0000’) 로 반환\n{ ts_nodash  }: timestamped 의 약자로 {ts}를 string 형태(‘YYYYMMDDT000000’) 로 반환\n\n모든 오퍼레이터, 모든 파라미터에 Template 변수 적용 가능하지는 않음!\nAirflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요 or google airflow operators\n\noperator 설명란에 parameters 란에 각 parameter의 설명 부분 맨 끝에 (templated) 라고 적혀 있는 parameter는 jinja template 사용 가능. template_fields에 요약되어 있음\n예를 들어, airflow.operators.bash 에서 Parameters를 보면\n\nbash_command (str) – The command, set of commands or reference to a bash script (must be ‘.sh’) to be executed. (templated) - jinja template 사용 가능\nenv (dict[str, str] | None) – If env is not None, it must be a dict that defines the environment variables for the new process; these are used instead of inheriting the current process environment, which is the default behavior. (templated) - jinja template 사용 가능\nappend_env (bool) – If False(default) uses the environment variables passed in env params and does not inherit the current process environment. If True, inherits the environment variables from current passes and then environment variable passed by the user will either update the existing inherited environment variables or the new variables gets appended to it - 사용 불가\noutput_encoding (str) – Output encoding of bash command - 사용 불가\nskip_on_exit_code (int | Container[int] | None) – If task exits with this exit code, leave the task in skipped state (default: 99). If set to None, any non-zero exit code will be treated as a failure. - 사용 불가\ncwd (str | None) – Working directory to execute the command in. If None (default), the command is run in a temporary directory. - 사용 불가\ntemplate_fields: Sequence[str]= (‘bash_command’, ‘env’)[source]\n\n하지만, parameter 설명란과 template_fields에 template 변수가 일치하지 않을 수 있음. 그럴 땐 template_fiedls를 기준으로 함\n예를 들어, airflow.operators.python에서 Parameters를 보면 아래와 같이 templates_dict만 사용 가능한 것 처럼 보이지만 template_fields를 보면 op_kwargs 와 op_args도 jinja template으로 사용 가능한 것을 알 수 있다.\n\npython_callable (Callable) – A reference to an object that is callable\nop_kwargs (Mapping[str, Any] | None) – a dictionary of keyword arguments that will get unpacked in your function\nop_args (Collection[Any] | None) – a list of positional arguments that will get unpacked when calling your callable\ntemplates_dict (dict[str, Any] | None) – a dictionary where the values are templates that will get templated by the Airflow engine sometime between __init__and execute takes place and are made available in your callable’s context after the template has been applied. (templated) - 사용 가능\ntemplates_exts (Sequence[str] | None) – a list of file extensions to resolve while processing templated fields, for examples [‘.sql’, ‘.hql’]\nshow_return_value_in_logs (bool) – a bool value whether to show return_value logs. Defaults to True, which allows return value log output. It can be set to False to prevent log output of return value when you return huge data such as transmission a large amount of XCom to TaskAPI.\ntemplate_fields: Sequence[str]= (‘templates_dict’, ‘op_args’, ‘op_kwargs’)[source]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#bashoperator",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#bashoperator",
    "title": "Template Variabler",
    "section": "",
    "text": "recap) Bash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n\n\n파라미터\n\nbash_command (str) (templated)\nenv (dict[str, str] | None) (templated)\nappend_env (bool)\noutput_encoding (str)\nskip_exit_code (int)\ncwd (str | None)\n\n\nbash_t1 = BashOperator(\n    task_id='bash_t1',\n    bash_command='echo \"End date is {{ data_interval_end }}\"'\n)\nbash_t2 = BashOperator(\n    task_id='bash_t2',\n    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},\n    bash_command='echo \"Start date is $START_DATE \" && ''echo \"End date is $END_DATE\"'\n)\n\nDAG Full example ```markdown from airflow import DAG import pendulum import datetime from airflow.operators.bash import BashOperator\nwith DAG( dag_id=“dags_bash_with_template”, schedule=“10 0 * * *“, start_date=pendulum.datetime(2023, 3, 1, tz=”Asia/Seoul”), catchup=False ) as dag: bash_t1 = BashOperator( task_id=‘bash_t1’, bash_command=‘echo “data_interval_end: {{ data_interval_end }}”’ )\n  bash_t2 = BashOperator(\n      task_id='bash_t2',\n      env={\n          'START_DATE':'{{data_interval_start | ds }}', #| ds: time stamped type을 YYYY-MM-DD로 변환\n          'END_DATE':'{{data_interval_end | ds }}' #| ds: time stamped type을 YYYY-MM-DD로 변환\n      },\n      bash_command='echo $START_DATE && echo $END_DATE' #shell script syntax: statement1 && statement2\n      # statment1이 성공하면 statement2를 실행한다.\n  )\n\n  bash_t1 &gt;&gt; bash_t2\n```\n\nAirflow Web Service Result\n\n[2023-06-17, 01:00:00 UTC] {taskinstance.py:1327} INFO - Executing &lt;Task(BashOperator): bash_t1&gt; on 2023-06-15 15:10:00+00:00 에서 2023-06-15 15:10:00+00:00의 +뒤는 time zone을 의미. 00:00 이면 utc (세계 표준시로 한국 보다 9시간 느림)를 의미. 한국 시간으로 변환하려면 9시간을 더해야한다. 즉, 2023-06-16 00:10:00이 한국 서울 시간임\n\n\n[2023-06-17, 01:00:02 UTC] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'echo $START_DATE && echo $END_DATE']\n[2023-06-17, 01:00:02 UTC] {subprocess.py:86} INFO - Output:\n[2023-06-17, 01:00:02 UTC] {subprocess.py:93} INFO - 2023-06-15\n[2023-06-17, 01:00:02 UTC] {subprocess.py:93} INFO - 2023-06-16`"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#데이터-추출-예시",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#데이터-추출-예시",
    "title": "Template Variabler",
    "section": "",
    "text": "상황\n\nDaily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행- 매일 00:00에 데이터 가져오기)\n전체 data는 너무 많기 때문에 증분된 데이터만 가져오기. 즉 오늘이 2023/02/25 라면 2023/02/24 와 2023/02/25 사이에 있는 data만 가져온다.\n\nexample: 등록 테이블\n\n\n\n\nREG_DATE\nNAME\nADDRESS\n\n\n\n\n2023-02-24 15:34:35\n홍길동\nBusan\n\n\n2023-02-24 19:14:42\n김태희\nSeoul\n\n\n2023-02-24 23:52:19\n조인성\nDaejeon\n\n\n\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')\nAND TIMESTAMP('2023-02-24 23:59:59')\n\n생각해볼 point: 각 관점에 따라 날짜가 다름\n\n데이터 관점의 시작일: 2023-02-24\n데이터 관점의 종료일: 2023-02-25\nDAG이 실행되는 시점: 2023-02-25\nairflow는 ETL을 위한 도구로 만들어졌기 때문에 data관점에서 전처리를 하는 사상이 담겨져 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#airflow-날짜-template-변수",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#airflow-날짜-template-변수",
    "title": "Template Variabler",
    "section": "",
    "text": "예시: 일 배치\n\nex. 2023-02-24 이전 배치일 (논리적 기준일)\n\n= data_interval_start (airflow new version - from 2.5.2 version)\n= dag_run.logical_date\n= ds (yyyy-mm-dd 형식)\n= ts (타임스탬프)\n= execution_date (airflow old version - until 2.5.1 version)\n위와 같이 airflow의 대부분의 변수들이 논리적 기준일을 데이터 관점의 시작일을 기준으로 한다.\nexecution_date 라는 명명법이 너무 혼란스러웠음 실행 날짜란 의미는 대부분의 사람들이 dag이 실행되는 날로 인식을 하는데 data관점에서 날짜를 출력함. 그래서 data_interval_start로 변수명을 바꿈\n\nex. 2023-02-25 배치일 (DAG이 실행되는 날짜)\n\n= data_interval_end (airflow new version - from 2.5.2 version)\n=\n=\n=\n= next_execution_date (airflow old version - until 2.5.1 version)\nnext execution_date 라는 명명법은 대부분의 사람들이 dag이 실행되는 날로 인식을 하기 때문에 혼란스러워서 data_interval_end로 바꿈. 왜냐면 현재 dag 실행 날짜가 next execution_date로 표시되고 그 이전 실행 날짜를 execution_date로 표기해서 실제 실행날짜와 변수 이름이 맞지가 않음.\n그러므로, 배치가 돌고있는 현재 날짜를 출력하고 싶으면 data_interval_end에 접근해야하고 그 이전 배치의 날짜를 출력하고 싶으면 data_interval_start에 접근해야한다.\n\n\nFull Exmaple\n\nDAG\n\n# dags_bash_with_template.py\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"dags_python_show_templates\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 6, 10, tz=\"Asia/Seoul\"),\n    catchup=True #catchup 할때 task 순서를 유념해서 연결시키지 않으면 dags실행을 pause/unpuase시 task들이 꼬일 수 있다.\n) as dag:\n\n    @task(task_id='python_task')\n    def show_templates(**kwargs):\n        from pprint import pprint \n        pprint(kwargs) #pprint는 리스트나 딕셔너리를 줄넘김으로 이쁘게 출력해줌\n\n    show_templates()\n\nAirflow Web Service Result\n\n[2023-06-17, 01:40:17 UTC] {logging_mixin.py:149} INFO - {'conf': &lt;***.configuration.AirflowConfigParser object at 0x7f668aeec910&gt;,\n'conn': None,\n'dag': &lt;DAG: dags_python_show_templates&gt;,\n'dag_run': &lt;DagRun dags_python_show_templates @ 2023-06-09 00:30:00+00:00: scheduled__2023-06-09T00:30:00+00:00, state:running, queued_at: 2023-06-17 01:40:15.833772+00:00. externally triggered: False&gt;,\n**'data_interval_end': DateTime(2023, 6, 10, 0, 30, 0, tzinfo=Timezone('UTC')),**\n**'data_interval_start': DateTime(2023, 6, 9, 0, 30, 0, tzinfo=Timezone('UTC')),**\n**'ds': '2023-06-09',**\n**'ds_nodash': '20230609',**\n*'execution_date': &lt;Proxy at 0x7f665d530640 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'execution_date', DateTime(2023, 6, 9, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n'expanded_ti_count': None,\n'inlets': [],\n**'logical_date': DateTime(2023, 6, 9, 0, 30, 0, tzinfo=Timezone('UTC')),**\n'macros': &lt;module '***.macros' from '/home/***/.local/lib/python3.7/site-packages/***/macros/__init__.py'&gt;,\n*'next_ds': &lt;Proxy at 0x7f665d530690 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'next_ds', '2023-06-10')&gt;*,\n*'next_ds_nodash': &lt;Proxy at 0x7f665d5306e0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'next_ds_nodash', '20230610')&gt;*,\n*'next_execution_date': &lt;Proxy at 0x7f665d530780 with factory functools.partial*(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'next_execution_date', DateTime(2023, 6, 10, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n'outlets': [],\n'params': {},\n'prev_data_interval_end_success': DateTime(2023, 6, 6, 0, 30, 0, tzinfo=Timezone('UTC')),\n'prev_data_interval_start_success': DateTime(2023, 6, 5, 0, 30, 0, tzinfo=Timezone('UTC')),\n*'prev_ds': &lt;Proxy at 0x7f665d5307d0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_ds', '2023-06-08')&gt;*,\n*'prev_ds_nodash': &lt;Proxy at 0x7f665d530820 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_ds_nodash', '20230608')&gt;*,\n*'prev_execution_date': &lt;Proxy at 0x7f665d530870 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_execution_date', DateTime(2023, 6, 8, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n*'prev_execution_date_success': &lt;Proxy at 0x7f665d5308c0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'prev_execution_date_success', DateTime(2023, 6, 5, 0, 30, 0, tzinfo=Timezone('UTC')))&gt;*,\n'prev_start_date_success': DateTime(2023, 6, 17, 1, 40, 15, 103936, tzinfo=Timezone('UTC')),\n'run_id': 'scheduled__2023-06-09T00:30:00+00:00',\n'task': &lt;Task(_PythonDecoratedOperator): python_task&gt;,\n'task_instance': &lt;TaskInstance: dags_python_show_templates.python_task scheduled__2023-06-09T00:30:00+00:00 [running]&gt;,\n'task_instance_key_str': 'dags_python_show_templates__python_task__20230609',\n'templates_dict': None,\n'test_mode': False,\n'ti': &lt;TaskInstance: dags_python_show_templates.python_task scheduled__2023-06-09T00:30:00+00:00 [running]&gt;,\n*'tomorrow_ds': &lt;Proxy at 0x7f665d530910 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'tomorrow_ds', '2023-06-10')&gt;*,\n*'tomorrow_ds_nodash': &lt;Proxy at 0x7f665d530960 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'tomorrow_ds_nodash', '20230610')&gt;*,\n'triggering_dataset_events': {},\n**'ts': '2023-06-09T00:30:00+00:00',**\n**'ts_nodash': '20230609T003000',**\n**'ts_nodash_with_tz': '20230609T003000+0000',**\n'var': {'json': None, 'value': None},\n*'yesterday_ds': &lt;Proxy at 0x7f665d5309b0 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'yesterday_ds', '2023-06-08')&gt;*,\n*'yesterday_ds_nodash': &lt;Proxy at 0x7f665d530a00 with factory functools.partial(&lt;function lazy_mapping_from_context.&lt;locals&gt;._deprecated_proxy_factory at 0x7f665d577e60&gt;, 'yesterday_ds_nodash', '20230608')&gt;}*\n\n위에서, 과거 혼란을 주는 변수들은 italic채로 표시를 했고 출력물을 보면 depreacted될 예정이라고 적혀져 있어 곧 안쓰일 예정이라고 적혀져 있다.\nbold채로 쓰여진 출력물이 개선된 명명법으로 이름 붙여진 변수들인데 대부분의 시간들이 data관점에서 logical date를 선정한 것을 알 수 있다. dag 배치 실행 날짜를 보기 위해선 data_interval_end를 보면 2023-06-10이 실행 날짜인 것을 알 수 있다. logical date의 2023-06-10 이전 배치 실행 날짜이다.\n실제 업무나 작업시 data_interval_end가 자주 쓰인다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#python-오퍼레이터에서-template-변수-사용",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#python-오퍼레이터에서-template-변수-사용",
    "title": "Template Variabler",
    "section": "",
    "text": "Python 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs (templated)\nop_args (templated)\ntemplates_dict (templated)\ntemplates_exts\nshow_return_value_in_logs\n\nOperator Template\n\njinja template을 이용하여 runtime date를 얻을 때 2가지 방식이 있음\n\n함수를 만들어 op_kwargs에 jinja template 변수를 만들고 이 변수에 저장된 값을 꺼내 쓰는 법\n**kwargs로부터 얻음 - 2번째 방법이 더 편한것 같지만 개인 취향에 따름\n\n함수를 만들어 jinja template를 이용해 연산\n\n\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()\nFull Example\n\nDAGS\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"dags_python_template\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 3, 10, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    def python_function1(start_date, end_date, **kwargs):\n        print(start_date)\n        print(end_date)\n\n    python_t1 = PythonOperator(\n        task_id='python_t1',\n        python_callable=python_function1,\n        op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n    )\n\n    @task(task_id='python_t2')\n    def python_function2(**kwargs):\n        print(kwargs)\n        print('ds:' + kwargs['ds'])\n        print('ts:' + kwargs['ts'])\n        print('data_interval_start:' + str(kwargs['data_interval_start']))\n        print('data_interval_end:' + str(kwargs['data_interval_end']))\n        print('task_instance:' + str(kwargs['ti']))\n\n\n    python_t1 &gt;&gt; python_function2() #decorator사용시 함수를 실행주기만 해도 task가 생성되기 때문에 함수를 task로 연결할 수 있다.\n\nAirflow Web Service Result"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#macro-변수의-이해",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#macro-변수의-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "jinja template 안에서 날짜 연산을 가능하게 해주는 기능\n\n파이썬의 datetime + dateutil library로 가능\n\nMacro 변수의 필요성\n\n가령, 어떤 DAG의 스케줄은 매일 말일에 도는 스케줄 (0 0 L * *)인데 BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고 싶은 상황. 즉,\n\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN ?? AND ??\n'''\n날짜 구간을 hard coding 해놓는게 아니라 DAG이 도는 시점에 따라 알맞게 들어가야 함.\n예를 들어, 배치일이 1월 31일이면 12월 31일부터 1월 30일 까지 배치일이 2월 28일이면 1월 31일부터 2월 27일까지 BETWEEN 이 설정되어야함 DAG 스케줄이 월 단위이니까 Template 변수에서 data_interval_start 값은 한달 전 말일이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지 생각해봐야함 (반드시, data_interval_end 에서 하루 뺀 값이 나와야 하는데)\nsql = f'''\nSELECT NAME, ADDRESS\nFROM TBL_REG\nWHERE REG_DATE BETWEEN {{ data_interval_start }} AND {{ data_interval_start }} - 1day\n'''\n{ data_interval_start } - 1day 이 부분 연산을 하는데 macro 변수가 쓰임\nTemplate 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmacros.datetime\nThe standard lib’s datetime.datetime, python의 datetime library 를 이용가능하게 하거나 datetime library를 template 변수내에서 날짜 연산 기능\n\n\nmacros.timedelta\nThe standard lib’s datetime.timedelta, 날짜 연산 기능\n\n\nmacros.dateutil\nA reference to the dateutil package, python의 dateutil library를 이용가능하게 하거나 dateutil library를 template 변수내에서 이용가능하게 하여 날짜 연산 기능\n\n\nmacros.time\nThe standard lib’s time, 날짜 연산 기능\n\n\nmacros.uuid\nThe standard lib’s uuid, 고유 ID 부여\n\n\nmacros.random\nThe standard lib’s random, python rand() 사용가능하게 해줌\n\n\n\n\nmacros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리, 매우 빈번하게 쓰임\n예를 들어, macros.dateutil에서 relativedelta.relativedelta() 함수를 쓸수 있도록 해줌. macros.dateutil.relativedelta.relativedelta()\n\nMacro를 잘 쓰려면 python의 datetime 및 dateutil library에 익숙해져야 함."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#파이썬-datetime-dateutil-라이브러리-이해",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#파이썬-datetime-dateutil-라이브러리-이해",
    "title": "Template Variabler",
    "section": "",
    "text": "만약, jupyter notebook (대화형 입력창)이 없는 환경인데 jupyter notebook에서 python을 실행하고 싶으면 terminal에 다음 명령어를 실행해서 설치\n\n대화형 입력창: 일련의 명령어들을 한번에 실행시키는 script code 형식이 아니라 명령어 한줄마다 결과값을 볼 수 있는 창\n\n\npip install jupyter # 약 5분 소요\npython -m notebook\n\n\nCode\nfrom datetime import datetime\nfrom dateutil import relativedelta\n\nnow = datetime(year=2003, month=3, day=30)\nprint('current time:'+str(now))\nprint('-------------month operation-------------')\nprint(now+relativedelta.relativedelta(month=1)) #월을 1월로 변경하는 명령어, relativedelta library 사용\nprint(now.replace(month=1)) # 월을 1월로 변경하는 명령어, datetime library 사용, print(now+relativedelta.relativedelta(month=1)) 와 같은 명령어\nprint(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기: 먼저 month 값에서 1을 빼고 그 결과 값(month)의 가장 가까운 말일을 자동으로 선택해줌\n\nprint('-------------day operation-------------')\nprint(now+relativedelta.relativedelta(day=1)) #1일로 변경\nprint(now.replace(day=1)) #1일로 변경\nprint(now+relativedelta.relativedelta(days=-1)) #1일 빼기\n\nprint('-------------multiple operations-------------')\nprint(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기. relativedelta library장점이 연산 연러개를 이어 붙일 수 있음\n\n\ncurrent time:2003-03-30 00:00:00\n-------------month operation-------------\n2003-01-30 00:00:00\n2003-01-30 00:00:00\n2003-02-28 00:00:00\n-------------day operation-------------\n2003-03-01 00:00:00\n2003-03-01 00:00:00\n2003-03-29 00:00:00\n-------------multiple operations-------------\n2003-02-27 00:00:00"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#bash-오퍼레이터에서-macro-변수-활용하기",
    "title": "Template Variabler",
    "section": "",
    "text": "예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기\n예시2. 매월 둘째주 토요일 (6#2)에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기\n변수는 YYYY-MM-DD 형식으로 나오도록 할 것\nt1 = BashOperator(\n    task_id='t1',\n    env={'START_DATE':''}, #env 변수에 template 변수를 작성\n)\n\n이 부분에 template + macro 활용\n\nDAG 예시1.\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_with_macro_eg1\",\n    schedule=\"10 0 L * *\", #매월 말일날 도는 DAG\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # START_DATE: 전월 말일, END_DATE: 1일 전\n    bash_task_1 = BashOperator(\n        task_id='bash_task_1',\n        env={'START_DATE':'{{ data_interval_start.in_timezone(\"Asia/Seoul\") | ds }}',\n                #template 변수에 꺼내쓰는 모든 날짜 변수는 default로 timezone이 UTC로 맞춰져있기 때문에 현지에 맞게 고쳐줘야한다. 한국 시간에 맞추려면 9시간을 더해야하는데, .in_timezone(\"Asia/Seoul\")로 해결 가능\n                #data_interval_start.in_timezone(\"Asia/Seoul\")는 timestamp형식으로 출력되기 때문에 yyyy-mm-dd로 출력하기위해 ds 연산 붙임\n             'END_DATE':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") - macros.dateutil.relativedelta.relativedelta(days=1)) | ds}}'\n             # 연산자가 -로 되어 있이기 때문에  days=-1로 할필요없음\n        },\n        bash_command='echo \"START_DATE: $START_DATE\" && echo \"END_DATE: $END_DATE\"'\n    )\n\n예시2. DAG full Exmaple\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_with_macro_eg2\",\n    schedule=\"10 0 * * 6#2\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # START_DATE: 2주전 월요일, END_DATE: 2주전 토요일\n    # 예를 들어, 2023-04-01 토요일은 첫째 주 토요일로 인식\n    # 2023-04-08 토요일은 둘째 주 토요일로 인식 (군대에서 순서를 세는 방식과 다름)\n    # 2023-04-08 토요일을 START_DATE(배치일)로 정하면 END_DATE는 배치일 기준으로부터 2 주를 뺀 토요일은 2023-03-25가 된다.\n    # 배치일 기준 (2023-04-08 토요일)으로 그 전 배치의 START_DATE를 구하려면 END_DATE로부터 5일을 뺀 날짜인 2023-03-20 (월요일)이 START_DATE가 된다.\n    # 이는 즉, 배치일 기준 (2023-04-08 토요일) 19일을 빼준 날짜와 같다.\n    bash_task_2 = BashOperator(\n        task_id='bash_task_2',\n        env={'START_DATE':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") - macros.dateutil.relativedelta.relativedelta(days=19)) | ds}}', #2주전 월요일\n             'END_DATE':'{{ (data_interval_end.in_timezone(\"Asia/Seoul\") - macros.dateutil.relativedelta.relativedelta(days=14)) | ds}}' #2주전 툐요일\n        },\n        bash_command='echo \"START_DATE: $START_DATE\" && echo \"END_DATE: $END_DATE\"'\n    )"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/05.template_variable.html#python-operator에서-template-변수-사용",
    "href": "docs/blog/posts/Engineering/airflow/05.template_variable.html#python-operator에서-template-변수-사용",
    "title": "Template Variabler",
    "section": "",
    "text": "Python 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?\n파라미터\n\npython_callable\nop_kwargs (templated)\nop_args (templated)\ntemplates_dict (templated)\ntemplates_exts\nshow_return_value_in_logs\n\nOperator Template\n\njinja template을 이용하여 runtime date를 얻을 때 2가지 방식이 있음\n\n함수를 만들어 op_kwargs에 jinja template 변수를 만들고 이 변수에 저장된 값을 꺼내 쓰는 법\n**kwargs로부터 얻음 - 2번째 방법이 더 편한것 같지만 개인 취향에 따름\n\n함수를 만들어 jinja template를 이용해 연산\n\n\ndef python_function1(start_date, end_date, **kwargs):\n    print(start_date)\n    print(end_date)\n\npython_t1 = PythonOperator(\n    task_id='python_t1',\n    python_callable=python_function,\n    op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n)\n\n파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음\n\n@task(task_id='python_t2')\ndef python_function2(**kwargs):\n    print(kwargs)\n    print('ds:' + kwargs['ds'])\n    print('ts:' + str(kwargs['ts']))\n    print('data_interval_start:' + str(kwargs['data_interval_start']))\n    print('data_interval_end:' + str(kwargs['data_interval_end']))\n    print('task_instance': + str(kwargs['ti']))\npython_function2()\nFull Example\n\nDAGS\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id=\"dags_python_template\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 3, 10, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    def python_function1(start_date, end_date, **kwargs):\n        print(start_date)\n        print(end_date)\n\n    python_t1 = PythonOperator(\n        task_id='python_t1',\n        python_callable=python_function1,\n        op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}\n    )\n\n    @task(task_id='python_t2')\n    def python_function2(**kwargs):\n        print(kwargs)\n        print('ds:' + kwargs['ds'])\n        print('ts:' + kwargs['ts'])\n        print('data_interval_start:' + str(kwargs['data_interval_start']))\n        print('data_interval_end:' + str(kwargs['data_interval_end']))\n        print('task_instance:' + str(kwargs['ti']))\n\n\n    python_t1 &gt;&gt; python_function2() #decorator사용시 함수를 실행주기만 해도 task가 생성되기 때문에 함수를 task로 연결할 수 있다.\n\nAirflow Web Service Result"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html",
    "title": "Data Share",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nXcom stands for Cross Communication.\nAirflow DAG 안 Task 간 작은 데이터 (or Message) 공유를 위해 사용되는 기술 (1개의 Dag 안에 있는 task끼리만 data 공유)\n\n예를 들어, Task1의 수행 중 내용이나 결과를 Task2에서 사용 또는 입력으로 주고 싶은 경우\ntask1 은 push, task2는 pull과 같은 tasks간 데이터 공유에 유용\n\n주로 작은 규모의 데이터 공유를 위해 사용\n\nXcom 내용은 meta DB의 Xcom 테이블에 값이 저장됨\n\n1GB 이상의 대용량 데이터 공유를 위해서는 외부 솔루션 사용 필요 (AWS의 S3, GCP의 GCS, HDFS (Hadoop File System) 등)\n\n\n\n\n\n\n\n\n크게 두 가지 방법으로 Xcom 사용 가능\n\n**kwargs에 존재하는 ti (task_instance) 객체 활용\n\n@task(task_id='python_xcom_push_task')\ndef xcom_push(**kwargs):\n    ti = kwargs['ti']\n    ti.xcom_push(key=\"result1\", value=\"value_1\") \n    ti.xcom_push(key=\"result2\", value=[1,2,3])\n    #xcom_push: xcom에다가 data를 올릴 수 있음\n    #data를 올릴 때는 key:value 형태로 올리기\n    #template 변수에서 task_instance 라는 객체를 얻을 수 있으며 task_instance 객체가 가진 xcom_push 메서드를 활용할 수 있음\n\n@task(task_id='python_xcom_pull_task')\ndef xcom_pull(**kwargs):\n    ti = kwargs['ti']\n    value_key1 = ti.xcom_pull(key=\"result1\") # value_1이 value_key1에 저장됨\n    value_key2 = ti.xcom_pull(key=\"result2\",\n    task_ids='python_xcom_push_task') # [1,2,3]이 value_key2에 저장됨\n    #xcom_pull: xcom으로부터 data를 내려 받을 수 있음\n    #data를 올릴 때는 key:value 형태로 올리기\n    print(value_key1)\n    print(value_key2)\n\nxcome_pull()을 할때 key값만 줘도 되고 key값과 task_ids값을 둘다 줘도 된다.\n\nkey값만 줘도 될때\n\nxcom_push를 한 task가 1개 밖에 없을 때 사용 가능\n혹은, key값이 중복될 때 xcom_push를 한 task가 여러 개 있을 때도 사용 가능한데 가장 마지막 (최신) task의 key값을 호출 한다.\n만약, key값이 중복이 되지 않는 다면 key값만으로도 data를 내려 받을 수 있다.\n\nkey값과 task_ids둘다 줘야할 때\n\nkey값이 중복되는 xcom_push를 한 task가 여러 개 있을 때 선택적으로 원하는 task의 data를 가지고 오고 싶으면 해당 task의 task_ids를 명시적으로 적어줘야한다.\n\n예를 들어,\n\n# 5개의 tasks 존재하는\n# task1: xcom_push(key='result1'...)\n# task2: xcom_push(key='result1'...)\n# task3: xcom_push(key='result2'...)\n# task4: xcom_pull(key='result1'...)\n# task5: xcom_pull(key='result1',task_ids=...)\n\ntask4가 수행이 될때 task1의 xcom을 가져우는게 아니라 가장 최신에 수행된 task2의 xcom을 가져오게 된다.\ntask1의 xcom을 가지고 오고 싶을땐 task5와 같이 task1의 task_id를 task5의 task_ids에 명시해주면 된다.\n가장 안전한 방법은 task의 key값과 task_ids를 명시적으로 적어주는 것이다. 아니면 tasks의 key값을 절대 중복이 되지않도록 적어주는 것이다.\n\n\n\n파이썬 함수의 return 값 활용\n\n(1안)\n\n@task(task_id='xcom_push_by_return')\ndef xcom_push_by_return(**kwargs):\n    transaction_value = 'status Good'\n    return transaction_value\n@task(task_id='xcom_pull_by_return')\ndef xcom_pull_by_return(status, **kwargs):\n    print(status)\nxcom_pull_by_return(xcom_push_by_return()) \n\nxcom을 이용한 task의 flow 정해주는 또 다른 방식\n암묵적인 task의 순서: xcom_push_by_return() &gt;&gt; xcom_pull_by_return()\n위의 스크립트에서 xcom_pull() 또는 xcom_push()가 명시적으로 쓰이지진 않았지만 airflow에서는 Task 데커레이터 사용시 함수 입력/출력 관계만으로 Task flow 정의가 된다. 즉, xcom_pull_by_return(xcom_push_by_return()) = xcom_push_by_return() &gt;&gt; xcom_pull_by_return()\nTask 데커레이터 사용시 custom 함수가 return을 하게 되면 자동으로 xcom에 data가 올라가게 된다.\n(2안)\n\n\n@task(task_id='xcom_push_by_return')\ndef xcom_push_return(**kwargs):\n    transaction_value = 'status Good'\n    return transaction_value\n    # return 한 값은 자동으로 xcom에 key='return_value', task_ids=task_id 로 저장됨\n\n@task(task_id='xcom_pull_by_return')\ndef xcom_pull_return_by_method(**kwargs):\n    ti = kwargs['ti']\n    pull_value = ti.xcom_pull(key='return_value', task_ids='xcom_push_by_return')\n    # ti.xcom_pull()을 이용하여 return 한 값을 꺼낼 때는 key를 명시하지 않아도 됨. (자동으로 key=return_value 를 찾음)\n    # task_ids='xcom_push_by_return' return한 Task가 여러개 있을 때는 task_ids 를 지정\n    print(pull_value)\n\nxcom_push_by_return() &gt;&gt; xcom_pull_by_return() # 2안에서는 task flow를 명시적으로 적어줘야함.\n\nDAG Full Example\n\n1안 DAG Full Exmaple ```markdown from airflow import DAG import pendulum import datetime from airflow.decorators import task\nwith DAG( dag_id=“dags_python_with_xcom_eg2”, schedule=“30 6 * * *“, start_date=pendulum.datetime(2023, 3, 1, tz=”Asia/Seoul”), catchup=False ) as dag:\n  @task(task_id='python_xcom_push_by_return')\n  def xcom_push_result(**kwargs):\n      return 'Success'\n\n\n  @task(task_id='python_xcom_pull_1')\n  def xcom_pull_1(**kwargs):\n      ti = kwargs['ti']\n      value1 = ti.xcom_pull(task_ids='python_xcom_push_by_return')\n      print('xcom_pull 메서드로 직접 찾은 리턴 값:' + value1)\n\n  @task(task_id='python_xcom_pull_2')\n  def xcom_pull_2(status, **kwargs):\n      print('함수 입력값으로 받은 값:' + status)\n\n\n  python_xcom_push_by_return = xcom_push_result() \n  # airflow의 task decorator가 쓰였기 때문에 python_xcom_push_by_return에 \n  # 단순한 'Sucess' 스트링이 할당되는게 아니라 decorator object가 할당된다.\n  xcom_pull_2(python_xcom_push_by_return)\n  python_xcom_push_by_return &gt;&gt; xcom_pull_1()\n\n  # 암묵적인 task flow는\n  # xcom_push_result &gt;&gt;[xcom_pull_2, xcom_pull_1] 형태임\n```\n2안 DAG Full Example ```markdown from airflow import DAG import pendulum import datetime from airflow.decorators import task\nwith DAG( dag_id=“dags_python_with_xcom_eg1”, schedule=“30 6 * * *“, start_date=pendulum.datetime(2023, 3, 1, tz=”Asia/Seoul”), catchup=False ) as dag:\n  @task(task_id='python_xcom_push_task1')\n  def xcom_push1(**kwargs):\n      ti = kwargs['ti']\n      ti.xcom_push(key=\"result1\", value=\"value_1\")\n      ti.xcom_push(key=\"result2\", value=[1,2,3])\n\n  @task(task_id='python_xcom_push_task2')\n  def xcom_push2(**kwargs):\n      ti = kwargs['ti']\n      ti.xcom_push(key=\"result1\", value=\"value_2\") \n      # python_xcom_push_task1의 key값은 같지만 value는 다름\n      ti.xcom_push(key=\"result2\", value=[1,2,3,4])\n\n  @task(task_id='python_xcom_pull_task')\n  def xcom_pull(**kwargs):\n      ti = kwargs['ti']\n      value1 = ti.xcom_pull(key=\"result1\")\n      value2 = ti.xcom_pull(key=\"result2\", task_ids='python_xcom_push_task1')\n      print(value1)\n      print(value2)\n\n\n  xcom_push1() &gt;&gt; xcom_push2() &gt;&gt; xcom_pull()\n  # xcom_pull()에서 key값이 result1으로만 명시되었기 때문에 value1에는 xcom_push2()의 'value_2'가 들어감    \n```\n\nairflow web service에서 log 대신 xcom을 사용해 결과값을 확인\n\n\n\n\n\nXcom push 방법\n\nti.xcom_push 명시적 사용\n함수 return\n\nXcom pull 방법\n\nti.xcom_pull 명시적 사용\nreturn 값을 input으로 사용\n\n\n\n\n\n\n\n\n\nBash 오퍼레이터에서 template 문법을 쓸수 있는 parameters: env, bash_command\ntemplate 이용하여 push/pull\n\nbash_push = BashOperator(\n    task_id='bash_push',\n    bash_command=\"echo START && \"\n                \"echo XCOM_PUSHED \"\n                \"{{ ti.xcom_push(key='bash_pushed',value='first_bash_message') }} && \"\n                \"echo COMPLETE\" \n                # bash 같은 경우엔 출력하는 값이 return값으로 간주됨. \n                # 위의 경우와 같이 여러 출력물(&&로 연결된 3개의 출력물)이 있을 경우 마지막 출력물(COMPLETE)이 자동으로 return_value 에 저장됨\n)\nbash_pull = BashOperator(\n    task_id='bash_pull',\n    env={'PUSHED_VALUE':\"{{ ti.xcom_pull(key='bash_pushed') }}\",\n        'RETURN_VALUE':\"{{ ti.xcom_pull(task_ids='bash_push') }}\"}, \n        # env 는 key: value 형태로 데이터를 받음\n        # task_ids 만 지정하면 key='return_value' 를 의미함\n        # RETURN_VALUE에 'complete'이 들어감\n    bash_command=\"echo $PUSHED_VALUE && echo $RETURN_VALUE \",\n    do_xcom_push=False \n    # bash_command에서 출력되는 \"echo $PUSHED_VALUE && echo $RETURN_VALUE \"의 \n    # 출력문을 자동으로 xcom에 올리지 말라는 의미\n)\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_with_xcom\",\n    schedule=\"10 0 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    bash_push = BashOperator(\n    task_id='bash_push',\n    bash_command=\"echo START && \"\n                 \"echo XCOM_PUSHED \"\n                 \"{{ ti.xcom_push(key='bash_pushed',value='first_bash_message') }} && \"\n                 \"echo COMPLETE\"\n    )\n\n    bash_pull = BashOperator(\n        task_id='bash_pull',\n        env={'PUSHED_VALUE':\"{{ ti.xcom_pull(key='bash_pushed') }}\",\n            'RETURN_VALUE':\"{{ ti.xcom_pull(task_ids='bash_push') }}\"},\n        bash_command=\"echo $PUSHED_VALUE && echo $RETURN_VALUE \",\n        do_xcom_push=False\n    )\n\n    bash_push &gt;&gt; bash_pull\n\n\n\n\nBash_command에 의해 출력된 값은 자동으로 return_value로 저장된다 (마지막 출력 문장만)\nreturn_value를 꺼낼 때는 xcom_pull에서 task_ids 값만 줘도 된다.\n키가 지정된 xcom 값을 꺼낼 때는 key 값만 줘도 된다 (단, 다른 task에서 동일 key로 push 하지 않았을 때만)\n\n\n\n\n\n\n\n@task task_id =='python push'\ndef python_push_xcom\n    result_dict = {'status':' Good','data':[1,2,3],'options_cnt': 100}\n    return result_dict\nbash_pull = BashOperator(\n    task_id='bash_pull',\n    env={\n        'STATUS': '{{ti.xcom_pull(task ids=\"python push\")[\"status\"]}}', #task_ids만 있으면 위의 파이썬 함수에서 리턴값을 자동으로 받음\n        'DATA': '{{ti.xcom_pull(task ids=\"python push\")[\"data\"]}}',\n        'OPTIONS_CNT': '{{ti.xcom_pull(task_ids=\"python_push\")[\"options_cnt\"]}}'\n    },\n    bash_command = 'echo $STATUS && echo $DATA && echo $OPTIONS_CNT'\n)\n \npython_push_xcom() &gt;&gt; bash_pull\n\n\n\nbash_push = BashOperator(\ntask_id ='bash_push',\nbash_command='echo PUSH_START'\n    '{{ti.xcom_push(key=\"bash_pushed\",value=200) }}&& 'echo PUSH_COMPLETE'\n)\n\n@task(task_id =='python_pull')\ndef python_pull_xcom(**kwargs): \n    ti = kwargs ['ti']\n    status_value= ti.xcom_pull(key ='bash_pushed')\n    return_value= ti.xcom_pull(task_ids ='bash_push')\n    print('status_value:'+ str (status_value))\n    print('return_value:'+ return_value)\n    bash_push&gt;&gt; python_pull_xcom()\n\nDAG Full Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_python_with_xcom\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    @task(task_id='python_push')\n    def python_push_xcom():\n        result_dict = {'status':'Good','data':[1,2,3],'options_cnt':100}\n        return result_dict\n\n    bash_pull = BashOperator(\n        task_id='bash_pull',\n        env={\n            'STATUS':'{{ti.xcom_pull(task_ids=\"python_push\")[\"status\"]}}',\n            'DATA':'{{ti.xcom_pull(task_ids=\"python_push\")[\"data\"]}}',\n            'OPTIONS_CNT':'{{ti.xcom_pull(task_ids=\"python_push\")[\"options_cnt\"]}}'\n\n        },\n        bash_command='echo $STATUS && echo $DATA && echo $OPTIONS_CNT'\n    )\n    python_push_xcom() &gt;&gt; bash_pull\n\n    bash_push = BashOperator(\n    task_id='bash_push',\n    bash_command='echo PUSH_START '\n                 '{{ti.xcom_push(key=\"bash_pushed\",value=200)}} && '\n                 'echo PUSH_COMPLETE'\n    )\n\n    @task(task_id='python_pull')\n    def python_pull_xcom(**kwargs):\n        ti = kwargs['ti']\n        status_value = ti.xcom_pull(key='bash_pushed')\n        return_value = ti.xcom_pull(task_ids='bash_push')\n        print('status_value:' + str(status_value))\n        print('return_value:' + return_value)\n\n    bash_push &gt;&gt; python_pull_xcom()\n\n\n\n\n\n\n\nEmail 오퍼레이터를 이용하여 Xcom을 받아와야함\nEmail 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nto\nsubject\nhtml_content\nfiles\ncc\nbcc\nnime_subtype\nmime_charset\ncustom_headers\n\n\n@task(task_id='something_task') # python operator를 task decorator로 만듦\ndef some_logic(**kwargs):\n    from random import choice \n    #choice 함수: list, tuple, string 중 아무 값이나 꺼낼 수 있게 해주는 함수\n    return choice(['Success','Fail']) # either Success or Fail is return됨\nsend_email = EmailOperator(\n    task_id='send_email',\n    to='hjkim_sun@naver.com',\n    subject='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} some_logic 처리결과',\n    html_content='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} 처리 결과는 &lt;br&gt; \\ {{ti.xcom_pull(task_ids=\"something_task\")}} 했습니다 &lt;br&gt;'\n)\n\nDAG Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.decorators import task\nfrom airflow.operators.email import EmailOperator\n\nwith DAG(\n    dag_id=\"dags_python_email_operator\",\n    schedule=\"0 8 1 * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    @task(task_id='something_task')\n    def some_logic(**kwargs):\n        from random import choice \n        return choice(['Success','Fail'])\n\n\n    send_email = EmailOperator(\n        task_id='send_email',\n        to='hjkim_sun@naver.com',\n        subject='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} some_logic 처리결과',\n        html_content='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} 처리 결과는 &lt;br&gt; \\\n                    {{ti.xcom_pull(task_ids=\"something_task\")}} 했습니다 &lt;br&gt;'\n    )\n\n    some_logic() &gt;&gt; send_email\n\n\n\n\n\n\nXcom: 특정 DAG, 특정 schedule 에 수행되는 Task 간에만 공유 (즉, 어제 수행한 task와 오늘 수행한 task간에는 xcom을 사용하여 데이터 공유가 안됨)\nvariable: 모든 DAG 이 공유할 수 있는 전역 변수 사용\nVariable 등록하기\n\nairflow web service에서 전역 변수 등록 가능\n\nairflow web service의 Admin &gt;&gt; Variables &gt;&gt; Plus Button &gt;&gt; Key, Val, Description 작성 &gt;&gt; save\n\n전역 변수 사용하기: 실제 Variable 의 Key, Value 값은 메타 DB 에 저장됨 (variable 테이블)\n\n방법1) Variable 라이브러리 이용 , 파이썬 문법을 이용해 미리 가져오기\n\nfrom airflow operators bash import BashOperator\nfrom airflow models import Variable \n\nvar_value = Variable.get('sample_key')\nbash_var_1= BashOperator(\n    task_id = \"bash_var_1\",\n    bash_command = f \"echo variable:{var_value}\"\" \n)\n\n스케줄러의 주기적 DAG 파싱시 Variable.get 개수만큼 DB 연결을 일으켜 불필요한 부하 발생 스케줄러 과부하 원인 중 하나 (권고하지 않음)\n\n주기적으로 아래 코드를 실행함\n\nfrom airflow models import Variable \nvar_value = Variable.get('sample_key')\n\n\n방법2) Jinja 템플릿 이용 , 오퍼레이터 내부에서 가져오기 (권고)\n\n스케쥴러는 Operator 안에 작성된 내용은 parsing 및 실행해보지 않음\n\nfrom airflow operators bash import BashOperator\nbash_var_2= BashOperator(\ntask_id=\"bash_var_2\",\nbash_command= f \"echo variable: {{var.value.sample_key}}\"\n)\n\n\n그런데 이 전역변수는 언제 , 어떻게 쓰면 좋을까\n\n협업 환경에서 표준화된 dag 을 만들기 위해 주로 사용. 개발자들마다 서로 다르게 사용하지 말아야할 주로 상수 (CONST) 로 지정해서 사용할 변수들 셋팅할 때 사용\n예) base_sh_dir = /opt/airflow/plugins/shell. shell file 의 위치를 고정\n예) base_file_dir = /opt/airflow/plugins/files\n예) email, Alert 메시지를 받을 담당자의 email 주소 정보\n\nDags Full Example\n\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.bash import BashOperator\nfrom airflow.models import Variable\n\nwith DAG(\n    dag_id=\"dags_bash_with_variable\",\n    schedule=\"10 9 * * *\",\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    #권고하지 않음\n    var_value = Variable.get(\"sample_key\")\n    bash_var_1 = BashOperator(\n    task_id=\"bash_var_1\",\n    bash_command=f\"echo variable:{var_value}\"\n    )\n\n    #권고함\n    bash_var_2 = BashOperator(\n    task_id=\"bash_var_2\",\n    bash_command=\"echo variable:{{var.value.sample_key}}\"\n    )\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#python-오퍼레이터에서-xcom-사용하기",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#python-오퍼레이터에서-xcom-사용하기",
    "title": "Data Share",
    "section": "",
    "text": "크게 두 가지 방법으로 Xcom 사용 가능\n\n**kwargs에 존재하는 ti (task_instance) 객체 활용\n\n@task(task_id='python_xcom_push_task')\ndef xcom_push(**kwargs):\n    ti = kwargs['ti']\n    ti.xcom_push(key=\"result1\", value=\"value_1\") \n    ti.xcom_push(key=\"result2\", value=[1,2,3])\n    #xcom_push: xcom에다가 data를 올릴 수 있음\n    #data를 올릴 때는 key:value 형태로 올리기\n    #template 변수에서 task_instance 라는 객체를 얻을 수 있으며 task_instance 객체가 가진 xcom_push 메서드를 활용할 수 있음\n\n@task(task_id='python_xcom_pull_task')\ndef xcom_pull(**kwargs):\n    ti = kwargs['ti']\n    value_key1 = ti.xcom_pull(key=\"result1\") # value_1이 value_key1에 저장됨\n    value_key2 = ti.xcom_pull(key=\"result2\",\n    task_ids='python_xcom_push_task') # [1,2,3]이 value_key2에 저장됨\n    #xcom_pull: xcom으로부터 data를 내려 받을 수 있음\n    #data를 올릴 때는 key:value 형태로 올리기\n    print(value_key1)\n    print(value_key2)\n\nxcome_pull()을 할때 key값만 줘도 되고 key값과 task_ids값을 둘다 줘도 된다.\n\nkey값만 줘도 될때\n\nxcom_push를 한 task가 1개 밖에 없을 때 사용 가능\n혹은, key값이 중복될 때 xcom_push를 한 task가 여러 개 있을 때도 사용 가능한데 가장 마지막 (최신) task의 key값을 호출 한다.\n만약, key값이 중복이 되지 않는 다면 key값만으로도 data를 내려 받을 수 있다.\n\nkey값과 task_ids둘다 줘야할 때\n\nkey값이 중복되는 xcom_push를 한 task가 여러 개 있을 때 선택적으로 원하는 task의 data를 가지고 오고 싶으면 해당 task의 task_ids를 명시적으로 적어줘야한다.\n\n예를 들어,\n\n# 5개의 tasks 존재하는\n# task1: xcom_push(key='result1'...)\n# task2: xcom_push(key='result1'...)\n# task3: xcom_push(key='result2'...)\n# task4: xcom_pull(key='result1'...)\n# task5: xcom_pull(key='result1',task_ids=...)\n\ntask4가 수행이 될때 task1의 xcom을 가져우는게 아니라 가장 최신에 수행된 task2의 xcom을 가져오게 된다.\ntask1의 xcom을 가지고 오고 싶을땐 task5와 같이 task1의 task_id를 task5의 task_ids에 명시해주면 된다.\n가장 안전한 방법은 task의 key값과 task_ids를 명시적으로 적어주는 것이다. 아니면 tasks의 key값을 절대 중복이 되지않도록 적어주는 것이다.\n\n\n\n파이썬 함수의 return 값 활용\n\n(1안)\n\n@task(task_id='xcom_push_by_return')\ndef xcom_push_by_return(**kwargs):\n    transaction_value = 'status Good'\n    return transaction_value\n@task(task_id='xcom_pull_by_return')\ndef xcom_pull_by_return(status, **kwargs):\n    print(status)\nxcom_pull_by_return(xcom_push_by_return()) \n\nxcom을 이용한 task의 flow 정해주는 또 다른 방식\n암묵적인 task의 순서: xcom_push_by_return() &gt;&gt; xcom_pull_by_return()\n위의 스크립트에서 xcom_pull() 또는 xcom_push()가 명시적으로 쓰이지진 않았지만 airflow에서는 Task 데커레이터 사용시 함수 입력/출력 관계만으로 Task flow 정의가 된다. 즉, xcom_pull_by_return(xcom_push_by_return()) = xcom_push_by_return() &gt;&gt; xcom_pull_by_return()\nTask 데커레이터 사용시 custom 함수가 return을 하게 되면 자동으로 xcom에 data가 올라가게 된다.\n(2안)\n\n\n@task(task_id='xcom_push_by_return')\ndef xcom_push_return(**kwargs):\n    transaction_value = 'status Good'\n    return transaction_value\n    # return 한 값은 자동으로 xcom에 key='return_value', task_ids=task_id 로 저장됨\n\n@task(task_id='xcom_pull_by_return')\ndef xcom_pull_return_by_method(**kwargs):\n    ti = kwargs['ti']\n    pull_value = ti.xcom_pull(key='return_value', task_ids='xcom_push_by_return')\n    # ti.xcom_pull()을 이용하여 return 한 값을 꺼낼 때는 key를 명시하지 않아도 됨. (자동으로 key=return_value 를 찾음)\n    # task_ids='xcom_push_by_return' return한 Task가 여러개 있을 때는 task_ids 를 지정\n    print(pull_value)\n\nxcom_push_by_return() &gt;&gt; xcom_pull_by_return() # 2안에서는 task flow를 명시적으로 적어줘야함.\n\nDAG Full Example\n\n1안 DAG Full Exmaple ```markdown from airflow import DAG import pendulum import datetime from airflow.decorators import task\nwith DAG( dag_id=“dags_python_with_xcom_eg2”, schedule=“30 6 * * *“, start_date=pendulum.datetime(2023, 3, 1, tz=”Asia/Seoul”), catchup=False ) as dag:\n  @task(task_id='python_xcom_push_by_return')\n  def xcom_push_result(**kwargs):\n      return 'Success'\n\n\n  @task(task_id='python_xcom_pull_1')\n  def xcom_pull_1(**kwargs):\n      ti = kwargs['ti']\n      value1 = ti.xcom_pull(task_ids='python_xcom_push_by_return')\n      print('xcom_pull 메서드로 직접 찾은 리턴 값:' + value1)\n\n  @task(task_id='python_xcom_pull_2')\n  def xcom_pull_2(status, **kwargs):\n      print('함수 입력값으로 받은 값:' + status)\n\n\n  python_xcom_push_by_return = xcom_push_result() \n  # airflow의 task decorator가 쓰였기 때문에 python_xcom_push_by_return에 \n  # 단순한 'Sucess' 스트링이 할당되는게 아니라 decorator object가 할당된다.\n  xcom_pull_2(python_xcom_push_by_return)\n  python_xcom_push_by_return &gt;&gt; xcom_pull_1()\n\n  # 암묵적인 task flow는\n  # xcom_push_result &gt;&gt;[xcom_pull_2, xcom_pull_1] 형태임\n```\n2안 DAG Full Example ```markdown from airflow import DAG import pendulum import datetime from airflow.decorators import task\nwith DAG( dag_id=“dags_python_with_xcom_eg1”, schedule=“30 6 * * *“, start_date=pendulum.datetime(2023, 3, 1, tz=”Asia/Seoul”), catchup=False ) as dag:\n  @task(task_id='python_xcom_push_task1')\n  def xcom_push1(**kwargs):\n      ti = kwargs['ti']\n      ti.xcom_push(key=\"result1\", value=\"value_1\")\n      ti.xcom_push(key=\"result2\", value=[1,2,3])\n\n  @task(task_id='python_xcom_push_task2')\n  def xcom_push2(**kwargs):\n      ti = kwargs['ti']\n      ti.xcom_push(key=\"result1\", value=\"value_2\") \n      # python_xcom_push_task1의 key값은 같지만 value는 다름\n      ti.xcom_push(key=\"result2\", value=[1,2,3,4])\n\n  @task(task_id='python_xcom_pull_task')\n  def xcom_pull(**kwargs):\n      ti = kwargs['ti']\n      value1 = ti.xcom_pull(key=\"result1\")\n      value2 = ti.xcom_pull(key=\"result2\", task_ids='python_xcom_push_task1')\n      print(value1)\n      print(value2)\n\n\n  xcom_push1() &gt;&gt; xcom_push2() &gt;&gt; xcom_pull()\n  # xcom_pull()에서 key값이 result1으로만 명시되었기 때문에 value1에는 xcom_push2()의 'value_2'가 들어감    \n```\n\nairflow web service에서 log 대신 xcom을 사용해 결과값을 확인"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#summary",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#summary",
    "title": "Data Share",
    "section": "",
    "text": "Xcom push 방법\n\nti.xcom_push 명시적 사용\n함수 return\n\nXcom pull 방법\n\nti.xcom_pull 명시적 사용\nreturn 값을 input으로 사용"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#bash-오퍼레이터에서-xcom-사용하기",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#bash-오퍼레이터에서-xcom-사용하기",
    "title": "Data Share",
    "section": "",
    "text": "Bash 오퍼레이터에서 template 문법을 쓸수 있는 parameters: env, bash_command\ntemplate 이용하여 push/pull\n\nbash_push = BashOperator(\n    task_id='bash_push',\n    bash_command=\"echo START && \"\n                \"echo XCOM_PUSHED \"\n                \"{{ ti.xcom_push(key='bash_pushed',value='first_bash_message') }} && \"\n                \"echo COMPLETE\" \n                # bash 같은 경우엔 출력하는 값이 return값으로 간주됨. \n                # 위의 경우와 같이 여러 출력물(&&로 연결된 3개의 출력물)이 있을 경우 마지막 출력물(COMPLETE)이 자동으로 return_value 에 저장됨\n)\nbash_pull = BashOperator(\n    task_id='bash_pull',\n    env={'PUSHED_VALUE':\"{{ ti.xcom_pull(key='bash_pushed') }}\",\n        'RETURN_VALUE':\"{{ ti.xcom_pull(task_ids='bash_push') }}\"}, \n        # env 는 key: value 형태로 데이터를 받음\n        # task_ids 만 지정하면 key='return_value' 를 의미함\n        # RETURN_VALUE에 'complete'이 들어감\n    bash_command=\"echo $PUSHED_VALUE && echo $RETURN_VALUE \",\n    do_xcom_push=False \n    # bash_command에서 출력되는 \"echo $PUSHED_VALUE && echo $RETURN_VALUE \"의 \n    # 출력문을 자동으로 xcom에 올리지 말라는 의미\n)\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_with_xcom\",\n    schedule=\"10 0 * * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    bash_push = BashOperator(\n    task_id='bash_push',\n    bash_command=\"echo START && \"\n                 \"echo XCOM_PUSHED \"\n                 \"{{ ti.xcom_push(key='bash_pushed',value='first_bash_message') }} && \"\n                 \"echo COMPLETE\"\n    )\n\n    bash_pull = BashOperator(\n        task_id='bash_pull',\n        env={'PUSHED_VALUE':\"{{ ti.xcom_pull(key='bash_pushed') }}\",\n            'RETURN_VALUE':\"{{ ti.xcom_pull(task_ids='bash_push') }}\"},\n        bash_command=\"echo $PUSHED_VALUE && echo $RETURN_VALUE \",\n        do_xcom_push=False\n    )\n\n    bash_push &gt;&gt; bash_pull"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#summary-1",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#summary-1",
    "title": "Data Share",
    "section": "",
    "text": "Bash_command에 의해 출력된 값은 자동으로 return_value로 저장된다 (마지막 출력 문장만)\nreturn_value를 꺼낼 때는 xcom_pull에서 task_ids 값만 줘도 된다.\n키가 지정된 xcom 값을 꺼낼 때는 key 값만 줘도 된다 (단, 다른 task에서 동일 key로 push 하지 않았을 때만)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#python-rightarrow-bash-오퍼레이터-xcom-전달",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#python-rightarrow-bash-오퍼레이터-xcom-전달",
    "title": "Data Share",
    "section": "",
    "text": "@task task_id =='python push'\ndef python_push_xcom\n    result_dict = {'status':' Good','data':[1,2,3],'options_cnt': 100}\n    return result_dict\nbash_pull = BashOperator(\n    task_id='bash_pull',\n    env={\n        'STATUS': '{{ti.xcom_pull(task ids=\"python push\")[\"status\"]}}', #task_ids만 있으면 위의 파이썬 함수에서 리턴값을 자동으로 받음\n        'DATA': '{{ti.xcom_pull(task ids=\"python push\")[\"data\"]}}',\n        'OPTIONS_CNT': '{{ti.xcom_pull(task_ids=\"python_push\")[\"options_cnt\"]}}'\n    },\n    bash_command = 'echo $STATUS && echo $DATA && echo $OPTIONS_CNT'\n)\n \npython_push_xcom() &gt;&gt; bash_pull"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#bash-rightarrow-python-오퍼레이터-xcom-전달",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#bash-rightarrow-python-오퍼레이터-xcom-전달",
    "title": "Data Share",
    "section": "",
    "text": "bash_push = BashOperator(\ntask_id ='bash_push',\nbash_command='echo PUSH_START'\n    '{{ti.xcom_push(key=\"bash_pushed\",value=200) }}&& 'echo PUSH_COMPLETE'\n)\n\n@task(task_id =='python_pull')\ndef python_pull_xcom(**kwargs): \n    ti = kwargs ['ti']\n    status_value= ti.xcom_pull(key ='bash_pushed')\n    return_value= ti.xcom_pull(task_ids ='bash_push')\n    print('status_value:'+ str (status_value))\n    print('return_value:'+ return_value)\n    bash_push&gt;&gt; python_pull_xcom()\n\nDAG Full Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"dags_bash_python_with_xcom\",\n    schedule=\"30 9 * * *\",\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n\n    @task(task_id='python_push')\n    def python_push_xcom():\n        result_dict = {'status':'Good','data':[1,2,3],'options_cnt':100}\n        return result_dict\n\n    bash_pull = BashOperator(\n        task_id='bash_pull',\n        env={\n            'STATUS':'{{ti.xcom_pull(task_ids=\"python_push\")[\"status\"]}}',\n            'DATA':'{{ti.xcom_pull(task_ids=\"python_push\")[\"data\"]}}',\n            'OPTIONS_CNT':'{{ti.xcom_pull(task_ids=\"python_push\")[\"options_cnt\"]}}'\n\n        },\n        bash_command='echo $STATUS && echo $DATA && echo $OPTIONS_CNT'\n    )\n    python_push_xcom() &gt;&gt; bash_pull\n\n    bash_push = BashOperator(\n    task_id='bash_push',\n    bash_command='echo PUSH_START '\n                 '{{ti.xcom_push(key=\"bash_pushed\",value=200)}} && '\n                 'echo PUSH_COMPLETE'\n    )\n\n    @task(task_id='python_pull')\n    def python_pull_xcom(**kwargs):\n        ti = kwargs['ti']\n        status_value = ti.xcom_pull(key='bash_pushed')\n        return_value = ti.xcom_pull(task_ids='bash_push')\n        print('status_value:' + str(status_value))\n        print('return_value:' + return_value)\n\n    bash_push &gt;&gt; python_pull_xcom()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/06.data_share.html#python-email-오퍼레이터-xcom-전달",
    "href": "docs/blog/posts/Engineering/airflow/06.data_share.html#python-email-오퍼레이터-xcom-전달",
    "title": "Data Share",
    "section": "",
    "text": "Email 오퍼레이터를 이용하여 Xcom을 받아와야함\nEmail 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?\n파라미터\n\nto\nsubject\nhtml_content\nfiles\ncc\nbcc\nnime_subtype\nmime_charset\ncustom_headers\n\n\n@task(task_id='something_task') # python operator를 task decorator로 만듦\ndef some_logic(**kwargs):\n    from random import choice \n    #choice 함수: list, tuple, string 중 아무 값이나 꺼낼 수 있게 해주는 함수\n    return choice(['Success','Fail']) # either Success or Fail is return됨\nsend_email = EmailOperator(\n    task_id='send_email',\n    to='hjkim_sun@naver.com',\n    subject='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} some_logic 처리결과',\n    html_content='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} 처리 결과는 &lt;br&gt; \\ {{ti.xcom_pull(task_ids=\"something_task\")}} 했습니다 &lt;br&gt;'\n)\n\nDAG Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.decorators import task\nfrom airflow.operators.email import EmailOperator\n\nwith DAG(\n    dag_id=\"dags_python_email_operator\",\n    schedule=\"0 8 1 * *\",\n    start_date=pendulum.datetime(2023, 3, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    @task(task_id='something_task')\n    def some_logic(**kwargs):\n        from random import choice \n        return choice(['Success','Fail'])\n\n\n    send_email = EmailOperator(\n        task_id='send_email',\n        to='hjkim_sun@naver.com',\n        subject='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} some_logic 처리결과',\n        html_content='{{ data_interval_end.in_timezone(\"Asia/Seoul\") | ds }} 처리 결과는 &lt;br&gt; \\\n                    {{ti.xcom_pull(task_ids=\"something_task\")}} 했습니다 &lt;br&gt;'\n    )\n\n    some_logic() &gt;&gt; send_email"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\nTask 분기처리가 필요한 이유\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask2_1\n\ntask2_1\n\n\n\ntask1-&gt;task2_1\n\n\n\n\n\ntask2_2\n\ntask2_2\n\n\n\ntask1-&gt;task2_2\n\n\n\n\n\ntask2_3\n\ntask2_3\n\n\n\ntask1-&gt;task2_3\n\n\n\n\n\n\n\n\n\n\n\n위와 같이 task1이 실행된 후 여러 후차적인 task를 병렬로 실행되어야 할 때\n\ntask flow에서 task1의 결과에 따라 선택적으로 task2-x 중 하나만 수행되도록 구성해야 할 때가 있다.\neg) Task1 의 결과로 ‘Good’,’Bad’,’Pending’ 이라는 결과 3 개 중 하나가 나오고 그에 따라 ask2-1 ~ task2-3 중 하나가 실행되도록 해야 할 경우\n\n\n\n\n\nTask 분기처리 방법 3가지\n\nBranchPythonOperator\ntask.branch decorator 이용\nBaseBranchOperator 클래스를 상속하여 직접 개발\n\n\n\n\n\ndef select_random():\n    import random\n\n    item_lst= ['A','B','C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A';\n        return 'task_a' # task_id를 string 값으로 return해야함\n    elif selected_item in ['B','C] \n        return ['task_b','task_c'] # 여러 task를 동시에 수행시킬 땐 string 리스트로 반환\n\n# 일반 operator의 parameter도 있음\npython_branch_task = BranchPythonOperator(\n    task_id ='python_branch_task',\n    python_callable=select_random #select_random function 호출\n)\n\npython_branch_task &gt;&gt; [task_a , task_b , task_c]\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\npython_branch_task\n\npython_branch_task\n\n\n\ntask_a\n\ntask_a\n\n\n\npython_branch_task-&gt;task_a\n\n\n\n\n\ntask_b\n\ntask_b\n\n\n\npython_branch_task-&gt;task_b\n\n\n\n\n\ntask_c\n\ntask_c\n\n\n\npython_branch_task-&gt;task_c\n\n\n\n\n\n\n\n\n\n\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.python import BranchPythonOperator\n\nwith DAG(\n    dag_id='dags_branch_python_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'), \n    schedule='0 1 * * *',\n    catchup=False\n) as dag:\n    def select_random():\n        import random\n\n        item_lst = ['A','B','C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a' # task_id를 string 값으로 return해야함\n        elif selected_item in ['B','C']:\n            return ['task_b','task_c'] # 여러 task를 동시에 수행시킬 땐 리스트로 반환\n    \n    # 일반 operator의 parameter도 있음\n    python_branch_task = BranchPythonOperator(\n        task_id='python_branch_task',\n        python_callable=select_random\n    )\n    \n    # 후행 task 3개\n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    python_branch_task &gt;&gt; [task_a, task_b, task_c]\n\n나의 경우 airflow web service상에서 1회 실행 시켰을 때 selected_item의 값이 task_b, task_b가 선택됐음\n\ngraph 버튼을 눌러 보면 가장 최근에 돌았던 task들이 return 된다.\ntask_a가 분홍색 박스로 skipped 상태인 것을 확인 할 수 있다.\ngraph에서 python_branch_task를 누르고 xcom을 누르면 다음과 같은 table을 확인할 수 있다.\n\n\n\n\nKey\nValue\n\n\n\n\nskipmixin_key\n{‘followed’: [‘task_c’, ‘task_b’]}\n\n\nreturn_value\n[‘task_b’, ‘task_c’]\n\n\n\n\n여기서 skipmixin_key 의 value값의 key 값이 ‘followed’ 이고 [‘task_c’, ‘task_b’] 인 것을 볼 수 있다. 필요시 어떤 task들이 선택되었는지 확인하려면 xcom을 통해 확인 가능하다.\nlog 를 보면\n\n[2023-06-23, 23:20:01 UTC] {python.py:183} INFO - Done. Returned value was: ['task_b', 'task_c']\n[2023-06-23, 23:20:01 UTC] {python.py:216} INFO - Branch callable return ['task_b', 'task_c']\n[2023-06-23, 23:20:01 UTC] {skipmixin.py:161} INFO - Following branch ['task_b', 'task_c']\n[2023-06-23, 23:20:01 UTC] {skipmixin.py:221} INFO - Skipping tasks ['task_a']    \n\n\n\n\n\n\n\n\n\n\nfrom airflow.operators.python import BranchPythonOperator\ndef select_random(): \n    import random\n    item_lst = ['A','B','C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A':\n        return 'task_a'\n    elif selected_item in ['B','C']\n        return ['task_b','task_c']\n\npython_branch_task = BranchPythonOperator(\n    task_id= 'branching',\n    python_callable = select_random\n)\npython_branch_task &gt;&gt; [task_a , task_b , task_c]\n\nfrom airflow.operators.python import task\n\n@task.branch(task_id='python_branch_task')\ndef select_random(): \n    import random\n    item_lst = ['A','B','C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A':\n        return 'task_a'\n    elif selected_item in ['B','C']\n        return ['task_b','task_c']\n\nselect_random() &gt;&gt; [task_a , task_b , task_c]\n\n\n\nBranchPythonOperator와 비교하여 select_random()을 호출 또는 맵핑 하는 방식이 decorator에서는 @task.branch(task_id='python_branch_task')으로 표현 되었고 task flow를 표현하는 task connection 방식도 select_random() &gt;&gt; [task_a , task_b , task_c] 로 표현 됐다.\nBranchPythonOperator의 python_branch_task object와 task.branch (decorator)의 select_random()는 사실상 같은 객체이다.\n차이점은 BranchPythonOperator(...)를 실행시킨 것과 select_random(...) 함수를 실행한 것 외엔 그 역할과 기능은 같다 (같은 object 반환).\nDags Full Example\n\nfrom airflow import DAG\nfrom datetime import datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id='dags_python_with_branch_decorator',\n    start_date=datetime(2023,4,1),\n    schedule=None,\n    catchup=False\n) as dag:\n    @task.branch(task_id='python_branch_task')\n    def select_random():\n        import random\n        item_lst = ['A', 'B', 'C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a'\n        elif selected_item in ['B','C']:\n            return ['task_b','task_c']\n    \n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    select_random() &gt;&gt; [task_a, task_b, task_c]\n\nairflow web service의 결과물은 BranchPythonOperator나 decorator나 같았음\n\n\n\n\n\nBaseBranchOperator 클래스 상속해서 직접 함수를 개발해서 사용해야함.\n\n\nfrom airflow.operators.branch import BaseBranchOperator\nwith DAG(...\n) as dag:\n    class CustomBranchOperator(BaseBranchOperator): #클래스 이름은 임의로 지정해 줌\n    #Python의 class 상속 syntax: class MyclassName(상속할className):\n    #Python은 다중 상속가능\n        def choose_branch(self,context): \n        # 함수 재정의 : Overriding, 함수 이름 바꾸면 안됨!\n        # parameter도 바꾸면 안됨\n            import random\n            print(context) # context에 어떤 내용이 있는지 출력\n\n            item_lst = ['A', 'B','C]\n            selected_item = random.choice(item_lst)\n            if selected_item == 'A':\n                return 'task_a'\n            elif selected_item in ['B','C']:\n                return ['task_b','task_c']\n\ncustom_branch_operator = CustomBranchOperator(task_id ='python_branch_task') # 클래스 실행하여 custom_branch_operator object 생성\ncustom_branch_operator &gt;&gt; [task_a , task_b , task_c]\n\n클래스 상속하여 새로운 클래스 만들어야함: BaseBranchOperator 상속시 choose_branch 함수를 구현해 줘야 함\nCustomBranchOperator 클래스 이름은 임의로 지정해준 이름\nclass 선언시 class childClass(상속할parentClass): 상속할 부모클래스를 2개이상 지정하는 다중 상속이 가능하긴 하지만 권고하지 않음.\nchoose_branch() 함수를 만든 이유를 알기 위해선 BaseBranchOperator class에 대해서 알아야함\n\nairflow operators-airflow.operators.branch or google ‘airflow operators’ :::{.callout-note} ## Description\n\nBases: airflow.models.baseoperator.BaseOperator, airflow.models.skipmixin.SkipMixin A base class for creating operators with branching functionality, like to BranchPythonOperator. Users should create a subclass from this operator and implement the function choose_branch(self, context). This should run whatever business logic is needed to determine the branch, and return either the task_id for a single task (as a str) or a list of task_ids. The operator will continue with the returned task_id(s), and all other tasks directly downstream of this operator will be skipped. :::\n\n함수명과 인자(argument)명도 반드시 일치시켜야함\nchoose_branch(self,context)의 context는 pythonOperator 쓸때 **kwargs의 parameters들을 사용할 수 있게 해주는 parameter\n\ncontext 인자엔 op_kargs와 같이 data_interval_start, data_interval_end 등과 같은 정보를 제공해주는 인자\n\nprint(context) 결과\n\n[2023-06-24, 00:29:33 UTC] {logging_mixin.py:149} INFO - {'conf': &lt;***.configuration.AirflowConfigParser object at 0x7fc3d5dd2cd0&gt;, 'dag': &lt;DAG: dags_base_branch_operator&gt;, 'dag_run': &lt;DagRun dags_base_branch_operator @ 2023-06-24 00:29:31.444830+00:00: manual__2023-06-24T00:29:31.444830+00:00, state:running, queued_at: 2023-06-24 00:29:31.455604+00:00. externally triggered: True&gt;, 'data_interval_end': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'data_interval_start': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'ds': '2023-06-24', 'ds_nodash': '20230624', 'execution_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'expanded_ti_count': None, 'inlets': [], 'logical_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'macros': &lt;module '***.macros' from '/home/***/.local/lib/python3.7/site-packages/***/macros/__init__.py'&gt;, 'next_ds': '2023-06-24', 'next_ds_nodash': '20230624', 'next_execution_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'outlets': [], 'params': {}, 'prev_data_interval_start_success': None, 'prev_data_interval_end_success': None, 'prev_ds': '2023-06-24', 'prev_ds_nodash': '20230624', 'prev_execution_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'prev_execution_date_success': None, 'prev_start_date_success': None, 'run_id': 'manual__2023-06-24T00:29:31.444830+00:00', 'task': &lt;Task(CustomBranchOperator): python_branch_task&gt;, 'task_instance': &lt;TaskInstance: dags_base_branch_operator.python_branch_task manual__2023-06-24T00:29:31.444830+00:00 [running]&gt;, 'task_instance_key_str': 'dags_base_branch_operator__python_branch_task__20230624', 'test_mode': False, 'ti': &lt;TaskInstance: dags_base_branch_operator.python_branch_task manual__2023-06-24T00:29:31.444830+00:00 [running]&gt;, 'tomorrow_ds': '2023-06-25', 'tomorrow_ds_nodash': '20230625', 'triggering_dataset_events': &lt;Proxy at 0x7fc3ab28c8c0 with factory &lt;function TaskInstance.get_template_context.&lt;locals&gt;.get_triggering_events at 0x7fc3ab277c20&gt;&gt;, 'ts': '2023-06-24T00:29:31.444830+00:00', 'ts_nodash': '20230624T002931', 'ts_nodash_with_tz': '20230624T002931.444830+0000', 'var': {'json': None, 'value': None}, 'conn': None, 'yesterday_ds': '2023-06-23', 'yesterday_ds_nodash': '20230623'}\ncontext결과물은 위와 같은 시간 정보를 담고 있기 때문에 꺼내쓸 수 있다.\n분기 처리 결과는 다른 2 방식의 결과와 같음\nDAG Full Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.branch import BaseBranchOperator\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n    dag_id='dags_base_branch_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n    class CustomBranchOperator(BaseBranchOperator):\n        def choose_branch(self, context):\n            import random\n            print(context) # context에 어떤 내용이 있는지 출력\n            \n            item_lst = ['A', 'B', 'C']\n            selected_item = random.choice(item_lst)\n            if selected_item == 'A':\n                return 'task_a'\n            elif selected_item in ['B','C']:\n                return ['task_b','task_c']\n\n    \n    custom_branch_operator = CustomBranchOperator(task_id='python_branch_task')\n\n    \n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    custom_branch_operator &gt;&gt; [task_a, task_b, task_c]\n\n\n\n\nTask 분기처리 방법\n\nBranchPythonOperator (자주 사용)\ntask.branch 데커레이터 이용 (자주 사용)\nBaseBranchOperator 상속 , choose_branch 를 재정의해야 함 (덜 사용)\n\n공통적으로 리턴 값으로 후행 Task 의 id 를 str 또는 list 로 리턴해야 함\n3가지 분기처리 방법은 방법만 다를 뿐 결과는 동일함\n3 보다는 1 또는 2를 주로 사용함\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask4\n\ntask4\n\n\n\ntask1-&gt;task4\n\n\n\n\n\ntask2\n\ntask2\n\n\n\ntask2-&gt;task4\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask3-&gt;task4\n\n\n\n\n\n\n\n\n\n\n\nbranch와 반대되는 개념으로 여러 상위 tasks가 하나의 하위 task로 연결되는 flow로 만들때 사용\n즉, 여러 상위 Task 들의 상태에 따라 후행 task의 수행여부 결정할 때 쓰인다\n기본 값 : 여러 상위 Task들이 모두 성공시에만 수행\n상위 task의 수행 상태에 따라 조건적으로 후행 task의 수행 여부를 결정할 수 있다.\ntrigger option은 하위 task를 이용하여 줄 수 있다.\n모든 airflow operator는 trigger rule option을 줄 수 있다.\n11가지 trigger rules\n\n\n\n\n\n\n\n\nDefault\nLeft\n\n\n\n\nall_success (default)\n상위 tasks 가 모두 성공하면 실행\n\n\nall_failed\n상위 tasks 가 모두 실패하면 실행\n\n\nall_done\n상위 tasks 가 모두 수행되면 실행 (실패도 수행된것에 포함)\n\n\nall_skipped\n상위 tasks 가 모두 Skipped 상태면 실행\n\n\none_failed\n상위 tasks 중 하나 이상 실패하면 실행 (모든 상위 Tasks의 완료를 기다리지 않음)\n\n\none_success\n상위 tasks 중 하나 이상 성공하면 실행 (모든 상위 Tasks의 완료를 기다리지 않음)\n\n\none_done\n상위 tasks 중 하나 이상 성공 또는 실패 하면 실행\n\n\nnone_failed\n상위 task s중 실패가 없는 경우 실행 (성공 또는 Skipped 상태)\n\n\nnone_failed_min_one_success\n상위 tasks 중 실패가 없고 성공한 Task가 적어도 1개 이상이면 실행\n\n\nnone_skipped\nSkip된 상위 Task가 없으면 실행 (상위 Task가 성공, 실패하여도 무방)\n\n\nalways\n언제나 실행\n\n\n\n\n위의 표에서 모든 상위 task를 기다리지 않음은 각 각의 상위 task들의 처리 시간이 다를 때 가장 빠르게 처리되는 상위 task에 따라서 후행 task가 수행된다는 것을 의미한다. 예를 들어, one_failed의 경우\n\n상위 task1 (2분소요)\n상위 task2 (10분소요)\n상위 task3 (20분소요) 일때,\n상위 task 3개 중 task1의 결과가 먼저 fail이 나올 경우 task2,3 을 기다리지 않고 바로 triger가 발동되어 하위 task4가 수행된다.\n\n\n\n\n\n\n아래 예시에서 4개의 task가 정의됨\n\n\n\n\n# 상위 task1\nbash_upstream_1 = BashOperator(\n    task_id = 'bash_upstream_1',\n    bash_command = 'echo upstream1'\n)\n\n@task(task_id =='python_upstream_1') # 상위 task2\ndef python_upstream_1():\n    AirflowException('downstream_1 Exception!') # AirflowException() fail을 반환하여 무조건 task 실패처리가되도록 설정\n\n@task(task_id =='python_upstream_2') # 상위 task3\ndef python_upstream_2():\n    print('정상 처리')\n\n@task(task_id ='python_downstream_1', trigger_rule ='all_done') #하위 task4\ndef python_downstream_1():\n    print('정상 처리')\n\n[bash_upstream_1 , python_upstream_1(), python_upstream_2()] &gt;&gt; python_downstream_1()\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nbash_upstream_1\n\nbash_upstream_1\n\n\n\npython_downstream_1\n\npython_downstream_1\n\n\n\nbash_upstream_1-&gt;python_downstream_1\n\n\n\n\n\npython_upstream_1\n\npython_upstream_1\n\n\n\npython_upstream_1-&gt;python_downstream_1\n\n\n\n\n\npython_upstream_2\n\npython_upstream_2\n\n\n\npython_upstream_2-&gt;python_downstream_1\n\n\n\n\n\n\n\n\n\n\n\n\n\nbash_upstream_1(성공), python_upstream_1(실패), python_upstream_2(성공).\ntriger rule이 all done이기 때문에 python_upstream_1(실패)여도 python_downstream_1은 수행되어야 한다.\n다른 Operator such as BashOperator, pythonOperator의 경우도 trigger_rule =='all_done' parameter 똑같이 넣어주면 됨\n\n\n\n\n\n\n@task.branch(task_id ='branching') #상위 task1\ndef random_branch():\n    import random\n    item_lst = [' A', ' B', 'C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A':\n        return 'task_a'\n    elif selected_item == 'B':\n        return 'task_b'\n    elif selected_item == 'C':\n        return 'task_c'\n\n#상위 task2\ntask_a = BashOperator(\n    task_id ='task_a',\n    bash_command = 'echo upstream1'\n    )\n\n#상위 task3\n@task(task_id ='task_b')\ndef task_b():\n    print('정상 처리')\n\n#상위 task4\n@task(task_id =='task_c')\ndef task_c():\n    print('정상 처리')\n\n#하위 task5\n@task(task_id =='task_d', trigger_rule ='none_skipped')\ndef task_d():\n    print('정상 처리')\n\nrandom_branch() &gt;&gt; [task_a , task_b(), task_c()] &gt;&gt; task_d()\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nrandom_branch\n\nrandom_branch\n\n\n\ntask_a\n\ntask_a\n\n\n\nrandom_branch-&gt;task_a\n\n\n\n\n\ntask_b\n\ntask_b\n\n\n\nrandom_branch-&gt;task_b\n\n\n\n\n\ntask_c\n\ntask_c\n\n\n\nrandom_branch-&gt;task_c\n\n\n\n\n\ntask_d\n\ntask_d\n\n\n\ntask_a-&gt;task_d\n\n\n\n\n\ntask_b-&gt;task_d\n\n\n\n\n\ntask_c-&gt;task_d\n\n\n\n\n\n\n\n\n\n\n\n\n\nskip이 있기 때문에 실제로 task_d가 돌지 말아야한다.\nDags Full Example\n\n\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.exceptions import AirflowException\n\nimport pendulum\n\nwith DAG(\n    dag_id='dags_python_with_trigger_rule_eg1',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n    bash_upstream_1 = BashOperator(\n        task_id='bash_upstream_1',\n        bash_command='echo upstream1'\n    )\n\n    @task(task_id='python_upstream_1')\n    def python_upstream_1():\n        raise AirflowException('downstream_1 Exception!')\n\n\n    @task(task_id='python_upstream_2')\n    def python_upstream_2():\n        print('정상 처리')\n\n    @task(task_id='python_downstream_1', trigger_rule='all_done')\n    def python_downstream_1():\n        print('정상 처리')\n\n    [bash_upstream_1, python_upstream_1(), python_upstream_2()] &gt;&gt; python_downstream_1()\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.exceptions import AirflowException\n\nimport pendulum\n\nwith DAG(\n    dag_id='dags_python_with_trigger_rule_eg2',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n    @task.branch(task_id='branching')\n    def random_branch():\n        import random\n        item_lst = ['A', 'B', 'C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a'\n        elif selected_item == 'B':\n            return 'task_b'\n        elif selected_item == 'C':\n            return 'task_c'\n\n    task_a = BashOperator(\n        task_id='task_a',\n        bash_command='echo upstream1'\n    )\n\n    @task(task_id='task_b')\n    def task_b():\n        print('정상 처리')\n\n\n    @task(task_id='task_c')\n    def task_c():\n        print('정상 처리')\n\n    @task(task_id='task_d', trigger_rule='none_skipped')\n    def task_d():\n        print('정상 처리')\n\n    random_branch() &gt;&gt; [task_a, task_b(), task_c()] &gt;&gt; task_d()\n\n\n\n\n\n\n\n\n\ntasks를 모아 관리\nTask들의 모음: dags안에 task가 많을 경우 비슷한 기능의 tasks 그룹으로 모아서 관리\n\n예를 들어, dag안에 50개의 tasks 있다고 할 때, 5개 tasks가 서로 연관성이 높은 connection을 이루고 이런 group이 10개가 있을 수 있다.\n\nlink: UI Graph탭에서 Task 들을 Group 화하여 보여줌-TaskGroups or google ‘airflow dags’\n\ncontent &gt;&gt; Core Concepts &gt;&gt; DAGs &gt;&gt; DAG Visualization &gt;&gt; Task Groups\n\nTask Group 안에 Task Group 을 중첩하여 계층적으로 구성 가능\n위의 링크에서 section1 과 section2 로 grouping되어 있고 section2에는 inner_section_2 라는 또 다른 task group이 있다.\n꼭 써야하는 이유는 성능적인 면에서 딱히 없지만 task flow의 가독성이 높아짐\n\n\n\n\n\ntask_group 데커레이터 이용\n\nfrom airflow.decorators import task_group\nwith DAG(...\n) as dag:\n    @task_group(group_id ='first_group')\n    def group_1():\n    ''' task_group 데커레이터를 이용한 첫 번째 그룹''' # docstring: 함수를 설명하는 기법\n    # airflow UI에서는 tooltip이라고 표시됨\n\n    @task(task_id ='inner_function1')\n    def inner_func1(**kwargs):\n        print('첫 번째 TaskGroup 내 첫 번째 task 입니다')\n\n    inner_function2 = PythonOperator(\n        task_id ='inner_function2',\n        python_callable = inner_func,\n        op_kwargs={'msg':'첫 번째 TaskGroup 내 두 번쨰 task 입니다.'}\n    )\n    inner_func1() &gt;&gt; inner_function2\n\ntask_group 데커레이터 이용하지 않음 (클래스 이용)\n\nfrom airflow.utils.task_group import TaskGroup\n    with TaskGroup(group_id ='second_group', tooltip='두 번째 그룹') as group_2: # with MyClassName(arg1,age2,...) \n    # tooltipe은 decorator를 이용한 task_group 생성때의 docstring과 같은 역할을 함\n        @task(task_id ='inner_function1')\n        def inner_func1 (**kwargs):\n            print('두 번째 TaskGroup 내 첫 번째 task 입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id = 'inner_function2',\n            python_collable = inner_func,\n            op_kwargs = {'msg': '두 번째 TaskGroup 내 두 번째 task 입니다'}\n        )\ninner_func1() &gt;&gt; inner_function2\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\nfrom airflow.decorators import task_group\nfrom airflow.utils.task_group import TaskGroup\n\nwith DAG(\n    dag_id=\"dags_python_with_task_group\",\n    schedule=None,\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    def inner_func(**kwargs):\n        msg = kwargs.get('msg') or '' \n        print(msg)\n\n    @task_group(group_id='first_group')\n    def group_1():\n        ''' task_group 데커레이터를 이용한 첫 번째 그룹 '''\n\n        @task(task_id='inner_function1')\n        def inner_func1(**kwargs):\n            print('첫 번째 TaskGroup 내 첫 번째 task입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id='inner_function2',\n            python_callable=inner_func,\n            op_kwargs={'msg':'첫 번째 TaskGroup내 두 번쨰 task입니다.'}\n        )\n\n        inner_func1() &gt;&gt; inner_function2\n\n    with TaskGroup(group_id='second_group', tooltip='두 번째 그룹') as group_2:\n        ''' 클래스 안에 적은 docstring은 표시되지 않음'''\n        @task(task_id='inner_function1')\n        def inner_func1(**kwargs):\n            print('두 번째 TaskGroup 내 첫 번째 task입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id='inner_function2',\n            python_callable=inner_func,\n            op_kwargs={'msg': '두 번째 TaskGroup내 두 번째 task입니다.'}\n        )\n        inner_func1() &gt;&gt; inner_function2\n\n    group_1() &gt;&gt; group_2\n\n위에서 task_id와 group_id가 같지만 에러가 안나는 이유가 task group이 다르기 때문.\n위에서 볼 수 있듯이 task group 또한 flow 설정할 수 있음 group_1() &gt;&gt; group_2\n\n\n\n\n\nTask Group 작성 방법은 2 가지가 존재함 (데커레이터 & 클래스)\nTask Group 안에 Task Group 중첩하여 정의 가능\nTask Group 간에도 Flow 정의 가능\nGroup이 다르면 task_id 가 같아도 무방\nTooltip 파라미터를 이용해 UI 화면에서 Task group 에 대한 설명 제공 가능 (데커레이터 활용시 docstring 으로도 가능)\n\n\n\n\n\n\n\n\nTask 연결에 대한 설명 (즉 edge에 대한 Comment)\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ningest\n\ningest\n\n\n\nanalyze\n\nanalyze\n\n\n\ningest-&gt;analyze\n\n\n\n\n\ncheck_integrity\n\ncheck_integrity\n\n\n\nanalyze-&gt;check_integrity\n\n\n\n\n\ndescribe_integrity\n\ndescribe_integrity\n\n\n\ncheck_integrity-&gt;describe_integrity\n\n\nErrors Found\n\n\n\nsave\n\nsave\n\n\n\ncheck_integrity-&gt;save\n\n\nNo Errors\n\n\n\nemail_error\n\nemail_error\n\n\n\ndescribe_integrity-&gt;email_error\n\n\n\n\n\nreport\n\nreport\n\n\n\nemail_error-&gt;report\n\n\n\n\n\nsave-&gt;report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom airflow.utils.edgemodifier import Label\nempty_1 = EmptyOperator(\n    task_id ='empty_1'\n)\n\nempty_2 = EmptyOperator(\n    task_id='empty_2'\n)\nempty_1 &gt;&gt; Label ('1 과 2 사이') &gt;&gt; empty_2\n\n\n\n\nfrom airflow.utils.edgemodifier import Label\nempty_2 = EmptyOperator(\n    task_id = 'empty_2'\n)\n\nempty_3 = EmptyOperator(\n    task_id ='empty_3'\n)\n\nempty_4 = EmptyOperator(\n    task_id ='empty_4'\n)\n\nempty_5 = EmptyOperator(\n    task_id ='empty_5'\n)\n\nempty_6 = EmptyOperator(\n    task_id ='empty_6'\n)\n\nempty_2 &gt;&gt; Label('Start Branch') &gt;&gt; [empty_3, empty_4, empty_5 ] &gt;&gt; Label('End Branch') &gt;&gt; empty_6\n\n이렇게 분기가 펼쳐지고 모아지는 경우 모든 분기 edges에 label이 붙게 된다.\nFull DAG Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\n\n\nwith DAG(\n    dag_id=\"dags_empty_with_edge_label\",\n    schedule=None,\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    empty_1 = EmptyOperator(\n        task_id='empty_1'\n    )\n\n    empty_2 = EmptyOperator(\n        task_id='empty_2'\n    )\n\n    empty_1 &gt;&gt; Label('1과 2사이') &gt;&gt; empty_2\n\n    empty_3 = EmptyOperator(\n        task_id='empty_3'\n    )\n\n    empty_4 = EmptyOperator(\n        task_id='empty_4'\n    )\n\n    empty_5 = EmptyOperator(\n        task_id='empty_5'\n    )\n\n    empty_6 = EmptyOperator(\n        task_id='empty_6'\n    )\n\n    empty_2 &gt;&gt; Label('Start Branch') &gt;&gt; [empty_3,empty_4,empty_5] &gt;&gt; Label('End Branch') &gt;&gt; empty_6\n\nempty operator이기 때문에 실행은 airflow web servce에서 실행은 안해도 된다.\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-분기-처리-유형",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-분기-처리-유형",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "Task 분기처리가 필요한 이유\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask2_1\n\ntask2_1\n\n\n\ntask1-&gt;task2_1\n\n\n\n\n\ntask2_2\n\ntask2_2\n\n\n\ntask1-&gt;task2_2\n\n\n\n\n\ntask2_3\n\ntask2_3\n\n\n\ntask1-&gt;task2_3\n\n\n\n\n\n\n\n\n\n\n\n위와 같이 task1이 실행된 후 여러 후차적인 task를 병렬로 실행되어야 할 때\n\ntask flow에서 task1의 결과에 따라 선택적으로 task2-x 중 하나만 수행되도록 구성해야 할 때가 있다.\neg) Task1 의 결과로 ‘Good’,’Bad’,’Pending’ 이라는 결과 3 개 중 하나가 나오고 그에 따라 ask2-1 ~ task2-3 중 하나가 실행되도록 해야 할 경우"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#airflow에서-지원하는-task-분기처리-방법",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#airflow에서-지원하는-task-분기처리-방법",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "Task 분기처리 방법 3가지\n\nBranchPythonOperator\ntask.branch decorator 이용\nBaseBranchOperator 클래스를 상속하여 직접 개발\n\n\n\n\n\ndef select_random():\n    import random\n\n    item_lst= ['A','B','C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A';\n        return 'task_a' # task_id를 string 값으로 return해야함\n    elif selected_item in ['B','C] \n        return ['task_b','task_c'] # 여러 task를 동시에 수행시킬 땐 string 리스트로 반환\n\n# 일반 operator의 parameter도 있음\npython_branch_task = BranchPythonOperator(\n    task_id ='python_branch_task',\n    python_callable=select_random #select_random function 호출\n)\n\npython_branch_task &gt;&gt; [task_a , task_b , task_c]\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\npython_branch_task\n\npython_branch_task\n\n\n\ntask_a\n\ntask_a\n\n\n\npython_branch_task-&gt;task_a\n\n\n\n\n\ntask_b\n\ntask_b\n\n\n\npython_branch_task-&gt;task_b\n\n\n\n\n\ntask_c\n\ntask_c\n\n\n\npython_branch_task-&gt;task_c\n\n\n\n\n\n\n\n\n\n\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.python import BranchPythonOperator\n\nwith DAG(\n    dag_id='dags_branch_python_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'), \n    schedule='0 1 * * *',\n    catchup=False\n) as dag:\n    def select_random():\n        import random\n\n        item_lst = ['A','B','C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a' # task_id를 string 값으로 return해야함\n        elif selected_item in ['B','C']:\n            return ['task_b','task_c'] # 여러 task를 동시에 수행시킬 땐 리스트로 반환\n    \n    # 일반 operator의 parameter도 있음\n    python_branch_task = BranchPythonOperator(\n        task_id='python_branch_task',\n        python_callable=select_random\n    )\n    \n    # 후행 task 3개\n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    python_branch_task &gt;&gt; [task_a, task_b, task_c]\n\n나의 경우 airflow web service상에서 1회 실행 시켰을 때 selected_item의 값이 task_b, task_b가 선택됐음\n\ngraph 버튼을 눌러 보면 가장 최근에 돌았던 task들이 return 된다.\ntask_a가 분홍색 박스로 skipped 상태인 것을 확인 할 수 있다.\ngraph에서 python_branch_task를 누르고 xcom을 누르면 다음과 같은 table을 확인할 수 있다.\n\n\n\n\nKey\nValue\n\n\n\n\nskipmixin_key\n{‘followed’: [‘task_c’, ‘task_b’]}\n\n\nreturn_value\n[‘task_b’, ‘task_c’]\n\n\n\n\n여기서 skipmixin_key 의 value값의 key 값이 ‘followed’ 이고 [‘task_c’, ‘task_b’] 인 것을 볼 수 있다. 필요시 어떤 task들이 선택되었는지 확인하려면 xcom을 통해 확인 가능하다.\nlog 를 보면\n\n[2023-06-23, 23:20:01 UTC] {python.py:183} INFO - Done. Returned value was: ['task_b', 'task_c']\n[2023-06-23, 23:20:01 UTC] {python.py:216} INFO - Branch callable return ['task_b', 'task_c']\n[2023-06-23, 23:20:01 UTC] {skipmixin.py:161} INFO - Following branch ['task_b', 'task_c']\n[2023-06-23, 23:20:01 UTC] {skipmixin.py:221} INFO - Skipping tasks ['task_a']"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task.branch-이해-branchpythonoperator-vs-task.branch-decorator",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task.branch-이해-branchpythonoperator-vs-task.branch-decorator",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "from airflow.operators.python import BranchPythonOperator\ndef select_random(): \n    import random\n    item_lst = ['A','B','C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A':\n        return 'task_a'\n    elif selected_item in ['B','C']\n        return ['task_b','task_c']\n\npython_branch_task = BranchPythonOperator(\n    task_id= 'branching',\n    python_callable = select_random\n)\npython_branch_task &gt;&gt; [task_a , task_b , task_c]\n\nfrom airflow.operators.python import task\n\n@task.branch(task_id='python_branch_task')\ndef select_random(): \n    import random\n    item_lst = ['A','B','C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A':\n        return 'task_a'\n    elif selected_item in ['B','C']\n        return ['task_b','task_c']\n\nselect_random() &gt;&gt; [task_a , task_b , task_c]\n\n\n\nBranchPythonOperator와 비교하여 select_random()을 호출 또는 맵핑 하는 방식이 decorator에서는 @task.branch(task_id='python_branch_task')으로 표현 되었고 task flow를 표현하는 task connection 방식도 select_random() &gt;&gt; [task_a , task_b , task_c] 로 표현 됐다.\nBranchPythonOperator의 python_branch_task object와 task.branch (decorator)의 select_random()는 사실상 같은 객체이다.\n차이점은 BranchPythonOperator(...)를 실행시킨 것과 select_random(...) 함수를 실행한 것 외엔 그 역할과 기능은 같다 (같은 object 반환).\nDags Full Example\n\nfrom airflow import DAG\nfrom datetime import datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id='dags_python_with_branch_decorator',\n    start_date=datetime(2023,4,1),\n    schedule=None,\n    catchup=False\n) as dag:\n    @task.branch(task_id='python_branch_task')\n    def select_random():\n        import random\n        item_lst = ['A', 'B', 'C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a'\n        elif selected_item in ['B','C']:\n            return ['task_b','task_c']\n    \n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    select_random() &gt;&gt; [task_a, task_b, task_c]\n\nairflow web service의 결과물은 BranchPythonOperator나 decorator나 같았음"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#basebranchoperator-이해-요약",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#basebranchoperator-이해-요약",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "from airflow.operators.branch import BaseBranchOperator\nwith DAG(...\n) as dag:\n    class CustomBranchOperator(BaseBranchOperator): #클래스 이름은 임의로 지정해 줌\n    #Python의 class 상속 syntax: class MyclassName(상속할className):\n    #Python은 다중 상속가능\n        def choose_branch(self,context): \n        # 함수 재정의 : Overriding, 함수 이름 바꾸면 안됨!\n        # parameter도 바꾸면 안됨\n            import random\n            print(context) # context에 어떤 내용이 있는지 출력\n\n            item_lst = ['A', 'B','C]\n            selected_item = random.choice(item_lst)\n            if selected_item == 'A':\n                return 'task_a'\n            elif selected_item in ['B','C']:\n                return ['task_b','task_c']\n\ncustom_branch_operator = CustomBranchOperator(task_id ='python_branch_task') # 클래스 실행하여 custom_branch_operator object 생성\ncustom_branch_operator &gt;&gt; [task_a , task_b , task_c]\n\n클래스 상속하여 새로운 클래스 만들어야함: BaseBranchOperator 상속시 choose_branch 함수를 구현해 줘야 함\nCustomBranchOperator 클래스 이름은 임의로 지정해준 이름\nclass 선언시 class childClass(상속할parentClass): 상속할 부모클래스를 2개이상 지정하는 다중 상속이 가능하긴 하지만 권고하지 않음.\nchoose_branch() 함수를 만든 이유를 알기 위해선 BaseBranchOperator class에 대해서 알아야함\n\nairflow operators-airflow.operators.branch or google ‘airflow operators’ :::{.callout-note} ## Description\n\nBases: airflow.models.baseoperator.BaseOperator, airflow.models.skipmixin.SkipMixin A base class for creating operators with branching functionality, like to BranchPythonOperator. Users should create a subclass from this operator and implement the function choose_branch(self, context). This should run whatever business logic is needed to determine the branch, and return either the task_id for a single task (as a str) or a list of task_ids. The operator will continue with the returned task_id(s), and all other tasks directly downstream of this operator will be skipped. :::\n\n함수명과 인자(argument)명도 반드시 일치시켜야함\nchoose_branch(self,context)의 context는 pythonOperator 쓸때 **kwargs의 parameters들을 사용할 수 있게 해주는 parameter\n\ncontext 인자엔 op_kargs와 같이 data_interval_start, data_interval_end 등과 같은 정보를 제공해주는 인자\n\nprint(context) 결과\n\n[2023-06-24, 00:29:33 UTC] {logging_mixin.py:149} INFO - {'conf': &lt;***.configuration.AirflowConfigParser object at 0x7fc3d5dd2cd0&gt;, 'dag': &lt;DAG: dags_base_branch_operator&gt;, 'dag_run': &lt;DagRun dags_base_branch_operator @ 2023-06-24 00:29:31.444830+00:00: manual__2023-06-24T00:29:31.444830+00:00, state:running, queued_at: 2023-06-24 00:29:31.455604+00:00. externally triggered: True&gt;, 'data_interval_end': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'data_interval_start': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'ds': '2023-06-24', 'ds_nodash': '20230624', 'execution_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'expanded_ti_count': None, 'inlets': [], 'logical_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'macros': &lt;module '***.macros' from '/home/***/.local/lib/python3.7/site-packages/***/macros/__init__.py'&gt;, 'next_ds': '2023-06-24', 'next_ds_nodash': '20230624', 'next_execution_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'outlets': [], 'params': {}, 'prev_data_interval_start_success': None, 'prev_data_interval_end_success': None, 'prev_ds': '2023-06-24', 'prev_ds_nodash': '20230624', 'prev_execution_date': DateTime(2023, 6, 24, 0, 29, 31, 444830, tzinfo=Timezone('UTC')), 'prev_execution_date_success': None, 'prev_start_date_success': None, 'run_id': 'manual__2023-06-24T00:29:31.444830+00:00', 'task': &lt;Task(CustomBranchOperator): python_branch_task&gt;, 'task_instance': &lt;TaskInstance: dags_base_branch_operator.python_branch_task manual__2023-06-24T00:29:31.444830+00:00 [running]&gt;, 'task_instance_key_str': 'dags_base_branch_operator__python_branch_task__20230624', 'test_mode': False, 'ti': &lt;TaskInstance: dags_base_branch_operator.python_branch_task manual__2023-06-24T00:29:31.444830+00:00 [running]&gt;, 'tomorrow_ds': '2023-06-25', 'tomorrow_ds_nodash': '20230625', 'triggering_dataset_events': &lt;Proxy at 0x7fc3ab28c8c0 with factory &lt;function TaskInstance.get_template_context.&lt;locals&gt;.get_triggering_events at 0x7fc3ab277c20&gt;&gt;, 'ts': '2023-06-24T00:29:31.444830+00:00', 'ts_nodash': '20230624T002931', 'ts_nodash_with_tz': '20230624T002931.444830+0000', 'var': {'json': None, 'value': None}, 'conn': None, 'yesterday_ds': '2023-06-23', 'yesterday_ds_nodash': '20230623'}\ncontext결과물은 위와 같은 시간 정보를 담고 있기 때문에 꺼내쓸 수 있다.\n분기 처리 결과는 다른 2 방식의 결과와 같음\nDAG Full Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.branch import BaseBranchOperator\nfrom airflow.operators.python import PythonOperator\n\nwith DAG(\n    dag_id='dags_base_branch_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n    class CustomBranchOperator(BaseBranchOperator):\n        def choose_branch(self, context):\n            import random\n            print(context) # context에 어떤 내용이 있는지 출력\n            \n            item_lst = ['A', 'B', 'C']\n            selected_item = random.choice(item_lst)\n            if selected_item == 'A':\n                return 'task_a'\n            elif selected_item in ['B','C']:\n                return ['task_b','task_c']\n\n    \n    custom_branch_operator = CustomBranchOperator(task_id='python_branch_task')\n\n    \n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    custom_branch_operator &gt;&gt; [task_a, task_b, task_c]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#summary",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#summary",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "Task 분기처리 방법\n\nBranchPythonOperator (자주 사용)\ntask.branch 데커레이터 이용 (자주 사용)\nBaseBranchOperator 상속 , choose_branch 를 재정의해야 함 (덜 사용)\n\n공통적으로 리턴 값으로 후행 Task 의 id 를 str 또는 list 로 리턴해야 함\n3가지 분기처리 방법은 방법만 다를 뿐 결과는 동일함\n3 보다는 1 또는 2를 주로 사용함"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#trigger-rule-종류",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#trigger-rule-종류",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "G\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask4\n\ntask4\n\n\n\ntask1-&gt;task4\n\n\n\n\n\ntask2\n\ntask2\n\n\n\ntask2-&gt;task4\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask3-&gt;task4\n\n\n\n\n\n\n\n\n\n\n\nbranch와 반대되는 개념으로 여러 상위 tasks가 하나의 하위 task로 연결되는 flow로 만들때 사용\n즉, 여러 상위 Task 들의 상태에 따라 후행 task의 수행여부 결정할 때 쓰인다\n기본 값 : 여러 상위 Task들이 모두 성공시에만 수행\n상위 task의 수행 상태에 따라 조건적으로 후행 task의 수행 여부를 결정할 수 있다.\ntrigger option은 하위 task를 이용하여 줄 수 있다.\n모든 airflow operator는 trigger rule option을 줄 수 있다.\n11가지 trigger rules\n\n\n\n\n\n\n\n\nDefault\nLeft\n\n\n\n\nall_success (default)\n상위 tasks 가 모두 성공하면 실행\n\n\nall_failed\n상위 tasks 가 모두 실패하면 실행\n\n\nall_done\n상위 tasks 가 모두 수행되면 실행 (실패도 수행된것에 포함)\n\n\nall_skipped\n상위 tasks 가 모두 Skipped 상태면 실행\n\n\none_failed\n상위 tasks 중 하나 이상 실패하면 실행 (모든 상위 Tasks의 완료를 기다리지 않음)\n\n\none_success\n상위 tasks 중 하나 이상 성공하면 실행 (모든 상위 Tasks의 완료를 기다리지 않음)\n\n\none_done\n상위 tasks 중 하나 이상 성공 또는 실패 하면 실행\n\n\nnone_failed\n상위 task s중 실패가 없는 경우 실행 (성공 또는 Skipped 상태)\n\n\nnone_failed_min_one_success\n상위 tasks 중 실패가 없고 성공한 Task가 적어도 1개 이상이면 실행\n\n\nnone_skipped\nSkip된 상위 Task가 없으면 실행 (상위 Task가 성공, 실패하여도 무방)\n\n\nalways\n언제나 실행\n\n\n\n\n위의 표에서 모든 상위 task를 기다리지 않음은 각 각의 상위 task들의 처리 시간이 다를 때 가장 빠르게 처리되는 상위 task에 따라서 후행 task가 수행된다는 것을 의미한다. 예를 들어, one_failed의 경우\n\n상위 task1 (2분소요)\n상위 task2 (10분소요)\n상위 task3 (20분소요) 일때,\n상위 task 3개 중 task1의 결과가 먼저 fail이 나올 경우 task2,3 을 기다리지 않고 바로 triger가 발동되어 하위 task4가 수행된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#trigger-rule-실습-trigger_rule-all_done",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#trigger-rule-실습-trigger_rule-all_done",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "아래 예시에서 4개의 task가 정의됨\n\n\n\n\n# 상위 task1\nbash_upstream_1 = BashOperator(\n    task_id = 'bash_upstream_1',\n    bash_command = 'echo upstream1'\n)\n\n@task(task_id =='python_upstream_1') # 상위 task2\ndef python_upstream_1():\n    AirflowException('downstream_1 Exception!') # AirflowException() fail을 반환하여 무조건 task 실패처리가되도록 설정\n\n@task(task_id =='python_upstream_2') # 상위 task3\ndef python_upstream_2():\n    print('정상 처리')\n\n@task(task_id ='python_downstream_1', trigger_rule ='all_done') #하위 task4\ndef python_downstream_1():\n    print('정상 처리')\n\n[bash_upstream_1 , python_upstream_1(), python_upstream_2()] &gt;&gt; python_downstream_1()\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nbash_upstream_1\n\nbash_upstream_1\n\n\n\npython_downstream_1\n\npython_downstream_1\n\n\n\nbash_upstream_1-&gt;python_downstream_1\n\n\n\n\n\npython_upstream_1\n\npython_upstream_1\n\n\n\npython_upstream_1-&gt;python_downstream_1\n\n\n\n\n\npython_upstream_2\n\npython_upstream_2\n\n\n\npython_upstream_2-&gt;python_downstream_1\n\n\n\n\n\n\n\n\n\n\n\n\n\nbash_upstream_1(성공), python_upstream_1(실패), python_upstream_2(성공).\ntriger rule이 all done이기 때문에 python_upstream_1(실패)여도 python_downstream_1은 수행되어야 한다.\n다른 Operator such as BashOperator, pythonOperator의 경우도 trigger_rule =='all_done' parameter 똑같이 넣어주면 됨"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#trigger-rule-실습-triger_rule-none_skipped",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#trigger-rule-실습-triger_rule-none_skipped",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "@task.branch(task_id ='branching') #상위 task1\ndef random_branch():\n    import random\n    item_lst = [' A', ' B', 'C']\n    selected_item = random.choice(item_lst)\n    if selected_item == 'A':\n        return 'task_a'\n    elif selected_item == 'B':\n        return 'task_b'\n    elif selected_item == 'C':\n        return 'task_c'\n\n#상위 task2\ntask_a = BashOperator(\n    task_id ='task_a',\n    bash_command = 'echo upstream1'\n    )\n\n#상위 task3\n@task(task_id ='task_b')\ndef task_b():\n    print('정상 처리')\n\n#상위 task4\n@task(task_id =='task_c')\ndef task_c():\n    print('정상 처리')\n\n#하위 task5\n@task(task_id =='task_d', trigger_rule ='none_skipped')\ndef task_d():\n    print('정상 처리')\n\nrandom_branch() &gt;&gt; [task_a , task_b(), task_c()] &gt;&gt; task_d()\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nrandom_branch\n\nrandom_branch\n\n\n\ntask_a\n\ntask_a\n\n\n\nrandom_branch-&gt;task_a\n\n\n\n\n\ntask_b\n\ntask_b\n\n\n\nrandom_branch-&gt;task_b\n\n\n\n\n\ntask_c\n\ntask_c\n\n\n\nrandom_branch-&gt;task_c\n\n\n\n\n\ntask_d\n\ntask_d\n\n\n\ntask_a-&gt;task_d\n\n\n\n\n\ntask_b-&gt;task_d\n\n\n\n\n\ntask_c-&gt;task_d\n\n\n\n\n\n\n\n\n\n\n\n\n\nskip이 있기 때문에 실제로 task_d가 돌지 말아야한다.\nDags Full Example\n\n\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.exceptions import AirflowException\n\nimport pendulum\n\nwith DAG(\n    dag_id='dags_python_with_trigger_rule_eg1',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n    bash_upstream_1 = BashOperator(\n        task_id='bash_upstream_1',\n        bash_command='echo upstream1'\n    )\n\n    @task(task_id='python_upstream_1')\n    def python_upstream_1():\n        raise AirflowException('downstream_1 Exception!')\n\n\n    @task(task_id='python_upstream_2')\n    def python_upstream_2():\n        print('정상 처리')\n\n    @task(task_id='python_downstream_1', trigger_rule='all_done')\n    def python_downstream_1():\n        print('정상 처리')\n\n    [bash_upstream_1, python_upstream_1(), python_upstream_2()] &gt;&gt; python_downstream_1()\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.exceptions import AirflowException\n\nimport pendulum\n\nwith DAG(\n    dag_id='dags_python_with_trigger_rule_eg2',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule=None,\n    catchup=False\n) as dag:\n    @task.branch(task_id='branching')\n    def random_branch():\n        import random\n        item_lst = ['A', 'B', 'C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a'\n        elif selected_item == 'B':\n            return 'task_b'\n        elif selected_item == 'C':\n            return 'task_c'\n\n    task_a = BashOperator(\n        task_id='task_a',\n        bash_command='echo upstream1'\n    )\n\n    @task(task_id='task_b')\n    def task_b():\n        print('정상 처리')\n\n\n    @task(task_id='task_c')\n    def task_c():\n        print('정상 처리')\n\n    @task(task_id='task_d', trigger_rule='none_skipped')\n    def task_d():\n        print('정상 처리')\n\n    random_branch() &gt;&gt; [task_a, task_b(), task_c()] &gt;&gt; task_d()"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-개념",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-개념",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "tasks를 모아 관리\nTask들의 모음: dags안에 task가 많을 경우 비슷한 기능의 tasks 그룹으로 모아서 관리\n\n예를 들어, dag안에 50개의 tasks 있다고 할 때, 5개 tasks가 서로 연관성이 높은 connection을 이루고 이런 group이 10개가 있을 수 있다.\n\nlink: UI Graph탭에서 Task 들을 Group 화하여 보여줌-TaskGroups or google ‘airflow dags’\n\ncontent &gt;&gt; Core Concepts &gt;&gt; DAGs &gt;&gt; DAG Visualization &gt;&gt; Task Groups\n\nTask Group 안에 Task Group 을 중첩하여 계층적으로 구성 가능\n위의 링크에서 section1 과 section2 로 grouping되어 있고 section2에는 inner_section_2 라는 또 다른 task group이 있다.\n꼭 써야하는 이유는 성능적인 면에서 딱히 없지만 task flow의 가독성이 높아짐"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-실습-task_group-데커레이터-이용",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-실습-task_group-데커레이터-이용",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "from airflow.decorators import task_group\nwith DAG(...\n) as dag:\n    @task_group(group_id ='first_group')\n    def group_1():\n    ''' task_group 데커레이터를 이용한 첫 번째 그룹입니다. ''' # docstring: 함수를 설명하는 글\n    # airflow UI에서는 tooltip이라고 표시됨\n\n    @task(task_id ='inner_function1')\n    def inner_func1(**kwargs):\n        print('첫 번째 TaskGroup 내 첫 번째 task 입니다')\n\n    inner_function2 = PythonOperator(\n        task_id ='inner_function2',\n        python_callable = inner_func,\n        op_kwargs={'msg':'첫 번째 TaskGroup 내 두 번쨰 task 입니다.'}\n    )\n    inner_func1() &gt;&gt; inner_function2"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-실습-클래스-이용",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-실습-클래스-이용",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "from airflow.utils.task_group import TaskGroup\n    with TaskGroup(group_id ='second_group', tooltip='두 번째 그룹입니다.') as group_2:\n    #tooltipe은 docstring과 같은 역할을 함\n        @task(task_id ='inner_function1')\n        def inner_func1 (**kwargs):\n            print('두 번째 TaskGroup 내 첫 번째 task 입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id = 'inner_function2',\n            python_collable = inner_func,\n            op_kwargs = {'msg': '두 번째 TaskGroup 내 두 번째 task 입니다'}\n        )\ninner_func1() &gt;&gt; inner_function2\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\nfrom airflow.decorators import task_group\nfrom airflow.utils.task_group import TaskGroup\n\nwith DAG(\n    dag_id=\"dags_python_with_task_group\",\n    schedule=None,\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    def inner_func(**kwargs):\n        msg = kwargs.get('msg') or '' \n        print(msg)\n\n    @task_group(group_id='first_group')\n    def group_1():\n        ''' task_group 데커레이터를 이용한 첫 번째 그룹입니다. '''\n\n        @task(task_id='inner_function1')\n        def inner_func1(**kwargs):\n            print('첫 번째 TaskGroup 내 첫 번째 task입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id='inner_function2',\n            python_callable=inner_func,\n            op_kwargs={'msg':'첫 번째 TaskGroup내 두 번쨰 task입니다.'}\n        )\n\n        inner_func1() &gt;&gt; inner_function2\n\n    with TaskGroup(group_id='second_group', tooltip='두 번째 그룹입니다') as group_2:\n        ''' 여기에 적은 docstring은 표시되지 않습니다'''\n        @task(task_id='inner_function1')\n        def inner_func1(**kwargs):\n            print('두 번째 TaskGroup 내 첫 번째 task입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id='inner_function2',\n            python_callable=inner_func,\n            op_kwargs={'msg': '두 번째 TaskGroup내 두 번째 task입니다.'}\n        )\n        inner_func1() &gt;&gt; inner_function2\n\n    group_1() &gt;&gt; group_2\n\n위에서 task_id와 group_id가 같지만 에러가 안나는 이유가 task group이 다르기 때문.\n위에서 볼 수 있듯이 task group 또한 flow 설정할 수 있음 group_1() &gt;&gt; group_2"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#요약",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#요약",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "Task Group 작성 방법은 2 가지가 존재함 (데커레이터 & 클래스)\nTask Group 안에 Task Group 중첩하여 정의 가능\nTask Group 간에도 Flow 정의 가능\nGroup이 다르면 task_id 가 같아도 무방\nTooltip 파라미터를 이용해 UI 화면에서 Task group 에 대한 설명 제공 가능 (데커레이터 활용시 docstring 으로도 가능)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-개념",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-개념",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "Task 연결에 대한 설명 (즉 edge에 대한 Comment)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-만들기",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-만들기",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "G\n\n\ncluster0\n\nTask Flow\n\n\n\ningest\n\ningest\n\n\n\nanalyze\n\nanalyze\n\n\n\ningest-&gt;analyze\n\n\n\n\n\ncheck_integrity\n\ncheck_integrity\n\n\n\nanalyze-&gt;check_integrity\n\n\n\n\n\ndescribe_integrity\n\ndescribe_integrity\n\n\n\ncheck_integrity-&gt;describe_integrity\n\n\nErrors Found\n\n\n\nsave\n\nsave\n\n\n\ncheck_integrity-&gt;save\n\n\nNo Errors\n\n\n\nemail_error\n\nemail_error\n\n\n\ndescribe_integrity-&gt;email_error\n\n\n\n\n\nreport\n\nreport\n\n\n\nemail_error-&gt;report\n\n\n\n\n\nsave-&gt;report"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-실습-1",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-실습-1",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "from airflow.utils.edgemodifier import Label\nempty_1 = EmptyOperator(\n    task_id ='empty_1'\n)\n\nempty_2 = EmptyOperator(\n    task_id='empty_2'\n)\nempty_1 &gt;&gt; Label ('1 과 2 사이') &gt;&gt; empty_2"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-실습-2",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#edge-label-실습-2",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "from airflow.utils.edgemodifier import Label\nempty_2 = EmptyOperator(\n    task_id = 'empty_2'\n)\n\nempty_3 = EmptyOperator(\n    task_id ='empty_3'\n)\n\nempty_4 = EmptyOperator(\n    task_id ='empty_4'\n)\n\nempty_5 = EmptyOperator(\n    task_id ='empty_5'\n)\n\nempty_6 = EmptyOperator(\n    task_id ='empty_6'\n)\n\nempty_2 &gt;&gt; Label('Start Branch') &gt;&gt; [empty_3, empty_4, empty_5 ] &gt;&gt; Label('End Branch') &gt;&gt; empty_6\n\n이렇게 분기가 펼쳐지고 모아지는 경우 모든 분기 edges에 label이 붙게 된다.\nFull DAG Example\n\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\n\n\nwith DAG(\n    dag_id=\"dags_empty_with_edge_label\",\n    schedule=None,\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    \n    empty_1 = EmptyOperator(\n        task_id='empty_1'\n    )\n\n    empty_2 = EmptyOperator(\n        task_id='empty_2'\n    )\n\n    empty_1 &gt;&gt; Label('1과 2사이') &gt;&gt; empty_2\n\n    empty_3 = EmptyOperator(\n        task_id='empty_3'\n    )\n\n    empty_4 = EmptyOperator(\n        task_id='empty_4'\n    )\n\n    empty_5 = EmptyOperator(\n        task_id='empty_5'\n    )\n\n    empty_6 = EmptyOperator(\n        task_id='empty_6'\n    )\n\n    empty_2 &gt;&gt; Label('Start Branch') &gt;&gt; [empty_3,empty_4,empty_5] &gt;&gt; Label('End Branch') &gt;&gt; empty_6\n\nempty operator이기 때문에 실행은 airflow web servce에서 실행은 안해도 된다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-실습",
    "href": "docs/blog/posts/Engineering/airflow/07.task_handling.html#task-group-실습",
    "title": "Task Handling Techniques",
    "section": "",
    "text": "task_group 데커레이터 이용\n\nfrom airflow.decorators import task_group\nwith DAG(...\n) as dag:\n    @task_group(group_id ='first_group')\n    def group_1():\n    ''' task_group 데커레이터를 이용한 첫 번째 그룹''' # docstring: 함수를 설명하는 기법\n    # airflow UI에서는 tooltip이라고 표시됨\n\n    @task(task_id ='inner_function1')\n    def inner_func1(**kwargs):\n        print('첫 번째 TaskGroup 내 첫 번째 task 입니다')\n\n    inner_function2 = PythonOperator(\n        task_id ='inner_function2',\n        python_callable = inner_func,\n        op_kwargs={'msg':'첫 번째 TaskGroup 내 두 번쨰 task 입니다.'}\n    )\n    inner_func1() &gt;&gt; inner_function2\n\ntask_group 데커레이터 이용하지 않음 (클래스 이용)\n\nfrom airflow.utils.task_group import TaskGroup\n    with TaskGroup(group_id ='second_group', tooltip='두 번째 그룹') as group_2: # with MyClassName(arg1,age2,...) \n    # tooltipe은 decorator를 이용한 task_group 생성때의 docstring과 같은 역할을 함\n        @task(task_id ='inner_function1')\n        def inner_func1 (**kwargs):\n            print('두 번째 TaskGroup 내 첫 번째 task 입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id = 'inner_function2',\n            python_collable = inner_func,\n            op_kwargs = {'msg': '두 번째 TaskGroup 내 두 번째 task 입니다'}\n        )\ninner_func1() &gt;&gt; inner_function2\n\nDags Full Example\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.python import PythonOperator\nfrom airflow.decorators import task\nfrom airflow.decorators import task_group\nfrom airflow.utils.task_group import TaskGroup\n\nwith DAG(\n    dag_id=\"dags_python_with_task_group\",\n    schedule=None,\n    start_date=pendulum.datetime(2023, 4, 1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    def inner_func(**kwargs):\n        msg = kwargs.get('msg') or '' \n        print(msg)\n\n    @task_group(group_id='first_group')\n    def group_1():\n        ''' task_group 데커레이터를 이용한 첫 번째 그룹 '''\n\n        @task(task_id='inner_function1')\n        def inner_func1(**kwargs):\n            print('첫 번째 TaskGroup 내 첫 번째 task입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id='inner_function2',\n            python_callable=inner_func,\n            op_kwargs={'msg':'첫 번째 TaskGroup내 두 번쨰 task입니다.'}\n        )\n\n        inner_func1() &gt;&gt; inner_function2\n\n    with TaskGroup(group_id='second_group', tooltip='두 번째 그룹') as group_2:\n        ''' 클래스 안에 적은 docstring은 표시되지 않음'''\n        @task(task_id='inner_function1')\n        def inner_func1(**kwargs):\n            print('두 번째 TaskGroup 내 첫 번째 task입니다.')\n\n        inner_function2 = PythonOperator(\n            task_id='inner_function2',\n            python_callable=inner_func,\n            op_kwargs={'msg': '두 번째 TaskGroup내 두 번째 task입니다.'}\n        )\n        inner_func1() &gt;&gt; inner_function2\n\n    group_1() &gt;&gt; group_2\n\n위에서 task_id와 group_id가 같지만 에러가 안나는 이유가 task group이 다르기 때문.\n위에서 볼 수 있듯이 task group 또한 flow 설정할 수 있음 group_1() &gt;&gt; group_2"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html",
    "title": "More Operators",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile Path\nOperator (Class)\nImportance\nNote\n\n\n\n\nairflow.models.bashoperator\nBaseOperator\n***\n* base operator는 직접 쓰는게 아니라 user가 custom operator를 직접 개발하고싶은 경우 이 클래스를 상속하여 개발하기 위해 만든 operator (execute() 함수를 재정의(Override)하여 사용)  * 아래 오퍼레이터들은 모두 이 클래스를 상속하여 개발되어 있음  * Airflow를 잘 쓰려면 이 오퍼레이터 상속/개발하는 것을 자유자재로 할 줄 알아야 함.\n\n\nairflow.operators.bash\nBashOperator\n***\n* bash 쉘 스크립트를 실행  * 가장 많이 사용하는 오퍼레이터 중 하나임\n\n\nairflow.operators.branch\nBaseBranchOperator\n**\n* 직접 사용할 수는 없음.  * 이 클래스를 상속하여 choose_branch 함수를 구현해야 함 (그냥 사용시 에러 발생)  * 그러나 이 클래스 상속해서 사용하는것보다 @task.branch 데코레이터 사용을 권장\n\n\nairflow.operators.datetime\nBranchDateTimeOperator\n\n* 특정 Datetime 구간을 주어 Job 수행 날짜가 구간에 속하는지 여부에 따라 분기 결정\n\n\nairflow.operators.email\nEmailOperator\n\n이메일 전송하는 오퍼레이터 (Airflow 파라미터에 SMTP 서버 등 사전 셋팅 필요)\n\n\nairflow.operators.generic_transfer\nGenericTransfer\n\n데이터를 소스에서 타겟으로 전송 (Airflow 커넥션에 등록되어 있는 대상만 가능)\n\n\nairflow.operators.latest_only\nLatestOnlyOperator\n\ndag을 수행시킬 때 스케쥴이 아니라 backfill(과거 날짜로 dag 수행) 이 Task 뒤에 연결되어 있는 task들을 모두 가장 최근에 설정된 스케쥴에 의해서만 task가 실행되게끔 하는 오퍼레이터. 수작업으로 dag을 수행 시켰거나 과거날짜로 dag을 수행시켰을 때는 후행 task들은 돌아가지 않음. 가장 최근에 설정된 job에 의해서만 후행 task들이 돌아감.\n\n\nairflow.operators.subdag\nSubDagOperator\n\n* 일종의 task 그룹화, dag안에 또다른 dag을 불러올 수 있음. 해당 오퍼레이터 안에 다른 오퍼레이터를 둘 수 있음 (Task group과 유사)\n\n\nairflow.operators.trigger_dagrun\nTriggerDagRunOperator\n**\n* 다른 DAG을 수행시키기 위한 오퍼레이터. 예를 들어 task1에 의해 다른 dag이 수행되도록 설정할 수 있다.\n\n\nairflow.operators.weekday\nBranchDayOfWeekOperator\n\n* 특정 요일에 따라 분기처리할 수 있는 오퍼레이터\n\n\nairflow.operators.python\nPythonOperator\n***\n* 어떤 파이썬 함수를 실행시키기 위한 오퍼레이터\n\n\nairflow.operators.python\nBranchPythonOperator\n*\n* 파이썬 함수 실행 결과에 따라 task를 선택적으로 실행시킬 때 사용되는 오퍼레이터\n\n\nairflow.operators.python\nShortCircuitOperator\n\n* 파이썬 함수 return 값이 False면 후행 Tasks를 Skip처리하고 dag을 종료시키는 오퍼레이터\n\n\nairflow.operators.python\nPythonVirtualenvOperator\n\n* 파이썬 가상환경 생성후 Job 수행하고 마무리되면 가상환경을 삭제해주는 오퍼레이터\n\n\nairflow.operators.python\nExternalPythonOperator\n\n* 기존에 존재하는 파이썬 가상환경에서 Job 수행하게 하는 오퍼레이터\n\n\n\n\nmore oeprators-airflow operators\n\n\n\n\nairflow web service &gt;&gt; Admin &gt;&gt; Providers 에서 확인할 수 있음\nsolution 제공 업체에서 본인들의 solution을 잘 다루게 하기 위해 만든 airflow에 제공한 operator\n솔루션 제공 업체: AWS, GCP, MS Azure 등\n솔루션이란?\n\n제품 또는 서비스 패키지: 여기서 “solution”은 특정 사용 사례에 맞춰진, 사전 구성된 클라우드 서비스의 세트를 말한다. 이는 고객이 보다 쉽고 빠르게 해당 기술을 도입하고 활용할 수 있도록 도와주는 ‘패키지’ 형태의 제품이나 서비스를 의미한다.\n기술적 해결책 또는 구현: 이 경우, “solution”은 특정 비즈니스 요구사항이나 기술적 문제를 해결하기 위한 방법론, 소프트웨어, 서비스, 아키텍처의 조합을 의미한다. 예를 들어, 데이터 분석, 웹 호스팅, 머신 러닝 모델 훈련과 같은 특정 목적을 위한 AWS나 GCP의 기능 및 서비스의 집합을 “solution”이라고 할 수 있다.\n첫 번째 뜻으로 이해해도 많은 context 커버 가능\n\n예를 들어, airflow web service &gt;&gt; Admin &gt;&gt; Providers &gt;&gt; apache-airflow-providers-amazon &gt;&gt; Python API &gt;&gt; [airflow.providers.amazon.aws.hooks, airflow.providers.amazon.aws.operators]\n참고로 airflow는 GCP와 궁합이 잘맞음\n\ntransfer: data 이동\n\napache-airflow-providers-http\n\nSimpleHttpOperator를 이용하여 API 값을 얻어 올 수 있음.\n\n\n\n\n\n\n\n\n\n의존 관계 : dag간 선 후행 관계\nDAG 의존관계 설정 방법\n\nTriggerDagRun Operator\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask2\n\ntask2\n\n\n\ntask1-&gt;task2\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask1-&gt;task3\n\n\n\n\n\ntask4\n\ntask4\n\n\n\ntask1-&gt;task4\n\n\n\n\n\n\n\n\n\n\n\ntask를 만들 때 task내에 dag_id를 명시하는 parameter가 있음\ntask1: PythonOperator 등\ntask2,3,4: TriggerDagRun 오퍼레이터로 다른 DAG 을 실행시키는 오퍼레이터\n\n\nExternalTask Sensor\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nsensor1\n\nsensor1\n\n\n\ntask1\n\ntask1\n\n\n\nsensor1-&gt;task1\n\n\n\n\n\nsensor2\n\nsensor2\n\n\n\nsensor2-&gt;task1\n\n\n\n\n\nsensor3\n\nsensor3\n\n\n\nsensor3-&gt;task1\n\n\n\n\n\nsensor4\n\nsensor4\n\n\n\nsensor4-&gt;task1\n\n\n\n\n\n\n\n\n\n\n\nsensor를 통해 task를 만들기 때문에 여기서 sensor는 task를 의미함\nsensor를 만들 때도 감지해야할 dag_id를 명시해줘야함 (task_id도 명시 가능)\nExternalTask Sensor는 다른 여러 DAGs의 Tasks의 완료를 기다리는 센서\nDAG간 의존관계 설정\n\n방식\n\nTriggerDagRun Operator: 실행할 다른 DAG 의 ID 를 지정하여 수행\n\\(A_Dag \\subset B_Dag\\) 일 때, B_Dag이 A_Dag을 trigger한다.\n\nExternalTask 센서: 본 Task 가 수행되기 전 다른 DAG 의 완료를 기다린 후 수행\n\n권고 사용 시점\n\nTriggerDagRun Operator: Trigger 되는 DAG 의 선행 DAG 이 하나만 있을 경우\nExternalTask 센서: Trigger 되는 DAG 의 선행 DAG 이 2 개 이상인 경우\n\n\n\n\n\n\n\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\nwith DAG(...) as dag:\n\n    start_task = BashOperator(\n        task_id='start_task',\n        bash_command='echo \"start!\"',\n    )\n\n    trigger_dag_task = TriggerDagRunOperator(\n        task_id='trigger_dag_task', #필수값\n        trigger_dag_id='dags_python_operator', #필수값\n        trigger_run_id=None, # 중요: run_id 값 직접 지정\n        execution_date='{{data_interval_start}}',\n        reset_dag_run=True,\n        wait_for_completion=False,\n        poke_interval=60,\n        allowed_states=['success'],\n        failed_states=None\n        )\n\n    start_task &gt;&gt; trigger_dag_task\n\n\n\n\nrun_id: DAG의 수행 방식과 시간을 유일하게 식별해주는 키\n같은 시간이라 해도 수행 방식 (Schedule, manual, Backfill) 에 따라 키가 달라짐\n\n스케쥴: 스케줄에 의해 실행된 경우 scheduled__{{data_interval_start}} 값을 가짐\nmanual: airflow ui web에서 수작업 수행. manual__{{data_interval_start}} 값을 가짐\n\nmanual__{{data_interval_start}}은 수작업 수행 시간이 아니라 수작업으로 실행시킨 스케쥴의 구간값 중 data_interval_start값을 의미\n\nBackfill: 과거 날짜를 이용해 수행. backfill__{{data_interval_start}} 값을 가짐\n\nrun_id를 보는 방법\n\nairflow ui web service &gt;&gt; dag &gt;&gt; grid &gt;&gt; 초록색 긴막대기 &gt;&gt; status Run ID 있음\n\n\n\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\nwith DAG(...) as dag:\n\n    start_task = BashOperator(\n        task_id='start_task',\n        bash_command='echo \"start!\"',\n    )\n\n    trigger_dag_task = TriggerDagRunOperator(\n        task_id='trigger_dag_task', \n        trigger_dag_id='dags_python_operator',\n        trigger_run_id=None, # rund_id 값 직접 지정\n        execution_date='{{data_interval_start}}', # manual_{{execution_date}} 로 수행 (여기에 값을 주면 메뉴얼 방식으로 trigger로 된걸로 간주)\n        reset_dag_run=True, # 이미 run_id 값으로 수행된 이력이 있는 경우에도 dag을 재수행할 것 인지 결정. True면 재수행\n        wait_for_completion=False,\n        poke_interval=60,\n        allowed_states=['success'],\n        failed_states=None\n        )\n\n    start_task &gt;&gt; trigger_dag_task\n\nwait_for_completion:\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask2_by_trigger_dag_run\n\ntask2_by_trigger_dag_run\n\n\n\ntask1-&gt;task2_by_trigger_dag_run\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask2_by_trigger_dag_run-&gt;task3\n\n\n\n\n\ndag_c\n\ndag_c\n\n\n\ntask2_by_trigger_dag_run-&gt;dag_c\n\n\n\n\n\n\n\n\n\n\n\nwait_for_completion=True 의 의미: task2가 dag_c를 돌리고 dag_c가 성공한 상태 후 task2역시 성공 상태가 된 후에 task3을 돌리는 경우\nwait_for_completion=False 의 의미: dag_c의 성공여부와 상관없이 task2가 성공 상태로 빠져나옴\npoke_interval=60 : dag_c의 성공여부를 모니터링 하는 주기 60초\nallowed_states=[‘success’]: trigger dag의 task2가 성공상태로 끝나기 위해 dag_c가 어떤 상태로 수행이 끝나야하는지 명시. 만약 dag_c가 fail상태여도 task2가 성공 상태로 마킹이 되길 원한다면 allowed_states=[‘success’,‘fail’] 로 명시\nfailed_states=None: task2가 실패 상태로 마킹이 되기 위해 dag_c가 어떤 상태로 작업 수행이 완료 되어야 하는지 명시. failed_states=fail이면 dag_c가 실패가 되어야 task2도 실패로 마킹 된다.\nDag Full example\n\n# Package Import\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\nimport pendulum\n\nwith DAG(\n    dag_id='dags_trigger_dag_run_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='30 9 * * *', #9시 30분 daily schedule\n    catchup=False\n) as dag:\n\n    start_task = BashOperator(\n        task_id='start_task',\n        bash_command='echo \"start!\"',\n    )\n\n    trigger_dag_task = TriggerDagRunOperator(\n        task_id='trigger_dag_task',\n        trigger_dag_id='dags_python_operator',\n        trigger_run_id=None,\n        execution_date='{{data_interval_start}}', #9시 30분 daily schedule\n        reset_dag_run=True,\n        wait_for_completion=False,\n        poke_interval=60,\n        allowed_states=['success'],\n        failed_states=None\n        )\n\n    start_task &gt;&gt; trigger_dag_task\n\n\n\n\nSimpleHttp 오퍼레이터를 이용하여 공공데이터 키 발급받기\n\n\n\n“Public Data API Key”를 발급받는 과정은 대개 다음과 같은 단계를 포함한다. 이는 일반적인 절차로, 특정 공공 데이터 API 제공자에 따라 약간의 차이가 있을 수 있다.\n\n공공 데이터 포털 접속: 대부분의 국가에서는 공공 데이터를 제공하기 위한 중앙화된 포털을 운영한다.. 예를 들어, 한국에서는 ’’이 있다.\n회원가입 및 로그인: 포털에 접속한 후, 사용자 계정을 생성하고 로그인한다.\nAPI 키 신청: 로그인 후, API 키를 신청할 수 있는 섹션을 찾는다. 이는 보통 ‘API 키 관리’, ‘내 어플리케이션’, ‘API 신청’ 등의 메뉴에서 찾을 수 있다.\n애플리케이션 등록: API 키를 신청하기 위해서는 대부분의 경우 애플리케이션을 등록해야 하는데 이 과정에서 애플리케이션 이름, 용도, URL 등의 정보를 입력해야 할 수도 있다.\nAPI 키 발급: 애플리케이션 등록이 완료되면, API 키가 발급된다. 이 키는 API를 호출할 때 필요하므로 안전하게 보관해야 한다.\nAPI 문서 확인: API를 효과적으로 사용하기 위해서는 해당 API의 문서를 확인하는 것이 중요한데 문서에서는 API의 엔드포인트, 요청 방식, 필요한 파라미터 등의 정보를 제공한다.\nAPI 호출 시험: API 키를 사용하여 간단한 API 호출하여 API가 정상적으로 작동하는지 확인한다.\n각 공공 데이터 포털의 구체적인 지침과 절차는 웹사이트를 참조해야 한다.\n\n\n\n\n\n\n서울시 열린데이터 광장 portal\n서울시가 보유한 데이터 다운로드 가능\n일회성 다운로드 : Csv, json, xml 등을 직접 다운로드\n스케쥴에 의한 주기성 다운로드 : openAPI(http method를 통해 data를 다운로드 할 수 있도록 개방해놓은 API) 통해 다운 가능\n데이터 검색\n\n먼저, 어떤 종류의 데이터를 다운로드 받을 수 있는지 확인하기ㅏ 위해 서울 열린데이터 광장 검색창에 데이터 검색하거나 데이터 카테고리 선택하여 openAI Tag붙은 데이터를 선택해야 한다 (본인은 문화/관광 선택).\n\n\nopen API tag없으면 manual로 데이터 셋 다운로드 받아야함. \n\nSample URL: 서울시립미술관 전시 현황 http://openapi.seoul.go.kr:8088/(인증키)/xml/ListExhibitionOfSeoulMOAInfo/1/5/\nSample URL 작성은 요청 인자를 참고하여 적어 넣으면 된다. 예를 들어,\n\nhttp://openapi.seoul.go.kr:8088/(인증키)/xml/ListExhibitionOfSeoulMOAInfo/1/5/ 은 다음과 같은 요청 인자 양식에 의해 적혀져 있다.\nhttp://openapi.seoul.go.kr:8088/(key)/(type)/(service)/(start_index)/(end_index)/\n1 부터 1000행 까지는 한번에 가져올 수 있지만 1000행 넘어가면 에러 발생\n\n그래서, 1~1000행, 10001행~2000행, … 와 같이 끊어서 가져가야 함\n\n요청 인자 중: DP_SEQ, DP_NAME이 있는데 특정 값을 입력하면 filtering되어 조건에 만족하는 데이터만 query해서 가져올 수 있음.\n\n다른 Sample URL 예시: 서울시 자랑스러운 한국음식점 정보 (한국어)\n\nurl 양식: http://openapi.seoul.go.kr:8088/(key:인증키)/(type)/(servicec)/(type)/(start_index)/(end_index)/(main_key 혹은 날짜)\n샘플 URL: http://openapi.seoul.go.kr:8088/(인증키)/xml/SebcKoreanRestaurantsKor/1/5\n\n이렇게 Sample URL 양식은 http://openapi.seoul.go.kr:8088/(key:인증키)/(type)/(servicec)/(type)/(start_index)/(end_index) 까지는 공통됨.\n\nopenAPI 이용할 경우 api KEY 발급받아야 함 로그인 필요\n\n로그인 &gt;&gt; 이용 안내 &gt;&gt; open API 소개 &gt;&gt;\nAPI Key 신청: 일반 인증키 신청 or 실시간 지하철 인증키 신청\n\nsheet는 최대 1,000건 (행) 까지 노출됨. 전체 데이터는 CSV파일을 내려받아 확인해야함.\n\n애플리케이션 등록\n\n서비스(사용) 환경: 웹사이트 개발\n사용 URL: localhost (or 이 데이터를 사용할 여러분들의 홈패이지 주소)\n관리용 대표 이메일: 홍길동@naver.com\n활용용도: 블로그 개발 (자유형식 - 적당히 내용 채워 넣음)\n내용: 문화/전시 관련 소식을 스케쥴을 이용해 전달 받음 (자유형식 - 적당히 내용 채워 넣음)\n\n\n\n\n\n\n\n\n\n\n\n\n\nHTTP 요청을 하고 결과로 text 를 리턴 받는 오퍼레이터 리턴 값은 Xcom 에 저장\nHTTP 를 이용하여 API 를 처리하는 RestAPI 호출시 사용 가능\n\nRestAPI: API 방법 중 하나로 http의 protocol을 이용해서 API data를 제공하거나, 다운로드, 변경할 수 있는 API를 제공하는 방식 :::{.callout-note}\nREST API는 Representational State Transfer의 약자로, 웹 기반의 서비스 간에 통신을 위한 일반적인 규칙(아키텍처 스타일)을 의미\nREST API는 인터넷에서 데이터를 교환하기 위한 표준 방법 중 하나로 널리 사용되는데 간단히 말해서, REST API는 웹 애플리케이션에서 다른 시스템과 정보를 주고받기 위한 방법이다.\nREST API의 주요 특징\n\n클라이언트-서버 구조: REST API는 클라이언트(예: 웹 브라우저)와 서버 간의 분리를 기반으로 함. 클라이언트는 사용자 인터페이스와 사용자 상호작용을 관리하고, 서버는 데이터 저장 및 백엔드 로직을 처리\n무상태(Stateless): 각 요청은 독립적. 즉, 이전 요청의 상태를 서버가 기억하지 않는다는 의미. 모든 필요한 정보는 각 요청과 함께 전송되어야 한다.\n캐시 가능: REST API 응답은 캐시될 수 있으므로, 성능을 향상시키고 서버의 부하를 줄일 수 있다.\n유니폼 인터페이스(Uniform Interface): REST API는 표준화된 방법을 사용하여 서버의 리소스에 접근. 이는 HTTP 메서드를 활용하는데, 예를 들어 GET(읽기), POST(생성), PUT(수정), DELETE(삭제) 등이 있다.\n리소스 기반: REST API에서 ’리소스’는 웹에서의 객체, 데이터 또는 서비스를 의미하며, 각 리소스는 고유한 URI(Uniform Resource Identifier)를 통해 식별됨.\n\n\n:::\n\nSimpleHttpOperator를 이용해서 RestAPI를 호출\nhttp://localhost:8080/provider/apache-airflow-providers-http 에서 오퍼레이터 명세 확인하기\npython API click\n\nhttp Operator click 의 자주 쓰는 parameters\n\nhttp_conn_id (str) – The http connection to run the operator against: full url의 ’.com/나머지’ 의 .com 을 넣어줌\nendpoint (str | None) – The relative part of the full url. (templated): full url의 ‘~.com/나머지’ 의 나머지를 넣어줌\nmethod (str) – The HTTP method to use, default = “POST”: http의 4개 methods\n\nGET: data 가져오기\nPOST: data insert\nPUT: data 변경/update\nDELETE: data 삭제하기\n\ndata (Any) – The data to pass. POST-data in POST/PUT and params in the URL for a GET request. (templated)\n\nPOST의 경우: insert할 data 값\nGET의 경우: HTTP Protocol을 GET으로 줬으면 GET요청의 parameter를 dictionary 형태로 입력해주면 됨\n\nheaders (dict[str, str] | None) – The HTTP headers to be added to the GET request\nresponse_check (Callable[…, bool] | None) – A check against the ‘requests’ response object. The callable takes the response object as the first positional argument and optionally any number of keyword arguments available in the context dictionary. It should return True for ‘pass’ and False otherwise.\n\ndata 요청시 응답이 제대로 왔는지 확인\ndata를 일회성으로 가져올 때 데이터 1000 행이 제대로 들어왔는지 간단한 조회로 알아볼 수 있지만\ndata를 open API를 이용하여 주기적으로 내려받는 자동화의 경우 일일히 확인하는게 아니라 데이터가 잘 내려 받았는지 확인하는 함수를 하나 만들어 이 parameter에 할당하면 됨.\ntrue로 resturn하면 API가 정상적으로 실행된 것으로 간주\n\nresponse_filter (Callable[…, Any] | None) – A function allowing you to manipulate the response text. e.g response_filter=lambda response: json.loads(response.text). The callable takes the response object as the first positional argument and optionally any number of keyword arguments available in the context dictionary.\n\nAPI의 요청 결과가 dictionary 형태의 string으로 출력됨. 나중에 dictionary type으로 바꿔주고 key: value 형태로 access하여 원하는 데이터를 가져 와야 한다.\n이런 전처리 과정을 수행하는 함수를 만들어 reponse_filter parameter에 넣어줘야함.\n\n\n\n\n\n\n\n\n\nhttp_conn_id에 들어갈 connection id 만들기\n\nairflow web service &gt;&gt; Admin &gt;&gt; Plus button\nConn_id: 다른 conn 이름과 중복되지 않게 string으로 작성\n\nConnection_type: HTTP\nHost: openapi.seoul.go.kr\ntoken값에 의해 인증되는 방식이기 때문에 schema/login/password 필요없음\nPort: 8088\nTest 버튼 클릭시 “405:Method Not Allowed” 가 뜨지만 무방함\n\n\n\n\nfrom airflow.providers.http.operators.http import SimpleHttpOperator\nwith DAG(...) as dag:\n  tb_cycle_station_info = SimpleHttpOperator(\n    task_id ='tb_cycle_station_info',\n    http_conn_id = 'openapi.seoul.go.kr',\n    endpoint ='{{var.value.apikey_openpai_seoul_go_kr}}/json/SebcKoreanRestaurantsKor/1/1000/',\n    method ='GET',\n    headers ={'Content-Type: 'application/json',\n              charset': 'utf-8',\n              'Accept': '*/*'\n              }\n  )\n\n\n\nSimpleHttpOperator를 1000개의 DAGs 에 작성했는데 API 키가 바뀐다면?\nDAG에다가 바로 인증키를 복붙하면 다른 사람들도 API키를 볼 수 있어 보안상의 문제가 될 수 있다.\n위의 2가지 문제를 해결하기 위해 global variable을 이용하여 적을 것.\n\nAPI key를 variable을 이용하여 등록: airflow web service &gt;&gt; admin &gt;&gt; Variables\n\nkey:value형태로 등록 가능\n관리자가 DB에 들어가면 API Key값 볼 수 있음\n\n\nKey에 아래 이름이 있을 경우 val 을 자동 마스킹처리하여 보여줌\n\n‘access_token’,\n‘api_key’,\n‘apikey’,\n‘authorization’,\n‘passphrase’,\n‘passwd’,\n‘password’,\n‘private_key’,\n‘secret’,\n‘token’\n\nglobal variable 설정하면\n\n서울시 공공데이터 추출하는 DAG 이 여러 개 있어도 API 키를 하나로 관리 가능\nAPI 키를 코드에 노출시키지 않음\n\n\n\n\n\n\n\n\n\n\nAirflow는 오퍼레이터를 직접 만들어 사용할 수 있도록 클래스를 제공 (BaseOperator)\n확장성을 비약적으로 높여주는 기능으로 airflow가 인기가 많은 이유가 됨\nBaseOperator 상속한 자식 operator가 custom operator가 됨\nBaseOperator 상속시 두 가지 메서드를 재정의해야 함 (Overriding)\n\nOverriding: 객체 지향 언어에서 부모 클래스가 가지고있던 method를 자식 class에서 재정의\n\n\n생성자 재정의: def__init__\n\n\n클래스에서 객체 생성시 객체에 대한 초기값 지정하는 함수\n\n\ndef execute(self, context) (자식 클래스에서 똑같은 이름으로 써야함)\n\n\ninit 생성자로 객체를 얻은 후 execute 메서드를 실행시키도록 되어 있음\n비즈니스 로직은 execute 에 구현 필요\n\n오퍼레이터 기능 정의\n\n기존에 있던 operator들로 job을 수행하기에 제한적이었던 점을 보완할 기능을 정의해야함\n\nsimpleHttpOperator에서 불편했던 점은 매번 endpoint에 시작행/끝행을 넣어서 호출 해줘야 했는데 이것을 1000행씩 불러오도록 하는 기능이 필요\nxcom에서 data를 가지고 온후 data에 접근할 수 있는 형태로 전처리를 해줘야하는 부분이 있었는데 전처리없이 local에다가 바로 저장할 수 있도록 하는 기능이 필요\n\n서울시 공공데이터 API 를 호출하여 전체 데이터를 받은 후 .csv 파일로 저장하기\n\n오퍼레이터와 dag 의 위치\n\n/dags/dags_seoul_api.py 생성\nfrom operators.seoul_api_to_csv_operator\nimport SeoulApiToCsvOperator\n/plugins/operators/seoul_api_to_csv_operator.py 생성\n\nTemplate 문법을 사용할 수 있는 Template을 지정 (사용 가능한 파라미터 지정하기)\n아래의 HelloOperator는 __init__과 execute(self, context) 둘다 재정의 해줬기 때문에 코드상으론 심플하지만 이미 custom operator로서의 기능을 할 수 있다.\n\nclass HelloOperator(BaseOperator):\n  template_fields: Sequence[str] = (\"name\",) # 이 line 어떤 parameter에 template 문법을 적용할지 지정해주면 됨\n  # 생성자 함수 __init__ 의 member들로 template 문법을 적용할 paratemer 지정\n  def __init__(self, name: str, world: str, **kwargs) -&gt; None:\n    super().__init__(**kwargs)\n    self.name = name\n    self.world =world\n  def execute(self, context):\n    message = f\"Hello {self.world} it's {self.name}!\"\n    print(message)\n    return message\nwith dag:\n  hello_task = HelloOperator(\n    task_id='task_id_1',\n    name '{{ task_instance.task_id }}',\n    world='Earth'\n  )\n\nGoogle Airflow Custom Operator\n\n\n\n\nAirflow Custom Operator Example\n\n\n\n\n\n\n\n\n\ninit 생성자 재정의\n\nclass SeoulApiToCsvOperator(BaseOperator):\n  template_fields = (' endpoint', ' path','file_ name','base_dt')\n  \n  def __init__(self , dataset_nm , path , file_name , base_dt=None , **kwargs):\n    #  dataset_nm , path , file_name , base_dt를 user로부터 받음\n    super().__init__(**kwargs)\n    self.http_conn_id = 'openapi.seoul.go.kr' #hard coding\n    self.path = path\n    self.file_name = file_name\n    self.endpoint = '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + datset_nm\n    self.base_dt =base_dt\n    # template 문법이 적용될 수 있도록 self.path 을 path로, \n    # self.file_name을 file_name로 \n    # self.endpoint 을 '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + datset_nm로,\n    # self.base_dt을 base_dt로 지정\n\ndag full example\n\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.hooks.base import BaseHook\nimport pandas as pd \n\nclass SeoulApiToCsvOperator(BaseOperator):\n    template_fields = ('endpoint', 'path','file_name','base_dt')\n\n    def __init__(self, dataset_nm, path, file_name, base_dt=None, **kwargs):\n        super().__init__(**kwargs)\n        self.http_conn_id = 'openapi.seoul.go.kr'\n        self.path = path\n        self.file_name = file_name\n        self.endpoint = '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + dataset_nm\n        self.base_dt = base_dt\n\n    def execute(self, context):\n    '''\n    url:8080/endpoint\n    endpoint=apikey/type/dataset_nm/start/end\n    즉, url:8080/apikey/type/dataset_nm/start/end 로 줬어야 했다.\n    '''\n        import os\n        \n        connection = BaseHook.get_connection(self.http_conn_id)\n        self.base_url = f'http://{connection.host}:{connection.port}/{self.endpoint}'\n\n        total_row_df = pd.DataFrame()\n        start_row = 1\n        end_row = 1000\n        while True:\n            self.log.info(f'시작:{start_row}')\n            self.log.info(f'끝:{end_row}')\n            row_df = self._call_api(self.base_url, start_row, end_row)\n            total_row_df = pd.concat([total_row_df, row_df])\n            if len(row_df) &lt; 1000:\n                break\n            else:\n                start_row = end_row + 1\n                end_row += 1000\n\n        if not os.path.exists(self.path):\n            os.system(f'mkdir -p {self.path}')\n        total_row_df.to_csv(self.path + '/' + self.file_name, encoding='utf-8', index=False)\n\n    def _call_api(self, base_url, start_row, end_row):\n        import requests #http의 get 요청을 하는 library\n        import json \n\n        headers = {'Content-Type': 'application/json',\n                   'charset': 'utf-8',\n                   'Accept': '*/*'\n                   }\n\n        request_url = f'{base_url}/{start_row}/{end_row}/'\n        if self.base_dt is not None:\n            request_url = f'{base_url}/{start_row}/{end_row}/{self.base_dt}'\n        response = requests.get(request_url, headers) # response는 dictionary형태의 string으로 들어옴\n        contents = json.loads(response.text) # json.loads() string이 dictionary로 반환됨\n\n        key_nm = list(contents.keys())[0]\n        row_data = contents.get(key_nm).get('row')\n        row_df = pd.DataFrame(row_data)\n\n        return row_df\n\nfrom operators.seoul_api_to_csv_operator import SeoulApiToCsvOperator\nfrom airflow import DAG\nimport pendulum\n\nwith DAG(\n    dag_id='dags_seoul_api_corona',\n    schedule='0 7 * * *',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    catchup=False\n) as dag:\n    '''서울시 코로나19 확진자 발생동향'''\n    tb_corona19_count_status = SeoulApiToCsvOperator(\n        task_id='tb_corona19_count_status',\n        dataset_nm='TbCorona19CountStatus',\n        path='/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}',\n        file_name='TbCorona19CountStatus.csv'\n    )\n    \n    '''서울시 코로나19 백신 예방접종 현황'''\n    tv_corona19_vaccine_stat_new = SeoulApiToCsvOperator(\n        task_id='tv_corona19_vaccine_stat_new',\n        dataset_nm='tvCorona19VaccinestatNew',\n        path='/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}',\n        file_name='tvCorona19VaccinestatNew.csv'\n    )\n\n    tb_corona19_count_status &gt;&gt; tv_corona19_vaccine_stat_new\n\ntb_corona19_count_status = SeoulApiToCsvOperator() 의 수행은 wokrer_container가 주체\nSeoulApiToCsvOperator()의 path=‘/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(“Asia/Seoul”) | ds_nodash }}’.\n\n여기서 worker container는 ‘/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(“Asia/Seoul”) | ds_nodash }}’ 연결이 안되어 있기 때문에\ncontainer가 내려갔다가 다시 올라오면 files의 내용은 다 사라짐\ndocker_compose.yaml에서 경로 지정을 해줘야 자동으로 인식하여 wsl/files에 csv가 자동으로 저장된다.\n\n  volumes:\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow/plugins:/opt/airflow/plugin\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow/files:/opt/airflow/files\n\n실행 날짜로 저장된 directory명안에 csv를 vi editor로 열고 se nu 명령어로 건수를 확인\n\n\n\n\n\n\nCustom 오퍼레이터를 만들면 왜 좋을까 ?\n\n비효율성 제거\n\n만약 custom 오퍼레이터를 만들지 않았다면\n개발자마다 각자 서울 공공데이터 데이터셋 추출 저장하는 파이썬 파일을 만들어 PythonOperator 를 이용해 개발했을 것\n비슷한 동작을 하는 파이썬 파일이 관리되지 않은 채 수십 개 만들어지면 그 자체로 비효율 발생\n운영하는 사람 입장에서 비슷한 script가 여러 개 있으면 이해할 수가 없음\n\n재사용성 강화\n\n특정기능을 하는 모듈을 만들어 놓고 , 상세 조건은 파라미터로 받게끔하여 모듈을 재사용할 수 있도록 유도\nCustom 오퍼레이터 개발\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List\nEngineering Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#provider-operator",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#provider-operator",
    "title": "More Operators",
    "section": "",
    "text": "airflow web service &gt;&gt; Admin &gt;&gt; Providers 에서 확인할 수 있음\nsolution 제공 업체에서 본인들의 solution을 잘 다루게 하기 위해 만든 airflow에 제공한 operator\n솔루션 제공 업체: AWS, GCP, MS Azure 등\n솔루션이란?\n\n제품 또는 서비스 패키지: 여기서 “solution”은 특정 사용 사례에 맞춰진, 사전 구성된 클라우드 서비스의 세트를 말한다. 이는 고객이 보다 쉽고 빠르게 해당 기술을 도입하고 활용할 수 있도록 도와주는 ‘패키지’ 형태의 제품이나 서비스를 의미한다.\n기술적 해결책 또는 구현: 이 경우, “solution”은 특정 비즈니스 요구사항이나 기술적 문제를 해결하기 위한 방법론, 소프트웨어, 서비스, 아키텍처의 조합을 의미한다. 예를 들어, 데이터 분석, 웹 호스팅, 머신 러닝 모델 훈련과 같은 특정 목적을 위한 AWS나 GCP의 기능 및 서비스의 집합을 “solution”이라고 할 수 있다.\n첫 번째 뜻으로 이해해도 많은 context 커버 가능\n\n예를 들어, airflow web service &gt;&gt; Admin &gt;&gt; Providers &gt;&gt; apache-airflow-providers-amazon &gt;&gt; Python API &gt;&gt; [airflow.providers.amazon.aws.hooks, airflow.providers.amazon.aws.operators]\n참고로 airflow는 GCP와 궁합이 잘맞음\n\ntransfer: data 이동\n\napache-airflow-providers-http\n\nSimpleHttpOperator를 이용하여 API 값을 얻어 올 수 있음."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#dag간-의존관계-설정",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#dag간-의존관계-설정",
    "title": "More Operators",
    "section": "",
    "text": "의존 관계 : dag간 선 후행 관계\nDAG 의존관계 설정 방법\n\nTriggerDagRun Operator\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask2\n\ntask2\n\n\n\ntask1-&gt;task2\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask1-&gt;task3\n\n\n\n\n\ntask4\n\ntask4\n\n\n\ntask1-&gt;task4\n\n\n\n\n\n\n\n\n\n\n\ntask를 만들 때 task내에 dag_id를 명시하는 parameter가 있음\ntask1: PythonOperator 등\ntask2,3,4: TriggerDagRun 오퍼레이터로 다른 DAG 을 실행시키는 오퍼레이터\n\n\nExternalTask Sensor\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\nsensor1\n\nsensor1\n\n\n\ntask1\n\ntask1\n\n\n\nsensor1-&gt;task1\n\n\n\n\n\nsensor2\n\nsensor2\n\n\n\nsensor2-&gt;task1\n\n\n\n\n\nsensor3\n\nsensor3\n\n\n\nsensor3-&gt;task1\n\n\n\n\n\nsensor4\n\nsensor4\n\n\n\nsensor4-&gt;task1\n\n\n\n\n\n\n\n\n\n\n\nsensor를 통해 task를 만들기 때문에 여기서 sensor는 task를 의미함\nsensor를 만들 때도 감지해야할 dag_id를 명시해줘야함 (task_id도 명시 가능)\nExternalTask Sensor는 다른 여러 DAGs의 Tasks의 완료를 기다리는 센서\nDAG간 의존관계 설정\n\n방식\n\nTriggerDagRun Operator: 실행할 다른 DAG 의 ID 를 지정하여 수행\n\\(A_Dag \\subset B_Dag\\) 일 때, B_Dag이 A_Dag을 trigger한다.\n\nExternalTask 센서: 본 Task 가 수행되기 전 다른 DAG 의 완료를 기다린 후 수행\n\n권고 사용 시점\n\nTriggerDagRun Operator: Trigger 되는 DAG 의 선행 DAG 이 하나만 있을 경우\nExternalTask 센서: Trigger 되는 DAG 의 선행 DAG 이 2 개 이상인 경우"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#triggerdagrun-오퍼레이터",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#triggerdagrun-오퍼레이터",
    "title": "More Operators",
    "section": "",
    "text": "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\nwith DAG(...) as dag:\n\n    start_task = BashOperator(\n        task_id='start_task',\n        bash_command='echo \"start!\"',\n    )\n\n    trigger_dag_task = TriggerDagRunOperator(\n        task_id='trigger_dag_task', #필수값\n        trigger_dag_id='dags_python_operator', #필수값\n        trigger_run_id=None, # 중요: run_id 값 직접 지정\n        execution_date='{{data_interval_start}}',\n        reset_dag_run=True,\n        wait_for_completion=False,\n        poke_interval=60,\n        allowed_states=['success'],\n        failed_states=None\n        )\n\n    start_task &gt;&gt; trigger_dag_task"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#triggerdagrun-오퍼레이터-의-run_id",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#triggerdagrun-오퍼레이터-의-run_id",
    "title": "More Operators",
    "section": "",
    "text": "run_id: DAG의 수행 방식과 시간을 유일하게 식별해주는 키\n같은 시간이라 해도 수행 방식 (Schedule, manual, Backfill) 에 따라 키가 달라짐\n\n스케쥴: 스케줄에 의해 실행된 경우 scheduled__{{data_interval_start}} 값을 가짐\nmanual: airflow ui web에서 수작업 수행. manual__{{data_interval_start}} 값을 가짐\n\nmanual__{{data_interval_start}}은 수작업 수행 시간이 아니라 수작업으로 실행시킨 스케쥴의 구간값 중 data_interval_start값을 의미\n\nBackfill: 과거 날짜를 이용해 수행. backfill__{{data_interval_start}} 값을 가짐\n\nrun_id를 보는 방법\n\nairflow ui web service &gt;&gt; dag &gt;&gt; grid &gt;&gt; 초록색 긴막대기 &gt;&gt; status Run ID 있음\n\n\n\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\nwith DAG(...) as dag:\n\n    start_task = BashOperator(\n        task_id='start_task',\n        bash_command='echo \"start!\"',\n    )\n\n    trigger_dag_task = TriggerDagRunOperator(\n        task_id='trigger_dag_task', \n        trigger_dag_id='dags_python_operator',\n        trigger_run_id=None, # rund_id 값 직접 지정\n        execution_date='{{data_interval_start}}', # manual_{{execution_date}} 로 수행 (여기에 값을 주면 메뉴얼 방식으로 trigger로 된걸로 간주)\n        reset_dag_run=True, # 이미 run_id 값으로 수행된 이력이 있는 경우에도 dag을 재수행할 것 인지 결정. True면 재수행\n        wait_for_completion=False,\n        poke_interval=60,\n        allowed_states=['success'],\n        failed_states=None\n        )\n\n    start_task &gt;&gt; trigger_dag_task\n\nwait_for_completion:\n\n\n\n\n\n\n\n\nG\n\n\ncluster0\n\nTask Flow\n\n\n\ntask1\n\ntask1\n\n\n\ntask2_by_trigger_dag_run\n\ntask2_by_trigger_dag_run\n\n\n\ntask1-&gt;task2_by_trigger_dag_run\n\n\n\n\n\ntask3\n\ntask3\n\n\n\ntask2_by_trigger_dag_run-&gt;task3\n\n\n\n\n\ndag_c\n\ndag_c\n\n\n\ntask2_by_trigger_dag_run-&gt;dag_c\n\n\n\n\n\n\n\n\n\n\n\nwait_for_completion=True 의 의미: task2가 dag_c를 돌리고 dag_c가 성공한 상태 후 task2역시 성공 상태가 된 후에 task3을 돌리는 경우\nwait_for_completion=False 의 의미: dag_c의 성공여부와 상관없이 task2가 성공 상태로 빠져나옴\npoke_interval=60 : dag_c의 성공여부를 모니터링 하는 주기 60초\nallowed_states=[‘success’]: trigger dag의 task2가 성공상태로 끝나기 위해 dag_c가 어떤 상태로 수행이 끝나야하는지 명시. 만약 dag_c가 fail상태여도 task2가 성공 상태로 마킹이 되길 원한다면 allowed_states=[‘success’,‘fail’] 로 명시\nfailed_states=None: task2가 실패 상태로 마킹이 되기 위해 dag_c가 어떤 상태로 작업 수행이 완료 되어야 하는지 명시. failed_states=fail이면 dag_c가 실패가 되어야 task2도 실패로 마킹 된다.\nDag Full example\n\n# Package Import\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\nimport pendulum\n\nwith DAG(\n    dag_id='dags_trigger_dag_run_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='30 9 * * *', #9시 30분 daily schedule\n    catchup=False\n) as dag:\n\n    start_task = BashOperator(\n        task_id='start_task',\n        bash_command='echo \"start!\"',\n    )\n\n    trigger_dag_task = TriggerDagRunOperator(\n        task_id='trigger_dag_task',\n        trigger_dag_id='dags_python_operator',\n        trigger_run_id=None,\n        execution_date='{{data_interval_start}}', #9시 30분 daily schedule\n        reset_dag_run=True,\n        wait_for_completion=False,\n        poke_interval=60,\n        allowed_states=['success'],\n        failed_states=None\n        )\n\n    start_task &gt;&gt; trigger_dag_task"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#서울시-공공데이터-보기",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#서울시-공공데이터-보기",
    "title": "More Operators",
    "section": "",
    "text": "서울시 열린데이터 광장 portal\n서울시가 보유한 데이터 다운로드 가능\n일회성 다운로드 : Csv, json, xml 등을 직접 다운로드\n스케쥴에 의한 주기성 다운로드 : openAPI(http method를 통해 data를 다운로드 할 수 있도록 개방해놓은 API) 통해 다운 가능\n데이터 검색\n\n먼저, 어떤 종류의 데이터를 다운로드 받을 수 있는지 확인하기ㅏ 위해 서울 열린데이터 광장 검색창에 데이터 검색하거나 데이터 카테고리 선택하여 openAI Tag붙은 데이터를 선택해야 한다 (본인은 문화/관광 선택).\n\n\nopen API tag없으면 manual로 데이터 셋 다운로드 받아야함. \n\nSample URL: 서울시립미술관 전시 현황 http://openapi.seoul.go.kr:8088/(인증키)/xml/ListExhibitionOfSeoulMOAInfo/1/5/\nSample URL 작성은 요청 인자를 참고하여 적어 넣으면 된다. 예를 들어,\n\nhttp://openapi.seoul.go.kr:8088/(인증키)/xml/ListExhibitionOfSeoulMOAInfo/1/5/ 은 다음과 같은 요청 인자 양식에 의해 적혀져 있다.\nhttp://openapi.seoul.go.kr:8088/(key)/(type)/(service)/(start_index)/(end_index)/\n1 부터 1000행 까지는 한번에 가져올 수 있지만 1000행 넘어가면 에러 발생\n\n그래서, 1~1000행, 10001행~2000행, … 와 같이 끊어서 가져가야 함\n\n요청 인자 중: DP_SEQ, DP_NAME이 있는데 특정 값을 입력하면 filtering되어 조건에 만족하는 데이터만 query해서 가져올 수 있음.\n\n다른 Sample URL 예시: 서울시 자랑스러운 한국음식점 정보 (한국어)\n\nurl 양식: http://openapi.seoul.go.kr:8088/(key:인증키)/(type)/(servicec)/(type)/(start_index)/(end_index)/(main_key 혹은 날짜)\n샘플 URL: http://openapi.seoul.go.kr:8088/(인증키)/xml/SebcKoreanRestaurantsKor/1/5\n\n이렇게 Sample URL 양식은 http://openapi.seoul.go.kr:8088/(key:인증키)/(type)/(servicec)/(type)/(start_index)/(end_index) 까지는 공통됨.\n\nopenAPI 이용할 경우 api KEY 발급받아야 함 로그인 필요\n\n로그인 &gt;&gt; 이용 안내 &gt;&gt; open API 소개 &gt;&gt;\nAPI Key 신청: 일반 인증키 신청 or 실시간 지하철 인증키 신청\n\nsheet는 최대 1,000건 (행) 까지 노출됨. 전체 데이터는 CSV파일을 내려받아 확인해야함.\n\n애플리케이션 등록\n\n서비스(사용) 환경: 웹사이트 개발\n사용 URL: localhost (or 이 데이터를 사용할 여러분들의 홈패이지 주소)\n관리용 대표 이메일: 홍길동@naver.com\n활용용도: 블로그 개발 (자유형식 - 적당히 내용 채워 넣음)\n내용: 문화/전시 관련 소식을 스케쥴을 이용해 전달 받음 (자유형식 - 적당히 내용 채워 넣음)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#simplehttpoperator-란",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#simplehttpoperator-란",
    "title": "More Operators",
    "section": "",
    "text": "HTTP 요청을 하고 결과로 text 를 리턴 받는 오퍼레이터 리턴 값은 Xcom 에 저장\nHTTP 를 이용하여 API 를 처리하는 RestAPI 호출시 사용 가능\n\nRestAPI: API 방법 중 하나로 http의 protocol을 이용해서 API data를 제공하거나, 다운로드, 변경할 수 있는 API를 제공하는 방식 :::{.callout-note}\nREST API는 Representational State Transfer의 약자로, 웹 기반의 서비스 간에 통신을 위한 일반적인 규칙(아키텍처 스타일)을 의미\nREST API는 인터넷에서 데이터를 교환하기 위한 표준 방법 중 하나로 널리 사용되는데 간단히 말해서, REST API는 웹 애플리케이션에서 다른 시스템과 정보를 주고받기 위한 방법이다.\nREST API의 주요 특징\n\n클라이언트-서버 구조: REST API는 클라이언트(예: 웹 브라우저)와 서버 간의 분리를 기반으로 함. 클라이언트는 사용자 인터페이스와 사용자 상호작용을 관리하고, 서버는 데이터 저장 및 백엔드 로직을 처리\n무상태(Stateless): 각 요청은 독립적. 즉, 이전 요청의 상태를 서버가 기억하지 않는다는 의미. 모든 필요한 정보는 각 요청과 함께 전송되어야 한다.\n캐시 가능: REST API 응답은 캐시될 수 있으므로, 성능을 향상시키고 서버의 부하를 줄일 수 있다.\n유니폼 인터페이스(Uniform Interface): REST API는 표준화된 방법을 사용하여 서버의 리소스에 접근. 이는 HTTP 메서드를 활용하는데, 예를 들어 GET(읽기), POST(생성), PUT(수정), DELETE(삭제) 등이 있다.\n리소스 기반: REST API에서 ’리소스’는 웹에서의 객체, 데이터 또는 서비스를 의미하며, 각 리소스는 고유한 URI(Uniform Resource Identifier)를 통해 식별됨.\n\n\n:::\n\nSimpleHttpOperator를 이용해서 RestAPI를 호출\nhttp://localhost:8080/provider/apache-airflow-providers-http 에서 오퍼레이터 명세 확인하기\npython API click\n\nhttp Operator click 의 자주 쓰는 parameters\n\nhttp_conn_id (str) – The http connection to run the operator against: full url의 ’.com/나머지’ 의 .com 을 넣어줌\nendpoint (str | None) – The relative part of the full url. (templated): full url의 ‘~.com/나머지’ 의 나머지를 넣어줌\nmethod (str) – The HTTP method to use, default = “POST”: http의 4개 methods\n\nGET: data 가져오기\nPOST: data insert\nPUT: data 변경/update\nDELETE: data 삭제하기\n\ndata (Any) – The data to pass. POST-data in POST/PUT and params in the URL for a GET request. (templated)\n\nPOST의 경우: insert할 data 값\nGET의 경우: HTTP Protocol을 GET으로 줬으면 GET요청의 parameter를 dictionary 형태로 입력해주면 됨\n\nheaders (dict[str, str] | None) – The HTTP headers to be added to the GET request\nresponse_check (Callable[…, bool] | None) – A check against the ‘requests’ response object. The callable takes the response object as the first positional argument and optionally any number of keyword arguments available in the context dictionary. It should return True for ‘pass’ and False otherwise.\n\ndata 요청시 응답이 제대로 왔는지 확인\ndata를 일회성으로 가져올 때 데이터 1000 행이 제대로 들어왔는지 간단한 조회로 알아볼 수 있지만\ndata를 open API를 이용하여 주기적으로 내려받는 자동화의 경우 일일히 확인하는게 아니라 데이터가 잘 내려 받았는지 확인하는 함수를 하나 만들어 이 parameter에 할당하면 됨.\ntrue로 resturn하면 API가 정상적으로 실행된 것으로 간주\n\nresponse_filter (Callable[…, Any] | None) – A function allowing you to manipulate the response text. e.g response_filter=lambda response: json.loads(response.text). The callable takes the response object as the first positional argument and optionally any number of keyword arguments available in the context dictionary.\n\nAPI의 요청 결과가 dictionary 형태의 string으로 출력됨. 나중에 dictionary type으로 바꿔주고 key: value 형태로 access하여 원하는 데이터를 가져 와야 한다.\n이런 전처리 과정을 수행하는 함수를 만들어 reponse_filter parameter에 넣어줘야함."
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#커넥션-등록",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#커넥션-등록",
    "title": "More Operators",
    "section": "",
    "text": "http_conn_id에 들어갈 connection id 만들기\n\nairflow web service &gt;&gt; Admin &gt;&gt; Plus button\nConn_id: 다른 conn 이름과 중복되지 않게 string으로 작성\n\nConnection_type: HTTP\nHost: openapi.seoul.go.kr\ntoken값에 의해 인증되는 방식이기 때문에 schema/login/password 필요없음\nPort: 8088\nTest 버튼 클릭시 “405:Method Not Allowed” 가 뜨지만 무방함"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#simplehttpoperator-작성",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#simplehttpoperator-작성",
    "title": "More Operators",
    "section": "",
    "text": "from airflow.providers.http.operators.http import SimpleHttpOperator\nwith DAG(...) as dag:\n  tb_cycle_station_info = SimpleHttpOperator(\n    task_id ='tb_cycle_station_info',\n    http_conn_id = 'openapi.seoul.go.kr',\n    endpoint ='{{var.value.apikey_openpai_seoul_go_kr}}/json/SebcKoreanRestaurantsKor/1/1000/',\n    method ='GET',\n    headers ={'Content-Type: 'application/json',\n              charset': 'utf-8',\n              'Accept': '*/*'\n              }\n  )\n\n\n\nSimpleHttpOperator를 1000개의 DAGs 에 작성했는데 API 키가 바뀐다면?\nDAG에다가 바로 인증키를 복붙하면 다른 사람들도 API키를 볼 수 있어 보안상의 문제가 될 수 있다.\n위의 2가지 문제를 해결하기 위해 global variable을 이용하여 적을 것.\n\nAPI key를 variable을 이용하여 등록: airflow web service &gt;&gt; admin &gt;&gt; Variables\n\nkey:value형태로 등록 가능\n관리자가 DB에 들어가면 API Key값 볼 수 있음\n\n\nKey에 아래 이름이 있을 경우 val 을 자동 마스킹처리하여 보여줌\n\n‘access_token’,\n‘api_key’,\n‘apikey’,\n‘authorization’,\n‘passphrase’,\n‘passwd’,\n‘password’,\n‘private_key’,\n‘secret’,\n‘token’\n\nglobal variable 설정하면\n\n서울시 공공데이터 추출하는 DAG 이 여러 개 있어도 API 키를 하나로 관리 가능\nAPI 키를 코드에 노출시키지 않음"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#airflow-의-꽃-custom-오퍼레이터",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#airflow-의-꽃-custom-오퍼레이터",
    "title": "More Operators",
    "section": "",
    "text": "Airflow는 오퍼레이터를 직접 만들어 사용할 수 있도록 클래스를 제공 (BaseOperator)\n확장성을 비약적으로 높여주는 기능으로 airflow가 인기가 많은 이유가 됨\nBaseOperator 상속한 자식 operator가 custom operator가 됨\nBaseOperator 상속시 두 가지 메서드를 재정의해야 함 (Overriding)\n\nOverriding: 객체 지향 언어에서 부모 클래스가 가지고있던 method를 자식 class에서 재정의\n\n\n생성자 재정의: def__init__\n\n\n클래스에서 객체 생성시 객체에 대한 초기값 지정하는 함수\n\n\ndef execute(self, context) (자식 클래스에서 똑같은 이름으로 써야함)\n\n\ninit 생성자로 객체를 얻은 후 execute 메서드를 실행시키도록 되어 있음\n비즈니스 로직은 execute 에 구현 필요\n\n오퍼레이터 기능 정의\n\n기존에 있던 operator들로 job을 수행하기에 제한적이었던 점을 보완할 기능을 정의해야함\n\nsimpleHttpOperator에서 불편했던 점은 매번 endpoint에 시작행/끝행을 넣어서 호출 해줘야 했는데 이것을 1000행씩 불러오도록 하는 기능이 필요\nxcom에서 data를 가지고 온후 data에 접근할 수 있는 형태로 전처리를 해줘야하는 부분이 있었는데 전처리없이 local에다가 바로 저장할 수 있도록 하는 기능이 필요\n\n서울시 공공데이터 API 를 호출하여 전체 데이터를 받은 후 .csv 파일로 저장하기\n\n오퍼레이터와 dag 의 위치\n\n/dags/dags_seoul_api.py 생성\nfrom operators.seoul_api_to_csv_operator\nimport SeoulApiToCsvOperator\n/plugins/operators/seoul_api_to_csv_operator.py 생성\n\nTemplate 문법을 사용할 수 있는 Template을 지정 (사용 가능한 파라미터 지정하기)\n아래의 HelloOperator는 __init__과 execute(self, context) 둘다 재정의 해줬기 때문에 코드상으론 심플하지만 이미 custom operator로서의 기능을 할 수 있다.\n\nclass HelloOperator(BaseOperator):\n  template_fields: Sequence[str] = (\"name\",) # 이 line 어떤 parameter에 template 문법을 적용할지 지정해주면 됨\n  # 생성자 함수 __init__ 의 member들로 template 문법을 적용할 paratemer 지정\n  def __init__(self, name: str, world: str, **kwargs) -&gt; None:\n    super().__init__(**kwargs)\n    self.name = name\n    self.world =world\n  def execute(self, context):\n    message = f\"Hello {self.world} it's {self.name}!\"\n    print(message)\n    return message\nwith dag:\n  hello_task = HelloOperator(\n    task_id='task_id_1',\n    name '{{ task_instance.task_id }}',\n    world='Earth'\n  )\n\nGoogle Airflow Custom Operator\n\n\n\n\nAirflow Custom Operator Example"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#custom-오퍼레이터-만들기",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#custom-오퍼레이터-만들기",
    "title": "More Operators",
    "section": "",
    "text": "init 생성자 재정의\n\nclass SeoulApiToCsvOperator(BaseOperator):\n  template_fields = (' endpoint', ' path','file_ name','base_dt')\n  \n  def __init__(self , dataset_nm , path , file_name , base_dt=None , **kwargs):\n    #  dataset_nm , path , file_name , base_dt를 user로부터 받음\n    super().__init__(**kwargs)\n    self.http_conn_id = 'openapi.seoul.go.kr' #hard coding\n    self.path = path\n    self.file_name = file_name\n    self.endpoint = '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + datset_nm\n    self.base_dt =base_dt\n    # template 문법이 적용될 수 있도록 self.path 을 path로, \n    # self.file_name을 file_name로 \n    # self.endpoint 을 '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + datset_nm로,\n    # self.base_dt을 base_dt로 지정\n\ndag full example\n\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.hooks.base import BaseHook\nimport pandas as pd \n\nclass SeoulApiToCsvOperator(BaseOperator):\n    template_fields = ('endpoint', 'path','file_name','base_dt')\n\n    def __init__(self, dataset_nm, path, file_name, base_dt=None, **kwargs):\n        super().__init__(**kwargs)\n        self.http_conn_id = 'openapi.seoul.go.kr'\n        self.path = path\n        self.file_name = file_name\n        self.endpoint = '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + dataset_nm\n        self.base_dt = base_dt\n\n    def execute(self, context):\n    '''\n    url:8080/endpoint\n    endpoint=apikey/type/dataset_nm/start/end\n    즉, url:8080/apikey/type/dataset_nm/start/end 로 줬어야 했다.\n    '''\n        import os\n        \n        connection = BaseHook.get_connection(self.http_conn_id)\n        self.base_url = f'http://{connection.host}:{connection.port}/{self.endpoint}'\n\n        total_row_df = pd.DataFrame()\n        start_row = 1\n        end_row = 1000\n        while True:\n            self.log.info(f'시작:{start_row}')\n            self.log.info(f'끝:{end_row}')\n            row_df = self._call_api(self.base_url, start_row, end_row)\n            total_row_df = pd.concat([total_row_df, row_df])\n            if len(row_df) &lt; 1000:\n                break\n            else:\n                start_row = end_row + 1\n                end_row += 1000\n\n        if not os.path.exists(self.path):\n            os.system(f'mkdir -p {self.path}')\n        total_row_df.to_csv(self.path + '/' + self.file_name, encoding='utf-8', index=False)\n\n    def _call_api(self, base_url, start_row, end_row):\n        import requests #http의 get 요청을 하는 library\n        import json \n\n        headers = {'Content-Type': 'application/json',\n                   'charset': 'utf-8',\n                   'Accept': '*/*'\n                   }\n\n        request_url = f'{base_url}/{start_row}/{end_row}/'\n        if self.base_dt is not None:\n            request_url = f'{base_url}/{start_row}/{end_row}/{self.base_dt}'\n        response = requests.get(request_url, headers) # response는 dictionary형태의 string으로 들어옴\n        contents = json.loads(response.text) # json.loads() string이 dictionary로 반환됨\n\n        key_nm = list(contents.keys())[0]\n        row_data = contents.get(key_nm).get('row')\n        row_df = pd.DataFrame(row_data)\n\n        return row_df\n\nfrom operators.seoul_api_to_csv_operator import SeoulApiToCsvOperator\nfrom airflow import DAG\nimport pendulum\n\nwith DAG(\n    dag_id='dags_seoul_api_corona',\n    schedule='0 7 * * *',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    catchup=False\n) as dag:\n    '''서울시 코로나19 확진자 발생동향'''\n    tb_corona19_count_status = SeoulApiToCsvOperator(\n        task_id='tb_corona19_count_status',\n        dataset_nm='TbCorona19CountStatus',\n        path='/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}',\n        file_name='TbCorona19CountStatus.csv'\n    )\n    \n    '''서울시 코로나19 백신 예방접종 현황'''\n    tv_corona19_vaccine_stat_new = SeoulApiToCsvOperator(\n        task_id='tv_corona19_vaccine_stat_new',\n        dataset_nm='tvCorona19VaccinestatNew',\n        path='/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}',\n        file_name='tvCorona19VaccinestatNew.csv'\n    )\n\n    tb_corona19_count_status &gt;&gt; tv_corona19_vaccine_stat_new\n\ntb_corona19_count_status = SeoulApiToCsvOperator() 의 수행은 wokrer_container가 주체\nSeoulApiToCsvOperator()의 path=‘/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(“Asia/Seoul”) | ds_nodash }}’.\n\n여기서 worker container는 ‘/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone(“Asia/Seoul”) | ds_nodash }}’ 연결이 안되어 있기 때문에\ncontainer가 내려갔다가 다시 올라오면 files의 내용은 다 사라짐\ndocker_compose.yaml에서 경로 지정을 해줘야 자동으로 인식하여 wsl/files에 csv가 자동으로 저장된다.\n\n  volumes:\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow/plugins:/opt/airflow/plugin\n  - ${AIRFLOW_PROJ_DIR:-.}/airflow/files:/opt/airflow/files\n\n실행 날짜로 저장된 directory명안에 csv를 vi editor로 열고 se nu 명령어로 건수를 확인"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#summary",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#summary",
    "title": "More Operators",
    "section": "",
    "text": "Custom 오퍼레이터를 만들면 왜 좋을까 ?\n\n비효율성 제거\n\n만약 custom 오퍼레이터를 만들지 않았다면\n개발자마다 각자 서울 공공데이터 데이터셋 추출 저장하는 파이썬 파일을 만들어 PythonOperator 를 이용해 개발했을 것\n비슷한 동작을 하는 파이썬 파일이 관리되지 않은 채 수십 개 만들어지면 그 자체로 비효율 발생\n운영하는 사람 입장에서 비슷한 script가 여러 개 있으면 이해할 수가 없음\n\n재사용성 강화\n\n특정기능을 하는 모듈을 만들어 놓고 , 상세 조건은 파라미터로 받게끔하여 모듈을 재사용할 수 있도록 유도\nCustom 오퍼레이터 개발"
  },
  {
    "objectID": "docs/blog/posts/Engineering/airflow/08.more_operators.html#public-data-api-key-obtaining-steps",
    "href": "docs/blog/posts/Engineering/airflow/08.more_operators.html#public-data-api-key-obtaining-steps",
    "title": "More Operators",
    "section": "",
    "text": "“Public Data API Key”를 발급받는 과정은 대개 다음과 같은 단계를 포함한다. 이는 일반적인 절차로, 특정 공공 데이터 API 제공자에 따라 약간의 차이가 있을 수 있다.\n\n공공 데이터 포털 접속: 대부분의 국가에서는 공공 데이터를 제공하기 위한 중앙화된 포털을 운영한다.. 예를 들어, 한국에서는 ’’이 있다.\n회원가입 및 로그인: 포털에 접속한 후, 사용자 계정을 생성하고 로그인한다.\nAPI 키 신청: 로그인 후, API 키를 신청할 수 있는 섹션을 찾는다. 이는 보통 ‘API 키 관리’, ‘내 어플리케이션’, ‘API 신청’ 등의 메뉴에서 찾을 수 있다.\n애플리케이션 등록: API 키를 신청하기 위해서는 대부분의 경우 애플리케이션을 등록해야 하는데 이 과정에서 애플리케이션 이름, 용도, URL 등의 정보를 입력해야 할 수도 있다.\nAPI 키 발급: 애플리케이션 등록이 완료되면, API 키가 발급된다. 이 키는 API를 호출할 때 필요하므로 안전하게 보관해야 한다.\nAPI 문서 확인: API를 효과적으로 사용하기 위해서는 해당 API의 문서를 확인하는 것이 중요한데 문서에서는 API의 엔드포인트, 요청 방식, 필요한 파라미터 등의 정보를 제공한다.\nAPI 호출 시험: API 키를 사용하여 간단한 API 호출하여 API가 정상적으로 작동하는지 확인한다.\n각 공공 데이터 포털의 구체적인 지침과 절차는 웹사이트를 참조해야 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#web",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#web",
    "title": "Content List, Engineering",
    "section": "9 Web",
    "text": "9 Web\n\n2023-05-01, HTTP Methods"
  },
  {
    "objectID": "docs/blog/posts/Engineering/web/http_method.html",
    "href": "docs/blog/posts/Engineering/web/http_method.html",
    "title": "HTTP Method",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n1 Basic HTTP Methods\nHTTP 메서드(HTTP Methods)는 클라이언트가 웹 서버에 어떤 동작을 원하는지 서버에 알리기 위해 사용되는 명령들이다. HTTP 프로토콜의 일부로 정의되어 있으며, 가장 일반적인 메서드들은 다음과 같다:\n\nGET: 서버로부터 정보를 조회하기 위해 사용. 데이터를 가져오는 데 사용되며, 서버의 상태를 변경하지 않는다.\nPOST: 서버에 데이터를 전송하기 위해 사용. 주로 데이터베이스에 새로운 데이터를 추가하거나, 폼을 제출할 때 사용.\nPUT: 서버에 있는 자원을 업데이트하기 위해 사용. 주로 기존 데이터를 새 데이터로 대체할 때 사용.\nDELETE: 서버의 특정 자원을 삭제하기 위해 사용.\nHEAD: GET과 유사하지만, 응답 본문(body) 없이 HTTP 헤더 정보만을 요청할 때 사용. 주로 자원의 메타데이터를 확인할 때 사용.\nPATCH: PUT과 유사하지만, 전체 자원을 대체하는 것이 아니라 일부를 수정할 때 사용.\nOPTIONS: 웹 서버에서 지원하는 HTTP 메서드를 알아보기 위해 사용. 주로 CORS(Cross-Origin Resource Sharing)에서 사전 요청을 처리하는 데 사용.\n\n이러한 메서드들은 웹 서버와의 통신에서 특정한 종류의 요청을 나타내며, RESTful API 디자인에서 핵심적인 역할을 한다.\n\n\n\n\n\n\n\n\n\n2 Go to Blog Content List\nBlog Content List\nEngineering Content List"
  }
]