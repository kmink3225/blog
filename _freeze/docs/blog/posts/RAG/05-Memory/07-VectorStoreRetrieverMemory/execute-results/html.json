{
  "hash": "c3ba53678a5d2ac70d41de28841321cb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"VectorStoreRetrieverMemory\"\nsubtitle: 대화 메모리\ndescription: |\n  대화 컨텍스트를 관리하는 다양한 메모리 시스템을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 12/31/2024\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n`VectorStoreRetrieverMemory` 는 벡터 스토어에 메모리를 저장하고 호출될 때마다 가장 '눈에 띄는' 상위 K개의 문서를 쿼리합니다.\n\n이는 대화내용의 순서를 **명시적으로 추적하지 않는다는 점** 에서 다른 대부분의 메모리 클래스와 다릅니다.\n\n::: {#8328b1b5 .cell execution_count=1}\n``` {.python .cell-code}\n# API KEY를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API KEY 정보로드\nload_dotenv()\n```\n:::\n\n\n먼저, 벡터 스토어를 초기화 합니다.\n\n::: {#058b4286 .cell execution_count=2}\n``` {.python .cell-code}\nimport faiss\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.docstore import InMemoryDocstore\nfrom langchain.vectorstores import FAISS\n\n\n# 임베딩 모델을 정의합니다.\nembeddings_model = OpenAIEmbeddings()\n\n# Vector Store 를 초기화 합니다.\nembedding_size = 1536\nindex = faiss.IndexFlatL2(embedding_size)\nvectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})\n```\n:::\n\n\n실제 사용에서는 `k`를 더 높은 값으로 설정하지만, 여기서는 `k=1` 을 사용하여 다음과 같이 표시합니다.\n\n::: {#5b295b3d .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain.memory import VectorStoreRetrieverMemory\n\n# 벡터 조회가 여전히 의미적으로 관련성 있는 정보를 반환한다는 것을 보여주기 위해서입니다.\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\nmemory = VectorStoreRetrieverMemory(retriever=retriever)\n\n# 임의의 대화를 저장합니다.\nmemory.save_context(\n    inputs={\n        \"human\": \"안녕하세요, 오늘 면접에 참석해주셔서 감사합니다. 자기소개 부탁드립니다.\"\n    },\n    outputs={\n        \"ai\": \"안녕하세요. 저는 컴퓨터 과학을 전공한 신입 개발자입니다. 대학에서는 주로 자바와 파이썬을 사용했으며, 최근에는 웹 개발 프로젝트에 참여하여 실제 사용자를 위한 서비스를 개발하는 경험을 했습니다.\"\n    },\n)\nmemory.save_context(\n    inputs={\"human\": \"프로젝트에서 어떤 역할을 맡았나요?\"},\n    outputs={\n        \"ai\": \"제가 맡은 역할은 백엔드 개발자였습니다. 사용자 데이터 처리와 서버 로직 개발을 담당했으며, RESTful API를 구현하여 프론트엔드와의 통신을 담당했습니다. 또한, 데이터베이스 설계에도 참여했습니다.\"\n    },\n)\nmemory.save_context(\n    inputs={\n        \"human\": \"팀 프로젝트에서 어려움을 겪었던 경험이 있다면 어떻게 해결했나요?\"\n    },\n    outputs={\n        \"ai\": \"프로젝트 초기에 의사소통 문제로 몇 가지 어려움이 있었습니다. 이를 해결하기 위해 저희 팀은 정기적인 미팅을 갖고 각자의 진행 상황을 공유했습니다. 또한, 문제가 발생했을 때는 적극적으로 의견을 나누고, 합리적인 해결책을 찾기 위해 노력했습니다.\"\n    },\n)\nmemory.save_context(\n    inputs={\"human\": \"개발자로서 자신의 강점은 무엇이라고 생각하나요?\"},\n    outputs={\n        \"ai\": \"제 강점은 빠른 학습 능력과 문제 해결 능력입니다. 새로운 기술이나 도구를 빠르게 습득할 수 있으며, 복잡한 문제에 직면했을 때 창의적인 해결책을 제시할 수 있습니다. 또한, 팀워크를 중시하며 동료들과 협력하는 것을 중요하게 생각합니다.\"\n    },\n)\n```\n:::\n\n\n다음의 질문을 했을 때 Vector Store 로 부터 1개(k=1 이기 때문)의 가장 관련성 높은 대화를 반환합니다.\n\n- 질문: \"면접자 전공은 무엇인가요?\"\n\n::: {#5527c112 .cell execution_count=4}\n``` {.python .cell-code}\n# 메모리에 질문을 통해 가장 연관성 높은 1개 대화를 추출합니다.\nprint(memory.load_memory_variables({\"human\": \"면접자 전공은 무엇인가요?\"})[\"history\"])\n```\n:::\n\n\n이번에는 다른 질문을 통해 가장 연관성 높은 1개 대화를 추출합니다.\n\n- 질문: \"면접자가 프로젝트에서 맡은 역할은 무엇인가요?\"\n\n::: {#20b7babd .cell execution_count=5}\n``` {.python .cell-code}\nprint(\n    memory.load_memory_variables(\n        {\"human\": \"면접자가 프로젝트에서 맡은 역할은 무엇인가요?\"}\n    )[\"history\"]\n)\n```\n:::\n\n\n",
    "supporting": [
      "07-VectorStoreRetrieverMemory_files"
    ],
    "filters": [],
    "includes": {}
  }
}