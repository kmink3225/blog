{
  "hash": "585fae8d6f42b81a89cf759f94abbab3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"CacheBackedEmbeddings\"\nsubtitle: 임베딩\ndescription: |\n  텍스트를 벡터로 변환하는 다양한 임베딩 모델을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 12/31/2024\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\nEmbeddings는 재계산을 피하기 위해 저장되거나 일시적으로 캐시될 수 있습니다.\n\nEmbeddings를 캐싱하는 것은 `CacheBackedEmbeddings`를 사용하여 수행될 수 있습니다. 캐시 지원 embedder는 embeddings를 키-값 저장소에 캐싱하는 embedder 주변에 래퍼입니다. 텍스트는 해시되고 이 해시는 캐시에서 키로 사용됩니다.\n\n`CacheBackedEmbeddings`를 초기화하는 주요 지원 방법은 `from_bytes_store`입니다. 이는 다음 매개변수를 받습니다:\n\n- `underlying_embeddings`: 임베딩을 위해 사용되는 embedder.\n- `document_embedding_cache`: 문서 임베딩을 캐싱하기 위한 `ByteStore` 중 하나.\n- `namespace`: (선택 사항, 기본값은 `\"\"`) 문서 캐시를 위해 사용되는 네임스페이스. 이 네임스페이스는 다른 캐시와의 충돌을 피하기 위해 사용됩니다. 예를 들어, 사용된 임베딩 모델의 이름으로 설정하세요.\n\n**주의**: 동일한 텍스트가 다른 임베딩 모델을 사용하여 임베딩될 때 충돌을 피하기 위해 `namespace` 매개변수를 설정하는 것이 중요합니다.\n\n::: {#0157c14e .cell execution_count=1}\n``` {.python .cell-code}\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n```\n:::\n\n\n::: {#f233a8df .cell execution_count=2}\n``` {.python .cell-code}\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH08-Embeddings\")\n```\n:::\n\n\n## LocalFileStore 에서 임베딩 사용 (영구 보관)\n\n먼저, 로컬 파일 시스템을 사용하여 임베딩을 저장하고 FAISS 벡터 스토어를 사용하여 검색하는 예제를 살펴보겠습니다.\n\n::: {#156d7b29 .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain.storage import LocalFileStore\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.embeddings import CacheBackedEmbeddings\nfrom langchain_community.vectorstores.faiss import FAISS\n\n# OpenAI 임베딩을 사용하여 기본 임베딩 설정\nembedding = OpenAIEmbeddings()\n\n# 로컬 파일 저장소 설정\nstore = LocalFileStore(\"./cache/\")\n\n# 캐시를 지원하는 임베딩 생성\ncached_embedder = CacheBackedEmbeddings.from_bytes_store(\n    underlying_embeddings=embedding,\n    document_embedding_cache=store,\n    namespace=embedding.model,  # 기본 임베딩과 저장소를 사용하여 캐시 지원 임베딩을 생성\n)\n```\n:::\n\n\n::: {#b26dc8f5 .cell execution_count=4}\n``` {.python .cell-code}\n# store에서 키들을 순차적으로 가져옵니다.\nlist(store.yield_keys())\n```\n:::\n\n\n문서를 로드하고, 청크로 분할한 다음, 각 청크를 임베딩하고 벡터 저장소에 로드합니다.\n\n::: {#78a849c8 .cell execution_count=5}\n``` {.python .cell-code}\nfrom langchain.document_loaders import TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\n\n# 문서 로드\nraw_documents = TextLoader(\"./data/appendix-keywords.txt\").load()\n# 문자 단위로 텍스트 분할 설정\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n# 문서 분할\ndocuments = text_splitter.split_documents(raw_documents)\n```\n:::\n\n\n::: {#1a45358e .cell execution_count=6}\n``` {.python .cell-code}\n# 코드 실행 시간을 측정합니다.\n%time db = FAISS.from_documents(documents, cached_embedder)  # 문서로부터 FAISS 데이터베이스 생성\n```\n:::\n\n\n벡터 저장소를 다시 생성하려고 하면, 임베딩을 다시 계산할 필요가 없기 때문에 훨씬 더 빠르게 처리됩니다.\n\n::: {#0bb1f536 .cell execution_count=7}\n``` {.python .cell-code}\n# 캐싱된 임베딩을 사용하여 FAISS 데이터베이스 생성\n%time db2 = FAISS.from_documents(documents, cached_embedder)\n```\n:::\n\n\n## `InmemoryByteStore` 사용 (비영구적)\n\n다른 `ByteStore`를 사용하기 위해서는 `CacheBackedEmbeddings`를 생성할 때 해당 `ByteStore`를 사용하면 됩니다.\n\n아래에서는, 비영구적인 `InMemoryByteStore`를 사용하여 동일한 캐시된 임베딩 객체를 생성하는 예시를 보여줍니다.\n\n::: {#1df292c4 .cell execution_count=8}\n``` {.python .cell-code}\nfrom langchain.embeddings import CacheBackedEmbeddings\nfrom langchain.storage import InMemoryByteStore\n\nstore = InMemoryByteStore()  # 메모리 내 바이트 저장소 생성\n\n# 캐시 지원 임베딩 생성\ncached_embedder = CacheBackedEmbeddings.from_bytes_store(\n    embedding, store, namespace=embedding.model\n)\n```\n:::\n\n\n",
    "supporting": [
      "02-CacheBackedEmbeddings_files"
    ],
    "filters": [],
    "includes": {}
  }
}