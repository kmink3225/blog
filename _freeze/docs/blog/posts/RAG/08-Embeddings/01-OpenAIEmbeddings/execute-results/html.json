{
  "hash": "bcf86b48e77f11426a804ba30752c821",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"OpenAIEmbeddings\"\nsubtitle: 임베딩\ndescription: |\n  텍스트를 벡터로 변환하는 다양한 임베딩 모델을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 12/31/2024\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n문서 임베딩은 문서의 내용을 수치적인 벡터로 변환하는 과정입니다. \n\n이 과정을 통해 문서의 의미를 수치화하고, 다양한 자연어 처리 작업에 활용할 수 있습니다. 대표적인 사전 학습된 언어 모델로는 BERT와 GPT가 있으며, 이러한 모델들은 문맥적 정보를 포착하여 문서의 의미를 인코딩합니다. \n\n문서 임베딩은 토큰화된 문서를 모델에 입력하여 임베딩 벡터를 생성하고, 이를 평균하여 전체 문서의 벡터를 생성합니다. 이 벡터는 문서 분류, 감성 분석, 문서 간 유사도 계산 등에 활용될 수 있습니다.\n\n[더 알아보기](https://platform.openai.com/docs/guides/embeddings/embedding-models)\n\n\n## 설정\n\n먼저 langchain-openai를 설치하고 필요한 환경 변수를 설정합니다.\n\n::: {#68102db0 .cell execution_count=1}\n``` {.python .cell-code}\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n```\n:::\n\n\n::: {#f1c8f152 .cell execution_count=2}\n``` {.python .cell-code}\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH08-Embeddings\")\n```\n:::\n\n\n지원되는 모델 목록\n\n| MODEL                  | PAGES PER DOLLAR | PERFORMANCE ON MTEB EVAL | MAX INPUT |\n|------------------------|------------------|---------------------------|-----------|\n| text-embedding-3-small | 62,500           | 62.3%                     | 8191      |\n| text-embedding-3-large | 9,615            | 64.6%                     | 8191      |\n| text-embedding-ada-002 | 12,500           | 61.0%                     | 8191      |\n\n::: {#90c86810 .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain_openai import OpenAIEmbeddings\n\n# OpenAI의 \"text-embedding-3-small\" 모델을 사용하여 임베딩을 생성합니다.\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n```\n:::\n\n\n::: {#a9785d98 .cell execution_count=4}\n``` {.python .cell-code}\ntext = \"임베딩 테스트를 하기 위한 샘플 문장입니다.\"\n```\n:::\n\n\n## 쿼리 임베딩\n\n`embeddings.embed_query(text)`는 주어진 텍스트를 임베딩 벡터로 변환하는 함수입니다.\n\n이 함수는 텍스트를 벡터 공간에 매핑하여 의미적으로 유사한 텍스트를 찾거나 텍스트 간의 유사도를 계산하는 데 사용될 수 있습니다.\n\n::: {#1d796205 .cell execution_count=5}\n``` {.python .cell-code}\n# 텍스트를 임베딩하여 쿼리 결과를 생성합니다.\nquery_result = embeddings.embed_query(text)\n```\n:::\n\n\n`query_result[:5]`는 `query_result` 리스트의 처음 5개 요소를 슬라이싱(slicing)하여 선택합니다.\n\n::: {#120357ce .cell execution_count=6}\n``` {.python .cell-code}\n# 쿼리 결과의 처음 5개 항목을 선택합니다.\nquery_result[:5]\n```\n:::\n\n\n## Document 임베딩\n\n`embeddings.embed_documents()` 함수를 사용하여 텍스트 문서를 임베딩합니다.\n\n- `[text]`를 인자로 전달하여 단일 문서를 리스트 형태로 임베딩 함수에 전달합니다.\n- 함수 호출 결과로 반환된 임베딩 벡터를 `doc_result` 변수에 할당합니다.\n\n::: {#1da487d8 .cell execution_count=7}\n``` {.python .cell-code}\ndoc_result = embeddings.embed_documents(\n    [text, text, text, text]\n)  # 텍스트를 임베딩하여 문서 벡터를 생성합니다.\n```\n:::\n\n\n`doc_result[0][:5]`는 `doc_result` 리스트의 첫 번째 요소에서 처음 5개의 문자를 슬라이싱하여 선택합니다.\n\n::: {#58c1a66c .cell execution_count=8}\n``` {.python .cell-code}\nlen(doc_result)  # 문서 벡터의 길이를 확인합니다.\n```\n:::\n\n\n::: {#52faf7e9 .cell execution_count=9}\n``` {.python .cell-code}\n# 문서 결과의 첫 번째 요소에서 처음 5개 항목을 선택합니다.\ndoc_result[0][:5]\n```\n:::\n\n\n## 차원 지정\n\n`text-embedding-3` 모델 클래스를 사용하면 반환되는 임베딩의 크기를 지정할 수 있습니다.\n\n예를 들어, 기본적으로 `text-embedding-3-small`는 1536 차원의 임베딩을 반환합니다.\n\n::: {#cbde7a7c .cell execution_count=10}\n``` {.python .cell-code}\n# 문서 결과의 첫 번째 요소의 길이를 반환합니다.\nlen(doc_result[0])\n```\n:::\n\n\n### 차원(dimensions) 조정\n\n하지만 `dimensions=1024`를 전달함으로써 임베딩의 크기를 1024로 줄일 수 있습니다.\n\n::: {#27e85ac1 .cell execution_count=11}\n``` {.python .cell-code}\n# OpenAI의 \"text-embedding-3-small\" 모델을 사용하여 1024차원의 임베딩을 생성하는 객체를 초기화합니다.\nembeddings_1024 = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1024)\n```\n:::\n\n\n::: {#0067c7b7 .cell execution_count=12}\n``` {.python .cell-code}\n# 주어진 텍스트를 임베딩하고 첫 번째 임베딩 벡터의 길이를 반환합니다.\nlen(embeddings_1024.embed_documents([text])[0])\n```\n:::\n\n\n## 유사도 계산\n\n::: {#e5a50874 .cell execution_count=13}\n``` {.python .cell-code}\nsentence1 = \"안녕하세요? 반갑습니다.\"\nsentence2 = \"안녕하세요? 반갑습니다!\"\nsentence3 = \"안녕하세요? 만나서 반가워요.\"\nsentence4 = \"Hi, nice to meet you.\"\nsentence5 = \"I like to eat apples.\"\n```\n:::\n\n\n::: {#a82c1625 .cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsentences = [sentence1, sentence2, sentence3, sentence4, sentence5]\nembedded_sentences = embeddings_1024.embed_documents(sentences)\n```\n:::\n\n\n::: {#4d331860 .cell execution_count=15}\n``` {.python .cell-code}\ndef similarity(a, b):\n    return cosine_similarity([a], [b])[0][0]\n```\n:::\n\n\n::: {#a589d708 .cell execution_count=16}\n``` {.python .cell-code}\n# sentence1 = \"안녕하세요? 반갑습니다.\"\n# sentence2 = \"안녕하세요? 만나서 반가워요.\"\n# sentence3 = \"Hi, nice to meet you.\"\n# sentence4 = \"I like to eat apples.\"\n\nfor i, sentence in enumerate(embedded_sentences):\n    for j, other_sentence in enumerate(embedded_sentences):\n        if i < j:\n            print(\n                f\"[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} \\t <=====> \\t {sentences[j]}\"\n            )\n```\n:::\n\n\n",
    "supporting": [
      "01-OpenAIEmbeddings_files"
    ],
    "filters": [],
    "includes": {}
  }
}