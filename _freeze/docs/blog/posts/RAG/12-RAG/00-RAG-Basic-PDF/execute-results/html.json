{
  "hash": "71fa6fdf3bba1cc308693e65669d2fc1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"RAG 기본 구조 이해하기\"\nsubtitle: RAG 시스템\ndescription: |\n  검색 증강 생성(RAG) 시스템의 구축과 고급 기법을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 12/31/2024\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n## 1. 사전작업(Pre-processing) - 1~4 단계\n\n![rag-1.png](./assets/rag-1.png)\n\n![rag-1-graphic](./assets/rag-graphic-1.png)\n\n사전 작업 단계에서는 데이터 소스를 Vector DB (저장소) 에 문서를 로드-분할-임베딩-저장 하는 4단계를 진행합니다.\n\n- 1단계 문서로드(Document Load): 문서 내용을 불러옵니다.\n- 2단계 분할(Text Split): 문서를 특정 기준(Chunk) 으로 분할합니다.\n- 3단계 임베딩(Embedding): 분할된(Chunk) 를 임베딩하여 저장합니다.\n- 4단계 벡터DB 저장: 임베딩된 Chunk 를 DB에 저장합니다.\n\n## 2. RAG 수행(RunTime) - 5~8 단계\n\n![rag-2.png](./assets/rag-2.png)\n\n![](./assets/rag-graphic-2.png)\n\n- 5단계 검색기(Retriever): 쿼리(Query) 를 바탕으로 DB에서 검색하여 결과를 가져오기 위하여 리트리버를 정의합니다. 리트리버는 검색 알고리즘이며(Dense, Sparse) 리트리버로 나뉘게 됩니다. Dense: 유사도 기반 검색, Sparse: 키워드 기반 검색\n- 6단계 프롬프트: RAG 를 수행하기 위한 프롬프트를 생성합니다. 프롬프트의 context 에는 문서에서 검색된 내용이 입력됩니다. 프롬프트 엔지니어링을 통하여 답변의 형식을 지정할 수 있습니다.\n- 7단계 LLM: 모델을 정의합니다.(GPT-3.5, GPT-4, Claude, etc..)\n- 8단계 Chain: 프롬프트 - LLM - 출력 에 이르는 체인을 생성합니다.\n\n\n## 실습에 활용한 문서\n\n소프트웨어정책연구소(SPRi) - 2023년 12월호\n\n- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n- 링크: https://spri.kr/posts/view/23669\n- 파일명: `SPRI_AI_Brief_2023년12월호_F.pdf`\n\n_실습을 위해 다운로드 받은 파일을 `data` 폴더로 복사해 주시기 바랍니다_\n\n## 환경설정\n\nAPI KEY 를 설정합니다.\n\n::: {#cb53717e .cell execution_count=1}\n``` {.python .cell-code}\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n```\n:::\n\n\nLangChain으로 구축한 애플리케이션은 여러 단계에 걸쳐 LLM 호출을 여러 번 사용하게 됩니다. 이러한 애플리케이션이 점점 더 복잡해짐에 따라, 체인이나 에이전트 내부에서 정확히 무슨 일이 일어나고 있는지 조사할 수 있는 능력이 매우 중요해집니다. 이를 위한 최선의 방법은 [LangSmith](https://smith.langchain.com)를 사용하는 것입니다.\n\nLangSmith가 필수는 아니지만, 유용합니다. LangSmith를 사용하고 싶다면, 위의 링크에서 가입한 후, 로깅 추적을 시작하기 위해 환경 변수를 설정해야 합니다.\n\n::: {#180d2ad6 .cell execution_count=2}\n``` {.python .cell-code}\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install -qU langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH12-RAG\")\n```\n:::\n\n\n## RAG 기본 파이프라인(1~8단계)\n\n::: {#75cf35c7 .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyMuPDFLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n```\n:::\n\n\n아래는 기본적인 RAG 구조 이해를 위한 뼈대코드(skeleton code) 입니다.\n\n각 단계별 모듈의 내용을 앞으로 상황에 맞게 변경하면서 문서에 적합한 구조를 찾아갈 수 있습니다.\n\n(각 단계별로 다양한 옵션을 설정하거나 새로운 기법을 적용할 수 있습니다.)\n\n::: {#9b993062 .cell execution_count=4}\n``` {.python .cell-code}\n# 단계 1: 문서 로드(Load Documents)\nloader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\ndocs = loader.load()\nprint(f\"문서의 페이지수: {len(docs)}\")\n```\n:::\n\n\n페이지의 내용을 출력합니다.\n\n::: {#7fc35e54 .cell execution_count=5}\n``` {.python .cell-code}\nprint(docs[10].page_content)\n```\n:::\n\n\n`metadata` 를 확인합니다.\n\n::: {#a090376f .cell execution_count=6}\n``` {.python .cell-code}\ndocs[10].__dict__\n```\n:::\n\n\n::: {#d67ed1d3 .cell execution_count=7}\n``` {.python .cell-code}\n# 단계 2: 문서 분할(Split Documents)\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\nsplit_documents = text_splitter.split_documents(docs)\nprint(f\"분할된 청크의수: {len(split_documents)}\")\n```\n:::\n\n\n::: {#4955bec4 .cell execution_count=8}\n``` {.python .cell-code}\n# 단계 3: 임베딩(Embedding) 생성\nembeddings = OpenAIEmbeddings()\n```\n:::\n\n\n::: {#abcd21f4 .cell execution_count=9}\n``` {.python .cell-code}\n# 단계 4: DB 생성(Create DB) 및 저장\n# 벡터스토어를 생성합니다.\nvectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n```\n:::\n\n\n::: {#c09e83b7 .cell execution_count=10}\n``` {.python .cell-code}\nfor doc in vectorstore.similarity_search(\"구글\"):\n    print(doc.page_content)\n```\n:::\n\n\n::: {#107e011d .cell execution_count=11}\n``` {.python .cell-code}\n# 단계 5: 검색기(Retriever) 생성\n# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\nretriever = vectorstore.as_retriever()\n```\n:::\n\n\n검색기에 쿼리를 날려 검색된 chunk 결과를 확인합니다.\n\n::: {#ddef26b3 .cell execution_count=12}\n``` {.python .cell-code}\n# 검색기에 쿼리를 날려 검색된 chunk 결과를 확인합니다.\nretriever.invoke(\"삼성전자가 자체 개발한 AI 의 이름은?\")\n```\n:::\n\n\n::: {#4b8efd59 .cell execution_count=13}\n``` {.python .cell-code}\n# 단계 6: 프롬프트 생성(Create Prompt)\n# 프롬프트를 생성합니다.\nprompt = PromptTemplate.from_template(\n    \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nAnswer in Korean.\n\n#Context: \n{context}\n\n#Question:\n{question}\n\n#Answer:\"\"\"\n)\n```\n:::\n\n\n::: {#34baf4b0 .cell execution_count=14}\n``` {.python .cell-code}\n# 단계 7: 언어모델(LLM) 생성\n# 모델(LLM) 을 생성합니다.\nllm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0)\n```\n:::\n\n\n::: {#f194c515 .cell execution_count=15}\n``` {.python .cell-code}\n# 단계 8: 체인(Chain) 생성\nchain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n```\n:::\n\n\n생성된 체인에 쿼리(질문)을 입력하고 실행합니다.\n\n::: {#41bcadd5 .cell execution_count=16}\n``` {.python .cell-code}\n# 체인 실행(Run Chain)\n# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\nquestion = \"삼성전자가 자체 개발한 AI 의 이름은?\"\nresponse = chain.invoke(question)\nprint(response)\n```\n:::\n\n\n## 전체 코드\n\n::: {#3fb1eaeb .cell execution_count=17}\n``` {.python .cell-code}\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyMuPDFLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# 단계 1: 문서 로드(Load Documents)\nloader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\ndocs = loader.load()\n\n# 단계 2: 문서 분할(Split Documents)\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\nsplit_documents = text_splitter.split_documents(docs)\n\n# 단계 3: 임베딩(Embedding) 생성\nembeddings = OpenAIEmbeddings()\n\n# 단계 4: DB 생성(Create DB) 및 저장\n# 벡터스토어를 생성합니다.\nvectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n\n# 단계 5: 검색기(Retriever) 생성\n# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\nretriever = vectorstore.as_retriever()\n\n# 단계 6: 프롬프트 생성(Create Prompt)\n# 프롬프트를 생성합니다.\nprompt = PromptTemplate.from_template(\n    \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nAnswer in Korean.\n\n#Context: \n{context}\n\n#Question:\n{question}\n\n#Answer:\"\"\"\n)\n\n# 단계 7: 언어모델(LLM) 생성\n# 모델(LLM) 을 생성합니다.\nllm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0)\n\n# 단계 8: 체인(Chain) 생성\nchain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n```\n:::\n\n\n::: {#f5c7c2c8 .cell execution_count=18}\n``` {.python .cell-code}\n# 체인 실행(Run Chain)\n# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\nquestion = \"삼성전자가 자체 개발한 AI 의 이름은?\"\nresponse = chain.invoke(question)\nprint(response)\n```\n:::\n\n\n",
    "supporting": [
      "00-RAG-Basic-PDF_files"
    ],
    "filters": [],
    "includes": {}
  }
}