{
  "hash": "e3261bb81980b2abfb71c022949be771",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"LangChain의 RAG 파헤치기\"\nsubtitle: RAG 시스템\ndescription: |\n  검색 증강 생성(RAG) 시스템의 구축과 고급 기법을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 12/31/2024\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n![rag-1.png](./assets/rag-1.png)\n\n![rag-2.png](./assets/rag-2.png)\n\n### 1. 질문 처리\n\n질문 처리 단계에서는 사용자의 질문을 받아 이를 처리하고, 관련 데이터를 찾는 작업이 이루어집니다. 이를 위해 다음과 같은 구성 요소들이 필요합니다:\n\n- **데이터 소스 연결**: 질문에 대한 답변을 찾기 위해 다양한 텍스트 데이터 소스에 연결해야 합니다. LangChain은 다양한 데이터 소스와의 연결을 간편하게 설정할 수 있도록 돕습니다.\n- **데이터 인덱싱 및 검색**: 데이터 소스에서 관련 정보를 효율적으로 찾기 위해, 데이터는 인덱싱되어야 합니다. LangChain은 인덱싱 과정을 자동화하고, 사용자의 질문과 관련된 데이터를 검색하는 데 필요한 도구를 제공합니다.\n\n### 2. 답변 생성\n\n관련 데이터를 찾은 후에는 이를 기반으로 사용자의 질문에 답변을 생성해야 합니다. 이 단계에서는 다음 구성 요소가 중요합니다:\n\n- **답변 생성 모델**: LangChain은 고급 자연어 처리(NLP) 모델을 사용하여 검색된 데이터로부터 답변을 생성할 수 있는 기능을 제공합니다. 이러한 모델은 사용자의 질문과 검색된 데이터를 입력으로 받아, 적절한 답변을 생성합니다.\n\n\n## 아키텍처\n\n우리는 [Q&A 소개](https://python.langchain.comhttps://python.langchain.com/docs/use_cases/question_answering/)에서 개요한 대로 전형적인 RAG 애플리케이션을 만들 것입니다. 이것은 두 가지 주요 구성 요소를 가지고 있습니다:\n\n- **인덱싱**: 소스에서 데이터를 수집하고 인덱싱하는 파이프라인입니다. _이 작업은 보통 오프라인에서 발생합니다._\n\n- **검색 및 생성**: 실제 RAG 체인으로, 사용자 쿼리를 실행 시간에 받아 인덱스에서 관련 데이터를 검색한 다음, 그 데이터를 모델에 전달합니다.\n\nRAW 데이터에서 답변을 받기까지의 전체 순서는 다음과 같습니다.\n\n### 인덱싱\n\n![](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)\n\n1. **로드**: 먼저 데이터를 로드해야 합니다. 이를 위해 [DocumentLoaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)를 사용할 것입니다.\n2. **분할**: [Text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)는 큰 `Documents`를 더 작은 청크로 나눕니다. 이는 데이터를 인덱싱하고 모델에 전달하는 데 유용하며, 큰 청크는 검색하기 어렵고 모델의 유한한 컨텍스트 창에 맞지 않습니다.\n3. **저장**: 나중에 검색할 수 있도록 분할을 저장하고 인덱싱할 장소가 필요합니다. 이는 종종 [VectorStore](https://python.langchain.com/docs/modules/data_connection/vectorstores/)와 [Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/) 모델을 사용하여 수행됩니다.\n\n### 검색 및 생성\n\n![](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)\n\n1. **검색**: 사용자 입력이 주어지면 [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/)를 사용하여 저장소에서 관련 분할을 검색합니다.\n2. **생성**: [ChatModel](https://python.langchain.com/docs/modules/model_io/chat/) / [LLM](https://python.langchain.com/docs/modules/model_io/llms/)은 질문과 검색된 데이터를 포함한 프롬프트를 사용하여 답변을 생성합니다\n\n## 실습에 활용한 문서\n\n소프트웨어정책연구소(SPRi) - 2023년 12월호\n\n- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n- 링크: https://spri.kr/posts/view/23669\n- 파일명: `SPRI_AI_Brief_2023년12월호_F.pdf`\n\n_실습을 위해 다운로드 받은 파일을 `data` 폴더로 복사해 주시기 바랍니다_\n\n## 환경설정\n\nAPI KEY 를 설정합니다.\n\n::: {#34a4f05a .cell execution_count=1}\n``` {.python .cell-code}\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n```\n:::\n\n\nLangChain으로 구축한 애플리케이션은 여러 단계에 걸쳐 LLM 호출을 여러 번 사용하게 됩니다. 이러한 애플리케이션이 점점 더 복잡해짐에 따라, 체인이나 에이전트 내부에서 정확히 무슨 일이 일어나고 있는지 조사할 수 있는 능력이 매우 중요해집니다. 이를 위한 최선의 방법은 [LangSmith](https://smith.langchain.com)를 사용하는 것입니다.\n\nLangSmith가 필수는 아니지만, 유용합니다. LangSmith를 사용하고 싶다면, 위의 링크에서 가입한 후, 로깅 추적을 시작하기 위해 환경 변수를 설정해야 합니다.\n\n::: {#d8c54f67 .cell execution_count=2}\n``` {.python .cell-code}\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install -qU langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH12-RAG\")\n```\n:::\n\n\n## 모듈별로 자세히 살펴보기\n\n::: {#42f51ab4 .cell execution_count=3}\n``` {.python .cell-code}\nimport bs4\nfrom langchain import hub\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma, FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n```\n:::\n\n\n아래는 [](https://teddylee777.github.io/langchain/rag-naver-news-qa/) 에서 다뤘던 기본적인 RAG 모델을 사용하는 예제입니다.\n\n여기서 각 단계별로 다양한 옵션을 설정하거나 새로운 기법을 적용할 수 있습니다.\n\n::: {#87c38d91 .cell execution_count=4}\n``` {.python .cell-code}\n# 단계 1: 문서 로드(Load Documents)\n# 뉴스기사 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\nurl = \"https://n.news.naver.com/article/437/0000378416\"\nloader = WebBaseLoader(\n    web_paths=(url,),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            \"div\",\n            attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]},\n        )\n    ),\n)\ndocs = loader.load()\n\n\n# 단계 2: 문서 분할(Split Documents)\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n\nsplits = text_splitter.split_documents(docs)\n\n# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n# 벡터스토어를 생성합니다.\nvectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n\n# 단계 4: 검색(Search)\n# 뉴스에 포함되어 있는 정보를 검색하고 생성합니다.\nretriever = vectorstore.as_retriever()\n\n# 단계 5: 프롬프트 생성(Create Prompt)\n# 프롬프트를 생성합니다.\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n# 단계 6: 언어모델 생성(Create LLM)\n# 모델(LLM) 을 생성합니다.\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n\ndef format_docs(docs):\n    # 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\n# 단계 7: 체인 생성(Create Chain)\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\n# 단계 8: 체인 실행(Run Chain)\n# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\nquestion = \"부영그룹의 출산 장려 정책에 대해 설명해주세요\"\nresponse = rag_chain.invoke(question)\n\n# 결과 출력\nprint(f\"URL: {url}\")\nprint(f\"문서의 수: {len(docs)}\")\nprint(\"===\" * 20)\nprint(f\"[HUMAN]\\n{question}\\n\")\nprint(f\"[AI]\\n{response}\")\n```\n:::\n\n\n## 단계 1: 문서 로드(Load Documents)\n\n- [공식문서 링크 - Document loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n\n### 웹페이지\n\n`WebBaseLoader`는 지정된 웹 페이지에서 필요한 부분만을 파싱하기 위해 `bs4.SoupStrainer`를 사용합니다.\n\n[참고]\n\n- `bs4.SoupStrainer` 는 편리하게 웹에서 원하는 요소를 가져올 수 있도록 해줍니다.\n\n(예시)\n\n```python\nbs4.SoupStrainer(\n    \"div\",\n    attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]}, # 클래스 명을 입력\n)\n\nbs4.SoupStrainer(\n    \"article\",\n    attrs={\"id\": [\"dic_area\"]}, # 클래스 명을 입력\n)\n```\n\n아래의 BBC 뉴스 기사입니다. 영문으로 작성된 기사로 시험해 보고 싶다면, 아래의 주석을 해제하고 실행해 보세요.\n\n::: {#f1ba4b71 .cell execution_count=5}\n``` {.python .cell-code}\n# 뉴스기사의 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\nloader = WebBaseLoader(\n    web_paths=(\"https://www.bbc.com/news/business-68092814\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            \"main\",\n            attrs={\"id\": [\"main-content\"]},\n        )\n    ),\n)\ndocs = loader.load()\nprint(f\"문서의 수: {len(docs)}\")\ndocs[0].page_content[:500]\n```\n:::\n\n\n### PDF\n\n::: {#32b444bb .cell execution_count=6}\n``` {.python .cell-code}\nfrom langchain.document_loaders import PyPDFLoader\n\n# PDF 파일 로드. 파일의 경로 입력\nloader = PyPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n\n# 페이지 별 문서 로드\ndocs = loader.load()\nprint(f\"문서의 수: {len(docs)}\")\n\n# 10번째 페이지의 내용 출력\nprint(f\"\\n[페이지내용]\\n{docs[10].page_content[:500]}\")\nprint(f\"\\n[metadata]\\n{docs[10].metadata}\\n\")\n```\n:::\n\n\n### CSV\n\nCSV 는 페이지 번호 대신 행번호로 데이터를 조회합니다.\n\n::: {#fa3c1218 .cell execution_count=7}\n``` {.python .cell-code}\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\n\n# CSV 파일 로드\nloader = CSVLoader(file_path=\"data/titanic.csv\")\ndocs = loader.load()\nprint(f\"문서의 수: {len(docs)}\")\n\n# 10번째 페이지의 내용 출력\nprint(f\"\\n[페이지내용]\\n{docs[10].page_content[:500]}\")\nprint(f\"\\n[metadata]\\n{docs[10].metadata}\\n\")\n```\n:::\n\n\n### TXT 파일\n\n::: {#01720055 .cell execution_count=8}\n``` {.python .cell-code}\nfrom langchain_community.document_loaders import TextLoader\n\nloader = TextLoader(\"data/appendix-keywords.txt\")\ndocs = loader.load()\nprint(f\"문서의 수: {len(docs)}\")\n\n# 10번째 페이지의 내용 출력\nprint(f\"\\n[페이지내용]\\n{docs[0].page_content[:500]}\")\nprint(f\"\\n[metadata]\\n{docs[0].metadata}\\n\")\n```\n:::\n\n\n### 폴더 내의 모든 파일 로드\n\n아래는 폴더 내 모든 `.txt` 파일을 로드하는 예시입니다.\n\n::: {#46ed3073 .cell execution_count=9}\n``` {.python .cell-code}\nfrom langchain_community.document_loaders import DirectoryLoader\n\nloader = DirectoryLoader(\".\", glob=\"data/*.txt\", show_progress=True)\ndocs = loader.load()\n\nprint(f\"문서의 수: {len(docs)}\")\n\n# 10번째 페이지의 내용 출력\nprint(f\"\\n[페이지내용]\\n{docs[0].page_content[:500]}\")\nprint(f\"\\n[metadata]\\n{docs[0].metadata}\\n\")\n```\n:::\n\n\n다음은 폴더내 모든 `.pdf` 파일을 로드하는 예제입니다.\n\n::: {#3f6d4073 .cell execution_count=10}\n``` {.python .cell-code}\nfrom langchain_community.document_loaders import DirectoryLoader\n\nloader = DirectoryLoader(\".\", glob=\"data/*.pdf\")\ndocs = loader.load()\n\nprint(f\"문서의 수: {len(docs)}\\n\")\nprint(\"[메타데이터]\\n\")\nprint(docs[0].metadata)\nprint(\"\\n========= [앞부분] 미리보기 =========\\n\")\nprint(docs[0].page_content[2500:3000])\n```\n:::\n\n\n### Python\n\n다음은 `.py` 파일을 로드하는 예제입니다.\n\n::: {#38a331d2 .cell execution_count=11}\n``` {.python .cell-code}\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\".\", glob=\"**/*.py\", loader_cls=PythonLoader)\ndocs = loader.load()\n\nprint(f\"문서의 수: {len(docs)}\\n\")\nprint(\"[메타데이터]\\n\")\nprint(docs[0].metadata)\nprint(\"\\n========= [앞부분] 미리보기 =========\\n\")\nprint(docs[0].page_content[:500])\n```\n:::\n\n\n---\n\n## 단계 2: 문서 분할(Split Documents)\n\n::: {#ac0e6d97 .cell execution_count=12}\n``` {.python .cell-code}\n# 뉴스기사의 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\nloader = WebBaseLoader(\n    web_paths=(\"https://www.bbc.com/news/business-68092814\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            \"main\",\n            attrs={\"id\": [\"main-content\"]},\n        )\n    ),\n)\ndocs = loader.load()\nprint(f\"문서의 수: {len(docs)}\")\ndocs[0].page_content[:500]\n```\n:::\n\n\n### CharacterTextSplitter\n\n이것은 가장 간단한 방법입니다. 이 방법은 문자를 기준으로 분할합니다(기본값은 \"\\n\\n\") 그리고 청크의 길이를 문자의 수로 측정합니다.\n\n1. 텍스트가 어떻게 분할되는지: 단일 문자 단위\n2. 청크 크기가 어떻게 측정되는지: `len` of characters.\n\n시각화 예제: https://chunkviz.up.railway.app/\n\n`CharacterTextSplitter` 클래스는 텍스트를 특정 크기의 청크로 분할하는 기능을 제공합니다.\n\n- `separator` 매개변수는 청크를 구분하는 데 사용되는 문자열을 지정하며, 여기서는 두 개의 개행 문자(`\"\\n\\n\"`)를 사용합니다\n- `chunk_size`는 각 청크의 최대 길이를 결정합니다\n- `chunk_overlap`은 인접한 청크 간에 겹치는 문자의 수를 지정합니다.\n- `length_function`은 청크의 길이를 계산하는 데 사용되는 함수를 결정하며, 기본적으로 문자열의 길이를 반환하는 `len` 함수가 사용됩니다.\n- `is_separator_regex`는 `separator`가 정규 표현식으로 해석될지 여부를 결정하는 불리언 값입니다.\n\n::: {#f222948c .cell execution_count=13}\n``` {.python .cell-code}\nfrom langchain.text_splitter import CharacterTextSplitter\n\ntext_splitter = CharacterTextSplitter(\n    separator=\"\\n\\n\",\n    chunk_size=100,\n    chunk_overlap=10,\n    length_function=len,\n    is_separator_regex=False,\n)\n```\n:::\n\n\n이 함수는 `text_splitter` 객체의 `create_documents` 메소드를 사용하여 주어진 텍스트(`state_of_the_union`)를 여러 문서로 분할하고, 그 결과를 `texts` 변수에 저장합니다. 이후 `texts`의 첫 번째 문서를 출력합니다. 이 과정은 텍스트 데이터를 처리하고 분석하기 위한 초기 단계로 볼 수 있으며, 특히 큰 텍스트 데이터를 관리 가능한 크기의 단위로 나누는 데 유용합니다.\n\n::: {#c3442e84 .cell execution_count=14}\n``` {.python .cell-code}\n# chain of density 논문의 일부 내용을 불러옵니다\nwith open(\"data/chain-of-density.txt\", \"r\") as f:\n    text = f.read()[:500]\n```\n:::\n\n\n::: {#8efc2040 .cell execution_count=15}\n``` {.python .cell-code}\ntext_splitter = CharacterTextSplitter(\n    chunk_size=100, chunk_overlap=10, separator=\"\\n\\n\"\n)\ntext_splitter.split_text(text)\n```\n:::\n\n\n::: {#aaa6f75e .cell execution_count=16}\n``` {.python .cell-code}\ntext_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\"\\n\")\ntext_splitter.split_text(text)\n```\n:::\n\n\n::: {#465d2c3f .cell execution_count=17}\n``` {.python .cell-code}\ntext_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\" \")\ntext_splitter.split_text(text)\n```\n:::\n\n\n::: {#c93f442e .cell execution_count=18}\n``` {.python .cell-code}\ntext_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0, separator=\" \")\ntext_splitter.split_text(text)\n```\n:::\n\n\n::: {#75514ea9 .cell execution_count=19}\n``` {.python .cell-code}\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\" \")\n# text 파일을 청크로 나누어줍니다.\ntext_splitter.split_text(text)\n\n# document를 청크로 나누어줍니다.\nsplit_docs = text_splitter.split_documents(docs)\nlen(split_docs)\n```\n:::\n\n\n::: {#1a28daad .cell execution_count=20}\n``` {.python .cell-code}\nsplit_docs[0]\n```\n:::\n\n\n::: {#edda66bd .cell execution_count=21}\n``` {.python .cell-code}\n# 뉴스기사의 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\nloader = WebBaseLoader(\n    web_paths=(\"https://www.bbc.com/news/business-68092814\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            \"main\",\n            attrs={\"id\": [\"main-content\"]},\n        )\n    ),\n)\n\n# splitter 를 정의합니다.\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\" \")\n\n# 문서를 로드시 바로 분할까지 수행합니다.\nsplit_docs = loader.load_and_split(text_splitter=text_splitter)\nprint(f\"문서의 수: {len(docs)}\")\ndocs[0].page_content[:500]\n```\n:::\n\n\n### RecursiveTextSplitter\n\n이 텍스트 분할기는 일반 텍스트에 권장되는 텍스트 분할기입니다.\n\n1. 텍스트가 어떻게 분할 규칙: list of `separators`\n2. 청크 크기가 어떻게 측정되는가: `len` of characters\n\n::: {#ba6cc359 .cell execution_count=22}\n``` {.python .cell-code}\n# langchain 패키지에서 RecursiveCharacterTextSplitter 클래스를 가져옵니다.\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n```\n:::\n\n\n`RecursiveCharacterTextSplitter` 클래스는 텍스트를 재귀적으로 분할하는 기능을 제공합니다. 이 클래스는 `chunk_size`로 분할할 청크의 크기, `chunk_overlap`으로 인접 청크 간의 겹침 크기, `length_function`으로 청크의 길이를 계산하는 함수, 그리고 `is_separator_regex`로 구분자가 정규 표현식인지 여부를 지정하는 매개변수를 받습니다. 예시에서는 청크 크기를 100, 겹침 크기를 20으로 설정하고, 길이 계산 함수로 `len`을 사용하며, 구분자가 정규 표현식이 아님을 나타내기 위해 `is_separator_regex`를 `False`로 설정합니다.\n\n::: {#2cc3da6a .cell execution_count=23}\n``` {.python .cell-code}\nrecursive_text_splitter = RecursiveCharacterTextSplitter(\n    # 정말 작은 청크 크기를 설정합니다.\n    chunk_size=100,\n    chunk_overlap=10,\n    length_function=len,\n    is_separator_regex=False,\n)\n```\n:::\n\n\n::: {#6fbb9b74 .cell execution_count=24}\n``` {.python .cell-code}\n# chain of density 논문의 일부 내용을 불러옵니다\nwith open(\"data/chain-of-density.txt\", \"r\") as f:\n    text = f.read()[:500]\n```\n:::\n\n\n::: {#17e58477 .cell execution_count=25}\n``` {.python .cell-code}\ncharacter_text_splitter = CharacterTextSplitter(\n    chunk_size=100, chunk_overlap=10, separator=\" \"\n)\nfor sent in character_text_splitter.split_text(text):\n    print(sent)\nprint(\"===\" * 20)\nrecursive_text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=100, chunk_overlap=10\n)\nfor sent in recursive_text_splitter.split_text(text):\n    print(sent)\n```\n:::\n\n\n- 지정한 separators 리스트를 순차적으로 시도하며 주어진 문서를 분할합니다.\n- 청크가 충분히 작아질 때까지 순서대로 분할을 시도합니다. 기본 목록은 [\"\\n\\n\", \"\\n\", \" \", \"\"]입니다.\n- 이는 일반적으로 의미적으로 가장 연관성이 강한 텍스트 조각인 것처럼 보이는 모든 단락(그리고 문장, 단어)을 가능한 한 길게 유지하려는 효과가 있습니다.\n\n::: {#e8a98417 .cell execution_count=26}\n``` {.python .cell-code}\n# recursive_text_splitter 에 기본 지정된 separators 를 확인합니다.\nrecursive_text_splitter._separators\n```\n:::\n\n\n### Semantic Similarity\n\n의미적 유사성을 기준으로 텍스트를 분할합니다.\n\n출처: [Greg Kamradt’s Notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb)\n\n높은 수준(high level)에서 문장으로 분할한 다음 3개 문장으로 그룹화한 다음 임베딩 공간에서 유사한 문장을 병합하는 방식입니다.\n\n::: {#6502fb43 .cell execution_count=27}\n``` {.python .cell-code}\n# 최신 버전으로 업데이트합니다.\n# !pip install -U langchain langchain_experimental -q\n```\n:::\n\n\n::: {#d9d1afb0 .cell execution_count=28}\n``` {.python .cell-code}\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\n# SemanticChunker 를 생성합니다.\nsemantic_text_splitter = SemanticChunker(OpenAIEmbeddings(), add_start_index=True)\n```\n:::\n\n\n::: {#fb50ab7a .cell execution_count=29}\n``` {.python .cell-code}\n# chain of density 논문의 일부 내용을 불러옵니다\nwith open(\"data/chain-of-density.txt\", \"r\") as f:\n    text = f.read()\n\nfor sent in semantic_text_splitter.split_text(text):\n    print(sent)\n    print(\"===\" * 20)\n```\n:::\n\n\n## 3 단계: 임베딩\n\n참고: https://python.langchain.com/docs/integrations/text_embedding\n\n### 유료 과금 임베딩(OpenAI)\n\n::: {#5459e24e .cell execution_count=30}\n``` {.python .cell-code}\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\n# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n# 벡터스토어를 생성합니다.\nvectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n```\n:::\n\n\n다음은 `OpenAI` 의 지원되는 Embedding 모델들의 목록입니다.\n\n- 기본 값은 `text-embeding-ada-002` 입니다.\n\n| MODEL                  | ROUGH PAGES PER DOLLAR | EXAMPLE PERFORMANCE ON MTEB EVAL |\n| ---------------------- | ---------------------- | -------------------------------- |\n| text-embedding-3-small | 62,500                 | 62.3%                            |\n| text-embedding-3-large | 9,615                  | 64.6%                            |\n| text-embedding-ada-002 | 12,500                 | 61.0%                            |\n\n::: {#8d2fddc7 .cell execution_count=31}\n``` {.python .cell-code}\nvectorstore = FAISS.from_documents(\n    documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n)\n```\n:::\n\n\n### 무료 Open Source 기반 임베딩\n\n::: {#495c5efd .cell execution_count=32}\n``` {.python .cell-code}\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\n\n# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n# 벡터스토어를 생성합니다.\nvectorstore = FAISS.from_documents(\n    documents=splits, embedding=HuggingFaceBgeEmbeddings()\n)\n```\n:::\n\n\n::: {#bbbfa0fc .cell execution_count=33}\n``` {.python .cell-code}\n# !pip install fastembed -q\n```\n:::\n\n\n::: {#4deb0787 .cell execution_count=34}\n``` {.python .cell-code}\nfrom langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n\nvectorstore = FAISS.from_documents(documents=splits, embedding=FastEmbedEmbeddings())\n```\n:::\n\n\n## 4단계: 벡터스토어 생성(Create Vectorstore)\n\n::: {#3679c5a4 .cell execution_count=35}\n``` {.python .cell-code}\nfrom langchain_community.vectorstores import FAISS\n\n# FAISS DB 적용\nvectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n```\n:::\n\n\n::: {#1a4f56bb .cell execution_count=36}\n``` {.python .cell-code}\nfrom langchain_community.vectorstores import Chroma\n\n# Chroma DB 적용\nvectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n```\n:::\n\n\n## 5단계: Retriever 생성\n\n리트리버는 구조화되지 않은 쿼리가 주어지면 문서를 반환하는 인터페이스입니다.\n\n리트리버는 문서를 저장할 필요 없이 문서를 반환(또는 검색)하기만 합니다.\n\n- [공식 도큐먼트](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n\n생성된 VectorStore 에 `as_retriver()` 로 가져와서 **Retriever** 를 생성합니다.\n\n### 유사도 기반 검색\n\n- 기본값은 코사인 유사도인 `similarity` 가 적용되어 있습니다.\n\n::: {#7d67238a .cell execution_count=37}\n``` {.python .cell-code}\nquery = \"회사의 저출생 정책이 뭐야?\"\n\nretriever = vectorstore.as_retriever(search_type=\"similarity\")\nsearch_result = retriever.get_relevant_documents(query)\nprint(search_result)\n```\n:::\n\n\n`similarity_score_threshold` 는 유사도 기반 검색에서 `score_threshold` 이상인 결과만 반환합니다.\n\n::: {#74f6c59a .cell execution_count=38}\n``` {.python .cell-code}\nquery = \"회사의 저출생 정책이 뭐야?\"\n\nretriever = vectorstore.as_retriever(\n    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.8}\n)\nsearch_result = retriever.get_relevant_documents(query)\nprint(search_result)\n```\n:::\n\n\n`maximum marginal search result` 를 사용하여 검색합니다.\n\n::: {#f1c026a1 .cell execution_count=39}\n``` {.python .cell-code}\nquery = \"회사의 저출생 정책이 뭐야?\"\n\nretriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})\nsearch_result = retriever.get_relevant_documents(query)\nprint(search_result)\n```\n:::\n\n\n### 다양한 쿼리 생성\n\n::: {#4d91d2f7 .cell execution_count=40}\n``` {.python .cell-code}\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\nfrom langchain_openai import ChatOpenAI\n\nquery = \"회사의 저출생 정책이 뭐야?\"\n\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n\nretriever_from_llm = MultiQueryRetriever.from_llm(\n    retriever=vectorstore.as_retriever(), llm=llm\n)\n```\n:::\n\n\n::: {#cfb96e5a .cell execution_count=41}\n``` {.python .cell-code}\n# Set logging for the queries\nimport logging\n\nlogging.basicConfig()\nlogging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n```\n:::\n\n\n::: {#c102535c .cell execution_count=42}\n``` {.python .cell-code}\nunique_docs = retriever_from_llm.get_relevant_documents(query=question)\nlen(unique_docs)\n```\n:::\n\n\n### Ensemble Retriever\n\n::: {#7e9a6dac .cell execution_count=43}\n``` {.python .cell-code}\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings\n```\n:::\n\n\n::: {#e2a9ca21 .cell execution_count=44}\n``` {.python .cell-code}\ndoc_list = [\n    \"난 오늘 많이 먹어서 배가 정말 부르다\",\n    \"떠나는 저 배가 오늘 마지막 배인가요?\",\n    \"내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\",\n]\n\n# initialize the bm25 retriever and faiss retriever\nbm25_retriever = BM25Retriever.from_texts(doc_list)\nbm25_retriever.k = 2\n\nfaiss_vectorstore = FAISS.from_texts(doc_list, OpenAIEmbeddings())\nfaiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n\n# initialize the ensemble retriever\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n)\n```\n:::\n\n\n::: {#48c2d715 .cell execution_count=45}\n``` {.python .cell-code}\ndef pretty_print(docs):\n    for i, doc in enumerate(docs):\n        print(f\"[{i+1}] {doc.page_content}\")\n```\n:::\n\n\n::: {#58876529 .cell execution_count=46}\n``` {.python .cell-code}\nsample_query = \"나 요즘 배에 정말 살이 많이 쪘어...\"\nprint(f\"[Query]\\n{sample_query}\\n\")\nrelevant_docs = bm25_retriever.get_relevant_documents(sample_query)\nprint(\"[BM25 Retriever]\")\npretty_print(relevant_docs)\nprint(\"===\" * 20)\nrelevant_docs = faiss_retriever.get_relevant_documents(sample_query)\nprint(\"[FAISS Retriever]\")\npretty_print(relevant_docs)\nprint(\"===\" * 20)\nrelevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\nprint(\"[Ensemble Retriever]\")\npretty_print(relevant_docs)\n```\n:::\n\n\n::: {#6130a6d3 .cell execution_count=47}\n``` {.python .cell-code}\nsample_query = \"바다 위에 떠다니는 배들이 많다\"\nprint(f\"[Query]\\n{sample_query}\\n\")\nrelevant_docs = bm25_retriever.get_relevant_documents(sample_query)\nprint(\"[BM25 Retriever]\")\npretty_print(relevant_docs)\nprint(\"===\" * 20)\nrelevant_docs = faiss_retriever.get_relevant_documents(sample_query)\nprint(\"[FAISS Retriever]\")\npretty_print(relevant_docs)\nprint(\"===\" * 20)\nrelevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\nprint(\"[Ensemble Retriever]\")\npretty_print(relevant_docs)\n```\n:::\n\n\n::: {#a23f1463 .cell execution_count=48}\n``` {.python .cell-code}\nsample_query = \"ships\"\nprint(f\"[Query]\\n{sample_query}\\n\")\nrelevant_docs = bm25_retriever.get_relevant_documents(sample_query)\nprint(\"[BM25 Retriever]\")\npretty_print(relevant_docs)\nprint(\"===\" * 20)\nrelevant_docs = faiss_retriever.get_relevant_documents(sample_query)\nprint(\"[FAISS Retriever]\")\npretty_print(relevant_docs)\nprint(\"===\" * 20)\nrelevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\nprint(\"[Ensemble Retriever]\")\npretty_print(relevant_docs)\n```\n:::\n\n\n::: {#8d03527d .cell execution_count=49}\n``` {.python .cell-code}\nsample_query = \"pear\"\nprint(f\"[Query]\\n{sample_query}\\n\")\nrelevant_docs = bm25_retriever.get_relevant_documents(sample_query)\nprint(\"[BM25 Retriever]\")\npretty_print(relevant_docs)\nprint(\"===\" * 20)\nrelevant_docs = faiss_retriever.get_relevant_documents(sample_query)\nprint(\"[FAISS Retriever]\")\npretty_print(relevant_docs)\nprint(\"===\" * 20)\nrelevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\nprint(\"[Ensemble Retriever]\")\npretty_print(relevant_docs)\n```\n:::\n\n\n## 6단계: 프롬프트 생성(Create Prompt)\n\n프롬프트 엔지니어링은 주어진 데이터(`context`)를 토대로 우리가 원하는 결과를 도출할 때 중요한 역할을 합니다.\n\n[TIP1]\n\n1. 만약, `retriever` 에서 도출한 결과에서 중요한 정보가 누락된다면 `retriever` 의 로직을 수정해야 합니다.\n2. 만약, `retriever` 에서 도출한 결과가 많은 정보를 포함하고 있지만, `llm` 이 그 중에서 중요한 정보를 찾지 못한거나 원하는 형태로 출력하지 않는다면 프롬프트를 수정해야 합니다.\n\n[TIP2]\n\n1. LangSmith 의 **hub** 에는 검증된 프롬프트가 많이 업로드 되어 있습니다.\n2. 검증된 프롬프트를 활용하거나 약간 수정한다면 비용과 시간을 절약할 수 있습니다.\n\n- https://smith.langchain.com/hub/search?q=rag\n\n::: {#f62a13f4 .cell execution_count=50}\n``` {.python .cell-code}\nfrom langchain import hub\n```\n:::\n\n\n::: {#af6c5d04 .cell execution_count=51}\n``` {.python .cell-code}\nprompt = hub.pull(\"rlm/rag-prompt\")\nprompt\n```\n:::\n\n\n## 7단계: 언어모델 생성(Create LLM)\n\nOpenAI 모델 중 하나를 선택합니다.\n\n- `gpt-3.5-turbo` : OpenAI의 GPT-3.5-turbo 모델\n- `gpt-4-turbo-preview` : OpenAI의 GPT-4-turbo-preview 모델\n\n자세한 비용 체계는 [OpenAI API 모델 리스트 / 요금표](https://teddylee777.github.io/openai/openai-models/)에서 확인할 수 있습니다.\n\n::: {#caa12239 .cell execution_count=52}\n``` {.python .cell-code}\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n```\n:::\n\n\n다음의 방식으로 토큰 사용량을 확인할 수 있습니다.\n\n::: {#2a5ff2f7 .cell execution_count=53}\n``` {.python .cell-code}\nfrom langchain.callbacks import get_openai_callback\n\nwith get_openai_callback() as cb:\n    result = model.invoke(\"대한민국의 수도는 어디인가요?\")\nprint(cb)\n```\n:::\n\n\nHuggingFace 에 업로드 되어 있는 오픈소스 모델을 손쉽게 다운로드 받아 사용할 수 있습니다.\n\n아래의 리더보드에서 날마다 성능을 개선하는 오픈소스 리더보드를 확인할 수 있습니다.\n\n- [HuggingFace LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n\n::: {#62dd1ebb .cell execution_count=54}\n``` {.python .cell-code}\n# HuggingFaceHub 객체 생성\nfrom langchain.llms import HuggingFaceHub\n\nrepo_id = \"google/flan-t5-xxl\"\n\nt5_model = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 512}\n)\n```\n:::\n\n\n::: {#ccb0836e .cell execution_count=55}\n``` {.python .cell-code}\nt5_model.invoke(\"Where is the capital of South Korea?\")\n```\n:::\n\n\n## RAG 템플릿 실험\n\n::: {#018d1722 .cell execution_count=56}\n``` {.python .cell-code}\n# 단계 1: 문서 로드(Load Documents)\n# 문서를 로드하고, 청크로 나누고, 인덱싱합니다.\nfrom langchain.document_loaders import PyPDFLoader\n\n# PDF 파일 로드. 파일의 경로 입력\nfile_path = \"data/SPRI_AI_Brief_2023년12월호_F.pdf\"\nloader = PyPDFLoader(file_path=file_path)\n\n# 단계 2: 문서 분할(Split Documents)\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n\nsplit_docs = loader.load_and_split(text_splitter=text_splitter)\n\n# 단계 3, 4: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n# 벡터스토어를 생성합니다.\nvectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n\n# 단계 5: 리트리버 생성(Create Retriever)\n# 사용자의 질문(query) 에 부합하는 문서를 검색합니다.\n\n# 유사도 높은 K 개의 문서를 검색합니다.\nk = 3\n\n# (Sparse) bm25 retriever and (Dense) faiss retriever 를 초기화 합니다.\nbm25_retriever = BM25Retriever.from_documents(split_docs)\nbm25_retriever.k = k\n\nfaiss_vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings())\nfaiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": k})\n\n# initialize the ensemble retriever\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n)\n\n# 단계 6: 프롬프트 생성(Create Prompt)\n# 프롬프트를 생성합니다.\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n# 단계 7: 언어모델 생성(Create LLM)\n# 모델(LLM) 을 생성합니다.\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n\ndef format_docs(docs):\n    # 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\n# 단계 8: 체인 생성(Create Chain)\nrag_chain = (\n    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\n# 결과 출력\nprint(f\"PDF Path: {file_path}\")\nprint(f\"문서의 수: {len(docs)}\")\nprint(\"===\" * 20)\nprint(f\"[HUMAN]\\n{question}\\n\")\nprint(f\"[AI]\\n{response}\")\n```\n:::\n\n\n> 문서: data/SPRI_AI_Brief_2023년12월호\\_F.pdf (페이지 10)\n\n- LangSmith: https://smith.langchain.com/public/4449e744-f0a0-42d2-a3df-855bd7f41652/r\n\n::: {#4fde8bea .cell execution_count=57}\n``` {.python .cell-code}\n# 단계 8: 체인 실행(Run Chain)\n# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\nquestion = \"삼성 가우스에 대해 설명해주세요\"\nresponse = rag_chain.invoke(question)\nprint(response)\n```\n:::\n\n\n> 문서: data/SPRI_AI_Brief_2023년12월호\\_F.pdf (페이지 12)\n\n- LangSmith: https://smith.langchain.com/public/2b2913c9-6b9c-4a19-bb16-dc2256e2fdbf/r\n\n::: {#743c0da0 .cell execution_count=58}\n``` {.python .cell-code}\n# 단계 8: 체인 실행(Run Chain)\n# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\nquestion = \"미래의 AI 소프트웨어 매출 전망은 어떻게 되나요?\"\nresponse = rag_chain.invoke(question)\nprint(response)\n```\n:::\n\n\n> 문서: data/SPRI_AI_Brief_2023년12월호\\_F.pdf (페이지 14)\n\n- LangSmith: https://smith.langchain.com/public/17ef6df2-b012-4f8e-b0a8-62894d82c097/r\n\n::: {#400a1c00 .cell execution_count=59}\n``` {.python .cell-code}\n# 단계 8: 체인 실행(Run Chain)\n# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\nquestion = \"YouTube 가 2024년에 의무화 한 것은 무엇인가요?\"\nresponse = rag_chain.invoke(question)\nprint(response)\n```\n:::\n\n\n",
    "supporting": [
      "02-RAG-Advanced_files"
    ],
    "filters": [],
    "includes": {}
  }
}