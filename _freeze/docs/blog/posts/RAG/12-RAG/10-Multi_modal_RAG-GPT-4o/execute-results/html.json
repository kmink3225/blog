{
  "hash": "90365017c31597baa8854542f5e8028b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"멀티모달 RAG\"\nsubtitle: RAG 시스템\ndescription: |\n  검색 증강 생성(RAG) 시스템의 구축과 고급 기법을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 12/31/2024\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n많은 문서들은 텍스트와 이미지를 포함한 다양한 콘텐츠 유형의 혼합을 담고 있습니다.\n\n그러나 대부분의 RAG 애플리케이션에서 이미지에 담긴 정보는 손실됩니다.\n\nGPT-4V, GPT4o 와 같은 다중 모달 LLM의 등장으로, RAG에서 이미지를 활용하는 방법을 고려하는 것이 가치가 있습니다:\n\n`옵션 1:`\n\n- 다중 모달 임베딩(예: [CLIP](https://openai.com/research/clip))을 사용하여 이미지와 텍스트를 임베딩합니다.\n- 유사성 검색을 사용하여 둘 다 검색합니다.\n- 다중 모달 LLM에 원본 이미지와 텍스트 조각을 전달하여 답변을 합성합니다.\n\n`옵션 2:`\n\n- 다중 모달 LLM(예: GPT-4V, GPT4o, [LLaVA](https://llava.hliu.cc/), [FUYU-8b](https://www.adept.ai/blog/fuyu-8b))을 사용하여 이미지에서 텍스트 요약을 생성합니다.\n- 텍스트를 임베딩하고 검색합니다.\n- LLM에 텍스트 조각을 전달하여 답변을 합성합니다.\n\n`옵션 3`\n\n- 다중 모달 LLM(예: GPT-4V, GPT4o, [LLaVA](https://llava.hliu.cc/), [FUYU-8b](https://www.adept.ai/blog/fuyu-8b))을 사용하여 이미지에서 텍스트 요약을 생성합니다.\n- 원본 이미지에 대한 참조와 함께 이미지 요약을 임베딩하고 검색합니다.\n- 다중 모달 LLM에 원본 이미지와 텍스트 조각을 전달하여 답변을 합성합니다.\n\n![capture-20240518-164005.png](attachment:capture-20240518-164005.png)\n\n\n## 데이터\n\n- [다운로드 링크](https://drive.google.com/file/d/1QlhGFIFwEkNEjQGOvV_hQe4bnOLDJwCR/view)\n- 출처: [Jamin Ball Blog](https://cloudedjudgement.substack.com/p/clouded-judgement-111023).\n\n## 패키지\n\n`unstructured`를 사용하기 위해서는 시스템에 `poppler`([설치 지침](https://pdf2image.readthedocs.io/en/latest/installation.html))와 `tesseract`([설치 지침](https://tesseract-ocr.github.io/tessdoc/Installation.html))가 필요합니다.\n\n[참고] `옵션 2`는 다중 모달 LLM을 답변 합성에 사용할 수 없는 경우(예: 비용 등)에 적합합니다.\n\n::: {#c39ac6ef .cell execution_count=1}\n``` {.python .cell-code}\n# ! pip install -U langchain openai chromadb langchain-experimental # 최신 버전이 필요합니다 (멀티 모달을 위해)\n```\n:::\n\n\n::: {#0fd5fabc .cell execution_count=2}\n``` {.python .cell-code}\n# ! pip install \"unstructured[all-docs]\" pillow pydantic lxml pillow matplotlib chromadb tiktoken\n```\n:::\n\n\n## 데이터 로딩\n\n### PDF 텍스트와 이미지 분할\n\n[Unstructured](https://unstructured-io.github.io/unstructured/introduction.html#key-concepts)에서 제공하는 `partition_pdf`를 사용하여 텍스트와 이미지를 추출할 수 있습니다.\n\n이미지를 추출하기 위해 다음을 사용합니다\n\n`extract_images_in_pdf=True`\n\n텍스트만 처리하려는 경우.\n\n`extract_images_in_pdf=False`\n\n::: {#023dcf28 .cell execution_count=3}\n``` {.python .cell-code}\n# 파일 경로\nfpath = \"multi-modal/\"\nfname = \"sample.pdf\"\n```\n:::\n\n\n::: {#f5d81ada .cell execution_count=4}\n``` {.python .cell-code}\nimport os\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom unstructured.partition.pdf import partition_pdf\n\n# PDF에서 요소 추출\n\n\ndef extract_pdf_elements(path, fname):\n    \"\"\"\n    PDF 파일에서 이미지, 테이블, 그리고 텍스트 조각을 추출합니다.\n    path: 이미지(.jpg)를 저장할 파일 경로\n    fname: 파일 이름\n    \"\"\"\n    return partition_pdf(\n        filename=os.path.join(path, fname),\n        extract_images_in_pdf=True,  # PDF 내 이미지 추출 활성화\n        infer_table_structure=True,  # 테이블 구조 추론 활성화\n        chunking_strategy=\"by_title\",  # 제목별로 텍스트 조각화\n        max_characters=4000,  # 최대 문자 수\n        new_after_n_chars=3800,  # 이 문자 수 이후에 새로운 조각 생성\n        combine_text_under_n_chars=2000,  # 이 문자 수 이하의 텍스트는 결합\n        image_output_dir_path=path,  # 이미지 출력 디렉토리 경로\n    )\n\n\n# 요소를 유형별로 분류\n\n\ndef categorize_elements(raw_pdf_elements):\n    \"\"\"\n    PDF에서 추출된 요소를 테이블과 텍스트로 분류합니다.\n    raw_pdf_elements: unstructured.documents.elements의 리스트\n    \"\"\"\n    tables = []  # 테이블 저장 리스트\n    texts = []  # 텍스트 저장 리스트\n    for element in raw_pdf_elements:\n        if \"unstructured.documents.elements.Table\" in str(type(element)):\n            tables.append(str(element))  # 테이블 요소 추가\n        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n            texts.append(str(element))  # 텍스트 요소 추가\n    return texts, tables\n\n\n# 요소 추출\nraw_pdf_elements = extract_pdf_elements(fpath, fname)\n\n# 텍스트, 테이블 추출\ntexts, tables = categorize_elements(raw_pdf_elements)\n\n# 선택사항: 텍스트에 대해 특정 토큰 크기 적용\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=4000, chunk_overlap=0  # 텍스트를 4000 토큰 크기로 분할, 중복 없음\n)\njoined_texts = \" \".join(texts)  # 텍스트 결합\ntexts_4k_token = text_splitter.split_text(joined_texts)  # 분할 실행\n```\n:::\n\n\n::: {#1f3a232e .cell execution_count=5}\n``` {.python .cell-code}\nlen(texts_4k_token)\n```\n:::\n\n\n## 멀티-벡터 검색기\n\n[multi-vector-retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary)를 사용하여 이미지(그리고/또는 텍스트, 테이블) 요약을 색인화하지만, 원본 이미지(원본 텍스트나 테이블과 함께)를 검색합니다.\n\n### 텍스트 및 테이블 요약\n\n테이블과 선택적으로 텍스트 요약을 생성하기 위해 `GPT-4-turbo`를 사용할 것입니다.\n\n큰 청크 크기를 사용하는 경우(예를 들어, 위에서 설정한 것처럼 4k 토큰 청크를 사용) 텍스트 요약이 권장됩니다.\n\n요약은 원본 테이블 및/또는 원본 텍스트 청크를 검색하는 데 사용됩니다.\n\n::: {#28a9a5e6 .cell execution_count=6}\n``` {.python .cell-code}\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# 텍스트 요소의 요약 생성\n\n\ndef generate_text_summaries(texts, tables, summarize_texts=False):\n    \"\"\"\n    텍스트 요소 요약\n    texts: 문자열 리스트\n    tables: 문자열 리스트\n    summarize_texts: 텍스트 요약 여부를 결정. True/False\n    \"\"\"\n\n    # 프롬프트 설정\n    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n    prompt = ChatPromptTemplate.from_template(prompt_text)\n\n    # 텍스트 요약 체인\n    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n\n    # 요약을 위한 빈 리스트 초기화\n    text_summaries = []\n    table_summaries = []\n\n    # 제공된 텍스트에 대해 요약이 요청되었을 경우 적용\n    if texts and summarize_texts:\n        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n    elif texts:\n        text_summaries = texts\n\n    # 제공된 테이블에 적용\n    if tables:\n        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n\n    return text_summaries, table_summaries\n\n\n# 텍스트, 테이블 요약 가져오기\ntext_summaries, table_summaries = generate_text_summaries(\n    texts_4k_token, tables, summarize_texts=True\n)\n```\n:::\n\n\n### 이미지 요약\n\n`GPT-4o`를 사용하여 이미지 요약을 생성할 것입니다.\n\n- base64로 인코딩된 이미지를 전달합니다.\n\n::: {#5b9f3f8b .cell execution_count=7}\n``` {.python .cell-code}\nimport base64\nimport os\n\nfrom langchain_core.messages import HumanMessage\n\n\ndef encode_image(image_path):\n    # 이미지 파일을 base64 문자열로 인코딩합니다.\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\ndef image_summarize(img_base64, prompt):\n    # 이미지 요약을 생성합니다.\n    chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=2048)\n\n    msg = chat.invoke(\n        [\n            HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": prompt},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n                    },\n                ]\n            )\n        ]\n    )\n    return msg.content\n\n\ndef generate_img_summaries(path):\n    \"\"\"\n    이미지에 대한 요약과 base64 인코딩된 문자열을 생성합니다.\n    path: Unstructured에 의해 추출된 .jpg 파일 목록의 경로\n    \"\"\"\n\n    # base64로 인코딩된 이미지를 저장할 리스트\n    img_base64_list = []\n\n    # 이미지 요약을 저장할 리스트\n    image_summaries = []\n\n    # 요약을 위한 프롬프트\n    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n    These summaries will be embedded and used to retrieve the raw image. \\\n    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n\n    # 이미지에 적용\n    for img_file in sorted(os.listdir(path)):\n        if img_file.endswith(\".jpg\"):\n            img_path = os.path.join(path, img_file)\n            base64_image = encode_image(img_path)\n            img_base64_list.append(base64_image)\n            image_summaries.append(image_summarize(base64_image, prompt))\n\n    return img_base64_list, image_summaries\n\n\n# 이미지 요약 실행\nimg_base64_list, image_summaries = generate_img_summaries(fpath)\n```\n:::\n\n\n::: {#1c916b34 .cell execution_count=8}\n``` {.python .cell-code}\nlen(image_summaries)\n```\n:::\n\n\n### 벡터 저장소에 추가하기\n\n원본 문서와 문서 요약을 [Multi Vector Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary)에 추가하는 방법:\n\n- 원본 텍스트, 테이블, 이미지를 `docstore`에 저장합니다.\n- 효율적인 의미론적 검색을 위해 텍스트, 테이블 요약, 이미지 요약을 `vectorstore`에 저장합니다.\n\n다양한 유형의 데이터(텍스트, 테이블, 이미지)를 색인화하고 검색할 수 있는 멀티 벡터 검색기를 생성하는 과정을 설명합니다.\n\n- `InMemoryStore`를 사용하여 저장 계층을 초기화합니다.\n- `MultiVectorRetriever`를 생성하여, 요약된 데이터를 색인화하지만 원본 텍스트나 이미지를 반환하도록 설정합니다.\n- 각 데이터 유형(텍스트, 테이블, 이미지)에 대해 요약과 원본 데이터를 `vectorstore`와 `docstore`에 추가하는 과정을 포함합니다.\n  - 각 문서에 대해 고유한 `doc_id`를 생성합니다.\n  - 요약된 데이터를 `vectorstore`에 추가하고, 원본 데이터와 `doc_id`를 `docstore`에 저장합니다.\n- 데이터 유형별로 요약이 비어 있지 않은 경우에만 해당 데이터를 추가하는 조건을 확인합니다.\n- `Chroma` 벡터 저장소를 사용하여 요약을 색인화하고, `OpenAIEmbeddings` 함수를 사용하여 임베딩을 생성합니다.\n- 생성된 멀티 벡터 검색기는 다양한 유형의 데이터에 대한 요약을 색인화하고, 검색 시 원본 데이터를 반환할 수 있습니다.\n\n::: {#4de4449c .cell execution_count=9}\n``` {.python .cell-code}\nimport uuid\n\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\n\n\ndef create_multi_vector_retriever(\n    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n):\n    \"\"\"\n    요약을 색인화하지만 원본 이미지나 텍스트를 반환하는 검색기를 생성합니다.\n    \"\"\"\n\n    # 저장 계층 초기화\n    store = InMemoryStore()\n    id_key = \"doc_id\"\n\n    # 멀티 벡터 검색기 생성\n    retriever = MultiVectorRetriever(\n        vectorstore=vectorstore,\n        docstore=store,\n        id_key=id_key,\n    )\n\n    # 문서를 벡터 저장소와 문서 저장소에 추가하는 헬퍼 함수\n    def add_documents(retriever, doc_summaries, doc_contents):\n        doc_ids = [\n            str(uuid.uuid4()) for _ in doc_contents\n        ]  # 문서 내용마다 고유 ID 생성\n        summary_docs = [\n            Document(page_content=s, metadata={id_key: doc_ids[i]})\n            for i, s in enumerate(doc_summaries)\n        ]\n        retriever.vectorstore.add_documents(\n            summary_docs\n        )  # 요약 문서를 벡터 저장소에 추가\n        retriever.docstore.mset(\n            list(zip(doc_ids, doc_contents))\n        )  # 문서 내용을 문서 저장소에 추가\n\n    # 텍스트, 테이블, 이미지 추가\n    if text_summaries:\n        add_documents(retriever, text_summaries, texts)\n\n    if table_summaries:\n        add_documents(retriever, table_summaries, tables)\n\n    if image_summaries:\n        add_documents(retriever, image_summaries, images)\n\n    return retriever\n```\n:::\n\n\n::: {#fde088a0 .cell execution_count=10}\n``` {.python .cell-code}\n# 요약을 색인화하기 위해 사용할 벡터 저장소\nvectorstore = Chroma(\n    collection_name=\"sample-rag-multi-modal\", embedding_function=OpenAIEmbeddings()\n)\n\n# 검색기 생성\nretriever_multi_vector_img = create_multi_vector_retriever(\n    vectorstore,\n    text_summaries,\n    texts,\n    table_summaries,\n    tables,\n    image_summaries,\n    img_base64_list,\n)\n```\n:::\n\n\n## RAG\n\n### 검색기 구축\n\n검색된 문서를 GPT-4o 프롬프트 템플릿의 올바른 부분에 할당해야 합니다.\n\n다음은 Base64 인코딩된 이미지와 텍스트를 처리하고, 이를 활용하여 다중 모달 질의응답(질문-응답) 체인을 구성하는 방법을 설명합니다.\n\n- Base64 인코딩된 문자열이 이미지인지 확인합니다. 지원하는 이미지 포맷은 JPG, PNG, GIF, WEBP입니다.\n- Base64 인코딩된 이미지를 주어진 크기로 리사이즈합니다.\n- 문서 집합에서 Base64 인코딩된 이미지와 텍스트를 분리합니다.\n- 분리된 이미지와 텍스트를 사용하여 다중 모달 질의응답 체인의 입력으로 사용될 메시지를 구성합니다. 이 과정에서 이미지 URL과 텍스트 정보를 포함하는 메시지를 생성합니다.\n- 다중 모달 질의응답 체인을 구성합니다. 이 체인은 입력된 이미지와 텍스트 정보를 바탕으로 질문에 대한 응답을 생성하는 과정을 포함합니다. 사용된 모델은 `ChatOpenAI`이며, `gpt-4o` 모델을 사용합니다.\n\n이 과정은 이미지와 텍스트 데이터를 모두 활용하여 질문에 대한 응답을 생성하는 다중 모달 질의응답 시스템을 구현하는 방법을 제시합니다. 여기서는 이미지 데이터를 처리하기 위한 Base64 인코딩 및 디코딩, 이미지 리사이징, 그리고 이를 텍스트 정보와 함께 처리하여 질문에 대한 응답을 생성하는 과정이 포함됩니다.\n\n::: {#874c39bb .cell execution_count=11}\n``` {.python .cell-code}\nimport io\nimport re\n\nfrom IPython.display import HTML, display\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom PIL import Image\n\n\ndef plt_img_base64(img_base64):\n    \"\"\"base64 인코딩된 문자열을 이미지로 표시\"\"\"\n    # base64 문자열을 소스로 사용하는 HTML img 태그 생성\n    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n    # HTML을 렌더링하여 이미지 표시\n    display(HTML(image_html))\n\n\ndef looks_like_base64(sb):\n    \"\"\"문자열이 base64로 보이는지 확인\"\"\"\n    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n\n\ndef is_image_data(b64data):\n    \"\"\"\n    base64 데이터가 이미지인지 시작 부분을 보고 확인\n    \"\"\"\n    image_signatures = {\n        b\"\\xff\\xd8\\xff\": \"jpg\",\n        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n    }\n    try:\n        header = base64.b64decode(b64data)[:8]  # 처음 8바이트를 디코드하여 가져옴\n        for sig, format in image_signatures.items():\n            if header.startswith(sig):\n                return True\n        return False\n    except Exception:\n        return False\n\n\ndef resize_base64_image(base64_string, size=(128, 128)):\n    \"\"\"\n    Base64 문자열로 인코딩된 이미지의 크기 조정\n    \"\"\"\n    # Base64 문자열 디코드\n    img_data = base64.b64decode(base64_string)\n    img = Image.open(io.BytesIO(img_data))\n\n    # 이미지 크기 조정\n    resized_img = img.resize(size, Image.LANCZOS)\n\n    # 조정된 이미지를 바이트 버퍼에 저장\n    buffered = io.BytesIO()\n    resized_img.save(buffered, format=img.format)\n\n    # 조정된 이미지를 Base64로 인코딩\n    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n\n\ndef split_image_text_types(docs):\n    \"\"\"\n    base64로 인코딩된 이미지와 텍스트 분리\n    \"\"\"\n    b64_images = []\n    texts = []\n    for doc in docs:\n        # 문서가 Document 타입인 경우 page_content 추출\n        if isinstance(doc, Document):\n            doc = doc.page_content\n        if looks_like_base64(doc) and is_image_data(doc):\n            doc = resize_base64_image(doc, size=(1300, 600))\n            b64_images.append(doc)\n        else:\n            texts.append(doc)\n    return {\"images\": b64_images, \"texts\": texts}\n\n\ndef img_prompt_func(data_dict):\n    \"\"\"\n    컨텍스트를 단일 문자열로 결합\n    \"\"\"\n    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n    messages = []\n\n    # 이미지가 있으면 메시지에 추가\n    if data_dict[\"context\"][\"images\"]:\n        for image in data_dict[\"context\"][\"images\"]:\n            image_message = {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n            }\n            messages.append(image_message)\n\n    # 분석을 위한 텍스트 추가\n    text_message = {\n        \"type\": \"text\",\n        \"text\": (\n            \"You are financial analyst tasking with providing investment advice.\\n\"\n            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n            \"Use this information to provide investment advice related to the user question. Answer in Korean. Do NOT translate company names.\\n\"\n            f\"User-provided question: {data_dict['question']}\\n\\n\"\n            \"Text and / or tables:\\n\"\n            f\"{formatted_texts}\"\n        ),\n    }\n    messages.append(text_message)\n    return [HumanMessage(content=messages)]\n\n\ndef multi_modal_rag_chain(retriever):\n    \"\"\"\n    멀티모달 RAG 체인\n    \"\"\"\n\n    # 멀티모달 LLM\n    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=2048)\n\n    # RAG 파이프라인\n    chain = (\n        {\n            \"context\": retriever | RunnableLambda(split_image_text_types),\n            \"question\": RunnablePassthrough(),\n        }\n        | RunnableLambda(img_prompt_func)\n        | model\n        | StrOutputParser()\n    )\n\n    return chain\n\n\n# RAG 체인 생성\nchain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)\n```\n:::\n\n\n### 검사\n\n우리가 질문에 관련된 이미지를 검색할 때, 관련성 있는 이미지들을 되돌려 받습니다.\n\n::: {#25d85a1a .cell execution_count=12}\n``` {.python .cell-code}\n# 검색 질의 실행\nquery = \"EV/NTM 및 NTM 매출 성장률을 기준으로 흥미로운 투자처인 기업 이름을 알려주세요. EV/NTM 멀티플과 과거를 고려하시나요?\"\n\n# 질의에 대한 문서 6개를 검색합니다.\ndocs = retriever_multi_vector_img.invoke(query, limit=6)\n\n# 문서의 개수 확인\nlen(docs)  # 검색된 문서의 개수를 반환합니다.\n```\n:::\n\n\n::: {#2ed1b3be .cell execution_count=13}\n``` {.python .cell-code}\n# 검색 결과 확인\nquery = \"Mongo DB, Cloudflare, Datadog 의 EV/NTM 및 NTM rev 성장률은 얼마인가요?\"\ndocs = retriever_multi_vector_img.invoke(query, limit=6)\n\n# 문서의 개수 확인\nlen(docs)\n```\n:::\n\n\n::: {#43ea0bc6 .cell execution_count=14}\n``` {.python .cell-code}\n# 관련 이미지를 반환합니다.\nplt_img_base64(docs[0])\n```\n:::\n\n\n### 검증\n\n이것이 왜 작동하는지, 우리가 저장했던 이미지를 다시 살펴보겠습니다.\n\n::: {#643fe4ea .cell execution_count=15}\n``` {.python .cell-code}\n# img_base64_list 리스트의 20번 index 이미지를 base64 형식으로 표시합니다.\nplt_img_base64(img_base64_list[20])\n```\n:::\n\n\n여기 해당 요약이 있으며, 우리는 이를 유사성 검색에 사용하기 위해 내장했습니다.\n\n이 이미지가 우리의 `query`를 기반으로 그 요약과의 유사성 때문에 검색된 것은 상당히 합리적입니다.\n\n::: {#2679e330 .cell execution_count=16}\n``` {.python .cell-code}\nimage_summaries[20]  # image_summaries 리스트의 네 번째 요소에 접근합니다.\n```\n:::\n\n\n### RAG\n\n이제 RAG를 실행하고 우리의 질문에 대한 답변을 종합하는 능력을 테스트해 보겠습니다.\n\n::: {#8f34b1d9 .cell execution_count=17}\n``` {.python .cell-code}\n# RAG 체인 실행\nprint(chain_multimodal_rag.invoke(query))\n```\n:::\n\n\n### 고려사항\n\n**검색**\n\n- 검색은 이미지 요약과 텍스트 Chunk 와의 유사성을 기반으로 수행됩니다.\n- 이는 이미지 요약 검색 결과보다 다른 Text Chunk 가 경쟁 우위에 있을 경우 실패할 수 있기 때문에 신중한 고려가 필요합니다.\n\n**이미지 크기**\n\n- 답변 합성의 품질은 이미지 크기에 민감한 것으로 보입니다. [가이드에 명시 된 것 처럼..](https://platform.openai.com/docs/guides/vision).\n\n",
    "supporting": [
      "10-Multi_modal_RAG-GPT-4o_files"
    ],
    "filters": [],
    "includes": {}
  }
}