{
  "hash": "31055a17b115c6c2edf0439bf3a74676",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\"\nsubtitle: RAG 시스템\ndescription: |\n  검색 증강 생성(RAG) 시스템의 구축과 고급 기법을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 12/31/2024\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n[RAPTOR](https://arxiv.org/pdf/2401.18059.pdf) 논문은 문서의 색인 생성 및 검색에 대한 흥미로운 접근 방식을 제시합니다.\n\n[테디노트 논문 요약글(노션)](https://teddylee777.notion.site/RAPTOR-e835d306fc664dc2ad76191dee1cd859?pvs=4)\n\n- `leafs` 는 가장 low-level 의 시작 문서 집합입니다. 이 문서들은 임베딩되어 클러스터링됩니다.\n- 그런 다음 클러스터는 유사한 문서들 간의 정보를 더 높은 수준(더 추상적인)으로 요약합니다.\n\n이 과정은 재귀적으로 수행되어, 원본 문서(`leafs`)에서 더 추상적인 요약으로 이어지는 \"트리\"를 형성합니다.\n\n`leafs`는 다음과 같은 문서들로 구성될 수 있습니다.\n\n- 단일 문서에서의 텍스트 청크(논문에서 보여준 것처럼)\n- 전체 문서(아래에서 보여주는 것처럼)\n\n이번 튜토리얼에서는 긴 문서(PDF) 에 대해서 RAPTOR 방법론을 적용해 보도록 하겠습니다.\n\n\n## 실습에 활용한 문서\n\n소프트웨어정책연구소(SPRi) - 2023년 12월호\n\n- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n- 링크: https://spri.kr/posts/view/23669\n- 파일명: `SPRI_AI_Brief_2023년12월호_F.pdf`\n\n_실습을 위해 다운로드 받은 파일을 `data` 폴더로 복사해 주시기 바랍니다_\n\n## 환경 설정\n\n**추가 패키지 설치**\n\n아래 주석을 해제하고 실행하여 추가 패키지를 설치 후 진행해 주세요.\n\n::: {#1c5aa0e3 .cell execution_count=1}\n``` {.python .cell-code}\n# !pip install -U umap-learn\n```\n:::\n\n\n상단의 **restart** 버튼을 눌러 재시작 한 뒤 다시 처음부터 진행해 주세요.\n\n::: {#2f6062c2 .cell execution_count=2}\n``` {.python .cell-code}\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\nimport warnings\n\n# 경고 메시지 무시\nwarnings.filterwarnings(\"ignore\")\n\n# API 키 정보 로드\nload_dotenv()\n```\n:::\n\n\n::: {#7ae7c94d .cell execution_count=3}\n``` {.python .cell-code}\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install -qU langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH12-RAPTOR\")\n```\n:::\n\n\n## 데이터 전처리\n\n`doc`은 PDF 파일입니다. 토큰 수는 100 토큰 미만에서 10,000 토큰 이상까지 다양합니다.\n\n웹 문서에서 텍스트 데이터를 추출하고, 텍스트의 토큰 수를 계산하여 히스토그램으로 시각화합니다.\n\n::: {#2db525fe .cell execution_count=4}\n``` {.python .cell-code}\nfrom langchain_community.document_loaders import PDFPlumberLoader\nimport tiktoken\nimport matplotlib.pyplot as plt\n\n\n# 토큰 수 계산\ndef num_tokens_from_string(string: str, encoding_name: str):\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\n\n# 문서 로드(Load Documents)\nloader = PDFPlumberLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\ndocs = loader.load()\nprint(f\"문서의 페이지수: {len(docs)}\")\n\n# 문서 텍스트\ndocs_texts = [d.page_content for d in docs]\n\n# 각 문서에 대한 토큰 수 계산\ncounts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n\n# 토큰 수의 히스토그램을 그립니다.\nplt.figure(figsize=(10, 6))\nplt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\nplt.title(\"Token Count Distribution\")\nplt.xlabel(\"Token Count\")\nplt.ylabel(\"Frequency\")\nplt.grid(axis=\"y\", alpha=0.75)\n\n# 히스토그램을 표시합니다.\nplt.show()\n```\n:::\n\n\n문서 텍스트를 정렬합니다. 이때 메타데이터의 `source` 를 기준으로 정렬한 뒤, 모든 문서를 연결합니다.\n\n::: {#3e8166c4 .cell execution_count=5}\n``` {.python .cell-code}\n# 문서를 출처 메타데이터 기준으로 정렬합니다.\nd_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\nd_reversed = list(reversed(d_sorted))\n\n# 역순으로 배열된 문서의 내용을 연결합니다.\nconcatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n    [doc.page_content for doc in d_reversed]\n)\n\nprint(\n    \"전체 토큰 수: %s\"  # 모든 문맥에서의 토큰 수를 출력합니다.\n    % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n)\n```\n:::\n\n\n`RecursiveCharacterTextSplitter`를 사용하여 텍스트를 분할합니다.\n\n::: {#1425d209 .cell execution_count=6}\n``` {.python .cell-code}\n# 텍스트 분할을 위한 코드\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# 기준 토큰수\nchunk_size = 100\n\n# 텍스트 분할기 초기화\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=chunk_size, chunk_overlap=0\n)\n\n# 주어진 텍스트를 분할\ntexts_split = text_splitter.split_text(concatenated_content)\n```\n:::\n\n\n다음으로는 분할된 chunk 들을 임베딩하여 vector store 에 저장합니다.\n\n::: {#e9dacf85 .cell execution_count=7}\n``` {.python .cell-code}\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.embeddings import CacheBackedEmbeddings\nfrom langchain.storage import LocalFileStore\n\n# cache 저장 경로 지정\nstore = LocalFileStore(\"./cache/\")\n\n# embeddings 인스턴스를 생성\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", disallowed_special=())\n\n# CacheBackedEmbeddings 인스턴스를 생성\ncached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n    embeddings, store, namespace=embeddings.model\n)\n```\n:::\n\n\n## 모델 설정\n\n::: {#9e97a9f9 .cell execution_count=8}\n``` {.python .cell-code}\nfrom langchain_teddynote.messages import stream_response\nfrom langchain_openai import ChatOpenAI\n\n\n# llm 모델 초기화\nllm = ChatOpenAI(\n    model=\"gpt-4.1-mini\",\n    temperature=0,\n)\n```\n:::\n\n\n## 트리 구축\n\n트리 구축에서의 클러스터링 접근 방식에 대한 주요 개요입니다.\n\n**GMM (가우시안 혼합 모델)**\n\n- 다양한 클러스터에 걸쳐 데이터 포인트의 분포를 모델링합니다.\n- 모델의 베이지안 정보 기준(BIC)을 평가하여 최적의 클러스터 수를 결정합니다.\n\n**UMAP (Uniform Manifold Approximation and Projection)**\n\n- 클러스터링을 지원합니다.\n- 고차원 데이터의 차원을 축소합니다.\n- UMAP은 데이터 포인트의 유사성에 기반하여 자연스러운 그룹화를 강조하는 데 도움을 줍니다.\n\n**지역 및 전역 클러스터링**\n\n- 데이터를 저차원으로 차원 축소하여 클러스터링을 수행합니다.\n\n**임계값 설정**\n\n- GMM의 맥락에서 클러스터 멤버십을 결정하기 위해 적용됩니다.\n- 확률 분포를 기반으로 합니다(데이터 포인트를 ≥ 1 클러스터에 할당).\n\n---\n\nGMM 및 임계값 설정에 대한 코드는 아래 두 출처에서 언급된 Sarthi et al의 것입니다. \n\n**참조**\n\n- [원본 저장소](https://github.com/parthsarthi03/raptor/blob/master/raptor/cluster_tree_builder.py)\n- [소소한 조정](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raptor/llama_index/packs/raptor/clustering.py)\n\n### 차원 축소\n\n`global_cluster_embeddings`\n\n- 입력된 임베딩 벡터를 전역적으로 차원 축소하기 위해 UMAP을 적용합니다. 전역적으로 차원을 축소한 결과물을 얻어 추후 클러스터링에 활용합니다.\n\n**과정**\n\n- n_neighbors: UMAP에 사용될 이웃(neighbor) 수를 정합니다. 데이터 포인트 하나를 이해할 때 주변 데이터 포인트 개수를 나타냅니다. 입력이 없으면 데이터 개수에 따라 자동으로 계산합니다.\n- umap.UMAP(...)를 사용하여, 고차원 임베딩을 dim 차원으로 축소합니다.\n- 축소된 벡터들은 전역적(global)인 구조 파악에 도움이 되는 저차원 표현입니다.\n\n---\n\n`local_cluster_embeddings`\n\n- 선택한 데이터 부분집합에 대해 로컬(국소적) 차원 축소를 수행합니다.\n\n**과정**\n\n- 글로벌 차원 축소와 유사하지만, 로컬 차원 축소는 이미 한 번 전역 클러스터링을 통해 추출한 특정 그룹(글로벌 클러스터) 내 데이터에 대해 다시 UMAP을 적용합니다.\n- 이 과정은 전역적으로 파악된 큰 구조 안에서 더 세밀한 클러스터 구조를 파악하는 데 도움이 됩니다.\n\n::: {#4b515e8e .cell execution_count=9}\n``` {.python .cell-code}\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport umap\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom sklearn.mixture import GaussianMixture\n\nRANDOM_SEED = 42  # 재현성을 위한 고정된 시드 값\n\n\ndef global_cluster_embeddings(\n    embeddings: np.ndarray,\n    dim: int,\n    n_neighbors: Optional[int] = None,\n    metric: str = \"cosine\",\n) -> np.ndarray:\n    \"\"\"전역적으로 임베딩 벡터의 차원을 축소하는 함수입니다.\n\n    Args:\n        embeddings (np.ndarray): 차원을 축소할 임베딩 벡터들\n        dim (int): 축소할 차원의 수\n        n_neighbors (Optional[int], optional): UMAP에서 사용할 이웃의 수. 기본값은 None으로, 이 경우 데이터 크기에 따라 자동 계산됨\n        metric (str, optional): 거리 계산에 사용할 메트릭. 기본값은 \"cosine\"\n\n    Returns:\n        np.ndarray: 차원이 축소된 임베딩 벡터들\n    \"\"\"\n    # 이웃 수 계산\n    if n_neighbors is None:\n        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n\n    # UMAP 적용\n    return umap.UMAP(\n        n_neighbors=n_neighbors, n_components=dim, metric=metric\n    ).fit_transform(embeddings)\n\n\ndef local_cluster_embeddings(\n    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n) -> np.ndarray:\n    \"\"\"로컬(국소적)하게 임베딩 벡터의 차원을 축소하는 함수입니다.\n\n    Args:\n        embeddings (np.ndarray): 차원을 축소할 임베딩 벡터들\n        dim (int): 축소할 차원의 수\n        num_neighbors (int, optional): UMAP에서 사용할 이웃의 수. 기본값은 10\n        metric (str, optional): 거리 계산에 사용할 메트릭. 기본값은 \"cosine\"\n\n    Returns:\n        np.ndarray: 차원이 축소된 임베딩 벡터들\n    \"\"\"\n    # UMAP 적용\n    return umap.UMAP(\n        n_neighbors=num_neighbors, n_components=dim, metric=metric\n    ).fit_transform(embeddings)\n```\n:::\n\n\n### 최적의 클러스터 수 계산\n\n`get_optimal_clusters` \n\n- 주어진 임베딩 데이터에 대해 가장 적절한 클러스터 수를 BIC 점수를 기반으로 결정합니다.\n- GMM과 BIC를 활용해 클러스터 개수를 자동으로 결정하므로, 사전에 클러스터 수를 지정할 필요가 없습니다.\n\n**과정**\n\n- 가능한 클러스터 수(1 ~ max_clusters 사이)를 순회하며 각 클러스터 개수로 GMM을 학습합니다.\n- 각 GMM에 대해 BIC 점수를 계산한 뒤 리스트에 저장합니다.\n- BIC 점수가 가장 낮은(가장 좋은 성능을 보이는) 클러스터 개수를 선택하여 반환합니다.\n\n::: {#020e31b6 .cell execution_count=10}\n``` {.python .cell-code}\ndef get_optimal_clusters(\n    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n) -> int:\n    \"\"\"BIC 점수를 기반으로 최적의 클러스터 수를 찾는 함수입니다.\n\n    Args:\n        embeddings (np.ndarray): 클러스터링할 임베딩 벡터들\n        max_clusters (int, optional): 탐색할 최대 클러스터 수. 기본값은 50\n        random_state (int, optional): 난수 생성을 위한 시드값. 기본값은 RANDOM_SEED\n\n    Returns:\n        int: BIC 점수가 가장 낮은(최적의) 클러스터 수\n    \"\"\"\n    # 최대 클러스터 수와 임베딩의 길이 중 작은 값을 최대 클러스터 수로 설정\n    max_clusters = min(max_clusters, len(embeddings))\n    # 1부터 최대 클러스터 수까지의 범위를 생성\n    n_clusters = np.arange(1, max_clusters)\n\n    # BIC 점수를 저장할 리스트\n    bics = []\n    for n in n_clusters:\n        gm = GaussianMixture(n_components=n, random_state=random_state)\n        gm.fit(embeddings)\n        # 학습된 모델의 BIC 점수를 리스트에 추가\n        bics.append(gm.bic(embeddings))\n\n    # BIC 점수가 가장 낮은 클러스터 수를 반환\n    return n_clusters[np.argmin(bics)]\n```\n:::\n\n\n### 클러스터링 수행\n\n`GMM_cluster` \n\n- GMM을 이용해 주어진 임베딩에 대해 클러스터를 할당합니다.\n\n**과정**\n\n- `get_optimal_clusters` 를 통해 최적의 클러스터 수를 찾습니다.\n- `GaussianMixture` 모델을 해당 클러스터 수로 학습합니다.\n- 각 데이터 포인트가 각 클러스터에 속할 확률(predict_proba)을 구합니다.\n- 주어진 threshold를 바탕으로, 확률이 임계값을 초과하는 클러스터만 레이블로 할당합니다.\n\n::: {#7b4c8d20 .cell execution_count=11}\n``` {.python .cell-code}\ndef GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n    # 최적의 클러스터 수 산정\n    n_clusters = get_optimal_clusters(embeddings)\n\n    # 가우시안 혼합 모델을 초기화\n    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n    gm.fit(embeddings)\n\n    # 임베딩이 각 클러스터에 속할 확률을 예측\n    probs = gm.predict_proba(embeddings)\n\n    # 임계값을 초과하는 확률을 가진 클러스터를 레이블로 선택\n    labels = [np.where(prob > threshold)[0] for prob in probs]\n\n    # 레이블과 클러스터 수를 반환\n    return labels, n_clusters\n```\n:::\n\n\n`perform_clustering` \n\n- 전역 차원 축소, 전역 클러스터링, 이후 로컬 차원 축소 및 로컬 클러스터링까지 전체 클러스터링 파이프라인을 수행하는 핵심 함수입니다.\n- 이전의 과정을 하나의 파이프라인으로 만들어 종합하는 역할을 수행합니다.\n\n**과정**\n\n- 입력된 embeddings가 충분한지 확인(적은 경우 단순 할당).\n- 전역 차원 축소: `global_cluster_embeddings` 로 전체 임베딩에 대해 UMAP 적용.\n- 전역 클러스터링: 전역 차원 축소 결과에 대해 `GMM_cluster` 를 사용하여 전역 클러스터 형성.\n- 각 전역 클러스터에 속하는 데이터만 추출 -> 해당 집합에 대해 로컬 차원 축소(`local_cluster_embeddings`) 수행.\n- 로컬 차원 축소 결과에 대해 다시 `GMM_cluster` 로 로컬 클러스터링 수행.\n- 최종적으로, 각 데이터 포인트에 대해서 전역 및 로컬 클러스터 레이블을 함께 반환합니다.\n\n::: {#0e175ad6 .cell execution_count=12}\n``` {.python .cell-code}\ndef perform_clustering(\n    embeddings: np.ndarray,\n    dim: int,\n    threshold: float,\n) -> List[np.ndarray]:\n    \"\"\"\n    임베딩에 대해 계층적 클러스터링을 수행하는 함수입니다.\n\n    전역 차원 축소와 클러스터링을 먼저 수행한 후, 각 전역 클러스터 내에서\n    로컬 차원 축소와 클러스터링을 수행합니다.\n\n    Args:\n        embeddings (np.ndarray): 클러스터링할 임베딩 벡터들\n        dim (int): 차원 축소 시 목표 차원 수\n        threshold (float): GMM 클러스터링에서 사용할 확률 임계값\n\n    Returns:\n        List[np.ndarray]: 각 데이터 포인트에 대한 로컬 클러스터 레이블 리스트.\n                         각 레이블은 해당 데이터 포인트가 속한 로컬 클러스터의 인덱스를 담은 numpy 배열입니다.\n    \"\"\"\n\n    if len(embeddings) <= dim + 1:\n        # 데이터가 충분하지 않을 때 클러스터링을 피합니다.\n        return [np.array([0]) for _ in range(len(embeddings))]\n\n    # 글로벌 차원 축소\n    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n\n    # 글로벌 클러스터링\n    global_clusters, n_global_clusters = GMM_cluster(\n        reduced_embeddings_global, threshold\n    )\n\n    # 로컬 클러스터링을 위한 초기화\n    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n    total_clusters = 0\n\n    # 각 글로벌 클러스터를 순회하며 로컬 클러스터링 수행\n    for i in range(n_global_clusters):\n        # 현재 글로벌 클러스터에 속하는 임베딩 추출\n        global_cluster_embeddings_ = embeddings[\n            np.array([i in gc for gc in global_clusters])\n        ]\n\n        if len(global_cluster_embeddings_) == 0:\n            continue\n        if len(global_cluster_embeddings_) <= dim + 1:\n            # 작은 클러스터는 직접 할당으로 처리\n            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n            n_local_clusters = 1\n        else:\n            # 로컬 차원 축소 및 클러스터링\n            reduced_embeddings_local = local_cluster_embeddings(\n                global_cluster_embeddings_, dim\n            )\n            local_clusters, n_local_clusters = GMM_cluster(\n                reduced_embeddings_local, threshold\n            )\n\n        # 로컬 클러스터 ID 할당, 이미 처리된 총 클러스터 수를 조정\n        for j in range(n_local_clusters):\n            local_cluster_embeddings_ = global_cluster_embeddings_[\n                np.array([j in lc for lc in local_clusters])\n            ]\n            indices = np.where(\n                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n            )[1]\n            for idx in indices:\n                all_local_clusters[idx] = np.append(\n                    all_local_clusters[idx], j + total_clusters\n                )\n\n        total_clusters += n_local_clusters\n\n    return all_local_clusters\n```\n:::\n\n\n주어진 텍스트 리스트를 임베딩 모델을 이용해 벡터로 변환합니다.\n\n::: {#b299042b .cell execution_count=13}\n``` {.python .cell-code}\ndef embed(texts):\n    \"\"\"\n    주어진 텍스트 리스트를 임베딩 벡터로 변환합니다.\n\n    Args:\n        texts (List[str]): 임베딩할 텍스트 리스트\n\n    Returns:\n        np.ndarray: 텍스트의 임베딩 벡터를 포함하는 numpy 배열\n                   shape은 (텍스트 개수, 임베딩 차원)입니다.\n    \"\"\"\n    text_embeddings = embeddings.embed_documents(texts)\n\n    # 임베딩을 numpy 배열로 변환\n    text_embeddings_np = np.array(text_embeddings)\n    return text_embeddings_np\n```\n:::\n\n\n`embed_cluster_texts` \n\n- 텍스트 리스트를 임베딩하고, 위에서 정의한 클러스터링 절차를 수행한 뒤 결과를 데이터프레임 형태로 반환합니다\n\n**과정**\n\n- embed 함수를 통해 텍스트를 임베딩합니다.\n- perform_clustering를 호출하여 클러스터 라벨을 얻습니다.\n- 원본 텍스트, 임베딩, 클러스터 정보를 하나의 DataFrame에 통합하여 반환합니다.\n\n::: {#c8899c8a .cell execution_count=14}\n``` {.python .cell-code}\ndef embed_cluster_texts(texts):\n    # 임베딩 생성\n    text_embeddings_np = embed(texts)\n    # 클러스터링 수행\n    cluster_labels = perform_clustering(text_embeddings_np, 10, 0.1)\n    # 결과를 저장할 DataFrame 초기화\n    df = pd.DataFrame()\n    # 원본 텍스트 저장\n    df[\"text\"] = texts\n    # DataFrame에 리스트로 임베딩 저장\n    df[\"embd\"] = list(text_embeddings_np)\n    # 클러스터 라벨 저장\n    df[\"cluster\"] = cluster_labels\n    return df\n```\n:::\n\n\n`fmt_txt` 함수는 `pandas`의 `DataFrame`에서 텍스트 문서를 단일 문자열로 포맷팅합니다.\n\n::: {#5db11558 .cell execution_count=15}\n``` {.python .cell-code}\ndef fmt_txt(df: pd.DataFrame) -> str:\n    \"\"\"\n    주어진 DataFrame에서 텍스트 문서를 단일 문자열로 포맷팅하는 함수입니다.\n\n    Args:\n        df (pd.DataFrame): 포맷팅할 텍스트 문서를 포함한 DataFrame\n\n    Returns:\n        str: 텍스트 문서들을 특정 구분자로 결합한 단일 문자열\n    \"\"\"\n    unique_txt = df[\"text\"].tolist()\n    return \"--- --- \\n --- --- \".join(unique_txt)\n```\n:::\n\n\n`embed_cluster_summarize_texts` \n\n- 텍스트 리스트에 대해 임베딩 → 클러스터링 → 요약 까지 전체 프로세스를 수행합니다.\n\n**과정**\n\n- 임베딩 & 클러스터링: `embed_cluster_texts` 함수를 이용해 입력된 텍스트를 임베딩하고 클러스터링한 결과를 `df_clusters` 로 얻습니다. 이 `df_clusters` 는 각 문서와 그 문서를 할당받은 (하나 이상일 수 있는) 클러스터를 가지고 있습니다.\n  \n- 클러스터 할당 확장: 어떤 문서가 여러 클러스터에 속할 수 있으므로, 이를 행 단위로 '문서-클러스터' 페어로 확장한 `expanded_df` 를 만듭니다. 이렇게 하면 이후 처리(특히 요약 단계)에서 각 클러스터별로 문서를 쉽게 그룹화할 수 있습니다.\n\n- LLM(대형 언어 모델)을 이용한 요약: 각 클러스터에 속한 문서들의 텍스트를 하나의 문자열로 합친 뒤(`fmt_txt` 사용), 프롬프트 템플릿을 통해 LLM에 전달합니다. LLM은 해당 클러스터에 대한 요약 문장을 생성합니다.\n\n- 요약 결과 정리: 클러스터별 요약 결과를 `df_summary` DataFrame에 저장합니다. 여기에는 summaries(요약문), level(입력 파라미터로 받은 처리 수준), cluster(클러스터 식별자)가 포함됩니다.\n\n::: {#29a7489a .cell execution_count=16}\n``` {.python .cell-code}\ndef embed_cluster_summarize_texts(\n    texts: List[str], level: int\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    텍스트 목록에 대해 임베딩, 클러스터링 및 요약을 수행합니다. 이 함수는 먼저 텍스트에 대한 임베딩을 생성하고,\n    유사성을 기반으로 클러스터링을 수행한 다음, 클러스터 할당을 확장하여 처리를 용이하게 하고 각 클러스터 내의 내용을 요약합니다.\n\n    매개변수:\n    - texts: 처리할 텍스트 문서 목록입니다.\n    - level: 처리의 깊이나 세부 사항을 정의할 수 있는 정수 매개변수입니다.\n\n    반환값:\n    - 두 개의 데이터프레임을 포함하는 튜플:\n      1. 첫 번째 데이터프레임(`df_clusters`)은 원본 텍스트, 그들의 임베딩, 그리고 클러스터 할당을 포함합니다.\n      2. 두 번째 데이터프레임(`df_summary`)은 각 클러스터에 대한 요약, 지정된 세부 수준, 그리고 클러스터 식별자를 포함합니다.\n    \"\"\"\n\n    # 텍스트를 임베딩하고 클러스터링하여 'text', 'embd', 'cluster' 열이 있는 데이터프레임을 생성합니다.\n    df_clusters = embed_cluster_texts(texts)\n\n    # 클러스터를 쉽게 조작하기 위해 데이터프레임을 확장할 준비를 합니다.\n    expanded_list = []\n\n    # 데이터프레임 항목을 문서-클러스터 쌍으로 확장하여 처리를 간단하게 합니다.\n    for index, row in df_clusters.iterrows():\n        for cluster in row[\"cluster\"]:\n            expanded_list.append(\n                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n            )\n\n    # 확장된 목록에서 새 데이터프레임을 생성합니다.\n    expanded_df = pd.DataFrame(expanded_list)\n\n    # 처리를 위해 고유한 클러스터 식별자를 검색합니다.\n    all_clusters = expanded_df[\"cluster\"].unique()\n\n    print(f\"--Generated {len(all_clusters)} clusters--\")\n\n    # 요약\n    template = \"\"\"여기 LangChain 표현 언어 문서의 하위 집합이 있습니다.\n    \n    LangChain 표현 언어는 LangChain에서 체인을 구성하는 방법을 제공합니다.\n    \n    제공된 문서의 자세한 요약을 제공하십시오.\n    \n    문서:\n    {context}\n    \"\"\"\n    prompt = ChatPromptTemplate.from_template(template)\n    chain = prompt | llm | StrOutputParser()\n\n    # 각 클러스터 내의 텍스트를 요약을 위해 포맷팅합니다.\n    summaries = []\n    for i in all_clusters:\n        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n        formatted_txt = fmt_txt(df_cluster)\n        summaries.append(chain.invoke({\"context\": formatted_txt}))\n\n    # 요약, 해당 클러스터 및 레벨을 저장할 데이터프레임을 생성합니다.\n    df_summary = pd.DataFrame(\n        {\n            \"summaries\": summaries,\n            \"level\": [level] * len(summaries),\n            \"cluster\": list(all_clusters),\n        }\n    )\n\n    return df_clusters, df_summary\n```\n:::\n\n\n`recursive_embed_cluster_summarize`\n\n- 텍스트 데이터에 대해 여러 \"단계(Level)\"에 걸쳐 클러스터링과 요약을 반복적으로 수행합니다.\n- 처음에는 원본 텍스트에 대해 클러스터링 및 요약을 수행한 뒤, 각 클러스터 요약을 다음 단계의 입력 텍스트로 삼아 다시 임베딩 → 클러스터링 → 요약을 반복합니다.\n\n::: {#3af15cf2 .cell execution_count=17}\n``` {.python .cell-code}\ndef recursive_embed_cluster_summarize(\n    texts: List[str], level: int = 1, n_levels: int = 3\n) -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n    # 각 레벨에서의 결과를 저장할 사전\n    results = {}\n\n    # 현재 레벨에 대해 임베딩, 클러스터링, 요약 수행\n    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n\n    # 현재 레벨의 결과 저장\n    results[level] = (df_clusters, df_summary)\n\n    # 추가 재귀가 가능하고 의미가 있는지 결정\n    unique_clusters = df_summary[\"cluster\"].nunique()\n\n    # 현재 레벨이 최대 레벨보다 낮고, 유니크한 클러스터가 1개 이상인 경우\n    if level < n_levels and unique_clusters > 1:\n        # 다음 레벨의 재귀 입력 텍스트로 요약 사용\n        new_texts = df_summary[\"summaries\"].tolist()\n        next_level_results = recursive_embed_cluster_summarize(\n            new_texts, level + 1, n_levels\n        )\n\n        # 다음 레벨의 결과를 현재 결과 사전에 병합\n        results.update(next_level_results)\n\n    return results\n```\n:::\n\n\n전체 문서의 개수를 확인합니다.\n\n::: {#53a3f89c .cell execution_count=18}\n``` {.python .cell-code}\n# 전체 문서의 개수\nlen(texts_split)\n```\n:::\n\n\n이제 `recursive_embed_cluster_summarize` 함수를 호출하여 트리 구축을 시작합니다.\n\n- `level=1` 은 첫 번째 단계의 클러스터링 및 요약부터 시작한다는 의미입니다.\n- `n_levels=3` 은 최대 세 단계까지(조건이 맞는 한) 클러스터링과 요약을 재귀적으로 반복할 수 있다는 뜻입니다.\n- \n결과적으로, 원본 텍스트(leaf_texts)는 먼저 level=1에서 요약되고 클러스터링됩니다. 그 결과로 나온 각 클러스터의 요약이 다음 단계의 입력(level=2)이 되고, 이를 다시 요약하여 클러스터링 한 결과가 level=3 단계의 입력이 될 수 있습니다. \n\n이 과정을 통해 점차 더 추상적이고 집약된 요약 정보를 얻을 수 있게 됩니다.\n\n::: {#4c864542 .cell execution_count=19}\n``` {.python .cell-code}\n# 트리 구축\nleaf_texts = texts_split.copy()\n\n# 재귀적으로 임베딩, 클러스터링 및 요약을 수행하여 결과를 얻음\nresults = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)\n```\n:::\n\n\n다음으로는 vectorstore를 생성하고 로컬에 저장합니다.\n\n::: {#1ec6e29b .cell execution_count=20}\n``` {.python .cell-code}\nleaf_texts[:10]\n```\n:::\n\n\n::: {#b7f1a582 .cell execution_count=21}\n``` {.python .cell-code}\nfrom langchain_community.vectorstores import FAISS\n\nall_texts = leaf_texts.copy()\n\n# 레벨을 정렬하여 순회\nfor level in sorted(results.keys()):\n    # 현재 레벨의 DataFrame에서 요약을 추출\n    summaries = results[level][1][\"summaries\"].tolist()\n    # 현재 레벨의 요약을 all_texts에 추가합니다.\n    all_texts.extend(summaries)\n\n# 이제 all_texts를 사용하여 FAISS vectorstore를 구축합니다.\nvectorstore = FAISS.from_texts(texts=all_texts, embedding=embeddings)\n```\n:::\n\n\nDB 를 로컬에 저장합니다.\n\n::: {#2b418207 .cell execution_count=22}\n``` {.python .cell-code}\nimport os\n\nDB_INDEX = \"RAPTOR\"\n\n# 기존 DB 인덱스가 존재하면 로드하여 vectorstore와 병합한 후 저장합니다.\nif os.path.exists(DB_INDEX):\n    local_index = FAISS.load_local(DB_INDEX, embeddings)\n    local_index.merge_from(vectorstore)\n    local_index.save_local(DB_INDEX)\nelse:\n    vectorstore.save_local(folder_path=DB_INDEX)\n```\n:::\n\n\n`vectorstore` 로부터 `retriever`를 생성합니다.\n\n::: {#4baed7f8 .cell execution_count=23}\n``` {.python .cell-code}\n# retriever 생성\nretriever = vectorstore.as_retriever()\n```\n:::\n\n\n## RAG 체인 정의\n\n이제 생성된 vectorstore를 이용해 RAG 체인을 정의하고 실행하여 결과를 확인합니다.\n\n::: {#3ed9fa61 .cell execution_count=24}\n``` {.python .cell-code}\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import PromptTemplate\n\n# 프롬프트 정의\nprompt = PromptTemplate.from_template(\n    \"\"\"\n    You are an AI assistant specializing in Question-Answering (QA) tasks within a Retrieval-Augmented Generation (RAG) system. \nYou are given PDF documents. Your primary mission is to answer questions based on provided context.\nEnsure your response is concise and directly addresses the question without any additional narration.\n\n###\n\nYour final answer should be written concisely (but include important numerical values, technical terms, jargon, and names).\n\n# Steps\n\n1. Carefully read and understand the context provided.\n2. Identify the key information related to the question within the context.\n3. Formulate a concise answer based on the relevant information.\n4. Ensure your final answer directly addresses the question.\n\n# Output Format:\n[General introduction of the answer]\n[Comprehensive answer to the question]\n\n###\n\nRemember:\n- It's crucial to base your answer solely on the **PROVIDED CONTEXT**. \n- DO NOT use any external knowledge or information not present in the given materials.\n\n###\n\n# Here is the user's QUESTION that you should answer:\n{question}\n\n# Here is the CONTEXT that you should use to answer the question:\n{context}\n\n[Note]\n- Answer should be written in Korean.\n\n# Your final ANSWER to the user's QUESTION:\"\"\"\n)\n\n\n# 문서 포맷팅\ndef format_docs(docs):\n    return \"\\n\\n\".join(f\"<document>{doc.page_content}</document>\" for doc in docs)\n\n\n# RAG 체인 정의\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n```\n:::\n\n\n[LangSmith 링크](https://smith.langchain.com/public/e5acd315-a662-4f93-aec3-04c80a8bd2a4/r)\n\n::: {#4b989ae3 .cell execution_count=25}\n``` {.python .cell-code}\n# 추상적인 질문 실행\nanswer = rag_chain.stream(\"전체 문서가 다루는 주요 내용에 대해 정리해주세요.\")\nstream_response(answer)\n```\n:::\n\n\n[LangSmith 링크](https://smith.langchain.com/public/bf58bdc0-ae03-4793-89ed-3d2bc95bd331/r)\n\n::: {#26a1a21d .cell execution_count=26}\n``` {.python .cell-code}\n# mid level 질문 실행\nanswer = rag_chain.stream(\"Anthropic 에 투자 관련된 내용을 요약하세요.\")\nstream_response(answer)\n```\n:::\n\n\n[LangSmith 링크](https://smith.langchain.com/public/d2869acc-ac9b-4d4d-85b9-33a73b43b535/r)\n\n::: {#b0777012 .cell execution_count=27}\n``` {.python .cell-code}\n# Low Level 질문 실행\nanswer = rag_chain.stream(\"삼성전자가 개발한 생성형 AI 의 이름과 발표일은?\")\nstream_response(answer)\n```\n:::\n\n\n",
    "supporting": [
      "05-RAPTOR-Long-Context-RAG-PDF_files"
    ],
    "filters": [],
    "includes": {}
  }
}