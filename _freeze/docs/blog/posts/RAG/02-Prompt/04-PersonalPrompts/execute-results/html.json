{
  "hash": "5b5967c2a0f18c0de81a4a4ec3fb4d91",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Personal Prompts\"\nsubtitle: 프롬프트 엔지니어링\ndescription: |\n  효과적인 프롬프트 템플릿 설계 및 관리 기법을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 01/15/2025\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n::: {#ae5a663e .cell execution_count=2}\n``` {.python .cell-code}\nfrom dotenv import load_dotenv\n\nload_dotenv()\n```\n:::\n\n\nLangChain 아이디를 입력합니다.\n\n::: {#f6212f7d .cell execution_count=3}\n``` {.python .cell-code}\n# Owner 지정\nPROMPT_OWNER = \"teddynote\"\n```\n:::\n\n\n## 요약: Stuff Documents\n\n::: {#197a4520 .cell execution_count=4}\n``` {.python .cell-code}\nfrom langchain import hub\nfrom langchain.prompts import PromptTemplate\n\nprompt_title = \"summary-stuff-documents\"\n\n# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\nprompt_template = \"\"\"Please summarize the sentence according to the following REQUEST.\nREQUEST:\n1. Summarize the main points in bullet points.\n2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\n3. Use various emojis to make the summary more interesting.\n4. DO NOT include any unnecessary information.\n\nCONTEXT:\n{context}\n\nSUMMARY:\"\n\"\"\"\nprompt = PromptTemplate.from_template(prompt_template)\nprompt\n```\n:::\n\n\n::: {#c550a3a3 .cell execution_count=5}\n``` {.python .cell-code}\nhub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)\n```\n:::\n\n\n## Map Prompt\n\n::: {#8941d416 .cell execution_count=6}\n``` {.python .cell-code}\nfrom langchain import hub\nfrom langchain.prompts import PromptTemplate\n\nprompt_title = \"map-prompt\"\n\n# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\nprompt_template = \"\"\"You are a helpful expert journalist in extracting the main themes from a GIVEN DOCUMENTS below.\nPlease provide a comprehensive summary of the GIVEN DOCUMENTS in numbered list format. \nThe summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format. \nPlease ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition. \nThe length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information.\n\nGIVEN DOCUMENTS:\n{docs}\n\nFORMAT:\n1. main theme 1\n2. main theme 2\n3. main theme 3\n...\n\nCAUTION:\n- DO NOT list more than 5 main themes.\n\nHelpful Answer:\n\"\"\"\nprompt = PromptTemplate.from_template(prompt_template)\nprompt\n```\n:::\n\n\n::: {#84940e02 .cell execution_count=7}\n``` {.python .cell-code}\nhub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)\n```\n:::\n\n\n## Reduce Prompt\n\n::: {#9a213e48 .cell execution_count=8}\n``` {.python .cell-code}\nfrom langchain import hub\nfrom langchain.prompts import PromptTemplate\n\nprompt_title = \"reduce-prompt\"\n\n# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\nprompt_template = \"\"\"You are a helpful expert in summary writing.\nYou are given numbered lists of summaries.\nExtract top 10 most important insights from the summaries.\nThen, write a summary of the insights in KOREAN.\n\nLIST OF SUMMARIES:\n{doc_summaries}\n\nHelpful Answer:\n\"\"\"\nprompt = PromptTemplate.from_template(prompt_template)\nprompt\n```\n:::\n\n\n::: {#89b7e0ca .cell execution_count=9}\n``` {.python .cell-code}\nhub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)\n```\n:::\n\n\n::: {#e1844ab1 .cell execution_count=10}\n``` {.python .cell-code}\nfrom langchain import hub\nfrom langchain.prompts import PromptTemplate\n\nprompt_title = \"chain-of-density-reduce-korean\"\n\n# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\nprompt_template = \"\"\"You are a helpful expert in summary writing. You are given lists of summaries.\nPlease sum up previously summarized sentences according to the following REQUEST.\nREQUEST:\n1. Summarize the main points in bullet points in KOREAN.\n2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\n3. Use various emojis to make the summary more interesting.\n4. MOST IMPORTANT points should be organized at the top of the list.\n5. DO NOT include any unnecessary information.\n\nLIST OF SUMMARIES:\n{doc_summaries}\n\nHelpful Answer: \"\"\"\nprompt = PromptTemplate.from_template(prompt_template)\nprompt\n```\n:::\n\n\n::: {#6a493312 .cell execution_count=11}\n``` {.python .cell-code}\nhub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)\n```\n:::\n\n\n## Metadata Tagger\n\n::: {#d670e968 .cell execution_count=12}\n``` {.python .cell-code}\nfrom langchain import hub\nfrom langchain.prompts import ChatPromptTemplate\n\nprompt_title = \"metadata-tagger\"\n\n# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\nprompt_template = \"\"\"Given the following product review, conduct a comprehensive analysis to extract key aspects mentioned by the customer, with a focus on evaluating the product's design and distinguishing between positive aspects and areas for improvement. \nIdentify primary features or attributes of the product that the customer appreciated or highlighted, specifically looking for mentions related to the feel of the keys, sound produced by the keys, overall user experience, charging aspect, and the design of the product, etc. \nAssess the overall tone of the review (positive, neutral, or negative) based on the sentiment expressed about these attributes. \nAdditionally, provide a detailed evaluation of the design, outline the positive aspects that the customer enjoyed, and note any areas of improvement or disappointment mentioned. \nExtract the customer's rating of the product on a scale of 1 to 5, as indicated at the beginning of the review. \nSummarize your findings in a structured JSON format, including an array of keywords, evaluations for design, satisfaction points, improvement areas, the assessed tone, and the numerical rating. \n\nINPUT:\n{input}\n\n\"\"\"\nprompt = ChatPromptTemplate.from_template(prompt_template)\nprompt\n```\n:::\n\n\n::: {#9a54af83 .cell execution_count=13}\n``` {.python .cell-code}\nhub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)\n```\n:::\n\n\n## Chain of Density 요약\n\n::: {#3f292153 .cell execution_count=14}\n``` {.python .cell-code}\nfrom langchain import hub\nfrom langchain.prompts import ChatPromptTemplate\n\nprompt_title = \"chain-of-density-korean\"\n\n# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\nprompt = ChatPromptTemplate.from_template(\n    \"\"\"Article: {ARTICLE}\nYou will generate increasingly concise, entity-dense summaries of the above article. \n\nRepeat the following 2 steps 5 times. \n\nStep 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary. \nStep 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. \n\nA missing entity is:\n- relevant to the main story, \n- specific yet concise (100 words or fewer), \n- novel (not in the previous summary), \n- faithful (present in the article), \n- anywhere (can be located anywhere in the article).\n\nGuidelines:\n\n- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~200 words.\n- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. \n- Missing entities can appear anywhere in the new summary.\n- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \n\nRemember, use the exact same number of words for each summary.\nAnswer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\nUse only KOREAN language to reply.\"\"\"\n)\n```\n:::\n\n\n::: {#695dde02 .cell execution_count=15}\n``` {.python .cell-code}\nhub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)\n```\n:::\n\n\n## Chain of Density (Korean) - 2\n\n::: {#f1269fc6 .cell execution_count=16}\n``` {.python .cell-code}\nfrom langchain import hub\nfrom langchain.prompts import ChatPromptTemplate\n\nprompt_title = \"chain-of-density-map-korean\"\n\n# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\nprompt = ChatPromptTemplate.from_template(\n    \"\"\"Article: {ARTICLE}\nYou will generate increasingly concise, entity-dense summaries of the above article. \n\nRepeat the following 2 steps 3 times. \n\nStep 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary. \nStep 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. \n\nA missing entity is:\n- relevant to the main story, \n- specific yet concise (100 words or fewer), \n- novel (not in the previous summary), \n- faithful (present in the article), \n- anywhere (can be located anywhere in the article).\n\nGuidelines:\n\n- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~200 words.\n- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. \n- Missing entities can appear anywhere in the new summary.\n- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \n\nRemember, use the exact same number of words for each summary.\nAnswer \"Missing Entities\" and \"Denser_Summary\" as in TEXT format.\nUse only KOREAN language to reply.\"\"\"\n)\n```\n:::\n\n\n::: {#fb5ab24a .cell execution_count=17}\n``` {.python .cell-code}\nhub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)\n```\n:::\n\n\n## RAG 문서 프롬프트\n\n::: {#c92f39db .cell execution_count=18}\n``` {.python .cell-code}\nprompt_title = \"rag-korean\"\n```\n:::\n\n\n::: {#30c6709d .cell execution_count=19}\n``` {.python .cell-code}\nsystem = \"\"\"당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\n검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\n기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요. 답변은 한글로 답변해 주세요.\"\"\"\n\nhuman = \"\"\"#Question: \n{question} \n\n#Context: \n{context} \n\n#Answer:\"\"\"\n\nfrom langchain.prompts import ChatPromptTemplate\n\n\nprompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n```\n:::\n\n\n::: {#0a34ed9e .cell execution_count=20}\n``` {.python .cell-code}\nhub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt, parent_commit_hash=\"latest\")\n```\n:::\n\n\n## RAG + 출처\n\n::: {#51d6aa9c .cell execution_count=21}\n``` {.python .cell-code}\nfrom langchain import hub\nfrom langchain.prompts import ChatPromptTemplate\n```\n:::\n\n\n::: {#ae7d2ea1 .cell execution_count=22}\n``` {.python .cell-code}\nprompt_title = \"rag-korean-with-source\"\n```\n:::\n\n\n::: {#0a44952d .cell execution_count=23}\n``` {.python .cell-code}\nsystem = \"\"\"당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\n검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\n기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요. 출처(page, source)를 답변헤 포함하세요. 답변은 한글로 답변해 주세요.\"\"\"\n\nhuman = \"\"\"#Question: \n{question} \n\n#Context: \n{context} \n\n#Answer:\"\"\"\n\nprompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n```\n:::\n\n\n::: {#4b8bf4eb .cell execution_count=24}\n``` {.python .cell-code}\nhub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt, parent_commit_hash=\"latest\")\n```\n:::\n\n\n## LLM Evaluation\n\n::: {#6a6bb60e .cell execution_count=25}\n``` {.python .cell-code}\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain import hub\n\nprompt = PromptTemplate.from_template(\n    \"\"\"\nAs an LLM evaluator (judge), please assess the LLM's response to the given question. Evaluate the response's accuracy, comprehensiveness, and context precision based on the provided context. After your evaluation, return only the numerical scores in the following format:\nAccuracy: [score]\nComprehensiveness: [score]\nContext Precision: [score]\nFinal: [normalized score]\nGrading rubric:\n\nAccuracy (0-10 points):\nEvaluate how well the answer aligns with the information provided in the given context.\n\n0 points: The answer is completely inaccurate or contradicts the provided context\n4 points: The answer partially aligns with the context but contains significant inaccuracies\n7 points: The answer mostly aligns with the context but has minor inaccuracies or omissions\n10 points: The answer fully aligns with the provided context and is completely accurate\n\n\nComprehensiveness (0-10 points):\n\n0 points: The answer is completely inadequate or irrelevant\n3 points: The answer is accurate but too brief to fully address the question\n7 points: The answer covers main aspects but lacks detail or misses minor points\n10 points: The answer comprehensively covers all aspects of the question\n\n\nContext Precision (0-10 points):\nEvaluate how precisely the answer uses the information from the provided context.\n\n0 points: The answer doesn't use any information from the context or uses it entirely incorrectly\n4 points: The answer uses some information from the context but with significant misinterpretations\n7 points: The answer uses most of the relevant context information correctly but with minor misinterpretations\n10 points: The answer precisely and correctly uses all relevant information from the context\n\n\nFinal Normalized Score:\nCalculate by summing the scores for accuracy, comprehensiveness, and context precision, then dividing by 30 to get a score between 0 and 1.\nFormula: (Accuracy + Comprehensiveness + Context Precision) / 30\n\n#Given question:\n{question}\n\n#LLM's response:\n{answer}\n\n#Provided context:\n{context}\n\nPlease evaluate the LLM's response according to the criteria above. \n\nIn your output, include only the numerical scores for FINAL NORMALIZED SCORE without any additional explanation or reasoning.\nex) 0.81\n\n#Final Normalized Score(Just the number):\n\n\"\"\"\n)\n\nhub.push(f\"teddynote/context-answer-evaluator\", prompt)\n```\n:::\n\n\n::: {#e142a87e .cell execution_count=26}\n``` {.python .cell-code}\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# 프롬프트\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n    If the document contains keyword(s) or semantic m   eaning related to the user question, grade it as relevant. \\n\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n\ngrade_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Retrieved document: \\n\\n {context} \\n\\n User question: {question}\"),\n    ]\n)\n\nhub.push(f\"teddynote/retrieval-question-grader\", grade_prompt)\n```\n:::\n\n\n::: {#d5153e20 .cell execution_count=27}\n``` {.python .cell-code}\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Prompt\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to the answer. \\n \n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n    If the document contains keyword(s) or semantic meaning related to the answer, grade it as relevant. \\n\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the answer.\"\"\"\n\ngrade_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Retrieved document: \\n\\n {context} \\n\\n Answer: {answer}\"),\n    ]\n)\n\nhub.push(f\"teddynote/retrieval-answer-grader\", grade_prompt)\n```\n:::\n\n\n",
    "supporting": [
      "04-PersonalPrompts_files"
    ],
    "filters": [],
    "includes": {}
  }
}