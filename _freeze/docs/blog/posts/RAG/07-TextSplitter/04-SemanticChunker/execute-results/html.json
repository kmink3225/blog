{
  "hash": "83d07bda676d8db319fabc21d0879344",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"SemanticChunker\"\nsubtitle: 텍스트 분할\ndescription: |\n  효율적인 문서 청킹을 위한 다양한 텍스트 분할 전략을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 12/31/2024\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n텍스트를 의미론적 유사성에 기반하여 분할합니다.\n\n**Reference**\n\n- [Greg Kamradt의 노트북](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n이 방법은 텍스트를 문장 단위로 분할한 후, 3개의 문장씩 그룹화하고, 임베딩 공간에서 유사한 문장들을 병합하는 과정을 거칩니다.\n\n\n샘플 텍스트를 로드하고 내용을 출력합니다.\n\n::: {#6994918c .cell execution_count=1}\n``` {.python .cell-code}\n# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\nwith open(\"./data/appendix-keywords.txt\") as f:\n    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n\n# 파일으로부터 읽은 내용을 일부 출력합니다.\nprint(file[:350])\n```\n:::\n\n\n## SemanticChunker 생성\n\n`SemanticChunker`는 LangChain의 실험적 기능 중 하나로, 텍스트를 의미론적으로 유사한 청크로 분할하는 역할을 합니다.\n\n이를 통해 텍스트 데이터를 보다 효과적으로 처리하고 분석할 수 있습니다.\n\n::: {#e40e8bc8 .cell execution_count=2}\n``` {.python .cell-code}\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n```\n:::\n\n\n`SemanticChunker`를 사용하여 텍스트를 의미적으로 관련된 청크로 분할합니다.\n\n::: {#16fbe781 .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\n# OpenAI 임베딩을 사용하여 의미론적 청크 분할기를 초기화합니다.\ntext_splitter = SemanticChunker(OpenAIEmbeddings())\n```\n:::\n\n\n## 텍스트 분할\n\n- `text_splitter`를 사용하여 `file` 텍스트를 문서 단위로 분할합니다.\n\n::: {#cf46b8cd .cell execution_count=4}\n``` {.python .cell-code}\nchunks = text_splitter.split_text(file)\n```\n:::\n\n\n분할된 청크를 확인합니다.\n\n::: {#0ed7dd52 .cell execution_count=5}\n``` {.python .cell-code}\n# 분할된 청크 중 첫 번째 청크를 출력합니다.\nprint(chunks[0])\n```\n:::\n\n\n`create_documents()` 함수를 사용하여 청크를 문서로 변환할 수 있습니다.\n\n::: {#023690a8 .cell execution_count=6}\n``` {.python .cell-code}\n# text_splitter를 사용하여 분할합니다.\ndocs = text_splitter.create_documents([file])\nprint(docs[0].page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n```\n:::\n\n\n## Breakpoints\n\n이 chunker는 문장을 \"분리\"할 시점을 결정하여 작동합니다. 이는 두 문장 간의 임베딩 차이를 살펴봄으로써 이루어집니다.\n\n그 차이가 특정 임계값을 넘으면 문장이 분리됩니다.\n\n- 참고 영상: https://youtu.be/8OJC21T2SL4?si=PzUtNGYJ_KULq3-w&t=2580\n\n### Percentile\n\n기본적인 분리 방식은 백분위수(`Percentile`) 를 기반으로 합니다.\n\n이 방법에서는 문장 간의 모든 차이를 계산한 다음, 지정한 백분위수를 기준으로 분리합니다.\n\n::: {#259a8aba .cell execution_count=7}\n``` {.python .cell-code}\ntext_splitter = SemanticChunker(\n    # OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.\n    OpenAIEmbeddings(),\n    # 분할 기준점 유형을 백분위수로 설정합니다.\n    breakpoint_threshold_type=\"percentile\",\n    breakpoint_threshold_amount=70,\n)\n```\n:::\n\n\n분할된 결과를 확인합니다.\n\n::: {#b86272a7 .cell execution_count=8}\n``` {.python .cell-code}\ndocs = text_splitter.create_documents([file])\nfor i, doc in enumerate(docs[:5]):\n    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n    print(\"===\" * 20)\n```\n:::\n\n\n`docs`의 길이를 출력합니다.\n\n::: {#f0ade8c9 .cell execution_count=9}\n``` {.python .cell-code}\nprint(len(docs))  # docs의 길이를 출력합니다.\n```\n:::\n\n\n### Standard Deviation\n\n이 방법에서는 지정한 `breakpoint_threshold_amount` 표준편차보다 큰 차이가 있는 경우 분할됩니다.\n\n- `breakpoint_threshold_type` 매개변수를 \"standard_deviation\"으로 설정하여 청크 분할 기준을 표준편차 기반으로 지정합니다.\n\n::: {#1f6b0dca .cell execution_count=10}\n``` {.python .cell-code}\ntext_splitter = SemanticChunker(\n    # OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.\n    OpenAIEmbeddings(),\n    # 분할 기준으로 표준 편차를 사용합니다.\n    breakpoint_threshold_type=\"standard_deviation\",\n    breakpoint_threshold_amount=1.25,\n)\n```\n:::\n\n\n분할된 결과를 확인합니다.\n\n::: {#095804a8 .cell execution_count=11}\n``` {.python .cell-code}\n# text_splitter를 사용하여 분할합니다.\ndocs = text_splitter.create_documents([file])\n```\n:::\n\n\n::: {#20e7b11a .cell execution_count=12}\n``` {.python .cell-code}\ndocs = text_splitter.create_documents([file])\nfor i, doc in enumerate(docs[:5]):\n    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n    print(\"===\" * 20)\n```\n:::\n\n\n`docs`의 길이를 출력합니다.\n\n::: {#e76b13c0 .cell execution_count=13}\n``` {.python .cell-code}\nprint(len(docs))  # docs의 길이를 출력합니다.\n```\n:::\n\n\n### Interquartile\n\n이 방법에서는 사분위수 범위(interquartile range)를 사용하여 청크를 분할합니다.\n\n- `breakpoint_threshold_type` 매개변수를 \"interquartile\"로 설정하여 청크 분할 기준을 사분위수 범위로 지정합니다.\n\n::: {#448a37a9 .cell execution_count=14}\n``` {.python .cell-code}\ntext_splitter = SemanticChunker(\n    # OpenAI의 임베딩 모델을 사용하여 의미론적 청크 분할기를 초기화합니다.\n    OpenAIEmbeddings(),\n    # 분할 기준점 임계값 유형을 사분위수 범위로 설정합니다.\n    breakpoint_threshold_type=\"interquartile\",\n    breakpoint_threshold_amount=0.5,\n)\n```\n:::\n\n\n::: {#ce74d05b .cell execution_count=15}\n``` {.python .cell-code}\n# text_splitter를 사용하여 분할합니다.\ndocs = text_splitter.create_documents([file])\n\n# 결과를 출력합니다.\nfor i, doc in enumerate(docs[:5]):\n    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n    print(\"===\" * 20)\n```\n:::\n\n\n`docs`의 길이를 출력합니다.\n\n::: {#b6f1f06d .cell execution_count=16}\n``` {.python .cell-code}\nprint(len(docs))  # docs의 길이를 출력합니다.\n```\n:::\n\n\n",
    "supporting": [
      "04-SemanticChunker_files"
    ],
    "filters": [],
    "includes": {}
  }
}