{
  "hash": "f46a31487c5a6bf19b7fd6541aac240a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"GPT4All\"\nsubtitle: 오픈소스 챗봇 GPT4All 활용 가이드\ndescription: |\n  Nomic AI의 GPT4All 오픈소스 챗봇 생태계를 LangChain에서 활용하는 방법을 다룬다.\n  로컬에서 실행 가능한 GPT4All 모델 설치 및 사용법을 설명한다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 03/24/2025\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n![](./images/gpt4all.png)\n\n[GitHub:nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all) 은 코드, 채팅 형식의 대화를 포함한 방대한 양의 데이터로 학습된 오픈 소스 챗봇 생태계입니다.\n\n이 예제에서는 LangChain을 사용하여 `GPT4All` 모델과 상호 작용하는 방법에 대해 설명합니다.\n\n\n## 설치방법\n\n1. 먼저, 공식 홈페이지에 접속하여 설치파일을 다운로드 받아 설치합니다\n2. [공식 홈페이지](https://gpt4all.io/index.html) 바로가기\n3. 파이썬 패키지를 설치합니다.\n4. [pip 를 활용한 설치 방법](https://github.com/nomic-ai/gpt4all/blob/main/gpt4all-bindings/python/README.md)\n\n::: {#5127d8d7 .cell execution_count=1}\n``` {.python .cell-code}\n# !pip install -qU gpt4all\n```\n:::\n\n\n## 모델 다운로드\n\n![](./images/gpt4all_models.png)\n\n[gpt4all 페이지](https://gpt4all.io/index.html)에는 `Model Explorer` 섹션이 있습니다.\n(더 많은 정보를 원하시면 https://github.com/nomic-ai/gpt4all 을 방문하세요.)\n\n1. [공식 홈페이지](https://gpt4all.io/index.html) 에서 다운로드 가능한 모델을 다운로드 받습니다. 본인의 PC 사양에서 구동가능한 모델을 선택하는 것이 좋습니다.\n2. 본 튜토리얼에서는 `EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf`(10.69GB) 모델을 다운로드 받아 진행하겠습니다.\n3. 다운로드 받은 모델은 `models` 폴더 생성 후 해당 폴더에 다운로드 받습니다.\n\n- `local_path` 변수에 로컬 파일 경로(`\"./models/EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf\"`)를 할당합니다.\n- 이 경로는 사용자가 원하는 로컬 파일 경로로 대체할 수 있습니다.\n\n::: {#83568cb4 .cell execution_count=2}\n``` {.python .cell-code}\nlocal_path = \"./models/EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf\"  # 원하는 로컬 파일 경로로 대체하세요.\n```\n:::\n\n\n## 모델 정보 설정\n\n로컬에서 실행하려면 호환되는 ggml 형식의 모델을 다운로드하세요.\n\n- 관심 있는 모델을 선택하세요.\n- UI를 사용하여 다운로드하고 `.bin` 파일을 `local_path`(아래 참고)로 이동시키세요.\n\n### GPT4ALL 모델 활용\n\n`GPT4All`은 GPT-3와 유사한 대규모 언어 모델로, 다양한 자연어 처리 작업에 활용될 수 있습니다.\n\n이 모듈을 사용하면 GPT4All 모델을 간편하게 로드하고 추론에 활용할 수 있습니다.\n\n::: {#538141b5 .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_community.llms import GPT4All\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.callbacks import StreamingStdOutCallbackHandler\n\n# 프롬프트\nprompt = ChatPromptTemplate.from_template(\n    \"\"\"<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n<s>Human: {question}</s>\n<s>Assistant:\n\"\"\"\n)\n\n# GPT4All 언어 모델 초기화\n# model는 GPT4All 모델 파일의 경로를 지정\nllm = GPT4All(\n    model=local_path,\n    backend=\"gpu\",  # GPU 설정\n    streaming=True,  # 스트리밍 설정\n    callbacks=[StreamingStdOutCallbackHandler()],  # 콜백 설정\n)\n\n# 체인 생성\nchain = prompt | llm | StrOutputParser()\n\n# 질의 실행\nresponse = chain.invoke({\"question\": \"대한민국의 수도는 어디인가요?\"})\n```\n:::\n\n\n",
    "supporting": [
      "10-GPT4ALL_files"
    ],
    "filters": [],
    "includes": {}
  }
}