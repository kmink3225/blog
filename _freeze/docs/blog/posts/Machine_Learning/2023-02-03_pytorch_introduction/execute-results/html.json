{
  "hash": "1ffc643ed74d45df2e6e658322baee66",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Pytorch Introduction\nsubtitle: overview, object creation, indexing, concatenating, casting, shape, transpose, arithematic operations, matrix multiplication, mean, max, argmax, dimension manipulation, automatic differenctiation\ndescription: |\n  Learn how to manipulate Pytorch, one of the most commonly used Python frameworks to implement machine learning algorithms using Python. 파이썬을 이용하여 머신러닝 알고리즘을 구현하기 위해 가장 대표적으로 쓰이는 파이썬 package중 하나인 Tensor flow조작법에 대해 알아본다. \ncategories:\n  - ML\nauthor: Kwangmin Kim\ndate: 02/03/2023\nformat: \n  html:\n    page-layout: full\n    code-fold: true\nexecute: \n  eval: false\n---\n\n아직 GPU를 못잡았음 -> Google Colab에서만 돌아감\n\n## Pytorch Overview\n\n* PyTorch는 기계 학습 프레임워크(framework) 중 하나다.\n  * PyTorch의 텐서(tensor)는 NumPy 배열과 매우 유사하다.\n  * Tensor flow 보다 사용 비중이 늘어나고 있다.\n  * Tensor: 고차원 데이터 (배열)를 의미, 3차원 배열 이상\n* PyTorch를 사용하면, GPU 연동을 통해 효율적으로 딥러닝 모델을 학습할 수 있다.\n* Google Colab을 이용하면, 손쉽게 PyTorch를 시작할 수 있다.\n* Google Colab에서는 <b>[런타임]</b> - <b>[런타임 유형 변경]</b>에서 <b>GPU를 선택</b>할 수 있다.\n* Google Colab에선 pytoch가 내장되어 있기 때문에 따로 설치할 필요 없음\n\n### GPU 사용 여부 체크하기\n\n* 텐서간의 연산을 수행할 때, 기본적으로 **두 텐서가 같은 장치**에 있어야 한다.\n* 연산을 수행하는 텐서들을 모두 GPU에 올린 뒤에 연산을 수행한다.\n* GPU는 고차원 행렬곱같은 병렬 처리 연산에 최적화 되어 있다.\n\ntensor 자체가 고차원 배열이기 때문에 데이터를 불러오면 tensor 형태로 바꿀 수 있다.\n\n### 텐서(tensor) 객체 생성 (기본 python 데이터 유형)\n\n::: {#83468909 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\n\n# data initialization: 초기화된 데이터는 gpu에 있음\ndata = [\n  [1, 2],\n  [3, 4]\n]\n\nx = torch.tensor(data) # list를 tensor 형태로 바꾸기. \nprint(x.is_cuda)\n\nx = x.cuda() # CPU -> GPU로 옮기기\nprint(x.is_cuda)\n\nx = x.cpu() # GPU -> CPU로 옮기기\nprint(x.is_cuda)\n```\n:::\n\n\n* <b>서로 다른 장치(device)</b>에 있는 텐서끼리 연산을 수행하면 오류가 발생한다.\n\n::: {#a6fd2d23 .cell execution_count=2}\n``` {.python .cell-code}\n# GPU 장치의 텐서\na = torch.tensor([\n    [1, 1],\n    [2, 2]\n]).cuda()\n\n# CPU 장치의 텐서\nb = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\n\n# print(torch.matmul(a, b)) # 오류 발생\nprint(torch.matmul(a.cpu(), b))\n```\n:::\n\n\n### <b>2. 텐서 소개 및 생성 방법</b>\n\n#### <b>1) 텐서의 속성</b>\n\n* 텐서의 <b>기본 속성</b>으로는 다음과 같은 것들이 있다.\n  * 모양(shape): 텐서 객체의 차원을 확인할 수 있다. (tensor_var.shape)\n  * 자료형(data type) : 텐서의 기본 자료형은 float type (tensor_var.dtype)\n  * 저장된 장치: CPU인지 GPU인지 확인 (tensor_var.device)\n\n::: {#5c981fbe .cell execution_count=3}\n``` {.python .cell-code}\ntensor = torch.rand(3, 4)\n\nprint(tensor)\nprint(f\"Shape: {tensor.shape}\")\nprint(f\"Data type: {tensor.dtype}\")\nprint(f\"Device: {tensor.device}\")\n```\n:::\n\n\n#### <b>2) 텐서 초기화</b>\n\n* 리스트 데이터에서 직접 텐서를 초기화할 수 있다.\n\n::: {#ceb41d3a .cell execution_count=4}\n``` {.python .cell-code}\ndata = [\n  [1, 2],\n  [3, 4]\n]\nx = torch.tensor(data)\n\nprint(x)\n\n```\n:::\n\n\n* NumPy 배열에서 텐서를 초기화할 수 있다.\n\n::: {#147c0563 .cell execution_count=5}\n``` {.python .cell-code}\na = torch.tensor([5])\nb = torch.tensor([7])\n\nc = (a + b).numpy() # tensor -> numpy array\nprint(c)\nprint(type(c)) # ndarray: numpy array \n\nresult = c * 10\ntensor = torch.from_numpy(result) # numpy array -> tensor \nprint(tensor)\nprint(type(tensor))\n```\n:::\n\n\n#### <b>3) 다른 텐서로부터 data를 가져와 텐서 초기화하기</b>\n\n* 다른 텐서의 정보를 토대로 텐서를 초기화할 수 있다.\n* <b>텐서의 속성</b>은 모양(shape)과 자료형(data type)이 있다\n\n::: {#be3812b5 .cell execution_count=6}\n``` {.python .cell-code}\nx = torch.tensor([\n    [5, 7],\n    [1, 2]\n])\n\n# x와 같은 shape와 data type을 가지지만, 값이 1인 텐서 생성\nx_ones = torch.ones_like(x)\nprint(x_ones)\n# x와 같은 shape를 가지되, 자료형은 float으로 덮어쓰고, 값은 랜덤으로 채우기\nx_rand = torch.rand_like(x, dtype=torch.float32) # uniform distribution [0, 1)\nprint(x_rand)\n```\n:::\n\n\n### <b>3. 텐서의 형변환 및 차원 조작</b>\n\n* 텐서는 넘파이(NumPy) 배열처럼 조작할 수 있다.\n\n#### <b>1) 텐서의 특정 차원 접근하기</b>\n\n* 텐서의 원하는 차원에 접근할 수 있다.\n\n::: {#8e33d841 .cell execution_count=7}\n``` {.python .cell-code}\ntensor = torch.tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12]\n])\n\nprint(tensor[0]) # the first row\nprint(tensor[:, 0]) # indexing the first column with all the rows\n# whatever the previous columns are, indexing the last column with all the rows\nprint(tensor[..., -1]) \n```\n:::\n\n\n#### <b>2) 텐서 이어붙이기(Concatenate)</b>\n\n* 두 텐서를 이어 붙여 연결하여 새로운 텐서를 만들 수 있다.\n\n::: {#8e114e6b .cell execution_count=8}\n``` {.python .cell-code}\ntensor = torch.tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12]\n])\n\n# dim: 텐서를 이어 붙이기 위한 축\n# 0번 축(행)을 기준으로 이어 붙이기: 즉, row bind로 연결\nresult = torch.cat([tensor, tensor, tensor], dim=0) \nprint(result)\nprint(result.shape) # 9x4\n\n# 1번 축(열)을 기준으로 이어 붙이기: 즉, column bind로 연결\nresult = torch.cat([tensor, tensor, tensor], dim=1)  \nprint(result)\nprint(result.shape) # 3x12\n```\n:::\n\n\n#### <b>3) 텐서 형변환(Type Casting)</b>\n\n* 텐서의 자료형(정수, 실수 등)을 변환할 수 있다.\n\n::: {#3bdfa316 .cell execution_count=9}\n``` {.python .cell-code}\na = torch.tensor([2], dtype=torch.int) # integers\nb = torch.tensor([5.0]) # real numbers\n\nprint(a.dtype)\nprint(b.dtype)\n\n# 텐서 a는 자동으로 float32형으로 형변환 처리\nprint(a + b)\n# 텐서 b를 int32형으로 형변환하여 덧셈 수행\nprint(a + b.type(torch.int32))\n```\n:::\n\n\n#### <b>4) 텐서의 모양 변경</b>\n\n* view()는 텐서의 shape를 변경할 때 사용한다.\n* 이때, 텐서(tensor)의 순서는 변경되지 않는다.\n\n::: {#3f5a0eb4 .cell execution_count=10}\n``` {.python .cell-code}\n# view()는 텐서의 모양을 변경할 때 사용한다.\n# 이때, 텐서(tensor)의 순서는 변경되지 않는다.\na = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])\nb = a.view(4, 2) # 4*2=8 개, # call by reference\nprint(b)\n\n# a의 값을 변경하면 b도 변경: 메모리 주소값을 공유\na[0] = 7\nprint(b)\n\n# a의 값을 복사(copy)한 뒤에 변경, 새로운 메모리값 할당\nc = a.clone().view(4, 2) # call by value, 아예 다른 객체\na[0] = 9\nprint(c)\n```\n:::\n\n\n#### <b>5) 텐서의 차원 교환</b>\n\n* 하나의 텐서에서 특정한 차원끼리 순서를 교체할 수 있다.\n\n::: {#05ff89d9 .cell execution_count=11}\n``` {.python .cell-code}\na = torch.rand((64, 32, 3))\nprint(a.shape)\n\nb = a.permute(2, 1, 0) # 차원을 바꿔줌\n# (2번째 축, 1번째 축, 0번째 축)의 형태가 되도록 한다.\nprint(b.shape)\n```\n:::\n\n\n### <b>4. 텐서의 연산과 함수</b>\n\n#### <b>1) 텐서의 연산</b>\n\n* 텐서에 대하여 사칙연산 등 기본적인 연산을 수행할 수 있다.\n\n::: {#43773eaf .cell execution_count=12}\n``` {.python .cell-code}\n# 같은 크기를 가진 두 개의 텐서에 대하여 사칙연산 가능\n# 기본적으로 요소별(element-wise) 연산, 행렬의 연산과 다름\na = torch.tensor([\n    [1, 2],\n    [3, 4]\n])\nb = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\nprint(a + b)\nprint(a - b)\nprint(a * b)\nprint(a / b)\n```\n:::\n\n\n* 행렬 곱을 수행할 수 있다.\n\n::: {#04d3b7c7 .cell execution_count=13}\n``` {.python .cell-code}\na = torch.tensor([\n    [1, 2],\n    [3, 4]\n])\nb = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\n# 행렬 곱(matrix multiplication) 수행\nprint(a.matmul(b))\nprint(torch.matmul(a, b))\n```\n:::\n\n\n#### <b>2) 텐서의 평균 함수</b>\n\n* 텐서의 평균(mean)을 계산할 수 있다.\n\n::: {#c363b734 .cell execution_count=14}\n``` {.python .cell-code}\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.mean()) # 전체 원소에 대한 평균\nprint(a.mean(dim=0)) # 각 열에 대하여 평균 계산\nprint(a.mean(dim=1)) # 각 행에 대하여 평균 계산\n\n```\n:::\n\n\n#### <b>3) 텐서의 합계 함수</b>\n\n* 텐서의 합계(sum)를 계산할 수 있다.\n\n::: {#9b928b5d .cell execution_count=15}\n``` {.python .cell-code}\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.sum()) # 전체 원소에 대한 합계\nprint(a.sum(dim=0)) # 각 열에 대하여 합계 계산\nprint(a.sum(dim=1)) # 각 행에 대하여 합계 계산\n```\n:::\n\n\n#### <b>4) 텐서의 최대 함수</b>\n\n* <b>max() 함수</b>는 원소의 최댓값을 반환한다.\n* <b>argmax() 함수</b>는 가장 큰 원소(최댓값)의 인덱스를 반환한다.\n\n::: {#dfab77ec .cell execution_count=16}\n``` {.python .cell-code}\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.max()) # 전체 원소에 대한 최댓값\nprint(a.max(dim=0)) # 각 열에 대하여 최댓값 계산\nprint(a.max(dim=1)) # 각 행에 대하여 최댓값 계산\n```\n:::\n\n\n::: {#895bd320 .cell execution_count=17}\n``` {.python .cell-code}\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.argmax()) # 전체 원소에 대한 최댓값의 인덱스\nprint(a.argmax(dim=0)) # 각 열에 대하여 최댓값의 인덱스 계산\nprint(a.argmax(dim=1)) # 각 행에 대하여 최댓값의 인덱스 계산\n```\n:::\n\n\n#### <b>5) 텐서의 차원 줄이기 혹은 늘리기</b>\n\n* <b>unsqueeze() 함수</b>는  크기가 1인 차원을 추가한다.\n  * 배치(batch) 차원을 추가하기 위한 목적으로 흔히 사용된다.\n* <b>squeeze() 함수</b>는 크기가 1인 차원을 제거한다.\n\n::: {#10a5f542 .cell execution_count=18}\n``` {.python .cell-code}\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a.shape)\n\n# 첫 번째 축에 차원 추가\na = a.unsqueeze(0)\nprint(a)\nprint(a.shape)\n\n# 네 번째 축에 차원 추가\na = a.unsqueeze(3)\nprint(a)\nprint(a.shape)\n```\n:::\n\n\n::: {#818001c5 .cell execution_count=19}\n``` {.python .cell-code}\n# 크기가 1인 차원 제거\na = a.squeeze()\nprint(a)\nprint(a.shape)\n```\n:::\n\n\n### <b>5. 자동 미분과 기울기(Gradient)</b>\n\n* PyTorch에서는 연산에 대하여 자동 미분을 수행할 수 있다.\n* 각 텐서 변수에 대해 gradient추적이 가능하여 텐서 연산에 각 텐서 변수의 기울기(민감도)를 추적할 수 있다.\n\n::: {#c8904b5a .cell execution_count=20}\n``` {.python .cell-code}\nimport torch\n\n# requires_grad를 설정할 때만 기울기 추적\nx = torch.tensor([3.0, 4.0], requires_grad=True)\ny = torch.tensor([1.0, 2.0], requires_grad=True)\nz = x + y #z를 연산하는데 x와 y의 민감도를 추적할 수 있다.\n# x or y의 민감도 즉 gradient가 크다는 것은 변수의 값이 조금만 바뀌어도 z값에 큰 영향을 미친다는것을 의미 \n\nprint(z) # [4.0, 6.0]\nprint(z.grad_fn) # 더하기(add), \n# AddBackward: 기울기를 구하는 과정에서 Add를 사용한다. 뭔뜻인지? ㅋ\n# Add를 연산하는 과정에서 기울기를 구하는거 아님?\n\nout = z.mean()\nprint(out) # 5.0\nprint(out.grad_fn) # 평균(mean)\n\nout.backward() # scalar에 대하여 모든 연산의 기울기를 추적 가능\nprint(x.grad) # tensor([0.5000, 0.5000]), 0.5: x의 값이 1만큼 바뀔 때 output값이 0.5만큼 바뀐다는것을 의미\nprint(y.grad) # tensor([0.5000, 0.5000]),\nprint(z.grad) # leaf variable에 대해서만 gradient 추적이 가능하다. 따라서 None.\n```\n:::\n\n\n* 일반적으로 모델을 학습할 때는 <b>기울기(gradient)를 추적</b>한다.\n  * 왜냐면, 가중치를 기울기에 따라 업데이트 해야하기 때문.\n* 하지만, 학습된 모델을 사용할 때는 파라미터를 업데이트하지 않으므로, 기울기를 추적하지 않는 것이 일반적이다.\n\n::: {#1a2f7b71 .cell execution_count=21}\n``` {.python .cell-code}\ntemp = torch.tensor([3.0, 4.0], requires_grad=True)# tape,라 부름. 왜?\nprint(temp.requires_grad)\nprint((temp ** 2).requires_grad)\n\n# 기울기 추적을 하지 않기 때문에 계산 속도가 더 빠르다.\nwith torch.no_grad():\n    temp = torch.tensor([3.0, 4.0], requires_grad=True)\n    print(temp.requires_grad)\n    print((temp ** 2).requires_grad)\n```\n:::\n\n\n",
    "supporting": [
      "2023-02-03_pytorch_introduction_files"
    ],
    "filters": [],
    "includes": {}
  }
}