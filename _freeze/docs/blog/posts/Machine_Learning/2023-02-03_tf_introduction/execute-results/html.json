{
  "hash": "41b330d5ff64f53ee4208962c39dbe24",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Tensor Introduction\nsubtitle: overview, object creation, indexing, concatenating, casting, shape, transpose, arithematic operations, matrix multiplication, mean, max, argmax, dimension manipulation, automatic differenctiation\ndescription: |\n  Learn how to manipulate Tensor flow, one of the most commonly used Python frameworks to implement machine learning algorithms using Python. 파이썬을 이용하여 머신러닝 알고리즘을 구현하기 위해 가장 대표적으로 쓰이는 파이썬 package중 하나인 Tensor flow조작법에 대해 알아본다. \ncategories:\n  - ML\nauthor: Kwangmin Kim\ndate: 02/03/2023\nformat: \n  html:\n    page-layout: full\n    code-fold: true\nexecute:\n  warning: false\n---\n\n## Tensor Flow\n\n* pytorch 이전 까지 deep learning을 위해 가장 많이 사용되었던 Framework\n* 2020년 이후로 pytorch를 더 많이 사용하지만 여전히 많은 사람들이 Tensor Flow 사용\n* 데이터 자료형으로 텐서(tensor) 객체를 사용\n* Tensorflow에서는 텐서(tensor)를 NumPy 배열처럼 사용할 수 있다.\n* GPU 사용 지원\n\n### GPU 사용 여부 체크하기\n\n* GPU를 사용하면 TensorFlow나 pytorch에서 딥러닝 모델을 효과적 구현 가능\n* 각 텐서(tensor)와 연산이 어떠한 장치에 할당되었는지 출력할 수 있다.\n\n::: {#5bc99c6d .cell execution_count=1}\n``` {.python .cell-code}\nimport tensorflow as tf\n# placement 함수: 각 텐서와 연산이 어떠한 장치에 할당되었는지 출력하기\n#tf.debugging.set_log_device_placement(True)\n\n# 텐서 생성\na = tf.constant([\n    [1, 1],\n    [2, 2]\n])\nb = tf.constant([\n    [5, 6],\n    [7, 8]\n])\n\nc = tf.matmul(a, b)\nprint(\"matrix multiplication: \", c)\n\n#tf.debugging.set_log_device_placement(False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmatrix multiplication:  tf.Tensor(\n[[12 14]\n [24 28]], shape=(2, 2), dtype=int32)\n```\n:::\n:::\n\n\n::: {#ec2f05be .cell execution_count=2}\n``` {.python .cell-code}\nfrom tensorflow.python.client import device_lib\n# 구체적으로 사용 중인 장치(device) 정보 출력\ndevice_lib.list_local_devices()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 7487076177939208079\n xla_global_id: -1]\n```\n:::\n:::\n\n\n### 텐서 소개 및 생성 방법\n\n* TensorFlow에서의 텐서(tensor)는 기능적으로 넘파이(NumPy)와 매우 유사하다.\n* 기본적으로 <b>다차원 배열</b>을 처리하기에 적합한 자료구조로 이해할 수 있다.\n* TensorFlow의 텐서는 \"자동 미분\" 기능을 제공한다.\n* TensorFlow는 기능적으로 Pytorch와 거의 같음, 하지만 문법이 불편함\n* TensorFlow 2.0부터는 pytorch와 문법적으로 유사\n\n### Tensor\n\n* 특징\n    * 기본적으로 <b>다차원 배열</b>을 처리하기에 적합한 자료구조로 이해할 수 있다\n    * TensorFlow에서의 텐서(tensor)는 기능적으로 넘파이(NumPy)의 ndarray 객체와 유사\n    * 기본 python 데이터 유형을 자동 변환 (e.g., list)\n    * TensorFlow의 텐서는 \"자동 미분\" 기능을 제공한다.\n* 속성\n    * 크기 (shape)\n    * 자료형 (data type)\n    * 저장된 장치, 가속기 메모리에 상주 가능 (e.g., GPU )\n* Numpy 배열과 tf.Tensor의 차이점\n    * 텐서는 가속기 메모리(GPU, TPU)에서 사용 가능\n    * 텐서는 불변성(immutable)\n\n### Tensor 초기화\n\n::: {#647be654 .cell execution_count=3}\n``` {.python .cell-code}\n# 기본적인 모양(shape), 자료형(data type) 출력\n\ndata = [\n    [1, 2],\n    [3, 4]\n]\nx = tf.constant(data) # list -> tensor object로 변환\nprint(x)\nprint(tf.rank(x)) # 축(axis)의 개수 출력 = 차원의 개수 출력\n\ndata = tf.constant(\"String\") # 문자열 (string)도 int형 tensor로 변환 가능\nprint(data)\n\n# NumPy 배열에서 텐서를 초기화할 수 있다.\n\n## 파이썬의 리스트 넘파이는 compatible하다. 상호보완적으로 교체가 가능\n\na = tf.constant([5])\nb = tf.constant([7])\n\nc = (a + b).numpy()\nprint(c)\nprint(type(c))\n\nresult = c * 10\ntensor = tf.convert_to_tensor(result) # numpy -> tensor\nprint(tensor)\nprint(type(tensor))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(b'String', shape=(), dtype=string)\n[12]\n<class 'numpy.ndarray'>\ntf.Tensor([120], shape=(1,), dtype=int32)\n<class 'tensorflow.python.framework.ops.EagerTensor'>\n```\n:::\n:::\n\n\n### 텐서(tensor) 객체 생성 (기본 python 데이터 유형)\n\n::: {#1c2c1f71 .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\n\nprint(tf.math.add(1, 2))\nprint(tf.math.add([1, 2], [3, 4]))\nprint(tf.math.square(5))\nprint(tf.math.reduce_sum([1, 2, 3]))\n\n# Operator overloading is also supported\nprint(tf.math.square(2) + tf.math.square(3))\n\ndata = [\n    [1,2],\n    [3,4]\n]\nx = tf.constant(data)\nprint(x)\nprint(x.shape)\nprint(x.dtype)\nprint(tf.rank(x)) # tf.rank() : 축(axis)의 개수 출력 (차원의 개수)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(3, shape=(), dtype=int32)\ntf.Tensor([4 6], shape=(2,), dtype=int32)\ntf.Tensor(25, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(13, shape=(), dtype=int32)\ntf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\n(2, 2)\n<dtype: 'int32'>\ntf.Tensor(2, shape=(), dtype=int32)\n```\n:::\n:::\n\n\n### 텐서(tensor) 객체 생성 (numpy)\n\n* TensorFlow 연산은 자동으로 NumPy 배열을 텐서(tensor)로 변환\n* NumPy 연산은 자동으로 텐서(tensor)를 NumPy 배열로 변환\n\n::: {#576bc67e .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nndarray = np.ones([3, 3])\nndarray\n\ntensor = tf.math.multiply(ndarray, 42)\ntensor\nnp.add(tensor, 1)\ntensor.numpy() # numpy.ndarray\ntype(tensor.numpy())\nctensor = tf.constant(ndarray)\nctensor\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<tf.Tensor: shape=(3, 3), dtype=float64, numpy=\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])>\n```\n:::\n:::\n\n\n### 다른 텐서로부터 텐서 초기화 \n\n* 텐서(tensor) 객체 생성 (tf.Tensor)\n* tf.ones_like(x) : 값이 1이고 x와 shape & data type이 동일한 텐서 생성\n\n::: {#1a62475e .cell execution_count=6}\n``` {.python .cell-code}\nx = tf.constant([\n    [5, 7],\n    [3, 2]\n])\n\nx_ones = tf.ones_like(x)\nx_ones\n     \n\nx = tf.constant([\n    [5.1, 7.0],\n    [3.4, 2.1]\n])\n\nx_ones = tf.ones_like(x)\nx_ones\n\n# tf.random.uniform(shape, dtype) : 랜덤 값으로 원하는 shape과 dtype을 갖는 텐서 생성\nx_rand = tf.random.uniform(shape=x.shape, dtype=tf.float32)\nx_rand\n\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[0.85156286, 0.8781513 ],\n       [0.10033619, 0.21846342]], dtype=float32)>\n```\n:::\n:::\n\n\n### 텐서(tensor) 사용\n\n특정 차원 접근\n\n::: {#fabe9361 .cell execution_count=7}\n``` {.python .cell-code}\ntensor = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8],\n    [9,10,11,12]\n])\n\nprint(tensor[0])       # first row\nprint(tensor[:, 0])    # first column\nprint(tensor[..., -1]) # last column\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\ntf.Tensor([1 5 9], shape=(3,), dtype=int32)\ntf.Tensor([ 4  8 12], shape=(3,), dtype=int32)\n```\n:::\n:::\n\n\n텐서 Concatenate\n\naxis : 어느 축을 기준으로 객체를 이어붙일지 결정\n\naxis=0 : 0번째 축 (=row)\naxis=1 : 1번째 축 (=column)\n\n::: {#293169c6 .cell execution_count=8}\n``` {.python .cell-code}\ntensor = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8],\n    [9,10,11,12]\n])\n\ntensor_concat = tf.concat([tensor, tensor, tensor], axis=0) # row\ntensor_concat\n\ntensor_concat = tf.concat([tensor, tensor, tensor], axis=1) # column\ntensor_concat\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n<tf.Tensor: shape=(3, 12), dtype=int32, numpy=\narray([[ 1,  2,  3,  4,  1,  2,  3,  4,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  5,  6,  7,  8,  5,  6,  7,  8],\n       [ 9, 10, 11, 12,  9, 10, 11, 12,  9, 10, 11, 12]], dtype=int32)>\n```\n:::\n:::\n\n\n#### 형변환 (Type Casting)\n\n::: {#a95f8fac .cell execution_count=9}\n``` {.python .cell-code}\na = tf.constant([2])   # dtype: int32\nb = tf.constant([5.0]) # dtype: float32\n\nprint('a dtype: ', a.dtype, '\\nb dtype: ', b.dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na dtype:  <dtype: 'int32'> \nb dtype:  <dtype: 'float32'>\n```\n:::\n:::\n\n\n::: {#c3d926e2 .cell execution_count=10}\n``` {.python .cell-code}\na + b # dtype 불일치 -> InvalidArgumentError 발생\n```\n:::\n\n\n::: {#74f27a57 .cell execution_count=11}\n``` {.python .cell-code}\ntf.cast(a, tf.float32) + b # a의 dtype을 b의 dtype으로 변환 후 계산\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n<tf.Tensor: shape=(1,), dtype=float32, numpy=array([7.], dtype=float32)>\n```\n:::\n:::\n\n\n#### 텐서 Shape 변경  \n\n::: {#44fbad33 .cell execution_count=12}\n``` {.python .cell-code}\nx = tf.Variable([1,2,3,4,5,6,7,8])\ny = tf.reshape(x, (4,2))           # row=4, col=2\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4],\n       [5, 6],\n       [7, 8]], dtype=int32)>\n```\n:::\n:::\n\n\n#### x와 y는 서로 다른 객체\n\n::: {#f27c7f94 .cell execution_count=13}\n``` {.python .cell-code}\nx.assign_add([1,1,1,1,1,1,1,1])\nprint(x) # 1씩 더해짐\nprint(y) # 값 변화 X\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(8,) dtype=int32, numpy=array([2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>\ntf.Tensor(\n[[1 2]\n [3 4]\n [5 6]\n [7 8]], shape=(4, 2), dtype=int32)\n```\n:::\n:::\n\n\n#### 텐서 차원 교환\n\n`tf.transpose(a, perm=[], ...)`\na의 차원 순서를 바꾼다.\nperm=[2, 1, 0]일 경우, a의 2번째 축을 첫번째로, 1번째 축을 두번째로, 0번째 축을 세번째로 교환하겠다는 의미\n\n::: {#0b483ac8 .cell execution_count=14}\n``` {.python .cell-code}\na = tf.random.uniform((64, 32, 3))\nprint(a.shape)\n\nb = tf.transpose(a, perm=[2, 1, 0]) # 차원 자체를 교환\nprint(b.shape)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(64, 32, 3)\n(3, 32, 64)\n```\n:::\n:::\n\n\n#### 사칙연산\n\nelement끼리 연산한다\n\n::: {#47de019d .cell execution_count=15}\n``` {.python .cell-code}\na = tf.constant([\n    [1,2],\n    [3,4]\n])\nb = tf.constant([\n    [1,2],\n    [3,4]\n])\n\nprint(a + b)\nprint(a - b)\nprint(a * b)\nprint(a / b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[2 4]\n [6 8]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[0 0]\n [0 0]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[ 1  4]\n [ 9 16]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float64)\n```\n:::\n:::\n\n\n#### 행렬 곱 (matrix multiplication)\n\n::: {#591f25f3 .cell execution_count=16}\n``` {.python .cell-code}\na = tf.constant([\n    [1,2],\n    [3,4]\n])\nb = tf.constant([\n    [1,2],\n    [3,4]\n])\ntf.matmul(a, b)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 7, 10],\n       [15, 22]], dtype=int32)>\n```\n:::\n:::\n\n\n#### 평균 함수 \n\n차원을 축소하며 평균을 계산\n\n* `tf.reduce_mean(a, axis=0)` : 0차원(행)을 축소하여 평균 계산 -> 각 열에 대한 평균\n* `tf.reduce_mean(a, axis=1)` : 1차원(열)을 축소하여 평균 계산 -> 각 행에 대한 평균\n\n::: {#6b319e84 .cell execution_count=17}\n``` {.python .cell-code}\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\n\nprint(tf.reduce_mean(a))         # a 전체 평균\nprint(tf.reduce_mean(a, axis=0)) # 각 column에 대한 평균\nprint(tf.reduce_mean(a, axis=1)) # 각 row에 대한 평균\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor([3 4 5 6], shape=(4,), dtype=int32)\ntf.Tensor([2 6], shape=(2,), dtype=int32)\n```\n:::\n:::\n\n\n#### 합계 함수\n\n차원을 축소하며 합계를 계산 (평균과 동일하게 동작)\n\n::: {#7dff02a3 .cell execution_count=18}\n``` {.python .cell-code}\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\n\nprint(tf.reduce_sum(a))         # a 전체 합계\nprint(tf.reduce_sum(a, axis=0)) # 각 column에 대한 합계\nprint(tf.reduce_sum(a, axis=1)) # 각 row에 대한 합계\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(36, shape=(), dtype=int32)\ntf.Tensor([ 6  8 10 12], shape=(4,), dtype=int32)\ntf.Tensor([10 26], shape=(2,), dtype=int32)\n```\n:::\n:::\n\n\n#### 최대 함수\n\n* max() : 원소의 최댓값 반환\n* argmax() : 최댓값의 index를 반환\n\n::: {#4d0bbefe .cell execution_count=19}\n``` {.python .cell-code}\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\n\nprint(tf.reduce_max(a))         # a 전체 원소의 최댓값\nprint(tf.reduce_max(a, axis=0)) # 각 column에 대한 최댓값\nprint(tf.reduce_max(a, axis=1)) # 각 row에 대한 최댓값\nprint(tf.argmax(a, axis=0)) # 각 column에 대한 최댓값의 index\nprint(tf.argmax(a, axis=1)) # 각 row에 대한 최댓값의 index\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(8, shape=(), dtype=int32)\ntf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\ntf.Tensor([4 8], shape=(2,), dtype=int32)\ntf.Tensor([1 1 1 1], shape=(4,), dtype=int64)\ntf.Tensor([3 3], shape=(2,), dtype=int64)\n```\n:::\n:::\n\n\n* 차원 축소\n  * squeeze() : 크기가 1인 차원을 제거\n* 차원 확장\n  * expand_dims() : 크기가 1인 차원을 추가\n  * 흔히 배치(batch) 차원을 추가하기 위한 목적으로 사용됨\n  * pytorch에서는 차원 축소 시, unsqueeze() 사용\n\n::: {#348abbf8 .cell execution_count=20}\n``` {.python .cell-code}\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\nprint('original a shape: ', a.shape)\n\na = tf.expand_dims(a, 0) # 첫번째 축에 차원 추가\nprint('add 0th dims: ', a.shape)\n\na = tf.expand_dims(a, 3) # 세번째 축에 차원 추가\nprint('add 3rd dims: ', a.shape)\n\n\nprint(tf.squeeze(a).shape)         # 크기가 1인 차원을 모두 제거 \nprint(tf.squeeze(a, axis=3).shape) # 세번째 차원을 제거\n\n\n#tf.squeeze(a, axis=1) # 제거하려는 차원의 크기가 1이 아닐 경우 오류 발생\n```\n\n::: {.cell-output .cell-output-stdout}\n```\noriginal a shape:  (2, 4)\nadd 0th dims:  (1, 2, 4)\nadd 3rd dims:  (1, 2, 4, 1)\n(2, 4)\n(1, 2, 4)\n```\n:::\n:::\n\n\n### 자동 미분과 기울기\n\n* 기울기 테이프 (Gradient Tape)\n* 중간 연산들을 테이프에 기록하고 역전파(back propagation)를 수행했을 때 기울기가 계산됨\n* TensorFlow에서는 변수가 아닌 상수에 대해 기본적으로 기울기를 측정하지 않음 (not watched). 또한 변수여도 학습 가능하지 않으면(not trainable) 자동 미분을 사용하지 않음\n\n::: {#ef2bb6a2 .cell execution_count=21}\n``` {.python .cell-code}\nx = tf.Variable([3.0, 4.0])\ny = tf.Variable([3.0, 4.0])\n\n# with 구문 안에서 진행되는 모든 연산들을 기록\nwith tf.GradientTape() as tape:\n  z = x + y\n  loss = tf.math.reduce_mean(z)\n\ndx = tape.gradient(loss, x) # loss가 scalar이므로 계산 가능\nprint(dx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([0.5 0.5], shape=(2,), dtype=float32)\n```\n:::\n:::\n\n\nTensorFlow에서는 변수가 아닌 상수에 대해 기본적으로 기울기를 측정하지 않음 (not watched). 또한 변수여도 학습 가능하지 않으면(not trainable) 자동 미분을 사용하지 않음\n\n::: {#070f3291 .cell execution_count=22}\n``` {.python .cell-code}\nx = tf.linspace(-10, 10, 100) # -10 ~ 10까지 100r개의 데이터 생성\n\nwith tf.GradientTape() as tape:\n  tape.watch(x) # x에 대해 기울기를 측정할거니까 기록해줘\n  y = tf.nn.sigmoid(x)\n\ndx = tape.gradient(y, x)\nprint(dx)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[4.53958077e-05 5.55575620e-05 6.79936937e-05 8.32130942e-05\n 1.01838442e-04 1.24631609e-04 1.52524715e-04 1.86658091e-04\n 2.28426653e-04 2.79536554e-04 3.42074339e-04 4.18591319e-04\n 5.12206458e-04 6.26731702e-04 7.66824507e-04 9.38173215e-04\n 1.14772200e-03 1.40394326e-03 1.71716676e-03 2.09997591e-03\n 2.56768332e-03 3.13889855e-03 3.83620191e-03 4.68693782e-03\n 5.72413978e-03 6.98759437e-03 8.52504404e-03 1.03935138e-02\n 1.26607241e-02 1.54065171e-02 1.87241696e-02 2.27213903e-02\n 2.75206964e-02 3.32587242e-02 4.00838615e-02 4.81513998e-02\n 5.76152215e-02 6.86149280e-02 8.12573764e-02 9.55919842e-02\n 1.11580066e-01 1.29060077e-01 1.47712989e-01 1.67034879e-01\n 1.86326443e-01 2.04710159e-01 2.21183725e-01 2.34711795e-01\n 2.44347497e-01 2.49363393e-01 2.49363393e-01 2.44347497e-01\n 2.34711795e-01 2.21183725e-01 2.04710159e-01 1.86326443e-01\n 1.67034879e-01 1.47712989e-01 1.29060077e-01 1.11580066e-01\n 9.55919842e-02 8.12573764e-02 6.86149280e-02 5.76152215e-02\n 4.81513998e-02 4.00838615e-02 3.32587242e-02 2.75206964e-02\n 2.27213903e-02 1.87241696e-02 1.54065171e-02 1.26607241e-02\n 1.03935138e-02 8.52504404e-03 6.98759437e-03 5.72413978e-03\n 4.68693782e-03 3.83620191e-03 3.13889855e-03 2.56768332e-03\n 2.09997591e-03 1.71716676e-03 1.40394326e-03 1.14772200e-03\n 9.38173215e-04 7.66824507e-04 6.26731702e-04 5.12206458e-04\n 4.18591319e-04 3.42074339e-04 2.79536554e-04 2.28426653e-04\n 1.86658091e-04 1.52524715e-04 1.24631609e-04 1.01838442e-04\n 8.32130942e-05 6.79936937e-05 5.55575620e-05 4.53958077e-05], shape=(100,), dtype=float64)\n```\n:::\n:::\n\n\n::: {#2dd12827 .cell execution_count=23}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.plot(x, y, 'r', label=\"y\")\nplt.plot(x, dx, 'b--', label=\"dy/dx\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2023-02-03_tf_introduction_files/figure-html/cell-24-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "2023-02-03_tf_introduction_files"
    ],
    "filters": [],
    "includes": {}
  }
}