{
  "hash": "df356c97a22a311d01c81a6009a9a219",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Eigenvalue\"\nsubtitle: 'Eigenvalue, Eigenvector, Eigendecomposition, Quadratic Form'\ndescription: |\n  Eigenvalues could be used for understanding characteristics or properties of a square matrix and determining what type of a quadratic form the matrix belongs to.\ncategories:\n  - Mathematics\nauthor: Kwangmin Kim\ndate: 05/27/2023\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\nexecute:\n  warning: false\n  meassage: false\ndraft: False\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(mvtnorm)\n```\n:::\n\n\n# Introduction\n\nEigenvalues could be used for understanding characteristics or properties of a square matrix and determining what type of a quadratic form the matrix belongs to.\n\n## Definition\n\n:::{#def-eigenvalue}\n## Eigenvalues and Eigenvectors\n\nLet $\\mathbf{A}$ be an $n × n$ square matrix. A scalar $\\lambda$ is called an eigenvalue of $\\mathbf{A}$ if there exists a non-zero vector $\\mathbf{v}$ such that\n$$\n\\mathbf{Av}=\\lambda\\mathbf{v}\n$$\n\nSuch a vector $\\mathbf{v}$ is called an eigenvector corresponding to $\\lambda$.\n:::\n\n\n\n## Properties\n\n* Eigenvalues are scalar values: Eigenvalues are scalars and represent the scaling factor by which the corresponding eigenvectors are stretched or shrunk when multiplied by a matrix.\n* When a matrix is ​​expressed in quadratic form, there exists a symmetric matrix that satisfies the uniqueness of the quadratic form and all eigenvalues ​​corresponding to the symmetric matrix having real numbers as elements are real numbers. Depending on the signs of the eigenvalues, a quadratic form can be classified into 5 types:\n  * $\\mathbf{A}$ is said to be a positive definite (PD) matrix if the eigenvalues of $\\mathbf{A}$ are all positive.\n  $$\n  \\mathbf{A} > 0 \\text{ iff } \\lambda_{\\text{min}}(\\mathbf{A}) > 0\n  $$\n  * $\\mathbf{A}$ is said to be a positive semi-definite (PSD) matrix if the eigenvalues of $\\mathbf{A}$ are not negative.\n  $$\n  \\mathbf{A} \\ge 0 \\text{ iff } \\lambda_{\\text{min}}(\\mathbf{A}) \\ge 0\n  $$\n  * $\\mathbf{A}$ is said to be a negative definite (ND) matrix if the eigenvalues of $\\mathbf{A}$ are all negative.\n  $$\n  \\mathbf{A} < 0 \\text{ iff } \\lambda_{\\text{max}}(\\mathbf{A}) < 0\n  $$\n  * $\\mathbf{A}$ is said to be a negative semi-definite (NSD) matrix if the eigenvalues of $\\mathbf{A}$ are not positive.\n  $$\n  \\mathbf{A} \\le 0 \\text{ iff } \\lambda_{\\text{max}}(\\mathbf{A}) \\le 0\n  $$\n  * $\\mathbf{A}$ is said to be a indefinite (NSD) matrix if the eigenvalues of $\\mathbf{A}$ have both signs.\n  $$\n  \\mathbf{A} \\lessgtr 0 \\text{ iff } \\lambda_{\\text{max}}(\\mathbf{A}) \\lessgtr 0\n  $$\n* Sum of eigenvalues: The sum of the eigenvalues of a matrix is equal to the trace of the matrix, where the trace is the sum of the diagonal elements. \n  $$\n  \\sum_{i=1}^n \\lambda_i = \\text{tr}(\\mathbf{A})\n  $$.\n* Product of eigenvalues: The product of the eigenvalues of a matrix is equal to the determinant of the matrix. \n  $$\n  \\prod_{i=1}^n \\lambda_i = \\det(\\mathbf{A})=|\\mathbf{A}|\n  $$.  \n* Eigenvalues are solutions to the characteristic equation: For a square matrix $\\mathbf{A}$ of size $n\\times n$, the eigenvalues $\\lambda$ satisfy the characteristic equation $\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0$, where $\\mathbf{I}$ is the identity matrix of size $n\\times n$.\n  * The characteristic equation is an equation associated with a square matrix that helps determine its eigenvalues, which are crucial for understanding the behavior and properties of the matrix $\\mathbf{A}$.\n  $$\n  \\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\n  $$\n  The equation indicates that the matrix $\\mathbf{A} - \\lambda\\mathbf{I}$ does not have full rank and has a nontrivial null space.\n* Eigenvalues of similar matrices: Similar matrices have the same eigenvalues. If two matrices $\\mathbf{A}$ and $\\mathbf{B}$ are similar (i.e., $\\mathbf{A} = \\mathbf{PBP}^{-1}$ for some invertible matrix $\\mathbf{P}$), then they have the same eigenvalues.\n* Eigenvalues of triangular matrices: The eigenvalues of a triangular matrix are equal to its diagonal entries. In other words, for an upper triangular matrix, the eigenvalues are the elements on its main diagonal.\n\n\n## Example\n\nLet $\\mathbf{A}$ be the matrix\n\n\nTo find the eigenvalues of $\\mathbf A$, we solve the characteristic equation $\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0$, where I is the n × n identity matrix.\n\n$$\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I ) \n  &= \n    \\begin{vmatrix} \n    3 - \\lambda & 1 \\\\ \n    1 & 3 - \\lambda \n    \\end{vmatrix} \\\\\n  &= \n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n$$\n\nSolving this quadratic equation gives us the eigenvalues of $\\mathbf A$: $\\lambda_1 = 2$ and $\\lambda_2 = 4$.\n\nTo find the eigenvectors corresponding to $\\lambda_1 = 2$, we solve the equation $(\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}$, where $\\mathbf I$ is the $2 \\times 2$ identity matrix.\n\n$$\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \n    \\begin{bmatrix} \n      1 & 1 \\\\ \n      1 & 1 \n    \\end{bmatrix}\n  \\begin{bmatrix} \n    x \\\\ \n    y \n  \\end{bmatrix} = \n  \\begin{bmatrix} \n    0 \\\\ \n    0 \n  \\end{bmatrix}\n\\end{align*}\n$$\n\nSolving this system of equations gives us the eigenvectors corresponding to $\\lambda_1 = 2$: $\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}$\n\nSimilarly, for $\\lambda_2 = 4$, we solve the equation $(\\mathbf A - 4\\mathbf I)\\mathbf{v}$ = $\\mathbf{0}$ to get the eigenvectors corresponding to $\\lambda_2 = 4$: $\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n\n::: {.panel-tabset}\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPD <-matrix(c(1,0,0,1),ncol=2,byrow = TRUE)\nPSD <- matrix(c(1,1,1,1),ncol=2,byrow = TRUE)\nND <- matrix(c(-1,0,0,-1),ncol=2,byrow = TRUE)\nNSD <- matrix(c(-1,-1,-1,-1),ncol=2,byrow = TRUE)\nInd <- matrix(c(1,0,0,1),ncol=2,byrow = TRUE)\n\nsym_mat <-matrix(c(6,-4,12,-4,6,-2,12,-2,3),ncol=3,byrow = TRUE)\neigen(sym_mat)$values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18.138954  4.556363 -7.695317\n```\n\n\n:::\n\n```{.r .cell-code}\n# trace\ndiag(sym_mat)%>%sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(eigen(sym_mat)$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15\n```\n\n\n:::\n\n```{.r .cell-code}\n# product of eigenvalues : more convenient than using a for loop\nexp(sum(log(eigen(sym_mat)$values)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] NaN\n```\n\n\n:::\n\n```{.r .cell-code}\ndet(sym_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -636\n```\n\n\n:::\n:::\n\n\n \n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\n\n# Define the matrix A\nA = np.array([[2, 1], [1, 3]])\n\n# Calculate the eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Print the eigenvalues\nprint(\"Eigenvalues:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEigenvalues:\n```\n\n\n:::\n\n```{.python .cell-code}\nfor eigenvalue in eigenvalues:\n    print(eigenvalue)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1.381966011250105\n3.618033988749895\n```\n\n\n:::\n:::\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}