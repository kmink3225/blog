{
  "hash": "4d58e41e63241dab787f44378969fd11",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Hugging Face: PLM 생태계의 중심\"\nsubtitle: \"실무에서 바로 사용할 수 있는 사전 학습 모델의 허브\"\ndescription: |\n  Hugging Face는 현재 NLP 분야에서 가장 중요한 라이브러리이자 플랫폼이다. 수만 개의 사전 학습 모델을 제공하며, 몇 줄의 코드만으로 최신 PLM을 활용할 수 있게 해준다. 토크나이저부터 파인튜닝, 배포까지 전체 ML 워크플로우를 지원하는 Hugging Face의 핵심 기능들과 실무 활용 전략을 상세히 분석한다.\ncategories:\n  - NLP\n  - Deep Learning\nauthor: Kwangmin Kim\ndate: 2025-01-27\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n  eval: false\n---\n\n# KorNLI 분류\n\n데이터 셋: https://github.com/kakaobrain/kor-nlu-datasets\n\n# 데이터셋 로드 및 구조 확인\n\n::: {#dfc15c9b .cell execution_count=1}\n``` {.python .cell-code}\nfrom datasets import load_dataset\n\ncs = load_dataset(\"klue\", \"nli\", split=\"train\")\ncs = cs.train_test_split(0.1)\ntest_cs = load_dataset(\"klue\", \"nli\", split=\"validation\")\ntrain_cs = cs[\"train\"]\nvalid_cs = cs[\"test\"]\n\ntrain_cs\nvalid_cs\ntest_cs\n```\n:::\n\n\n# 전처리\n\n::: {#02fa1ad0 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport datetime\nfrom tqdm import tqdm\n\nimport csv\nimport os\n\nimport tensorflow as tf\nimport torch\n\n# BERT 사용을 위함\nfrom transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# for padding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences \n\n# 전처리 및 평가 지표\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score, hamming_loss\n```\n:::\n\n\n훈련 데이터, 검증 데이터, 테스트 데이터에 대해서 \\[CLS\\] 문장 \\[SEP\\] 구조를 만듭니다. \\[CLS\\]는 분류를 하기 위해 BERT가 사용하는 첫번째 입력 토큰이며, \\[SEP\\]는 입력 문장의 종료를 나타내기 위해 사용하는 스페셜 토큰입니다.\n\n::: {#88a4f495 .cell execution_count=3}\n``` {.python .cell-code}\n# 훈련 데이터, 검증 데이터, 테스트 데이터에 대해서 `[CLS] 문장 [SEP]` 구조를 만듭니다.\n\ntrain_sentences = list(map(lambda train_cs: '[CLS] ' + str(train_cs['premise']) + ' [SEP] ' + str(train_cs['hypothesis']) + ' [SEP]', train_cs))\nvalidation_sentences = list(map(lambda valid_cs: '[CLS] ' + str(valid_cs['premise']) + ' [SEP] ' + str(valid_cs['hypothesis']) + ' [SEP]', valid_cs))\ntest_sentences = list(map(lambda test_cs: '[CLS] ' + str(test_cs['premise']) + ' [SEP] ' + str(test_cs['hypothesis']) + ' [SEP]', test_cs))\n\ntrain_labels = train_cs['label']\nvalidation_labels = valid_cs['label']\ntest_labels = test_cs['label']\n\ntest_sentences[:5]\ntest_labels[:5]\n```\n:::\n\n\n# BERT 토크나이저를 이용한 전처리\n\nBERT를 사용하기 위해서는 토크나이저와 모델이 반드시 맵핑 관계여야만 합니다. 다시 말해 아래의 이름에 들어가는 모델이름은 반드시 동일해야 합니다.\n\n-   `BertTokenizer.from_pretrained('모델이름')`\n-   `BertForSequenceClassification.from_pretrained(\"모델이름\")`\n\n토크나이저는 내부적으로 Vocabulary를 갖고 있어 정수 인코딩을 수행해주는 모듈입니다.\n\n::: {#675ed9c0 .cell execution_count=4}\n``` {.python .cell-code}\n# 한국어 BERT 중 하나인 'klue/bert-base'를 사용.\ntokenizer = BertTokenizer.from_pretrained('klue/bert-base')\ntokenized_text = tokenizer.tokenize('안녕하세요. 자연어 처리를 배울거에요.')\ninput_id = tokenizer.convert_tokens_to_ids(tokenized_text)\n\nprint('토큰화 된 문장 :', tokenized_text)\nprint('정수 인코딩 된 문장 :', input_id)\n\nMAX_LEN = 128\n\ndef data_to_tensor (sentences, labels):\n  # 정수 인코딩 과정. 각 텍스트를 토큰화한 후에 Vocabulary에 맵핑되는 정수 시퀀스로 변환한다.\n  # ex) ['안녕하세요'] ==> ['안', '녕', '하세요'] ==> [231, 52, 45]\n  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\n  # pad_sequences는 패딩을 위한 모듈. 주어진 최대 길이를 위해서 뒤에서 0으로 채워준다.\n  # ex) [231, 52, 45] ==> [231, 52, 45, 0, 0, 0]\n  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\") \n\n  attention_masks = []\n\n  for seq in input_ids:\n      seq_mask = [float(i > 0) for i in seq]\n      attention_masks.append(seq_mask)\n\n  tensor_inputs = torch.tensor(input_ids)\n  tensor_labels = torch.tensor(labels)\n  tensor_masks = torch.tensor(attention_masks)\n\n  return tensor_inputs, tensor_labels, tensor_masks\n\ntrain_inputs, train_labels, train_masks = data_to_tensor(train_sentences, train_labels)\nvalidation_inputs, validation_labels, validation_masks = data_to_tensor(validation_sentences, validation_labels)\ntest_inputs, test_labels, test_masks = data_to_tensor(test_sentences, test_labels)\n\ntokenizer.decode([2])\ntokenizer.decode([3])\ntest_inputs[0]\ntokenizer.decode(test_inputs[0])\n\n```\n:::\n\n\n훈련 데이터, 검증 데이터, 텍스트 데이터에 대해서 data_to_tensor 함수를 통해서 정수 인코딩 된 데이터, 레이블, 어텐션 마스크를 얻습니다.\n\n배치 크기는 32로 하고 파이토치의 데이터로더(배치 단위로 데이터를 꺼내올 수 있도록 하는 모듈)로 변환합니다.\n\n::: {#0a3e5cf5 .cell execution_count=5}\n``` {.python .cell-code}\nbatch_size = 32\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n```\n:::\n\n\n# GPU가 정상 셋팅되었는지 확인.\n\nColab에서 GPU를 사용하기 위해서는 아래와 같이 설정이 되어있어야만 합니다.\n\n런타임 \\> 런타임 유형 변경 \\> 하드웨어 가속기 \\> 'GPU' 선택\n\n::: {#eac8be45 .cell execution_count=6}\n``` {.python .cell-code}\nif torch.cuda.is_available():    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")\n    print('No GPU available, using the CPU instead.')\n```\n:::\n\n\n# 모델로드\n\nBERT를 사용하여 텍스트를 분류하는 BERT 아키텍처는 BertForSequenceClassification.from_pretrained(\"모델이름\")을 넣어서 가능합니다. 레이블 수로 num_labels라는 인자값에 레이블의 수를 기재해줍니다.\n\n::: {#620eea60 .cell execution_count=7}\n``` {.python .cell-code}\nnum_labels = 3\n\nmodel = BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=num_labels)\nmodel.cuda()\n\n# 옵티마이저 선택\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5,\n                  eps = 1e-8\n                )\n# 몇 번의 에포크(전체 데이터에 대한 학습 횟수)를 할 것인지 선택\nepochs = 2\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))  # hh:mm:ss\n\ndef metrics(predictions, labels):\n    y_pred = predictions\n    y_true = labels\n\n    # 사용 가능한 메트릭들을 사용한다.\n    accuracy = accuracy_score(y_true, y_pred)\n    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n    f1_weighted_average = f1_score(y_true=y_true, y_pred=y_pred, average='weighted', zero_division=0)\n\n    # 메트릭 결과에 대해서 리턴\n    metrics = {'accuracy': accuracy,\n               'f1_macro': f1_macro_average,\n               'f1_micro': f1_micro_average,\n               'f1_weighted': f1_weighted_average}\n\n    return metrics\n```\n:::\n\n\n# 모델 학습\n\n::: {#ea707b0d .cell execution_count=8}\n``` {.python .cell-code}\n# 랜덤 시드값.\nseed_val = 777\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\nmodel.zero_grad()\nfor epoch_i in range(0, epochs):\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    t0 = time.time()\n    total_loss = 0\n\n    model.train()\n\n    for step, batch in tqdm(enumerate(train_dataloader)):\n        if step % 500 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask, \n                        labels=b_labels)\n        \n        loss = outputs[0]\n        total_loss += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # gradient clipping if it is over a threshold\n        optimizer.step()\n        scheduler.step()\n\n        model.zero_grad()\n\n    avg_train_loss = total_loss / len(train_dataloader)            \n\n    print(\"\")\n    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n```\n:::\n\n\n# 검증 데이터에 대한 평가\n\n::: {#728a7f59 .cell execution_count=9}\n``` {.python .cell-code}\nt0 = time.time()\nmodel.eval()\naccum_logits, accum_label_ids = [], []\n\nfor batch in validation_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n\n    with torch.no_grad():\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    for b in logits:\n        # 3개의 값 중 가장 큰 값을 예측한 인덱스로 결정\n        # ex) [ 3.5134246  -0.30875662 -2.111316  ] ==> 0\n        accum_logits.append(np.argmax(b))\n\n    for b in label_ids:\n        accum_label_ids.append(b)\n\naccum_logits = np.array(accum_logits)\naccum_label_ids = np.array(accum_label_ids)\nresults = metrics(accum_logits, accum_label_ids)\n\nprint(\"Accuracy: {0:.4f}\".format(results['accuracy']))\nprint(\"F1 (Macro) Score: {0:.4f}\".format(results['f1_macro']))\nprint(\"F1 (Micro) Score: {0:.4f}\".format(results['f1_micro']))\nprint(\"F1 (Weighted) Score: {0:.4f}\".format(results['f1_weighted']))\n```\n:::\n\n\n# 모델 저장과 로드\n\n::: {#09446fb0 .cell execution_count=10}\n``` {.python .cell-code}\n%pwd\n\n# 폴더 생성\n%mkdir model\n# 모델 저장\ntorch.save(model.state_dict(), path+\"BERT_kornli.pt\")\n\n# 모델 로드\nmodel.load_state_dict(torch.load(path+\"BERT_kornli.pt\"))\n```\n:::\n\n\n# 테스트 데이터에 대한 평가\n\n::: {#0201eca7 .cell execution_count=11}\n``` {.python .cell-code}\nt0 = time.time()\nmodel.eval()\naccum_logits, accum_label_ids = [], []\n\nfor step, batch in tqdm(enumerate(test_dataloader)):\n    if step % 100 == 0 and not step == 0:\n        elapsed = format_time(time.time() - t0)\n        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n\n    with torch.no_grad():\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    \n    for b in logits:\n        # 3개의 값 중 가장 큰 값을 예측한 인덱스로 결정\n        # ex) [ 3.5134246  -0.30875662 -2.111316  ] ==> 0\n        accum_logits.append(np.argmax(b))\n\n    for b in label_ids:\n        accum_label_ids.append(b)\n\naccum_logits = np.array(accum_logits)\naccum_label_ids = np.array(accum_label_ids)\nresults = metrics(accum_logits, accum_label_ids)\n\nprint(\"Accuracy: {0:.4f}\".format(results['accuracy']))\nprint(\"F1 (Macro) Score: {0:.4f}\".format(results['f1_macro']))\nprint(\"F1 (Micro) Score: {0:.4f}\".format(results['f1_micro']))\nprint(\"F1 (Weighted) Score: {0:.4f}\".format(results['f1_weighted']))\n```\n:::\n\n\n# 예측\n\n::: {#ac6fdeca .cell execution_count=12}\n``` {.python .cell-code}\nfrom transformers import pipeline\n\npipe = pipeline(\"text-classification\", model=model.cuda(), tokenizer=tokenizer, device=0, max_length=512,\n                return_all_scores=True, function_to_apply='softmax')\ninputs = {\"text\" : \"흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.\", \"text_pair\" : \"어떤 방에서도 흡연은 금지됩니다.\"}\nresult = pipe([inputs])\nprint(result)\n```\n:::\n\n\nreturn_all_scores를 제거하면 정답으로 확신하는 레이블만 리턴합니다.\n\n::: {#d495b1ca .cell execution_count=13}\n``` {.python .cell-code}\n# return_all_scores 제거\npipe = pipeline(\"text-classification\", model=model.cuda(), tokenizer=tokenizer, device=0, max_length=512, function_to_apply='softmax')\nresult = pipe([inputs])\nprint(result)\n```\n:::\n\n\n한글로 된 레이블 예측을 얻기 위해서 label_dict를 만듭니다.\n\n::: {#b8516bc9 .cell execution_count=14}\n``` {.python .cell-code}\nlabel_dict = {'LABEL_0' : '얽힘', 'LABEL_1' : '중립', 'LABEL_2' : '모순'}\ndef prediction(sent1, sent2):\n  text = {\"text\" : sent1, \"text_pair\" : sent2}\n  result = pipe(text)\n  return [label_dict[result['label']]]\n\nsent1 = \"흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.\"\nsent2 = \"어떤 방에서도 흡연은 금지됩니다\"\n\nprediction(sent1, sent2)\n\nsent1 = \"저는, 그냥 알아내려고 거기 있었어요.\"\nsent2 = \"나는 돈이 어디로 갔는지 이해하려고 했어요.\"\n\nprediction(sent1, sent2)\n\nsent1 = \"저는 그것을 이해하려고 거기 있었어요.\"\nsent2 = \"저는 이해하려고 노력하고 있었어요.\"\n\nprediction(sent1, sent2)\n```\n:::\n\n\n",
    "supporting": [
      "29.plm_multi_class_files"
    ],
    "filters": [],
    "includes": {}
  }
}