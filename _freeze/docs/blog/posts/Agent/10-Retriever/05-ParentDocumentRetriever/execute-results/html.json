{
  "hash": "53895276cda2c8e35ffbdbff5a923674",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Parent Document Retriever\"\nsubtitle: \"계층적 문서 구조를 활용한 RAG 최적화\"\ndescription: |\n  문서 청킹의 딜레마(정확한 임베딩 vs 충분한 맥락)를 해결하기 위해 ParentDocumentRetriever가 작은 청크로 검색하고 큰 청크를 반환하는 계층적 검색 전략을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 12/31/2024\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n## RAG의 청킹 딜레마\n\n### 문제: 상충되는 두 가지 요구사항\n\nRAG 시스템에서 문서를 청크(chunk)로 분할할 때 다음의 **트레이드오프**에 직면하게 된다.\n\n**요구사항 1: 작은 청크 (정확한 임베딩)**\n- **목적**: 임베딩이 청크의 의미를 정확하게 표현\n- **이유**: 긴 텍스트의 임베딩은 다양한 주제가 섞여 의미가 희석된다\n- **장점**: 검색 정확도 향상\n- **단점**: 맥락 손실\n\n**요구사항 2: 큰 청크 (충분한 맥락)**\n- **목적**: LLM이 답변 생성 시 필요한 맥락 제공\n- **이유**: 작은 청크만으로는 질문에 답하기 어려울 수 있다\n- **장점**: 풍부한 맥락 정보\n- **단점**: 임베딩 품질 저하\n\n**청킹 딜레마 예시**\n\n| 청크 크기 | 임베딩 품질 | 맥락 유지 | 검색 정확도 | LLM 답변 품질 |\n|:---------|:----------|:---------|:-----------|:------------|\n| **작음 (200자)** | ✅ 높음 | ❌ 낮음 | ✅ 높음 | ❌ 낮음 |\n| **중간 (500자)** | 중간 | 중간 | 중간 | 중간 |\n| **큼 (1000자)** | ❌ 낮음 | ✅ 높음 | ❌ 낮음 | ✅ 높음 |\n\n### 해결책: ParentDocumentRetriever\n\n**핵심 아이디어: \"작게 검색하고, 크게 반환한다\"**\n\n`ParentDocumentRetriever`는 계층적 문서 구조를 활용하여 이 딜레마를 해결한다.\n\n**작동 원리**\n\n1. **문서 분할 (Splitting)**\n   - 원본 문서를 큰 청크(Parent)로 분할\n   - 큰 청크를 다시 작은 청크(Child)로 분할\n\n2. **인덱싱 (Indexing)**\n   - 작은 청크(Child)만 벡터 DB에 임베딩\n   - 큰 청크(Parent)는 별도 문서 저장소(Docstore)에 보관\n   - Child와 Parent 간 ID로 연결\n\n3. **검색 (Retrieval)**\n   - 작은 청크로 유사도 검색 (정확한 매칭)\n   - 검색된 작은 청크의 Parent ID 확인\n   - 큰 청크(Parent)를 LLM에 전달\n\n**계층 구조 예시**\n\n```\n원본 문서 (5000자)\n├── Parent Chunk 1 (1000자)\n│   ├── Child Chunk 1-1 (200자) ← 벡터 DB에 저장\n│   ├── Child Chunk 1-2 (200자) ← 벡터 DB에 저장\n│   └── Child Chunk 1-3 (200자) ← 벡터 DB에 저장\n├── Parent Chunk 2 (1000자)\n│   ├── Child Chunk 2-1 (200자)\n│   └── Child Chunk 2-2 (200자)\n...\n```\n\n**검색 프로세스**\n\n```\n사용자 질문: \"Word2Vec이란?\"\n         ↓\n벡터 검색 (Child Chunk에서)\n         ↓\nChild Chunk 1-2 발견 (유사도: 0.95)\n         ↓\nParent ID 확인 → Parent Chunk 1\n         ↓\nParent Chunk 1 (1000자) 반환\n         ↓\nLLM에 충분한 맥락 제공\n```\n\n### 핵심 장점\n\n**1. 검색 정확도 향상**\n- 작은 청크로 검색하므로 임베딩이 명확한 의미 표현\n- 노이즈가 적어 관련 문서를 정확히 찾음\n\n**2. 맥락 보존**\n- 큰 청크를 반환하므로 충분한 맥락 제공\n- LLM이 더 정확한 답변 생성 가능\n\n**3. 유연한 구조**\n- Parent 크기를 조절하여 맥락 범위 조정 가능\n- Child 크기를 조절하여 검색 정밀도 조정 가능\n\n**4. 저장 공간 효율**\n- Child만 벡터화하므로 벡터 DB 크기 절감\n- Parent는 일반 문서 저장소에 보관\n\n## 환경 설정\n\n\n여러 개의 텍스트 파일을 로드하기 위해 `TextLoader` 객체를 생성하고 데이터를 로드합니다.\n\n::: {#ffc2a3b0 .cell execution_count=2}\n``` {.python .cell-code}\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n```\n:::\n\n\n::: {#d5427f6f .cell execution_count=3}\n``` {.python .cell-code}\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH10-Retriever\")\n```\n:::\n\n\n::: {#03a65811 .cell execution_count=4}\n``` {.python .cell-code}\nfrom langchain.storage import InMemoryStore\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.retrievers import ParentDocumentRetriever\n```\n:::\n\n\n::: {#87ef919d .cell execution_count=5}\n``` {.python .cell-code}\nloaders = [\n    # 파일을 로드합니다.\n    TextLoader(\"./data/appendix-keywords.txt\"),\n]\n\ndocs = []\nfor loader in loaders:\n    # 로더를 사용하여 문서를 로드하고 docs 리스트에 추가합니다.\n    docs.extend(loader.load())\n```\n:::\n\n\n## 실습 1: 전체 문서 검색\n\n### 전략: Child만 분할, Parent는 원본\n\n이 모드에서는 `parent_splitter`를 지정하지 않고 `child_splitter`만 사용한다.\n\n**동작 방식**\n- Child: 200자 청크로 분할 → 벡터 DB 저장\n- Parent: 원본 문서 그대로 → Docstore 저장\n- 검색: Child로 찾고 → 원본 문서 전체 반환\n\n**장점**: 전체 문서 맥락 유지\n**단점**: 문서가 너무 길면 LLM 컨텍스트 낭비\n\n::: {#9d0ce0d5 .cell execution_count=6}\n``` {.python .cell-code}\n# 자식 분할기를 생성합니다.\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n\n# DB를 생성합니다.\nvectorstore = Chroma(\n    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n)\n\nstore = InMemoryStore()\n\n# Retriever 를 생성합니다.\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\n```\n:::\n\n\n`retriever.add_documents(docs, ids=None)` 함수로 문서목록을 추가합니다.\n\n- `ids` 가 `None` 이면 자동으로 생성됩니다.\n- `add_to_docstore=False` 로 설정시 document 를 중복으로 추가하지 않습니다. 단, 중복을 체크하기 위한 `ids` 값이 필수 값으로 요구됩니다.\n\n::: {#a0c51967 .cell execution_count=7}\n``` {.python .cell-code}\n# 문서를 검색기에 추가합니다. docs는 문서 목록이고, ids는 문서의 고유 식별자 목록입니다.\nretriever.add_documents(docs, ids=None, add_to_docstore=True)\n```\n:::\n\n\n이 코드는 두 개의 키를 반환해야 합니다. 그 이유는 우리가 두 개의 문서를 추가했기 때문입니다.\n\n- `store` 객체의 `yield_keys()` 메서드를 호출하여 반환된 키(key) 값들을 리스트로 변환합니다.\n\n::: {#b78a877c .cell execution_count=8}\n``` {.python .cell-code}\n# 저장소의 모든 키를 리스트로 반환합니다.\nlist(store.yield_keys())\n```\n:::\n\n\n```\n['c2a89a0f-a690-4915-af68-2ea432fb6e51']\n```\n\n이제 벡터 스토어 검색 기능을 호출해 보겠습니다.\n\n우리가 작은 청크(chunk)들을 저장하고 있기 때문에, 검색 결과로 작은 청크들이 반환되는 것을 확인할 수 있을 것입니다.\n\n`vectorstore` 객체의 `similarity_search` 메서드를 사용하여 유사도 검색을 수행합니다.\n\n::: {#1fa0533a .cell execution_count=9}\n``` {.python .cell-code}\n# 유사도 검색을 수행합니다.\nsub_docs = vectorstore.similarity_search(\"Word2Vec\")\n```\n:::\n\n\n`sub_docs[0].page_content`를 출력합니다.\n\n::: {#2c008c30 .cell execution_count=10}\n``` {.python .cell-code}\n# sub_docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.\nprint(sub_docs[0].page_content)\n```\n:::\n\n\n```\n정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n```\n\n이제 전체 retriever에서 검색해 보겠습니다. 이 과정에서는 작은 청크(chunk)들이 위치한 **문서를 반환** 하기 때문에 상대적으로 큰 문서들이 반환될 것입니다.\n\n`retriever` 객체의 `invoke()` 메서드를 사용하여 쿼리와 관련된 문서를 검색합니다.\n\n::: {#95f714ce .cell execution_count=11}\n``` {.python .cell-code}\n# 문서를 검색하여 가져옵니다.\nretrieved_docs = retriever.invoke(\"Word2Vec\")\n```\n:::\n\n\n검색된 문서(`retrieved_docs[0]`)의 일부 내용을 출력합니다.\n\n::: {#ba4f8496 .cell execution_count=12}\n``` {.python .cell-code}\n# 검색된 문서의 문서의 페이지 내용의 길이를 출력합니다.\nprint(\n    f\"문서의 길이: {len(retrieved_docs[0].page_content)}\",\n    end=\"\\n\\n=====================\\n\\n\",\n)\n\n# 문서의 일부를 출력합니다.\nprint(retrieved_docs[0].page_content[2000:2500])\n```\n:::\n\n\n**결과 분석**\n\n| 항목 | Child 검색 | Parent 반환 |\n|:----|:---------|:-----------|\n| **크기** | 200자 내외 | 5733자 (원본 전체) |\n| **내용** | Word2Vec 정의만 | 전체 키워드 사전 |\n| **장점** | 정확한 매칭 | 풍부한 맥락 |\n| **단점** | 맥락 부족 | 불필요한 정보 포함 |\n\n**문제점**: 원본 문서가 5733자로 너무 길어 LLM에 전달 시 비효율적이다. 다음 섹션에서 Parent 크기를 조절하여 이를 해결한다.\n\n```\n문서의 길이: 5733\n\n=====================\n\n 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\n연관키워드: 혁신, 기술, 비즈니스 모델\n\nCrawling\n\n정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n\nWord2Vec\n\n정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\nLLM (Large Language Model)\n\n정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을\n```\n\n## 실습 2: 계층적 청킹 (권장 방식)\n\n### 전략: Parent와 Child 모두 분할\n\n이전 방식의 문제점을 해결하기 위해 **2단계 청킹**을 적용한다.\n\n**분할 전략**\n1. **1단계**: 원본 문서 → Parent Chunk (1000자)\n2. **2단계**: Parent Chunk → Child Chunk (200자)\n3. **저장**: Child만 벡터화, Parent는 Docstore\n4. **검색**: Child로 찾고 → Parent 반환 (1000자)\n\n**크기 비교**\n\n| 방식 | Child 크기 | Parent 크기 | Parent 개수 | 장점 |\n|:-----|:----------|:-----------|:-----------|:-----|\n| **실습 1** | 200자 | 5733자 (원본) | 1개 | 전체 맥락 | \n| **실습 2** | 200자 | 1000자 | 7개 | 적절한 맥락 + 효율성 |\n\n**왜 1000자인가?**\n- 대부분의 LLM은 4-8K 토큰 컨텍스트 사용\n- 1000자 ≈ 250 토큰 (영문 기준)\n- 4-5개 청크 = 1000-1250 토큰 (적정 범위)\n- 충분한 맥락 + 효율적인 토큰 사용\n\n### 코드 구현\n\n`RecursiveCharacterTextSplitter`를 사용하여 부모 문서와 자식 문서를 생성한다.\n- 부모 문서는 `chunk_size`가 1000으로 설정되어 있다\n- 자식 문서는 `chunk_size`가 200으로 설정되어 있으며, 부모 문서보다 작은 크기로 생성된다\n\n::: {#b10ca090 .cell execution_count=13}\n``` {.python .cell-code}\n# 부모 문서를 생성하는 데 사용되는 텍스트 분할기입니다.\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n# 자식 문서를 생성하는 데 사용되는 텍스트 분할기입니다.\n# 부모보다 작은 문서를 생성해야 합니다.\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소입니다.\nvectorstore = Chroma(\n    collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings()\n)\n# 부모 문서의 저장 계층입니다.\nstore = InMemoryStore()\n```\n:::\n\n\n`ParentDocumentRetriever`를 초기화하는 코드입니다.\n\n- `vectorstore` 매개변수는 문서 벡터를 저장하는 벡터 저장소를 지정합니다.\n- `docstore` 매개변수는 문서 데이터를 저장하는 문서 저장소를 지정합니다.\n- `child_splitter` 매개변수는 하위 문서를 분할하는 데 사용되는 문서 분할기를 지정합니다.\n- `parent_splitter` 매개변수는 상위 문서를 분할하는 데 사용되는 문서 분할기를 지정합니다.\n\n`ParentDocumentRetriever`는 계층적 문서 구조를 처리하며, 상위 문서와 하위 문서를 별도로 분할하고 저장합니다. 이를 통해 검색 시 상위 문서와 하위 문서를 효과적으로 활용할 수 있습니다.\n\n::: {#96511375 .cell execution_count=14}\n``` {.python .cell-code}\nretriever = ParentDocumentRetriever(\n    # 벡터 저장소를 지정합니다.\n    vectorstore=vectorstore,\n    # 문서 저장소를 지정합니다.\n    docstore=store,\n    # 하위 문서 분할기를 지정합니다.\n    child_splitter=child_splitter,\n    # 상위 문서 분할기를 지정합니다.\n    parent_splitter=parent_splitter,\n)\n```\n:::\n\n\n`retriever` 객체에 `docs`를 추가합니다. `retriever`가 검색할 수 있는 문서 집합에 새로운 문서들을 추가하는 역할을 합니다.\n\n::: {#87ca9fcb .cell execution_count=15}\n``` {.python .cell-code}\nretriever.add_documents(docs)  # 문서를 retriever에 추가합니다.\n```\n:::\n\n\n이제 문서의 수가 훨씬 더 많아진 것을 볼 수 있습니다. 이는 더 큰 청크(chunk)들입니다.\n\n::: {#8add1dfa .cell execution_count=16}\n``` {.python .cell-code}\n# 저장소에서 키를 생성하고 리스트로 변환한 후 길이를 반환합니다.\nlen(list(store.yield_keys()))\n```\n:::\n\n\n``` \n7\n```\n\n기본 벡터 저장소가 여전히 작은 청크를 검색하는지 확인해 보겠습니다.\n\n`vectorstore` 객체의 `similarity_search` 메서드를 사용하여 유사도 검색을 수행합니다.\n\n::: {#d6fac344 .cell execution_count=17}\n``` {.python .cell-code}\n# 유사도 검색을 수행합니다.\nsub_docs = vectorstore.similarity_search(\"Word2Vec\")\n# sub_docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.\nprint(sub_docs[0].page_content)\n```\n:::\n\n\n```\n정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n```\n\n이번에는 `retriever` 객체의 `invoke()` 메서드를 사용하여 문서를 검색합니다.\n\n::: {#18b1fd7d .cell execution_count=18}\n``` {.python .cell-code}\n# 문서를 검색하여 가져옵니다.\nretrieved_docs = retriever.invoke(\"Word2Vec\")\n\n# 검색된 문서의 첫 번째 문서의 페이지 내용의 길이를 반환합니다.\nprint(retrieved_docs[0].page_content)\n```\n:::\n\n\n## 두 가지 방식 비교\n\n### 검색 결과 분석\n\n**실습 1 (Parent = 원본 문서)**\n- Parent 크기: 5733자\n- Parent 개수: 1개\n- 반환 내용: 전체 키워드 사전\n- 문제: 불필요한 정보 과다 포함\n\n**실습 2 (Parent = 1000자 청크)**\n- Parent 크기: 약 1000자\n- Parent 개수: 7개\n- 반환 내용: Word2Vec 관련 키워드 집합\n- 장점: 관련성 높은 정보만 포함\n\n### 성능 비교\n\n| 지표 | 실습 1 (원본) | 실습 2 (1000자) | 개선 |\n|:----|:------------|:---------------|:----|\n| **검색 정확도** | 높음 | 높음 | 동일 |\n| **맥락 충분성** | 과다 | 적절 | ✅ 개선 |\n| **토큰 효율성** | 낮음 | 높음 | ✅ 5배 개선 |\n| **답변 품질** | 중간 | 높음 | ✅ 개선 |\n| **처리 속도** | 느림 | 빠름 | ✅ 개선 |\n\n## 실무 적용 가이드\n\n### 청크 크기 설정 전략\n\n**Child Chunk 크기 (검색용)**\n\n| 크기 | 권장 상황 | 장점 | 단점 |\n|:----|:---------|:-----|:-----|\n| **100-200자** | 정확한 검색 필요 | 정밀한 매칭 | 맥락 부족 |\n| **200-400자** | 일반적인 경우 | 균형 잡힘 | - |\n| **400-600자** | 넓은 검색 필요 | 다양한 매칭 | 정확도 저하 |\n\n**Parent Chunk 크기 (반환용)**\n\n| 크기 | 권장 상황 | 토큰 (영문 기준) | 예상 비용 |\n|:----|:---------|:----------------|:--------|\n| **500-1000자** | 짧은 답변 | 125-250 | 낮음 |\n| **1000-2000자** | 일반적인 경우 | 250-500 | 중간 |\n| **2000-4000자** | 긴 맥락 필요 | 500-1000 | 높음 |\n\n### 사용 시나리오\n\n**✅ ParentDocumentRetriever 사용 권장**\n\n1. **긴 문서 처리**\n   - 논문, 보고서, 매뉴얼 등\n   - 전체 문서는 너무 크지만 맥락은 필요한 경우\n\n2. **토큰 비용 절감**\n   - LLM API 비용이 중요한 경우\n   - 대량의 문서 처리 시\n\n3. **계층적 정보 구조**\n   - 섹션별로 나뉜 문서\n   - 챕터/절 구조가 있는 경우\n\n**❌ 일반 Retriever 사용 권장**\n\n1. **짧은 문서**\n   - 이미 400자 이하의 작은 단위\n   - 추가 분할이 불필요한 경우\n\n2. **단순 구조**\n   - FAQ, 짧은 뉴스 기사\n   - 계층 구조가 없는 경우\n\n3. **전체 맥락 필수**\n   - 문서 전체를 봐야 하는 경우\n   - 코드 전체, 계약서 전문 등\n\n### 최적화 팁\n\n**1. 동적 크기 조정**\n```python\n# 문서 길이에 따라 Parent 크기 조정\nif avg_doc_length < 2000:\n    parent_size = 1000\nelif avg_doc_length < 5000:\n    parent_size = 2000\nelse:\n    parent_size = 3000\n```\n\n**2. Overlap 설정**\n```python\nparent_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200  # 20% 오버랩으로 맥락 연결\n)\n```\n\n**3. 메타데이터 활용**\n```python\n# 섹션 정보 추가로 검색 정확도 향상\nchild_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=200,\n    add_start_index=True  # 위치 정보 추가\n)\n```\n\n## 핵심 요약\n\n**ParentDocumentRetriever의 핵심**\n- 작은 청크로 정확하게 검색\n- 큰 청크로 충분한 맥락 제공\n- 계층 구조로 효율성과 정확도 동시 달성\n\n**주요 장점**\n1. ✅ 검색 정확도 향상 (작은 청크)\n2. ✅ 맥락 보존 (큰 청크 반환)\n3. ✅ 토큰 비용 절감 (적절한 크기)\n4. ✅ 유연한 구조 (크기 조절 가능)\n\n**실무 적용**\n- Child: 200-400자 (검색 정밀도)\n- Parent: 1000-2000자 (맥락 + 효율)\n- 문서 특성에 따라 동적 조정\n- Overlap으로 맥락 연결성 향상\n\n```\n정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.\n예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\n연관키워드: 딥러닝, 자연어 처리, Attention\n\nHuggingFace\n\n정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.\n예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.\n연관키워드: 자연어 처리, 딥러닝, 라이브러리\n\nDigital Transformation\n\n정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\n예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\n연관키워드: 혁신, 기술, 비즈니스 모델\n\nCrawling\n\n정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n\nWord2Vec\n\n정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\nLLM (Large Language Model)\n```\n\n",
    "supporting": [
      "05-ParentDocumentRetriever_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}