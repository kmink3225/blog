{
  "hash": "b98c5a92eae148a41ccb779419b18cd1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Parent Document Retriever\"\nsubtitle: 검색기\ndescription: |\n  문서 검색을 위한 다양한 Retriever 패턴과 최적화 기법을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 12/31/2024\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n**문서 검색과 문서 분할의 균형 잡기**\n\n문서 검색 과정에서 문서를 적절한 크기의 조각(청크)으로 나누는 것은 다음의 **상충되는 두 가지 중요한 요소를 고려**해야 합니다.\n\n1. 작은 문서를 원하는 경우: 이렇게 하면 문서의 임베딩이 그 의미를 가장 정확하게 반영할 수 있습니다. 문서가 너무 길면 임베딩이 의미를 잃어버릴 수 있습니다.\n2. 각 청크의 맥락이 유지되도록 충분히 긴 문서를 원하는 경우입니다.\n\n**`ParentDocumentRetriever`의 역할**\n\n이 두 요구 사항 사이의 균형을 맞추기 위해 `ParentDocumentRetriever`라는 도구가 사용됩니다. 이 도구는 문서를 작은 조각으로 나누고, 이 조각들을 관리합니다. 검색을 진행할 때는, 먼저 이 작은 조각들을 찾아낸 다음, 이 조각들이 속한 원본 문서(또는 더 큰 조각)의 식별자(ID)를 통해 전체적인 맥락을 파악할 수 있습니다.\n\n여기서 '부모 문서'란, 작은 조각이 나누어진 원본 문서를 말합니다. 이는 전체 문서일 수도 있고, 비교적 큰 다른 조각일 수도 있습니다. 이 방식을 통해 문서의 의미를 정확하게 파악하면서도, 전체적인 맥락을 유지할 수 있게 됩니다.\n\n**정리**\n\n- **문서 간의 계층 구조 활용**: `ParentDocumentRetriever`는 문서 검색의 효율성을 높이기 위해 문서 간의 계층 구조를 활용합니다.\n- **검색 성능 향상**: 관련성 높은 문서를 빠르게 찾아내며, 주어진 질문에 대한 가장 적합한 답변을 제공하는 문서를 효과적으로 찾아낼 수 있습니다.\n  문서를 검색할 때 자주 발생하는 두 가지 상충되는 요구 사항이 있습니다:\n\n\n여러 개의 텍스트 파일을 로드하기 위해 `TextLoader` 객체를 생성하고 데이터를 로드합니다.\n\n::: {#ffc2a3b0 .cell execution_count=1}\n``` {.python .cell-code}\n# API 키를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API 키 정보 로드\nload_dotenv()\n```\n:::\n\n\n::: {#d5427f6f .cell execution_count=2}\n``` {.python .cell-code}\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH10-Retriever\")\n```\n:::\n\n\n::: {#03a65811 .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain.storage import InMemoryStore\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.retrievers import ParentDocumentRetriever\n```\n:::\n\n\n::: {#87ef919d .cell execution_count=4}\n``` {.python .cell-code}\nloaders = [\n    # 파일을 로드합니다.\n    TextLoader(\"./data/appendix-keywords.txt\"),\n]\n\ndocs = []\nfor loader in loaders:\n    # 로더를 사용하여 문서를 로드하고 docs 리스트에 추가합니다.\n    docs.extend(loader.load())\n```\n:::\n\n\n## 전체 문서 검색\n\n이 모드에서는 전체 문서를 검색하고자 합니다. 따라서 `child_splitter` 만 지정하도록 하겠습니다.\n\n- 나중에는 `parent_splitter` 도 지정하여 결과를 비교해 보겠습니다.\n\n::: {#9d0ce0d5 .cell execution_count=5}\n``` {.python .cell-code}\n# 자식 분할기를 생성합니다.\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n\n# DB를 생성합니다.\nvectorstore = Chroma(\n    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n)\n\nstore = InMemoryStore()\n\n# Retriever 를 생성합니다.\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\n```\n:::\n\n\n`retriever.add_documents(docs, ids=None)` 함수로 문서목록을 추가합니다.\n\n- `ids` 가 `None` 이면 자동으로 생성됩니다.\n- `add_to_docstore=False` 로 설정시 document 를 중복으로 추가하지 않습니다. 단, 중복을 체크하기 위한 `ids` 값이 필수 값으로 요구됩니다.\n\n::: {#a0c51967 .cell execution_count=6}\n``` {.python .cell-code}\n# 문서를 검색기에 추가합니다. docs는 문서 목록이고, ids는 문서의 고유 식별자 목록입니다.\nretriever.add_documents(docs, ids=None, add_to_docstore=True)\n```\n:::\n\n\n이 코드는 두 개의 키를 반환해야 합니다. 그 이유는 우리가 두 개의 문서를 추가했기 때문입니다.\n\n- `store` 객체의 `yield_keys()` 메서드를 호출하여 반환된 키(key) 값들을 리스트로 변환합니다.\n\n::: {#b78a877c .cell execution_count=7}\n``` {.python .cell-code}\n# 저장소의 모든 키를 리스트로 반환합니다.\nlist(store.yield_keys())\n```\n:::\n\n\n이제 벡터 스토어 검색 기능을 호출해 보겠습니다.\n\n우리가 작은 청크(chunk)들을 저장하고 있기 때문에, 검색 결과로 작은 청크들이 반환되는 것을 확인할 수 있을 것입니다.\n\n`vectorstore` 객체의 `similarity_search` 메서드를 사용하여 유사도 검색을 수행합니다.\n\n::: {#1fa0533a .cell execution_count=8}\n``` {.python .cell-code}\n# 유사도 검색을 수행합니다.\nsub_docs = vectorstore.similarity_search(\"Word2Vec\")\n```\n:::\n\n\n`sub_docs[0].page_content`를 출력합니다.\n\n::: {#2c008c30 .cell execution_count=9}\n``` {.python .cell-code}\n# sub_docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.\nprint(sub_docs[0].page_content)\n```\n:::\n\n\n이제 전체 retriever에서 검색해 보겠습니다. 이 과정에서는 작은 청크(chunk)들이 위치한 **문서를 반환** 하기 때문에 상대적으로 큰 문서들이 반환될 것입니다.\n\n`retriever` 객체의 `invoke()` 메서드를 사용하여 쿼리와 관련된 문서를 검색합니다.\n\n::: {#95f714ce .cell execution_count=10}\n``` {.python .cell-code}\n# 문서를 검색하여 가져옵니다.\nretrieved_docs = retriever.invoke(\"Word2Vec\")\n```\n:::\n\n\n검색된 문서(`retrieved_docs[0]`)의 일부 내용을 출력합니다.\n\n::: {#ba4f8496 .cell execution_count=11}\n``` {.python .cell-code}\n# 검색된 문서의 문서의 페이지 내용의 길이를 출력합니다.\nprint(\n    f\"문서의 길이: {len(retrieved_docs[0].page_content)}\",\n    end=\"\\n\\n=====================\\n\\n\",\n)\n\n# 문서의 일부를 출력합니다.\nprint(retrieved_docs[0].page_content[2000:2500])\n```\n:::\n\n\n## 더 큰 Chunk 의 크기를 조절\n\n이전의 결과처럼 **전체 문서가 너무 커서 있는 그대로 검색하기에는 부적합** 할 수 있습니다.\n\n이런 경우, 실제로 우리가 하고 싶은 것은 먼저 원시 문서를 더 큰 청크로 분할한 다음, 더 작은 청크로 분할하는 것입니다.\n\n그런 다음 작은 청크들을 인덱싱하지만, 검색 시에는 더 큰 청크를 검색합니다 (그러나 여전히 전체 문서는 아닙니다).\n\n- `RecursiveCharacterTextSplitter`를 사용하여 부모 문서와 자식 문서를 생성합니다.\n  - 부모 문서는 `chunk_size`가 1000으로 설정되어 있습니다.\n  - 자식 문서는 `chunk_size`가 200으로 설정되어 있으며, 부모 문서보다 작은 크기로 생성됩니다.\n\n::: {#b10ca090 .cell execution_count=12}\n``` {.python .cell-code}\n# 부모 문서를 생성하는 데 사용되는 텍스트 분할기입니다.\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n# 자식 문서를 생성하는 데 사용되는 텍스트 분할기입니다.\n# 부모보다 작은 문서를 생성해야 합니다.\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소입니다.\nvectorstore = Chroma(\n    collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings()\n)\n# 부모 문서의 저장 계층입니다.\nstore = InMemoryStore()\n```\n:::\n\n\n`ParentDocumentRetriever`를 초기화하는 코드입니다.\n\n- `vectorstore` 매개변수는 문서 벡터를 저장하는 벡터 저장소를 지정합니다.\n- `docstore` 매개변수는 문서 데이터를 저장하는 문서 저장소를 지정합니다.\n- `child_splitter` 매개변수는 하위 문서를 분할하는 데 사용되는 문서 분할기를 지정합니다.\n- `parent_splitter` 매개변수는 상위 문서를 분할하는 데 사용되는 문서 분할기를 지정합니다.\n\n`ParentDocumentRetriever`는 계층적 문서 구조를 처리하며, 상위 문서와 하위 문서를 별도로 분할하고 저장합니다. 이를 통해 검색 시 상위 문서와 하위 문서를 효과적으로 활용할 수 있습니다.\n\n::: {#96511375 .cell execution_count=13}\n``` {.python .cell-code}\nretriever = ParentDocumentRetriever(\n    # 벡터 저장소를 지정합니다.\n    vectorstore=vectorstore,\n    # 문서 저장소를 지정합니다.\n    docstore=store,\n    # 하위 문서 분할기를 지정합니다.\n    child_splitter=child_splitter,\n    # 상위 문서 분할기를 지정합니다.\n    parent_splitter=parent_splitter,\n)\n```\n:::\n\n\n`retriever` 객체에 `docs`를 추가합니다. `retriever`가 검색할 수 있는 문서 집합에 새로운 문서들을 추가하는 역할을 합니다.\n\n::: {#87ca9fcb .cell execution_count=14}\n``` {.python .cell-code}\nretriever.add_documents(docs)  # 문서를 retriever에 추가합니다.\n```\n:::\n\n\n이제 문서의 수가 훨씬 더 많아진 것을 볼 수 있습니다. 이는 더 큰 청크(chunk)들입니다.\n\n::: {#8add1dfa .cell execution_count=15}\n``` {.python .cell-code}\n# 저장소에서 키를 생성하고 리스트로 변환한 후 길이를 반환합니다.\nlen(list(store.yield_keys()))\n```\n:::\n\n\n기본 벡터 저장소가 여전히 작은 청크를 검색하는지 확인해 보겠습니다.\n\n`vectorstore` 객체의 `similarity_search` 메서드를 사용하여 유사도 검색을 수행합니다.\n\n::: {#d6fac344 .cell execution_count=16}\n``` {.python .cell-code}\n# 유사도 검색을 수행합니다.\nsub_docs = vectorstore.similarity_search(\"Word2Vec\")\n# sub_docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.\nprint(sub_docs[0].page_content)\n```\n:::\n\n\n이번에는 `retriever` 객체의 `invoke()` 메서드를 사용하여 문서를 검색합니다.\n\n::: {#18b1fd7d .cell execution_count=17}\n``` {.python .cell-code}\n# 문서를 검색하여 가져옵니다.\nretrieved_docs = retriever.invoke(\"Word2Vec\")\n\n# 검색된 문서의 첫 번째 문서의 페이지 내용의 길이를 반환합니다.\nprint(retrieved_docs[0].page_content)\n```\n:::\n\n\n",
    "supporting": [
      "05-ParentDocumentRetriever_files"
    ],
    "filters": [],
    "includes": {}
  }
}