{
  "hash": "ae3cf5b6f62ae595dae465c40d700f48",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Vector Store 개요\"\nsubtitle: 벡터 스토어\ndescription: |\n  임베딩 벡터를 저장하고 검색하는 벡터 데이터베이스를 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 04/15/2025\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n## VectorStore란?\n\n* VectorStore(벡터 스토어)는 임베딩 벡터를 효율적으로 저장하고 검색하는 특수한 데이터베이스다.\n* RAG(Retrieval-Augmented Generation) 시스템에서 VectorStore는 문서 검색의 핵심 구성 요소로, 의미 기반 검색(Semantic Search)을 가능하게 한다.\n* 문서 load > 문서 split > Embedding > Store\n* Vector Store는 Vector DB 검색 기능은 없고 데이터만 저장하는 공간 Vector Store에 따라 검색 성능은 차이가 나지 않음. \n* 하지만, 실제로 Vector DB에 따라 검색 성능이 차이가 나는데 검색 알고리즘이 차이가 나기 때문이다.\n    * 검색 알고리즘\n        * Semantic Search: 거의 모든 Vector DB가 갖고 있는 검색 기능\n        * Keyword Search: 일부 Vector DB가 지원하는 기능\n* 로컬 DB는 일정 용량 이상이면 검색 성능이 급격히 저하되는 확장성 문제가 있다.\n  - 예: FAISS, Chroma (로컬 모드)는 메모리 기반으로 동작하여 대규모 데이터셋에서 한계 존재\n  - 단일 서버의 리소스(메모리, CPU)에 의존하므로 수평 확장(horizontal scaling)이 어려움 \n* **FAISS (Facebook AI Similarity Search)**\n  - Meta AI에서 개발한 고성능 벡터 검색 라이브러리\n  - 주요 특징:\n    - CPU/GPU 모두 지원하여 하드웨어 가속 활용 가능\n    - 다양한 인덱스 알고리즘 제공 (Flat, IVF, HNSW, PQ 등)\n    - 10억 개 이상의 벡터도 효율적으로 검색 가능\n  - 장점:\n    - 매우 빠른 검색 속도 (특히 GPU 사용 시)\n    - 메모리 효율적인 압축 기법 (Product Quantization)\n    - 오픈소스이며 Python/C++ 인터페이스 제공\n  - 단점:\n    - 기본적으로 인메모리 저장 (영구 저장은 수동으로 save/load 필요)\n    - 실시간 업데이트가 어려움 (인덱스 재구축 필요)\n    - 분산 환경 지원 부족\n  - 적합한 사용 사례:\n    - 프로토타이핑 및 실험 단계\n    - 로컬 개발 환경\n    - 배치 처리 중심의 검색 시스템\n    - 초고속 검색이 필요하지만 업데이트가 적은 경우\n* **Chroma**\n  - AI 네이티브 오픈소스 임베딩 데이터베이스\n  - **주요 특징**:\n    - 내장 DB로 별도 서버 설치 불필요\n    - SQLite 기반으로 자동 영구 저장\n    - Python과 JavaScript 클라이언트 지원\n    - 간단한 API로 빠른 시작 가능\n  - 장점:\n    - 설치 및 사용이 매우 간편 (`pip install chromadb`만으로 시작)\n    - 자동으로 디스크에 데이터 저장 (별도 save/load 불필요)\n    - 메타데이터 필터링 강력 (복잡한 쿼리 지원)\n    - 멀티모달 지원 (텍스트 + 이미지)\n    - 로컬과 클라우드 모두 지원 (Chroma Cloud)\n  - 단점:\n    - FAISS 대비 검색 속도가 느림\n    - 대규모 데이터셋(수백만 이상)에서 성능 저하\n    - GPU 가속 미지원\n    - 프로덕션 환경에서는 제한적\n  - 적합한 사용 사례:\n    - 빠른 프로토타이핑이 필요한 경우\n    - 소규모 프로젝트 (10만 개 이하 문서)\n    - 개발/테스트 환경\n    - 멀티모달 검색이 필요한 경우 (이미지)\n    - 간단한 RAG 애플리케이션\n  * 클라우드 DB는 확장성에 최적화되어 있다\n    - 자동 스케일링: 데이터 증가에 따라 자동으로 인프라 확장 (예: Pinecone, Weaviate Cloud)\n    - 분산 아키텍처: 여러 노드에 데이터를 분산 저장하여 병렬 처리 가능 (예: Milvus, Qdrant)\n    - 관리형 서비스: 백업, 복제, 모니터링 등의 운영 작업을 자동으로 처리\n    - 고가용성(High Availability): 장애 발생 시에도 서비스 중단 없이 운영 가능\n\n### 기존 데이터베이스와의 차이점\n\n| 구분 | 기존 DB (관계형/NoSQL) | VectorStore |\n|------|----------------------|-------------|\n| **저장 방식** | 텍스트, 숫자, 구조화된 데이터 | 고차원 벡터 (임베딩) |\n| **검색 방식** | 키워드 매칭, 정확한 일치 | 유사도 기반 검색 (코사인 유사도, 유클리드 거리) |\n| **인덱싱** | B-Tree, Hash Index | HNSW, IVF, Product Quantization |\n| **사용 사례** | 트랜잭션 처리, CRUD 작업 | 의미 검색, 추천 시스템, RAG |\n\n## RAG 파이프라인에서의 위치\n\nVectorStore는 RAG 시스템의 4단계 중 네 번째 단계에 해당한다:\n\n1. 문서 로딩(Document Loading): 다양한 형식의 문서를 불러온다\n2. 텍스트 분할(Text Splitting): 문서를 작은 청크(chunk)로 나눈다\n3. 임베딩 생성(Embedding): 텍스트 청크를 벡터로 변환한다\n4. 벡터 저장(Vector Store): 임베딩 벡터를 VectorStore에 저장한다 ← **현재 단계**\n5. 검색(Retrieval): 질의와 유사한 벡터를 찾아 관련 문서를 반환한다\n\n## VectorStore의 필요성\n\n### 빠른 검색 속도\n\n- 수백만 개의 벡터 중에서도 밀리초 단위로 유사 벡터를 찾는다\n- 근사 최근접 이웃(ANN, Approximate Nearest Neighbor) 알고리즘 활용\n- 대규모 데이터셋에서도 실시간 검색 가능\n\n### 의미 기반 검색(Semantic Search)\n\n기존 키워드 검색의 한계를 극복한다:\n\n**키워드 검색의 문제점:**\n```\n질문: \"모바일 디바이스 상에서 동작하는 인공지능 기술을 소개한 기업명은?\"\n키워드 매칭: \"모바일\", \"디바이스\", \"인공지능\" 등의 단어가 정확히 포함된 문서만 검색\n→ 유의어나 문맥을 고려하지 못함\n```\n\n**의미 기반 검색:**\n```\n질문: \"모바일 디바이스 상에서 동작하는 인공지능 기술을 소개한 기업명은?\"\n벡터 유사도: 질문과 의미적으로 유사한 문서를 검색\n→ \"스마트폰 AI\", \"온디바이스 머신러닝\" 등 유의어가 포함된 문서도 검색 가능\n```\n\n### 스케일러빌리티(Scalability)\n\n- 데이터가 지속적으로 증가해도 성능 저하 없이 관리 가능\n- 분산 저장 및 병렬 처리 지원\n- 인덱스 최적화를 통한 효율적인 메모리 사용\n\n## 주요 VectorStore 비교\n\n| VectorStore | 배포 방식 | 라이선스 | 특징 | 장점 | 단점 | 적합한 사용 사례 |\n|-------------|----------|---------|------|------|------|-----------------|\n| **FAISS** | 로컬 | 오픈소스 (MIT) | Meta AI 개발, CPU/GPU 지원,벡터 검색 전용\n | 빠른 검색 속도, 다양한 인덱스 옵션 | 메모리 내 저장, 영구 저장 추가 설정 필요 | 프로토타이핑, 로컬 개발 |\n| **Chroma** | 로컬/클라우드 | 오픈소스 (Apache 2.0) | 내장 DB, 멀티모달 지원, 벡터 검색 전용\n | 쉬운 사용, 자동 영구 저장 | 상대적으로 느린 속도 | 소규모 프로젝트, 빠른 프로토타이핑 |\n| **Pinecone** | 클라우드 | 유료 (Freemium) | 완전 관리형, 서버리스 | 스케일링 자동화, 안정성, 관리 불필요 | 비용 발생, 온프레미스 불가, 벤더 종속 | 프로덕션 환경, 대규모 서비스 |\n| **Qdrant** | 로컬/클라우드 | 오픈소스 (Apache 2.0) | Rust 기반, 빠른 성능, 벡터 검색 전용 | 필터링 강력, 메모리 효율적, RESTful API | 상대적으로 작은 커뮤니티 | 실시간 추천, 시맨틱 검색 |\n| **Weaviate** | 로컬/클라우드 | 오픈소스 (BSD-3) | GraphQL API, 하이브리드 검색 | 풍부한 기능, 필터링 강력, 멀티테넌시 | 설정 복잡도 높음 | 복잡한 검색 요구사항, 엔터프라이즈 |\n| **Milvus** | 로컬/클라우드 | 오픈소스 (Apache 2.0) | 분산 아키텍처, 엔터프라이즈급 | 높은 성능, 확장성 우수, GPU 지원 | 설치/관리 복잡 | 대규모 엔터프라이즈, 프로덕션 |\n| **Azure AI Search** | 클라우드 (Azure) | 유료 | Azure 네이티브, 하이브리드 검색 | Azure 생태계 통합, 벡터+키워드 검색, 보안 | Azure 종속, 비용, 설정 복잡 | Azure 환경, 엔터프라이즈 검색 |\n| **pgvector** | 로컬/클라우드 | 오픈소스 (PostgreSQL) | PostgreSQL 확장, 하이브리드 검색  | 기존 DB 활용, SQL 쿼리, 트랜잭션 지원 | PostgreSQL 성능 의존, 대규모 제한적 | 기존 PostgreSQL 사용, 하이브리드 앱 |\n| **Elasticsearch** | 로컬/클라우드 | 오픈소스/유료 (Elastic License) | 검색 엔진 기반, 벡터 검색 추가 | 강력한 전문 검색, 분석 기능, 성숙한 생태계 | 리소스 사용 많음, 복잡한 설정 | 로그 분석, 하이브리드 검색 |\n| **Redis** | 로컬/클라우드 | 오픈소스/유료 (Redis Stack) | 인메모리 DB, RediSearch 모듈, 하이브리드 검색  | 매우 빠른 속도, 익숙한 Redis 인터페이스 | 메모리 비용, 영구 저장 제한적 | 실시간 검색, 캐싱과 검색 통합 |\n\n## VectorStore 사용 예시\n\n### 기본 사용법 (FAISS)\n\n::: {#7df68789 .cell execution_count=1}\n``` {.python .cell-code}\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import CharacterTextSplitter\n\n# 1. 텍스트 분할\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\ndocuments = text_splitter.create_documents([\n    \"LangChain은 대규모 언어 모델을 활용한 애플리케이션 개발 프레임워크다.\",\n    \"VectorStore는 임베딩 벡터를 효율적으로 저장하고 검색한다.\",\n    \"RAG 시스템은 외부 지식을 활용하여 LLM의 답변 품질을 향상시킨다.\"\n])\n\n# 2. 임베딩 생성 및 VectorStore 구축\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\nvectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n\n# 3. 유사도 검색\nquery = \"RAG 시스템이란 무엇인가?\"\nresults = vectorstore.similarity_search(query, k=2)\n\nfor doc in results:\n    print(doc.page_content)\n```\n:::\n\n\n### 메타데이터와 함께 저장\n\n::: {#3e289de3 .cell execution_count=2}\n``` {.python .cell-code}\nfrom langchain_core.documents import Document\n\n# 메타데이터 포함 문서 생성\ndocuments = [\n    Document(\n        page_content=\"LangChain은 LLM 애플리케이션 개발 프레임워크다.\",\n        metadata={\"source\": \"docs\", \"page\": 1, \"category\": \"framework\"}\n    ),\n    Document(\n        page_content=\"FAISS는 효율적인 벡터 검색 라이브러리다.\",\n        metadata={\"source\": \"docs\", \"page\": 2, \"category\": \"vectorstore\"}\n    ),\n]\n\nvectorstore = FAISS.from_documents(documents, embeddings)\n\n# 메타데이터 필터링과 함께 검색\nresults = vectorstore.similarity_search(\n    query=\"벡터 검색\",\n    k=1,\n    filter={\"category\": \"vectorstore\"}\n)\n```\n:::\n\n\n### VectorStore 저장 및 로드\n\n::: {#635406a4 .cell execution_count=3}\n``` {.python .cell-code}\n# VectorStore 저장\nvectorstore.save_local(\"./vector_db\")\n\n# VectorStore 로드\nloaded_vectorstore = FAISS.load_local(\n    \"./vector_db\",\n    embeddings,\n    allow_dangerous_deserialization=True\n)\n```\n:::\n\n\n## 검색 방법 비교\n\n### Similarity Search (유사도 검색)\n\n가장 기본적인 검색 방법으로, 코사인 유사도를 기반으로 상위 k개 문서를 반환한다.\n\n::: {#382fbd81 .cell execution_count=4}\n``` {.python .cell-code}\nresults = vectorstore.similarity_search(query=\"RAG 시스템\", k=3)\n```\n:::\n\n\n### Similarity Search with Score\n\n유사도 점수와 함께 문서를 반환한다.\n\n::: {#1c4af86f .cell execution_count=5}\n``` {.python .cell-code}\nresults = vectorstore.similarity_search_with_score(query=\"RAG 시스템\", k=3)\nfor doc, score in results:\n    print(f\"Score: {score}, Content: {doc.page_content}\")\n```\n:::\n\n\n### Max Marginal Relevance (MMR)\n\n다양성을 고려한 검색으로, 관련성은 높지만 서로 다른 내용의 문서를 반환한다.\n\n::: {#5ef7d9e5 .cell execution_count=6}\n``` {.python .cell-code}\nresults = vectorstore.max_marginal_relevance_search(\n    query=\"RAG 시스템\",\n    k=3,\n    fetch_k=10,  # 후보 문서 수\n    lambda_mult=0.5  # 0: 다양성 최대, 1: 관련성 최대\n)\n```\n:::\n\n\n## 성능 최적화 팁\n\n### 적절한 청크 크기 설정\n\n- 너무 작은 청크: 문맥 손실, 검색 정확도 저하\n- 너무 큰 청크: 관련 없는 정보 포함, 토큰 낭비\n- 권장 크기: 500-1000 토큰 (사용 사례에 따라 조정)\n\n### 임베딩 모델 선택\n\n- 속도 우선: `text-embedding-3-small`\n- 정확도 우선: `text-embedding-3-large`\n- 비용 절감**: 캐시 활용 (`CacheBackedEmbeddings`)\n\n### 인덱스 타입 선택 (FAISS)\n\n::: {#b9f0d0bd .cell execution_count=7}\n``` {.python .cell-code}\n# Flat Index: 정확하지만 느림\nvectorstore = FAISS.from_documents(documents, embeddings)\n\n# IVF Index: 빠르지만 근사치\nimport faiss\nindex = faiss.IndexIVFFlat(dimension, nlist=100)\nvectorstore = FAISS(embeddings, index)\n```\n:::\n\n\n## 참고 자료\n\n### 공식 문서 및 GitHub\n\n**LangChain & 통합:**\n- [LangChain VectorStores 공식 문서](https://python.langchain.com/docs/modules/data_connection/vectorstores/)\n- [LangChain VectorStore 통합 목록](https://python.langchain.com/docs/integrations/vectorstores/)\n\n**개별 VectorStore:**\n- [FAISS GitHub Repository](https://github.com/facebookresearch/faiss)\n- [FAISS Wiki 문서](https://github.com/facebookresearch/faiss/wiki)\n- [Chroma 공식 문서](https://docs.trychroma.com/)\n- [Chroma GitHub Repository](https://github.com/chroma-core/chroma)\n- [Pinecone 공식 문서](https://docs.pinecone.io/)\n- [Weaviate 공식 문서](https://weaviate.io/developers/weaviate)\n- [Milvus 공식 문서](https://milvus.io/docs)\n- [Qdrant 공식 문서](https://qdrant.tech/documentation/)\n- [Azure AI Search 문서](https://learn.microsoft.com/en-us/azure/search/)\n- [pgvector GitHub](https://github.com/pgvector/pgvector)\n- [Elasticsearch Vector Search](https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html)\n\n### 벤치마크 및 비교\n\n**성능 비교:**\n- [VectorDBBench - Vector Database Benchmark](https://github.com/zilliztech/VectorDBBench)\n- [ANN Benchmarks](https://ann-benchmarks.com/) - 근사 최근접 이웃 알고리즘 벤치마크\n- [Vector Database Comparison 2024](https://benchmark.vectorview.ai/)\n\n**선택 가이드:**\n- [Choosing a Vector Database](https://www.pinecone.io/learn/vector-database/)\n- [Vector Database Comparison Guide](https://thedataquarry.com/posts/vector-db-1/)\n\n### 학습 자료\n\n**개념 및 이론:**\n- [What is a Vector Database?](https://www.pinecone.io/learn/vector-database/) - Pinecone 학습 자료\n- [Vector Search Explained](https://weaviate.io/blog/vector-search-explained) - Weaviate 블로그\n- [Understanding HNSW](https://www.pinecone.io/learn/series/faiss/hnsw/) - HNSW 알고리즘 설명\n\n**실습 튜토리얼:**\n- [Building a RAG System with LangChain](https://python.langchain.com/docs/tutorials/rag/)\n- [FAISS Tutorial](https://www.pinecone.io/learn/series/faiss/)\n- [Chroma Cookbook](https://cookbook.chromadb.dev/)\n\n### 연구 논문\n\n**핵심 알고리즘:**\n- [Billion-scale similarity search with GPUs (FAISS)](https://arxiv.org/abs/1702.08734) - Facebook AI Research, 2017\n- [Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs (HNSW)](https://arxiv.org/abs/1603.09320) - 2016\n- [Product Quantization for Nearest Neighbor Search](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf) - 2011\n\n**RAG 시스템:**\n- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) - Meta AI, 2020\n- [In-Context Retrieval-Augmented Language Models](https://arxiv.org/abs/2302.00083) - 2023\n\n### 커뮤니티 및 포럼\n\n- [LangChain Discord](https://discord.gg/langchain)\n- [Chroma Discord](https://discord.gg/MMeYNTmh3x)\n- [Weaviate Community Forum](https://forum.weaviate.io/)\n- [r/MachineLearning - Vector Database 토론](https://www.reddit.com/r/MachineLearning/)\n\n### 블로그 및 기술 아티클\n\n- [Vector Databases: A Beginner's Guide](https://www.datacamp.com/tutorial/vector-databases-guide) - DataCamp\n- [The Illustrated Vector Database](https://www.youtube.com/watch?v=dN0lsF2cvm4) - YouTube 강의\n- [LangChain Blog - Vector Stores](https://blog.langchain.dev/tag/vector-stores/)\n- [Towards Data Science - Vector Database Articles](https://towardsdatascience.com/tagged/vector-database)\n\n",
    "supporting": [
      "00-VectorStore_files"
    ],
    "filters": [],
    "includes": {}
  }
}