{
  "hash": "19382318ec0949d4c3ff8b1519f7db7f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"기본 예시: 프롬프트 + 모델 + 출력 파서\"\nsubtitle: LangChain 기초\ndescription: |\n  LangChain의 기본 개념과 OpenAI API 활용법을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 01/09/2025\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n가장 기본적이고 일반적인 사용 사례는 prompt 템플릿과 모델을 함께 연결하는 것입니다. 이것이 어떻게 작동하는지 보기 위해, 각 나라별 수도를 물어보는 Chain을 생성해 보겠습니다.\n\n::: {#1a2cbbcf .cell execution_count=1}\n``` {.python .cell-code}\n# API KEY를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API KEY 정보로드\nload_dotenv()\n```\n:::\n\n\n::: {#d664d50c .cell execution_count=2}\n``` {.python .cell-code}\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install -qU langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH01-Basic\")\n```\n:::\n\n\n## 프롬프트 템플릿의 활용\n\n`PromptTemplate`\n\n- 사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다\n- 사용법\n  - `template`: 템플릿 문자열입니다. 이 문자열 내에서 중괄호 `{}`는 변수를 나타냅니다.\n  - `input_variables`: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.\n\n`input_variables`\n\n- input_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.\n\n::: {#d3bf9917 .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain_teddynote.messages import stream_response  # 스트리밍 출력\nfrom langchain_core.prompts import PromptTemplate\n```\n:::\n\n\n`from_template()` 메소드를 사용하여 PromptTemplate 객체 생성\n\n::: {#0643e99d .cell execution_count=4}\n``` {.python .cell-code}\n# template 정의\ntemplate = \"{country}의 수도는 어디인가요?\"\n\n# from_template 메소드를 이용하여 PromptTemplate 객체 생성\nprompt_template = PromptTemplate.from_template(template)\nprompt_template\n```\n:::\n\n\n::: {#e3c54da8 .cell execution_count=5}\n``` {.python .cell-code}\n# prompt 생성\nprompt = prompt_template.format(country=\"대한민국\")\nprompt\n```\n:::\n\n\n::: {#fa7ff548 .cell execution_count=6}\n``` {.python .cell-code}\n# prompt 생성\nprompt = prompt_template.format(country=\"미국\")\nprompt\n```\n:::\n\n\n::: {#16123eb7 .cell execution_count=7}\n``` {.python .cell-code}\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"gpt-4.1-nano\",\n    temperature=0.1,\n)\n```\n:::\n\n\n## Chain 생성\n\n### LCEL(LangChain Expression Language)\n\n![lcel.png](./images/lcel.png)\n\n여기서 우리는 LCEL을 사용하여 다양한 구성 요소를 단일 체인으로 결합합니다\n\n```\nchain = prompt | model | output_parser\n```\n\n`|` 기호는 [unix 파이프 연산자](<https://en.wikipedia.org/wiki/Pipeline_(Unix)>)와 유사하며, 서로 다른 구성 요소를 연결하고 한 구성 요소의 출력을 다음 구성 요소의 입력으로 전달합니다.\n\n이 체인에서 사용자 입력은 프롬프트 템플릿으로 전달되고, 그런 다음 프롬프트 템플릿 출력은 모델로 전달됩니다. 각 구성 요소를 개별적으로 살펴보면 무슨 일이 일어나고 있는지 이해할 수 있습니다.\n\n::: {#bd4cefa8 .cell execution_count=8}\n``` {.python .cell-code}\n# prompt 를 PromptTemplate 객체로 생성합니다.\nprompt = PromptTemplate.from_template(\"{topic} 에 대해 쉽게 설명해주세요.\")\n\nmodel = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0.1)\n\nchain = prompt | model\n```\n:::\n\n\n### invoke() 호출\n\n- python 딕셔너리 형태로 입력값을 전달합니다.(키: 값)\n- invoke() 함수 호출 시, 입력값을 전달합니다.\n\n::: {#06203420 .cell execution_count=9}\n``` {.python .cell-code}\n# input 딕셔너리에 주제를 '인공지능 모델의 학습 원리'으로 설정합니다.\ninput = {\"topic\": \"인공지능 모델의 학습 원리\"}\n```\n:::\n\n\n::: {#79eb5650 .cell execution_count=10}\n``` {.python .cell-code}\n# prompt 객체와 model 객체를 파이프(|) 연산자로 연결하고 invoke 메서드를 사용하여 input을 전달합니다.\n# 이를 통해 AI 모델이 생성한 메시지를 반환합니다.\nchain.invoke(input)\n```\n:::\n\n\n아래는 스트리밍을 출력하는 예시 입니다.\n\n::: {#deb571d5 .cell execution_count=11}\n``` {.python .cell-code}\n# 스트리밍 출력을 위한 요청\nanswer = chain.stream(input)\n# 스트리밍 출력\nstream_response(answer)\n```\n:::\n\n\n### 출력파서(Output Parser)\n\n::: {#63288e2d .cell execution_count=12}\n``` {.python .cell-code}\nfrom langchain_core.output_parsers import StrOutputParser\n\noutput_parser = StrOutputParser()\n```\n:::\n\n\nChain 에 출력파서를 추가합니다.\n\n::: {#9ec82477 .cell execution_count=13}\n``` {.python .cell-code}\n# 프롬프트, 모델, 출력 파서를 연결하여 처리 체인을 구성합니다.\nchain = prompt | model | output_parser\n```\n:::\n\n\n::: {#9669e082 .cell execution_count=14}\n``` {.python .cell-code}\n# chain 객체의 invoke 메서드를 사용하여 input을 전달합니다.\ninput = {\"topic\": \"인공지능 모델의 학습 원리\"}\nchain.invoke(input)\n```\n:::\n\n\n::: {#110c7fdc .cell execution_count=15}\n``` {.python .cell-code}\n# 스트리밍 출력을 위한 요청\nanswer = chain.stream(input)\n# 스트리밍 출력\nstream_response(answer)\n```\n:::\n\n\n### 템플릿을 변경하여 적용\n\n- 아래의 프롬프트 내용을 얼마든지 **변경** 하여 테스트 해볼 수 있습니다.\n- `model_name` 역시 변경하여 테스트가 가능합니다.\n\n::: {#2e284ebe .cell execution_count=16}\n``` {.python .cell-code}\ntemplate = \"\"\"\n당신은 영어를 가르치는 10년차 영어 선생님입니다. 주어진 상황에 맞는 영어 회화를 작성해 주세요.\n양식은 [FORMAT]을 참고하여 작성해 주세요.\n\n#상황:\n{question}\n\n#FORMAT:\n- 영어 회화:\n- 한글 해석:\n\"\"\"\n\n# 프롬프트 템플릿을 이용하여 프롬프트를 생성합니다.\nprompt = PromptTemplate.from_template(template)\n\n# ChatOpenAI 챗모델을 초기화합니다.\nmodel = ChatOpenAI(model_name=\"gpt-4.1-nano\")\n\n# 문자열 출력 파서를 초기화합니다.\noutput_parser = StrOutputParser()\n```\n:::\n\n\n::: {#48d7dc0a .cell execution_count=17}\n``` {.python .cell-code}\n# 체인을 구성합니다.\nchain = prompt | model | output_parser\n```\n:::\n\n\n::: {#06c63a1b .cell execution_count=18}\n``` {.python .cell-code}\n# 완성된 Chain을 실행하여 답변을 얻습니다.\nprint(chain.invoke({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"}))\n```\n:::\n\n\n::: {#fc9dd0cf .cell execution_count=19}\n``` {.python .cell-code}\n# 완성된 Chain을 실행하여 답변을 얻습니다.\n# 스트리밍 출력을 위한 요청\nanswer = chain.stream({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"})\n# 스트리밍 출력\nstream_response(answer)\n```\n:::\n\n\n::: {#bfda9b47 .cell execution_count=20}\n``` {.python .cell-code}\n# 이번에는 question 을 '미국에서 피자 주문'으로 설정하여 실행합니다.\n# 스트리밍 출력을 위한 요청\nanswer = chain.stream({\"question\": \"미국에서 피자 주문\"})\n# 스트리밍 출력\nstream_response(answer)\n```\n:::\n\n\n",
    "supporting": [
      "03-LCEL_files"
    ],
    "filters": [],
    "includes": {}
  }
}