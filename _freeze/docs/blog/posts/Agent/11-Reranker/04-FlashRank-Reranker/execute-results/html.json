{
  "hash": "d879a89be78565574a3c994780ccf208",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"FlashRank reranker\"\nsubtitle: 재순위화\ndescription: |\n  검색 결과의 관련성을 개선하는 Reranker 모델을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 12/31/2024\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n>[FlashRank](https://github.com/PrithivirajDamodaran/FlashRank)는 기존 검색 및 `retrieval` 파이프라인에 재순위를 추가하기 위한 초경량 및 초고속 Python 라이브러리입니다. 이는 SoTA `cross-encoders`를 기반으로 합니다.\n\n이 노트북은 문서 압축 및 `retrieval`을 위해 [flashrank](https://github.com/PrithivirajDamodaran/FlashRank)를 사용하는 방법을 보여줍니다.\n\n\n## 환경설정\n\n::: {#4af3b0c3 .cell execution_count=1}\n``` {.python .cell-code}\n# 설치\n# !pip install -qU flashrank\n```\n:::\n\n\n::: {#743dc370 .cell execution_count=2}\n``` {.python .cell-code}\ndef pretty_print_docs(docs):\n    print(\n        f\"\\n{'-' * 100}\\n\".join(\n            [\n                f\"Document {i+1}:\\n\\n{d.page_content}\\nMetadata: {d.metadata}\"\n                for i, d in enumerate(docs)\n            ]\n        )\n    )\n```\n:::\n\n\n## FlashrankRerank\n\n간단한 예시를 위한 데이터를 로드하고 retriever 를 생성합니다.\n\n::: {#a7a342b4 .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\n\n# 문서 로드\ndocuments = TextLoader(\"./data/appendix-keywords.txt\").load()\n\n# 텍스트 분할기 초기화\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n\n# 문서 분할\ntexts = text_splitter.split_documents(documents)\n\n# 각 텍스트에 고유 ID 추가\nfor idx, text in enumerate(texts):\n    text.metadata[\"id\"] = idx\n    \n# 검색기 초기화\nretriever = FAISS.from_documents(\n    texts, OpenAIEmbeddings()\n).as_retriever(search_kwargs={\"k\": 10})\n\n# 질의문\nquery = \"Word2Vec 에 대해서 설명해줘.\"\n\n# 문서 검색\ndocs = retriever.invoke(query)\n\n# 문서 출력\npretty_print_docs(docs)\n```\n:::\n\n\n이제 기본 `retriever`를 `ContextualCompressionRetriever`로 감싸고, `FlashrankRerank`를 압축기로 사용해 봅시다.\n\n::: {#9c00c497 .cell execution_count=4}\n``` {.python .cell-code}\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import FlashrankRerank\nfrom langchain_openai import ChatOpenAI\n\n# LLM 초기화\nllm = ChatOpenAI(temperature=0)\n\n# 문서 압축기 초기화\ncompressor = FlashrankRerank(model=\"ms-marco-MultiBERT-L-12\")\n\n# 문맥 압축 검색기 초기화\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor, base_retriever=retriever\n)\n\n# 압축된 문서 검색\ncompressed_docs = compression_retriever.invoke(\n    \"Word2Vec 에 대해서 설명해줘.\"\n)\n\n# 문서 ID 출력\nprint([doc.metadata[\"id\"] for doc in compressed_docs])\n```\n:::\n\n\nreranker 가 적용된 후 결과를 비교합니다.\n\n::: {#e8bbda41 .cell execution_count=5}\n``` {.python .cell-code}\n# 문서 압축 결과 출력\npretty_print_docs(compressed_docs)\n```\n:::\n\n\n",
    "supporting": [
      "04-FlashRank-Reranker_files"
    ],
    "filters": [],
    "includes": {}
  }
}