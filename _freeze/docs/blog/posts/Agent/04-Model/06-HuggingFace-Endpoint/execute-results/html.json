{
  "hash": "b75e64570804049cd9dfce2e4b360667",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"HuggingFace Endpoints\"\nsubtitle: 언어 모델\ndescription: |\n  다양한 LLM 제공자와 모델 활용법을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 02/06/2025\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\nHugging Face Hub은 12만 개 이상의 모델, 2만 개의 데이터셋, 5만 개의 데모 앱(Spaces)을 보유한 플랫폼으로, 모두 오픈 소스이며 공개적으로 사용 가능합니다. 이 온라인 플랫폼에서 사람들은 쉽게 협업하고 함께 머신러닝을 구축할 수 있습니다.\n\nHugging Face Hub은 또한 다양한 ML 애플리케이션을 구축하기 위한 다양한 엔드포인트를 제공합니다. 이 예제는 다양한 유형의 엔드포인트에 연결하는 방법을 보여줍니다.\n\n특히, 텍스트 생성 추론은 Text Generation Inference에 의해 구동됩니다. 이는 매우 빠른 텍스트 생성 추론을 위해 맞춤 제작된 Rust, Python, gRPC 서버입니다.\n\n\n## 허깅페이스 토큰 발급\n\n허깅페이스(https://huggingface.co) 에 회원가입을 한 뒤, 아래의 주소에서 토큰 발급을 신청합니다.\n\n- 토큰 발급주소: https://huggingface.co/docs/hub/security-tokens\n\n## 참고 모델 리스트\n\n- 허깅페이스 LLM 리더보드: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n- 모델 리스트: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n- LogicKor 리더보드: https://lk.instruct.kr/\n\n## Huggingface Endpoints 사용\n\n사용하기 위해서는 Python의 `huggingface_hub` [패키지를 설치](https://huggingface.co/docs/huggingface_hub/installation)해야 합니다.\n\n::: {#095306ee .cell execution_count=1}\n``` {.python .cell-code}\n# !pip install -qU huggingface_hub\n```\n:::\n\n\n`.env` 파일에 이미 발급받은 토큰을 `HUGGINGFACEHUB_API_TOKEN` 을 저장한 뒤 다음 단계롤 진행합니다.\n\n`HUGGINGFACEHUB_API_TOKEN` 을 불러옵니다.\n\n::: {#4e9a8d2b .cell execution_count=2}\n``` {.python .cell-code}\nfrom dotenv import load_dotenv\n\nload_dotenv()\n```\n:::\n\n\n::: {#516e2eef .cell execution_count=3}\n``` {.python .cell-code}\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH04-Models\")\n```\n:::\n\n\n허깅페이스 토큰을 입력합니다\n\n::: {#33042aab .cell execution_count=4}\n``` {.python .cell-code}\nfrom huggingface_hub import login\n\nlogin()\n```\n:::\n\n\n간단한 프롬프트를 생성합니다.\n\n::: {#4fd23889 .cell execution_count=5}\n``` {.python .cell-code}\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"Question: {question}\n\nAnswer:\"\"\"\n\ntemplate = \"\"\"<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\n{question}<|end|>\n<|assistant|>\"\"\"\n\nprompt = PromptTemplate.from_template(template)\n```\n:::\n\n\n## Serverless Endpoints\n\nInference API 는 무료로 사용할 수 있으며 요금은 제한되어 있습니다. 프로덕션을 위한 추론 솔루션이 필요한 경우, [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) 서비스를 확인하세요. Inference Endpoints 를 사용하면 모든 머신 러닝 모델을 전용 및 완전 관리형 인프라에 손쉽게 배포할 수 있습니다. 클라우드, 지역, 컴퓨팅 인스턴스, 자동 확장 범위 및 보안 수준을 선택하여 모델, 지연 시간, 처리량 및 규정 준수 요구 사항에 맞게 설정하세요.\n\n다음은 Inference API 에 액세스하는 방법의 예시입니다.\n\n**참고**\n\n- [Serverless Endpoints](https://huggingface.co/docs/api-inference/index)\n- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index)\n\n`repo_id` 변수에 HuggingFace 모델의 `repo ID`(저장소 ID) 를 할당합니다.\n- `microsoft/Phi-3-mini-4k-instruct` 모델: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n\n::: {#63d5d84f .cell execution_count=6}\n``` {.python .cell-code}\nimport os\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_huggingface import HuggingFaceEndpoint\n\n# 사용할 모델의 저장소 ID를 설정합니다.\nrepo_id = \"microsoft/Phi-3-mini-4k-instruct\"\n\nllm = HuggingFaceEndpoint(\n    repo_id=repo_id,  # 모델 저장소 ID를 지정합니다.\n    max_new_tokens=256,  # 생성할 최대 토큰 길이를 설정합니다.\n    temperature=0.1,\n    task=\"text-generation\",\n    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],  # 허깅페이스 토큰\n)\n\n# LLMChain을 초기화하고 프롬프트와 언어 모델을 전달합니다.\nchain = prompt | llm | StrOutputParser()\n# 질문을 전달하여 LLMChain을 실행하고 결과를 출력합니다.\nresponse = chain.invoke({\"question\": \"what is the capital of South Korea?\"})\nprint(response)\n```\n:::\n\n\n::: {#9ca55289 .cell execution_count=7}\n``` {.python .cell-code}\nprint(response)\n```\n:::\n\n\n## 전용 엔드포인트(Dedicated Endpoint)\n\n무료 서버리스 API를 사용하면 솔루션을 빠르게 구현하고 반복할 수 있습니다. 하지만 로드가 다른 요청과 공유되기 때문에 대용량 사용 사례에서는 속도 제한이 있을 수 있습니다.\n\n엔터프라이즈 워크로드의 경우, [Inference Endpoints - Dedicated](https://huggingface.co/inference-endpoints/dedicated)를 사용하는 것이 가장 좋습니다. 이를 통해 더 많은 유연성과 속도를 제공하는 완전 관리형 인프라에 액세스할 수 있습니다.\n\n이러한 리소스에는 지속적인 지원과 가동 시간 보장은 물론 AutoScaling과 같은 옵션도 포함됩니다.\n\n![](./images/hf-endpoint-1.png)\n\n- `hf_endpoint_url` 변수에 Inference Endpoint의 URL을 설정합니다.\n\n::: {#556f1847 .cell execution_count=8}\n``` {.python .cell-code}\n# Inference Endpoint URL을 아래에 설정합니다.\nhf_endpoint_url = \"https://slcalzucia3n7y3g.us-east-1.aws.endpoints.huggingface.cloud\"\n```\n:::\n\n\n::: {#5ed70124 .cell execution_count=9}\n``` {.python .cell-code}\nllm = HuggingFaceEndpoint(\n    # 엔드포인트 URL을 설정합니다.\n    endpoint_url=hf_endpoint_url,\n    max_new_tokens=512,\n    temperature=0.01,\n    task=\"text-generation\",\n)\n\n# 주어진 프롬프트에 대해 언어 모델을 실행합니다.\nllm.invoke(input=\"대한민국의 수도는 어디인가요?\")\n```\n:::\n\n\n::: {#f71dbb2d .cell execution_count=10}\n``` {.python .cell-code}\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"A chat between a curious user and an artificial intelligence assistant. \"\n            \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n        ),\n        (\"user\", \"Human: {question}\\nAssistant: \"),\n    ]\n)\n\nchain = prompt | llm | StrOutputParser()\n```\n:::\n\n\n::: {#bd24426e .cell execution_count=11}\n``` {.python .cell-code}\nchain.invoke(\"대한민국의 수도는?\")\n```\n:::\n\n\n",
    "supporting": [
      "06-HuggingFace-Endpoint_files"
    ],
    "filters": [],
    "includes": {}
  }
}