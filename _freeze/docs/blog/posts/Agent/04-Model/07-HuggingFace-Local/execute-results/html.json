{
  "hash": "ef3a23768c0683b164e717013028209b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"HuggingFace Local Models\"\nsubtitle: 언어 모델\ndescription: |\n  다양한 LLM 제공자와 모델 활용법을 다룬다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 02/07/2025\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n::: {#5f843e21 .cell execution_count=1}\n``` {.python .cell-code}\n# 토큰 정보로드를 위한 라이브러리\n# 설치: pip install python-dotenv\nfrom dotenv import load_dotenv\n\n# 토큰 정보로드\nload_dotenv()\n```\n:::\n\n\n::: {#ecc94133 .cell execution_count=2}\n``` {.python .cell-code}\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH04-Models\")\n```\n:::\n\n\n모델을 다운로드 받을 경로를 설정\n\n::: {#d444f331 .cell execution_count=3}\n``` {.python .cell-code}\n# 허깅페이스 모델/토크나이저를 다운로드 받을 경로\nimport os\n\n# ./cache/ 경로에 다운로드 받도록 설정\nos.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/\"\nos.environ[\"HF_HOME\"] = \"./cache/\"\n```\n:::\n\n\n`repo_id` 변수에 HuggingFace 모델의 `repo ID`(저장소 ID) 를 할당합니다.\n- `microsoft/Phi-3-mini-4k-instruct` 모델: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n\n::: {#e62cd21f .cell execution_count=4}\n``` {.python .cell-code}\nfrom langchain_huggingface import HuggingFacePipeline\n\nllm = HuggingFacePipeline.from_model_id(\n    model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n    task=\"text-generation\",\n    pipeline_kwargs={\n        \"max_new_tokens\": 256,\n        \"top_k\": 50,\n        \"temperature\": 0.1,\n    },\n)\nllm.invoke(\"Hugging Face is\")\n```\n:::\n\n\n::: {#8265745a .cell execution_count=5}\n``` {.python .cell-code}\n%%time\nfrom langchain_core.prompts import PromptTemplate\n\ntemplate = \"\"\"Summarizes TEXT in simple bullet points ordered from most important to least important.\nTEXT:\n{text}\n\nKeyPoints: \"\"\"\n\n# 프롬프트 템플릿 생성\nprompt = PromptTemplate.from_template(template)\n\n# 체인 생성\nchain = prompt | llm\n\ntext = \"\"\"A Large Language Model (LLM) like me, ChatGPT, is a type of artificial intelligence (AI) model designed to understand, generate, and interact with human language. These models are \"large\" because they're built from vast amounts of text data and have billions or even trillions of parameters. Parameters are the aspects of the model that are learned from training data; they are essentially the internal settings that determine how the model interprets and generates language. LLMs work by predicting the next word in a sequence given the words that precede it, which allows them to generate coherent and contextually relevant text based on a given prompt. This capability can be applied in a variety of ways, from answering questions and composing emails to writing essays and even creating computer code. The training process for these models involves exposing them to a diverse array of text sources, such as books, articles, and websites, allowing them to learn language patterns, grammar, facts about the world, and even styles of writing. However, it's important to note that while LLMs can provide information that seems knowledgeable, their responses are generated based on patterns in the data they were trained on and not from a sentient understanding or awareness. The development and deployment of LLMs raise important considerations regarding accuracy, bias, ethical use, and the potential impact on various aspects of society, including employment, privacy, and misinformation. Researchers and developers continue to work on ways to address these challenges while improving the models' capabilities and applications.\"\"\"\nprint(f\"입력 텍스트:\\n\\n{text}\")\n```\n:::\n\n\n::: {#670e1155 .cell execution_count=6}\n``` {.python .cell-code}\n# chain 실행\nresponse = chain.invoke({\"text\": text})\n\n# 결과 출력\nprint(response)\n```\n:::\n\n\n",
    "supporting": [
      "07-HuggingFace-Local_files"
    ],
    "filters": [],
    "includes": {}
  }
}