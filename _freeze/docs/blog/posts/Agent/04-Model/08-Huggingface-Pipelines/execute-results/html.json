{
  "hash": "7a43b0a65601d58303bea2397d39eab7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"HuggingFace Pipelines\"\nsubtitle: HuggingFace Transformers 파이프라인 활용\ndescription: |\n  HuggingFace Model Hub의 120,000개 이상 모델을 로컬 파이프라인으로 실행하는 방법을 다룬다.\n  PyTorch와 Transformers를 활용한 로컬 모델 실행 및 메모리 효율적인 추론 기법을 설명한다.\ncategories:\n  - AI\n  - RAG\n  - LangChain\nauthor: Kwangmin Kim\ndate: 02/08/2025\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n`HuggingFacePipeline` 클래스를 통해 Hugging Face 모델을 로컬에서 실행할 수 있습니다.\n\n[Hugging Face Model Hub](https://huggingface.co/models)는 온라인 플랫폼에서 120,000개 이상의 모델, 20,000개의 데이터셋, 50,000개의 데모 앱(Spaces)을 호스팅하며, 모두 오픈 소스이고 공개적으로 사용 가능하여 사람들이 쉽게 협업하고 함께 ML을 구축할 수 있습니다.\n\n이러한 모델은 LangChain에서 이 로컬 파이프라인 래퍼를 통해 호출하거나, HuggingFaceHub 클래스를 통해 호스팅된 추론 엔드포인트를 호출하여 사용할 수 있습니다. 호스팅된 파이프라인에 대한 자세한 내용은 [HuggingFaceHub](./huggingface_hub) 노트북을 참조하세요.\n\n\n사용하기 위해서는 [PyTorch](https://pytorch.org/get-started/locally/)와 함께 Python [패키지 transformers](https://pypi.org/project/transformers/)가 설치되어 있어야 합니다.\n\n또한, 보다 메모리 효율적인 attention 구현을 위해 `xformer`를 설치할 수도 있습니다.\n\n::: {#595feed6 .cell execution_count=1}\n``` {.python .cell-code}\n# 설치\n# !pip install -qU transformers\n```\n:::\n\n\n모델을 다운로드 받을 경로를 설정\n\n::: {#c5411cb9 .cell execution_count=2}\n``` {.python .cell-code}\n# 허깅페이스 모델/토크나이저를 다운로드 받을 경로\nimport os\n\n# ./cache/ 경로에 다운로드 받도록 설정\nos.environ[\"HF_HOME\"] = \"./cache/\"\n```\n:::\n\n\n::: {#db9d004c .cell execution_count=3}\n``` {.python .cell-code}\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\nfrom langchain_teddynote.messages import stream_response\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"CH04-Models\")\n```\n:::\n\n\n### Model Loading\n\n모델은 `from_model_id` 메서드를 사용하여 모델 매개변수를 지정함으로써 로드할 수 있습니다.\n\n- `HuggingFacePipeline` 클래스를 사용하여 Hugging Face의 사전 학습된 모델을 로드합니다.\n- `from_model_id` 메서드를 사용하여 `beomi/llama-2-ko-7b` 모델을 지정하고, 작업을 \"text-generation\"으로 설정합니다.\n- `pipeline_kwargs` 매개변수를 사용하여 생성할 최대 토큰 수를 10으로 제한합니다.\n- 로드된 모델은 `hf` 변수에 할당되며, 이를 통해 텍스트 생성 작업을 수행할 수 있습니다.\n\n사용한 모델: https://huggingface.co/beomi/llama-2-ko-7b\n\n::: {#fff5ae2b .cell execution_count=4}\n``` {.python .cell-code}\nfrom langchain_huggingface import HuggingFacePipeline\n\n# HuggingFace 모델을 다운로드 받습니다.\nhf = HuggingFacePipeline.from_model_id(\n    model_id=\"beomi/llama-2-ko-7b\",  # 사용할 모델의 ID를 지정합니다.\n    task=\"text-generation\",  # 수행할 작업을 지정합니다. 여기서는 텍스트 생성입니다.\n    # 파이프라인에 전달할 추가 인자를 설정합니다. 여기서는 생성할 최대 토큰 수를 10으로 제한합니다.\n    pipeline_kwargs={\"max_new_tokens\": 512},\n)\n```\n:::\n\n\n기존의 `transformers` pipeline을 직접 전달하여 로드할 수도 있습니다.\n\nHuggingFacePipeline을 사용하여 텍스트 생성 모델을 구현합니다.\n\n- `AutoTokenizer`와 `AutoModelForCausalLM`을 사용하여 `beomi/llama-2-ko-7b` 모델과 토크나이저를 로드합니다.\n- `pipeline` 함수를 사용하여 \"text-generation\" 파이프라인을 생성하고, 모델과 토크나이저를 설정합니다. 최대 생성 토큰 수는 10으로 제한합니다.\n- `HuggingFacePipeline` 클래스를 사용하여 `hf` 객체를 생성하고, 생성된 파이프라인을 전달합니다.\n\n이렇게 생성된 `hf` 객체를 사용하여 주어진 프롬프트에 대한 텍스트 생성을 수행할 수 있습니다.\n\n::: {#b862ed4a .cell execution_count=5}\n``` {.python .cell-code}\nfrom langchain_huggingface import HuggingFacePipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# huggingface repository에 등록된 모델 ID를 지정합니다.\nmodel_id = \"microsoft/Phi-3-mini-4k-instruct\"\n\n# 토크나이저를 로드합니다.\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# 지정된 모델을 로드합니다.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    # load_in_4bit=True, # bitsandbytes 설치되어 있는 경우(linux)\n    # attn_implementation=\"flash_attention_2\", # ampere GPU 가 있는 경우\n)\n\n# 파이프라인을 생성합니다.\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n\n# HuggingFacePipeline 객체를 생성하고, 생성된 파이프라인을 전달합니다.\nhf = HuggingFacePipeline(pipeline=pipe)\n```\n:::\n\n\n### Gated Model 의 사용\n\nGated Model 은 HuggingFace 에서 라이센스 동의 하에 이용할 수 있는 모델입니다.\n\n사전에 모델 페이지에 접속하여 동의하신 후, Hugging Face 토큰을 발급 받아야 합니다.\n\n아래는 Gated 모델을 사용하기 위한 예제입니다. HuggingFace 에서 발급받은 토큰을 아래와 같이 지정해 주어야 합니다.\n\n::: {#f6215c39 .cell execution_count=6}\n``` {.python .cell-code}\nfrom langchain_huggingface import HuggingFacePipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# huggingface repository에 등록된 모델 ID를 지정합니다.\nmodel_id = \"google/gemma-7b-it\"\n\n# 이곳에 발급 받은 Hugging Face 토큰을 입력합니다.\nyour_huggingface_token = \"hf_............................\"\n\n# 토크나이저를 로드합니다.\ntokenizer = AutoTokenizer.from_pretrained(model_id, token=your_huggingface_token)\n\n# 지정된 모델을 로드합니다.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    token=your_huggingface_token,\n    # load_in_4bit=True, # bitsandbytes 설치되어 있는 경우(linux)\n    # attn_implementation=\"flash_attention_2\", # ampere GPU 가 있는 경우\n)\n\n# 파이프라인을 생성합니다.\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n\n# HuggingFacePipeline 객체를 생성하고, 생성된 파이프라인을 전달합니다.\nhf_llm = HuggingFacePipeline(pipeline=pipe)\n```\n:::\n\n\n실행 및 결과 확인\n\n::: {#12490852 .cell execution_count=7}\n``` {.python .cell-code}\nfor token in hf_llm.stream(\"대한민국의 수도는 어디야?\"):\n    print(token, end=\"\", flush=True)\n```\n:::\n\n\n### Create Chain\n\n모델이 메모리에 로드되면 프롬프트와 함께 구성하여 체인을 형성할 수 있습니다.\n\n- `PromptTemplate` 클래스를 사용하여 질문과 답변 형식을 정의하는 프롬프트 템플릿을 생성합니다.\n- `prompt` 객체와 `hf` 객체를 파이프라인으로 연결하여 `chain` 객체를 생성합니다.\n- `chain.invoke()` 메서드를 호출하여 주어진 질문에 대한 답변을 생성하고 출력합니다.\n\n::: {#ef576ad4 .cell execution_count=8}\n``` {.python .cell-code}\nfrom huggingface_hub import login\n\nlogin()\n```\n:::\n\n\n::: {#0cf27396 .cell execution_count=9}\n``` {.python .cell-code}\nfrom langchain_huggingface import ChatHuggingFace\n\nllm = ChatHuggingFace(llm=hf)\n```\n:::\n\n\n::: {#6a097d86 .cell execution_count=10}\n``` {.python .cell-code}\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\ntemplate = \"\"\"<|system|>You are a helpful assistant.<|end|>\n<|user|>{question}<|end|>\n<|assistant|>\"\"\"  # 질문과 답변 형식을 정의하는 템플릿\n\nprompt = PromptTemplate.from_template(template)  # 템플릿을 사용하여 프롬프트 객체 생성\n\n# 프롬프트와 언어 모델을 연결하여 체인 생성\nchain = prompt | llm | StrOutputParser()\n\nquestion = \"대한민국의 수도는 어디야?\"  # 질문 정의\n\n# print(\n# chain.invoke({\"question\": question})\n# )  # 체인을 호출하여 질문에 대한 답변 생성 및 출력\n\nanswer = chain.stream({\"question\": question})\nstream_response(answer)\n```\n:::\n\n\n### GPU Inference\n\nGPU에서 실행할 때는 `device=n` 매개변수를 지정하여 모델을 특정 디바이스에 배치할 수 있습니다.\n\n기본값은 `-1`로, CPU에서 추론을 수행합니다.\n\n여러 개의 GPU가 있거나 모델이 단일 GPU에 비해 너무 큰 경우에는 `device_map=\"auto\"`를 지정할 수 있습니다.\n\n이 경우 [Accelerate](https://huggingface.co/docs/accelerate/index) 라이브러리가 필요하며, 모델 가중치를 어떻게 로드할지 자동으로 결정하는 데 사용됩니다.\n\n_주의_: `device`와 `device_map`은 함께 지정해서는 안 되며, 예기치 않은 동작을 유발할 수 있습니다.\n\n- `HuggingFacePipeline`을 사용하여 `gpt2` 모델을 로드하고, `device` 매개변수를 0으로 설정하여 GPU에서 실행되도록 합니다.\n- `pipeline_kwargs` 매개변수를 사용하여 생성할 최대 토큰 수를 10으로 제한합니다.\n- `prompt`와 `gpu_llm`을 파이프라인으로 연결하여 `gpu_chain`을 생성합니다.\n- `gpu_chain.invoke()` 메서드를 호출하여 주어진 질문에 대한 답변을 생성하고 출력합니다.\n\n::: {#cfdaf032 .cell execution_count=11}\n``` {.python .cell-code}\ngpu_llm = HuggingFacePipeline.from_model_id(\n    model_id=\"beomi/llama-2-ko-7b\",  # 사용할 모델의 ID를 지정합니다.\n    task=\"text-generation\",  # 수행할 작업을 설정합니다. 여기서는 텍스트 생성입니다.\n    # 사용할 GPU 디바이스 번호를 지정합니다. \"auto\"로 설정하면 accelerate 라이브러리를 사용합니다.\n    device=0,\n    # 파이프라인에 전달할 추가 인자를 설정합니다. 여기서는 생성할 최대 토큰 수를 10으로 제한합니다.\n    pipeline_kwargs={\"max_new_tokens\": 64},\n)\n\ngpu_chain = prompt | gpu_llm  # prompt와 gpu_llm을 연결하여 gpu_chain을 생성합니다.\n\n# 프롬프트와 언어 모델을 연결하여 체인 생성\ngpu_chain = prompt | gpu_llm | StrOutputParser()\n\nquestion = \"대한민국의 수도는 어디야?\"  # 질문 정의\n\n# 체인을 호출하여 질문에 대한 답변 생성 및 출력\nprint(gpu_chain.invoke({\"question\": question}))\n```\n:::\n\n\n### Batch GPU Inference\n\nGPU 장치에서 실행하는 경우, 배치 모드로 GPU에서 추론을 실행할 수 있습니다.\n\n- `HuggingFacePipeline`을 사용하여 `beomi/llama-2-ko-7b` 모델을 로드하고, GPU에서 실행되도록 설정합니다.\n- `gpu_llm`을 생성할 때 `batch_size`를 2로 설정하고, `temperature`를 0으로, `max_length`를 64로 설정합니다.\n- `prompt`와 `gpu_llm`을 파이프라인으로 연결하여 `gpu_chain`을 생성하고, 종료 토큰을 \"\\n\\n\"로 설정합니다.\n- `gpu_chain.batch()`를 사용하여 `questions`의 질문들에 대한 답변을 병렬로 생성합니다.\n- 생성된 답변을 반복문을 통해 출력합니다.\n\n::: {#e136f8e3 .cell execution_count=12}\n``` {.python .cell-code}\ngpu_llm = HuggingFacePipeline.from_model_id(\n    model_id=\"beomi/llama-2-ko-7b\",  # 사용할 모델의 ID를 지정합니다.\n    task=\"text-generation\",  # 수행할 작업을 설정합니다.\n    device=0,  # GPU 디바이스 번호를 지정합니다. -1은 CPU를 의미합니다.\n    batch_size=2,  # 배치 크기s를 조정합니다. GPU 메모리와 모델 크기에 따라 적절히 설정합니다.\n    model_kwargs={\n        \"temperature\": 0,\n        \"max_length\": 256,\n    },  # 모델에 전달할 추가 인자를 설정합니다.\n)\n\n# 프롬프트와 언어 모델을 연결하여 체인을 생성합니다.\ngpu_chain = prompt | gpu_llm.bind(stop=[\"\\n\\n\"])\n\nquestions = []\nfor i in range(4):\n    # 질문 리스트를 생성합니다.\n    questions.append({\"question\": f\"숫자 {i} 이 한글로 뭐에요?\"})\n\nanswers = gpu_chain.batch(questions)  # 질문 리스트를 배치 처리하여 답변을 생성합니다.\nfor answer in answers:\n    print(answer)  # 생성된 답변을 출력합니다.\n```\n:::\n\n\n",
    "supporting": [
      "08-Huggingface-Pipelines_files"
    ],
    "filters": [],
    "includes": {}
  }
}