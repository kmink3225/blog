{
  "hash": "29b67d707c5141a154cfcfaeb2969841",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Document Intelligence\"\nsubtitle: OCR ë° ë¬¸ì„œ ë ˆì´ì•„ì›ƒ ë¶„ì„\ndescription: |\n  Azure Document Intelligenceë¥¼ í™œìš©í•œ ë¬¸ì„œ OCR ë° ë ˆì´ì•„ì›ƒ ë¶„ì„ ë°©ë²•ì„ ì„¤ëª…í•œë‹¤.\ncategories:\n  - AI\n  - RAG\n  - Azure\nauthor: Kwangmin Kim\ndate: 11/03/2025\nformat: \n  html:\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n## Azure Document Intelligenceë€?\n\n* Azure Document Intelligence(ì´ì „ Form Recognizer)ëŠ” AI ê¸°ë°˜ ë¬¸ì„œ ì´í•´ ì„œë¹„ìŠ¤ë¡œ, ìŠ¤ìº” ë¬¸ì„œë‚˜ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸, í‘œ, êµ¬ì¡°ë¥¼ ì¶”ì¶œí•œë‹¤. \n* Azure Document Intelligenceì˜ ì£¼ìš” ê¸°ëŠ¥\n    * **ë¬¸ì„œ ë¶„ì„ ëª¨ë¸**\n    * **ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸**\n    * **ì‚¬ìš©ì ì§€ì • ëª¨ë¸** \n* RAG ì‹œìŠ¤í…œì—ì„œ PDF, ì´ë¯¸ì§€ ë“±ì˜ ë¹„ì •í˜• ë¬¸ì„œë¥¼ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•µì‹¬ ì—­í• ì„ í•œë‹¤.\n\n**RAG íŒŒì´í”„ë¼ì¸ì—ì„œì˜ ì—­í• :**\n- ìŠ¤ìº” PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)\n- ì´ë¯¸ì§€ ì† í…ìŠ¤íŠ¸ ì¸ì‹\n- í‘œ, ì œëª©, ë‹¨ë½ ë“± ë¬¸ì„œ êµ¬ì¡° íŒŒì•…\n- í•œê¸€ ë° ë‹¤êµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬\n\n**í•µì‹¬ ê¸°ëŠ¥:**\n- **ë†’ì€ OCR ì •í™•ë„**: í•œêµ­ì–´ 95% ì´ìƒ\n- **ë ˆì´ì•„ì›ƒ ë¶„ì„**: ì œëª©, ë‹¨ë½, í‘œ, ë¦¬ìŠ¤íŠ¸ êµ¬ë¶„\n- **ë‹¤êµ­ì–´ ì§€ì›**: 100ê°œ ì´ìƒ ì–¸ì–´ (í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ë“±)\n- **ì‚¬ì „ í›ˆë ¨ ëª¨ë¸**: ì˜ìˆ˜ì¦, ëª…í•¨, ì²­êµ¬ì„œ ë“± íŠ¹í™” ëª¨ë¸ ì œê³µ\n\n## Document Intelligence ëª¨ë¸\n\nAzure Document IntelligenceëŠ” ë¬¸ì„œ ìœ í˜•ì— ë”°ë¼ ë‹¤ì–‘í•œ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ì„ ì œê³µí•œë‹¤:\n\n### Layout API (ë²”ìš© ë ˆì´ì•„ì›ƒ ë¶„ì„)\n**ìš©ë„**: ëª¨ë“  ì¢…ë¥˜ì˜ ë¬¸ì„œ ë ˆì´ì•„ì›ƒ ë¶„ì„\n\n**ì¶”ì¶œ ì •ë³´:**\n- í…ìŠ¤íŠ¸ (OCR)\n- í‘œ (í–‰, ì—´ êµ¬ì¡°)\n- ì œëª© ë° ì„¹ì…˜\n- ë‹¨ë½ ë° ì¤„ë°”ê¿ˆ\n- ì„ íƒ ë§ˆí¬ (ì²´í¬ë°•ìŠ¤)\n\n**RAG ì‹œìŠ¤í…œ ê¶Œì¥**: ê°€ì¥ ë§ì´ ì‚¬ìš©\n\n**ê°€ê²©**: í˜ì´ì§€ë‹¹ $0.01\n\n### Read API (í…ìŠ¤íŠ¸ ì¶”ì¶œ ì „ìš©)\n**ìš©ë„**: ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ë¬´ì‹œ)\n\n**ì¶”ì¶œ ì •ë³´:**\n- í…ìŠ¤íŠ¸ë§Œ (êµ¬ì¡° ì •ë³´ ì—†ìŒ)\n- ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„\n\n**RAG ì‹œìŠ¤í…œ ê¶Œì¥**: ê°„ë‹¨í•œ ë¬¸ì„œ, ë¹„ìš© ì ˆê° í•„ìš” ì‹œ\n\n**ê°€ê²©**: í˜ì´ì§€ë‹¹ $0.0015\n\n### Prebuilt Models (íŠ¹í™” ëª¨ë¸)\n| ëª¨ë¸ | ìš©ë„ | ì¶”ì¶œ ì •ë³´ |\n|------|------|-----------|\n| **Invoice** | ì²­êµ¬ì„œ, ì„¸ê¸ˆê³„ì‚°ì„œ | ë‚ ì§œ, ê¸ˆì•¡, í’ˆëª©, ê³µê¸‰ì |\n| **Receipt** | ì˜ìˆ˜ì¦ | ìƒì ëª…, ë‚ ì§œ, ì´ì•¡, í•­ëª© |\n| **ID Document** | ì‹ ë¶„ì¦, ì—¬ê¶Œ | ì´ë¦„, ìƒë…„ì›”ì¼, ì£¼ì†Œ |\n| **Business Card** | ëª…í•¨ | ì´ë¦„, ì§í•¨, ì—°ë½ì²˜ |\n| **W-2** | ë¯¸êµ­ ì„¸ê¸ˆ ì–‘ì‹ | ê¸‰ì—¬, ì„¸ê¸ˆ |\n\n**RAG ì‹œìŠ¤í…œ ê¶Œì¥**: íŠ¹ì • ë¬¸ì„œ íƒ€ì…ë§Œ ì²˜ë¦¬ ì‹œ ìœ ìš©\n\n## Document Intelligence ë¦¬ì†ŒìŠ¤ ìƒì„±\n\n### Azure Portalì—ì„œ ìƒì„±\n\n**ë¦¬ì†ŒìŠ¤ ë§Œë“¤ê¸°:**\n- [portal.azure.com](https://portal.azure.com) â†’ \"ë¦¬ì†ŒìŠ¤ ë§Œë“¤ê¸°\"\n- \"Document Intelligence\" ê²€ìƒ‰ ë° ì„ íƒ\n\n**ê¸°ë³¸ ì„¤ì •:**\n- **êµ¬ë…**: ì‚¬ìš©í•  êµ¬ë… ì„ íƒ\n- **ë¦¬ì†ŒìŠ¤ ê·¸ë£¹**: `rg-rag-prod`\n- **ì§€ì—­**: Korea Central (í•œêµ­ì–´ ì²˜ë¦¬ ìµœì í™”)\n- **ì´ë¦„**: `doc-intel-rag-prod`\n- **ê°€ê²© ì±…ì • ê³„ì¸µ**:\n  - **Free F0**: 500í˜ì´ì§€/ì›” ë¬´ë£Œ (ê°œë°œ/í…ŒìŠ¤íŠ¸)\n  - **Standard S0**: ì¢…ëŸ‰ì œ (í”„ë¡œë•ì…˜)\n\n**ê²€í†  + ë§Œë“¤ê¸°** â†’ ìƒì„± ì™„ë£Œ\n\n**í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ í™•ì¸:**\n- ìƒì„±ëœ ë¦¬ì†ŒìŠ¤ â†’ \"í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸\"\n- **KEY 1** ë° **ì—”ë“œí¬ì¸íŠ¸** ë³µì‚¬\n\n### Azure CLIë¡œ ìƒì„±\n\n```bash\n# Document Intelligence ë¦¬ì†ŒìŠ¤ ìƒì„±\naz cognitiveservices account create \\\n    --name doc-intel-rag-prod \\\n    --resource-group rg-rag-prod \\\n    --kind FormRecognizer \\\n    --sku S0 \\\n    --location koreacentral \\\n    --yes\n\n# í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ ì¡°íšŒ\naz cognitiveservices account keys list \\\n    --name doc-intel-rag-prod \\\n    --resource-group rg-rag-prod\n\naz cognitiveservices account show \\\n    --name doc-intel-rag-prod \\\n    --resource-group rg-rag-prod \\\n    --query properties.endpoint\n```\n\n## í™˜ê²½ ì„¤ì •\n\n### Python SDK ì„¤ì¹˜\n\n```bash\npip install azure-ai-formrecognizer\npip install azure-storage-blob\npip install python-dotenv\n```\n\n### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n\n`.env` íŒŒì¼:\n```\nAZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://doc-intel-rag-prod.cognitiveservices.azure.com/\nAZURE_DOCUMENT_INTELLIGENCE_KEY=your-key-here\n```\n\n## Layout API ì‚¬ìš©ë²•\n\n### ë¡œì»¬ íŒŒì¼ ë¶„ì„\n\n::: {#77e856fc .cell execution_count=1}\n``` {.python .cell-code}\nfrom azure.ai.formrecognizer import DocumentAnalysisClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# í´ë¼ì´ì–¸íŠ¸ ìƒì„±\nendpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\nkey = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n\ndocument_analysis_client = DocumentAnalysisClient(\n    endpoint=endpoint,\n    credential=AzureKeyCredential(key)\n)\n\n# ë¡œì»¬ PDF íŒŒì¼ ë¶„ì„\nwith open(\"sample.pdf\", \"rb\") as f:\n    poller = document_analysis_client.begin_analyze_document(\n        \"prebuilt-layout\", document=f\n    )\n    result = poller.result()\n\n# ê²°ê³¼ ì¶œë ¥\nprint(f\"ë¶„ì„ëœ í˜ì´ì§€ ìˆ˜: {len(result.pages)}\")\nprint(f\"ì¶”ì¶œëœ ë‹¨ë½ ìˆ˜: {len(result.paragraphs)}\")\nprint(f\"ì¶”ì¶œëœ í‘œ ìˆ˜: {len(result.tables)}\")\n```\n:::\n\n\n### URLì—ì„œ ë¬¸ì„œ ë¶„ì„\n\n::: {#e256fad7 .cell execution_count=2}\n``` {.python .cell-code}\n# Azure Blob Storage URLë¡œ ë¶„ì„\ndocument_url = \"https://stragdocs2025.blob.core.windows.net/rag-documents/sample.pdf?<sas-token>\"\n\npoller = document_analysis_client.begin_analyze_document_from_url(\n    \"prebuilt-layout\", document_url=document_url\n)\nresult = poller.result()\n\nprint(\"ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ\")\n```\n:::\n\n\n## í…ìŠ¤íŠ¸ ì¶”ì¶œ\n\n### ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n\n::: {#e8eddc15 .cell execution_count=3}\n``` {.python .cell-code}\ndef extract_full_text(result):\n    \"\"\"ë¬¸ì„œì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n    full_text = []\n    \n    # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n    for page in result.pages:\n        page_text = []\n        for line in page.lines:\n            page_text.append(line.content)\n        \n        full_text.append(\"\\n\".join(page_text))\n    \n    return \"\\n\\n\".join(full_text)\n\n# ì‚¬ìš© ì˜ˆì‹œ\ntext = extract_full_text(result)\nprint(f\"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ì• 500ì):\\n{text[:500]}\")\n```\n:::\n\n\n### ë‹¨ë½ ë‹¨ìœ„ ì¶”ì¶œ\n\n::: {#085de46f .cell execution_count=4}\n``` {.python .cell-code}\ndef extract_paragraphs(result):\n    \"\"\"ë‹¨ë½ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ìœ ì§€)\"\"\"\n    paragraphs = []\n    \n    for paragraph in result.paragraphs:\n        paragraphs.append({\n            \"content\": paragraph.content,\n            \"role\": paragraph.role,  # title, sectionHeading, paragraph ë“±\n            \"page_number\": paragraph.bounding_regions[0].page_number if paragraph.bounding_regions else None\n        })\n    \n    return paragraphs\n\n# ì‚¬ìš© ì˜ˆì‹œ\nparagraphs = extract_paragraphs(result)\nprint(f\"ì´ {len(paragraphs)}ê°œ ë‹¨ë½ ì¶”ì¶œ\")\n\n# ì œëª©ë§Œ ì¶”ì¶œ\ntitles = [p for p in paragraphs if p[\"role\"] == \"title\"]\nprint(f\"\\në¬¸ì„œ ì œëª©ë“¤:\")\nfor title in titles:\n    print(f\"- {title['content']} (í˜ì´ì§€ {title['page_number']})\")\n```\n:::\n\n\n## í‘œ ì¶”ì¶œ\n\n### í‘œ êµ¬ì¡° íŒŒì‹±\n\n::: {#6e10546e .cell execution_count=5}\n``` {.python .cell-code}\ndef extract_tables(result):\n    \"\"\"ë¬¸ì„œì—ì„œ í‘œ ì¶”ì¶œ\"\"\"\n    tables_data = []\n    \n    for table_idx, table in enumerate(result.tables):\n        # í‘œ ë©”íƒ€ë°ì´í„°\n        table_info = {\n            \"table_id\": table_idx + 1,\n            \"row_count\": table.row_count,\n            \"column_count\": table.column_count,\n            \"page_number\": table.bounding_regions[0].page_number,\n            \"cells\": []\n        }\n        \n        # ì…€ ë°ì´í„°\n        for cell in table.cells:\n            table_info[\"cells\"].append({\n                \"row_index\": cell.row_index,\n                \"column_index\": cell.column_index,\n                \"content\": cell.content,\n                \"kind\": cell.kind  # columnHeader, rowHeader, content, stub\n            })\n        \n        tables_data.append(table_info)\n    \n    return tables_data\n\n# ì‚¬ìš© ì˜ˆì‹œ\ntables = extract_tables(result)\nprint(f\"ì¶”ì¶œëœ í‘œ ê°œìˆ˜: {len(tables)}\")\n\nfor table in tables:\n    print(f\"\\n[í‘œ {table['table_id']}] í˜ì´ì§€ {table['page_number']}\")\n    print(f\"í¬ê¸°: {table['row_count']}í–‰ Ã— {table['column_count']}ì—´\")\n```\n:::\n\n\n### í‘œë¥¼ Pandas DataFrameìœ¼ë¡œ ë³€í™˜\n\n::: {#3928d04b .cell execution_count=6}\n``` {.python .cell-code}\nimport pandas as pd\n\ndef table_to_dataframe(table):\n    \"\"\"í‘œë¥¼ Pandas DataFrameìœ¼ë¡œ ë³€í™˜\"\"\"\n    # í‘œ ì´ˆê¸°í™” (ë¹ˆ ì…€ í¬í•¨)\n    data = [[None] * table[\"column_count\"] for _ in range(table[\"row_count\"])]\n    \n    # ì…€ ë°ì´í„° ì±„ìš°ê¸°\n    for cell in table[\"cells\"]:\n        data[cell[\"row_index\"]][cell[\"column_index\"]] = cell[\"content\"]\n    \n    # DataFrame ìƒì„± (ì²« í–‰ì„ í—¤ë”ë¡œ)\n    df = pd.DataFrame(data[1:], columns=data[0])\n    return df\n\n# ì‚¬ìš© ì˜ˆì‹œ\nif tables:\n    df = table_to_dataframe(tables[0])\n    print(\"\\ní‘œ ë°ì´í„° (DataFrame):\")\n    print(df)\n```\n:::\n\n\n## ë ˆì´ì•„ì›ƒ ê¸°ë°˜ ë¬¸ì„œ ë¶„í• \n\nRAG ì‹œìŠ¤í…œì—ì„œëŠ” ë¬¸ì„œ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ì²­í¬ ë¶„í• ì´ ì¤‘ìš”í•˜ë‹¤.\n\n::: {#16eeb874 .cell execution_count=7}\n``` {.python .cell-code}\nfrom langchain_core.documents import Document\nfrom typing import List\n\ndef split_by_layout(result, max_chunk_size: int = 1000) -> List[Document]:\n    \"\"\"ë ˆì´ì•„ì›ƒ ê¸°ë°˜ ë¬¸ì„œ ë¶„í• \"\"\"\n    documents = []\n    current_chunk = []\n    current_size = 0\n    current_page = 1\n    \n    for paragraph in result.paragraphs:\n        # ë‹¨ë½ ì •ë³´\n        content = paragraph.content\n        role = paragraph.role or \"paragraph\"\n        page_num = paragraph.bounding_regions[0].page_number if paragraph.bounding_regions else current_page\n        \n        # ì œëª©ì€ ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘\n        if role in [\"title\", \"sectionHeading\"] and current_chunk:\n            # ì´ì „ ì²­í¬ ì €ì¥\n            doc = Document(\n                page_content=\"\\n\\n\".join(current_chunk),\n                metadata={\n                    \"page\": current_page,\n                    \"chunk_type\": \"section\"\n                }\n            )\n            documents.append(doc)\n            current_chunk = []\n            current_size = 0\n        \n        # í˜„ì¬ ë‹¨ë½ ì¶”ê°€\n        current_chunk.append(content)\n        current_size += len(content)\n        current_page = page_num\n        \n        # ìµœëŒ€ í¬ê¸° ì´ˆê³¼ ì‹œ ì²­í¬ ë¶„í• \n        if current_size >= max_chunk_size:\n            doc = Document(\n                page_content=\"\\n\\n\".join(current_chunk),\n                metadata={\n                    \"page\": current_page,\n                    \"chunk_type\": \"paragraph\"\n                }\n            )\n            documents.append(doc)\n            current_chunk = []\n            current_size = 0\n    \n    # ë§ˆì§€ë§‰ ì²­í¬\n    if current_chunk:\n        doc = Document(\n            page_content=\"\\n\\n\".join(current_chunk),\n            metadata={\n                \"page\": current_page,\n                \"chunk_type\": \"paragraph\"\n            }\n        )\n        documents.append(doc)\n    \n    return documents\n\n# ì‚¬ìš© ì˜ˆì‹œ\ndocuments = split_by_layout(result, max_chunk_size=1000)\nprint(f\"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(documents)}\")\nprint(f\"\\nì²« ë²ˆì§¸ ì²­í¬ (ì• 300ì):\\n{documents[0].page_content[:300]}\")\n```\n:::\n\n\n## Azure Blob Storage ì—°ë™\n\n### Blobì—ì„œ ì§ì ‘ ë¶„ì„\n\n::: {#dc5101f1 .cell execution_count=8}\n``` {.python .cell-code}\nfrom azure.storage.blob import BlobServiceClient\n\n# Blob Storage í´ë¼ì´ì–¸íŠ¸\nblob_service_client = BlobServiceClient.from_connection_string(\n    os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n)\n\ndef analyze_blob_document(container_name: str, blob_name: str):\n    \"\"\"Blob Storageì˜ ë¬¸ì„œë¥¼ Document Intelligenceë¡œ ë¶„ì„\"\"\"\n    \n    # Blob SAS URL ìƒì„±\n    from azure.storage.blob import generate_blob_sas, BlobSasPermissions\n    from datetime import datetime, timedelta\n    \n    sas_token = generate_blob_sas(\n        account_name=\"stragdocs2025\",\n        container_name=container_name,\n        blob_name=blob_name,\n        account_key=os.getenv(\"AZURE_STORAGE_KEY\"),\n        permission=BlobSasPermissions(read=True),\n        expiry=datetime.utcnow() + timedelta(hours=1)\n    )\n    \n    blob_url = f\"https://stragdocs2025.blob.core.windows.net/{container_name}/{blob_name}?{sas_token}\"\n    \n    # Document Intelligenceë¡œ ë¶„ì„\n    poller = document_analysis_client.begin_analyze_document_from_url(\n        \"prebuilt-layout\", document_url=blob_url\n    )\n    result = poller.result()\n    \n    return result\n\n# ì‚¬ìš© ì˜ˆì‹œ\nresult = analyze_blob_document(\"rag-documents\", \"sample.pdf\")\nprint(\"Blob ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ\")\n```\n:::\n\n\n### ë°°ì¹˜ ì²˜ë¦¬\n\n::: {#40fafa10 .cell execution_count=9}\n``` {.python .cell-code}\ndef analyze_all_blobs(container_name: str):\n    \"\"\"ì»¨í…Œì´ë„ˆì˜ ëª¨ë“  ë¬¸ì„œ ë¶„ì„\"\"\"\n    container_client = blob_service_client.get_container_client(container_name)\n    blob_list = container_client.list_blobs()\n    \n    results = []\n    for blob in blob_list:\n        # PDF íŒŒì¼ë§Œ ì²˜ë¦¬\n        if blob.name.endswith('.pdf'):\n            print(f\"ë¶„ì„ ì¤‘: {blob.name}\")\n            try:\n                result = analyze_blob_document(container_name, blob.name)\n                text = extract_full_text(result)\n                \n                results.append({\n                    \"blob_name\": blob.name,\n                    \"page_count\": len(result.pages),\n                    \"text\": text,\n                    \"status\": \"success\"\n                })\n            except Exception as e:\n                print(f\"ì˜¤ë¥˜: {blob.name} - {str(e)}\")\n                results.append({\n                    \"blob_name\": blob.name,\n                    \"status\": \"error\",\n                    \"error\": str(e)\n                })\n    \n    return results\n\n# ì‚¬ìš© ì˜ˆì‹œ\n# results = analyze_all_blobs(\"rag-documents\")\n# print(f\"ì´ {len(results)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ\")\n```\n:::\n\n\n## Read API ì‚¬ìš© (ê²½ëŸ‰ OCR)\n\në‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œë§Œ í•„ìš”í•  ê²½ìš° Read APIê°€ ë” ë¹ ë¥´ê³  ì €ë ´í•˜ë‹¤.\n\n::: {#1db25855 .cell execution_count=10}\n``` {.python .cell-code}\n# Read APIë¡œ ë¬¸ì„œ ë¶„ì„\nwith open(\"simple_document.pdf\", \"rb\") as f:\n    poller = document_analysis_client.begin_analyze_document(\n        \"prebuilt-read\", document=f\n    )\n    result = poller.result()\n\n# í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ë¬´ì‹œ)\nfull_text = result.content\nprint(f\"ì¶”ì¶œëœ í…ìŠ¤íŠ¸:\\n{full_text}\")\n```\n:::\n\n\n**Layout vs Read ë¹„êµ:**\n| í•­ëª© | Layout API | Read API |\n|------|-----------|----------|\n| **ê°€ê²©** | $0.01/í˜ì´ì§€ | $0.0015/í˜ì´ì§€ |\n| **ì†ë„** | ëŠë¦¼ | ë¹ ë¦„ |\n| **êµ¬ì¡° ì •ë³´** | âœ… (ì œëª©, í‘œ, ë‹¨ë½) | âŒ |\n| **RAG ê¶Œì¥** | ë³µì¡í•œ ë¬¸ì„œ | ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¬¸ì„œ |\n\n## í•œêµ­ì–´ ë¬¸ì„œ ìµœì í™”\n\n### ì–¸ì–´ íŒíŠ¸ ì œê³µ\n\n::: {#f843862f .cell execution_count=11}\n``` {.python .cell-code}\n# í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ (ì–¸ì–´ íŒíŠ¸)\nwith open(\"korean_document.pdf\", \"rb\") as f:\n    poller = document_analysis_client.begin_analyze_document(\n        \"prebuilt-layout\",\n        document=f,\n        locale=\"ko-KR\"  # í•œêµ­ì–´ íŒíŠ¸\n    )\n    result = poller.result()\n\nprint(\"í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ\")\n```\n:::\n\n\n### í•œê¸€ OCR í›„ì²˜ë¦¬\n\n::: {#274cfa55 .cell execution_count=12}\n``` {.python .cell-code}\nimport re\n\ndef clean_korean_text(text: str) -> str:\n    \"\"\"í•œê¸€ OCR ê²°ê³¼ ì •ë¦¬\"\"\"\n    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # ì¤„ë°”ê¿ˆ ì •ë¦¬\n    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n    \n    # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬\n    text = text.replace('ã€ƒ', '\"').replace('ã€ƒ', '\"')\n    \n    return text.strip()\n\n# ì‚¬ìš© ì˜ˆì‹œ\nraw_text = extract_full_text(result)\ncleaned_text = clean_korean_text(raw_text)\nprint(f\"ì •ë¦¬ëœ í…ìŠ¤íŠ¸:\\n{cleaned_text[:500]}\")\n```\n:::\n\n\n## LangChain í†µí•©\n\n### AzureAIDocumentIntelligenceLoader\n\n::: {#cc5980df .cell execution_count=13}\n``` {.python .cell-code}\nfrom langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n\n# Document Intelligence Loader ìƒì„±\nloader = AzureAIDocumentIntelligenceLoader(\n    api_endpoint=endpoint,\n    api_key=key,\n    file_path=\"sample.pdf\",\n    api_model=\"prebuilt-layout\"\n)\n\n# ë¬¸ì„œ ë¡œë”©\ndocuments = loader.load()\n\nprint(f\"ë¡œë”©ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}\")\nprint(f\"ì²« ë²ˆì§¸ ì²­í¬:\\n{documents[0].page_content[:300]}\")\nprint(f\"ë©”íƒ€ë°ì´í„°: {documents[0].metadata}\")\n```\n:::\n\n\n### RAG íŒŒì´í”„ë¼ì¸ í†µí•©\n\n::: {#26f898ce .cell execution_count=14}\n``` {.python .cell-code}\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Document Intelligenceë¡œ ë¬¸ì„œ ë¡œë”©\nloader = AzureAIDocumentIntelligenceLoader(\n    api_endpoint=endpoint,\n    api_key=key,\n    file_path=\"long_document.pdf\",\n    api_model=\"prebuilt-layout\"\n)\ndocuments = loader.load()\n\n# í…ìŠ¤íŠ¸ ë¶„í•  (ë ˆì´ì•„ì›ƒ ê³ ë ¤)\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\nsplits = text_splitter.split_documents(documents)\n\nprint(f\"ì´ {len(splits)}ê°œ ì²­í¬ ìƒì„±\")\n```\n:::\n\n\n## ë¹„ìš© ìµœì í™”\n\n### ìºì‹± ì „ëµ\n\n::: {#dc139f75 .cell execution_count=15}\n``` {.python .cell-code}\nimport json\nimport hashlib\n\ndef get_cache_key(file_path: str) -> str:\n    \"\"\"íŒŒì¼ í•´ì‹œë¡œ ìºì‹œ í‚¤ ìƒì„±\"\"\"\n    with open(file_path, \"rb\") as f:\n        file_hash = hashlib.md5(f.read()).hexdigest()\n    return f\"doc_intel_{file_hash}\"\n\ndef analyze_with_cache(file_path: str):\n    \"\"\"ìºì‹±ì„ ì‚¬ìš©í•œ ë¬¸ì„œ ë¶„ì„\"\"\"\n    cache_key = get_cache_key(file_path)\n    cache_file = f\".cache/{cache_key}.json\"\n    \n    # ìºì‹œ í™•ì¸\n    try:\n        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n            cached_result = json.load(f)\n            print(\"ìºì‹œì—ì„œ ê²°ê³¼ ë¡œë”©\")\n            return cached_result\n    except FileNotFoundError:\n        pass\n    \n    # Document Intelligence ì‹¤í–‰\n    with open(file_path, \"rb\") as f:\n        poller = document_analysis_client.begin_analyze_document(\n            \"prebuilt-layout\", document=f\n        )\n        result = poller.result()\n    \n    # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n    result_dict = {\n        \"content\": result.content,\n        \"pages\": [{\"page_number\": p.page_number, \"width\": p.width, \"height\": p.height} for p in result.pages],\n        \"paragraphs\": [{\"content\": p.content, \"role\": p.role} for p in result.paragraphs]\n    }\n    \n    # ìºì‹œ ì €ì¥\n    os.makedirs(\".cache\", exist_ok=True)\n    with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(result_dict, f, ensure_ascii=False, indent=2)\n    \n    print(\"Document Intelligence ì‹¤í–‰ ë° ìºì‹œ ì €ì¥\")\n    return result_dict\n\n# ì‚¬ìš© ì˜ˆì‹œ\n# cached_result = analyze_with_cache(\"sample.pdf\")\n```\n:::\n\n\n### ë°°ì¹˜ í¬ê¸° ì¡°ì •\n\n::: {#cb7da4e7 .cell execution_count=16}\n``` {.python .cell-code}\ndef analyze_documents_batch(file_paths: List[str], batch_size: int = 5):\n    \"\"\"ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¸ì„œ ë¶„ì„ (API ì œí•œ ê³ ë ¤)\"\"\"\n    import time\n    \n    results = []\n    for i in range(0, len(file_paths), batch_size):\n        batch = file_paths[i:i+batch_size]\n        \n        print(f\"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ íŒŒì¼)\")\n        for file_path in batch:\n            with open(file_path, \"rb\") as f:\n                poller = document_analysis_client.begin_analyze_document(\n                    \"prebuilt-layout\", document=f\n                )\n                result = poller.result()\n                results.append({\n                    \"file\": file_path,\n                    \"result\": result\n                })\n        \n        # API ì œí•œ ë°©ì§€ (ì´ˆë‹¹ 15 ìš”ì²­)\n        time.sleep(1)\n    \n    return results\n\n# ì‚¬ìš© ì˜ˆì‹œ\n# file_list = [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]\n# results = analyze_documents_batch(file_list, batch_size=5)\n```\n:::\n\n\n## ëª¨ë‹ˆí„°ë§\n\n### ë¶„ì„ í†µê³„ ì¶”ì \n\n::: {#cad9098d .cell execution_count=17}\n``` {.python .cell-code}\ndef analyze_with_metrics(file_path: str):\n    \"\"\"ë¶„ì„ ë©”íŠ¸ë¦­ ì¶”ì \"\"\"\n    import time\n    \n    start_time = time.time()\n    \n    with open(file_path, \"rb\") as f:\n        file_size = os.path.getsize(file_path)\n        \n        poller = document_analysis_client.begin_analyze_document(\n            \"prebuilt-layout\", document=f\n        )\n        result = poller.result()\n    \n    end_time = time.time()\n    duration = end_time - start_time\n    \n    metrics = {\n        \"file_path\": file_path,\n        \"file_size_mb\": file_size / (1024 * 1024),\n        \"page_count\": len(result.pages),\n        \"duration_seconds\": duration,\n        \"pages_per_second\": len(result.pages) / duration\n    }\n    \n    print(f\"ë¶„ì„ ì™„ë£Œ:\")\n    print(f\"- íŒŒì¼ í¬ê¸°: {metrics['file_size_mb']:.2f} MB\")\n    print(f\"- í˜ì´ì§€ ìˆ˜: {metrics['page_count']}\")\n    print(f\"- ì²˜ë¦¬ ì‹œê°„: {metrics['duration_seconds']:.2f}ì´ˆ\")\n    print(f\"- ì†ë„: {metrics['pages_per_second']:.2f} í˜ì´ì§€/ì´ˆ\")\n    \n    return result, metrics\n\n# ì‚¬ìš© ì˜ˆì‹œ\n# result, metrics = analyze_with_metrics(\"large_document.pdf\")\n```\n:::\n\n\n## ë¬¸ì œ í•´ê²°\n\n### ì¼ë°˜ì ì¸ ì˜¤ë¥˜\n\n**InvalidRequest: íŒŒì¼ í¬ê¸° ì´ˆê³¼**\n```\nì˜¤ë¥˜: íŒŒì¼ì´ ë„ˆë¬´ í¼ (ìµœëŒ€ 500MB)\ní•´ê²°: íŒŒì¼ì„ ë¶„í• í•˜ê±°ë‚˜ ì••ì¶•\n```\n\n**InvalidImage: ì´ë¯¸ì§€ í•´ìƒë„ ë¶€ì¡±**\n```python\n# í•´ê²°: ì´ë¯¸ì§€ í’ˆì§ˆ í™•ì¸ (ìµœì†Œ 150 DPI ê¶Œì¥)\nfrom PIL import Image\n\nimg = Image.open(\"low_quality.png\")\nprint(f\"í•´ìƒë„: {img.size}\")\nprint(f\"DPI: {img.info.get('dpi', 'Unknown')}\")\n```\n\n**Unauthorized: ì¸ì¦ ì‹¤íŒ¨**\n```python\n# í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ í™•ì¸\nprint(f\"Endpoint: {endpoint}\")\nprint(f\"Key: {key[:10]}... (ê¸¸ì´: {len(key)})\")\n```\n\n## ì°¸ê³  ìë£Œ\n\n### ê³µì‹ ë¬¸ì„œ\n- [Document Intelligence ê°œìš”](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/)\n- [Python SDK ì°¸ì¡°](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-formrecognizer-readme)\n- [ëª¨ë¸ ê°€ì´ë“œ](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-model-overview)\n\n### ìƒ˜í”Œ ì½”ë“œ\n- [Document Intelligence Samples](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/formrecognizer/azure-ai-formrecognizer/samples)\n- [LangChain í†µí•©](https://python.langchain.com/docs/integrations/document_loaders/azure_document_intelligence)\n\n## ë‹¤ìŒ ë‹¨ê³„\n\në¬¸ì„œ OCR ë° ë ˆì´ì•„ì›ƒ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆë‹¤ë©´, ì´ì œ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì:\n\nğŸ‘‰ [03-Azure-OpenAI-Embeddings.qmd](./03-Azure-OpenAI-Embeddings.qmd) - Azure OpenAIë¡œ ë¬¸ì„œ ì„ë² ë”© ìƒì„±\n\n",
    "supporting": [
      "02-Document-Intelligence_files"
    ],
    "filters": [],
    "includes": {}
  }
}