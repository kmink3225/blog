{
  "hash": "40cc45294507a8bf816872f76c09dad5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"LangChain to LangGraph\"\nsubtitle: RAG êµ¬í˜„ - LangChainì—ì„œ LangGraphë¡œ ì „í™˜\ndescription: |\n  LangChain ì²´ì¸ì—ì„œ LangGraph ìƒíƒœ ë¨¸ì‹ ìœ¼ë¡œ RAG ì‹œìŠ¤í…œì„ ì „í™˜í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¬ë‹¤.\ncategories:\n  - AI\n  - RAG\n  - LangChain\n  - LangGraph\nauthor: Kwangmin Kim\ndate: 11/06/2025\nformat: \n  html:\n    page-layout: full\n    code-fold: true\n    toc: true\n    number-sections: true\ndraft: False\nexecute:\n    eval: false\n---\n\n## LangChain vs LangGraph\n\n### LangChainì˜ í•œê³„\n\n**LangChain (LCEL):**\n- ì„ í˜•ì ì¸ ì²´ì¸ êµ¬ì¡°\n- ë‹¨ë°©í–¥ ë°ì´í„° íë¦„\n- ì¡°ê±´ë¶€ ë¡œì§ êµ¬í˜„ ì–´ë ¤ì›€\n- ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° ì œí•œì \n\n```python\n# LangChain ì²´ì¸ ì˜ˆì‹œ\nchain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n```\n\n**ë¬¸ì œì :**\n- ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë¶ˆê°€\n- ë‹µë³€ í’ˆì§ˆ ê²€ì¦ ì—†ìŒ\n- ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ë³µì¡\n- ë©€í‹°í„´ ëŒ€í™” êµ¬í˜„ ì–´ë ¤ì›€\n\n### LangGraphì˜ ì¥ì \n\n**LangGraph:**\n- ìƒíƒœ ê¸°ë°˜ ê·¸ë˜í”„ êµ¬ì¡°\n- ì¡°ê±´ë¶€ ë¼ìš°íŒ…\n- ë£¨í”„ ë° ì¬ì‹œë„ ë¡œì§\n- ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬í˜„\n\n**ì£¼ìš” ê°œë…:**\n- **State**: ìƒíƒœ ê´€ë¦¬ (TypedDict)\n- **Nodes**: ì‘ì—… ë‹¨ìœ„ (í•¨ìˆ˜)\n- **Edges**: ë…¸ë“œ ê°„ ì—°ê²°\n- **Conditional Edges**: ì¡°ê±´ë¶€ ë¶„ê¸°\n\n## í™˜ê²½ ì„¤ì •\n\n### ì„¤ì¹˜\n\n```bash\npip install langgraph\npip install langchain-openai\npip install azure-search-documents\n```\n\n### í™˜ê²½ ë³€ìˆ˜\n\n`.env` íŒŒì¼:\n```\nAZURE_OPENAI_ENDPOINT=https://openai-rag-prod.openai.azure.com/\nAZURE_OPENAI_API_KEY=your-key\nAZURE_OPENAI_DEPLOYMENT=gpt-4\nAZURE_OPENAI_API_VERSION=2024-02-01\n\nAZURE_SEARCH_ENDPOINT=https://search-rag-prod.search.windows.net\nAZURE_SEARCH_API_KEY=your-key\nAZURE_SEARCH_INDEX_NAME=rag-documents\n```\n\n## ê¸°ë³¸ LangChain RAG\n\në¨¼ì € ê¸°ë³¸ LangChain ì²´ì¸ì„ êµ¬í˜„í•œë‹¤.\n\n::: {#99be19c2 .cell execution_count=1}\n``` {.python .cell-code}\nfrom langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\nfrom langchain_community.vectorstores.azuresearch import AzureSearch\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Embeddings\nembeddings = AzureOpenAIEmbeddings(\n    azure_deployment=\"text-embedding-3-small\",\n    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n)\n\n# Vector Store\nvector_store = AzureSearch(\n    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n    azure_search_key=os.getenv(\"AZURE_SEARCH_API_KEY\"),\n    index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n    embedding_function=embeddings.embed_query\n)\n\n# Retriever\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n\n# LLM\nllm = AzureChatOpenAI(\n    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n    temperature=0\n)\n\n# Prompt\nprompt = ChatPromptTemplate.from_template(\n    \"\"\"ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n\nì»¨í…ìŠ¤íŠ¸:\n{context}\n\nì§ˆë¬¸: {question}\n\në‹µë³€:\"\"\"\n)\n\n# Chain\ndef format_docs(docs):\n    return \"\\n\\n\".join([doc.page_content for doc in docs])\n\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\n# ì‹¤í–‰\nanswer = rag_chain.invoke(\"Azure AI Searchë€ ë¬´ì—‡ì¸ê°€?\")\nprint(answer)\n```\n:::\n\n\n## LangGraphë¡œ ì „í™˜\n\n### ìƒíƒœ ì •ì˜\n\n::: {#f10c0078 .cell execution_count=2}\n``` {.python .cell-code}\nfrom typing import TypedDict, List\nfrom langchain_core.documents import Document\n\nclass RAGState(TypedDict):\n    \"\"\"RAG ìƒíƒœ\"\"\"\n    question: str  # ì‚¬ìš©ì ì§ˆë¬¸\n    context: List[Document]  # ê²€ìƒ‰ëœ ë¬¸ì„œ\n    answer: str  # ìƒì„±ëœ ë‹µë³€\n    retrieval_success: bool  # ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€\n```\n:::\n\n\n### ë…¸ë“œ ì •ì˜\n\n::: {#f6b2ec86 .cell execution_count=3}\n``` {.python .cell-code}\ndef retrieve(state: RAGState) -> RAGState:\n    \"\"\"ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ\"\"\"\n    question = state[\"question\"]\n    \n    # ê²€ìƒ‰ ì‹¤í–‰\n    docs = retriever.invoke(question)\n    \n    # ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€ í™•ì¸\n    success = len(docs) > 0\n    \n    return {\n        **state,\n        \"context\": docs,\n        \"retrieval_success\": success\n    }\n\ndef generate(state: RAGState) -> RAGState:\n    \"\"\"ë‹µë³€ ìƒì„± ë…¸ë“œ\"\"\"\n    question = state[\"question\"]\n    context = state[\"context\"]\n    \n    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…\n    context_text = \"\\n\\n\".join([doc.page_content for doc in context])\n    \n    # í”„ë¡¬í”„íŠ¸ ìƒì„±\n    messages = prompt.invoke({\n        \"context\": context_text,\n        \"question\": question\n    })\n    \n    # ë‹µë³€ ìƒì„±\n    response = llm.invoke(messages)\n    \n    return {\n        **state,\n        \"answer\": response.content\n    }\n\ndef no_context_fallback(state: RAGState) -> RAGState:\n    \"\"\"ì»¨í…ìŠ¤íŠ¸ ì—†ì„ ë•Œ ê¸°ë³¸ ë‹µë³€\"\"\"\n    return {\n        **state,\n        \"answer\": \"ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.\"\n    }\n```\n:::\n\n\n### ê·¸ë˜í”„ êµ¬ì„±\n\n::: {#310196c7 .cell execution_count=4}\n``` {.python .cell-code}\nfrom langgraph.graph import StateGraph, END\n\n# ê·¸ë˜í”„ ìƒì„±\nworkflow = StateGraph(RAGState)\n\n# ë…¸ë“œ ì¶”ê°€\nworkflow.add_node(\"retrieve\", retrieve)\nworkflow.add_node(\"generate\", generate)\nworkflow.add_node(\"fallback\", no_context_fallback)\n\n# ì‹œì‘ì  ì„¤ì •\nworkflow.set_entry_point(\"retrieve\")\n\n# ì¡°ê±´ë¶€ ì—£ì§€ (ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€)\ndef should_generate(state: RAGState) -> str:\n    \"\"\"ê²€ìƒ‰ ì„±ê³µ ì‹œ generate, ì‹¤íŒ¨ ì‹œ fallback\"\"\"\n    if state[\"retrieval_success\"]:\n        return \"generate\"\n    else:\n        return \"fallback\"\n\nworkflow.add_conditional_edges(\n    \"retrieve\",\n    should_generate,\n    {\n        \"generate\": \"generate\",\n        \"fallback\": \"fallback\"\n    }\n)\n\n# ì¢…ë£Œ ì—£ì§€\nworkflow.add_edge(\"generate\", END)\nworkflow.add_edge(\"fallback\", END)\n\n# ì»´íŒŒì¼\napp = workflow.compile()\n```\n:::\n\n\n### ì‹¤í–‰\n\n::: {#5048d3d0 .cell execution_count=5}\n``` {.python .cell-code}\n# ê·¸ë˜í”„ ì‹¤í–‰\nresult = app.invoke({\n    \"question\": \"Azure AI Searchë€ ë¬´ì—‡ì¸ê°€?\"\n})\n\nprint(f\"ì§ˆë¬¸: {result['question']}\")\nprint(f\"ê²€ìƒ‰ ì„±ê³µ: {result['retrieval_success']}\")\nprint(f\"ë‹µë³€: {result['answer']}\")\n```\n:::\n\n\n## ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€\n\n### ìƒíƒœ í™•ì¥\n\n::: {#12a02323 .cell execution_count=6}\n``` {.python .cell-code}\nfrom langchain_core.messages import BaseMessage\n\nclass ConversationState(TypedDict):\n    \"\"\"ëŒ€í™” ìƒíƒœ\"\"\"\n    question: str\n    chat_history: List[BaseMessage]  # ëŒ€í™” íˆìŠ¤í† ë¦¬\n    context: List[Document]\n    answer: str\n    retrieval_success: bool\n```\n:::\n\n\n### íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ê²€ìƒ‰\n\n::: {#2c887a58 .cell execution_count=7}\n``` {.python .cell-code}\nfrom langchain_core.messages import HumanMessage, AIMessage\n\ndef retrieve_with_history(state: ConversationState) -> ConversationState:\n    \"\"\"ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê³ ë ¤í•œ ê²€ìƒ‰\"\"\"\n    question = state[\"question\"]\n    chat_history = state.get(\"chat_history\", [])\n    \n    # íˆìŠ¤í† ë¦¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n    history_text = \"\\n\".join([\n        f\"{'User' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}\"\n        for msg in chat_history[-3:]  # ìµœê·¼ 3ê°œë§Œ\n    ])\n    \n    # ì§ˆë¬¸ ì¬êµ¬ì„± (íˆìŠ¤í† ë¦¬ í¬í•¨)\n    if history_text:\n        enhanced_question = f\"ëŒ€í™” íˆìŠ¤í† ë¦¬:\\n{history_text}\\n\\ní˜„ì¬ ì§ˆë¬¸: {question}\"\n    else:\n        enhanced_question = question\n    \n    # ê²€ìƒ‰\n    docs = retriever.invoke(enhanced_question)\n    \n    return {\n        **state,\n        \"context\": docs,\n        \"retrieval_success\": len(docs) > 0\n    }\n\ndef generate_with_history(state: ConversationState) -> ConversationState:\n    \"\"\"íˆìŠ¤í† ë¦¬ë¥¼ ê³ ë ¤í•œ ë‹µë³€ ìƒì„±\"\"\"\n    question = state[\"question\"]\n    context = state[\"context\"]\n    chat_history = state.get(\"chat_history\", [])\n    \n    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…\n    context_text = \"\\n\\n\".join([doc.page_content for doc in context])\n    \n    # íˆìŠ¤í† ë¦¬ í¬í•¨ í”„ë¡¬í”„íŠ¸\n    history_prompt = ChatPromptTemplate.from_template(\n        \"\"\"ì´ì „ ëŒ€í™”:\n{history}\n\nì»¨í…ìŠ¤íŠ¸:\n{context}\n\nì§ˆë¬¸: {question}\n\në‹µë³€:\"\"\"\n    )\n    \n    history_text = \"\\n\".join([\n        f\"User: {msg.content}\" if isinstance(msg, HumanMessage) else f\"AI: {msg.content}\"\n        for msg in chat_history[-3:]\n    ])\n    \n    messages = history_prompt.invoke({\n        \"history\": history_text or \"ì—†ìŒ\",\n        \"context\": context_text,\n        \"question\": question\n    })\n    \n    response = llm.invoke(messages)\n    \n    # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸\n    updated_history = chat_history + [\n        HumanMessage(content=question),\n        AIMessage(content=response.content)\n    ]\n    \n    return {\n        **state,\n        \"answer\": response.content,\n        \"chat_history\": updated_history\n    }\n```\n:::\n\n\n### ëŒ€í™”í˜• ê·¸ë˜í”„\n\n::: {#891a9d5f .cell execution_count=8}\n``` {.python .cell-code}\n# ëŒ€í™”í˜• ì›Œí¬í”Œë¡œìš°\nconversation_workflow = StateGraph(ConversationState)\n\nconversation_workflow.add_node(\"retrieve\", retrieve_with_history)\nconversation_workflow.add_node(\"generate\", generate_with_history)\nconversation_workflow.add_node(\"fallback\", no_context_fallback)\n\nconversation_workflow.set_entry_point(\"retrieve\")\n\nconversation_workflow.add_conditional_edges(\n    \"retrieve\",\n    should_generate,\n    {\n        \"generate\": \"generate\",\n        \"fallback\": \"fallback\"\n    }\n)\n\nconversation_workflow.add_edge(\"generate\", END)\nconversation_workflow.add_edge(\"fallback\", END)\n\nconversation_app = conversation_workflow.compile()\n\n# ë©€í‹°í„´ ëŒ€í™” ì‹¤í–‰\nstate = {\n    \"question\": \"Azure AI Searchë€ ë¬´ì—‡ì¸ê°€?\",\n    \"chat_history\": []\n}\n\nresult1 = conversation_app.invoke(state)\nprint(f\"ë‹µë³€ 1: {result1['answer']}\\n\")\n\n# í›„ì† ì§ˆë¬¸\nstate2 = {\n    \"question\": \"ê·¸ê²ƒì˜ ì£¼ìš” ê¸°ëŠ¥ì€?\",\n    \"chat_history\": result1[\"chat_history\"]\n}\n\nresult2 = conversation_app.invoke(state2)\nprint(f\"ë‹µë³€ 2: {result2['answer']}\")\n```\n:::\n\n\n## ì¬ì‹œë„ ë¡œì§ ì¶”ê°€\n\n### ì¬ì‹œë„ ìƒíƒœ\n\n::: {#0457505d .cell execution_count=9}\n``` {.python .cell-code}\nclass RetryState(TypedDict):\n    \"\"\"ì¬ì‹œë„ ê°€ëŠ¥í•œ RAG ìƒíƒœ\"\"\"\n    question: str\n    context: List[Document]\n    answer: str\n    retrieval_success: bool\n    retry_count: int  # ì¬ì‹œë„ íšŸìˆ˜\n    max_retries: int  # ìµœëŒ€ ì¬ì‹œë„\n```\n:::\n\n\n### ì¬ì‹œë„ ë…¸ë“œ\n\n::: {#256eb925 .cell execution_count=10}\n``` {.python .cell-code}\ndef retrieve_with_retry(state: RetryState) -> RetryState:\n    \"\"\"ì¬ì‹œë„ ê°€ëŠ¥í•œ ê²€ìƒ‰\"\"\"\n    question = state[\"question\"]\n    retry_count = state.get(\"retry_count\", 0)\n    \n    # ì¬ì‹œë„ ì‹œ ì¿¼ë¦¬ í™•ì¥\n    if retry_count > 0:\n        expanded_question = f\"{question} (ê´€ë ¨ ì •ë³´, ì„¤ëª…, ê°œìš” í¬í•¨)\"\n    else:\n        expanded_question = question\n    \n    docs = retriever.invoke(expanded_question)\n    \n    return {\n        **state,\n        \"context\": docs,\n        \"retrieval_success\": len(docs) > 0,\n        \"retry_count\": retry_count + 1\n    }\n\ndef should_retry(state: RetryState) -> str:\n    \"\"\"ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •\"\"\"\n    if state[\"retrieval_success\"]:\n        return \"generate\"\n    elif state[\"retry_count\"] < state.get(\"max_retries\", 2):\n        return \"retry\"\n    else:\n        return \"fallback\"\n```\n:::\n\n\n### ì¬ì‹œë„ ê·¸ë˜í”„\n\n::: {#3ade4d95 .cell execution_count=11}\n``` {.python .cell-code}\nretry_workflow = StateGraph(RetryState)\n\nretry_workflow.add_node(\"retrieve\", retrieve_with_retry)\nretry_workflow.add_node(\"generate\", generate)\nretry_workflow.add_node(\"fallback\", no_context_fallback)\n\nretry_workflow.set_entry_point(\"retrieve\")\n\nretry_workflow.add_conditional_edges(\n    \"retrieve\",\n    should_retry,\n    {\n        \"generate\": \"generate\",\n        \"retry\": \"retrieve\",  # ë£¨í”„ë°±\n        \"fallback\": \"fallback\"\n    }\n)\n\nretry_workflow.add_edge(\"generate\", END)\nretry_workflow.add_edge(\"fallback\", END)\n\nretry_app = retry_workflow.compile()\n\n# ì‹¤í–‰\nresult = retry_app.invoke({\n    \"question\": \"í¬ê·€í•œ ì£¼ì œ ê²€ìƒ‰\",\n    \"max_retries\": 2\n})\n\nprint(f\"ì¬ì‹œë„ íšŸìˆ˜: {result['retry_count']}\")\nprint(f\"ë‹µë³€: {result['answer']}\")\n```\n:::\n\n\n## ë‹µë³€ í’ˆì§ˆ ê²€ì¦\n\n### ê²€ì¦ ìƒíƒœ\n\n::: {#76487a84 .cell execution_count=12}\n``` {.python .cell-code}\nclass QualityState(TypedDict):\n    \"\"\"í’ˆì§ˆ ê²€ì¦ ìƒíƒœ\"\"\"\n    question: str\n    context: List[Document]\n    answer: str\n    retrieval_success: bool\n    answer_quality: str  # \"good\", \"bad\", \"unknown\"\n```\n:::\n\n\n### í’ˆì§ˆ í‰ê°€ ë…¸ë“œ\n\n::: {#8ae721b6 .cell execution_count=13}\n``` {.python .cell-code}\ndef evaluate_answer(state: QualityState) -> QualityState:\n    \"\"\"ë‹µë³€ í’ˆì§ˆ í‰ê°€\"\"\"\n    answer = state[\"answer\"]\n    question = state[\"question\"]\n    \n    # í‰ê°€ í”„ë¡¬í”„íŠ¸\n    eval_prompt = ChatPromptTemplate.from_template(\n        \"\"\"ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì„¸ìš”.\n\nì§ˆë¬¸: {question}\në‹µë³€: {answer}\n\në‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆíˆ ëŒ€ë‹µí•˜ê³  ìˆë‚˜ìš”?\n- \"good\": ì ì ˆí•œ ë‹µë³€\n- \"bad\": ë¶€ì ì ˆí•˜ê±°ë‚˜ ê´€ë ¨ ì—†ëŠ” ë‹µë³€\n- \"unknown\": íŒë‹¨ ë¶ˆê°€\n\ní‰ê°€ ê²°ê³¼ (good/bad/unknownë§Œ ì¶œë ¥):\"\"\"\n    )\n    \n    messages = eval_prompt.invoke({\n        \"question\": question,\n        \"answer\": answer\n    })\n    \n    evaluation = llm.invoke(messages).content.strip().lower()\n    \n    return {\n        **state,\n        \"answer_quality\": evaluation\n    }\n\ndef regenerate_answer(state: QualityState) -> QualityState:\n    \"\"\"ë‹µë³€ ì¬ìƒì„±\"\"\"\n    question = state[\"question\"]\n    context = state[\"context\"]\n    \n    # ë” ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸\n    detailed_prompt = ChatPromptTemplate.from_template(\n        \"\"\"ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ìƒì„¸íˆ ë‹µë³€í•˜ì„¸ìš”.\në°˜ë“œì‹œ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ í™œìš©í•˜ê³ , êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n\nì»¨í…ìŠ¤íŠ¸:\n{context}\n\nì§ˆë¬¸: {question}\n\nìƒì„¸ ë‹µë³€:\"\"\"\n    )\n    \n    context_text = \"\\n\\n\".join([doc.page_content for doc in context])\n    messages = detailed_prompt.invoke({\n        \"context\": context_text,\n        \"question\": question\n    })\n    \n    response = llm.invoke(messages)\n    \n    return {\n        **state,\n        \"answer\": response.content\n    }\n\ndef route_by_quality(state: QualityState) -> str:\n    \"\"\"í’ˆì§ˆì— ë”°ë¼ ë¼ìš°íŒ…\"\"\"\n    quality = state[\"answer_quality\"]\n    \n    if quality == \"good\":\n        return \"end\"\n    elif quality == \"bad\":\n        return \"regenerate\"\n    else:\n        return \"end\"\n```\n:::\n\n\n### í’ˆì§ˆ ê²€ì¦ ê·¸ë˜í”„\n\n::: {#710e8ee0 .cell execution_count=14}\n``` {.python .cell-code}\nquality_workflow = StateGraph(QualityState)\n\nquality_workflow.add_node(\"retrieve\", retrieve)\nquality_workflow.add_node(\"generate\", generate)\nquality_workflow.add_node(\"evaluate\", evaluate_answer)\nquality_workflow.add_node(\"regenerate\", regenerate_answer)\nquality_workflow.add_node(\"fallback\", no_context_fallback)\n\nquality_workflow.set_entry_point(\"retrieve\")\n\nquality_workflow.add_conditional_edges(\n    \"retrieve\",\n    should_generate,\n    {\n        \"generate\": \"generate\",\n        \"fallback\": \"fallback\"\n    }\n)\n\nquality_workflow.add_edge(\"generate\", \"evaluate\")\n\nquality_workflow.add_conditional_edges(\n    \"evaluate\",\n    route_by_quality,\n    {\n        \"end\": END,\n        \"regenerate\": \"regenerate\"\n    }\n)\n\nquality_workflow.add_edge(\"regenerate\", END)\nquality_workflow.add_edge(\"fallback\", END)\n\nquality_app = quality_workflow.compile()\n\n# ì‹¤í–‰\nresult = quality_app.invoke({\n    \"question\": \"Azure AI Searchì˜ ì¥ì ì€?\"\n})\n\nprint(f\"ë‹µë³€ í’ˆì§ˆ: {result['answer_quality']}\")\nprint(f\"ìµœì¢… ë‹µë³€: {result['answer']}\")\n```\n:::\n\n\n## ì‹œê°í™”\n\n### ê·¸ë˜í”„ êµ¬ì¡° í™•ì¸\n\n::: {#dd729b2a .cell execution_count=15}\n``` {.python .cell-code}\nfrom IPython.display import Image, display\n\n# ê·¸ë˜í”„ ì‹œê°í™”\ntry:\n    display(Image(app.get_graph().draw_mermaid_png()))\nexcept Exception:\n    print(app.get_graph().draw_ascii())\n```\n:::\n\n\n## ìŠ¤íŠ¸ë¦¬ë°\n\n### ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰\n\n::: {#9ab583f1 .cell execution_count=16}\n``` {.python .cell-code}\n# ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì¤‘ê°„ ë‹¨ê³„ í™•ì¸\nfor event in app.stream({\n    \"question\": \"Azure AI Searchë€?\"\n}):\n    print(f\"ì´ë²¤íŠ¸: {event}\")\n    print(\"---\")\n```\n:::\n\n\n## ì°¸ê³  ìë£Œ\n\n### ê³µì‹ ë¬¸ì„œ\n- [LangGraph ë¬¸ì„œ](https://langchain-ai.github.io/langgraph/)\n- [LangGraph íŠœí† ë¦¬ì–¼](https://langchain-ai.github.io/langgraph/tutorials/)\n\n### ì˜ˆì œ\n- [LangGraph RAG ì˜ˆì œ](https://github.com/langchain-ai/langgraph/tree/main/examples)\n\n## ë‹¤ìŒ ë‹¨ê³„\n\nLangGraphë¡œ RAG ë¡œì§ì„ êµ¬í˜„í–ˆë‹¤ë©´, ì´ì œ Azure OpenAI LLMì„ ìµœì í™”í•˜ì:\n\nğŸ‘‰ [06-Azure-OpenAI-LLM.qmd](./06-Azure-OpenAI-LLM.qmd) - Azure OpenAI LLM ìµœì í™” ë° í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n\n",
    "supporting": [
      "05-LangChain-to-LangGraph_files"
    ],
    "filters": [],
    "includes": {}
  }
}