{"title":"RNN 기반 언어 모델","markdown":{"yaml":{"title":"RNN 기반 언어 모델","subtitle":"순환 신경망을 이용한 다음 단어 예측","description":"RNN을 활용한 언어 모델의 구조와 작동 원리를 다룬다. 이전 단어들로부터 다음 단어를 예측하는 과정과 Teacher Forcing 학습 기법, 그리고 실제 구현 시 고려사항들을 설명한다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-14","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false},"headingText":"요약","containsRefs":false,"markdown":"\n\n\n이 문서는 RNN(순환 신경망)을 활용한 언어 모델의 기본 원리와 구현 방법을 설명한다. 언어 모델은 이전 단어들의 문맥을 바탕으로 다음에 올 단어를 예측하는 모델로, 자연어 생성과 이해에서 핵심적인 역할을 한다.\n\n주요 내용은 다음과 같다:\n\n* **RNN 언어 모델의 기본 구조**: \n  - 이전 단어들로부터 다음 단어를 예측하는 순차적 구조\n  - 입력 길이가 가변적이며, 각 시점에서 현재 단어를 입력받아 다음 단어를 예측\n  - 수식: $x_1 \\rightarrow c_1 \\rightarrow y_1 \\rightarrow x_2 \\rightarrow c_2 \\rightarrow y_2 \\rightarrow \\cdots$\n\n* **Teacher Forcing 학습 기법**:\n  - 훈련 시에는 실제 정답 토큰을 디코더 입력으로 사용하는 방법\n  - 예측값을 반복 사용할 때 발생하는 오류 누적과 불안정성 문제를 해결\n  - 훈련 과정과 테스트 과정의 차이점: 훈련시엔 실제값 사용, 테스트시엔 이전 예측값 사용\n\n* **모델 구조와 구현**:\n  - Embedding Layer, Hidden Layer, Output Layer로 구성\n  - Output Layer에서 전체 어휘 크기만큼의 벡터로 다중 클래스 분류 수행\n  - Softmax 함수를 통한 확률 분포 출력과 Cross Entropy Loss 사용\n\nRNN 언어 모델은 자연스러운 텍스트 생성의 기초가 되며, 번역기나 챗봇 등 다양한 NLP 애플리케이션에서 활용된다.\n\n\n\n# 텍스트 인코딩 및 벡터화\n\n```\nRNN Language Model\n├── Seq2Seq\n├── Beam Search\n├── Subword Tokenization\n├── Attention\n├── Transformer Encoder (Vaswani et al., 2017)\n|   ├── Positional Encoding\n|   ├── Multi-Head Attention\n|   └── Feed Forward Neural Network\n|\n├── Transformer Decoder (Vaswani et al., 2017)\n|\n├── BERT 시리즈 (Google,2018~)\n|   ├── BERT\n|   ├── RoBERTa\n|   └── ALBERT\n|\n├── GPT 시리즈 (OpenAI,2018~)\n|   ├── GPT-1~4\n|   └── ChatGPT (OpenAI,2022~)\n|\n├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)\n└── 기타 발전 모델\n    ├── T5, XLNet, ELECTRA\n    └── PaLM, LaMDA, Gemini, Claude 등\n```\n\n# RNN 기반 언어 모델\n\n## 언어 모델의 정의와 목적\n\n언어 모델(Language Model)은 자연어의 확률적 성질을 모델링하는 것으로, 주어진 단어 시퀀스에 대해 다음 단어가 나올 확률을 예측한다. 수학적으로는 다음과 같이 표현할 수 있다:\n\n$$\nP(w_1, w_2, \\ldots, w_n) = \\prod_{i=1}^{n} P(w_i | w_1, w_2, \\ldots, w_{i-1})\n$$\n\n여기서 $w_i$ 는 $i$ 번째 단어이고, $P(w_i | w_1, w_2, \\ldots, w_{i-1})$ 는 이전 단어들이 주어졌을 때 현재 단어가 나올 조건부 확률이다.\n\n## RNN을 이용한 구현\n\nRNN 언어 모델은 위의 조건부 확률 분포를 신경망으로 근사한다. 각 시점에서 이전 단어들의 정보를 은닉 상태(hidden state)에 누적하고, 이를 바탕으로 다음 단어를 예측한다.\n\n### 모델 구조와 정보 흐름\n\n$$\nx_1 \\rightarrow h_1 \\rightarrow y_1 \\rightarrow x_2 \\rightarrow h_2 \\rightarrow y_2 \\rightarrow \\cdots \\rightarrow x_n \\rightarrow h_n \\rightarrow y_n\n$$\n\n각 시점 $t$ 에서의 계산 과정:\n\n1. **입력 임베딩**: $e_t = \\text{Embedding}(x_t)$\n2. **은닉 상태 갱신**: $h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} e_t + b_h)$\n3. **출력 계산**: $o_t = W_{ho} h_t + b_o$\n4. **확률 분포**: $P(w_t | w_1, \\ldots, w_{t-1}) = \\text{softmax}(o_t)$\n\n여기서:\n- $W_{hh}$: 은닉 상태 간 연결 가중치\n- $W_{xh}$: 입력-은닉 상태 연결 가중치  \n- $W_{ho}$: 은닉 상태-출력 연결 가중치\n- $b_h, b_o$: 편향 벡터\n\n### 구체적인 예시\n\n예문: \"what will the side effects of the drug be?\"\n\n**추론 과정 (테스트 시)**:\n- $x_1 = \\text{\"what\"}$ → $h_1$ 계산 → $P(\\text{next word} | \\text{\"what\"})$ → $y_1 = \\text{\"will\"}$\n- $x_2 = \\text{\"will\"}$ → $h_2$ 계산 → $P(\\text{next word} | \\text{\"what will\"})$ → $y_2 = \\text{\"the\"}$\n- $x_3 = \\text{\"the\"}$ → $h_3$ 계산 → $P(\\text{next word} | \\text{\"what will the\"})$ → $y_3 = \\text{\"side\"}$\n- $x_4 = \\text{\"side\"}$ → $h_4$ 계산 → $P(\\text{next word} | \\text{\"what will the side\"})$ → $y_4 = \\text{\"effects\"}$\n\n이 과정에서 $h_t$ 는 시점 $t$까지의 모든 이전 단어들의 문맥 정보를 압축적으로 담고 있다.\n    \n## Teacher Forcing (교사 강요)\n\n### Teacher Forcing의 개념과 필요성\n\nTeacher Forcing은 시퀀스 생성 모델의 학습에서 디코더 입력으로 실제 정답 토큰을 사용하는 훈련 기법이다. 이 기법은 학습의 안정성과 효율성을 크게 향상시킨다.\n\n### 학습과 추론의 차이점\n\n**학습 시 (Teacher Forcing 적용)**:\n- 각 시점에서 실제 정답 단어를 입력으로 사용\n- 모든 시점의 손실을 병렬적으로 계산 가능\n- 안정적이고 빠른 학습\n\n**추론 시 (자기회귀적 생성)**:\n- 이전 시점의 예측 결과를 다음 시점의 입력으로 사용\n- 순차적으로 단어를 생성\n- 오류 누적 가능성 존재\n\n### 구체적인 학습 과정\n\n예문: \"what will the side effects of the drug be?\"\n\n**Teacher Forcing을 적용한 학습**:\n\n```\n입력 시퀀스: [<SOS>, what, will, the, side, effects, of, the, drug]\n목표 시퀀스: [what, will, the, side, effects, of, the, drug, be]\n```\n\n각 시점에서:\n- $t=1$: 입력 `<SOS>` → 예측 목표 `what`\n- $t=2$: 입력 `what` → 예측 목표 `will`  \n- $t=3$: 입력 `will` → 예측 목표 `the`\n- $t=4$: 입력 `the` → 예측 목표 `side`\n- $t=5$: 입력 `side` → 예측 목표 `effects`\n\n### Teacher Forcing의 장단점\n\n**장점**:\n- 학습 속도 향상: 모든 시점을 병렬 처리 가능\n- 학습 안정성: 올바른 문맥 정보 제공으로 gradient 안정화\n- 수렴 속도: 더 빠른 수렴과 안정적인 학습 곡선\n\n**단점**:\n- Exposure Bias: 학습 시와 추론 시의 입력 분포 차이\n- 추론 시 오류 누적: 잘못된 예측이 후속 예측에 영향\n- 일반화 문제: 실제 사용 환경과 학습 환경의 불일치\n\n### 실제 동작 예시\n\n**Papago, ChatGPT 등의 실제 서비스**에서는 다음과 같이 동작한다:\n\n```\n사용자 입력: \"What will the\"\n시스템 동작:\n1. \"What\" → 다음 단어 예측 → \"will\" (확률: 0.8)\n2. \"What will\" → 다음 단어 예측 → \"the\" (확률: 0.7)  \n3. \"What will the\" → 다음 단어 예측 → \"weather\" (확률: 0.6)\n4. 계속 진행...\n```\n\n## 모델 아키텍처와 구현\n\n### 전체 아키텍처\n\nRNN 언어 모델은 다음 세 가지 주요 구성 요소로 이루어진다:\n\n```\nInput → Embedding Layer → RNN Layer → Output Layer → Probability Distribution\n```\n\n### 각 레이어의 상세 구조\n\n#### Embedding Layer\n- **역할**: 단어 인덱스를 고정 크기의 벡터로 변환\n- **수식**: $e_t = E[w_t]$, 여기서 $E \\in \\mathbb{R}^{V \\times d}$\n- **파라미터**: \n  - $V$: 어휘 사전 크기 (Vocabulary size)\n  - $d$: 임베딩 차원수 (일반적으로 100-300차원)\n\n#### RNN Hidden Layer  \n- **역할**: 시퀀스의 문맥 정보를 누적하여 표현\n- **수식**: $h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} e_t + b_h)$\n- **파라미터**:\n  - $W_{hh} \\in \\mathbb{R}^{H \\times H}$: 은닉 상태 간 가중치\n  - $W_{xh} \\in \\mathbb{R}^{H \\times d}$: 입력-은닉 가중치\n  - $H$: 은닉 상태 차원수 (일반적으로 128-512차원)\n\n#### Output Layer\n- **역할**: 은닉 상태를 어휘 크기의 로짓 벡터로 변환\n- **수식**: $o_t = W_{ho} h_t + b_o$\n- **파라미터**: $W_{ho} \\in \\mathbb{R}^{V \\times H}$, $b_o \\in \\mathbb{R}^{V}$\n\n#### Softmax & Loss\n- **확률 분포**: $P(w_t | w_{<t}) = \\text{softmax}(o_t) = \\frac{e^{o_t^{(i)}}}{\\sum_{j=1}^{V} e^{o_t^{(j)}}}$\n- **손실 함수**: $\\mathcal{L} = -\\sum_{t=1}^{T} \\log P(w_t^* | w_{<t})$\n\n### 계산 복잡도와 메모리 요구사항\n\n- **파라미터 수**: $|E| + |W_{hh}| + |W_{xh}| + |W_{ho}| = V \\times d + H^2 + H \\times d + V \\times H$\n- **시간 복잡도**: $O(T \\times (H^2 + H \\times V))$ (시퀀스 길이 $T$에 대해)\n- **공간 복잡도**: $O(H + V)$ (배치 크기 1 기준)\n\n### 실제 구현 고려사항\n\n#### Perplexity (혼란도)\n언어 모델의 성능은 주로 Perplexity로 측정된다:\n\n$$\\text{PPL} = \\exp\\left(-\\frac{1}{T}\\sum_{t=1}^{T} \\log P(w_t | w_{<t})\\right)$$\n\n낮은 Perplexity 값이 더 좋은 성능을 의미한다.\n\n#### 샘플링 기법\n추론 시 다음 단어 선택 방법:\n- **Greedy Decoding**: $\\arg\\max_w P(w | w_{<t})$\n- **Temperature Sampling**: $P(w) = \\frac{e^{o_w/\\tau}}{\\sum_j e^{o_j/\\tau}}$ (τ는 temperature)\n- **Top-k Sampling**: 상위 k개 후보 중에서 샘플링\n\n\n\n## 결론\n\nRNN 기반 언어 모델은 자연어 처리에서 텍스트 생성과 이해의 기초가 되는 중요한 모델이다. 이전 단어들의 순차적 정보를 활용하여 다음 단어를 예측하는 간단하면서도 효과적인 구조를 가지고 있다.\n\n* **핵심 특징 요약**:\n  - 가변 길이 입력을 처리할 수 있는 순환 구조\n  - 각 시점에서 이전 문맥 정보를 누적하여 다음 단어 예측\n  - Teacher Forcing을 통한 안정적인 학습 과정\n\n* **Teacher Forcing의 중요성**:\n  - 훈련 시 정답 토큰 사용으로 학습 안정성 확보\n  - 오류 누적 방지와 학습 효율성 향상\n  - 실제 서비스에서는 이전 예측값을 사용하는 자기회귀적 생성\n\n* **실용적 의의**:\n  - 기계 번역, 텍스트 요약, 대화 시스템 등에 광범위하게 활용\n  - 현대 언어 모델들의 기초 개념 제공\n  - Embedding-Hidden-Output 구조의 표준 패턴 확립\n\nRNN 언어 모델은 비교적 단순한 구조임에도 불구하고 자연어의 순차적 특성을 효과적으로 모델링할 수 있어, 이후 등장한 LSTM, GRU, Transformer 등 더 복잡한 모델들의 토대가 되었다. 현재도 많은 NLP 애플리케이션에서 기본 구성 요소로 활용되고 있으며, 언어 모델의 기본 원리를 이해하는 데 필수적인 개념이다.","srcMarkdownNoYaml":"\n\n# 요약\n\n이 문서는 RNN(순환 신경망)을 활용한 언어 모델의 기본 원리와 구현 방법을 설명한다. 언어 모델은 이전 단어들의 문맥을 바탕으로 다음에 올 단어를 예측하는 모델로, 자연어 생성과 이해에서 핵심적인 역할을 한다.\n\n주요 내용은 다음과 같다:\n\n* **RNN 언어 모델의 기본 구조**: \n  - 이전 단어들로부터 다음 단어를 예측하는 순차적 구조\n  - 입력 길이가 가변적이며, 각 시점에서 현재 단어를 입력받아 다음 단어를 예측\n  - 수식: $x_1 \\rightarrow c_1 \\rightarrow y_1 \\rightarrow x_2 \\rightarrow c_2 \\rightarrow y_2 \\rightarrow \\cdots$\n\n* **Teacher Forcing 학습 기법**:\n  - 훈련 시에는 실제 정답 토큰을 디코더 입력으로 사용하는 방법\n  - 예측값을 반복 사용할 때 발생하는 오류 누적과 불안정성 문제를 해결\n  - 훈련 과정과 테스트 과정의 차이점: 훈련시엔 실제값 사용, 테스트시엔 이전 예측값 사용\n\n* **모델 구조와 구현**:\n  - Embedding Layer, Hidden Layer, Output Layer로 구성\n  - Output Layer에서 전체 어휘 크기만큼의 벡터로 다중 클래스 분류 수행\n  - Softmax 함수를 통한 확률 분포 출력과 Cross Entropy Loss 사용\n\nRNN 언어 모델은 자연스러운 텍스트 생성의 기초가 되며, 번역기나 챗봇 등 다양한 NLP 애플리케이션에서 활용된다.\n\n\n\n# 텍스트 인코딩 및 벡터화\n\n```\nRNN Language Model\n├── Seq2Seq\n├── Beam Search\n├── Subword Tokenization\n├── Attention\n├── Transformer Encoder (Vaswani et al., 2017)\n|   ├── Positional Encoding\n|   ├── Multi-Head Attention\n|   └── Feed Forward Neural Network\n|\n├── Transformer Decoder (Vaswani et al., 2017)\n|\n├── BERT 시리즈 (Google,2018~)\n|   ├── BERT\n|   ├── RoBERTa\n|   └── ALBERT\n|\n├── GPT 시리즈 (OpenAI,2018~)\n|   ├── GPT-1~4\n|   └── ChatGPT (OpenAI,2022~)\n|\n├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)\n└── 기타 발전 모델\n    ├── T5, XLNet, ELECTRA\n    └── PaLM, LaMDA, Gemini, Claude 등\n```\n\n# RNN 기반 언어 모델\n\n## 언어 모델의 정의와 목적\n\n언어 모델(Language Model)은 자연어의 확률적 성질을 모델링하는 것으로, 주어진 단어 시퀀스에 대해 다음 단어가 나올 확률을 예측한다. 수학적으로는 다음과 같이 표현할 수 있다:\n\n$$\nP(w_1, w_2, \\ldots, w_n) = \\prod_{i=1}^{n} P(w_i | w_1, w_2, \\ldots, w_{i-1})\n$$\n\n여기서 $w_i$ 는 $i$ 번째 단어이고, $P(w_i | w_1, w_2, \\ldots, w_{i-1})$ 는 이전 단어들이 주어졌을 때 현재 단어가 나올 조건부 확률이다.\n\n## RNN을 이용한 구현\n\nRNN 언어 모델은 위의 조건부 확률 분포를 신경망으로 근사한다. 각 시점에서 이전 단어들의 정보를 은닉 상태(hidden state)에 누적하고, 이를 바탕으로 다음 단어를 예측한다.\n\n### 모델 구조와 정보 흐름\n\n$$\nx_1 \\rightarrow h_1 \\rightarrow y_1 \\rightarrow x_2 \\rightarrow h_2 \\rightarrow y_2 \\rightarrow \\cdots \\rightarrow x_n \\rightarrow h_n \\rightarrow y_n\n$$\n\n각 시점 $t$ 에서의 계산 과정:\n\n1. **입력 임베딩**: $e_t = \\text{Embedding}(x_t)$\n2. **은닉 상태 갱신**: $h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} e_t + b_h)$\n3. **출력 계산**: $o_t = W_{ho} h_t + b_o$\n4. **확률 분포**: $P(w_t | w_1, \\ldots, w_{t-1}) = \\text{softmax}(o_t)$\n\n여기서:\n- $W_{hh}$: 은닉 상태 간 연결 가중치\n- $W_{xh}$: 입력-은닉 상태 연결 가중치  \n- $W_{ho}$: 은닉 상태-출력 연결 가중치\n- $b_h, b_o$: 편향 벡터\n\n### 구체적인 예시\n\n예문: \"what will the side effects of the drug be?\"\n\n**추론 과정 (테스트 시)**:\n- $x_1 = \\text{\"what\"}$ → $h_1$ 계산 → $P(\\text{next word} | \\text{\"what\"})$ → $y_1 = \\text{\"will\"}$\n- $x_2 = \\text{\"will\"}$ → $h_2$ 계산 → $P(\\text{next word} | \\text{\"what will\"})$ → $y_2 = \\text{\"the\"}$\n- $x_3 = \\text{\"the\"}$ → $h_3$ 계산 → $P(\\text{next word} | \\text{\"what will the\"})$ → $y_3 = \\text{\"side\"}$\n- $x_4 = \\text{\"side\"}$ → $h_4$ 계산 → $P(\\text{next word} | \\text{\"what will the side\"})$ → $y_4 = \\text{\"effects\"}$\n\n이 과정에서 $h_t$ 는 시점 $t$까지의 모든 이전 단어들의 문맥 정보를 압축적으로 담고 있다.\n    \n## Teacher Forcing (교사 강요)\n\n### Teacher Forcing의 개념과 필요성\n\nTeacher Forcing은 시퀀스 생성 모델의 학습에서 디코더 입력으로 실제 정답 토큰을 사용하는 훈련 기법이다. 이 기법은 학습의 안정성과 효율성을 크게 향상시킨다.\n\n### 학습과 추론의 차이점\n\n**학습 시 (Teacher Forcing 적용)**:\n- 각 시점에서 실제 정답 단어를 입력으로 사용\n- 모든 시점의 손실을 병렬적으로 계산 가능\n- 안정적이고 빠른 학습\n\n**추론 시 (자기회귀적 생성)**:\n- 이전 시점의 예측 결과를 다음 시점의 입력으로 사용\n- 순차적으로 단어를 생성\n- 오류 누적 가능성 존재\n\n### 구체적인 학습 과정\n\n예문: \"what will the side effects of the drug be?\"\n\n**Teacher Forcing을 적용한 학습**:\n\n```\n입력 시퀀스: [<SOS>, what, will, the, side, effects, of, the, drug]\n목표 시퀀스: [what, will, the, side, effects, of, the, drug, be]\n```\n\n각 시점에서:\n- $t=1$: 입력 `<SOS>` → 예측 목표 `what`\n- $t=2$: 입력 `what` → 예측 목표 `will`  \n- $t=3$: 입력 `will` → 예측 목표 `the`\n- $t=4$: 입력 `the` → 예측 목표 `side`\n- $t=5$: 입력 `side` → 예측 목표 `effects`\n\n### Teacher Forcing의 장단점\n\n**장점**:\n- 학습 속도 향상: 모든 시점을 병렬 처리 가능\n- 학습 안정성: 올바른 문맥 정보 제공으로 gradient 안정화\n- 수렴 속도: 더 빠른 수렴과 안정적인 학습 곡선\n\n**단점**:\n- Exposure Bias: 학습 시와 추론 시의 입력 분포 차이\n- 추론 시 오류 누적: 잘못된 예측이 후속 예측에 영향\n- 일반화 문제: 실제 사용 환경과 학습 환경의 불일치\n\n### 실제 동작 예시\n\n**Papago, ChatGPT 등의 실제 서비스**에서는 다음과 같이 동작한다:\n\n```\n사용자 입력: \"What will the\"\n시스템 동작:\n1. \"What\" → 다음 단어 예측 → \"will\" (확률: 0.8)\n2. \"What will\" → 다음 단어 예측 → \"the\" (확률: 0.7)  \n3. \"What will the\" → 다음 단어 예측 → \"weather\" (확률: 0.6)\n4. 계속 진행...\n```\n\n## 모델 아키텍처와 구현\n\n### 전체 아키텍처\n\nRNN 언어 모델은 다음 세 가지 주요 구성 요소로 이루어진다:\n\n```\nInput → Embedding Layer → RNN Layer → Output Layer → Probability Distribution\n```\n\n### 각 레이어의 상세 구조\n\n#### Embedding Layer\n- **역할**: 단어 인덱스를 고정 크기의 벡터로 변환\n- **수식**: $e_t = E[w_t]$, 여기서 $E \\in \\mathbb{R}^{V \\times d}$\n- **파라미터**: \n  - $V$: 어휘 사전 크기 (Vocabulary size)\n  - $d$: 임베딩 차원수 (일반적으로 100-300차원)\n\n#### RNN Hidden Layer  \n- **역할**: 시퀀스의 문맥 정보를 누적하여 표현\n- **수식**: $h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} e_t + b_h)$\n- **파라미터**:\n  - $W_{hh} \\in \\mathbb{R}^{H \\times H}$: 은닉 상태 간 가중치\n  - $W_{xh} \\in \\mathbb{R}^{H \\times d}$: 입력-은닉 가중치\n  - $H$: 은닉 상태 차원수 (일반적으로 128-512차원)\n\n#### Output Layer\n- **역할**: 은닉 상태를 어휘 크기의 로짓 벡터로 변환\n- **수식**: $o_t = W_{ho} h_t + b_o$\n- **파라미터**: $W_{ho} \\in \\mathbb{R}^{V \\times H}$, $b_o \\in \\mathbb{R}^{V}$\n\n#### Softmax & Loss\n- **확률 분포**: $P(w_t | w_{<t}) = \\text{softmax}(o_t) = \\frac{e^{o_t^{(i)}}}{\\sum_{j=1}^{V} e^{o_t^{(j)}}}$\n- **손실 함수**: $\\mathcal{L} = -\\sum_{t=1}^{T} \\log P(w_t^* | w_{<t})$\n\n### 계산 복잡도와 메모리 요구사항\n\n- **파라미터 수**: $|E| + |W_{hh}| + |W_{xh}| + |W_{ho}| = V \\times d + H^2 + H \\times d + V \\times H$\n- **시간 복잡도**: $O(T \\times (H^2 + H \\times V))$ (시퀀스 길이 $T$에 대해)\n- **공간 복잡도**: $O(H + V)$ (배치 크기 1 기준)\n\n### 실제 구현 고려사항\n\n#### Perplexity (혼란도)\n언어 모델의 성능은 주로 Perplexity로 측정된다:\n\n$$\\text{PPL} = \\exp\\left(-\\frac{1}{T}\\sum_{t=1}^{T} \\log P(w_t | w_{<t})\\right)$$\n\n낮은 Perplexity 값이 더 좋은 성능을 의미한다.\n\n#### 샘플링 기법\n추론 시 다음 단어 선택 방법:\n- **Greedy Decoding**: $\\arg\\max_w P(w | w_{<t})$\n- **Temperature Sampling**: $P(w) = \\frac{e^{o_w/\\tau}}{\\sum_j e^{o_j/\\tau}}$ (τ는 temperature)\n- **Top-k Sampling**: 상위 k개 후보 중에서 샘플링\n\n\n\n## 결론\n\nRNN 기반 언어 모델은 자연어 처리에서 텍스트 생성과 이해의 기초가 되는 중요한 모델이다. 이전 단어들의 순차적 정보를 활용하여 다음 단어를 예측하는 간단하면서도 효과적인 구조를 가지고 있다.\n\n* **핵심 특징 요약**:\n  - 가변 길이 입력을 처리할 수 있는 순환 구조\n  - 각 시점에서 이전 문맥 정보를 누적하여 다음 단어 예측\n  - Teacher Forcing을 통한 안정적인 학습 과정\n\n* **Teacher Forcing의 중요성**:\n  - 훈련 시 정답 토큰 사용으로 학습 안정성 확보\n  - 오류 누적 방지와 학습 효율성 향상\n  - 실제 서비스에서는 이전 예측값을 사용하는 자기회귀적 생성\n\n* **실용적 의의**:\n  - 기계 번역, 텍스트 요약, 대화 시스템 등에 광범위하게 활용\n  - 현대 언어 모델들의 기초 개념 제공\n  - Embedding-Hidden-Output 구조의 표준 패턴 확립\n\nRNN 언어 모델은 비교적 단순한 구조임에도 불구하고 자연어의 순차적 특성을 효과적으로 모델링할 수 있어, 이후 등장한 LSTM, GRU, Transformer 등 더 복잡한 모델들의 토대가 되었다. 현재도 많은 NLP 애플리케이션에서 기본 구성 요소로 활용되고 있으며, 언어 모델의 기본 원리를 이해하는 데 필수적인 개념이다."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"14.RNN_language_model.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"RNN 기반 언어 모델","subtitle":"순환 신경망을 이용한 다음 단어 예측","description":"RNN을 활용한 언어 모델의 구조와 작동 원리를 다룬다. 이전 단어들로부터 다음 단어를 예측하는 과정과 Teacher Forcing 학습 기법, 그리고 실제 구현 시 고려사항들을 설명한다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-14","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}