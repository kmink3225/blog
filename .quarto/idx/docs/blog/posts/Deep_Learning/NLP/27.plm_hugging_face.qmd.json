{"title":"Hugging Face: PLM ìƒíƒœê³„ì˜ ì¤‘ì‹¬","markdown":{"yaml":{"title":"Hugging Face: PLM ìƒíƒœê³„ì˜ ì¤‘ì‹¬","subtitle":"ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì˜ í—ˆë¸Œ","description":"Hugging FaceëŠ” í˜„ì¬ NLP ë¶„ì•¼ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ì í”Œë«í¼ì´ë‹¤. ìˆ˜ë§Œ ê°œì˜ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ì œê³µí•˜ë©°, ëª‡ ì¤„ì˜ ì½”ë“œë§Œìœ¼ë¡œ ìµœì‹  PLMì„ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤. í† í¬ë‚˜ì´ì €ë¶€í„° íŒŒì¸íŠœë‹, ë°°í¬ê¹Œì§€ ì „ì²´ ML ì›Œí¬í”Œë¡œìš°ë¥¼ ì§€ì›í•˜ëŠ” Hugging Faceì˜ í•µì‹¬ ê¸°ëŠ¥ë“¤ê³¼ ì‹¤ë¬´ í™œìš© ì „ëµì„ ìƒì„¸íˆ ë¶„ì„í•œë‹¤.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-27","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false},"headingText":"ìš”ì•½","containsRefs":false,"markdown":"\n\n\nHugging FaceëŠ” í˜„ì¬ **NLP ë¶„ì•¼ì˜ ì‚¬ì‹¤ìƒ í‘œì¤€**ì´ ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ì í”Œë«í¼ì´ë‹¤. PyTorchì™€ TensorFlow ëª¨ë‘ë¥¼ ì§€ì›í•˜ë©°, ìˆ˜ë§Œ ê°œì˜ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ì œê³µí•˜ëŠ” ê±°ëŒ€í•œ ìƒíƒœê³„ë¥¼ êµ¬ì¶•í–ˆë‹¤.\n\n## í•µì‹¬ ê°€ì¹˜ ì œì•ˆ\n\n* **ì ‘ê·¼ì„± í˜ëª…**: ëª‡ ì¤„ì˜ ì½”ë“œë¡œ ìµœì‹  PLM ì‚¬ìš© ê°€ëŠ¥\n* **í‘œì¤€í™”**: ëª¨ë“  ëª¨ë¸ì´ ë™ì¼í•œ APIë¡œ í†µì¼\n* **ì™„ì „í•œ ì›Œí¬í”Œë¡œìš°**: ì „ì²˜ë¦¬ë¶€í„° ë°°í¬ê¹Œì§€ ì›ìŠ¤í†± ì§€ì›\n* **ê±°ëŒ€í•œ ì»¤ë®¤ë‹ˆí‹°**: ì „ ì„¸ê³„ ê°œë°œìë“¤ì´ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ê³µìœ \n\n## ì£¼ìš” êµ¬ì„± ìš”ì†Œ\n\n* **Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬**: í•µì‹¬ ëª¨ë¸ êµ¬í˜„ì²´\n* **Model Hub**: 10ë§Œ+ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ì €ì¥ì†Œ\n* **Datasets**: í‘œì¤€í™”ëœ ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n* **Tokenizers**: ê³ ì„±ëŠ¥ í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬\n* **Accelerate**: ë¶„ì‚° í•™ìŠµ ë° ìµœì í™” ë„êµ¬\n* **Gradio**: ë¹ ë¥¸ ë°ëª¨ ë° í”„ë¡œí† íƒ€ì… êµ¬ì¶•\n* **Spaces**: ëª¨ë¸ ë°°í¬ ë° ê³µìœ  í”Œë«í¼\n\n## ì‹¤ë¬´ì—ì„œì˜ ê°•ë ¥í•¨\n\n**Before Hugging Face** (2019ë…„ ì´ì „):\n```python\n# ë³µì¡í•œ ëª¨ë¸ êµ¬í˜„ê³¼ ì „ì²˜ë¦¬ í•„ìš”\nclass CustomBERTModel(nn.Module):\n    def __init__(self):\n        # ìˆ˜ë°± ì¤„ì˜ êµ¬í˜„ ì½”ë“œ...\n        pass\n    \n# í† í¬ë‚˜ì´ì € ì§ì ‘ êµ¬í˜„\n# ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì½”ë“œ ì‘ì„±\n# ê° ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¸ API\n```\n\n**After Hugging Face**:\n```python\n# 3ì¤„ë¡œ ë\nfrom transformers import pipeline\nclassifier = pipeline(\"sentiment-analysis\")\nresult = classifier(\"ì´ ì˜í™” ì •ë§ ì¬ë°Œì—ˆì–´!\")\n```\n\nì´ëŸ¬í•œ **ì½”ë“œ ë‹¨ìˆœí™”**ëŠ” NLP ê¸°ìˆ ì˜ ë¯¼ì£¼í™”ë¥¼ ì´ëŒì—ˆìœ¼ë©°, ì—°êµ¬ìë¿ë§Œ ì•„ë‹ˆë¼ ì¼ë°˜ ê°œë°œìë“¤ë„ ì‰½ê²Œ ìµœì‹  AI ê¸°ìˆ ì„ í™œìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì—ˆë‹¤.\n\n# Hugging Face ìƒíƒœê³„ ì „ì²´ êµ¬ì¡°\n\n## í”Œë«í¼ ì•„í‚¤í…ì²˜\n\n```{mermaid}\ngraph TD\n    A[Hugging Face Hub] --> B[Models]\n    A --> C[Datasets] \n    A --> D[Spaces]\n    \n    B --> E[Transformers Library]\n    C --> F[Datasets Library]\n    D --> G[Gradio/Streamlit]\n    \n    E --> H[PyTorch]\n    E --> I[TensorFlow]\n    E --> J[JAX]\n    \n    K[ì‚¬ìš©ì] --> L[Pipeline API]\n    K --> M[AutoModel API]\n    K --> N[Trainer API]\n    \n    L --> E\n    M --> E\n    N --> E\n```\n\n## í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤\n\n### 1. Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬\n**ì—­í• **: ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì˜ í†µí•© êµ¬í˜„ì²´\n```python\n# ì§€ì›í•˜ëŠ” ì£¼ìš” ëª¨ë¸ë“¤\nmodels = [\n    \"BERT\", \"GPT-2\", \"T5\", \"BART\", \"RoBERTa\",\n    \"ALBERT\", \"DistilBERT\", \"ELECTRA\", \"DeBERTa\",\n    \"GPT-Neo\", \"GPT-J\", \"OPT\", \"BLOOM\", \"LLaMA\"\n]\n\n# ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ë“¤\ntasks = [\n    \"text-classification\", \"token-classification\",\n    \"question-answering\", \"text-generation\",\n    \"summarization\", \"translation\", \"fill-mask\"\n]\n```\n\n### 2. Model Hubì˜ ê·œëª¨\n```python\n# 2024ë…„ ê¸°ì¤€ í†µê³„\nhub_stats = {\n    'total_models': 100000+,\n    'organizations': 10000+,\n    'downloads_per_month': '10ì–µ+',\n    'supported_languages': 100+,\n    'korean_models': 1000+\n}\n```\n\n### 3. ì§€ì›í•˜ëŠ” í”„ë ˆì„ì›Œí¬\n```python\n# ë©€í‹° í”„ë ˆì„ì›Œí¬ ì§€ì›\nframeworks = {\n    'PyTorch': 'ê¸°ë³¸ ì§€ì›, ê°€ì¥ ë§ì€ ëª¨ë¸',\n    'TensorFlow': 'TF 2.x ì™„ì „ ì§€ì›',\n    'JAX/Flax': 'Google ì—°êµ¬íŒ€ í˜‘ì—…',\n    'ONNX': 'ì¶”ë¡  ìµœì í™” ì§€ì›'\n}\n```\n\n# í† í°í™”ì™€ ì „ì²˜ë¦¬\n\n## í† í¬ë‚˜ì´ì €ì˜ ì¤‘ìš”ì„±\n\n**í•µì‹¬ ì›ì¹™**: ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ëŠ” í•­ìƒ ìŒìœ¼ë¡œ ì‚¬ìš©í•´ì•¼ í•œë‹¤.\n\n```python\n# ì˜¬ë°”ë¥¸ ì‚¬ìš©ë²• âœ…\nfrom transformers import AutoTokenizer, AutoModel\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n```\n\n```python\n# ì˜ëª»ëœ ì‚¬ìš©ë²• âŒ - ë‹¤ë¥¸ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # ì˜ì–´\nmodel = AutoModel.from_pretrained(\"klue/bert-base\")  # í•œêµ­ì–´\n# â†’ ì™„ì „íˆ ë‹¤ë¥¸ vocabularyë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜\n```\n\n## í† í°í™” ê³¼ì • ìƒì„¸ ë¶„ì„\n\n```python\nfrom transformers import BertTokenizer\n\n# í•œêµ­ì–´ BERT í† í¬ë‚˜ì´ì €\ntokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")\n\n# ë‹¨ê³„ë³„ í† í°í™” ê³¼ì •\ntext = \"ì•ˆë…•í•˜ì„¸ìš”. ìì—°ì–´ ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤.\"\n\n# 1ë‹¨ê³„: í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• \ntokens = tokenizer.tokenize(text)\nprint(f\"í† í°í™”: {tokens}\")\n# ['ì•ˆë…•', '##í•˜', '##ì„¸ìš”', '.', 'ìì—°ì–´', 'ì²˜ë¦¬ë¥¼', 'ê³µë¶€', '##í•©ë‹ˆë‹¤', '.']\n\n# 2ë‹¨ê³„: í† í°ì„ IDë¡œ ë³€í™˜\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(f\"í† í° ID: {token_ids}\")\n# [2374, 8910, 4567, 119, 15234, 9876, 3456, 7890, 119]\n\n# 3ë‹¨ê³„: íŠ¹ìˆ˜ í† í° ì¶”ê°€ ë° íŒ¨ë”©\nencoded = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\nprint(f\"ìµœì¢… ì¸ì½”ë”©: {encoded}\")\n# {'input_ids': tensor([[101, 2374, 8910, ..., 102]]), \n#  'attention_mask': tensor([[1, 1, 1, ..., 1]])}\n```\n\n## ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì € ì¢…ë¥˜\n\n```python\n# 1. WordPiece (BERT ê³„ì—´)\nwordpiece_tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")\nresult1 = wordpiece_tokenizer.tokenize(\"ìì—°ì–´ì²˜ë¦¬\")\nprint(f\"WordPiece: {result1}\")  # ['ìì—°ì–´', '##ì²˜ë¦¬']\n\n# 2. BPE (GPT ê³„ì—´)\nbpe_tokenizer = GPT2Tokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\nresult2 = bpe_tokenizer.tokenize(\"ìì—°ì–´ì²˜ë¦¬\")\nprint(f\"BPE: {result2}\")  # ['ìì—°ì–´', 'ì²˜ë¦¬']\n\n# 3. SentencePiece (T5, ALBERT ê³„ì—´)\nsp_tokenizer = T5Tokenizer.from_pretrained(\"KETI-AIR/ke-t5-base\")\nresult3 = sp_tokenizer.tokenize(\"ìì—°ì–´ì²˜ë¦¬\")\nprint(f\"SentencePiece: {result3}\")  # ['â–ìì—°ì–´', 'ì²˜ë¦¬']\n```\n\n## í† í¬ë‚˜ì´ì € ì„±ëŠ¥ ìµœì í™”\n\n```python\n# ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ\ntexts = [\"ë¬¸ì¥ 1\", \"ë¬¸ì¥ 2\", \"ë¬¸ì¥ 3\"] * 1000\n\n# ëŠë¦° ë°©ë²• âŒ\nslow_results = []\nfor text in texts:\n    result = tokenizer(text)\n    slow_results.append(result)\n\n# ë¹ ë¥¸ ë°©ë²• âœ… (10-100ë°° ë¹ ë¦„)\nfast_results = tokenizer(texts, padding=True, truncation=True)\n\n# Fast Tokenizer ì‚¬ìš© (Rust êµ¬í˜„)\nfast_tokenizer = AutoTokenizer.from_pretrained(\n    \"klue/bert-base\", \n    use_fast=True  # Rust ê¸°ë°˜ ê³ ì† í† í¬ë‚˜ì´ì €\n)\n```\n\n# ëª¨ë¸ ë¡œë”©ê³¼ í™œìš©\n\n## AutoModel ê³„ì—´ì˜ ê°•ë ¥í•¨\n\n```python\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n\n# ìë™ìœ¼ë¡œ ì ì ˆí•œ ëª¨ë¸ í´ë˜ìŠ¤ ì„ íƒ\nmodel_name = \"klue/bert-base\"\n\n# ë²”ìš© ì¸ì½”ë”\nencoder = AutoModel.from_pretrained(model_name)\n\n# ë¶„ë¥˜ìš© ëª¨ë¸ (ë¶„ë¥˜ í—¤ë“œ ìë™ ì¶”ê°€)\nclassifier = AutoModelForSequenceClassification.from_pretrained(\n    model_name, \n    num_labels=3\n)\n\n# ì§ˆì˜ì‘ë‹µìš© ëª¨ë¸\nqa_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n```\n\n## ëª¨ë¸ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•\n\n```python\nfrom transformers import BertConfig, BertForSequenceClassification\n\n# ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸° ë° ìˆ˜ì •\nconfig = BertConfig.from_pretrained(\"klue/bert-base\")\nconfig.num_labels = 5  # ë¶„ë¥˜ í´ë˜ìŠ¤ ìˆ˜ ë³€ê²½\nconfig.dropout = 0.2   # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ì¡°ì •\nconfig.attention_probs_dropout_prob = 0.1\n\n# ìˆ˜ì •ëœ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ìƒì„±\nmodel = BertForSequenceClassification.from_pretrained(\n    \"klue/bert-base\",\n    config=config\n)\n\n# ëª¨ë¸ êµ¬ì¡° í™•ì¸\nprint(model)\n```\n\n## ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëª¨ë¸ ë¡œë”©\n\n```python\n# 1. ì ˆë°˜ ì •ë°€ë„ ì‚¬ìš© (ë©”ëª¨ë¦¬ 50% ì ˆì•½)\nmodel = AutoModel.from_pretrained(\n    \"klue/bert-base\",\n    torch_dtype=torch.float16\n)\n\n# 2. CPU ì˜¤í”„ë¡œë”© (í° ëª¨ë¸ìš©)\nmodel = AutoModel.from_pretrained(\n    \"microsoft/DialoGPT-large\",\n    device_map=\"auto\",  # ìë™ìœ¼ë¡œ GPU/CPU ë°°ì¹˜\n    low_cpu_mem_usage=True\n)\n\n# 3. 8ë¹„íŠ¸ ì–‘ìí™” (ë©”ëª¨ë¦¬ 75% ì ˆì•½)\nfrom transformers import BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = AutoModel.from_pretrained(\n    \"facebook/opt-6.7b\",\n    quantization_config=quantization_config\n)\n```\n\n# Pipeline API: ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ NLP\n\n## ê¸°ë³¸ Pipeline ì‚¬ìš©ë²•\n\n```python\nfrom transformers import pipeline\n\n# 1. ê°ì • ë¶„ì„\nsentiment_analyzer = pipeline(\n    \"sentiment-analysis\",\n    model=\"klue/bert-base-en-ko-cased\",\n    return_all_scores=True\n)\n\nresult = sentiment_analyzer(\"ì´ ì˜í™” ì •ë§ ì¬ë°Œì—ˆì–´!\")\nprint(result)\n# [{'label': 'POSITIVE', 'score': 0.9998}]\n\n# 2. í…ìŠ¤íŠ¸ ìƒì„±\ngenerator = pipeline(\n    \"text-generation\",\n    model=\"skt/kogpt2-base-v2\",\n    max_length=100,\n    do_sample=True,\n    temperature=0.8\n)\n\nresult = generator(\"ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ”\")\nprint(result[0]['generated_text'])\n\n# 3. ì§ˆì˜ì‘ë‹µ\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=\"klue/bert-base\"\n)\n\ncontext = \"íŒŒì´ì¬ì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ê°œë°œí•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë‹¤.\"\nquestion = \"íŒŒì´ì¬ì„ ê°œë°œí•œ ì‚¬ëŒì€ ëˆ„êµ¬ì¸ê°€?\"\n\nresult = qa_pipeline(question=question, context=context)\nprint(f\"ë‹µ: {result['answer']}, ì‹ ë¢°ë„: {result['score']:.4f}\")\n```\n\n## ê³ ê¸‰ Pipeline ì„¤ì •\n\n```python\n# ë°°ì¹˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\nclassifier = pipeline(\n    \"text-classification\",\n    model=\"klue/bert-base\",\n    device=0,  # GPU ì‚¬ìš©\n    batch_size=16,  # ë°°ì¹˜ í¬ê¸°\n    max_length=512,\n    truncation=True\n)\n\n# ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ ì²˜ë¦¬\ntexts = [\"í…ìŠ¤íŠ¸ 1\", \"í…ìŠ¤íŠ¸ 2\"] * 1000\nresults = classifier(texts)\n\n# ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ í•¨ìˆ˜\ndef preprocess_function(examples):\n    # ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ë¡œì§\n    return tokenizer(examples, truncation=True, padding=True)\n\n# íŒŒì´í”„ë¼ì¸ì— ì»¤ìŠ¤í…€ í•¨ìˆ˜ ì ìš©\ncustom_pipeline = pipeline(\n    \"text-classification\",\n    model=model,\n    tokenizer=tokenizer,\n    preprocessing=preprocess_function\n)\n```\n\n## ì‹¤ë¬´ìš© Pipeline ìµœì í™”\n\n```python\nclass OptimizedPipeline:\n    def __init__(self, model_name, task, device=\"cuda\"):\n        self.pipeline = pipeline(\n            task,\n            model=model_name,\n            device=0 if device == \"cuda\" else -1,\n            batch_size=32,\n            return_all_scores=False\n        )\n        \n    def batch_predict(self, texts, batch_size=32):\n        \"\"\"ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ íš¨ìœ¨ì  ì²˜ë¦¬\"\"\"\n        results = []\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i+batch_size]\n            batch_results = self.pipeline(batch)\n            results.extend(batch_results)\n        return results\n    \n    def predict_with_confidence(self, text, threshold=0.8):\n        \"\"\"ì‹ ë¢°ë„ ê¸°ë°˜ ì˜ˆì¸¡\"\"\"\n        result = self.pipeline(text, return_all_scores=True)\n        max_score = max(result[0], key=lambda x: x['score'])\n        \n        if max_score['score'] >= threshold:\n            return max_score\n        else:\n            return {\"label\": \"UNCERTAIN\", \"score\": max_score['score']}\n\n# ì‚¬ìš© ì˜ˆì‹œ\nclassifier = OptimizedPipeline(\"klue/bert-base\", \"text-classification\")\nresults = classifier.batch_predict([\"í…ìŠ¤íŠ¸1\", \"í…ìŠ¤íŠ¸2\", \"í…ìŠ¤íŠ¸3\"])\n```\n\n# íŒŒì¸íŠœë‹ê³¼ í•™ìŠµ\n\n## Trainer API í™œìš©\n\n```python\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nfrom torch.utils.data import Dataset\n\n# 1. ë°ì´í„°ì…‹ ì¤€ë¹„\nclass CustomDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n# 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì¤€ë¹„\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=3\n)\n\n# 3. í•™ìŠµ ì„¤ì •\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=100,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_accuracy\",\n    greater_is_better=True\n)\n\n# 4. í‰ê°€ í•¨ìˆ˜\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = predictions.argmax(axis=-1)\n    \n    accuracy = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted'\n    )\n    \n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# 5. í•™ìŠµ ì‹¤í–‰\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n```\n\n## íš¨ìœ¨ì  íŒŒì¸íŠœë‹ ê¸°ë²•\n\n### LoRA (Low-Rank Adaptation)\n\n```python\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n# LoRA ì„¤ì •\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=16,  # ë­í¬\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"]  # ì ìš©í•  ë ˆì´ì–´\n)\n\n# LoRA ì ìš©\nmodel = get_peft_model(model, lora_config)\n\n# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸\nmodel.print_trainable_parameters()\n# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: 294,912 (ì „ì²´ì˜ 0.27%)\n```\n\n### Gradient Checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)\n\n```python\n# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì„ (ì†ë„ëŠ” ì•½ê°„ ëŠë ¤ì§)\nmodel.gradient_checkpointing_enable()\n\ntraining_args = TrainingArguments(\n    # ... ê¸°íƒ€ ì„¤ì •\n    gradient_checkpointing=True,\n    dataloader_pin_memory=False,  # ë©”ëª¨ë¦¬ ì ˆì•½\n    remove_unused_columns=True\n)\n```\n\n### DeepSpeed í†µí•© (ëŒ€ê·œëª¨ ëª¨ë¸)\n\n```python\n# deepspeed_config.json\ndeepspeed_config = {\n    \"train_batch_size\": 16,\n    \"gradient_accumulation_steps\": 1,\n    \"fp16\": {\n        \"enabled\": True\n    },\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": True\n        }\n    }\n}\n\ntraining_args = TrainingArguments(\n    # ... ê¸°íƒ€ ì„¤ì •\n    deepspeed=deepspeed_config,\n    fp16=True\n)\n```\n\n# ì‹¤ë¬´ í™œìš© ì‚¬ë¡€ì™€ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤\n\n## í•œêµ­ì–´ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ\n\n```python\n# íƒœìŠ¤í¬ë³„ ì¶”ì²œ í•œêµ­ì–´ ëª¨ë¸\nkorean_models = {\n    # ë²”ìš© ì¸ì½”ë”\n    'general_encoder': [\n        \"klue/bert-base\",           # ê°€ì¥ ì•ˆì •ì \n        \"klue/roberta-large\",       # ë†’ì€ ì„±ëŠ¥\n        \"monologg/kobert\",          # ê²½ëŸ‰í™”\n        \"beomi/kcbert-base\"         # ëŒ“ê¸€/ë¦¬ë·° íŠ¹í™”\n    ],\n    \n    # í…ìŠ¤íŠ¸ ìƒì„±\n    'text_generation': [\n        \"skt/kogpt2-base-v2\",       # ì¼ë°˜ì  ìš©ë„\n        \"kakaobrain/kogpt\",         # ê³ í’ˆì§ˆ ìƒì„±\n        \"EleutherAI/polyglot-ko-12.8b\"  # ëŒ€ê·œëª¨ ëª¨ë¸\n    ],\n    \n    # ìš”ì•½\n    'summarization': [\n        \"eenzeenee/t5-base-korean-summarization\",\n        \"psyche/KoT5-summarization\"\n    ],\n    \n    # ë²ˆì—­\n    'translation': [\n        \"Helsinki-NLP/opus-mt-ko-en\",   # í•œâ†’ì˜\n        \"Helsinki-NLP/opus-mt-en-ko\"    # ì˜â†’í•œ\n    ]\n}\n```\n\n## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹\n\n```python\nimport time\nimport torch\nfrom transformers import pipeline\n\ndef benchmark_model(model_name, texts, task=\"text-classification\"):\n    \"\"\"ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\"\"\"\n    \n    # GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”\n    torch.cuda.empty_cache()\n    \n    # ëª¨ë¸ ë¡œë“œ ì‹œê°„\n    start_time = time.time()\n    pipe = pipeline(task, model=model_name, device=0)\n    load_time = time.time() - start_time\n    \n    # ì¶”ë¡  ì‹œê°„ (ë‹¨ì¼)\n    start_time = time.time()\n    single_result = pipe(texts[0])\n    single_inference_time = time.time() - start_time\n    \n    # ì¶”ë¡  ì‹œê°„ (ë°°ì¹˜)\n    start_time = time.time()\n    batch_results = pipe(texts)\n    batch_inference_time = time.time() - start_time\n    \n    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n    memory_usage = torch.cuda.max_memory_allocated() / 1024**3  # GB\n    \n    return {\n        'model': model_name,\n        'load_time': load_time,\n        'single_inference_time': single_inference_time,\n        'batch_inference_time': batch_inference_time,\n        'throughput': len(texts) / batch_inference_time,\n        'memory_usage_gb': memory_usage\n    }\n\n# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰\ntest_texts = [\"í…ŒìŠ¤íŠ¸ ë¬¸ì¥\"] * 100\nmodels_to_test = [\n    \"klue/bert-base\",\n    \"monologg/distilkobert\",\n    \"beomi/kcbert-base\"\n]\n\nfor model in models_to_test:\n    result = benchmark_model(model, test_texts)\n    print(f\"{model}: {result}\")\n```\n\n## í”„ë¡œë•ì…˜ ë°°í¬ ì „ëµ\n\n```python\n# 1. ëª¨ë¸ ìµœì í™” ë° ì €ì¥\nclass ProductionModel:\n    def __init__(self, model_path):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            model_path,\n            torch_dtype=torch.float16,  # ë°˜ì •ë°€ë„\n            return_dict=True\n        )\n        self.model.eval()  # í‰ê°€ ëª¨ë“œ\n        \n    @torch.no_grad()\n    def predict(self, texts):\n        \"\"\"ì¶”ë¡  ì „ìš© ë©”ì„œë“œ\"\"\"\n        inputs = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\",\n            max_length=512\n        )\n        \n        outputs = self.model(**inputs)\n        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n        return predictions.cpu().numpy()\n\n# 2. API ì„œë²„ ì˜ˆì‹œ (FastAPI)\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom typing import List\n\napp = FastAPI()\nmodel = ProductionModel(\"./trained_model\")\n\nclass PredictionRequest(BaseModel):\n    texts: List[str]\n\n@app.post(\"/predict\")\nasync def predict(request: PredictionRequest):\n    predictions = model.predict(request.texts)\n    return {\n        \"predictions\": predictions.tolist(),\n        \"model_info\": \"klue/bert-base\",\n        \"timestamp\": time.time()\n    }\n\n# 3. ë„ì»¤ ì»¨í…Œì´ë„ˆí™”\ndockerfile_content = \"\"\"\nFROM python:3.9-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\"\"\"\n```\n\n## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…\n\n```python\nimport logging\nfrom datetime import datetime\nimport json\n\nclass ModelMonitor:\n    def __init__(self, model_name):\n        self.model_name = model_name\n        self.logger = self._setup_logger()\n        self.prediction_count = 0\n        self.error_count = 0\n        \n    def _setup_logger(self):\n        logger = logging.getLogger(f\"model_{self.model_name}\")\n        handler = logging.FileHandler(f\"model_logs_{self.model_name}.log\")\n        formatter = logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n        )\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        logger.setLevel(logging.INFO)\n        return logger\n    \n    def log_prediction(self, input_text, prediction, confidence, latency):\n        \"\"\"ì˜ˆì¸¡ ê²°ê³¼ ë¡œê¹…\"\"\"\n        self.prediction_count += 1\n        \n        log_data = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"input_length\": len(input_text),\n            \"prediction\": prediction,\n            \"confidence\": confidence,\n            \"latency_ms\": latency * 1000,\n            \"prediction_id\": self.prediction_count\n        }\n        \n        self.logger.info(json.dumps(log_data))\n        \n        # ë‚®ì€ ì‹ ë¢°ë„ ê²½ê³ \n        if confidence < 0.7:\n            self.logger.warning(f\"Low confidence prediction: {confidence}\")\n    \n    def log_error(self, error_msg, input_text):\n        \"\"\"ì˜¤ë¥˜ ë¡œê¹…\"\"\"\n        self.error_count += 1\n        self.logger.error(f\"Error: {error_msg}, Input: {input_text[:100]}...\")\n    \n    def get_stats(self):\n        \"\"\"í†µê³„ ì •ë³´ ë°˜í™˜\"\"\"\n        return {\n            \"total_predictions\": self.prediction_count,\n            \"total_errors\": self.error_count,\n            \"error_rate\": self.error_count / max(self.prediction_count, 1)\n        }\n\n# ì‚¬ìš© ì˜ˆì‹œ\nmonitor = ModelMonitor(\"sentiment_classifier\")\n\ndef monitored_predict(text):\n    start_time = time.time()\n    try:\n        result = model.predict([text])[0]\n        latency = time.time() - start_time\n        \n        prediction = result.argmax()\n        confidence = result.max()\n        \n        monitor.log_prediction(text, prediction, confidence, latency)\n        return {\"prediction\": prediction, \"confidence\": confidence}\n        \n    except Exception as e:\n        monitor.log_error(str(e), text)\n        raise\n```\n\n# í•œêµ­ì–´ NLP íŠ¹í™” ê³ ë ¤ì‚¬í•­\n\n## í•œêµ­ì–´ í† í°í™”ì˜ íŠ¹ìˆ˜ì„±\n\n```python\n# í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ì™€ ì¡°í•©\nfrom konlpy.tag import Mecab\nfrom transformers import AutoTokenizer\n\nclass KoreanPreprocessor:\n    def __init__(self, model_name):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.mecab = Mecab()  # í˜•íƒœì†Œ ë¶„ì„ê¸°\n    \n    def preprocess_korean(self, text):\n        \"\"\"í•œêµ­ì–´ íŠ¹í™” ì „ì²˜ë¦¬\"\"\"\n        \n        # 1. ì •ê·œí™”\n        text = text.strip()\n        text = re.sub(r'[ã„±-ã…ã…-ã…£]+', '', text)  # ììŒ/ëª¨ìŒë§Œ ìˆëŠ” ê²½ìš° ì œê±°\n        text = re.sub(r'\\s+', ' ', text)  # ì—°ì† ê³µë°± ì œê±°\n        \n        # 2. ì´ëª¨ì§€/íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬\n        text = re.sub(r'[^\\w\\sê°€-í£]', '', text)\n        \n        # 3. í˜•íƒœì†Œ ë¶„ì„ (ì„ íƒì )\n        # morphs = self.mecab.morphs(text)\n        # text = ' '.join(morphs)\n        \n        return text\n    \n    def tokenize_korean(self, text):\n        \"\"\"í•œêµ­ì–´ í† í°í™”\"\"\"\n        preprocessed = self.preprocess_korean(text)\n        tokens = self.tokenizer(\n            preprocessed,\n            max_length=512,\n            truncation=True,\n            padding=True,\n            return_tensors=\"pt\"\n        )\n        return tokens\n\n# ì‚¬ìš© ì˜ˆì‹œ\npreprocessor = KoreanPreprocessor(\"klue/bert-base\")\nresult = preprocessor.tokenize_korean(\"ì•ˆë…•í•˜ì„¸ìš”!!! ğŸ˜Š í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤ã…‹ã…‹ã…‹\")\n```\n\n## í•œêµ­ì–´ ë°ì´í„° ì¦ê°•\n\n```python\nimport random\n\nclass KoreanDataAugmentation:\n    def __init__(self):\n        # ìœ ì˜ì–´ ì‚¬ì „ (ì‹¤ì œë¡œëŠ” ë” í° ì‚¬ì „ í•„ìš”)\n        self.synonyms = {\n            \"ì¢‹ë‹¤\": [\"í›Œë¥­í•˜ë‹¤\", \"ë©‹ì§€ë‹¤\", \"ê´œì°®ë‹¤\", \"ë‚˜ì˜ì§€ì•Šë‹¤\"],\n            \"ë‚˜ì˜ë‹¤\": [\"ë³„ë¡œë‹¤\", \"ì•ˆì¢‹ë‹¤\", \"í˜•í¸ì—†ë‹¤\"],\n            \"í¬ë‹¤\": [\"ê±°ëŒ€í•˜ë‹¤\", \"í°\", \"ëŒ€í˜•ì˜\"],\n            \"ì‘ë‹¤\": [\"ì†Œí˜•ì˜\", \"ì‘ì€\", \"ë¯¸ë‹ˆ\"]\n        }\n    \n    def synonym_replacement(self, text, n=1):\n        \"\"\"ìœ ì˜ì–´ ì¹˜í™˜\"\"\"\n        words = text.split()\n        new_words = words.copy()\n        \n        random_word_list = list(set([word for word in words if word in self.synonyms]))\n        random.shuffle(random_word_list)\n        \n        num_replaced = 0\n        for random_word in random_word_list:\n            if num_replaced < n:\n                synonyms = self.synonyms[random_word]\n                synonym = random.choice(synonyms)\n                new_words = [synonym if word == random_word else word for word in new_words]\n                num_replaced += 1\n        \n        return ' '.join(new_words)\n    \n    def random_insertion(self, text, n=1):\n        \"\"\"ì„ì˜ ì‚½ì…\"\"\"\n        words = text.split()\n        \n        for _ in range(n):\n            new_word = self._get_random_synonym()\n            random_idx = random.randint(0, len(words))\n            words.insert(random_idx, new_word)\n        \n        return ' '.join(words)\n    \n    def _get_random_synonym(self):\n        \"\"\"ì„ì˜ ìœ ì˜ì–´ ì„ íƒ\"\"\"\n        word = random.choice(list(self.synonyms.keys()))\n        return random.choice(self.synonyms[word])\n\n# ì‚¬ìš© ì˜ˆì‹œ\naugmenter = KoreanDataAugmentation()\noriginal = \"ì´ ì˜í™”ëŠ” ì •ë§ ì¢‹ë‹¤\"\naugmented = augmenter.synonym_replacement(original)\nprint(f\"ì›ë³¸: {original}\")\nprint(f\"ì¦ê°•: {augmented}\")\n```\n\n# ê²°ë¡ \n\nHugging FaceëŠ” **NLP ë¶„ì•¼ì˜ ê²Œì„ì²´ì¸ì €**ë¡œì„œ, ì—°êµ¬ì™€ ì‹¤ë¬´ ì‚¬ì´ì˜ ê°„ê²©ì„ í˜ì‹ ì ìœ¼ë¡œ ì¤„ì˜€ë‹¤. \n\n## í•µì‹¬ ì„±ê³¼\n\n* **ê¸°ìˆ  ë¯¼ì£¼í™”**: ë³µì¡í•œ PLMì„ ëª‡ ì¤„ì˜ ì½”ë“œë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦\n* **í‘œì¤€í™”**: ëª¨ë“  ëª¨ë¸ì´ ë™ì¼í•œ APIë¡œ í†µì¼ë˜ì–´ í•™ìŠµ ê³¡ì„  ë‹¨ì¶•\n* **ìƒíƒœê³„ êµ¬ì¶•**: 10ë§Œ+ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ê³µìœ í•˜ëŠ” ê±°ëŒ€í•œ ì»¤ë®¤ë‹ˆí‹° í˜•ì„±\n* **ê°œë°œ ê°€ì†í™”**: í”„ë¡œí† íƒ€ì…ë¶€í„° í”„ë¡œë•ì…˜ê¹Œì§€ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì§€ì›\n\n## ì‹¤ë¬´ì—ì„œì˜ ê°€ì¹˜\n\n**Before Hugging Face** ì‹œëŒ€ì—ëŠ” ìƒˆë¡œìš´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë…¼ë¬¸ì„ ì½ê³ , ì½”ë“œë¥¼ ì§ì ‘ êµ¬í˜„í•˜ë©°, ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì•„ í—¤ë§¤ì•¼ í–ˆë‹¤. **After Hugging Face** ì‹œëŒ€ì—ëŠ” `pipeline(\"task\", model=\"model_name\")`ë§Œìœ¼ë¡œ ìµœì‹  ê¸°ìˆ ì„ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤.\n\nì´ëŸ¬í•œ ì ‘ê·¼ì„± í–¥ìƒì€ ë‹¨ìˆœí•œ í¸ì˜ì„±ì„ ë„˜ì–´ì„œ, **AI ê¸°ìˆ ì˜ ì§„ì… ì¥ë²½ì„ ë‚®ì¶° ë” ë§ì€ ê°œë°œìë“¤ì´ NLP ë¶„ì•¼ì— ì°¸ì—¬í•  ìˆ˜ ìˆê²Œ** ë§Œë“¤ì—ˆë‹¤. ìŠ¤íƒ€íŠ¸ì—…ë¶€í„° ëŒ€ê¸°ì—…ê¹Œì§€, ì—°êµ¬ìë¶€í„° ì‹¤ë¬´ ê°œë°œìê¹Œì§€ ëª¨ë“  ë ˆë²¨ì—ì„œ í˜œíƒì„ ë°›ê³  ìˆë‹¤.\n\n## ë¯¸ë˜ ì „ë§\n\nHugging FaceëŠ” ê³„ì†í•´ì„œ ë°œì „í•˜ê³  ìˆë‹¤:\n\n* **ë” í° ëª¨ë¸ë“¤**: LLaMA, Falcon ë“± ì˜¤í”ˆì†ŒìŠ¤ ëŒ€ê·œëª¨ ëª¨ë¸ ì§€ì› í™•ëŒ€\n* **ë©€í‹°ëª¨ë‹¬**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ìŒì„±ì„ í†µí•©í•˜ëŠ” ëª¨ë¸ë“¤\n* **íš¨ìœ¨ì„±**: ì–‘ìí™”, í”„ë£¨ë‹, LoRA ë“± íš¨ìœ¨ì  í•™ìŠµ/ì¶”ë¡  ê¸°ë²•\n* **AutoML**: ìë™ ëª¨ë¸ ì„ íƒê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n* **Edge ë°°í¬**: ëª¨ë°”ì¼ê³¼ ì„ë² ë””ë“œ í™˜ê²½ ì§€ì› ê°•í™”\n\n## ì‹¤ë¬´ì§„ì„ ìœ„í•œ ì¡°ì–¸\n\n1. **Pipelineë¶€í„° ì‹œì‘**: ë³µì¡í•œ êµ¬í˜„ë³´ë‹¤ëŠ” Pipeline APIë¡œ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… ì œì‘\n2. **í•œêµ­ì–´ ëª¨ë¸ í™œìš©**: KLUE ë“± ê²€ì¦ëœ í•œêµ­ì–´ ëª¨ë¸ ìš°ì„  ê³ ë ¤\n3. **ë‹¨ê³„ì  ì ‘ê·¼**: API â†’ íŒŒì¸íŠœë‹ â†’ ì»¤ìŠ¤í…€ ëª¨ë¸ ìˆœìœ¼ë¡œ ì ì§„ì  ë°œì „\n4. **ì»¤ë®¤ë‹ˆí‹° í™œìš©**: Model Hubì˜ í‰ê°€ì™€ ë¦¬ë·°ë¥¼ ì°¸ê³ í•œ í˜„ëª…í•œ ëª¨ë¸ ì„ íƒ\n5. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë°˜ë“œì‹œ ì„±ëŠ¥ê³¼ í’ˆì§ˆ ì¶”ì \n\nHugging FaceëŠ” **\"ì—°êµ¬ì‹¤ì˜ ìµœì‹  ê¸°ìˆ ì„ í˜„ì‹¤ì˜ ë¬¸ì œ í•´ê²°ì— ë°”ë¡œ ì ìš©\"**í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê°•ë ¥í•œ ë„êµ¬ë‹¤. ì´ì œ ì¤‘ìš”í•œ ê²ƒì€ ê¸°ìˆ  ìì²´ê°€ ì•„ë‹ˆë¼, **ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•  ê²ƒì¸ê°€**ì™€ **ì–´ë–»ê²Œ ì‚¬ìš©ìì—ê²Œ ê°€ì¹˜ë¥¼ ì œê³µí•  ê²ƒì¸ê°€**ì´ë‹¤. Hugging FaceëŠ” ê·¸ ì—¬ì •ì„ ìœ„í•œ ìµœê³ ì˜ ë™ë°˜ìê°€ ë  ê²ƒì´ë‹¤.\n\n","srcMarkdownNoYaml":"\n\n# ìš”ì•½\n\nHugging FaceëŠ” í˜„ì¬ **NLP ë¶„ì•¼ì˜ ì‚¬ì‹¤ìƒ í‘œì¤€**ì´ ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ì í”Œë«í¼ì´ë‹¤. PyTorchì™€ TensorFlow ëª¨ë‘ë¥¼ ì§€ì›í•˜ë©°, ìˆ˜ë§Œ ê°œì˜ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ì œê³µí•˜ëŠ” ê±°ëŒ€í•œ ìƒíƒœê³„ë¥¼ êµ¬ì¶•í–ˆë‹¤.\n\n## í•µì‹¬ ê°€ì¹˜ ì œì•ˆ\n\n* **ì ‘ê·¼ì„± í˜ëª…**: ëª‡ ì¤„ì˜ ì½”ë“œë¡œ ìµœì‹  PLM ì‚¬ìš© ê°€ëŠ¥\n* **í‘œì¤€í™”**: ëª¨ë“  ëª¨ë¸ì´ ë™ì¼í•œ APIë¡œ í†µì¼\n* **ì™„ì „í•œ ì›Œí¬í”Œë¡œìš°**: ì „ì²˜ë¦¬ë¶€í„° ë°°í¬ê¹Œì§€ ì›ìŠ¤í†± ì§€ì›\n* **ê±°ëŒ€í•œ ì»¤ë®¤ë‹ˆí‹°**: ì „ ì„¸ê³„ ê°œë°œìë“¤ì´ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ê³µìœ \n\n## ì£¼ìš” êµ¬ì„± ìš”ì†Œ\n\n* **Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬**: í•µì‹¬ ëª¨ë¸ êµ¬í˜„ì²´\n* **Model Hub**: 10ë§Œ+ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ì €ì¥ì†Œ\n* **Datasets**: í‘œì¤€í™”ëœ ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n* **Tokenizers**: ê³ ì„±ëŠ¥ í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬\n* **Accelerate**: ë¶„ì‚° í•™ìŠµ ë° ìµœì í™” ë„êµ¬\n* **Gradio**: ë¹ ë¥¸ ë°ëª¨ ë° í”„ë¡œí† íƒ€ì… êµ¬ì¶•\n* **Spaces**: ëª¨ë¸ ë°°í¬ ë° ê³µìœ  í”Œë«í¼\n\n## ì‹¤ë¬´ì—ì„œì˜ ê°•ë ¥í•¨\n\n**Before Hugging Face** (2019ë…„ ì´ì „):\n```python\n# ë³µì¡í•œ ëª¨ë¸ êµ¬í˜„ê³¼ ì „ì²˜ë¦¬ í•„ìš”\nclass CustomBERTModel(nn.Module):\n    def __init__(self):\n        # ìˆ˜ë°± ì¤„ì˜ êµ¬í˜„ ì½”ë“œ...\n        pass\n    \n# í† í¬ë‚˜ì´ì € ì§ì ‘ êµ¬í˜„\n# ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì½”ë“œ ì‘ì„±\n# ê° ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¸ API\n```\n\n**After Hugging Face**:\n```python\n# 3ì¤„ë¡œ ë\nfrom transformers import pipeline\nclassifier = pipeline(\"sentiment-analysis\")\nresult = classifier(\"ì´ ì˜í™” ì •ë§ ì¬ë°Œì—ˆì–´!\")\n```\n\nì´ëŸ¬í•œ **ì½”ë“œ ë‹¨ìˆœí™”**ëŠ” NLP ê¸°ìˆ ì˜ ë¯¼ì£¼í™”ë¥¼ ì´ëŒì—ˆìœ¼ë©°, ì—°êµ¬ìë¿ë§Œ ì•„ë‹ˆë¼ ì¼ë°˜ ê°œë°œìë“¤ë„ ì‰½ê²Œ ìµœì‹  AI ê¸°ìˆ ì„ í™œìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì—ˆë‹¤.\n\n# Hugging Face ìƒíƒœê³„ ì „ì²´ êµ¬ì¡°\n\n## í”Œë«í¼ ì•„í‚¤í…ì²˜\n\n```{mermaid}\ngraph TD\n    A[Hugging Face Hub] --> B[Models]\n    A --> C[Datasets] \n    A --> D[Spaces]\n    \n    B --> E[Transformers Library]\n    C --> F[Datasets Library]\n    D --> G[Gradio/Streamlit]\n    \n    E --> H[PyTorch]\n    E --> I[TensorFlow]\n    E --> J[JAX]\n    \n    K[ì‚¬ìš©ì] --> L[Pipeline API]\n    K --> M[AutoModel API]\n    K --> N[Trainer API]\n    \n    L --> E\n    M --> E\n    N --> E\n```\n\n## í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤\n\n### 1. Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬\n**ì—­í• **: ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì˜ í†µí•© êµ¬í˜„ì²´\n```python\n# ì§€ì›í•˜ëŠ” ì£¼ìš” ëª¨ë¸ë“¤\nmodels = [\n    \"BERT\", \"GPT-2\", \"T5\", \"BART\", \"RoBERTa\",\n    \"ALBERT\", \"DistilBERT\", \"ELECTRA\", \"DeBERTa\",\n    \"GPT-Neo\", \"GPT-J\", \"OPT\", \"BLOOM\", \"LLaMA\"\n]\n\n# ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ë“¤\ntasks = [\n    \"text-classification\", \"token-classification\",\n    \"question-answering\", \"text-generation\",\n    \"summarization\", \"translation\", \"fill-mask\"\n]\n```\n\n### 2. Model Hubì˜ ê·œëª¨\n```python\n# 2024ë…„ ê¸°ì¤€ í†µê³„\nhub_stats = {\n    'total_models': 100000+,\n    'organizations': 10000+,\n    'downloads_per_month': '10ì–µ+',\n    'supported_languages': 100+,\n    'korean_models': 1000+\n}\n```\n\n### 3. ì§€ì›í•˜ëŠ” í”„ë ˆì„ì›Œí¬\n```python\n# ë©€í‹° í”„ë ˆì„ì›Œí¬ ì§€ì›\nframeworks = {\n    'PyTorch': 'ê¸°ë³¸ ì§€ì›, ê°€ì¥ ë§ì€ ëª¨ë¸',\n    'TensorFlow': 'TF 2.x ì™„ì „ ì§€ì›',\n    'JAX/Flax': 'Google ì—°êµ¬íŒ€ í˜‘ì—…',\n    'ONNX': 'ì¶”ë¡  ìµœì í™” ì§€ì›'\n}\n```\n\n# í† í°í™”ì™€ ì „ì²˜ë¦¬\n\n## í† í¬ë‚˜ì´ì €ì˜ ì¤‘ìš”ì„±\n\n**í•µì‹¬ ì›ì¹™**: ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ëŠ” í•­ìƒ ìŒìœ¼ë¡œ ì‚¬ìš©í•´ì•¼ í•œë‹¤.\n\n```python\n# ì˜¬ë°”ë¥¸ ì‚¬ìš©ë²• âœ…\nfrom transformers import AutoTokenizer, AutoModel\n\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n```\n\n```python\n# ì˜ëª»ëœ ì‚¬ìš©ë²• âŒ - ë‹¤ë¥¸ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # ì˜ì–´\nmodel = AutoModel.from_pretrained(\"klue/bert-base\")  # í•œêµ­ì–´\n# â†’ ì™„ì „íˆ ë‹¤ë¥¸ vocabularyë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜\n```\n\n## í† í°í™” ê³¼ì • ìƒì„¸ ë¶„ì„\n\n```python\nfrom transformers import BertTokenizer\n\n# í•œêµ­ì–´ BERT í† í¬ë‚˜ì´ì €\ntokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")\n\n# ë‹¨ê³„ë³„ í† í°í™” ê³¼ì •\ntext = \"ì•ˆë…•í•˜ì„¸ìš”. ìì—°ì–´ ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤.\"\n\n# 1ë‹¨ê³„: í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• \ntokens = tokenizer.tokenize(text)\nprint(f\"í† í°í™”: {tokens}\")\n# ['ì•ˆë…•', '##í•˜', '##ì„¸ìš”', '.', 'ìì—°ì–´', 'ì²˜ë¦¬ë¥¼', 'ê³µë¶€', '##í•©ë‹ˆë‹¤', '.']\n\n# 2ë‹¨ê³„: í† í°ì„ IDë¡œ ë³€í™˜\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(f\"í† í° ID: {token_ids}\")\n# [2374, 8910, 4567, 119, 15234, 9876, 3456, 7890, 119]\n\n# 3ë‹¨ê³„: íŠ¹ìˆ˜ í† í° ì¶”ê°€ ë° íŒ¨ë”©\nencoded = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\nprint(f\"ìµœì¢… ì¸ì½”ë”©: {encoded}\")\n# {'input_ids': tensor([[101, 2374, 8910, ..., 102]]), \n#  'attention_mask': tensor([[1, 1, 1, ..., 1]])}\n```\n\n## ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì € ì¢…ë¥˜\n\n```python\n# 1. WordPiece (BERT ê³„ì—´)\nwordpiece_tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")\nresult1 = wordpiece_tokenizer.tokenize(\"ìì—°ì–´ì²˜ë¦¬\")\nprint(f\"WordPiece: {result1}\")  # ['ìì—°ì–´', '##ì²˜ë¦¬']\n\n# 2. BPE (GPT ê³„ì—´)\nbpe_tokenizer = GPT2Tokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\nresult2 = bpe_tokenizer.tokenize(\"ìì—°ì–´ì²˜ë¦¬\")\nprint(f\"BPE: {result2}\")  # ['ìì—°ì–´', 'ì²˜ë¦¬']\n\n# 3. SentencePiece (T5, ALBERT ê³„ì—´)\nsp_tokenizer = T5Tokenizer.from_pretrained(\"KETI-AIR/ke-t5-base\")\nresult3 = sp_tokenizer.tokenize(\"ìì—°ì–´ì²˜ë¦¬\")\nprint(f\"SentencePiece: {result3}\")  # ['â–ìì—°ì–´', 'ì²˜ë¦¬']\n```\n\n## í† í¬ë‚˜ì´ì € ì„±ëŠ¥ ìµœì í™”\n\n```python\n# ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ\ntexts = [\"ë¬¸ì¥ 1\", \"ë¬¸ì¥ 2\", \"ë¬¸ì¥ 3\"] * 1000\n\n# ëŠë¦° ë°©ë²• âŒ\nslow_results = []\nfor text in texts:\n    result = tokenizer(text)\n    slow_results.append(result)\n\n# ë¹ ë¥¸ ë°©ë²• âœ… (10-100ë°° ë¹ ë¦„)\nfast_results = tokenizer(texts, padding=True, truncation=True)\n\n# Fast Tokenizer ì‚¬ìš© (Rust êµ¬í˜„)\nfast_tokenizer = AutoTokenizer.from_pretrained(\n    \"klue/bert-base\", \n    use_fast=True  # Rust ê¸°ë°˜ ê³ ì† í† í¬ë‚˜ì´ì €\n)\n```\n\n# ëª¨ë¸ ë¡œë”©ê³¼ í™œìš©\n\n## AutoModel ê³„ì—´ì˜ ê°•ë ¥í•¨\n\n```python\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n\n# ìë™ìœ¼ë¡œ ì ì ˆí•œ ëª¨ë¸ í´ë˜ìŠ¤ ì„ íƒ\nmodel_name = \"klue/bert-base\"\n\n# ë²”ìš© ì¸ì½”ë”\nencoder = AutoModel.from_pretrained(model_name)\n\n# ë¶„ë¥˜ìš© ëª¨ë¸ (ë¶„ë¥˜ í—¤ë“œ ìë™ ì¶”ê°€)\nclassifier = AutoModelForSequenceClassification.from_pretrained(\n    model_name, \n    num_labels=3\n)\n\n# ì§ˆì˜ì‘ë‹µìš© ëª¨ë¸\nqa_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n```\n\n## ëª¨ë¸ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•\n\n```python\nfrom transformers import BertConfig, BertForSequenceClassification\n\n# ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸° ë° ìˆ˜ì •\nconfig = BertConfig.from_pretrained(\"klue/bert-base\")\nconfig.num_labels = 5  # ë¶„ë¥˜ í´ë˜ìŠ¤ ìˆ˜ ë³€ê²½\nconfig.dropout = 0.2   # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ì¡°ì •\nconfig.attention_probs_dropout_prob = 0.1\n\n# ìˆ˜ì •ëœ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ìƒì„±\nmodel = BertForSequenceClassification.from_pretrained(\n    \"klue/bert-base\",\n    config=config\n)\n\n# ëª¨ë¸ êµ¬ì¡° í™•ì¸\nprint(model)\n```\n\n## ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëª¨ë¸ ë¡œë”©\n\n```python\n# 1. ì ˆë°˜ ì •ë°€ë„ ì‚¬ìš© (ë©”ëª¨ë¦¬ 50% ì ˆì•½)\nmodel = AutoModel.from_pretrained(\n    \"klue/bert-base\",\n    torch_dtype=torch.float16\n)\n\n# 2. CPU ì˜¤í”„ë¡œë”© (í° ëª¨ë¸ìš©)\nmodel = AutoModel.from_pretrained(\n    \"microsoft/DialoGPT-large\",\n    device_map=\"auto\",  # ìë™ìœ¼ë¡œ GPU/CPU ë°°ì¹˜\n    low_cpu_mem_usage=True\n)\n\n# 3. 8ë¹„íŠ¸ ì–‘ìí™” (ë©”ëª¨ë¦¬ 75% ì ˆì•½)\nfrom transformers import BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = AutoModel.from_pretrained(\n    \"facebook/opt-6.7b\",\n    quantization_config=quantization_config\n)\n```\n\n# Pipeline API: ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ NLP\n\n## ê¸°ë³¸ Pipeline ì‚¬ìš©ë²•\n\n```python\nfrom transformers import pipeline\n\n# 1. ê°ì • ë¶„ì„\nsentiment_analyzer = pipeline(\n    \"sentiment-analysis\",\n    model=\"klue/bert-base-en-ko-cased\",\n    return_all_scores=True\n)\n\nresult = sentiment_analyzer(\"ì´ ì˜í™” ì •ë§ ì¬ë°Œì—ˆì–´!\")\nprint(result)\n# [{'label': 'POSITIVE', 'score': 0.9998}]\n\n# 2. í…ìŠ¤íŠ¸ ìƒì„±\ngenerator = pipeline(\n    \"text-generation\",\n    model=\"skt/kogpt2-base-v2\",\n    max_length=100,\n    do_sample=True,\n    temperature=0.8\n)\n\nresult = generator(\"ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ”\")\nprint(result[0]['generated_text'])\n\n# 3. ì§ˆì˜ì‘ë‹µ\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=\"klue/bert-base\"\n)\n\ncontext = \"íŒŒì´ì¬ì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ê°œë°œí•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë‹¤.\"\nquestion = \"íŒŒì´ì¬ì„ ê°œë°œí•œ ì‚¬ëŒì€ ëˆ„êµ¬ì¸ê°€?\"\n\nresult = qa_pipeline(question=question, context=context)\nprint(f\"ë‹µ: {result['answer']}, ì‹ ë¢°ë„: {result['score']:.4f}\")\n```\n\n## ê³ ê¸‰ Pipeline ì„¤ì •\n\n```python\n# ë°°ì¹˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\nclassifier = pipeline(\n    \"text-classification\",\n    model=\"klue/bert-base\",\n    device=0,  # GPU ì‚¬ìš©\n    batch_size=16,  # ë°°ì¹˜ í¬ê¸°\n    max_length=512,\n    truncation=True\n)\n\n# ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ ì²˜ë¦¬\ntexts = [\"í…ìŠ¤íŠ¸ 1\", \"í…ìŠ¤íŠ¸ 2\"] * 1000\nresults = classifier(texts)\n\n# ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ í•¨ìˆ˜\ndef preprocess_function(examples):\n    # ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ë¡œì§\n    return tokenizer(examples, truncation=True, padding=True)\n\n# íŒŒì´í”„ë¼ì¸ì— ì»¤ìŠ¤í…€ í•¨ìˆ˜ ì ìš©\ncustom_pipeline = pipeline(\n    \"text-classification\",\n    model=model,\n    tokenizer=tokenizer,\n    preprocessing=preprocess_function\n)\n```\n\n## ì‹¤ë¬´ìš© Pipeline ìµœì í™”\n\n```python\nclass OptimizedPipeline:\n    def __init__(self, model_name, task, device=\"cuda\"):\n        self.pipeline = pipeline(\n            task,\n            model=model_name,\n            device=0 if device == \"cuda\" else -1,\n            batch_size=32,\n            return_all_scores=False\n        )\n        \n    def batch_predict(self, texts, batch_size=32):\n        \"\"\"ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ íš¨ìœ¨ì  ì²˜ë¦¬\"\"\"\n        results = []\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i+batch_size]\n            batch_results = self.pipeline(batch)\n            results.extend(batch_results)\n        return results\n    \n    def predict_with_confidence(self, text, threshold=0.8):\n        \"\"\"ì‹ ë¢°ë„ ê¸°ë°˜ ì˜ˆì¸¡\"\"\"\n        result = self.pipeline(text, return_all_scores=True)\n        max_score = max(result[0], key=lambda x: x['score'])\n        \n        if max_score['score'] >= threshold:\n            return max_score\n        else:\n            return {\"label\": \"UNCERTAIN\", \"score\": max_score['score']}\n\n# ì‚¬ìš© ì˜ˆì‹œ\nclassifier = OptimizedPipeline(\"klue/bert-base\", \"text-classification\")\nresults = classifier.batch_predict([\"í…ìŠ¤íŠ¸1\", \"í…ìŠ¤íŠ¸2\", \"í…ìŠ¤íŠ¸3\"])\n```\n\n# íŒŒì¸íŠœë‹ê³¼ í•™ìŠµ\n\n## Trainer API í™œìš©\n\n```python\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nfrom torch.utils.data import Dataset\n\n# 1. ë°ì´í„°ì…‹ ì¤€ë¹„\nclass CustomDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n# 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì¤€ë¹„\nmodel_name = \"klue/bert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=3\n)\n\n# 3. í•™ìŠµ ì„¤ì •\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=100,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_accuracy\",\n    greater_is_better=True\n)\n\n# 4. í‰ê°€ í•¨ìˆ˜\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = predictions.argmax(axis=-1)\n    \n    accuracy = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted'\n    )\n    \n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# 5. í•™ìŠµ ì‹¤í–‰\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n```\n\n## íš¨ìœ¨ì  íŒŒì¸íŠœë‹ ê¸°ë²•\n\n### LoRA (Low-Rank Adaptation)\n\n```python\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n# LoRA ì„¤ì •\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=16,  # ë­í¬\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"]  # ì ìš©í•  ë ˆì´ì–´\n)\n\n# LoRA ì ìš©\nmodel = get_peft_model(model, lora_config)\n\n# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸\nmodel.print_trainable_parameters()\n# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: 294,912 (ì „ì²´ì˜ 0.27%)\n```\n\n### Gradient Checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)\n\n```python\n# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì„ (ì†ë„ëŠ” ì•½ê°„ ëŠë ¤ì§)\nmodel.gradient_checkpointing_enable()\n\ntraining_args = TrainingArguments(\n    # ... ê¸°íƒ€ ì„¤ì •\n    gradient_checkpointing=True,\n    dataloader_pin_memory=False,  # ë©”ëª¨ë¦¬ ì ˆì•½\n    remove_unused_columns=True\n)\n```\n\n### DeepSpeed í†µí•© (ëŒ€ê·œëª¨ ëª¨ë¸)\n\n```python\n# deepspeed_config.json\ndeepspeed_config = {\n    \"train_batch_size\": 16,\n    \"gradient_accumulation_steps\": 1,\n    \"fp16\": {\n        \"enabled\": True\n    },\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": True\n        }\n    }\n}\n\ntraining_args = TrainingArguments(\n    # ... ê¸°íƒ€ ì„¤ì •\n    deepspeed=deepspeed_config,\n    fp16=True\n)\n```\n\n# ì‹¤ë¬´ í™œìš© ì‚¬ë¡€ì™€ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤\n\n## í•œêµ­ì–´ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ\n\n```python\n# íƒœìŠ¤í¬ë³„ ì¶”ì²œ í•œêµ­ì–´ ëª¨ë¸\nkorean_models = {\n    # ë²”ìš© ì¸ì½”ë”\n    'general_encoder': [\n        \"klue/bert-base\",           # ê°€ì¥ ì•ˆì •ì \n        \"klue/roberta-large\",       # ë†’ì€ ì„±ëŠ¥\n        \"monologg/kobert\",          # ê²½ëŸ‰í™”\n        \"beomi/kcbert-base\"         # ëŒ“ê¸€/ë¦¬ë·° íŠ¹í™”\n    ],\n    \n    # í…ìŠ¤íŠ¸ ìƒì„±\n    'text_generation': [\n        \"skt/kogpt2-base-v2\",       # ì¼ë°˜ì  ìš©ë„\n        \"kakaobrain/kogpt\",         # ê³ í’ˆì§ˆ ìƒì„±\n        \"EleutherAI/polyglot-ko-12.8b\"  # ëŒ€ê·œëª¨ ëª¨ë¸\n    ],\n    \n    # ìš”ì•½\n    'summarization': [\n        \"eenzeenee/t5-base-korean-summarization\",\n        \"psyche/KoT5-summarization\"\n    ],\n    \n    # ë²ˆì—­\n    'translation': [\n        \"Helsinki-NLP/opus-mt-ko-en\",   # í•œâ†’ì˜\n        \"Helsinki-NLP/opus-mt-en-ko\"    # ì˜â†’í•œ\n    ]\n}\n```\n\n## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹\n\n```python\nimport time\nimport torch\nfrom transformers import pipeline\n\ndef benchmark_model(model_name, texts, task=\"text-classification\"):\n    \"\"\"ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\"\"\"\n    \n    # GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”\n    torch.cuda.empty_cache()\n    \n    # ëª¨ë¸ ë¡œë“œ ì‹œê°„\n    start_time = time.time()\n    pipe = pipeline(task, model=model_name, device=0)\n    load_time = time.time() - start_time\n    \n    # ì¶”ë¡  ì‹œê°„ (ë‹¨ì¼)\n    start_time = time.time()\n    single_result = pipe(texts[0])\n    single_inference_time = time.time() - start_time\n    \n    # ì¶”ë¡  ì‹œê°„ (ë°°ì¹˜)\n    start_time = time.time()\n    batch_results = pipe(texts)\n    batch_inference_time = time.time() - start_time\n    \n    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n    memory_usage = torch.cuda.max_memory_allocated() / 1024**3  # GB\n    \n    return {\n        'model': model_name,\n        'load_time': load_time,\n        'single_inference_time': single_inference_time,\n        'batch_inference_time': batch_inference_time,\n        'throughput': len(texts) / batch_inference_time,\n        'memory_usage_gb': memory_usage\n    }\n\n# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰\ntest_texts = [\"í…ŒìŠ¤íŠ¸ ë¬¸ì¥\"] * 100\nmodels_to_test = [\n    \"klue/bert-base\",\n    \"monologg/distilkobert\",\n    \"beomi/kcbert-base\"\n]\n\nfor model in models_to_test:\n    result = benchmark_model(model, test_texts)\n    print(f\"{model}: {result}\")\n```\n\n## í”„ë¡œë•ì…˜ ë°°í¬ ì „ëµ\n\n```python\n# 1. ëª¨ë¸ ìµœì í™” ë° ì €ì¥\nclass ProductionModel:\n    def __init__(self, model_path):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            model_path,\n            torch_dtype=torch.float16,  # ë°˜ì •ë°€ë„\n            return_dict=True\n        )\n        self.model.eval()  # í‰ê°€ ëª¨ë“œ\n        \n    @torch.no_grad()\n    def predict(self, texts):\n        \"\"\"ì¶”ë¡  ì „ìš© ë©”ì„œë“œ\"\"\"\n        inputs = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\",\n            max_length=512\n        )\n        \n        outputs = self.model(**inputs)\n        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n        return predictions.cpu().numpy()\n\n# 2. API ì„œë²„ ì˜ˆì‹œ (FastAPI)\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom typing import List\n\napp = FastAPI()\nmodel = ProductionModel(\"./trained_model\")\n\nclass PredictionRequest(BaseModel):\n    texts: List[str]\n\n@app.post(\"/predict\")\nasync def predict(request: PredictionRequest):\n    predictions = model.predict(request.texts)\n    return {\n        \"predictions\": predictions.tolist(),\n        \"model_info\": \"klue/bert-base\",\n        \"timestamp\": time.time()\n    }\n\n# 3. ë„ì»¤ ì»¨í…Œì´ë„ˆí™”\ndockerfile_content = \"\"\"\nFROM python:3.9-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\"\"\"\n```\n\n## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…\n\n```python\nimport logging\nfrom datetime import datetime\nimport json\n\nclass ModelMonitor:\n    def __init__(self, model_name):\n        self.model_name = model_name\n        self.logger = self._setup_logger()\n        self.prediction_count = 0\n        self.error_count = 0\n        \n    def _setup_logger(self):\n        logger = logging.getLogger(f\"model_{self.model_name}\")\n        handler = logging.FileHandler(f\"model_logs_{self.model_name}.log\")\n        formatter = logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n        )\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        logger.setLevel(logging.INFO)\n        return logger\n    \n    def log_prediction(self, input_text, prediction, confidence, latency):\n        \"\"\"ì˜ˆì¸¡ ê²°ê³¼ ë¡œê¹…\"\"\"\n        self.prediction_count += 1\n        \n        log_data = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"input_length\": len(input_text),\n            \"prediction\": prediction,\n            \"confidence\": confidence,\n            \"latency_ms\": latency * 1000,\n            \"prediction_id\": self.prediction_count\n        }\n        \n        self.logger.info(json.dumps(log_data))\n        \n        # ë‚®ì€ ì‹ ë¢°ë„ ê²½ê³ \n        if confidence < 0.7:\n            self.logger.warning(f\"Low confidence prediction: {confidence}\")\n    \n    def log_error(self, error_msg, input_text):\n        \"\"\"ì˜¤ë¥˜ ë¡œê¹…\"\"\"\n        self.error_count += 1\n        self.logger.error(f\"Error: {error_msg}, Input: {input_text[:100]}...\")\n    \n    def get_stats(self):\n        \"\"\"í†µê³„ ì •ë³´ ë°˜í™˜\"\"\"\n        return {\n            \"total_predictions\": self.prediction_count,\n            \"total_errors\": self.error_count,\n            \"error_rate\": self.error_count / max(self.prediction_count, 1)\n        }\n\n# ì‚¬ìš© ì˜ˆì‹œ\nmonitor = ModelMonitor(\"sentiment_classifier\")\n\ndef monitored_predict(text):\n    start_time = time.time()\n    try:\n        result = model.predict([text])[0]\n        latency = time.time() - start_time\n        \n        prediction = result.argmax()\n        confidence = result.max()\n        \n        monitor.log_prediction(text, prediction, confidence, latency)\n        return {\"prediction\": prediction, \"confidence\": confidence}\n        \n    except Exception as e:\n        monitor.log_error(str(e), text)\n        raise\n```\n\n# í•œêµ­ì–´ NLP íŠ¹í™” ê³ ë ¤ì‚¬í•­\n\n## í•œêµ­ì–´ í† í°í™”ì˜ íŠ¹ìˆ˜ì„±\n\n```python\n# í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ì™€ ì¡°í•©\nfrom konlpy.tag import Mecab\nfrom transformers import AutoTokenizer\n\nclass KoreanPreprocessor:\n    def __init__(self, model_name):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.mecab = Mecab()  # í˜•íƒœì†Œ ë¶„ì„ê¸°\n    \n    def preprocess_korean(self, text):\n        \"\"\"í•œêµ­ì–´ íŠ¹í™” ì „ì²˜ë¦¬\"\"\"\n        \n        # 1. ì •ê·œí™”\n        text = text.strip()\n        text = re.sub(r'[ã„±-ã…ã…-ã…£]+', '', text)  # ììŒ/ëª¨ìŒë§Œ ìˆëŠ” ê²½ìš° ì œê±°\n        text = re.sub(r'\\s+', ' ', text)  # ì—°ì† ê³µë°± ì œê±°\n        \n        # 2. ì´ëª¨ì§€/íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬\n        text = re.sub(r'[^\\w\\sê°€-í£]', '', text)\n        \n        # 3. í˜•íƒœì†Œ ë¶„ì„ (ì„ íƒì )\n        # morphs = self.mecab.morphs(text)\n        # text = ' '.join(morphs)\n        \n        return text\n    \n    def tokenize_korean(self, text):\n        \"\"\"í•œêµ­ì–´ í† í°í™”\"\"\"\n        preprocessed = self.preprocess_korean(text)\n        tokens = self.tokenizer(\n            preprocessed,\n            max_length=512,\n            truncation=True,\n            padding=True,\n            return_tensors=\"pt\"\n        )\n        return tokens\n\n# ì‚¬ìš© ì˜ˆì‹œ\npreprocessor = KoreanPreprocessor(\"klue/bert-base\")\nresult = preprocessor.tokenize_korean(\"ì•ˆë…•í•˜ì„¸ìš”!!! ğŸ˜Š í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤ã…‹ã…‹ã…‹\")\n```\n\n## í•œêµ­ì–´ ë°ì´í„° ì¦ê°•\n\n```python\nimport random\n\nclass KoreanDataAugmentation:\n    def __init__(self):\n        # ìœ ì˜ì–´ ì‚¬ì „ (ì‹¤ì œë¡œëŠ” ë” í° ì‚¬ì „ í•„ìš”)\n        self.synonyms = {\n            \"ì¢‹ë‹¤\": [\"í›Œë¥­í•˜ë‹¤\", \"ë©‹ì§€ë‹¤\", \"ê´œì°®ë‹¤\", \"ë‚˜ì˜ì§€ì•Šë‹¤\"],\n            \"ë‚˜ì˜ë‹¤\": [\"ë³„ë¡œë‹¤\", \"ì•ˆì¢‹ë‹¤\", \"í˜•í¸ì—†ë‹¤\"],\n            \"í¬ë‹¤\": [\"ê±°ëŒ€í•˜ë‹¤\", \"í°\", \"ëŒ€í˜•ì˜\"],\n            \"ì‘ë‹¤\": [\"ì†Œí˜•ì˜\", \"ì‘ì€\", \"ë¯¸ë‹ˆ\"]\n        }\n    \n    def synonym_replacement(self, text, n=1):\n        \"\"\"ìœ ì˜ì–´ ì¹˜í™˜\"\"\"\n        words = text.split()\n        new_words = words.copy()\n        \n        random_word_list = list(set([word for word in words if word in self.synonyms]))\n        random.shuffle(random_word_list)\n        \n        num_replaced = 0\n        for random_word in random_word_list:\n            if num_replaced < n:\n                synonyms = self.synonyms[random_word]\n                synonym = random.choice(synonyms)\n                new_words = [synonym if word == random_word else word for word in new_words]\n                num_replaced += 1\n        \n        return ' '.join(new_words)\n    \n    def random_insertion(self, text, n=1):\n        \"\"\"ì„ì˜ ì‚½ì…\"\"\"\n        words = text.split()\n        \n        for _ in range(n):\n            new_word = self._get_random_synonym()\n            random_idx = random.randint(0, len(words))\n            words.insert(random_idx, new_word)\n        \n        return ' '.join(words)\n    \n    def _get_random_synonym(self):\n        \"\"\"ì„ì˜ ìœ ì˜ì–´ ì„ íƒ\"\"\"\n        word = random.choice(list(self.synonyms.keys()))\n        return random.choice(self.synonyms[word])\n\n# ì‚¬ìš© ì˜ˆì‹œ\naugmenter = KoreanDataAugmentation()\noriginal = \"ì´ ì˜í™”ëŠ” ì •ë§ ì¢‹ë‹¤\"\naugmented = augmenter.synonym_replacement(original)\nprint(f\"ì›ë³¸: {original}\")\nprint(f\"ì¦ê°•: {augmented}\")\n```\n\n# ê²°ë¡ \n\nHugging FaceëŠ” **NLP ë¶„ì•¼ì˜ ê²Œì„ì²´ì¸ì €**ë¡œì„œ, ì—°êµ¬ì™€ ì‹¤ë¬´ ì‚¬ì´ì˜ ê°„ê²©ì„ í˜ì‹ ì ìœ¼ë¡œ ì¤„ì˜€ë‹¤. \n\n## í•µì‹¬ ì„±ê³¼\n\n* **ê¸°ìˆ  ë¯¼ì£¼í™”**: ë³µì¡í•œ PLMì„ ëª‡ ì¤„ì˜ ì½”ë“œë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦\n* **í‘œì¤€í™”**: ëª¨ë“  ëª¨ë¸ì´ ë™ì¼í•œ APIë¡œ í†µì¼ë˜ì–´ í•™ìŠµ ê³¡ì„  ë‹¨ì¶•\n* **ìƒíƒœê³„ êµ¬ì¶•**: 10ë§Œ+ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ê³µìœ í•˜ëŠ” ê±°ëŒ€í•œ ì»¤ë®¤ë‹ˆí‹° í˜•ì„±\n* **ê°œë°œ ê°€ì†í™”**: í”„ë¡œí† íƒ€ì…ë¶€í„° í”„ë¡œë•ì…˜ê¹Œì§€ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì§€ì›\n\n## ì‹¤ë¬´ì—ì„œì˜ ê°€ì¹˜\n\n**Before Hugging Face** ì‹œëŒ€ì—ëŠ” ìƒˆë¡œìš´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë…¼ë¬¸ì„ ì½ê³ , ì½”ë“œë¥¼ ì§ì ‘ êµ¬í˜„í•˜ë©°, ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì•„ í—¤ë§¤ì•¼ í–ˆë‹¤. **After Hugging Face** ì‹œëŒ€ì—ëŠ” `pipeline(\"task\", model=\"model_name\")`ë§Œìœ¼ë¡œ ìµœì‹  ê¸°ìˆ ì„ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤.\n\nì´ëŸ¬í•œ ì ‘ê·¼ì„± í–¥ìƒì€ ë‹¨ìˆœí•œ í¸ì˜ì„±ì„ ë„˜ì–´ì„œ, **AI ê¸°ìˆ ì˜ ì§„ì… ì¥ë²½ì„ ë‚®ì¶° ë” ë§ì€ ê°œë°œìë“¤ì´ NLP ë¶„ì•¼ì— ì°¸ì—¬í•  ìˆ˜ ìˆê²Œ** ë§Œë“¤ì—ˆë‹¤. ìŠ¤íƒ€íŠ¸ì—…ë¶€í„° ëŒ€ê¸°ì—…ê¹Œì§€, ì—°êµ¬ìë¶€í„° ì‹¤ë¬´ ê°œë°œìê¹Œì§€ ëª¨ë“  ë ˆë²¨ì—ì„œ í˜œíƒì„ ë°›ê³  ìˆë‹¤.\n\n## ë¯¸ë˜ ì „ë§\n\nHugging FaceëŠ” ê³„ì†í•´ì„œ ë°œì „í•˜ê³  ìˆë‹¤:\n\n* **ë” í° ëª¨ë¸ë“¤**: LLaMA, Falcon ë“± ì˜¤í”ˆì†ŒìŠ¤ ëŒ€ê·œëª¨ ëª¨ë¸ ì§€ì› í™•ëŒ€\n* **ë©€í‹°ëª¨ë‹¬**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ìŒì„±ì„ í†µí•©í•˜ëŠ” ëª¨ë¸ë“¤\n* **íš¨ìœ¨ì„±**: ì–‘ìí™”, í”„ë£¨ë‹, LoRA ë“± íš¨ìœ¨ì  í•™ìŠµ/ì¶”ë¡  ê¸°ë²•\n* **AutoML**: ìë™ ëª¨ë¸ ì„ íƒê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n* **Edge ë°°í¬**: ëª¨ë°”ì¼ê³¼ ì„ë² ë””ë“œ í™˜ê²½ ì§€ì› ê°•í™”\n\n## ì‹¤ë¬´ì§„ì„ ìœ„í•œ ì¡°ì–¸\n\n1. **Pipelineë¶€í„° ì‹œì‘**: ë³µì¡í•œ êµ¬í˜„ë³´ë‹¤ëŠ” Pipeline APIë¡œ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… ì œì‘\n2. **í•œêµ­ì–´ ëª¨ë¸ í™œìš©**: KLUE ë“± ê²€ì¦ëœ í•œêµ­ì–´ ëª¨ë¸ ìš°ì„  ê³ ë ¤\n3. **ë‹¨ê³„ì  ì ‘ê·¼**: API â†’ íŒŒì¸íŠœë‹ â†’ ì»¤ìŠ¤í…€ ëª¨ë¸ ìˆœìœ¼ë¡œ ì ì§„ì  ë°œì „\n4. **ì»¤ë®¤ë‹ˆí‹° í™œìš©**: Model Hubì˜ í‰ê°€ì™€ ë¦¬ë·°ë¥¼ ì°¸ê³ í•œ í˜„ëª…í•œ ëª¨ë¸ ì„ íƒ\n5. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë°˜ë“œì‹œ ì„±ëŠ¥ê³¼ í’ˆì§ˆ ì¶”ì \n\nHugging FaceëŠ” **\"ì—°êµ¬ì‹¤ì˜ ìµœì‹  ê¸°ìˆ ì„ í˜„ì‹¤ì˜ ë¬¸ì œ í•´ê²°ì— ë°”ë¡œ ì ìš©\"**í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê°•ë ¥í•œ ë„êµ¬ë‹¤. ì´ì œ ì¤‘ìš”í•œ ê²ƒì€ ê¸°ìˆ  ìì²´ê°€ ì•„ë‹ˆë¼, **ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•  ê²ƒì¸ê°€**ì™€ **ì–´ë–»ê²Œ ì‚¬ìš©ìì—ê²Œ ê°€ì¹˜ë¥¼ ì œê³µí•  ê²ƒì¸ê°€**ì´ë‹¤. Hugging FaceëŠ” ê·¸ ì—¬ì •ì„ ìœ„í•œ ìµœê³ ì˜ ë™ë°˜ìê°€ ë  ê²ƒì´ë‹¤.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"27.plm_hugging_face.html"},"language":{"toc-title-document":"ëª©ì°¨","toc-title-website":"ëª©ì°¨","related-formats-title":"ê¸°íƒ€ í˜•ì‹","related-notebooks-title":"Notebooks","source-notebooks-prefix":"ì›ì²œ","other-links-title":"ê¸°íƒ€ ë§í¬","code-links-title":"ì½”ë“œ ë§í¬","launch-dev-container-title":"Dev ì»¨í…Œì´ë„ˆ ì‹¤í–‰","launch-binder-title":"ëœì¹˜ Binder","article-notebook-label":"ê¸°ì‚¬ ë…¸íŠ¸ë¶","notebook-preview-download":"ë…¸íŠ¸ë¶ ë‹¤ìš´ë¡œë“œ","notebook-preview-download-src":"ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ","notebook-preview-back":"ê¸°ì‚¬ë¡œ ëŒì•„ê°€ê¸°","manuscript-meca-bundle":"MECA ì•„ì¹´ì´ë¸Œ","section-title-abstract":"ì´ˆë¡","section-title-appendices":"ë¶€ë¡","section-title-footnotes":"ê°ì£¼","section-title-references":"ì°¸ê³ ë¬¸í—Œ","section-title-reuse":"ë¼ì´ì„¼ìŠ¤","section-title-copyright":"ì €ì‘ê¶Œ","section-title-citation":"ì¸ìš©","appendix-attribution-cite-as":"ì¸ìš©ë°©ë²•","appendix-attribution-bibtex":"BibTeX ì¸ìš©:","title-block-author-single":"ì €ì","title-block-author-plural":"ì €ì","title-block-affiliation-single":"ì†Œì†","title-block-affiliation-plural":"ì†Œì†","title-block-published":"ê³µê°œ","title-block-modified":"Modified","title-block-keywords":"í‚¤ì›Œë“œ","callout-tip-title":"íŒíŠ¸","callout-note-title":"ë…¸íŠ¸","callout-warning-title":"ê²½ê³ ","callout-important-title":"ì¤‘ìš”","callout-caution-title":"ì£¼ì˜","code-summary":"ì½”ë“œ","code-tools-menu-caption":"ì½”ë“œ","code-tools-show-all-code":"ì „ì²´ ì½”ë“œ í‘œì‹œ","code-tools-hide-all-code":"ì „ì²´ ì½”ë“œ ìˆ¨ê¸°ê¸°","code-tools-view-source":"ì†ŒìŠ¤ ì½”ë“œ í‘œì‹œ","code-tools-source-code":"ì†ŒìŠ¤ ì½”ë“œ","tools-share":"Share","tools-download":"Download","code-line":"ì„ ","code-lines":"ìœ¤ê³½","copy-button-tooltip":"í´ë¦½ë³´ë“œ ë³µì‚¬","copy-button-tooltip-success":"ë³µì‚¬ì™„ë£Œ!","repo-action-links-edit":"í¸ì§‘","repo-action-links-source":"ì†ŒìŠ¤ì½”ë“œ ë³´ê¸°","repo-action-links-issue":"ì´ìŠˆ ë³´ê³ ","back-to-top":"ë§¨ ìœ„ë¡œ","search-no-results-text":"ì¼ì¹˜ ì—†ìŒ","search-matching-documents-text":"ì¼ì¹˜ëœ ë¬¸ì„œ","search-copy-link-title":"ê²€ìƒ‰ ë§í¬ ë³µì‚¬","search-hide-matches-text":"ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼ ìˆ¨ê¸°ê¸°","search-more-match-text":"ì¶”ê°€ ê²€ìƒ‰ê²°ê³¼","search-more-matches-text":"ì¶”ê°€ ê²€ìƒ‰ê²°ê³¼","search-clear-button-title":"ì œê±°","search-text-placeholder":"","search-detached-cancel-button-title":"ì·¨ì†Œ","search-submit-button-title":"ê²€ìƒ‰","search-label":"ê²€ìƒ‰","toggle-section":"í† ê¸€ ì„¹ì…˜","toggle-sidebar":"ì‚¬ì´ë“œë°” ì „í™˜","toggle-dark-mode":"ë‹¤í¬ ëª¨ë“œ ì „í™˜","toggle-reader-mode":"ë¦¬ë” ëª¨ë“œ ì „í™˜","toggle-navigation":"íƒìƒ‰ ì „í™˜","crossref-fig-title":"ê·¸ë¦¼","crossref-tbl-title":"í‘œ","crossref-lst-title":"ëª©ë¡","crossref-thm-title":"ì •ë¦¬","crossref-lem-title":"ë³´ì¡°ì •ë¦¬","crossref-cor-title":"ë”°ë¦„ì •ë¦¬","crossref-prp-title":"ëª…ì œ","crossref-cnj-title":"ì¶”ì¸¡","crossref-def-title":"ì •ì˜","crossref-exm-title":"ë³´ê¸°","crossref-exr-title":"ì˜ˆì œ","crossref-ch-prefix":"ì¥","crossref-apx-prefix":"ë¶€ë¡","crossref-sec-prefix":"ì„¹ì…˜","crossref-eq-prefix":"ë°©ì •ì‹","crossref-lof-title":"ê·¸ë¦¼ ëª©ë¡","crossref-lot-title":"í‘œ ëª©ë¡","crossref-lol-title":"ì½”ë“œ ëª©ë¡","environment-proof-title":"ì¦ëª…","environment-remark-title":"ì£¼ì„","environment-solution-title":"í•´ë‹µ","listing-page-order-by":"ì •ë ¬","listing-page-order-by-default":"ë””í´íŠ¸","listing-page-order-by-date-asc":"ë‚ ì§œ(ì˜¤ë¦„ì°¨ìˆœ)","listing-page-order-by-date-desc":"ë‚ ì§œ(ë‚´ë¦¼ì°¨ìˆœ)","listing-page-order-by-number-desc":"í˜ì´ì§€ ë²ˆí˜¸(ë‚´ë¦¼ì°¨ìˆœ)","listing-page-order-by-number-asc":"í˜ì´ì§€ ë²ˆí˜¸(ì˜¤ë¦„ì°¨ìˆœ)","listing-page-field-date":"ë‚ ì§œ","listing-page-field-title":"ì œëª©","listing-page-field-description":"ì„¤ëª…","listing-page-field-author":"ì €ì","listing-page-field-filename":"íŒŒì¼ëª…","listing-page-field-filemodified":"ê°±ì‹ ì¼","listing-page-field-subtitle":"ë¶€ì œëª©","listing-page-field-readingtime":"ì½ê¸° ì‹œê°„","listing-page-field-wordcount":"ë‹¨ì–´ ìˆ˜","listing-page-field-categories":"ë¶„ë¥˜","listing-page-minutes-compact":"{0} ë¶„","listing-page-category-all":"ì „ì²´","listing-page-no-matches":"ì¼ì¹˜ ì—†ìŒ","listing-page-words":"{0} ë‹¨ì–´"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYYë…„ MMì›” DDì¼","title":"Hugging Face: PLM ìƒíƒœê³„ì˜ ì¤‘ì‹¬","subtitle":"ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì˜ í—ˆë¸Œ","description":"Hugging FaceëŠ” í˜„ì¬ NLP ë¶„ì•¼ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ì í”Œë«í¼ì´ë‹¤. ìˆ˜ë§Œ ê°œì˜ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ì œê³µí•˜ë©°, ëª‡ ì¤„ì˜ ì½”ë“œë§Œìœ¼ë¡œ ìµœì‹  PLMì„ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤. í† í¬ë‚˜ì´ì €ë¶€í„° íŒŒì¸íŠœë‹, ë°°í¬ê¹Œì§€ ì „ì²´ ML ì›Œí¬í”Œë¡œìš°ë¥¼ ì§€ì›í•˜ëŠ” Hugging Faceì˜ í•µì‹¬ ê¸°ëŠ¥ë“¤ê³¼ ì‹¤ë¬´ í™œìš© ì „ëµì„ ìƒì„¸íˆ ë¶„ì„í•œë‹¤.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-27","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}