{"title":"Hugging Face: PLM 생태계의 중심","markdown":{"yaml":{"title":"Hugging Face: PLM 생태계의 중심","subtitle":"실무에서 바로 사용할 수 있는 사전 학습 모델의 허브","description":"Hugging Face는 현재 NLP 분야에서 가장 중요한 라이브러리이자 플랫폼이다. 수만 개의 사전 학습 모델을 제공하며, 몇 줄의 코드만으로 최신 PLM을 활용할 수 있게 해준다. 토크나이저부터 파인튜닝, 배포까지 전체 ML 워크플로우를 지원하는 Hugging Face의 핵심 기능들과 실무 활용 전략을 상세히 분석한다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-27","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false,"execute":{"echo":true,"eval":false}},"headingText":"한국어 금융 뉴스 긍정, 부정 분류","containsRefs":false,"markdown":"\n\n\ndataset: [https://github.com/ukairia777/finance_sentiment_corpus](https://github.com/ukairia777/finance_sentiment_corpus)\n\n\n```{python}\n!pip install transformers\n!pip install datasets\n\n# 데이터 다운로드\n\n!wget https://raw.githubusercontent.com/ukairia777/finance_sentiment_corpus/main/finance_data.csv\n```\n\n\n```{python}\nimport pandas as pd\ndf = pd.read_csv('../data/NLP/finance_data.csv')\nprint('샘플의 개수 :', len(df)) # 샘플의 개수 : 4846\n\ndf.head()\ndf['labels'] = df['labels'].replace(['neutral', 'positive', 'negative'],[0, 1, 2])\ndf.head()\n\ndf.to_csv('finance_data.csv', index=False, encoding='utf-8-sig') # 값을 변경한 데이터프레임을 다시 csv로 저장합니다.\n\n# csv 파일로부터 datasets을 로드할 수 있습니다.\nfrom datasets import load_dataset\n\nall_data = load_dataset(\n        \"csv\",\n        data_files={\n            \"train\": \"finance_data.csv\",\n        },\n    )\n\n#현재 train에 모든 데이터가 저장되어져 있습니다.\nall_data\n\n#DatasetDict({\n#    train: Dataset({\n#        features: ['labels', 'sentence', 'kor_sentence'],\n#        num_rows: 4846\n#    })\n#})\n```\n\n이를 dataset의 train_test_split() 기능을 사용하여 8:2 비율로 분리하고 훈련 데이터와 테스트 데이터로 저장합니다.\n\n```{python}\n\ncs = all_data['train'].train_test_split(0.2)\ntrain_cs = cs[\"train\"]\ntest_cs = cs[\"test\"]\n\nprint(train_cs)\nprint(test_cs)\n```\n\n검증 데이터를 위해 훈련 데이터를 다시 8:2로 훈련 데이터와 검증 데이터로 저장합니다.\n\n```{python}\n# 훈련 데이터를 다시 8:2로 분리 후 훈련 데이터와 검증 데이터로 저장\ncs = train_cs.train_test_split(0.2)\ntrain_cs = cs[\"train\"]\nvalid_cs = cs[\"test\"]\n```\n\n데이터셋의 구조는 다음과 같습니다. 훈련 데이터, 검증 데이터, 테스트 데이터로 구성되며 우리가 사용할 열은 kor_text열과 labels열입니다.\n\n\n```{python}\nprint(train_cs)\nprint(valid_cs)\nprint(test_cs)\n\nprint('두번째 샘플 출력 :', train_cs['kor_sentence'][1])\nprint('두번째 샘플의 레이블 출력 :', train_cs['labels'][1])\n```\n\n\n## 데이터셋 전처리\n\n```{python}\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport datetime\nfrom tqdm import tqdm\n\nimport csv\nimport os\n\nimport tensorflow as tf\nimport torch\n\n# BERT 사용을 위함\nfrom transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# for padding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences \n\n# 전처리 및 평가 지표\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score, hamming_loss\n\n```\n\n훈련 데이터, 검증 데이터, 테스트 데이터에 대해서 [CLS] 문장 [SEP] 구조를 만듭니다. [CLS]는 분류를 하기 위해 BERT가 사용하는 첫번째 입력 토큰이며, [SEP]는 입력 문장의 종료를 나타내기 위해 사용하는 스페셜 토큰입니다.\n\n```{python}\n# 훈련 데이터, 검증 데이터, 테스트 데이터에 대해서 `[CLS] 문장 [SEP]` 구조를 만듭니다.\n\ntrain_sentences = list(map(lambda x: '[CLS] ' + str(x) + ' [SEP]', train_cs['kor_sentence']))\nvalidation_sentences = list(map(lambda x: '[CLS] ' + str(x) + ' [SEP]', valid_cs['kor_sentence']))\ntest_sentences = list(map(lambda x: '[CLS] ' + str(x) + ' [SEP]', test_cs['kor_sentence']))\n\ntrain_labels = train_cs['labels']\nvalidation_labels = valid_cs['labels']\ntest_labels = test_cs['labels']\n\ntest_sentences[:5]\n```\n\n['[CLS] 덴마크 로열유니브루가 소유한 칼나필리오타우로그루페(칼나필리스타우라스그룹)는 7개월 동안 맥주 판매량이 14.5% 급증한 4050만ℓ를 기록하며 시장점유율을 23.74%에서 25.18%로 끌어올렸다. [SEP]',\n '[CLS] 순이자 수입은 152.2 mn으로 2008년 101.0 mn에서 증가하였다. [SEP]',\n '[CLS] 인도된 충전기 수가 6590만대로 41% 증가하면서 순매출액은 전년 대비 25.5% 증가한 59.6m를 기록했다. [SEP]',\n '[CLS] 뉴스, 의견 또는 배포에 대한 보상 없음. [SEP]',\n '[CLS] 국내 및 지역에서의 강력한 브랜드 가시성은 가정 판매, 차량 및 소비자 광고에서 가장 중요합니다. [SEP]']\n\n\n 중립 = 0\n긍정 = 1\n부정 = 2\n\n```{python}\ntest_labels[:5]\n```\n\n[1, 1, 1, 0, 0]\n\n\n## BERT 토크나이저를 이용한 전처리\n\nBERT를 사용하기 위해서는 토크나이저와 모델이 반드시 맵핑 관계여야만 합니다. 다시 말해 아래의 이름에 들어가는 모델이름은 반드시 동일해야 합니다.\n\n`BertTokenizer.from_pretrained('모델이름')`\n`BertForSequenceClassification.from_pretrained(\"모델이름\")`\n\n토크나이저는 내부적으로 Vocabulary를 갖고 있어 정수 인코딩을 수행해주는 모듈입니다.\n\n```{python}\n# 한국어 BERT 중 하나인 'klue/bert-base'를 사용.\ntokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n\nMAX_LEN = 128\n\ndef data_to_tensor (sentences, labels):\n  # 정수 인코딩 과정. 각 텍스트를 토큰화한 후에 Vocabulary에 맵핑되는 정수 시퀀스로 변환한다.\n  # ex) ['안녕하세요'] ==> ['안', '녕', '하세요'] ==> [231, 52, 45]\n  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\n  # pad_sequences는 패딩을 위한 모듈. 주어진 최대 길이를 위해서 뒤에서 0으로 채워준다.\n  # ex) [231, 52, 45] ==> [231, 52, 45, 0, 0, 0]\n  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\") \n\n  attention_masks = []\n\n  for seq in input_ids:\n      seq_mask = [float(i > 0) for i in seq]\n      attention_masks.append(seq_mask)\n\n  tensor_inputs = torch.tensor(input_ids)\n  tensor_labels = torch.tensor(labels)\n  tensor_masks = torch.tensor(attention_masks)\n\n  return tensor_inputs, tensor_labels, tensor_masks\n```\n\n훈련 데이터, 검증 데이터, 텍스트 데이터에 대해서 data_to_tensor 함수를 통해서 정수 인코딩 된 데이터, 레이블, 어텐션 마스크를 얻습니다.\n\n```{python}\ntrain_inputs, train_labels, train_masks = data_to_tensor(train_sentences, train_labels)\nvalidation_inputs, validation_labels, validation_masks = data_to_tensor(validation_sentences, validation_labels)\ntest_inputs, test_labels, test_masks = data_to_tensor(test_sentences, test_labels)\n\nprint(train_inputs[0])\nprint(train_masks[0])\n\ntokenizer.decode([2]) # [CLS]\ntokenizer.decode([3]) # [SEP]\n\n```\n\n배치 크기는 32로 하고 파이토치의 데이터로더(배치 단위로 데이터를 꺼내올 수 있도록 하는 모듈)로 변환합니다.\n\n```{python}\nbatch_size = 32\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\nprint('훈련 데이터의 크기:', len(train_labels))\nprint('검증 데이터의 크기:', len(validation_labels))\nprint('테스트 데이터의 크기:', len(test_labels))\n```\n\n## GPU가 정상 셋팅되었는지 확인.  \nColab에서 GPU를 사용하기 위해서는 아래와 같이 설정이 되어있어야만 합니다.  \n\n* 런타임 > 런타임 유형 변경 > 하드웨어 가속기 > 'GPU' 선택\n\n\n```{python}\n\nif torch.cuda.is_available():    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")\n    print('No GPU available, using the CPU instead.')\n\n```\n\n\n## 모델 로드\n\nBERT를 사용하여 텍스트를 분류하는 BERT 아키텍처는 BertForSequenceClassification.from_pretrained(\"모델이름\")을 넣어서 가능합니다. 레이블 수로 num_labels라는 인자값에 레이블의 수를 기재해줍니다.\n\n```{python}\nnum_labels = 3\n\nmodel = BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=num_labels)\nmodel.cuda()\n\n# 옵티마이저 선택\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5,\n                  eps = 1e-8\n                )\n\n\n# 몇 번의 에포크(전체 데이터에 대한 학습 횟수)를 할 것인지 선택\nepochs = 2\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)\n\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))  # hh:mm:ss\n\ndef metrics(predictions, labels):\n    y_pred = predictions\n    y_true = labels\n\n    # 사용 가능한 메트릭들을 사용한다.\n    accuracy = accuracy_score(y_true, y_pred)\n    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n    f1_weighted_average = f1_score(y_true=y_true, y_pred=y_pred, average='weighted', zero_division=0)\n\n    # 메트릭 결과에 대해서 리턴\n    metrics = {'accuracy': accuracy,\n               'f1_macro': f1_macro_average,\n               'f1_micro': f1_micro_average,\n               'f1_weighted': f1_weighted_average}\n\n    return metrics\n```\n\n## 모델 학습\n\n```{python}\n# 랜덤 시드값.\nseed_val = 777\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\nmodel.zero_grad()\nfor epoch_i in range(0, epochs):\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    t0 = time.time()\n    total_loss = 0\n\n    model.train()\n\n    for step, batch in tqdm(enumerate(train_dataloader)):\n        if step % 500 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask, \n                        labels=b_labels)\n        \n        loss = outputs[0]\n        total_loss += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # gradient clipping if it is over a threshold\n        optimizer.step()\n        scheduler.step()\n\n        model.zero_grad()\n\n    avg_train_loss = total_loss / len(train_dataloader)            \n\n    print(\"\")\n    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n\n```\n\n\n## 검증 데이터에 대한 평가\n\n```{python}\nt0 = time.time()\nmodel.eval()\naccum_logits, accum_label_ids = [], []\n\nfor batch in validation_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n\n    with torch.no_grad():\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    for b in logits:\n        # 3개의 값 중 가장 큰 값을 예측한 인덱스로 결정\n        # ex) [ 3.5134246  -0.30875662 -2.111316  ] ==> 0\n        accum_logits.append(np.argmax(b))\n\n    for b in label_ids:\n        accum_label_ids.append(b)\n\naccum_logits = np.array(accum_logits)\naccum_label_ids = np.array(accum_label_ids)\nresults = metrics(accum_logits, accum_label_ids)\n\nprint(\"Accuracy: {0:.4f}\".format(results['accuracy']))\nprint(\"F1 (Macro) Score: {0:.4f}\".format(results['f1_macro']))\nprint(\"F1 (Micro) Score: {0:.4f}\".format(results['f1_micro']))\nprint(\"F1 (Weighted) Score: {0:.4f}\".format(results['f1_weighted']))\n```\n\n\n### 모델 저장과 로드\n\n```{python}\n%pwd\n# 폴더 생성\n%mkdir model\n\npath = '/content/model/'\n\n# 모델 저장\ntorch.save(model.state_dict(), path+\"BERT_news_positive_negative_model.pt\")\n\n# 모델 로드\nmodel.load_state_dict(torch.load(path+\"BERT_news_positive_negative_model.pt\"))\n```\n\n# 테스트 데이터에 대한 평가\n\n```{python}\nt0 = time.time()\nmodel.eval()\naccum_logits, accum_label_ids = [], []\n\nfor step, batch in tqdm(enumerate(test_dataloader)):\n    if step % 100 == 0 and not step == 0:\n        elapsed = format_time(time.time() - t0)\n        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n\n    with torch.no_grad():\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    \n    for b in logits:\n        # 3개의 값 중 가장 큰 값을 예측한 인덱스로 결정\n        # ex) [ 3.5134246  -0.30875662 -2.111316  ] ==> 0\n        accum_logits.append(np.argmax(b))\n\n    for b in label_ids:\n        accum_label_ids.append(b)\n\naccum_logits = np.array(accum_logits)\naccum_label_ids = np.array(accum_label_ids)\nresults = metrics(accum_logits, accum_label_ids)\n\nprint(\"Accuracy: {0:.4f}\".format(results['accuracy']))\nprint(\"F1 (Macro) Score: {0:.4f}\".format(results['f1_macro']))\nprint(\"F1 (Micro) Score: {0:.4f}\".format(results['f1_micro']))\nprint(\"F1 (Weighted) Score: {0:.4f}\".format(results['f1_weighted']))\n```\n\n## 예측\n\n```{python}\nfrom transformers import pipeline\npipe = pipeline(\"text-classification\", model=model.cuda(), tokenizer=tokenizer, device=0, max_length=512,\n                return_all_scores=True, function_to_apply='softmax')\nresult = pipe('SK하이닉스가 매출이 급성장하였다')\nprint(result)\n\npipe = pipeline(\"text-classification\", model=model.cuda(), tokenizer=tokenizer, device=0, max_length=512, function_to_apply='softmax')\nresult = pipe('SK하이닉스가 매출이 급성장하였다')\nprint(result)\n\nlabel_dict = {'LABEL_0' : '중립', 'LABEL_1' : '긍정', 'LABEL_2' : '부정'}\n\ndef prediction(text):\n  result = pipe(text)\n  \n  return [label_dict[result[0]['label']]]\n\nprediction('패스트캠퍼스가 매출이 급성장하였다')\nprediction('ChatGPT의 등장으로 인공지능 스타트업들은 비상이다')\nprediction('인공지능 기술의 발전으로 누군가는 기회를 얻을 것이고, 누군가는 얻지 못할 것이다')\n```\n\n현재 데이터셋의 경우 레이블이 3개이지만 만약 레이블이 2개(긍정, 부정)인 이진 분류 문제였다면?\n모델 로드 시에 num_labels를 바꿔주면 된다.\n\n```{python}\nnum_labels = 2\n\nmodel = BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=num_labels)\nmodel.cuda()\n```","srcMarkdownNoYaml":"\n\n# 한국어 금융 뉴스 긍정, 부정 분류\n\ndataset: [https://github.com/ukairia777/finance_sentiment_corpus](https://github.com/ukairia777/finance_sentiment_corpus)\n\n\n```{python}\n!pip install transformers\n!pip install datasets\n\n# 데이터 다운로드\n\n!wget https://raw.githubusercontent.com/ukairia777/finance_sentiment_corpus/main/finance_data.csv\n```\n\n\n```{python}\nimport pandas as pd\ndf = pd.read_csv('../data/NLP/finance_data.csv')\nprint('샘플의 개수 :', len(df)) # 샘플의 개수 : 4846\n\ndf.head()\ndf['labels'] = df['labels'].replace(['neutral', 'positive', 'negative'],[0, 1, 2])\ndf.head()\n\ndf.to_csv('finance_data.csv', index=False, encoding='utf-8-sig') # 값을 변경한 데이터프레임을 다시 csv로 저장합니다.\n\n# csv 파일로부터 datasets을 로드할 수 있습니다.\nfrom datasets import load_dataset\n\nall_data = load_dataset(\n        \"csv\",\n        data_files={\n            \"train\": \"finance_data.csv\",\n        },\n    )\n\n#현재 train에 모든 데이터가 저장되어져 있습니다.\nall_data\n\n#DatasetDict({\n#    train: Dataset({\n#        features: ['labels', 'sentence', 'kor_sentence'],\n#        num_rows: 4846\n#    })\n#})\n```\n\n이를 dataset의 train_test_split() 기능을 사용하여 8:2 비율로 분리하고 훈련 데이터와 테스트 데이터로 저장합니다.\n\n```{python}\n\ncs = all_data['train'].train_test_split(0.2)\ntrain_cs = cs[\"train\"]\ntest_cs = cs[\"test\"]\n\nprint(train_cs)\nprint(test_cs)\n```\n\n검증 데이터를 위해 훈련 데이터를 다시 8:2로 훈련 데이터와 검증 데이터로 저장합니다.\n\n```{python}\n# 훈련 데이터를 다시 8:2로 분리 후 훈련 데이터와 검증 데이터로 저장\ncs = train_cs.train_test_split(0.2)\ntrain_cs = cs[\"train\"]\nvalid_cs = cs[\"test\"]\n```\n\n데이터셋의 구조는 다음과 같습니다. 훈련 데이터, 검증 데이터, 테스트 데이터로 구성되며 우리가 사용할 열은 kor_text열과 labels열입니다.\n\n\n```{python}\nprint(train_cs)\nprint(valid_cs)\nprint(test_cs)\n\nprint('두번째 샘플 출력 :', train_cs['kor_sentence'][1])\nprint('두번째 샘플의 레이블 출력 :', train_cs['labels'][1])\n```\n\n\n## 데이터셋 전처리\n\n```{python}\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport datetime\nfrom tqdm import tqdm\n\nimport csv\nimport os\n\nimport tensorflow as tf\nimport torch\n\n# BERT 사용을 위함\nfrom transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# for padding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences \n\n# 전처리 및 평가 지표\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score, hamming_loss\n\n```\n\n훈련 데이터, 검증 데이터, 테스트 데이터에 대해서 [CLS] 문장 [SEP] 구조를 만듭니다. [CLS]는 분류를 하기 위해 BERT가 사용하는 첫번째 입력 토큰이며, [SEP]는 입력 문장의 종료를 나타내기 위해 사용하는 스페셜 토큰입니다.\n\n```{python}\n# 훈련 데이터, 검증 데이터, 테스트 데이터에 대해서 `[CLS] 문장 [SEP]` 구조를 만듭니다.\n\ntrain_sentences = list(map(lambda x: '[CLS] ' + str(x) + ' [SEP]', train_cs['kor_sentence']))\nvalidation_sentences = list(map(lambda x: '[CLS] ' + str(x) + ' [SEP]', valid_cs['kor_sentence']))\ntest_sentences = list(map(lambda x: '[CLS] ' + str(x) + ' [SEP]', test_cs['kor_sentence']))\n\ntrain_labels = train_cs['labels']\nvalidation_labels = valid_cs['labels']\ntest_labels = test_cs['labels']\n\ntest_sentences[:5]\n```\n\n['[CLS] 덴마크 로열유니브루가 소유한 칼나필리오타우로그루페(칼나필리스타우라스그룹)는 7개월 동안 맥주 판매량이 14.5% 급증한 4050만ℓ를 기록하며 시장점유율을 23.74%에서 25.18%로 끌어올렸다. [SEP]',\n '[CLS] 순이자 수입은 152.2 mn으로 2008년 101.0 mn에서 증가하였다. [SEP]',\n '[CLS] 인도된 충전기 수가 6590만대로 41% 증가하면서 순매출액은 전년 대비 25.5% 증가한 59.6m를 기록했다. [SEP]',\n '[CLS] 뉴스, 의견 또는 배포에 대한 보상 없음. [SEP]',\n '[CLS] 국내 및 지역에서의 강력한 브랜드 가시성은 가정 판매, 차량 및 소비자 광고에서 가장 중요합니다. [SEP]']\n\n\n 중립 = 0\n긍정 = 1\n부정 = 2\n\n```{python}\ntest_labels[:5]\n```\n\n[1, 1, 1, 0, 0]\n\n\n## BERT 토크나이저를 이용한 전처리\n\nBERT를 사용하기 위해서는 토크나이저와 모델이 반드시 맵핑 관계여야만 합니다. 다시 말해 아래의 이름에 들어가는 모델이름은 반드시 동일해야 합니다.\n\n`BertTokenizer.from_pretrained('모델이름')`\n`BertForSequenceClassification.from_pretrained(\"모델이름\")`\n\n토크나이저는 내부적으로 Vocabulary를 갖고 있어 정수 인코딩을 수행해주는 모듈입니다.\n\n```{python}\n# 한국어 BERT 중 하나인 'klue/bert-base'를 사용.\ntokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n\nMAX_LEN = 128\n\ndef data_to_tensor (sentences, labels):\n  # 정수 인코딩 과정. 각 텍스트를 토큰화한 후에 Vocabulary에 맵핑되는 정수 시퀀스로 변환한다.\n  # ex) ['안녕하세요'] ==> ['안', '녕', '하세요'] ==> [231, 52, 45]\n  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\n  # pad_sequences는 패딩을 위한 모듈. 주어진 최대 길이를 위해서 뒤에서 0으로 채워준다.\n  # ex) [231, 52, 45] ==> [231, 52, 45, 0, 0, 0]\n  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\") \n\n  attention_masks = []\n\n  for seq in input_ids:\n      seq_mask = [float(i > 0) for i in seq]\n      attention_masks.append(seq_mask)\n\n  tensor_inputs = torch.tensor(input_ids)\n  tensor_labels = torch.tensor(labels)\n  tensor_masks = torch.tensor(attention_masks)\n\n  return tensor_inputs, tensor_labels, tensor_masks\n```\n\n훈련 데이터, 검증 데이터, 텍스트 데이터에 대해서 data_to_tensor 함수를 통해서 정수 인코딩 된 데이터, 레이블, 어텐션 마스크를 얻습니다.\n\n```{python}\ntrain_inputs, train_labels, train_masks = data_to_tensor(train_sentences, train_labels)\nvalidation_inputs, validation_labels, validation_masks = data_to_tensor(validation_sentences, validation_labels)\ntest_inputs, test_labels, test_masks = data_to_tensor(test_sentences, test_labels)\n\nprint(train_inputs[0])\nprint(train_masks[0])\n\ntokenizer.decode([2]) # [CLS]\ntokenizer.decode([3]) # [SEP]\n\n```\n\n배치 크기는 32로 하고 파이토치의 데이터로더(배치 단위로 데이터를 꺼내올 수 있도록 하는 모듈)로 변환합니다.\n\n```{python}\nbatch_size = 32\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\nprint('훈련 데이터의 크기:', len(train_labels))\nprint('검증 데이터의 크기:', len(validation_labels))\nprint('테스트 데이터의 크기:', len(test_labels))\n```\n\n## GPU가 정상 셋팅되었는지 확인.  \nColab에서 GPU를 사용하기 위해서는 아래와 같이 설정이 되어있어야만 합니다.  \n\n* 런타임 > 런타임 유형 변경 > 하드웨어 가속기 > 'GPU' 선택\n\n\n```{python}\n\nif torch.cuda.is_available():    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")\n    print('No GPU available, using the CPU instead.')\n\n```\n\n\n## 모델 로드\n\nBERT를 사용하여 텍스트를 분류하는 BERT 아키텍처는 BertForSequenceClassification.from_pretrained(\"모델이름\")을 넣어서 가능합니다. 레이블 수로 num_labels라는 인자값에 레이블의 수를 기재해줍니다.\n\n```{python}\nnum_labels = 3\n\nmodel = BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=num_labels)\nmodel.cuda()\n\n# 옵티마이저 선택\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5,\n                  eps = 1e-8\n                )\n\n\n# 몇 번의 에포크(전체 데이터에 대한 학습 횟수)를 할 것인지 선택\nepochs = 2\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)\n\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))  # hh:mm:ss\n\ndef metrics(predictions, labels):\n    y_pred = predictions\n    y_true = labels\n\n    # 사용 가능한 메트릭들을 사용한다.\n    accuracy = accuracy_score(y_true, y_pred)\n    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n    f1_weighted_average = f1_score(y_true=y_true, y_pred=y_pred, average='weighted', zero_division=0)\n\n    # 메트릭 결과에 대해서 리턴\n    metrics = {'accuracy': accuracy,\n               'f1_macro': f1_macro_average,\n               'f1_micro': f1_micro_average,\n               'f1_weighted': f1_weighted_average}\n\n    return metrics\n```\n\n## 모델 학습\n\n```{python}\n# 랜덤 시드값.\nseed_val = 777\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\nmodel.zero_grad()\nfor epoch_i in range(0, epochs):\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    t0 = time.time()\n    total_loss = 0\n\n    model.train()\n\n    for step, batch in tqdm(enumerate(train_dataloader)):\n        if step % 500 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask, \n                        labels=b_labels)\n        \n        loss = outputs[0]\n        total_loss += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # gradient clipping if it is over a threshold\n        optimizer.step()\n        scheduler.step()\n\n        model.zero_grad()\n\n    avg_train_loss = total_loss / len(train_dataloader)            \n\n    print(\"\")\n    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n\n```\n\n\n## 검증 데이터에 대한 평가\n\n```{python}\nt0 = time.time()\nmodel.eval()\naccum_logits, accum_label_ids = [], []\n\nfor batch in validation_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n\n    with torch.no_grad():\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    for b in logits:\n        # 3개의 값 중 가장 큰 값을 예측한 인덱스로 결정\n        # ex) [ 3.5134246  -0.30875662 -2.111316  ] ==> 0\n        accum_logits.append(np.argmax(b))\n\n    for b in label_ids:\n        accum_label_ids.append(b)\n\naccum_logits = np.array(accum_logits)\naccum_label_ids = np.array(accum_label_ids)\nresults = metrics(accum_logits, accum_label_ids)\n\nprint(\"Accuracy: {0:.4f}\".format(results['accuracy']))\nprint(\"F1 (Macro) Score: {0:.4f}\".format(results['f1_macro']))\nprint(\"F1 (Micro) Score: {0:.4f}\".format(results['f1_micro']))\nprint(\"F1 (Weighted) Score: {0:.4f}\".format(results['f1_weighted']))\n```\n\n\n### 모델 저장과 로드\n\n```{python}\n%pwd\n# 폴더 생성\n%mkdir model\n\npath = '/content/model/'\n\n# 모델 저장\ntorch.save(model.state_dict(), path+\"BERT_news_positive_negative_model.pt\")\n\n# 모델 로드\nmodel.load_state_dict(torch.load(path+\"BERT_news_positive_negative_model.pt\"))\n```\n\n# 테스트 데이터에 대한 평가\n\n```{python}\nt0 = time.time()\nmodel.eval()\naccum_logits, accum_label_ids = [], []\n\nfor step, batch in tqdm(enumerate(test_dataloader)):\n    if step % 100 == 0 and not step == 0:\n        elapsed = format_time(time.time() - t0)\n        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n\n    with torch.no_grad():\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    \n    for b in logits:\n        # 3개의 값 중 가장 큰 값을 예측한 인덱스로 결정\n        # ex) [ 3.5134246  -0.30875662 -2.111316  ] ==> 0\n        accum_logits.append(np.argmax(b))\n\n    for b in label_ids:\n        accum_label_ids.append(b)\n\naccum_logits = np.array(accum_logits)\naccum_label_ids = np.array(accum_label_ids)\nresults = metrics(accum_logits, accum_label_ids)\n\nprint(\"Accuracy: {0:.4f}\".format(results['accuracy']))\nprint(\"F1 (Macro) Score: {0:.4f}\".format(results['f1_macro']))\nprint(\"F1 (Micro) Score: {0:.4f}\".format(results['f1_micro']))\nprint(\"F1 (Weighted) Score: {0:.4f}\".format(results['f1_weighted']))\n```\n\n## 예측\n\n```{python}\nfrom transformers import pipeline\npipe = pipeline(\"text-classification\", model=model.cuda(), tokenizer=tokenizer, device=0, max_length=512,\n                return_all_scores=True, function_to_apply='softmax')\nresult = pipe('SK하이닉스가 매출이 급성장하였다')\nprint(result)\n\npipe = pipeline(\"text-classification\", model=model.cuda(), tokenizer=tokenizer, device=0, max_length=512, function_to_apply='softmax')\nresult = pipe('SK하이닉스가 매출이 급성장하였다')\nprint(result)\n\nlabel_dict = {'LABEL_0' : '중립', 'LABEL_1' : '긍정', 'LABEL_2' : '부정'}\n\ndef prediction(text):\n  result = pipe(text)\n  \n  return [label_dict[result[0]['label']]]\n\nprediction('패스트캠퍼스가 매출이 급성장하였다')\nprediction('ChatGPT의 등장으로 인공지능 스타트업들은 비상이다')\nprediction('인공지능 기술의 발전으로 누군가는 기회를 얻을 것이고, 누군가는 얻지 못할 것이다')\n```\n\n현재 데이터셋의 경우 레이블이 3개이지만 만약 레이블이 2개(긍정, 부정)인 이진 분류 문제였다면?\n모델 로드 시에 num_labels를 바꿔주면 된다.\n\n```{python}\nnum_labels = 2\n\nmodel = BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=num_labels)\nmodel.cuda()\n```"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"28.plm_finance_binary.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Hugging Face: PLM 생태계의 중심","subtitle":"실무에서 바로 사용할 수 있는 사전 학습 모델의 허브","description":"Hugging Face는 현재 NLP 분야에서 가장 중요한 라이브러리이자 플랫폼이다. 수만 개의 사전 학습 모델을 제공하며, 몇 줄의 코드만으로 최신 PLM을 활용할 수 있게 해준다. 토크나이저부터 파인튜닝, 배포까지 전체 ML 워크플로우를 지원하는 Hugging Face의 핵심 기능들과 실무 활용 전략을 상세히 분석한다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-27","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}