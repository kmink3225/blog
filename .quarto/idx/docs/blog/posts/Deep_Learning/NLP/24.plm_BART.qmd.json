{"title":"BART: Bidirectional and Auto-Regressive Transformers","markdown":{"yaml":{"title":"BART: Bidirectional and Auto-Regressive Transformers","subtitle":"인코더-디코더 구조로 NLU와 NLG를 통합한 혁신적 언어 모델","description":"BART는 Facebook AI Research에서 2019년 발표한 혁신적인 사전 학습 모델로, BERT의 양방향 이해 능력과 GPT의 생성 능력을 결합한 encoder-decoder 구조를 특징으로 한다. 다양한 노이즈 함수를 사용한 denoising autoencoder 방식의 사전 학습을 통해 자연어 이해와 생성 모두에서 뛰어난 성능을 보여준다. 텍스트 요약, 기계번역, 질의응답 등 다양한 생성 태스크에서의 활용과 성능을 분석한다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-24","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false},"headingText":"요약","containsRefs":false,"markdown":"\n\n\nBART(Bidirectional and Auto-Regressive Transformers)는 2019년 Facebook AI Research에서 발표한 혁신적인 사전 학습 언어 모델이다. 기존 BERT의 이해 능력과 GPT의 생성 능력을 하나의 모델에서 통합한 encoder-decoder 구조를 통해 자연어 이해(NLU)와 자연어 생성(NLG) 모두에서 탁월한 성능을 보여준다.\n\n주요 특징과 혁신 사항은 다음과 같다:\n\n* **Encoder-Decoder 통합 구조**:\n  - BERT 스타일의 bidirectional encoder로 입력 텍스트의 풍부한 표현 학습\n  - GPT 스타일의 autoregressive decoder로 순차적 텍스트 생성\n  - 양방향 이해와 일방향 생성의 최적 결합\n  - Cross-attention을 통한 encoder-decoder 간 정보 전달\n\n* **다양한 노이즈 함수를 활용한 사전 학습**:\n  - **Denoising Autoencoder**: 손상된 텍스트를 원본으로 복원하는 방식으로 학습\n  - **Token Masking**: BERT와 유사하지만 더 다양한 마스킹 패턴 적용\n  - **Token Deletion**: 임의 토큰 삭제 후 복원\n  - **Text Infilling**: 연속된 토큰 스팬을 하나의 마스크로 대체\n  - **Sentence Permutation**: 문장 순서 무작위 셔플 후 원래 순서 복원\n  - **Document Rotation**: 문서의 시작점을 임의로 회전시킨 후 복원\n\n* **범용적 생성 능력**:\n  - 텍스트 요약, 기계번역, 질의응답, 대화 생성 등 다양한 생성 태스크에 적용\n  - Fine-tuning을 통한 태스크별 최적화\n  - 긴 텍스트 생성에서의 일관성과 품질 향상\n\n* **성능과 효율성**:\n  - BART-Base: 140M 파라미터, BART-Large: 400M 파라미터\n  - CNN/DailyMail 요약 태스크에서 SOTA 달성\n  - WMT 기계번역에서 경쟁력 있는 성능\n  - ConvAI2 대화 생성에서 뛰어난 성능\n\nBART는 단일 모델로 이해와 생성을 모두 잘 수행할 수 있는 가능성을 보여주었으며, 이후 T5, PEGASUS 등 encoder-decoder 기반 모델들의 발전에 중요한 영향을 미쳤다.\n\n# NLP 모델 발전 과정\n\n```\nRNN Language Model\n├── Seq2Seq\n├── Beam Search\n├── Subword Tokenization\n├── Attention\n├── Transformer Encoder (Vaswani et al., 2017)\n|   ├── Positional Encoding\n|   ├── Multi-Head Attention\n|   └── Feed Forward Neural Network\n|\n├── Transformer Decoder (Vaswani et al., 2017)\n|\n├── GPT 시리즈 (OpenAI,2018~)\n|   ├── GPT-1~4\n|   └── ChatGPT (OpenAI,2022~)\n|\n├── BERT 시리즈 (Google,2018~)\n|   ├── BERT\n|   ├── RoBERTa\n|   └── ALBERT\n|\n├── BERT 변형 모델들\n|   ├── RoBERTa (Facebook, 2019)\n|   ├── ALBERT (Google, 2019)\n|   ├── DistilBERT (Hugging Face, 2019)\n|   └── ELECTRA (Google, 2020)\n|\n└── 후속 발전 모델들\n    ├── T5, XLNet, DeBERTa\n    └── GPT-2/3/4, ChatGPT, PaLM 등\n```\n\n# BART 이전 모델들의 한계점\n\n## 기존 언어 모델의 문제점\n\n**BERT의 한계**:\n- Encoder-only 구조로 인한 생성 능력 부족\n- Masked Language Model은 이해에는 강하지만 자연스러운 텍스트 생성에는 부적합\n- 순차적 디코딩이 불가능하여 autoregressive 생성 태스크에 활용 어려움\n- 긴 텍스트 생성 시 일관성과 품질 저하\n\n**GPT의 한계**:\n- Decoder-only 구조로 인한 양방향 문맥 이해 부족\n- Causal masking으로 인해 미래 정보 활용 불가\n- 이해 태스크에서 BERT 대비 상대적으로 낮은 성능\n- 입력 문맥의 전체적 이해보다 순차적 예측에 집중\n\n**Seq2Seq 모델의 한계**:\n- 사전 학습 없이 태스크별 학습으로 인한 데이터 효율성 문제\n- 작은 규모의 모델로 인한 표현력 한계\n- Transfer learning의 혜택을 충분히 활용하지 못함\n- 복잡한 언어 패턴 학습의 어려움\n\n**통합적 접근의 필요성**:\n- NLU와 NLG를 모두 잘 수행하는 단일 모델의 부재\n- 이해와 생성 태스크를 위해 별도 모델 필요\n- 효율적인 전이 학습을 위한 범용적 사전 학습 방법 부족\n\n# BART (Bidirectional and Auto-Regressive Transformers)\n\n## 개요와 기본 개념\n\n* BART는 **양방향 인코더와 자기회귀 디코더를 결합한 혁신적인 언어 모델**\n* 2019년 Facebook AI Research에서 발표되었다. \n* 기존 BERT의 강력한 이해 능력과 GPT의 뛰어난 생성 능력을 하나의 모델에서 통합\n* 자연어 이해(NLU)와 자연어 생성(NLG) 모두에서 탁월한 성능을 보여준다.\n* 모델 크기는 BART-Base (140M), BART-Large (400M) 두 가지가 있다.\n\n### 핵심 아이디어\n\n* **최고의 결합**: BERT의 bidirectional encoder + GPT의 autoregressive decoder\n* **Denoising 사전 학습**: 다양한 노이즈로 손상된 텍스트를 원본으로 복원하며 학습\n* **범용성**: 하나의 모델로 분류, 생성, 번역, 요약 등 다양한 태스크 수행\n* **효율성**: Encoder-decoder 구조의 장점을 최대한 활용\n\n## 아키텍처 상세\n\n### Encoder-Decoder 구조\n\n```\n입력 텍스트 (노이즈 추가)\n    ↓\nBERT-style Bidirectional Encoder\n├── Multi-Head Self-Attention (양방향)\n├── Feed-Forward Network\n└── Layer Normalization\n    ↓\nEncoder 표현 (풍부한 문맥 정보)\n    ↓\nGPT-style Autoregressive Decoder\n├── Masked Multi-Head Self-Attention (인과적)\n├── Cross-Attention (Encoder 참조)\n├── Feed-Forward Network\n└── Layer Normalization\n    ↓\n복원된 원본 텍스트\n```\n\n### 핵심 구성 요소\n\n**Bidirectional Encoder**:\n- BERT와 동일한 구조로 양방향 문맥 처리\n- 입력 시퀀스의 모든 위치를 동시에 참조\n- 풍부한 표현 학습으로 깊은 이해 능력 제공\n- 손상된 입력에서도 robust한 표현 추출\n\n**Autoregressive Decoder**:\n- GPT와 유사한 구조로 순차적 생성\n- 이전 생성 토큰들만 참조하는 causal masking\n- Cross-attention을 통해 encoder 정보 활용\n- 자연스럽고 일관성 있는 텍스트 생성\n\n**Cross-Attention 메커니즘**:\n- Decoder의 각 층에서 encoder 출력 참조\n- Query는 decoder, Key와 Value는 encoder에서 생성\n- 입력 문맥 정보를 생성 과정에 효과적으로 반영\n- Attention visualization으로 해석 가능성 제공\n\n## 사전 학습 방법론\n\n### Denoising Autoencoder 패러다임\n\n**기본 원리**:\n$$\\text{BART}(\\text{corrupt}(x)) = x$$\n\n- 원본 텍스트 x에 다양한 노이즈 함수 적용\n- 손상된 입력에서 원본 복원을 학습 목표로 설정\n- 모델이 언어의 구조와 의미를 깊이 이해하도록 유도\n- 다양한 downstream 태스크에 효과적으로 전이\n\n### 다양한 노이즈 함수\n\n**1. Token Masking**\n```\n원본: \"The quick brown fox jumps over the lazy dog\"\n마스킹: \"The quick [MASK] fox jumps [MASK] the lazy dog\"\n```\n- BERT의 MLM과 유사하지만 더 유연한 패턴\n- 임의의 토큰을 [MASK]로 대체\n- 문맥을 통한 단어 의미 추론 능력 학습\n\n**2. Token Deletion**\n```\n원본: \"The quick brown fox jumps over the lazy dog\"\n삭제: \"The brown fox jumps the lazy dog\"\n```\n- 임의의 토큰들을 완전히 제거\n- 모델이 누락된 정보를 추론하여 복원\n- 압축된 정보에서 전체 의미 재구성 능력 향상\n\n**3. Text Infilling**\n```\n원본: \"The quick brown fox jumps over the lazy dog\"\nInfilling: \"The quick [MASK] jumps over [MASK] dog\"\n```\n- 연속된 토큰 스팬을 하나의 [MASK]로 대체\n- 다양한 길이의 스팬을 무작위로 선택\n- 긴 구문의 의미와 구조 학습에 효과적\n\n**4. Sentence Permutation**\n```\n원본: \"First sentence. Second sentence. Third sentence.\"\n셔플: \"Third sentence. First sentence. Second sentence.\"\n```\n- 문장 순서를 무작위로 섞은 후 원래 순서 복원\n- 문서 구조와 논리적 흐름 이해 능력 향상\n- 긴 텍스트의 일관성 유지 학습\n\n**5. Document Rotation**\n```\n원본: \"A B C D E F G H\"\n회전: \"D E F G H A B C\" (토큰 D부터 시작)\n```\n- 문서의 시작점을 임의로 설정한 후 원래 시작점 찾기\n- 문서 전체의 구조적 이해 능력 향상\n- 시작과 끝의 구분 없이 전체 맥락 파악\n\n### 노이즈 함수 조합 및 효과\n\n**Text Infilling + Sentence Permutation (최적 조합)**:\n- BART에서 가장 효과적인 것으로 확인된 조합\n- Text infilling: 지역적 언어 이해 능력\n- Sentence permutation: 전역적 문서 구조 이해\n- 두 방식의 시너지로 최고 성능 달성\n\n**다른 조합들과의 비교**:\n- Token masking only: BERT와 유사한 성능\n- Token deletion only: 정보 손실로 인한 성능 저하\n- 모든 노이즈 함수 조합: 과도한 복잡성으로 최적화 어려움\n\n## 학습 과정\n\n### 사전 학습 세부사항\n\n**데이터셋**:\n- 16GB의 diverse text corpus 사용\n- Book corpus, English Wikipedia, CC-News, OpenWebText\n- 도메인 다양성을 통한 robust한 표현 학습\n\n**학습 설정**:\n- 모델 크기: BART-Base (140M), BART-Large (400M)\n- Batch size: 8,000 sequences\n- Learning rate: 3e-4 (Adam optimizer)\n- 500,000 스텝 학습 (약 250 epochs)\n\n**토큰화**:\n- GPT-2 스타일의 BPE (Byte Pair Encoding)\n- 50,265개의 vocabulary\n- Subword 단위로 효율적 처리\n\n### Fine-tuning 전략\n\n**Generation Tasks**:\n- 전체 encoder-decoder 구조 그대로 사용\n- Task-specific 출력 형식에 맞게 조정\n- 요약, 번역, 질의응답 등에 직접 적용\n\n**Classification Tasks**:\n- Encoder 출력을 classification head에 연결\n- [CLS] 토큰 또는 전체 시퀀스 평균 사용\n- BERT와 유사한 방식으로 fine-tuning\n\n## 주요 특징과 혁신\n\n### 1. 양방향 이해 + 순차적 생성\n\n**Encoder의 양방향 처리**:\n- 입력의 모든 위치를 동시에 참조\n- 문맥의 완전한 이해를 통한 rich representation\n- BERT 수준의 깊은 언어 이해 능력\n\n**Decoder의 순차적 생성**:\n- 이전 토큰들만 참조하는 autoregressive 생성\n- 자연스럽고 일관성 있는 출력 생성\n- GPT 수준의 유창한 텍스트 생성 능력\n\n### 2. 유연한 입력-출력 매핑\n\n**다양한 태스크 형태**:\n- Sequence-to-Sequence: 번역, 요약, 질의응답\n- Sequence-to-Label: 분류, 개체명 인식\n- Conditional Generation: 조건부 텍스트 생성\n\n**길이 변화 처리**:\n- 입력보다 짧은 출력: 요약, 압축\n- 입력보다 긴 출력: 확장, 상세화\n- 동일 길이 출력: 번역, 패러프레이징\n\n### 3. Robust한 표현 학습\n\n**노이즈에 강한 인코딩**:\n- 다양한 노이즈 함수로 훈련되어 robust함\n- 불완전한 입력에서도 의미 추출 가능\n- 실제 사용 환경의 noisy input에 효과적\n\n**Transfer Learning 효과**:\n- 사전 학습의 풍부한 언어 지식 활용\n- 적은 양의 태스크별 데이터로도 high performance\n- Domain adaptation에 효과적\n\n## 성능 및 벤치마크\n\n### 텍스트 요약\n\n**CNN/DailyMail 데이터셋**:\n- ROUGE-1: 44.16 (당시 SOTA)\n- ROUGE-2: 21.28 \n- ROUGE-L: 40.90\n- 기존 모델들 대비 평균 2-3점 향상\n\n**XSum 데이터셋**:\n- ROUGE-1: 45.14\n- ROUGE-2: 22.27\n- ROUGE-L: 37.25\n- Abstractive summarization에서 특히 강점\n\n### 기계번역\n\n**WMT16 English-German**:\n- BLEU: 35.0 (Transformer baseline 대비 +2.3)\n- 특히 긴 문장에서 품질 향상 확인\n- Encoder-decoder 구조의 장점 활용\n\n**WMT16 English-French**:\n- BLEU: 41.5\n- Cross-attention을 통한 정확한 alignment\n- 문맥 정보 활용도 개선\n\n### 대화 생성\n\n**ConvAI2 데이터셋**:\n- Perplexity: 16.3 (당시 최고 성능)\n- F1 score: 20.3\n- 일관성 있는 긴 대화 생성 능력\n\n### 독해 및 질의응답\n\n**SQuAD 1.1**:\n- F1: 88.8\n- EM: 84.9\n- BERT와 경쟁적 성능 달성\n\n**SQuAD 2.0**:\n- F1: 86.1\n- EM: 83.2\n- 답변 생성의 자연스러움 향상\n\n# 결론\n\nBART는 자연어 처리 분야에서 **이해와 생성을 통합한 새로운 패러다임**을 제시한 혁신적인 모델이다. 2019년 Facebook AI Research에서 발표된 이후, encoder-decoder 기반 사전 학습 모델의 가능성을 보여주며 NLP 분야에 중요한 영향을 미쳤다.\n\n## BART의 핵심 기여\n\n* **이해와 생성의 통합**: BERT의 양방향 이해 능력과 GPT의 순차적 생성 능력을 하나의 모델에서 효과적으로 결합하여, 단일 모델로 다양한 NLP 태스크를 해결할 수 있는 가능성을 제시했다.\n\n* **혁신적인 사전 학습 방법**: 다양한 노이즈 함수를 활용한 denoising autoencoder 방식으로 언어의 구조와 의미를 깊이 학습하여, 기존 MLM보다 더 robust하고 효과적인 표현을 학습할 수 있음을 보여주었다.\n\n* **Encoder-Decoder 사전 학습의 효과성**: Transformer의 전체 encoder-decoder 구조를 사전 학습에 활용하여, sequence-to-sequence 태스크에서 탁월한 성능을 달성할 수 있음을 입증했다.\n\n* **유연한 적용성**: 텍스트 요약, 기계번역, 질의응답, 대화 생성 등 다양한 생성 태스크뿐만 아니라 분류 태스크에서도 경쟁력 있는 성능을 보여주었다.\n\n## NLP 발전에 미친 영향\n\n**Encoder-Decoder 모델의 부흥**: BART의 성공은 이후 T5, PEGASUS, mT5 등 다양한 encoder-decoder 기반 사전 학습 모델들의 개발을 촉진했다. 특히 생성 태스크에서 encoder-decoder 구조의 우수성을 입증했다.\n\n**사전 학습 방법의 다양화**: 단순한 token masking을 넘어 text infilling, sentence permutation 등 다양한 노이즈 함수를 활용한 사전 학습 방법의 중요성을 보여주었다. 이는 이후 모델들에서 더욱 창의적인 사전 학습 방법들이 개발되는 계기가 되었다.\n\n**생성 태스크의 성능 향상**: 텍스트 요약, 기계번역 등 생성 태스크에서 기존 모델들을 크게 앞서는 성능을 보여주어, 실용적인 NLP 응용 분야의 발전을 가속화했다.\n\n**통합 모델의 가능성**: 하나의 모델로 이해와 생성을 모두 잘 수행할 수 있다는 가능성을 보여주어, 범용 언어 모델 개발의 방향성을 제시했다.\n\n## 실용적 활용과 파급 효과\n\n**산업 응용**: BART는 뉴스 요약, 문서 번역, 챗봇 개발 등 다양한 산업 분야에서 실제로 활용되기 시작했다. 특히 자동 요약 시스템에서는 BART 기반 모델들이 널리 사용되고 있다.\n\n**연구 도구**: 연구자들이 다양한 sequence-to-sequence 태스크를 실험할 수 있는 강력한 baseline을 제공하여, NLP 연구의 효율성을 높였다.\n\n**오픈 소스 생태계**: Hugging Face Transformers 등을 통해 쉽게 사용할 수 있게 되어, 학계와 산업계 모두에서 광범위하게 활용되고 있다.\n\n## 한계와 개선 영역\n\n**계산 복잡도**: Encoder-decoder 구조로 인해 encoder-only나 decoder-only 모델에 비해 더 많은 계산 자원이 필요하다. 특히 inference 시에 decoder의 순차적 생성으로 인한 속도 제약이 있다.\n\n**긴 시퀀스 처리**: Transformer의 quadratic attention complexity로 인해 매우 긴 문서 처리에는 여전히 한계가 있다. 메모리 사용량과 계산 시간이 시퀀스 길이에 따라 급격히 증가한다.\n\n**도메인 특화**: 특정 도메인에 특화된 성능을 위해서는 여전히 상당한 양의 domain-specific 데이터와 fine-tuning이 필요하다.\n\n**해석 가능성**: 복잡한 encoder-decoder 구조로 인해 모델의 의사결정 과정을 이해하고 해석하기가 어렵다.\n\n## 미래 발전 방향\n\n**효율성 개선**: Sparse attention, linear attention 등을 활용한 더 효율적인 BART 변형들이 개발될 것이다. 또한 model compression과 knowledge distillation을 통한 경량화 연구도 활발해질 것이다.\n\n**다중 모달 확장**: 텍스트뿐만 아니라 이미지, 음성 등 다양한 모달리티를 함께 처리할 수 있는 multimodal BART의 개발이 진행될 것이다.\n\n**도메인 특화**: 의료, 법률, 과학 등 특정 도메인에 특화된 BART 모델들이 개발되어 전문 분야에서의 활용도가 높아질 것이다.\n\n**Few-shot Learning**: GPT-3와 같이 few-shot learning 능력을 갖춘 대규모 BART 모델들이 개발되어, 적은 데이터로도 새로운 태스크에 적응할 수 있게 될 것이다.\n\n**실시간 응용**: 더 빠른 inference를 위한 최적화 기술들이 개발되어, 실시간 번역, 실시간 요약 등의 응용이 가능해질 것이다.\n\n## 역사적 의미와 전망\n\nBART는 NLP 역사에서 **이해와 생성의 통합**이라는 중요한 이정표를 세웠다. BERT가 이해 태스크에서, GPT가 생성 태스크에서 각각 혁신을 가져왔다면, BART는 이 두 능력을 하나의 모델에서 효과적으로 결합할 수 있음을 보여주었다.\n\n이는 현재 우리가 목표로 하는 **범용 인공지능(AGI)**의 관점에서도 중요한 의미를 가진다. 인간이 언어를 이해하고 생성하는 능력을 하나의 통합된 시스템에서 수행하듯이, AI도 이해와 생성을 분리하지 않고 통합적으로 처리할 수 있어야 한다.\n\n앞으로 BART의 핵심 아이디어들은 더욱 발전되어, 인간 수준의 언어 이해와 생성 능력을 갖춘 AI 시스템 개발의 중요한 기초가 될 것이다. 특히 대화형 AI, 창작 보조 도구, 자동 번역 시스템 등에서 BART의 영향은 계속해서 확대될 것으로 예상된다.\n\nBART는 단순한 기술적 개선을 넘어, AI가 언어를 통해 인간과 더욱 자연스럽고 효과적으로 소통할 수 있는 미래를 열어가는 중요한 발걸음이었다.\n","srcMarkdownNoYaml":"\n\n# 요약\n\nBART(Bidirectional and Auto-Regressive Transformers)는 2019년 Facebook AI Research에서 발표한 혁신적인 사전 학습 언어 모델이다. 기존 BERT의 이해 능력과 GPT의 생성 능력을 하나의 모델에서 통합한 encoder-decoder 구조를 통해 자연어 이해(NLU)와 자연어 생성(NLG) 모두에서 탁월한 성능을 보여준다.\n\n주요 특징과 혁신 사항은 다음과 같다:\n\n* **Encoder-Decoder 통합 구조**:\n  - BERT 스타일의 bidirectional encoder로 입력 텍스트의 풍부한 표현 학습\n  - GPT 스타일의 autoregressive decoder로 순차적 텍스트 생성\n  - 양방향 이해와 일방향 생성의 최적 결합\n  - Cross-attention을 통한 encoder-decoder 간 정보 전달\n\n* **다양한 노이즈 함수를 활용한 사전 학습**:\n  - **Denoising Autoencoder**: 손상된 텍스트를 원본으로 복원하는 방식으로 학습\n  - **Token Masking**: BERT와 유사하지만 더 다양한 마스킹 패턴 적용\n  - **Token Deletion**: 임의 토큰 삭제 후 복원\n  - **Text Infilling**: 연속된 토큰 스팬을 하나의 마스크로 대체\n  - **Sentence Permutation**: 문장 순서 무작위 셔플 후 원래 순서 복원\n  - **Document Rotation**: 문서의 시작점을 임의로 회전시킨 후 복원\n\n* **범용적 생성 능력**:\n  - 텍스트 요약, 기계번역, 질의응답, 대화 생성 등 다양한 생성 태스크에 적용\n  - Fine-tuning을 통한 태스크별 최적화\n  - 긴 텍스트 생성에서의 일관성과 품질 향상\n\n* **성능과 효율성**:\n  - BART-Base: 140M 파라미터, BART-Large: 400M 파라미터\n  - CNN/DailyMail 요약 태스크에서 SOTA 달성\n  - WMT 기계번역에서 경쟁력 있는 성능\n  - ConvAI2 대화 생성에서 뛰어난 성능\n\nBART는 단일 모델로 이해와 생성을 모두 잘 수행할 수 있는 가능성을 보여주었으며, 이후 T5, PEGASUS 등 encoder-decoder 기반 모델들의 발전에 중요한 영향을 미쳤다.\n\n# NLP 모델 발전 과정\n\n```\nRNN Language Model\n├── Seq2Seq\n├── Beam Search\n├── Subword Tokenization\n├── Attention\n├── Transformer Encoder (Vaswani et al., 2017)\n|   ├── Positional Encoding\n|   ├── Multi-Head Attention\n|   └── Feed Forward Neural Network\n|\n├── Transformer Decoder (Vaswani et al., 2017)\n|\n├── GPT 시리즈 (OpenAI,2018~)\n|   ├── GPT-1~4\n|   └── ChatGPT (OpenAI,2022~)\n|\n├── BERT 시리즈 (Google,2018~)\n|   ├── BERT\n|   ├── RoBERTa\n|   └── ALBERT\n|\n├── BERT 변형 모델들\n|   ├── RoBERTa (Facebook, 2019)\n|   ├── ALBERT (Google, 2019)\n|   ├── DistilBERT (Hugging Face, 2019)\n|   └── ELECTRA (Google, 2020)\n|\n└── 후속 발전 모델들\n    ├── T5, XLNet, DeBERTa\n    └── GPT-2/3/4, ChatGPT, PaLM 등\n```\n\n# BART 이전 모델들의 한계점\n\n## 기존 언어 모델의 문제점\n\n**BERT의 한계**:\n- Encoder-only 구조로 인한 생성 능력 부족\n- Masked Language Model은 이해에는 강하지만 자연스러운 텍스트 생성에는 부적합\n- 순차적 디코딩이 불가능하여 autoregressive 생성 태스크에 활용 어려움\n- 긴 텍스트 생성 시 일관성과 품질 저하\n\n**GPT의 한계**:\n- Decoder-only 구조로 인한 양방향 문맥 이해 부족\n- Causal masking으로 인해 미래 정보 활용 불가\n- 이해 태스크에서 BERT 대비 상대적으로 낮은 성능\n- 입력 문맥의 전체적 이해보다 순차적 예측에 집중\n\n**Seq2Seq 모델의 한계**:\n- 사전 학습 없이 태스크별 학습으로 인한 데이터 효율성 문제\n- 작은 규모의 모델로 인한 표현력 한계\n- Transfer learning의 혜택을 충분히 활용하지 못함\n- 복잡한 언어 패턴 학습의 어려움\n\n**통합적 접근의 필요성**:\n- NLU와 NLG를 모두 잘 수행하는 단일 모델의 부재\n- 이해와 생성 태스크를 위해 별도 모델 필요\n- 효율적인 전이 학습을 위한 범용적 사전 학습 방법 부족\n\n# BART (Bidirectional and Auto-Regressive Transformers)\n\n## 개요와 기본 개념\n\n* BART는 **양방향 인코더와 자기회귀 디코더를 결합한 혁신적인 언어 모델**\n* 2019년 Facebook AI Research에서 발표되었다. \n* 기존 BERT의 강력한 이해 능력과 GPT의 뛰어난 생성 능력을 하나의 모델에서 통합\n* 자연어 이해(NLU)와 자연어 생성(NLG) 모두에서 탁월한 성능을 보여준다.\n* 모델 크기는 BART-Base (140M), BART-Large (400M) 두 가지가 있다.\n\n### 핵심 아이디어\n\n* **최고의 결합**: BERT의 bidirectional encoder + GPT의 autoregressive decoder\n* **Denoising 사전 학습**: 다양한 노이즈로 손상된 텍스트를 원본으로 복원하며 학습\n* **범용성**: 하나의 모델로 분류, 생성, 번역, 요약 등 다양한 태스크 수행\n* **효율성**: Encoder-decoder 구조의 장점을 최대한 활용\n\n## 아키텍처 상세\n\n### Encoder-Decoder 구조\n\n```\n입력 텍스트 (노이즈 추가)\n    ↓\nBERT-style Bidirectional Encoder\n├── Multi-Head Self-Attention (양방향)\n├── Feed-Forward Network\n└── Layer Normalization\n    ↓\nEncoder 표현 (풍부한 문맥 정보)\n    ↓\nGPT-style Autoregressive Decoder\n├── Masked Multi-Head Self-Attention (인과적)\n├── Cross-Attention (Encoder 참조)\n├── Feed-Forward Network\n└── Layer Normalization\n    ↓\n복원된 원본 텍스트\n```\n\n### 핵심 구성 요소\n\n**Bidirectional Encoder**:\n- BERT와 동일한 구조로 양방향 문맥 처리\n- 입력 시퀀스의 모든 위치를 동시에 참조\n- 풍부한 표현 학습으로 깊은 이해 능력 제공\n- 손상된 입력에서도 robust한 표현 추출\n\n**Autoregressive Decoder**:\n- GPT와 유사한 구조로 순차적 생성\n- 이전 생성 토큰들만 참조하는 causal masking\n- Cross-attention을 통해 encoder 정보 활용\n- 자연스럽고 일관성 있는 텍스트 생성\n\n**Cross-Attention 메커니즘**:\n- Decoder의 각 층에서 encoder 출력 참조\n- Query는 decoder, Key와 Value는 encoder에서 생성\n- 입력 문맥 정보를 생성 과정에 효과적으로 반영\n- Attention visualization으로 해석 가능성 제공\n\n## 사전 학습 방법론\n\n### Denoising Autoencoder 패러다임\n\n**기본 원리**:\n$$\\text{BART}(\\text{corrupt}(x)) = x$$\n\n- 원본 텍스트 x에 다양한 노이즈 함수 적용\n- 손상된 입력에서 원본 복원을 학습 목표로 설정\n- 모델이 언어의 구조와 의미를 깊이 이해하도록 유도\n- 다양한 downstream 태스크에 효과적으로 전이\n\n### 다양한 노이즈 함수\n\n**1. Token Masking**\n```\n원본: \"The quick brown fox jumps over the lazy dog\"\n마스킹: \"The quick [MASK] fox jumps [MASK] the lazy dog\"\n```\n- BERT의 MLM과 유사하지만 더 유연한 패턴\n- 임의의 토큰을 [MASK]로 대체\n- 문맥을 통한 단어 의미 추론 능력 학습\n\n**2. Token Deletion**\n```\n원본: \"The quick brown fox jumps over the lazy dog\"\n삭제: \"The brown fox jumps the lazy dog\"\n```\n- 임의의 토큰들을 완전히 제거\n- 모델이 누락된 정보를 추론하여 복원\n- 압축된 정보에서 전체 의미 재구성 능력 향상\n\n**3. Text Infilling**\n```\n원본: \"The quick brown fox jumps over the lazy dog\"\nInfilling: \"The quick [MASK] jumps over [MASK] dog\"\n```\n- 연속된 토큰 스팬을 하나의 [MASK]로 대체\n- 다양한 길이의 스팬을 무작위로 선택\n- 긴 구문의 의미와 구조 학습에 효과적\n\n**4. Sentence Permutation**\n```\n원본: \"First sentence. Second sentence. Third sentence.\"\n셔플: \"Third sentence. First sentence. Second sentence.\"\n```\n- 문장 순서를 무작위로 섞은 후 원래 순서 복원\n- 문서 구조와 논리적 흐름 이해 능력 향상\n- 긴 텍스트의 일관성 유지 학습\n\n**5. Document Rotation**\n```\n원본: \"A B C D E F G H\"\n회전: \"D E F G H A B C\" (토큰 D부터 시작)\n```\n- 문서의 시작점을 임의로 설정한 후 원래 시작점 찾기\n- 문서 전체의 구조적 이해 능력 향상\n- 시작과 끝의 구분 없이 전체 맥락 파악\n\n### 노이즈 함수 조합 및 효과\n\n**Text Infilling + Sentence Permutation (최적 조합)**:\n- BART에서 가장 효과적인 것으로 확인된 조합\n- Text infilling: 지역적 언어 이해 능력\n- Sentence permutation: 전역적 문서 구조 이해\n- 두 방식의 시너지로 최고 성능 달성\n\n**다른 조합들과의 비교**:\n- Token masking only: BERT와 유사한 성능\n- Token deletion only: 정보 손실로 인한 성능 저하\n- 모든 노이즈 함수 조합: 과도한 복잡성으로 최적화 어려움\n\n## 학습 과정\n\n### 사전 학습 세부사항\n\n**데이터셋**:\n- 16GB의 diverse text corpus 사용\n- Book corpus, English Wikipedia, CC-News, OpenWebText\n- 도메인 다양성을 통한 robust한 표현 학습\n\n**학습 설정**:\n- 모델 크기: BART-Base (140M), BART-Large (400M)\n- Batch size: 8,000 sequences\n- Learning rate: 3e-4 (Adam optimizer)\n- 500,000 스텝 학습 (약 250 epochs)\n\n**토큰화**:\n- GPT-2 스타일의 BPE (Byte Pair Encoding)\n- 50,265개의 vocabulary\n- Subword 단위로 효율적 처리\n\n### Fine-tuning 전략\n\n**Generation Tasks**:\n- 전체 encoder-decoder 구조 그대로 사용\n- Task-specific 출력 형식에 맞게 조정\n- 요약, 번역, 질의응답 등에 직접 적용\n\n**Classification Tasks**:\n- Encoder 출력을 classification head에 연결\n- [CLS] 토큰 또는 전체 시퀀스 평균 사용\n- BERT와 유사한 방식으로 fine-tuning\n\n## 주요 특징과 혁신\n\n### 1. 양방향 이해 + 순차적 생성\n\n**Encoder의 양방향 처리**:\n- 입력의 모든 위치를 동시에 참조\n- 문맥의 완전한 이해를 통한 rich representation\n- BERT 수준의 깊은 언어 이해 능력\n\n**Decoder의 순차적 생성**:\n- 이전 토큰들만 참조하는 autoregressive 생성\n- 자연스럽고 일관성 있는 출력 생성\n- GPT 수준의 유창한 텍스트 생성 능력\n\n### 2. 유연한 입력-출력 매핑\n\n**다양한 태스크 형태**:\n- Sequence-to-Sequence: 번역, 요약, 질의응답\n- Sequence-to-Label: 분류, 개체명 인식\n- Conditional Generation: 조건부 텍스트 생성\n\n**길이 변화 처리**:\n- 입력보다 짧은 출력: 요약, 압축\n- 입력보다 긴 출력: 확장, 상세화\n- 동일 길이 출력: 번역, 패러프레이징\n\n### 3. Robust한 표현 학습\n\n**노이즈에 강한 인코딩**:\n- 다양한 노이즈 함수로 훈련되어 robust함\n- 불완전한 입력에서도 의미 추출 가능\n- 실제 사용 환경의 noisy input에 효과적\n\n**Transfer Learning 효과**:\n- 사전 학습의 풍부한 언어 지식 활용\n- 적은 양의 태스크별 데이터로도 high performance\n- Domain adaptation에 효과적\n\n## 성능 및 벤치마크\n\n### 텍스트 요약\n\n**CNN/DailyMail 데이터셋**:\n- ROUGE-1: 44.16 (당시 SOTA)\n- ROUGE-2: 21.28 \n- ROUGE-L: 40.90\n- 기존 모델들 대비 평균 2-3점 향상\n\n**XSum 데이터셋**:\n- ROUGE-1: 45.14\n- ROUGE-2: 22.27\n- ROUGE-L: 37.25\n- Abstractive summarization에서 특히 강점\n\n### 기계번역\n\n**WMT16 English-German**:\n- BLEU: 35.0 (Transformer baseline 대비 +2.3)\n- 특히 긴 문장에서 품질 향상 확인\n- Encoder-decoder 구조의 장점 활용\n\n**WMT16 English-French**:\n- BLEU: 41.5\n- Cross-attention을 통한 정확한 alignment\n- 문맥 정보 활용도 개선\n\n### 대화 생성\n\n**ConvAI2 데이터셋**:\n- Perplexity: 16.3 (당시 최고 성능)\n- F1 score: 20.3\n- 일관성 있는 긴 대화 생성 능력\n\n### 독해 및 질의응답\n\n**SQuAD 1.1**:\n- F1: 88.8\n- EM: 84.9\n- BERT와 경쟁적 성능 달성\n\n**SQuAD 2.0**:\n- F1: 86.1\n- EM: 83.2\n- 답변 생성의 자연스러움 향상\n\n# 결론\n\nBART는 자연어 처리 분야에서 **이해와 생성을 통합한 새로운 패러다임**을 제시한 혁신적인 모델이다. 2019년 Facebook AI Research에서 발표된 이후, encoder-decoder 기반 사전 학습 모델의 가능성을 보여주며 NLP 분야에 중요한 영향을 미쳤다.\n\n## BART의 핵심 기여\n\n* **이해와 생성의 통합**: BERT의 양방향 이해 능력과 GPT의 순차적 생성 능력을 하나의 모델에서 효과적으로 결합하여, 단일 모델로 다양한 NLP 태스크를 해결할 수 있는 가능성을 제시했다.\n\n* **혁신적인 사전 학습 방법**: 다양한 노이즈 함수를 활용한 denoising autoencoder 방식으로 언어의 구조와 의미를 깊이 학습하여, 기존 MLM보다 더 robust하고 효과적인 표현을 학습할 수 있음을 보여주었다.\n\n* **Encoder-Decoder 사전 학습의 효과성**: Transformer의 전체 encoder-decoder 구조를 사전 학습에 활용하여, sequence-to-sequence 태스크에서 탁월한 성능을 달성할 수 있음을 입증했다.\n\n* **유연한 적용성**: 텍스트 요약, 기계번역, 질의응답, 대화 생성 등 다양한 생성 태스크뿐만 아니라 분류 태스크에서도 경쟁력 있는 성능을 보여주었다.\n\n## NLP 발전에 미친 영향\n\n**Encoder-Decoder 모델의 부흥**: BART의 성공은 이후 T5, PEGASUS, mT5 등 다양한 encoder-decoder 기반 사전 학습 모델들의 개발을 촉진했다. 특히 생성 태스크에서 encoder-decoder 구조의 우수성을 입증했다.\n\n**사전 학습 방법의 다양화**: 단순한 token masking을 넘어 text infilling, sentence permutation 등 다양한 노이즈 함수를 활용한 사전 학습 방법의 중요성을 보여주었다. 이는 이후 모델들에서 더욱 창의적인 사전 학습 방법들이 개발되는 계기가 되었다.\n\n**생성 태스크의 성능 향상**: 텍스트 요약, 기계번역 등 생성 태스크에서 기존 모델들을 크게 앞서는 성능을 보여주어, 실용적인 NLP 응용 분야의 발전을 가속화했다.\n\n**통합 모델의 가능성**: 하나의 모델로 이해와 생성을 모두 잘 수행할 수 있다는 가능성을 보여주어, 범용 언어 모델 개발의 방향성을 제시했다.\n\n## 실용적 활용과 파급 효과\n\n**산업 응용**: BART는 뉴스 요약, 문서 번역, 챗봇 개발 등 다양한 산업 분야에서 실제로 활용되기 시작했다. 특히 자동 요약 시스템에서는 BART 기반 모델들이 널리 사용되고 있다.\n\n**연구 도구**: 연구자들이 다양한 sequence-to-sequence 태스크를 실험할 수 있는 강력한 baseline을 제공하여, NLP 연구의 효율성을 높였다.\n\n**오픈 소스 생태계**: Hugging Face Transformers 등을 통해 쉽게 사용할 수 있게 되어, 학계와 산업계 모두에서 광범위하게 활용되고 있다.\n\n## 한계와 개선 영역\n\n**계산 복잡도**: Encoder-decoder 구조로 인해 encoder-only나 decoder-only 모델에 비해 더 많은 계산 자원이 필요하다. 특히 inference 시에 decoder의 순차적 생성으로 인한 속도 제약이 있다.\n\n**긴 시퀀스 처리**: Transformer의 quadratic attention complexity로 인해 매우 긴 문서 처리에는 여전히 한계가 있다. 메모리 사용량과 계산 시간이 시퀀스 길이에 따라 급격히 증가한다.\n\n**도메인 특화**: 특정 도메인에 특화된 성능을 위해서는 여전히 상당한 양의 domain-specific 데이터와 fine-tuning이 필요하다.\n\n**해석 가능성**: 복잡한 encoder-decoder 구조로 인해 모델의 의사결정 과정을 이해하고 해석하기가 어렵다.\n\n## 미래 발전 방향\n\n**효율성 개선**: Sparse attention, linear attention 등을 활용한 더 효율적인 BART 변형들이 개발될 것이다. 또한 model compression과 knowledge distillation을 통한 경량화 연구도 활발해질 것이다.\n\n**다중 모달 확장**: 텍스트뿐만 아니라 이미지, 음성 등 다양한 모달리티를 함께 처리할 수 있는 multimodal BART의 개발이 진행될 것이다.\n\n**도메인 특화**: 의료, 법률, 과학 등 특정 도메인에 특화된 BART 모델들이 개발되어 전문 분야에서의 활용도가 높아질 것이다.\n\n**Few-shot Learning**: GPT-3와 같이 few-shot learning 능력을 갖춘 대규모 BART 모델들이 개발되어, 적은 데이터로도 새로운 태스크에 적응할 수 있게 될 것이다.\n\n**실시간 응용**: 더 빠른 inference를 위한 최적화 기술들이 개발되어, 실시간 번역, 실시간 요약 등의 응용이 가능해질 것이다.\n\n## 역사적 의미와 전망\n\nBART는 NLP 역사에서 **이해와 생성의 통합**이라는 중요한 이정표를 세웠다. BERT가 이해 태스크에서, GPT가 생성 태스크에서 각각 혁신을 가져왔다면, BART는 이 두 능력을 하나의 모델에서 효과적으로 결합할 수 있음을 보여주었다.\n\n이는 현재 우리가 목표로 하는 **범용 인공지능(AGI)**의 관점에서도 중요한 의미를 가진다. 인간이 언어를 이해하고 생성하는 능력을 하나의 통합된 시스템에서 수행하듯이, AI도 이해와 생성을 분리하지 않고 통합적으로 처리할 수 있어야 한다.\n\n앞으로 BART의 핵심 아이디어들은 더욱 발전되어, 인간 수준의 언어 이해와 생성 능력을 갖춘 AI 시스템 개발의 중요한 기초가 될 것이다. 특히 대화형 AI, 창작 보조 도구, 자동 번역 시스템 등에서 BART의 영향은 계속해서 확대될 것으로 예상된다.\n\nBART는 단순한 기술적 개선을 넘어, AI가 언어를 통해 인간과 더욱 자연스럽고 효과적으로 소통할 수 있는 미래를 열어가는 중요한 발걸음이었다.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"24.plm_BART.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"BART: Bidirectional and Auto-Regressive Transformers","subtitle":"인코더-디코더 구조로 NLU와 NLG를 통합한 혁신적 언어 모델","description":"BART는 Facebook AI Research에서 2019년 발표한 혁신적인 사전 학습 모델로, BERT의 양방향 이해 능력과 GPT의 생성 능력을 결합한 encoder-decoder 구조를 특징으로 한다. 다양한 노이즈 함수를 사용한 denoising autoencoder 방식의 사전 학습을 통해 자연어 이해와 생성 모두에서 뛰어난 성능을 보여준다. 텍스트 요약, 기계번역, 질의응답 등 다양한 생성 태스크에서의 활용과 성능을 분석한다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-24","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}