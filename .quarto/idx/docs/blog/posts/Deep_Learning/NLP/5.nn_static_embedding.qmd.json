{"title":"텍스트 벡터화: 신경망 기반 방법론","markdown":{"yaml":{"title":"텍스트 벡터화: 신경망 기반 방법론","subtitle":"워드 임베딩을 이용한 벡터 표현 소개","description":"자연어 처리(NLP)에서 텍스트의 의미와 문맥을 벡터로 표현하는 신경망 기반의 고급 벡터화 방법들을 심층적으로 탐구한다. 정적 워드 임베딩의 원리, 특징, 활용 방안을 다룬다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-05","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false},"headingText":"요약","containsRefs":false,"markdown":"\n\n\n이 문서는 통계 기반 벡터화의 한계를 넘어 텍스트 데이터로부터 풍부한 의미론적, 문맥적 정보를 추출하는 신경망 기반 벡터화 방법론을 소개한다.\n\n*   **DTM 방식의 한계와 신경망 접근법의 등장**:\n    *   전통적인 DTM(문서-단어 행렬) 방식의 문제점(차원의 저주, 희소성, 의미 관계 표현 불가)을 지적하고, 이를 극복하기 위한 신경망 기반 밀집 벡터 표현(워드 임베딩)의 필요성을 설명한다.\n*   **워드 임베딩 (Word Embedding) - 정적 임베딩**:\n    *   **핵심 원리**: \"같은 문맥에 나타나는 단어는 비슷한 의미를 가진다\"는 분포 가설에 기반하여 단어를 저차원 밀집 벡터로 표현한다.\n    *   **Embedding Layer**: 정수 인코딩된 단어를 밀집 벡터로 변환하는 신경망의 핵심 구성 요소로, 그 구조와 Look-up Table 방식을 설명한다.\n*   **실용적 응용 및 평가**:\n    *   임베딩 모델의 성능을 평가하는 내재적 평가(단어 유사도, 관계 유추)와 외재적 평가(다운스트림 태스크 성능) 방법을 소개한다.\n*   **결론**: 신경망 기반 벡터화 기법들의 발전 과정과 그 의의를 요약한다.\n\n# 텍스트 인코딩 및 벡터화\n\n```\n텍스트 벡터화\n├── 1. 전통적 방법 (통계 기반)\n│   ├── BoW\n│   ├── DTM\n│   └── TF-IDF\n│\n├── 2. 신경망 기반 (문맥 독립)\n│   ├── 문맥 독립적 임베딩\n│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)\n│   ├── Word2Vec (CBOW, Skip-gram)\n│   ├── FastText\n│   ├── GloVe\n│   └── 기타 모델: Swivel, LexVec 등\n│\n└── 3. 문맥 기반 임베딩 (Contextual Embedding)\n    └── RNN 계열\n        ├── LSTM\n        ├── GRU\n        └── ELMo\n```\n\n### 신경망 사용 (2008~2018)\n\n### DTM 방식의 한계와 신경망 접근법의 등장\n\n**DTM 방식의 문제점:**\n- **차원의 저주**: 어휘집 크기 = 벡터 차원 (예: 50,000개 단어 → 50,000차원)\n- **희소성**: 대부분의 값이 0인 sparse vector\n- **의미적 관계 부재**: \"왕\"과 \"여왕\"의 관계를 벡터가 표현하지 못함\n- **문제 상황:**\n\n```python\n# 기존 방식 (원-핫 인코딩)\n\"사과\" = [1, 0, 0, 0, 0, ...]  # 50,000차원 벡터\n\"바나나\" = [0, 1, 0, 0, 0, ...]\n\"과일\" = [0, 0, 1, 0, 0, ...]\n```\n\n- 모든 단어가 서로 똑같이 멀어 보임 (유클리드 거리 = √2)\n- \"사과\"와 \"바나나\"가 비슷한 과일이라는 정보가 없음\n- 메모리 낭비 (대부분이 0)\n\n**신경망 접근법의 혁신:**\n- **밀집 벡터(Dense Vector)**: 고정된 낮은 차원 (예: 300차원)에 0/1 값이 아닌 실수 값을 가짐.\n- **의미적 유사도**: 벡터 간 거리로 단어 유사도 측정 가능\n- **문맥 학습**: 주변 단어들을 통해 의미 학습\n\n```python\n# 신경망 모델: 워드 임베딩 (300차원)\n\"사과\" = [0.2, -0.4, 0.7, 0.1, ...]     # 300개 실수\n\"바나나\" = [0.3, -0.5, 0.6, 0.2, ...]   # 비슷한 값들\n\"과일\" = [0.25, -0.45, 0.65, 0.15, ...] # 과일 카테고리\n```\n\n- 문맥 고려 방법 (Neural / Context-dependent)\n- 신경망을 통해 단어의 의미를 주변 문맥을 고려하여 학습하고, 이를 밀집 벡터(Dense Vector)로 표현.\n\n\n### 워드 임베딩 (Word Embedding) \n\n* 문맥 속에서 각 단어가 어떻게 사용되는지까지 신경망을 통해 벡터값을 구해 벡터에 담아내려 시도.\n* [체험: 단어 유사도 측정 -  https://projector.tensorflow.org/](https://projector.tensorflow.org/)\n* 학습 후에는 각 단어 벡터 간의 유사도(의미반영)를 계산할 수 있다.\n* 즉, 신경망 기반의 벡터화라는 것은 벡터의 값이 학습에 의해 결정된다는 것을 의미.\n* 워드 임베딩 모델의 예시\n  * Word2Vec, GloVe, FastText, 모델 내 `Embedding` Layer 사용.      \n* 어떻게 단어를 벡터화?\n   * 단어가 정수화 되면 차원이 정해진 임의의 가중치 테이블의 내적으로 벡터화된다.\n   * 이때, 이 가중치 테이블을 embedding table (= embedding layer) 이라고 하고 각 행이 단어를 의미한다.\n   * 따라서, embedding table의 행의 크기 = vocab_size가 되고 내적의 결과가 단어의 벡터가 되어 딥러닝 입력값으로 사용된다.\n   * 딥러닝의 학습을 통해 이 embedding table (가중치 행렬)의 값이 최적화되고 이 값이 단어의 벡터가 된다.\n   * 딥러닝 자연어 처리 시 거의 항상 하게 되는 작업\n   * vocab_size (고차원, 20k~30k) x embedding_dim (저차원, 128,256,512 등) 크기의 가중치 행렬이 학습되어 임베딩 벡터가 된다.\n   * 단어 -> 정수 인코딩 -> Embedding Layer -> 임베딩 벡터(=밀집 벡터)\n   * 자연어 처리에서 단어를 정수로 바꿔주는 이유가 Embedding Layer를 통해 밀집 벡터로 변환하기 위해서이다.\n   * Lookup table: 정수 인코딩을 밀집 벡터로 변환하는 테이블\n* 워드 임베딩 2가지 유형\n   * 랜덤 초기화 임베딩\n      * NNLM(Neural Network Language Model)과 마찬가지로 초기에 랜덤값의 가중치를 가지고 오차를 구하는 과정에서 embedding table의 값이 학습\n      * NNLM은 이전 단어가 주어졌을 때, 다음 단어를 구하는 학습과정에서 오차를 줄이면서 학습되었으나 텍스트 분류, 개체명 인식등 수많은 task에서도 오차를 줄이며 학습 가능\n      * task에 맞도록 embedding vector값이 최적화된다.\n      * pytorch, keras 등 딥러닝 프레임워크에서 랜덤 초기화된 embedding layer를 제공한다.\n      * 모델이 역전파하는 과정에서 embedding layer의 가중치가 학습되어 최적화된다.\n   * 사전 훈련된 임베딩 (Pre-trained Word Embedding)\n      * 이미 만들어진 임베딩 테이블을 사용\n      * 정해진 특정 알고리즘에 방대한 데이터를 입력으로 학습시킨 후 여러 task의 입력으로 사용\n      * 대표적인 알고리즘으로 word2vec, glove, fasttext가 있음\n      * 이미 방대한 양의 텍스트 데이터로 훈련되어져 있는 임베딩 벡터값들을 갖고와서 딥러닝 모델의 입력값으로 사용\n      * 이때 이 임베딩 벡터들은 word2vec, glove, fasttext 등 특정 알고리즘으로 훈련되어져 있는 임베딩 벡터값들이다.\n      * 이미 학습된 임베딩 벡터를 사용하므로 모델 학습 시간이 줄어들고 더 좋은 성능을 보임\n* 핵심 원리\n  * **분포 가설(Distributional Hypothesis):**\n    * \"같은 문맥에서 나타나는 단어들은 유사한 의미를 가진다\"\n    * 수학적으로 표현하면: $\\text{similarity}(w_i, w_j) \\propto \\text{context\\_overlap}(w_i, w_j)$\n   * **\"같은 문맥에 나타나는 단어들은 비슷한 의미를 가진다\"**\n   * **예시:**\n   ```\n   문장1: \"나는 사과를 먹었다\"\n   문장2: \"나는 바나나를 먹었다\"  \n   문장3: \"나는 딸기를 먹었다\"\n   ```\n   \n   → \"사과\", \"바나나\", \"딸기\"는 같은 위치(문맥)에 나타남\n   → 비슷한 벡터를 가져야 함\n\n* 임베딩 벡터의 의미\n   * 벡터 간 유사도\n\n      ```python\n      cosine_similarity(v_사과, v_바나나) = 0.8  # 높음 (비슷함)\n      cosine_similarity(v_사과, v_컴퓨터) = 0.1  # 낮음 (다름)\n      ```\n   \n      * 수식: $\\text{similarity}(\\mathbf{v}_1, \\mathbf{v}_2) = \\frac{\\mathbf{v}_1 \\cdot \\mathbf{v}_2}{||\\mathbf{v}_1|| \\cdot ||\\mathbf{v}_2||}$\n      * 직관적 해석\n         * 1에 가까울수록 비슷한 의미\n         * 0에 가까울수록 관련 없음\n         * -1에 가까울수록 반대 의미\n   * 벡터 연산의 마법\n      * 유명한 예시: \n         * $\\vec{\\text{king}} - \\vec{\\text{man}} + \\vec{\\text{woman}} \\approx \\vec{\\text{queen}}$\n         * $\\vec{\\text{king}} - \\vec{\\text{man}}$: \"남성성\"을 제거 → \"왕권\" 개념만 남음\n         * $+ \\vec{\\text{woman}}$: \"여성성\" 추가\n         * 결과: \"여성 + 왕권\" → \"여왕\"\n      * 수학적 설명:\n         * 각 벡터를 의미 성분들의 조합으로 생각:\n         ```\n         king = [왕권: 0.9, 남성: 0.8, 권력: 0.7, ...]\n         man = [남성: 0.9, 성인: 0.6, ...]  \n         woman = [여성: 0.9, 성인: 0.6, ...]\n         ```\n         * 연산 후:\n         ```\n         king - man + woman ≈ [왕권: 0.9, 여성: 0.9, 권력: 0.7, ...]\n         ```\n         → \"queen\"과 가장 유사!\n* 실제 학습 예시\n   * 초기 상태 (랜덤)\n      \n      ```\n      \"사과\" = [0.1, -0.3, 0.7, ...]  (랜덤)\n      \"바나나\" = [-0.8, 0.2, -0.1, ...] (랜덤)\n      ```\n      \n      * 서로 전혀 관련 없어 보임\n   * 학습 진행\n      \n      ```\n      문장들을 계속 보면서:\n      \"사과를 먹었다\", \"바나나를 먹었다\", \"딸기를 먹었다\"\n      ...\n\n      * 점차 비슷한 벡터로 수렴:\n      \n      ```\n      \"사과\" = [0.2, -0.4, 0.6, ...]\n      \"바나나\" = [0.3, -0.5, 0.7, ...]  \n      \"딸기\" = [0.25, -0.45, 0.65, ...]\n      ```\n\n   * 학습 완료 후\n      * 의미가 비슷한 단어들 → 벡터 공간에서 가까운 위치\n      * 반대 의미 단어들 → 먼 위치 또는 반대 방향\n      * 유추 관계 → 벡터 연산으로 표현 가능\n\n* **핵심**: 신경망이 \"문맥\"이라는 단서를 통해 **단어의 의미**를 수치로 학습\n\n#### Embedding Layer 구조 분석\n\n* Embedding Layer란?\n   * 정수 인코딩 → 밀집 벡터 변환기\n\n```python\n# 이런 변환을 해주는 것\n15 → [0.2, -0.4, 0.7, 0.1, -0.3]  # 300차원 벡터\n23 → [0.1, 0.3, -0.2, 0.8, 0.5]   # 300차원 벡터  \n7  → [-0.1, 0.6, 0.4, -0.2, 0.9]  # 300차원 벡터\n```\n\n* 왜 정수 인코딩이 필요한가?\n   * 문제: 컴퓨터는 \"사과\"라는 글자를 직접 처리할 수 없음\n   * 해결 과정:\n   \n```python\n# 1단계: 단어 → 정수 (정수 인코딩)\n\"사과\" → 15\n\"바나나\" → 23  \n\"딸기\" → 7\n\n# 2단계: 정수 → 벡터 (Embedding Layer)\n15 → [0.2, -0.4, 0.7, ...]\n23 → [0.1, 0.3, -0.2, ...]\n7  → [-0.1, 0.6, 0.4, ...]\n```\n\n* Look-up Table의 구체적 동작\n   * Embedding Matrix 구조: $\\mathbf{E} \\in \\mathbb{R}^{V \\times d}$\n   * 실제 예시:\n\n   ```python\n   # V = 5 (어휘 크기), d = 3 (임베딩 차원)\n   embedding_matrix = [\n      [0.1, 0.2, 0.3],    # 단어 ID 0의 벡터\n      [0.4, 0.5, 0.6],    # 단어 ID 1의 벡터  \n      [0.7, 0.8, 0.9],    # 단어 ID 2의 벡터\n      [0.2, -0.1, 0.4],   # 단어 ID 3의 벡터\n      [0.5, 0.3, -0.2],   # 단어 ID 4의 벡터\n   ]\n   ```\n\n* 행렬의 의미:\n   * 행(row): 각 단어의 임베딩 벡터\n   * 열(column): 임베딩 벡터의 각 차원\n   * 전체: 모든 단어의 벡터를 저장하는 \"사전\"\n\n* Look-up 연산 과정\n\n   * 입력: `input_ids = [2, 0, 4]`\n\n   ```python\n   # 1단계: 각 ID에 해당하는 행을 추출\n   embedding_matrix[2] → [0.7, 0.8, 0.9]    # ID 2의 벡터\n   embedding_matrix[0] → [0.1, 0.2, 0.3]    # ID 0의 벡터  \n   embedding_matrix[4] → [0.5, 0.3, -0.2]   # ID 4의 벡터\n\n   # 2단계: 결과 (3개 벡터)\n   output = [\n      [0.7, 0.8, 0.9],     # 첫 번째 단어\n      [0.1, 0.2, 0.3],     # 두 번째 단어\n      [0.5, 0.3, -0.2]     # 세 번째 단어  \n   ]\n   ```\n\n* 수학적 의미\n\n   * $\\text{embedding}(i) = \\mathbf{E}[i, :]$\n      * $i$: 단어의 정수 ID\n      * $\\mathbf{E}[i, :]$: 행렬 E의 i번째 행 전체\n      * 결과: i번째 단어의 임베딩 벡터\n   * 구체적 예시:\n\n   ```python\n   i = 2  # \"딸기\"의 ID라고 가정\n   embedding(2) = E[2, :] = [0.7, 0.8, 0.9]  # 2번째 행\n   ```\n\n* 실제 PyTorch 코드\n\n```python\nimport torch\nimport torch.nn as nn\n\n# 1. Embedding Layer 생성\nvocab_size = 1000      # 어휘 크기\nembedding_dim = 300    # 벡터 차원\nembedding = nn.Embedding(vocab_size, embedding_dim)\n\n# 2. 내부 구조 확인\nprint(embedding.weight.shape)  # torch.Size([1000, 300])\n# → 1000×300 크기의 look-up table\n\n# 3. 입력 데이터\ninput_ids = torch.tensor([15, 23, 7])  # 3개 단어의 ID\n\n# 4. 임베딩 변환\noutput = embedding(input_ids)\nprint(output.shape)  # torch.Size([3, 300])\n# → 3개 단어 × 300차원 벡터\n```\n\n* Look-up Table이 학습되는 과정\n   * 초기화 (랜덤)\n\n   ```python\n   # 처음에는 랜덤 값들\n   embedding_matrix = torch.randn(vocab_size, embedding_dim)\n   ```\n\n* 학습 과정\n\n```python\n# 예: \"사과는 맛있다\"라는 문장 학습\ninput_ids = [15, 23, 7]  # [사과는, 맛있다, <END>]\n\n# 1. 현재 임베딩으로 예측\nembeddings = embedding_matrix[input_ids]  # Look-up\nprediction = model(embeddings)\n\n# 2. 손실 계산\nloss = criterion(prediction, target)\n\n# 3. 역전파로 embedding_matrix 업데이트  \nloss.backward()  # embedding_matrix의 gradient 계산\noptimizer.step()  # embedding_matrix 값들 업데이트\n```\n\n* 핵심: 학습이 진행되면서 embedding_matrix의 각 행(단어 벡터)이 점점 더 의미 있는 값으로 변함!\n\n* 왜 \"Look-up Table\"이라고 부르는가?\n   * 전통적인 사전과 비교\n\n   ```python\n   # 일반 사전\n   사전 = {\n      \"사과\": \"빨간 과일\",\n      \"바나나\": \"노란 과일\", \n      \"컴퓨터\": \"전자 기기\"\n   }\n   의미 = 사전[\"사과\"]  # \"빨간 과일\"\n\n   # Embedding Table  \n   임베딩_테이블 = {\n      15: [0.2, -0.4, 0.7, ...],   # \"사과\"\n      23: [0.1, 0.3, -0.2, ...],   # \"바나나\"\n      78: [-0.5, 0.8, 0.1, ...]    # \"컴퓨터\"\n   }\n   벡터 = 임베딩_테이블[15]  # [0.2, -0.4, 0.7, ...]\n   ```\n\n   * 차이점:\n      * 일반 사전: 단어 → 설명 (텍스트)\n      * 임베딩 테이블: 단어 ID → 숫자 벡터\n\n* 전체 과정 정리\n\n```python\n# 전체 파이프라인\n\"사과는 맛있다\" \n→ [\"사과는\", \"맛있다\"]           # 토큰화\n→ [15, 23]                      # 정수 인코딩  \n→ [[0.2, -0.4, 0.7],           # Embedding Layer (Look-up)\n   [0.1, 0.3, -0.2]]\n→ 신경망 처리                    # 후속 레이어들\n```\n\n* 핵심\n   * Embedding Layer는 단순히 \"정수 ID를 인덱스로 사용해서 미리 저장된 벡터를 가져오는\" 매우 단순한 연산. \n   * 하지만 이 벡터들이 학습을 통해 의미 있는 값으로 변하기 때문에 강력한 도구가 되는 것\n\n# 결론\n\n본 문서에서는 자연어 처리(NLP) 분야에서 텍스트 데이터의 의미를 효과적으로 포착하기 위해 통계 기반 방법의 한계를 넘어, 신경망은 단어와 문맥의 복잡한 관계를 학습하여 풍부한 정보를 담은 벡터 표현을 생성한다.\n\n*   **워드 임베딩의 발전**:\n    *   **정적 임베딩 (Word2Vec, GloVe, FastText)**: '분포 가설'에 기반하여 단어를 저차원 밀집 벡터로 표현함으로써 단어 간 의미적 유사성과 관계(예: 유추)를 포착했다. `Embedding Layer`는 이러한 변환의 핵심이며, FastText는 하위 단어(subword) 정보를 활용하여 OOV 문제와 형태론적 특징 처리에 강점을 보였다.\n    *   이러한 초기 신경망 기반 방법들은 단어의 의미를 고정된 벡터로 표현하여 NLP 성능을 크게 향상시켰다.\n*   **벡터화 방법 선택의 중요성**:\n    *   단순한 단어 유사도 측정부터 복잡한 문서 이해 및 생성에 이르기까지, 해결하고자 하는 문제의 특성, 데이터의 규모와 성격, 그리고 사용하려는 모델의 요구사항을 종합적으로 고려하여 적절한 벡터화 전략을 선택하는 것이 중요하다.\n    *   이러한 신경망 기반 벡터화 기법들은 현대 대규모 언어 모델(LLM) 발전의 핵심적인 토대가 되었으며, 자연어 이해 및 생성 능력의 비약적인 발전을 이끌고 있다.","srcMarkdownNoYaml":"\n\n# 요약\n\n이 문서는 통계 기반 벡터화의 한계를 넘어 텍스트 데이터로부터 풍부한 의미론적, 문맥적 정보를 추출하는 신경망 기반 벡터화 방법론을 소개한다.\n\n*   **DTM 방식의 한계와 신경망 접근법의 등장**:\n    *   전통적인 DTM(문서-단어 행렬) 방식의 문제점(차원의 저주, 희소성, 의미 관계 표현 불가)을 지적하고, 이를 극복하기 위한 신경망 기반 밀집 벡터 표현(워드 임베딩)의 필요성을 설명한다.\n*   **워드 임베딩 (Word Embedding) - 정적 임베딩**:\n    *   **핵심 원리**: \"같은 문맥에 나타나는 단어는 비슷한 의미를 가진다\"는 분포 가설에 기반하여 단어를 저차원 밀집 벡터로 표현한다.\n    *   **Embedding Layer**: 정수 인코딩된 단어를 밀집 벡터로 변환하는 신경망의 핵심 구성 요소로, 그 구조와 Look-up Table 방식을 설명한다.\n*   **실용적 응용 및 평가**:\n    *   임베딩 모델의 성능을 평가하는 내재적 평가(단어 유사도, 관계 유추)와 외재적 평가(다운스트림 태스크 성능) 방법을 소개한다.\n*   **결론**: 신경망 기반 벡터화 기법들의 발전 과정과 그 의의를 요약한다.\n\n# 텍스트 인코딩 및 벡터화\n\n```\n텍스트 벡터화\n├── 1. 전통적 방법 (통계 기반)\n│   ├── BoW\n│   ├── DTM\n│   └── TF-IDF\n│\n├── 2. 신경망 기반 (문맥 독립)\n│   ├── 문맥 독립적 임베딩\n│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)\n│   ├── Word2Vec (CBOW, Skip-gram)\n│   ├── FastText\n│   ├── GloVe\n│   └── 기타 모델: Swivel, LexVec 등\n│\n└── 3. 문맥 기반 임베딩 (Contextual Embedding)\n    └── RNN 계열\n        ├── LSTM\n        ├── GRU\n        └── ELMo\n```\n\n### 신경망 사용 (2008~2018)\n\n### DTM 방식의 한계와 신경망 접근법의 등장\n\n**DTM 방식의 문제점:**\n- **차원의 저주**: 어휘집 크기 = 벡터 차원 (예: 50,000개 단어 → 50,000차원)\n- **희소성**: 대부분의 값이 0인 sparse vector\n- **의미적 관계 부재**: \"왕\"과 \"여왕\"의 관계를 벡터가 표현하지 못함\n- **문제 상황:**\n\n```python\n# 기존 방식 (원-핫 인코딩)\n\"사과\" = [1, 0, 0, 0, 0, ...]  # 50,000차원 벡터\n\"바나나\" = [0, 1, 0, 0, 0, ...]\n\"과일\" = [0, 0, 1, 0, 0, ...]\n```\n\n- 모든 단어가 서로 똑같이 멀어 보임 (유클리드 거리 = √2)\n- \"사과\"와 \"바나나\"가 비슷한 과일이라는 정보가 없음\n- 메모리 낭비 (대부분이 0)\n\n**신경망 접근법의 혁신:**\n- **밀집 벡터(Dense Vector)**: 고정된 낮은 차원 (예: 300차원)에 0/1 값이 아닌 실수 값을 가짐.\n- **의미적 유사도**: 벡터 간 거리로 단어 유사도 측정 가능\n- **문맥 학습**: 주변 단어들을 통해 의미 학습\n\n```python\n# 신경망 모델: 워드 임베딩 (300차원)\n\"사과\" = [0.2, -0.4, 0.7, 0.1, ...]     # 300개 실수\n\"바나나\" = [0.3, -0.5, 0.6, 0.2, ...]   # 비슷한 값들\n\"과일\" = [0.25, -0.45, 0.65, 0.15, ...] # 과일 카테고리\n```\n\n- 문맥 고려 방법 (Neural / Context-dependent)\n- 신경망을 통해 단어의 의미를 주변 문맥을 고려하여 학습하고, 이를 밀집 벡터(Dense Vector)로 표현.\n\n\n### 워드 임베딩 (Word Embedding) \n\n* 문맥 속에서 각 단어가 어떻게 사용되는지까지 신경망을 통해 벡터값을 구해 벡터에 담아내려 시도.\n* [체험: 단어 유사도 측정 -  https://projector.tensorflow.org/](https://projector.tensorflow.org/)\n* 학습 후에는 각 단어 벡터 간의 유사도(의미반영)를 계산할 수 있다.\n* 즉, 신경망 기반의 벡터화라는 것은 벡터의 값이 학습에 의해 결정된다는 것을 의미.\n* 워드 임베딩 모델의 예시\n  * Word2Vec, GloVe, FastText, 모델 내 `Embedding` Layer 사용.      \n* 어떻게 단어를 벡터화?\n   * 단어가 정수화 되면 차원이 정해진 임의의 가중치 테이블의 내적으로 벡터화된다.\n   * 이때, 이 가중치 테이블을 embedding table (= embedding layer) 이라고 하고 각 행이 단어를 의미한다.\n   * 따라서, embedding table의 행의 크기 = vocab_size가 되고 내적의 결과가 단어의 벡터가 되어 딥러닝 입력값으로 사용된다.\n   * 딥러닝의 학습을 통해 이 embedding table (가중치 행렬)의 값이 최적화되고 이 값이 단어의 벡터가 된다.\n   * 딥러닝 자연어 처리 시 거의 항상 하게 되는 작업\n   * vocab_size (고차원, 20k~30k) x embedding_dim (저차원, 128,256,512 등) 크기의 가중치 행렬이 학습되어 임베딩 벡터가 된다.\n   * 단어 -> 정수 인코딩 -> Embedding Layer -> 임베딩 벡터(=밀집 벡터)\n   * 자연어 처리에서 단어를 정수로 바꿔주는 이유가 Embedding Layer를 통해 밀집 벡터로 변환하기 위해서이다.\n   * Lookup table: 정수 인코딩을 밀집 벡터로 변환하는 테이블\n* 워드 임베딩 2가지 유형\n   * 랜덤 초기화 임베딩\n      * NNLM(Neural Network Language Model)과 마찬가지로 초기에 랜덤값의 가중치를 가지고 오차를 구하는 과정에서 embedding table의 값이 학습\n      * NNLM은 이전 단어가 주어졌을 때, 다음 단어를 구하는 학습과정에서 오차를 줄이면서 학습되었으나 텍스트 분류, 개체명 인식등 수많은 task에서도 오차를 줄이며 학습 가능\n      * task에 맞도록 embedding vector값이 최적화된다.\n      * pytorch, keras 등 딥러닝 프레임워크에서 랜덤 초기화된 embedding layer를 제공한다.\n      * 모델이 역전파하는 과정에서 embedding layer의 가중치가 학습되어 최적화된다.\n   * 사전 훈련된 임베딩 (Pre-trained Word Embedding)\n      * 이미 만들어진 임베딩 테이블을 사용\n      * 정해진 특정 알고리즘에 방대한 데이터를 입력으로 학습시킨 후 여러 task의 입력으로 사용\n      * 대표적인 알고리즘으로 word2vec, glove, fasttext가 있음\n      * 이미 방대한 양의 텍스트 데이터로 훈련되어져 있는 임베딩 벡터값들을 갖고와서 딥러닝 모델의 입력값으로 사용\n      * 이때 이 임베딩 벡터들은 word2vec, glove, fasttext 등 특정 알고리즘으로 훈련되어져 있는 임베딩 벡터값들이다.\n      * 이미 학습된 임베딩 벡터를 사용하므로 모델 학습 시간이 줄어들고 더 좋은 성능을 보임\n* 핵심 원리\n  * **분포 가설(Distributional Hypothesis):**\n    * \"같은 문맥에서 나타나는 단어들은 유사한 의미를 가진다\"\n    * 수학적으로 표현하면: $\\text{similarity}(w_i, w_j) \\propto \\text{context\\_overlap}(w_i, w_j)$\n   * **\"같은 문맥에 나타나는 단어들은 비슷한 의미를 가진다\"**\n   * **예시:**\n   ```\n   문장1: \"나는 사과를 먹었다\"\n   문장2: \"나는 바나나를 먹었다\"  \n   문장3: \"나는 딸기를 먹었다\"\n   ```\n   \n   → \"사과\", \"바나나\", \"딸기\"는 같은 위치(문맥)에 나타남\n   → 비슷한 벡터를 가져야 함\n\n* 임베딩 벡터의 의미\n   * 벡터 간 유사도\n\n      ```python\n      cosine_similarity(v_사과, v_바나나) = 0.8  # 높음 (비슷함)\n      cosine_similarity(v_사과, v_컴퓨터) = 0.1  # 낮음 (다름)\n      ```\n   \n      * 수식: $\\text{similarity}(\\mathbf{v}_1, \\mathbf{v}_2) = \\frac{\\mathbf{v}_1 \\cdot \\mathbf{v}_2}{||\\mathbf{v}_1|| \\cdot ||\\mathbf{v}_2||}$\n      * 직관적 해석\n         * 1에 가까울수록 비슷한 의미\n         * 0에 가까울수록 관련 없음\n         * -1에 가까울수록 반대 의미\n   * 벡터 연산의 마법\n      * 유명한 예시: \n         * $\\vec{\\text{king}} - \\vec{\\text{man}} + \\vec{\\text{woman}} \\approx \\vec{\\text{queen}}$\n         * $\\vec{\\text{king}} - \\vec{\\text{man}}$: \"남성성\"을 제거 → \"왕권\" 개념만 남음\n         * $+ \\vec{\\text{woman}}$: \"여성성\" 추가\n         * 결과: \"여성 + 왕권\" → \"여왕\"\n      * 수학적 설명:\n         * 각 벡터를 의미 성분들의 조합으로 생각:\n         ```\n         king = [왕권: 0.9, 남성: 0.8, 권력: 0.7, ...]\n         man = [남성: 0.9, 성인: 0.6, ...]  \n         woman = [여성: 0.9, 성인: 0.6, ...]\n         ```\n         * 연산 후:\n         ```\n         king - man + woman ≈ [왕권: 0.9, 여성: 0.9, 권력: 0.7, ...]\n         ```\n         → \"queen\"과 가장 유사!\n* 실제 학습 예시\n   * 초기 상태 (랜덤)\n      \n      ```\n      \"사과\" = [0.1, -0.3, 0.7, ...]  (랜덤)\n      \"바나나\" = [-0.8, 0.2, -0.1, ...] (랜덤)\n      ```\n      \n      * 서로 전혀 관련 없어 보임\n   * 학습 진행\n      \n      ```\n      문장들을 계속 보면서:\n      \"사과를 먹었다\", \"바나나를 먹었다\", \"딸기를 먹었다\"\n      ...\n\n      * 점차 비슷한 벡터로 수렴:\n      \n      ```\n      \"사과\" = [0.2, -0.4, 0.6, ...]\n      \"바나나\" = [0.3, -0.5, 0.7, ...]  \n      \"딸기\" = [0.25, -0.45, 0.65, ...]\n      ```\n\n   * 학습 완료 후\n      * 의미가 비슷한 단어들 → 벡터 공간에서 가까운 위치\n      * 반대 의미 단어들 → 먼 위치 또는 반대 방향\n      * 유추 관계 → 벡터 연산으로 표현 가능\n\n* **핵심**: 신경망이 \"문맥\"이라는 단서를 통해 **단어의 의미**를 수치로 학습\n\n#### Embedding Layer 구조 분석\n\n* Embedding Layer란?\n   * 정수 인코딩 → 밀집 벡터 변환기\n\n```python\n# 이런 변환을 해주는 것\n15 → [0.2, -0.4, 0.7, 0.1, -0.3]  # 300차원 벡터\n23 → [0.1, 0.3, -0.2, 0.8, 0.5]   # 300차원 벡터  \n7  → [-0.1, 0.6, 0.4, -0.2, 0.9]  # 300차원 벡터\n```\n\n* 왜 정수 인코딩이 필요한가?\n   * 문제: 컴퓨터는 \"사과\"라는 글자를 직접 처리할 수 없음\n   * 해결 과정:\n   \n```python\n# 1단계: 단어 → 정수 (정수 인코딩)\n\"사과\" → 15\n\"바나나\" → 23  \n\"딸기\" → 7\n\n# 2단계: 정수 → 벡터 (Embedding Layer)\n15 → [0.2, -0.4, 0.7, ...]\n23 → [0.1, 0.3, -0.2, ...]\n7  → [-0.1, 0.6, 0.4, ...]\n```\n\n* Look-up Table의 구체적 동작\n   * Embedding Matrix 구조: $\\mathbf{E} \\in \\mathbb{R}^{V \\times d}$\n   * 실제 예시:\n\n   ```python\n   # V = 5 (어휘 크기), d = 3 (임베딩 차원)\n   embedding_matrix = [\n      [0.1, 0.2, 0.3],    # 단어 ID 0의 벡터\n      [0.4, 0.5, 0.6],    # 단어 ID 1의 벡터  \n      [0.7, 0.8, 0.9],    # 단어 ID 2의 벡터\n      [0.2, -0.1, 0.4],   # 단어 ID 3의 벡터\n      [0.5, 0.3, -0.2],   # 단어 ID 4의 벡터\n   ]\n   ```\n\n* 행렬의 의미:\n   * 행(row): 각 단어의 임베딩 벡터\n   * 열(column): 임베딩 벡터의 각 차원\n   * 전체: 모든 단어의 벡터를 저장하는 \"사전\"\n\n* Look-up 연산 과정\n\n   * 입력: `input_ids = [2, 0, 4]`\n\n   ```python\n   # 1단계: 각 ID에 해당하는 행을 추출\n   embedding_matrix[2] → [0.7, 0.8, 0.9]    # ID 2의 벡터\n   embedding_matrix[0] → [0.1, 0.2, 0.3]    # ID 0의 벡터  \n   embedding_matrix[4] → [0.5, 0.3, -0.2]   # ID 4의 벡터\n\n   # 2단계: 결과 (3개 벡터)\n   output = [\n      [0.7, 0.8, 0.9],     # 첫 번째 단어\n      [0.1, 0.2, 0.3],     # 두 번째 단어\n      [0.5, 0.3, -0.2]     # 세 번째 단어  \n   ]\n   ```\n\n* 수학적 의미\n\n   * $\\text{embedding}(i) = \\mathbf{E}[i, :]$\n      * $i$: 단어의 정수 ID\n      * $\\mathbf{E}[i, :]$: 행렬 E의 i번째 행 전체\n      * 결과: i번째 단어의 임베딩 벡터\n   * 구체적 예시:\n\n   ```python\n   i = 2  # \"딸기\"의 ID라고 가정\n   embedding(2) = E[2, :] = [0.7, 0.8, 0.9]  # 2번째 행\n   ```\n\n* 실제 PyTorch 코드\n\n```python\nimport torch\nimport torch.nn as nn\n\n# 1. Embedding Layer 생성\nvocab_size = 1000      # 어휘 크기\nembedding_dim = 300    # 벡터 차원\nembedding = nn.Embedding(vocab_size, embedding_dim)\n\n# 2. 내부 구조 확인\nprint(embedding.weight.shape)  # torch.Size([1000, 300])\n# → 1000×300 크기의 look-up table\n\n# 3. 입력 데이터\ninput_ids = torch.tensor([15, 23, 7])  # 3개 단어의 ID\n\n# 4. 임베딩 변환\noutput = embedding(input_ids)\nprint(output.shape)  # torch.Size([3, 300])\n# → 3개 단어 × 300차원 벡터\n```\n\n* Look-up Table이 학습되는 과정\n   * 초기화 (랜덤)\n\n   ```python\n   # 처음에는 랜덤 값들\n   embedding_matrix = torch.randn(vocab_size, embedding_dim)\n   ```\n\n* 학습 과정\n\n```python\n# 예: \"사과는 맛있다\"라는 문장 학습\ninput_ids = [15, 23, 7]  # [사과는, 맛있다, <END>]\n\n# 1. 현재 임베딩으로 예측\nembeddings = embedding_matrix[input_ids]  # Look-up\nprediction = model(embeddings)\n\n# 2. 손실 계산\nloss = criterion(prediction, target)\n\n# 3. 역전파로 embedding_matrix 업데이트  \nloss.backward()  # embedding_matrix의 gradient 계산\noptimizer.step()  # embedding_matrix 값들 업데이트\n```\n\n* 핵심: 학습이 진행되면서 embedding_matrix의 각 행(단어 벡터)이 점점 더 의미 있는 값으로 변함!\n\n* 왜 \"Look-up Table\"이라고 부르는가?\n   * 전통적인 사전과 비교\n\n   ```python\n   # 일반 사전\n   사전 = {\n      \"사과\": \"빨간 과일\",\n      \"바나나\": \"노란 과일\", \n      \"컴퓨터\": \"전자 기기\"\n   }\n   의미 = 사전[\"사과\"]  # \"빨간 과일\"\n\n   # Embedding Table  \n   임베딩_테이블 = {\n      15: [0.2, -0.4, 0.7, ...],   # \"사과\"\n      23: [0.1, 0.3, -0.2, ...],   # \"바나나\"\n      78: [-0.5, 0.8, 0.1, ...]    # \"컴퓨터\"\n   }\n   벡터 = 임베딩_테이블[15]  # [0.2, -0.4, 0.7, ...]\n   ```\n\n   * 차이점:\n      * 일반 사전: 단어 → 설명 (텍스트)\n      * 임베딩 테이블: 단어 ID → 숫자 벡터\n\n* 전체 과정 정리\n\n```python\n# 전체 파이프라인\n\"사과는 맛있다\" \n→ [\"사과는\", \"맛있다\"]           # 토큰화\n→ [15, 23]                      # 정수 인코딩  \n→ [[0.2, -0.4, 0.7],           # Embedding Layer (Look-up)\n   [0.1, 0.3, -0.2]]\n→ 신경망 처리                    # 후속 레이어들\n```\n\n* 핵심\n   * Embedding Layer는 단순히 \"정수 ID를 인덱스로 사용해서 미리 저장된 벡터를 가져오는\" 매우 단순한 연산. \n   * 하지만 이 벡터들이 학습을 통해 의미 있는 값으로 변하기 때문에 강력한 도구가 되는 것\n\n# 결론\n\n본 문서에서는 자연어 처리(NLP) 분야에서 텍스트 데이터의 의미를 효과적으로 포착하기 위해 통계 기반 방법의 한계를 넘어, 신경망은 단어와 문맥의 복잡한 관계를 학습하여 풍부한 정보를 담은 벡터 표현을 생성한다.\n\n*   **워드 임베딩의 발전**:\n    *   **정적 임베딩 (Word2Vec, GloVe, FastText)**: '분포 가설'에 기반하여 단어를 저차원 밀집 벡터로 표현함으로써 단어 간 의미적 유사성과 관계(예: 유추)를 포착했다. `Embedding Layer`는 이러한 변환의 핵심이며, FastText는 하위 단어(subword) 정보를 활용하여 OOV 문제와 형태론적 특징 처리에 강점을 보였다.\n    *   이러한 초기 신경망 기반 방법들은 단어의 의미를 고정된 벡터로 표현하여 NLP 성능을 크게 향상시켰다.\n*   **벡터화 방법 선택의 중요성**:\n    *   단순한 단어 유사도 측정부터 복잡한 문서 이해 및 생성에 이르기까지, 해결하고자 하는 문제의 특성, 데이터의 규모와 성격, 그리고 사용하려는 모델의 요구사항을 종합적으로 고려하여 적절한 벡터화 전략을 선택하는 것이 중요하다.\n    *   이러한 신경망 기반 벡터화 기법들은 현대 대규모 언어 모델(LLM) 발전의 핵심적인 토대가 되었으며, 자연어 이해 및 생성 능력의 비약적인 발전을 이끌고 있다."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"5.nn_static_embedding.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"텍스트 벡터화: 신경망 기반 방법론","subtitle":"워드 임베딩을 이용한 벡터 표현 소개","description":"자연어 처리(NLP)에서 텍스트의 의미와 문맥을 벡터로 표현하는 신경망 기반의 고급 벡터화 방법들을 심층적으로 탐구한다. 정적 워드 임베딩의 원리, 특징, 활용 방안을 다룬다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-05","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}