{"title":"텍스트 벡터화: RNN의 이해","markdown":{"yaml":{"title":"텍스트 벡터화: RNN의 이해","subtitle":"순차적 데이터와 문맥 처리를 위한 순환 신경망","description":"자연어 처리(NLP)에서 순차적인 텍스트 데이터를 처리하기 위한 RNN(Recurrent Neural Network)의 기본 원리와 구조를 소개한다. RNN이 어떻게 이전 시점의 정보를 현재 시점의 입력과 함께 활용하여 문맥을 파악하는지 살펴본다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-10","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false},"headingText":"요약","containsRefs":false,"markdown":"\n\n\n이 문서는 순차적인 데이터, 특히 텍스트와 같은 시퀀스 정보를 처리하기 위해 설계된 RNN(Recurrent Neural Network, 순환 신경망)의 기본적인 작동 원리와 구조를 설명한다.\n\n*   **RNN의 핵심 아이디어**:\n    *   기존의 피드포워드 신경망(FFNN)과 달리, RNN은 내부에 순환하는 루프(loop)를 가지고 있어 이전 단계의 정보를 기억하고 현재 단계의 처리에 활용한다.\n    *   각 시점(time step)에서 입력값과 이전 시점의 은닉 상태(hidden state)를 함께 사용하여 현재 시점의 은닉 상태를 갱신한다. 이 은닉 상태가 문맥 정보를 담고 있다고 간주된다.\n*   **기본 구조 및 처리 과정**:\n    *   RNN은 입력 시퀀스의 길이에 따라 네트워크가 펼쳐지며(unrolled), 각 시점마다 동일한 가중치를 공유한다.\n    *   주요 구성 요소로는 입력 벡터($x_t$), 은닉 상태 벡터($h_t$), 출력 벡터($y_t$) 및 이들 간의 변환을 위한 가중치 행렬($W_x, W_h, W_y$)과 편향($b_h, b$)이 있다.\n    *   은닉 상태는 주로 하이퍼볼릭 탄젠트(tanh) 활성화 함수를 통해 계산된다.\n*   **다양한 RNN 구조**:\n    *   입력과 출력의 관계에 따라 One-to-Many (예: 이미지 캡셔닝), Many-to-One (예: 텍스트 분류), Many-to-Many (예: 시퀀스 레이블링) 등 다양한 형태로 설계될 수 있다.\n*   **의의**: RNN은 단어의 순서가 중요한 자연어 처리 분야에서 문맥을 이해하는 모델의 기초를 제공했다. 비록 장기 의존성 문제 등의 한계로 LSTM, GRU와 같은 개선된 모델이나 트랜스포머와 같은 새로운 아키텍처로 발전했지만, 순차 정보 처리의 기본적인 아이디어를 제시했다는 점에서 중요하다.\n\n이 문서를 통해 독자는 RNN이 어떻게 순차적 정보를 처리하고 문맥 정보를 기억하며 다음 예측에 활용하는지에 대한 기본적인 이해를 얻을 수 있다.\n\n# 텍스트 인코딩 및 벡터화\n\n```\n텍스트 벡터화\n├── 1. 전통적 방법 (통계 기반)\n│   ├── BoW\n│   ├── DTM\n│   └── TF-IDF\n│\n├── 2. 신경망 기반 (문맥 독립)\n│   ├── 문맥 독립적 임베딩\n│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)\n│   ├── Word2Vec (CBOW, Skip-gram)\n│   ├── FastText\n│   ├── GloVe\n│   └── 기타 모델: Swivel, LexVec 등\n│\n└── 3. 문맥 기반 임베딩 (Contextual Embedding)\n    ├── RNN 계열\n        ├── LSTM\n        ├── GRU\n        └── ELMo\n```\n\n## 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩\n\n### RNN (Recurrent Neural Network)\n\n* FFNN(Feed Forward Neural Network): 행렬과 벡터 연산으로 이루어진다.\n* RNN: 행렬과 벡터 연산 + 자기 자신의 출력을 다시 입력으로 사용한다.\n   * 연속적인 시퀀스를 처리하기 위한 신경망\n   * 사람은 이전 단어들에 대한 이해를 바탕으로 다음 단어를 이해한다.\n   * 기존의 MLP에 비해서 RNN은 이러한 이슈를 다루며, 내부에 정보를 지속하는 루프로 구성된 신경망\n   * 단순한 행렬과 벡터 연산을 넘어, **이전 시점의 은닉 상태(hidden state)를 현재 시점의 입력으로 다시 활용**하는 순환 구조\n   * 이러한 \"기억\" 메커니즘 덕분에 RNN은 시간의 흐름에 따른 연속적인 데이터(시퀀스 데이터) 처리에 매우 효과적\n   * **핵심 원리**: 신경망 내부에 루프(loop)를 만들어 정보가 지속되도록 함으로써, 마치 사람이 이전 대화 내용을 기억하며 다음 문장을 이해하는 것과 유사한 방식으로 작동\n   * RNN은 입력의 길이만큼 신경망이 펼쳐진다. (unrolled)\n   * 이때, 입력 받는 각 순간을 시점(time step)이라고 한다.\n   * 시점 $t$ 에서 입력 $x_t$ 와 이전 시점의 은닉 상태 $h_{t-1}$ 을 받아 현재 시점의 은닉 상태 $h_t$ 를 계산\n   * 매시점마다 새로운 입력값을 받고 은닉층에서 이전 시점의 정보를 다음 시점의 은닉층에 전달하는데 이것을 시간순대로 쭉 나열하여 도식화하면 그림이 너무 길어져 은닉층을 하나의 loop형태로 표현한다.\n   * RNN은 FFNN (or MLP)에 시점을 도입한 개념과 같다.\n   * RNN의 입력과 출력은 모두 기본적으로 벡터 단위를 가정한다. 따라서, 일반 RNN다이어 그램에선 입력층, 은닉층과 출력층이 소문자로 되어 있지만 모두 벡터라고 생각해야한다.\n   * NLP에서 각 시점(time step)은 주로 단어 하나 (단어 벡터값) 또는 형태소 (한국어) 하나가 (형태소 벡터)가 된다.\n\n#### RNN의 설계\n\n* RNN의 구조는 설계하기 나름이지만 다음과 같은 유형을 갖는다.\n* One to Many\n   * Image Captioning\n   * 이미지를 첫 시점에서 입력받아 각 시점에서 출력\n* Many to One\n   * 단어를 각 시점에서 입력받아 맨 마지막 시점의 은닉 상태를 출력\n   * Text Classification\n      * 단어들을 입력 받아 이것이 스펨메일인지 아닌지 맨 마지막 시점에서 분류\n* Many to Many\n   * 각 시점에서 입력받은 단어를 각 시점에서 출력\n   * sequence labeling: 각 단어에 대해 특정 레이블을 할당하는 작업으로, 주로 품사 태깅이나 개체명 인식과 같은 태스크에 사용된다.\n      * 개체명 인식(Named Entity Recognition, NER): 문장에서 인물, 장소, 조직 등과 같은 고유명사를 식별하고 분류하는 작업\n\n#### Basic Architecture of RNN\n\n* Cell: 은닉층에 있는 RNN의 처리 단위 도식상에서 부르는 명칭, 보통 cell 이나 hidden state 구분없이 부르기도 한다.\n* Hidden State: Cell의 출력, RNN에서 부르는 명칭\n* RNN은 시점(time step)마다 입력을 받는데 현재 시점의 hidden state인 $h_t$ 연산을 위해 직전 시점의 hidden state인 $h_{t-1}$ 을 입력받는다.\n* 이게 RNN이 과거의 정보를 기억하는 원리이다.\n* 이러한 구조 덕분에 RNN은 시퀀스 데이터를 처리하는 데 매우 효과적이다.\n* 문장 내 각 단어는 시점(time step)이 되며, 각 단어는 벡터 형태로 입력된다.\n* 각 시점에서 입력된 벡터와 이전 시점의 hidden state를 받아 현재 시점의 hidden state를 계산한다.\n* 이렇게 계산된 hidden state는 다음 시점의 입력을 받을 때 사용된다.\n* 이 과정을 모든 시점에 반복하여 수행하면 문장 전체에 대한 정보를 효과적으로 표현할 수 있다.\n* 입력층: $x_t$\n* 은닉층 (cell): $h_t = \\tanh(W_{h}h_{t-1} + W_{x}x_t + b_h)$ \n   * ex) $h_t = \\tanh(W_{h}h_{t-1} + W_{x}x_t + b_h)$\n* 출력층 (output): $y_t = activation(W_{y}h_t + b)$ \n   * ex) $y_t = \\text{softmax}(W_{y}h_t + b)$\n* 도식화\n```{dot}\n   digraph RNN_Cell {\n      rankdir=TB;\n      node [shape=box, style=filled];\n      \n      // 맨 위: y_t\n      y_t [label=\"y_t\", fillcolor=lightgray];\n      \n      // 중간: 빈 녹색 사각형(왼쪽)과 Cell(가운데)\n      h_prev [label=\"\", fillcolor=lightgreen];  // 빈 녹색 사각형\n      cell [label=\"Cell\", fillcolor=lightgreen, shape=box];\n      \n      // 맨 아래: x_t\n      x_t [label=\"x_t\", fillcolor=lightgray];\n      \n      // 연결선들과 라벨들\n      h_prev -> cell [label=\"W_h\", color=red, fontcolor=red, fontsize=10];\n      x_t -> cell [label=\"W_x\", color=red, fontcolor=red, fontsize=10];\n      cell -> y_t [label=\"W_y\", color=red, fontcolor=red, fontsize=10];\n      cell -> h_prev [label=\"h_{t-1}\", color=red, fontcolor=red, fontsize=10, dir=back];\n      \n      // 레이아웃 조정 - 수직 배치\n      {rank=source; y_t;}  // 맨 위\n      {rank=same; h_prev; cell;}  // 중간, 같은 레벨\n      {rank=sink; x_t;}  // 맨 아래\n      \n      // 왼쪽-오른쪽 순서 조정\n      h_prev -> cell [weight=10];\n   }\n```\n\n![RNN matrix](../../../../../images/rnn/rnn_matrix.PNG)\n\n* d: t time step의 단어 벡터의 차원\n* $D_h$: hidden state의 차원 (RNN의 주요 파라미터)\n* $W_h$: hidden state에 대한 가중치, 역전파로 최적화되는 파라미터\n* $W_x$: 입력에 대한 가중치, 역전파로 최적화되는 파라미터\n* $b_h$: hidden state에 대한 편향, 역전파로 최적화되는 파라미터\n* $W_y$: 출력에 대한 가중치, 역전파로 최적화되는 파라미터\n* $b$: 출력에 대한 편향\n\n* tanh: hyperbolic tangent, RNN에서 주로 사용되는 활성화 함수\n   * sigmoid함수와 달리 -1~1 사이의 값을 가지며, 이는 모델의 출력이 sigmoid 함수(0~1)보다 더 넓은 범위의 값을 가지게 됨을 의미한다.\n   * tanh 함수는 출력 범위가 -1에서 1로 넓어, 시그모이드 함수의 0에서 1 범위보다 기울기 소실 문제를 줄여준다. 이는 학습 시 더 안정적이고 빠른 수렴을 가능하게 하여 은닉층에서 연산적으로 유리하다.\n   * 따라서, tanh 함수는 시그모이드 함수보다 더 안정적이고 빠른 수렴을 가능하게 하여 은닉층에서 연산적으로 유리하다.\n\n## 결론\n\n본 문서에서는 순차적인 데이터를 효과적으로 처리하기 위해 고안된 RNN(Recurrent Neural Network)의 기본적인 구조와 작동 원리를 살펴보았다. RNN은 이전 시점의 처리 결과를 현재 시점의 입력과 함께 활용하는 순환 구조를 통해 시간의 흐름에 따른 정보의 연속성을 모델링한다.\n\n*   **RNN의 핵심 원리 요약**:\n    *   RNN은 각 시점(time step)에서 입력 벡터와 이전 시점의 은닉 상태(hidden state)를 입력으로 받아 현재 시점의 은닉 상태를 계산한다. 이 은닉 상태는 과거의 정보를 요약하고 있으며, 다음 시점으로 전달되어 문맥 정보를 누적한다.\n    *   이러한 순환적인 정보 전달 메커니즘은 단어의 순서가 중요한 텍스트 데이터나 시계열 데이터 분석에 적합하다.\n*   **구조적 특징과 다양성**:\n    *   하나의 셀(cell)이 반복적으로 사용되며, 입력과 출력의 관계에 따라 다양한 형태(One-to-Many, Many-to-One, Many-to-Many)로 구성될 수 있어 여러 종류의 시퀀스 처리 문제에 적용될 수 있다.\n    *   활성화 함수로는 주로 하이퍼볼릭 탄젠트(tanh)가 사용되어 은닉 상태 값의 범위를 조절한다.\n*   **문맥 이해의 초기 단계와 한계점**:\n    *   RNN은 문맥을 고려한 동적 임베딩의 초기 아이디어를 제공하며, 특히 단어의 순서 정보를 자연스럽게 처리할 수 있는 능력을 보여주었다.\n    *   그러나 기본적인 RNN 구조는 시퀀스가 길어질수록 앞부분의 정보가 소실되는 장기 의존성 문제(vanishing/exploding gradient)에 취약하다는 단점이 있다. 이러한 한계를 극복하기 위해 LSTM, GRU와 같은 개선된 RNN 셀 구조가 등장했으며, 더 나아가 어텐션 메커니즘과 트랜스포머 아키텍처로 발전하는 계기가 되었다.\n\n결론적으로, RNN은 순차적 데이터 처리와 문맥 정보 활용의 기본적인 패러다임을 제시한 중요한 신경망 모델이다. 비록 자체적인 한계로 인해 현재는 더 발전된 모델들이 주로 사용되지만, 그 핵심 아이디어는 여전히 많은 시퀀스 모델링 기법의 근간을 이루고 있다.\n","srcMarkdownNoYaml":"\n\n# 요약\n\n이 문서는 순차적인 데이터, 특히 텍스트와 같은 시퀀스 정보를 처리하기 위해 설계된 RNN(Recurrent Neural Network, 순환 신경망)의 기본적인 작동 원리와 구조를 설명한다.\n\n*   **RNN의 핵심 아이디어**:\n    *   기존의 피드포워드 신경망(FFNN)과 달리, RNN은 내부에 순환하는 루프(loop)를 가지고 있어 이전 단계의 정보를 기억하고 현재 단계의 처리에 활용한다.\n    *   각 시점(time step)에서 입력값과 이전 시점의 은닉 상태(hidden state)를 함께 사용하여 현재 시점의 은닉 상태를 갱신한다. 이 은닉 상태가 문맥 정보를 담고 있다고 간주된다.\n*   **기본 구조 및 처리 과정**:\n    *   RNN은 입력 시퀀스의 길이에 따라 네트워크가 펼쳐지며(unrolled), 각 시점마다 동일한 가중치를 공유한다.\n    *   주요 구성 요소로는 입력 벡터($x_t$), 은닉 상태 벡터($h_t$), 출력 벡터($y_t$) 및 이들 간의 변환을 위한 가중치 행렬($W_x, W_h, W_y$)과 편향($b_h, b$)이 있다.\n    *   은닉 상태는 주로 하이퍼볼릭 탄젠트(tanh) 활성화 함수를 통해 계산된다.\n*   **다양한 RNN 구조**:\n    *   입력과 출력의 관계에 따라 One-to-Many (예: 이미지 캡셔닝), Many-to-One (예: 텍스트 분류), Many-to-Many (예: 시퀀스 레이블링) 등 다양한 형태로 설계될 수 있다.\n*   **의의**: RNN은 단어의 순서가 중요한 자연어 처리 분야에서 문맥을 이해하는 모델의 기초를 제공했다. 비록 장기 의존성 문제 등의 한계로 LSTM, GRU와 같은 개선된 모델이나 트랜스포머와 같은 새로운 아키텍처로 발전했지만, 순차 정보 처리의 기본적인 아이디어를 제시했다는 점에서 중요하다.\n\n이 문서를 통해 독자는 RNN이 어떻게 순차적 정보를 처리하고 문맥 정보를 기억하며 다음 예측에 활용하는지에 대한 기본적인 이해를 얻을 수 있다.\n\n# 텍스트 인코딩 및 벡터화\n\n```\n텍스트 벡터화\n├── 1. 전통적 방법 (통계 기반)\n│   ├── BoW\n│   ├── DTM\n│   └── TF-IDF\n│\n├── 2. 신경망 기반 (문맥 독립)\n│   ├── 문맥 독립적 임베딩\n│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)\n│   ├── Word2Vec (CBOW, Skip-gram)\n│   ├── FastText\n│   ├── GloVe\n│   └── 기타 모델: Swivel, LexVec 등\n│\n└── 3. 문맥 기반 임베딩 (Contextual Embedding)\n    ├── RNN 계열\n        ├── LSTM\n        ├── GRU\n        └── ELMo\n```\n\n## 문맥을 고려한 벡터화 (2018-현재): 동적 임베딩\n\n### RNN (Recurrent Neural Network)\n\n* FFNN(Feed Forward Neural Network): 행렬과 벡터 연산으로 이루어진다.\n* RNN: 행렬과 벡터 연산 + 자기 자신의 출력을 다시 입력으로 사용한다.\n   * 연속적인 시퀀스를 처리하기 위한 신경망\n   * 사람은 이전 단어들에 대한 이해를 바탕으로 다음 단어를 이해한다.\n   * 기존의 MLP에 비해서 RNN은 이러한 이슈를 다루며, 내부에 정보를 지속하는 루프로 구성된 신경망\n   * 단순한 행렬과 벡터 연산을 넘어, **이전 시점의 은닉 상태(hidden state)를 현재 시점의 입력으로 다시 활용**하는 순환 구조\n   * 이러한 \"기억\" 메커니즘 덕분에 RNN은 시간의 흐름에 따른 연속적인 데이터(시퀀스 데이터) 처리에 매우 효과적\n   * **핵심 원리**: 신경망 내부에 루프(loop)를 만들어 정보가 지속되도록 함으로써, 마치 사람이 이전 대화 내용을 기억하며 다음 문장을 이해하는 것과 유사한 방식으로 작동\n   * RNN은 입력의 길이만큼 신경망이 펼쳐진다. (unrolled)\n   * 이때, 입력 받는 각 순간을 시점(time step)이라고 한다.\n   * 시점 $t$ 에서 입력 $x_t$ 와 이전 시점의 은닉 상태 $h_{t-1}$ 을 받아 현재 시점의 은닉 상태 $h_t$ 를 계산\n   * 매시점마다 새로운 입력값을 받고 은닉층에서 이전 시점의 정보를 다음 시점의 은닉층에 전달하는데 이것을 시간순대로 쭉 나열하여 도식화하면 그림이 너무 길어져 은닉층을 하나의 loop형태로 표현한다.\n   * RNN은 FFNN (or MLP)에 시점을 도입한 개념과 같다.\n   * RNN의 입력과 출력은 모두 기본적으로 벡터 단위를 가정한다. 따라서, 일반 RNN다이어 그램에선 입력층, 은닉층과 출력층이 소문자로 되어 있지만 모두 벡터라고 생각해야한다.\n   * NLP에서 각 시점(time step)은 주로 단어 하나 (단어 벡터값) 또는 형태소 (한국어) 하나가 (형태소 벡터)가 된다.\n\n#### RNN의 설계\n\n* RNN의 구조는 설계하기 나름이지만 다음과 같은 유형을 갖는다.\n* One to Many\n   * Image Captioning\n   * 이미지를 첫 시점에서 입력받아 각 시점에서 출력\n* Many to One\n   * 단어를 각 시점에서 입력받아 맨 마지막 시점의 은닉 상태를 출력\n   * Text Classification\n      * 단어들을 입력 받아 이것이 스펨메일인지 아닌지 맨 마지막 시점에서 분류\n* Many to Many\n   * 각 시점에서 입력받은 단어를 각 시점에서 출력\n   * sequence labeling: 각 단어에 대해 특정 레이블을 할당하는 작업으로, 주로 품사 태깅이나 개체명 인식과 같은 태스크에 사용된다.\n      * 개체명 인식(Named Entity Recognition, NER): 문장에서 인물, 장소, 조직 등과 같은 고유명사를 식별하고 분류하는 작업\n\n#### Basic Architecture of RNN\n\n* Cell: 은닉층에 있는 RNN의 처리 단위 도식상에서 부르는 명칭, 보통 cell 이나 hidden state 구분없이 부르기도 한다.\n* Hidden State: Cell의 출력, RNN에서 부르는 명칭\n* RNN은 시점(time step)마다 입력을 받는데 현재 시점의 hidden state인 $h_t$ 연산을 위해 직전 시점의 hidden state인 $h_{t-1}$ 을 입력받는다.\n* 이게 RNN이 과거의 정보를 기억하는 원리이다.\n* 이러한 구조 덕분에 RNN은 시퀀스 데이터를 처리하는 데 매우 효과적이다.\n* 문장 내 각 단어는 시점(time step)이 되며, 각 단어는 벡터 형태로 입력된다.\n* 각 시점에서 입력된 벡터와 이전 시점의 hidden state를 받아 현재 시점의 hidden state를 계산한다.\n* 이렇게 계산된 hidden state는 다음 시점의 입력을 받을 때 사용된다.\n* 이 과정을 모든 시점에 반복하여 수행하면 문장 전체에 대한 정보를 효과적으로 표현할 수 있다.\n* 입력층: $x_t$\n* 은닉층 (cell): $h_t = \\tanh(W_{h}h_{t-1} + W_{x}x_t + b_h)$ \n   * ex) $h_t = \\tanh(W_{h}h_{t-1} + W_{x}x_t + b_h)$\n* 출력층 (output): $y_t = activation(W_{y}h_t + b)$ \n   * ex) $y_t = \\text{softmax}(W_{y}h_t + b)$\n* 도식화\n```{dot}\n   digraph RNN_Cell {\n      rankdir=TB;\n      node [shape=box, style=filled];\n      \n      // 맨 위: y_t\n      y_t [label=\"y_t\", fillcolor=lightgray];\n      \n      // 중간: 빈 녹색 사각형(왼쪽)과 Cell(가운데)\n      h_prev [label=\"\", fillcolor=lightgreen];  // 빈 녹색 사각형\n      cell [label=\"Cell\", fillcolor=lightgreen, shape=box];\n      \n      // 맨 아래: x_t\n      x_t [label=\"x_t\", fillcolor=lightgray];\n      \n      // 연결선들과 라벨들\n      h_prev -> cell [label=\"W_h\", color=red, fontcolor=red, fontsize=10];\n      x_t -> cell [label=\"W_x\", color=red, fontcolor=red, fontsize=10];\n      cell -> y_t [label=\"W_y\", color=red, fontcolor=red, fontsize=10];\n      cell -> h_prev [label=\"h_{t-1}\", color=red, fontcolor=red, fontsize=10, dir=back];\n      \n      // 레이아웃 조정 - 수직 배치\n      {rank=source; y_t;}  // 맨 위\n      {rank=same; h_prev; cell;}  // 중간, 같은 레벨\n      {rank=sink; x_t;}  // 맨 아래\n      \n      // 왼쪽-오른쪽 순서 조정\n      h_prev -> cell [weight=10];\n   }\n```\n\n![RNN matrix](../../../../../images/rnn/rnn_matrix.PNG)\n\n* d: t time step의 단어 벡터의 차원\n* $D_h$: hidden state의 차원 (RNN의 주요 파라미터)\n* $W_h$: hidden state에 대한 가중치, 역전파로 최적화되는 파라미터\n* $W_x$: 입력에 대한 가중치, 역전파로 최적화되는 파라미터\n* $b_h$: hidden state에 대한 편향, 역전파로 최적화되는 파라미터\n* $W_y$: 출력에 대한 가중치, 역전파로 최적화되는 파라미터\n* $b$: 출력에 대한 편향\n\n* tanh: hyperbolic tangent, RNN에서 주로 사용되는 활성화 함수\n   * sigmoid함수와 달리 -1~1 사이의 값을 가지며, 이는 모델의 출력이 sigmoid 함수(0~1)보다 더 넓은 범위의 값을 가지게 됨을 의미한다.\n   * tanh 함수는 출력 범위가 -1에서 1로 넓어, 시그모이드 함수의 0에서 1 범위보다 기울기 소실 문제를 줄여준다. 이는 학습 시 더 안정적이고 빠른 수렴을 가능하게 하여 은닉층에서 연산적으로 유리하다.\n   * 따라서, tanh 함수는 시그모이드 함수보다 더 안정적이고 빠른 수렴을 가능하게 하여 은닉층에서 연산적으로 유리하다.\n\n## 결론\n\n본 문서에서는 순차적인 데이터를 효과적으로 처리하기 위해 고안된 RNN(Recurrent Neural Network)의 기본적인 구조와 작동 원리를 살펴보았다. RNN은 이전 시점의 처리 결과를 현재 시점의 입력과 함께 활용하는 순환 구조를 통해 시간의 흐름에 따른 정보의 연속성을 모델링한다.\n\n*   **RNN의 핵심 원리 요약**:\n    *   RNN은 각 시점(time step)에서 입력 벡터와 이전 시점의 은닉 상태(hidden state)를 입력으로 받아 현재 시점의 은닉 상태를 계산한다. 이 은닉 상태는 과거의 정보를 요약하고 있으며, 다음 시점으로 전달되어 문맥 정보를 누적한다.\n    *   이러한 순환적인 정보 전달 메커니즘은 단어의 순서가 중요한 텍스트 데이터나 시계열 데이터 분석에 적합하다.\n*   **구조적 특징과 다양성**:\n    *   하나의 셀(cell)이 반복적으로 사용되며, 입력과 출력의 관계에 따라 다양한 형태(One-to-Many, Many-to-One, Many-to-Many)로 구성될 수 있어 여러 종류의 시퀀스 처리 문제에 적용될 수 있다.\n    *   활성화 함수로는 주로 하이퍼볼릭 탄젠트(tanh)가 사용되어 은닉 상태 값의 범위를 조절한다.\n*   **문맥 이해의 초기 단계와 한계점**:\n    *   RNN은 문맥을 고려한 동적 임베딩의 초기 아이디어를 제공하며, 특히 단어의 순서 정보를 자연스럽게 처리할 수 있는 능력을 보여주었다.\n    *   그러나 기본적인 RNN 구조는 시퀀스가 길어질수록 앞부분의 정보가 소실되는 장기 의존성 문제(vanishing/exploding gradient)에 취약하다는 단점이 있다. 이러한 한계를 극복하기 위해 LSTM, GRU와 같은 개선된 RNN 셀 구조가 등장했으며, 더 나아가 어텐션 메커니즘과 트랜스포머 아키텍처로 발전하는 계기가 되었다.\n\n결론적으로, RNN은 순차적 데이터 처리와 문맥 정보 활용의 기본적인 패러다임을 제시한 중요한 신경망 모델이다. 비록 자체적인 한계로 인해 현재는 더 발전된 모델들이 주로 사용되지만, 그 핵심 아이디어는 여전히 많은 시퀀스 모델링 기법의 근간을 이루고 있다.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"10.nn_contextual_RNN.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","appendix-view-license":"라이센스 보기","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어","listing-page-filter":"필터","draft":"초안"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.5.56","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"텍스트 벡터화: RNN의 이해","subtitle":"순차적 데이터와 문맥 처리를 위한 순환 신경망","description":"자연어 처리(NLP)에서 순차적인 텍스트 데이터를 처리하기 위한 RNN(Recurrent Neural Network)의 기본 원리와 구조를 소개한다. RNN이 어떻게 이전 시점의 정보를 현재 시점의 입력과 함께 활용하여 문맥을 파악하는지 살펴본다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-10","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"draft":false,"projectFormats":["html"]}