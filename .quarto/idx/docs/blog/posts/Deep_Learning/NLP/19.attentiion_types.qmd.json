{"title":"Attention 메커니즘의 종류와 발전","markdown":{"yaml":{"title":"Attention 메커니즘의 종류와 발전","subtitle":"Self-Attention에서 Multi-Head Attention까지의 체계적 분류","description":"자연어 처리에서 혁신을 가져온 Attention 메커니즘의 다양한 종류와 발전 과정을 체계적으로 분석한다. Self-Attention, Cross-Attention, Multi-Head Attention 등의 핵심 개념과 작동 원리, 그리고 각각의 특징과 활용 분야를 살펴본다. RNN의 순차적 처리 한계를 극복하고 Transformer 아키텍처의 핵심이 된 Attention 메커니즘이 현대 NLP에 미친 영향을 다룬다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-19","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false},"headingText":"요약","containsRefs":false,"markdown":"\n\n\nAttention 메커니즘은 자연어 처리에서 가장 중요한 혁신 중 하나로, 기존 RNN의 순차적 처리 한계를 극복하고 현대 NLP의 핵심 기술이 되었다. 이 문서는 다양한 Attention 메커니즘의 종류와 특징을 체계적으로 분류하고 분석한다.\n\n주요 내용은 다음과 같다:\n\n* **Attention 메커니즘의 기본 개념**:\n  - 시퀀스의 모든 위치를 동시에 참조하여 중요한 정보에 집중하는 메커니즘\n  - RNN의 순차적 처리와 장기 의존성 문제를 해결하는 핵심 기술\n  - \"어디에 주의를 기울일 것인가\"를 자동으로 학습하는 가중치 메커니즘\n\n* **참조 대상에 따른 분류**:\n  - **Self-Attention**: 같은 시퀀스 내 요소들 간의 관계 모델링\n  - **Cross-Attention**: 서로 다른 시퀀스 간의 대응 관계 학습\n  - 각각의 용도와 활용 분야의 차이점\n\n* **처리 방식에 따른 분류**:\n  - **Single-Head Attention**: 하나의 관점에서 관계 계산\n  - **Multi-Head Attention**: 여러 관점에서 동시에 관계를 포착하여 더 풍부한 표현 학습\n  - 병렬 처리를 통한 계산 효율성과 표현력 향상\n\n* **방향성에 따른 분류**:\n  - **Bidirectional Attention**: 양방향으로 모든 위치 참조 (BERT 방식)\n  - **Causal/Unidirectional Attention**: 과거 정보만 참조 (GPT 방식)\n  - 각각의 장단점과 적용 분야\n\n* **현대 NLP에서의 활용**:\n  - Transformer 아키텍처의 핵심 구성 요소\n  - BERT, GPT 등 사전 학습 모델의 기반 기술\n  - 기계번역, 텍스트 생성, 문서 이해 등 다양한 태스크에 적용\n\nAttention 메커니즘의 이해는 현대 NLP 모델의 작동 원리를 파악하는 데 필수적이며, 향후 더욱 발전된 언어 모델 설계의 기초가 된다.\n\n# 텍스트 인코딩 및 벡터화\n\n```\n텍스트 벡터화\n├── 1. 전통적 방법 (통계 기반)\n│   ├── BoW\n│   ├── DTM\n│   └── TF-IDF\n│\n├── 2. 신경망 기반 (문맥 독립)\n│   ├── 문맥 독립적 임베딩\n│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)\n│   ├── Word2Vec (CBOW, Skip-gram)\n│   ├── FastText\n│   ├── GloVe\n│   └── 기타 모델: Swivel, LexVec 등\n│\n└── 3. 문맥 기반 임베딩 (Contextual Embedding)\n    ├── RNN 계열\n    │   ├── LSTM\n    │   ├── GRU\n    │   └── ELMo\n    └── Attention 메커니즘\n        ├── Basic Attention\n        ├── Self-Attention\n        └── Multi-Head Attention\n\nRNN Language Model\n├── Seq2Seq\n├── Beam Search\n├── Subword Tokenization\n├── Attention\n├── Transformer Encoder (Vaswani et al., 2017)\n|   ├── Positional Encoding\n|   ├── Multi-Head Attention\n|   └── Feed Forward Neural Network\n|\n├── Transformer Decoder (Vaswani et al., 2017)\n|\n├── GPT 시리즈 (OpenAI,2018~)\n|   ├── GPT-1~4\n|   └── ChatGPT (OpenAI,2022~)\n|\n├── BERT 시리즈 (Google,2018~)\n|   ├── BERT\n|   ├── RoBERTa\n|   └── ALBERT\n|\n├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)\n└── 기타 발전 모델\n    ├── T5, XLNet, ELECTRA\n    └── PaLM, LaMDA, Gemini, Claude 등\n```\n\n\n# Attention 메커니즘의 등장 배경\n\n## RNN의 한계점\n\n**순차적 처리의 문제**:\n- RNN은 시퀀스를 왼쪽에서 오른쪽으로 순서대로 처리\n- 병렬 처리가 불가능하여 계산 속도가 느림\n- GPU의 병렬 연산 능력을 제대로 활용하지 못함\n\n**장기 의존성 문제**:\n- 문장이 길어질수록 초기 정보가 점점 희석됨\n- \"그 강아지는 공원에서... (중간에 여러 단어) ...즐거워했다\"\n- \"강아지\"와 \"즐거워했다\"의 연결 관계가 약화됨\n- Gradient vanishing 문제로 인한 학습 한계\n\n**고정된 표현의 한계**:\n- 마지막 은닉 상태 하나로 전체 시퀀스 표현\n- 중간 과정의 중요한 정보 손실\n- 문맥에 따른 동적 표현 부족\n\n## Attention의 혁신\n\n**전체 시퀀스 동시 참조**:\n- 모든 위치를 한 번에 보면서 중요한 부분 식별\n- 거리에 상관없이 관련성 높은 정보에 집중\n- 병렬 처리로 계산 속도 대폭 향상\n\n**동적 가중치 계산**:\n- 각 단어를 처리할 때마다 전체 시퀀스에서 관련성 점수 계산\n- 문맥에 따라 같은 단어도 다른 중요도 부여\n- 학습을 통해 최적의 주의 패턴 자동 발견\n\n# Attention 메커니즘의 기본 원리\n\n## 핵심 개념\n\n**\"어디에 주의를 기울일 것인가?\"**\n- 인간이 긴 글을 읽을 때 중요한 부분에 집중하는 것과 유사\n- 전체 정보 중에서 현재 처리 중인 요소와 관련성이 높은 부분 찾기\n- 관련성을 수치로 계산하여 가중 평균으로 정보 통합\n\n## 기본 수식\n\n**Attention 기본 형태**:\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\n**구성 요소**:\n- **Query (Q)**: \"무엇을 찾고 있는가?\" - 현재 처리 중인 요소\n- **Key (K)**: \"각 위치의 특징은 무엇인가?\" - 참조할 수 있는 모든 위치들\n- **Value (V)**: \"실제 정보는 무엇인가?\" - 가져올 실제 정보들\n\n**작동 과정**:\n1. **유사도 계산**: Query와 각 Key 간의 유사도 측정 (QK^T)\n2. **스케일링**: 차원 수의 제곱근으로 나누어 안정화 (√d_k)\n3. **확률화**: Softmax로 가중치를 확률 분포로 변환\n4. **가중 합**: 가중치를 Value에 적용하여 최종 출력 계산\n\n## 직관적 이해\n\n**도서관에서 책 찾기 비유**:\n- Query: \"머신러닝에 대한 정보가 필요해\"\n- Key: 각 책의 제목과 키워드들\n- Value: 각 책의 실제 내용\n- Attention: 제목/키워드 매칭으로 관련도 계산 → 관련 있는 책들을 가중치에 따라 참조\n\n# Attention 메커니즘의 종류\n\n## 1. 참조 대상에 따른 분류\n\n### Self-Attention\n\n**정의**: 같은 시퀀스 내에서 각 요소가 다른 모든 요소들과의 관계를 계산\n\n**특징**:\n- Query, Key, Value가 모두 같은 시퀀스에서 생성\n- 문장 내 단어들 간의 상호작용 모델링\n- 각 단어가 문장의 다른 모든 단어들을 \"참조\"\n\n**구체적 예시**:\n```\n문장: \"그 강아지는 공원에서 뛰어다니며 즐거워했다\"\n\nSelf-Attention 결과:\n- \"강아지\" 처리 시: \"뛰어다니며\"(0.4), \"즐거워했다\"(0.3), \"공원에서\"(0.2), \"그\"(0.1)\n- \"뛰어다니며\" 처리 시: \"강아지\"(0.5), \"공원에서\"(0.3), \"즐거워했다\"(0.2)\n- \"즐거워했다\" 처리 시: \"강아지\"(0.6), \"뛰어다니며\"(0.3), \"공원에서\"(0.1)\n```\n\n**장점**:\n- 장거리 의존성 효과적 포착\n- 병렬 처리 가능\n- 문맥 정보 풍부하게 활용\n\n**활용 분야**:\n- 문서 이해 (BERT)\n- 텍스트 생성 (GPT)\n- 기계번역의 인코더\n\n### Cross-Attention\n\n**정의**: 서로 다른 시퀀스 간의 관계를 계산\n\n**특징**:\n- Query는 한 시퀀스, Key와 Value는 다른 시퀀스에서 생성\n- 두 시퀀스 간의 대응 관계 학습\n- 정보를 한 시퀀스에서 다른 시퀀스로 전달\n\n**구체적 예시**:\n```\n기계번역: \"I love dogs\" → \"나는 개를 좋아한다\"\n\nCross-Attention:\n- \"나는\" 생성 시: \"I\"(0.8), \"love\"(0.1), \"dogs\"(0.1)\n- \"개를\" 생성 시: \"dogs\"(0.7), \"love\"(0.2), \"I\"(0.1)  \n- \"좋아한다\" 생성 시: \"love\"(0.6), \"dogs\"(0.3), \"I\"(0.1)\n```\n\n**활용 분야**:\n- 기계번역 (원문 ↔ 번역문)\n- 이미지 캡셔닝 (이미지 ↔ 텍스트)\n- 질의응답 (질문 ↔ 문서)\n- 요약 (원문 ↔ 요약문)\n\n## 2. 처리 방식에 따른 분류\n\n### Single-Head Attention\n\n**특징**:\n- 하나의 Attention Head만 사용\n- 단일 관점에서 관계 계산\n- 계산량이 적지만 표현력 제한\n\n**한계점**:\n- 복잡한 관계 패턴 포착의 어려움\n- 다양한 유형의 관계를 동시에 모델링 불가\n- 정보의 다면적 측면 반영 부족\n\n### Multi-Head Attention\n\n**정의**: 여러 개의 Attention Head를 병렬로 사용하여 다양한 관점에서 관계 포착\n\n**핵심 아이디어**:\n- 각 Head가 서로 다른 종류의 관계에 특화\n- 여러 관점을 종합하여 더 풍부한 표현 생성\n- 전문가들이 각자 다른 관점에서 분석 후 종합하는 것과 유사\n\n**수식**:\n$$ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O $$\n$$ \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n\n**각 Head의 역할 예시**:\n\n```\n8개 Head를 가진 Multi-Head Attention:\n- Head 1: 문법적 관계 (주어-동사, 형용사-명사)\n- Head 2: 의미적 관계 (유의어, 반의어) \n- Head 3: 위치적 관계 (인접한 단어들)\n- Head 4: 장거리 의존성 (문장 시작과 끝)\n- Head 5: 개체 관계 (사람, 장소, 시간)\n- Head 6: 감정적 관계 (긍정/부정 표현)\n- Head 7: 논리적 관계 (원인-결과, 조건)\n- Head 8: 화제 관계 (주제와 관련 단어들)\n```\n\n**장점**:\n\n- 다양한 종류의 관계 동시 포착\n- 표현력 대폭 향상\n- 각 Head의 특화를 통한 효율적 정보 처리\n- 해석 가능성 향상 (각 Head의 역할 분석 가능)\n\n## 3. 방향성에 따른 분류\n\n### Bidirectional Attention\n\n**정의**: 양방향으로 모든 위치를 참조할 수 있는 Attention\n\n**특징**:\n- 현재 위치에서 과거와 미래 모든 위치 참조 가능\n- 전체 문맥 정보를 활용한 풍부한 표현\n- 이해 태스크에 적합\n\n**활용 예시**:\n```\n문장: \"그 강아지는 공원에서 뛰어다니며 즐거워했다\"\n\n\"공원에서\" 처리 시:\n- 과거 참조: \"그\", \"강아지는\" \n- 미래 참조: \"뛰어다니며\", \"즐거워했다\"\n- 모든 정보를 종합하여 \"공원에서\"의 의미 결정\n```\n\n**대표 모델**: BERT, RoBERTa, ALBERT\n\n### Causal/Unidirectional Attention\n\n**정의**: 현재 위치 이전의 정보만 참조할 수 있는 Attention\n\n**특징**:\n- 미래 정보 차단으로 순차적 생성 모델링\n- 텍스트 생성 시 \"부정행위\" 방지\n- Attention mask를 통한 구현\n\n**마스킹 메커니즘**:\n\n```\nAttention Score Matrix (4×4 예시):\n[[ 0.2, -∞,  -∞,  -∞],     →     [[1.0, 0.0, 0.0, 0.0],\n [ 0.1, 0.3, -∞,  -∞],      →      [0.4, 0.6, 0.0, 0.0],\n [ 0.4, 0.2, 0.1, -∞],      →      [0.5, 0.3, 0.2, 0.0],\n [ 0.3, 0.1, 0.2, 0.5]]     →      [0.2, 0.1, 0.2, 0.5]]\n\n-∞ 부분이 Softmax 후 0이 되어 미래 정보 차단\n```\n\n**대표 모델**: GPT 시리즈, 대부분의 디코더 모델\n\n## 구조적 분류\n\n### Scaled Dot-Product Attention\n\n**정의**: 현재 가장 널리 사용되는 표준 Attention 방식\n\n**특징**:\n- 내적(Dot Product)을 통한 유사도 계산\n- 차원으로 스케일링하여 안정성 확보\n- 계산 효율성과 성능의 균형\n\n**장점**:\n- 구현이 간단하고 직관적\n- 행렬 연산 최적화 가능\n- GPU에서 빠른 계산\n- 메모리 효율적\n\n### Additive Attention\n\n**정의**: 초기 seq2seq 모델에서 사용된 Attention 방식\n\n**수식**:\n$$ e_{ij} = v^T \\tanh(W_1 h_i + W_2 s_j) $$\n\n**특징**:\n- Neural Network를 통한 유사도 계산\n- 더 많은 파라미터와 계산량 필요\n- 현재는 잘 사용되지 않음\n\n### Sparse Attention\n\n**정의**: 계산 효율성을 위해 일부 위치만 참조하는 Attention\n\n**필요성**:\n- 긴 시퀀스에서 O(n²) 복잡도 문제\n- 모든 위치를 참조할 필요가 없는 경우\n- 메모리와 계산 자원 절약\n\n**종류**:\n- **Local Attention**: 주변 k개 위치만 참조\n- **Strided Attention**: 일정 간격으로 참조\n- **Random Attention**: 무작위로 선택된 위치 참조\n\n# 현대 NLP에서의 Attention 활용\n\n## Transformer 아키텍처\n\n**Encoder의 Self-Attention**:\n- 입력 시퀀스 내 모든 위치 간 관계 모델링\n- 양방향 정보 처리로 풍부한 표현 생성\n- 병렬 처리로 빠른 인코딩\n\n**Decoder의 Multi-Attention**:\n1. **Masked Self-Attention**: 생성된 부분 내에서의 관계\n2. **Cross-Attention**: 인코더 출력과의 관계\n3. 순차적 생성을 위한 미래 정보 차단\n\n## 사전 학습 모델에서의 활용\n\n### BERT 계열 (Bidirectional)\n\n- **Self-Attention**: 양방향 문맥 이해\n- **Multi-Head**: 다양한 언어학적 관계 포착\n- **Deep Architecture**: 12-24층의 Attention 레이어\n\n### GPT 계열 (Causal)\n\n- **Causal Self-Attention**: 순차적 텍스트 생성\n- **Multi-Head**: 다양한 생성 패턴 학습\n- **Scaling**: 모델 크기 증가에 따른 성능 향상\n\n### T5 (Text-to-Text)\n\n- **Encoder-Decoder**: 양방향 이해 + 순차적 생성\n- **Cross-Attention**: 입력과 출력 간 관계\n- **다양한 태스크**: 하나의 구조로 모든 NLP 태스크\n\n## 특화된 Attention 변형\n\n### Longformer\n\n- **Sliding Window**: 지역적 Attention\n- **Global Attention**: 특정 토큰에 전역 Attention\n- **긴 문서**: 수천 개 토큰 처리 가능\n\n### BigBird\n\n- **Random Attention**: 무작위 위치 참조\n- **Window Attention**: 지역적 참조\n- **Global Attention**: 중요 토큰 전역 참조\n\n### Linformer\n\n- **Linear Complexity**: O(n) 복잡도로 축소\n- **Projection**: Key, Value를 저차원으로 투영\n- **효율성**: 긴 시퀀스에서 메모리 절약\n\n# 결론\n\nAttention 메커니즘은 자연어 처리 분야에서 **패러다임의 전환점**이 된 가장 중요한 기술적 혁신 중 하나다. RNN의 순차적 처리 한계를 극복하고 현대 NLP의 기반을 마련했다.\n\n## Attention 메커니즘의 핵심 기여\n\n* **계산 효율성 혁신**: 순차적 처리에서 병렬 처리로 전환하여 학습 속도를 획기적으로 향상시켰다. GPU의 병렬 연산 능력을 최대한 활용할 수 있게 되었다.\n* **장기 의존성 해결**: 시퀀스 길이에 관계없이 모든 위치 간 직접적 연결을 통해 멀리 떨어진 요소들 간의 관계를 효과적으로 포착한다.\n* **표현력 향상**: Multi-Head Attention을 통해 문법적, 의미적, 위치적 관계 등 다양한 종류의 언어학적 패턴을 동시에 학습할 수 있다.\n* **유연성 제공**: Self-Attention과 Cross-Attention의 조합으로 다양한 NLP 태스크에 적용 가능한 범용적 구조를 제공한다.\n\n## 현대 AI에 미친 영향\n\n* **Transformer 혁명**: Attention 메커니즘을 기반으로 한 Transformer는 자연어 처리의 표준 아키텍처가 되었다. \"Attention is All You Need\"라는 제목처럼 실제로 Attention만으로도 충분히 강력한 언어 모델을 만들 수 있음을 증명했다.\n* **사전 학습 모델의 기반**: BERT, GPT 등 현재 널리 사용되는 모든 대규모 언어 모델의 핵심 구성 요소가 되었다. 이들 모델의 놀라운 성능은 Attention 메커니즘의 강력함을 보여준다.\n* **다중 모달 확장**: 텍스트를 넘어 이미지, 음성, 비디오 등 다양한 모달리티를 처리하는 모델들도 Attention 메커니즘을 활용하고 있다.\n\n## 한계와 도전 과제\n\n* **계산 복잡도**: 시퀀스 길이의 제곱에 비례하는 O(n²) 복잡도로 인해 매우 긴 시퀀스 처리에 한계가 있다. 이를 해결하기 위한 Sparse Attention, Linear Attention 등의 연구가 활발히 진행되고 있다.\n\n* **해석 가능성**: Attention 가중치가 모델의 의사결정을 완전히 설명하지는 못한다는 연구 결과들이 있다. 모델이 실제로 무엇에 \"주의\"를 기울이는지 정확히 파악하기는 여전히 어렵다.\n\n* **편향성 문제**: 학습 데이터의 편향이 Attention 패턴에 반영될 수 있으며, 이는 공정성 문제로 이어질 수 있다.\n\n## 미래 전망\n\n* **효율성 개선**: Sparse Attention, Linear Attention, Flash Attention 등 계산 효율성을 높이는 다양한 변형들이 개발되고 있다. 이를 통해 더 긴 시퀀스를 처리하거나 더 적은 자원으로 같은 성능을 달성할 수 있을 것이다.\n* **특화된 Attention**: 특정 도메인이나 태스크에 최적화된 Attention 메커니즘들이 계속 개발될 것이다. 예를 들어, 그래프 구조나 계층적 구조를 고려한 Attention 등이 있다.\n* **다중 모달 통합**: 텍스트, 이미지, 음성, 비디오 등을 통합적으로 처리하는 다중 모달 Attention 메커니즘이 더욱 발전할 것이다.\n**하드웨어 최적화**: Attention 연산에 특화된 하드웨어나 최적화 기법들이 개발되어 더욱 빠르고 효율적인 처리가 가능해질 것이다.\n\n## 결론적 의미\n\nAttention 메커니즘의 등장은 단순한 기술적 개선을 넘어 **AI가 정보를 처리하는 방식 자체를 바꾸었다**. 순차적이고 제한적인 처리에서 전역적이고 동적인 처리로의 전환은 현재 우리가 경험하고 있는 대화형 AI, 창작 AI, 번역 AI 등 모든 혁신의 기반이 되었다.\n\n특히 \"어디에 주의를 기울일 것인가\"라는 Attention의 핵심 아이디어는 인간의 인지 과정과 매우 유사하다. 이는 AI가 인간의 사고 과정에 한 걸음 더 가까워졌음을 의미하며, 앞으로도 더욱 자연스럽고 효과적인 인간-AI 상호작용의 기초가 될 것이다.\n\nAttention 메커니즘의 이해는 현대 NLP 모델의 작동 원리를 파악하는 데 필수적이며, 향후 AI 발전의 방향을 예측하고 새로운 모델을 설계하는 데 중요한 기초 지식이 된다.","srcMarkdownNoYaml":"\n\n# 요약\n\nAttention 메커니즘은 자연어 처리에서 가장 중요한 혁신 중 하나로, 기존 RNN의 순차적 처리 한계를 극복하고 현대 NLP의 핵심 기술이 되었다. 이 문서는 다양한 Attention 메커니즘의 종류와 특징을 체계적으로 분류하고 분석한다.\n\n주요 내용은 다음과 같다:\n\n* **Attention 메커니즘의 기본 개념**:\n  - 시퀀스의 모든 위치를 동시에 참조하여 중요한 정보에 집중하는 메커니즘\n  - RNN의 순차적 처리와 장기 의존성 문제를 해결하는 핵심 기술\n  - \"어디에 주의를 기울일 것인가\"를 자동으로 학습하는 가중치 메커니즘\n\n* **참조 대상에 따른 분류**:\n  - **Self-Attention**: 같은 시퀀스 내 요소들 간의 관계 모델링\n  - **Cross-Attention**: 서로 다른 시퀀스 간의 대응 관계 학습\n  - 각각의 용도와 활용 분야의 차이점\n\n* **처리 방식에 따른 분류**:\n  - **Single-Head Attention**: 하나의 관점에서 관계 계산\n  - **Multi-Head Attention**: 여러 관점에서 동시에 관계를 포착하여 더 풍부한 표현 학습\n  - 병렬 처리를 통한 계산 효율성과 표현력 향상\n\n* **방향성에 따른 분류**:\n  - **Bidirectional Attention**: 양방향으로 모든 위치 참조 (BERT 방식)\n  - **Causal/Unidirectional Attention**: 과거 정보만 참조 (GPT 방식)\n  - 각각의 장단점과 적용 분야\n\n* **현대 NLP에서의 활용**:\n  - Transformer 아키텍처의 핵심 구성 요소\n  - BERT, GPT 등 사전 학습 모델의 기반 기술\n  - 기계번역, 텍스트 생성, 문서 이해 등 다양한 태스크에 적용\n\nAttention 메커니즘의 이해는 현대 NLP 모델의 작동 원리를 파악하는 데 필수적이며, 향후 더욱 발전된 언어 모델 설계의 기초가 된다.\n\n# 텍스트 인코딩 및 벡터화\n\n```\n텍스트 벡터화\n├── 1. 전통적 방법 (통계 기반)\n│   ├── BoW\n│   ├── DTM\n│   └── TF-IDF\n│\n├── 2. 신경망 기반 (문맥 독립)\n│   ├── 문맥 독립적 임베딩\n│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)\n│   ├── Word2Vec (CBOW, Skip-gram)\n│   ├── FastText\n│   ├── GloVe\n│   └── 기타 모델: Swivel, LexVec 등\n│\n└── 3. 문맥 기반 임베딩 (Contextual Embedding)\n    ├── RNN 계열\n    │   ├── LSTM\n    │   ├── GRU\n    │   └── ELMo\n    └── Attention 메커니즘\n        ├── Basic Attention\n        ├── Self-Attention\n        └── Multi-Head Attention\n\nRNN Language Model\n├── Seq2Seq\n├── Beam Search\n├── Subword Tokenization\n├── Attention\n├── Transformer Encoder (Vaswani et al., 2017)\n|   ├── Positional Encoding\n|   ├── Multi-Head Attention\n|   └── Feed Forward Neural Network\n|\n├── Transformer Decoder (Vaswani et al., 2017)\n|\n├── GPT 시리즈 (OpenAI,2018~)\n|   ├── GPT-1~4\n|   └── ChatGPT (OpenAI,2022~)\n|\n├── BERT 시리즈 (Google,2018~)\n|   ├── BERT\n|   ├── RoBERTa\n|   └── ALBERT\n|\n├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)\n└── 기타 발전 모델\n    ├── T5, XLNet, ELECTRA\n    └── PaLM, LaMDA, Gemini, Claude 등\n```\n\n\n# Attention 메커니즘의 등장 배경\n\n## RNN의 한계점\n\n**순차적 처리의 문제**:\n- RNN은 시퀀스를 왼쪽에서 오른쪽으로 순서대로 처리\n- 병렬 처리가 불가능하여 계산 속도가 느림\n- GPU의 병렬 연산 능력을 제대로 활용하지 못함\n\n**장기 의존성 문제**:\n- 문장이 길어질수록 초기 정보가 점점 희석됨\n- \"그 강아지는 공원에서... (중간에 여러 단어) ...즐거워했다\"\n- \"강아지\"와 \"즐거워했다\"의 연결 관계가 약화됨\n- Gradient vanishing 문제로 인한 학습 한계\n\n**고정된 표현의 한계**:\n- 마지막 은닉 상태 하나로 전체 시퀀스 표현\n- 중간 과정의 중요한 정보 손실\n- 문맥에 따른 동적 표현 부족\n\n## Attention의 혁신\n\n**전체 시퀀스 동시 참조**:\n- 모든 위치를 한 번에 보면서 중요한 부분 식별\n- 거리에 상관없이 관련성 높은 정보에 집중\n- 병렬 처리로 계산 속도 대폭 향상\n\n**동적 가중치 계산**:\n- 각 단어를 처리할 때마다 전체 시퀀스에서 관련성 점수 계산\n- 문맥에 따라 같은 단어도 다른 중요도 부여\n- 학습을 통해 최적의 주의 패턴 자동 발견\n\n# Attention 메커니즘의 기본 원리\n\n## 핵심 개념\n\n**\"어디에 주의를 기울일 것인가?\"**\n- 인간이 긴 글을 읽을 때 중요한 부분에 집중하는 것과 유사\n- 전체 정보 중에서 현재 처리 중인 요소와 관련성이 높은 부분 찾기\n- 관련성을 수치로 계산하여 가중 평균으로 정보 통합\n\n## 기본 수식\n\n**Attention 기본 형태**:\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\n**구성 요소**:\n- **Query (Q)**: \"무엇을 찾고 있는가?\" - 현재 처리 중인 요소\n- **Key (K)**: \"각 위치의 특징은 무엇인가?\" - 참조할 수 있는 모든 위치들\n- **Value (V)**: \"실제 정보는 무엇인가?\" - 가져올 실제 정보들\n\n**작동 과정**:\n1. **유사도 계산**: Query와 각 Key 간의 유사도 측정 (QK^T)\n2. **스케일링**: 차원 수의 제곱근으로 나누어 안정화 (√d_k)\n3. **확률화**: Softmax로 가중치를 확률 분포로 변환\n4. **가중 합**: 가중치를 Value에 적용하여 최종 출력 계산\n\n## 직관적 이해\n\n**도서관에서 책 찾기 비유**:\n- Query: \"머신러닝에 대한 정보가 필요해\"\n- Key: 각 책의 제목과 키워드들\n- Value: 각 책의 실제 내용\n- Attention: 제목/키워드 매칭으로 관련도 계산 → 관련 있는 책들을 가중치에 따라 참조\n\n# Attention 메커니즘의 종류\n\n## 1. 참조 대상에 따른 분류\n\n### Self-Attention\n\n**정의**: 같은 시퀀스 내에서 각 요소가 다른 모든 요소들과의 관계를 계산\n\n**특징**:\n- Query, Key, Value가 모두 같은 시퀀스에서 생성\n- 문장 내 단어들 간의 상호작용 모델링\n- 각 단어가 문장의 다른 모든 단어들을 \"참조\"\n\n**구체적 예시**:\n```\n문장: \"그 강아지는 공원에서 뛰어다니며 즐거워했다\"\n\nSelf-Attention 결과:\n- \"강아지\" 처리 시: \"뛰어다니며\"(0.4), \"즐거워했다\"(0.3), \"공원에서\"(0.2), \"그\"(0.1)\n- \"뛰어다니며\" 처리 시: \"강아지\"(0.5), \"공원에서\"(0.3), \"즐거워했다\"(0.2)\n- \"즐거워했다\" 처리 시: \"강아지\"(0.6), \"뛰어다니며\"(0.3), \"공원에서\"(0.1)\n```\n\n**장점**:\n- 장거리 의존성 효과적 포착\n- 병렬 처리 가능\n- 문맥 정보 풍부하게 활용\n\n**활용 분야**:\n- 문서 이해 (BERT)\n- 텍스트 생성 (GPT)\n- 기계번역의 인코더\n\n### Cross-Attention\n\n**정의**: 서로 다른 시퀀스 간의 관계를 계산\n\n**특징**:\n- Query는 한 시퀀스, Key와 Value는 다른 시퀀스에서 생성\n- 두 시퀀스 간의 대응 관계 학습\n- 정보를 한 시퀀스에서 다른 시퀀스로 전달\n\n**구체적 예시**:\n```\n기계번역: \"I love dogs\" → \"나는 개를 좋아한다\"\n\nCross-Attention:\n- \"나는\" 생성 시: \"I\"(0.8), \"love\"(0.1), \"dogs\"(0.1)\n- \"개를\" 생성 시: \"dogs\"(0.7), \"love\"(0.2), \"I\"(0.1)  \n- \"좋아한다\" 생성 시: \"love\"(0.6), \"dogs\"(0.3), \"I\"(0.1)\n```\n\n**활용 분야**:\n- 기계번역 (원문 ↔ 번역문)\n- 이미지 캡셔닝 (이미지 ↔ 텍스트)\n- 질의응답 (질문 ↔ 문서)\n- 요약 (원문 ↔ 요약문)\n\n## 2. 처리 방식에 따른 분류\n\n### Single-Head Attention\n\n**특징**:\n- 하나의 Attention Head만 사용\n- 단일 관점에서 관계 계산\n- 계산량이 적지만 표현력 제한\n\n**한계점**:\n- 복잡한 관계 패턴 포착의 어려움\n- 다양한 유형의 관계를 동시에 모델링 불가\n- 정보의 다면적 측면 반영 부족\n\n### Multi-Head Attention\n\n**정의**: 여러 개의 Attention Head를 병렬로 사용하여 다양한 관점에서 관계 포착\n\n**핵심 아이디어**:\n- 각 Head가 서로 다른 종류의 관계에 특화\n- 여러 관점을 종합하여 더 풍부한 표현 생성\n- 전문가들이 각자 다른 관점에서 분석 후 종합하는 것과 유사\n\n**수식**:\n$$ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O $$\n$$ \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n\n**각 Head의 역할 예시**:\n\n```\n8개 Head를 가진 Multi-Head Attention:\n- Head 1: 문법적 관계 (주어-동사, 형용사-명사)\n- Head 2: 의미적 관계 (유의어, 반의어) \n- Head 3: 위치적 관계 (인접한 단어들)\n- Head 4: 장거리 의존성 (문장 시작과 끝)\n- Head 5: 개체 관계 (사람, 장소, 시간)\n- Head 6: 감정적 관계 (긍정/부정 표현)\n- Head 7: 논리적 관계 (원인-결과, 조건)\n- Head 8: 화제 관계 (주제와 관련 단어들)\n```\n\n**장점**:\n\n- 다양한 종류의 관계 동시 포착\n- 표현력 대폭 향상\n- 각 Head의 특화를 통한 효율적 정보 처리\n- 해석 가능성 향상 (각 Head의 역할 분석 가능)\n\n## 3. 방향성에 따른 분류\n\n### Bidirectional Attention\n\n**정의**: 양방향으로 모든 위치를 참조할 수 있는 Attention\n\n**특징**:\n- 현재 위치에서 과거와 미래 모든 위치 참조 가능\n- 전체 문맥 정보를 활용한 풍부한 표현\n- 이해 태스크에 적합\n\n**활용 예시**:\n```\n문장: \"그 강아지는 공원에서 뛰어다니며 즐거워했다\"\n\n\"공원에서\" 처리 시:\n- 과거 참조: \"그\", \"강아지는\" \n- 미래 참조: \"뛰어다니며\", \"즐거워했다\"\n- 모든 정보를 종합하여 \"공원에서\"의 의미 결정\n```\n\n**대표 모델**: BERT, RoBERTa, ALBERT\n\n### Causal/Unidirectional Attention\n\n**정의**: 현재 위치 이전의 정보만 참조할 수 있는 Attention\n\n**특징**:\n- 미래 정보 차단으로 순차적 생성 모델링\n- 텍스트 생성 시 \"부정행위\" 방지\n- Attention mask를 통한 구현\n\n**마스킹 메커니즘**:\n\n```\nAttention Score Matrix (4×4 예시):\n[[ 0.2, -∞,  -∞,  -∞],     →     [[1.0, 0.0, 0.0, 0.0],\n [ 0.1, 0.3, -∞,  -∞],      →      [0.4, 0.6, 0.0, 0.0],\n [ 0.4, 0.2, 0.1, -∞],      →      [0.5, 0.3, 0.2, 0.0],\n [ 0.3, 0.1, 0.2, 0.5]]     →      [0.2, 0.1, 0.2, 0.5]]\n\n-∞ 부분이 Softmax 후 0이 되어 미래 정보 차단\n```\n\n**대표 모델**: GPT 시리즈, 대부분의 디코더 모델\n\n## 구조적 분류\n\n### Scaled Dot-Product Attention\n\n**정의**: 현재 가장 널리 사용되는 표준 Attention 방식\n\n**특징**:\n- 내적(Dot Product)을 통한 유사도 계산\n- 차원으로 스케일링하여 안정성 확보\n- 계산 효율성과 성능의 균형\n\n**장점**:\n- 구현이 간단하고 직관적\n- 행렬 연산 최적화 가능\n- GPU에서 빠른 계산\n- 메모리 효율적\n\n### Additive Attention\n\n**정의**: 초기 seq2seq 모델에서 사용된 Attention 방식\n\n**수식**:\n$$ e_{ij} = v^T \\tanh(W_1 h_i + W_2 s_j) $$\n\n**특징**:\n- Neural Network를 통한 유사도 계산\n- 더 많은 파라미터와 계산량 필요\n- 현재는 잘 사용되지 않음\n\n### Sparse Attention\n\n**정의**: 계산 효율성을 위해 일부 위치만 참조하는 Attention\n\n**필요성**:\n- 긴 시퀀스에서 O(n²) 복잡도 문제\n- 모든 위치를 참조할 필요가 없는 경우\n- 메모리와 계산 자원 절약\n\n**종류**:\n- **Local Attention**: 주변 k개 위치만 참조\n- **Strided Attention**: 일정 간격으로 참조\n- **Random Attention**: 무작위로 선택된 위치 참조\n\n# 현대 NLP에서의 Attention 활용\n\n## Transformer 아키텍처\n\n**Encoder의 Self-Attention**:\n- 입력 시퀀스 내 모든 위치 간 관계 모델링\n- 양방향 정보 처리로 풍부한 표현 생성\n- 병렬 처리로 빠른 인코딩\n\n**Decoder의 Multi-Attention**:\n1. **Masked Self-Attention**: 생성된 부분 내에서의 관계\n2. **Cross-Attention**: 인코더 출력과의 관계\n3. 순차적 생성을 위한 미래 정보 차단\n\n## 사전 학습 모델에서의 활용\n\n### BERT 계열 (Bidirectional)\n\n- **Self-Attention**: 양방향 문맥 이해\n- **Multi-Head**: 다양한 언어학적 관계 포착\n- **Deep Architecture**: 12-24층의 Attention 레이어\n\n### GPT 계열 (Causal)\n\n- **Causal Self-Attention**: 순차적 텍스트 생성\n- **Multi-Head**: 다양한 생성 패턴 학습\n- **Scaling**: 모델 크기 증가에 따른 성능 향상\n\n### T5 (Text-to-Text)\n\n- **Encoder-Decoder**: 양방향 이해 + 순차적 생성\n- **Cross-Attention**: 입력과 출력 간 관계\n- **다양한 태스크**: 하나의 구조로 모든 NLP 태스크\n\n## 특화된 Attention 변형\n\n### Longformer\n\n- **Sliding Window**: 지역적 Attention\n- **Global Attention**: 특정 토큰에 전역 Attention\n- **긴 문서**: 수천 개 토큰 처리 가능\n\n### BigBird\n\n- **Random Attention**: 무작위 위치 참조\n- **Window Attention**: 지역적 참조\n- **Global Attention**: 중요 토큰 전역 참조\n\n### Linformer\n\n- **Linear Complexity**: O(n) 복잡도로 축소\n- **Projection**: Key, Value를 저차원으로 투영\n- **효율성**: 긴 시퀀스에서 메모리 절약\n\n# 결론\n\nAttention 메커니즘은 자연어 처리 분야에서 **패러다임의 전환점**이 된 가장 중요한 기술적 혁신 중 하나다. RNN의 순차적 처리 한계를 극복하고 현대 NLP의 기반을 마련했다.\n\n## Attention 메커니즘의 핵심 기여\n\n* **계산 효율성 혁신**: 순차적 처리에서 병렬 처리로 전환하여 학습 속도를 획기적으로 향상시켰다. GPU의 병렬 연산 능력을 최대한 활용할 수 있게 되었다.\n* **장기 의존성 해결**: 시퀀스 길이에 관계없이 모든 위치 간 직접적 연결을 통해 멀리 떨어진 요소들 간의 관계를 효과적으로 포착한다.\n* **표현력 향상**: Multi-Head Attention을 통해 문법적, 의미적, 위치적 관계 등 다양한 종류의 언어학적 패턴을 동시에 학습할 수 있다.\n* **유연성 제공**: Self-Attention과 Cross-Attention의 조합으로 다양한 NLP 태스크에 적용 가능한 범용적 구조를 제공한다.\n\n## 현대 AI에 미친 영향\n\n* **Transformer 혁명**: Attention 메커니즘을 기반으로 한 Transformer는 자연어 처리의 표준 아키텍처가 되었다. \"Attention is All You Need\"라는 제목처럼 실제로 Attention만으로도 충분히 강력한 언어 모델을 만들 수 있음을 증명했다.\n* **사전 학습 모델의 기반**: BERT, GPT 등 현재 널리 사용되는 모든 대규모 언어 모델의 핵심 구성 요소가 되었다. 이들 모델의 놀라운 성능은 Attention 메커니즘의 강력함을 보여준다.\n* **다중 모달 확장**: 텍스트를 넘어 이미지, 음성, 비디오 등 다양한 모달리티를 처리하는 모델들도 Attention 메커니즘을 활용하고 있다.\n\n## 한계와 도전 과제\n\n* **계산 복잡도**: 시퀀스 길이의 제곱에 비례하는 O(n²) 복잡도로 인해 매우 긴 시퀀스 처리에 한계가 있다. 이를 해결하기 위한 Sparse Attention, Linear Attention 등의 연구가 활발히 진행되고 있다.\n\n* **해석 가능성**: Attention 가중치가 모델의 의사결정을 완전히 설명하지는 못한다는 연구 결과들이 있다. 모델이 실제로 무엇에 \"주의\"를 기울이는지 정확히 파악하기는 여전히 어렵다.\n\n* **편향성 문제**: 학습 데이터의 편향이 Attention 패턴에 반영될 수 있으며, 이는 공정성 문제로 이어질 수 있다.\n\n## 미래 전망\n\n* **효율성 개선**: Sparse Attention, Linear Attention, Flash Attention 등 계산 효율성을 높이는 다양한 변형들이 개발되고 있다. 이를 통해 더 긴 시퀀스를 처리하거나 더 적은 자원으로 같은 성능을 달성할 수 있을 것이다.\n* **특화된 Attention**: 특정 도메인이나 태스크에 최적화된 Attention 메커니즘들이 계속 개발될 것이다. 예를 들어, 그래프 구조나 계층적 구조를 고려한 Attention 등이 있다.\n* **다중 모달 통합**: 텍스트, 이미지, 음성, 비디오 등을 통합적으로 처리하는 다중 모달 Attention 메커니즘이 더욱 발전할 것이다.\n**하드웨어 최적화**: Attention 연산에 특화된 하드웨어나 최적화 기법들이 개발되어 더욱 빠르고 효율적인 처리가 가능해질 것이다.\n\n## 결론적 의미\n\nAttention 메커니즘의 등장은 단순한 기술적 개선을 넘어 **AI가 정보를 처리하는 방식 자체를 바꾸었다**. 순차적이고 제한적인 처리에서 전역적이고 동적인 처리로의 전환은 현재 우리가 경험하고 있는 대화형 AI, 창작 AI, 번역 AI 등 모든 혁신의 기반이 되었다.\n\n특히 \"어디에 주의를 기울일 것인가\"라는 Attention의 핵심 아이디어는 인간의 인지 과정과 매우 유사하다. 이는 AI가 인간의 사고 과정에 한 걸음 더 가까워졌음을 의미하며, 앞으로도 더욱 자연스럽고 효과적인 인간-AI 상호작용의 기초가 될 것이다.\n\nAttention 메커니즘의 이해는 현대 NLP 모델의 작동 원리를 파악하는 데 필수적이며, 향후 AI 발전의 방향을 예측하고 새로운 모델을 설계하는 데 중요한 기초 지식이 된다."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"19.attentiion_types.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Attention 메커니즘의 종류와 발전","subtitle":"Self-Attention에서 Multi-Head Attention까지의 체계적 분류","description":"자연어 처리에서 혁신을 가져온 Attention 메커니즘의 다양한 종류와 발전 과정을 체계적으로 분석한다. Self-Attention, Cross-Attention, Multi-Head Attention 등의 핵심 개념과 작동 원리, 그리고 각각의 특징과 활용 분야를 살펴본다. RNN의 순차적 처리 한계를 극복하고 Transformer 아키텍처의 핵심이 된 Attention 메커니즘이 현대 NLP에 미친 영향을 다룬다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-19","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}