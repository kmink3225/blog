{"title":"사전 학습 모델의 발전","markdown":{"yaml":{"title":"사전 학습 모델의 발전","subtitle":"ELMo부터 LLaMA까지: 현대 NLP의 혁신적 변화","description":"ELMo, BERT, GPT, T5, LLaMA 등 주요 사전 학습 모델들의 발전 과정과 핵심 원리를 다룬다. 문맥 기반 임베딩부터 대규모 언어 모델까지, 각 모델의 혁신적 기여와 특징을 상세히 설명한다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-21","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false},"headingText":"요약","containsRefs":false,"markdown":"\n\n\n이 문서는 자연어 처리 분야에서 패러다임 전환을 이끈 **사전 학습 모델(Pre-trained Model)**들의 발전 과정과 핵심 원리를 탐구한다. 2015년 Google의 LSTM 사전 학습 실험부터 2023년 LLaMA까지, 각 모델이 가져온 혁신적 변화와 기술적 특징을 상세히 설명한다.\n\n주요 내용은 다음과 같다:\n\n* **사전 학습의 개념과 발전**:\n  - 초기에는 Word2Vec, GloVe 같은 정적 임베딩에서 시작\n  - Google의 LSTM 사전 학습 실험(2015)이 사전 학습의 효과를 입증\n  - 대규모 데이터로 미리 학습한 모델이 무작위 초기화보다 우수한 성능 확인\n\n* **문맥 기반 임베딩의 등장**:\n  - **ELMo (2017)**: BiLSTM 기반 양방향 문맥 임베딩의 선구자\n  - 동일한 단어라도 문맥에 따라 다른 벡터 표현 생성\n  - \"bank\"가 금융기관과 강가에서 서로 다른 의미로 표현되는 혁신\n\n* **Transformer 아키텍처의 혁명**:\n  - **Transformer (2017)**: Self-Attention과 Position Encoding으로 순차 처리 방식 탈피\n  - 병렬 처리 가능, 장거리 의존성 포착 능력 향상\n  - 현대 모든 대규모 언어 모델의 기초 구조 제공\n\n* **특화 모델들의 분화**:\n  - **GPT (2018)**: Transformer 디코더 기반 생성 특화 모델\n  - **BERT (2018)**: Transformer 인코더 기반 이해 특화 모델\n  - **BART (2019)**: 인코더-디코더 결합으로 이해와 생성 모두 강화\n  - **T5 (2020)**: 모든 NLP 태스크를 텍스트-투-텍스트로 통합\n\n* **최신 발전 동향**:\n  - **LLaMA (2023)**: 효율적인 대규모 언어 모델의 새로운 표준\n  - **UL2 (2023)**: 다양한 사전 학습 방식의 융합\n  - **FLAN (2022)**: Instruction Tuning을 통한 명령어 이해 능력 강화\n\n각 모델의 핵심 아이디어, 학습 방식, 활용 분야를 통해 현대 NLP 기술의 발전 궤적과 미래 방향성을 이해할 수 있다.\n\n# 텍스트 인코딩 및 벡터화\n\n```\nRNN Language Model\n├── Seq2Seq\n├── Beam Search\n├── Subword Tokenization\n├── Attention\n├── Transformer Encoder (Vaswani et al., 2017)\n|   ├── Positional Encoding\n|   ├── Multi-Head Attention\n|   └── Feed Forward Neural Network\n|\n├── Transformer Decoder (Vaswani et al., 2017)\n|\n├── GPT 시리즈 (OpenAI,2018~)\n|   ├── GPT-1~4\n|   └── ChatGPT (OpenAI,2022~)\n|\n├── BERT 시리즈 (Google,2018~)\n|   ├── BERT\n|   ├── RoBERTa\n|   └── ALBERT\n|\n├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)\n└── 기타 발전 모델\n    ├── T5, XLNet, ELECTRA\n    └── PaLM, LaMDA, Gemini, Claude 등\n```\n\n# 사전 학습 모델 (Pre-trained Model)\n\n* 원래는 언어모델보다는 word embedding에서 사용되던 개념이었음\n* 사전 훈련된 임베딩 (Word2Vec, GloVe)은 대용량 텍스트의 단어들의 동시 등장 통계로부터 훈련시키는 방법\n* 미리 학습시켜 놓은 모델을 가리고 새로운 문제를 풀었었음\n* Google의 LSTM 사전학습 실험 (Semi-Supervised Sequence Learning, 2015)\n   * LSTM 언어 모델을 사전 학습한 후에 텍스트 분류에 적용해봄\n   * LSTM을 사전 학습하지 않은 상태에서 텍스트 분류를 학습한 것과 성능 비교\n   * 사전 학습된 LSTM이 random 초기화된 LSTM보다 성능이 좋았음\n\n## ELMo(Embeddings from Language Models. 2017)\n\n* 사전 학습된 LSTM 언어 모델 2가지를 결합하여 좋은 임베딩 벡터값을 얻는 방법론\n* 사전 학습된 언어 모델이 NLP에서 좋은 성능을 얻을 수 있다는 강한 인상을 줌\n* ELMo는 문장에서 단어의 의미를 상황에 맞게 다르게 표현해주는 단어 임베딩 기법\n* 예를 들어,\n  * \"He went to the bank to deposit money.\"\n  * \"She sat by the bank of the river.\"\n  * 여기서 bank는 같은 철자지만 의미가 전혀 다르지만 기존 방식(Word2Vec, GloVe)은 이걸 같은 의미로 취급\n  * 그런데 ELMo는 문맥을 보고 각각 다른 벡터로 표현\n* 동작 방식\n  * 문장을 왼쪽에서 읽는 모델 + 오른쪽에서 읽는 모델 두 개를 활용하여 해당 단어가 문장 속에서 어떤 의미로 쓰였는지를 분석\n  * 문장을 양방향으로 읽어 단어의 문맥 정보를 파악함\n    * 앞에서부터 → 순방향 LSTM\n    * 뒤에서부터 → 역방향 LSTM\n    * 순방향에서의 bank에 대한 hidden state와 역방향에서의 bank에 대한 hidden state를 결합하여 임베딩을 생성\n  * 모든 단어의 의미는 \"문맥 기반\"\n    * \"bank\"가 앞뒤에 어떤 단어들과 쓰였는지 보면서 이게 '돈 관련 은행'인지, '강가'인지 판단함\n  * 그리고, 임베딩을 뽑음\n    * 이렇게 판단한 의미를 벡터(숫자 집합)로 표현\n* ELMo는 등장하자마자 기존 NLP 모델들의 정확도를 확 뛰어넘었다\n* 개체명 인식(NER), 질의응답(QA), 문장 분류 등에 광범위하게 쓰였다\n* 지금은 BERT, GPT 같은 트랜스포머가 주도하고 있지만, ELMo는 문맥 기반 임베딩의 출발점이었음\n\n## Transformer (2017)\n\n* 기존의 RNN, LSTM과 달리 **순서를 따라 처리하지 않고**, 문장 전체를 **한꺼번에 보고** 이해할 수 있도록 만든 모델이다.\n* Transformer는 두 가지 핵심 구조\n   * 1. **Self-Attention**\n      * 문장 안에서 **각 단어가 다른 단어들과 얼마나 중요한 관계가 있는지를 계산**한다.\n      * 예를 들어, \"The cat sat on the mat\"에서 \"sat\"와 \"cat\" 사이의 연결이 중요하다면, 모델은 그 둘의 관계를 더 강하게 본다.\n   * 2. **Position Encoding**\n      * Transformer는 순서를 따라 읽지 않기 때문에, **각 단어의 위치 정보**를 따로 추가해줘야 한다.\n      * 위치 정보를 더해줘서 \"첫 번째 단어\", \"두 번째 단어\" 등의 순서를 인식하게 한다.\n* Transformer의 구성요소\n  * **인코더(Encoder)**: 입력 문장을 이해하고 벡터로 변환함\n  * **디코더(Decoder)**: 그 벡터를 바탕으로 결과(예: 번역, 요약 등)를 생성함\n  * 예를 들어, 영어 문장을 프랑스어로 번역하는 경우,\n    * 인코더는 영어 문장을 이해하고\n    * 디코더는 그것을 프랑스어로 바꾸어 생성한다.\n* Transformer는 다음과 같은 이유로 혁신적이다:\n   * **병렬처리 가능**: RNN처럼 순서대로 처리하지 않기 때문에 연산 속도가 빠르다.\n   * **문맥 파악 능력 향상**: 문장의 멀리 떨어진 단어들 간의 관계도 잘 이해한다.\n   * **기반 기술로 발전**: BERT, GPT, T5, LLaMA 등 오늘날의 대부분의 대형 언어모델은 Transformer 기반이다.\n* 오늘날 대부분의 LLM은 이 구조를 바탕으로 하고 있다.\n* ELMo는 RNN 구조 기반이고,\n  * Transformer는 그 한계를 뛰어넘기 위해 등장한 구조라는 점에서\n  * 이 둘의 **패러다임 자체가 다르다**는 것도 같이 기억해두면 좋겠다.\n\n## GPT(Generative Pre-trained Transformer. 2018)\n\n* GPT는 문장을 생성하는 모델이다.\n* 기존 모델들이 문장을 이해하는 데 집중했다면, GPT는 문장을 생성하는 데 집중한다.\n* OpenAI는 Google의 Transformer을 보고 STM이 사전 학습되어 사용되면 성능이 좋은 것을 확인.  \n* Transformer의 디코더 구조를 분석하고 다음 단어를 예측하는 모듈에 해당하는 디코더로 사전 학습 언어 모델을 구현했다.\n* Transformer의 encoder를 버리고 decoder만 사용하여 문장을 생성하는 모델을 만들었다.\n* 동작 방식\n  * GPT는 **Transformer의 디코더 구조만** 사용한다.\n  * 즉, 문장을 생성하는 데 특화되어 있으며,\n  * 문장을 이해하는 인코더 구조는 없다.\n  * 핵심 동작 방식\n    * **좌→우 방향의 언어 생성 학습**\n      * 문장을 왼쪽에서 오른쪽으로 읽으면서,\n      * 다음에 올 단어를 예측하는 방식으로 학습한다.\n      * 예:\n        * 입력: \"I want to eat\"\n        * 목표: 다음 단어가 무엇일까? → \"pizza\"\n        * 이런 식으로 엄청나게 많은 문장 데이터를 통해 **패턴을 학습**한다.\n   * **사전학습 후 활용**\n     * 대규모 텍스트로 먼저 학습(Pre-training)\n     * 이후 별도 작업(챗봇, 작문, 번역 등)에 맞게 사용(Fine-tuning or Prompting)\n* 특징\n   * **텍스트 생성 능력**이 매우 뛰어남\n   * 다양한 태스크를 **명시적 미세조정 없이 프롬프트만으로 해결 가능**\n     * 이게 GPT 계열 모델의 큰 장점이다 (Few-shot, Zero-shot, etc.)\n   * **문장 완성, 요약, 번역, 창작, 대화 등**\n     * 생성형 작업에 모두 강하다\n* **GPT는 문장을 생성하는 데 특화된 모델**이다.\n\n\n## BERT(Bidirectional Encoder Representations from Transformers. 2018)\n\n* Google이 OpenAI의 GPT 모델을 보고 반대로 문장을 이해하는 데 집중하는 모델을 만들었다.\n* Transformer의 인코더 구조를 사용하여 문장을 이해하는 모델을 만들었다.\n* NLU가 특화된 부분이기 때문에 Text 분류에서 GPT보다 더 나은 성능을 보인다.\n* BERT는 문장을 **양방향으로 이해하는** 언어 모델이다.\n* 기존 모델들이 **왼쪽에서 오른쪽** 혹은 **오른쪽에서 왼쪽**으로만 문장을 해석했다면,\n* BERT는 문장을 **양쪽 방향에서 동시에** 해석한다.\n* 그래서 더 깊은 문맥 이해가 가능하다.\n* 동작 방식\n  * BERT는 **Transformer의 인코더 구조만** 사용한다.\n  * 즉, 문장을 이해하고 벡터로 바꾸는 데 특화되어 있으며,\n  * 문장을 새로 생성하는 디코더 구조는 없다.\n  * 핵심 동작 방식은 두 가지 학습 방식으로 이루어진다:\n    * **Masked Language Modeling (MLM)**\n      * 입력 문장 중 일부 단어를 가려놓고, 그 단어가 무엇인지 맞히도록 학습한다.\n      * 예: \"The cat sat on the \\[MASK].\" → 모델은 'mat'이라고 예측해야 한다.\n      * 이를 통해 문장의 앞뒤 **모든 문맥**을 참고해서 단어를 이해하는 법을 배운다.\n    * **Next Sentence Prediction (NSP)**\n      * 두 문장을 입력한 뒤, 두 번째 문장이 첫 번째 문장의 **진짜 다음 문장인지** 판단하게 학습한다.\n      * 예:\n        * 문장1: \"She opened the door.\"\n        * 문장2: \"She picked up the package.\" → 연결된 문장 (True)\n        * 문장2: \"The sun is a star.\" → 무관한 문장 (False)\n      * 문장 간의 관계 이해 능력을 키우기 위한 학습이다.\n* 문맥 기반 단어 임베딩 제공\n  * 같은 단어라도 문맥에 따라 다른 벡터로 표현됨\n* 사전학습 + 미세조정 구조 (Pretraining + Fine-tuning)\n  * 대규모 텍스트로 미리 학습해두고,\n  * 이후 실제 태스크(NER, 분류, QA 등)에 맞게 추가 학습만 하면 됨.\n* BERT는 다양한 NLP 태스크에서 **기록적인 성능 향상**을 이끌었다.\n* 이후 등장한 RoBERTa, ALBERT, DistilBERT, DeBERTa 등은 BERT의 변형이다.\n* 현재 GPT 계열이 생성에 특화되어 있다면,\n  * **BERT는 이해(이해 기반 태스크)에 특화된 모델**이라고 보면 된다.\n* BERT는 **Transformer 인코더 기반의 양방향 문맥 이해 모델**이다.\n* **단어를 문맥 안에서 정확하게 해석**하기 위해 만들어졌다.\n* **문장 분류, 질의응답, 개체명 인식 등 NLP의 다양한 작업에 폭넓게 활용**된다.\n\n## BART(Bidirectional and Auto-Regressive Transformers. 2019)\n\n* Encoder-Decoder 가 결합된 구조를 사용하여 문장을 이해하고 생성하는 모델을 만들었다.\n* BART는 **BERT + GPT의 장점**을 결합한 모델로, **이해와 생성 모두에 강한 모델**이다.\n* BART는 **문장을 이해하고 → 그것을 기반으로 문장을 생성하는 모델**이다.\n* 즉, **BERT처럼 입력을 양방향으로 이해**하고, **GPT처럼 자연스럽게 문장을 생성**한다.\n* 구조적으로는 **Transformer 인코더 + 디코더**를 모두 사용한다.\n* 쉽게 말해 **BERT + GPT를 합친 하이브리드 모델**이다.\n* 동작 방식\n   * **Denoising Autoencoder**: BART의 학습은 **\"노이즈 추가 → 원문 복원\"** 방식으로 이루어진다.\n   * 입력 문장에 노이즈를 준다\n   * 예:\n      * 원래 문장: `The cat sat on the mat.`\n      * 망가뜨린 문장: `The [MASK] on the mat.` 또는 `sat the mat on the cat.` (순서 뒤섞기)\n   * 모델은 이 망가진 문장을 보고 **원래 문장을 복원**한다. 즉, 문장을 이해하고, 적절한 형태로 **다시 생성**할 수 있어야 한다.\n   * 이 과정을 통해 BART는 **이해 능력**과 **생성 능력**을 동시에 학습하게 된다.\n* 구성\n  * **인코더**는 BERT처럼 **양방향 문맥 이해**\n  * **디코더**는 GPT처럼 **왼쪽→오른쪽 순서대로 문장 생성**\n* 이 구조 덕분에 **복잡한 입력을 해석하고**, 그에 맞는 **정확하고 자연스러운 출력**을 만들어낼 수 있다.\n* 활용 분야\n  * BART는 다음과 같은 **생성 기반 작업**에 특히 강하다:\n    * 텍스트 요약 (Summarization)\n    * 문장 생성 (Text Generation)\n    * 기계 번역 (Machine Translation)\n    * 문법 오류 수정 (Grammatical Error Correction)\n    * 질문 생성 (Question Generation)\n* 특히 **요약 모델로서 매우 뛰어난 성능**을 보여주었고, Facebook AI에서 개발한 이후 HuggingFace에서도 적극적으로 채택되었다.\n\n\n## T5 (Text-to-Text Transfer Transformer. 2020)\n\n* T5는 모든 NLP 문제를 텍스트 → 텍스트 문제로 바꾸자는 발상에서 출발했다.\n* 즉, 입력도 텍스트, 출력도 텍스트로 통일된 프레임워크를 사용한다.\n* T5는 구글이 제안한 범용 NLP 모델이다.\n* 자연어처리에서 벌어지는 거의 모든 작업을 **텍스트 입력 → 텍스트 출력**으로 통합하는 것이 핵심이다.\n* 요약, 번역, 문장 분류, 질문 생성, 질의 응답 등 모두 동일한 구조에서 처리 가능하다.\n* 동작 방식\n   * T5는 BART처럼 **Transformer 인코더 + 디코더 구조**를 사용한다.\n   * 하지만 BART와 달리, **모든 태스크를 통일된 방식으로 표현**하는 철학이 핵심이다.\n* 예시:\n   * 문장 분류\n      * 입력: `\"sst2 sentence: I love this movie.\"`\n      * 출력: `\"positive\"`\n   * 문장 요약\n      * 입력: `\"summarize: The cat sat on the mat. It was sleepy.\"`\n      * 출력: `\"The cat was sleepy.\"`\n   * 질의 응답\n      * 입력: `\"question: Where is the Eiffel Tower? context: The Eiffel Tower is in Paris.\"`\n      * 출력: `\"Paris\"`\n   * 이처럼 **작업을 구분하는 태그 + 텍스트 입력**을 넣으면 **디코더가 원하는 정답 텍스트를 생성**한다.\n* 사전학습 방식\n  * T5도 BART처럼 **Denoising Autoencoder** 방식으로 학습된다.\n  * 하지만 T5는 자체적으로 만든 **Span Corruption**이라는 방식을 쓴다:\n  * 문장에서 일부 구간(span)을 가리고, 그 구간을 `<extra_id_0>`, `<extra_id_1>` 같은 토큰으로 대체\n  * 모델이 이 빈칸들을 복원하게 함\n  * 이 과정을 통해 **문장 이해 + 생성 능력**을 동시에 기른다.\n* 특징\n  * **모든 NLP 태스크를 텍스트 → 텍스트 문제로 통일**\n  * 다양한 태스크를 하나의 모델로 처리 가능\n  * 프롬프트 기반으로 유연하게 태스크 전환\n  * 학습 구조는 BART와 유사하지만, **설계 철학은 더 범용적**임\n* **T5는 BART와 구조는 유사하나, 태스크 프레임워크가 다르다.**\n* **모든 입력과 출력을 텍스트로 다루며, 태스크 이름을 붙여 명시함**\n* 따라서 여러 NLP 작업을 하나의 파이프라인에서 처리할 수 있다.\n* 대표적인 범용 자연어 처리 모델 중 하나로, 후속 버전으로 T5.1.1, UL2, FLAN-T5 등이 있다.\n\n\n## LLaMA(Large Language Model Meta AI. 2023)\n\n* LLaMA는 Meta(구 Facebook)에서 제안한 **대규모 언어 모델 시리즈**이다.\n* 이름은 **Large Language Model Meta AI**의 약자이다.\n* LLaMA는 범용 텍스트 생성 능력을 목표로 하는 **GPT 계열 디코더 기반 모델**이다.\n* 고성능 언어 모델을 **비교적 작은 파라미터 수로도 구현 가능**하다는 점을 증명하고자 설계되었다.\n* 논문 및 모델은 공개되어 **학계, 오픈소스 커뮤니티에서 널리 활용되고 있음**.\n* LLaMA는 1\\~2단계 모델 학습을 통해 \\*\\*사전학습된 기반 모델 (Base LM)\\*\\*만 제공하며,\n  **대화, 요약, 추론 등에 맞게 파인튜닝은 사용자가 직접 수행**하도록 설계되어 있다.\n* 동작 방식\n  * LLaMA는 GPT처럼 **Transformer 디코더 구조**만 사용한다.\n  * 입력을 **왼쪽에서 오른쪽**으로 읽으며 **다음 토큰을 예측**하는 방식으로 작동한다.\n  * 학습 시점에 **자기회귀 언어모델 (Autoregressive Language Modeling)** 방식 사용.\n* 특징\n  * GPT처럼 텍스트 생성 중심의 모델이지만,\n    **효율성과 학습 품질 향상에 집중된 다양한 설계 전략**을 포함한다.\n  * 예:\n    * **Norm 위치 변경 (Pre-normalization)**\n    * **GEGLU 활성화 함수**\n    * **더 긴 시퀀스 학습 (최대 2,048 토큰)**\n    * **높은 품질의 텍스트 코퍼스만 선별하여 학습**\n  * 성능 대비 **모델 크기 효율이 매우 우수**하여,\n    작은 파라미터 수로도 **GPT-3 수준의 성능**을 낼 수 있음.\n* 버전별 주요 모델\n  * **LLaMA 1 (2023 초)**\n    * 파라미터 크기: 7B, 13B, 33B, 65B\n    * 공개 후 오픈소스 생태계에서 광범위한 활용이 시작됨\n  * **LLaMA 2 (2023 중반)**\n    * 성능 개선 및 다양한 크기: 7B, 13B, 70B\n    * **LLaMA 2-Chat**: 대화용으로 미세조정된 모델\n  * **LLaMA 3 (2024 출시)**\n    * Meta가 직접 **SOTA급 성능**을 표방하며 출시\n    * 8B, 70B 모델 제공, 대화형 fine-tuning 포함\n    * 상용 사용 가능, HuggingFace 등에서 공개됨\n* 활용 방식\n  * LLaMA는 **기반 모델만 제공**하기 때문에,\n    대화형 모델로 쓰려면 **Alpaca, Vicuna, OpenChat, Zephyr** 등의\n    **LoRA 파인튜닝** 모델을 함께 사용하는 경우가 많다.\n  * 특히 LLaMA는 **프롬프트 기반 제어**, **추론**, **코드 생성** 등\n    다양한 태스크에서 활용 가능하며,\n    **고품질 오픈소스 AI 개발의 중심**으로 자리 잡았다.\n* LLaMA는 GPT 계열의 디코더 언어 모델이다.\n* **작은 모델 크기로도 높은 성능**을 발휘할 수 있도록 최적화됨.\n* 다양한 오픈소스 프로젝트에서 핵심 기반 모델로 활용됨.\n* 후속 시리즈인 **LLaMA 2, 3**는 대화형 파인튜닝 모델과 함께 상용 및 학술용으로 널리 쓰이고 있음.\n\n좋다. 이번에는 **UL2**와 **FLAN**에 대해 각각 T5 형식에 맞춰 설명하겠다.\n둘 다 **구글이 T5 이후에 개발한 모델 또는 학습 기법**이며,\n**텍스트 생성과 이해를 모두 강화**하기 위한 방향성을 가진다.\n\n## FLAN (Fine-tuned LAnguage Net, 2022)\n\n* **FLAN은 \"Instruction Tuning\"의 대표적 구현**으로,\n  **UL2와 같은 사전학습된 언어 모델에 다양한 명령어(Task Prompt)를 학습시키는 과정**이다.\n* FLAN은 T5 또는 UL2-T5에 적용되어 등장했으며,\n  FLAN-T5, FLAN-UL2 같은 이름으로 모델이 배포된다.\n* 목표는 **명령어(prompt)를 이해하고 정확히 수행하는 능력 강화**이다.\n* 동작 방식\n  * 먼저 T5 또는 UL2-T5 모델을 준비한다.\n  * 그런 다음 **Instruction Tuning**을 수행한다.\n    → 다양한 NLP 작업(요약, 번역, 추론, QA 등)을 명시적인 지시문 형식으로 학습\n* 예시:\n  * 입력: `\"Translate English to French: I am happy.\"`\n  * 출력: `\"Je suis heureux.\"`\n  * 입력: `\"Summarize: The sun is hot. It rises in the east.\"`\n  * 출력: `\"The sun is hot and rises in the east.\"`\n* 특징\n  * 다양한 명령어와 태스크를 학습시켜, **프롬프트 이해 능력을 대폭 향상**시킴\n  * **Zero-shot / Few-shot** 능력이 크게 향상됨\n  * 단순한 텍스트 생성 모델이 아닌 **명령어 기반의 범용 도우미로 진화**\n* 요약\n  * FLAN은 모델이 아니라 **Instruction tuning 과정 또는 결과 모델 이름**이다.\n  * 일반 언어모델에 **명령어 기반 태스크 학습을 추가한 것**\n  * 대표적인 모델: **FLAN-T5**, **FLAN-UL2**\n  * 현재 Gemini, PaLM, ChatGPT 등의 **Instruction Following 능력의 기초가 된 전략**\n\n## UL2(Unifying Language Learning. 2023)\n\n* **UL2는 \"Unifying Language Learning\"의 약자**로, 구글이 제안한 새로운 **사전학습 방식**이다.\n* 목적은 기존 T5, BERT, GPT 계열 모델들의 **단점을 보완하고 장점을 융합**하는 것에 있다.\n* **이해 중심 태스크와 생성 중심 태스크 모두에 잘 작동**하는 범용 사전학습 전략이다.\n* UL2는 기존 T5 구조를 사용하지만, **학습 방식(프리트레이닝)이 완전히 다르다.**\n* 동작 방식\n  * UL2는 한 가지 방식이 아닌 **세 가지 프리트레이닝 모드**를 혼합하여 학습한다:\n    1. **R-denoising (Random span masking)**: T5처럼 일부 span을 가리고 복원\n    2. **X-denoising (Extreme masking)**: 전체 문장을 거의 다 가리고 생성\n    3. **Causal LM**: GPT처럼 왼쪽에서 오른쪽으로 생성 (Autoregressive)\n  → 이 세 가지 모드를 비율에 따라 섞어 학습시킴 (Multi-task 사전학습)\n* 예시:\n  * 입력: `fill in the blanks: The <extra_id_0> sat on the <extra_id_1>.`\n  * 출력: `cat`, `mat`\n  * 또는 GPT처럼 입력: `\"The cat sat on\"` → 출력: `\" the mat.\"`\n* 특징\n  * **세 가지 방식의 장점을 융합**하여\n    → 문장 이해 + 생성 모두에 강함\n  * 기존 T5는 디코더에서도 마스킹된 부분을 전부 본다 (비자연스러움)\n    → UL2는 자연스러운 생성을 위해 **Causal 방식도 병행**\n  * 다양한 태스크에 잘 작동하도록 설계됨\n* 요약\n  * UL2는 모델이 아니라 **사전학습 전략**이다.\n  * 기존 Transformer 구조에 적용할 수 있음 (예: T5에 적용하면 UL2-T5)\n  * **다양한 프리트레이닝 모드를 섞어 범용성과 자연스러움 극대화**\n  * 이후 FLAN 및 다양한 구글 LLM 개발의 기반이 됨\n\n## 결론\n\n사전 학습 모델의 발전은 자연어 처리 분야에서 가장 중요한 패러다임 변화 중 하나다. 2015년 Google의 LSTM 실험부터 시작된 이 여정은 현재 우리가 사용하는 ChatGPT, Claude, Gemini 등 모든 대규모 언어 모델의 토대가 되었다.\n\n* **기술적 진화의 핵심**:\n  - **정적 → 동적**: Word2Vec에서 ELMo로의 전환으로 문맥 기반 표현 실현\n  - **순차 → 병렬**: Transformer의 등장으로 Self-Attention 기반 병렬 처리 가능\n  - **단일 → 통합**: T5의 텍스트-투-텍스트 프레임워크로 모든 NLP 태스크 통합\n  - **일반 → 특화**: GPT(생성), BERT(이해), BART(양방향) 등 목적별 특화\n\n* **패러다임의 변화**:\n  - **Pre-training + Fine-tuning**: 대규모 데이터로 사전 학습 후 특정 태스크 미세조정\n  - **Transfer Learning**: 학습된 지식을 다양한 하위 태스크로 전이\n  - **Few-shot Learning**: GPT 계열에서 보여준 예시 기반 학습 능력\n  - **Instruction Following**: FLAN으로 대표되는 명령어 이해 및 수행 능력\n\n* **각 모델의 독특한 기여**:\n  - **ELMo**: 문맥 기반 임베딩의 가능성 입증\n  - **Transformer**: 현대 AI의 기초 아키텍처 제공\n  - **BERT**: 양방향 문맥 이해의 혁신적 접근\n  - **GPT**: 생성형 AI와 In-context Learning의 선구자\n  - **T5**: 통합 프레임워크를 통한 범용성 확보\n  - **LLaMA**: 효율성과 성능의 균형점 제시\n\n* **현재와 미래의 의미**:\n  - 이들 모델은 단순한 기술적 발전을 넘어 AI와 인간의 상호작용 방식을 근본적으로 변화시켰다\n  - ChatGPT의 성공으로 이어진 생성형 AI 붐의 기술적 토대 제공\n  - 언어 이해와 생성 능력의 비약적 향상으로 다양한 실용적 응용 가능\n\n* **지속되는 혁신**:\n  - 모델 크기와 성능의 지속적 확장\n  - 효율성과 접근성 개선을 위한 경량화 연구\n  - 다중 모달(텍스트, 이미지, 음성) 통합 모델로의 발전\n  - 더 나은 Instruction Following과 안전성 확보\n\n사전 학습 모델의 발전은 여전히 진행 중이며, 각 모델이 제시한 핵심 아이디어들은 미래 AI 시스템의 기초가 되고 있다. 이러한 기술적 토대 위에서 더욱 강력하고 유용한 AI 시스템들이 계속 등장할 것으로 예상된다.\n","srcMarkdownNoYaml":"\n\n# 요약\n\n이 문서는 자연어 처리 분야에서 패러다임 전환을 이끈 **사전 학습 모델(Pre-trained Model)**들의 발전 과정과 핵심 원리를 탐구한다. 2015년 Google의 LSTM 사전 학습 실험부터 2023년 LLaMA까지, 각 모델이 가져온 혁신적 변화와 기술적 특징을 상세히 설명한다.\n\n주요 내용은 다음과 같다:\n\n* **사전 학습의 개념과 발전**:\n  - 초기에는 Word2Vec, GloVe 같은 정적 임베딩에서 시작\n  - Google의 LSTM 사전 학습 실험(2015)이 사전 학습의 효과를 입증\n  - 대규모 데이터로 미리 학습한 모델이 무작위 초기화보다 우수한 성능 확인\n\n* **문맥 기반 임베딩의 등장**:\n  - **ELMo (2017)**: BiLSTM 기반 양방향 문맥 임베딩의 선구자\n  - 동일한 단어라도 문맥에 따라 다른 벡터 표현 생성\n  - \"bank\"가 금융기관과 강가에서 서로 다른 의미로 표현되는 혁신\n\n* **Transformer 아키텍처의 혁명**:\n  - **Transformer (2017)**: Self-Attention과 Position Encoding으로 순차 처리 방식 탈피\n  - 병렬 처리 가능, 장거리 의존성 포착 능력 향상\n  - 현대 모든 대규모 언어 모델의 기초 구조 제공\n\n* **특화 모델들의 분화**:\n  - **GPT (2018)**: Transformer 디코더 기반 생성 특화 모델\n  - **BERT (2018)**: Transformer 인코더 기반 이해 특화 모델\n  - **BART (2019)**: 인코더-디코더 결합으로 이해와 생성 모두 강화\n  - **T5 (2020)**: 모든 NLP 태스크를 텍스트-투-텍스트로 통합\n\n* **최신 발전 동향**:\n  - **LLaMA (2023)**: 효율적인 대규모 언어 모델의 새로운 표준\n  - **UL2 (2023)**: 다양한 사전 학습 방식의 융합\n  - **FLAN (2022)**: Instruction Tuning을 통한 명령어 이해 능력 강화\n\n각 모델의 핵심 아이디어, 학습 방식, 활용 분야를 통해 현대 NLP 기술의 발전 궤적과 미래 방향성을 이해할 수 있다.\n\n# 텍스트 인코딩 및 벡터화\n\n```\nRNN Language Model\n├── Seq2Seq\n├── Beam Search\n├── Subword Tokenization\n├── Attention\n├── Transformer Encoder (Vaswani et al., 2017)\n|   ├── Positional Encoding\n|   ├── Multi-Head Attention\n|   └── Feed Forward Neural Network\n|\n├── Transformer Decoder (Vaswani et al., 2017)\n|\n├── GPT 시리즈 (OpenAI,2018~)\n|   ├── GPT-1~4\n|   └── ChatGPT (OpenAI,2022~)\n|\n├── BERT 시리즈 (Google,2018~)\n|   ├── BERT\n|   ├── RoBERTa\n|   └── ALBERT\n|\n├── 한국어 특화: KoBERT, KoGPT, KLU-BERT 등 (Kakao,2019~)\n└── 기타 발전 모델\n    ├── T5, XLNet, ELECTRA\n    └── PaLM, LaMDA, Gemini, Claude 등\n```\n\n# 사전 학습 모델 (Pre-trained Model)\n\n* 원래는 언어모델보다는 word embedding에서 사용되던 개념이었음\n* 사전 훈련된 임베딩 (Word2Vec, GloVe)은 대용량 텍스트의 단어들의 동시 등장 통계로부터 훈련시키는 방법\n* 미리 학습시켜 놓은 모델을 가리고 새로운 문제를 풀었었음\n* Google의 LSTM 사전학습 실험 (Semi-Supervised Sequence Learning, 2015)\n   * LSTM 언어 모델을 사전 학습한 후에 텍스트 분류에 적용해봄\n   * LSTM을 사전 학습하지 않은 상태에서 텍스트 분류를 학습한 것과 성능 비교\n   * 사전 학습된 LSTM이 random 초기화된 LSTM보다 성능이 좋았음\n\n## ELMo(Embeddings from Language Models. 2017)\n\n* 사전 학습된 LSTM 언어 모델 2가지를 결합하여 좋은 임베딩 벡터값을 얻는 방법론\n* 사전 학습된 언어 모델이 NLP에서 좋은 성능을 얻을 수 있다는 강한 인상을 줌\n* ELMo는 문장에서 단어의 의미를 상황에 맞게 다르게 표현해주는 단어 임베딩 기법\n* 예를 들어,\n  * \"He went to the bank to deposit money.\"\n  * \"She sat by the bank of the river.\"\n  * 여기서 bank는 같은 철자지만 의미가 전혀 다르지만 기존 방식(Word2Vec, GloVe)은 이걸 같은 의미로 취급\n  * 그런데 ELMo는 문맥을 보고 각각 다른 벡터로 표현\n* 동작 방식\n  * 문장을 왼쪽에서 읽는 모델 + 오른쪽에서 읽는 모델 두 개를 활용하여 해당 단어가 문장 속에서 어떤 의미로 쓰였는지를 분석\n  * 문장을 양방향으로 읽어 단어의 문맥 정보를 파악함\n    * 앞에서부터 → 순방향 LSTM\n    * 뒤에서부터 → 역방향 LSTM\n    * 순방향에서의 bank에 대한 hidden state와 역방향에서의 bank에 대한 hidden state를 결합하여 임베딩을 생성\n  * 모든 단어의 의미는 \"문맥 기반\"\n    * \"bank\"가 앞뒤에 어떤 단어들과 쓰였는지 보면서 이게 '돈 관련 은행'인지, '강가'인지 판단함\n  * 그리고, 임베딩을 뽑음\n    * 이렇게 판단한 의미를 벡터(숫자 집합)로 표현\n* ELMo는 등장하자마자 기존 NLP 모델들의 정확도를 확 뛰어넘었다\n* 개체명 인식(NER), 질의응답(QA), 문장 분류 등에 광범위하게 쓰였다\n* 지금은 BERT, GPT 같은 트랜스포머가 주도하고 있지만, ELMo는 문맥 기반 임베딩의 출발점이었음\n\n## Transformer (2017)\n\n* 기존의 RNN, LSTM과 달리 **순서를 따라 처리하지 않고**, 문장 전체를 **한꺼번에 보고** 이해할 수 있도록 만든 모델이다.\n* Transformer는 두 가지 핵심 구조\n   * 1. **Self-Attention**\n      * 문장 안에서 **각 단어가 다른 단어들과 얼마나 중요한 관계가 있는지를 계산**한다.\n      * 예를 들어, \"The cat sat on the mat\"에서 \"sat\"와 \"cat\" 사이의 연결이 중요하다면, 모델은 그 둘의 관계를 더 강하게 본다.\n   * 2. **Position Encoding**\n      * Transformer는 순서를 따라 읽지 않기 때문에, **각 단어의 위치 정보**를 따로 추가해줘야 한다.\n      * 위치 정보를 더해줘서 \"첫 번째 단어\", \"두 번째 단어\" 등의 순서를 인식하게 한다.\n* Transformer의 구성요소\n  * **인코더(Encoder)**: 입력 문장을 이해하고 벡터로 변환함\n  * **디코더(Decoder)**: 그 벡터를 바탕으로 결과(예: 번역, 요약 등)를 생성함\n  * 예를 들어, 영어 문장을 프랑스어로 번역하는 경우,\n    * 인코더는 영어 문장을 이해하고\n    * 디코더는 그것을 프랑스어로 바꾸어 생성한다.\n* Transformer는 다음과 같은 이유로 혁신적이다:\n   * **병렬처리 가능**: RNN처럼 순서대로 처리하지 않기 때문에 연산 속도가 빠르다.\n   * **문맥 파악 능력 향상**: 문장의 멀리 떨어진 단어들 간의 관계도 잘 이해한다.\n   * **기반 기술로 발전**: BERT, GPT, T5, LLaMA 등 오늘날의 대부분의 대형 언어모델은 Transformer 기반이다.\n* 오늘날 대부분의 LLM은 이 구조를 바탕으로 하고 있다.\n* ELMo는 RNN 구조 기반이고,\n  * Transformer는 그 한계를 뛰어넘기 위해 등장한 구조라는 점에서\n  * 이 둘의 **패러다임 자체가 다르다**는 것도 같이 기억해두면 좋겠다.\n\n## GPT(Generative Pre-trained Transformer. 2018)\n\n* GPT는 문장을 생성하는 모델이다.\n* 기존 모델들이 문장을 이해하는 데 집중했다면, GPT는 문장을 생성하는 데 집중한다.\n* OpenAI는 Google의 Transformer을 보고 STM이 사전 학습되어 사용되면 성능이 좋은 것을 확인.  \n* Transformer의 디코더 구조를 분석하고 다음 단어를 예측하는 모듈에 해당하는 디코더로 사전 학습 언어 모델을 구현했다.\n* Transformer의 encoder를 버리고 decoder만 사용하여 문장을 생성하는 모델을 만들었다.\n* 동작 방식\n  * GPT는 **Transformer의 디코더 구조만** 사용한다.\n  * 즉, 문장을 생성하는 데 특화되어 있으며,\n  * 문장을 이해하는 인코더 구조는 없다.\n  * 핵심 동작 방식\n    * **좌→우 방향의 언어 생성 학습**\n      * 문장을 왼쪽에서 오른쪽으로 읽으면서,\n      * 다음에 올 단어를 예측하는 방식으로 학습한다.\n      * 예:\n        * 입력: \"I want to eat\"\n        * 목표: 다음 단어가 무엇일까? → \"pizza\"\n        * 이런 식으로 엄청나게 많은 문장 데이터를 통해 **패턴을 학습**한다.\n   * **사전학습 후 활용**\n     * 대규모 텍스트로 먼저 학습(Pre-training)\n     * 이후 별도 작업(챗봇, 작문, 번역 등)에 맞게 사용(Fine-tuning or Prompting)\n* 특징\n   * **텍스트 생성 능력**이 매우 뛰어남\n   * 다양한 태스크를 **명시적 미세조정 없이 프롬프트만으로 해결 가능**\n     * 이게 GPT 계열 모델의 큰 장점이다 (Few-shot, Zero-shot, etc.)\n   * **문장 완성, 요약, 번역, 창작, 대화 등**\n     * 생성형 작업에 모두 강하다\n* **GPT는 문장을 생성하는 데 특화된 모델**이다.\n\n\n## BERT(Bidirectional Encoder Representations from Transformers. 2018)\n\n* Google이 OpenAI의 GPT 모델을 보고 반대로 문장을 이해하는 데 집중하는 모델을 만들었다.\n* Transformer의 인코더 구조를 사용하여 문장을 이해하는 모델을 만들었다.\n* NLU가 특화된 부분이기 때문에 Text 분류에서 GPT보다 더 나은 성능을 보인다.\n* BERT는 문장을 **양방향으로 이해하는** 언어 모델이다.\n* 기존 모델들이 **왼쪽에서 오른쪽** 혹은 **오른쪽에서 왼쪽**으로만 문장을 해석했다면,\n* BERT는 문장을 **양쪽 방향에서 동시에** 해석한다.\n* 그래서 더 깊은 문맥 이해가 가능하다.\n* 동작 방식\n  * BERT는 **Transformer의 인코더 구조만** 사용한다.\n  * 즉, 문장을 이해하고 벡터로 바꾸는 데 특화되어 있으며,\n  * 문장을 새로 생성하는 디코더 구조는 없다.\n  * 핵심 동작 방식은 두 가지 학습 방식으로 이루어진다:\n    * **Masked Language Modeling (MLM)**\n      * 입력 문장 중 일부 단어를 가려놓고, 그 단어가 무엇인지 맞히도록 학습한다.\n      * 예: \"The cat sat on the \\[MASK].\" → 모델은 'mat'이라고 예측해야 한다.\n      * 이를 통해 문장의 앞뒤 **모든 문맥**을 참고해서 단어를 이해하는 법을 배운다.\n    * **Next Sentence Prediction (NSP)**\n      * 두 문장을 입력한 뒤, 두 번째 문장이 첫 번째 문장의 **진짜 다음 문장인지** 판단하게 학습한다.\n      * 예:\n        * 문장1: \"She opened the door.\"\n        * 문장2: \"She picked up the package.\" → 연결된 문장 (True)\n        * 문장2: \"The sun is a star.\" → 무관한 문장 (False)\n      * 문장 간의 관계 이해 능력을 키우기 위한 학습이다.\n* 문맥 기반 단어 임베딩 제공\n  * 같은 단어라도 문맥에 따라 다른 벡터로 표현됨\n* 사전학습 + 미세조정 구조 (Pretraining + Fine-tuning)\n  * 대규모 텍스트로 미리 학습해두고,\n  * 이후 실제 태스크(NER, 분류, QA 등)에 맞게 추가 학습만 하면 됨.\n* BERT는 다양한 NLP 태스크에서 **기록적인 성능 향상**을 이끌었다.\n* 이후 등장한 RoBERTa, ALBERT, DistilBERT, DeBERTa 등은 BERT의 변형이다.\n* 현재 GPT 계열이 생성에 특화되어 있다면,\n  * **BERT는 이해(이해 기반 태스크)에 특화된 모델**이라고 보면 된다.\n* BERT는 **Transformer 인코더 기반의 양방향 문맥 이해 모델**이다.\n* **단어를 문맥 안에서 정확하게 해석**하기 위해 만들어졌다.\n* **문장 분류, 질의응답, 개체명 인식 등 NLP의 다양한 작업에 폭넓게 활용**된다.\n\n## BART(Bidirectional and Auto-Regressive Transformers. 2019)\n\n* Encoder-Decoder 가 결합된 구조를 사용하여 문장을 이해하고 생성하는 모델을 만들었다.\n* BART는 **BERT + GPT의 장점**을 결합한 모델로, **이해와 생성 모두에 강한 모델**이다.\n* BART는 **문장을 이해하고 → 그것을 기반으로 문장을 생성하는 모델**이다.\n* 즉, **BERT처럼 입력을 양방향으로 이해**하고, **GPT처럼 자연스럽게 문장을 생성**한다.\n* 구조적으로는 **Transformer 인코더 + 디코더**를 모두 사용한다.\n* 쉽게 말해 **BERT + GPT를 합친 하이브리드 모델**이다.\n* 동작 방식\n   * **Denoising Autoencoder**: BART의 학습은 **\"노이즈 추가 → 원문 복원\"** 방식으로 이루어진다.\n   * 입력 문장에 노이즈를 준다\n   * 예:\n      * 원래 문장: `The cat sat on the mat.`\n      * 망가뜨린 문장: `The [MASK] on the mat.` 또는 `sat the mat on the cat.` (순서 뒤섞기)\n   * 모델은 이 망가진 문장을 보고 **원래 문장을 복원**한다. 즉, 문장을 이해하고, 적절한 형태로 **다시 생성**할 수 있어야 한다.\n   * 이 과정을 통해 BART는 **이해 능력**과 **생성 능력**을 동시에 학습하게 된다.\n* 구성\n  * **인코더**는 BERT처럼 **양방향 문맥 이해**\n  * **디코더**는 GPT처럼 **왼쪽→오른쪽 순서대로 문장 생성**\n* 이 구조 덕분에 **복잡한 입력을 해석하고**, 그에 맞는 **정확하고 자연스러운 출력**을 만들어낼 수 있다.\n* 활용 분야\n  * BART는 다음과 같은 **생성 기반 작업**에 특히 강하다:\n    * 텍스트 요약 (Summarization)\n    * 문장 생성 (Text Generation)\n    * 기계 번역 (Machine Translation)\n    * 문법 오류 수정 (Grammatical Error Correction)\n    * 질문 생성 (Question Generation)\n* 특히 **요약 모델로서 매우 뛰어난 성능**을 보여주었고, Facebook AI에서 개발한 이후 HuggingFace에서도 적극적으로 채택되었다.\n\n\n## T5 (Text-to-Text Transfer Transformer. 2020)\n\n* T5는 모든 NLP 문제를 텍스트 → 텍스트 문제로 바꾸자는 발상에서 출발했다.\n* 즉, 입력도 텍스트, 출력도 텍스트로 통일된 프레임워크를 사용한다.\n* T5는 구글이 제안한 범용 NLP 모델이다.\n* 자연어처리에서 벌어지는 거의 모든 작업을 **텍스트 입력 → 텍스트 출력**으로 통합하는 것이 핵심이다.\n* 요약, 번역, 문장 분류, 질문 생성, 질의 응답 등 모두 동일한 구조에서 처리 가능하다.\n* 동작 방식\n   * T5는 BART처럼 **Transformer 인코더 + 디코더 구조**를 사용한다.\n   * 하지만 BART와 달리, **모든 태스크를 통일된 방식으로 표현**하는 철학이 핵심이다.\n* 예시:\n   * 문장 분류\n      * 입력: `\"sst2 sentence: I love this movie.\"`\n      * 출력: `\"positive\"`\n   * 문장 요약\n      * 입력: `\"summarize: The cat sat on the mat. It was sleepy.\"`\n      * 출력: `\"The cat was sleepy.\"`\n   * 질의 응답\n      * 입력: `\"question: Where is the Eiffel Tower? context: The Eiffel Tower is in Paris.\"`\n      * 출력: `\"Paris\"`\n   * 이처럼 **작업을 구분하는 태그 + 텍스트 입력**을 넣으면 **디코더가 원하는 정답 텍스트를 생성**한다.\n* 사전학습 방식\n  * T5도 BART처럼 **Denoising Autoencoder** 방식으로 학습된다.\n  * 하지만 T5는 자체적으로 만든 **Span Corruption**이라는 방식을 쓴다:\n  * 문장에서 일부 구간(span)을 가리고, 그 구간을 `<extra_id_0>`, `<extra_id_1>` 같은 토큰으로 대체\n  * 모델이 이 빈칸들을 복원하게 함\n  * 이 과정을 통해 **문장 이해 + 생성 능력**을 동시에 기른다.\n* 특징\n  * **모든 NLP 태스크를 텍스트 → 텍스트 문제로 통일**\n  * 다양한 태스크를 하나의 모델로 처리 가능\n  * 프롬프트 기반으로 유연하게 태스크 전환\n  * 학습 구조는 BART와 유사하지만, **설계 철학은 더 범용적**임\n* **T5는 BART와 구조는 유사하나, 태스크 프레임워크가 다르다.**\n* **모든 입력과 출력을 텍스트로 다루며, 태스크 이름을 붙여 명시함**\n* 따라서 여러 NLP 작업을 하나의 파이프라인에서 처리할 수 있다.\n* 대표적인 범용 자연어 처리 모델 중 하나로, 후속 버전으로 T5.1.1, UL2, FLAN-T5 등이 있다.\n\n\n## LLaMA(Large Language Model Meta AI. 2023)\n\n* LLaMA는 Meta(구 Facebook)에서 제안한 **대규모 언어 모델 시리즈**이다.\n* 이름은 **Large Language Model Meta AI**의 약자이다.\n* LLaMA는 범용 텍스트 생성 능력을 목표로 하는 **GPT 계열 디코더 기반 모델**이다.\n* 고성능 언어 모델을 **비교적 작은 파라미터 수로도 구현 가능**하다는 점을 증명하고자 설계되었다.\n* 논문 및 모델은 공개되어 **학계, 오픈소스 커뮤니티에서 널리 활용되고 있음**.\n* LLaMA는 1\\~2단계 모델 학습을 통해 \\*\\*사전학습된 기반 모델 (Base LM)\\*\\*만 제공하며,\n  **대화, 요약, 추론 등에 맞게 파인튜닝은 사용자가 직접 수행**하도록 설계되어 있다.\n* 동작 방식\n  * LLaMA는 GPT처럼 **Transformer 디코더 구조**만 사용한다.\n  * 입력을 **왼쪽에서 오른쪽**으로 읽으며 **다음 토큰을 예측**하는 방식으로 작동한다.\n  * 학습 시점에 **자기회귀 언어모델 (Autoregressive Language Modeling)** 방식 사용.\n* 특징\n  * GPT처럼 텍스트 생성 중심의 모델이지만,\n    **효율성과 학습 품질 향상에 집중된 다양한 설계 전략**을 포함한다.\n  * 예:\n    * **Norm 위치 변경 (Pre-normalization)**\n    * **GEGLU 활성화 함수**\n    * **더 긴 시퀀스 학습 (최대 2,048 토큰)**\n    * **높은 품질의 텍스트 코퍼스만 선별하여 학습**\n  * 성능 대비 **모델 크기 효율이 매우 우수**하여,\n    작은 파라미터 수로도 **GPT-3 수준의 성능**을 낼 수 있음.\n* 버전별 주요 모델\n  * **LLaMA 1 (2023 초)**\n    * 파라미터 크기: 7B, 13B, 33B, 65B\n    * 공개 후 오픈소스 생태계에서 광범위한 활용이 시작됨\n  * **LLaMA 2 (2023 중반)**\n    * 성능 개선 및 다양한 크기: 7B, 13B, 70B\n    * **LLaMA 2-Chat**: 대화용으로 미세조정된 모델\n  * **LLaMA 3 (2024 출시)**\n    * Meta가 직접 **SOTA급 성능**을 표방하며 출시\n    * 8B, 70B 모델 제공, 대화형 fine-tuning 포함\n    * 상용 사용 가능, HuggingFace 등에서 공개됨\n* 활용 방식\n  * LLaMA는 **기반 모델만 제공**하기 때문에,\n    대화형 모델로 쓰려면 **Alpaca, Vicuna, OpenChat, Zephyr** 등의\n    **LoRA 파인튜닝** 모델을 함께 사용하는 경우가 많다.\n  * 특히 LLaMA는 **프롬프트 기반 제어**, **추론**, **코드 생성** 등\n    다양한 태스크에서 활용 가능하며,\n    **고품질 오픈소스 AI 개발의 중심**으로 자리 잡았다.\n* LLaMA는 GPT 계열의 디코더 언어 모델이다.\n* **작은 모델 크기로도 높은 성능**을 발휘할 수 있도록 최적화됨.\n* 다양한 오픈소스 프로젝트에서 핵심 기반 모델로 활용됨.\n* 후속 시리즈인 **LLaMA 2, 3**는 대화형 파인튜닝 모델과 함께 상용 및 학술용으로 널리 쓰이고 있음.\n\n좋다. 이번에는 **UL2**와 **FLAN**에 대해 각각 T5 형식에 맞춰 설명하겠다.\n둘 다 **구글이 T5 이후에 개발한 모델 또는 학습 기법**이며,\n**텍스트 생성과 이해를 모두 강화**하기 위한 방향성을 가진다.\n\n## FLAN (Fine-tuned LAnguage Net, 2022)\n\n* **FLAN은 \"Instruction Tuning\"의 대표적 구현**으로,\n  **UL2와 같은 사전학습된 언어 모델에 다양한 명령어(Task Prompt)를 학습시키는 과정**이다.\n* FLAN은 T5 또는 UL2-T5에 적용되어 등장했으며,\n  FLAN-T5, FLAN-UL2 같은 이름으로 모델이 배포된다.\n* 목표는 **명령어(prompt)를 이해하고 정확히 수행하는 능력 강화**이다.\n* 동작 방식\n  * 먼저 T5 또는 UL2-T5 모델을 준비한다.\n  * 그런 다음 **Instruction Tuning**을 수행한다.\n    → 다양한 NLP 작업(요약, 번역, 추론, QA 등)을 명시적인 지시문 형식으로 학습\n* 예시:\n  * 입력: `\"Translate English to French: I am happy.\"`\n  * 출력: `\"Je suis heureux.\"`\n  * 입력: `\"Summarize: The sun is hot. It rises in the east.\"`\n  * 출력: `\"The sun is hot and rises in the east.\"`\n* 특징\n  * 다양한 명령어와 태스크를 학습시켜, **프롬프트 이해 능력을 대폭 향상**시킴\n  * **Zero-shot / Few-shot** 능력이 크게 향상됨\n  * 단순한 텍스트 생성 모델이 아닌 **명령어 기반의 범용 도우미로 진화**\n* 요약\n  * FLAN은 모델이 아니라 **Instruction tuning 과정 또는 결과 모델 이름**이다.\n  * 일반 언어모델에 **명령어 기반 태스크 학습을 추가한 것**\n  * 대표적인 모델: **FLAN-T5**, **FLAN-UL2**\n  * 현재 Gemini, PaLM, ChatGPT 등의 **Instruction Following 능력의 기초가 된 전략**\n\n## UL2(Unifying Language Learning. 2023)\n\n* **UL2는 \"Unifying Language Learning\"의 약자**로, 구글이 제안한 새로운 **사전학습 방식**이다.\n* 목적은 기존 T5, BERT, GPT 계열 모델들의 **단점을 보완하고 장점을 융합**하는 것에 있다.\n* **이해 중심 태스크와 생성 중심 태스크 모두에 잘 작동**하는 범용 사전학습 전략이다.\n* UL2는 기존 T5 구조를 사용하지만, **학습 방식(프리트레이닝)이 완전히 다르다.**\n* 동작 방식\n  * UL2는 한 가지 방식이 아닌 **세 가지 프리트레이닝 모드**를 혼합하여 학습한다:\n    1. **R-denoising (Random span masking)**: T5처럼 일부 span을 가리고 복원\n    2. **X-denoising (Extreme masking)**: 전체 문장을 거의 다 가리고 생성\n    3. **Causal LM**: GPT처럼 왼쪽에서 오른쪽으로 생성 (Autoregressive)\n  → 이 세 가지 모드를 비율에 따라 섞어 학습시킴 (Multi-task 사전학습)\n* 예시:\n  * 입력: `fill in the blanks: The <extra_id_0> sat on the <extra_id_1>.`\n  * 출력: `cat`, `mat`\n  * 또는 GPT처럼 입력: `\"The cat sat on\"` → 출력: `\" the mat.\"`\n* 특징\n  * **세 가지 방식의 장점을 융합**하여\n    → 문장 이해 + 생성 모두에 강함\n  * 기존 T5는 디코더에서도 마스킹된 부분을 전부 본다 (비자연스러움)\n    → UL2는 자연스러운 생성을 위해 **Causal 방식도 병행**\n  * 다양한 태스크에 잘 작동하도록 설계됨\n* 요약\n  * UL2는 모델이 아니라 **사전학습 전략**이다.\n  * 기존 Transformer 구조에 적용할 수 있음 (예: T5에 적용하면 UL2-T5)\n  * **다양한 프리트레이닝 모드를 섞어 범용성과 자연스러움 극대화**\n  * 이후 FLAN 및 다양한 구글 LLM 개발의 기반이 됨\n\n## 결론\n\n사전 학습 모델의 발전은 자연어 처리 분야에서 가장 중요한 패러다임 변화 중 하나다. 2015년 Google의 LSTM 실험부터 시작된 이 여정은 현재 우리가 사용하는 ChatGPT, Claude, Gemini 등 모든 대규모 언어 모델의 토대가 되었다.\n\n* **기술적 진화의 핵심**:\n  - **정적 → 동적**: Word2Vec에서 ELMo로의 전환으로 문맥 기반 표현 실현\n  - **순차 → 병렬**: Transformer의 등장으로 Self-Attention 기반 병렬 처리 가능\n  - **단일 → 통합**: T5의 텍스트-투-텍스트 프레임워크로 모든 NLP 태스크 통합\n  - **일반 → 특화**: GPT(생성), BERT(이해), BART(양방향) 등 목적별 특화\n\n* **패러다임의 변화**:\n  - **Pre-training + Fine-tuning**: 대규모 데이터로 사전 학습 후 특정 태스크 미세조정\n  - **Transfer Learning**: 학습된 지식을 다양한 하위 태스크로 전이\n  - **Few-shot Learning**: GPT 계열에서 보여준 예시 기반 학습 능력\n  - **Instruction Following**: FLAN으로 대표되는 명령어 이해 및 수행 능력\n\n* **각 모델의 독특한 기여**:\n  - **ELMo**: 문맥 기반 임베딩의 가능성 입증\n  - **Transformer**: 현대 AI의 기초 아키텍처 제공\n  - **BERT**: 양방향 문맥 이해의 혁신적 접근\n  - **GPT**: 생성형 AI와 In-context Learning의 선구자\n  - **T5**: 통합 프레임워크를 통한 범용성 확보\n  - **LLaMA**: 효율성과 성능의 균형점 제시\n\n* **현재와 미래의 의미**:\n  - 이들 모델은 단순한 기술적 발전을 넘어 AI와 인간의 상호작용 방식을 근본적으로 변화시켰다\n  - ChatGPT의 성공으로 이어진 생성형 AI 붐의 기술적 토대 제공\n  - 언어 이해와 생성 능력의 비약적 향상으로 다양한 실용적 응용 가능\n\n* **지속되는 혁신**:\n  - 모델 크기와 성능의 지속적 확장\n  - 효율성과 접근성 개선을 위한 경량화 연구\n  - 다중 모달(텍스트, 이미지, 음성) 통합 모델로의 발전\n  - 더 나은 Instruction Following과 안전성 확보\n\n사전 학습 모델의 발전은 여전히 진행 중이며, 각 모델이 제시한 핵심 아이디어들은 미래 AI 시스템의 기초가 되고 있다. 이러한 기술적 토대 위에서 더욱 강력하고 유용한 AI 시스템들이 계속 등장할 것으로 예상된다.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"21.ptm_overview.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"사전 학습 모델의 발전","subtitle":"ELMo부터 LLaMA까지: 현대 NLP의 혁신적 변화","description":"ELMo, BERT, GPT, T5, LLaMA 등 주요 사전 학습 모델들의 발전 과정과 핵심 원리를 다룬다. 문맥 기반 임베딩부터 대규모 언어 모델까지, 각 모델의 혁신적 기여와 특징을 상세히 설명한다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-21","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}