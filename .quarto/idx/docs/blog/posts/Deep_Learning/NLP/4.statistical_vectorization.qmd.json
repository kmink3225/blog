{"title":"텍스트 인코딩 및 벡터화: NLP 숫자 변환의 모든 것","markdown":{"yaml":{"title":"텍스트 인코딩 및 벡터화: NLP 숫자 변환의 모든 것","subtitle":"정수 인코딩, OOV 처리, 패딩부터 원-핫, BoW, TF-IDF까지 핵심 개념 총정리","description":"자연어 처리(NLP)에서 텍스트 데이터를 기계가 이해하고 처리할 수 있는 숫자 형태로 변환하는 인코딩 및 벡터화의 주요 개념과 방법들을 살펴본다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-04","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false},"headingText":"이 문서 한눈에 보기","containsRefs":false,"markdown":"\n\n\n이 문서는 자연어 처리(NLP)를 위해 텍스트 데이터를 기계가 이해할 수 있는 숫자 형태로 변환하는 주요 과정인 **인코딩(Encoding)**과 **벡터화(Vectorization)**에 대해 심층적으로 다룬다.\n\n주요 내용은 다음과 같다.\n\n*   **인코딩 (Encoding)**:\n    *   **정의 및 필요성**: 텍스트(문자열)를 모델이 처리할 수 있는 형태로 바꾸는 모든 과정으로 텍스트를 숫자(주로 정수 인덱스)로 변환하는 과정이다.\n    *   **정수 인코딩**: 각 단어에 고유 정수를 부여하는 방법과 어휘 집합 크기 결정, 그리고 학습 데이터에 없는 단어(OOV: Out-of-Vocabulary)를 `UNK` 토큰으로 처리하는 방법을 설명한다.\n*   **패딩 (Padding)**:\n    *   **정의 및 필요성**: 길이가 다른 텍스트 시퀀스들을 모델 입력을 위해 동일한 길이로 맞춰주는 작업으로, `PAD` 토큰을 사용한다.\n*   **벡터화 (Vectorization)**:\n    *   **정의**: 정수 인코딩된 데이터를 숫자 벡터로 변환하여 텍스트의 의미나 통계적 정보를 표현한다.\n    *   **통계적 방법**:\n        *   **원-핫 인코딩**: 각 단어를 고유한 희소 벡터로 표현하며, 장단점(차원의 저주)을 다룬다.\n        *   **빈도 기반 방법 (DTM, BoW, TF-IDF)**: 문서 내 단어 빈도를 기반으로 문서를 벡터화하는 DTM(Document Term Matrix), Bag-of-Words(BoW), TF-IDF 기법의 개념과 특징, 활용 방안을 설명한다.\n    *   **벡터 표현의 단위**: 단어는 벡터로, 문서는 벡터 또는 행렬로 표현될 수 있음을 설명한다.\n*   **결론**: 효과적인 인코딩 및 벡터화 전략 선택의 중요성을 강조한다.\n\n이 문서를 통해 텍스트 데이터가 NLP 모델에서 어떻게 처리될 수 있도록 준비되는지에 대한 기본적인 이해를 얻을 수 있다.\n\n# 텍스트 인코딩 및 벡터화\n\n```\n텍스트 벡터화\n├── 1. 전통적 방법 (통계 기반)\n│   ├── BoW\n│   ├── DTM\n│   └── TF-IDF\n│\n├── 2. 신경망 기반 (문맥 독립)\n│   ├── 문맥 독립적 임베딩\n│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)\n│   ├── Word2Vec (CBOW, Skip-gram)\n│   ├── FastText\n│   ├── GloVe\n│   └── 기타 모델: Swivel, LexVec 등\n│\n└── 3. 문맥 기반 임베딩 (Contextual Embedding)\n    └── RNN 계열\n        ├── LSTM\n        ├── GRU\n        └── ELMo\n```\n\n## 인코딩(Encoding)\n\n*   **인코딩 (Encoding)**:\n    *   **정의**: 자연어 처리(NLP)에서 **텍스트 데이터를 기계가 이해하고 처리할 수 있는 숫자 형태로 변환하는 과정** 전반을 의미. 컴퓨터는 텍스트를 직접 이해할 수 없으므로, 토큰화된 각 구성 요소(단어 등)를 숫자 표현(주로 정수 인덱스)으로 바꾸는 단계.\n    *   **핵심**: 토큰화 후, 토큰들을 숫자로 매핑.\n*   **주요 기법**\n   * 정수 인코딩: 보통 사전 정의된 정수, 의미 보존 x, 비학습, 코사인 유사도 측정 불가\n      * 예: 원-핫 인코딩, Indexing, 빈도 기반 인코딩 등\n   * embedding: 학습 가능, 코사인 유사도 측정 가능, 문맥 표현 가능\n      * 예: word2vec, fasttext, glove 등.\n\n### 정수 인코딩 (Integer Encoding)\n\n*   **개념**: 어휘 집합(Vocabulary) 내 각 고유 토큰에 **고유한 정수 인덱스를 부여**.\n*   **과정**:\n    1.  어휘 집합 구축: 전체 텍스트에서 고유 토큰 추출 \n      * 빈도, 최대 크기 고려하여 어휘 집합 크기 제한하는 것이 현실적인 전략\n    2.  각 토큰에 정수 할당\n      * 빈도 높은 단어에 낮은 숫자 할당\n    3.  텍스트의 토큰 시퀀스를 정수 시퀀스로 변환\n* **예시**:\n   * 어휘 집합 구축:\n       *   예시 문장: `\"I love natural language processing\"`\n       *   토큰화 결과: `[\"i\", \"love\", \"natural\", \"language\", \"processing\"]`\n       *   초기 어휘 집합 (`word_to_index`)의 정수 인코딩\n         * `{\"i\": 1, \"love\": 2, \"natural\": 3, \"language\": 4, \"processing\": 5}`\n       *   정수 시퀀스: `[1, 2, 3, 4, 5]`\n       *   이때 어휘 집합의 크기는 5이다.\n*   **한계**\n    *   텍스트간 유사도 측정 불가: 숫자 값 자체가 단어 간 의미/관계 표현 못 함 (모델 오해 가능성).\n\n### OOV (Out-of-Vocabulary) 문제\n\n*   **정의**: 학습 시 구축된 어휘 집합에 **포함되지 않은 단어**가 입력될 때 발생.\n*   **원인**: 신조어, 전문 용어, 오타, 제한된 어휘 크기 등.\n*   **영향**: 정보 손실, 잘못된 예측, 모델 성능 저하.\n*   **`UNK` (Unknown) 토큰 처리**:\n    *   OOV 단어를 미리 정의된 `UNK` 토큰의 인덱스로 일괄 대체.\n    *   모든 OOV가 동일 토큰으로 매핑되어 원래 단어의 고유 정보는 손실.\n*   **어휘 집합 크기 및 신규 단어(OOV) 처리 예시 (영문)**:\n    *   어휘 집합의 크기는 모델의 성능과 효율성에 영향을 미치며, 너무 작으면 OOV 문제가, 너무 크면 계산 비용 및 과적합 문제가 발생할 수 있다.\n    *   **신규 단어(OOV) 발생 시 `UNK` 토큰 처리**:\n        *   기존 예시 문장: `\"I love natural language processing\"`\n        *   새로운 문장: `\"I also love deep learning\"` 이 입력되었다고 가정\n        *   이 문장의 토큰: `[\"i\", \"also\", \"love\", \"deep\", \"learning\"]`\n        *   여기서 \"also\", \"deep\", \"learning\"은 기존 어휘 집합에 없는 새로운 단어(OOV)이다.\n        *   이런 OOV 단어를 처리하기 위해, 특별 토큰인 `\"UNK\"` (Unknown)를 어휘 집합에 추가하고, 이 `\"UNK\"` 토큰에 어휘 집합의 **가장 마지막 다음 번호**를 부여\n        *   업데이트된 어휘 집합 (`word_to_index`): `{\"i\": 1, \"love\": 2, \"natural\": 3, \"language\": 4, \"processing\": 5, \"UNK\": 6}`\n        *   어휘 집합의 크기: 6 (`UNK` 토큰 포함).\n        *   새로운 문장 `\"I also love deep learning\"`의 정수 시퀀스\n         *   OOV 단어인 \"also\", \"deep\", \"learning\"은 `\"UNK\"` 토큰의 인덱스인 6으로 매핑되어 `[1, 6, 2, 6, 6]` 이 된다.\n        *   이렇게 `UNK` 토큰을 사용하면 모델이 학습하지 않은 단어에 대해서도 일관된 처리가 가능하지만, 모든 OOV 단어가 하나의 인덱스로 매핑되므로 원래 단어의 정보는 일부 손실된다.\n*   **근본적 해결 시도**: 서브워드 토큰화 (BPE, WordPiece 등)는 단어를 더 작은 단위로 나눠 OOV 발생 빈도를 크게 줄임.\n\n## 패딩 (Padding)\n\n*   **개념**: 입력되는 텍스트 길이가 다르기 때문에 토큰화 후 서로 다른 길이의 정수 시퀀스들의 **길이를 동일하게 맞춰주는** 작업.\n*   **필요성**: 딥러닝 모델의 고정된 입력 크기 요구 충족, 배치 단위 병렬 처리 효율 증대.\n*   **`PAD` 토큰 사용**: 특별한 `PAD` 토큰 (주로 인덱스 0)을 사용.\n*   **방법**:\n    *   `post-padding` (뒷부분 채움): 시퀀스 뒤에 `PAD` 인덱스 추가. (일반적)\n        *   예: `[[1,2,3,4], [5,6]]` -> `maxlen=4` 가정 시 `[[1,2,3,4], [5,6,0,0]]`, 집합의 크기는 4\n    *   `pre-padding` (앞부분 채움): 시퀀스 앞에 `PAD` 인덱스 추가. (RNN 계열에서 마지막 정보 중요시할 때)\n        *   예: `[[1,2,3,4], [5,6]]` -> `maxlen=4` 가정 시 `[[1,2,3,4], [0,0,5,6]]`, 집합의 크기는 4\n*   **어휘 집합 크기**: `PAD` 토큰 사용 시 어휘 집합 크기에 영향\n*   **주의**: 과도한 패딩은 실제 정보 비율 낮춰 학습에 부정적 영향 가능.\n\n## 벡터화 (Vectorization)\n\n*   **정의**: 정수 인코딩 등 숫자 표현을 바탕으로, 각 텍스트 단위(단어, 문장, 문서)를 **숫자 벡터(Numeric Vector)로 변환**하는 과정.\n*   **목적**: 기계 학습 모델 처리 가능 형태 변환, 텍스트의 의미/문맥 정보 표현, 데이터 효율적 처리.\n\n### 통계적 방법\n\n* 신경망 미사용: 신경망을 사용하지 않는 전통적 통계 방식으로 벡터화\n* 문맥 미고려 방법 (Non-neural / Context-independent)\n* 각 단어를 주변 문맥과 독립적으로 고정된 벡터로 표현 (Sparse Vector). \n    \n#### 단어 벡터 표현 방법: 원-핫 인코딩 (One-Hot Encoding)\n\n*   어휘 집합 크기의 벡터에서, 해당 단어의 정수 인덱스 위치만 1이고 나머지는 모두 0인 벡터로 표현.\n*   예: 어휘집 `{\"apple\":0, \"banana\":1, \"cherry\":2, \"PAD\":3}` (4은 패딩용 가정), `vocab_size=4`\n*   `apple` (인덱스 0) -> `[1, 0, 0, 0]`\n*   `banana` (인덱스 1) -> `[0, 1, 0, 0]`\n*   `cherry` (인덱스 2) -> `[0, 0, 1, 0]`\n*   `PAD` (인덱스 3) -> `[0, 0, 0, 1]`\n*   장점: 단어 간 순서/크기에 의한 관계 없음 명확히 표현.\n*   단점:\n   * 차원의 저주: 어휘 집합 크면 벡터 차원 매우 커짐 (희소 벡터) -> 계산 비효율, 데이터 부족 문제.\n      * 차원의 저주란? 고차원 공간(많은 feature 또는 여기서는 매우 큰 어휘 집합으로 인한 고차원 벡터)으로 갈수록 데이터 포인트들이 해당 공간을 매우 드문드문(희소하게, sparsely) 채우게 되는 현상\n      * 희소성의 문제점:\n         * **거리 계산의 무의미화**: 고차원 공간에서는 대부분의 데이터 포인트들이 서로 멀리 떨어져 있게 되어, 유클리드 거리와 같은 전통적인 거리 척도가 의미를 잃어갑니다. 즉, 가장 가까운 이웃과 가장 먼 이웃 간의 거리 차이가 거의 없어지거나, 모든 점이 샘플링된 점들의 껍질(hull)에 가깝게 위치한다. 이는 최근접 이웃(Nearest Neighbor)과 같은 거리 기반 알고리즘의 성능을 저하시킵니다.\n         * **데이터 부족 심화**: 동일한 밀도로 데이터를 채우기 위해서는 차원이 증가할수록 기하급수적으로 더 많은 데이터가 필요합니다. 예를 들어, 1차원에서 10개의 구간을 커버하는데 10개의 데이터 포인트가 필요했다면, 10차원에서는 각 차원마다 10개의 구간을 커버하기 위해 \\(10^{10}\\)개의 데이터 포인트가 필요하게 된다. 현실적으로 이만큼의 데이터를 확보하기는 매우 어렵다.\n         * **과적합(Overfitting) 가능성 증가**: 제한된 데이터로 고차원 모델을 학습시키면, 모델이 실제 데이터의 분포보다는 학습 데이터의 노이즈에 과도하게 적응하여 새로운 데이터에 대한 일반화 성능이 떨어질 수 있다.\n         * **모델 학습의 어려움**: 데이터가 희소해지면, 의미 있는 패턴을 찾거나 변수 간의 관계를 모델링하는 것이 더욱 어려워지고, 모델의 복잡도에 비해 학습할 수 있는 정보가 부족해집니다.\n         * **단어 간 유사도 표현 불가**: 모든 단어 벡터 직교.\n*   **최근 동향**: 단점들로 인해 NLP 딥러닝에서는 단어 임베딩으로 대체되는 추세.\n\n#### 문서 벡터 표현 방법: 빈도 기반 방법 (Frequency-based Methods)\n\n* Document Term Matrix (DTM): 텍스트 마이닝과 자연어처리에서 문서 내 단어 빈도를 표현하는  **문서-단어 행렬의 데이터 구조**이다.\n   * 벡터가 단어 집합의 크기를 가지며 대부분의 원소가 0을 가진다.\n   * DTM의 기본 구조\n      * **행(rows)**: 각 문서 (document), 즉, 문서가 행벡터가 된다.\n      * **열(columns)**: 어휘집(vocabulary)의 각 단어 (term)\n      * **셀 값**: 해당 문서에서 해당 단어의 빈도 또는 가중치\n   * 수학적으로 표현하면 $M \\in \\mathbb{R}^{d \\times v}$ 형태의 행렬이며, 여기서 $d$ 는 문서 수, $v$ 는 어휘집 크기\n* DTM(행렬)의 셀 값을 채우는 방법\n   * **Bag-of-Words (BoW)**:\n      * Raw Count (단순 빈도): 단어를 한 가방에 넣고 흔들면 단어의 순서는 무의미해지기 때문에 단어가 등장한 빈도수를 벡터화하는 이론\n      * 표현: 각 문서는 어휘 집합 크기의 벡터로, 각 차원은 해당 단어의 빈도수 (또는 존재 유무 0/1).\n      * 예: 문서1: \"나는 바나나 사과 바나나\", 어휘집: `{\"나는\":0, \"바나나\":1, \"사과\":2}` -> `[1, 2, 1]`\n      * 장점: 단순하고 구현 용이.\n      * 단점: 어순 무시로 문맥 정보 손실, 단어 의미 모호성 해결 불가, 차원이 크고 희소(sparse)할 수 있음.\n      * DTM[i,j] = count(word_j in document_i)\n   * Binary (이진)\n      * DTM[i,j] = 1 if word_j appears in document_i, else 0\n      * {\"데이터\": 1, \"과학\": 1, \"분석\": 1}\n   * **TF (Term Frequency)**\n      * 특정 문서 내 특정 단어의 등장 빈도를 정규화하여 BoW로 표현된 벡터에 가중치를 주는 방법\n      * DTM[i,j] = TF(word_j, document_i)\n      * {\"데이터\": 2/4=0.5, \"과학\": 1/4=0.25, \"분석\": 1/4=0.25}\n   * **TF-IDF (Inverse Document Frequency)**  \n      * 여러 문서가 있을때 단어의 변별력을 측정하는 지표\n      * 문서의 유사도, 검색 시스템에서 검색 결과의 순위 등을 구하는데 사용\n      * 단어 빈도(TF)와 역문서 빈도(IDF)를 곱하여, 특정 문서 내 단어의 상대적 중요도를 가중치로 부여. \n      * 통계적 방법 (신경망 미사용)론 중 여전히 실무에서 괴장히 많이 쓰이는 벡터화 방법.           \n      * 결과값이 벡터이므로 신경망의 입력값으로도 사용될 수 있다.\n      * 문서를 벡터화 한다면 문서 간 유사도 구할 수 있다.\n      * 문서 간 유사도 구하면 가능한 작업\n         * 문서 클러스터링, 유사한 문서 찾기, 문서 분류 문제\n      * IDF: 특정 문서에만 자주 나오는 단어일수록 높은 값을 가짐. 전체 문서 중 해당 단어가 등장한 문서 수의 역수\n      * 예: \"the\" 같이 여러 문서에 자주 나오는 단어는 낮은 TF-IDF, 특정 주제 문서에만 나오는 전문용어는 높은 TF-IDF.\n      * 장점: BoW보다 단어의 중요도를 더 잘 반영 (예: 불용어의 영향력 감소).\n      * 단점: 어순 무시, 의미적 유사도 표현에는 여전히 한계.\n      * $\\text{IDF}(w_j) = \\log\\left(\\frac{N}{1+\\text{df}(w_j)}\\right)$\n      * 여기서:\n         * $N$: 전체 문서 수\n         * $\\text{df}(w_j)$: 단어 $w_j$가 등장한 문서 수\n         * 특정 단어가 문서2,3에서 각 각 등장할 때 특정단어가 문서2에서 10000번 등장했더라도 df의 값은 2가 된다.\n         * log 스케일: df값이 매우 크거나 작을 때 IDF값의 범위를 줄이기 위해 사용 (IDF값이 기하급수적으로 커질 수 있음)\n         * 불용어 등과 같이 자주 쓰이는 단어들은 비교적 자주 쓰이지 않는 단어들보다 최소 수십배 더 자주 등장한다.\n         * 비교적 자주 쓰이지 않는 단어들조차 희귀 단어들과 비교하면 최소 수백 배는 더 자주 등장하는 편이다. (is, a, I, the, and, ...)\n         * log를 씌우지 않으면 희귀 단어들에 엄청난 가중치가 부여될 위험이 있다.\n      * 높은 IDF: 적은 문서에만 등장 → 희귀하고 특별한 단어\n      * 낮은 IDF: 많은 문서에 등장 → 일반적이고 흔한 단어\n* 계산 예시\n\n```\n문서1: \"머신러닝 알고리즘 연구\"\n문서2: \"딥러닝 모델 개발\" \n문서3: \"데이터 과학 연구\"\n문서4: \"인공지능 연구 동향\"\n\n\"연구\": df=4 → IDF = log(4/4) = 0  (낮음 - 흔한 단어)\n\"머신러닝\": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)\n\"딥러닝\": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)\n\"데이터\": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)\n\"알고리즘\": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)\n\nTF만 사용할 경우 (문서1 기준):\n\"연구\": TF = 1/3 = 0.333\n\"머신러닝\": TF = 1/3 = 0.333\n\"알고리즘\": TF = 1/3 = 0.333   \n\nIDF (문서1 기준):\n\"연구\": IDF = log(4/4) = 0\n\"머신러닝\": IDF = log(4/2) = 0.693\n\"알고리즘\": IDF = log(4/2) = 0.693         \n\nTF-IDF 사용할 경우 (문서1 기준):\n\"연구\": TF × IDF = 0.333 × 0 = 0  (낮음 - 일반적 단어)\n\"머신러닝\": TF × IDF = 0.333 × 0.693 = 0.231  (높음 - 문서의 특징)\n\"알고리즘\": TF × IDF = 0.333 × 0.693 = 0.231  (높음 - 문서의 특징)\n→ 문서를 특징짓는 단어들이 강조됨\n```   \n\n* 구체적 예시\n   * **DTM with BoW:**\n      * 문서 $D_i$ 에 대해 BoW 벡터 $\\mathbf{x}_i = [x_{i1}, x_{i2}, ..., x_{iv}]$ 는 다음과 같이 정의:\n      $x_{ij} = \\text{count}(w_j, D_i)$\n\n      * 여기서 $\\text{count}(w_j, D_i)$는 문서 $D_i$에서 단어 $w_j$의 등장 횟수이다.\n      * **문서 집합:**\n         - 문서1: \"데이터 과학은 흥미로운 분야다\"\n         - 문서2: \"머신러닝은 데이터 과학의 핵심이다\"  \n         - 문서3: \"딥러닝도 머신러닝의 한 분야다\"\n      * **어휘집 구성:** {\"데이터\", \"과학은\", \"흥미로운\", \"분야다\", \"머신러닝은\", \"과학의\", \"핵심이다\", \"딥러닝도\", \"머신러닝의\", \"한\"}\n      * **DTM (BoW):**\n    \n      ```\n               데이터 과학은 흥미로운 분야다 머신러닝은 과학의 핵심이다 딥러닝도 머신러닝의 한\n      문서1        1     1      1      1       0      0      0      0       0    0\n      문서2        1     0      0      0       1      1      1      0       0    0  \n      문서3        0     0      0      1       0      0      0      1       1    1\n      ```\n\n      * **통계적 특성**\n         * **희소성(Sparsity)**: 대부분의 셀이 0인 희소 행렬\n         * **차원의 저주**: 어휘집 크기가 클수록 벡터 차원이 증가\n         * **코사인 유사도**: 문서 간 유사도 측정에 자주 사용\n      $$\\text{cosine\\_similarity}(D_i, D_j) = \\frac{\\mathbf{x}_i \\cdot \\mathbf{x}_j}{||\\mathbf{x}_i|| \\cdot ||\\mathbf{x}_j||}$$\n\n   * **DTM with TF-IDF**\n      * TF-IDF는 두 구성요소의 곱으로 정의:\n         * $\\text{TF-IDF}(w_j, D_i) = \\text{TF}(w_j, D_i) \\times \\text{IDF}(w_j)$\n\n      * Term Frequency (TF)\n         * $\\text{TF}(w_j, D_i) = \\frac{\\text{count}(w_j, D_i)}{\\sum_{k=1}^{|D_i|} \\text{count}(w_k, D_i)}$\n         * 또는 로그 정규화 (가장 보편적으로 사용):\n            * $\\text{TF}(w_j, D_i) = \\log(1 + \\text{count}(w_j, D_i))$\n      * Inverse Document Frequency (IDF)\n         * $\\text{IDF}(w_j) = \\log\\left(\\frac{N}{\\text{df}(w_j)}\\right)$\n         * 여기서:\n            * $N$ : 전체 문서 수\n            * $\\text{df}(w_j)$ : 단어 $w_j$가 등장한 문서 수\n      * 예시 (위의 동일한 문서 집합 사용):\n         * **1단계: TF 계산 (로그 정규화 사용)**\n            * TF(\"데이터\") = 1/4 = 0.25\n            * TF(\"과학은\") = 1/4 = 0.25  \n            * TF(\"흥미로운\") = 1/4 = 0.25\n            * TF(\"분야다\") = 1/4 = 0.25\n            * TF(\"머신러닝은\") = 1/4 = 0.25\n            * TF(\"데이터\") = 1/4 = 0.25\n            * TF(\"과학의\") = 1/4 = 0.25  \n            * TF(\"핵심이다\") = 1/4 = 0.25\n            * TF(\"딥러닝도\") = 1/4 = 0.25\n            * TF(\"머신러닝의\") = 1/4 = 0.25\n            * TF(\"한\") = 1/4 = 0.25\n            * TF(\"분야다\") = 1/4 = 0.25\n\n               ```\n                        데이터  과학은  흥미로운  분야다  머신러닝은  과학의  핵심이다  딥러닝도  머신러닝의  한\n               문서1      0.25    0.25    0.25    0.25     0       0       0       0       0       0\n               문서2      0.25    0       0       0      0.25     0.25    0.25     0        0      0\n               문서3       0       0       0      0.25     0        0       0     0.25    0.25  0.25\n               ```\n\n         * **2단계: IDF 계산**\n            * \"데이터\": 문서1, 문서2 → df = 2\n            * \"분야다\": 문서1, 문서3 → df = 2  \n            * \"과학은\": 문서1만 → df = 1\n            * \"흥미로운\": 문서1만 → df = 1\n            * \"머신러닝은\": 문서2만 → df = 1\n            * \"과학의\": 문서2만 → df = 1\n            * \"핵심이다\": 문서2만 → df = 1\n            * \"딥러닝도\": 문서3만 → df = 1\n            * \"머신러닝의\": 문서3만 → df = 1  \n            * \"한\": 문서3만 → df = 1\n            * IDF(\"데이터\") = log(3/(1+2)) = log(1) = 0.000\n            * IDF(\"분야다\") = log(3/(1+2)) = log(1) = 0.000\n            * IDF(\"과학은\") = log(3/(1+1)) = log(1.5) = 0.405\n            * IDF(\"흥미로운\") = log(3/(1+1)) = log(1.5) = 0.405  \n            * IDF(\"머신러닝은\") = log(3/(1+1)) = log(1.5) = 0.405\n            * IDF(\"과학의\") = log(3/(1+1)) = log(1.5) = 0.405\n            * IDF(\"핵심이다\") = log(3/(1+1)) = log(1.5) = 0.405\n            * IDF(\"딥러닝도\") = log(3/(1+1)) = log(1.5) = 0.405\n            * IDF(\"머신러닝의\") = log(3/(1+1)) = log(1.5) = 0.405\n            * IDF(\"한\") = log(3/(1+1)) = log(1.5) = 0.405\n         \n         * **3단계: TF-IDF 최종 계산**\n            \n            ```\n                     데이터   과학은   흥미로운  분야다   머신러닝은  과학의   핵심이다  딥러닝도  머신러닝의   한\n            문서1      0     0.101    0.101   0.101      0        0        0       0        0      0\n            문서2      0       0        0       0       0.081    0.081    0.081    0        0      0\n            문서3      0       0        0       0        0        0        0     0.081    0.081  0.081\n            ```\n\n      * TF-IDF의 통계적 해석\n         * **정보 이론적 관점**: IDF는 단어의 정보량(information content)을 측정\n            * $\\text{Information}(w_j) = -\\log P(w_j) \\approx -\\log\\left(\\frac{\\text{df}(w_j)}{N}\\right) = \\text{IDF}(w_j)$\n         * **가중치 효과**: \n            * 높은 TF: 해당 문서에서 **중요한 단어**\n            * 높은 IDF: 전체 문서 집합에서 **희귀한 단어**\n            * 높은 TF-IDF: 특정 문서의 **특징을 잘 나타내는 중요한 단어**\n         * **정규화 효과**: 문서 길이에 따른 편향을 줄임\n\n   * 실용적 고려사항\n      * **불용어 처리**: \"은\", \"는\", \"이\" 등은 보통 전처리 단계에서 제거\n      * **최소 문서 빈도**: 너무 희귀한 단어들을 필터링하여 차원 축소\n      * **최대 문서 빈도**: 너무 일반적인 단어들도 제외 가능\n\n## 결론\n\n본 문서에서는 자연어 처리(NLP)의 핵심 전처리 단계인 텍스트 인코딩과 벡터화 기법들을 살펴보았다. 주요 내용을 요약하면 다음과 같다.\n\n*   **텍스트 인코딩의 중요성**: 기계가 텍스트를 이해하기 위한 첫걸음으로, 정수 인코딩과 같은 방법을 통해 토큰을 숫자 표현으로 변환합니다. 이때 어휘 집합에 없는 단어(OOV) 처리가 중요하며, `UNK` 토큰 사용이 일반적인 해결책이다.\n*   **일관된 입력 처리를 위한 패딩**: 모델의 고정된 입력 크기 요구를 충족시키기 위해, 다양한 길이의 시퀀스를 `PAD` 토큰을 사용하여 동일한 길이로 맞추는 패딩 작업이 필수적이다.\n*   **다양한 벡터화 기법**:\n    *   **통계 기반 방법**: 원-핫 인코딩은 간단하지만 차원의 저주 문제가 있으며, 단어 빈도 기반의 DTM, BoW, TF-IDF 등은 문서 수준의 벡터 표현에 효과적이다. 특히 TF-IDF는 단어의 중요도를 반영하여 널리 사용된다.\n    *   **벡터 표현의 의미**: 이러한 기법들은 단어를 벡터로, 문서를 벡터 또는 행렬로 변환하여 기계 학습 모델이 처리할 수 있도록 한다.\n*   **방법 선택의 기준**: 최적의 인코딩 및 벡터화 전략은 당면한 문제의 특성, 데이터의 양과 질, 그리고 사용하려는 모델 등을 종합적으로 고려하여 선택해야 한다.\n","srcMarkdownNoYaml":"\n\n# 이 문서 한눈에 보기\n\n이 문서는 자연어 처리(NLP)를 위해 텍스트 데이터를 기계가 이해할 수 있는 숫자 형태로 변환하는 주요 과정인 **인코딩(Encoding)**과 **벡터화(Vectorization)**에 대해 심층적으로 다룬다.\n\n주요 내용은 다음과 같다.\n\n*   **인코딩 (Encoding)**:\n    *   **정의 및 필요성**: 텍스트(문자열)를 모델이 처리할 수 있는 형태로 바꾸는 모든 과정으로 텍스트를 숫자(주로 정수 인덱스)로 변환하는 과정이다.\n    *   **정수 인코딩**: 각 단어에 고유 정수를 부여하는 방법과 어휘 집합 크기 결정, 그리고 학습 데이터에 없는 단어(OOV: Out-of-Vocabulary)를 `UNK` 토큰으로 처리하는 방법을 설명한다.\n*   **패딩 (Padding)**:\n    *   **정의 및 필요성**: 길이가 다른 텍스트 시퀀스들을 모델 입력을 위해 동일한 길이로 맞춰주는 작업으로, `PAD` 토큰을 사용한다.\n*   **벡터화 (Vectorization)**:\n    *   **정의**: 정수 인코딩된 데이터를 숫자 벡터로 변환하여 텍스트의 의미나 통계적 정보를 표현한다.\n    *   **통계적 방법**:\n        *   **원-핫 인코딩**: 각 단어를 고유한 희소 벡터로 표현하며, 장단점(차원의 저주)을 다룬다.\n        *   **빈도 기반 방법 (DTM, BoW, TF-IDF)**: 문서 내 단어 빈도를 기반으로 문서를 벡터화하는 DTM(Document Term Matrix), Bag-of-Words(BoW), TF-IDF 기법의 개념과 특징, 활용 방안을 설명한다.\n    *   **벡터 표현의 단위**: 단어는 벡터로, 문서는 벡터 또는 행렬로 표현될 수 있음을 설명한다.\n*   **결론**: 효과적인 인코딩 및 벡터화 전략 선택의 중요성을 강조한다.\n\n이 문서를 통해 텍스트 데이터가 NLP 모델에서 어떻게 처리될 수 있도록 준비되는지에 대한 기본적인 이해를 얻을 수 있다.\n\n# 텍스트 인코딩 및 벡터화\n\n```\n텍스트 벡터화\n├── 1. 전통적 방법 (통계 기반)\n│   ├── BoW\n│   ├── DTM\n│   └── TF-IDF\n│\n├── 2. 신경망 기반 (문맥 독립)\n│   ├── 문맥 독립적 임베딩\n│   │   └── Embedding Layer (딥러닝 모델 내 구성 요소)\n│   ├── Word2Vec (CBOW, Skip-gram)\n│   ├── FastText\n│   ├── GloVe\n│   └── 기타 모델: Swivel, LexVec 등\n│\n└── 3. 문맥 기반 임베딩 (Contextual Embedding)\n    └── RNN 계열\n        ├── LSTM\n        ├── GRU\n        └── ELMo\n```\n\n## 인코딩(Encoding)\n\n*   **인코딩 (Encoding)**:\n    *   **정의**: 자연어 처리(NLP)에서 **텍스트 데이터를 기계가 이해하고 처리할 수 있는 숫자 형태로 변환하는 과정** 전반을 의미. 컴퓨터는 텍스트를 직접 이해할 수 없으므로, 토큰화된 각 구성 요소(단어 등)를 숫자 표현(주로 정수 인덱스)으로 바꾸는 단계.\n    *   **핵심**: 토큰화 후, 토큰들을 숫자로 매핑.\n*   **주요 기법**\n   * 정수 인코딩: 보통 사전 정의된 정수, 의미 보존 x, 비학습, 코사인 유사도 측정 불가\n      * 예: 원-핫 인코딩, Indexing, 빈도 기반 인코딩 등\n   * embedding: 학습 가능, 코사인 유사도 측정 가능, 문맥 표현 가능\n      * 예: word2vec, fasttext, glove 등.\n\n### 정수 인코딩 (Integer Encoding)\n\n*   **개념**: 어휘 집합(Vocabulary) 내 각 고유 토큰에 **고유한 정수 인덱스를 부여**.\n*   **과정**:\n    1.  어휘 집합 구축: 전체 텍스트에서 고유 토큰 추출 \n      * 빈도, 최대 크기 고려하여 어휘 집합 크기 제한하는 것이 현실적인 전략\n    2.  각 토큰에 정수 할당\n      * 빈도 높은 단어에 낮은 숫자 할당\n    3.  텍스트의 토큰 시퀀스를 정수 시퀀스로 변환\n* **예시**:\n   * 어휘 집합 구축:\n       *   예시 문장: `\"I love natural language processing\"`\n       *   토큰화 결과: `[\"i\", \"love\", \"natural\", \"language\", \"processing\"]`\n       *   초기 어휘 집합 (`word_to_index`)의 정수 인코딩\n         * `{\"i\": 1, \"love\": 2, \"natural\": 3, \"language\": 4, \"processing\": 5}`\n       *   정수 시퀀스: `[1, 2, 3, 4, 5]`\n       *   이때 어휘 집합의 크기는 5이다.\n*   **한계**\n    *   텍스트간 유사도 측정 불가: 숫자 값 자체가 단어 간 의미/관계 표현 못 함 (모델 오해 가능성).\n\n### OOV (Out-of-Vocabulary) 문제\n\n*   **정의**: 학습 시 구축된 어휘 집합에 **포함되지 않은 단어**가 입력될 때 발생.\n*   **원인**: 신조어, 전문 용어, 오타, 제한된 어휘 크기 등.\n*   **영향**: 정보 손실, 잘못된 예측, 모델 성능 저하.\n*   **`UNK` (Unknown) 토큰 처리**:\n    *   OOV 단어를 미리 정의된 `UNK` 토큰의 인덱스로 일괄 대체.\n    *   모든 OOV가 동일 토큰으로 매핑되어 원래 단어의 고유 정보는 손실.\n*   **어휘 집합 크기 및 신규 단어(OOV) 처리 예시 (영문)**:\n    *   어휘 집합의 크기는 모델의 성능과 효율성에 영향을 미치며, 너무 작으면 OOV 문제가, 너무 크면 계산 비용 및 과적합 문제가 발생할 수 있다.\n    *   **신규 단어(OOV) 발생 시 `UNK` 토큰 처리**:\n        *   기존 예시 문장: `\"I love natural language processing\"`\n        *   새로운 문장: `\"I also love deep learning\"` 이 입력되었다고 가정\n        *   이 문장의 토큰: `[\"i\", \"also\", \"love\", \"deep\", \"learning\"]`\n        *   여기서 \"also\", \"deep\", \"learning\"은 기존 어휘 집합에 없는 새로운 단어(OOV)이다.\n        *   이런 OOV 단어를 처리하기 위해, 특별 토큰인 `\"UNK\"` (Unknown)를 어휘 집합에 추가하고, 이 `\"UNK\"` 토큰에 어휘 집합의 **가장 마지막 다음 번호**를 부여\n        *   업데이트된 어휘 집합 (`word_to_index`): `{\"i\": 1, \"love\": 2, \"natural\": 3, \"language\": 4, \"processing\": 5, \"UNK\": 6}`\n        *   어휘 집합의 크기: 6 (`UNK` 토큰 포함).\n        *   새로운 문장 `\"I also love deep learning\"`의 정수 시퀀스\n         *   OOV 단어인 \"also\", \"deep\", \"learning\"은 `\"UNK\"` 토큰의 인덱스인 6으로 매핑되어 `[1, 6, 2, 6, 6]` 이 된다.\n        *   이렇게 `UNK` 토큰을 사용하면 모델이 학습하지 않은 단어에 대해서도 일관된 처리가 가능하지만, 모든 OOV 단어가 하나의 인덱스로 매핑되므로 원래 단어의 정보는 일부 손실된다.\n*   **근본적 해결 시도**: 서브워드 토큰화 (BPE, WordPiece 등)는 단어를 더 작은 단위로 나눠 OOV 발생 빈도를 크게 줄임.\n\n## 패딩 (Padding)\n\n*   **개념**: 입력되는 텍스트 길이가 다르기 때문에 토큰화 후 서로 다른 길이의 정수 시퀀스들의 **길이를 동일하게 맞춰주는** 작업.\n*   **필요성**: 딥러닝 모델의 고정된 입력 크기 요구 충족, 배치 단위 병렬 처리 효율 증대.\n*   **`PAD` 토큰 사용**: 특별한 `PAD` 토큰 (주로 인덱스 0)을 사용.\n*   **방법**:\n    *   `post-padding` (뒷부분 채움): 시퀀스 뒤에 `PAD` 인덱스 추가. (일반적)\n        *   예: `[[1,2,3,4], [5,6]]` -> `maxlen=4` 가정 시 `[[1,2,3,4], [5,6,0,0]]`, 집합의 크기는 4\n    *   `pre-padding` (앞부분 채움): 시퀀스 앞에 `PAD` 인덱스 추가. (RNN 계열에서 마지막 정보 중요시할 때)\n        *   예: `[[1,2,3,4], [5,6]]` -> `maxlen=4` 가정 시 `[[1,2,3,4], [0,0,5,6]]`, 집합의 크기는 4\n*   **어휘 집합 크기**: `PAD` 토큰 사용 시 어휘 집합 크기에 영향\n*   **주의**: 과도한 패딩은 실제 정보 비율 낮춰 학습에 부정적 영향 가능.\n\n## 벡터화 (Vectorization)\n\n*   **정의**: 정수 인코딩 등 숫자 표현을 바탕으로, 각 텍스트 단위(단어, 문장, 문서)를 **숫자 벡터(Numeric Vector)로 변환**하는 과정.\n*   **목적**: 기계 학습 모델 처리 가능 형태 변환, 텍스트의 의미/문맥 정보 표현, 데이터 효율적 처리.\n\n### 통계적 방법\n\n* 신경망 미사용: 신경망을 사용하지 않는 전통적 통계 방식으로 벡터화\n* 문맥 미고려 방법 (Non-neural / Context-independent)\n* 각 단어를 주변 문맥과 독립적으로 고정된 벡터로 표현 (Sparse Vector). \n    \n#### 단어 벡터 표현 방법: 원-핫 인코딩 (One-Hot Encoding)\n\n*   어휘 집합 크기의 벡터에서, 해당 단어의 정수 인덱스 위치만 1이고 나머지는 모두 0인 벡터로 표현.\n*   예: 어휘집 `{\"apple\":0, \"banana\":1, \"cherry\":2, \"PAD\":3}` (4은 패딩용 가정), `vocab_size=4`\n*   `apple` (인덱스 0) -> `[1, 0, 0, 0]`\n*   `banana` (인덱스 1) -> `[0, 1, 0, 0]`\n*   `cherry` (인덱스 2) -> `[0, 0, 1, 0]`\n*   `PAD` (인덱스 3) -> `[0, 0, 0, 1]`\n*   장점: 단어 간 순서/크기에 의한 관계 없음 명확히 표현.\n*   단점:\n   * 차원의 저주: 어휘 집합 크면 벡터 차원 매우 커짐 (희소 벡터) -> 계산 비효율, 데이터 부족 문제.\n      * 차원의 저주란? 고차원 공간(많은 feature 또는 여기서는 매우 큰 어휘 집합으로 인한 고차원 벡터)으로 갈수록 데이터 포인트들이 해당 공간을 매우 드문드문(희소하게, sparsely) 채우게 되는 현상\n      * 희소성의 문제점:\n         * **거리 계산의 무의미화**: 고차원 공간에서는 대부분의 데이터 포인트들이 서로 멀리 떨어져 있게 되어, 유클리드 거리와 같은 전통적인 거리 척도가 의미를 잃어갑니다. 즉, 가장 가까운 이웃과 가장 먼 이웃 간의 거리 차이가 거의 없어지거나, 모든 점이 샘플링된 점들의 껍질(hull)에 가깝게 위치한다. 이는 최근접 이웃(Nearest Neighbor)과 같은 거리 기반 알고리즘의 성능을 저하시킵니다.\n         * **데이터 부족 심화**: 동일한 밀도로 데이터를 채우기 위해서는 차원이 증가할수록 기하급수적으로 더 많은 데이터가 필요합니다. 예를 들어, 1차원에서 10개의 구간을 커버하는데 10개의 데이터 포인트가 필요했다면, 10차원에서는 각 차원마다 10개의 구간을 커버하기 위해 \\(10^{10}\\)개의 데이터 포인트가 필요하게 된다. 현실적으로 이만큼의 데이터를 확보하기는 매우 어렵다.\n         * **과적합(Overfitting) 가능성 증가**: 제한된 데이터로 고차원 모델을 학습시키면, 모델이 실제 데이터의 분포보다는 학습 데이터의 노이즈에 과도하게 적응하여 새로운 데이터에 대한 일반화 성능이 떨어질 수 있다.\n         * **모델 학습의 어려움**: 데이터가 희소해지면, 의미 있는 패턴을 찾거나 변수 간의 관계를 모델링하는 것이 더욱 어려워지고, 모델의 복잡도에 비해 학습할 수 있는 정보가 부족해집니다.\n         * **단어 간 유사도 표현 불가**: 모든 단어 벡터 직교.\n*   **최근 동향**: 단점들로 인해 NLP 딥러닝에서는 단어 임베딩으로 대체되는 추세.\n\n#### 문서 벡터 표현 방법: 빈도 기반 방법 (Frequency-based Methods)\n\n* Document Term Matrix (DTM): 텍스트 마이닝과 자연어처리에서 문서 내 단어 빈도를 표현하는  **문서-단어 행렬의 데이터 구조**이다.\n   * 벡터가 단어 집합의 크기를 가지며 대부분의 원소가 0을 가진다.\n   * DTM의 기본 구조\n      * **행(rows)**: 각 문서 (document), 즉, 문서가 행벡터가 된다.\n      * **열(columns)**: 어휘집(vocabulary)의 각 단어 (term)\n      * **셀 값**: 해당 문서에서 해당 단어의 빈도 또는 가중치\n   * 수학적으로 표현하면 $M \\in \\mathbb{R}^{d \\times v}$ 형태의 행렬이며, 여기서 $d$ 는 문서 수, $v$ 는 어휘집 크기\n* DTM(행렬)의 셀 값을 채우는 방법\n   * **Bag-of-Words (BoW)**:\n      * Raw Count (단순 빈도): 단어를 한 가방에 넣고 흔들면 단어의 순서는 무의미해지기 때문에 단어가 등장한 빈도수를 벡터화하는 이론\n      * 표현: 각 문서는 어휘 집합 크기의 벡터로, 각 차원은 해당 단어의 빈도수 (또는 존재 유무 0/1).\n      * 예: 문서1: \"나는 바나나 사과 바나나\", 어휘집: `{\"나는\":0, \"바나나\":1, \"사과\":2}` -> `[1, 2, 1]`\n      * 장점: 단순하고 구현 용이.\n      * 단점: 어순 무시로 문맥 정보 손실, 단어 의미 모호성 해결 불가, 차원이 크고 희소(sparse)할 수 있음.\n      * DTM[i,j] = count(word_j in document_i)\n   * Binary (이진)\n      * DTM[i,j] = 1 if word_j appears in document_i, else 0\n      * {\"데이터\": 1, \"과학\": 1, \"분석\": 1}\n   * **TF (Term Frequency)**\n      * 특정 문서 내 특정 단어의 등장 빈도를 정규화하여 BoW로 표현된 벡터에 가중치를 주는 방법\n      * DTM[i,j] = TF(word_j, document_i)\n      * {\"데이터\": 2/4=0.5, \"과학\": 1/4=0.25, \"분석\": 1/4=0.25}\n   * **TF-IDF (Inverse Document Frequency)**  \n      * 여러 문서가 있을때 단어의 변별력을 측정하는 지표\n      * 문서의 유사도, 검색 시스템에서 검색 결과의 순위 등을 구하는데 사용\n      * 단어 빈도(TF)와 역문서 빈도(IDF)를 곱하여, 특정 문서 내 단어의 상대적 중요도를 가중치로 부여. \n      * 통계적 방법 (신경망 미사용)론 중 여전히 실무에서 괴장히 많이 쓰이는 벡터화 방법.           \n      * 결과값이 벡터이므로 신경망의 입력값으로도 사용될 수 있다.\n      * 문서를 벡터화 한다면 문서 간 유사도 구할 수 있다.\n      * 문서 간 유사도 구하면 가능한 작업\n         * 문서 클러스터링, 유사한 문서 찾기, 문서 분류 문제\n      * IDF: 특정 문서에만 자주 나오는 단어일수록 높은 값을 가짐. 전체 문서 중 해당 단어가 등장한 문서 수의 역수\n      * 예: \"the\" 같이 여러 문서에 자주 나오는 단어는 낮은 TF-IDF, 특정 주제 문서에만 나오는 전문용어는 높은 TF-IDF.\n      * 장점: BoW보다 단어의 중요도를 더 잘 반영 (예: 불용어의 영향력 감소).\n      * 단점: 어순 무시, 의미적 유사도 표현에는 여전히 한계.\n      * $\\text{IDF}(w_j) = \\log\\left(\\frac{N}{1+\\text{df}(w_j)}\\right)$\n      * 여기서:\n         * $N$: 전체 문서 수\n         * $\\text{df}(w_j)$: 단어 $w_j$가 등장한 문서 수\n         * 특정 단어가 문서2,3에서 각 각 등장할 때 특정단어가 문서2에서 10000번 등장했더라도 df의 값은 2가 된다.\n         * log 스케일: df값이 매우 크거나 작을 때 IDF값의 범위를 줄이기 위해 사용 (IDF값이 기하급수적으로 커질 수 있음)\n         * 불용어 등과 같이 자주 쓰이는 단어들은 비교적 자주 쓰이지 않는 단어들보다 최소 수십배 더 자주 등장한다.\n         * 비교적 자주 쓰이지 않는 단어들조차 희귀 단어들과 비교하면 최소 수백 배는 더 자주 등장하는 편이다. (is, a, I, the, and, ...)\n         * log를 씌우지 않으면 희귀 단어들에 엄청난 가중치가 부여될 위험이 있다.\n      * 높은 IDF: 적은 문서에만 등장 → 희귀하고 특별한 단어\n      * 낮은 IDF: 많은 문서에 등장 → 일반적이고 흔한 단어\n* 계산 예시\n\n```\n문서1: \"머신러닝 알고리즘 연구\"\n문서2: \"딥러닝 모델 개발\" \n문서3: \"데이터 과학 연구\"\n문서4: \"인공지능 연구 동향\"\n\n\"연구\": df=4 → IDF = log(4/4) = 0  (낮음 - 흔한 단어)\n\"머신러닝\": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)\n\"딥러닝\": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)\n\"데이터\": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)\n\"알고리즘\": df=2 → IDF = log(4/2) = 0.693  (높음 - 희귀한 단어)\n\nTF만 사용할 경우 (문서1 기준):\n\"연구\": TF = 1/3 = 0.333\n\"머신러닝\": TF = 1/3 = 0.333\n\"알고리즘\": TF = 1/3 = 0.333   \n\nIDF (문서1 기준):\n\"연구\": IDF = log(4/4) = 0\n\"머신러닝\": IDF = log(4/2) = 0.693\n\"알고리즘\": IDF = log(4/2) = 0.693         \n\nTF-IDF 사용할 경우 (문서1 기준):\n\"연구\": TF × IDF = 0.333 × 0 = 0  (낮음 - 일반적 단어)\n\"머신러닝\": TF × IDF = 0.333 × 0.693 = 0.231  (높음 - 문서의 특징)\n\"알고리즘\": TF × IDF = 0.333 × 0.693 = 0.231  (높음 - 문서의 특징)\n→ 문서를 특징짓는 단어들이 강조됨\n```   \n\n* 구체적 예시\n   * **DTM with BoW:**\n      * 문서 $D_i$ 에 대해 BoW 벡터 $\\mathbf{x}_i = [x_{i1}, x_{i2}, ..., x_{iv}]$ 는 다음과 같이 정의:\n      $x_{ij} = \\text{count}(w_j, D_i)$\n\n      * 여기서 $\\text{count}(w_j, D_i)$는 문서 $D_i$에서 단어 $w_j$의 등장 횟수이다.\n      * **문서 집합:**\n         - 문서1: \"데이터 과학은 흥미로운 분야다\"\n         - 문서2: \"머신러닝은 데이터 과학의 핵심이다\"  \n         - 문서3: \"딥러닝도 머신러닝의 한 분야다\"\n      * **어휘집 구성:** {\"데이터\", \"과학은\", \"흥미로운\", \"분야다\", \"머신러닝은\", \"과학의\", \"핵심이다\", \"딥러닝도\", \"머신러닝의\", \"한\"}\n      * **DTM (BoW):**\n    \n      ```\n               데이터 과학은 흥미로운 분야다 머신러닝은 과학의 핵심이다 딥러닝도 머신러닝의 한\n      문서1        1     1      1      1       0      0      0      0       0    0\n      문서2        1     0      0      0       1      1      1      0       0    0  \n      문서3        0     0      0      1       0      0      0      1       1    1\n      ```\n\n      * **통계적 특성**\n         * **희소성(Sparsity)**: 대부분의 셀이 0인 희소 행렬\n         * **차원의 저주**: 어휘집 크기가 클수록 벡터 차원이 증가\n         * **코사인 유사도**: 문서 간 유사도 측정에 자주 사용\n      $$\\text{cosine\\_similarity}(D_i, D_j) = \\frac{\\mathbf{x}_i \\cdot \\mathbf{x}_j}{||\\mathbf{x}_i|| \\cdot ||\\mathbf{x}_j||}$$\n\n   * **DTM with TF-IDF**\n      * TF-IDF는 두 구성요소의 곱으로 정의:\n         * $\\text{TF-IDF}(w_j, D_i) = \\text{TF}(w_j, D_i) \\times \\text{IDF}(w_j)$\n\n      * Term Frequency (TF)\n         * $\\text{TF}(w_j, D_i) = \\frac{\\text{count}(w_j, D_i)}{\\sum_{k=1}^{|D_i|} \\text{count}(w_k, D_i)}$\n         * 또는 로그 정규화 (가장 보편적으로 사용):\n            * $\\text{TF}(w_j, D_i) = \\log(1 + \\text{count}(w_j, D_i))$\n      * Inverse Document Frequency (IDF)\n         * $\\text{IDF}(w_j) = \\log\\left(\\frac{N}{\\text{df}(w_j)}\\right)$\n         * 여기서:\n            * $N$ : 전체 문서 수\n            * $\\text{df}(w_j)$ : 단어 $w_j$가 등장한 문서 수\n      * 예시 (위의 동일한 문서 집합 사용):\n         * **1단계: TF 계산 (로그 정규화 사용)**\n            * TF(\"데이터\") = 1/4 = 0.25\n            * TF(\"과학은\") = 1/4 = 0.25  \n            * TF(\"흥미로운\") = 1/4 = 0.25\n            * TF(\"분야다\") = 1/4 = 0.25\n            * TF(\"머신러닝은\") = 1/4 = 0.25\n            * TF(\"데이터\") = 1/4 = 0.25\n            * TF(\"과학의\") = 1/4 = 0.25  \n            * TF(\"핵심이다\") = 1/4 = 0.25\n            * TF(\"딥러닝도\") = 1/4 = 0.25\n            * TF(\"머신러닝의\") = 1/4 = 0.25\n            * TF(\"한\") = 1/4 = 0.25\n            * TF(\"분야다\") = 1/4 = 0.25\n\n               ```\n                        데이터  과학은  흥미로운  분야다  머신러닝은  과학의  핵심이다  딥러닝도  머신러닝의  한\n               문서1      0.25    0.25    0.25    0.25     0       0       0       0       0       0\n               문서2      0.25    0       0       0      0.25     0.25    0.25     0        0      0\n               문서3       0       0       0      0.25     0        0       0     0.25    0.25  0.25\n               ```\n\n         * **2단계: IDF 계산**\n            * \"데이터\": 문서1, 문서2 → df = 2\n            * \"분야다\": 문서1, 문서3 → df = 2  \n            * \"과학은\": 문서1만 → df = 1\n            * \"흥미로운\": 문서1만 → df = 1\n            * \"머신러닝은\": 문서2만 → df = 1\n            * \"과학의\": 문서2만 → df = 1\n            * \"핵심이다\": 문서2만 → df = 1\n            * \"딥러닝도\": 문서3만 → df = 1\n            * \"머신러닝의\": 문서3만 → df = 1  \n            * \"한\": 문서3만 → df = 1\n            * IDF(\"데이터\") = log(3/(1+2)) = log(1) = 0.000\n            * IDF(\"분야다\") = log(3/(1+2)) = log(1) = 0.000\n            * IDF(\"과학은\") = log(3/(1+1)) = log(1.5) = 0.405\n            * IDF(\"흥미로운\") = log(3/(1+1)) = log(1.5) = 0.405  \n            * IDF(\"머신러닝은\") = log(3/(1+1)) = log(1.5) = 0.405\n            * IDF(\"과학의\") = log(3/(1+1)) = log(1.5) = 0.405\n            * IDF(\"핵심이다\") = log(3/(1+1)) = log(1.5) = 0.405\n            * IDF(\"딥러닝도\") = log(3/(1+1)) = log(1.5) = 0.405\n            * IDF(\"머신러닝의\") = log(3/(1+1)) = log(1.5) = 0.405\n            * IDF(\"한\") = log(3/(1+1)) = log(1.5) = 0.405\n         \n         * **3단계: TF-IDF 최종 계산**\n            \n            ```\n                     데이터   과학은   흥미로운  분야다   머신러닝은  과학의   핵심이다  딥러닝도  머신러닝의   한\n            문서1      0     0.101    0.101   0.101      0        0        0       0        0      0\n            문서2      0       0        0       0       0.081    0.081    0.081    0        0      0\n            문서3      0       0        0       0        0        0        0     0.081    0.081  0.081\n            ```\n\n      * TF-IDF의 통계적 해석\n         * **정보 이론적 관점**: IDF는 단어의 정보량(information content)을 측정\n            * $\\text{Information}(w_j) = -\\log P(w_j) \\approx -\\log\\left(\\frac{\\text{df}(w_j)}{N}\\right) = \\text{IDF}(w_j)$\n         * **가중치 효과**: \n            * 높은 TF: 해당 문서에서 **중요한 단어**\n            * 높은 IDF: 전체 문서 집합에서 **희귀한 단어**\n            * 높은 TF-IDF: 특정 문서의 **특징을 잘 나타내는 중요한 단어**\n         * **정규화 효과**: 문서 길이에 따른 편향을 줄임\n\n   * 실용적 고려사항\n      * **불용어 처리**: \"은\", \"는\", \"이\" 등은 보통 전처리 단계에서 제거\n      * **최소 문서 빈도**: 너무 희귀한 단어들을 필터링하여 차원 축소\n      * **최대 문서 빈도**: 너무 일반적인 단어들도 제외 가능\n\n## 결론\n\n본 문서에서는 자연어 처리(NLP)의 핵심 전처리 단계인 텍스트 인코딩과 벡터화 기법들을 살펴보았다. 주요 내용을 요약하면 다음과 같다.\n\n*   **텍스트 인코딩의 중요성**: 기계가 텍스트를 이해하기 위한 첫걸음으로, 정수 인코딩과 같은 방법을 통해 토큰을 숫자 표현으로 변환합니다. 이때 어휘 집합에 없는 단어(OOV) 처리가 중요하며, `UNK` 토큰 사용이 일반적인 해결책이다.\n*   **일관된 입력 처리를 위한 패딩**: 모델의 고정된 입력 크기 요구를 충족시키기 위해, 다양한 길이의 시퀀스를 `PAD` 토큰을 사용하여 동일한 길이로 맞추는 패딩 작업이 필수적이다.\n*   **다양한 벡터화 기법**:\n    *   **통계 기반 방법**: 원-핫 인코딩은 간단하지만 차원의 저주 문제가 있으며, 단어 빈도 기반의 DTM, BoW, TF-IDF 등은 문서 수준의 벡터 표현에 효과적이다. 특히 TF-IDF는 단어의 중요도를 반영하여 널리 사용된다.\n    *   **벡터 표현의 의미**: 이러한 기법들은 단어를 벡터로, 문서를 벡터 또는 행렬로 변환하여 기계 학습 모델이 처리할 수 있도록 한다.\n*   **방법 선택의 기준**: 최적의 인코딩 및 벡터화 전략은 당면한 문제의 특성, 데이터의 양과 질, 그리고 사용하려는 모델 등을 종합적으로 고려하여 선택해야 한다.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"4.statistical_vectorization.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"텍스트 인코딩 및 벡터화: NLP 숫자 변환의 모든 것","subtitle":"정수 인코딩, OOV 처리, 패딩부터 원-핫, BoW, TF-IDF까지 핵심 개념 총정리","description":"자연어 처리(NLP)에서 텍스트 데이터를 기계가 이해하고 처리할 수 있는 숫자 형태로 변환하는 인코딩 및 벡터화의 주요 개념과 방법들을 살펴본다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-04","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}