{"title":"토큰화 (Tokenization)","markdown":{"yaml":{"title":"토큰화 (Tokenization)","subtitle":"자연어 처리의 첫 번째 단계 - 텍스트를 기계가 이해할 수 있는 단위로 분할","description":"토큰화는 자연어 처리의 첫 번째이자 가장 중요한 전처리 과정이다. 텍스트를 의미 있는 단위로 분할하여 기계가 처리할 수 있도록 변환하는 과정을 다룬다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-02","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false},"headingText":"내용 요약","containsRefs":false,"markdown":"\n\n\n* 토큰화란?: \n  * 자연어 처리(NLP)의 첫 단계로, 텍스트를 컴퓨터가 이해할 수 있는 의미 있는 단위(토큰)로 분할하는 과정\n* 왜 중요한가?: \n  * 텍스트를 구조화하고, 표준화하며, 후속 NLP 작업의 효율성을 높이고, 언어적 의미를 보존\n* 어떤 종류가 있나?: \n    * 문장 토큰화: 텍스트를 문장 단위로 나눈다. (예: 마침표, 물음표 등 구분)\n    * 단어 토큰화: 문장을 단어 단위로 나눈다. (예: 공백, 구두점 등 기준)\n    * 서브워드 토큰화: 단어를 더 작은 의미 단위(subword)로 나눈다. (예: BPE, WordPiece, SentencePiece)\n* 고려할 점은?: \n  * 각 토큰화 방법은 장단점이 있으며, 특히 문장/단어 토큰화는 다양한 예외 케이스(약어, 특수문자, 이메일 주소 등)로 인해 복잡할 수 있다.\n  * 언어별 특성(한국어 교착어 등), 도메인 특성(소셜미디어 약어 등)에 따라 상세한 규칙을 적용해 처리해야 한다.\n  * 규칙을 만들어 구현하는 것 보다는 이미 검증된 도구를 활용하는 것이 좋다.\n* 어떤 도구를 쓰나?: \n  * NLTK, KoNLPy, spaCy, Transformers 라이브러리 등 검증된 도구를 활용하는 것이 좋다.\n     * **NLTK (Natural Language Toolkit)**:\n       * 설명: 영어 자연어 처리를 위한 대표적인 라이브러리. 교육 및 연구 목적으로 많이 사용됨.\n       * 특징: TreebankWordTokenizer (단어), sent_tokenize (문장) 등 다양한 토큰화 기능 제공.\n       * 장점: 사용이 간편하고 다양한 기능을 제공. Penn Treebank 규칙 등 검증된 방식 사용.\n       * 단점: 다른 라이브러리에 비해 처리 속도가 느릴 수 있음.\n     * **KoNLPy (Korean NLP Library)**:\n       * 설명: 한국어 형태소 분석 및 자연어 처리를 위한 파이썬 패키지 모음.\n       * 특징: Okt, Komoran, Hannanum 등 다양한 형태소 분석기 포함.\n       * 장점: 한국어의 교착어 특성을 고려한 형태소 분석 가능. 다양한 분석기 선택 가능.\n       * 단점: 형태소 분석기마다 성능과 속도 차이가 있으며, 일부는 설치가 복잡할 수 있음.\n     * **spaCy (English Language Library)**:\n       * 설명: 산업 수준의 빠르고 효율적인 NLP 라이브러리.\n       * 특징: 다국어 지원(한국어 포함), 빠른 처리 속도, 통합된 NLP 파이프라인 제공.\n       * 장점: 속도가 매우 빠르고 안정적이며, 다양한 언어 및 기능을 지원하여 실제 제품 환경에 적합.\n       * 단점: 특정 언어나 도메인에 대한 세밀한 조정은 NLTK나 KoNLPy보다 유연성이 떨어질 수 있음.\n     * **Transformers (Hugging Face)**:\n       * 설명: 최신 딥러닝 모델(BERT, GPT 등)에서 사용되는 토크나이저를 제공하는 라이브러리.\n       * 특징: 서브워드 토큰화(BPE, WordPiece, SentencePiece) 기반. AutoTokenizer로 다양한 모델의 토크나이저 쉽게 로드.\n       * 장점: OOV(Out-of-Vocabulary) 문제에 강하고, 딥러닝 모델의 성능을 극대화. 다국어 모델 지원.\n       * 단점: 기존 규칙 기반 토큰화보다 직관성이 떨어질 수 있으며, 모델별 토큰화 방식이 다를 수 있음.\n* 추가적으로: \n  * 언어별 특성(한국어 교착어 등), 도메인 특성(소셜미디어 약어 등)에 따라 적절한 도구와 전략을 선택해야 한다. \n* 이러한 도전 과제, 평가 방법, 실제 적용 사례 및 최신 연구 동향도 간략히 소개한다.\n\n# 토큰화 개요\n\n* 토큰화(Tokenization)는 자연어 처리에서 가장 기본이 되는 전처리 과정이다. \n*컴퓨터가 이해할 수 있도록 연속된 텍스트를 의미 있는 단위(토큰)로 분할하는 작업을 의미한다.\n\n## 토큰화의 정의와 목적\n\n**토큰화**란 기계에게 어느 구간까지가 문장이고 단어인지를 알려주는 과정이다. 인간은 자연스럽게 문장의 구조를 이해하지만, 컴퓨터는 명시적으로 경계를 정해줘야 한다.\n\n### 주요 목적\n\n- 구조화: 비구조화된 텍스트를 구조화된 형태로 변환한다.\n- 표준화: 일관된 처리 단위를 제공한다.\n- 효율성: 후속 NLP 작업의 효율성을 향상시킨다.\n- 의미 보존: 언어의 의미를 최대한 보존하면서 분할한다.\n\n## 토큰화의 종류\n\n토큰화는 분할 단위에 따라 여러 가지 방법으로 분류할 수 있다.\n\n\n### 문장 토큰화 (Sentence Tokenization)\n\n텍스트를 문장 단위로 분할하는 과정이다.\n\n**한글 예시:**\n\n```\n입력: \"자연어 처리는 매우 흥미로운 분야입니다. 컴퓨터가 인간의 언어를 이해하고 처리할 수 있게 만드는 기술이죠. 최근 딥러닝 기술의 발전으로 놀라운 성과를 보이고 있습니다. 특히 GPT나 BERT 같은 모델들이 주목받고 있어요.\"\n\n출력: [\n  \"자연어 처리는 매우 흥미로운 분야입니다.\",\n  \"컴퓨터가 인간의 언어를 이해하고 처리할 수 있게 만드는 기술이죠.\",\n  \"최근 딥러닝 기술의 발전으로 놀라운 성과를 보이고 있습니다.\",\n  \"특히 GPT나 BERT 같은 모델들이 주목받고 있어요.\"\n]\n```\n\n**영어 예시:**\n\n```\n입력: \"Natural language processing is a fascinating field of study. It combines linguistics, computer science, and artificial intelligence to help computers understand human language. Recent advances in deep learning have revolutionized this area. Models like GPT and BERT have achieved remarkable performance on various NLP tasks.\"\n\n출력: [\n  \"Natural language processing is a fascinating field of study.\",\n  \"It combines linguistics, computer science, and artificial intelligence to help computers understand human language.\",\n  \"Recent advances in deep learning have revolutionized this area.\",\n  \"Models like GPT and BERT have achieved remarkable performance on various NLP tasks.\"\n]\n```\n\n**주요 고려사항:**\n- 마침표, 느낌표, 물음표 등의 문장 종결 부호\n- 줄임말과 약어 처리 (예: \"Dr.\", \"etc.\")\n- 인용문 내의 문장 부호\n\n\n#### 문장 토큰화에 대한 고민\n\n* 문장 토큰화는 겉보기에는 간단해 보이지만, 실제로는 매우 복잡한 언어학적 문제들을 내포하고 있다. \n* 비전문가들은 마침표, 쉼표, 느낌표, 물음표만 있으면 문장을 쉽게 구분할 수 있다고 생각하지만, 현실은 그렇게 단순하지 않다.\n\n**1. 마침표의 다중 의미 딜레마**\n\n마침표가 항상 문장의 끝을 의미하는 것은 아니다. 동일한 기호가 완전히 다른 용도로 사용되는 경우가 빈번하다.\n\n```\n예시 1: 약어와 문장 종료의 혼재\n\"Dr. Smith earned his Ph.D. from MIT in 1995. He now works at NASA.\"\n\n잘못된 분할:\n- \"Dr.\"\n- \"Smith earned his Ph.\"\n- \"D.\"\n- \"from MIT in 1995.\"\n- \"He now works at NASA.\"\n\n올바른 분할:\n- \"Dr. Smith earned his Ph.D. from MIT in 1995.\"\n- \"He now works at NASA.\"\n```\n\n같은 마침표이지만 \"Dr.\"와 \"Ph.D.\"는 약어를 나타내고, 마지막 마침표만이 실제 문장의 끝을 의미한다.\n\n**2. 이메일 주소의 함정**\n\n```\n예시 2: 이메일과 URL이 포함된 텍스트\n\"Contact john.doe@company.com for details. Visit www.company.com. Our office is open 9 A.M. to 5 P.M.\"\n\n잘못된 분할:\n- \"Contact john.\"\n- \"doe@company.\"\n- \"com for details.\"\n- \"Visit www.\"\n- \"company.\"\n- \"com.\"\n- \"Our office is open 9 A.\"\n- \"M.\"\n- \"to 5 P.\"\n- \"M.\"\n\n올바른 분할:\n- \"Contact john.doe@company.com for details.\"\n- \"Visit www.company.com.\"\n- \"Our office is open 9 A.M. to 5 P.M.\"\n```\n\n이메일 주소와 웹사이트 URL 내의 마침표들은 문장 구분자가 아닙니다.\n\n**3. 숫자와 소수점의 복잡성**\n\n```\n예시 3: 숫자와 측정값\n\"The temperature reached 98.6°F yesterday. Sales increased by 15.7% this quarter. Our target is $1.5M.\"\n\n잘못된 분할:\n- \"The temperature reached 98.\"\n- \"6°F yesterday.\"\n- \"Sales increased by 15.\"\n- \"7% this quarter.\"\n- \"Our target is $1.\"\n- \"5M.\"\n\n올바른 분할:\n- \"The temperature reached 98.6°F yesterday.\"\n- \"Sales increased by 15.7% this quarter.\"\n- \"Our target is $1.5M.\"\n```\n\n소수점, 통화 표시, 백분율 등에서 사용되는 마침표는 문장 구분자가 아니다.\n\n**4. IP 주소와 기술 용어**\n\n```\n예시 4: 기술 문서\n\"Connect to server 192.168.1.1 on port 8080. Use API version 2.3. Check logs at /var/log/app.log for errors.\"\n\n잘못된 분할:\n- \"Connect to server 192.\"\n- \"168.\"\n- \"1.\"\n- \"1 on port 8080.\"\n- \"Use API version 2.\"\n- \"3.\"\n- \"Check logs at /var/log/app.\"\n- \"log for errors.\"\n\n올바른 분할:\n- \"Connect to server 192.168.1.1 on port 8080.\"\n- \"Use API version 2.3.\"\n- \"Check logs at /var/log/app.log for errors.\"\n```\n\nIP 주소, 버전 번호, 파일 경로에서 사용되는 마침표들은 모두 문장 구분자가 아니다.\n\n**5. 인용문과 대화체의 복잡성**\n\n```\n예시 5: 직접 인용문\n'He said, \"I don't think so. Maybe tomorrow?\" Then he left.'\n\n잘못된 분할:\n- 'He said, \"I don't think so.'\n- 'Maybe tomorrow?\"'\n- 'Then he left.'\n\n올바른 분할:\n- 'He said, \"I don't think so. Maybe tomorrow?\" Then he left.'\n```\n\n인용문 내부의 문장 부호는 전체 문장의 구조를 고려해야 한다.\n\n**6. 줄임표와 생략 표현**\n\n```\n예시 6: 줄임표의 혼란\n\"Well... I'm not sure. He seemed hesitant... maybe nervous? The meeting went on and on...\"\n\n문제점:\n- \"Well...\"의 \"...\"는 망설임을 표현\n- \"hesitant...\"의 \"...\"는 말끝을 흐림  \n- \"on and on...\"의 \"...\"는 문장의 실제 종료\n\n이런 경우 어디서 문장을 나눠야 할지 판단하기 어렵다.\n```\n\n7. 이외에도, 다양한 유형의 문장 토큰화 문제가 있다.\n\n* **프로그래밍 코드가 포함된 텍스트**\n* **시간과 날짜 표기법**\n* **학술 논문과 참고문헌**\n\n결론적으로, 문장 토큰화는 단순한 규칙 기반 접근법으로는 해결되지 않는다. 다음과 같은 요소들을 종합적으로 고려해야 한다:\n\n- **문맥 정보**: 주변 단어와 문장의 맥락\n- **도메인 지식**: 의학, 법률, 기술 문서의 특수성\n- **언어적 규칙**: 각 언어의 고유한 문법과 표기법\n- **의미론적 이해**: 문장의 완결성과 논리적 구조\n- **다중 언어 처리**: 코드 스위칭과 외래어 처리\n\n이러한 복잡성 때문에 현대의 문장 토큰화 도구들은 기계학습 기반의 접근법을 사용하여 문맥을 이해하고 더 정확한 분할을 수행하려고 시도한다. 하지만 여전히 완벽한 해결책은 없으며, 지속적인 연구와 개선이 필요한 영역이다.\n\n### 단어 토큰화 (Word Tokenization)\n\n문장을 단어 단위로 분할하는 과정이다.\n\n**한글 예시:**\n\n```\n입력: \"자연어 처리는 매우 흥미로운 분야입니다. 컴퓨터가 인간의 언어를 이해하고 처리할 수 있게 만드는 기술이죠. 최근 딥러닝 기술의 발전으로 놀라운 성과를 보이고 있습니다. 특히 GPT나 BERT 같은 모델들이 주목받고 있어요.\"\n\n출력: [\n  \"자연어\", \"처리\", \"는\", \"매우\", \"흥미로운\", \"분야입니다\", \".\", \"컴퓨터가\", \"인간의\", \"언어를\", \"이해하고\", \"처리할\", \"수\", \"있게\", \"만드는\", \"기술이죠\", \".\", \"최근\", \"딥러닝\", \"기술의\", \"발전으로\", \"놀라운\", \"성과를\", \"보이고\", \"있습니다\", \".\", \"특히\", \"GPT나\", \"BERT\", \"같은\", \"모델들이\", \"주목받고\", \"있어요\"\n]\n```\n\n**영어 예시:**\n\n```\n입력: \"Natural language processing is a fascinating field of study. It combines linguistics, computer science, and artificial intelligence to help computers understand human language. Recent advances in deep learning have revolutionized this area. Models like GPT and BERT have achieved remarkable performance on various NLP tasks.\"\n\n출력: [\n  \"Natural\", \"language\", \"processing\", \"is\", \"a\", \"fascinating\", \"field\", \"of\", \"study\", \".\", \"It\", \"combines\", \"linguistics\", \"computer\", \"science\", \"and\", \"artificial\", \"intelligence\", \"to\", \"help\", \"computers\", \"understand\", \"human\", \"language\", \".\", \"Recent\", \"advances\", \"in\", \"deep\", \"learning\", \"have\", \"revolutionized\", \"this\", \"area\", \".\", \"Models\", \"like\", \"GPT\", \"and\", \"BERT\", \"have\", \"achieved\", \"remarkable\", \"performance\", \"on\", \"various\", \"NLP\", \"tasks\"\n]\n```\n\n**언어별 특성:**\n\n- 영어\n  - 공백을 기준으로 비교적 쉽게 분할 가능\n  - 구두점 처리가 주요 과제\n- 한국어\n  - 교착어 특성으로 복잡한 어미 변화\n  - 공백만으로는 정확한 분할 어려움\n- 형태소 분석이 필요\n\n#### 단어 토큰화에 대한 고민\n\n문장 토큰화와 마찬가지로, 단어 토큰화 역시 많은 복잡한 문제들을 내포하고 있다.\n\n**1. 특수문자 처리의 딜레마**\n\n문장 내에서 단어를 어떻게 구분할까? 느낌표나 어포스트로피 같은 특수문자가 들어갔을 때 문제가 발생한다.\n\n```\n예시:\n\"I can't believe it!\" \n\n처리 방법 1: [\"I\", \"can't\", \"believe\", \"it!\"]\n처리 방법 2: [\"I\", \"can\", \"'\", \"t\", \"believe\", \"it\", \"!\"]\n처리 방법 3: [\"I\", \"cannot\", \"believe\", \"it\"]\n```\n\n어떤 방법이 정답일까? 각각 장단점이 있어 선택하기 어렵다.\n\n**2. 동일한 의미, 다른 토큰화 결과**\n\n의미가 동일한 문장들에 대해서 **띄어쓰기 단위**로 단어를 나눠본다면, 같은 의미이지만 컴퓨터는 다르게 취급한다.\n\n```\n문장 1: \"He is a hero.\"\n문장 2: \"He is a hero?\"\n문장 3: \"He is a hero!\"\n\n토큰화 결과:\n문장 1: [\"He\", \"is\", \"a\", \"hero.\"]\n문장 2: [\"He\", \"is\", \"a\", \"hero?\"]  \n문장 3: [\"He\", \"is\", \"a\", \"hero!\"]\n```\n\n의미가 동일함에도 전부 다른 결과를 얻는다. `hero` ≠ `hero?` ≠ `hero!` 왜냐하면 컴퓨터가 인식하기에는 전부 다른 단어들이기 때문이다.\n\n**3. 특수문자 제거의 부작용**\n\n**특수 문자**가 토큰화에 방해가 된다고 해서 모든 특수 문자를 제거하는 규칙을 넣는다면?\n\n```\n원본: \"He has a Ph.D in computer science.\"\n특수문자 제거 후: \"He has a Ph D in computer science.\"\n\n토큰화 결과:\n원본: [\"He\", \"has\", \"a\", \"Ph.D\", \"in\", \"computer\", \"science\", \".\"]\n제거 후: [\"He\", \"has\", \"a\", \"Ph\", \"D\", \"in\", \"computer\", \"science\"]\n```\n\n`Ph.D` ≠ `Ph D` - 특수 문자 제거로 인해 본래 의미를 상실하는 경우가 발생한다.\n\n**4. 더 복잡한 사례들**\n\n```\n- \"U.S.A\" vs \"USA\" vs \"U S A\"\n- \"don't\" vs \"do not\" vs \"dont\"  \n- \"New York\" vs \"New-York\" vs \"NewYork\"\n- \"COVID-19\" vs \"COVID 19\" vs \"COVID19\"\n- \"machine-learning\" vs \"machine learning\"\n```\n\n각각은 같은 의미를 담고 있지만, 토큰화 결과는 완전히 다르다.\n\n**5. 언어별 특수성**\n\n```\n한국어:\n\"안녕하세요\" vs \"안녕 하세요\" vs \"안녕하 세요\"\n\"먹었습니다\" → [\"먹\", \"었\", \"습니다\"] vs [\"먹었습니다\"]\n\n영어:\n\"state-of-the-art\" → [\"state\", \"of\", \"the\", \"art\"] vs [\"state-of-the-art\"]\n```\n\n**결론: 섬세한 규칙 설계의 필요성**\n\n* 단어 토큰화 작업은 상당히 섬세한 규칙을 설계해야만 한다. \n* 단순히 공백으로 나누는 것으로는 해결되지 않으며, 다음과 같은 요소들을 종합적으로 고려해야 한다:\n\n- **도메인 특성**: 의학, 법률, 소셜미디어 등\n- **언어 특성**: 형태소, 문법 구조\n- **목적**: 번역, 감정분석, 검색 등\n- **일관성**: 동일한 규칙의 지속적 적용\n\n이러한 복잡성 때문에 최근에는 서브워드 토큰화 방법들이 주목받고 있다.\n\n### 서브워드 토큰화 (Subword Tokenization)\n\n단어보다 작은 단위로 분할하는 방법으로, 최근 딥러닝 모델에서 널리 사용된다.\n\n#### Byte Pair Encoding (BPE)\n\n가장 빈번하게 등장하는 문자 쌍을 반복적으로 병합하는 방법이다.\n\n**과정:**\n1. 모든 단어를 문자 단위로 분할\n2. 가장 빈번한 문자 쌍 찾기\n3. 해당 쌍을 하나의 토큰으로 병합\n4. 원하는 어휘 크기까지 반복\n\n**예시:**\n```\n초기: [\"l\", \"o\", \"w\", \"e\", \"s\", \"t\"]\n1단계: [\"lo\", \"w\", \"e\", \"s\", \"t\"]    # \"l\"+\"o\" 병합\n2단계: [\"low\", \"e\", \"s\", \"t\"]        # \"lo\"+\"w\" 병합\n3단계: [\"low\", \"es\", \"t\"]            # \"e\"+\"s\" 병합\n최종: [\"low\", \"est\"]                 # \"es\"+\"t\" 병합\n```\n\n#### WordPiece\n\nGoogle에서 개발한 방법으로, BERT 등에서 사용된다.\n\n**특징:**\n- 가능도(likelihood)를 최대화하는 방향으로 병합\n- \"##\" 접두사로 서브워드 표시\n\n**예시:**\n```\n\"playing\" → [\"play\", \"##ing\"]\n\"walked\" → [\"walk\", \"##ed\"]\n```\n\n#### SentencePiece\n\n언어에 독립적인 토큰화 방법이다.\n\n**특징:**\n- 공백도 특수 문자로 처리\n- 다양한 언어에 적용 가능\n- 전처리 없이 raw text 직접 처리\n\n## 토큰화 도구와 라이브러리\n\n* 앞서 살펴본 토큰화의 복잡성과 다양한 예외 상황들을 고려할 때, 개발자가 직접 상세한 규칙을 만들어 구현하려 하지 말고 이미 잘 개발되고 검증된 패키지들을 사용하는 것이 현명한 접근법이다.\n* 기존에 검증된 패키지로 처리 불가한 예외적인 부분들만 커스터마이징해서 처리하면 된다.\n\n### 영어 토큰화 도구\n\n#### 단어 토큰화: TreebankWordTokenizer\n\nPenn Treebank 코퍼스의 토큰화 규칙을 따르는 검증된 도구이다.\n\n```python\nfrom nltk.tokenize import TreebankWordTokenizer\n\ntokenizer = TreebankWordTokenizer()\ntext = \"He said, \\\"I can't believe it's working!\\\" Dr. Smith agreed.\"\ntokens = tokenizer.tokenize(text)\nprint(tokens)\n# ['He', 'said', ',', '``', 'I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'working', '!', \"''\", 'Dr.', 'Smith', 'agreed', '.']\n```\n\n**장점:**\n- 축약형을 적절히 분리 (\"can't\" → \"ca\", \"n't\")\n- 인용문 처리 (`` `` 와 '' 로 변환)\n- 약어 보존 (\"Dr.\" 유지)\n- 수십 년간 검증된 규칙\n\n#### 문장 토큰화: NLTK Sentence Tokenizer\n\n```python\nfrom nltk.tokenize import sent_tokenize\n\ntext = \"Dr. Smith earned his Ph.D. from MIT. He works at NASA. Contact him at john.doe@company.com.\"\nsentences = sent_tokenize(text)\nprint(sentences)\n# ['Dr. Smith earned his Ph.D. from MIT.', 'He works at NASA.', 'Contact him at john.doe@company.com.']\n```\n\n**특징:**\n- 약어 목록을 내장하여 문맥 고려\n- 이메일 주소 내 마침표 구분\n- 다양한 언어 지원\n\n### 한국어 토큰화 도구\n\n#### 문장 토큰화: KSS (Korean Sentence Splitter)\n\n한국어 문장 분할에 특화된 고성능 라이브러리이다.\n\n```python\nimport kss\n\ntext = \"안녕하세요. 제 이메일은 user@domain.co.kr입니다. 연락 주세요!\"\nsentences = kss.split_sentences(text)\nprint(sentences)\n# ['안녕하세요.', '제 이메일은 user@domain.co.kr입니다.', '연락 주세요!']\n```\n\n**장점:**\n- 한국어 특화 규칙\n- 이메일, URL 등 특수 패턴 인식\n- 높은 정확도와 빠른 처리 속도\n\n#### 단어 토큰화: KoNLPy 생태계\n\n```python\nfrom konlpy.tag import Okt, Komoran, Hannanum\n\n# Okt (Open Korean Text)\nokt = Okt()\ntext = \"아버지가방에들어가신다\"\ntokens = okt.morphs(text)\nprint(tokens)\n# ['아버지', '가', '방', '에', '들어가', '신다']\n\n# Komoran\nkomoran = Komoran()\ntokens = komoran.morphs(text)\nprint(tokens)\n# ['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다']\n```\n\n### 다국어 및 고급 토큰화 도구\n\n#### spaCy\n\n산업 수준의 NLP 라이브러리\n\n```python\nimport spacy\n\n# 영어\nnlp_en = spacy.load(\"en_core_web_sm\")\ndoc = nlp_en(\"Dr. Smith's email is john@company.com. He earned his Ph.D. in 1995.\")\nsentences = [sent.text for sent in doc.sents]\ntokens = [token.text for token in doc]\n\n# 한국어 (spacy-korean)\nnlp_ko = spacy.load(\"ko_core_news_sm\")\ndoc = nlp_ko(\"안녕하세요. 저는 데이터 과학자입니다.\")\nsentences = [sent.text for sent in doc.sents]\n```\n\n**특징:**\n- 다양한 언어 지원\n- 빠른 처리 속도\n- 통합된 NLP 파이프라인\n- 산업 환경에 최적화\n\n#### Transformers 토크나이저\n\n최신 딥러닝 모델에서 사용되는 서브워드 토큰화이다.\n\n```python\nfrom transformers import AutoTokenizer\n\n# BERT 토크나이저\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntext = \"Hello, I can't believe it's working!\"\ntokens = tokenizer.tokenize(text)\nprint(tokens)\n# ['hello', ',', 'i', 'can', \"'\", 't', 'believe', 'it', \"'\", 's', 'working', '!']\n\n# 한국어 BERT\ntokenizer_ko = AutoTokenizer.from_pretrained(\"klue/bert-base\")\ntext = \"안녕하세요. 한국어 토큰화입니다.\"\ntokens = tokenizer_ko.tokenize(text)\nprint(tokens)\n# ['안녕', '##하세요', '.', '한국어', '토큰', '##화', '##입니다', '.']\n```\n\n### 도메인별 특화 도구\n\n#### 소셜미디어: TweetTokenizer\n\n```python\nfrom nltk.tokenize import TweetTokenizer\n\ntknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\ntext = \"@user This is sooooo cool! 😊 http://example.com #NLP\"\ntokens = tknzr.tokenize(text)\nprint(tokens)\n# ['This', 'is', 'sooo', 'cool', '!', '😊', 'http://example.com', '#NLP']\n```\n\n#### 의학/과학 텍스트: SciSpaCy\n\n```python\nimport spacy\nimport scispacy\n\nnlp = spacy.load(\"en_core_sci_sm\")\ntext = \"The patient has a temperature of 101.5°F. Administer 2.5mg of medication.\"\ndoc = nlp(text)\nsentences = [sent.text for sent in doc.sents]\n```\n\n### 언어별 권장 도구 조합\n\n#### 영어\n```python\n# 실용적인 영어 처리 파이프라인\nfrom nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n\ndef process_english_text(text):\n    # 1. 문장 분할\n    sentences = sent_tokenize(text)\n    \n    # 2. 단어 토큰화\n    word_tokenizer = TreebankWordTokenizer()\n    tokenized_sentences = []\n    for sentence in sentences:\n        words = word_tokenizer.tokenize(sentence)\n        tokenized_sentences.append(words)\n    \n    return sentences, tokenized_sentences\n```\n\n#### 한국어\n```python\n# 실용적인 한국어 처리 파이프라인\nimport kss\nfrom konlpy.tag import Okt\n\ndef process_korean_text(text):\n    # 1. 문장 분할\n    sentences = kss.split_sentences(text)\n    \n    # 2. 형태소 분석\n    okt = Okt()\n    tokenized_sentences = []\n    for sentence in sentences:\n        morphs = okt.morphs(sentence)\n        tokenized_sentences.append(morphs)\n    \n    return sentences, tokenized_sentences\n```\n\n### 성능과 정확도 비교\n\n#### 처리 속도 (상대적 비교)\n- **spaCy**: 매우 빠름 (산업용)\n- **KSS**: 빠름 (한국어 특화)\n- **TreebankWordTokenizer**: 보통\n- **KoNLPy**: 보통-느림 (정확도 높음)\n\n#### 정확도 (도메인별)\n- **일반 텍스트**: spaCy, NLTK\n- **소셜미디어**: TweetTokenizer\n- **학술/의학**: SciSpaCy\n- **한국어**: KSS + KoNLPy\n\n### 선택 기준\n\n#### 프로젝트 요구사항별 선택\n1. **프로토타이핑**: NLTK (간편함)\n2. **제품 환경**: spaCy (속도와 안정성)\n3. **한국어 중심**: KSS + KoNLPy\n4. **딥러닝 모델**: Transformers 토크나이저\n5. **특수 도메인**: 도메인별 특화 도구\n\n#### 실무 권장사항\n```python\n# 범용적인 다국어 처리 환경 구축\nimport spacy\nimport kss\nfrom transformers import AutoTokenizer\n\nclass UniversalTokenizer:\n    def __init__(self):\n        self.en_nlp = spacy.load(\"en_core_web_sm\")\n        self.bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n    \n    def tokenize_sentences(self, text, language='auto'):\n        if language == 'ko':\n            return kss.split_sentences(text)\n        elif language == 'en':\n            doc = self.en_nlp(text)\n            return [sent.text for sent in doc.sents]\n        else:\n            # 자동 언어 감지 로직\n            return self._auto_detect_and_tokenize(text)\n    \n    def tokenize_for_model(self, text):\n        return self.bert_tokenizer.tokenize(text)\n```\n\n### 결론\n\n* 토큰화는 복잡한 언어학적 문제이므로, 수년간 연구되고 검증된 도구들을 활용하는 것이 가장 효율적이고 안정적인 접근법이다. \n* 각 도구의 특성을 이해하고 프로젝트의 요구사항에 맞는 적절한 조합을 선택하는 것이 성공적인 NLP 프로젝트의 첫걸음이다.\n\n## 토큰화의 도전과제\n\n### 언어별 특성\n\n**한국어:**\n- 어미 변화가 복잡\n- 띄어쓰기 규칙이 일관되지 않음\n- 복합어 처리 어려움\n\n**영어:**\n- 축약형 처리 (예: \"don't\", \"I'm\")\n- 하이픈으로 연결된 단어\n- 대소문자 처리\n\n### 도메인별 특수성\n\n**소셜미디어:**\n- 이모티콘과 이모지\n- 해시태그와 멘션\n- 비표준 언어 사용\n\n**의학/법률 텍스트:**\n- 전문 용어\n- 약어와 기호\n- 정확성이 중요\n\n### Vocabulary\n\n* **Vocabulary**란 텍스트를 토큰화하고, 고유한 토큰들로 이루어진 집합으로 쉽게 말해 기계가 알고있는 단어들의 집합이다.\n* 기계가 텍스트를 처리하기 위해서는 이를 숫자 형태로 변환하는 과정이 필요하다.\n* 이를 위해 먼저 텍스트를 토큰화하고, 토큰을 숫자로 매핑하는 과정이 필요하다.\n  1. 텍스트를 의미 있는 단위인 토큰으로 분할하고(토큰화), \n  2. 각 고유 토큰에 숫자(정수 인덱스 등)를 부여하여 어휘집(Vocabulary)을 구축한 후, \n  3. 이 어휘집을 바탕으로 원래의 토큰 시퀀스를 숫자 시퀀스로 변환하는 단계를 포함합니다.\n* 이렇게 매핑된 숫자를 통해 기계가 텍스트를 이해할 수 있도록 한다.\n\n### Out-of-Vocabulary (OOV) 문제\n\n**문제:**\n- 모델이 학습한 어휘집(Vocabulary)에 포함되지 않은 단어가 입력으로 들어올 때 발생\n- 이는 특히 다음과 같은 경우에 문제가 된다:\n  - **새로운 단어**: 신조어, 기술 용어, 브랜드 이름 등\n  - **오타 및 비표준 표현**: 사용자의 입력 실수나 비공식적인 언어 사용\n  - **다양한 언어적 변형**: 복합어, 축약형, 방언 등\n- OOV 단어가 포함된 문장을 제대로 이해하지 못하거나, 잘못된 예측을 할 수 있다.\n\n**해결 방안:**\n- **서브워드 토큰화 사용**\n  - 최근 가장 널리 사용되는 방법 중 하나로, 다양한 자연어 처리 모델에서 효과적으로 활용되고 있다.\n  - 이 방법들은 단어를 더 작은 의미 단위로 분할하여 처리하므로, 새로운 단어가 등장하더라도 부분적으로 이해할 수 있다.\n- **문자 단위 처리**\n  - 단어를 문자 단위로 분할하여 처리하는 방법도 있다. \n  - 이는 모든 단어를 개별 문자로 분해하여 처리하므로, OOV 문제를 근본적으로 해결할 수 있다. \n  - 그러나 이 방법은 문맥 이해가 어려워질 수 있다.\n- **사전 확장**\n  - 지속적으로 어휘집을 업데이트하여 새로운 단어를 포함시키는 방법. \n  - 이는 시간이 지남에 따라 어휘집을 확장하여 OOV 문제를 줄일 수 있다.\n\n\n## 평가 메트릭\n\n### 어휘 크기 (Vocabulary Size)\n\n- 모델의 메모리 사용량과 직결\n- 너무 크면 비효율적, 너무 작으면 표현력 부족\n\n### 압축률 (Compression Rate)\n\n- 원본 텍스트 대비 토큰 수의 비율\n- 효율적인 표현을 위해 중요\n\n### 의미 보존도\n\n- 토큰화 후에도 원래 의미가 유지되는 정도\n- 정성적 평가가 주로 사용됨\n\n## 실제 적용 사례\n\n### 기계 번역\n\n- 다국어 처리를 위한 공통 서브워드 어휘\n- 언어 간 토큰 정렬\n\n### 감정 분석\n\n- 감정을 나타내는 키워드 보존\n- 이모티콘과 특수 문자 처리\n\n### 질의응답 시스템\n\n- 질문과 답변의 일관된 토큰화\n- 개체명 인식과의 연계\n\n## 최신 동향\n\n### 다국어 토큰화\n\n- mBERT, XLM-R 등 다국어 모델\n- 언어별 특성을 고려한 통합 토큰화\n\n### 적응형 토큰화\n\n- 도메인별 최적화\n- 동적 어휘 확장\n\n### 신경망 기반 토큰화\n\n- 학습 가능한 토큰화\n- End-to-end 학습\n\n## 결론\n\n* 토큰화는 자연어 처리의 첫 단계이자 전체 성능을 좌우하는 중요한 과정. \n* 언어의 특성과 도메인의 요구사항을 고려하여 적절한 토큰화 방법을 선택하는 것이 중요. \n* 최근에는 서브워드 토큰화가 주류가 되었지만, 여전히 각 방법의 장단점을 이해하고 상황에 맞게 적용하는 것이 필요. \n\n","srcMarkdownNoYaml":"\n\n# 내용 요약\n\n* 토큰화란?: \n  * 자연어 처리(NLP)의 첫 단계로, 텍스트를 컴퓨터가 이해할 수 있는 의미 있는 단위(토큰)로 분할하는 과정\n* 왜 중요한가?: \n  * 텍스트를 구조화하고, 표준화하며, 후속 NLP 작업의 효율성을 높이고, 언어적 의미를 보존\n* 어떤 종류가 있나?: \n    * 문장 토큰화: 텍스트를 문장 단위로 나눈다. (예: 마침표, 물음표 등 구분)\n    * 단어 토큰화: 문장을 단어 단위로 나눈다. (예: 공백, 구두점 등 기준)\n    * 서브워드 토큰화: 단어를 더 작은 의미 단위(subword)로 나눈다. (예: BPE, WordPiece, SentencePiece)\n* 고려할 점은?: \n  * 각 토큰화 방법은 장단점이 있으며, 특히 문장/단어 토큰화는 다양한 예외 케이스(약어, 특수문자, 이메일 주소 등)로 인해 복잡할 수 있다.\n  * 언어별 특성(한국어 교착어 등), 도메인 특성(소셜미디어 약어 등)에 따라 상세한 규칙을 적용해 처리해야 한다.\n  * 규칙을 만들어 구현하는 것 보다는 이미 검증된 도구를 활용하는 것이 좋다.\n* 어떤 도구를 쓰나?: \n  * NLTK, KoNLPy, spaCy, Transformers 라이브러리 등 검증된 도구를 활용하는 것이 좋다.\n     * **NLTK (Natural Language Toolkit)**:\n       * 설명: 영어 자연어 처리를 위한 대표적인 라이브러리. 교육 및 연구 목적으로 많이 사용됨.\n       * 특징: TreebankWordTokenizer (단어), sent_tokenize (문장) 등 다양한 토큰화 기능 제공.\n       * 장점: 사용이 간편하고 다양한 기능을 제공. Penn Treebank 규칙 등 검증된 방식 사용.\n       * 단점: 다른 라이브러리에 비해 처리 속도가 느릴 수 있음.\n     * **KoNLPy (Korean NLP Library)**:\n       * 설명: 한국어 형태소 분석 및 자연어 처리를 위한 파이썬 패키지 모음.\n       * 특징: Okt, Komoran, Hannanum 등 다양한 형태소 분석기 포함.\n       * 장점: 한국어의 교착어 특성을 고려한 형태소 분석 가능. 다양한 분석기 선택 가능.\n       * 단점: 형태소 분석기마다 성능과 속도 차이가 있으며, 일부는 설치가 복잡할 수 있음.\n     * **spaCy (English Language Library)**:\n       * 설명: 산업 수준의 빠르고 효율적인 NLP 라이브러리.\n       * 특징: 다국어 지원(한국어 포함), 빠른 처리 속도, 통합된 NLP 파이프라인 제공.\n       * 장점: 속도가 매우 빠르고 안정적이며, 다양한 언어 및 기능을 지원하여 실제 제품 환경에 적합.\n       * 단점: 특정 언어나 도메인에 대한 세밀한 조정은 NLTK나 KoNLPy보다 유연성이 떨어질 수 있음.\n     * **Transformers (Hugging Face)**:\n       * 설명: 최신 딥러닝 모델(BERT, GPT 등)에서 사용되는 토크나이저를 제공하는 라이브러리.\n       * 특징: 서브워드 토큰화(BPE, WordPiece, SentencePiece) 기반. AutoTokenizer로 다양한 모델의 토크나이저 쉽게 로드.\n       * 장점: OOV(Out-of-Vocabulary) 문제에 강하고, 딥러닝 모델의 성능을 극대화. 다국어 모델 지원.\n       * 단점: 기존 규칙 기반 토큰화보다 직관성이 떨어질 수 있으며, 모델별 토큰화 방식이 다를 수 있음.\n* 추가적으로: \n  * 언어별 특성(한국어 교착어 등), 도메인 특성(소셜미디어 약어 등)에 따라 적절한 도구와 전략을 선택해야 한다. \n* 이러한 도전 과제, 평가 방법, 실제 적용 사례 및 최신 연구 동향도 간략히 소개한다.\n\n# 토큰화 개요\n\n* 토큰화(Tokenization)는 자연어 처리에서 가장 기본이 되는 전처리 과정이다. \n*컴퓨터가 이해할 수 있도록 연속된 텍스트를 의미 있는 단위(토큰)로 분할하는 작업을 의미한다.\n\n## 토큰화의 정의와 목적\n\n**토큰화**란 기계에게 어느 구간까지가 문장이고 단어인지를 알려주는 과정이다. 인간은 자연스럽게 문장의 구조를 이해하지만, 컴퓨터는 명시적으로 경계를 정해줘야 한다.\n\n### 주요 목적\n\n- 구조화: 비구조화된 텍스트를 구조화된 형태로 변환한다.\n- 표준화: 일관된 처리 단위를 제공한다.\n- 효율성: 후속 NLP 작업의 효율성을 향상시킨다.\n- 의미 보존: 언어의 의미를 최대한 보존하면서 분할한다.\n\n## 토큰화의 종류\n\n토큰화는 분할 단위에 따라 여러 가지 방법으로 분류할 수 있다.\n\n\n### 문장 토큰화 (Sentence Tokenization)\n\n텍스트를 문장 단위로 분할하는 과정이다.\n\n**한글 예시:**\n\n```\n입력: \"자연어 처리는 매우 흥미로운 분야입니다. 컴퓨터가 인간의 언어를 이해하고 처리할 수 있게 만드는 기술이죠. 최근 딥러닝 기술의 발전으로 놀라운 성과를 보이고 있습니다. 특히 GPT나 BERT 같은 모델들이 주목받고 있어요.\"\n\n출력: [\n  \"자연어 처리는 매우 흥미로운 분야입니다.\",\n  \"컴퓨터가 인간의 언어를 이해하고 처리할 수 있게 만드는 기술이죠.\",\n  \"최근 딥러닝 기술의 발전으로 놀라운 성과를 보이고 있습니다.\",\n  \"특히 GPT나 BERT 같은 모델들이 주목받고 있어요.\"\n]\n```\n\n**영어 예시:**\n\n```\n입력: \"Natural language processing is a fascinating field of study. It combines linguistics, computer science, and artificial intelligence to help computers understand human language. Recent advances in deep learning have revolutionized this area. Models like GPT and BERT have achieved remarkable performance on various NLP tasks.\"\n\n출력: [\n  \"Natural language processing is a fascinating field of study.\",\n  \"It combines linguistics, computer science, and artificial intelligence to help computers understand human language.\",\n  \"Recent advances in deep learning have revolutionized this area.\",\n  \"Models like GPT and BERT have achieved remarkable performance on various NLP tasks.\"\n]\n```\n\n**주요 고려사항:**\n- 마침표, 느낌표, 물음표 등의 문장 종결 부호\n- 줄임말과 약어 처리 (예: \"Dr.\", \"etc.\")\n- 인용문 내의 문장 부호\n\n\n#### 문장 토큰화에 대한 고민\n\n* 문장 토큰화는 겉보기에는 간단해 보이지만, 실제로는 매우 복잡한 언어학적 문제들을 내포하고 있다. \n* 비전문가들은 마침표, 쉼표, 느낌표, 물음표만 있으면 문장을 쉽게 구분할 수 있다고 생각하지만, 현실은 그렇게 단순하지 않다.\n\n**1. 마침표의 다중 의미 딜레마**\n\n마침표가 항상 문장의 끝을 의미하는 것은 아니다. 동일한 기호가 완전히 다른 용도로 사용되는 경우가 빈번하다.\n\n```\n예시 1: 약어와 문장 종료의 혼재\n\"Dr. Smith earned his Ph.D. from MIT in 1995. He now works at NASA.\"\n\n잘못된 분할:\n- \"Dr.\"\n- \"Smith earned his Ph.\"\n- \"D.\"\n- \"from MIT in 1995.\"\n- \"He now works at NASA.\"\n\n올바른 분할:\n- \"Dr. Smith earned his Ph.D. from MIT in 1995.\"\n- \"He now works at NASA.\"\n```\n\n같은 마침표이지만 \"Dr.\"와 \"Ph.D.\"는 약어를 나타내고, 마지막 마침표만이 실제 문장의 끝을 의미한다.\n\n**2. 이메일 주소의 함정**\n\n```\n예시 2: 이메일과 URL이 포함된 텍스트\n\"Contact john.doe@company.com for details. Visit www.company.com. Our office is open 9 A.M. to 5 P.M.\"\n\n잘못된 분할:\n- \"Contact john.\"\n- \"doe@company.\"\n- \"com for details.\"\n- \"Visit www.\"\n- \"company.\"\n- \"com.\"\n- \"Our office is open 9 A.\"\n- \"M.\"\n- \"to 5 P.\"\n- \"M.\"\n\n올바른 분할:\n- \"Contact john.doe@company.com for details.\"\n- \"Visit www.company.com.\"\n- \"Our office is open 9 A.M. to 5 P.M.\"\n```\n\n이메일 주소와 웹사이트 URL 내의 마침표들은 문장 구분자가 아닙니다.\n\n**3. 숫자와 소수점의 복잡성**\n\n```\n예시 3: 숫자와 측정값\n\"The temperature reached 98.6°F yesterday. Sales increased by 15.7% this quarter. Our target is $1.5M.\"\n\n잘못된 분할:\n- \"The temperature reached 98.\"\n- \"6°F yesterday.\"\n- \"Sales increased by 15.\"\n- \"7% this quarter.\"\n- \"Our target is $1.\"\n- \"5M.\"\n\n올바른 분할:\n- \"The temperature reached 98.6°F yesterday.\"\n- \"Sales increased by 15.7% this quarter.\"\n- \"Our target is $1.5M.\"\n```\n\n소수점, 통화 표시, 백분율 등에서 사용되는 마침표는 문장 구분자가 아니다.\n\n**4. IP 주소와 기술 용어**\n\n```\n예시 4: 기술 문서\n\"Connect to server 192.168.1.1 on port 8080. Use API version 2.3. Check logs at /var/log/app.log for errors.\"\n\n잘못된 분할:\n- \"Connect to server 192.\"\n- \"168.\"\n- \"1.\"\n- \"1 on port 8080.\"\n- \"Use API version 2.\"\n- \"3.\"\n- \"Check logs at /var/log/app.\"\n- \"log for errors.\"\n\n올바른 분할:\n- \"Connect to server 192.168.1.1 on port 8080.\"\n- \"Use API version 2.3.\"\n- \"Check logs at /var/log/app.log for errors.\"\n```\n\nIP 주소, 버전 번호, 파일 경로에서 사용되는 마침표들은 모두 문장 구분자가 아니다.\n\n**5. 인용문과 대화체의 복잡성**\n\n```\n예시 5: 직접 인용문\n'He said, \"I don't think so. Maybe tomorrow?\" Then he left.'\n\n잘못된 분할:\n- 'He said, \"I don't think so.'\n- 'Maybe tomorrow?\"'\n- 'Then he left.'\n\n올바른 분할:\n- 'He said, \"I don't think so. Maybe tomorrow?\" Then he left.'\n```\n\n인용문 내부의 문장 부호는 전체 문장의 구조를 고려해야 한다.\n\n**6. 줄임표와 생략 표현**\n\n```\n예시 6: 줄임표의 혼란\n\"Well... I'm not sure. He seemed hesitant... maybe nervous? The meeting went on and on...\"\n\n문제점:\n- \"Well...\"의 \"...\"는 망설임을 표현\n- \"hesitant...\"의 \"...\"는 말끝을 흐림  \n- \"on and on...\"의 \"...\"는 문장의 실제 종료\n\n이런 경우 어디서 문장을 나눠야 할지 판단하기 어렵다.\n```\n\n7. 이외에도, 다양한 유형의 문장 토큰화 문제가 있다.\n\n* **프로그래밍 코드가 포함된 텍스트**\n* **시간과 날짜 표기법**\n* **학술 논문과 참고문헌**\n\n결론적으로, 문장 토큰화는 단순한 규칙 기반 접근법으로는 해결되지 않는다. 다음과 같은 요소들을 종합적으로 고려해야 한다:\n\n- **문맥 정보**: 주변 단어와 문장의 맥락\n- **도메인 지식**: 의학, 법률, 기술 문서의 특수성\n- **언어적 규칙**: 각 언어의 고유한 문법과 표기법\n- **의미론적 이해**: 문장의 완결성과 논리적 구조\n- **다중 언어 처리**: 코드 스위칭과 외래어 처리\n\n이러한 복잡성 때문에 현대의 문장 토큰화 도구들은 기계학습 기반의 접근법을 사용하여 문맥을 이해하고 더 정확한 분할을 수행하려고 시도한다. 하지만 여전히 완벽한 해결책은 없으며, 지속적인 연구와 개선이 필요한 영역이다.\n\n### 단어 토큰화 (Word Tokenization)\n\n문장을 단어 단위로 분할하는 과정이다.\n\n**한글 예시:**\n\n```\n입력: \"자연어 처리는 매우 흥미로운 분야입니다. 컴퓨터가 인간의 언어를 이해하고 처리할 수 있게 만드는 기술이죠. 최근 딥러닝 기술의 발전으로 놀라운 성과를 보이고 있습니다. 특히 GPT나 BERT 같은 모델들이 주목받고 있어요.\"\n\n출력: [\n  \"자연어\", \"처리\", \"는\", \"매우\", \"흥미로운\", \"분야입니다\", \".\", \"컴퓨터가\", \"인간의\", \"언어를\", \"이해하고\", \"처리할\", \"수\", \"있게\", \"만드는\", \"기술이죠\", \".\", \"최근\", \"딥러닝\", \"기술의\", \"발전으로\", \"놀라운\", \"성과를\", \"보이고\", \"있습니다\", \".\", \"특히\", \"GPT나\", \"BERT\", \"같은\", \"모델들이\", \"주목받고\", \"있어요\"\n]\n```\n\n**영어 예시:**\n\n```\n입력: \"Natural language processing is a fascinating field of study. It combines linguistics, computer science, and artificial intelligence to help computers understand human language. Recent advances in deep learning have revolutionized this area. Models like GPT and BERT have achieved remarkable performance on various NLP tasks.\"\n\n출력: [\n  \"Natural\", \"language\", \"processing\", \"is\", \"a\", \"fascinating\", \"field\", \"of\", \"study\", \".\", \"It\", \"combines\", \"linguistics\", \"computer\", \"science\", \"and\", \"artificial\", \"intelligence\", \"to\", \"help\", \"computers\", \"understand\", \"human\", \"language\", \".\", \"Recent\", \"advances\", \"in\", \"deep\", \"learning\", \"have\", \"revolutionized\", \"this\", \"area\", \".\", \"Models\", \"like\", \"GPT\", \"and\", \"BERT\", \"have\", \"achieved\", \"remarkable\", \"performance\", \"on\", \"various\", \"NLP\", \"tasks\"\n]\n```\n\n**언어별 특성:**\n\n- 영어\n  - 공백을 기준으로 비교적 쉽게 분할 가능\n  - 구두점 처리가 주요 과제\n- 한국어\n  - 교착어 특성으로 복잡한 어미 변화\n  - 공백만으로는 정확한 분할 어려움\n- 형태소 분석이 필요\n\n#### 단어 토큰화에 대한 고민\n\n문장 토큰화와 마찬가지로, 단어 토큰화 역시 많은 복잡한 문제들을 내포하고 있다.\n\n**1. 특수문자 처리의 딜레마**\n\n문장 내에서 단어를 어떻게 구분할까? 느낌표나 어포스트로피 같은 특수문자가 들어갔을 때 문제가 발생한다.\n\n```\n예시:\n\"I can't believe it!\" \n\n처리 방법 1: [\"I\", \"can't\", \"believe\", \"it!\"]\n처리 방법 2: [\"I\", \"can\", \"'\", \"t\", \"believe\", \"it\", \"!\"]\n처리 방법 3: [\"I\", \"cannot\", \"believe\", \"it\"]\n```\n\n어떤 방법이 정답일까? 각각 장단점이 있어 선택하기 어렵다.\n\n**2. 동일한 의미, 다른 토큰화 결과**\n\n의미가 동일한 문장들에 대해서 **띄어쓰기 단위**로 단어를 나눠본다면, 같은 의미이지만 컴퓨터는 다르게 취급한다.\n\n```\n문장 1: \"He is a hero.\"\n문장 2: \"He is a hero?\"\n문장 3: \"He is a hero!\"\n\n토큰화 결과:\n문장 1: [\"He\", \"is\", \"a\", \"hero.\"]\n문장 2: [\"He\", \"is\", \"a\", \"hero?\"]  \n문장 3: [\"He\", \"is\", \"a\", \"hero!\"]\n```\n\n의미가 동일함에도 전부 다른 결과를 얻는다. `hero` ≠ `hero?` ≠ `hero!` 왜냐하면 컴퓨터가 인식하기에는 전부 다른 단어들이기 때문이다.\n\n**3. 특수문자 제거의 부작용**\n\n**특수 문자**가 토큰화에 방해가 된다고 해서 모든 특수 문자를 제거하는 규칙을 넣는다면?\n\n```\n원본: \"He has a Ph.D in computer science.\"\n특수문자 제거 후: \"He has a Ph D in computer science.\"\n\n토큰화 결과:\n원본: [\"He\", \"has\", \"a\", \"Ph.D\", \"in\", \"computer\", \"science\", \".\"]\n제거 후: [\"He\", \"has\", \"a\", \"Ph\", \"D\", \"in\", \"computer\", \"science\"]\n```\n\n`Ph.D` ≠ `Ph D` - 특수 문자 제거로 인해 본래 의미를 상실하는 경우가 발생한다.\n\n**4. 더 복잡한 사례들**\n\n```\n- \"U.S.A\" vs \"USA\" vs \"U S A\"\n- \"don't\" vs \"do not\" vs \"dont\"  \n- \"New York\" vs \"New-York\" vs \"NewYork\"\n- \"COVID-19\" vs \"COVID 19\" vs \"COVID19\"\n- \"machine-learning\" vs \"machine learning\"\n```\n\n각각은 같은 의미를 담고 있지만, 토큰화 결과는 완전히 다르다.\n\n**5. 언어별 특수성**\n\n```\n한국어:\n\"안녕하세요\" vs \"안녕 하세요\" vs \"안녕하 세요\"\n\"먹었습니다\" → [\"먹\", \"었\", \"습니다\"] vs [\"먹었습니다\"]\n\n영어:\n\"state-of-the-art\" → [\"state\", \"of\", \"the\", \"art\"] vs [\"state-of-the-art\"]\n```\n\n**결론: 섬세한 규칙 설계의 필요성**\n\n* 단어 토큰화 작업은 상당히 섬세한 규칙을 설계해야만 한다. \n* 단순히 공백으로 나누는 것으로는 해결되지 않으며, 다음과 같은 요소들을 종합적으로 고려해야 한다:\n\n- **도메인 특성**: 의학, 법률, 소셜미디어 등\n- **언어 특성**: 형태소, 문법 구조\n- **목적**: 번역, 감정분석, 검색 등\n- **일관성**: 동일한 규칙의 지속적 적용\n\n이러한 복잡성 때문에 최근에는 서브워드 토큰화 방법들이 주목받고 있다.\n\n### 서브워드 토큰화 (Subword Tokenization)\n\n단어보다 작은 단위로 분할하는 방법으로, 최근 딥러닝 모델에서 널리 사용된다.\n\n#### Byte Pair Encoding (BPE)\n\n가장 빈번하게 등장하는 문자 쌍을 반복적으로 병합하는 방법이다.\n\n**과정:**\n1. 모든 단어를 문자 단위로 분할\n2. 가장 빈번한 문자 쌍 찾기\n3. 해당 쌍을 하나의 토큰으로 병합\n4. 원하는 어휘 크기까지 반복\n\n**예시:**\n```\n초기: [\"l\", \"o\", \"w\", \"e\", \"s\", \"t\"]\n1단계: [\"lo\", \"w\", \"e\", \"s\", \"t\"]    # \"l\"+\"o\" 병합\n2단계: [\"low\", \"e\", \"s\", \"t\"]        # \"lo\"+\"w\" 병합\n3단계: [\"low\", \"es\", \"t\"]            # \"e\"+\"s\" 병합\n최종: [\"low\", \"est\"]                 # \"es\"+\"t\" 병합\n```\n\n#### WordPiece\n\nGoogle에서 개발한 방법으로, BERT 등에서 사용된다.\n\n**특징:**\n- 가능도(likelihood)를 최대화하는 방향으로 병합\n- \"##\" 접두사로 서브워드 표시\n\n**예시:**\n```\n\"playing\" → [\"play\", \"##ing\"]\n\"walked\" → [\"walk\", \"##ed\"]\n```\n\n#### SentencePiece\n\n언어에 독립적인 토큰화 방법이다.\n\n**특징:**\n- 공백도 특수 문자로 처리\n- 다양한 언어에 적용 가능\n- 전처리 없이 raw text 직접 처리\n\n## 토큰화 도구와 라이브러리\n\n* 앞서 살펴본 토큰화의 복잡성과 다양한 예외 상황들을 고려할 때, 개발자가 직접 상세한 규칙을 만들어 구현하려 하지 말고 이미 잘 개발되고 검증된 패키지들을 사용하는 것이 현명한 접근법이다.\n* 기존에 검증된 패키지로 처리 불가한 예외적인 부분들만 커스터마이징해서 처리하면 된다.\n\n### 영어 토큰화 도구\n\n#### 단어 토큰화: TreebankWordTokenizer\n\nPenn Treebank 코퍼스의 토큰화 규칙을 따르는 검증된 도구이다.\n\n```python\nfrom nltk.tokenize import TreebankWordTokenizer\n\ntokenizer = TreebankWordTokenizer()\ntext = \"He said, \\\"I can't believe it's working!\\\" Dr. Smith agreed.\"\ntokens = tokenizer.tokenize(text)\nprint(tokens)\n# ['He', 'said', ',', '``', 'I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'working', '!', \"''\", 'Dr.', 'Smith', 'agreed', '.']\n```\n\n**장점:**\n- 축약형을 적절히 분리 (\"can't\" → \"ca\", \"n't\")\n- 인용문 처리 (`` `` 와 '' 로 변환)\n- 약어 보존 (\"Dr.\" 유지)\n- 수십 년간 검증된 규칙\n\n#### 문장 토큰화: NLTK Sentence Tokenizer\n\n```python\nfrom nltk.tokenize import sent_tokenize\n\ntext = \"Dr. Smith earned his Ph.D. from MIT. He works at NASA. Contact him at john.doe@company.com.\"\nsentences = sent_tokenize(text)\nprint(sentences)\n# ['Dr. Smith earned his Ph.D. from MIT.', 'He works at NASA.', 'Contact him at john.doe@company.com.']\n```\n\n**특징:**\n- 약어 목록을 내장하여 문맥 고려\n- 이메일 주소 내 마침표 구분\n- 다양한 언어 지원\n\n### 한국어 토큰화 도구\n\n#### 문장 토큰화: KSS (Korean Sentence Splitter)\n\n한국어 문장 분할에 특화된 고성능 라이브러리이다.\n\n```python\nimport kss\n\ntext = \"안녕하세요. 제 이메일은 user@domain.co.kr입니다. 연락 주세요!\"\nsentences = kss.split_sentences(text)\nprint(sentences)\n# ['안녕하세요.', '제 이메일은 user@domain.co.kr입니다.', '연락 주세요!']\n```\n\n**장점:**\n- 한국어 특화 규칙\n- 이메일, URL 등 특수 패턴 인식\n- 높은 정확도와 빠른 처리 속도\n\n#### 단어 토큰화: KoNLPy 생태계\n\n```python\nfrom konlpy.tag import Okt, Komoran, Hannanum\n\n# Okt (Open Korean Text)\nokt = Okt()\ntext = \"아버지가방에들어가신다\"\ntokens = okt.morphs(text)\nprint(tokens)\n# ['아버지', '가', '방', '에', '들어가', '신다']\n\n# Komoran\nkomoran = Komoran()\ntokens = komoran.morphs(text)\nprint(tokens)\n# ['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다']\n```\n\n### 다국어 및 고급 토큰화 도구\n\n#### spaCy\n\n산업 수준의 NLP 라이브러리\n\n```python\nimport spacy\n\n# 영어\nnlp_en = spacy.load(\"en_core_web_sm\")\ndoc = nlp_en(\"Dr. Smith's email is john@company.com. He earned his Ph.D. in 1995.\")\nsentences = [sent.text for sent in doc.sents]\ntokens = [token.text for token in doc]\n\n# 한국어 (spacy-korean)\nnlp_ko = spacy.load(\"ko_core_news_sm\")\ndoc = nlp_ko(\"안녕하세요. 저는 데이터 과학자입니다.\")\nsentences = [sent.text for sent in doc.sents]\n```\n\n**특징:**\n- 다양한 언어 지원\n- 빠른 처리 속도\n- 통합된 NLP 파이프라인\n- 산업 환경에 최적화\n\n#### Transformers 토크나이저\n\n최신 딥러닝 모델에서 사용되는 서브워드 토큰화이다.\n\n```python\nfrom transformers import AutoTokenizer\n\n# BERT 토크나이저\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntext = \"Hello, I can't believe it's working!\"\ntokens = tokenizer.tokenize(text)\nprint(tokens)\n# ['hello', ',', 'i', 'can', \"'\", 't', 'believe', 'it', \"'\", 's', 'working', '!']\n\n# 한국어 BERT\ntokenizer_ko = AutoTokenizer.from_pretrained(\"klue/bert-base\")\ntext = \"안녕하세요. 한국어 토큰화입니다.\"\ntokens = tokenizer_ko.tokenize(text)\nprint(tokens)\n# ['안녕', '##하세요', '.', '한국어', '토큰', '##화', '##입니다', '.']\n```\n\n### 도메인별 특화 도구\n\n#### 소셜미디어: TweetTokenizer\n\n```python\nfrom nltk.tokenize import TweetTokenizer\n\ntknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\ntext = \"@user This is sooooo cool! 😊 http://example.com #NLP\"\ntokens = tknzr.tokenize(text)\nprint(tokens)\n# ['This', 'is', 'sooo', 'cool', '!', '😊', 'http://example.com', '#NLP']\n```\n\n#### 의학/과학 텍스트: SciSpaCy\n\n```python\nimport spacy\nimport scispacy\n\nnlp = spacy.load(\"en_core_sci_sm\")\ntext = \"The patient has a temperature of 101.5°F. Administer 2.5mg of medication.\"\ndoc = nlp(text)\nsentences = [sent.text for sent in doc.sents]\n```\n\n### 언어별 권장 도구 조합\n\n#### 영어\n```python\n# 실용적인 영어 처리 파이프라인\nfrom nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n\ndef process_english_text(text):\n    # 1. 문장 분할\n    sentences = sent_tokenize(text)\n    \n    # 2. 단어 토큰화\n    word_tokenizer = TreebankWordTokenizer()\n    tokenized_sentences = []\n    for sentence in sentences:\n        words = word_tokenizer.tokenize(sentence)\n        tokenized_sentences.append(words)\n    \n    return sentences, tokenized_sentences\n```\n\n#### 한국어\n```python\n# 실용적인 한국어 처리 파이프라인\nimport kss\nfrom konlpy.tag import Okt\n\ndef process_korean_text(text):\n    # 1. 문장 분할\n    sentences = kss.split_sentences(text)\n    \n    # 2. 형태소 분석\n    okt = Okt()\n    tokenized_sentences = []\n    for sentence in sentences:\n        morphs = okt.morphs(sentence)\n        tokenized_sentences.append(morphs)\n    \n    return sentences, tokenized_sentences\n```\n\n### 성능과 정확도 비교\n\n#### 처리 속도 (상대적 비교)\n- **spaCy**: 매우 빠름 (산업용)\n- **KSS**: 빠름 (한국어 특화)\n- **TreebankWordTokenizer**: 보통\n- **KoNLPy**: 보통-느림 (정확도 높음)\n\n#### 정확도 (도메인별)\n- **일반 텍스트**: spaCy, NLTK\n- **소셜미디어**: TweetTokenizer\n- **학술/의학**: SciSpaCy\n- **한국어**: KSS + KoNLPy\n\n### 선택 기준\n\n#### 프로젝트 요구사항별 선택\n1. **프로토타이핑**: NLTK (간편함)\n2. **제품 환경**: spaCy (속도와 안정성)\n3. **한국어 중심**: KSS + KoNLPy\n4. **딥러닝 모델**: Transformers 토크나이저\n5. **특수 도메인**: 도메인별 특화 도구\n\n#### 실무 권장사항\n```python\n# 범용적인 다국어 처리 환경 구축\nimport spacy\nimport kss\nfrom transformers import AutoTokenizer\n\nclass UniversalTokenizer:\n    def __init__(self):\n        self.en_nlp = spacy.load(\"en_core_web_sm\")\n        self.bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n    \n    def tokenize_sentences(self, text, language='auto'):\n        if language == 'ko':\n            return kss.split_sentences(text)\n        elif language == 'en':\n            doc = self.en_nlp(text)\n            return [sent.text for sent in doc.sents]\n        else:\n            # 자동 언어 감지 로직\n            return self._auto_detect_and_tokenize(text)\n    \n    def tokenize_for_model(self, text):\n        return self.bert_tokenizer.tokenize(text)\n```\n\n### 결론\n\n* 토큰화는 복잡한 언어학적 문제이므로, 수년간 연구되고 검증된 도구들을 활용하는 것이 가장 효율적이고 안정적인 접근법이다. \n* 각 도구의 특성을 이해하고 프로젝트의 요구사항에 맞는 적절한 조합을 선택하는 것이 성공적인 NLP 프로젝트의 첫걸음이다.\n\n## 토큰화의 도전과제\n\n### 언어별 특성\n\n**한국어:**\n- 어미 변화가 복잡\n- 띄어쓰기 규칙이 일관되지 않음\n- 복합어 처리 어려움\n\n**영어:**\n- 축약형 처리 (예: \"don't\", \"I'm\")\n- 하이픈으로 연결된 단어\n- 대소문자 처리\n\n### 도메인별 특수성\n\n**소셜미디어:**\n- 이모티콘과 이모지\n- 해시태그와 멘션\n- 비표준 언어 사용\n\n**의학/법률 텍스트:**\n- 전문 용어\n- 약어와 기호\n- 정확성이 중요\n\n### Vocabulary\n\n* **Vocabulary**란 텍스트를 토큰화하고, 고유한 토큰들로 이루어진 집합으로 쉽게 말해 기계가 알고있는 단어들의 집합이다.\n* 기계가 텍스트를 처리하기 위해서는 이를 숫자 형태로 변환하는 과정이 필요하다.\n* 이를 위해 먼저 텍스트를 토큰화하고, 토큰을 숫자로 매핑하는 과정이 필요하다.\n  1. 텍스트를 의미 있는 단위인 토큰으로 분할하고(토큰화), \n  2. 각 고유 토큰에 숫자(정수 인덱스 등)를 부여하여 어휘집(Vocabulary)을 구축한 후, \n  3. 이 어휘집을 바탕으로 원래의 토큰 시퀀스를 숫자 시퀀스로 변환하는 단계를 포함합니다.\n* 이렇게 매핑된 숫자를 통해 기계가 텍스트를 이해할 수 있도록 한다.\n\n### Out-of-Vocabulary (OOV) 문제\n\n**문제:**\n- 모델이 학습한 어휘집(Vocabulary)에 포함되지 않은 단어가 입력으로 들어올 때 발생\n- 이는 특히 다음과 같은 경우에 문제가 된다:\n  - **새로운 단어**: 신조어, 기술 용어, 브랜드 이름 등\n  - **오타 및 비표준 표현**: 사용자의 입력 실수나 비공식적인 언어 사용\n  - **다양한 언어적 변형**: 복합어, 축약형, 방언 등\n- OOV 단어가 포함된 문장을 제대로 이해하지 못하거나, 잘못된 예측을 할 수 있다.\n\n**해결 방안:**\n- **서브워드 토큰화 사용**\n  - 최근 가장 널리 사용되는 방법 중 하나로, 다양한 자연어 처리 모델에서 효과적으로 활용되고 있다.\n  - 이 방법들은 단어를 더 작은 의미 단위로 분할하여 처리하므로, 새로운 단어가 등장하더라도 부분적으로 이해할 수 있다.\n- **문자 단위 처리**\n  - 단어를 문자 단위로 분할하여 처리하는 방법도 있다. \n  - 이는 모든 단어를 개별 문자로 분해하여 처리하므로, OOV 문제를 근본적으로 해결할 수 있다. \n  - 그러나 이 방법은 문맥 이해가 어려워질 수 있다.\n- **사전 확장**\n  - 지속적으로 어휘집을 업데이트하여 새로운 단어를 포함시키는 방법. \n  - 이는 시간이 지남에 따라 어휘집을 확장하여 OOV 문제를 줄일 수 있다.\n\n\n## 평가 메트릭\n\n### 어휘 크기 (Vocabulary Size)\n\n- 모델의 메모리 사용량과 직결\n- 너무 크면 비효율적, 너무 작으면 표현력 부족\n\n### 압축률 (Compression Rate)\n\n- 원본 텍스트 대비 토큰 수의 비율\n- 효율적인 표현을 위해 중요\n\n### 의미 보존도\n\n- 토큰화 후에도 원래 의미가 유지되는 정도\n- 정성적 평가가 주로 사용됨\n\n## 실제 적용 사례\n\n### 기계 번역\n\n- 다국어 처리를 위한 공통 서브워드 어휘\n- 언어 간 토큰 정렬\n\n### 감정 분석\n\n- 감정을 나타내는 키워드 보존\n- 이모티콘과 특수 문자 처리\n\n### 질의응답 시스템\n\n- 질문과 답변의 일관된 토큰화\n- 개체명 인식과의 연계\n\n## 최신 동향\n\n### 다국어 토큰화\n\n- mBERT, XLM-R 등 다국어 모델\n- 언어별 특성을 고려한 통합 토큰화\n\n### 적응형 토큰화\n\n- 도메인별 최적화\n- 동적 어휘 확장\n\n### 신경망 기반 토큰화\n\n- 학습 가능한 토큰화\n- End-to-end 학습\n\n## 결론\n\n* 토큰화는 자연어 처리의 첫 단계이자 전체 성능을 좌우하는 중요한 과정. \n* 언어의 특성과 도메인의 요구사항을 고려하여 적절한 토큰화 방법을 선택하는 것이 중요. \n* 최근에는 서브워드 토큰화가 주류가 되었지만, 여전히 각 방법의 장단점을 이해하고 상황에 맞게 적용하는 것이 필요. \n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"2.pre_prcs_tokenization.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"토큰화 (Tokenization)","subtitle":"자연어 처리의 첫 번째 단계 - 텍스트를 기계가 이해할 수 있는 단위로 분할","description":"토큰화는 자연어 처리의 첫 번째이자 가장 중요한 전처리 과정이다. 텍스트를 의미 있는 단위로 분할하여 기계가 처리할 수 있도록 변환하는 과정을 다룬다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-02","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}