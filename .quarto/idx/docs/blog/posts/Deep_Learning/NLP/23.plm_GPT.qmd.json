{"title":"GPT: Generative Pre-trained Transformer","markdown":{"yaml":{"title":"GPT: Generative Pre-trained Transformer","subtitle":"생성형 언어 모델의 혁신과 대화형 AI의 출발점","description":"GPT는 Transformer 디코더 기반의 생성형 사전 학습 모델로 자연어 생성 분야에 혁신을 가져왔다. Next Token Prediction을 통한 사전 학습 방식, 강력한 텍스트 생성 능력, 그리고 In-Context Learning을 통한 Few-Shot 학습 능력을 분석한다. GPT의 구조, 학습 방법, 각 버전별 발전 과정과 함께 ChatGPT로 이어지는 생성형 AI 혁명의 시작점으로서의 의미를 다룬다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-23","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false},"headingText":"요약","containsRefs":false,"markdown":"\n\n\nGPT(Generative Pre-trained Transformer)는 2018년 OpenAI에서 발표한 혁신적인 생성형 사전 학습 언어 모델이다. 기존의 이해 중심 모델들과 달리 텍스트 생성에 특화되어 강력한 언어 생성 능력을 보여주었으며, 현재 ChatGPT로 이어지는 생성형 AI 혁명의 출발점이 되었다.\n\n주요 특징과 혁신 사항은 다음과 같다:\n\n* **생성형 언어 모델링**:\n  - Transformer 디코더 구조를 사용하여 순차적 텍스트 생성에 최적화\n  - Causal Self-Attention으로 이전 토큰들만을 참조하는 일방향 처리\n  - Next Token Prediction을 통한 자기회귀적 텍스트 생성\n* **혁신적인 사전 학습 방식**:\n  - **Next Token Prediction**: 이전 토큰들을 바탕으로 다음 토큰을 예측하는 단순하면서도 강력한 학습 목표\n  - 대규모 텍스트 데이터에서 언어의 패턴과 구조를 학습\n  - 문맥을 이해하고 일관성 있는 긴 텍스트 생성 능력 획득\n* **확장성과 성능 향상**:\n  - GPT-1(117M) → GPT-2(1.5B) → GPT-3(175B) → GPT-4(추정 1T+)로 모델 크기 확장\n  - 데이터 양과 모델 크기 증가만으로도 성능이 지속적으로 향상됨을 입증\n  - Scaling Laws를 통한 성능 예측 가능성 제시\n* **In-Context Learning과 Few-Shot 능력**:\n  - 별도 Fine-tuning 없이도 예시만으로 새로운 태스크 수행\n  - Prompt Engineering을 통한 다양한 응용 가능성\n  - Zero-shot, One-shot, Few-shot Learning의 강력한 성능\n* **생성형 AI 패러다임 확립**:\n  - 단순한 분류/이해를 넘어선 창조적 텍스트 생성\n  - ChatGPT, GPT-4 등으로 이어지는 대화형 AI의 기반 마련\n  - 코드 생성, 창작, 번역, 요약 등 광범위한 생성 태스크에서 인간 수준 성능\n\nGPT의 등장은 자연어 처리를 이해 중심에서 생성 중심으로 패러다임을 전환시켰으며, 현재 우리가 경험하고 있는 생성형 AI 시대의 기초를 마련했다.\n\n# NLP 모델 발전 과정\n\n```\nRNN Language Model\n├── Seq2Seq\n├── Beam Search\n├── Subword Tokenization\n├── Attention\n├── Transformer Encoder (Vaswani et al., 2017)\n|   ├── Positional Encoding\n|   ├── Multi-Head Attention\n|   └── Feed Forward Neural Network\n|\n├── Transformer Decoder (Vaswani et al., 2017)\n|\n├── GPT 시리즈 (OpenAI,2018~)\n|   ├── GPT-1~4\n|   └── ChatGPT (OpenAI,2022~)\n|\n├── BERT 시리즈 (Google,2018~)\n|   ├── BERT\n|   ├── RoBERTa\n|   └── ALBERT\n|\n├── BERT 변형 모델들\n|   ├── RoBERTa (Facebook, 2019)\n|   ├── ALBERT (Google, 2019)\n|   ├── DistilBERT (Hugging Face, 2019)\n|   └── ELECTRA (Google, 2020)\n|\n└── 후속 발전 모델들\n    ├── T5, XLNet, DeBERTa\n    └── ChatGPT, PaLM, Claude, Gemini 등\n```\n\n# GPT 이전 모델들의 한계점\n\n## 기존 언어 모델의 문제점\n\n* **일방향성의 한계**:\n  - 기존 RNN 기반 언어 모델은 순차적 처리로 인한 병렬화 불가\n  - 긴 시퀀스에서의 그래디언트 소실 문제\n  - 문맥 정보의 제한적 활용\n  \n* **제한적인 전이 학습**:\n  - 태스크별로 별도의 모델 아키텍처 필요\n  - 사전 학습된 표현의 활용도 제한\n  - Fine-tuning 과정에서 많은 라벨 데이터 요구\n\n* **생성 능력의 부족**:\n  - 주로 분류나 이해 태스크에 집중\n  - 창조적이고 일관성 있는 텍스트 생성의 어려움\n  - 다양한 도메인과 스타일에 대한 적응력 부족\n\n# GPT (Generative Pre-trained Transformer)\n\n## 개요와 기본 개념\n\n* GPT는 **생성형 사전 학습 트랜스포머**로, 2018년 OpenAI에서 발표한 혁신적인 언어 모델이다. \n* 기존의 이해 중심 모델들과 달리 **텍스트 생성**에 특화되어 설계되었으며, 현재 ChatGPT로까지 이어지는 생성형 AI 혁명의 시발점이 되었다.\n\n### 핵심 아이디어\n\n* **단순함의 힘**: Next Token Prediction이라는 단순한 목표로 복잡한 언어 능력 학습\n* **확장성**: 모델 크기와 데이터 양을 늘리는 것만으로도 성능 향상 가능\n* **범용성**: 하나의 모델로 다양한 생성 태스크 수행\n\n## 아키텍처 상세\n\n### Transformer 디코더 기반 구조\n\n```\n입력 토큰들 → 토큰 임베딩 + 위치 임베딩\n    ↓\nTransformer 디코더 블록 × N\n├── Masked Multi-Head Self-Attention\n├── Layer Normalization\n├── Feed-Forward Network\n└── Layer Normalization\n    ↓\n출력 Linear Layer → 다음 토큰 확률 분포\n```\n\n### 핵심 구성 요소\n\n* **Causal Self-Attention**:\n  - 이전 위치의 토큰들만 참조 가능 (미래 정보 차단)\n  - 순차적 생성 과정에서 정보 누출 방지\n  - 문장을 왼쪽부터 오른쪽으로 읽으면서, 현재 단어를 이해할 때 이미 본 단어들만 참고할 수 있다\n  - 예: \"나는 오늘 학교에 갔다\"에서 \"학교\"를 처리할 때, \"나는\", \"오늘\"만 참고 가능하고 \"갔다\"는 참고 불가\n  - 이렇게 하는 이유는 텍스트 생성 시 다음 단어를 예측해야 하는데, 미래 단어를 미리 알면 부정행위가 되기 때문\n  - Attention score matrix에서 상삼각 부분을 -∞로 마스킹\n  - Softmax 적용 후 미래 위치의 attention weight가 0이 됨\n  - 결과적으로 현재 위치 이전의 토큰들만 정보 제공\n  - 참고: Self-Attention\n    - 한 문장 내에서 각 단어가 다른 모든 단어들과 얼마나 관련이 있는지를 계산하는 메커니즘\n    - 문장: \"그 강아지는 공원에서 뛰어다니며 즐거워했다\"\n    - 각 단어를 처리할 때:\n      - \"강아지\"를 이해하려면 → \"뛰어다니며\", \"즐거워했다\"와 연결해서 생각\n      - \"뛰어다니며\"를 이해하려면 → \"강아지\", \"공원에서\"와 연결해서 생각\n      - \"즐거워했다\"를 이해하려면 → \"강아지\"와 강하게 연결해서 생각\n    - 핵심: 각 단어가 문장의 다른 모든 단어들을 \"쳐다보면서\" 관련성을 파악\n\n* **위치 인코딩**:\n  - Transformer는 본질적으로 순서를 모르는 구조이므로, 토큰의 위치 정보를 별도로 주입해야 한다.\n  - 학습 가능한 절대 위치 임베딩 사용\n  - 토큰의 순서 정보를 모델에 제공\n  - 단어 카드를 무작위로 섞어놓으면 문장의 의미가 바뀌는 것처럼, AI도 단어의 순서를 알아야 함\n  - \"강아지가 고양이를 쫓았다\"와 \"고양이가 강아지를 쫓았다\"는 완전히 다른 의미\n  - 각 위치(1번째, 2번째, 3번째...)에 고유한 \"위치 ID카드\"를 부여하는 개념\n  - 학습 가능한 위치 임베딩 사용 (고정된 삼각함수 대신)\n  - 각 위치마다 별도의 벡터를 학습하여 토큰 임베딩에 더함\n  - 최대 시퀀스 길이까지만 위치 정보 제공 가능\n\n* **Layer Normalization**:\n  - 딥러닝에서 학습을 안정화하고 빠르게 만드는 정규화 기법\n  - 각 서브레이어 이후 정규화 적용\n  - 요리할 때 재료들의 크기를 비슷하게 맞춰서 골고루 익도록 하는 것과 유사\n  - 신경망 각 층에서 데이터의 분포가 너무 치우치거나 분산되지 않도록 조정\n  - 마치 각 층마다 \"데이터 정리정돈\"을 해주는 역할\n  - 학습 안정성과 수렴 속도 향상\n  - Pre-LN(Pre Layer Normalization) 구조로 깊은 네트워크 학습 가능\n    - 각 Transformer 블록 내부의 서브레이어(attention, feedforward) 이후에 적용\n    - Post-LN 구조: 서브레이어 입력 전에 정규화 (원래 Transformer는 Post-LN)\n    - 깊은 네트워크에서도 gradient가 잘 전달되어 학습이 안정적\n    - 정규화 없이는 깊은 네트워크에서 gradient vanishing/exploding 문제 발생\n    - 학습 속도가 빨라지고 더 높은 학습률 사용 가능\n    - 각 층의 입력 분포가 안정적이어서 일관된 학습 가능\n\n## 학습 방법\n\n### 사전 학습 (Pre-training)\n\n**목표**: Next Token Prediction\n$$ P(w_t | w_1, w_2, ..., w_{t-1}) = \\text{softmax}(W_o \\cdot h_t) $$\n\n* **데이터**: 대규모 인터넷 텍스트 (CommonCrawl, WebText, Books, Wikipedia 등)\n* **손실 함수**: Cross-Entropy Loss\n$$ \\mathcal{L} = -\\sum_{t=1}^{T} \\log P(w_t | w_1, ..., w_{t-1}) $$\n\n* **학습 과정**:\n  1. 입력 시퀀스의 각 위치에서 다음 토큰 예측\n  2. 실제 토큰과 예측 분포 간의 크로스 엔트로피 최소화\n  3. 자기회귀적 생성을 통한 언어 패턴 학습\n\n### Fine-tuning\n\n**기존 방식**:\n* 태스크별 특수 토큰 추가 (예: `[CLS]`, `[SEP]`)\n* 출력 레이어만 태스크에 맞게 수정\n* 적은 양의 라벨 데이터로 추가 학습\n\n**GPT-3 이후**:\n* **In-Context Learning**: Fine-tuning 없이 예시만으로 태스크 수행\n* **Prompt Engineering**: 적절한 프롬프트 설계를 통한 성능 최적화\n* **Few-Shot Learning**: 소수 예시로 새로운 태스크 학습\n\n## GPT 버전별 발전 과정\n\n### GPT-1 (2018년 6월)\n\n* **모델 크기**: 117M 파라미터\n* **데이터**: BooksCorpus (7,000권의 책)\n* **혁신 사항**:\n  - Transformer 디코더 기반 언어 모델 최초 제안\n  - Unsupervised Pre-training + Supervised Fine-tuning 패러다임 확립\n  - 다양한 NLP 태스크에서 기존 모델 대비 성능 향상\n\n### GPT-2 (2019년 2월)\n\n* **모델 크기**: 1.5B 파라미터 (Small: 117M, Medium: 345M, Large: 762M, XL: 1.5B)\n* **데이터**: WebText (800만 개 웹페이지, 40GB)\n* **혁신 사항**:\n  - 모델 크기 대폭 확장 (10배 증가)\n  - Zero-shot 태스크 수행 능력 확인\n  - 고품질 텍스트 생성으로 인한 오남용 우려 (초기 공개 제한)\n  - \"더 큰 모델이 더 좋은 성능\"이라는 스케일링 법칙 입증\n\n### GPT-3 (2020년 5월)\n\n* **모델 크기**: 175B 파라미터\n* **데이터**: 570GB 텍스트 (CommonCrawl, WebText2, Books1/2, Wikipedia)\n* **혁신 사항**:\n  - **In-Context Learning**: Fine-tuning 없이 예시만으로 태스크 수행\n  - **Few-Shot Learning**: 소수 예시로 새로운 태스크 학습\n  - 인간 수준의 텍스트 생성 품질\n  - 코딩, 수학, 추론 등 다양한 도메인에서 놀라운 성능\n  - API 형태로 서비스 제공하여 생성형 AI 생태계 구축\n\n### GPT-4 (2023년 3월)\n\n* **모델 크기**: 공개되지 않음 (추정 1T+ 파라미터)\n* **데이터**: 다중 모달 데이터 (텍스트 + 이미지)\n* **혁신 사항**:\n  - **다중 모달 능력**: 텍스트와 이미지 동시 처리\n  - 더욱 향상된 추론 능력과 안전성\n  - 긴 컨텍스트 처리 능력 (32K 토큰)\n  - 전문 시험에서 인간 수준 또는 그 이상의 성능\n\n## GPT의 핵심 혁신\n\n### 스케일링 법칙 (Scaling Laws)\n$$\\text{Loss} \\propto N^{-\\alpha}$$\n\n* 모델 크기(N), 데이터 양, 계산량이 증가할수록 성능 향상\n* 예측 가능한 성능 개선 곡선\n* \"더 크면 더 좋다\"는 단순하지만 강력한 원리\n\n### 창발적 능력 (Emergent Abilities)\n\n* **특정 임계점**을 넘으면 갑자기 나타나는 새로운 능력들\n* **In-Context Learning**: GPT-3에서 처음 관찰\n* **Chain-of-Thought Reasoning**: 단계별 추론 능력\n* **코드 생성**: 프로그래밍 언어 이해와 생성\n\n### 인간 피드백 강화 학습 (RLHF)\n\n**InstructGPT와 ChatGPT에 도입**:\n1. **Supervised Fine-tuning**: 고품질 대화 데이터로 학습\n2. **Reward Model**: 인간 선호도 기반 보상 모델 학습\n3. **PPO**: 보상 모델을 사용한 강화 학습\n\n### Prompt Engineering의 발전\n\n* **Zero-shot**: 예시 없이 태스크 수행\n* **Few-shot**: 소수 예시로 태스크 학습\n* **Chain-of-Thought**: 단계별 추론 과정 명시\n* **Constitutional AI**: 원칙 기반 행동 유도\n\n## GPT의 강점과 한계\n\n### 강점\n\n* **뛰어난 생성 능력**: 일관성 있고 창의적인 텍스트 생성\n* **범용성**: 하나의 모델로 다양한 태스크 수행\n* **적응성**: 프롬프트만으로 새로운 태스크 수행\n* **확장성**: 모델 크기 증가로 성능 향상 가능\n\n### 한계\n\n* **사실성 문제**: 할루시네이션 (거짓 정보 생성)\n* **편향성**: 학습 데이터의 편향 반영\n* **해석 가능성**: 내부 동작 원리의 불투명성\n* **계산 비용**: 대규모 모델의 높은 추론 비용\n\n## 현재적 의미와 영향\n\nGPT는 단순한 기술 발전을 넘어 **인간-AI 상호작용의 패러다임**을 바꾸었다:\n\n* **ChatGPT 현상**: 일반 대중의 AI 접근성 혁신\n* **생성형 AI 생태계**: 수많은 응용 서비스와 스타트업 등장\n* **업무 방식 변화**: 글쓰기, 코딩, 창작 등 지식 작업의 혁신\n* **교육 패러다임 변화**: AI 활용 능력의 중요성 대두\n\n# 결론\n\n* GPT는 자연어 처리 분야에서 **생성형 AI 혁명**의 출발점이 된 가장 중요한 혁신 중 하나다. \n* 2018년 첫 발표 이후 현재까지 NLP 연구와 실용 AI의 패러다임을 완전히 바꾸어 놓았다.\n\n### GPT의 핵심 기여\n\n* **생성형 언어 모델의 확립**: 이해 중심에서 생성 중심으로 NLP 패러다임 전환, Next Token Prediction이라는 단순한 목표로 복잡한 언어 능력 획득\n* **스케일링 법칙의 입증**: 모델 크기와 데이터 양 증가만으로도 성능이 예측 가능하게 향상됨을 보여주어 대규모 AI 개발의 방향 제시\n* **In-Context Learning의 발견**: Fine-tuning 없이도 예시만으로 새로운 태스크를 수행할 수 있는 혁신적 능력으로 AI 활용 방식을 근본적으로 변화\n* **창발적 능력의 관찰**: 특정 규모를 넘으면 갑자기 나타나는 추론, 코딩, 창작 등의 고차원적 능력으로 AI 발전의 새로운 가능성 제시\n\n### 생성형 AI 생태계의 탄생\n\nGPT의 등장은 단순한 기술 발전을 넘어 완전히 새로운 **생성형 AI 생태계**를 만들어냈다:\n\n* **ChatGPT 현상**: 2022년 ChatGPT 출시로 일반 대중이 AI와 자연어로 대화하는 새로운 경험 제공, 전 세계적인 AI 관심과 활용 폭발적 증가\n* **산업 생태계 변화**: 수많은 GPT 기반 서비스와 스타트업 등장, 기존 산업의 디지털 전환 가속화\n* **생산성 혁신**: 글쓰기, 코딩, 번역, 요약 등 지식 작업의 자동화 및 보조 도구로 활용되어 업무 효율성 획기적 향상\n* **창작과 교육 변화**: AI와의 협업을 통한 새로운 창작 방식 등장, 교육 방법과 평가 체계의 근본적 재고\n\n### 기술적 영향과 후속 발전\n\nGPT가 확립한 기술적 기반은 이후 모든 언어 AI 발전의 토대가 되었다:\n\n* **아키텍처 표준화**: Transformer 디코더 기반 구조가 생성형 언어 모델의 표준이 됨\n* **훈련 방법론**: Next Token Prediction과 RLHF(인간 피드백 강화 학습)가 대화형 AI 개발의 핵심 방법론으로 자리잡음\n* **다중 모달 확장**: GPT-4의 텍스트-이미지 처리 능력을 시작으로 멀티모달 AI 발전의 기반 마련\n* **경쟁 모델들의 등장**: Google의 Gemini, Anthropic의 Claude, Meta의 Llama 등 다양한 대안 모델들의 개발 촉진\n\n### 사회적 변화와 도전 과제\n\nGPT는 기술적 혁신을 넘어 사회 전반에 깊은 영향을 미치고 있다:\n\n### 긍정적 영향\n\n* **접근성 혁신**: 복잡한 기술 지식 없이도 자연어로 AI 활용 가능\n* **창의성 증진**: AI와의 협업을 통한 새로운 아이디어 창출과 표현 방식 확장\n* **교육 개인화**: 맞춤형 학습 지원과 즉시 피드백 제공\n* **언어 장벽 해소**: 실시간 번역과 다국어 소통 지원\n\n### 해결 과제\n\n* **진실성과 신뢰성**: 할루시네이션 문제와 잘못된 정보 생성 위험\n* **윤리적 책임**: AI 생성 콘텐츠의 책임 소재와 저작권 문제\n* **사회적 불평등**: AI 접근성 격차와 일자리 대체 우려\n* **안전성 확보**: 악용 방지와 AI 정렬(Alignment) 문제\n\n## 미래 전망과 발전 방향\n\nGPT가 열어놓은 생성형 AI의 미래는 다음과 같은 방향으로 발전할 것으로 예상된다:\n\n### 기술적 발전\n\n* **더 큰 규모**: 수조 파라미터 규모의 모델과 더욱 방대한 학습 데이터\n* **효율성 개선**: 추론 속도 향상과 계산 비용 감소를 위한 최적화 기술\n* **전문화**: 도메인별 특화 모델과 개인화된 AI 어시스턴트\n* **다중 모달**: 텍스트, 이미지, 음성, 비디오를 통합한 범용 AI\n\n### 응용 분야 확장\n\n* **과학 연구**: 논문 작성, 가설 생성, 실험 설계 지원\n* **의료 분야**: 진단 보조, 치료법 연구, 의료 문서 작성\n* **법률 서비스**: 계약서 분석, 판례 검색, 법률 문서 작성\n* **예술과 미디어**: 소설, 시나리오, 음악 창작의 새로운 방법론\n\n## 역사적 의미\n\nGPT의 등장은 **인공지능 역사의 중요한 전환점**이다. 1950년대 튜링 테스트 제안, 1980년대 신경망 부흥, 2010년대 딥러닝 혁명에 이어, GPT는 **AI가 인간과 자연스럽게 소통할 수 있는 시대**를 열었다.\n\n특히 ChatGPT의 대중적 성공은 AI를 전문가들만의 도구에서 일반인도 일상적으로 사용하는 기술로 바꾸어 놓았다. 이는 개인용 컴퓨터나 인터넷의 등장에 비견될 만한 기술적, 사회적 변화를 의미한다.\n\nGPT로 시작된 생성형 AI 시대는 이제 막 시작되었으며, 향후 인간의 창조적 활동, 학습 방식, 의사소통 패턴까지 근본적으로 변화시킬 것이다. 이러한 변화의 중심에서 GPT는 **AI와 인간이 협력하는 새로운 시대의 출발점**으로 역사에 기록될 것이다.\n","srcMarkdownNoYaml":"\n\n# 요약\n\nGPT(Generative Pre-trained Transformer)는 2018년 OpenAI에서 발표한 혁신적인 생성형 사전 학습 언어 모델이다. 기존의 이해 중심 모델들과 달리 텍스트 생성에 특화되어 강력한 언어 생성 능력을 보여주었으며, 현재 ChatGPT로 이어지는 생성형 AI 혁명의 출발점이 되었다.\n\n주요 특징과 혁신 사항은 다음과 같다:\n\n* **생성형 언어 모델링**:\n  - Transformer 디코더 구조를 사용하여 순차적 텍스트 생성에 최적화\n  - Causal Self-Attention으로 이전 토큰들만을 참조하는 일방향 처리\n  - Next Token Prediction을 통한 자기회귀적 텍스트 생성\n* **혁신적인 사전 학습 방식**:\n  - **Next Token Prediction**: 이전 토큰들을 바탕으로 다음 토큰을 예측하는 단순하면서도 강력한 학습 목표\n  - 대규모 텍스트 데이터에서 언어의 패턴과 구조를 학습\n  - 문맥을 이해하고 일관성 있는 긴 텍스트 생성 능력 획득\n* **확장성과 성능 향상**:\n  - GPT-1(117M) → GPT-2(1.5B) → GPT-3(175B) → GPT-4(추정 1T+)로 모델 크기 확장\n  - 데이터 양과 모델 크기 증가만으로도 성능이 지속적으로 향상됨을 입증\n  - Scaling Laws를 통한 성능 예측 가능성 제시\n* **In-Context Learning과 Few-Shot 능력**:\n  - 별도 Fine-tuning 없이도 예시만으로 새로운 태스크 수행\n  - Prompt Engineering을 통한 다양한 응용 가능성\n  - Zero-shot, One-shot, Few-shot Learning의 강력한 성능\n* **생성형 AI 패러다임 확립**:\n  - 단순한 분류/이해를 넘어선 창조적 텍스트 생성\n  - ChatGPT, GPT-4 등으로 이어지는 대화형 AI의 기반 마련\n  - 코드 생성, 창작, 번역, 요약 등 광범위한 생성 태스크에서 인간 수준 성능\n\nGPT의 등장은 자연어 처리를 이해 중심에서 생성 중심으로 패러다임을 전환시켰으며, 현재 우리가 경험하고 있는 생성형 AI 시대의 기초를 마련했다.\n\n# NLP 모델 발전 과정\n\n```\nRNN Language Model\n├── Seq2Seq\n├── Beam Search\n├── Subword Tokenization\n├── Attention\n├── Transformer Encoder (Vaswani et al., 2017)\n|   ├── Positional Encoding\n|   ├── Multi-Head Attention\n|   └── Feed Forward Neural Network\n|\n├── Transformer Decoder (Vaswani et al., 2017)\n|\n├── GPT 시리즈 (OpenAI,2018~)\n|   ├── GPT-1~4\n|   └── ChatGPT (OpenAI,2022~)\n|\n├── BERT 시리즈 (Google,2018~)\n|   ├── BERT\n|   ├── RoBERTa\n|   └── ALBERT\n|\n├── BERT 변형 모델들\n|   ├── RoBERTa (Facebook, 2019)\n|   ├── ALBERT (Google, 2019)\n|   ├── DistilBERT (Hugging Face, 2019)\n|   └── ELECTRA (Google, 2020)\n|\n└── 후속 발전 모델들\n    ├── T5, XLNet, DeBERTa\n    └── ChatGPT, PaLM, Claude, Gemini 등\n```\n\n# GPT 이전 모델들의 한계점\n\n## 기존 언어 모델의 문제점\n\n* **일방향성의 한계**:\n  - 기존 RNN 기반 언어 모델은 순차적 처리로 인한 병렬화 불가\n  - 긴 시퀀스에서의 그래디언트 소실 문제\n  - 문맥 정보의 제한적 활용\n  \n* **제한적인 전이 학습**:\n  - 태스크별로 별도의 모델 아키텍처 필요\n  - 사전 학습된 표현의 활용도 제한\n  - Fine-tuning 과정에서 많은 라벨 데이터 요구\n\n* **생성 능력의 부족**:\n  - 주로 분류나 이해 태스크에 집중\n  - 창조적이고 일관성 있는 텍스트 생성의 어려움\n  - 다양한 도메인과 스타일에 대한 적응력 부족\n\n# GPT (Generative Pre-trained Transformer)\n\n## 개요와 기본 개념\n\n* GPT는 **생성형 사전 학습 트랜스포머**로, 2018년 OpenAI에서 발표한 혁신적인 언어 모델이다. \n* 기존의 이해 중심 모델들과 달리 **텍스트 생성**에 특화되어 설계되었으며, 현재 ChatGPT로까지 이어지는 생성형 AI 혁명의 시발점이 되었다.\n\n### 핵심 아이디어\n\n* **단순함의 힘**: Next Token Prediction이라는 단순한 목표로 복잡한 언어 능력 학습\n* **확장성**: 모델 크기와 데이터 양을 늘리는 것만으로도 성능 향상 가능\n* **범용성**: 하나의 모델로 다양한 생성 태스크 수행\n\n## 아키텍처 상세\n\n### Transformer 디코더 기반 구조\n\n```\n입력 토큰들 → 토큰 임베딩 + 위치 임베딩\n    ↓\nTransformer 디코더 블록 × N\n├── Masked Multi-Head Self-Attention\n├── Layer Normalization\n├── Feed-Forward Network\n└── Layer Normalization\n    ↓\n출력 Linear Layer → 다음 토큰 확률 분포\n```\n\n### 핵심 구성 요소\n\n* **Causal Self-Attention**:\n  - 이전 위치의 토큰들만 참조 가능 (미래 정보 차단)\n  - 순차적 생성 과정에서 정보 누출 방지\n  - 문장을 왼쪽부터 오른쪽으로 읽으면서, 현재 단어를 이해할 때 이미 본 단어들만 참고할 수 있다\n  - 예: \"나는 오늘 학교에 갔다\"에서 \"학교\"를 처리할 때, \"나는\", \"오늘\"만 참고 가능하고 \"갔다\"는 참고 불가\n  - 이렇게 하는 이유는 텍스트 생성 시 다음 단어를 예측해야 하는데, 미래 단어를 미리 알면 부정행위가 되기 때문\n  - Attention score matrix에서 상삼각 부분을 -∞로 마스킹\n  - Softmax 적용 후 미래 위치의 attention weight가 0이 됨\n  - 결과적으로 현재 위치 이전의 토큰들만 정보 제공\n  - 참고: Self-Attention\n    - 한 문장 내에서 각 단어가 다른 모든 단어들과 얼마나 관련이 있는지를 계산하는 메커니즘\n    - 문장: \"그 강아지는 공원에서 뛰어다니며 즐거워했다\"\n    - 각 단어를 처리할 때:\n      - \"강아지\"를 이해하려면 → \"뛰어다니며\", \"즐거워했다\"와 연결해서 생각\n      - \"뛰어다니며\"를 이해하려면 → \"강아지\", \"공원에서\"와 연결해서 생각\n      - \"즐거워했다\"를 이해하려면 → \"강아지\"와 강하게 연결해서 생각\n    - 핵심: 각 단어가 문장의 다른 모든 단어들을 \"쳐다보면서\" 관련성을 파악\n\n* **위치 인코딩**:\n  - Transformer는 본질적으로 순서를 모르는 구조이므로, 토큰의 위치 정보를 별도로 주입해야 한다.\n  - 학습 가능한 절대 위치 임베딩 사용\n  - 토큰의 순서 정보를 모델에 제공\n  - 단어 카드를 무작위로 섞어놓으면 문장의 의미가 바뀌는 것처럼, AI도 단어의 순서를 알아야 함\n  - \"강아지가 고양이를 쫓았다\"와 \"고양이가 강아지를 쫓았다\"는 완전히 다른 의미\n  - 각 위치(1번째, 2번째, 3번째...)에 고유한 \"위치 ID카드\"를 부여하는 개념\n  - 학습 가능한 위치 임베딩 사용 (고정된 삼각함수 대신)\n  - 각 위치마다 별도의 벡터를 학습하여 토큰 임베딩에 더함\n  - 최대 시퀀스 길이까지만 위치 정보 제공 가능\n\n* **Layer Normalization**:\n  - 딥러닝에서 학습을 안정화하고 빠르게 만드는 정규화 기법\n  - 각 서브레이어 이후 정규화 적용\n  - 요리할 때 재료들의 크기를 비슷하게 맞춰서 골고루 익도록 하는 것과 유사\n  - 신경망 각 층에서 데이터의 분포가 너무 치우치거나 분산되지 않도록 조정\n  - 마치 각 층마다 \"데이터 정리정돈\"을 해주는 역할\n  - 학습 안정성과 수렴 속도 향상\n  - Pre-LN(Pre Layer Normalization) 구조로 깊은 네트워크 학습 가능\n    - 각 Transformer 블록 내부의 서브레이어(attention, feedforward) 이후에 적용\n    - Post-LN 구조: 서브레이어 입력 전에 정규화 (원래 Transformer는 Post-LN)\n    - 깊은 네트워크에서도 gradient가 잘 전달되어 학습이 안정적\n    - 정규화 없이는 깊은 네트워크에서 gradient vanishing/exploding 문제 발생\n    - 학습 속도가 빨라지고 더 높은 학습률 사용 가능\n    - 각 층의 입력 분포가 안정적이어서 일관된 학습 가능\n\n## 학습 방법\n\n### 사전 학습 (Pre-training)\n\n**목표**: Next Token Prediction\n$$ P(w_t | w_1, w_2, ..., w_{t-1}) = \\text{softmax}(W_o \\cdot h_t) $$\n\n* **데이터**: 대규모 인터넷 텍스트 (CommonCrawl, WebText, Books, Wikipedia 등)\n* **손실 함수**: Cross-Entropy Loss\n$$ \\mathcal{L} = -\\sum_{t=1}^{T} \\log P(w_t | w_1, ..., w_{t-1}) $$\n\n* **학습 과정**:\n  1. 입력 시퀀스의 각 위치에서 다음 토큰 예측\n  2. 실제 토큰과 예측 분포 간의 크로스 엔트로피 최소화\n  3. 자기회귀적 생성을 통한 언어 패턴 학습\n\n### Fine-tuning\n\n**기존 방식**:\n* 태스크별 특수 토큰 추가 (예: `[CLS]`, `[SEP]`)\n* 출력 레이어만 태스크에 맞게 수정\n* 적은 양의 라벨 데이터로 추가 학습\n\n**GPT-3 이후**:\n* **In-Context Learning**: Fine-tuning 없이 예시만으로 태스크 수행\n* **Prompt Engineering**: 적절한 프롬프트 설계를 통한 성능 최적화\n* **Few-Shot Learning**: 소수 예시로 새로운 태스크 학습\n\n## GPT 버전별 발전 과정\n\n### GPT-1 (2018년 6월)\n\n* **모델 크기**: 117M 파라미터\n* **데이터**: BooksCorpus (7,000권의 책)\n* **혁신 사항**:\n  - Transformer 디코더 기반 언어 모델 최초 제안\n  - Unsupervised Pre-training + Supervised Fine-tuning 패러다임 확립\n  - 다양한 NLP 태스크에서 기존 모델 대비 성능 향상\n\n### GPT-2 (2019년 2월)\n\n* **모델 크기**: 1.5B 파라미터 (Small: 117M, Medium: 345M, Large: 762M, XL: 1.5B)\n* **데이터**: WebText (800만 개 웹페이지, 40GB)\n* **혁신 사항**:\n  - 모델 크기 대폭 확장 (10배 증가)\n  - Zero-shot 태스크 수행 능력 확인\n  - 고품질 텍스트 생성으로 인한 오남용 우려 (초기 공개 제한)\n  - \"더 큰 모델이 더 좋은 성능\"이라는 스케일링 법칙 입증\n\n### GPT-3 (2020년 5월)\n\n* **모델 크기**: 175B 파라미터\n* **데이터**: 570GB 텍스트 (CommonCrawl, WebText2, Books1/2, Wikipedia)\n* **혁신 사항**:\n  - **In-Context Learning**: Fine-tuning 없이 예시만으로 태스크 수행\n  - **Few-Shot Learning**: 소수 예시로 새로운 태스크 학습\n  - 인간 수준의 텍스트 생성 품질\n  - 코딩, 수학, 추론 등 다양한 도메인에서 놀라운 성능\n  - API 형태로 서비스 제공하여 생성형 AI 생태계 구축\n\n### GPT-4 (2023년 3월)\n\n* **모델 크기**: 공개되지 않음 (추정 1T+ 파라미터)\n* **데이터**: 다중 모달 데이터 (텍스트 + 이미지)\n* **혁신 사항**:\n  - **다중 모달 능력**: 텍스트와 이미지 동시 처리\n  - 더욱 향상된 추론 능력과 안전성\n  - 긴 컨텍스트 처리 능력 (32K 토큰)\n  - 전문 시험에서 인간 수준 또는 그 이상의 성능\n\n## GPT의 핵심 혁신\n\n### 스케일링 법칙 (Scaling Laws)\n$$\\text{Loss} \\propto N^{-\\alpha}$$\n\n* 모델 크기(N), 데이터 양, 계산량이 증가할수록 성능 향상\n* 예측 가능한 성능 개선 곡선\n* \"더 크면 더 좋다\"는 단순하지만 강력한 원리\n\n### 창발적 능력 (Emergent Abilities)\n\n* **특정 임계점**을 넘으면 갑자기 나타나는 새로운 능력들\n* **In-Context Learning**: GPT-3에서 처음 관찰\n* **Chain-of-Thought Reasoning**: 단계별 추론 능력\n* **코드 생성**: 프로그래밍 언어 이해와 생성\n\n### 인간 피드백 강화 학습 (RLHF)\n\n**InstructGPT와 ChatGPT에 도입**:\n1. **Supervised Fine-tuning**: 고품질 대화 데이터로 학습\n2. **Reward Model**: 인간 선호도 기반 보상 모델 학습\n3. **PPO**: 보상 모델을 사용한 강화 학습\n\n### Prompt Engineering의 발전\n\n* **Zero-shot**: 예시 없이 태스크 수행\n* **Few-shot**: 소수 예시로 태스크 학습\n* **Chain-of-Thought**: 단계별 추론 과정 명시\n* **Constitutional AI**: 원칙 기반 행동 유도\n\n## GPT의 강점과 한계\n\n### 강점\n\n* **뛰어난 생성 능력**: 일관성 있고 창의적인 텍스트 생성\n* **범용성**: 하나의 모델로 다양한 태스크 수행\n* **적응성**: 프롬프트만으로 새로운 태스크 수행\n* **확장성**: 모델 크기 증가로 성능 향상 가능\n\n### 한계\n\n* **사실성 문제**: 할루시네이션 (거짓 정보 생성)\n* **편향성**: 학습 데이터의 편향 반영\n* **해석 가능성**: 내부 동작 원리의 불투명성\n* **계산 비용**: 대규모 모델의 높은 추론 비용\n\n## 현재적 의미와 영향\n\nGPT는 단순한 기술 발전을 넘어 **인간-AI 상호작용의 패러다임**을 바꾸었다:\n\n* **ChatGPT 현상**: 일반 대중의 AI 접근성 혁신\n* **생성형 AI 생태계**: 수많은 응용 서비스와 스타트업 등장\n* **업무 방식 변화**: 글쓰기, 코딩, 창작 등 지식 작업의 혁신\n* **교육 패러다임 변화**: AI 활용 능력의 중요성 대두\n\n# 결론\n\n* GPT는 자연어 처리 분야에서 **생성형 AI 혁명**의 출발점이 된 가장 중요한 혁신 중 하나다. \n* 2018년 첫 발표 이후 현재까지 NLP 연구와 실용 AI의 패러다임을 완전히 바꾸어 놓았다.\n\n### GPT의 핵심 기여\n\n* **생성형 언어 모델의 확립**: 이해 중심에서 생성 중심으로 NLP 패러다임 전환, Next Token Prediction이라는 단순한 목표로 복잡한 언어 능력 획득\n* **스케일링 법칙의 입증**: 모델 크기와 데이터 양 증가만으로도 성능이 예측 가능하게 향상됨을 보여주어 대규모 AI 개발의 방향 제시\n* **In-Context Learning의 발견**: Fine-tuning 없이도 예시만으로 새로운 태스크를 수행할 수 있는 혁신적 능력으로 AI 활용 방식을 근본적으로 변화\n* **창발적 능력의 관찰**: 특정 규모를 넘으면 갑자기 나타나는 추론, 코딩, 창작 등의 고차원적 능력으로 AI 발전의 새로운 가능성 제시\n\n### 생성형 AI 생태계의 탄생\n\nGPT의 등장은 단순한 기술 발전을 넘어 완전히 새로운 **생성형 AI 생태계**를 만들어냈다:\n\n* **ChatGPT 현상**: 2022년 ChatGPT 출시로 일반 대중이 AI와 자연어로 대화하는 새로운 경험 제공, 전 세계적인 AI 관심과 활용 폭발적 증가\n* **산업 생태계 변화**: 수많은 GPT 기반 서비스와 스타트업 등장, 기존 산업의 디지털 전환 가속화\n* **생산성 혁신**: 글쓰기, 코딩, 번역, 요약 등 지식 작업의 자동화 및 보조 도구로 활용되어 업무 효율성 획기적 향상\n* **창작과 교육 변화**: AI와의 협업을 통한 새로운 창작 방식 등장, 교육 방법과 평가 체계의 근본적 재고\n\n### 기술적 영향과 후속 발전\n\nGPT가 확립한 기술적 기반은 이후 모든 언어 AI 발전의 토대가 되었다:\n\n* **아키텍처 표준화**: Transformer 디코더 기반 구조가 생성형 언어 모델의 표준이 됨\n* **훈련 방법론**: Next Token Prediction과 RLHF(인간 피드백 강화 학습)가 대화형 AI 개발의 핵심 방법론으로 자리잡음\n* **다중 모달 확장**: GPT-4의 텍스트-이미지 처리 능력을 시작으로 멀티모달 AI 발전의 기반 마련\n* **경쟁 모델들의 등장**: Google의 Gemini, Anthropic의 Claude, Meta의 Llama 등 다양한 대안 모델들의 개발 촉진\n\n### 사회적 변화와 도전 과제\n\nGPT는 기술적 혁신을 넘어 사회 전반에 깊은 영향을 미치고 있다:\n\n### 긍정적 영향\n\n* **접근성 혁신**: 복잡한 기술 지식 없이도 자연어로 AI 활용 가능\n* **창의성 증진**: AI와의 협업을 통한 새로운 아이디어 창출과 표현 방식 확장\n* **교육 개인화**: 맞춤형 학습 지원과 즉시 피드백 제공\n* **언어 장벽 해소**: 실시간 번역과 다국어 소통 지원\n\n### 해결 과제\n\n* **진실성과 신뢰성**: 할루시네이션 문제와 잘못된 정보 생성 위험\n* **윤리적 책임**: AI 생성 콘텐츠의 책임 소재와 저작권 문제\n* **사회적 불평등**: AI 접근성 격차와 일자리 대체 우려\n* **안전성 확보**: 악용 방지와 AI 정렬(Alignment) 문제\n\n## 미래 전망과 발전 방향\n\nGPT가 열어놓은 생성형 AI의 미래는 다음과 같은 방향으로 발전할 것으로 예상된다:\n\n### 기술적 발전\n\n* **더 큰 규모**: 수조 파라미터 규모의 모델과 더욱 방대한 학습 데이터\n* **효율성 개선**: 추론 속도 향상과 계산 비용 감소를 위한 최적화 기술\n* **전문화**: 도메인별 특화 모델과 개인화된 AI 어시스턴트\n* **다중 모달**: 텍스트, 이미지, 음성, 비디오를 통합한 범용 AI\n\n### 응용 분야 확장\n\n* **과학 연구**: 논문 작성, 가설 생성, 실험 설계 지원\n* **의료 분야**: 진단 보조, 치료법 연구, 의료 문서 작성\n* **법률 서비스**: 계약서 분석, 판례 검색, 법률 문서 작성\n* **예술과 미디어**: 소설, 시나리오, 음악 창작의 새로운 방법론\n\n## 역사적 의미\n\nGPT의 등장은 **인공지능 역사의 중요한 전환점**이다. 1950년대 튜링 테스트 제안, 1980년대 신경망 부흥, 2010년대 딥러닝 혁명에 이어, GPT는 **AI가 인간과 자연스럽게 소통할 수 있는 시대**를 열었다.\n\n특히 ChatGPT의 대중적 성공은 AI를 전문가들만의 도구에서 일반인도 일상적으로 사용하는 기술로 바꾸어 놓았다. 이는 개인용 컴퓨터나 인터넷의 등장에 비견될 만한 기술적, 사회적 변화를 의미한다.\n\nGPT로 시작된 생성형 AI 시대는 이제 막 시작되었으며, 향후 인간의 창조적 활동, 학습 방식, 의사소통 패턴까지 근본적으로 변화시킬 것이다. 이러한 변화의 중심에서 GPT는 **AI와 인간이 협력하는 새로운 시대의 출발점**으로 역사에 기록될 것이다.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"23.plm_GPT.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"GPT: Generative Pre-trained Transformer","subtitle":"생성형 언어 모델의 혁신과 대화형 AI의 출발점","description":"GPT는 Transformer 디코더 기반의 생성형 사전 학습 모델로 자연어 생성 분야에 혁신을 가져왔다. Next Token Prediction을 통한 사전 학습 방식, 강력한 텍스트 생성 능력, 그리고 In-Context Learning을 통한 Few-Shot 학습 능력을 분석한다. GPT의 구조, 학습 방법, 각 버전별 발전 과정과 함께 ChatGPT로 이어지는 생성형 AI 혁명의 시작점으로서의 의미를 다룬다.\n","categories":["NLP","Deep Learning"],"author":"Kwangmin Kim","date":"2025-01-23","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}