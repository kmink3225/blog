{"title":"Sensor","markdown":{"yaml":{"title":"Sensor","subtitle":"Sensor Concept, Bash Sensor, File Sensor, Python Sensor, External Task Sensor, Custom Sensor Creation","description":"Sensor는 특정 조건이 만족하면 task를 실행하게하는 Operator. 실시간에 가까운 workflow를 가능하게 하는 기능.\n","categories":["Engineering"],"author":"Kwangmin Kim","date":"05/01/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"comments":{"utterances":{"repo":"./docs/comments"}},"draft":false},"headingText":"센서","containsRefs":false,"markdown":"\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n\n## 센서의 개념\n\n* 일종의 특화된 오퍼레이터\n* 특정 조건이 만족되기를 주기적으로 확인 및 기다리고 만족되면 True를 반환하는 Task\n* 모든 센서는 BaseSensorOperator를 상속하여 구현되며 (BaseSensorOperator는 BaseOperator를 상속함)\n상속시에는 __init()__ 함수와 poke(context) 함수 재정의 해야한다\n* 센싱하는 로직은 poke 함수에 정의: 특정 조건이 만족하는지 체크하고 true를 return 하도록 정의\n\n## BaseSensor 오퍼레이터 명세 확인\n\n[airflow BaseSensorOperator](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.html#airflow.sensors.base.BaseSensorOperator)\n\n* Bases: airflow.models.baseoperator.BaseOperator, airflow.models.skipmixin.SkipMixin\n  Sensor operators are derived from this class and inherit these attributes. Sensor operators keep executing at a time interval and succeed when a criteria is met and fail if and when they time out.\n  base operator를 상속했음.\n* parameter\n    * poke_interval (float) – Time in seconds that the job should wait in between each try: sensor task의 특정 조건이 만족하는지 체크하는 주기로 **초단위**로 입력하면 된다. 60 = 1mins 은 1분 단위로 체크\n    * timeout (float) – Time, in seconds before the task times out and fails. task가 계속 false 가 나올때 task failure 로 규정할 maximum 시간. 초단위로 입력. 보통 daily dag을 많이 만드므로 timeout도 보통 24시간 으로 입력한다. ex) 60*60*24 (=24 시간)\n    * soft_fail (bool) – Set to true to mark the task as SKIPPED on failure. timeout을 만났을 때 sensor task fail로 marking하지 말고 skip으로 marking하도록 설정\n    * mode (str) (**중요**) – How the sensor operates. 'poke' 아니면 'reschedule' 로만 값을 넣을 수 있음. 중요하기 때문에 아래에서 따로 설명.\n      **Options are: { poke | reschedule }**, default is poke. When set to poke the sensor is taking up a worker slot for its whole execution time and sleeps between pokes. Use this mode if the expected runtime of the sensor is short or if a short poke interval is required. Note that the sensor will hold onto a worker slot and a pool slot for the duration of the sensor’s runtime in this mode. When set to reschedule the sensor task frees the worker slot when the criteria is not yet met and it’s rescheduled at a later time. Use this mode if the time before the criteria is met is expected to be quite long. The poke interval should be more than one minute to prevent too much load on the scheduler.\n    * exponential_backoff (bool) – allow progressive longer waits between pokes by using exponential backoff algorithm. sensor task를 체크하는 주기가 $2^n$ 으로 늘어지기 된다. 즉, 2초, 4초, 8초, $\\ldots$\n    * max_wait (datetime.timedelta | float | None) \n      * maximum wait interval between pokes, can be timedelta or float seconds. \n      * exponential_backoff가 true 일 때만 홠성화 되며 exponential_backoff 의 상한선을 의미\n    * silent_fail (bool) – If true, and poke method raises an exception different from AirflowSensorTimeout, AirflowTaskTimeout, AirflowSkipException and AirflowFailException, the sensor will log the error and continue its execution. Otherwise, the sensor task fails, and it can be retried based on the provided retries parameter.\n* `poke(context)[source]`: Function defined by the sensors while deriving this class should override.\n  * poke method를 재정의 하지 않으면 error 발생하게 되어 있음\n\n    ```markdown\n    [docs]    def poke(self, context: Context) -> bool | PokeReturnValue:\n            \"\"\"Function defined by the sensors while deriving this class should override.\"\"\"\n            raise AirflowException(\"Override me.\")\n    ```\n* execute(self, context: Context): 재정의할 필요없음. 이미 정의가 되어 있음. overiding된 poke함수의 값이 있어야 밑의 while loop를 탈출하게 되어 있음. 다시 말해서, `poke_return = self.poke(context)`이 false를 반환하게 되면 infinite loop. 결국 `poke()` 가 중요\n  ```markdown\n  [docs]    def execute(self, context: Context) -> Any:\n        started_at: datetime.datetime | float \n        \n        (...)\n\n        while True:\n            try:\n                poke_return = self.poke(context)\n            except (\n                AirflowSensorTimeout,\n                AirflowTaskTimeout,\n                AirflowSkipException,\n                AirflowFailException,\n            ) as e:\n                raise e\n            except Exception as e:\n                if self.silent_fail:\n                    logging.error(\"Sensor poke failed: \\n %s\", traceback.format_exc())\n                    poke_return = False\n                else:\n                    raise e\n\n            if poke_return: # poke_return = true이면 while loop 탈출\n                if isinstance(poke_return, PokeReturnValue):\n                    xcom_value = poke_return.xcom_value\n                break\n          (...)       \n  ```\n\n* BaseSensor 오퍼레이터 Mode 유형\n  * mode 유형\n\n  | Comparison          | Poke Mode    | Reschedule Mode   |\n  |---------------------|-----------------|:------------------------|\n  | 원리                | DAG이 수행되는 내내 Running Slot(task가 수행될 때 차지하는 공간) 을 차지. sensor가 특정 조건을 체킹할때나 안할때나 항상 slot 차지.  다만 Slot 안에서 Sleep, active 를 반복   | 센서가 조건을 체킹하는 동작 시기에만 Slot을 차지. 그 외에는 Slot을 점유하지 않음. slot을 들어갔다 나왔다를 반복 |\n  | Wait에서의 Task 상태 | running (airflow web ui 에서 task bar가 연두색)           | up_for_reschedule  (task bar가 민트색) |\n  | 유리한 적용 시점 |  짧은 센싱 간격 (interval, 초 단위)    | 긴 센싱 간격, 주로 분 단위 Reschedule될 때 (5분, 10분) 스케줄러의 부하 발생 |\n\n* Slot의 이해\n  * Pool\n    * 모든 operator로 만들어진 Task는 특정 Pool에서 수행되며 Pool은 Slot이라는 것을 가지고 있음.\n    * 기본적으로 Task 1개당 Slot 1개를 점유하며 Pool을 지정하지 않으면 default_pool에서 수행\n    ![Poke Mode vs Reschedule Mode](../../../../../images/airflow/poke.PNG)\n    * `airflow web service ui >> admin >> pools`\n      * pool: pool name\n      * slots: 128개의 공간\n      * Running Slots, Queued Slots, Schedulued Slots.\n* 사용자 입장에서는 operator의 mode의 이해는 그렇게 중요하진 않지만 airflow를 운영하는 사람 입장에서는 중요한 변수가 될 수 있다.\n\n# Bash Sensor\n\n## Bash 센서 명세 확인\n\n* [airflow.sensors.bash](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/bash/index.html)\n  * Parameters\n    * bash_command – 조건문을 여기에다가 적음\n      * Return True if and only if the return code is 0.\n      * shell 스크립트에서 return True를 주는 방법\n        * 파이썬에서의 `return True`와 같은 의미로 shell script에서는 `exit 0` 를 사용\n      * 모든 shell은 수행을 마친 후 EXIT_STATUS를 가지고 있으며 0~255 사이의 값을 가짐.\n        * EXIT 0 만 정상이며 나머지는 모두 비정상의 의미를 가짐\n        * 마지막 명령 수행의 EXIT_STATUS를 확인하려면 `echo $?` 로 확인\n      ```markdown\n      kmkim@K100230201051:~/airflow$ ls\n      airflow  custom_image  docker-compose.20230708  files  plugins\n      config   dags          docker-compose.yaml      logs\n      kmkim@K100230201051:~/airflow$ echo $?\n      0 # ls란 명령이 정상적으로 실행됐기 때문에 0을 반환\n      kmkim@K100230201051:~/airflow$ ls sdf\n      ls: cannot access 'sdf': No such file or directory\n      kmkim@K100230201051:~/airflow$ echo $?\n      2\n      kmkim@K100230201051:~/airflow$ sdfsd\n      sdfsd: command not found\n      kmkim@K100230201051:~/airflow$ echo $?\n      127\n      ```\n      * exit status 변경하기\n      ```markdown\n      vi test.sh #exit status 변경할 shell script\n      # vi editor: test.sh\n      ls\n      exit 1\n\n      ```\n      ```markdown\n      chmod +x test.sh #실행권한 부여\n      ./test.sh #실행\n      echo $? #1 출력됨\n      ```\n    * env – If env is not None, it must be a mapping that defines the environment variables for the new process; these are used instead of inheriting the current process environment, which is the default behavior. (templated)\n    * output_encoding – output encoding of bash command.\n    * retry_exit_code (int | None) – If task exits with this code, treat the sensor as not-yet-complete and retry the check later according to the usual retry/timeout settings. Any other non-zero return code will be treated as an error, and cause the sensor to fail. If set to None (the default), any non-zero exit code will cause a retry and the task will never raise an error except on time-out.\n* Dag Example\n  * csv file 있는지 없는지 확인\n\n```markdown\nfrom airflow.sensors.bash import BashSensor\nfrom airflow.operators.bash import BashOperator\nfrom airflow import DAG\nimport pendulum\n\nwith DAG(\n    dag_id='dags_bash_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='0 6 * * *',\n    catchup=False\n) as dag:\n\n    sensor_task_by_poke = BashSensor(\n        task_id='sensor_task_by_poke',\n        # 오늘 날짜로 tvCorona19VaccinestatNew.csv 가 있는지 없는지 확인 \n        env={'FILE':'/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv'},\n        bash_command=f'''echo $FILE && \n                        if [ -f $FILE ]; then \n                              exit 0\n                        else \n                              exit 1\n                        fi''',\n        poke_interval=30,      #30초\n        timeout=60*2,          #2분\n        mode='poke',\n        soft_fail=False\n    )\n\n    sensor_task_by_reschedule = BashSensor(\n        task_id='sensor_task_by_reschedule',\n        env={'FILE':'/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv'},\n        bash_command=f'''echo $FILE && \n                        if [ -f $FILE ]; then \n                              exit 0\n                        else \n                              exit 1\n                        fi''',\n        poke_interval=60*3,    # 3분\n        timeout=60*9,          #9분\n        mode='reschedule',\n        soft_fail=True\n    )\n\n    bash_task = BashOperator(\n        task_id='bash_task',\n        env={'FILE': '/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv'},\n        bash_command='echo \"건수: `cat $FILE | wc -l`\"',\n    )\n\n    [sensor_task_by_poke,sensor_task_by_reschedule] >> bash_task\n```\n\n# File Sensor\n\n## File 센서 명세 확인\n\n* [airflow.sensors.filesystem](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/filesystem/index.html#airflow.sensors.filesystem.FileSensor)\n  * Waits for a file or folder to land in a filesystem. (file이나 folder가 file system안에 들어왔는지 체크)\n  * Parameters\n    * fs_conn_id – reference to the File (path) connection id (file connection id을 airflow web에 미리 등록해놔야함)\n    * filepath – File or folder name (relative to the base path set within the connection), can be a glob. 상대 경로를 입력해야함\n    * recursive – when set to True, enables recursive directory matching behavior of ** in glob filepath parameter. Defaults to False. (glob(**))\n  * 구체적인 로직을 이해하기 위해 source를 봐야함\n    * 모든 sensor를 만들때는 poke를 재정의해야만 함. \n    * 즉, poke만 잘 이해하면 거의 모든 sensor를 잘 이해할 수 있다.\n    * `poke()`의 `FSHook()` 은 `imported from airflow.hooks.filesystem`\n      * `hook = FSHook(self.fs_conn_id)`: hook을 만들 때 fs_conn_id를 넘겨받고 있음\n    * glob()을 이해하는 것이 핵심\n      * file path안에 있는 files 또는 directories를 list로 반환\n      ```markdown\n      glob('/home/kmkim') # 이 path 안에 있는 모든 files, directories이 리스트로 반환됨\n      glob('/home/kmkim/docker-compose.yaml') # docker-compose.yaml 자체가 리스트로 반환됨\n      glob('/home/kmkim/**', recursive=True) # /home/kmkim/ 하위에 있는 모든 files 또는 directories이 리스트로 반환됨\n      ```\n      * 주로 file을 찾기 위해 사용되는 함수이기 때문에 glob('/home/kmkim/docker-compose.yaml') 이 형태가 가장 많이 사용된다.\n\n  ```markdown\n  from __future__ import annotations\n\n  import datetime\n  import os\n  from glob import glob\n  from typing import Sequence\n\n  from airflow.hooks.filesystem import FSHook\n  from airflow.sensors.base import BaseSensorOperator\n  from airflow.utils.context import Context\n\n\n  [docs]class FileSensor(BaseSensorOperator):\n      \"\"\"\n      Waits for a file or folder to land in a filesystem.\n\n      If the path given is a directory then this sensor will only return true if\n      any files exist inside it (either directly, or within a subdirectory)\n\n      :param fs_conn_id: reference to the File (path)\n          connection id\n      :param filepath: File or folder name (relative to\n          the base path set within the connection), can be a glob.\n      :param recursive: when set to ``True``, enables recursive directory matching behavior of\n          ``**`` in glob filepath parameter. Defaults to ``False``.\n\n      .. seealso::\n          For more information on how to use this sensor, take a look at the guide:\n          :ref:`howto/operator:FileSensor`\n\n\n      \"\"\"\n\n  [docs]    template_fields: Sequence[str] = (\"filepath\",)\n  [docs]    ui_color = \"#91818a\"\n\n      def __init__(self, *, filepath, fs_conn_id=\"fs_default\", recursive=False, **kwargs):\n          super().__init__(**kwargs)\n          self.filepath = filepath\n          self.fs_conn_id = fs_conn_id\n          self.recursive = recursive\n\n  [docs]    def poke(self, context: Context):\n          hook = FSHook(self.fs_conn_id)\n          basepath = hook.get_path() # hook이 가지고 있는 method\n          full_path = os.path.join(basepath, self.filepath)\n          self.log.info(\"Poking for file %s\", full_path)\n\n          for path in glob(full_path, recursive=self.recursive):\n              if os.path.isfile(path): # glob에 의한 결과물이 file이면 for loop 탈출\n                  mod_time = datetime.datetime.fromtimestamp(os.path.getmtime(path)).strftime(\"%Y%m%d%H%M%S\")\n                  self.log.info(\"Found File %s last modified: %s\", str(path), mod_time)\n                  return True\n\n              for _, _, files in os.walk(path):\n                  if len(files) > 0:\n                      return True\n          return False\n    ```\n  * glob: file path안에 있는 file이나 directory를 찾아서 list로 반환\n    * `glob('/home/kmkim')` : 모든 files and directories list 반환\n    * `glob('/home/kmkim/docker-compose.yaml')` : docker-compose.yaml만 리스트로 반환\n    * `glob('/home/kmkim/**',recursive=True)` : 하위 디렉토리 안에 있는 파일과 디렉토리들의 리스트로 반환\n  * `FSHook()` : [명세서](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/hooks/filesystem.html#FSHook)\n  \n    ```markdown\n    [docs]class FSHook(BaseHook):\n    \"\"\"\n    Allows for interaction with an file server.\n\n    Connection should have a name and a path specified under extra:\n\n    example:\n    Connection Id: fs_test\n    Connection Type: File (path)\n    Host, Schema, Login, Password, Port: empty\n    Extra: {\"path\": \"/tmp\"} # dictionary 형태\n    \"\"\"\n\n    def __init__(self, conn_id: str = \"fs_default\"):\n        super().__init__()\n        conn = self.get_connection(conn_id)\n        self.basepath = conn.extra_dejson.get(\"path\", \"\")\n        self.conn = conn\n\n    [docs]    def get_conn(self) -> None:\n            pass\n\n\n    [docs]    def get_path(self) -> str:\n            \"\"\"\n            Get the path to the filesystem location.\n\n            :return: the path.\n            \"\"\"\n            return self.basepath\n\n    ```\n    * get_path(): self.baspath return - 별 내용 없음\n    * 생성자만 잘 이해하면 됨\n      * Connection should have a name and a path specified under extra (예시 잘 볼 것):\n![File Sensor Sequential Diagram](../../../../../images/airflow/File_sensor_diagram.PNG)\n\n* Connection 작성\n\n  | Connection_id   | conn_file_opt_airflow_files   |\n  |-----------------|:------------------------------|\n  | Connection_type | File (path)                   |\n  | Host            |                               |\n  | Schema          |                               |\n  | Login           |                               |\n  | Password        |                               |\n  | Port            |                               |\n  | Extra           | {\"path\":\"/opt/airflow/files\"} |\n\n* Dag 작성\n\n```markdown\nfrom airflow import DAG\nfrom airflow.sensors.filesystem import FileSensor\nimport pendulum\n\nwith DAG(\n    dag_id='dags_file_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='0 7 * * *',\n    catchup=False\n) as dag:\n    tvCorona19VaccinestatNew_sensor = FileSensor(\n        task_id='tvCorona19VaccinestatNew_sensor',\n        fs_conn_id='conn_file_opt_airflow_files',\n        filepath='tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv',\n        recursive=False,\n        poke_interval=60,\n        timeout=60*60*24, # 1일\n        mode='reschedule'\n    )\n\n```\n\n# Python Sensor\n\n* python operator를 이용하여 특정 날짜에 데이터가 업데이트 되었는지 확인하는 sensor를 만들기 \n* [airflow.sensors.python 명세서 보기](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/python/index.html)\n* python operator와 매우 유사: 다른점은 python collabe function의 return값이 반드시 true로 되어야함\n```markdown\n\nclassairflow.sensors.python.PythonSensor(*, python_callable, op_args=None, op_kwargs=None, templates_dict=None, **kwargs)[source]\nBases: airflow.sensors.base.BaseSensorOperator\n\nWaits for a Python callable to return True.\n\nUser could put input argument in templates_dict e.g templates_dict = {'start_ds': 1970} and access the argument by calling kwargs['templates_dict']['start_ds'] in the callable\n```\n\nParameters\n* python_callable (Callable) – A reference to an object that is callable. python callable에 넘겨줄 인수들은 아래의 op_kwars, op_args\n* op_kwargs (Mapping[str, Any] | None) – a dictionary of keyword arguments that will get unpacked in your function\n* op_args (list | None) – a list of positional arguments that will get unpacked when calling your callable\n* templates_dict (dict | None) – a dictionary where the values are templates that will get templated by the Airflow engine sometime between __init__ and execute takes place and are made available in your callable’s context after the template has been applied. templates_dict도 python collable에 넘겨주는 인수로 {'start_ds':'{{data_interval_start}}'} 와 같이 사용하면 된다.\n  * 예를 들어, 다음과 같이 사용된다. 하지만 실무에서는 tamplates_dict보단 op_kwargs가 더 많이 사용된다.\n\n  ```markdown\n  def somefunc(x1,x2,**kwargs):\n    v=kwargs['templates_dict']['start_ds'] #{'start_ds':'{{data_interval_start}}'}의 '{{data_interval_start}}'값이 호출된다.\n  ```\n\n\n## Python 센서 DAG 작성\n\n* 무엇을 센싱할 것인가\n  * 서울시 공공데이터에서 당일 날짜로 데이터가 생성되었는지 센싱하기(날짜 컬럼이 있는 경우)\n  * 당일 날짜가 몇시에 업로드가 되는지 모름 (12시? ,00시, 1시?)\n* [서울 열린 데이터 광장](https://data.seoul.go.kr/)\n  * 검색창 >> '코로나' 검색 >> 서울시 코로나19 확진자 발생동향 (2023.05.31.이전) >> 미리보기\n  * 목표: 센서를 공용적으로 만들어 다른 dataset에도 적용할 수 있도록 작성\n    * python collable function(check_api_update())을 일반화 시켜서 작성\n```markdown\nfrom airflow import DAG\nfrom airflow.sensors.python import PythonSensor\nimport pendulum\nfrom airflow.hooks.base import BaseHook\n\nwith DAG(\n    dag_id='dags_python_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='10 1 * * *',\n    catchup=False\n) as dag:\n    def check_api_update(http_conn_id, endpoint, base_dt_col, **kwargs):\n        import requests\n        import json\n        from dateutil import relativedelta\n        connection = BaseHook.get_connection(http_conn_id)\n        url = f'http://{connection.host}:{connection.port}/{endpoint}/1/100' #1부터 100행 까지만 가지고옴\n        response = requests.get(url)\n        \n        contents = json.loads(response.text)\n        key_nm = list(contents.keys())[0]\n        row_data = contents.get(key_nm).get('row') \n          # row_data에 list형태로 데이터가 들어감\n          # [{1행},\n             {2행},\n             {3행},\n             {4행},...]\n        last_dt = row_data[0].get(base_dt_col) #row_data의 첫번째 행, base_dt_col의 key에 대한 value를 추출\n        last_date = last_dt[:10] \n          # 열번째 글자 까지만 slicing\n          # 왜냐면, 서울시 기준일(S_DT)가 date 형식이 아니라 string 형식으로 입력되어 있음\n          # 연/월/일 정보만 필요하기 때문에 시간은 제외\n        last_date = last_date.replace('.', '-').replace('/', '-')\n\n        # 밑에 try구문은 last_date 의 date 형식을 검증\n        try:\n            pendulum.from_format(last_date,'YYYY-MM-DD') # last_date가 'YYYY-MM-DD' 형식으로 parsing이 가능하면 코드 진행이 계속되고 안되면 except문으로 들어가게됨\n        except:\n            from airflow.exceptions import AirflowException\n            AirflowException(f'{base_dt_col} 컬럼은 YYYY.MM.DD 또는 YYYY/MM/DD 형태가 아닙니다.')\n            # 서울시 코로나19 확진자 발생동향 (2023.05.31.이전) 의 데이텉 명세에 따르면\n            # S_DT: 서울시 기준일 (데이터 기준일) 을 의미\n\n        today_ymd = kwargs.get('data_interval_end').in_timezone('Asia/Seoul').strftime('%Y-%m-%d') #2023-05-01 형태로 할당됨\n          # time stamp를 string형태로 바꿈\n          # today_ymd는 batch(DAG)가 도는 날짜\n        if last_date >= today_ymd: # string 형태지만 크기는 비교 가능\n            print(f'생성 확인(배치 날짜: {today_ymd} / API Last 날짜: {last_date})')\n            return True\n        else:\n            print(f'Update 미완료 (배치 날짜: {today_ymd} / API Last 날짜:{last_date})')\n            return False\n\n    sensor_task = PythonSensor(\n        task_id='sensor_task',\n        python_callable=check_api_update,\n        op_kwargs={'http_conn_id':'openapi.seoul.go.kr',\n                   'endpoint':'{{var.value.apikey_openapi_seoul_go_kr}}/json/TbCorona19CountStatus',\n                   'base_dt_col':'S_DT'},\n        poke_interval=600,   #10분\n        mode='reschedule'\n\n    )\n```\n위의 dag의 `check_api_update()` 모든 서울시 공공데이터에 적용할 수 있도록 적고 코로나 데이터에 대한 수행은 `sensor_task`로 이루어지도록 했다. 위와 같이, 코드의 재사용성이 있는 형태로 스크립트를 짜야함\n\n# ExternalTask 센서\n\n## DAG 간 의존관계 설정\n\n* DAG 의존관계 설정 방법\n  * TriggerDagRun 오퍼레이터    \n    ```{dot}\n    digraph G {\n      compound=true;\n      rankdir=LR;\n      subgraph cluster0 {\n        rankdir=TB;\n        task1 [shape=box];\n        task2_1 [shape=box];\n        task2_2 [shape=box];\n        task2_3 [shape=box];\n\n        label= \"Task Flow\";\n      }\n\n      task1 -> task2_1;\n      task1 -> task2_2;\n      task1 -> task2_3;\n    \n    }\n    ```\n\n    * task1: PythonOperator\n    * TriggerDagRun Operator: task1이 끝난 후 다른 dag을 돌리고 싶을 경우에 TriggerDagRun Operator사용. task2,3,4는 task1 후에 돌아가는 task로 triggerDagRun Operator에 의해 task2,3,4에 대응되는 각 각 다른 dag을 돌아가도록 수행된다.\n  * ExternalTask 센서\n    ```{dot}\n    digraph G {\n      compound=true;\n      rankdir=LR;\n      subgraph cluster0 {\n        rankdir=TB;\n        sensor1 [shape=box];\n        sensor2 [shape=box];\n        sensor3 [shape=box];\n        sensor4 [shape=box];\n        task2 [shape=box];\n\n        label= \"Task Flow\";\n      }\n    \n      sensor1 -> task2;\n      sensor2 -> task2;\n      sensor3 -> task2; \n      sensor4 -> task2; \n    }\n    ```\n\n    * task2를 돌리기 위해 다른 dag의 task가 완료된 후에 돌아가야 하는 조건이 붙었을 때 사용\n    * 예를 들어, 위의 그림 처럼, sensor1는 Dag A, sensor2는 Dag B, sensor3은 Dag C, sensor3은 Dag D에 속해있다고 할 때, 4개의 dag이 완료가 되어야 task2가 돌아갈 수 있다.\n  * dataset\n    * airflow 2.4 version에 나온 기능으로 dag의 dependency를 설정하는 새로운 방법\n\n## ExternalTask 센서 명세 확인\n\n* [airflow.sensors.external_task 명세](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/external_task/index.html#module-airflow.sensors.external_task)\n  * Bases: airflow.sensors.base.BaseSensorOperator\n  * Waits for a different DAG, task group, or task to complete for a specific logical date.\n* Parameter\n\n| Parameter   | 필수여부   | 설명                  |\n|-----------------|-----------------|:------------------------------|\n| external_dag_id | O | 센싱할 dag 명  | \n| **external_task_id** | X (셋 중 하나만 입력 가능 없으면 안써도 되고 없을 경우 dag만 센싱) | 센싱할 task_id 명 (string) |\n| **external_task_ids** |  X (셋 중 하나만 입력 가능 없으면 안써도 되고 없을 경우 dag만 센싱) | 센싱할 1 개 이상의 task_id 명 (list) |\n| **external_task_group_id** |  X (셋 중 하나만 입력 가능 없으면 dag만 센싱) | 센싱할 task_group_id명 |\n| allowed_states | X (같은 상태가 입력되면 안됨)  | 센서가 Success 되기 위한 센싱 대상의 상태 (기본적으로 센싱할 task가 success로 끝나야함)      |\n| skipped_states | X (같은 상태가 입력되면 안됨)  | 센서가 Skipped 되기 위한 센싱 대상의 상태 (기본적으로 none, 아무런 정의가 안되어있음)     |\n| failed_states  | X (같은 상태가 입력되면 안됨)  | 센서가 Fail 되기 위한 센싱 대상의 상태 (기본적으로 none, 아무런 정의가 안되어있음)         |\n| execution_delta | Login           | 현재 dag과 센싱할 dag의 data_interval_start의 차이 (스케쥴 차이)를 입력  |\n| execution_date_fn | Password      | 스케쥴 차이를 상수로 구할 수 없는 경우, 센싱할 dag의 data_interval_start를 구하기 위한 함수를 넣어주면 됨        |\n| check_existence | Password        | sensidng dag의 dag_id 또는 task_id 가 있는지 확인 true or false로 입력해야한다. default=false  |\n\n* 위의 allowed_states, skipped_states, failed_states의 states 적을 수 있는 항목은 airflow.utils.state import State (State class) 에 있는 member들만 적을 수 있다.\n  * State.SKIPPED, State.SUCCESS, State.FAILED, State.QUEUED, State.SCHEDULED, State.UP_FOR_RESCHEDULE 등\n(from airflow.utils.state import State 필요)\n* 위의 execution_delta 에는 timedelta() 를 이용하여 입력. \n  * External Task가 있는 dag보다 센싱할 dag이 얼마나 과거에 있는지 양수로 입력\n  * 예를 들어, 센싱할 dag이 6시간 앞선다면 timedelta(hours=6)으로 입력. 주의) -6으로 쓰면 안됨\n* execution_date_fn의 경우 함수가 들어가게 되는데 주로 주 dag의 주기가 다를 때 사용한다.\n  * 예를 들어, externalTask의 dag이 시간단위 스케쥴이고 센싱할 dag의 스케쥴이 월단위로 적혀 있을때, data_interval_schedule값이 점점 벌어지게 된다.\n  * 센싱할 dag의 스케쥴: dag(0 1 1 * *)\n  * external task의 dag의 스케쥴 : dag(0 7 * * *)\n  * data_interval_schedule = 6h (1일차), 1d+6h(2일차), 2d+6h(3일차), $\\ldots$\n* **가장 빈번하게 쓰이고 중요한 parameter: external_dag_id와 execution_delta**\n\n## Dag Full Example\n\n* Monitoring or sensing 할 dag: dags_branch_python_operator.py\n\n```markdown\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.python import BranchPythonOperator\n\nwith DAG(\n    dag_id='dags_branch_python_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'), \n    schedule='0 1 * * *',\n    catchup=False\n) as dag:\n    def select_random():\n        import random\n\n        item_lst = ['A','B','C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a' # task_id를 string 값으로 return해야함\n        elif selected_item in ['B','C']:\n            return ['task_b','task_c'] # 여러 task를 동시에 수행시킬 땐 리스트로 반환\n    \n    # 일반 operator의 parameter도 있음\n    python_branch_task = BranchPythonOperator(\n        task_id='python_branch_task',\n        python_callable=select_random\n    )\n    \n    # 후행 task 3개\n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    python_branch_task >> [task_a, task_b, task_c]\n```\n* ExternalTask dag: dags_external_task_sensor.py\n\n```markdown\nfrom airflow import DAG\nfrom airflow.sensors.external_task import ExternalTaskSensor\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.utils.state import State \n\nwith DAG(\n    dag_id='dags_external_task_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='0 7 * * *',\n    catchup=False\n) as dag:\n    external_task_sensor_a = ExternalTaskSensor(\n        task_id='external_task_sensor_a',\n        external_dag_id='dags_branch_python_operator',\n        external_task_id='task_a',\n        allowed_states=[State.SKIPPED],\n        execution_delta=timedelta(hours=6),\n        poke_interval=10        #10초\n    )\n\n    external_task_sensor_b = ExternalTaskSensor(\n        task_id='external_task_sensor_b',\n        external_dag_id='dags_branch_python_operator',\n        external_task_id='task_b',\n        failed_states=[State.SKIPPED],\n        execution_delta=timedelta(hours=6),\n        poke_interval=10        #10초\n    )\n\n    external_task_sensor_c = ExternalTaskSensor(\n        task_id='external_task_sensor_c',\n        external_dag_id='dags_branch_python_operator',\n        external_task_id='task_c',\n        allowed_states=[State.SUCCESS],\n        execution_delta=timedelta(hours=6),\n        poke_interval=10        #10초\n    )\n```\n\n# Custom Sensor\n\n## Custom Sensor 만들기\n\n* 어떤 센서를 만들 것인가?\n    * Python 센서에서 만들었던 로직을 Custom Sensor화 하기\n    (서울시 공공데이터에서 날짜 컬럼이 있는 경우 날짜 기준 update되었는지 센싱)\n* 재활용성이 높아 다른 DAG에서 활용될 가능성이 높다면 가급적이면 Custom 오퍼레이터화 해놓는 것이 좋다. 그 이유는\n  * 협업 환경에서 코드 중복 구현의 방지\n  * 로직의 일원화 등\n* 위치: `plugins/sensors/seoul_api_date_sensors.py`\n\n## Dag Example\n\n```markdown\nfrom airflow.sensors.base import BaseSensorOperator\nfrom airflow.hooks.base import BaseHook\n\n'''\n서울시 공공데이터 API 추출시 특정 날짜 컬럼을 조사하여 \n배치 날짜 기준 전날 데이터가 존재하는지 체크하는 센서 \n1. 데이터셋에 날짜 컬럼이 존재하고 \n2. API 사용시 그 날짜 컬럼으로 ORDER BY DESC 되어 가져온다는 가정하에 사용 가능\n'''\n\nclass SeoulApiDateSensor(BaseSensorOperator):\n    template_fields = ('endpoint',)\n    def __init__(self, dataset_nm, base_dt_col, day_off=0, **kwargs):\n        '''\n        dataset_nm: 서울시 공공데이터 포털에서 센싱하고자 하는 데이터셋 명\n        base_dt_col: 센싱 기준 컬럼 (yyyy.mm.dd... or yyyy/mm/dd... 형태만 가능)\n        day_off: 배치일 기준 생성여부를 확인하고자 하는 날짜 차이를 입력 (기본값: 0). 즉, 1일 전, 2일 전 데이터가 업데이트 되었는지 확인하는 것.\n        '''\n        super().__init__(**kwargs)\n        self.http_conn_id = 'openapi.seoul.go.kr'\n        self.endpoint = '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + dataset_nm + '/1/100'   # 100건만 추출\n        self.base_dt_col = base_dt_col\n        self.day_off = day_off\n\n        \n    def poke(self, context): # context에 templates 변수들을 호출할 수 있음\n        import requests\n        import json\n        from dateutil.relativedelta import relativedelta\n        connection = BaseHook.get_connection(self.http_conn_id)\n        url = f'http://{connection.host}:{connection.port}/{self.endpoint}'\n        self.log.info(f'request url:{url}')\n        response = requests.get(url)\n\n        contents = json.loads(response.text)\n        key_nm = list(contents.keys())[0]\n        row_data = contents.get(key_nm).get('row')\n        last_dt = row_data[0].get(self.base_dt_col)\n        last_date = last_dt[:10]\n        last_date = last_date.replace('.', '-').replace('/', '-')\n        search_ymd = (context.get('data_interval_end').in_timezone('Asia/Seoul') + relativedelta(days=self.day_off)).strftime('%Y-%m-%d')\n        try:\n            import pendulum\n            pendulum.from_format(last_date, 'YYYY-MM-DD')\n        except:\n            from airflow.exceptions import AirflowException\n            AirflowException(f'{self.base_dt_col} 컬럼은 YYYY.MM.DD 또는 YYYY/MM/DD 형태가 아닙니다.')\n\n        \n        if last_date >= search_ymd: # last_date: API의 날짜, search_ymd: 조회할 날짜\n            self.log.info(f'생성 확인(기준 날짜: {search_ymd} / API Last 날짜: {last_date})')\n            return True\n        else:\n            self.log.info(f'Update 미완료 (기준 날짜: {search_ymd} / API Last 날짜:{last_date})')\n            return False\n\n```\n\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n\n</div>\n\n# Go to Blog Content List\n\n[Blog Content List](../../content_list.qmd)  \n[Engineering Content List](../../Engineering/guide_map/index.qmd)","srcMarkdownNoYaml":"\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n# 센서\n\n## 센서의 개념\n\n* 일종의 특화된 오퍼레이터\n* 특정 조건이 만족되기를 주기적으로 확인 및 기다리고 만족되면 True를 반환하는 Task\n* 모든 센서는 BaseSensorOperator를 상속하여 구현되며 (BaseSensorOperator는 BaseOperator를 상속함)\n상속시에는 __init()__ 함수와 poke(context) 함수 재정의 해야한다\n* 센싱하는 로직은 poke 함수에 정의: 특정 조건이 만족하는지 체크하고 true를 return 하도록 정의\n\n## BaseSensor 오퍼레이터 명세 확인\n\n[airflow BaseSensorOperator](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.html#airflow.sensors.base.BaseSensorOperator)\n\n* Bases: airflow.models.baseoperator.BaseOperator, airflow.models.skipmixin.SkipMixin\n  Sensor operators are derived from this class and inherit these attributes. Sensor operators keep executing at a time interval and succeed when a criteria is met and fail if and when they time out.\n  base operator를 상속했음.\n* parameter\n    * poke_interval (float) – Time in seconds that the job should wait in between each try: sensor task의 특정 조건이 만족하는지 체크하는 주기로 **초단위**로 입력하면 된다. 60 = 1mins 은 1분 단위로 체크\n    * timeout (float) – Time, in seconds before the task times out and fails. task가 계속 false 가 나올때 task failure 로 규정할 maximum 시간. 초단위로 입력. 보통 daily dag을 많이 만드므로 timeout도 보통 24시간 으로 입력한다. ex) 60*60*24 (=24 시간)\n    * soft_fail (bool) – Set to true to mark the task as SKIPPED on failure. timeout을 만났을 때 sensor task fail로 marking하지 말고 skip으로 marking하도록 설정\n    * mode (str) (**중요**) – How the sensor operates. 'poke' 아니면 'reschedule' 로만 값을 넣을 수 있음. 중요하기 때문에 아래에서 따로 설명.\n      **Options are: { poke | reschedule }**, default is poke. When set to poke the sensor is taking up a worker slot for its whole execution time and sleeps between pokes. Use this mode if the expected runtime of the sensor is short or if a short poke interval is required. Note that the sensor will hold onto a worker slot and a pool slot for the duration of the sensor’s runtime in this mode. When set to reschedule the sensor task frees the worker slot when the criteria is not yet met and it’s rescheduled at a later time. Use this mode if the time before the criteria is met is expected to be quite long. The poke interval should be more than one minute to prevent too much load on the scheduler.\n    * exponential_backoff (bool) – allow progressive longer waits between pokes by using exponential backoff algorithm. sensor task를 체크하는 주기가 $2^n$ 으로 늘어지기 된다. 즉, 2초, 4초, 8초, $\\ldots$\n    * max_wait (datetime.timedelta | float | None) \n      * maximum wait interval between pokes, can be timedelta or float seconds. \n      * exponential_backoff가 true 일 때만 홠성화 되며 exponential_backoff 의 상한선을 의미\n    * silent_fail (bool) – If true, and poke method raises an exception different from AirflowSensorTimeout, AirflowTaskTimeout, AirflowSkipException and AirflowFailException, the sensor will log the error and continue its execution. Otherwise, the sensor task fails, and it can be retried based on the provided retries parameter.\n* `poke(context)[source]`: Function defined by the sensors while deriving this class should override.\n  * poke method를 재정의 하지 않으면 error 발생하게 되어 있음\n\n    ```markdown\n    [docs]    def poke(self, context: Context) -> bool | PokeReturnValue:\n            \"\"\"Function defined by the sensors while deriving this class should override.\"\"\"\n            raise AirflowException(\"Override me.\")\n    ```\n* execute(self, context: Context): 재정의할 필요없음. 이미 정의가 되어 있음. overiding된 poke함수의 값이 있어야 밑의 while loop를 탈출하게 되어 있음. 다시 말해서, `poke_return = self.poke(context)`이 false를 반환하게 되면 infinite loop. 결국 `poke()` 가 중요\n  ```markdown\n  [docs]    def execute(self, context: Context) -> Any:\n        started_at: datetime.datetime | float \n        \n        (...)\n\n        while True:\n            try:\n                poke_return = self.poke(context)\n            except (\n                AirflowSensorTimeout,\n                AirflowTaskTimeout,\n                AirflowSkipException,\n                AirflowFailException,\n            ) as e:\n                raise e\n            except Exception as e:\n                if self.silent_fail:\n                    logging.error(\"Sensor poke failed: \\n %s\", traceback.format_exc())\n                    poke_return = False\n                else:\n                    raise e\n\n            if poke_return: # poke_return = true이면 while loop 탈출\n                if isinstance(poke_return, PokeReturnValue):\n                    xcom_value = poke_return.xcom_value\n                break\n          (...)       \n  ```\n\n* BaseSensor 오퍼레이터 Mode 유형\n  * mode 유형\n\n  | Comparison          | Poke Mode    | Reschedule Mode   |\n  |---------------------|-----------------|:------------------------|\n  | 원리                | DAG이 수행되는 내내 Running Slot(task가 수행될 때 차지하는 공간) 을 차지. sensor가 특정 조건을 체킹할때나 안할때나 항상 slot 차지.  다만 Slot 안에서 Sleep, active 를 반복   | 센서가 조건을 체킹하는 동작 시기에만 Slot을 차지. 그 외에는 Slot을 점유하지 않음. slot을 들어갔다 나왔다를 반복 |\n  | Wait에서의 Task 상태 | running (airflow web ui 에서 task bar가 연두색)           | up_for_reschedule  (task bar가 민트색) |\n  | 유리한 적용 시점 |  짧은 센싱 간격 (interval, 초 단위)    | 긴 센싱 간격, 주로 분 단위 Reschedule될 때 (5분, 10분) 스케줄러의 부하 발생 |\n\n* Slot의 이해\n  * Pool\n    * 모든 operator로 만들어진 Task는 특정 Pool에서 수행되며 Pool은 Slot이라는 것을 가지고 있음.\n    * 기본적으로 Task 1개당 Slot 1개를 점유하며 Pool을 지정하지 않으면 default_pool에서 수행\n    ![Poke Mode vs Reschedule Mode](../../../../../images/airflow/poke.PNG)\n    * `airflow web service ui >> admin >> pools`\n      * pool: pool name\n      * slots: 128개의 공간\n      * Running Slots, Queued Slots, Schedulued Slots.\n* 사용자 입장에서는 operator의 mode의 이해는 그렇게 중요하진 않지만 airflow를 운영하는 사람 입장에서는 중요한 변수가 될 수 있다.\n\n# Bash Sensor\n\n## Bash 센서 명세 확인\n\n* [airflow.sensors.bash](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/bash/index.html)\n  * Parameters\n    * bash_command – 조건문을 여기에다가 적음\n      * Return True if and only if the return code is 0.\n      * shell 스크립트에서 return True를 주는 방법\n        * 파이썬에서의 `return True`와 같은 의미로 shell script에서는 `exit 0` 를 사용\n      * 모든 shell은 수행을 마친 후 EXIT_STATUS를 가지고 있으며 0~255 사이의 값을 가짐.\n        * EXIT 0 만 정상이며 나머지는 모두 비정상의 의미를 가짐\n        * 마지막 명령 수행의 EXIT_STATUS를 확인하려면 `echo $?` 로 확인\n      ```markdown\n      kmkim@K100230201051:~/airflow$ ls\n      airflow  custom_image  docker-compose.20230708  files  plugins\n      config   dags          docker-compose.yaml      logs\n      kmkim@K100230201051:~/airflow$ echo $?\n      0 # ls란 명령이 정상적으로 실행됐기 때문에 0을 반환\n      kmkim@K100230201051:~/airflow$ ls sdf\n      ls: cannot access 'sdf': No such file or directory\n      kmkim@K100230201051:~/airflow$ echo $?\n      2\n      kmkim@K100230201051:~/airflow$ sdfsd\n      sdfsd: command not found\n      kmkim@K100230201051:~/airflow$ echo $?\n      127\n      ```\n      * exit status 변경하기\n      ```markdown\n      vi test.sh #exit status 변경할 shell script\n      # vi editor: test.sh\n      ls\n      exit 1\n\n      ```\n      ```markdown\n      chmod +x test.sh #실행권한 부여\n      ./test.sh #실행\n      echo $? #1 출력됨\n      ```\n    * env – If env is not None, it must be a mapping that defines the environment variables for the new process; these are used instead of inheriting the current process environment, which is the default behavior. (templated)\n    * output_encoding – output encoding of bash command.\n    * retry_exit_code (int | None) – If task exits with this code, treat the sensor as not-yet-complete and retry the check later according to the usual retry/timeout settings. Any other non-zero return code will be treated as an error, and cause the sensor to fail. If set to None (the default), any non-zero exit code will cause a retry and the task will never raise an error except on time-out.\n* Dag Example\n  * csv file 있는지 없는지 확인\n\n```markdown\nfrom airflow.sensors.bash import BashSensor\nfrom airflow.operators.bash import BashOperator\nfrom airflow import DAG\nimport pendulum\n\nwith DAG(\n    dag_id='dags_bash_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='0 6 * * *',\n    catchup=False\n) as dag:\n\n    sensor_task_by_poke = BashSensor(\n        task_id='sensor_task_by_poke',\n        # 오늘 날짜로 tvCorona19VaccinestatNew.csv 가 있는지 없는지 확인 \n        env={'FILE':'/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv'},\n        bash_command=f'''echo $FILE && \n                        if [ -f $FILE ]; then \n                              exit 0\n                        else \n                              exit 1\n                        fi''',\n        poke_interval=30,      #30초\n        timeout=60*2,          #2분\n        mode='poke',\n        soft_fail=False\n    )\n\n    sensor_task_by_reschedule = BashSensor(\n        task_id='sensor_task_by_reschedule',\n        env={'FILE':'/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv'},\n        bash_command=f'''echo $FILE && \n                        if [ -f $FILE ]; then \n                              exit 0\n                        else \n                              exit 1\n                        fi''',\n        poke_interval=60*3,    # 3분\n        timeout=60*9,          #9분\n        mode='reschedule',\n        soft_fail=True\n    )\n\n    bash_task = BashOperator(\n        task_id='bash_task',\n        env={'FILE': '/opt/airflow/files/tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv'},\n        bash_command='echo \"건수: `cat $FILE | wc -l`\"',\n    )\n\n    [sensor_task_by_poke,sensor_task_by_reschedule] >> bash_task\n```\n\n# File Sensor\n\n## File 센서 명세 확인\n\n* [airflow.sensors.filesystem](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/filesystem/index.html#airflow.sensors.filesystem.FileSensor)\n  * Waits for a file or folder to land in a filesystem. (file이나 folder가 file system안에 들어왔는지 체크)\n  * Parameters\n    * fs_conn_id – reference to the File (path) connection id (file connection id을 airflow web에 미리 등록해놔야함)\n    * filepath – File or folder name (relative to the base path set within the connection), can be a glob. 상대 경로를 입력해야함\n    * recursive – when set to True, enables recursive directory matching behavior of ** in glob filepath parameter. Defaults to False. (glob(**))\n  * 구체적인 로직을 이해하기 위해 source를 봐야함\n    * 모든 sensor를 만들때는 poke를 재정의해야만 함. \n    * 즉, poke만 잘 이해하면 거의 모든 sensor를 잘 이해할 수 있다.\n    * `poke()`의 `FSHook()` 은 `imported from airflow.hooks.filesystem`\n      * `hook = FSHook(self.fs_conn_id)`: hook을 만들 때 fs_conn_id를 넘겨받고 있음\n    * glob()을 이해하는 것이 핵심\n      * file path안에 있는 files 또는 directories를 list로 반환\n      ```markdown\n      glob('/home/kmkim') # 이 path 안에 있는 모든 files, directories이 리스트로 반환됨\n      glob('/home/kmkim/docker-compose.yaml') # docker-compose.yaml 자체가 리스트로 반환됨\n      glob('/home/kmkim/**', recursive=True) # /home/kmkim/ 하위에 있는 모든 files 또는 directories이 리스트로 반환됨\n      ```\n      * 주로 file을 찾기 위해 사용되는 함수이기 때문에 glob('/home/kmkim/docker-compose.yaml') 이 형태가 가장 많이 사용된다.\n\n  ```markdown\n  from __future__ import annotations\n\n  import datetime\n  import os\n  from glob import glob\n  from typing import Sequence\n\n  from airflow.hooks.filesystem import FSHook\n  from airflow.sensors.base import BaseSensorOperator\n  from airflow.utils.context import Context\n\n\n  [docs]class FileSensor(BaseSensorOperator):\n      \"\"\"\n      Waits for a file or folder to land in a filesystem.\n\n      If the path given is a directory then this sensor will only return true if\n      any files exist inside it (either directly, or within a subdirectory)\n\n      :param fs_conn_id: reference to the File (path)\n          connection id\n      :param filepath: File or folder name (relative to\n          the base path set within the connection), can be a glob.\n      :param recursive: when set to ``True``, enables recursive directory matching behavior of\n          ``**`` in glob filepath parameter. Defaults to ``False``.\n\n      .. seealso::\n          For more information on how to use this sensor, take a look at the guide:\n          :ref:`howto/operator:FileSensor`\n\n\n      \"\"\"\n\n  [docs]    template_fields: Sequence[str] = (\"filepath\",)\n  [docs]    ui_color = \"#91818a\"\n\n      def __init__(self, *, filepath, fs_conn_id=\"fs_default\", recursive=False, **kwargs):\n          super().__init__(**kwargs)\n          self.filepath = filepath\n          self.fs_conn_id = fs_conn_id\n          self.recursive = recursive\n\n  [docs]    def poke(self, context: Context):\n          hook = FSHook(self.fs_conn_id)\n          basepath = hook.get_path() # hook이 가지고 있는 method\n          full_path = os.path.join(basepath, self.filepath)\n          self.log.info(\"Poking for file %s\", full_path)\n\n          for path in glob(full_path, recursive=self.recursive):\n              if os.path.isfile(path): # glob에 의한 결과물이 file이면 for loop 탈출\n                  mod_time = datetime.datetime.fromtimestamp(os.path.getmtime(path)).strftime(\"%Y%m%d%H%M%S\")\n                  self.log.info(\"Found File %s last modified: %s\", str(path), mod_time)\n                  return True\n\n              for _, _, files in os.walk(path):\n                  if len(files) > 0:\n                      return True\n          return False\n    ```\n  * glob: file path안에 있는 file이나 directory를 찾아서 list로 반환\n    * `glob('/home/kmkim')` : 모든 files and directories list 반환\n    * `glob('/home/kmkim/docker-compose.yaml')` : docker-compose.yaml만 리스트로 반환\n    * `glob('/home/kmkim/**',recursive=True)` : 하위 디렉토리 안에 있는 파일과 디렉토리들의 리스트로 반환\n  * `FSHook()` : [명세서](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/hooks/filesystem.html#FSHook)\n  \n    ```markdown\n    [docs]class FSHook(BaseHook):\n    \"\"\"\n    Allows for interaction with an file server.\n\n    Connection should have a name and a path specified under extra:\n\n    example:\n    Connection Id: fs_test\n    Connection Type: File (path)\n    Host, Schema, Login, Password, Port: empty\n    Extra: {\"path\": \"/tmp\"} # dictionary 형태\n    \"\"\"\n\n    def __init__(self, conn_id: str = \"fs_default\"):\n        super().__init__()\n        conn = self.get_connection(conn_id)\n        self.basepath = conn.extra_dejson.get(\"path\", \"\")\n        self.conn = conn\n\n    [docs]    def get_conn(self) -> None:\n            pass\n\n\n    [docs]    def get_path(self) -> str:\n            \"\"\"\n            Get the path to the filesystem location.\n\n            :return: the path.\n            \"\"\"\n            return self.basepath\n\n    ```\n    * get_path(): self.baspath return - 별 내용 없음\n    * 생성자만 잘 이해하면 됨\n      * Connection should have a name and a path specified under extra (예시 잘 볼 것):\n![File Sensor Sequential Diagram](../../../../../images/airflow/File_sensor_diagram.PNG)\n\n* Connection 작성\n\n  | Connection_id   | conn_file_opt_airflow_files   |\n  |-----------------|:------------------------------|\n  | Connection_type | File (path)                   |\n  | Host            |                               |\n  | Schema          |                               |\n  | Login           |                               |\n  | Password        |                               |\n  | Port            |                               |\n  | Extra           | {\"path\":\"/opt/airflow/files\"} |\n\n* Dag 작성\n\n```markdown\nfrom airflow import DAG\nfrom airflow.sensors.filesystem import FileSensor\nimport pendulum\n\nwith DAG(\n    dag_id='dags_file_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='0 7 * * *',\n    catchup=False\n) as dag:\n    tvCorona19VaccinestatNew_sensor = FileSensor(\n        task_id='tvCorona19VaccinestatNew_sensor',\n        fs_conn_id='conn_file_opt_airflow_files',\n        filepath='tvCorona19VaccinestatNew/{{data_interval_end.in_timezone(\"Asia/Seoul\") | ds_nodash }}/tvCorona19VaccinestatNew.csv',\n        recursive=False,\n        poke_interval=60,\n        timeout=60*60*24, # 1일\n        mode='reschedule'\n    )\n\n```\n\n# Python Sensor\n\n* python operator를 이용하여 특정 날짜에 데이터가 업데이트 되었는지 확인하는 sensor를 만들기 \n* [airflow.sensors.python 명세서 보기](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/python/index.html)\n* python operator와 매우 유사: 다른점은 python collabe function의 return값이 반드시 true로 되어야함\n```markdown\n\nclassairflow.sensors.python.PythonSensor(*, python_callable, op_args=None, op_kwargs=None, templates_dict=None, **kwargs)[source]\nBases: airflow.sensors.base.BaseSensorOperator\n\nWaits for a Python callable to return True.\n\nUser could put input argument in templates_dict e.g templates_dict = {'start_ds': 1970} and access the argument by calling kwargs['templates_dict']['start_ds'] in the callable\n```\n\nParameters\n* python_callable (Callable) – A reference to an object that is callable. python callable에 넘겨줄 인수들은 아래의 op_kwars, op_args\n* op_kwargs (Mapping[str, Any] | None) – a dictionary of keyword arguments that will get unpacked in your function\n* op_args (list | None) – a list of positional arguments that will get unpacked when calling your callable\n* templates_dict (dict | None) – a dictionary where the values are templates that will get templated by the Airflow engine sometime between __init__ and execute takes place and are made available in your callable’s context after the template has been applied. templates_dict도 python collable에 넘겨주는 인수로 {'start_ds':'{{data_interval_start}}'} 와 같이 사용하면 된다.\n  * 예를 들어, 다음과 같이 사용된다. 하지만 실무에서는 tamplates_dict보단 op_kwargs가 더 많이 사용된다.\n\n  ```markdown\n  def somefunc(x1,x2,**kwargs):\n    v=kwargs['templates_dict']['start_ds'] #{'start_ds':'{{data_interval_start}}'}의 '{{data_interval_start}}'값이 호출된다.\n  ```\n\n\n## Python 센서 DAG 작성\n\n* 무엇을 센싱할 것인가\n  * 서울시 공공데이터에서 당일 날짜로 데이터가 생성되었는지 센싱하기(날짜 컬럼이 있는 경우)\n  * 당일 날짜가 몇시에 업로드가 되는지 모름 (12시? ,00시, 1시?)\n* [서울 열린 데이터 광장](https://data.seoul.go.kr/)\n  * 검색창 >> '코로나' 검색 >> 서울시 코로나19 확진자 발생동향 (2023.05.31.이전) >> 미리보기\n  * 목표: 센서를 공용적으로 만들어 다른 dataset에도 적용할 수 있도록 작성\n    * python collable function(check_api_update())을 일반화 시켜서 작성\n```markdown\nfrom airflow import DAG\nfrom airflow.sensors.python import PythonSensor\nimport pendulum\nfrom airflow.hooks.base import BaseHook\n\nwith DAG(\n    dag_id='dags_python_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='10 1 * * *',\n    catchup=False\n) as dag:\n    def check_api_update(http_conn_id, endpoint, base_dt_col, **kwargs):\n        import requests\n        import json\n        from dateutil import relativedelta\n        connection = BaseHook.get_connection(http_conn_id)\n        url = f'http://{connection.host}:{connection.port}/{endpoint}/1/100' #1부터 100행 까지만 가지고옴\n        response = requests.get(url)\n        \n        contents = json.loads(response.text)\n        key_nm = list(contents.keys())[0]\n        row_data = contents.get(key_nm).get('row') \n          # row_data에 list형태로 데이터가 들어감\n          # [{1행},\n             {2행},\n             {3행},\n             {4행},...]\n        last_dt = row_data[0].get(base_dt_col) #row_data의 첫번째 행, base_dt_col의 key에 대한 value를 추출\n        last_date = last_dt[:10] \n          # 열번째 글자 까지만 slicing\n          # 왜냐면, 서울시 기준일(S_DT)가 date 형식이 아니라 string 형식으로 입력되어 있음\n          # 연/월/일 정보만 필요하기 때문에 시간은 제외\n        last_date = last_date.replace('.', '-').replace('/', '-')\n\n        # 밑에 try구문은 last_date 의 date 형식을 검증\n        try:\n            pendulum.from_format(last_date,'YYYY-MM-DD') # last_date가 'YYYY-MM-DD' 형식으로 parsing이 가능하면 코드 진행이 계속되고 안되면 except문으로 들어가게됨\n        except:\n            from airflow.exceptions import AirflowException\n            AirflowException(f'{base_dt_col} 컬럼은 YYYY.MM.DD 또는 YYYY/MM/DD 형태가 아닙니다.')\n            # 서울시 코로나19 확진자 발생동향 (2023.05.31.이전) 의 데이텉 명세에 따르면\n            # S_DT: 서울시 기준일 (데이터 기준일) 을 의미\n\n        today_ymd = kwargs.get('data_interval_end').in_timezone('Asia/Seoul').strftime('%Y-%m-%d') #2023-05-01 형태로 할당됨\n          # time stamp를 string형태로 바꿈\n          # today_ymd는 batch(DAG)가 도는 날짜\n        if last_date >= today_ymd: # string 형태지만 크기는 비교 가능\n            print(f'생성 확인(배치 날짜: {today_ymd} / API Last 날짜: {last_date})')\n            return True\n        else:\n            print(f'Update 미완료 (배치 날짜: {today_ymd} / API Last 날짜:{last_date})')\n            return False\n\n    sensor_task = PythonSensor(\n        task_id='sensor_task',\n        python_callable=check_api_update,\n        op_kwargs={'http_conn_id':'openapi.seoul.go.kr',\n                   'endpoint':'{{var.value.apikey_openapi_seoul_go_kr}}/json/TbCorona19CountStatus',\n                   'base_dt_col':'S_DT'},\n        poke_interval=600,   #10분\n        mode='reschedule'\n\n    )\n```\n위의 dag의 `check_api_update()` 모든 서울시 공공데이터에 적용할 수 있도록 적고 코로나 데이터에 대한 수행은 `sensor_task`로 이루어지도록 했다. 위와 같이, 코드의 재사용성이 있는 형태로 스크립트를 짜야함\n\n# ExternalTask 센서\n\n## DAG 간 의존관계 설정\n\n* DAG 의존관계 설정 방법\n  * TriggerDagRun 오퍼레이터    \n    ```{dot}\n    digraph G {\n      compound=true;\n      rankdir=LR;\n      subgraph cluster0 {\n        rankdir=TB;\n        task1 [shape=box];\n        task2_1 [shape=box];\n        task2_2 [shape=box];\n        task2_3 [shape=box];\n\n        label= \"Task Flow\";\n      }\n\n      task1 -> task2_1;\n      task1 -> task2_2;\n      task1 -> task2_3;\n    \n    }\n    ```\n\n    * task1: PythonOperator\n    * TriggerDagRun Operator: task1이 끝난 후 다른 dag을 돌리고 싶을 경우에 TriggerDagRun Operator사용. task2,3,4는 task1 후에 돌아가는 task로 triggerDagRun Operator에 의해 task2,3,4에 대응되는 각 각 다른 dag을 돌아가도록 수행된다.\n  * ExternalTask 센서\n    ```{dot}\n    digraph G {\n      compound=true;\n      rankdir=LR;\n      subgraph cluster0 {\n        rankdir=TB;\n        sensor1 [shape=box];\n        sensor2 [shape=box];\n        sensor3 [shape=box];\n        sensor4 [shape=box];\n        task2 [shape=box];\n\n        label= \"Task Flow\";\n      }\n    \n      sensor1 -> task2;\n      sensor2 -> task2;\n      sensor3 -> task2; \n      sensor4 -> task2; \n    }\n    ```\n\n    * task2를 돌리기 위해 다른 dag의 task가 완료된 후에 돌아가야 하는 조건이 붙었을 때 사용\n    * 예를 들어, 위의 그림 처럼, sensor1는 Dag A, sensor2는 Dag B, sensor3은 Dag C, sensor3은 Dag D에 속해있다고 할 때, 4개의 dag이 완료가 되어야 task2가 돌아갈 수 있다.\n  * dataset\n    * airflow 2.4 version에 나온 기능으로 dag의 dependency를 설정하는 새로운 방법\n\n## ExternalTask 센서 명세 확인\n\n* [airflow.sensors.external_task 명세](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/external_task/index.html#module-airflow.sensors.external_task)\n  * Bases: airflow.sensors.base.BaseSensorOperator\n  * Waits for a different DAG, task group, or task to complete for a specific logical date.\n* Parameter\n\n| Parameter   | 필수여부   | 설명                  |\n|-----------------|-----------------|:------------------------------|\n| external_dag_id | O | 센싱할 dag 명  | \n| **external_task_id** | X (셋 중 하나만 입력 가능 없으면 안써도 되고 없을 경우 dag만 센싱) | 센싱할 task_id 명 (string) |\n| **external_task_ids** |  X (셋 중 하나만 입력 가능 없으면 안써도 되고 없을 경우 dag만 센싱) | 센싱할 1 개 이상의 task_id 명 (list) |\n| **external_task_group_id** |  X (셋 중 하나만 입력 가능 없으면 dag만 센싱) | 센싱할 task_group_id명 |\n| allowed_states | X (같은 상태가 입력되면 안됨)  | 센서가 Success 되기 위한 센싱 대상의 상태 (기본적으로 센싱할 task가 success로 끝나야함)      |\n| skipped_states | X (같은 상태가 입력되면 안됨)  | 센서가 Skipped 되기 위한 센싱 대상의 상태 (기본적으로 none, 아무런 정의가 안되어있음)     |\n| failed_states  | X (같은 상태가 입력되면 안됨)  | 센서가 Fail 되기 위한 센싱 대상의 상태 (기본적으로 none, 아무런 정의가 안되어있음)         |\n| execution_delta | Login           | 현재 dag과 센싱할 dag의 data_interval_start의 차이 (스케쥴 차이)를 입력  |\n| execution_date_fn | Password      | 스케쥴 차이를 상수로 구할 수 없는 경우, 센싱할 dag의 data_interval_start를 구하기 위한 함수를 넣어주면 됨        |\n| check_existence | Password        | sensidng dag의 dag_id 또는 task_id 가 있는지 확인 true or false로 입력해야한다. default=false  |\n\n* 위의 allowed_states, skipped_states, failed_states의 states 적을 수 있는 항목은 airflow.utils.state import State (State class) 에 있는 member들만 적을 수 있다.\n  * State.SKIPPED, State.SUCCESS, State.FAILED, State.QUEUED, State.SCHEDULED, State.UP_FOR_RESCHEDULE 등\n(from airflow.utils.state import State 필요)\n* 위의 execution_delta 에는 timedelta() 를 이용하여 입력. \n  * External Task가 있는 dag보다 센싱할 dag이 얼마나 과거에 있는지 양수로 입력\n  * 예를 들어, 센싱할 dag이 6시간 앞선다면 timedelta(hours=6)으로 입력. 주의) -6으로 쓰면 안됨\n* execution_date_fn의 경우 함수가 들어가게 되는데 주로 주 dag의 주기가 다를 때 사용한다.\n  * 예를 들어, externalTask의 dag이 시간단위 스케쥴이고 센싱할 dag의 스케쥴이 월단위로 적혀 있을때, data_interval_schedule값이 점점 벌어지게 된다.\n  * 센싱할 dag의 스케쥴: dag(0 1 1 * *)\n  * external task의 dag의 스케쥴 : dag(0 7 * * *)\n  * data_interval_schedule = 6h (1일차), 1d+6h(2일차), 2d+6h(3일차), $\\ldots$\n* **가장 빈번하게 쓰이고 중요한 parameter: external_dag_id와 execution_delta**\n\n## Dag Full Example\n\n* Monitoring or sensing 할 dag: dags_branch_python_operator.py\n\n```markdown\nfrom airflow import DAG\nimport pendulum\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.python import BranchPythonOperator\n\nwith DAG(\n    dag_id='dags_branch_python_operator',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'), \n    schedule='0 1 * * *',\n    catchup=False\n) as dag:\n    def select_random():\n        import random\n\n        item_lst = ['A','B','C']\n        selected_item = random.choice(item_lst)\n        if selected_item == 'A':\n            return 'task_a' # task_id를 string 값으로 return해야함\n        elif selected_item in ['B','C']:\n            return ['task_b','task_c'] # 여러 task를 동시에 수행시킬 땐 리스트로 반환\n    \n    # 일반 operator의 parameter도 있음\n    python_branch_task = BranchPythonOperator(\n        task_id='python_branch_task',\n        python_callable=select_random\n    )\n    \n    # 후행 task 3개\n    def common_func(**kwargs):\n        print(kwargs['selected'])\n\n    task_a = PythonOperator(\n        task_id='task_a',\n        python_callable=common_func,\n        op_kwargs={'selected':'A'}\n    )\n\n    task_b = PythonOperator(\n        task_id='task_b',\n        python_callable=common_func,\n        op_kwargs={'selected':'B'}\n    )\n\n    task_c = PythonOperator(\n        task_id='task_c',\n        python_callable=common_func,\n        op_kwargs={'selected':'C'}\n    )\n\n    python_branch_task >> [task_a, task_b, task_c]\n```\n* ExternalTask dag: dags_external_task_sensor.py\n\n```markdown\nfrom airflow import DAG\nfrom airflow.sensors.external_task import ExternalTaskSensor\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.utils.state import State \n\nwith DAG(\n    dag_id='dags_external_task_sensor',\n    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),\n    schedule='0 7 * * *',\n    catchup=False\n) as dag:\n    external_task_sensor_a = ExternalTaskSensor(\n        task_id='external_task_sensor_a',\n        external_dag_id='dags_branch_python_operator',\n        external_task_id='task_a',\n        allowed_states=[State.SKIPPED],\n        execution_delta=timedelta(hours=6),\n        poke_interval=10        #10초\n    )\n\n    external_task_sensor_b = ExternalTaskSensor(\n        task_id='external_task_sensor_b',\n        external_dag_id='dags_branch_python_operator',\n        external_task_id='task_b',\n        failed_states=[State.SKIPPED],\n        execution_delta=timedelta(hours=6),\n        poke_interval=10        #10초\n    )\n\n    external_task_sensor_c = ExternalTaskSensor(\n        task_id='external_task_sensor_c',\n        external_dag_id='dags_branch_python_operator',\n        external_task_id='task_c',\n        allowed_states=[State.SUCCESS],\n        execution_delta=timedelta(hours=6),\n        poke_interval=10        #10초\n    )\n```\n\n# Custom Sensor\n\n## Custom Sensor 만들기\n\n* 어떤 센서를 만들 것인가?\n    * Python 센서에서 만들었던 로직을 Custom Sensor화 하기\n    (서울시 공공데이터에서 날짜 컬럼이 있는 경우 날짜 기준 update되었는지 센싱)\n* 재활용성이 높아 다른 DAG에서 활용될 가능성이 높다면 가급적이면 Custom 오퍼레이터화 해놓는 것이 좋다. 그 이유는\n  * 협업 환경에서 코드 중복 구현의 방지\n  * 로직의 일원화 등\n* 위치: `plugins/sensors/seoul_api_date_sensors.py`\n\n## Dag Example\n\n```markdown\nfrom airflow.sensors.base import BaseSensorOperator\nfrom airflow.hooks.base import BaseHook\n\n'''\n서울시 공공데이터 API 추출시 특정 날짜 컬럼을 조사하여 \n배치 날짜 기준 전날 데이터가 존재하는지 체크하는 센서 \n1. 데이터셋에 날짜 컬럼이 존재하고 \n2. API 사용시 그 날짜 컬럼으로 ORDER BY DESC 되어 가져온다는 가정하에 사용 가능\n'''\n\nclass SeoulApiDateSensor(BaseSensorOperator):\n    template_fields = ('endpoint',)\n    def __init__(self, dataset_nm, base_dt_col, day_off=0, **kwargs):\n        '''\n        dataset_nm: 서울시 공공데이터 포털에서 센싱하고자 하는 데이터셋 명\n        base_dt_col: 센싱 기준 컬럼 (yyyy.mm.dd... or yyyy/mm/dd... 형태만 가능)\n        day_off: 배치일 기준 생성여부를 확인하고자 하는 날짜 차이를 입력 (기본값: 0). 즉, 1일 전, 2일 전 데이터가 업데이트 되었는지 확인하는 것.\n        '''\n        super().__init__(**kwargs)\n        self.http_conn_id = 'openapi.seoul.go.kr'\n        self.endpoint = '{{var.value.apikey_openapi_seoul_go_kr}}/json/' + dataset_nm + '/1/100'   # 100건만 추출\n        self.base_dt_col = base_dt_col\n        self.day_off = day_off\n\n        \n    def poke(self, context): # context에 templates 변수들을 호출할 수 있음\n        import requests\n        import json\n        from dateutil.relativedelta import relativedelta\n        connection = BaseHook.get_connection(self.http_conn_id)\n        url = f'http://{connection.host}:{connection.port}/{self.endpoint}'\n        self.log.info(f'request url:{url}')\n        response = requests.get(url)\n\n        contents = json.loads(response.text)\n        key_nm = list(contents.keys())[0]\n        row_data = contents.get(key_nm).get('row')\n        last_dt = row_data[0].get(self.base_dt_col)\n        last_date = last_dt[:10]\n        last_date = last_date.replace('.', '-').replace('/', '-')\n        search_ymd = (context.get('data_interval_end').in_timezone('Asia/Seoul') + relativedelta(days=self.day_off)).strftime('%Y-%m-%d')\n        try:\n            import pendulum\n            pendulum.from_format(last_date, 'YYYY-MM-DD')\n        except:\n            from airflow.exceptions import AirflowException\n            AirflowException(f'{self.base_dt_col} 컬럼은 YYYY.MM.DD 또는 YYYY/MM/DD 형태가 아닙니다.')\n\n        \n        if last_date >= search_ymd: # last_date: API의 날짜, search_ymd: 조회할 날짜\n            self.log.info(f'생성 확인(기준 날짜: {search_ymd} / API Last 날짜: {last_date})')\n            return True\n        else:\n            self.log.info(f'Update 미완료 (기준 날짜: {search_ymd} / API Last 날짜:{last_date})')\n            return False\n\n```\n\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n\n</div>\n\n# Go to Blog Content List\n\n[Blog Content List](../../content_list.qmd)  \n[Engineering Content List](../../Engineering/guide_map/index.qmd)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"10.sensor.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"},"utterances":{"repo":"./docs/comments"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Sensor","subtitle":"Sensor Concept, Bash Sensor, File Sensor, Python Sensor, External Task Sensor, Custom Sensor Creation","description":"Sensor는 특정 조건이 만족하면 task를 실행하게하는 Operator. 실시간에 가까운 workflow를 가능하게 하는 기능.\n","categories":["Engineering"],"author":"Kwangmin Kim","date":"05/01/2023","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}