{"title":"Airflow Additional Function","markdown":{"yaml":{"title":"Airflow Additional Function","subtitle":"Dag Triggering Using Dataset, defult_args Parameter of Dag, Sending an Email When a Task Fails, Task Operation Monitoring using sla and Emailing, Setting timeout, CLI Usage of dag trigger and backfill clear, Triggerer","description":"template\n","categories":["Engineering"],"author":"Kwangmin Kim","date":"05/01/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"comments":{"utterances":{"repo":"./docs/comments"}},"draft":false},"headingText":"Dataset을 이용한 Dag 트리거","containsRefs":false,"markdown":"\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n몰라도 되지만 알면 좋은 고급 기능들\n\n\n## Dataset의 필요성\n\nDAG간의 의존관계를 설정할 때 TriggerDagRun 과 ExternalTask Sensor를 이용한다. 그런데 이 2가지 방법을 이용하면 관리가 복잡해진다. 이를 해결하기 위해 고안된 기술이 dataset을 이용하여 DAG간의 의존관계를 설정하는 것이다. 어떤 상황에서 DAG 관리가 복잡해지고 dataset을 이용하면 어떻게 관리의 효율성이 증가하는지 다음의 cases를 통해 알아보자. \n\n* case 1\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  subgraph cluster0 {\n    rankdir=TB;\n    task_1 [shape=box];\n    task_3 [shape=box];\n    label= \"DAG A\";\n  }\n  task_1 -> task_3;\n  \n  subgraph cluster1 {\n    rankdir=TB;\n    task_k [shape=box];\n   label= \"DAG B\";\n }\n}\n```\n\n* 왼쪽 그림(요구사항) 에서, task3가 TriggerDagRun operator 으로 만들어진 task라 가정한다. task3가 DAG C의 task x를 trigger 한다고 가정한다. 이 경우를 변경하여 Task_1 뿐만 아니라 DAG_B의 task_k 에도 선행 의존관계가 걸려야 한다고 가정한다. 그렇게 하기위해서 External Task 센서를 하나 달아 개선해야 한다는 상황이 있다고 가정한다. 결국, task1과 task k가 성공적으로 끝나야 task3가 수행되고 이어서 DAG C의 task3가 돌아가야 하는 상황\n\n:::\n\n::: {.column width=\"50%\"}\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  subgraph cluster0 {\n    rankdir=TB;\n    task_1 [shape=box];\n    label= \"DAG A\";\n  }\n  \n  subgraph cluster1 {\n    rankdir=TB;\n    Sensor_task_1 [shape=box];\n    Sensor_task_2 [shape=box];\n    task_x [shape=box];\n   label= \"DAG C\";\n }\n subgraph cluster2 {\n    rankdir=TB;\n    task_k [shape=box];\n   label= \"DAG B\";\n }\n\n Sensor_task_1-> task_x;\n Sensor_task_2-> task_x;\n Sensor_task_1-> task_1[label=\"sensing\", style=\"dashed\"];\n Sensor_task_2-> task_k[label=\"sensing\", style=\"dashed\"];\n\n}\n```\n\n:::\n* 오른쪽 그림(해결책)은 왼쪽 그림을 개선하고자 만든 DAG의 의존관계, task3는 제거되어야 한다. 대신, DAG C를 external task sensor를 추가하도록 수정한다. sensor task1은 task1을 수행이 되고 sensor task2가 taskk의 수행이 성공적으로 완료가 된 후에 이 두 sensor task들이 taskx를 수행하도록 개선해야 한다.\n::::\n\n* 이렇게 DAG을 관리할 때 추가적인 DAG의 의존관계 설정을 해야할 때 추가적인 task들이 생기게 되는 요구사항을 해결해야할 때 오른쪽 그림과 같이 복잡한 workflow가 생기게 되는 case가 발생할 수 있다. 또 다른 case는 case2와 같다.\n\n* case 2\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  subgraph cluster0 {\n    rankdir=TB;\n    task_1 [shape=box];\n    task_3 [shape=box];\n    task_4 [shape=box];\n    label= \"DAG A\";\n  }\n  subgraph cluster {\n    rankdir=TB;\n    task_y [shape=box];    \n   label= \"DAG Y\";\n }\n  task_1 -> task_3;\n  task_y;\n  task_4;\n  \n}\n```\n\n* 왼쪽 그림(요구사항)에서, Task_1이 완료되면 또 다른 dag 을 trigger 되고 싶은데 DAG A 를 수정해야 할 경우이다. 다시 말해서, task1이 완료되면 또 다른 dag Y의 task y를 trigger하는 TriggerDagRun task4를 만들어 주고 싶도록 dag A를 수정해야하는 상황. 즉, task y는 task 1 과 task 4가 성공적으로 완료되어야만 수행되도록 설정해주고 싶은 상황. 뿐만 아니라 DAG A에 지속적인 변경사항 있을시 계속 workflow를 수정해야하는 문제점이 있다. \n:::\n\n::: {.column width=\"50%\"}\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  \n  subgraph cluster {\n    rankdir=TB;\n    task_1 [shape=box];\n    task_3 [shape=box];\n    task_4 [shape=box];\n   label= \"DAG A\";\n }\n  subgraph cluster {\n    rankdir=TB;\n    task_y [shape=box];    \n   label= \"DAG Y\";\n }\n    task_1 -> task_3;\n    task_1 -> task_4;\n    task_4 -> task_y;\n}\n```\n\n:::\n\n* 오른쪽 그림이 개선된 workflow\n\n::::\n\n* 이렇게, TriggerDagRun 오퍼레이터와 ExternalTask 센서를 많이 사용하다보면 아래 그림과 같이 연결관리에 많은 노력이 필요\n* 특히, ExternalTask은 execution_delta, 즉 data_interval_start 차이를 정확하게 연결해야 작동하도록 되어 있기 때문에 매우 강한 연결이 전제가 된다. 즉, parameter 중 일부가 안맞거나 틀리게 되면 제대로 sensing이 안됨\n\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  \n  subgraph cluster {\n    rankdir=TB;\n    DAG_A [shape=box];\n    DAG_B [shape=box];\n    DAG_C [shape=box];\n    DAG_D [shape=box];\n    DAG_E [shape=box];\n    DAG_G [shape=box];\n   label= \"DAG Flow\";\n }\n    DAG_A -> DAG_B;\n    DAG_A -> DAG_C;\n    DAG_D -> DAG_C[style=\"dashed\"];\n    DAG_D -> DAG_E[style=\"dashed\"];\n    DAG_E -> DAG_G[style=\"dashed\"];\n}\n```\n\n* 위와 같은 복잡하고 rigid한 workflow를 개선한 것이 dataset을 이용한 방식이다.   \n* 큐시스템과 같이 Job 수행이 완료된 Task 는 Push 하고 센싱이 필요한 task 은 큐 시스템을 구독하는 방식을 쓴다면 더 유연한 workflow가 생성될 수 있다. (약한 연결)\n\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  \n  subgraph cluster {\n    rankdir=TB;\n    DAG_A [shape=box];\n    DAG_B [shape=box];\n    DAG_C [shape=box];\n    DAG_D [shape=box];\n    DAG_E [shape=box];\n    DAG_G [shape=box];\n    Queue [shape=cylinder];\n   label= \"DAG Flow\";\n }\n    DAG_A -> Queue;\n    DAG_D -> Queue;\n    Queue -> DAG_B;\n    Queue -> DAG_C;\n    Queue -> DAG_E;\n    \n}\n```\n\n* DAG A가 작업이 끝났다라는 메시지를 큐에 넣고 DAG A가 완료되면 자동으로 DAG B가 triggering됨\n* 나중에 DAG F가 새로 생겨 DAG A가 끝났을 때 수행되도록 triggering 되어야 한다면 큐에 있는 메시지를 인식하고 돌아가도록 설정하면 된다. \n* 그러므로, DAG A를 구독하는 후행 DAG들이 아무리 많더라도 큐에 저장된 메시지만 바라보게 만들면 된다. DAG A는 수정할 필요가 없게 된다. \n* Produce / Consume 구조를 이용하여 Task 완료를 알리기 위해 특정 키 값으로 Produce 하고 해당 키를 Consume 하는 DAG 을 트리거 스케줄 할 수 있는 기능\n  * produce: queue에 메세지를 넣는 행위. 다른 솔루션에서는 produce를 publish라고도 부름  \n  * consume: queue로부터 메세지를 읽거나 꺼내는 행위. 보통은 이 consume다른 솔루션에서는 subscribe라고도 많이 부름. airflow에서는 subscribe가 consume으로 사용된다.\n  * 선행 Dag이 produce하고 후행 Dag이 consume한다.\n  * 실제 큐가 있는 것은 아니고 DB 에 Produce / Consume 내역을 기록\n\n## DAG Full Example\n\n* dags_dataset_producer_1.py\n\n```markdown\nfrom airflow import Dataset #airflow의 dataset이라는 library 호출\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_1 = Dataset(\"dags_dataset_producer_1\")\n# Dataset의 인수값으로 string을 주는데 DAG이 큐에 메세지를 넣을 때 메세지가  dictionary형태로 저장이 되는데 그 키값을 input으로 넣게 된다. \n\n\nwith DAG(\n        dag_id='dags_dataset_producer_1',\n        schedule='0 7 * * *',\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        outlets=[dataset_dags_dataset_producer_1], # outlets 인수는 baseOperator로부터 상속된다. 모든 operator는 outlets인수를 가지고 있다. outlets에 다가는 리스트 구조로 선언된다. queue에다가 publish되는 메세지는 dag안에 있는 task('bash_task')가 dataset_dags_dataset_producer_2 이름으로  publish된다.  dataset_dags_dataset_producer_2가 들어가 있다. \n        bash_command='echo \"producer_1 수행 완료\"'\n    )\n```\n\n* dags_dataset_producer_2.py\n\n```markdown\nfrom airflow import Dataset\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_2 = Dataset(\"dags_dataset_producer_2\")\n\nwith DAG(\n        dag_id='dags_dataset_producer_2',\n        schedule='0 7 * * *',\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        outlets=[dataset_dags_dataset_producer_2], \n        bash_command='echo \"producer_2 수행 완료\"'\n    )\n```\n\n\n* dags_dataset_consumer_1.py\n\n```markdown\nfrom airflow import Dataset \nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_1 = Dataset(\"dags_dataset_producer_1\") # dags_dataset_producer_1.py의 producer1의 키값 'dags_dataset_producer_1'을 넣어 호출하고 있음\n\nwith DAG(\n        dag_id='dags_dataset_consumer_1',\n        schedule=[dataset_dags_dataset_producer_1], # 스케쥴이 다른 형식으로 들어가 있는데 dags_dataset_producer_1.py의 producer1 (정확히 말하면 task)을 구독하겠다는 뜻. 스케쥴은 없고 publish하고 있는 dag 수행이 끝나면 실행하겠다는 뜻.\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        bash_command='echo {{ ti.run_id }} && echo \"producer_1 이 완료되면 수행\"'\n    )\n```\n\n* dags_dataset_consumer_2.py\n\n```markdown\nfrom airflow import Dataset\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_1 = Dataset(\"dags_dataset_producer_1\")\ndataset_dags_dataset_producer_2 = Dataset(\"dags_dataset_producer_2\")\n# producer1,2 2개를 동시에 구독하고있음\n\nwith DAG(\n        dag_id='dags_dataset_consumer_2',\n        schedule=[dataset_dags_dataset_producer_1, dataset_dags_dataset_producer_2],\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        bash_command='echo {{ ti.run_id }} && echo \"producer_1 와 producer_2 완료되면 수행\"'\n    )\n```\n\nairflow web service에서 dag 상태를 확인할 때 초록색 긴막대기를 누르면 Dataset Events에서 구독 depndencies를 확인할 수 있다. Run ID도 특이하게 나오게 되는 것을 확인 할 수 있다. \n\n## Dataset Workflow \n\nAirflow Web Service의 dataset 메뉴를 클릭하면 publish와 subscribe의 의존관계를 graph로 볼 수 있다. \n\n## DAG Dependencies\n\nAirflow Web Service의 Browse 메뉴의 Dag Dependencies를 클릭하면 Dag간의 의존관계를 볼 수 있다. \n\n## 정리\n\n* Dataset 은 Publish(produce)/subscribe(consume) 구조로 DAG 간 의존관계를 주는 방법\n* Unique 한 String 형태의 Key 값을 부여하여 Dataset Publish\n* Dataset 을 Consume 하는 DAG 은 스케줄을 별도로 주지 않고 리스트 형태로 구독할 Dataset 요소들을 명시\n* Dataset에 의해 시작된 DAG 의 Run_id 는 dataset_triggered__{trigger 된 시간 } 으로 표현됨\n* Airflow 화면 메뉴 중 Datasets 메뉴를 통해 별도로 Dataset 현황 모니터링 가능\n* N개의 Dataset 을 구독할 때 스케줄링 시점은 ?\n    * 마지막 수행 이후 N 개의 Dataset 이 모두 재 업데이트되는 시점\n    ![Trigger using dataset - N개의 dataset 구독](../../../../../images/airflow/dataset_trigger.PNG)\n\n# DAG의 default_args 파라미터\n\n## DAG 의 default_args 파라미터\n\n* 목적: DAG 하위 모든 오퍼레이터에 공통 적용될 파라미터를 입력\n* 어떤 파라미터들이 적용 가능할까?\n    * 오퍼레이터에 공통 적용할 수 있는 파라미터들\n    * BaseOperator 클래스 생성자가 가지고 있는 파라미터\n* [airflow doc](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/baseoperator.html#BaseOperator)\n* Code \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n```markdown\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n) as dag:\n    \n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='sleep 10m',\n        sla: timedelta(seconds=70), #SLA 설정\n        email: 'sdf@sdfsfd.com'\n    )\n    \n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='sleep 2m',\n        sla: timedelta(seconds=70), #SLA 설정\n        email: 'sdf@sdfsfd.com'\n    )\n\n   task_1 >> task_2\n\n```\n:::\n\n::: {.column width=\"50%\"}\n\n```markdown\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': 'sdf@sdfsfd.com'\n    }\n) as dag:\n    \n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='sleep 10m'\n    )\n    \n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='sleep 2m'\n    )\n\n   task_1 >> task_2\n\n```\n\n:::\n\n::::\n\n## BaseOperator 파라미터 vs Dag 파라미터\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n### DAG Parameter\n\n* DAG 파라미터는 DAG 단위로 적용될 파라미터\n* 개별 오퍼레이터에 적용되지 않음\n* DAG 파라미터는 default_args 에 전달하면 안됨\n\n:::\n\n::: {.column width=\"60%\"}\n\n### BaseOperator Parameter\n\n* Base 오퍼레이터 파라미터는 개별 Task 단위로 적용될 파라미터.\n* Task 마다 선언해줄 수 있지만 DAG 하위 모든 오퍼레이터에 적용 필요시 default_args 를 통해 전달 가능\n:::\n\n::::\n\n* default_args 에 전달된 파라미터보다 개별 오퍼레이터에 선언된 파라미터가 우선순위를 가짐\n\n```markdown\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': 'sdf@sdfsfd.com'\n    }\n) as dag:\n    \n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='sleep 10m'\n    )\n    \n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='sleep 2m',\n        sla=timedelta(minutes=1)\n    )\n\n   task_1 >> task_2\n\n```\n\n# Task 실패시 Email 발송하기\n\n## Email 발송 위한 파라미터 확인\n\n* [airflow doc](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/baseoperator.html#BaseOperator)\n* email 파라미터와 email_on_failure 파라미터를 이용\n* email 파라미터만 입력하면 email_on_failure 파라미터는 True 로 자동설정됨\n\n## Email 발송 대상 등록\n\n```markdown\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.decorators import task\nfrom airflow.exceptions import AirflowException\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_email_on_failure',\n    start_date=pendulum.datetime(2023,5,1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule='0 1 * * *',\n    dagrun_timeout=timedelta(minutes=2),\n    default_args={\n        'email_on_failure': True,\n        'email': email_lst\n    }\n) as dag:\n    @task(task_id='python_fail')\n    def python_task_func():\n        raise AirflowException('에러 발생')\n    python_task_func()\n\n    bash_fail = BashOperator(\n        task_id='bash_fail',\n        bash_command='exit 1',\n    )\n\n    bash_success = BashOperator(\n        task_id='bash_success',\n        bash_command='exit 0',\n    )\n\n```\n\n* Email 받을 대상이 1 명이면 string 형식으로 , 2 명 이상이면 list 로 전달\n* 그런데 협업 환경에서 DAG 담당자는 수시로 바뀔 수 있고 인원도 수시로 바뀔 수 있는데 그때마다 DAG 을 뒤져서 email 리스트를 수정해야 할까 ?\n* 이럴때 Variable 을 이용하자\n\n# SLA로 task 수행 감시 & Email 발송하기\n\n## SLA파라미터 이해\n\n* 개념: 오퍼레이터 수행시 정해놓은 시간을 초과하였는지를 판단할 수 있도록 설정해놓은 시간 값 파이썬의 timedelta 로 정의\n* 동작: 설정해놓은 SLA 를 초과하여 오퍼레이터 running 시 SLA Miss 발생 , Airflow UI 화면에서 Miss 건만 조회 가능 + email 발송 가능\n    * SLA Miss 발생시 task 가 실패되는 것은 아니며 단순 Miss 대상에 기록만 DB에 남기게 됨\n    * SLA 는 DAG 파라미터가 아니며 BaseOperator 의 파라미터\n* [airflow docs- def __init__](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/baseoperator.html#BaseOperator)\n  ```markdown\n   def __init__(\n        self,\n        task_id: str,\n        owner: str = DEFAULT_OWNER,\n        email: str | Iterable[str] | None = None,\n        email_on_retry: bool = conf.getboolean(\"email\", \"default_email_on_retry\", fallback=True),\n        email_on_failure: bool = conf.getboolean(\"email\", \"default_email_on_failure\", fallback=True),\n        retries: int | None = DEFAULT_RETRIES,\n        retry_delay: timedelta | float = DEFAULT_RETRY_DELAY,\n        retry_exponential_backoff: bool = False,\n        max_retry_delay: timedelta | float | None = None,\n        start_date: datetime | None = None,\n        end_date: datetime | None = None,\n        depends_on_past: bool = False,\n        ignore_first_depends_on_past: bool = DEFAULT_IGNORE_FIRST_DEPENDS_ON_PAST,\n        wait_for_past_depends_before_skipping: bool = DEFAULT_WAIT_FOR_PAST_DEPENDS_BEFORE_SKIPPING,\n        wait_for_downstream: bool = False,\n        dag: DAG | None = None,\n        params: collections.abc.MutableMapping | None = None,\n        default_args: dict | None = None,\n        priority_weight: int = DEFAULT_PRIORITY_WEIGHT,\n        weight_rule: str = DEFAULT_WEIGHT_RULE,\n        queue: str = DEFAULT_QUEUE,\n        pool: str | None = None,\n        pool_slots: int = DEFAULT_POOL_SLOTS,\n        sla: timedelta | None = None, # sla 변수\n        execution_timeout: timedelta | None = DEFAULT_TASK_EXECUTION_TIMEOUT,\n        on_execute_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n        on_failure_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n        on_success_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n        on_retry_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n        pre_execute: TaskPreExecuteHook | None = None,\n        post_execute: TaskPostExecuteHook | None = None,\n        trigger_rule: str = DEFAULT_TRIGGER_RULE,\n        resources: dict[str, Any] | None = None,\n        run_as_user: str | None = None,\n        task_concurrency: int | None = None,\n        max_active_tis_per_dag: int | None = None,\n        max_active_tis_per_dagrun: int | None = None,\n        executor_config: dict | None = None,\n        do_xcom_push: bool = True,\n        inlets: Any | None = None,\n        outlets: Any | None = None,\n        task_group: TaskGroup | None = None,\n        doc: str | None = None,\n        doc_md: str | None = None,\n        doc_json: str | None = None,\n        doc_yaml: str | None = None,\n        doc_rst: str | None = None,\n        **kwargs,\n    ):\n  ```\n  * sla: timedelta | None = None, # sla 변수\n    * timedelta보다 오랜 시간동안 task가 더 오래 수행되면 SLA Miss 기록만 남기고 실패처리는 안하게 됨\n    * default_arg에 넣을 수 있기 때문에 모든 task, 즉 모든 operator에 공통 적용할 수 있다.\n* [sla_miss_callback: DAG에 있는 SLA 관련 파라미터로 SLA Miss시 수행할 함수 지정 가능-Source code for airflow.models.dag](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/dag.html)\n    * sla_miss_callback 파라미터\n      * sla miss시 수행할 함수명을 입력 받음\n      * dag에 있는 파라미터이기 때문에 default_arg에 넣을 수 없음\n    * DAG의 파라미터임 (BaseOperator 의 파라미터가 아니라는 것에 유의)\n\n## SLA 제약사항\n\n* 1번째 제약사항: 각 Task 의 SLA timeout 카운트 시작은 DAG 의 시작시간 기준임\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n### 사람이 생각하는 SLA timeout 카운트 방식\n\n![](../../../../../images/airflow/SLA_timeout_count1.PNG)\n\n* 처음 사용하는 사용자들은 다음과 같이 생각할 수 있다.\n  * task1은 task1의 sla구간보다 수행시간이 짧게 완료됐기 때문에 성공 처리\n  * task2는 sla 구간보다 더 길기 때문에 miss 처리됨\n  * task3는 sla 구간보다 짧기 때문에 성공 처리\n* airflow는 위와 같이 동작하지 않음 옆의 그림과 같이 동작함\n:::\n\n::: {.column width=\"60%\"}\n\n### 실제 Airflow 의 SLA timeout 카운트 방식\n\n![](../../../../../images/airflow/SLA_timeout_count2.PNG)\n\n* 실제 airflow에서는 수행 시간을 dag의 최초 실행시점 부터 카운트 하기 시작함\n* task1의 수행완료 상태 및 시점과 상관없이 task2와 task3의 수행 시간은 task1 수행 시점 부터 카운트 되기 시작한다.\n* 그러므로 task2, task3는 수행된적이 없음에도 miss sla 상태로 처리됨\n:::\n\n::::\n\n* 2번째 제약사항: DAG의 시작시간 기준 $\\rightarrow$ 명목적인 시작 시간 (data_interval_start)\n  * 위의 SLA timeout 카운트 시작은 dag의 시작시간 기준이라는 제약 사항과 연결되는 제약사항\n  * 만약 Pool 이 부족하거나 스케줄러의 부하로 인해 DAG 이 스케줄보다 늦게 시작했다면 늦게 시작한만큼 이미 SLA 시간 카운트는 진행되어 시간이 소요되고 있음\n    * 실질적 DAG 시작 시간을 기준으로 카운트하지 않음\n  * 즉, 특정 task 수행에 computing 부하가 걸리면 후차적인 task들은 모두 miss 처리됨\n  * 대규모 프로젝트에서 흔히 발생하는 문제로 dag이 밀리는 현상이 자주 관찰되므로 sla는 자주쓰이는 방식은 아님.\n* 3번째 제약사항: 첫 스케줄에서는 SLA Miss 가 기록되지 않음 (두 번째 스케줄부터 기록)\n* 4번째 제약사항: sla miss처리가 분명히 발생됐는데 가끔 기록이 안될때가 있고 email역시 발송이 되지 않을때가 있음. 엄격이 관리가 되지 않는 기능\n\n## Full Dag Example\n\n```markdown\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': email_lst\n    }\n) as dag:\n    \n    # 30초 sleep\n    task_slp_30s_sla_70s = BashOperator( \n        task_id='task_slp_30s_sla_70s',\n        bash_command='sleep 30'\n    )\n    # 60초 sleep\n    task_slp_60_sla_70s = BashOperator(\n        task_id='task_slp_60_sla_70s',\n        bash_command='sleep 60'\n    )\n\n    # 10초 sleep\n    task_slp_10s_sla_70s = BashOperator( \n        task_id='task_slp_10s_sla_70s',\n        bash_command='sleep 10'\n    )\n\n    # 10초 sleep\n    # sla의 timedelta를 명시적으로 30초 선언\n    # default argument보다 명시적 선언이 우선 순위가 더 높음\n    # 그래서, 처음 3개의 task는 timedelta가 70초로 설정됐고 4번째 task는 30초로 설정됨 \n    task_slp_10s_sla_30s = BashOperator( \n        task_id='task_slp_10s_sla_30s',\n        bash_command='sleep 10',\n        sla=timedelta(seconds=30)\n    )\n\n    task_slp_30s_sla_70s >> task_slp_60_sla_70s >> task_slp_10s_sla_70s >> task_slp_10s_sla_30s\n\n```\n\n* 위 코드를 보면,\n  * 1번째 task는 30초 sleep이 70초 sla timedelta보다 짧기 때문에 성공 처리\n  * 2번째 task는 30,40초 돌다가 miss 처리됨\n  * 3,4 번째 task는 수행되기도 전에 miss처리됨\n  * 4번째 timedelta가장 짧기 때문에 가장 먼저 miss 처리 됨\n* Airflow web service에서 SLA Miss 현황 조회하기 \n  * airflow web service >> browse \n\n## SLA Miss 시 email 발송하기\n\n```markdown\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': email_lst\n    }\n) as dag:\n    \n    task_slp_30s_sla_70s = BashOperator(\n        task_id='task_slp_30s_sla_70s',\n        bash_command='sleep 30'\n    )\n    \n    task_slp_60_sla_70s = BashOperator(\n        task_id='task_slp_60_sla_70s',\n        bash_command='sleep 60'\n    )\n\n    task_slp_10s_sla_70s = BashOperator(\n        task_id='task_slp_10s_sla_70s',\n        bash_command='sleep 10'\n    )\n\n    task_slp_10s_sla_30s = BashOperator(\n        task_id='task_slp_10s_sla_30s',\n        bash_command='sleep 10',\n        sla=timedelta(seconds=30)\n    )\n\n    task_slp_30s_sla_70s >> task_slp_60_sla_70s >> task_slp_10s_sla_70s >> task_slp_10s_sla_30s\n\n```\n\n# timeout 설정하기\n\n## Timeout 파라미터 이해\n\n* Task수준의 timeout 과 DAG 수준의 timeout 이 존재\n* execution_timeout: [Task수준의 timeout 파라미터: baseOperator 명세에서 __init__ 생성자에서 sla 파라미터 아래에 있음](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/baseoperator.html#BaseOperator)\n  * execution_timeout: timedelta | None = DEFAULT_TASK_EXECUTION_TIMEOUT,\n  * timedelta보다 오래 task가 수행됐을 때 task는 fail 처리됨\n  * task가 fialure 됐을 때 __init__ 생성자 안의 다른 파라미터인 ` email_on_failure: bool = conf.getboolean(\"email\", \"default_email_on_failure\", fallback=True)` 과 `email: str | Iterable[str] | None = None,`  에 의해 지정 대상에게 email을 보낼 수 있다.\n  * 요약하면, timedelta, email_on_failure, email 이렇게 3개의 파라미터를 이용하여 timedelta 를 초과하는 task 수행시 fail 처리를 하여 이메일을 보낼 수 있다.\n* dagrun_timeout: [DAG수준에서도 timeout 파라미터를 걸 수 도 있음](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/dag.html) : dagrun_timeout\n  * [docs]class DAG(LoggingMixin): 의 __init__ 생성자\n  ```markdown\n\n      def __init__(\n        self,\n        dag_id: str,\n        description: str | None = None,\n        schedule: ScheduleArg = NOTSET,\n        schedule_interval: ScheduleIntervalArg = NOTSET,\n        timetable: Timetable | None = None,\n        start_date: datetime | None = None,\n        end_date: datetime | None = None,\n        full_filepath: str | None = None,\n        template_searchpath: str | Iterable[str] | None = None,\n        template_undefined: type[jinja2.StrictUndefined] = jinja2.StrictUndefined,\n        user_defined_macros: dict | None = None,\n        user_defined_filters: dict | None = None,\n        default_args: dict | None = None,\n        concurrency: int | None = None,\n        max_active_tasks: int = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\"),\n        max_active_runs: int = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\"),\n        dagrun_timeout: timedelta | None = None,\n        sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None,\n        default_view: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),\n        orientation: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),\n        catchup: bool = airflow_conf.getboolean(\"scheduler\", \"catchup_by_default\"),\n        on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,\n        on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,\n        doc_md: str | None = None,\n        params: collections.abc.MutableMapping | None = None,\n        access_control: dict | None = None,\n        is_paused_upon_creation: bool | None = None,\n        jinja_environment_kwargs: dict | None = None,\n        render_template_as_native_obj: bool = False,\n        tags: list[str] | None = None,\n        owner_links: dict[str, str] | None = None,\n        auto_register: bool = True,\n    ):\n\n    ```\n    * 다음 파라미터를 이용하여 dag을 관리할 수 있다.\n      * schedule: ScheduleArg = NOTSET, \n      * start_date: datetime | None = None, \n      * default_args: dict | None = None, \n      * dagrun_timeout: timedelta | None = None,\n* task1 >> task2 >> task3 예시\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n### execution_timeout\n\n* dag은 timeout이 안됐지만 task가 timeout이 되는 case\n\n![](../../../../../images/airflow/execution_timeout1.PNG)\n\n* task2의 실행시간이 task2의 execution_timeout보다 길게 수행되어 fail되었고 task3은 task2로 인해 upstream failed 이 발생\n* 하지만, task1,2,3 의 수행시간의 총합이 dagrun_timeout 보다 짧기 때문에 dag 자체는 실패처리 안됨\n:::\n\n::: {.column width=\"50%\"}\n\n### datgrun_timeout\n\n* task는 성공 처리 됐지만 dag이 오랜시간동안 돌아 timeout되어 실패처리되는 case\n\n![](../../../../../images/airflow/dagrun_timeout.PNG)\n\n* 각 task들이 execution_timeout보다 짧아 모두 정상 처리 되었지만 task1,2,3의 총 실행시간이 dagrun_timeout보다 길어 dag자체는 failure이 된 상황\n* dag이 failure되는 시점에서의 task는 skipped 상태로 처리된다.\n:::\n\n::::\n\n## Dag Full Example\n\n* case1: tasks는 실패, dargrun 정상 \n\n```markdown\n# Package Import\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.models import Variable\n\n# email 수신자 리스트\nemail_str = Variable.get(\"email_target\") \nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_timeout_example_1',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule=None,\n    dagrun_timeout=timedelta(minutes=1),\n    default_args={\n        #각 task들이 20초안에 끝나야 성공 처리됨\n        'execution_timeout': timedelta(seconds=20), \n        'email_on_failure': True,\n        'email': email_lst\n    }\n) as dag:\n    # execution_timeout보다 길기 때문에 task는 실패 처리됨\n    bash_sleep_30 = BashOperator(\n        task_id='bash_sleep_30', \n        bash_command='sleep 30',\n    )\n    # execution_timeout보다 짧기 때문에 task는 성공 처리됨\n    bash_sleep_10 = BashOperator(\n        trigger_rule='all_done', # upstream fail에도 task 실행시키기 위해 triggering\n        task_id='bash_sleep_10',\n        bash_command='sleep 10',\n    )\n    \n    # upstream failure 발생해도 trigger_rule을 all_done을 줬기 때문에 bash_sleep_10 은 실행됨\n    # dagrun_timeout을 1분으로 설정했기 때문에 task run의 총합이 40초이기 때문에 dagrun은 정상 처리됨\n    bash_sleep_30 >> bash_sleep_10 \n    \n\n```\n\n* case2: tasks는 정상, dargrun 실패\n\n```markdown\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_timeout_example_2',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule=None,\n    dagrun_timeout=timedelta(minutes=1),\n    default_args={\n        'execution_timeout': timedelta(seconds=40),\n        'email_on_failure': True,\n        'email': email_lst\n    }\n) as dag:\n    bash_sleep_35 = BashOperator(\n        task_id='bash_sleep_35',\n        bash_command='sleep 35',\n    )\n\n    bash_sleep_36 = BashOperator(\n        trigger_rule='all_done',\n        task_id='bash_sleep_36',\n        bash_command='sleep 36',\n    )\n\n    bash_go = BashOperator(\n        task_id='bash_go',\n        bash_command='exit 0',\n    )\n# 모든 task들이 40초안에 실행완료가 되기때문에 성공 처리됨\n# dagrun은 1분으로 설정됐기 때문에 2번째 task 가 실행될 때 dag이 fail되고\n# 2번째task는 skipped 처리가 됨, 이 task에 대해서는 email도 안감 \n# 3번째 task는  no status 처리됨 , 이 task에 대해서는 email도 안감\n    bash_sleep_35 >> bash_sleep_36 >> bash_go\n    \n```\n\n* dagrun_timeout의 한계점\n  * 모든 task들이 40초안에 실행완료가 되기때문에 성공 처리됨\n  * dagrun은 1분으로 설정됐기 때문에 2번째 task 가 실행될 때 dag이 fail되고\n  * 2번째task는 skipped 처리가 됨, 이 task에 대해서는 email도 안감 \n  * 3번째 task는  no status 처리됨 , 이 task에 대해서는 email도 안감\n\n## 정리\n\n| Comparision       | sla               | execution_timeout | dagrun_timeout    |\n|-------------------|-------------------|-------------------|-------------------|\n| 파라미터 정의 위치 | BaseOperator      | BaseOperator    | DAG    |\n| 적용 수준          | Task              | task    | DAG    |\n| 기능    | 지정한 시간 초과시 Miss 기록  | 지정한 시간 초과시 task fail 처리 | 지정한 시간 초과시 DAG fail 처리    |\n| email 발송 가능 여부| O                | O    | X    |\n| timeout 발생시 후행 task 상태 | 상관없이 지속 | Upstream_failed    | Skipped (current) /No status (not run)    |\n| 스케쥴 필요       | O                  | X    | X    |\n\n* sla, execution_timeout에는 email 발송 paraemeter가 있지만 execution_timeout에는 없다.\n* dagrun timeout이 fail 됐을 때 반드시 email 발송 하고싶으면 dag의 파라미터 중 on_failure_callback 에 dag이 실패됐을 때 이메일을 전송하는 함수를 만들어 그 함수명을 할당해준다.\n* upstream_failed 상태는 execution_timeout만이 갖는 특징이 아니라 airflow의 디폴트 설정이다. 상위 task들이 fail되면 후행 task들은 upstream_failed로 남는다.\n* 공통점: 파이썬의 timedelta 함수로 timeout 기준 시간 정의  \n\n# Airflow CLI 사용하기\n\n* Airflow가 설치되어 있는 서버 또는 환경에서 shell 명령을 이용하여 Airflow를 컨트롤 할 수 있도록 많은 기능들을 제공하고 있음\n* [airflow cli doc](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html)\n  * 대표적인 content\n    * dags: dag을 다룰 수 잇는 커맨드  \n      * `airflow dags [-h] COMMAND ...`\n      * backfill: airflow web ui 의 grid 기능을 보면 dag이 돌았던 이력을 볼 수 있는데 grid 상 가장 과거 날짜 뿐만 아니라 그 이전의 과거 날짜 또한 command의 옵션으로 모두 돌릴 수 있음\n      ```markdown\n        airflow dags backfill [-h] [-c CONF] [--continue-on-failures]\n                      [--delay-on-limit DELAY_ON_LIMIT] [--disable-retry] [-x]\n                      [-n] [-e END_DATE] [-i] [-I] [-l] [-m] [--pool POOL]\n                      [--rerun-failed-tasks] [--reset-dagruns] [-B]\n                      [-s START_DATE] [-S SUBDIR] [-t TASK_REGEX]\n                      [--treat-dag-as-regex] [-v] [-y]\n                      dag_id\n      ```\n      * delete: Delete all DB records related to the specified DAG\n      ```markdown\n      airflow dags delete [-h] [-v] [-y] dag_id\n      ```  \n      * details: Get DAG details given a DAG id\n      ```markdown\n      airflow dags details [-h] [-o table, json, yaml, plain] [-v] dag_id\n      * list: List all the DAGs\n      ```markdown\n      airflow dags list [-h] [-o table, json, yaml, plain] [-S SUBDIR] [-v]\n      ```\n    * variables: variables을 관리하는 command\n      * `airflow variables [-h] COMMAND ...`\n      * delete: `airflow variables delete [-h] [-v] key`\n        * 등록되있는 variables을 key값을 입력하여 삭제\n      * export: `airflow variables export [-h] [-v] file`\n        * 등록되있는 variables을 json file로 추출\n      * get: `airflow variables get [-h] [-d VAL] [-j] [-v] key`\n        * 특정 variables의 key값을 주어 values을 꺼내옴\n      * import: `airflow variables import [-h] [-v] file`\n        * json file에 variables을 작성해놓고 list를 한번에 입력한다.\n      * list: `airflow variables list [-h] [-o table, json, yaml, plain] [-v]`\n      * set: `airflow variables set [-h] [-j] [-v] key VALUE`\n\n* Cli를 잘 쓰면 좋은 이유\n    * 일괄작업: Airflow UI에서 할 수 없는 일괄 작업 방식을 제공\n      * ex: connection 일괄 등록. 만약 airflow ui로 등록하면 일일히 등록해야한다.\n      * 물론 CLI를 이용하는 방법 외에 metaDB table에 직접 insert하는 방법도 있음\n    * 특수기능: Airflow UI에서는 할 수 없는 기능을 제공\n      * ex: backfill 은 airflow ui 를 통해서는 실행 불가\n    * 자동화: Airflow UI에서 직접 눈으로 보고 클릭하는 방식이 아닌 프로그래밍에 의한 제어가 가능해짐\n      * CLI 커맨드는 shell 명령어로 이루저있기 때문에 shell script를 작성하여 자동화 할 수 있다.\n\n## cli - dag trigger\n\n* dag trigger: airflow ui 상에서 manual 로 dag trigger 하거나 run_id 를 직접 넣어 trigger 할 수 있는 기능으로 CLI로 실행시킬 수 있다.\n* CLI 명령은 WSL2에서 하는게 아니라 docker container안에서 해야함\n\n  ```markdown\n  airflow dags trigger [-h] [-c CONF] [-e EXEC_DATE] \n                       [--no-replace-microseconds] [-o table, json, yaml, plain] [-r RUN_ID] [-S SUBDIR] [-v]\n                     dag_id\n  ```\n  * EXEC_DATE: execution_date parameter는 모두 data_interval_start 기준이며 String 형식으로 입력하면 기본 UTC로 계산됨\n  * RUN_ID를 입력하면 기본적으로 manual__로 시작하며 run_id를 직접 입력도 가능\n  * 예시: `#> airflow dags trigger dags_seoul_api_corona`\n* Full Example\n```markdown\n# docker container list 확인\nsudo docker ps \n\n# webserver container 선택 (어떤 것을 골라도 상관없음)\n\n# airflow container 들어가기: sudo docker exec -it [docker_container_id] bash\nsudo docker exec -it 8b755cb5aa70 bash \n\n# trigger 명령어 실행\nairflow dags trigger dags_base_branch_operator\n\n\n```\n## 결과\n\n![](../../../../../images/airflow/cli_example_1.PNG)\n\n* dag_run_id: manual__2023-07-22T05:05:59+00:00.\n* run type: manual__\n  * ClI 로 돌렸기 때문에 manual_이 붙어있음\n  * run types: schedule, manual, backfill 등이 있음\n\n## cli - dag backfill\n\n* 입력 스케줄 구간에 대해 일괄 (재)실행 (스케줄 이력이 없는 과거 날짜도 가능)\n\n```markdown\n# start (-s), end(-ㄷ) 파라미터를 dashed string 형태로 입력하면 UTC로 간주(아래의 start 날짜에 시간:분:초가 나와있지 않지만 날짜뒤에 00:00:00 가 붙음) \nairflow dags backfill -s 2023-04-19 -e 2023-04-21 dags_seoul_api_corona\n\n# 타임스탬프 형태로 직접 작성도 가능 (run_id에서 해당하는 날짜 구간을 찾아 실행)\nairflow dags backfill -s 2023-04-19T22:00:00+00:00 -e 2023-04-20T22:00:00+00:00 —reset-dagruns dags_seoul_api_corona\n```\n\n* 위의 예시에서, -s 2023-04-19 -e 2023-04-21 옵션이 있고 grid에서 task 수행 이력의 가장 최근 날짜가 2023-04-21 이라고 가정해보자.\n  * run_id가 scheduled__2023-04-21T22:00:00+00:00 일때\n  * 위의 날짜 구간 옵션에 있고 dag이 실행되지 않았던 04/20 22:00, 04/19 22:00 2개가 돌아가게 됨\n\n## cli - task clear\n\n* Clear 작업을 start / end 구간으로 일괄 재실행\n  * backfill의 경우 task가 수행이 됐건 안됐건 무조건 실행 (무조건 재실행)\n  * 하지만 clear 이미 실행됐던 task에 한해서 재실행됨\n  * Backfill과 달리 수행되지 않은 스케줄 구간은 실행할 수 없음\n\n  ```markdown\n  airflow tasks clear [-h] [-R] [-d] [-e END_DATE] [-X] [-x] [-f] [-r]\n                      [-s START_DATE] [-S SUBDIR] [-t TASK_REGEX] [-u] [-v] [-y]\n                      dag_id\n  ```\n\n  ```markdown\n  airflow tasks clear -s 2023-05-07 -e 2023-05-12 dags_seoul_api_corona\n  airflow tasks clear -s 2023-05-07T22:00:00+00:00 -e 2023-05-12T22:00:00+00:00 dags_seoul_api_corona\n  ```\n* Backfill되었던 DAG은 clear 불가함. reset-dagruns 옵션과 함께 다시 Backfill 수행해야 함\n\n## 정리\n\n| Comparision       | trigger               | backfill | clear    |\n|-------------------|-------------------|-------------------|-------------------|\n| 목적 | 특정 날짜로 DAG Trigger      | Start ~ end 구간의 스케줄 실행    | Start ~ end 구간 내 이미 수\n행되었던 스케줄 재실행   |\n| Run type          | -r 옵션으로 지정 가능. 없으면 Manual | Backfill    | 원래의 run_type    |\n| 기 수행된 run_id가 존재하는 경우  | 동일 run_id 가 존재하는 경우 에러 발생  | Run_type 을 Backfill 로 덮어쓰며\n재실행 | 재실행   |\n| 구간 지정| 불가              | 가능    | 가능    |\n| 과거 날짜 적용 가능 | 가능   | 가능    | 불가 |\n| task 선택 가능    | 불가     | 가능    | 가능    |\n\n* 공통점: CLI 명령으로 DAG 실행 가능\n \n# Triggerer \n\nScheduler, worker, webserver, triggerer containers 중 하나\n\n## Airflow Triggerer의 필요성\n\n* Airflow는 그 자체로 ETL 툴이라기보다 오케스트레이션 솔루션\n* 왜냐면 airflow와 연계되는 외부 솔루션에 작업 제출, 상태 확인, 완료 확인 등의 절차를 통해 관리\n* 예를 들어 airflow의 worker container가 python logic을 직접 처리하는게 아니라 python logic 을 python이 처리하도록 명령을 제출하고 로직 확인 및 결과 확인을 수행한다.\n\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  subgraph cluster0 {\n    rankdir=TB;\n    airflow [shape=circle];\n    PostgresqlDB [shape=box];\n    Hive [shape=box];\n    HDFS [shape=box];\n    Bigquerty [shape=box];\n    Python_func [shape=box];\n    Spark [shape=box];\n  \n  }\n  \n\n airflow-> PostgresqlDB[label=\"작업 실행\"];\n airflow-> Hive[label=\"\"];\n airflow-> HDFS[label=\"작업 실행\"];\n airflow-> Bigquerty[label=\"완료 확인\"];\n airflow-> Python_func[label=\"Python func\"];\n airflow-> Spark[label=\"상태 확인\"];\n\n}\n```\n\n* bigquerty: google에 있는 data 저장소 서비스\n* 외부 솔루션에 작업이 제출되어 완료될 때까지 Airflow의 Slot은 점유됨\n\n![](../../../../../images/airflow/triggerer.PNG)\n  * airflow 의 task가 들어왔을 때 task는 airflow worker의 slot을 차지하게 됨 \n  * 후에, 작업 대상에 작업을 제출하고 작업이 시작된다\n    * 작업대상: python function 또는 postgres, HDFS, Spark와 같은 외부 솔루션\n  * 작업이 진행되면 airflow는 작업이 완료 되었는지 작업상태를 polling 하면서 지속적으로 체크\n  * 작업 처리가 진행되는 동안 task는 차지했던 worker의 slot을 게속해서 차지한다.\n    * task가 많아지면 airflow의 slot이 부족할 수도 있는 상황이 있음\n    * 그럼 작업 처리동안 task는 slot 점유할 필요는 없지 않나라는 생각이 들 수 있다.\n    * triggerer가 이 문제를 해결\n* 작업을 제출하고 Task는 작업 처리가 시작될 때 Slot을 비우고 작업 상태 Polling 작업은 Triggerer 에게 위임\n  * 작업 처리 시작 전까지는 slot을 점유\n  * 작업상태를 끊임없이 polling하면서 확인해야하는데 triggerer를 이용하면 이 작업이 없어짐\n  * triggerer는 작업 처리 완료가 되는 event (작업 완료 callback message를 수신)를 받아 scheduluer container에게 message를 전달하고 scheduler는 task가 다시 비워진 slot을 점유하게 한다.\n\n![](../../../../../images/airflow/triggerer2.PNG)\n\n## Triggerer란\n\n* 워커를 대신하여 작업 상태 완료를 수신하고, 그때까지 Slot을 비워둘 수 있도록 해주는 Airflow의 서비스 \n  * airflow service: 스케줄러, 워커 같은 요소 중 하나\n* Python의 비동기 작업 라이브러리인 asyncio를 이용하여 작업상태 수신\n  * 사용 조건: Airflow 2.2 부터 & Python 3.7부터 사용 가능\n* 어떻게 사용하나?\n  * Deferrable Operator 이용하여 Task 생성\n  * 기본 Operator 중에서는 아래의 Sensor 종류만 사용 가능\n    - TimeSensorAsync\n    - DateTimeSensorAsync\n    - TimeDeltaSensorAsync\n  * 끝에 Async 가 붙은 오퍼레이터를 Deferrable Operator라 부르며 Triggerer에게 작업 완료 수신을 맡기는 오퍼레이터라는 의미\n\n## Triggerer 실습\n\n비교 실험\n\n* dags/dags_time_sensor.py (함수를 그냥 짬)\n* dags/dags_time_sensor_with_async.py (asyncio library 이용)\n\n## Dag Full Example\n\n* dag_time_sensor.py\n\n```markdown\nimport pendulum\nfrom airflow import DAG\nfrom airflow.sensors.date_time import DateTimeSensor\n\nwith DAG(\n    dag_id=\"dags_time_sensor\",\n    # 1시간 차이\n    start_date=pendulum.datetime(2023, 5, 1, 0, 0, 0), #5월1일 0시\n    end_date=pendulum.datetime(2023, 5, 1, 1, 0, 0), #5월1일 1시\n    schedule=\"*/10 * * * *\", #10 분마다 1시간안에 7번 돌게함\n    # 00분, 10분, 20분, 30분, 40분, 50분, 60분 총 7번\n    catchup=True, # catchup을 true이기 때문에 작업 상태bar 7개가 동시에 뜸\n    # airflow ui의 작업 pool을 보면 시작할 때 7개를 모두 차지 하도록 나옴\n) as dag:\n    # DateTimeSensor는 목표로 하는 시간까지 기다리는 sensor\n    sync_sensor = DateTimeSensor(\n        task_id=\"sync_sensor\",\n        # 현재 시간 + 5분\n        target_time=\"\"\"{{ macros.datetime.utcnow() + macros.timedelta(minutes=5) }}\"\"\",\n    )\n```\n\n* dags_time_sensor_with_async.py\n\n```markdown\nimport pendulum\nfrom airflow import DAG\nfrom airflow.sensors.date_time import DateTimeSensorAsync\n\nwith DAG(\n    dag_id=\"dags_time_sensor_with_async\",\n    start_date=pendulum.datetime(2023, 5, 1, 0, 0, 0),\n    end_date=pendulum.datetime(2023, 5, 1, 1, 0, 0),\n    schedule=\"*/10 * * * *\",\n    catchup=True,\n) as dag:\n    sync_sensor = DateTimeSensorAsync(\n        task_id=\"sync_sensor\",\n        target_time=\"\"\"{{ macros.datetime.utcnow() + macros.timedelta(minutes=5) }}\"\"\",\n    )\n\n```\n* airflow ui>> browse >> triggers 에서 trigger가 작업하는 대상 목록을 보여줌\n* trigger는 triggerer에게 작업을 맡길 event 또는 ticket이라고 생각하면 됨\n* triggerer id는 ticket id\n* defered status 는 보라색을 띄고 이 상태에서는 worker slot을 차지 않는 상태이다.\n\n## 요약\n\n* 끝에 Async가 붙은 오퍼레이터는 Deferrable Operator 라 부르며 Triggerer에 의해 Polling이 수행되는 오퍼레이터임을 의미\n* Deferrable Operator는 작업 제출 후 Slot을 차지하지 않으며 Polling 내역에 대해 Trigger 제출 후 deferred 상태가 됨.\n* Triggerer는 제출된 Trigger 내역을 보고 작업 완료시(조건 만족시) Worker에게 알려줘 작업이 마무리될 수 있도록 함.\n\n</div> \n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n\n\n</div>\n\n# Go to Blog Content List\n\n[Blog Content List](../../content_list.qmd)  \n[Engineering Content List](../../Engineering/guide_map/index.qmd)","srcMarkdownNoYaml":"\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n몰라도 되지만 알면 좋은 고급 기능들\n\n# Dataset을 이용한 Dag 트리거\n\n## Dataset의 필요성\n\nDAG간의 의존관계를 설정할 때 TriggerDagRun 과 ExternalTask Sensor를 이용한다. 그런데 이 2가지 방법을 이용하면 관리가 복잡해진다. 이를 해결하기 위해 고안된 기술이 dataset을 이용하여 DAG간의 의존관계를 설정하는 것이다. 어떤 상황에서 DAG 관리가 복잡해지고 dataset을 이용하면 어떻게 관리의 효율성이 증가하는지 다음의 cases를 통해 알아보자. \n\n* case 1\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  subgraph cluster0 {\n    rankdir=TB;\n    task_1 [shape=box];\n    task_3 [shape=box];\n    label= \"DAG A\";\n  }\n  task_1 -> task_3;\n  \n  subgraph cluster1 {\n    rankdir=TB;\n    task_k [shape=box];\n   label= \"DAG B\";\n }\n}\n```\n\n* 왼쪽 그림(요구사항) 에서, task3가 TriggerDagRun operator 으로 만들어진 task라 가정한다. task3가 DAG C의 task x를 trigger 한다고 가정한다. 이 경우를 변경하여 Task_1 뿐만 아니라 DAG_B의 task_k 에도 선행 의존관계가 걸려야 한다고 가정한다. 그렇게 하기위해서 External Task 센서를 하나 달아 개선해야 한다는 상황이 있다고 가정한다. 결국, task1과 task k가 성공적으로 끝나야 task3가 수행되고 이어서 DAG C의 task3가 돌아가야 하는 상황\n\n:::\n\n::: {.column width=\"50%\"}\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  subgraph cluster0 {\n    rankdir=TB;\n    task_1 [shape=box];\n    label= \"DAG A\";\n  }\n  \n  subgraph cluster1 {\n    rankdir=TB;\n    Sensor_task_1 [shape=box];\n    Sensor_task_2 [shape=box];\n    task_x [shape=box];\n   label= \"DAG C\";\n }\n subgraph cluster2 {\n    rankdir=TB;\n    task_k [shape=box];\n   label= \"DAG B\";\n }\n\n Sensor_task_1-> task_x;\n Sensor_task_2-> task_x;\n Sensor_task_1-> task_1[label=\"sensing\", style=\"dashed\"];\n Sensor_task_2-> task_k[label=\"sensing\", style=\"dashed\"];\n\n}\n```\n\n:::\n* 오른쪽 그림(해결책)은 왼쪽 그림을 개선하고자 만든 DAG의 의존관계, task3는 제거되어야 한다. 대신, DAG C를 external task sensor를 추가하도록 수정한다. sensor task1은 task1을 수행이 되고 sensor task2가 taskk의 수행이 성공적으로 완료가 된 후에 이 두 sensor task들이 taskx를 수행하도록 개선해야 한다.\n::::\n\n* 이렇게 DAG을 관리할 때 추가적인 DAG의 의존관계 설정을 해야할 때 추가적인 task들이 생기게 되는 요구사항을 해결해야할 때 오른쪽 그림과 같이 복잡한 workflow가 생기게 되는 case가 발생할 수 있다. 또 다른 case는 case2와 같다.\n\n* case 2\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  subgraph cluster0 {\n    rankdir=TB;\n    task_1 [shape=box];\n    task_3 [shape=box];\n    task_4 [shape=box];\n    label= \"DAG A\";\n  }\n  subgraph cluster {\n    rankdir=TB;\n    task_y [shape=box];    \n   label= \"DAG Y\";\n }\n  task_1 -> task_3;\n  task_y;\n  task_4;\n  \n}\n```\n\n* 왼쪽 그림(요구사항)에서, Task_1이 완료되면 또 다른 dag 을 trigger 되고 싶은데 DAG A 를 수정해야 할 경우이다. 다시 말해서, task1이 완료되면 또 다른 dag Y의 task y를 trigger하는 TriggerDagRun task4를 만들어 주고 싶도록 dag A를 수정해야하는 상황. 즉, task y는 task 1 과 task 4가 성공적으로 완료되어야만 수행되도록 설정해주고 싶은 상황. 뿐만 아니라 DAG A에 지속적인 변경사항 있을시 계속 workflow를 수정해야하는 문제점이 있다. \n:::\n\n::: {.column width=\"50%\"}\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  \n  subgraph cluster {\n    rankdir=TB;\n    task_1 [shape=box];\n    task_3 [shape=box];\n    task_4 [shape=box];\n   label= \"DAG A\";\n }\n  subgraph cluster {\n    rankdir=TB;\n    task_y [shape=box];    \n   label= \"DAG Y\";\n }\n    task_1 -> task_3;\n    task_1 -> task_4;\n    task_4 -> task_y;\n}\n```\n\n:::\n\n* 오른쪽 그림이 개선된 workflow\n\n::::\n\n* 이렇게, TriggerDagRun 오퍼레이터와 ExternalTask 센서를 많이 사용하다보면 아래 그림과 같이 연결관리에 많은 노력이 필요\n* 특히, ExternalTask은 execution_delta, 즉 data_interval_start 차이를 정확하게 연결해야 작동하도록 되어 있기 때문에 매우 강한 연결이 전제가 된다. 즉, parameter 중 일부가 안맞거나 틀리게 되면 제대로 sensing이 안됨\n\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  \n  subgraph cluster {\n    rankdir=TB;\n    DAG_A [shape=box];\n    DAG_B [shape=box];\n    DAG_C [shape=box];\n    DAG_D [shape=box];\n    DAG_E [shape=box];\n    DAG_G [shape=box];\n   label= \"DAG Flow\";\n }\n    DAG_A -> DAG_B;\n    DAG_A -> DAG_C;\n    DAG_D -> DAG_C[style=\"dashed\"];\n    DAG_D -> DAG_E[style=\"dashed\"];\n    DAG_E -> DAG_G[style=\"dashed\"];\n}\n```\n\n* 위와 같은 복잡하고 rigid한 workflow를 개선한 것이 dataset을 이용한 방식이다.   \n* 큐시스템과 같이 Job 수행이 완료된 Task 는 Push 하고 센싱이 필요한 task 은 큐 시스템을 구독하는 방식을 쓴다면 더 유연한 workflow가 생성될 수 있다. (약한 연결)\n\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  \n  subgraph cluster {\n    rankdir=TB;\n    DAG_A [shape=box];\n    DAG_B [shape=box];\n    DAG_C [shape=box];\n    DAG_D [shape=box];\n    DAG_E [shape=box];\n    DAG_G [shape=box];\n    Queue [shape=cylinder];\n   label= \"DAG Flow\";\n }\n    DAG_A -> Queue;\n    DAG_D -> Queue;\n    Queue -> DAG_B;\n    Queue -> DAG_C;\n    Queue -> DAG_E;\n    \n}\n```\n\n* DAG A가 작업이 끝났다라는 메시지를 큐에 넣고 DAG A가 완료되면 자동으로 DAG B가 triggering됨\n* 나중에 DAG F가 새로 생겨 DAG A가 끝났을 때 수행되도록 triggering 되어야 한다면 큐에 있는 메시지를 인식하고 돌아가도록 설정하면 된다. \n* 그러므로, DAG A를 구독하는 후행 DAG들이 아무리 많더라도 큐에 저장된 메시지만 바라보게 만들면 된다. DAG A는 수정할 필요가 없게 된다. \n* Produce / Consume 구조를 이용하여 Task 완료를 알리기 위해 특정 키 값으로 Produce 하고 해당 키를 Consume 하는 DAG 을 트리거 스케줄 할 수 있는 기능\n  * produce: queue에 메세지를 넣는 행위. 다른 솔루션에서는 produce를 publish라고도 부름  \n  * consume: queue로부터 메세지를 읽거나 꺼내는 행위. 보통은 이 consume다른 솔루션에서는 subscribe라고도 많이 부름. airflow에서는 subscribe가 consume으로 사용된다.\n  * 선행 Dag이 produce하고 후행 Dag이 consume한다.\n  * 실제 큐가 있는 것은 아니고 DB 에 Produce / Consume 내역을 기록\n\n## DAG Full Example\n\n* dags_dataset_producer_1.py\n\n```markdown\nfrom airflow import Dataset #airflow의 dataset이라는 library 호출\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_1 = Dataset(\"dags_dataset_producer_1\")\n# Dataset의 인수값으로 string을 주는데 DAG이 큐에 메세지를 넣을 때 메세지가  dictionary형태로 저장이 되는데 그 키값을 input으로 넣게 된다. \n\n\nwith DAG(\n        dag_id='dags_dataset_producer_1',\n        schedule='0 7 * * *',\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        outlets=[dataset_dags_dataset_producer_1], # outlets 인수는 baseOperator로부터 상속된다. 모든 operator는 outlets인수를 가지고 있다. outlets에 다가는 리스트 구조로 선언된다. queue에다가 publish되는 메세지는 dag안에 있는 task('bash_task')가 dataset_dags_dataset_producer_2 이름으로  publish된다.  dataset_dags_dataset_producer_2가 들어가 있다. \n        bash_command='echo \"producer_1 수행 완료\"'\n    )\n```\n\n* dags_dataset_producer_2.py\n\n```markdown\nfrom airflow import Dataset\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_2 = Dataset(\"dags_dataset_producer_2\")\n\nwith DAG(\n        dag_id='dags_dataset_producer_2',\n        schedule='0 7 * * *',\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        outlets=[dataset_dags_dataset_producer_2], \n        bash_command='echo \"producer_2 수행 완료\"'\n    )\n```\n\n\n* dags_dataset_consumer_1.py\n\n```markdown\nfrom airflow import Dataset \nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_1 = Dataset(\"dags_dataset_producer_1\") # dags_dataset_producer_1.py의 producer1의 키값 'dags_dataset_producer_1'을 넣어 호출하고 있음\n\nwith DAG(\n        dag_id='dags_dataset_consumer_1',\n        schedule=[dataset_dags_dataset_producer_1], # 스케쥴이 다른 형식으로 들어가 있는데 dags_dataset_producer_1.py의 producer1 (정확히 말하면 task)을 구독하겠다는 뜻. 스케쥴은 없고 publish하고 있는 dag 수행이 끝나면 실행하겠다는 뜻.\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        bash_command='echo {{ ti.run_id }} && echo \"producer_1 이 완료되면 수행\"'\n    )\n```\n\n* dags_dataset_consumer_2.py\n\n```markdown\nfrom airflow import Dataset\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\n\ndataset_dags_dataset_producer_1 = Dataset(\"dags_dataset_producer_1\")\ndataset_dags_dataset_producer_2 = Dataset(\"dags_dataset_producer_2\")\n# producer1,2 2개를 동시에 구독하고있음\n\nwith DAG(\n        dag_id='dags_dataset_consumer_2',\n        schedule=[dataset_dags_dataset_producer_1, dataset_dags_dataset_producer_2],\n        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),\n        catchup=False\n) as dag:\n    bash_task = BashOperator(\n        task_id='bash_task',\n        bash_command='echo {{ ti.run_id }} && echo \"producer_1 와 producer_2 완료되면 수행\"'\n    )\n```\n\nairflow web service에서 dag 상태를 확인할 때 초록색 긴막대기를 누르면 Dataset Events에서 구독 depndencies를 확인할 수 있다. Run ID도 특이하게 나오게 되는 것을 확인 할 수 있다. \n\n## Dataset Workflow \n\nAirflow Web Service의 dataset 메뉴를 클릭하면 publish와 subscribe의 의존관계를 graph로 볼 수 있다. \n\n## DAG Dependencies\n\nAirflow Web Service의 Browse 메뉴의 Dag Dependencies를 클릭하면 Dag간의 의존관계를 볼 수 있다. \n\n## 정리\n\n* Dataset 은 Publish(produce)/subscribe(consume) 구조로 DAG 간 의존관계를 주는 방법\n* Unique 한 String 형태의 Key 값을 부여하여 Dataset Publish\n* Dataset 을 Consume 하는 DAG 은 스케줄을 별도로 주지 않고 리스트 형태로 구독할 Dataset 요소들을 명시\n* Dataset에 의해 시작된 DAG 의 Run_id 는 dataset_triggered__{trigger 된 시간 } 으로 표현됨\n* Airflow 화면 메뉴 중 Datasets 메뉴를 통해 별도로 Dataset 현황 모니터링 가능\n* N개의 Dataset 을 구독할 때 스케줄링 시점은 ?\n    * 마지막 수행 이후 N 개의 Dataset 이 모두 재 업데이트되는 시점\n    ![Trigger using dataset - N개의 dataset 구독](../../../../../images/airflow/dataset_trigger.PNG)\n\n# DAG의 default_args 파라미터\n\n## DAG 의 default_args 파라미터\n\n* 목적: DAG 하위 모든 오퍼레이터에 공통 적용될 파라미터를 입력\n* 어떤 파라미터들이 적용 가능할까?\n    * 오퍼레이터에 공통 적용할 수 있는 파라미터들\n    * BaseOperator 클래스 생성자가 가지고 있는 파라미터\n* [airflow doc](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/baseoperator.html#BaseOperator)\n* Code \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n```markdown\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n) as dag:\n    \n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='sleep 10m',\n        sla: timedelta(seconds=70), #SLA 설정\n        email: 'sdf@sdfsfd.com'\n    )\n    \n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='sleep 2m',\n        sla: timedelta(seconds=70), #SLA 설정\n        email: 'sdf@sdfsfd.com'\n    )\n\n   task_1 >> task_2\n\n```\n:::\n\n::: {.column width=\"50%\"}\n\n```markdown\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': 'sdf@sdfsfd.com'\n    }\n) as dag:\n    \n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='sleep 10m'\n    )\n    \n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='sleep 2m'\n    )\n\n   task_1 >> task_2\n\n```\n\n:::\n\n::::\n\n## BaseOperator 파라미터 vs Dag 파라미터\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n### DAG Parameter\n\n* DAG 파라미터는 DAG 단위로 적용될 파라미터\n* 개별 오퍼레이터에 적용되지 않음\n* DAG 파라미터는 default_args 에 전달하면 안됨\n\n:::\n\n::: {.column width=\"60%\"}\n\n### BaseOperator Parameter\n\n* Base 오퍼레이터 파라미터는 개별 Task 단위로 적용될 파라미터.\n* Task 마다 선언해줄 수 있지만 DAG 하위 모든 오퍼레이터에 적용 필요시 default_args 를 통해 전달 가능\n:::\n\n::::\n\n* default_args 에 전달된 파라미터보다 개별 오퍼레이터에 선언된 파라미터가 우선순위를 가짐\n\n```markdown\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': 'sdf@sdfsfd.com'\n    }\n) as dag:\n    \n    task_1 = BashOperator(\n        task_id='task_1',\n        bash_command='sleep 10m'\n    )\n    \n    task_2 = BashOperator(\n        task_id='task_2',\n        bash_command='sleep 2m',\n        sla=timedelta(minutes=1)\n    )\n\n   task_1 >> task_2\n\n```\n\n# Task 실패시 Email 발송하기\n\n## Email 발송 위한 파라미터 확인\n\n* [airflow doc](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/baseoperator.html#BaseOperator)\n* email 파라미터와 email_on_failure 파라미터를 이용\n* email 파라미터만 입력하면 email_on_failure 파라미터는 True 로 자동설정됨\n\n## Email 발송 대상 등록\n\n```markdown\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.decorators import task\nfrom airflow.exceptions import AirflowException\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_email_on_failure',\n    start_date=pendulum.datetime(2023,5,1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule='0 1 * * *',\n    dagrun_timeout=timedelta(minutes=2),\n    default_args={\n        'email_on_failure': True,\n        'email': email_lst\n    }\n) as dag:\n    @task(task_id='python_fail')\n    def python_task_func():\n        raise AirflowException('에러 발생')\n    python_task_func()\n\n    bash_fail = BashOperator(\n        task_id='bash_fail',\n        bash_command='exit 1',\n    )\n\n    bash_success = BashOperator(\n        task_id='bash_success',\n        bash_command='exit 0',\n    )\n\n```\n\n* Email 받을 대상이 1 명이면 string 형식으로 , 2 명 이상이면 list 로 전달\n* 그런데 협업 환경에서 DAG 담당자는 수시로 바뀔 수 있고 인원도 수시로 바뀔 수 있는데 그때마다 DAG 을 뒤져서 email 리스트를 수정해야 할까 ?\n* 이럴때 Variable 을 이용하자\n\n# SLA로 task 수행 감시 & Email 발송하기\n\n## SLA파라미터 이해\n\n* 개념: 오퍼레이터 수행시 정해놓은 시간을 초과하였는지를 판단할 수 있도록 설정해놓은 시간 값 파이썬의 timedelta 로 정의\n* 동작: 설정해놓은 SLA 를 초과하여 오퍼레이터 running 시 SLA Miss 발생 , Airflow UI 화면에서 Miss 건만 조회 가능 + email 발송 가능\n    * SLA Miss 발생시 task 가 실패되는 것은 아니며 단순 Miss 대상에 기록만 DB에 남기게 됨\n    * SLA 는 DAG 파라미터가 아니며 BaseOperator 의 파라미터\n* [airflow docs- def __init__](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/baseoperator.html#BaseOperator)\n  ```markdown\n   def __init__(\n        self,\n        task_id: str,\n        owner: str = DEFAULT_OWNER,\n        email: str | Iterable[str] | None = None,\n        email_on_retry: bool = conf.getboolean(\"email\", \"default_email_on_retry\", fallback=True),\n        email_on_failure: bool = conf.getboolean(\"email\", \"default_email_on_failure\", fallback=True),\n        retries: int | None = DEFAULT_RETRIES,\n        retry_delay: timedelta | float = DEFAULT_RETRY_DELAY,\n        retry_exponential_backoff: bool = False,\n        max_retry_delay: timedelta | float | None = None,\n        start_date: datetime | None = None,\n        end_date: datetime | None = None,\n        depends_on_past: bool = False,\n        ignore_first_depends_on_past: bool = DEFAULT_IGNORE_FIRST_DEPENDS_ON_PAST,\n        wait_for_past_depends_before_skipping: bool = DEFAULT_WAIT_FOR_PAST_DEPENDS_BEFORE_SKIPPING,\n        wait_for_downstream: bool = False,\n        dag: DAG | None = None,\n        params: collections.abc.MutableMapping | None = None,\n        default_args: dict | None = None,\n        priority_weight: int = DEFAULT_PRIORITY_WEIGHT,\n        weight_rule: str = DEFAULT_WEIGHT_RULE,\n        queue: str = DEFAULT_QUEUE,\n        pool: str | None = None,\n        pool_slots: int = DEFAULT_POOL_SLOTS,\n        sla: timedelta | None = None, # sla 변수\n        execution_timeout: timedelta | None = DEFAULT_TASK_EXECUTION_TIMEOUT,\n        on_execute_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n        on_failure_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n        on_success_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n        on_retry_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,\n        pre_execute: TaskPreExecuteHook | None = None,\n        post_execute: TaskPostExecuteHook | None = None,\n        trigger_rule: str = DEFAULT_TRIGGER_RULE,\n        resources: dict[str, Any] | None = None,\n        run_as_user: str | None = None,\n        task_concurrency: int | None = None,\n        max_active_tis_per_dag: int | None = None,\n        max_active_tis_per_dagrun: int | None = None,\n        executor_config: dict | None = None,\n        do_xcom_push: bool = True,\n        inlets: Any | None = None,\n        outlets: Any | None = None,\n        task_group: TaskGroup | None = None,\n        doc: str | None = None,\n        doc_md: str | None = None,\n        doc_json: str | None = None,\n        doc_yaml: str | None = None,\n        doc_rst: str | None = None,\n        **kwargs,\n    ):\n  ```\n  * sla: timedelta | None = None, # sla 변수\n    * timedelta보다 오랜 시간동안 task가 더 오래 수행되면 SLA Miss 기록만 남기고 실패처리는 안하게 됨\n    * default_arg에 넣을 수 있기 때문에 모든 task, 즉 모든 operator에 공통 적용할 수 있다.\n* [sla_miss_callback: DAG에 있는 SLA 관련 파라미터로 SLA Miss시 수행할 함수 지정 가능-Source code for airflow.models.dag](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/dag.html)\n    * sla_miss_callback 파라미터\n      * sla miss시 수행할 함수명을 입력 받음\n      * dag에 있는 파라미터이기 때문에 default_arg에 넣을 수 없음\n    * DAG의 파라미터임 (BaseOperator 의 파라미터가 아니라는 것에 유의)\n\n## SLA 제약사항\n\n* 1번째 제약사항: 각 Task 의 SLA timeout 카운트 시작은 DAG 의 시작시간 기준임\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n### 사람이 생각하는 SLA timeout 카운트 방식\n\n![](../../../../../images/airflow/SLA_timeout_count1.PNG)\n\n* 처음 사용하는 사용자들은 다음과 같이 생각할 수 있다.\n  * task1은 task1의 sla구간보다 수행시간이 짧게 완료됐기 때문에 성공 처리\n  * task2는 sla 구간보다 더 길기 때문에 miss 처리됨\n  * task3는 sla 구간보다 짧기 때문에 성공 처리\n* airflow는 위와 같이 동작하지 않음 옆의 그림과 같이 동작함\n:::\n\n::: {.column width=\"60%\"}\n\n### 실제 Airflow 의 SLA timeout 카운트 방식\n\n![](../../../../../images/airflow/SLA_timeout_count2.PNG)\n\n* 실제 airflow에서는 수행 시간을 dag의 최초 실행시점 부터 카운트 하기 시작함\n* task1의 수행완료 상태 및 시점과 상관없이 task2와 task3의 수행 시간은 task1 수행 시점 부터 카운트 되기 시작한다.\n* 그러므로 task2, task3는 수행된적이 없음에도 miss sla 상태로 처리됨\n:::\n\n::::\n\n* 2번째 제약사항: DAG의 시작시간 기준 $\\rightarrow$ 명목적인 시작 시간 (data_interval_start)\n  * 위의 SLA timeout 카운트 시작은 dag의 시작시간 기준이라는 제약 사항과 연결되는 제약사항\n  * 만약 Pool 이 부족하거나 스케줄러의 부하로 인해 DAG 이 스케줄보다 늦게 시작했다면 늦게 시작한만큼 이미 SLA 시간 카운트는 진행되어 시간이 소요되고 있음\n    * 실질적 DAG 시작 시간을 기준으로 카운트하지 않음\n  * 즉, 특정 task 수행에 computing 부하가 걸리면 후차적인 task들은 모두 miss 처리됨\n  * 대규모 프로젝트에서 흔히 발생하는 문제로 dag이 밀리는 현상이 자주 관찰되므로 sla는 자주쓰이는 방식은 아님.\n* 3번째 제약사항: 첫 스케줄에서는 SLA Miss 가 기록되지 않음 (두 번째 스케줄부터 기록)\n* 4번째 제약사항: sla miss처리가 분명히 발생됐는데 가끔 기록이 안될때가 있고 email역시 발송이 되지 않을때가 있음. 엄격이 관리가 되지 않는 기능\n\n## Full Dag Example\n\n```markdown\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': email_lst\n    }\n) as dag:\n    \n    # 30초 sleep\n    task_slp_30s_sla_70s = BashOperator( \n        task_id='task_slp_30s_sla_70s',\n        bash_command='sleep 30'\n    )\n    # 60초 sleep\n    task_slp_60_sla_70s = BashOperator(\n        task_id='task_slp_60_sla_70s',\n        bash_command='sleep 60'\n    )\n\n    # 10초 sleep\n    task_slp_10s_sla_70s = BashOperator( \n        task_id='task_slp_10s_sla_70s',\n        bash_command='sleep 10'\n    )\n\n    # 10초 sleep\n    # sla의 timedelta를 명시적으로 30초 선언\n    # default argument보다 명시적 선언이 우선 순위가 더 높음\n    # 그래서, 처음 3개의 task는 timedelta가 70초로 설정됐고 4번째 task는 30초로 설정됨 \n    task_slp_10s_sla_30s = BashOperator( \n        task_id='task_slp_10s_sla_30s',\n        bash_command='sleep 10',\n        sla=timedelta(seconds=30)\n    )\n\n    task_slp_30s_sla_70s >> task_slp_60_sla_70s >> task_slp_10s_sla_70s >> task_slp_10s_sla_30s\n\n```\n\n* 위 코드를 보면,\n  * 1번째 task는 30초 sleep이 70초 sla timedelta보다 짧기 때문에 성공 처리\n  * 2번째 task는 30,40초 돌다가 miss 처리됨\n  * 3,4 번째 task는 수행되기도 전에 miss처리됨\n  * 4번째 timedelta가장 짧기 때문에 가장 먼저 miss 처리 됨\n* Airflow web service에서 SLA Miss 현황 조회하기 \n  * airflow web service >> browse \n\n## SLA Miss 시 email 발송하기\n\n```markdown\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import timedelta\nimport pendulum\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_sla_email_example',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    schedule='*/10 * * * *',\n    catchup=False,\n    default_args={\n        'sla': timedelta(seconds=70),\n        'email': email_lst\n    }\n) as dag:\n    \n    task_slp_30s_sla_70s = BashOperator(\n        task_id='task_slp_30s_sla_70s',\n        bash_command='sleep 30'\n    )\n    \n    task_slp_60_sla_70s = BashOperator(\n        task_id='task_slp_60_sla_70s',\n        bash_command='sleep 60'\n    )\n\n    task_slp_10s_sla_70s = BashOperator(\n        task_id='task_slp_10s_sla_70s',\n        bash_command='sleep 10'\n    )\n\n    task_slp_10s_sla_30s = BashOperator(\n        task_id='task_slp_10s_sla_30s',\n        bash_command='sleep 10',\n        sla=timedelta(seconds=30)\n    )\n\n    task_slp_30s_sla_70s >> task_slp_60_sla_70s >> task_slp_10s_sla_70s >> task_slp_10s_sla_30s\n\n```\n\n# timeout 설정하기\n\n## Timeout 파라미터 이해\n\n* Task수준의 timeout 과 DAG 수준의 timeout 이 존재\n* execution_timeout: [Task수준의 timeout 파라미터: baseOperator 명세에서 __init__ 생성자에서 sla 파라미터 아래에 있음](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/baseoperator.html#BaseOperator)\n  * execution_timeout: timedelta | None = DEFAULT_TASK_EXECUTION_TIMEOUT,\n  * timedelta보다 오래 task가 수행됐을 때 task는 fail 처리됨\n  * task가 fialure 됐을 때 __init__ 생성자 안의 다른 파라미터인 ` email_on_failure: bool = conf.getboolean(\"email\", \"default_email_on_failure\", fallback=True)` 과 `email: str | Iterable[str] | None = None,`  에 의해 지정 대상에게 email을 보낼 수 있다.\n  * 요약하면, timedelta, email_on_failure, email 이렇게 3개의 파라미터를 이용하여 timedelta 를 초과하는 task 수행시 fail 처리를 하여 이메일을 보낼 수 있다.\n* dagrun_timeout: [DAG수준에서도 timeout 파라미터를 걸 수 도 있음](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/dag.html) : dagrun_timeout\n  * [docs]class DAG(LoggingMixin): 의 __init__ 생성자\n  ```markdown\n\n      def __init__(\n        self,\n        dag_id: str,\n        description: str | None = None,\n        schedule: ScheduleArg = NOTSET,\n        schedule_interval: ScheduleIntervalArg = NOTSET,\n        timetable: Timetable | None = None,\n        start_date: datetime | None = None,\n        end_date: datetime | None = None,\n        full_filepath: str | None = None,\n        template_searchpath: str | Iterable[str] | None = None,\n        template_undefined: type[jinja2.StrictUndefined] = jinja2.StrictUndefined,\n        user_defined_macros: dict | None = None,\n        user_defined_filters: dict | None = None,\n        default_args: dict | None = None,\n        concurrency: int | None = None,\n        max_active_tasks: int = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\"),\n        max_active_runs: int = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\"),\n        dagrun_timeout: timedelta | None = None,\n        sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None,\n        default_view: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),\n        orientation: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),\n        catchup: bool = airflow_conf.getboolean(\"scheduler\", \"catchup_by_default\"),\n        on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,\n        on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,\n        doc_md: str | None = None,\n        params: collections.abc.MutableMapping | None = None,\n        access_control: dict | None = None,\n        is_paused_upon_creation: bool | None = None,\n        jinja_environment_kwargs: dict | None = None,\n        render_template_as_native_obj: bool = False,\n        tags: list[str] | None = None,\n        owner_links: dict[str, str] | None = None,\n        auto_register: bool = True,\n    ):\n\n    ```\n    * 다음 파라미터를 이용하여 dag을 관리할 수 있다.\n      * schedule: ScheduleArg = NOTSET, \n      * start_date: datetime | None = None, \n      * default_args: dict | None = None, \n      * dagrun_timeout: timedelta | None = None,\n* task1 >> task2 >> task3 예시\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n### execution_timeout\n\n* dag은 timeout이 안됐지만 task가 timeout이 되는 case\n\n![](../../../../../images/airflow/execution_timeout1.PNG)\n\n* task2의 실행시간이 task2의 execution_timeout보다 길게 수행되어 fail되었고 task3은 task2로 인해 upstream failed 이 발생\n* 하지만, task1,2,3 의 수행시간의 총합이 dagrun_timeout 보다 짧기 때문에 dag 자체는 실패처리 안됨\n:::\n\n::: {.column width=\"50%\"}\n\n### datgrun_timeout\n\n* task는 성공 처리 됐지만 dag이 오랜시간동안 돌아 timeout되어 실패처리되는 case\n\n![](../../../../../images/airflow/dagrun_timeout.PNG)\n\n* 각 task들이 execution_timeout보다 짧아 모두 정상 처리 되었지만 task1,2,3의 총 실행시간이 dagrun_timeout보다 길어 dag자체는 failure이 된 상황\n* dag이 failure되는 시점에서의 task는 skipped 상태로 처리된다.\n:::\n\n::::\n\n## Dag Full Example\n\n* case1: tasks는 실패, dargrun 정상 \n\n```markdown\n# Package Import\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.models import Variable\n\n# email 수신자 리스트\nemail_str = Variable.get(\"email_target\") \nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_timeout_example_1',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule=None,\n    dagrun_timeout=timedelta(minutes=1),\n    default_args={\n        #각 task들이 20초안에 끝나야 성공 처리됨\n        'execution_timeout': timedelta(seconds=20), \n        'email_on_failure': True,\n        'email': email_lst\n    }\n) as dag:\n    # execution_timeout보다 길기 때문에 task는 실패 처리됨\n    bash_sleep_30 = BashOperator(\n        task_id='bash_sleep_30', \n        bash_command='sleep 30',\n    )\n    # execution_timeout보다 짧기 때문에 task는 성공 처리됨\n    bash_sleep_10 = BashOperator(\n        trigger_rule='all_done', # upstream fail에도 task 실행시키기 위해 triggering\n        task_id='bash_sleep_10',\n        bash_command='sleep 10',\n    )\n    \n    # upstream failure 발생해도 trigger_rule을 all_done을 줬기 때문에 bash_sleep_10 은 실행됨\n    # dagrun_timeout을 1분으로 설정했기 때문에 task run의 총합이 40초이기 때문에 dagrun은 정상 처리됨\n    bash_sleep_30 >> bash_sleep_10 \n    \n\n```\n\n* case2: tasks는 정상, dargrun 실패\n\n```markdown\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nimport pendulum\nfrom datetime import timedelta\nfrom airflow.models import Variable\n\nemail_str = Variable.get(\"email_target\")\nemail_lst = [email.strip() for email in email_str.split(',')]\n\nwith DAG(\n    dag_id='dags_timeout_example_2',\n    start_date=pendulum.datetime(2023, 5, 1, tz='Asia/Seoul'),\n    catchup=False,\n    schedule=None,\n    dagrun_timeout=timedelta(minutes=1),\n    default_args={\n        'execution_timeout': timedelta(seconds=40),\n        'email_on_failure': True,\n        'email': email_lst\n    }\n) as dag:\n    bash_sleep_35 = BashOperator(\n        task_id='bash_sleep_35',\n        bash_command='sleep 35',\n    )\n\n    bash_sleep_36 = BashOperator(\n        trigger_rule='all_done',\n        task_id='bash_sleep_36',\n        bash_command='sleep 36',\n    )\n\n    bash_go = BashOperator(\n        task_id='bash_go',\n        bash_command='exit 0',\n    )\n# 모든 task들이 40초안에 실행완료가 되기때문에 성공 처리됨\n# dagrun은 1분으로 설정됐기 때문에 2번째 task 가 실행될 때 dag이 fail되고\n# 2번째task는 skipped 처리가 됨, 이 task에 대해서는 email도 안감 \n# 3번째 task는  no status 처리됨 , 이 task에 대해서는 email도 안감\n    bash_sleep_35 >> bash_sleep_36 >> bash_go\n    \n```\n\n* dagrun_timeout의 한계점\n  * 모든 task들이 40초안에 실행완료가 되기때문에 성공 처리됨\n  * dagrun은 1분으로 설정됐기 때문에 2번째 task 가 실행될 때 dag이 fail되고\n  * 2번째task는 skipped 처리가 됨, 이 task에 대해서는 email도 안감 \n  * 3번째 task는  no status 처리됨 , 이 task에 대해서는 email도 안감\n\n## 정리\n\n| Comparision       | sla               | execution_timeout | dagrun_timeout    |\n|-------------------|-------------------|-------------------|-------------------|\n| 파라미터 정의 위치 | BaseOperator      | BaseOperator    | DAG    |\n| 적용 수준          | Task              | task    | DAG    |\n| 기능    | 지정한 시간 초과시 Miss 기록  | 지정한 시간 초과시 task fail 처리 | 지정한 시간 초과시 DAG fail 처리    |\n| email 발송 가능 여부| O                | O    | X    |\n| timeout 발생시 후행 task 상태 | 상관없이 지속 | Upstream_failed    | Skipped (current) /No status (not run)    |\n| 스케쥴 필요       | O                  | X    | X    |\n\n* sla, execution_timeout에는 email 발송 paraemeter가 있지만 execution_timeout에는 없다.\n* dagrun timeout이 fail 됐을 때 반드시 email 발송 하고싶으면 dag의 파라미터 중 on_failure_callback 에 dag이 실패됐을 때 이메일을 전송하는 함수를 만들어 그 함수명을 할당해준다.\n* upstream_failed 상태는 execution_timeout만이 갖는 특징이 아니라 airflow의 디폴트 설정이다. 상위 task들이 fail되면 후행 task들은 upstream_failed로 남는다.\n* 공통점: 파이썬의 timedelta 함수로 timeout 기준 시간 정의  \n\n# Airflow CLI 사용하기\n\n* Airflow가 설치되어 있는 서버 또는 환경에서 shell 명령을 이용하여 Airflow를 컨트롤 할 수 있도록 많은 기능들을 제공하고 있음\n* [airflow cli doc](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html)\n  * 대표적인 content\n    * dags: dag을 다룰 수 잇는 커맨드  \n      * `airflow dags [-h] COMMAND ...`\n      * backfill: airflow web ui 의 grid 기능을 보면 dag이 돌았던 이력을 볼 수 있는데 grid 상 가장 과거 날짜 뿐만 아니라 그 이전의 과거 날짜 또한 command의 옵션으로 모두 돌릴 수 있음\n      ```markdown\n        airflow dags backfill [-h] [-c CONF] [--continue-on-failures]\n                      [--delay-on-limit DELAY_ON_LIMIT] [--disable-retry] [-x]\n                      [-n] [-e END_DATE] [-i] [-I] [-l] [-m] [--pool POOL]\n                      [--rerun-failed-tasks] [--reset-dagruns] [-B]\n                      [-s START_DATE] [-S SUBDIR] [-t TASK_REGEX]\n                      [--treat-dag-as-regex] [-v] [-y]\n                      dag_id\n      ```\n      * delete: Delete all DB records related to the specified DAG\n      ```markdown\n      airflow dags delete [-h] [-v] [-y] dag_id\n      ```  \n      * details: Get DAG details given a DAG id\n      ```markdown\n      airflow dags details [-h] [-o table, json, yaml, plain] [-v] dag_id\n      * list: List all the DAGs\n      ```markdown\n      airflow dags list [-h] [-o table, json, yaml, plain] [-S SUBDIR] [-v]\n      ```\n    * variables: variables을 관리하는 command\n      * `airflow variables [-h] COMMAND ...`\n      * delete: `airflow variables delete [-h] [-v] key`\n        * 등록되있는 variables을 key값을 입력하여 삭제\n      * export: `airflow variables export [-h] [-v] file`\n        * 등록되있는 variables을 json file로 추출\n      * get: `airflow variables get [-h] [-d VAL] [-j] [-v] key`\n        * 특정 variables의 key값을 주어 values을 꺼내옴\n      * import: `airflow variables import [-h] [-v] file`\n        * json file에 variables을 작성해놓고 list를 한번에 입력한다.\n      * list: `airflow variables list [-h] [-o table, json, yaml, plain] [-v]`\n      * set: `airflow variables set [-h] [-j] [-v] key VALUE`\n\n* Cli를 잘 쓰면 좋은 이유\n    * 일괄작업: Airflow UI에서 할 수 없는 일괄 작업 방식을 제공\n      * ex: connection 일괄 등록. 만약 airflow ui로 등록하면 일일히 등록해야한다.\n      * 물론 CLI를 이용하는 방법 외에 metaDB table에 직접 insert하는 방법도 있음\n    * 특수기능: Airflow UI에서는 할 수 없는 기능을 제공\n      * ex: backfill 은 airflow ui 를 통해서는 실행 불가\n    * 자동화: Airflow UI에서 직접 눈으로 보고 클릭하는 방식이 아닌 프로그래밍에 의한 제어가 가능해짐\n      * CLI 커맨드는 shell 명령어로 이루저있기 때문에 shell script를 작성하여 자동화 할 수 있다.\n\n## cli - dag trigger\n\n* dag trigger: airflow ui 상에서 manual 로 dag trigger 하거나 run_id 를 직접 넣어 trigger 할 수 있는 기능으로 CLI로 실행시킬 수 있다.\n* CLI 명령은 WSL2에서 하는게 아니라 docker container안에서 해야함\n\n  ```markdown\n  airflow dags trigger [-h] [-c CONF] [-e EXEC_DATE] \n                       [--no-replace-microseconds] [-o table, json, yaml, plain] [-r RUN_ID] [-S SUBDIR] [-v]\n                     dag_id\n  ```\n  * EXEC_DATE: execution_date parameter는 모두 data_interval_start 기준이며 String 형식으로 입력하면 기본 UTC로 계산됨\n  * RUN_ID를 입력하면 기본적으로 manual__로 시작하며 run_id를 직접 입력도 가능\n  * 예시: `#> airflow dags trigger dags_seoul_api_corona`\n* Full Example\n```markdown\n# docker container list 확인\nsudo docker ps \n\n# webserver container 선택 (어떤 것을 골라도 상관없음)\n\n# airflow container 들어가기: sudo docker exec -it [docker_container_id] bash\nsudo docker exec -it 8b755cb5aa70 bash \n\n# trigger 명령어 실행\nairflow dags trigger dags_base_branch_operator\n\n\n```\n## 결과\n\n![](../../../../../images/airflow/cli_example_1.PNG)\n\n* dag_run_id: manual__2023-07-22T05:05:59+00:00.\n* run type: manual__\n  * ClI 로 돌렸기 때문에 manual_이 붙어있음\n  * run types: schedule, manual, backfill 등이 있음\n\n## cli - dag backfill\n\n* 입력 스케줄 구간에 대해 일괄 (재)실행 (스케줄 이력이 없는 과거 날짜도 가능)\n\n```markdown\n# start (-s), end(-ㄷ) 파라미터를 dashed string 형태로 입력하면 UTC로 간주(아래의 start 날짜에 시간:분:초가 나와있지 않지만 날짜뒤에 00:00:00 가 붙음) \nairflow dags backfill -s 2023-04-19 -e 2023-04-21 dags_seoul_api_corona\n\n# 타임스탬프 형태로 직접 작성도 가능 (run_id에서 해당하는 날짜 구간을 찾아 실행)\nairflow dags backfill -s 2023-04-19T22:00:00+00:00 -e 2023-04-20T22:00:00+00:00 —reset-dagruns dags_seoul_api_corona\n```\n\n* 위의 예시에서, -s 2023-04-19 -e 2023-04-21 옵션이 있고 grid에서 task 수행 이력의 가장 최근 날짜가 2023-04-21 이라고 가정해보자.\n  * run_id가 scheduled__2023-04-21T22:00:00+00:00 일때\n  * 위의 날짜 구간 옵션에 있고 dag이 실행되지 않았던 04/20 22:00, 04/19 22:00 2개가 돌아가게 됨\n\n## cli - task clear\n\n* Clear 작업을 start / end 구간으로 일괄 재실행\n  * backfill의 경우 task가 수행이 됐건 안됐건 무조건 실행 (무조건 재실행)\n  * 하지만 clear 이미 실행됐던 task에 한해서 재실행됨\n  * Backfill과 달리 수행되지 않은 스케줄 구간은 실행할 수 없음\n\n  ```markdown\n  airflow tasks clear [-h] [-R] [-d] [-e END_DATE] [-X] [-x] [-f] [-r]\n                      [-s START_DATE] [-S SUBDIR] [-t TASK_REGEX] [-u] [-v] [-y]\n                      dag_id\n  ```\n\n  ```markdown\n  airflow tasks clear -s 2023-05-07 -e 2023-05-12 dags_seoul_api_corona\n  airflow tasks clear -s 2023-05-07T22:00:00+00:00 -e 2023-05-12T22:00:00+00:00 dags_seoul_api_corona\n  ```\n* Backfill되었던 DAG은 clear 불가함. reset-dagruns 옵션과 함께 다시 Backfill 수행해야 함\n\n## 정리\n\n| Comparision       | trigger               | backfill | clear    |\n|-------------------|-------------------|-------------------|-------------------|\n| 목적 | 특정 날짜로 DAG Trigger      | Start ~ end 구간의 스케줄 실행    | Start ~ end 구간 내 이미 수\n행되었던 스케줄 재실행   |\n| Run type          | -r 옵션으로 지정 가능. 없으면 Manual | Backfill    | 원래의 run_type    |\n| 기 수행된 run_id가 존재하는 경우  | 동일 run_id 가 존재하는 경우 에러 발생  | Run_type 을 Backfill 로 덮어쓰며\n재실행 | 재실행   |\n| 구간 지정| 불가              | 가능    | 가능    |\n| 과거 날짜 적용 가능 | 가능   | 가능    | 불가 |\n| task 선택 가능    | 불가     | 가능    | 가능    |\n\n* 공통점: CLI 명령으로 DAG 실행 가능\n \n# Triggerer \n\nScheduler, worker, webserver, triggerer containers 중 하나\n\n## Airflow Triggerer의 필요성\n\n* Airflow는 그 자체로 ETL 툴이라기보다 오케스트레이션 솔루션\n* 왜냐면 airflow와 연계되는 외부 솔루션에 작업 제출, 상태 확인, 완료 확인 등의 절차를 통해 관리\n* 예를 들어 airflow의 worker container가 python logic을 직접 처리하는게 아니라 python logic 을 python이 처리하도록 명령을 제출하고 로직 확인 및 결과 확인을 수행한다.\n\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  subgraph cluster0 {\n    rankdir=TB;\n    airflow [shape=circle];\n    PostgresqlDB [shape=box];\n    Hive [shape=box];\n    HDFS [shape=box];\n    Bigquerty [shape=box];\n    Python_func [shape=box];\n    Spark [shape=box];\n  \n  }\n  \n\n airflow-> PostgresqlDB[label=\"작업 실행\"];\n airflow-> Hive[label=\"\"];\n airflow-> HDFS[label=\"작업 실행\"];\n airflow-> Bigquerty[label=\"완료 확인\"];\n airflow-> Python_func[label=\"Python func\"];\n airflow-> Spark[label=\"상태 확인\"];\n\n}\n```\n\n* bigquerty: google에 있는 data 저장소 서비스\n* 외부 솔루션에 작업이 제출되어 완료될 때까지 Airflow의 Slot은 점유됨\n\n![](../../../../../images/airflow/triggerer.PNG)\n  * airflow 의 task가 들어왔을 때 task는 airflow worker의 slot을 차지하게 됨 \n  * 후에, 작업 대상에 작업을 제출하고 작업이 시작된다\n    * 작업대상: python function 또는 postgres, HDFS, Spark와 같은 외부 솔루션\n  * 작업이 진행되면 airflow는 작업이 완료 되었는지 작업상태를 polling 하면서 지속적으로 체크\n  * 작업 처리가 진행되는 동안 task는 차지했던 worker의 slot을 게속해서 차지한다.\n    * task가 많아지면 airflow의 slot이 부족할 수도 있는 상황이 있음\n    * 그럼 작업 처리동안 task는 slot 점유할 필요는 없지 않나라는 생각이 들 수 있다.\n    * triggerer가 이 문제를 해결\n* 작업을 제출하고 Task는 작업 처리가 시작될 때 Slot을 비우고 작업 상태 Polling 작업은 Triggerer 에게 위임\n  * 작업 처리 시작 전까지는 slot을 점유\n  * 작업상태를 끊임없이 polling하면서 확인해야하는데 triggerer를 이용하면 이 작업이 없어짐\n  * triggerer는 작업 처리 완료가 되는 event (작업 완료 callback message를 수신)를 받아 scheduluer container에게 message를 전달하고 scheduler는 task가 다시 비워진 slot을 점유하게 한다.\n\n![](../../../../../images/airflow/triggerer2.PNG)\n\n## Triggerer란\n\n* 워커를 대신하여 작업 상태 완료를 수신하고, 그때까지 Slot을 비워둘 수 있도록 해주는 Airflow의 서비스 \n  * airflow service: 스케줄러, 워커 같은 요소 중 하나\n* Python의 비동기 작업 라이브러리인 asyncio를 이용하여 작업상태 수신\n  * 사용 조건: Airflow 2.2 부터 & Python 3.7부터 사용 가능\n* 어떻게 사용하나?\n  * Deferrable Operator 이용하여 Task 생성\n  * 기본 Operator 중에서는 아래의 Sensor 종류만 사용 가능\n    - TimeSensorAsync\n    - DateTimeSensorAsync\n    - TimeDeltaSensorAsync\n  * 끝에 Async 가 붙은 오퍼레이터를 Deferrable Operator라 부르며 Triggerer에게 작업 완료 수신을 맡기는 오퍼레이터라는 의미\n\n## Triggerer 실습\n\n비교 실험\n\n* dags/dags_time_sensor.py (함수를 그냥 짬)\n* dags/dags_time_sensor_with_async.py (asyncio library 이용)\n\n## Dag Full Example\n\n* dag_time_sensor.py\n\n```markdown\nimport pendulum\nfrom airflow import DAG\nfrom airflow.sensors.date_time import DateTimeSensor\n\nwith DAG(\n    dag_id=\"dags_time_sensor\",\n    # 1시간 차이\n    start_date=pendulum.datetime(2023, 5, 1, 0, 0, 0), #5월1일 0시\n    end_date=pendulum.datetime(2023, 5, 1, 1, 0, 0), #5월1일 1시\n    schedule=\"*/10 * * * *\", #10 분마다 1시간안에 7번 돌게함\n    # 00분, 10분, 20분, 30분, 40분, 50분, 60분 총 7번\n    catchup=True, # catchup을 true이기 때문에 작업 상태bar 7개가 동시에 뜸\n    # airflow ui의 작업 pool을 보면 시작할 때 7개를 모두 차지 하도록 나옴\n) as dag:\n    # DateTimeSensor는 목표로 하는 시간까지 기다리는 sensor\n    sync_sensor = DateTimeSensor(\n        task_id=\"sync_sensor\",\n        # 현재 시간 + 5분\n        target_time=\"\"\"{{ macros.datetime.utcnow() + macros.timedelta(minutes=5) }}\"\"\",\n    )\n```\n\n* dags_time_sensor_with_async.py\n\n```markdown\nimport pendulum\nfrom airflow import DAG\nfrom airflow.sensors.date_time import DateTimeSensorAsync\n\nwith DAG(\n    dag_id=\"dags_time_sensor_with_async\",\n    start_date=pendulum.datetime(2023, 5, 1, 0, 0, 0),\n    end_date=pendulum.datetime(2023, 5, 1, 1, 0, 0),\n    schedule=\"*/10 * * * *\",\n    catchup=True,\n) as dag:\n    sync_sensor = DateTimeSensorAsync(\n        task_id=\"sync_sensor\",\n        target_time=\"\"\"{{ macros.datetime.utcnow() + macros.timedelta(minutes=5) }}\"\"\",\n    )\n\n```\n* airflow ui>> browse >> triggers 에서 trigger가 작업하는 대상 목록을 보여줌\n* trigger는 triggerer에게 작업을 맡길 event 또는 ticket이라고 생각하면 됨\n* triggerer id는 ticket id\n* defered status 는 보라색을 띄고 이 상태에서는 worker slot을 차지 않는 상태이다.\n\n## 요약\n\n* 끝에 Async가 붙은 오퍼레이터는 Deferrable Operator 라 부르며 Triggerer에 의해 Polling이 수행되는 오퍼레이터임을 의미\n* Deferrable Operator는 작업 제출 후 Slot을 차지하지 않으며 Polling 내역에 대해 Trigger 제출 후 deferred 상태가 됨.\n* Triggerer는 제출된 Trigger 내역을 보고 작업 완료시(조건 만족시) Worker에게 알려줘 작업이 마무리될 수 있도록 함.\n\n</div> \n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n\n\n</div>\n\n# Go to Blog Content List\n\n[Blog Content List](../../content_list.qmd)  \n[Engineering Content List](../../Engineering/guide_map/index.qmd)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"11.airflow_functions.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"},"utterances":{"repo":"./docs/comments"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Airflow Additional Function","subtitle":"Dag Triggering Using Dataset, defult_args Parameter of Dag, Sending an Email When a Task Fails, Task Operation Monitoring using sla and Emailing, Setting timeout, CLI Usage of dag trigger and backfill clear, Triggerer","description":"template\n","categories":["Engineering"],"author":"Kwangmin Kim","date":"05/01/2023","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}