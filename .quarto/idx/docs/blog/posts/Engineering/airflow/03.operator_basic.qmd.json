{"title":"Operator Baisc (Bash Operator)","markdown":{"yaml":{"title":"Operator Baisc (Bash Operator)","subtitle":"Basic Operator(Bash Operator), Cron Scheduling, Task Dependencies(Connection), External Script File Operation, Email Operator","description":"template\n","categories":["Engineering"],"author":"Kwangmin Kim","date":"05/01/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"comments":{"utterances":{"repo":"./docs/comments"}},"draft":false},"headingText":"DAG Basic","containsRefs":false,"markdown":"\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n::: {#Korean .tab-pane .fade .show .active role=\"tabpanel\" aria-labelledby=\"Korean-tab\"}\n\n\n## Airflow DAG 생성\n\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  subgraph cluster0 {\n    rankdir=TB;\n    Bash_Operator [shape=box];\n    Python_Operator [shape=box];\n    S3_Operator [shape=box];\n    GCS_Operator [shape=box];\n    label= \"DAG\";\n  }\n\n  Bash_Operator -> Task1 [lhead=cluster0];\n  Python_Operator -> Task2 [lhead=cluster0];\n  S3_Operator -> Task3 [lhead=cluster3];\n  GCS_Operator -> Task4 [lhead=cluster3];\n}\n```\n\n* workflow = DAG\n* Opeartor\n  * 특정 행위를 할 수 있는 기능을 모아 놓은 클래스 또는 설계도\n* Task\n  * operator가 객체화(instantiation)되어 DAG에서 실행 가능한 object\n  * 방향성을 갖고 순환되지 않음 (DAG)\n* Bash Operator\n  * Linux에서 shell script 명령을 수행하는 operator\n* Python Operator\n  * python 함수를 실행하는 operator\n* S3 Operator\n  * AWS의 S3 solution (object storage)을 control할 수 있는 operator\n* GCS Operator\n  * GCP의 GCS solution (object storage)을 control할 수 있는 operator\n* operators을 사용하여 dags을 작성하여 git을 통해 배포한다.\n* dag 작성 및 배포\n\n  ```markdown\n    from __future__ import annotations\n\n    import datetime # python에는 datatime이라는 data type이 있음\n    import pendulum # datetime data type을 처리하는 library\n\n    from airflow import DAG\n    from airflow.operators.bash import BashOperator\n    from airflow.operators.empty import EmptyOperator\n\n    with DAG(\n        dag_id=\"dags_bash_operator\", \n        # airflow service web 상에서 보여지는 이름, python file명과는 무관하지만 \n        # 실무에서는 일반적으로 python 파일명과 dag_id는 일치시키는 것이 다수의 dags 관리에 편리하다.\n        schedule=\"0 0 * * *\", # \"분 시 일 월 요일\", cron schedule로서 매일 0분 0시에 실행\n        start_date=pendulum.datetime(2023, 6, 9, tz=\"Asia/Seoul\"), #dags이 언제 실행될지 설정\n        # UTC: 세계 표준시로 한국 보다 9시간이 느림. Asia/Seoul로 변경해야 지정한 날짜에 0분 0시에 실행될 수 있다.\n        catchup=False, # start_date를 현재보다 과거로 설정하게 될 경우 \n        # catchup=True면 과거 부터 현재까지 소급해서 실행. \n        # 시간 순서대로 실행하는게 아니라 병렬로 한번에 실행하기 때문에 메모리를 많이 잡아먹을 수 있음. \n        # 그래서 보통 False로 처리. catchup=False면 현재부터만 실행\n        # dagrun_timeout=datetime.timedelta(minutes=60), # dag이 60분 이상 구동시 실패가 되도록 설정\n        # tags=[\"example\", \"example2\"], #airflow service web browser상 dag의 tag를 의미. 즉 dag id 바로 밑 파란색 박스를 의미. tag를 누르면 같은 tag를 가진 dags들만 filtering돼서 선택됨 \n        ## dags 이 수 백개가 될 때 tag로 filtering 하면 용이함 \n        # params={\"example_key\": \"example_value\"}, # as dag: 이하 tasks를 정의할 때, \n        ## tasks에 공통 passing parameters가 있을 때 씀\n    ) as dag:\n        # [START how to_operator_bash]\n        bash_task1 = BashOperator(\n            task_id=\"bash_task1\", # airflow web service의 dag graph에 표시될 task명\n            # task역시 task object name (bash_task1)과 task_id(bash_task1)를 일치시키는 것이 좋음\n            bash_command=\"echo this task works well!\",\n            # bash_command 이하는 shell script를 적어주면 됨\n        )\n        # [END how to_operator_bash]\n        bash_task2 = BashOperator(\n            task_id=\"bash_task2\",  \n            bash_command=\"echo $HOSTNAME\", # $HOSTNAME: HOSTNAME 환경변수 호출\n            # WSL terminal 이름이 출력된다.\n        )\n        bash_task1 >> bash_task2 # 수행될 tasks의 관계 설정\n  ```\n\n* 배포된 dags을 airflow containers과 연결 시키기 위해 `docker-compose.yaml` 실행\n  * `vi docker-compose.yaml` 실행 후 `docker-compose.yaml` 안에서 `Volumns` 항목이 wsl의 directory와 container directory를 연결(mount)해주는 요소\n   ```markdown\n   Volumes\n     - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n     - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n     - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n     - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n   ```\n  * 위와 같이 Volumns 항목이 뜨는데 `:`을 기준으로 왼쪽이 WSL directories(volumns), 오른쪽이 Docker container directories(volumns)\n  * 다른 WSL창을 열어 `echo ${AIRFLOW_PROJ_DIR:-.}` 실행하면 `AIRFLOW_PROJ_DIR`에 값이 없기 때문에 `.` 출력됨\n    * `AIRFLOW_PROJ_DIR:-.` : shell script문법으로 `AIRFLOW_PROJ_DIR`에 값이 있으면 출력하고 없으면 `.`을 출력하라는 의미\n    * `echo AIRFLOW_PROJ_DIR`: 아무것도 출력 안됨\n  * `${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags`는 `./dags`를 `/opt/airflow/dags`에 연결시키라는 의미\n    * `./`: `docker-compose.yaml`이 위치하고있는 현재 directory를 의미 \n  * 배포된 dags를 자동으로 docker container에 연동시키기 위해 `Volumns`을 다음과 같이 편집\n    ```markdown\n      volumes:\n        - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n        - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n        - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n        - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n    ```\n    * directory hierarchy에 따라 위의 volumes path를 다르게 설정해야한다. docker service web browse(i.e. localhost:8080) 껏다 켜가면서 확인하면서 설정\n  * 새로운 dags 배포할 때마다 airflow service 껐다가 켜야 한다.\n* airflow service 껐다 켜서 잘 반영됐는지 확인\n  * docker가 설치된 wsl directory이동 먼저 할 것\n  * airflow service 끄기: `sudo docker compose down`\n  * airflow service 켜기: `sudo docker compose up`\n* airflow web service상에서 dags이 잘 mount 되었는지 확인\n  * 기본적으로 dags은 airflow web service상에 올라올 때 unpaused 상태로 올라옴\n  * 하지만 schedule이 걸려있는 dags은 unpaused상태에서 한번 돌고 올라옴\n  * dag을 클릭하면 긴 녹색 막대기를 누르면 수행된 schedule내용이 나오고\n  * 각 각의 task에 대응되는 녹색 네모 박스를 누르면 결과들을 조회할 수 있다.\n    * 네모 박스를 누르고 log (audit log 아님)를 누르면 결과가 자세히 조회된다.\n    * `bash_task2` 의 `bash_command=\"echo $HOSTNAME\"` 의 결과값으로 조회된 값은 docker worker container id 를 의미한다. \n      * 하지만 본인의 경우, airflow web service에서 `794f3b56824a`가 출력된 것을 확인했고\n      * `sudo docker ps`로 container ID를 확인한 결과 `airflow-airflow-worker-1` 의 `32092b201878` 로 달랐다.\n    * 실제 worker container로 들어가 `echo $HOSTNAME` 실행하면 worker container id 출력되어야 함\n      * worker container로 들어가기: `sudo docker exec -it container-name bash` $\\rightarrow$ 본인의 경우: `sudo docker exec -it airflow-airflow-worker-1 bash` 이 과정이 dag을 돌린과정과 같은 mechanism임\n      * `echo $HOSTNAME` 실행 : `32092b201878` 출력됨 (어쨌든 airflow web service상의 `794f3b56824a`와 달랐음)\n      * `sudo docker exec -it 794f3b56824a bash` 결과 Error response from daemon: No such container: 794f3b56824a 라는 에러메세지 뜸\n    * 즉, worker container가 실제 `task`를 처리하는 것을 볼 수 있었다.\n\n## Subject of Task Performance\n\n```{dot}\n\ndigraph G {\n  compound=true;\n  rankdir=TB;\n  subgraph cluster0 {\n    rankdir=TB;\n    Scheduler [shape=box];\n    DAG_file [shape=box];\n    Worker [shape=box, style=filled, fillcolor=yellow];\n    Queue [shape=box];\n    Meta_DB [shape=box];\n    label= \"Task Process\";\n  }\n\n  Scheduler -> DAG_file [label=\"1.parsing\"];\n  Scheduler -> Meta_DB [label=\"2.save information\"];\n  Scheduler -> Scheduler [label=\"3.check start time\"];\n  Scheduler -> Queue;\n  Queue -> Worker [label=\"4.start instruction\"];\n  DAG_file -> Worker [label=\"5.Processing after reading\"];\n  Worker -> Meta_DB [label=\"6.Results update\"];\n}\n```\n\n* scheduler\n  * airflow에서 brain역할 \n    1. parsing: a user가 만든 dag 파일을 읽어들여 문법적 오류 여부와 tasks 간의 관계를 분석\n    2. save information: DAG Parsing 후 DB에 정보저장 (tasks, task relations, schedule, etc.)\n    3. check start time: DAG 시작 실행 시간 확인\n    4. start instruction: DAG 시작 실행 시간마다 worker에 실행 지시\n      * scheduler와 workder 사이에 queue 상태가 있을 수 있음\n* worker (Worker Container)\n  * airflow 처리 주체 (subject)\n    5. Processing after reading: scheduler가 시킨 DAG 파일을 찾아 읽고 처리\n    6. Results update: 처리가 되기 전/후를 Meta DB에 update함\n\n# Cron Schedule\n\n## Cron Scheduling\n\n* task가 실행되어야 하는 시간(주기)을 정하기 위한 다섯개의 필드로 구성된 문자열\n* Cron을 이용하면 왠만한 scheduling 모두 가능\n\n`{minutes} {hour} {day} {month} {weekday}`\n\n|Number|Special Characters|Description|\n|:-:|:----:|-------------|\n|1|*|모든 값 |\n|2|-|범위 지정|\n|3|,|여러 값 지정|\n|4|/|증가값 지정. staring-value/ending-value|\n|5|L|마지막 값 (일, 요일에만 설정 가능) <br> * 일에 L 입력시 해당 월의 마지막 일 의미 <br> ※ 요일에 L 입력시 토요일 의미|\n|6|#|몇 번째 요일인지 지정|선형 증가 필터 모듈을 제외한 DSP 알고리즘|\n\n|Cron schedule|Description|Note|\n|:--|-----|-------|\n|15 2 * * *|매일 02시 15분에 도는 daily batch ||\n|0 * * * *|매시 정각에 도는 시간 단위 batch||\n|0 0 1 * *|매월 1일 0시 0분 도는 monthly batch||\n|10 1 * * 1|매주 월요일 1시 10분에 도는 weekly batch| 0: 일요일, 1: 월요일, 2: 화요일, 3:수요일, 4: 목요일, 5: 금요일, 6: 토요일 |\n|0 9-18 * * *|매일 9시부터 18시까지 정각마다 도는 daily batch| 보통 이렇게 scheduling하지는 않음. 하지만 구현할 수 있음|\n|0 1 1,2,3 * *|매월 1일, 2일 3일만 1시에 도는 monthly batch|보통 이렇게 scheduling하지는 않음. 하지만 구현할 수 있음|\n|\\*/30 * * *|삼십분마다 (0분, 30분) ||\n|10-59/30 * * * *|10분부터 삼십분마다 (10분, 40분에 도는 작업)||\n|10 1 * * 1-5|평일만 01시 10분||\n|0 \\*/2 * * *|2시간 마다 (0시, 02시, 04시 …)|1-23/2: 1시부터 2시간 마다\n|0 0 \\*/2 * *|짝수일 0시 0분||\n|10 1 L * *|매월 마지막 일 01시 10분에 도는 montly batch|빈번하게 사용되는 schedule|\n|10 1 * * 6#3|매월 세 번째 토요일 01시 10분 도는 montly batch||\n\n\n# Task Dependencies(Connection)\n\n## Task Connection Methods\n\n* Task 연결 방법 종류\n  * \\>>, << 사용하기 (Airflow 공식 추천방식)\n  * 함수 사용하기\n* 복잡한 Task 는 어떻게 연결하는가?\n\n```{dot}\n\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  subgraph cluster0 {\n    rankdir=TB;\n    task1 [shape=box];\n    task2 [shape=box];\n    task3 [shape=box];\n    task4 [shape=box];\n    task5 [shape=box];\n    task6 [shape=box];\n    task7 [shape=box];\n    task8 [shape=box];\n    label= \"Task Connection\";\n  }\n\n  task1 -> task3 ;\n  task1 -> task2 ;\n  task2 -> task4 ;\n  task3 -> task4 ;\n  task5 -> task4 ;\n  task4 -> task6 ;\n  task7 -> task6 ;\n  task6 -> task8 ;\n}\n```\n\n### \\>>, << 사용하기 (Airflow 공식 추천방식)\n\n  * 방법1 : 모든 경우의 수에 대해서 연결 가능하지만 가독성 떨어짐\n  ```markdown\n  task1 >> task2\n  task1 >> task3\n  task2 >> task4\n  task3 >> task4\n  task5 >> task4\n  task4 >> task6\n  task7 >> task6\n  task6 >> task8\n  ```\n  * 방법2: 같은 레벨의 tasks는 list로 묶어 준다. 가독성이 높지만 구현이 안되는 경우 있을 수 있음\n  ```markdown\n  task1 >> [task2, task3] >> task4\n  task5 >> task4\n  [task4, task7] >> task6 >> task8\n  ```\n  * 방법3: 역방향은 <<를 이용 (권장 하지 않음)\n  ```markdown\n  task1 >> [task2, task3] >> task4 << task5\n  task4 >> task 6 << task7\n  task6 >> task8\n  ```\n\n#### Example\n\n```markdown\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.empty import EmptyOperator\n#EmptyOperator는 어떤 연산도 하지 않는 class\n\nwith DAG(\n    dag_id=\"dags_task_connection\",\n    schedule=None,\n    start_date=pendulum.datetime(2023,3,1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # 8개의 instances: task1~task8\n    task1=EmptyOperator(\n        task_id='task1'\n    )\n    task2=EmptyOperator(\n        task_id='task2'\n    )\n    task3=EmptyOperator(\n        task_id='task3'\n    )\n    task4=EmptyOperator(\n        task_id='task4'\n    )\n    task5=EmptyOperator(\n        task_id='task5'\n    )\n    task6=EmptyOperator(\n        task_id='task6'\n    )\n    task7=EmptyOperator(\n        task_id='task7'\n    )\n    task8=EmptyOperator(\n        task_id='task8'\n    )\n  \n  task1 >> [task2, task3] >> task4\n  task5 >> task4\n  [task4, task7] >> task6 >> task8\n\n```\n\n### 함수 사용하기\n\n* [Reference: Airflow Official Document](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)\n  * Content/Core Concepts/DAGs 참고\n  * DAGs에 대한 숙련도가 올라가면 이 링크를 참고하면 매우 유용\n    * DAG을 어떤 상황에서 어떻게 짜야하는지에 대한 guidance가 자세히 적혀 있음\n    * 예를 들어, dag을 생성하는 방법 (dag declaration)에는 with 문을 사용하는 방법과 standard constructor (표준 생성자)를 사용하는 방법이 있음\n      1. with statement\n      ```markdown\n      import datetime\n\n      from airflow import DAG\n      from airflow.operators.empty import EmptyOperator\n      \n      with DAG(\n          dag_id=\"my_dag_name\",\n          start_date=datetime.datetime(2021, 1, 1),\n          schedule=\"@daily\",\n      ):\n      EmptyOperator(task_id=\"task\")\n      ```\n      2. standard constructor (class)\n      ```markdown\n      import datetime\n\n      from airflow import DAG\n      from airflow.operators.empty import EmptyOperator\n      \n      #class 생성\n      my_dag = DAG( \n          dag_id=\"my_dag_name\",\n          start_date=datetime.datetime(2021, 1, 1),\n          schedule=\"@daily\",\n      )\n      EmptyOperator(task_id=\"task\", dag=my_dag)\n      ```\n      3. python의 decorator기능 활용 (dag decorator to turn a function into a DAG generator)\n      ```markdown\n      import datetime\n\n      from airflow.decorators import dag\n      from airflow.operators.empty import EmptyOperator\n      \n      \n      @dag(start_date=datetime.datetime(2021, 1, 1), schedule=\"@daily\")\n      def generate_dag():\n          EmptyOperator(task_id=\"task\")\n      \n      \n      generate_dag()\n      ```\n* task dependencies 설정을 위한 emplicit methods. \n  * `set_upstream` and `set_downstream`\n  ```markdown\n  first_task.set_downstream(second_task, third_task)\n  third_task.set_upstream(fourth_task)\n  ```\n  * `cross_downstream`\n\n  ```markdown\n  from airflow.models.baseoperator import cross_downstream\n\n  #Replaces\n  #[op1, op2] >> op3\n  #[op1, op2] >> op4\n  cross_downstream([op1, op2], [op3, op4])\n  ```\n  * `chain`\n  ```markdown\n  from airflow.models.baseoperator import chain\n\n  #Replaces op1 >> op2 >> op3 >> op4\n  chain(op1, op2, op3, op4)\n\n  #You can also do it dynamically\n  chain(*[EmptyOperator(task_id='op' + i) for i in range(1, 6)])\n\n  #or\n\n  from airflow.models.baseoperator import chain\n\n  #Replaces\n  #op1 >> op2 >> op4 >> op6\n  #op1 >> op3 >> op5 >> op6\n  chain(op1, [op2, op3], [op4, op5], op6)\n  ```\n\n\n# External Customized Script Operation\n\n* 외부 script file such as `*.py` and `*.sh` 은 docker가 인식할 수 있도록 docker의 plugins directory안에 넣어줘야 실행된다.\n\n## What is Shell Script ?\n\n* Unix/Linux Shell 명령어로 적혀진 파일로 인터프리터에 의해 한 줄씩 처리된다.\n  * interpreter: CPU가 programming 언어를 처리하는데 크게 compiling 방식과 interpreting 방식 2가지 방식이 있다.\n    * compiling\n      * programming language를 목적 코드인 2진수로 처리한다음 읽음\n      * compile 할 때 연산 시간은 다소 소요되지만 한 번 compile 된 script는 실행 속도가 매우 빠름\n      * C, Java\n    * interpreting: compiling없이 한줄씩 읽는 방식\n      * compiling방식에 비해 실행 속도가 느림\n      * python, shell\n* bashOperator를 이용하여 shell script 처리\n* Echo, mkdir, cd, cp, tar, touch 등의 기본적인 쉘 명령어를 입력하여 작성하며 변수를 입력받거나 For 문, if 문 그리고 함수도 사용 가능\n* 확장자가 없어도 동작하지만 주로 파일명에 .sh 확장자를 붙인다.\n\n## Why to Need Shell Script?\n\n* bashOperator를 이용하다면 bashOperator안에 shell 명령어들을 써서 넣어도 동작은 하지만\n* 쉘 명령어를 이용하여 **복잡한 로직을 처리하는 경우** shell script를 이용하는 것이 좋다\n  * 예를들어, sftp (source sever)를 통해 csv나 json같은 파일을 받은 후 전처리하여 DB에 Insert & tar.gz으로 압축하고 싶을때, 이렇게 복잡한 tasks를 bashOperator에 모두 기입하기 보다는 script를 짜서 bashOperator에서 호출하는 방식이 가독성이나 유지보수 측면에서 더 효율적이다.\n* **쉘 명령어 재사용을 위한 경우**\n  * 위의 예시를 server 100대에 대하여 반복 수행할 때 logic이 같으면 shell script를 100번 호출하는 것이 더 간편\n  * sftp: 접속할 때 IP, Port, account, pw 가 필요한데 이런 것을 변수화 시키고 DB전처리 로직을 shell script에 짜 놓으면 됨.\n\n## Worker 컨테이너가 외부 스크립트(shell)를 수행하려면?\n\n* 문제점\n  * 컨테이너는 외부의 파일을 인식할 수 없다. shell script를 wsl directory 어딘가에 넣으면 container가 인식을 못함.\n  * 컨테이너 안에 파일을 만들어주면 컨테이너 재시작시 파일이 사라진다. docker에서 이미지를 띄우는 것을 container를 만들었다라고 하는데 container 재 실행시 초기화 되어 실행된다. (docker의 특징). 그래서 컨테이너 안에 shell script 파일 넣어도 재시작시 삭제가 됨.\n* 해결방법\n\n  ![](../../../../../images/airflow/worker-container-shell-operation.PNG)\n\n  * 빨간 네모박스의 plugins에 shell script를 저장한다. airflow document에서는 customized python and shell script를 plugins에 저장하는 것을 권장\n* example\n\n```markdown\ncd github-repository/plugins/shell\nvi select_fruit.sh #i 누르면 편집가능하고 편집 후 esc+wq! 입력하고 enter치면 저장하고 나감\nchmod +x select_fruit.sh #실행 권한을 부여\n./select_fruit.sh kmkim # ./test2.sh 는 test2.sh을 실행한다는 의미 출력물: kmkim 출력됨\ngit add -A\ngit commit -m \"shell script example\"\ngit push\n```\n\n```select_fruit.sh\n# echo $1 #첫 번째 인수 출력\n\nFRUIT=$1\nif [ $FRUIT == APPLE ]; then\n  echo \"You selected Apple!\"\nelif [ $FRUIT == ORANGE ]; then\n  echo \"You selected Orange!\"\nelif [ $FRUIT == Grape ]; then\n  echo \"You selected Grape!\"\nelse \n  echo \"You selected other Fruit!\"\nfi\n```\n\n* container에서 github repository에 있는 plugins/shell에 있는 shell script 인식하게 하기\n  * `vi docker-compose.yaml` 에서 67line 수정\n    ```markdown\n    volumes:\n      - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n      - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n      - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n      - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n    ```\n\n* select_fruit.sh 실행 권한 부여\n\n아래와 같이 6번의 task 수행 실패가 발생했는데 처음엔 volumne의 path 설정이 잘못 됐는지 알고 계속 `docker-compose.yaml`을 살펴봤다. 하지만 이상이 없는 것을 확인하고 task의 log를 확인해 봤는데 다음과 같은 error가 뜬것을 확인할 수 있었다.\n\n![execution error](../../../../../images/airflow/external_script_operation.PNG)\n\n`/bin/bash: line 1: /opt/***/plugins/shell/select_fruit.sh: Permission denied`\n\n이럴 땐 다음과 같이 실행권한을 부여하게 되면 해결된다.\n\n`(airflow) kmkim@K100230201051:~/airflow/plugins/shell$ chmod +x select_fruit.sh`\n\n# Email Operator \n\n* 이메일 전송해주는 오퍼레이터\n  ```markdown\n  email_t1 = EmailOperator(\n    task_id='email_t1',\n    to='hjkim_sun@naver.com',\n    subject='Airflow 처리결과',\n    html_content='정상 처리되었습니다.'\n  )\n  ```\n* 구글 메일 서버 사용\n\n\n## Presetting \n\n### Google Settings\n\n* 이메일 전송을 위해 사전 셋팅 작업 필요(Google)\n  * google mail server사용\n  * gmail >> settings(설정) >> See all settings (모든 설정 보기) >> Forwarding and POP/IMAP (전달 및 POP/IMAP) >> IMAP access (IMAP 접근): Enable IMAP (IMAP  사용)\n  * Manage Your Google Acccount (구글 계정 관리) >> Security (보안) >> 2-Step Verification (2단계 인증) >> App Passwords: 앱비밀번호 setting >> select app: Mail , Select device: Windows Computer >> Generate app pasword message window popped up\n\n### Airflow Settings\n\n* 사전 설정 작업 (airflow)\n  * docker-compose.yaml 편집 (environment 항목에 추가)\n  ```markdown \n  # 띄어쓰기 주의\n  AIRFLOW__SMTP__SMTP_HOST: 'smtp.gmail.com'  \n  AIRFLOW__SMTP__SMTP_USER: '{gmail 계정}'\n  AIRFLOW__SMTP__SMTP_PASSWORD: '{앱비밀번호}'\n  AIRFLOW__SMTP__SMTP_PORT: 587\n  AIRFLOW__SMTP__SMTP_MAIL_FROM: '{gmail 계정}' # 이메일을 누가 보내는 것으로 할건지 정함\n  ```\n\n## EmailOperator 작성\n\n```markdown\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.email import EmailOperator\n\nwith DAG(\n    dag_id=\"dags_email_operator\",\n    schedule=\"0 8 1 * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    sending_email_task=EmailOperator(\n        task_id='sending_email_task',\n        to='sdf@naver.com',\n        cc=['sdf2@gmail.com', 'sdf3@gmail.com'],\n        subject='Airflow Test',\n        html_content= \"\"\"\n            this is a test for airflow.<br/><br/>\n            \n            {{ ds }}<br/>\n        \"\"\"\n    )\n\n```\n \n:::\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n::: {#English .tab-pane .fade role=\"tabpanel\" aria-labelledby=\"English-tab\"}\n\n:::\n\n\n</div>\n\n# Go to Blog Content List\n\n[Blog Content List](../../content_list.qmd)  \n[Engineering Content List](../../Engineering/guide_map/index.qmd)","srcMarkdownNoYaml":"\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n::: {#Korean .tab-pane .fade .show .active role=\"tabpanel\" aria-labelledby=\"Korean-tab\"}\n\n# DAG Basic\n\n## Airflow DAG 생성\n\n```{dot}\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  subgraph cluster0 {\n    rankdir=TB;\n    Bash_Operator [shape=box];\n    Python_Operator [shape=box];\n    S3_Operator [shape=box];\n    GCS_Operator [shape=box];\n    label= \"DAG\";\n  }\n\n  Bash_Operator -> Task1 [lhead=cluster0];\n  Python_Operator -> Task2 [lhead=cluster0];\n  S3_Operator -> Task3 [lhead=cluster3];\n  GCS_Operator -> Task4 [lhead=cluster3];\n}\n```\n\n* workflow = DAG\n* Opeartor\n  * 특정 행위를 할 수 있는 기능을 모아 놓은 클래스 또는 설계도\n* Task\n  * operator가 객체화(instantiation)되어 DAG에서 실행 가능한 object\n  * 방향성을 갖고 순환되지 않음 (DAG)\n* Bash Operator\n  * Linux에서 shell script 명령을 수행하는 operator\n* Python Operator\n  * python 함수를 실행하는 operator\n* S3 Operator\n  * AWS의 S3 solution (object storage)을 control할 수 있는 operator\n* GCS Operator\n  * GCP의 GCS solution (object storage)을 control할 수 있는 operator\n* operators을 사용하여 dags을 작성하여 git을 통해 배포한다.\n* dag 작성 및 배포\n\n  ```markdown\n    from __future__ import annotations\n\n    import datetime # python에는 datatime이라는 data type이 있음\n    import pendulum # datetime data type을 처리하는 library\n\n    from airflow import DAG\n    from airflow.operators.bash import BashOperator\n    from airflow.operators.empty import EmptyOperator\n\n    with DAG(\n        dag_id=\"dags_bash_operator\", \n        # airflow service web 상에서 보여지는 이름, python file명과는 무관하지만 \n        # 실무에서는 일반적으로 python 파일명과 dag_id는 일치시키는 것이 다수의 dags 관리에 편리하다.\n        schedule=\"0 0 * * *\", # \"분 시 일 월 요일\", cron schedule로서 매일 0분 0시에 실행\n        start_date=pendulum.datetime(2023, 6, 9, tz=\"Asia/Seoul\"), #dags이 언제 실행될지 설정\n        # UTC: 세계 표준시로 한국 보다 9시간이 느림. Asia/Seoul로 변경해야 지정한 날짜에 0분 0시에 실행될 수 있다.\n        catchup=False, # start_date를 현재보다 과거로 설정하게 될 경우 \n        # catchup=True면 과거 부터 현재까지 소급해서 실행. \n        # 시간 순서대로 실행하는게 아니라 병렬로 한번에 실행하기 때문에 메모리를 많이 잡아먹을 수 있음. \n        # 그래서 보통 False로 처리. catchup=False면 현재부터만 실행\n        # dagrun_timeout=datetime.timedelta(minutes=60), # dag이 60분 이상 구동시 실패가 되도록 설정\n        # tags=[\"example\", \"example2\"], #airflow service web browser상 dag의 tag를 의미. 즉 dag id 바로 밑 파란색 박스를 의미. tag를 누르면 같은 tag를 가진 dags들만 filtering돼서 선택됨 \n        ## dags 이 수 백개가 될 때 tag로 filtering 하면 용이함 \n        # params={\"example_key\": \"example_value\"}, # as dag: 이하 tasks를 정의할 때, \n        ## tasks에 공통 passing parameters가 있을 때 씀\n    ) as dag:\n        # [START how to_operator_bash]\n        bash_task1 = BashOperator(\n            task_id=\"bash_task1\", # airflow web service의 dag graph에 표시될 task명\n            # task역시 task object name (bash_task1)과 task_id(bash_task1)를 일치시키는 것이 좋음\n            bash_command=\"echo this task works well!\",\n            # bash_command 이하는 shell script를 적어주면 됨\n        )\n        # [END how to_operator_bash]\n        bash_task2 = BashOperator(\n            task_id=\"bash_task2\",  \n            bash_command=\"echo $HOSTNAME\", # $HOSTNAME: HOSTNAME 환경변수 호출\n            # WSL terminal 이름이 출력된다.\n        )\n        bash_task1 >> bash_task2 # 수행될 tasks의 관계 설정\n  ```\n\n* 배포된 dags을 airflow containers과 연결 시키기 위해 `docker-compose.yaml` 실행\n  * `vi docker-compose.yaml` 실행 후 `docker-compose.yaml` 안에서 `Volumns` 항목이 wsl의 directory와 container directory를 연결(mount)해주는 요소\n   ```markdown\n   Volumes\n     - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n     - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n     - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n     - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n   ```\n  * 위와 같이 Volumns 항목이 뜨는데 `:`을 기준으로 왼쪽이 WSL directories(volumns), 오른쪽이 Docker container directories(volumns)\n  * 다른 WSL창을 열어 `echo ${AIRFLOW_PROJ_DIR:-.}` 실행하면 `AIRFLOW_PROJ_DIR`에 값이 없기 때문에 `.` 출력됨\n    * `AIRFLOW_PROJ_DIR:-.` : shell script문법으로 `AIRFLOW_PROJ_DIR`에 값이 있으면 출력하고 없으면 `.`을 출력하라는 의미\n    * `echo AIRFLOW_PROJ_DIR`: 아무것도 출력 안됨\n  * `${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags`는 `./dags`를 `/opt/airflow/dags`에 연결시키라는 의미\n    * `./`: `docker-compose.yaml`이 위치하고있는 현재 directory를 의미 \n  * 배포된 dags를 자동으로 docker container에 연동시키기 위해 `Volumns`을 다음과 같이 편집\n    ```markdown\n      volumes:\n        - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n        - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n        - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n        - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n    ```\n    * directory hierarchy에 따라 위의 volumes path를 다르게 설정해야한다. docker service web browse(i.e. localhost:8080) 껏다 켜가면서 확인하면서 설정\n  * 새로운 dags 배포할 때마다 airflow service 껐다가 켜야 한다.\n* airflow service 껐다 켜서 잘 반영됐는지 확인\n  * docker가 설치된 wsl directory이동 먼저 할 것\n  * airflow service 끄기: `sudo docker compose down`\n  * airflow service 켜기: `sudo docker compose up`\n* airflow web service상에서 dags이 잘 mount 되었는지 확인\n  * 기본적으로 dags은 airflow web service상에 올라올 때 unpaused 상태로 올라옴\n  * 하지만 schedule이 걸려있는 dags은 unpaused상태에서 한번 돌고 올라옴\n  * dag을 클릭하면 긴 녹색 막대기를 누르면 수행된 schedule내용이 나오고\n  * 각 각의 task에 대응되는 녹색 네모 박스를 누르면 결과들을 조회할 수 있다.\n    * 네모 박스를 누르고 log (audit log 아님)를 누르면 결과가 자세히 조회된다.\n    * `bash_task2` 의 `bash_command=\"echo $HOSTNAME\"` 의 결과값으로 조회된 값은 docker worker container id 를 의미한다. \n      * 하지만 본인의 경우, airflow web service에서 `794f3b56824a`가 출력된 것을 확인했고\n      * `sudo docker ps`로 container ID를 확인한 결과 `airflow-airflow-worker-1` 의 `32092b201878` 로 달랐다.\n    * 실제 worker container로 들어가 `echo $HOSTNAME` 실행하면 worker container id 출력되어야 함\n      * worker container로 들어가기: `sudo docker exec -it container-name bash` $\\rightarrow$ 본인의 경우: `sudo docker exec -it airflow-airflow-worker-1 bash` 이 과정이 dag을 돌린과정과 같은 mechanism임\n      * `echo $HOSTNAME` 실행 : `32092b201878` 출력됨 (어쨌든 airflow web service상의 `794f3b56824a`와 달랐음)\n      * `sudo docker exec -it 794f3b56824a bash` 결과 Error response from daemon: No such container: 794f3b56824a 라는 에러메세지 뜸\n    * 즉, worker container가 실제 `task`를 처리하는 것을 볼 수 있었다.\n\n## Subject of Task Performance\n\n```{dot}\n\ndigraph G {\n  compound=true;\n  rankdir=TB;\n  subgraph cluster0 {\n    rankdir=TB;\n    Scheduler [shape=box];\n    DAG_file [shape=box];\n    Worker [shape=box, style=filled, fillcolor=yellow];\n    Queue [shape=box];\n    Meta_DB [shape=box];\n    label= \"Task Process\";\n  }\n\n  Scheduler -> DAG_file [label=\"1.parsing\"];\n  Scheduler -> Meta_DB [label=\"2.save information\"];\n  Scheduler -> Scheduler [label=\"3.check start time\"];\n  Scheduler -> Queue;\n  Queue -> Worker [label=\"4.start instruction\"];\n  DAG_file -> Worker [label=\"5.Processing after reading\"];\n  Worker -> Meta_DB [label=\"6.Results update\"];\n}\n```\n\n* scheduler\n  * airflow에서 brain역할 \n    1. parsing: a user가 만든 dag 파일을 읽어들여 문법적 오류 여부와 tasks 간의 관계를 분석\n    2. save information: DAG Parsing 후 DB에 정보저장 (tasks, task relations, schedule, etc.)\n    3. check start time: DAG 시작 실행 시간 확인\n    4. start instruction: DAG 시작 실행 시간마다 worker에 실행 지시\n      * scheduler와 workder 사이에 queue 상태가 있을 수 있음\n* worker (Worker Container)\n  * airflow 처리 주체 (subject)\n    5. Processing after reading: scheduler가 시킨 DAG 파일을 찾아 읽고 처리\n    6. Results update: 처리가 되기 전/후를 Meta DB에 update함\n\n# Cron Schedule\n\n## Cron Scheduling\n\n* task가 실행되어야 하는 시간(주기)을 정하기 위한 다섯개의 필드로 구성된 문자열\n* Cron을 이용하면 왠만한 scheduling 모두 가능\n\n`{minutes} {hour} {day} {month} {weekday}`\n\n|Number|Special Characters|Description|\n|:-:|:----:|-------------|\n|1|*|모든 값 |\n|2|-|범위 지정|\n|3|,|여러 값 지정|\n|4|/|증가값 지정. staring-value/ending-value|\n|5|L|마지막 값 (일, 요일에만 설정 가능) <br> * 일에 L 입력시 해당 월의 마지막 일 의미 <br> ※ 요일에 L 입력시 토요일 의미|\n|6|#|몇 번째 요일인지 지정|선형 증가 필터 모듈을 제외한 DSP 알고리즘|\n\n|Cron schedule|Description|Note|\n|:--|-----|-------|\n|15 2 * * *|매일 02시 15분에 도는 daily batch ||\n|0 * * * *|매시 정각에 도는 시간 단위 batch||\n|0 0 1 * *|매월 1일 0시 0분 도는 monthly batch||\n|10 1 * * 1|매주 월요일 1시 10분에 도는 weekly batch| 0: 일요일, 1: 월요일, 2: 화요일, 3:수요일, 4: 목요일, 5: 금요일, 6: 토요일 |\n|0 9-18 * * *|매일 9시부터 18시까지 정각마다 도는 daily batch| 보통 이렇게 scheduling하지는 않음. 하지만 구현할 수 있음|\n|0 1 1,2,3 * *|매월 1일, 2일 3일만 1시에 도는 monthly batch|보통 이렇게 scheduling하지는 않음. 하지만 구현할 수 있음|\n|\\*/30 * * *|삼십분마다 (0분, 30분) ||\n|10-59/30 * * * *|10분부터 삼십분마다 (10분, 40분에 도는 작업)||\n|10 1 * * 1-5|평일만 01시 10분||\n|0 \\*/2 * * *|2시간 마다 (0시, 02시, 04시 …)|1-23/2: 1시부터 2시간 마다\n|0 0 \\*/2 * *|짝수일 0시 0분||\n|10 1 L * *|매월 마지막 일 01시 10분에 도는 montly batch|빈번하게 사용되는 schedule|\n|10 1 * * 6#3|매월 세 번째 토요일 01시 10분 도는 montly batch||\n\n\n# Task Dependencies(Connection)\n\n## Task Connection Methods\n\n* Task 연결 방법 종류\n  * \\>>, << 사용하기 (Airflow 공식 추천방식)\n  * 함수 사용하기\n* 복잡한 Task 는 어떻게 연결하는가?\n\n```{dot}\n\ndigraph G {\n  compound=true;\n  rankdir=LR;\n  subgraph cluster0 {\n    rankdir=TB;\n    task1 [shape=box];\n    task2 [shape=box];\n    task3 [shape=box];\n    task4 [shape=box];\n    task5 [shape=box];\n    task6 [shape=box];\n    task7 [shape=box];\n    task8 [shape=box];\n    label= \"Task Connection\";\n  }\n\n  task1 -> task3 ;\n  task1 -> task2 ;\n  task2 -> task4 ;\n  task3 -> task4 ;\n  task5 -> task4 ;\n  task4 -> task6 ;\n  task7 -> task6 ;\n  task6 -> task8 ;\n}\n```\n\n### \\>>, << 사용하기 (Airflow 공식 추천방식)\n\n  * 방법1 : 모든 경우의 수에 대해서 연결 가능하지만 가독성 떨어짐\n  ```markdown\n  task1 >> task2\n  task1 >> task3\n  task2 >> task4\n  task3 >> task4\n  task5 >> task4\n  task4 >> task6\n  task7 >> task6\n  task6 >> task8\n  ```\n  * 방법2: 같은 레벨의 tasks는 list로 묶어 준다. 가독성이 높지만 구현이 안되는 경우 있을 수 있음\n  ```markdown\n  task1 >> [task2, task3] >> task4\n  task5 >> task4\n  [task4, task7] >> task6 >> task8\n  ```\n  * 방법3: 역방향은 <<를 이용 (권장 하지 않음)\n  ```markdown\n  task1 >> [task2, task3] >> task4 << task5\n  task4 >> task 6 << task7\n  task6 >> task8\n  ```\n\n#### Example\n\n```markdown\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.empty import EmptyOperator\n#EmptyOperator는 어떤 연산도 하지 않는 class\n\nwith DAG(\n    dag_id=\"dags_task_connection\",\n    schedule=None,\n    start_date=pendulum.datetime(2023,3,1, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    # 8개의 instances: task1~task8\n    task1=EmptyOperator(\n        task_id='task1'\n    )\n    task2=EmptyOperator(\n        task_id='task2'\n    )\n    task3=EmptyOperator(\n        task_id='task3'\n    )\n    task4=EmptyOperator(\n        task_id='task4'\n    )\n    task5=EmptyOperator(\n        task_id='task5'\n    )\n    task6=EmptyOperator(\n        task_id='task6'\n    )\n    task7=EmptyOperator(\n        task_id='task7'\n    )\n    task8=EmptyOperator(\n        task_id='task8'\n    )\n  \n  task1 >> [task2, task3] >> task4\n  task5 >> task4\n  [task4, task7] >> task6 >> task8\n\n```\n\n### 함수 사용하기\n\n* [Reference: Airflow Official Document](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)\n  * Content/Core Concepts/DAGs 참고\n  * DAGs에 대한 숙련도가 올라가면 이 링크를 참고하면 매우 유용\n    * DAG을 어떤 상황에서 어떻게 짜야하는지에 대한 guidance가 자세히 적혀 있음\n    * 예를 들어, dag을 생성하는 방법 (dag declaration)에는 with 문을 사용하는 방법과 standard constructor (표준 생성자)를 사용하는 방법이 있음\n      1. with statement\n      ```markdown\n      import datetime\n\n      from airflow import DAG\n      from airflow.operators.empty import EmptyOperator\n      \n      with DAG(\n          dag_id=\"my_dag_name\",\n          start_date=datetime.datetime(2021, 1, 1),\n          schedule=\"@daily\",\n      ):\n      EmptyOperator(task_id=\"task\")\n      ```\n      2. standard constructor (class)\n      ```markdown\n      import datetime\n\n      from airflow import DAG\n      from airflow.operators.empty import EmptyOperator\n      \n      #class 생성\n      my_dag = DAG( \n          dag_id=\"my_dag_name\",\n          start_date=datetime.datetime(2021, 1, 1),\n          schedule=\"@daily\",\n      )\n      EmptyOperator(task_id=\"task\", dag=my_dag)\n      ```\n      3. python의 decorator기능 활용 (dag decorator to turn a function into a DAG generator)\n      ```markdown\n      import datetime\n\n      from airflow.decorators import dag\n      from airflow.operators.empty import EmptyOperator\n      \n      \n      @dag(start_date=datetime.datetime(2021, 1, 1), schedule=\"@daily\")\n      def generate_dag():\n          EmptyOperator(task_id=\"task\")\n      \n      \n      generate_dag()\n      ```\n* task dependencies 설정을 위한 emplicit methods. \n  * `set_upstream` and `set_downstream`\n  ```markdown\n  first_task.set_downstream(second_task, third_task)\n  third_task.set_upstream(fourth_task)\n  ```\n  * `cross_downstream`\n\n  ```markdown\n  from airflow.models.baseoperator import cross_downstream\n\n  #Replaces\n  #[op1, op2] >> op3\n  #[op1, op2] >> op4\n  cross_downstream([op1, op2], [op3, op4])\n  ```\n  * `chain`\n  ```markdown\n  from airflow.models.baseoperator import chain\n\n  #Replaces op1 >> op2 >> op3 >> op4\n  chain(op1, op2, op3, op4)\n\n  #You can also do it dynamically\n  chain(*[EmptyOperator(task_id='op' + i) for i in range(1, 6)])\n\n  #or\n\n  from airflow.models.baseoperator import chain\n\n  #Replaces\n  #op1 >> op2 >> op4 >> op6\n  #op1 >> op3 >> op5 >> op6\n  chain(op1, [op2, op3], [op4, op5], op6)\n  ```\n\n\n# External Customized Script Operation\n\n* 외부 script file such as `*.py` and `*.sh` 은 docker가 인식할 수 있도록 docker의 plugins directory안에 넣어줘야 실행된다.\n\n## What is Shell Script ?\n\n* Unix/Linux Shell 명령어로 적혀진 파일로 인터프리터에 의해 한 줄씩 처리된다.\n  * interpreter: CPU가 programming 언어를 처리하는데 크게 compiling 방식과 interpreting 방식 2가지 방식이 있다.\n    * compiling\n      * programming language를 목적 코드인 2진수로 처리한다음 읽음\n      * compile 할 때 연산 시간은 다소 소요되지만 한 번 compile 된 script는 실행 속도가 매우 빠름\n      * C, Java\n    * interpreting: compiling없이 한줄씩 읽는 방식\n      * compiling방식에 비해 실행 속도가 느림\n      * python, shell\n* bashOperator를 이용하여 shell script 처리\n* Echo, mkdir, cd, cp, tar, touch 등의 기본적인 쉘 명령어를 입력하여 작성하며 변수를 입력받거나 For 문, if 문 그리고 함수도 사용 가능\n* 확장자가 없어도 동작하지만 주로 파일명에 .sh 확장자를 붙인다.\n\n## Why to Need Shell Script?\n\n* bashOperator를 이용하다면 bashOperator안에 shell 명령어들을 써서 넣어도 동작은 하지만\n* 쉘 명령어를 이용하여 **복잡한 로직을 처리하는 경우** shell script를 이용하는 것이 좋다\n  * 예를들어, sftp (source sever)를 통해 csv나 json같은 파일을 받은 후 전처리하여 DB에 Insert & tar.gz으로 압축하고 싶을때, 이렇게 복잡한 tasks를 bashOperator에 모두 기입하기 보다는 script를 짜서 bashOperator에서 호출하는 방식이 가독성이나 유지보수 측면에서 더 효율적이다.\n* **쉘 명령어 재사용을 위한 경우**\n  * 위의 예시를 server 100대에 대하여 반복 수행할 때 logic이 같으면 shell script를 100번 호출하는 것이 더 간편\n  * sftp: 접속할 때 IP, Port, account, pw 가 필요한데 이런 것을 변수화 시키고 DB전처리 로직을 shell script에 짜 놓으면 됨.\n\n## Worker 컨테이너가 외부 스크립트(shell)를 수행하려면?\n\n* 문제점\n  * 컨테이너는 외부의 파일을 인식할 수 없다. shell script를 wsl directory 어딘가에 넣으면 container가 인식을 못함.\n  * 컨테이너 안에 파일을 만들어주면 컨테이너 재시작시 파일이 사라진다. docker에서 이미지를 띄우는 것을 container를 만들었다라고 하는데 container 재 실행시 초기화 되어 실행된다. (docker의 특징). 그래서 컨테이너 안에 shell script 파일 넣어도 재시작시 삭제가 됨.\n* 해결방법\n\n  ![](../../../../../images/airflow/worker-container-shell-operation.PNG)\n\n  * 빨간 네모박스의 plugins에 shell script를 저장한다. airflow document에서는 customized python and shell script를 plugins에 저장하는 것을 권장\n* example\n\n```markdown\ncd github-repository/plugins/shell\nvi select_fruit.sh #i 누르면 편집가능하고 편집 후 esc+wq! 입력하고 enter치면 저장하고 나감\nchmod +x select_fruit.sh #실행 권한을 부여\n./select_fruit.sh kmkim # ./test2.sh 는 test2.sh을 실행한다는 의미 출력물: kmkim 출력됨\ngit add -A\ngit commit -m \"shell script example\"\ngit push\n```\n\n```select_fruit.sh\n# echo $1 #첫 번째 인수 출력\n\nFRUIT=$1\nif [ $FRUIT == APPLE ]; then\n  echo \"You selected Apple!\"\nelif [ $FRUIT == ORANGE ]; then\n  echo \"You selected Orange!\"\nelif [ $FRUIT == Grape ]; then\n  echo \"You selected Grape!\"\nelse \n  echo \"You selected other Fruit!\"\nfi\n```\n\n* container에서 github repository에 있는 plugins/shell에 있는 shell script 인식하게 하기\n  * `vi docker-compose.yaml` 에서 67line 수정\n    ```markdown\n    volumes:\n      - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n      - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n      - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n      - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n    ```\n\n* select_fruit.sh 실행 권한 부여\n\n아래와 같이 6번의 task 수행 실패가 발생했는데 처음엔 volumne의 path 설정이 잘못 됐는지 알고 계속 `docker-compose.yaml`을 살펴봤다. 하지만 이상이 없는 것을 확인하고 task의 log를 확인해 봤는데 다음과 같은 error가 뜬것을 확인할 수 있었다.\n\n![execution error](../../../../../images/airflow/external_script_operation.PNG)\n\n`/bin/bash: line 1: /opt/***/plugins/shell/select_fruit.sh: Permission denied`\n\n이럴 땐 다음과 같이 실행권한을 부여하게 되면 해결된다.\n\n`(airflow) kmkim@K100230201051:~/airflow/plugins/shell$ chmod +x select_fruit.sh`\n\n# Email Operator \n\n* 이메일 전송해주는 오퍼레이터\n  ```markdown\n  email_t1 = EmailOperator(\n    task_id='email_t1',\n    to='hjkim_sun@naver.com',\n    subject='Airflow 처리결과',\n    html_content='정상 처리되었습니다.'\n  )\n  ```\n* 구글 메일 서버 사용\n\n\n## Presetting \n\n### Google Settings\n\n* 이메일 전송을 위해 사전 셋팅 작업 필요(Google)\n  * google mail server사용\n  * gmail >> settings(설정) >> See all settings (모든 설정 보기) >> Forwarding and POP/IMAP (전달 및 POP/IMAP) >> IMAP access (IMAP 접근): Enable IMAP (IMAP  사용)\n  * Manage Your Google Acccount (구글 계정 관리) >> Security (보안) >> 2-Step Verification (2단계 인증) >> App Passwords: 앱비밀번호 setting >> select app: Mail , Select device: Windows Computer >> Generate app pasword message window popped up\n\n### Airflow Settings\n\n* 사전 설정 작업 (airflow)\n  * docker-compose.yaml 편집 (environment 항목에 추가)\n  ```markdown \n  # 띄어쓰기 주의\n  AIRFLOW__SMTP__SMTP_HOST: 'smtp.gmail.com'  \n  AIRFLOW__SMTP__SMTP_USER: '{gmail 계정}'\n  AIRFLOW__SMTP__SMTP_PASSWORD: '{앱비밀번호}'\n  AIRFLOW__SMTP__SMTP_PORT: 587\n  AIRFLOW__SMTP__SMTP_MAIL_FROM: '{gmail 계정}' # 이메일을 누가 보내는 것으로 할건지 정함\n  ```\n\n## EmailOperator 작성\n\n```markdown\n\nfrom airflow import DAG\nimport pendulum\nimport datetime\nfrom airflow.operators.email import EmailOperator\n\nwith DAG(\n    dag_id=\"dags_email_operator\",\n    schedule=\"0 8 1 * *\", #montly batch: 매월 1일 08:00에 시작\n    start_date=pendulum.datetime(2023, 6, 13, tz=\"Asia/Seoul\"),\n    catchup=False\n) as dag:\n    sending_email_task=EmailOperator(\n        task_id='sending_email_task',\n        to='sdf@naver.com',\n        cc=['sdf2@gmail.com', 'sdf3@gmail.com'],\n        subject='Airflow Test',\n        html_content= \"\"\"\n            this is a test for airflow.<br/><br/>\n            \n            {{ ds }}<br/>\n        \"\"\"\n    )\n\n```\n \n:::\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n::: {#English .tab-pane .fade role=\"tabpanel\" aria-labelledby=\"English-tab\"}\n\n:::\n\n\n</div>\n\n# Go to Blog Content List\n\n[Blog Content List](../../content_list.qmd)  \n[Engineering Content List](../../Engineering/guide_map/index.qmd)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"03.operator_basic.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"},"utterances":{"repo":"./docs/comments"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Operator Baisc (Bash Operator)","subtitle":"Basic Operator(Bash Operator), Cron Scheduling, Task Dependencies(Connection), External Script File Operation, Email Operator","description":"template\n","categories":["Engineering"],"author":"Kwangmin Kim","date":"05/01/2023","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}