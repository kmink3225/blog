{"title":"Basic Vector(2) - Vector Norm and Dot Product","markdown":{"yaml":{"title":"Basic Vector(2) - Vector Norm and Dot Product","subtitle":"Norm, Unit Vector, Euclidean Distance, Manhttan Distance Inner Product, Dot Product","description":"Basic Linear Algebra\n","categories":["Mathematics"],"author":"Kwangmin Kim","date":"03/30/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"execute":{"echo":false},"draft":false},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n::: {#Korean .tab-pane .fade .show .active role=\"tabpanel\" aria-labelledby=\"Korean-tab\"}\n\n\n## Norm\n\n벡터 $\\mathbf{x}$ 의 norm은 벡터의 크기 또는 길이를 나타내는 0 이상의 스칼라 값이다. norm은 $||\\mathbf{x}||$ 로 표기되며 다음과 같은 성질을 만족한다.\n\n간단한 예를 들면, 벡터 $\\mathbf{x}=\\begin{bmatrix}1 \\ -2 \\ 2\\end{bmatrix}$ 가 주어졌다고 하자. 우리는 다음과 같이 유클리드 norm을 구할 수 있다:\n\n$$\n||\\mathbf x||=\\sqrt{1^2+(-2)^2+2^2}=\\sqrt{9}=3\n$$\n\n따라서, the norm of $\\mathbf{x}$ 은 3 이다.\n\n## Properties\n\n* Non-negativity (비음수성): $||\\mathbf{x}||\\geq 0$, 등호가 성립하는 경우는 $\\mathbf{x}=\\mathbf{0}$ 인 경우\n* Definiteness (명확성): 벡터 $\\mathbf{v}$ 의 norm이 0인 경우에만 벡터 자체가 영 벡터인 경우입니다:\n  $$\n  \\|\\mathbf{v}\\| = 0 \\text{ if and only if } \\mathbf{v} = \\mathbf{0}\n  $$\n* 스칼라 곱: 벡터 $\\mathbf{v}$ 의 스칼라 배수의 norm은 스칼라의 절댓값에 벡터의 노름을 곱한 것과 같다.\n  $$\n  \\|c\\mathbf{v}\\| = |c|\\|\\mathbf{v}\\|\n  $$\n* Homogeneity (동차성): 임의의 스칼라 $\\alpha$ 에 대해 $||\\alpha\\mathbf{x}||=|\\alpha| ||\\mathbf{x}||$ 이다.\n* Triangle Inequality (삼각부등식): $||\\mathbf{x}+\\mathbf{y}||\\leq ||\\mathbf{x}||+||\\mathbf{y}||$.\n\n## Norm Types\n\n여러 종류의 norms이 있다:\n\n\n* Manhattan Norm or Absolute Norm or $l_1$-norm\n$$\n\\begin{equation*}\n||\\mathbf{x}||_{l_1} = \\sum_{i=1}^{n} |x_i|\n\\end{equation*}\n$$\n\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, -2, 3]$, $||\\mathbf{x}||_{l_1} = |1| + |-2| + |3| = 6$.\n\n* Euclidean Norm or $l_2$-norm\n$$\n\\begin{equation*}\n||\\mathbf{x}||_{l_2} = \\sqrt{\\sum_{i=1}^{n} x_i^2}\n\\end{equation*}\n$$\n\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, 2, 3]$, $||\\mathbf{x}||_{l_2} = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14}$.\n\n![$l_2$-norm](images/chap02_05.PNG)\n\n* p-norm($l_2$-norm) \n\nFor $p \\geq 1$, \n$$\n\\begin{equation*}\n||\\mathbf{x}||_p = (\\sum_{i=1}^n |x_i|^p)^{\\frac{1}{p}}\n\\end{equation*}\n$$\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, 2, 3]$, $||\\mathbf{x}||_{l_p} = \\sqrt{1^p + 2^p + 3^p}$.\n\n* Maximum Norm\n$$\n\\begin{equation*}\n||\\mathbf{x}||_{\\infty} = \\max_{1 \\leq i \\leq n} |x_i|\n\\end{equation*}\n$$\n\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, -2, 3]$, $||\\mathbf{x}||_{\\infty} = \\max{(1, |-2|, 3)} = 3$.\n\n![$l_1$-norm vs $l_2$-norm vs $\\max$-norm](images/chap02_06.PNG)\n\n* Frobenius Norm:\n$$\n\\begin{equation*}\n||\\mathbf{a}||_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^2}\n\\end{equation*}\n$$\nwhere $\\mathbf{A}$ is an $m \\times n$ matrix.\nExample: For $\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$, $||\\mathbf{A}||_{F} = \\sqrt{1^2 + 2^2 + 3^2 + 4^2} = \\sqrt{30}$.\n\n## Unit Vector\n\n단위 벡터는 크기가 1인 벡터로 비영 벡터 $\\mathbf{v}$ 를 그 벡터의 크기 $||\\mathbf{v}||$ 로 나누어서 얻을 수 있다.\n\n$$\n\\begin{equation*}\n  \\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{||\\mathbf{v}||}\n\\end{equation*}\n$$\n\n여기서 $\\mathbf{\\hat{v}}$는 $\\mathbf{v}$ 의 방향을 나타내는 단위 벡터이다.\n\n단위 벡터는 벡터의 크기에 관심이 없이 방향에 초점을 맞출 때 사용할 수 있다.\n\n예를 들어, $\\mathbf{v} = \\begin{bmatrix} 1 \\ 2 \\end{bmatrix}$ 를 $\\mathbb{R}^2$ 의 비영 벡터라고 할 때 $\\mathbf{v}$ 의 크기는 $||\\mathbf{v}|| = \\sqrt{1^2 + 2^2} = \\sqrt{5}$ 이다. 따라서 $\\mathbf{v}$ 의 방향을 나타내는 단위 벡터는 다음과 같다:\n\n$$\n\\begin{equation*}\n\\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{||\\mathbf{v}||} = \\frac{1}{\\sqrt{5}}\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{bmatrix}\n\\end{equation*}\n$$\n\n따라서 $\\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\ \\frac{2}{\\sqrt{5}} \\end{bmatrix}$ 는 $\\mathbf{v}$ 의 방향을 나타내는 단위 벡터에디.\n\n### Properteis\n\n1. Normalization (정규화): $\\|\\mathbf{u}\\| = 1$\n2. Direction: 단위 벡터는 공간에서 방향을 나타낸다.\n3. Scaling: 단위 벡터에 스칼라를 곱해도 방향은 변하지 않지만 크기는 변할 수 있다.\n4. Orthogonality: 서로 다른 방향의 단위 벡터는 서로 직교(수직)한다.\n\n### Standard Unit Vectors\n\n표준 단위 벡터는 $\\mathbf{e}_i$ 로 표기되며, 여기서 $i$ 는 좌표축을 나타낸다. 예를 들어, 2차원 공간에서 $\\mathbf{e}_1$ 은 $x$ 축을 따라 단위 벡터를 나타내고, $\\mathbf{e}_2$ 는 $y$ 축을 따라 단위 벡터를 나타낸다.\n\n## Distance\n\n두 벡터 사이의 거리는 유클리드 거리(Euclidean distance) 또는 맨하탄 거리(Manhattan distance)와 같은 거리 측정 방법을 사용하여 계산할 수 있다.\n\n* Euclidean Distance:\n\n길이가 n인 두 벡터 $\\mathbf{v}$ 와 $\\mathbf{w}$ 사이의 유클리드 거리는 다음 공식을 사용하여 계산할 수 있다:\n\n$$\n\\begin{aligned}\n  \\text{distance}(\\mathbf{v}, \\mathbf{w}) &= d(\\mathbf{v},\\mathbf{w})=||\\mathbf{v}-\\mathbf{w}||= \\sqrt{\\sum_{i=1}^{n} (v_i - w_i)^2}\\\\\n\\end{aligned}\n$$\n\n이 공식은 $\\mathbf{v}$와 $\\mathbf{w}$ 의 해당 요소들 간의 차이를 제곱하여 합한 값의 제곱근을 계산한다. 이는 다차원 공간에서 두 벡터 사이의 직선 거리를 측정한다.\n\n```{python}\n#| echo: fenced\n\nv = np.array([1, 2, 3])\nw = np.array([4, 5, 6])\n\ndistance = np.linalg.norm(v - w)\nprint(distance)\n\n```\n\n* Manhattan Distance:\n\n맨하탄 거리, 또는 시티 블록 거리 또는 $L_1$ 거리로 알려진, 길이가 n인 두 벡터 $\\mathbf{v}$ 와 $\\mathbf{w}$ 사이의 맨하탄 거리는 다음 공식을 사용하여 계산할 수 있다:\n\n$$\n\\text{distance}(\\mathbf{v}, \\mathbf{w}) = \\sum_{i=1}^{n} |v_i - w_i|\n$$\n\n이 공식은 $\\mathbf{v}$와 $\\mathbf{w}$ 의 해당 요소들 간의 절댓값 차이의 합을 계산한다. 이는 두 벡터 사이의 거리를 각 차원별로 더하여 마치 격자 모양 도시의 도로 블록을 따라 이동하는 것과 같이 측정한다.\n\n```{python}\n#| echo: fenced\n\ndistance = np.sum(np.abs(v - w))\nprint(distance)\n```\n\n## Dot Product\n\nDot product is also known as scalar product or inner product.\n\n두 벡터의 dot product은 해당 요소들의 곱의 합으로 계산된다. 만약 $\\mathbf{a}$ 와 $\\mathbf{b}$ 가 같은 차원의 두 벡터라면, 그들의 도트 곱 $c = \\mathbf{a} \\cdot \\mathbf{b}$ 은 다음 공식에 의해 나타내어지는 스칼라이다:\n\n$$\n\\begin{align*}\n  c&=\\textbf{a}\\cdot \\textbf{b}\\\\\n  &= \\sum_{i=1}^{n}a_ib_i\n\\end{align*}\n$$\n\n여기서 $a_i$ 와 $b_i$ 는 각 벡터의 해당 요소이다. dot product 은 두 벡터 사이의 유사성, 직교성, 또는 벡터의 방향과 크기에 대한 정보를 제공하는 데 사용될 수 있다.\n\n* Dot product은 두 벡터 간의 유사성을 측정하는 데 사용될 수 있다.\n* 두 벡터 $\\mathbf{a} = [a_1, a_2, \\cdots a_n]$ , $\\mathbf{b} = [b_1, b_2, \\cdots b_n]$ 에 대해, dot product는 다음과 같이 정의된다:\n$$\n\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}^{T} \\mathbf{b} = ||\\mathbf{a}||\\text{ } ||\\mathbf{b}|| \\cos \\theta \n$$\n\n* 두 벡터가 직교할 때, $\\cos 90^{\\circ} = 0$ 이므로, 두 벡터의 유사성은 0 이다.\n* 유클리드 공간에서 도트 곱은 종종 내적(inner product)이라고 불린다.\n\n### Properties\n\n* Commutativity: $\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{w} \\cdot \\mathbf{v}$\n* Distributivity over vector addition: $\\mathbf{v} \\cdot (\\mathbf{w} + \\mathbf{u}) = \\mathbf{v} \\cdot \\mathbf{w} + \\mathbf{v} \\cdot \\mathbf{u}$\n* Scalar associativity: $(c \\mathbf{v}) \\cdot \\mathbf{w} = c (\\mathbf{v} \\cdot \\mathbf{w}) = \\mathbf{v} \\cdot (c \\mathbf{w})$\n* Linearity: $(c \\mathbf{v} + d \\mathbf{w}) \\cdot \\mathbf{u} = c (\\mathbf{v} \\cdot \\mathbf{u}) + d (\\mathbf{w} \\cdot \\mathbf{u})$\n* Orthogonality: $\\mathbf{v} \\cdot \\mathbf{w} = 0 \\text{ if and only if } \\mathbf{v} \\perp \\mathbf{w}$\n* simmilarity\n\n  ::: {.callout-note}\n  ### the Law of Cosines\n\n  선형 대수학에서의 제 2 코사인 법칙은 dot product을 벡터의 크기와 그들 사이의 각도와 연관 시킨다. 이는 코사인 법칙(Cosine Law) 또는 코사인의 법칙이라고도 알려져 있다.\n  \n  $$\n  \\begin{align*}\n    \\cos\\theta &= \\frac{b^2+c^2-a^2}{2bc} \\rightarrow \n    \\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\cos\\theta\n  \\end{align*}\n  $$\n  \n  :::\n\n* The geometric meaning of the dot product \n  * $\\mathbf{v}_1=(x_1,y_1), \\mathbf{v}_2=(x_2,y_2), \\mathbf{v}_2-\\mathbf{v}_1=(x_2-x_1,y_2-y_1)$ \n\n  $$\n  \\begin{aligned}\n    \\cos\\theta&=\\frac{||\\mathbf{v}_2||^2+||\\mathbf{v}_1||^2-||\\mathbf{v}_2-\\mathbf{v}_1||}{2||\\mathbf{v}_2||||\\mathbf{v}_1||} \\\\\n    ||\\mathbf{v}||&=\\sqrt{\\mathbf{v}\\cdot \\mathbf{v}}\\\\ \n    ||\\mathbf{v}_2||^2+||\\mathbf{v}_1||^2-||\\mathbf{v}_2-\\mathbf{v}_1||&=(\\sqrt{\\mathbf{v}_2\\cdot \\mathbf{v}_2})^2+(\\sqrt{\\mathbf{v}_1\\cdot \\mathbf{v}_1})^2-(\\sqrt{(\\mathbf{v}_2-\\mathbf{v}_1)\\cdot (\\mathbf{v}_2-\\mathbf{v}_1)})^2\\\\\n    &=\\mathbf{v}_2\\cdot \\mathbf{v}_2+\\mathbf{v}_1\\cdot \\mathbf{v}_1-(\\mathbf{v}_2-\\mathbf{v}_1)\\cdot (\\mathbf{v}_2-\\mathbf{v}_1)\\\\\n    &=\\mathbf{v}_2\\cdot \\mathbf{v}_2+\\mathbf{v}_1\\cdot \\mathbf{v}_1-\\mathbf{v}_2\\cdot \\mathbf{v}_2+2\\mathbf{v}_1\\cdot \\mathbf{v}_2-\\mathbf{v}_1\\cdot \\mathbf{v}_1\\\\\n    &=2\\mathbf{v}_1\\cdot \\mathbf{v}_2 \\\\\n    \\cos\\theta&=\\frac{\\mathbf{v}_1\\cdot \\mathbf{v}_2}{||\\mathbf{v}_2||||\\mathbf{v}_1||}\\\\\n    \\mathbf{v}_1\\cdot \\mathbf{v}_2&=||\\mathbf{v}_1||||\\mathbf{v}_2||\\cos\\theta\n  \\end{aligned}\n  $$\n\n    * the dot product $\\ge 0$ if $0\\le \\theta\\le \\frac{\\pi}{2}$\n    * the dot product $< 0$ if $\\frac{\\pi}{2}< \\theta\\le \\pi$\n    * dot product의 기하학적 해석은 한 벡터를 다른 벡터 위에 투영하는 것을 측정한다는 것이다. dot product이 양수인 경우, 벡터는 비슷한 방향을 가리키고 있는 것이며, 음수인 경우에는 반대 방향을 가리키고 있다는 것을 의미한다. dot product의 크기는 벡터들의 평행성, 일렬성 또는 유사성을 측정하는 지표 역할을 한다.\n  * Projection\n    * Let $\\mathbf{v}_1$ and $\\mathbf{v}_2$ be two vectors. The projection of $\\mathbf{v}_1$ onto $\\mathbf{v}_2$ is defined as the vector, $\\mathbf{w}$ :\n\n    $$\n    \\begin{aligned}\n      \\mathbf{v_1}\\cdot \\mathbf{v_2}&=||\\mathbf{w}||||\\mathbf{v_2}||\\\\\n      \\mathbf w&=\\text{proj}_{\\mathbf v_2}\\mathbf v_1\\\\ \n      &=||\\mathbf{w}||\\mathbf{u}_\\mathbf{w} \\quad (\\because \\text{the univt vector of } \\mathbf{w} = \\mathbf{u}_w)\\\\\n      \\text{the unit vector of } \\mathbf{w} &= \\frac{\\mathbf{v}_2}{||\\mathbf{v}_2||}  \\quad(\\because \\text{the direction of } \\mathbf{w} = \\text{the direction of } \\mathbf{v_2}) \\\\\n      \\mathbf w&=\\text{proj}_{\\mathbf v_2}\\mathbf v_1\\\\ \n      &=\\frac{\\mathbf v_1 \\cdot \\mathbf v_2}{||\\mathbf v_2||} \\frac{\\mathbf v_2}{||\\mathbf v_2||} \\\\\n      &=\\frac{\\mathbf v_1 \\cdot \\mathbf v_2}{||\\mathbf v_2||^2} \\mathbf v_2 \\\\\n      &=\\frac{\\mathbf v_1 \\cdot \\mathbf v_2}{\\mathbf{v}_2\\cdot \\mathbf{v}_2}\\mathbf{v}_2 \\quad (\\because ||\\mathbf v_2||=\\sqrt{\\mathbf{v}_2\\cdot \\mathbf{v}_2})\n    \\end{aligned}\n    $$\n\n    * 이 투영된 벡터는 $\\mathbf{v}_2$ 에 의해 생성된 직선 상에서 $\\mathbf{v}_1$ 에 가장 가까운 벡터이다. 이는 $\\mathbf{v}_2$ 와 평행하며, 그 직선 상에서 $\\mathbf{v}_1$ 을 가장 근사하는 가능한 가까운 벡터를 의미한다.\n    * [Reference: Read This Article with Interactive Visualization - Projection](http://immersivemath.com/ila/ch03_dotproduct/ch03.html#auto_label_107)\n* Cauchy-Schwarz Inequality\n\n$$\n\\begin{aligned}\n(ax+by)^2 &\\le (a^2+b^2)(x^2+y^2)\\\\\n|\\langle \\mathbf u,\\mathbf v\\rangle| = |\\mathbf{u}\\cdot\\mathbf{v}| &\\le ||\\mathbf{u}|| ||\\mathbf{v} || \\quad (\\text{where }\\mathbf{u}=(a,b), \\quad \\mathbf{v}=(x,y))\\\\\n&\\text{putting the absolute value is because the dot product could be negative} \\\\\n\\text{Proof)} \\mathbf{u}\\cdot\\mathbf{v}&= ||\\mathbf{u}|| ||\\mathbf{v} ||\\cos\\theta \\\\\n-1 \\le \\mathbf{u}\\cdot\\mathbf{v}&= ||\\mathbf{u}|| ||\\mathbf{v} || \\le 1 \\\\\n-||\\mathbf{u}|| ||\\mathbf{v} || \\le \\mathbf{u}\\cdot\\mathbf{v}&  \\le ||\\mathbf{u}|| ||\\mathbf{v} || \\\\\n|\\mathbf{u}\\cdot\\mathbf{v}| &  \\le ||\\mathbf{u}|| ||\\mathbf{v}||\n\\end{aligned} \n$$\n\n  * 기하학적으로, Schwarz 부등식은 한 벡터를 다른 벡터에 투영한 크기가 투영된 벡터의 길이를 초과할 수 없다는 것을 나타낸다. 다시 말해, 이는 두 벡터 간의 상관관계를 제한하고, 그들의 내적이 항상 그들의 norm의 곱보다 작거나 같도록 보장한다.\n* Triangle Inequality\n\n$$\n\\begin{aligned}\n  ||\\mathbf{u} + \\mathbf{v}|| &\\le ||\\mathbf{u}|| + ||\\mathbf{v}|| \\\\\n  ||\\mathbf{u} + \\mathbf{v}||^2 &= (\\mathbf{u}+\\mathbf{v})\\cdot(\\mathbf{u}+\\mathbf{v}) \\\\\n  &= \\mathbf{u}\\cdot\\mathbf{u}+2\\mathbf{u}\\cdot\\mathbf{v}+\\mathbf{v}\\cdot\\mathbf{v}\\\\\n  &\\le \\mathbf{u}\\cdot\\mathbf{u}+2|\\mathbf{u}\\cdot\\mathbf{v}|+\\mathbf{v}\\cdot\\mathbf{v} \\\\ &(\\because \\mathbf{u}\\cdot\\mathbf{v} \\text{ is a scalar and could be negative})\n\\end{aligned} \n$$\n\n  * 삼각 부등식은 두 벡터로 나타낸 공간에서 두 점 사이의 거리는 항상 두 벡터의 거리의 합보다 짧거나 같다는 것을 의미한다. 다시 말해, 두 점 사이의 거리를 나타내는 두 벡터보다 더 짧은 직선을 그리는 것은 불가능하다.\n\n\n```{python}\n#| echo: fenced\n\nu = [3, 4]\nv = [-1, 2]\n\n# Plot the vectors\nplt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='g')\nplt.quiver(u[0], u[1], -1, 2, angles='xy', scale_units='xy', scale=1, color='g')\nplt.quiver(0, 0, u[0]+v[0], u[1]+v[1], angles='xy', scale_units='xy', scale=1, color='b')\n\nplt.text(u[0]+0.2, u[1], 'u', fontsize=12)\nplt.text(u[0]+v[0], u[1]+v[1], 'v', fontsize=12)\nplt.text(u[0]+v[0]-0.8, u[1]+v[1], 'u+v', fontsize=12)\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xlim(-2, 7)\nplt.ylim(-2, 7)\n\nplt.plot([0, u[0], u[0]+v[0], v[0], 0], [0, u[1], u[1]+v[1], v[1], 0], 'k--')\nplt.text((u[0]+v[0])/2, (u[1]+v[1])/2+0.5, '||u+v||', fontsize=12)\nplt.show()\n```\n\n\n:::{.callout-note}\n\n#### Inner Product vs Dot Product\n\n일반적으로 내적은 두 벡터를 받아서 스칼라를 출력하는 수학적 연산이다. 내적은 첫 번째 성분에 대해 선형이고 두 번째 성분에 대해서는 켤레 선형이며,  positive-definite과 같은 특정 속성을 만족한다. \n디사 말해, 내적은 다음과 같은 속성을 만족하는 모든 벡터 $\\mathbf{x}$, $\\mathbf{y}$, $\\mathbf{z}$ 및 스칼라 $a$, $b$ 에 대해 다음과 같은 이중 선형 형식이다:\n\n* \"첫 번째 인수에 대해 선형\"은 임의의 고정된 벡터 $\\mathbf{u}$ 에 대해 $f(\\mathbf{v}) = \\langle\\mathbf{u}, \\mathbf{v}\\rangle$ 로 정의된 함수 $f$ 가 $\\mathbf{v}$ 에 대해 선형 함수임을 의미한다. 즉, 모든 스칼라 $a$, $b$ 와 벡터 $\\mathbf{x}$, $\\mathbf{y}$ 에 대해 $f(a\\mathbf{x} + b\\mathbf{y}) = af(\\mathbf{x}) + bf(\\mathbf{y})$ 이다.\n  * $\\langle a\\mathbf{x} + b\\mathbf{y}, \\mathbf{z}\\rangle = a\\langle\\mathbf{x}, \\mathbf{z}\\rangle + b\\langle\\mathbf{y}, \\mathbf{z}\\rangle$ 는 내적이 첫 번째 성분에 대해 선형이라는 것을 의미한다. 벡터를 스칼라로 곱하고 다른 벡터에 더한 결과의 내적은 각 벡터의 내적을 개별적으로 계산한 다음 더한 것과 동일하다.\n* \"두 번째 인수에 대해 켤레 선형\"은 임의의 고정된 벡터 $\\mathbf{v}$ 에 대해 $g(\\mathbf{u}) = \\langle\\mathbf{u}, \\mathbf{v}\\rangle$ 로 정의된 함수 $g$ 가 $\\mathbf{u}$ 에 대해 켤레 선형 함수임을 의미한다. 즉, 모든 스칼라 $a$, $b$ 와 벡터 $\\mathbf x$, $\\mathbf y$ 에 대해 $g(a \\mathbf x + b \\mathbf y) = \\bar{a} g(\\mathbf x) + \\bar{b} * g(\\mathbf y)$ 입니다. 여기서 $\\bar{a}$ 는 $a$ 의 켤레 복소수를 나타낸다.\n  * $\\langle \\mathbf{x}, a\\mathbf{y}, b\\mathbf{z}\\rangle = a\\langle\\mathbf{x}, \\mathbf{y}\\rangle + b\\langle\\mathbf{x}, \\mathbf{z}\\rangle$ 는 내적이 두 번째 인수에 대해 선형이지만 복소 수를 켤레로 취한다는 것을 의미한다. 벡터를 스칼라로 곱하고 다른 벡터에 더한 결과의 내적은 각 벡터의 내적을 개별적으로 계산한 다음 두 번째 벡터를 복소수로 켤레한 후 더한 것과 동일하다.\n* \"대칭성\"은 $\\langle \\mathbf{x},\\mathbf{y}\\rangle= \\langle \\mathbf{y},\\mathbf{x}\\rangle$ 을 의미한다. 내적을 계산할 때 벡터의 순서는 중요하지 않다.\n* \"양의 정부호성\"은 어떤 영벡터도 아닌 벡터 $\\mathbf{v}$ 에 대해 내적 $\\langle\\mathbf{u}, \\mathbf{v}\\rangle$ 가 양의 실수임을 의미한다. 다시 말해, 벡터 자체와의 내적은 항상 양수이며, 벡터가 영벡터가 아닌 경우에만 0이 아니다.\n$\\langle \\mathbf{x},\\mathbf{x}\\rangle\\ge 0$ 이며, $\\langle \\mathbf{x},\\mathbf{x}\\rangle=0$ 은 $\\mathbf{x}=0$ 인 경우에만 성립한다.\n:::\n\n:::\n\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n::: {#English .tab-pane .fade role=\"tabpanel\" aria-labelledby=\"English-tab\"}\n\n# Introduction\n\n## Norm\n\nThe norm of a vector $\\mathbf{x}$ is a non-negative scalar value that represents **the size or length** of the vector. The norm is denoted by $||\\mathbf{x}||$ and satisfies the following properties:\n\n### Properties\n\n* Non-negativity: $||\\mathbf{x}||\\geq 0$, with equality if and only if $\\mathbf{x}=\\mathbf{0}$.\n* Definiteness: The norm of a vector $\\mathbf{v}$ is zero if and only if the vector itself is the zero vector:\n  $$\n  \\|\\mathbf{v}\\| = 0 \\text{ if and only if } \\mathbf{v} = \\mathbf{0}\n  $$\n* Scalar Multiplication: The norm of a scalar multiple of a vector $\\mathbf{v}$ is equal to the absolute value of the scalar multiplied by the norm of the vector:\n  $$\n  \\|c\\mathbf{v}\\| = |c|\\|\\mathbf{v}\\|\n  $$\n* Homogeneity: $||\\alpha\\mathbf{x}||=|\\alpha| \\quad ||\\mathbf{x}||$ for any scalar $\\alpha$.\n* Triangle Inequality: $||\\mathbf{x}+\\mathbf{y}||\\leq ||\\mathbf{x}||+||\\mathbf{y}||$.\n\n\nSuppose we have a vector $\\mathbf{x}=\\begin{bmatrix}1 \\\\ -2 \\\\ 2\\end{bmatrix}$. We can find its Euclidean norm as follows:\n\n$$\n||\\mathbf x||=\\sqrt{1^2+(-2)^2+2^2}=\\sqrt{9}=3\n$$\n\nTherefore, the norm of $\\mathbf{x}$ is 3.\n\n### Norm Types\n\nThere are several types of norms:\n\n* Manhattan Norm or Absolute Norm or $l_1$-norm\n$$\n\\begin{equation*}\n||\\mathbf{x}||_{l_1} = \\sum_{i=1}^{n} |x_i|\n\\end{equation*}\n$$\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, -2, 3]$, $||\\mathbf{x}||_{l_1} = |1| + |-2| + |3| = 6$.\n\n\n* Euclidean Norm or $l_2$-norm\n$$\n\\begin{equation*}\n||\\mathbf{x}||_{l_2} = \\sqrt{\\sum_{i=1}^{n} x_i^2}\n\\end{equation*}\n$$\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, 2, 3]$, $||\\mathbf{x}||_{l_2} = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14}$.\n\n![$l_2$-norm](images/chap02_05.PNG)\n\n* p-norm($l_2$-norm) \n\nFor $p \\geq 1$, \n$$\n\\begin{equation*}\n||\\mathbf{x}||_p = (\\sum_{i=1}^n |x_i|^p)^{\\frac{1}{p}}\n\\end{equation*}\n$$\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, 2, 3]$, $||\\mathbf{x}||_{l_p} = \\sqrt{1^p + 2^p + 3^p}$.\n\n* Maximum Norm\n$$\n\\begin{equation*}\n||\\mathbf{x}||_{\\infty} = \\max_{1 \\leq i \\leq n} |x_i|\n\\end{equation*}\n$$\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, -2, 3]$, $||\\mathbf{x}||_{\\infty} = \\max{(1, |-2|, 3)} = 3$.\n\n![$l_1$-norm vs $l_2$-norm vs $\\max$-norm](images/chap02_06.PNG)\n\n* Frobenius Norm:\n$$\n\\begin{equation*}\n||\\mathbf{a}||_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^2}\n\\end{equation*}\n$$\nwhere $\\mathbf{A}$ is an $m \\times n$ matrix.\nExample: For $\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$, $||\\mathbf{A}||_{F} = \\sqrt{1^2 + 2^2 + 3^2 + 4^2} = \\sqrt{30}$.\n\n## Unit Vector\n\nA unit vector is a vector that has a magnitude of 1. A unit vector can be obtained by dividing a non-zero vector $\\mathbf{v}$ by its magnitude $||\\mathbf{v}||$, \n\n$$\n\\begin{equation*}\n  \\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{||\\mathbf{v}||}\n\\end{equation*}\n$$\n\nwhere $\\mathbf{\\hat{v}}$ is the unit vector in the direction of $\\mathbf{v}$.\n\nA unit vector can be used to focus on a direction with no interest in the size of the vector.\n\nFor example, let $\\mathbf{v} = \\begin{bmatrix} 1 \\ 2 \\end{bmatrix}$ be a non-zero vector in $\\mathbb{R}^2$. The magnitude of $\\mathbf{v}$ is $||\\mathbf{v}|| = \\sqrt{1^2 + 2^2} = \\sqrt{5}$. Therefore, a unit vector in the direction of $\\mathbf{v}$ is:\n\n$$\n\\begin{equation*}\n\\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{||\\mathbf{v}||} = \\frac{1}{\\sqrt{5}}\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{bmatrix}\n\\end{equation*}\n$$\n\nThus, $\\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{bmatrix}$ is a unit vector in the direction of $\\mathbf{v}$.\n\n### Properteis\n\n1. Normalization: $\\|\\mathbf{u}\\| = 1$\n2. Direction: A unit vector represents a direction in space.\n3. Scaling: Multiplying a unit vector by a scalar does not change its direction, but it may change its magnitude.\n4. Orthogonality: Unit vectors in different directions are orthogonal (perpendicular) to each other.\n\n### Standard Unit Vectors\n\nA standard unit vector is denoted as $\\mathbf{e}_i$, where $i$ represents the coordinate axis. For example, in 2D space, we have $\\mathbf{e}_1$ representing the unit vector along the $x$-axis and $\\mathbf{e}_2$ representing the unit vector along the $y$-axis.\n\n\n## Distance\n\nThe distance between two vectors can be computed using a distance metric, such as the Euclidean distance or the Manhattan distance. \n\n* Euclidean Distance:\n\nThe Euclidean distance between two vectors $\\mathbf{v}$ and $\\mathbf{w}$ of length $n$ can be calculated using the following formula:\n\n$$\n\\begin{aligned}\n  \\text{distance}(\\mathbf{v}, \\mathbf{w}) &= d(\\mathbf{v},\\mathbf{w})=||\\mathbf{v}-\\mathbf{w}||= \\sqrt{\\sum_{i=1}^{n} (v_i - w_i)^2}\\\\\n\\end{aligned}\n$$\n\n```{python}\n#| echo: fenced\n\nv = np.array([1, 2, 3])\nw = np.array([4, 5, 6])\n\ndistance = np.linalg.norm(v - w)\nprint(distance)\n\n```\n\n* Manhattan Distance:\n\nThe Manhattan distance (also known as the city block distance or L1 distance) between two vectors $\\mathbf{v}$ and $\\mathbf{w}$ of length $n$ can be calculated using the following formula:\n\n$$\n\\text{distance}(\\mathbf{v}, \\mathbf{w}) = \\sum_{i=1}^{n} |v_i - w_i|\n$$\n\n```{python}\n#| echo: fenced\n\ndistance = np.sum(np.abs(v - w))\nprint(distance)\n```\n\n## Dot Product\n\nDot product is also known as scalar product or inner product.\n\nThe dot product of two vectors is the sum of the products of their corresponding components (a.k.a inner product & scalar product). If $\\textbf{a}$ and $\\textbf{b}$ are two vectors of the same dimension, then their dot product $c = \\textbf{a} \\cdot \\textbf{b}$ is a scalar given by the formula:\n\n$$\n\\begin{align*}\n  c&=\\textbf{a}\\cdot \\textbf{b}\\\\\n  &= \\sum_{i=1}^{n}a_ib_i\n\\end{align*}\n$$\n\n* Dot product can be used to measure the similarity between two vectors.\n* For the two vectors, $\\mathbf{a} = [a_1, a_2, \\cdots a_n]$ , $\\mathbf{b} = [b_1, b_2, \\cdots b_n]$, dot product can be defined as\n$$\n\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}^{T} \\mathbf{b} = ||\\mathbf{a}||\\text{ } ||\\mathbf{b}|| \\cos \\theta \n$$\n* When two vectors are orthogonal, $\\cos 90^{\\circ} = 0$, the similarity of the two vectors is 0.\n* In the Euclidean space, dot product is often called inner product (inner product is a generalization of dot product)\n\n### Properties\n\n* Commutativity: $\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{w} \\cdot \\mathbf{v}$\n* Distributivity over vector addition: $\\mathbf{v} \\cdot (\\mathbf{w} + \\mathbf{u}) = \\mathbf{v} \\cdot \\mathbf{w} + \\mathbf{v} \\cdot \\mathbf{u}$\n* Scalar associativity: $(c \\mathbf{v}) \\cdot \\mathbf{w} = c (\\mathbf{v} \\cdot \\mathbf{w}) = \\mathbf{v} \\cdot (c \\mathbf{w})$\n* Linearity: $(c \\mathbf{v} + d \\mathbf{w}) \\cdot \\mathbf{u} = c (\\mathbf{v} \\cdot \\mathbf{u}) + d (\\mathbf{w} \\cdot \\mathbf{u})$\n* Orthogonality: $\\mathbf{v} \\cdot \\mathbf{w} = 0 \\text{ if and only if } \\mathbf{v} \\perp \\mathbf{w}$\n* simmilarity\n\n  ::: {.callout-note}\n  ### the Law of Cosines\n\n  The second cosine rule in linear algebra, also known as the Law of Cosines, relates the dot product of vectors to their magnitudes and the angle between them.\n\n  $$\n  \\begin{align*}\n    \\cos\\theta &= \\frac{b^2+c^2-a^2}{2bc} \\rightarrow \n    \\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\cos\\theta\n  \\end{align*}\n  $$\n  \n  :::\n\n* The geometric meaning of the dot product \n  * $\\mathbf{v}_1=(x_1,y_1), \\mathbf{v}_2=(x_2,y_2), \\mathbf{v}_2-\\mathbf{v}_1=(x_2-x_1,y_2-y_1)$ \n\n  $$\n  \\begin{aligned}\n    \\cos\\theta&=\\frac{||\\mathbf{v}_2||^2+||\\mathbf{v}_1||^2-||\\mathbf{v}_2-\\mathbf{v}_1||}{2||\\mathbf{v}_2||||\\mathbf{v}_1||} \\\\\n    ||\\mathbf{v}||&=\\sqrt{\\mathbf{v}\\cdot \\mathbf{v}}\\\\ \n    ||\\mathbf{v}_2||^2+||\\mathbf{v}_1||^2-||\\mathbf{v}_2-\\mathbf{v}_1||&=(\\sqrt{\\mathbf{v}_2\\cdot \\mathbf{v}_2})^2+(\\sqrt{\\mathbf{v}_1\\cdot \\mathbf{v}_1})^2-(\\sqrt{(\\mathbf{v}_2-\\mathbf{v}_1)\\cdot (\\mathbf{v}_2-\\mathbf{v}_1)})^2\\\\\n    &=\\mathbf{v}_2\\cdot \\mathbf{v}_2+\\mathbf{v}_1\\cdot \\mathbf{v}_1-(\\mathbf{v}_2-\\mathbf{v}_1)\\cdot (\\mathbf{v}_2-\\mathbf{v}_1)\\\\\n    &=\\mathbf{v}_2\\cdot \\mathbf{v}_2+\\mathbf{v}_1\\cdot \\mathbf{v}_1-\\mathbf{v}_2\\cdot \\mathbf{v}_2+2\\mathbf{v}_1\\cdot \\mathbf{v}_2-\\mathbf{v}_1\\cdot \\mathbf{v}_1\\\\\n    &=2\\mathbf{v}_1\\cdot \\mathbf{v}_2 \\\\\n    \\cos\\theta&=\\frac{\\mathbf{v}_1\\cdot \\mathbf{v}_2}{||\\mathbf{v}_2||||\\mathbf{v}_1||}\\\\\n    \\mathbf{v}_1\\cdot \\mathbf{v}_2&=||\\mathbf{v}_1||||\\mathbf{v}_2||\\cos\\theta\n  \\end{aligned}\n  $$\n\n    * the dot product $\\ge 0$ if $0\\le \\theta\\le \\frac{\\pi}{2}$\n    * the dot product $< 0$ if $\\frac{\\pi}{2}< \\theta\\le \\pi$\n    * The geometric interpretation of the dot product is that it measures the *projection* of one vector onto another. When the dot product is positive, it means the vectors are pointing in a similar direction, and when it is negative, it means they are pointing in opposite directions. The magnitude of the dot product provides a measure of how *parallelness*, *aligned* or *similar* the vectors are.\n  * Projection\n    * Let $\\mathbf{v}_1$ and $\\mathbf{v}_2$ be two vectors. The projection of $\\mathbf{v}_1$ onto $\\mathbf{v}_2$ is defined as the vector, $\\mathbf{w}$ :\n\n    $$\n    \\begin{aligned}\n      \\mathbf{v_1}\\cdot \\mathbf{v_2}&=||\\mathbf{w}||||\\mathbf{v_2}||\\\\\n      \\mathbf w&=\\text{proj}_{\\mathbf v_2}\\mathbf v_1\\\\ \n      &=||\\mathbf{w}||\\mathbf{u}_\\mathbf{w} \\quad (\\because \\text{the univt vector of } \\mathbf{w} = \\mathbf{u}_w)\\\\\n      \\text{the unit vector of } \\mathbf{w} &= \\frac{\\mathbf{v}_2}{||\\mathbf{v}_2||}  \\quad(\\because \\text{the direction of } \\mathbf{w} = \\text{the direction of } \\mathbf{v_2}) \\\\\n      \\mathbf w&=\\text{proj}_{\\mathbf v_2}\\mathbf v_1\\\\ \n      &=\\frac{\\mathbf v_1 \\cdot \\mathbf v_2}{||\\mathbf v_2||} \\frac{\\mathbf v_2}{||\\mathbf v_2||} \\\\\n      &=\\frac{\\mathbf v_1 \\cdot \\mathbf v_2}{||\\mathbf v_2||^2} \\mathbf v_2 \\\\\n      &=\\frac{\\mathbf v_1 \\cdot \\mathbf v_2}{\\mathbf{v}_2\\cdot \\mathbf{v}_2}\\mathbf{v}_2 \\quad (\\because ||\\mathbf v_2||=\\sqrt{\\mathbf{v}_2\\cdot \\mathbf{v}_2})\n    \\end{aligned}\n    $$\n\n    * This projected vector is the closest vector to $\\mathbf{v}_1$ that lies on the line spanned by $\\mathbf{v}_2$. It means that a vector that is parallel to $\\mathbf{v}_2$ and is the closest possible approximation of $\\mathbf{v}_1$ along that line.\n    * [Reference: Read This Article with Interactive Visualization - Projection](http://immersivemath.com/ila/ch03_dotproduct/ch03.html#auto_label_107)\n* Cauchy-Schwarz Inequality\n\n$$\n\\begin{aligned}\n(ax+by)^2 &\\le (a^2+b^2)(x^2+y^2)\\\\\n|\\langle \\mathbf u,\\mathbf v\\rangle| = |\\mathbf{u}\\cdot\\mathbf{v}| &\\le ||\\mathbf{u}|| ||\\mathbf{v} || \\quad (\\text{where }\\mathbf{u}=(a,b), \\quad \\mathbf{v}=(x,y))\\\\\n&\\text{putting the absolute value is because the dot product could be negative} \\\\\n\\text{Proof)} \\mathbf{u}\\cdot\\mathbf{v}&= ||\\mathbf{u}|| ||\\mathbf{v} ||\\cos\\theta \\\\\n-1 \\le \\mathbf{u}\\cdot\\mathbf{v}&= ||\\mathbf{u}|| ||\\mathbf{v} || \\le 1 \\\\\n-||\\mathbf{u}|| ||\\mathbf{v} || \\le \\mathbf{u}\\cdot\\mathbf{v}&  \\le ||\\mathbf{u}|| ||\\mathbf{v} || \\\\\n|\\mathbf{u}\\cdot\\mathbf{v}| &  \\le ||\\mathbf{u}|| ||\\mathbf{v}||\n\\end{aligned} \n$$\n\n  * Geometrically, the Schwarz inequality states that the magnitude of the projection of one vector onto the other cannot exceed the length of the vector being projected. In other words, it bounds the correlation between two vectors and ensures that their inner product is always less than or equal to the product of their norms.\n* Triangle Inequality\n\n$$\n\\begin{aligned}\n  ||\\mathbf{u} + \\mathbf{v}|| &\\le ||\\mathbf{u}|| + ||\\mathbf{v}|| \\\\\n  ||\\mathbf{u} + \\mathbf{v}||^2 &= (\\mathbf{u}+\\mathbf{v})\\cdot(\\mathbf{u}+\\mathbf{v}) \\\\\n  &= \\mathbf{u}\\cdot\\mathbf{u}+2\\mathbf{u}\\cdot\\mathbf{v}+\\mathbf{v}\\cdot\\mathbf{v}\\\\\n  &\\le \\mathbf{u}\\cdot\\mathbf{u}+2|\\mathbf{u}\\cdot\\mathbf{v}|+\\mathbf{v}\\cdot\\mathbf{v} \\\\ &(\\because \\mathbf{u}\\cdot\\mathbf{v} \\text{ is a scalar and could be negative})\n\\end{aligned} \n$$\n\n  * this inequality means that the distance between two points in a space, represented by vectors, is always shorter than or equal to the sum of the distances between the two vectors. In other words, it is impossible to make a straight line from one point to another that is shorter than the distance represented by the two vectors.\n\n\n```{python}\n#| echo: fenced\n\nu = [3, 4]\nv = [-1, 2]\n\n# Plot the vectors\nplt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='g')\nplt.quiver(u[0], u[1], -1, 2, angles='xy', scale_units='xy', scale=1, color='g')\nplt.quiver(0, 0, u[0]+v[0], u[1]+v[1], angles='xy', scale_units='xy', scale=1, color='b')\n\nplt.text(u[0]+0.2, u[1], 'u', fontsize=12)\nplt.text(u[0]+v[0], u[1]+v[1], 'v', fontsize=12)\nplt.text(u[0]+v[0]-0.8, u[1]+v[1], 'u+v', fontsize=12)\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xlim(-2, 7)\nplt.ylim(-2, 7)\n\nplt.plot([0, u[0], u[0]+v[0], v[0], 0], [0, u[1], u[1]+v[1], v[1], 0], 'k--')\nplt.text((u[0]+v[0])/2, (u[1]+v[1])/2+0.5, '||u+v||', fontsize=12)\nplt.show()\n```\n\n\n:::{.callout-note}\n\n#### Inner Product vs Dot Product\n\nIn general, an inner product is a mathematical operation that takes two vectors and produces a scalar. It satisfies certain properties, such as being linear in the first argument, conjugate linear in the second argument, and positive-definite.\nIn other words, an inner product is a bilinear form that satisfies the following properties for all vectors $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z}$, and all scalars $a$ and $b$:\n\n* \"Linear in the first argument\" means that for any fixed vector $\\mathbf u$, the function $f$ defined by $f(\\mathbf v) = \\langle\\mathbf u, \\mathbf v\\rangle$ is a linear function of $\\mathbf v$, i.e., $f(a\\mathbf x + b\\mathbf y) = af(\\mathbf x) + bf(\\mathbf y)$ for any scalars $a$, $b$, and vectors $\\mathbf{x}$, $\\mathbf{y}$.\n  * $\\langle a\\mathbf{x} + b\\mathbf{y}, \\mathbf{z}\\rangle = a\\langle\\mathbf{x}, \\mathbf{z}\\rangle + b\\langle\\mathbf{y}, \\mathbf{z}\\rangle$, the inner product is linear with respect to the first argument. If we multiply a vector by a scalar and add it to another vector, the resulting inner product is the same as if we had calculated the inner product of each vector separately and then added them.\n* \"Conjugate linear in the second argument\" means that for any fixed vector $\\mathbf v$, the function $g$ defined by $g(\\mathbf u) = \\langle\\mathbf u, \\mathbf v\\rangle$ is a conjugate linear function of $\\mathbf u$, i.e., $g(a \\mathbf x + b \\mathbf y) = \\bar{a} g(\\mathbf x) + \\bar{b} * g(\\mathbf y)$ for any scalars $a$, $b$, and vectors $\\mathbf x$, $\\mathbf y$, where $\\bar{a}$ denotes the complex conjugate of $a$.\n  * $\\langle \\mathbf{x}, a\\mathbf{y}, b\\mathbf{z}\\rangle = a\\langle\\mathbf{x}, \\mathbf{y}\\rangle + b\\langle\\mathbf{x}, \\mathbf{z}\\rangle$. this property says that the inner product is linear with respect to the second argument, but with complex conjugation. If we multiply a vector by a scalar and add it to another vector, the resulting inner product is the same as if we had calculated the inner product of each vector separately, complex-conjugated the second vector, and then added them.\n* \"Symmetry\" means $\\langle \\mathbf{x},\\mathbf{y}\\rangle= \\langle \\mathbf{y},\\mathbf{x}\\rangle$\n  * the order of the vectors doesn't matter when calculating the inner product.  \n* \"Positive-definite\" means that for any nonzero vector v, the inner product $\\langle\\mathbf u, \\mathbf v\\rangle$ is a positive real number. In other words, the inner product of a vector with itself is always positive, except when the vector is the zero vector.\n  * $\\langle \\mathbf{x},\\mathbf{x}\\rangle\\ge 0, \\langle \\mathbf{x},\\mathbf{x}\\rangle=0$ only if $\\mathbf{x}=0$\n\n:::\n\n\n:::\n\n\n</div>\n","srcMarkdownNoYaml":"\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n::: {#Korean .tab-pane .fade .show .active role=\"tabpanel\" aria-labelledby=\"Korean-tab\"}\n\n# Introduction\n\n## Norm\n\n벡터 $\\mathbf{x}$ 의 norm은 벡터의 크기 또는 길이를 나타내는 0 이상의 스칼라 값이다. norm은 $||\\mathbf{x}||$ 로 표기되며 다음과 같은 성질을 만족한다.\n\n간단한 예를 들면, 벡터 $\\mathbf{x}=\\begin{bmatrix}1 \\ -2 \\ 2\\end{bmatrix}$ 가 주어졌다고 하자. 우리는 다음과 같이 유클리드 norm을 구할 수 있다:\n\n$$\n||\\mathbf x||=\\sqrt{1^2+(-2)^2+2^2}=\\sqrt{9}=3\n$$\n\n따라서, the norm of $\\mathbf{x}$ 은 3 이다.\n\n## Properties\n\n* Non-negativity (비음수성): $||\\mathbf{x}||\\geq 0$, 등호가 성립하는 경우는 $\\mathbf{x}=\\mathbf{0}$ 인 경우\n* Definiteness (명확성): 벡터 $\\mathbf{v}$ 의 norm이 0인 경우에만 벡터 자체가 영 벡터인 경우입니다:\n  $$\n  \\|\\mathbf{v}\\| = 0 \\text{ if and only if } \\mathbf{v} = \\mathbf{0}\n  $$\n* 스칼라 곱: 벡터 $\\mathbf{v}$ 의 스칼라 배수의 norm은 스칼라의 절댓값에 벡터의 노름을 곱한 것과 같다.\n  $$\n  \\|c\\mathbf{v}\\| = |c|\\|\\mathbf{v}\\|\n  $$\n* Homogeneity (동차성): 임의의 스칼라 $\\alpha$ 에 대해 $||\\alpha\\mathbf{x}||=|\\alpha| ||\\mathbf{x}||$ 이다.\n* Triangle Inequality (삼각부등식): $||\\mathbf{x}+\\mathbf{y}||\\leq ||\\mathbf{x}||+||\\mathbf{y}||$.\n\n## Norm Types\n\n여러 종류의 norms이 있다:\n\n\n* Manhattan Norm or Absolute Norm or $l_1$-norm\n$$\n\\begin{equation*}\n||\\mathbf{x}||_{l_1} = \\sum_{i=1}^{n} |x_i|\n\\end{equation*}\n$$\n\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, -2, 3]$, $||\\mathbf{x}||_{l_1} = |1| + |-2| + |3| = 6$.\n\n* Euclidean Norm or $l_2$-norm\n$$\n\\begin{equation*}\n||\\mathbf{x}||_{l_2} = \\sqrt{\\sum_{i=1}^{n} x_i^2}\n\\end{equation*}\n$$\n\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, 2, 3]$, $||\\mathbf{x}||_{l_2} = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14}$.\n\n![$l_2$-norm](images/chap02_05.PNG)\n\n* p-norm($l_2$-norm) \n\nFor $p \\geq 1$, \n$$\n\\begin{equation*}\n||\\mathbf{x}||_p = (\\sum_{i=1}^n |x_i|^p)^{\\frac{1}{p}}\n\\end{equation*}\n$$\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, 2, 3]$, $||\\mathbf{x}||_{l_p} = \\sqrt{1^p + 2^p + 3^p}$.\n\n* Maximum Norm\n$$\n\\begin{equation*}\n||\\mathbf{x}||_{\\infty} = \\max_{1 \\leq i \\leq n} |x_i|\n\\end{equation*}\n$$\n\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, -2, 3]$, $||\\mathbf{x}||_{\\infty} = \\max{(1, |-2|, 3)} = 3$.\n\n![$l_1$-norm vs $l_2$-norm vs $\\max$-norm](images/chap02_06.PNG)\n\n* Frobenius Norm:\n$$\n\\begin{equation*}\n||\\mathbf{a}||_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^2}\n\\end{equation*}\n$$\nwhere $\\mathbf{A}$ is an $m \\times n$ matrix.\nExample: For $\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$, $||\\mathbf{A}||_{F} = \\sqrt{1^2 + 2^2 + 3^2 + 4^2} = \\sqrt{30}$.\n\n## Unit Vector\n\n단위 벡터는 크기가 1인 벡터로 비영 벡터 $\\mathbf{v}$ 를 그 벡터의 크기 $||\\mathbf{v}||$ 로 나누어서 얻을 수 있다.\n\n$$\n\\begin{equation*}\n  \\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{||\\mathbf{v}||}\n\\end{equation*}\n$$\n\n여기서 $\\mathbf{\\hat{v}}$는 $\\mathbf{v}$ 의 방향을 나타내는 단위 벡터이다.\n\n단위 벡터는 벡터의 크기에 관심이 없이 방향에 초점을 맞출 때 사용할 수 있다.\n\n예를 들어, $\\mathbf{v} = \\begin{bmatrix} 1 \\ 2 \\end{bmatrix}$ 를 $\\mathbb{R}^2$ 의 비영 벡터라고 할 때 $\\mathbf{v}$ 의 크기는 $||\\mathbf{v}|| = \\sqrt{1^2 + 2^2} = \\sqrt{5}$ 이다. 따라서 $\\mathbf{v}$ 의 방향을 나타내는 단위 벡터는 다음과 같다:\n\n$$\n\\begin{equation*}\n\\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{||\\mathbf{v}||} = \\frac{1}{\\sqrt{5}}\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{bmatrix}\n\\end{equation*}\n$$\n\n따라서 $\\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\ \\frac{2}{\\sqrt{5}} \\end{bmatrix}$ 는 $\\mathbf{v}$ 의 방향을 나타내는 단위 벡터에디.\n\n### Properteis\n\n1. Normalization (정규화): $\\|\\mathbf{u}\\| = 1$\n2. Direction: 단위 벡터는 공간에서 방향을 나타낸다.\n3. Scaling: 단위 벡터에 스칼라를 곱해도 방향은 변하지 않지만 크기는 변할 수 있다.\n4. Orthogonality: 서로 다른 방향의 단위 벡터는 서로 직교(수직)한다.\n\n### Standard Unit Vectors\n\n표준 단위 벡터는 $\\mathbf{e}_i$ 로 표기되며, 여기서 $i$ 는 좌표축을 나타낸다. 예를 들어, 2차원 공간에서 $\\mathbf{e}_1$ 은 $x$ 축을 따라 단위 벡터를 나타내고, $\\mathbf{e}_2$ 는 $y$ 축을 따라 단위 벡터를 나타낸다.\n\n## Distance\n\n두 벡터 사이의 거리는 유클리드 거리(Euclidean distance) 또는 맨하탄 거리(Manhattan distance)와 같은 거리 측정 방법을 사용하여 계산할 수 있다.\n\n* Euclidean Distance:\n\n길이가 n인 두 벡터 $\\mathbf{v}$ 와 $\\mathbf{w}$ 사이의 유클리드 거리는 다음 공식을 사용하여 계산할 수 있다:\n\n$$\n\\begin{aligned}\n  \\text{distance}(\\mathbf{v}, \\mathbf{w}) &= d(\\mathbf{v},\\mathbf{w})=||\\mathbf{v}-\\mathbf{w}||= \\sqrt{\\sum_{i=1}^{n} (v_i - w_i)^2}\\\\\n\\end{aligned}\n$$\n\n이 공식은 $\\mathbf{v}$와 $\\mathbf{w}$ 의 해당 요소들 간의 차이를 제곱하여 합한 값의 제곱근을 계산한다. 이는 다차원 공간에서 두 벡터 사이의 직선 거리를 측정한다.\n\n```{python}\n#| echo: fenced\n\nv = np.array([1, 2, 3])\nw = np.array([4, 5, 6])\n\ndistance = np.linalg.norm(v - w)\nprint(distance)\n\n```\n\n* Manhattan Distance:\n\n맨하탄 거리, 또는 시티 블록 거리 또는 $L_1$ 거리로 알려진, 길이가 n인 두 벡터 $\\mathbf{v}$ 와 $\\mathbf{w}$ 사이의 맨하탄 거리는 다음 공식을 사용하여 계산할 수 있다:\n\n$$\n\\text{distance}(\\mathbf{v}, \\mathbf{w}) = \\sum_{i=1}^{n} |v_i - w_i|\n$$\n\n이 공식은 $\\mathbf{v}$와 $\\mathbf{w}$ 의 해당 요소들 간의 절댓값 차이의 합을 계산한다. 이는 두 벡터 사이의 거리를 각 차원별로 더하여 마치 격자 모양 도시의 도로 블록을 따라 이동하는 것과 같이 측정한다.\n\n```{python}\n#| echo: fenced\n\ndistance = np.sum(np.abs(v - w))\nprint(distance)\n```\n\n## Dot Product\n\nDot product is also known as scalar product or inner product.\n\n두 벡터의 dot product은 해당 요소들의 곱의 합으로 계산된다. 만약 $\\mathbf{a}$ 와 $\\mathbf{b}$ 가 같은 차원의 두 벡터라면, 그들의 도트 곱 $c = \\mathbf{a} \\cdot \\mathbf{b}$ 은 다음 공식에 의해 나타내어지는 스칼라이다:\n\n$$\n\\begin{align*}\n  c&=\\textbf{a}\\cdot \\textbf{b}\\\\\n  &= \\sum_{i=1}^{n}a_ib_i\n\\end{align*}\n$$\n\n여기서 $a_i$ 와 $b_i$ 는 각 벡터의 해당 요소이다. dot product 은 두 벡터 사이의 유사성, 직교성, 또는 벡터의 방향과 크기에 대한 정보를 제공하는 데 사용될 수 있다.\n\n* Dot product은 두 벡터 간의 유사성을 측정하는 데 사용될 수 있다.\n* 두 벡터 $\\mathbf{a} = [a_1, a_2, \\cdots a_n]$ , $\\mathbf{b} = [b_1, b_2, \\cdots b_n]$ 에 대해, dot product는 다음과 같이 정의된다:\n$$\n\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}^{T} \\mathbf{b} = ||\\mathbf{a}||\\text{ } ||\\mathbf{b}|| \\cos \\theta \n$$\n\n* 두 벡터가 직교할 때, $\\cos 90^{\\circ} = 0$ 이므로, 두 벡터의 유사성은 0 이다.\n* 유클리드 공간에서 도트 곱은 종종 내적(inner product)이라고 불린다.\n\n### Properties\n\n* Commutativity: $\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{w} \\cdot \\mathbf{v}$\n* Distributivity over vector addition: $\\mathbf{v} \\cdot (\\mathbf{w} + \\mathbf{u}) = \\mathbf{v} \\cdot \\mathbf{w} + \\mathbf{v} \\cdot \\mathbf{u}$\n* Scalar associativity: $(c \\mathbf{v}) \\cdot \\mathbf{w} = c (\\mathbf{v} \\cdot \\mathbf{w}) = \\mathbf{v} \\cdot (c \\mathbf{w})$\n* Linearity: $(c \\mathbf{v} + d \\mathbf{w}) \\cdot \\mathbf{u} = c (\\mathbf{v} \\cdot \\mathbf{u}) + d (\\mathbf{w} \\cdot \\mathbf{u})$\n* Orthogonality: $\\mathbf{v} \\cdot \\mathbf{w} = 0 \\text{ if and only if } \\mathbf{v} \\perp \\mathbf{w}$\n* simmilarity\n\n  ::: {.callout-note}\n  ### the Law of Cosines\n\n  선형 대수학에서의 제 2 코사인 법칙은 dot product을 벡터의 크기와 그들 사이의 각도와 연관 시킨다. 이는 코사인 법칙(Cosine Law) 또는 코사인의 법칙이라고도 알려져 있다.\n  \n  $$\n  \\begin{align*}\n    \\cos\\theta &= \\frac{b^2+c^2-a^2}{2bc} \\rightarrow \n    \\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\cos\\theta\n  \\end{align*}\n  $$\n  \n  :::\n\n* The geometric meaning of the dot product \n  * $\\mathbf{v}_1=(x_1,y_1), \\mathbf{v}_2=(x_2,y_2), \\mathbf{v}_2-\\mathbf{v}_1=(x_2-x_1,y_2-y_1)$ \n\n  $$\n  \\begin{aligned}\n    \\cos\\theta&=\\frac{||\\mathbf{v}_2||^2+||\\mathbf{v}_1||^2-||\\mathbf{v}_2-\\mathbf{v}_1||}{2||\\mathbf{v}_2||||\\mathbf{v}_1||} \\\\\n    ||\\mathbf{v}||&=\\sqrt{\\mathbf{v}\\cdot \\mathbf{v}}\\\\ \n    ||\\mathbf{v}_2||^2+||\\mathbf{v}_1||^2-||\\mathbf{v}_2-\\mathbf{v}_1||&=(\\sqrt{\\mathbf{v}_2\\cdot \\mathbf{v}_2})^2+(\\sqrt{\\mathbf{v}_1\\cdot \\mathbf{v}_1})^2-(\\sqrt{(\\mathbf{v}_2-\\mathbf{v}_1)\\cdot (\\mathbf{v}_2-\\mathbf{v}_1)})^2\\\\\n    &=\\mathbf{v}_2\\cdot \\mathbf{v}_2+\\mathbf{v}_1\\cdot \\mathbf{v}_1-(\\mathbf{v}_2-\\mathbf{v}_1)\\cdot (\\mathbf{v}_2-\\mathbf{v}_1)\\\\\n    &=\\mathbf{v}_2\\cdot \\mathbf{v}_2+\\mathbf{v}_1\\cdot \\mathbf{v}_1-\\mathbf{v}_2\\cdot \\mathbf{v}_2+2\\mathbf{v}_1\\cdot \\mathbf{v}_2-\\mathbf{v}_1\\cdot \\mathbf{v}_1\\\\\n    &=2\\mathbf{v}_1\\cdot \\mathbf{v}_2 \\\\\n    \\cos\\theta&=\\frac{\\mathbf{v}_1\\cdot \\mathbf{v}_2}{||\\mathbf{v}_2||||\\mathbf{v}_1||}\\\\\n    \\mathbf{v}_1\\cdot \\mathbf{v}_2&=||\\mathbf{v}_1||||\\mathbf{v}_2||\\cos\\theta\n  \\end{aligned}\n  $$\n\n    * the dot product $\\ge 0$ if $0\\le \\theta\\le \\frac{\\pi}{2}$\n    * the dot product $< 0$ if $\\frac{\\pi}{2}< \\theta\\le \\pi$\n    * dot product의 기하학적 해석은 한 벡터를 다른 벡터 위에 투영하는 것을 측정한다는 것이다. dot product이 양수인 경우, 벡터는 비슷한 방향을 가리키고 있는 것이며, 음수인 경우에는 반대 방향을 가리키고 있다는 것을 의미한다. dot product의 크기는 벡터들의 평행성, 일렬성 또는 유사성을 측정하는 지표 역할을 한다.\n  * Projection\n    * Let $\\mathbf{v}_1$ and $\\mathbf{v}_2$ be two vectors. The projection of $\\mathbf{v}_1$ onto $\\mathbf{v}_2$ is defined as the vector, $\\mathbf{w}$ :\n\n    $$\n    \\begin{aligned}\n      \\mathbf{v_1}\\cdot \\mathbf{v_2}&=||\\mathbf{w}||||\\mathbf{v_2}||\\\\\n      \\mathbf w&=\\text{proj}_{\\mathbf v_2}\\mathbf v_1\\\\ \n      &=||\\mathbf{w}||\\mathbf{u}_\\mathbf{w} \\quad (\\because \\text{the univt vector of } \\mathbf{w} = \\mathbf{u}_w)\\\\\n      \\text{the unit vector of } \\mathbf{w} &= \\frac{\\mathbf{v}_2}{||\\mathbf{v}_2||}  \\quad(\\because \\text{the direction of } \\mathbf{w} = \\text{the direction of } \\mathbf{v_2}) \\\\\n      \\mathbf w&=\\text{proj}_{\\mathbf v_2}\\mathbf v_1\\\\ \n      &=\\frac{\\mathbf v_1 \\cdot \\mathbf v_2}{||\\mathbf v_2||} \\frac{\\mathbf v_2}{||\\mathbf v_2||} \\\\\n      &=\\frac{\\mathbf v_1 \\cdot \\mathbf v_2}{||\\mathbf v_2||^2} \\mathbf v_2 \\\\\n      &=\\frac{\\mathbf v_1 \\cdot \\mathbf v_2}{\\mathbf{v}_2\\cdot \\mathbf{v}_2}\\mathbf{v}_2 \\quad (\\because ||\\mathbf v_2||=\\sqrt{\\mathbf{v}_2\\cdot \\mathbf{v}_2})\n    \\end{aligned}\n    $$\n\n    * 이 투영된 벡터는 $\\mathbf{v}_2$ 에 의해 생성된 직선 상에서 $\\mathbf{v}_1$ 에 가장 가까운 벡터이다. 이는 $\\mathbf{v}_2$ 와 평행하며, 그 직선 상에서 $\\mathbf{v}_1$ 을 가장 근사하는 가능한 가까운 벡터를 의미한다.\n    * [Reference: Read This Article with Interactive Visualization - Projection](http://immersivemath.com/ila/ch03_dotproduct/ch03.html#auto_label_107)\n* Cauchy-Schwarz Inequality\n\n$$\n\\begin{aligned}\n(ax+by)^2 &\\le (a^2+b^2)(x^2+y^2)\\\\\n|\\langle \\mathbf u,\\mathbf v\\rangle| = |\\mathbf{u}\\cdot\\mathbf{v}| &\\le ||\\mathbf{u}|| ||\\mathbf{v} || \\quad (\\text{where }\\mathbf{u}=(a,b), \\quad \\mathbf{v}=(x,y))\\\\\n&\\text{putting the absolute value is because the dot product could be negative} \\\\\n\\text{Proof)} \\mathbf{u}\\cdot\\mathbf{v}&= ||\\mathbf{u}|| ||\\mathbf{v} ||\\cos\\theta \\\\\n-1 \\le \\mathbf{u}\\cdot\\mathbf{v}&= ||\\mathbf{u}|| ||\\mathbf{v} || \\le 1 \\\\\n-||\\mathbf{u}|| ||\\mathbf{v} || \\le \\mathbf{u}\\cdot\\mathbf{v}&  \\le ||\\mathbf{u}|| ||\\mathbf{v} || \\\\\n|\\mathbf{u}\\cdot\\mathbf{v}| &  \\le ||\\mathbf{u}|| ||\\mathbf{v}||\n\\end{aligned} \n$$\n\n  * 기하학적으로, Schwarz 부등식은 한 벡터를 다른 벡터에 투영한 크기가 투영된 벡터의 길이를 초과할 수 없다는 것을 나타낸다. 다시 말해, 이는 두 벡터 간의 상관관계를 제한하고, 그들의 내적이 항상 그들의 norm의 곱보다 작거나 같도록 보장한다.\n* Triangle Inequality\n\n$$\n\\begin{aligned}\n  ||\\mathbf{u} + \\mathbf{v}|| &\\le ||\\mathbf{u}|| + ||\\mathbf{v}|| \\\\\n  ||\\mathbf{u} + \\mathbf{v}||^2 &= (\\mathbf{u}+\\mathbf{v})\\cdot(\\mathbf{u}+\\mathbf{v}) \\\\\n  &= \\mathbf{u}\\cdot\\mathbf{u}+2\\mathbf{u}\\cdot\\mathbf{v}+\\mathbf{v}\\cdot\\mathbf{v}\\\\\n  &\\le \\mathbf{u}\\cdot\\mathbf{u}+2|\\mathbf{u}\\cdot\\mathbf{v}|+\\mathbf{v}\\cdot\\mathbf{v} \\\\ &(\\because \\mathbf{u}\\cdot\\mathbf{v} \\text{ is a scalar and could be negative})\n\\end{aligned} \n$$\n\n  * 삼각 부등식은 두 벡터로 나타낸 공간에서 두 점 사이의 거리는 항상 두 벡터의 거리의 합보다 짧거나 같다는 것을 의미한다. 다시 말해, 두 점 사이의 거리를 나타내는 두 벡터보다 더 짧은 직선을 그리는 것은 불가능하다.\n\n\n```{python}\n#| echo: fenced\n\nu = [3, 4]\nv = [-1, 2]\n\n# Plot the vectors\nplt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='g')\nplt.quiver(u[0], u[1], -1, 2, angles='xy', scale_units='xy', scale=1, color='g')\nplt.quiver(0, 0, u[0]+v[0], u[1]+v[1], angles='xy', scale_units='xy', scale=1, color='b')\n\nplt.text(u[0]+0.2, u[1], 'u', fontsize=12)\nplt.text(u[0]+v[0], u[1]+v[1], 'v', fontsize=12)\nplt.text(u[0]+v[0]-0.8, u[1]+v[1], 'u+v', fontsize=12)\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xlim(-2, 7)\nplt.ylim(-2, 7)\n\nplt.plot([0, u[0], u[0]+v[0], v[0], 0], [0, u[1], u[1]+v[1], v[1], 0], 'k--')\nplt.text((u[0]+v[0])/2, (u[1]+v[1])/2+0.5, '||u+v||', fontsize=12)\nplt.show()\n```\n\n\n:::{.callout-note}\n\n#### Inner Product vs Dot Product\n\n일반적으로 내적은 두 벡터를 받아서 스칼라를 출력하는 수학적 연산이다. 내적은 첫 번째 성분에 대해 선형이고 두 번째 성분에 대해서는 켤레 선형이며,  positive-definite과 같은 특정 속성을 만족한다. \n디사 말해, 내적은 다음과 같은 속성을 만족하는 모든 벡터 $\\mathbf{x}$, $\\mathbf{y}$, $\\mathbf{z}$ 및 스칼라 $a$, $b$ 에 대해 다음과 같은 이중 선형 형식이다:\n\n* \"첫 번째 인수에 대해 선형\"은 임의의 고정된 벡터 $\\mathbf{u}$ 에 대해 $f(\\mathbf{v}) = \\langle\\mathbf{u}, \\mathbf{v}\\rangle$ 로 정의된 함수 $f$ 가 $\\mathbf{v}$ 에 대해 선형 함수임을 의미한다. 즉, 모든 스칼라 $a$, $b$ 와 벡터 $\\mathbf{x}$, $\\mathbf{y}$ 에 대해 $f(a\\mathbf{x} + b\\mathbf{y}) = af(\\mathbf{x}) + bf(\\mathbf{y})$ 이다.\n  * $\\langle a\\mathbf{x} + b\\mathbf{y}, \\mathbf{z}\\rangle = a\\langle\\mathbf{x}, \\mathbf{z}\\rangle + b\\langle\\mathbf{y}, \\mathbf{z}\\rangle$ 는 내적이 첫 번째 성분에 대해 선형이라는 것을 의미한다. 벡터를 스칼라로 곱하고 다른 벡터에 더한 결과의 내적은 각 벡터의 내적을 개별적으로 계산한 다음 더한 것과 동일하다.\n* \"두 번째 인수에 대해 켤레 선형\"은 임의의 고정된 벡터 $\\mathbf{v}$ 에 대해 $g(\\mathbf{u}) = \\langle\\mathbf{u}, \\mathbf{v}\\rangle$ 로 정의된 함수 $g$ 가 $\\mathbf{u}$ 에 대해 켤레 선형 함수임을 의미한다. 즉, 모든 스칼라 $a$, $b$ 와 벡터 $\\mathbf x$, $\\mathbf y$ 에 대해 $g(a \\mathbf x + b \\mathbf y) = \\bar{a} g(\\mathbf x) + \\bar{b} * g(\\mathbf y)$ 입니다. 여기서 $\\bar{a}$ 는 $a$ 의 켤레 복소수를 나타낸다.\n  * $\\langle \\mathbf{x}, a\\mathbf{y}, b\\mathbf{z}\\rangle = a\\langle\\mathbf{x}, \\mathbf{y}\\rangle + b\\langle\\mathbf{x}, \\mathbf{z}\\rangle$ 는 내적이 두 번째 인수에 대해 선형이지만 복소 수를 켤레로 취한다는 것을 의미한다. 벡터를 스칼라로 곱하고 다른 벡터에 더한 결과의 내적은 각 벡터의 내적을 개별적으로 계산한 다음 두 번째 벡터를 복소수로 켤레한 후 더한 것과 동일하다.\n* \"대칭성\"은 $\\langle \\mathbf{x},\\mathbf{y}\\rangle= \\langle \\mathbf{y},\\mathbf{x}\\rangle$ 을 의미한다. 내적을 계산할 때 벡터의 순서는 중요하지 않다.\n* \"양의 정부호성\"은 어떤 영벡터도 아닌 벡터 $\\mathbf{v}$ 에 대해 내적 $\\langle\\mathbf{u}, \\mathbf{v}\\rangle$ 가 양의 실수임을 의미한다. 다시 말해, 벡터 자체와의 내적은 항상 양수이며, 벡터가 영벡터가 아닌 경우에만 0이 아니다.\n$\\langle \\mathbf{x},\\mathbf{x}\\rangle\\ge 0$ 이며, $\\langle \\mathbf{x},\\mathbf{x}\\rangle=0$ 은 $\\mathbf{x}=0$ 인 경우에만 성립한다.\n:::\n\n:::\n\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n::: {#English .tab-pane .fade role=\"tabpanel\" aria-labelledby=\"English-tab\"}\n\n# Introduction\n\n## Norm\n\nThe norm of a vector $\\mathbf{x}$ is a non-negative scalar value that represents **the size or length** of the vector. The norm is denoted by $||\\mathbf{x}||$ and satisfies the following properties:\n\n### Properties\n\n* Non-negativity: $||\\mathbf{x}||\\geq 0$, with equality if and only if $\\mathbf{x}=\\mathbf{0}$.\n* Definiteness: The norm of a vector $\\mathbf{v}$ is zero if and only if the vector itself is the zero vector:\n  $$\n  \\|\\mathbf{v}\\| = 0 \\text{ if and only if } \\mathbf{v} = \\mathbf{0}\n  $$\n* Scalar Multiplication: The norm of a scalar multiple of a vector $\\mathbf{v}$ is equal to the absolute value of the scalar multiplied by the norm of the vector:\n  $$\n  \\|c\\mathbf{v}\\| = |c|\\|\\mathbf{v}\\|\n  $$\n* Homogeneity: $||\\alpha\\mathbf{x}||=|\\alpha| \\quad ||\\mathbf{x}||$ for any scalar $\\alpha$.\n* Triangle Inequality: $||\\mathbf{x}+\\mathbf{y}||\\leq ||\\mathbf{x}||+||\\mathbf{y}||$.\n\n\nSuppose we have a vector $\\mathbf{x}=\\begin{bmatrix}1 \\\\ -2 \\\\ 2\\end{bmatrix}$. We can find its Euclidean norm as follows:\n\n$$\n||\\mathbf x||=\\sqrt{1^2+(-2)^2+2^2}=\\sqrt{9}=3\n$$\n\nTherefore, the norm of $\\mathbf{x}$ is 3.\n\n### Norm Types\n\nThere are several types of norms:\n\n* Manhattan Norm or Absolute Norm or $l_1$-norm\n$$\n\\begin{equation*}\n||\\mathbf{x}||_{l_1} = \\sum_{i=1}^{n} |x_i|\n\\end{equation*}\n$$\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, -2, 3]$, $||\\mathbf{x}||_{l_1} = |1| + |-2| + |3| = 6$.\n\n\n* Euclidean Norm or $l_2$-norm\n$$\n\\begin{equation*}\n||\\mathbf{x}||_{l_2} = \\sqrt{\\sum_{i=1}^{n} x_i^2}\n\\end{equation*}\n$$\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, 2, 3]$, $||\\mathbf{x}||_{l_2} = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14}$.\n\n![$l_2$-norm](images/chap02_05.PNG)\n\n* p-norm($l_2$-norm) \n\nFor $p \\geq 1$, \n$$\n\\begin{equation*}\n||\\mathbf{x}||_p = (\\sum_{i=1}^n |x_i|^p)^{\\frac{1}{p}}\n\\end{equation*}\n$$\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, 2, 3]$, $||\\mathbf{x}||_{l_p} = \\sqrt{1^p + 2^p + 3^p}$.\n\n* Maximum Norm\n$$\n\\begin{equation*}\n||\\mathbf{x}||_{\\infty} = \\max_{1 \\leq i \\leq n} |x_i|\n\\end{equation*}\n$$\nwhere $\\mathbf{x}$ is a vector of length $n$.\nExample: For $\\mathbf{x} = [1, -2, 3]$, $||\\mathbf{x}||_{\\infty} = \\max{(1, |-2|, 3)} = 3$.\n\n![$l_1$-norm vs $l_2$-norm vs $\\max$-norm](images/chap02_06.PNG)\n\n* Frobenius Norm:\n$$\n\\begin{equation*}\n||\\mathbf{a}||_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^2}\n\\end{equation*}\n$$\nwhere $\\mathbf{A}$ is an $m \\times n$ matrix.\nExample: For $\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$, $||\\mathbf{A}||_{F} = \\sqrt{1^2 + 2^2 + 3^2 + 4^2} = \\sqrt{30}$.\n\n## Unit Vector\n\nA unit vector is a vector that has a magnitude of 1. A unit vector can be obtained by dividing a non-zero vector $\\mathbf{v}$ by its magnitude $||\\mathbf{v}||$, \n\n$$\n\\begin{equation*}\n  \\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{||\\mathbf{v}||}\n\\end{equation*}\n$$\n\nwhere $\\mathbf{\\hat{v}}$ is the unit vector in the direction of $\\mathbf{v}$.\n\nA unit vector can be used to focus on a direction with no interest in the size of the vector.\n\nFor example, let $\\mathbf{v} = \\begin{bmatrix} 1 \\ 2 \\end{bmatrix}$ be a non-zero vector in $\\mathbb{R}^2$. The magnitude of $\\mathbf{v}$ is $||\\mathbf{v}|| = \\sqrt{1^2 + 2^2} = \\sqrt{5}$. Therefore, a unit vector in the direction of $\\mathbf{v}$ is:\n\n$$\n\\begin{equation*}\n\\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{||\\mathbf{v}||} = \\frac{1}{\\sqrt{5}}\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{bmatrix}\n\\end{equation*}\n$$\n\nThus, $\\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{bmatrix}$ is a unit vector in the direction of $\\mathbf{v}$.\n\n### Properteis\n\n1. Normalization: $\\|\\mathbf{u}\\| = 1$\n2. Direction: A unit vector represents a direction in space.\n3. Scaling: Multiplying a unit vector by a scalar does not change its direction, but it may change its magnitude.\n4. Orthogonality: Unit vectors in different directions are orthogonal (perpendicular) to each other.\n\n### Standard Unit Vectors\n\nA standard unit vector is denoted as $\\mathbf{e}_i$, where $i$ represents the coordinate axis. For example, in 2D space, we have $\\mathbf{e}_1$ representing the unit vector along the $x$-axis and $\\mathbf{e}_2$ representing the unit vector along the $y$-axis.\n\n\n## Distance\n\nThe distance between two vectors can be computed using a distance metric, such as the Euclidean distance or the Manhattan distance. \n\n* Euclidean Distance:\n\nThe Euclidean distance between two vectors $\\mathbf{v}$ and $\\mathbf{w}$ of length $n$ can be calculated using the following formula:\n\n$$\n\\begin{aligned}\n  \\text{distance}(\\mathbf{v}, \\mathbf{w}) &= d(\\mathbf{v},\\mathbf{w})=||\\mathbf{v}-\\mathbf{w}||= \\sqrt{\\sum_{i=1}^{n} (v_i - w_i)^2}\\\\\n\\end{aligned}\n$$\n\n```{python}\n#| echo: fenced\n\nv = np.array([1, 2, 3])\nw = np.array([4, 5, 6])\n\ndistance = np.linalg.norm(v - w)\nprint(distance)\n\n```\n\n* Manhattan Distance:\n\nThe Manhattan distance (also known as the city block distance or L1 distance) between two vectors $\\mathbf{v}$ and $\\mathbf{w}$ of length $n$ can be calculated using the following formula:\n\n$$\n\\text{distance}(\\mathbf{v}, \\mathbf{w}) = \\sum_{i=1}^{n} |v_i - w_i|\n$$\n\n```{python}\n#| echo: fenced\n\ndistance = np.sum(np.abs(v - w))\nprint(distance)\n```\n\n## Dot Product\n\nDot product is also known as scalar product or inner product.\n\nThe dot product of two vectors is the sum of the products of their corresponding components (a.k.a inner product & scalar product). If $\\textbf{a}$ and $\\textbf{b}$ are two vectors of the same dimension, then their dot product $c = \\textbf{a} \\cdot \\textbf{b}$ is a scalar given by the formula:\n\n$$\n\\begin{align*}\n  c&=\\textbf{a}\\cdot \\textbf{b}\\\\\n  &= \\sum_{i=1}^{n}a_ib_i\n\\end{align*}\n$$\n\n* Dot product can be used to measure the similarity between two vectors.\n* For the two vectors, $\\mathbf{a} = [a_1, a_2, \\cdots a_n]$ , $\\mathbf{b} = [b_1, b_2, \\cdots b_n]$, dot product can be defined as\n$$\n\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}^{T} \\mathbf{b} = ||\\mathbf{a}||\\text{ } ||\\mathbf{b}|| \\cos \\theta \n$$\n* When two vectors are orthogonal, $\\cos 90^{\\circ} = 0$, the similarity of the two vectors is 0.\n* In the Euclidean space, dot product is often called inner product (inner product is a generalization of dot product)\n\n### Properties\n\n* Commutativity: $\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{w} \\cdot \\mathbf{v}$\n* Distributivity over vector addition: $\\mathbf{v} \\cdot (\\mathbf{w} + \\mathbf{u}) = \\mathbf{v} \\cdot \\mathbf{w} + \\mathbf{v} \\cdot \\mathbf{u}$\n* Scalar associativity: $(c \\mathbf{v}) \\cdot \\mathbf{w} = c (\\mathbf{v} \\cdot \\mathbf{w}) = \\mathbf{v} \\cdot (c \\mathbf{w})$\n* Linearity: $(c \\mathbf{v} + d \\mathbf{w}) \\cdot \\mathbf{u} = c (\\mathbf{v} \\cdot \\mathbf{u}) + d (\\mathbf{w} \\cdot \\mathbf{u})$\n* Orthogonality: $\\mathbf{v} \\cdot \\mathbf{w} = 0 \\text{ if and only if } \\mathbf{v} \\perp \\mathbf{w}$\n* simmilarity\n\n  ::: {.callout-note}\n  ### the Law of Cosines\n\n  The second cosine rule in linear algebra, also known as the Law of Cosines, relates the dot product of vectors to their magnitudes and the angle between them.\n\n  $$\n  \\begin{align*}\n    \\cos\\theta &= \\frac{b^2+c^2-a^2}{2bc} \\rightarrow \n    \\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\cos\\theta\n  \\end{align*}\n  $$\n  \n  :::\n\n* The geometric meaning of the dot product \n  * $\\mathbf{v}_1=(x_1,y_1), \\mathbf{v}_2=(x_2,y_2), \\mathbf{v}_2-\\mathbf{v}_1=(x_2-x_1,y_2-y_1)$ \n\n  $$\n  \\begin{aligned}\n    \\cos\\theta&=\\frac{||\\mathbf{v}_2||^2+||\\mathbf{v}_1||^2-||\\mathbf{v}_2-\\mathbf{v}_1||}{2||\\mathbf{v}_2||||\\mathbf{v}_1||} \\\\\n    ||\\mathbf{v}||&=\\sqrt{\\mathbf{v}\\cdot \\mathbf{v}}\\\\ \n    ||\\mathbf{v}_2||^2+||\\mathbf{v}_1||^2-||\\mathbf{v}_2-\\mathbf{v}_1||&=(\\sqrt{\\mathbf{v}_2\\cdot \\mathbf{v}_2})^2+(\\sqrt{\\mathbf{v}_1\\cdot \\mathbf{v}_1})^2-(\\sqrt{(\\mathbf{v}_2-\\mathbf{v}_1)\\cdot (\\mathbf{v}_2-\\mathbf{v}_1)})^2\\\\\n    &=\\mathbf{v}_2\\cdot \\mathbf{v}_2+\\mathbf{v}_1\\cdot \\mathbf{v}_1-(\\mathbf{v}_2-\\mathbf{v}_1)\\cdot (\\mathbf{v}_2-\\mathbf{v}_1)\\\\\n    &=\\mathbf{v}_2\\cdot \\mathbf{v}_2+\\mathbf{v}_1\\cdot \\mathbf{v}_1-\\mathbf{v}_2\\cdot \\mathbf{v}_2+2\\mathbf{v}_1\\cdot \\mathbf{v}_2-\\mathbf{v}_1\\cdot \\mathbf{v}_1\\\\\n    &=2\\mathbf{v}_1\\cdot \\mathbf{v}_2 \\\\\n    \\cos\\theta&=\\frac{\\mathbf{v}_1\\cdot \\mathbf{v}_2}{||\\mathbf{v}_2||||\\mathbf{v}_1||}\\\\\n    \\mathbf{v}_1\\cdot \\mathbf{v}_2&=||\\mathbf{v}_1||||\\mathbf{v}_2||\\cos\\theta\n  \\end{aligned}\n  $$\n\n    * the dot product $\\ge 0$ if $0\\le \\theta\\le \\frac{\\pi}{2}$\n    * the dot product $< 0$ if $\\frac{\\pi}{2}< \\theta\\le \\pi$\n    * The geometric interpretation of the dot product is that it measures the *projection* of one vector onto another. When the dot product is positive, it means the vectors are pointing in a similar direction, and when it is negative, it means they are pointing in opposite directions. The magnitude of the dot product provides a measure of how *parallelness*, *aligned* or *similar* the vectors are.\n  * Projection\n    * Let $\\mathbf{v}_1$ and $\\mathbf{v}_2$ be two vectors. The projection of $\\mathbf{v}_1$ onto $\\mathbf{v}_2$ is defined as the vector, $\\mathbf{w}$ :\n\n    $$\n    \\begin{aligned}\n      \\mathbf{v_1}\\cdot \\mathbf{v_2}&=||\\mathbf{w}||||\\mathbf{v_2}||\\\\\n      \\mathbf w&=\\text{proj}_{\\mathbf v_2}\\mathbf v_1\\\\ \n      &=||\\mathbf{w}||\\mathbf{u}_\\mathbf{w} \\quad (\\because \\text{the univt vector of } \\mathbf{w} = \\mathbf{u}_w)\\\\\n      \\text{the unit vector of } \\mathbf{w} &= \\frac{\\mathbf{v}_2}{||\\mathbf{v}_2||}  \\quad(\\because \\text{the direction of } \\mathbf{w} = \\text{the direction of } \\mathbf{v_2}) \\\\\n      \\mathbf w&=\\text{proj}_{\\mathbf v_2}\\mathbf v_1\\\\ \n      &=\\frac{\\mathbf v_1 \\cdot \\mathbf v_2}{||\\mathbf v_2||} \\frac{\\mathbf v_2}{||\\mathbf v_2||} \\\\\n      &=\\frac{\\mathbf v_1 \\cdot \\mathbf v_2}{||\\mathbf v_2||^2} \\mathbf v_2 \\\\\n      &=\\frac{\\mathbf v_1 \\cdot \\mathbf v_2}{\\mathbf{v}_2\\cdot \\mathbf{v}_2}\\mathbf{v}_2 \\quad (\\because ||\\mathbf v_2||=\\sqrt{\\mathbf{v}_2\\cdot \\mathbf{v}_2})\n    \\end{aligned}\n    $$\n\n    * This projected vector is the closest vector to $\\mathbf{v}_1$ that lies on the line spanned by $\\mathbf{v}_2$. It means that a vector that is parallel to $\\mathbf{v}_2$ and is the closest possible approximation of $\\mathbf{v}_1$ along that line.\n    * [Reference: Read This Article with Interactive Visualization - Projection](http://immersivemath.com/ila/ch03_dotproduct/ch03.html#auto_label_107)\n* Cauchy-Schwarz Inequality\n\n$$\n\\begin{aligned}\n(ax+by)^2 &\\le (a^2+b^2)(x^2+y^2)\\\\\n|\\langle \\mathbf u,\\mathbf v\\rangle| = |\\mathbf{u}\\cdot\\mathbf{v}| &\\le ||\\mathbf{u}|| ||\\mathbf{v} || \\quad (\\text{where }\\mathbf{u}=(a,b), \\quad \\mathbf{v}=(x,y))\\\\\n&\\text{putting the absolute value is because the dot product could be negative} \\\\\n\\text{Proof)} \\mathbf{u}\\cdot\\mathbf{v}&= ||\\mathbf{u}|| ||\\mathbf{v} ||\\cos\\theta \\\\\n-1 \\le \\mathbf{u}\\cdot\\mathbf{v}&= ||\\mathbf{u}|| ||\\mathbf{v} || \\le 1 \\\\\n-||\\mathbf{u}|| ||\\mathbf{v} || \\le \\mathbf{u}\\cdot\\mathbf{v}&  \\le ||\\mathbf{u}|| ||\\mathbf{v} || \\\\\n|\\mathbf{u}\\cdot\\mathbf{v}| &  \\le ||\\mathbf{u}|| ||\\mathbf{v}||\n\\end{aligned} \n$$\n\n  * Geometrically, the Schwarz inequality states that the magnitude of the projection of one vector onto the other cannot exceed the length of the vector being projected. In other words, it bounds the correlation between two vectors and ensures that their inner product is always less than or equal to the product of their norms.\n* Triangle Inequality\n\n$$\n\\begin{aligned}\n  ||\\mathbf{u} + \\mathbf{v}|| &\\le ||\\mathbf{u}|| + ||\\mathbf{v}|| \\\\\n  ||\\mathbf{u} + \\mathbf{v}||^2 &= (\\mathbf{u}+\\mathbf{v})\\cdot(\\mathbf{u}+\\mathbf{v}) \\\\\n  &= \\mathbf{u}\\cdot\\mathbf{u}+2\\mathbf{u}\\cdot\\mathbf{v}+\\mathbf{v}\\cdot\\mathbf{v}\\\\\n  &\\le \\mathbf{u}\\cdot\\mathbf{u}+2|\\mathbf{u}\\cdot\\mathbf{v}|+\\mathbf{v}\\cdot\\mathbf{v} \\\\ &(\\because \\mathbf{u}\\cdot\\mathbf{v} \\text{ is a scalar and could be negative})\n\\end{aligned} \n$$\n\n  * this inequality means that the distance between two points in a space, represented by vectors, is always shorter than or equal to the sum of the distances between the two vectors. In other words, it is impossible to make a straight line from one point to another that is shorter than the distance represented by the two vectors.\n\n\n```{python}\n#| echo: fenced\n\nu = [3, 4]\nv = [-1, 2]\n\n# Plot the vectors\nplt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='g')\nplt.quiver(u[0], u[1], -1, 2, angles='xy', scale_units='xy', scale=1, color='g')\nplt.quiver(0, 0, u[0]+v[0], u[1]+v[1], angles='xy', scale_units='xy', scale=1, color='b')\n\nplt.text(u[0]+0.2, u[1], 'u', fontsize=12)\nplt.text(u[0]+v[0], u[1]+v[1], 'v', fontsize=12)\nplt.text(u[0]+v[0]-0.8, u[1]+v[1], 'u+v', fontsize=12)\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xlim(-2, 7)\nplt.ylim(-2, 7)\n\nplt.plot([0, u[0], u[0]+v[0], v[0], 0], [0, u[1], u[1]+v[1], v[1], 0], 'k--')\nplt.text((u[0]+v[0])/2, (u[1]+v[1])/2+0.5, '||u+v||', fontsize=12)\nplt.show()\n```\n\n\n:::{.callout-note}\n\n#### Inner Product vs Dot Product\n\nIn general, an inner product is a mathematical operation that takes two vectors and produces a scalar. It satisfies certain properties, such as being linear in the first argument, conjugate linear in the second argument, and positive-definite.\nIn other words, an inner product is a bilinear form that satisfies the following properties for all vectors $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z}$, and all scalars $a$ and $b$:\n\n* \"Linear in the first argument\" means that for any fixed vector $\\mathbf u$, the function $f$ defined by $f(\\mathbf v) = \\langle\\mathbf u, \\mathbf v\\rangle$ is a linear function of $\\mathbf v$, i.e., $f(a\\mathbf x + b\\mathbf y) = af(\\mathbf x) + bf(\\mathbf y)$ for any scalars $a$, $b$, and vectors $\\mathbf{x}$, $\\mathbf{y}$.\n  * $\\langle a\\mathbf{x} + b\\mathbf{y}, \\mathbf{z}\\rangle = a\\langle\\mathbf{x}, \\mathbf{z}\\rangle + b\\langle\\mathbf{y}, \\mathbf{z}\\rangle$, the inner product is linear with respect to the first argument. If we multiply a vector by a scalar and add it to another vector, the resulting inner product is the same as if we had calculated the inner product of each vector separately and then added them.\n* \"Conjugate linear in the second argument\" means that for any fixed vector $\\mathbf v$, the function $g$ defined by $g(\\mathbf u) = \\langle\\mathbf u, \\mathbf v\\rangle$ is a conjugate linear function of $\\mathbf u$, i.e., $g(a \\mathbf x + b \\mathbf y) = \\bar{a} g(\\mathbf x) + \\bar{b} * g(\\mathbf y)$ for any scalars $a$, $b$, and vectors $\\mathbf x$, $\\mathbf y$, where $\\bar{a}$ denotes the complex conjugate of $a$.\n  * $\\langle \\mathbf{x}, a\\mathbf{y}, b\\mathbf{z}\\rangle = a\\langle\\mathbf{x}, \\mathbf{y}\\rangle + b\\langle\\mathbf{x}, \\mathbf{z}\\rangle$. this property says that the inner product is linear with respect to the second argument, but with complex conjugation. If we multiply a vector by a scalar and add it to another vector, the resulting inner product is the same as if we had calculated the inner product of each vector separately, complex-conjugated the second vector, and then added them.\n* \"Symmetry\" means $\\langle \\mathbf{x},\\mathbf{y}\\rangle= \\langle \\mathbf{y},\\mathbf{x}\\rangle$\n  * the order of the vectors doesn't matter when calculating the inner product.  \n* \"Positive-definite\" means that for any nonzero vector v, the inner product $\\langle\\mathbf u, \\mathbf v\\rangle$ is a positive real number. In other words, the inner product of a vector with itself is always positive, except when the vector is the zero vector.\n  * $\\langle \\mathbf{x},\\mathbf{x}\\rangle\\ge 0, \\langle \\mathbf{x},\\mathbf{x}\\rangle=0$ only if $\\mathbf{x}=0$\n\n:::\n\n\n:::\n\n\n</div>\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"02.norm_dot-product.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","appendix-view-license":"라이센스 보기","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어","listing-page-filter":"필터","draft":"초안"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.5.56","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Basic Vector(2) - Vector Norm and Dot Product","subtitle":"Norm, Unit Vector, Euclidean Distance, Manhttan Distance Inner Product, Dot Product","description":"Basic Linear Algebra\n","categories":["Mathematics"],"author":"Kwangmin Kim","date":"03/30/2023","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"draft":false,"projectFormats":["html"]}