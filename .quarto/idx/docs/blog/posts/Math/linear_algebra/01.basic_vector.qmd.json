{"title":"Basic Vector(1) - Vector Operations","markdown":{"yaml":{"title":"Basic Vector(1) - Vector Operations","subtitle":"Introduction, Scalr, Vector, Addition, Scalar Multiplication","description":"Basic Linear Algebra\n","categories":["Mathematics"],"author":"Kwangmin Kim","date":"03/30/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"execute":{"echo":false},"draft":false},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n\n딥러닝은 상호 연결된 노드 또는 뉴런들의 계층으로 이루어진 신경망의 집합이며, 이들 간의 연결 가중치는 역전파라는 과정을 통해 학습된다\n\n선형 대수학은 딥러닝에 있어서 근본이 되는 역할을 하는데, 신경망의 학습에 관련된 많은 계산은 선형 대수 연산으로 표현될 수 있다. 예를 들어, 행렬 곱셈은 신경망의 각 계층의 출력을 계산하는 데 사용되며, 가중치에 대한 손실 함수의 그래디언트는 행렬 곱셈과 벡터 연산을 포함한 미분의 연쇄 법칙을 사용하여 계산된다.\n\n행렬 곱셈 외에도, 고유벡터, 고유값, 특이값 분해(SVD)와 같은 다른 선형 대수학적 개념들도 딥러닝에서 중요하다. 예를 들어, SVD는 데이터셋의 차원을 축소하거나 주성분을 계산하는 데 사용된다. 이는 데이터 시각화와 특징 추출에 유용하다.\n\nNumpy, Scipy, PyTorch와 같은 선형 대수학 라이브러리들은 이러한 연산의 효율적인 구현을 제공하여 GPU에서 대규모 신경망을 훈련하는 데 필수적이다. 이러한 라이브러리 없이 딥러닝 알고리즘을 구현하는 것은 훨씬 어렵고 시간이 많이 소요된다.\n\n[Reference: Motivation to Learn Linear Algebra](https://nbviewer.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb)\n\n# Physical Quantity\n\n## Scalar\n\n* 스칼라는 일반적으로 실수인 하나의 수학적인 양으로, 단일 값으로 나타낼 수 있다. 스칼라는 일반적으로 $a, b, c$ 와 같은 소문자로 표기된다.\n* 간단히 말해서, 스칼라는 온도나 높이와 같은 특정 물리적 양의 크기를 의미한다.\n\n## Vector\n\n* 벡터 $\\textbf{v}$ 는 속도와 힘과 같은 크기와 방향을 모두 가진 수학적 개체이다.\n* $n$ 차원 유클리드 공간 $\\mathbb{R}^n$ 에서, 벡터 $\\textbf{v}$ 는 일반적으로 $n$ 개의 실수로 이루어진 순서가 있는 리스트로 표현된다\n* $\\mathbf{v} =\\begin{bmatrix} x\\ y \\end{bmatrix}$ 일 때, $\\mathbf{v}$ 의 크기는 $||\\mathbf{v}|| = \\sqrt{x^{2} + y^{2}}$ 이다.\n* $\\mathbf{v}$ 의 방향은 $x$ 축과 이루는 각도로 표현되며, $\\theta=\\tan^{-1}(\\frac{y}{x})$ 이다. 만약 크기와 벡터가 같다면, 그들은 같은 벡터이다.\n\n::: {.callout-note}\n\n## Vector in Mathematics vs Physics\n\n### Vecotr in Mathematics\n\n* 벡터는 크기와 방향을 함께 나타내는 추상적인 수학적 객체이다.\n* 벡터는 일반적으로 순서가 있는 숫자들의 집합으로 표현된다(프로그래밍에서의 리스트와 유사한 형태), 종종 열 또는 행으로 배열되며 이를 벡터 구성 요소라고 한다.\n* 수학에서 벡터는 유클리드 공간이나 추상적인 벡터 공간과 같은 다양한 벡터 공간에서 정의되며, 덧셈, 스칼라 곱셈, 내적과 같은 특정 대수적 규칙을 따른다.\n\n### Vecotr in Physics\n\n* 벡터는 물리적인 양과 현상을 나타내고 분석하는 데 사용된다.\n* 벡터는 이동, 속도, 가속도, 힘, 운동량과 같이 크기와 방향을 모두 갖는 물리적인 양을 설명하는 데 사용된다.\n* 물리학에서 벡터는 종종 화살표로 표현되며, 화살표의 길이는 벡터의 크기를 나타내고, 화살표의 방향은 벡터의 방향을 나타낸다.\n* 물리학에서의 벡터는 벡터 덧셈과 스칼라 곱셈의 법칙을 따르며, 벡터 대수와 미적분을 사용하여 다양한 물리학 분야에서 문제를 분석하고 해결하는 데 활용될 수 있다.\n\n이 블로그는 벡터의 수학적인 관점에 초점을 맞춘다.\n:::\n\n$$\n\\textbf{v}=\n\\begin{bmatrix}\n  v_1 \\\\\n  v_2 \\\\\n  \\vdots \\\\\n  v_n\n\\end{bmatrix}\n$$\n\n여기서 $v_1, v_2, \\ldots, v_n$ 은 벡터 $\\textbf{v}$ 의 구성 요소이다.\n\n### Plotting Vectors on the Coordinate Plane\n\nMap $\\begin{bmatrix} 3\\\\  2 \\end{bmatrix}$ into $x=3$, $y=2$ on the Coordinate Plane\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.quiver(0, 0, 3, 2, color='b', angles='xy', scale_units='xy', scale=1)\nplt.xlim(-0.5, 3.5)\nplt.ylim(-0.5, 2.5)\nplt.grid()\nplt.show()\n\n```\n\n[Reference: Read This Article with Interactive Visualization - Points and Vectors](http://immersivemath.com/ila/ch02_vectors/ch02.html#auto_label_33)\n\n# Basic Vector Operations\n\n## Addition of Vectors\n\n두 벡터의 덧셈은 해당하는 구성 요소들을 더하는 과정이다. 만약 $\\textbf{a}$ 와 $\\textbf{b}$ 가 같은 차원의 두 벡터라면, 그들의 합인 $\\textbf{c} = \\textbf{a} + \\textbf{b}$ 는 $\\textbf{a}$ 와 $\\textbf{b}$ 의 $i$ 번째 구성 요소를 더한 값으로 이루어진 벡터이다.\n\n$$\n\\begin{align*}\n  \\textbf{c}&=\\textbf{a}+\\textbf{b}\\\\\n  c_i &= a_i + b_i\n\\end{align*}\n$$\n\n예를 들어, 만약 $\\textbf{a} = [3, 2]$ 이고 $\\textbf{b} = [-2, 1]$ 이라면, 그들의 합인 $\\textbf{c} = [1, 3]$ 이 된다.\n\n::: {layout-ncol=3}\n![](images/chap02_01.PNG)\n\n![](images/chap02_02.PNG)\n\n![](images/chap02_03.PNG)\n:::\n\n### Properties\n\n* 교환 법칙(Commutative Property): 덧셈의 순서는 결과에 영향을 주지 않는다.\n  $$ \n  \\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\n  $$\n* 결합 법칙(Associative Property): 덧셈에서 벡터들의 그룹화는 결과에 영향을 주지 않는다.\n  $$ \n  \\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}\n  $$\n* 항등 원소(Identity Element): $\\mathbf{0}$ 로 표시되는 항등 벡터가 존재하여, 어떤 벡터에 더해져도 동일한 벡터를 얻을 수 있다. \n  $$ \n  \\mathbf{v} + \\mathbf{0} = \\mathbf{v}\n  $$\n* 역원소(Inverse Element): 모든 벡터 $\\mathbf{v}$ 에 대해, $−\\mathbf{v}$ 로 표시되는 덧셈 역원소가 존재하며, 이를 $\\mathbf{v}$ 에 더하면 항등 벡터를 얻을 수 있다.\n  $$\n  \\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\n  $$\n\n::: {.callout-tip}\n## Why to check properties of definitions?\n\n특정 연산과 특정 집합이 정의되었을 때, 그들의 속성을 알아야 하는 것이 중요하다. 예를 들어, 만약 그것이 우리가 알고 있는 실수 집합의 속성과 완전히 동일하다는 것이 증명된다면, 실수 집합에서 증명된 모든 규칙과 정리를 새로 정의된 집합에도 적용할 수 있다.\n\n연산과 집합의 속성을 이해하는 것은 우리가 기존 도메인에서 얻은 지식과 정리를 다른 도메인에 적용할 수 있도록 해준다. 이는 특히 새로운 수학적 구조나 실수와 유사한 속성을 갖는 집합과 함께 작업할 때 유용하다. 이러한 연결을 확립함으로써, 우리는 기존의 이론과 결과를 활용하여 새로운 문맥에서 문제를 분석하고 해결하는 데 활용할 수 있다. 이는 수학적 개념을 확장하고 기존의 원칙을 새로운 도메인에 적용하는 강력한 프레임워크를 제공한다.\n:::\n\n## Subtraction of Vectors\n\n두 벡터의 뺄셈은 해당하는 구성 요소들을 빼는 과정이다. 만약 $\\textbf{a}$ 와 $\\textbf{b}$ 가 같은 차원의 두 벡터라면, 그들의 차인 $\\textbf{c} = \\textbf{a} - \\textbf{b}$ 는 $\\textbf{a}$ 와 $\\textbf{b}$ 의 $i$ 번째 구성 요소를 빼서 얻은 값으로 이루어진 벡터이다. 형식적인 정의는 다음과 같다:\n\n$$\n\\begin{align*}\n  \\textbf{c}&=\\textbf{a} - \\textbf{b}\\\\\n  c_i &= a_i - b_i\n\\end{align*}\n$$\n\n예를 들어, 만약 $\\textbf{a} = [1, 2, 3]$ 이고 $\\textbf{b} = [4, 5, 6]$ 라면, 그들의 차인 $\\textbf{c} = [-3, -3, -3]$ 이다.\n\n### Properties\n\n* 닫힘(Closure): 벡터 뺄셈은 주어진 벡터 공간에서 닫혀 있다. 만약 두 벡터가 주어진 벡터 공간에 속한다면, $\\mathbf{u} - \\mathbf{v}$ 도 벡터이다.\n* 가감 역원의 존재(Existence of Additive Inverse): 모든 벡터 $\\mathbf{u}$ 에 대해, $-\\mathbf{u}$ 라는 가감 역원이 존재하여 $\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}$ 이다.\n* 교환 법칙(Commutativity): $\\mathbf{u} - \\mathbf{v} = \\mathbf{v} - \\mathbf{u}$.\n* 결합 법칙(Associativity): $(\\mathbf{u} - \\mathbf{v}) - \\mathbf{w} = \\mathbf{u} - (\\mathbf{v} + \\mathbf{w})$.\n\n## Scalar Multiplication of Vectors\n\n벡터의 스칼라 곱은 벡터의 각 구성 요소를 스칼라 값으로 곱하는 과정이다. 만약 $\\textbf{a}$ 가 벡터이고 $k$ 가 스칼라라면, 스칼라 곱 $\\textbf{c} = k\\textbf{a}$ 는 $\\textbf{a}$ 의 $i$ 번째 구성 요소를 $k$ 배한 값으로 이루어진 벡터이다. 형식적인 정의는 다음과 같다:\n\n$$\n\\begin{align*}\n  \\textbf{c}&=k\\textbf{a}\\\\\n  c_i &= ka_i\n\\end{align*}\n$$\n\n예를 들어, 만약 $\\textbf{a} = [1, 2, 3]$ 이고 $k = 2$ 라면, 그들의 스칼라 곱인 $\\textbf{c} = [2, 4, 6]$ 이다.\n\n[Reference: Read This Article with Interactive Visualization - Properties of Vector Arithmetic](http://immersivemath.com/ila/ch02_vectors/ch02.html#sec_vec_arithmetic)\n\n### Properteis\n\n* Closure: 스칼라 $c$ 와 벡터 $\\mathbf{v}$ 에 대해, 스칼라 곱 $c\\mathbf{v}$ 역시 벡터이다.\n* 스칼라와의 결합법칙: 스칼라 $c$ 와 $d$ 와 벡터 $\\mathbf{v}$ 에 대해, $(cd)\\mathbf{v} = c(d\\mathbf{v})$ 이다.\n* 스칼라와의 분배법칙: 스칼라 $c$ 와 $d$ 와 벡터 $\\mathbf{v}$ 에 대해, $(c+d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}$ 이다.\n* 벡터와의 분배법칙: 스칼라 $c$와 벡터 $\\mathbf{u}$, $\\mathbf{v}$에 대해, $c(\\mathbf{u}+\\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}$ 이다.\n* 곱의 항등원: 벡터 $\\mathbf{v}$ 에 대해, $1\\mathbf{v} = \\mathbf{v}$ 이며, 여기서 $1$ 은 곱의 항등원이다.\n\n## Location Vector\n\n점 $P$ 에 대한 원점에 상대적인 위치 벡터는 벡터 $\\mathbf{OP}$ 로 정의될 수 있으며, 여기서 $\\mathbf{O}$ 는 원점을 나타낸다. 이는 원점의 위치 벡터 ($\\mathbf{O}$)를 점의 위치 벡터 ($\\mathbf{P}$)에서 빼는 것으로 계산할 수 있다:\n\n$$ \n\\mathbf{OP} = \\mathbf{P} - \\mathbf{O}\n$$\n\n$$\nP_1=(x_1,y_1)$, $P_2=(x_2,y_2)$, $P_1P_2=(x_2-x_1,y_2-y_1)\n$$\n\n위치 벡터는 벡터를 한 점으로 표현한다.\n\n예를 들어, 두 벡터 $ \\mathbf{v} = \\begin{bmatrix} 2 \\ 3 \\ -1 \\end{bmatrix} $ 와 $ \\mathbf{w} = \\begin{bmatrix} -4 \\ 1 \\ 5 \\end{bmatrix} $ 를 고려할 때 원점에 대한 점 $ P $ 의 위치 벡터는 다음과 같이 계산된다:\n\n$$ \n\\mathbf{OP} = \\mathbf{P} - \\mathbf{O} = \\mathbf{w} - \\mathbf{v} = \\begin{bmatrix} -4 \\\\ 1 \\\\ 5 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} -6 \\\\ -2 \\\\ 6 \\end{bmatrix} \n$$\n\n따라서, 위치 벡터 $ \\mathbf{OP}$ 는 실제로 $\\begin{bmatrix} -6 \\ -2 \\ 6 \\end{bmatrix}$ 이다.\n\n### Properties\n\n1. Identity Element: 원점의 위치 벡터를 나타내는 $ \\mathbf{0} $ 은 벡터 덧셈에 대한 항등원 역할을 한다.\n  $$\n  \\mathbf{v} + \\mathbf{0} = \\mathbf{v}\n  $$\n2. Additive Inverse: 모든 위치 벡터 $ \\mathbf{v} $ 에 대해, $ -\\mathbf{v} $ 라는 벡터가 존재하여 그들의 합이 영 벡터가 된다.\n  $$\n  \\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\n  $$\n3. Scalar Multiplication: 위치 벡터 $\\mathbf{v}$ 의 스칼라 곱은 스칼라 $c$ 와의 곱셈이 벡터 덧셈과 스칼라 덧셈에 대해 분배 법칙을 따른다:\n  $$\n  c(\\mathbf{v} + \\mathbf{w}) = c\\mathbf{v} + c\\mathbf{w} and (c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}\n  $$\n4. Scalar Identity: 위치 벡터 $\\mathbf{v}$ 를 스칼라 1로 곱하면 벡터는 변경되지 않는다.\n  $$\n  1 \\cdot \\mathbf{v} = \\mathbf{v}\n  $$\n5. Associativity of Vector Addition: 벡터 덧셈은 결합법칙을 따른다. 즉, 위치 벡터 $\\mathbf{u}$, $\\mathbf{v}$, $\\mathbf{w}$ 에 대하여 다음이 성립한다.\n  $$\n  (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\n  $$\n6. Commutativity of Vector Addition: 벡터 덧셈은 교환법칙을 따른다. 즉, 위치 벡터 $\\mathbf{u}$ 와 $\\mathbf{v}$ 에 대하여 다음이 성립한다:\n  $$\n  \\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\n  $$\n\n\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n# Introduction\n\nDeep learning is a pile of neural networks that are made up of layers of interconnected nodes or neurons, and the weights of the connections between them are learned through a process called backpropagation.\n\nLinear algebra is fundamental to deep learning because many of the computations involved in training neural networks can be expressed as linear algebra operations. For example, matrix multiplication is used to compute the output of each layer in a neural network, and the gradients of the loss function with respect to the weights are computed using the chain rule of calculus, which involves matrix multiplication and vector operations.\n\nIn addition to matrix multiplication, other linear algebra concepts such as eigenvectors, eigenvalues, and singular value decomposition (SVD) are also important in deep learning. For example, SVD can be used to reduce the dimensionality of a dataset or to compute principal components, which are useful for data visualization and feature extraction.\n\nLinear algebra libraries such as Numpy, Scipy, and PyTorch provide efficient implementations of these operations, which are essential for training large-scale neural networks on GPUs. Without these libraries, implementing deep learning algorithms would be much more difficult and time-consuming.\n\n[Reference: Motivation to Learn Linear Algebra](https://nbviewer.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb)\n\n# Physical Quantity\n\n## Scalar\n\n* A scalar is a single mathematical quantity, usually a real number, which can be represented by a single value. Scalars are typically denoted by lowercase letters, such as $a, b, c,$ and so on.\n* Simply, a scalar means a magnitude of a certain physical quantity such as temperature and height.\n\n## Vector\n\n* A vector $\\textbf{v}$ is a mathematical object that represents a quantity with both a magnitude and a direction such as velocity and force.\n* In $n$-dimensional Euclidean space $\\mathbb{R}^n$, a vector $\\textbf{v}$ is typically represented as an ordered list of $n$ real numbers:\n\n* the magnitude of $\\mathbf{v} =\\begin{bmatrix} x\\\\  y \\end{bmatrix}$ is $||\\mathbf{v}|| = \\sqrt{x^{2} + y^{2}}$\n* the direction of it is the angle with the x axis, $\\theta=\\tan^{-1}(\\frac{y}{x})$\n* If magnitude and vector are equal, then they are equal vectors\n\n::: {.callout-note}\n\n## Vector in Mathematics vs Physics\n\n### Vecotr in Mathematics\n\n* A vector is an abstract mathematical object that represents a quantity with both magnitude and direction. \n* Vectors are typically represented as ordered sets of numbers (a list in programming), often arranged in a column or row, known as vector components. \n* Vectors in mathematics can be defined in various vector spaces, such as Euclidean space or abstract vector spaces, and they follow specific algebraic rules for addition, scalar multiplication, and dot product.\n\n### Vecotr in Physics\n\n* Vectors are used to represent and analyze physical quantities and phenomena.\n* Vectors are used to describe physical quantities that have both magnitude and direction such as displacement, velocity, acceleration, force, and momentum. \n* Vectors in physics are often represented by **arrows**, where the length of the arrow represents the **magnitude** of the vector, and the **direction** of the arrow represents the direction of the vector. \n* Vectors in physics obey the laws of vector addition and scalar multiplication, and they can be combined and manipulated using vector algebra and calculus to analyze and solve problems in various branches of physics.\n\nThis blog focuses on the mathematical perspective of a vector.\n:::\n\n$$\n\\textbf{v}=\n\\begin{bmatrix}\n  v_1 \\\\\n  v_2 \\\\\n  \\vdots \\\\\n  v_n\n\\end{bmatrix}\n$$\n\nwhere $v_1, v_2, \\ldots, v_n$ are the components of the vector $\\textbf{v}$.\n\n### Plotting Vectors on the Coordinate Plane\n\nExample \n\nMap $\\begin{bmatrix} 3\\\\  2 \\end{bmatrix}$ into $x=3$, $y=2$ on the Coordinate Plane\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.quiver(0, 0, 3, 2, color='b', angles='xy', scale_units='xy', scale=1)\nplt.xlim(-0.5, 3.5)\nplt.ylim(-0.5, 2.5)\nplt.grid()\nplt.show()\n\n```\n\n[Reference: Read This Article with Interactive Visualization - Points and Vectors](http://immersivemath.com/ila/ch02_vectors/ch02.html#auto_label_33)\n\n# Basic Vector Operations\n\n## Addition of Vectors\n\nThe addition of two vectors is the process of adding their corresponding components. If $\\textbf{a}$ and $\\textbf{b}$ are two vectors of the same dimension, then their sum $\\textbf{c} = \\textbf{a} + \\textbf{b}$ is a vector whose $i$-th component is the sum of the $i$-th components of $\\textbf{a}$ and $\\textbf{b}$. \n\n$$\n\\begin{align*}\n  \\textbf{c}&=\\textbf{a}+\\textbf{b}\\\\\n  c_i &= a_i + b_i\n\\end{align*}\n$$\n\nFor example, if $\\textbf{a} = [3, 2]$ and $\\textbf{b} = [-2,1]$, then their sum $\\textbf{c} = [1,3]$.\n\n::: {layout-ncol=3}\n![](images/chap02_01.PNG)\n\n![](images/chap02_02.PNG)\n\n![](images/chap02_03.PNG)\n:::\n\n### Properties\n\n* Commutative Property:  The order of addition does not affect the result\n  $$ \n  \\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\n  $$\n* Associative Property: The grouping of vectors in an addition does not affect the result.\n  $$ \n  \\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}\n  $$\n* Identity Element: There exists an identity vector, denoted as $\\mathbf{0}$ such that adding it to any vector yields the same vector. \n  $$ \n  \\mathbf{v} + \\mathbf{0} = \\mathbf{v}\n  $$\n* Inverse Element: For every vector $\\mathbf{v}$, there exists an additive inverse vector, denoted as $−\\mathbf{v}$, such that adding it to $\\mathbf{v}$ yields the identity vector. \n  $$\n  \\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\n  $$\n\n::: {.callout-tip}\n## Why to check properties of definitions?\n\nWhen certain operations and particular sets are defined, it is important to know their properties. For example, if it is proven that it is exactly the same as the properties of the real set we know, all the rules and theorems proved in the real set can be applied to the newly defined set.\n\nUnderstanding the properties of operations and sets allows us to apply existing knowledge and theorems from one domain to another, as long as the properties align. This is particularly useful when working with new mathematical structures or sets that have similar properties to well-known sets like the real numbers. By establishing these connections, we can leverage existing theories and results to analyze and solve problems in the new context. It provides a powerful framework for extending mathematical concepts and applying established principles to new domains.\n:::\n\n## Subtraction of Vectors\n\nThe subtraction of two vectors is the process of subtracting their corresponding components. If $\\textbf{a}$ and $\\textbf{b}$ are two vectors of the same dimension, then their difference $\\textbf{c} = \\textbf{a} - \\textbf{b}$ is a vector whose $i$-th component is the difference between the $i$-th components of $\\textbf{a}$ and $\\textbf{b}$. The formal definition is:\n\n$$\n\\begin{align*}\n  \\textbf{c}&=\\textbf{a} - \\textbf{b}\\\\\n  c_i &= a_i - b_i\n\\end{align*}\n$$\n\nFor example, if $\\textbf{a} = [1, 2, 3]$ and $\\textbf{b} = [4, 5, 6]$, then their difference $\\textbf{c} = [-3, -3, -3]$.\n\n### Properties\n\n* Closure: Vector subtraction is closed under the vector space. If two vectors are in the given vector space, then $\\mathbf{u} - \\mathbf{v}$ is a vector.\n* Existence of Additive Inverse: For every vector $\\mathbf{u}$, there exists an additive inverse $-\\mathbf{u}$ such that $\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}$.\n* Commutativity: $\\mathbf{u} - \\mathbf{v} = \\mathbf{v} - \\mathbf{u}$.\n* Associativity: $(\\mathbf{u} - \\mathbf{v}) - \\mathbf{w} = \\mathbf{u} - (\\mathbf{v} + \\mathbf{w})$.\n\n## Scalar Multiplication of Vectors\n\nThe scalar multiplication of a vector is the process of multiplying each component of the vector by a scalar. If $\\textbf{a}$ is a vector and $k$ is a scalar, then the scalar multiple $\\textbf{c} = k\\textbf{a}$ is a vector whose $i$-th component is $k$ times the $i$-th component of $\\textbf{a}$. The formal definition is:\n\n$$\n\\begin{align*}\n  \\textbf{c}&=k\\textbf{a}\\\\\n  c_i &= ka_i\n\\end{align*}\n$$\n\nFor example, if $\\textbf{a} = [1, 2, 3]$ and $k = 2$, then their scalar multiple $\\textbf{c} = [2, 4, 6]$.\n\n[Reference: Read This Article with Interactive Visualization - Properties of Vector Arithmetic](http://immersivemath.com/ila/ch02_vectors/ch02.html#sec_vec_arithmetic)\n\n### Properteis\n\n* Closure: For a scalar $c$ and a vector $\\mathbf{v}$, the scalar multiple $c\\mathbf{v}$ is also a vector.\n* Associativity with Scalars: For scalars $c$ and $d$ and a vector $\\mathbf{v}$, $(cd)\\mathbf{v} = c(d\\mathbf{v})$.\n* Distributivity with Scalars: For scalars $c$ and $d$ and a vector $\\mathbf{v}$, $(c+d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}$.\n* Distributivity with Vectors: For a scalar $c$ and vectors $\\mathbf{u}$ and $\\mathbf{v}$, $c(\\mathbf{u}+\\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}$.\n* Multiplicative Identity: For a vector $\\mathbf{v}$, $1\\mathbf{v} = \\mathbf{v}$, where $1$ is the multiplicative identity.\n\n## Location Vector\n\nThe location vector of a point $P$ relative to the origin can be defined as the vector $\\mathbf{OP}$, where $ \\mathbf{O}$ is the origin. It can be calculated by subtracting the position vector of the origin ($ \\mathbf{O} $) from the position vector of the point ($ \\mathbf{P}$). Mathematically, it is represented as:\n\n$$ \n\\mathbf{OP} = \\mathbf{P} - \\mathbf{O}\n$$\n\na location vector represent a vector as a point.\n\nFor example, let's consider two vectors $ \\mathbf{v} = \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\end{bmatrix} $ and $ \\mathbf{w} = \\begin{bmatrix} -4 \\\\ 1 \\\\ 5 \\end{bmatrix} $. The location vector of point $ P $ relative to the origin can be calculated as:\n\n$$ \n\\mathbf{OP} = \\mathbf{P} - \\mathbf{O} = \\mathbf{w} - \\mathbf{v} = \\begin{bmatrix} -4 \\\\ 1 \\\\ 5 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} -6 \\\\ -2 \\\\ 6 \\end{bmatrix} \n$$\n\nSo, the location vector $ \\mathbf{OP} $ is $ \\begin{bmatrix} -6 \\\\ -2 \\\\ 6 \\end{bmatrix} $.\n\n### Properties\n\n1. Identity Element: The location vector of the origin, denoted as $\\mathbf{0}$, acts as the identity element for vector addition\n  $$\n  \\mathbf{v} + \\mathbf{0} = \\mathbf{v}\n  $$\n2. Additive Inverse: For every location vector $\\mathbf{v}$, there exists a vector $-\\mathbf{v}$ such that their sum is the zero vector:\n  $$\n  \\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\n  $$\n3. Scalar Multiplication: Scalar multiplication of a location vector $\\mathbf{v}$ by a scalar $c$ distributes over vector addition and scalar addition:\n  $$\n  c(\\mathbf{v} + \\mathbf{w}) = c\\mathbf{v} + c\\mathbf{w} and (c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}\n  $$\n4. Scalar Identity: Multiplying a location vector $\\mathbf{v}$ by a scalar 1 leaves the vector unchanged:\n  $$\n  1 \\cdot \\mathbf{v} = \\mathbf{v}\n  $$\n5. Associativity of Vector Addition: Vector addition is associative, meaning that for location vectors $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{w}$:\n  $$\n  (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\n  $$\n6. Commutativity of Vector Addition: Vector addition is commutative, meaning that for location vectors $\\mathbf{u}$ and $\\mathbf{v}$:\n  $$\n  \\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\n  $$\n\n:::{#thm-locationVector}\n$P_1=(x_1,y_1)$, $P_2=(x_2,y_2)$, $P_1P_2=(x_2-x_1,y_2-y_1)$\n:::\n\n\n</div>\n","srcMarkdownNoYaml":"\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n# Introduction\n\n딥러닝은 상호 연결된 노드 또는 뉴런들의 계층으로 이루어진 신경망의 집합이며, 이들 간의 연결 가중치는 역전파라는 과정을 통해 학습된다\n\n선형 대수학은 딥러닝에 있어서 근본이 되는 역할을 하는데, 신경망의 학습에 관련된 많은 계산은 선형 대수 연산으로 표현될 수 있다. 예를 들어, 행렬 곱셈은 신경망의 각 계층의 출력을 계산하는 데 사용되며, 가중치에 대한 손실 함수의 그래디언트는 행렬 곱셈과 벡터 연산을 포함한 미분의 연쇄 법칙을 사용하여 계산된다.\n\n행렬 곱셈 외에도, 고유벡터, 고유값, 특이값 분해(SVD)와 같은 다른 선형 대수학적 개념들도 딥러닝에서 중요하다. 예를 들어, SVD는 데이터셋의 차원을 축소하거나 주성분을 계산하는 데 사용된다. 이는 데이터 시각화와 특징 추출에 유용하다.\n\nNumpy, Scipy, PyTorch와 같은 선형 대수학 라이브러리들은 이러한 연산의 효율적인 구현을 제공하여 GPU에서 대규모 신경망을 훈련하는 데 필수적이다. 이러한 라이브러리 없이 딥러닝 알고리즘을 구현하는 것은 훨씬 어렵고 시간이 많이 소요된다.\n\n[Reference: Motivation to Learn Linear Algebra](https://nbviewer.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb)\n\n# Physical Quantity\n\n## Scalar\n\n* 스칼라는 일반적으로 실수인 하나의 수학적인 양으로, 단일 값으로 나타낼 수 있다. 스칼라는 일반적으로 $a, b, c$ 와 같은 소문자로 표기된다.\n* 간단히 말해서, 스칼라는 온도나 높이와 같은 특정 물리적 양의 크기를 의미한다.\n\n## Vector\n\n* 벡터 $\\textbf{v}$ 는 속도와 힘과 같은 크기와 방향을 모두 가진 수학적 개체이다.\n* $n$ 차원 유클리드 공간 $\\mathbb{R}^n$ 에서, 벡터 $\\textbf{v}$ 는 일반적으로 $n$ 개의 실수로 이루어진 순서가 있는 리스트로 표현된다\n* $\\mathbf{v} =\\begin{bmatrix} x\\ y \\end{bmatrix}$ 일 때, $\\mathbf{v}$ 의 크기는 $||\\mathbf{v}|| = \\sqrt{x^{2} + y^{2}}$ 이다.\n* $\\mathbf{v}$ 의 방향은 $x$ 축과 이루는 각도로 표현되며, $\\theta=\\tan^{-1}(\\frac{y}{x})$ 이다. 만약 크기와 벡터가 같다면, 그들은 같은 벡터이다.\n\n::: {.callout-note}\n\n## Vector in Mathematics vs Physics\n\n### Vecotr in Mathematics\n\n* 벡터는 크기와 방향을 함께 나타내는 추상적인 수학적 객체이다.\n* 벡터는 일반적으로 순서가 있는 숫자들의 집합으로 표현된다(프로그래밍에서의 리스트와 유사한 형태), 종종 열 또는 행으로 배열되며 이를 벡터 구성 요소라고 한다.\n* 수학에서 벡터는 유클리드 공간이나 추상적인 벡터 공간과 같은 다양한 벡터 공간에서 정의되며, 덧셈, 스칼라 곱셈, 내적과 같은 특정 대수적 규칙을 따른다.\n\n### Vecotr in Physics\n\n* 벡터는 물리적인 양과 현상을 나타내고 분석하는 데 사용된다.\n* 벡터는 이동, 속도, 가속도, 힘, 운동량과 같이 크기와 방향을 모두 갖는 물리적인 양을 설명하는 데 사용된다.\n* 물리학에서 벡터는 종종 화살표로 표현되며, 화살표의 길이는 벡터의 크기를 나타내고, 화살표의 방향은 벡터의 방향을 나타낸다.\n* 물리학에서의 벡터는 벡터 덧셈과 스칼라 곱셈의 법칙을 따르며, 벡터 대수와 미적분을 사용하여 다양한 물리학 분야에서 문제를 분석하고 해결하는 데 활용될 수 있다.\n\n이 블로그는 벡터의 수학적인 관점에 초점을 맞춘다.\n:::\n\n$$\n\\textbf{v}=\n\\begin{bmatrix}\n  v_1 \\\\\n  v_2 \\\\\n  \\vdots \\\\\n  v_n\n\\end{bmatrix}\n$$\n\n여기서 $v_1, v_2, \\ldots, v_n$ 은 벡터 $\\textbf{v}$ 의 구성 요소이다.\n\n### Plotting Vectors on the Coordinate Plane\n\nMap $\\begin{bmatrix} 3\\\\  2 \\end{bmatrix}$ into $x=3$, $y=2$ on the Coordinate Plane\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.quiver(0, 0, 3, 2, color='b', angles='xy', scale_units='xy', scale=1)\nplt.xlim(-0.5, 3.5)\nplt.ylim(-0.5, 2.5)\nplt.grid()\nplt.show()\n\n```\n\n[Reference: Read This Article with Interactive Visualization - Points and Vectors](http://immersivemath.com/ila/ch02_vectors/ch02.html#auto_label_33)\n\n# Basic Vector Operations\n\n## Addition of Vectors\n\n두 벡터의 덧셈은 해당하는 구성 요소들을 더하는 과정이다. 만약 $\\textbf{a}$ 와 $\\textbf{b}$ 가 같은 차원의 두 벡터라면, 그들의 합인 $\\textbf{c} = \\textbf{a} + \\textbf{b}$ 는 $\\textbf{a}$ 와 $\\textbf{b}$ 의 $i$ 번째 구성 요소를 더한 값으로 이루어진 벡터이다.\n\n$$\n\\begin{align*}\n  \\textbf{c}&=\\textbf{a}+\\textbf{b}\\\\\n  c_i &= a_i + b_i\n\\end{align*}\n$$\n\n예를 들어, 만약 $\\textbf{a} = [3, 2]$ 이고 $\\textbf{b} = [-2, 1]$ 이라면, 그들의 합인 $\\textbf{c} = [1, 3]$ 이 된다.\n\n::: {layout-ncol=3}\n![](images/chap02_01.PNG)\n\n![](images/chap02_02.PNG)\n\n![](images/chap02_03.PNG)\n:::\n\n### Properties\n\n* 교환 법칙(Commutative Property): 덧셈의 순서는 결과에 영향을 주지 않는다.\n  $$ \n  \\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\n  $$\n* 결합 법칙(Associative Property): 덧셈에서 벡터들의 그룹화는 결과에 영향을 주지 않는다.\n  $$ \n  \\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}\n  $$\n* 항등 원소(Identity Element): $\\mathbf{0}$ 로 표시되는 항등 벡터가 존재하여, 어떤 벡터에 더해져도 동일한 벡터를 얻을 수 있다. \n  $$ \n  \\mathbf{v} + \\mathbf{0} = \\mathbf{v}\n  $$\n* 역원소(Inverse Element): 모든 벡터 $\\mathbf{v}$ 에 대해, $−\\mathbf{v}$ 로 표시되는 덧셈 역원소가 존재하며, 이를 $\\mathbf{v}$ 에 더하면 항등 벡터를 얻을 수 있다.\n  $$\n  \\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\n  $$\n\n::: {.callout-tip}\n## Why to check properties of definitions?\n\n특정 연산과 특정 집합이 정의되었을 때, 그들의 속성을 알아야 하는 것이 중요하다. 예를 들어, 만약 그것이 우리가 알고 있는 실수 집합의 속성과 완전히 동일하다는 것이 증명된다면, 실수 집합에서 증명된 모든 규칙과 정리를 새로 정의된 집합에도 적용할 수 있다.\n\n연산과 집합의 속성을 이해하는 것은 우리가 기존 도메인에서 얻은 지식과 정리를 다른 도메인에 적용할 수 있도록 해준다. 이는 특히 새로운 수학적 구조나 실수와 유사한 속성을 갖는 집합과 함께 작업할 때 유용하다. 이러한 연결을 확립함으로써, 우리는 기존의 이론과 결과를 활용하여 새로운 문맥에서 문제를 분석하고 해결하는 데 활용할 수 있다. 이는 수학적 개념을 확장하고 기존의 원칙을 새로운 도메인에 적용하는 강력한 프레임워크를 제공한다.\n:::\n\n## Subtraction of Vectors\n\n두 벡터의 뺄셈은 해당하는 구성 요소들을 빼는 과정이다. 만약 $\\textbf{a}$ 와 $\\textbf{b}$ 가 같은 차원의 두 벡터라면, 그들의 차인 $\\textbf{c} = \\textbf{a} - \\textbf{b}$ 는 $\\textbf{a}$ 와 $\\textbf{b}$ 의 $i$ 번째 구성 요소를 빼서 얻은 값으로 이루어진 벡터이다. 형식적인 정의는 다음과 같다:\n\n$$\n\\begin{align*}\n  \\textbf{c}&=\\textbf{a} - \\textbf{b}\\\\\n  c_i &= a_i - b_i\n\\end{align*}\n$$\n\n예를 들어, 만약 $\\textbf{a} = [1, 2, 3]$ 이고 $\\textbf{b} = [4, 5, 6]$ 라면, 그들의 차인 $\\textbf{c} = [-3, -3, -3]$ 이다.\n\n### Properties\n\n* 닫힘(Closure): 벡터 뺄셈은 주어진 벡터 공간에서 닫혀 있다. 만약 두 벡터가 주어진 벡터 공간에 속한다면, $\\mathbf{u} - \\mathbf{v}$ 도 벡터이다.\n* 가감 역원의 존재(Existence of Additive Inverse): 모든 벡터 $\\mathbf{u}$ 에 대해, $-\\mathbf{u}$ 라는 가감 역원이 존재하여 $\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}$ 이다.\n* 교환 법칙(Commutativity): $\\mathbf{u} - \\mathbf{v} = \\mathbf{v} - \\mathbf{u}$.\n* 결합 법칙(Associativity): $(\\mathbf{u} - \\mathbf{v}) - \\mathbf{w} = \\mathbf{u} - (\\mathbf{v} + \\mathbf{w})$.\n\n## Scalar Multiplication of Vectors\n\n벡터의 스칼라 곱은 벡터의 각 구성 요소를 스칼라 값으로 곱하는 과정이다. 만약 $\\textbf{a}$ 가 벡터이고 $k$ 가 스칼라라면, 스칼라 곱 $\\textbf{c} = k\\textbf{a}$ 는 $\\textbf{a}$ 의 $i$ 번째 구성 요소를 $k$ 배한 값으로 이루어진 벡터이다. 형식적인 정의는 다음과 같다:\n\n$$\n\\begin{align*}\n  \\textbf{c}&=k\\textbf{a}\\\\\n  c_i &= ka_i\n\\end{align*}\n$$\n\n예를 들어, 만약 $\\textbf{a} = [1, 2, 3]$ 이고 $k = 2$ 라면, 그들의 스칼라 곱인 $\\textbf{c} = [2, 4, 6]$ 이다.\n\n[Reference: Read This Article with Interactive Visualization - Properties of Vector Arithmetic](http://immersivemath.com/ila/ch02_vectors/ch02.html#sec_vec_arithmetic)\n\n### Properteis\n\n* Closure: 스칼라 $c$ 와 벡터 $\\mathbf{v}$ 에 대해, 스칼라 곱 $c\\mathbf{v}$ 역시 벡터이다.\n* 스칼라와의 결합법칙: 스칼라 $c$ 와 $d$ 와 벡터 $\\mathbf{v}$ 에 대해, $(cd)\\mathbf{v} = c(d\\mathbf{v})$ 이다.\n* 스칼라와의 분배법칙: 스칼라 $c$ 와 $d$ 와 벡터 $\\mathbf{v}$ 에 대해, $(c+d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}$ 이다.\n* 벡터와의 분배법칙: 스칼라 $c$와 벡터 $\\mathbf{u}$, $\\mathbf{v}$에 대해, $c(\\mathbf{u}+\\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}$ 이다.\n* 곱의 항등원: 벡터 $\\mathbf{v}$ 에 대해, $1\\mathbf{v} = \\mathbf{v}$ 이며, 여기서 $1$ 은 곱의 항등원이다.\n\n## Location Vector\n\n점 $P$ 에 대한 원점에 상대적인 위치 벡터는 벡터 $\\mathbf{OP}$ 로 정의될 수 있으며, 여기서 $\\mathbf{O}$ 는 원점을 나타낸다. 이는 원점의 위치 벡터 ($\\mathbf{O}$)를 점의 위치 벡터 ($\\mathbf{P}$)에서 빼는 것으로 계산할 수 있다:\n\n$$ \n\\mathbf{OP} = \\mathbf{P} - \\mathbf{O}\n$$\n\n$$\nP_1=(x_1,y_1)$, $P_2=(x_2,y_2)$, $P_1P_2=(x_2-x_1,y_2-y_1)\n$$\n\n위치 벡터는 벡터를 한 점으로 표현한다.\n\n예를 들어, 두 벡터 $ \\mathbf{v} = \\begin{bmatrix} 2 \\ 3 \\ -1 \\end{bmatrix} $ 와 $ \\mathbf{w} = \\begin{bmatrix} -4 \\ 1 \\ 5 \\end{bmatrix} $ 를 고려할 때 원점에 대한 점 $ P $ 의 위치 벡터는 다음과 같이 계산된다:\n\n$$ \n\\mathbf{OP} = \\mathbf{P} - \\mathbf{O} = \\mathbf{w} - \\mathbf{v} = \\begin{bmatrix} -4 \\\\ 1 \\\\ 5 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} -6 \\\\ -2 \\\\ 6 \\end{bmatrix} \n$$\n\n따라서, 위치 벡터 $ \\mathbf{OP}$ 는 실제로 $\\begin{bmatrix} -6 \\ -2 \\ 6 \\end{bmatrix}$ 이다.\n\n### Properties\n\n1. Identity Element: 원점의 위치 벡터를 나타내는 $ \\mathbf{0} $ 은 벡터 덧셈에 대한 항등원 역할을 한다.\n  $$\n  \\mathbf{v} + \\mathbf{0} = \\mathbf{v}\n  $$\n2. Additive Inverse: 모든 위치 벡터 $ \\mathbf{v} $ 에 대해, $ -\\mathbf{v} $ 라는 벡터가 존재하여 그들의 합이 영 벡터가 된다.\n  $$\n  \\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\n  $$\n3. Scalar Multiplication: 위치 벡터 $\\mathbf{v}$ 의 스칼라 곱은 스칼라 $c$ 와의 곱셈이 벡터 덧셈과 스칼라 덧셈에 대해 분배 법칙을 따른다:\n  $$\n  c(\\mathbf{v} + \\mathbf{w}) = c\\mathbf{v} + c\\mathbf{w} and (c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}\n  $$\n4. Scalar Identity: 위치 벡터 $\\mathbf{v}$ 를 스칼라 1로 곱하면 벡터는 변경되지 않는다.\n  $$\n  1 \\cdot \\mathbf{v} = \\mathbf{v}\n  $$\n5. Associativity of Vector Addition: 벡터 덧셈은 결합법칙을 따른다. 즉, 위치 벡터 $\\mathbf{u}$, $\\mathbf{v}$, $\\mathbf{w}$ 에 대하여 다음이 성립한다.\n  $$\n  (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\n  $$\n6. Commutativity of Vector Addition: 벡터 덧셈은 교환법칙을 따른다. 즉, 위치 벡터 $\\mathbf{u}$ 와 $\\mathbf{v}$ 에 대하여 다음이 성립한다:\n  $$\n  \\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\n  $$\n\n\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n# Introduction\n\nDeep learning is a pile of neural networks that are made up of layers of interconnected nodes or neurons, and the weights of the connections between them are learned through a process called backpropagation.\n\nLinear algebra is fundamental to deep learning because many of the computations involved in training neural networks can be expressed as linear algebra operations. For example, matrix multiplication is used to compute the output of each layer in a neural network, and the gradients of the loss function with respect to the weights are computed using the chain rule of calculus, which involves matrix multiplication and vector operations.\n\nIn addition to matrix multiplication, other linear algebra concepts such as eigenvectors, eigenvalues, and singular value decomposition (SVD) are also important in deep learning. For example, SVD can be used to reduce the dimensionality of a dataset or to compute principal components, which are useful for data visualization and feature extraction.\n\nLinear algebra libraries such as Numpy, Scipy, and PyTorch provide efficient implementations of these operations, which are essential for training large-scale neural networks on GPUs. Without these libraries, implementing deep learning algorithms would be much more difficult and time-consuming.\n\n[Reference: Motivation to Learn Linear Algebra](https://nbviewer.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb)\n\n# Physical Quantity\n\n## Scalar\n\n* A scalar is a single mathematical quantity, usually a real number, which can be represented by a single value. Scalars are typically denoted by lowercase letters, such as $a, b, c,$ and so on.\n* Simply, a scalar means a magnitude of a certain physical quantity such as temperature and height.\n\n## Vector\n\n* A vector $\\textbf{v}$ is a mathematical object that represents a quantity with both a magnitude and a direction such as velocity and force.\n* In $n$-dimensional Euclidean space $\\mathbb{R}^n$, a vector $\\textbf{v}$ is typically represented as an ordered list of $n$ real numbers:\n\n* the magnitude of $\\mathbf{v} =\\begin{bmatrix} x\\\\  y \\end{bmatrix}$ is $||\\mathbf{v}|| = \\sqrt{x^{2} + y^{2}}$\n* the direction of it is the angle with the x axis, $\\theta=\\tan^{-1}(\\frac{y}{x})$\n* If magnitude and vector are equal, then they are equal vectors\n\n::: {.callout-note}\n\n## Vector in Mathematics vs Physics\n\n### Vecotr in Mathematics\n\n* A vector is an abstract mathematical object that represents a quantity with both magnitude and direction. \n* Vectors are typically represented as ordered sets of numbers (a list in programming), often arranged in a column or row, known as vector components. \n* Vectors in mathematics can be defined in various vector spaces, such as Euclidean space or abstract vector spaces, and they follow specific algebraic rules for addition, scalar multiplication, and dot product.\n\n### Vecotr in Physics\n\n* Vectors are used to represent and analyze physical quantities and phenomena.\n* Vectors are used to describe physical quantities that have both magnitude and direction such as displacement, velocity, acceleration, force, and momentum. \n* Vectors in physics are often represented by **arrows**, where the length of the arrow represents the **magnitude** of the vector, and the **direction** of the arrow represents the direction of the vector. \n* Vectors in physics obey the laws of vector addition and scalar multiplication, and they can be combined and manipulated using vector algebra and calculus to analyze and solve problems in various branches of physics.\n\nThis blog focuses on the mathematical perspective of a vector.\n:::\n\n$$\n\\textbf{v}=\n\\begin{bmatrix}\n  v_1 \\\\\n  v_2 \\\\\n  \\vdots \\\\\n  v_n\n\\end{bmatrix}\n$$\n\nwhere $v_1, v_2, \\ldots, v_n$ are the components of the vector $\\textbf{v}$.\n\n### Plotting Vectors on the Coordinate Plane\n\nExample \n\nMap $\\begin{bmatrix} 3\\\\  2 \\end{bmatrix}$ into $x=3$, $y=2$ on the Coordinate Plane\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.quiver(0, 0, 3, 2, color='b', angles='xy', scale_units='xy', scale=1)\nplt.xlim(-0.5, 3.5)\nplt.ylim(-0.5, 2.5)\nplt.grid()\nplt.show()\n\n```\n\n[Reference: Read This Article with Interactive Visualization - Points and Vectors](http://immersivemath.com/ila/ch02_vectors/ch02.html#auto_label_33)\n\n# Basic Vector Operations\n\n## Addition of Vectors\n\nThe addition of two vectors is the process of adding their corresponding components. If $\\textbf{a}$ and $\\textbf{b}$ are two vectors of the same dimension, then their sum $\\textbf{c} = \\textbf{a} + \\textbf{b}$ is a vector whose $i$-th component is the sum of the $i$-th components of $\\textbf{a}$ and $\\textbf{b}$. \n\n$$\n\\begin{align*}\n  \\textbf{c}&=\\textbf{a}+\\textbf{b}\\\\\n  c_i &= a_i + b_i\n\\end{align*}\n$$\n\nFor example, if $\\textbf{a} = [3, 2]$ and $\\textbf{b} = [-2,1]$, then their sum $\\textbf{c} = [1,3]$.\n\n::: {layout-ncol=3}\n![](images/chap02_01.PNG)\n\n![](images/chap02_02.PNG)\n\n![](images/chap02_03.PNG)\n:::\n\n### Properties\n\n* Commutative Property:  The order of addition does not affect the result\n  $$ \n  \\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\n  $$\n* Associative Property: The grouping of vectors in an addition does not affect the result.\n  $$ \n  \\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}\n  $$\n* Identity Element: There exists an identity vector, denoted as $\\mathbf{0}$ such that adding it to any vector yields the same vector. \n  $$ \n  \\mathbf{v} + \\mathbf{0} = \\mathbf{v}\n  $$\n* Inverse Element: For every vector $\\mathbf{v}$, there exists an additive inverse vector, denoted as $−\\mathbf{v}$, such that adding it to $\\mathbf{v}$ yields the identity vector. \n  $$\n  \\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\n  $$\n\n::: {.callout-tip}\n## Why to check properties of definitions?\n\nWhen certain operations and particular sets are defined, it is important to know their properties. For example, if it is proven that it is exactly the same as the properties of the real set we know, all the rules and theorems proved in the real set can be applied to the newly defined set.\n\nUnderstanding the properties of operations and sets allows us to apply existing knowledge and theorems from one domain to another, as long as the properties align. This is particularly useful when working with new mathematical structures or sets that have similar properties to well-known sets like the real numbers. By establishing these connections, we can leverage existing theories and results to analyze and solve problems in the new context. It provides a powerful framework for extending mathematical concepts and applying established principles to new domains.\n:::\n\n## Subtraction of Vectors\n\nThe subtraction of two vectors is the process of subtracting their corresponding components. If $\\textbf{a}$ and $\\textbf{b}$ are two vectors of the same dimension, then their difference $\\textbf{c} = \\textbf{a} - \\textbf{b}$ is a vector whose $i$-th component is the difference between the $i$-th components of $\\textbf{a}$ and $\\textbf{b}$. The formal definition is:\n\n$$\n\\begin{align*}\n  \\textbf{c}&=\\textbf{a} - \\textbf{b}\\\\\n  c_i &= a_i - b_i\n\\end{align*}\n$$\n\nFor example, if $\\textbf{a} = [1, 2, 3]$ and $\\textbf{b} = [4, 5, 6]$, then their difference $\\textbf{c} = [-3, -3, -3]$.\n\n### Properties\n\n* Closure: Vector subtraction is closed under the vector space. If two vectors are in the given vector space, then $\\mathbf{u} - \\mathbf{v}$ is a vector.\n* Existence of Additive Inverse: For every vector $\\mathbf{u}$, there exists an additive inverse $-\\mathbf{u}$ such that $\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}$.\n* Commutativity: $\\mathbf{u} - \\mathbf{v} = \\mathbf{v} - \\mathbf{u}$.\n* Associativity: $(\\mathbf{u} - \\mathbf{v}) - \\mathbf{w} = \\mathbf{u} - (\\mathbf{v} + \\mathbf{w})$.\n\n## Scalar Multiplication of Vectors\n\nThe scalar multiplication of a vector is the process of multiplying each component of the vector by a scalar. If $\\textbf{a}$ is a vector and $k$ is a scalar, then the scalar multiple $\\textbf{c} = k\\textbf{a}$ is a vector whose $i$-th component is $k$ times the $i$-th component of $\\textbf{a}$. The formal definition is:\n\n$$\n\\begin{align*}\n  \\textbf{c}&=k\\textbf{a}\\\\\n  c_i &= ka_i\n\\end{align*}\n$$\n\nFor example, if $\\textbf{a} = [1, 2, 3]$ and $k = 2$, then their scalar multiple $\\textbf{c} = [2, 4, 6]$.\n\n[Reference: Read This Article with Interactive Visualization - Properties of Vector Arithmetic](http://immersivemath.com/ila/ch02_vectors/ch02.html#sec_vec_arithmetic)\n\n### Properteis\n\n* Closure: For a scalar $c$ and a vector $\\mathbf{v}$, the scalar multiple $c\\mathbf{v}$ is also a vector.\n* Associativity with Scalars: For scalars $c$ and $d$ and a vector $\\mathbf{v}$, $(cd)\\mathbf{v} = c(d\\mathbf{v})$.\n* Distributivity with Scalars: For scalars $c$ and $d$ and a vector $\\mathbf{v}$, $(c+d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}$.\n* Distributivity with Vectors: For a scalar $c$ and vectors $\\mathbf{u}$ and $\\mathbf{v}$, $c(\\mathbf{u}+\\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}$.\n* Multiplicative Identity: For a vector $\\mathbf{v}$, $1\\mathbf{v} = \\mathbf{v}$, where $1$ is the multiplicative identity.\n\n## Location Vector\n\nThe location vector of a point $P$ relative to the origin can be defined as the vector $\\mathbf{OP}$, where $ \\mathbf{O}$ is the origin. It can be calculated by subtracting the position vector of the origin ($ \\mathbf{O} $) from the position vector of the point ($ \\mathbf{P}$). Mathematically, it is represented as:\n\n$$ \n\\mathbf{OP} = \\mathbf{P} - \\mathbf{O}\n$$\n\na location vector represent a vector as a point.\n\nFor example, let's consider two vectors $ \\mathbf{v} = \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\end{bmatrix} $ and $ \\mathbf{w} = \\begin{bmatrix} -4 \\\\ 1 \\\\ 5 \\end{bmatrix} $. The location vector of point $ P $ relative to the origin can be calculated as:\n\n$$ \n\\mathbf{OP} = \\mathbf{P} - \\mathbf{O} = \\mathbf{w} - \\mathbf{v} = \\begin{bmatrix} -4 \\\\ 1 \\\\ 5 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} -6 \\\\ -2 \\\\ 6 \\end{bmatrix} \n$$\n\nSo, the location vector $ \\mathbf{OP} $ is $ \\begin{bmatrix} -6 \\\\ -2 \\\\ 6 \\end{bmatrix} $.\n\n### Properties\n\n1. Identity Element: The location vector of the origin, denoted as $\\mathbf{0}$, acts as the identity element for vector addition\n  $$\n  \\mathbf{v} + \\mathbf{0} = \\mathbf{v}\n  $$\n2. Additive Inverse: For every location vector $\\mathbf{v}$, there exists a vector $-\\mathbf{v}$ such that their sum is the zero vector:\n  $$\n  \\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\n  $$\n3. Scalar Multiplication: Scalar multiplication of a location vector $\\mathbf{v}$ by a scalar $c$ distributes over vector addition and scalar addition:\n  $$\n  c(\\mathbf{v} + \\mathbf{w}) = c\\mathbf{v} + c\\mathbf{w} and (c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}\n  $$\n4. Scalar Identity: Multiplying a location vector $\\mathbf{v}$ by a scalar 1 leaves the vector unchanged:\n  $$\n  1 \\cdot \\mathbf{v} = \\mathbf{v}\n  $$\n5. Associativity of Vector Addition: Vector addition is associative, meaning that for location vectors $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{w}$:\n  $$\n  (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\n  $$\n6. Commutativity of Vector Addition: Vector addition is commutative, meaning that for location vectors $\\mathbf{u}$ and $\\mathbf{v}$:\n  $$\n  \\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\n  $$\n\n:::{#thm-locationVector}\n$P_1=(x_1,y_1)$, $P_2=(x_2,y_2)$, $P_1P_2=(x_2-x_1,y_2-y_1)$\n:::\n\n\n</div>\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"01.basic_vector.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Basic Vector(1) - Vector Operations","subtitle":"Introduction, Scalr, Vector, Addition, Scalar Multiplication","description":"Basic Linear Algebra\n","categories":["Mathematics"],"author":"Kwangmin Kim","date":"03/30/2023","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}