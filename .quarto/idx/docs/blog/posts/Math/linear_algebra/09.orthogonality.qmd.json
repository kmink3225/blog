{"title":"Orthogonality","markdown":{"yaml":{"title":"Orthogonality","subtitle":"Orthogonality of the Four Subspaces, Gram-Schmidt, QR Decomposition,","description":"template\n","categories":["Mathematics"],"author":"Kwangmin Kim","date":"04/21/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false,"execute":{"warning":false}},"headingText":"Orthogonality","containsRefs":false,"markdown":"\n\n```{python}\nimport numpy as np\nimport matplotlib.animation as animation\nimport matplotlib_inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport sympy as sym # for RREF\nimport scipy.linalg # for LU\nimport matplotlib.gridspec as gridspec # used to create non-regular subplots\nfrom scipy.linalg import lstsq # for least square example\n```\n\n\n* Orthogonality of the Four Subspaces\n  * Orthogonal Vectors\n  * Orthogonal Subspaces\n    * Orthogonal Components\n  * Orthogonal Bases\n  * Orthogonal Matrices\n* Orthogonal Vector Decomposition,\n* QR decomposition\n  * 'Q' stands for an orthogonal matrix, and \n  * 'R' stands for an upper triangular matrix. \n* Gram-Schmidt Decomposition, \n* Eigen Decomposition, and \n* Singular Value Decomposition\n\n## Orthogonality of the Four Subspaces\n\nThe four subspaces: vectors, subspaces, orthogonal bases, and orthogonal matrices\n\n### Orthogonal Vectors\n\n:::{#def-orthogonalVec}\nTwo vectors $\\mathbf{v}$ and $\\mathbf{w}$ in $\\mathbb{R}^n$ are said to be orthogonal if their dot product is zero:\n\n$$\n\\mathbf{v} \\cdot \\mathbf{w} = \\sum_{i=1}^n v_i w_i = 0 \\text{ and }||\\mathbf{v}||^2+||\\mathbf{w}||^2=||\\mathbf{v}+\\mathbf{w}||^2\n$$\n:::\n\nGeometrically, two vectors are orthogonal if they are perpendicular to each other.\n\nOrthogonality is an important concept in linear algebra and has many applications, including in the construction of orthonormal bases and in least-squares regression.\n\n#### Examples\n\n**Example1**\n$\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$ and $\\mathbf{w} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$ are orthogonal because $\\mathbf{v} \\cdot \\mathbf{w} = 1 \\cdot 0 + 0 \\cdot 1 + 0 \\cdot 0 = 0$. These vectors are also perpendicular to each other in 3D space.\n\n**Example2**\n$\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$ and $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ -2 \\\\ 1 \\end{bmatrix}$ are orthogonal because $\\mathbf{v} \\cdot \\mathbf{w} = 1 \\cdot 1 + 1 \\cdot (-2) + 1 \\cdot 1 = 0$. These vectors are also perpendicular to each other in 3D space.\n\n**Example3**\n$\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$ and $\\mathbf{w} = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}$ are orthogonal because $\\mathbf{v} \\cdot \\mathbf{w} = 1 \\cdot (-2) + 2 \\cdot 1 = 0$. These vectors are also perpendicular to each other in 2D space.\n\n```{python}\n# Example 1\nv1 = np.array([1, 0, 0])\nw1 = np.array([0, 1, 0])\nprint(np.dot(v1, w1))  # Output: 0\n\n# Example 2\nv2 = np.array([1, 1, 1])\nw2 = np.array([1, -2, 1])\nprint(np.dot(v2, w2))  # Output: 0\n\n# Example 3\nv3 = np.array([1, 2])\nw3 = np.array([-2, 1])\nprint(np.dot(v3, w3))  # Output: 0\n\n# Example 1\nv2 = np.array([1, 1, 1])\nw2 = np.array([1, -2, 1])\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.quiver(0, 0, 0, v2[0], v2[1], v2[2], colors='b', arrow_length_ratio=0.1)\nax.quiver(0, 0, 0, w2[0], w2[1], w2[2], colors='r', arrow_length_ratio=0.1)\nax.set_xlim([-1, 2])\nax.set_ylim([-1, 2])\nax.set_zlim([-1, 2])\nax.set_title(\"Example 1: Orthogonal Vectors\")\nax.legend([\"v\", \"w\"])\nplt.show()\n\n# Example 2\nv1 = np.array([1, 0])\nw1 = np.array([0, 1])\nplt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='b')\nplt.quiver(0, 0, w1[0], w1[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.xlim(-1, 2)\nplt.ylim(-1, 2)\nplt.title(\"Example 2: Orthogonal Vectors\")\nplt.legend([\"v\", \"w\"])\nplt.grid(True)\nplt.show()\n\n\n# Example 3\nv3 = np.array([1, 2])\nw3 = np.array([-2, 1])\nplt.quiver(0, 0, v3[0], v3[1], angles='xy', scale_units='xy', scale=1, color='b')\nplt.quiver(0, 0, w3[0], w3[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nplt.title(\"Example 3: Orthogonal Vectors\")\nplt.legend([\"v\", \"w\"])\nplt.grid(True)\nplt.show()\n```\n\n#### Properties\n\n* Orthogonal vectors have a dot product of zero:\n$$\n\\mathbf{v} \\cdot \\mathbf{w} = 0\n$$\n\n* The magnitude (length) of the projection of a vector onto an orthogonal vector is given by:\n$$\n\\text{proj}_{\\mathbf{w}}(\\mathbf{v}) = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{\\|\\mathbf{w}\\|^2} \\mathbf{w} = 0\n$$\n\n* The Pythagorean theorem holds for orthogonal vectors:\n$$\n\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\n$$\n\n* The angle between two orthogonal vectors is $\\frac{\\pi}{2}$ radians or $90$ degrees:\n$$\n\\theta = \\frac{\\pi}{2}\n$$\n\n* Orthogonal vectors are linearly independent, which means that no vector in the span of one vector can be expressed as a linear combination of the other vector:\n$$\n\\text{span}\\{\\mathbf{v}\\} \\cap \\text{span}\\{\\mathbf{w}\\} = \\{\\mathbf{0}\\}\n$$\n\n![Orthogonal Space-Gilbert Strang: Introduction to Linear Algebra](../../../../../images/linear_algebra/orthogonal_space.PNG)\n\n* The row space is perpendicular to the nullspace\n* The column space is perpendicular to the nullspace of $\\mathbf{A}^T$.\n  * This peroperty $\\mathbf{A}$ plays a key role in solving the equation $\\mathbf{Ax=b}$ but $\\mathbf{b}$ is outside the column space (meaning we can't solve the equation directly). In this case, we use the nullspace of $\\mathbf{A}^T$ to find the \"least-squares\" solution, which gives us the smallest possible error $\\mathbf{e = b - Ax}$ in the solution.\n\n:::{.callout-note}\nWhen $\\mathbf{b}$ is outside the column space of $\\mathbf{A}$, there is no exact solution to the equation $\\mathbf{Ax = b}$. Instead, we seek a solution that minimizes the error $\\mathbf{e = b - Ax}$. The least-squares solution achieves this by finding the projection of $\\mathbf{b}$ onto the column space of $\\mathbf{A}$. It turns out that the projection of $\\mathbf{b}$ onto the column space of $\\mathbf{A}$ is exactly equal to the solution of the equation $\\mathbf{A}^T\\mathbf{Ax} = \\mathbf{A}^T\\mathbf{b}$, which can be solved using the nullspace of $\\mathbf{A}^T$.\n\nIn summary, the statement \"the column space is perpendicular to the nullspace of $\\mathbf{A}^T$\" tells us that the column space and nullspace are orthogonal (i.e., perpendicular) subspaces, and this fact allows us to use the nullspace of $\\mathbf{A}^T$ to find the least-squares solution to $\\mathbf{Ax = b}$.\n:::\n\n##### Least Square Example\n\nSuppose we have a system of equations $\\mathbf{Ax} = \\mathbf{b}$ where $\\mathbf{A}$ is an $m \\times n$ matrix and $\\mathbf{b}$ is an $m \\times 1$ vector, and we want to find the least squares solution to this system (i.e., the solution that minimizes the residual $|\\mathbf{Ax} - \\mathbf{b}|$). If $\\mathbf{A}$ has linearly independent columns, then we can solve for $\\mathbf{x}$ using the formula $\\mathbf{x} = (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{b}$. \n\nHowever, if $\\mathbf{A}$ does not have linearly independent columns, then we can use the fact that the column space of $\\mathbf{A}$ is perpendicular to the nullspace of $\\mathbf{A}^T$ to find the least squares solution.\n\nTo do this, we first find a basis for the column space of $\\mathbf{A}$ and a basis for the nullspace of $\\mathbf{A}^T$. Let $\\mathbf{P}$ be the projection matrix onto the column space of $\\mathbf{A}$, given by $\\mathbf{P} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T$. Then the least squares solution to $\\mathbf{Ax} = \\mathbf{b}$ is given by $\\mathbf{x} = \\mathbf{P}\\mathbf{b}$, and the residual $\\mathbf{e} = \\mathbf{b} - \\mathbf{Ax}$ is in the nullspace of $\\mathbf{A}^T$.\n\n\n```{python}\n#| echo: false\n#| eval: false\n\n# Define the matrix A and the vector b\nA = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9],\n              [10, 11, 12]])\nb = np.array([1, 2, 3, 4])\n\n# Compute the least-squares solution using lstsq from SciPy\nx, res, rank, s = lstsq(A, b)\n\n# Compute the error e = b - Ax\ne = b - A @ x # matrix multiplication\n\n# Compute the projection of b onto the column space of A\nProjected_b = A @ x\n\n# Compute the projection of b onto the orthogonal complement of the column space of A\nProjected_b_perp = b - Projected_b\n\n# Compute the projection of e onto the nullspace of A^T\nProjected_e = np.linalg.pinv(A.T).T @ e\n\n'''\nThe expected value of Projected_e = np.linalg.pinv(A.T) @ e is the projection of the true value x \nonto the column space of A. \nThis is because the least squares solution x_hat is the orthogonal projection of the vector b onto the column space of A, \nwhich is given by x_hat = A @ np.linalg.lstsq(A, b)[0].\n\nSince e = b - Ax_hat is the error in the least squares solution, \nnp.linalg.pinv(A.T) @ e computes the projection of this error vector onto the nullspace of A^T. \nTherefore, the expected value of Projected_e is zero, since the error e is orthogonal to the column space of A and \nits projection onto the nullspace of A^T is also orthogonal to the column space of A.\n\nIn other words, Projected_e is the component of the error e that lies in the nullspace of A^T, and \nsince the nullspace of A^T is orthogonal to the column space of A, the expected value of Projected_e is zero.\n'''\n\n# Compute the projection of e onto the orthogonal complement of the nullspace of A^T\nProjected_e_perp = e - A @ Projected_e\n\n# Verify that the column space is perpendicular to the nullspace of A^T\nassert np.allclose(A @ Projected_e, np.zeros((A.shape[0],)))\n#assert np.allclose(Projected_e_perp @ x, np.zeros((x.shape[0],)))\n\n'''\nIn this example, we define the matrix `A` and the vector `b`, and use the lstsq function from SciPy to compute the least-squares solution `x`. We then compute the error `e = b - Ax`, and project `b` onto the column space of `A` to obtain `Projected_b`, and onto the orthogonal complement of the column space of `A` to obtain `Projected_b_perp`. We also project `e` onto the nullspace of `A^T` to obtain `Projected_e`, and onto the orthogonal complement of the nullspace of `A^T` to obtain `Projected_e_perp`. Finally, we verify that the column space of `A` is perpendicular to the nullspace of `A^T` by checking that `A^T` `Projected_e = 0` and `Projected_e_perp @ x = 0`.\n'''\n```\n\n\n\n```{python}\n#| eval: false\n#| echo: false\n\n# define the matrix A and vector b\nA = np.array([[1, 1], [1, -1], [2, 1], [2, -1]])\nb = np.array([3, 1, 5, 3])\n\n# calculate the least-squares solution\nx = np.linalg.lstsq(A, b, rcond=None)[0]\n\n# create a figure with a 3D axes object\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# plot the points in the column space of A\nx1 = np.linspace(-1, 3, 10)\nx2 = np.linspace(-1, 3, 10)\nX1, X2 = np.meshgrid(x1, x2)\nY = x[0]*X1 + x[1]*X2\nax.plot_surface(X1, X2, Y, alpha=0.2)\n\n# plot the points in the nullspace of A^T\nx1 = np.linspace(-1, 3, 10)\nx3 = np.linspace(-1, 3, 10)\nX1, X3 = np.meshgrid(x1, x3)\nY = np.zeros_like(X1)\nZ = x[0]*X1 + x[1]*Y + x[2]*X3\nax.plot_surface(X1, Y, X3, color='red', alpha=0.2)\n\n# plot the data points and the least-squares solution\nax.scatter(A[:,0], A[:,1], b, color='blue', marker='o', s=50)\nax.scatter(x[0], x[1], x[2], color='green', marker='*', s=100)\n\n# set the axis labels and limits\nax.set_xlabel('x1')\nax.set_ylabel('x2')\nax.set_zlabel('b')\nax.set_xlim(-1, 3)\nax.set_ylim(-1, 3)\nax.set_zlim(0, 6)\n\n# show the plot\nplt.show()\n```\n\n### Orthogonal Subspaces\n\n:::{#def-orthogonalSubspaces}\nTwo subspaces $U$ and $V$ of a vector space $W$ are said to be orthogonal subspaces if every vector in $U$ is orthogonal to every vector in $V$. Symbolically, we write $U \\perp V$ if and only if $\\mathbf{u} \\cdot \\mathbf{v} = 0$ for all $\\mathbf{u} \\in U$ and $\\mathbf{v} \\in V$.\n\n$$\n\\mathbf u^T \\mathbf v = 0\n$$\n:::\n\nEvery vector $\\mathbf{x}$ in the nullspace is perpendicular to every row of $\\mathbf{A}$, because $\\mathbf{Ax=0}$. \nThe $\\operatorname{null}(\\mathbf{A})$ and the row space $\\operatorname{Col}(\\mathbf{A}^T)$ are orthogonal subspaces of $\\mathbb{R}^n$\n\nEvery vector $\\mathbf{y}$ in the nullspace of $\\mathbf{A}^T$ is perpendicular to every column of $\\mathbf{A}$. The left $\\operatorname{null}(\\mathbf{A}^T)$ and the column space $\\operatorname{Col}(\\mathbf{A})$ are orthogonal subspaces in $\\mathbb{R}^n$\n\n![Null(A) $\\perp$ Col(A^T)-Gilbert Strang: Introduction to Linear Algebra](../../../../../images/linear_algebra/Null(A)_Col(A%5ET)_orthogonal.PNG)\n\n\n#### Examples\n\n**Example1**\nLet $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$ and $\\mathbf{u} = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}$ be two vectors in $\\mathbb{R}^3$. Then the subspaces $U = \\text{span}\\{\\mathbf{u}\\}$ and $V = \\text{span}\\{\\mathbf{v}\\}$ are orthogonal subspaces, since $\\mathbf{u} \\cdot \\mathbf{v} = 0$.\n\n$$\nU = \\text{span}\\{\\mathbf{u}\\} = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\right\\}, \\quad\nV = \\text{span}\\{\\mathbf{v}\\} = \\text{span}\\left\\{\\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}\\right\\}\n$$\n\n**Example2**\nLet $U$ be the subspace of $\\mathbb{R}^3$ spanned by the vectors $\\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}$ and $\\begin{bmatrix} 1 \\\\ 0 \\\\ -2 \\end{bmatrix}$, and let $V$ be the subspace spanned by the vector $\\begin{bmatrix} 2 \\\\ -1 \\\\ 1 \\end{bmatrix}$. Then $U$ and $V$ are orthogonal subspaces, since every vector in $U$ is orthogonal to every vector in $V$.\n\n$$\nU = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\\\ -2 \\end{bmatrix}\\right\\}, \\quad\nV = \\text{span}\\left\\{\\begin{bmatrix} 2 \\\\ -1 \\\\ 1 \\end{bmatrix}\\right\\}\n$$\n\n**Example3**\nLet $U$ and $V$ be the subspaces of $\\mathbb{R}^2$ spanned by the vectors $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ and $\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$, respectively. Then $U$ and $V$ are orthogonal subspaces, since $\\begin{bmatrix} 1 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = 0$.\n\n$$\nU = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\right\\}, \\quad\nV = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\right\\}\n$$\n\n```{python}\nu = np.array([1, 1])\nv = np.array([1, -1])\n\n# plot vectors\nplt.figure()\nplt.plot([0, u[0]], [0, u[1]], 'b', label=r'$\\mathbf{u}$')\nplt.plot([0, v[0]], [0, v[1]], 'r', label=r'$\\mathbf{v}$')\nplt.legend()\n\n# plot subspaces\nplt.axline((0, 0), slope=u[1]/u[0], color='b', linestyle='--', label=r'$U$')\nplt.axline((0, 0), slope=v[1]/v[0], color='r', linestyle='--', label=r'$V$')\n\nplt.xlim(-1.5, 1.5)\nplt.ylim(-1.5, 1.5)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n```\n\nThe vectors $\\mathbf{u}$ and $\\mathbf{v}$ are in blue and red, respectively, and the subspaces $U$ and $V$ as dashed lines with corresponding colors. Since $\\mathbf{u} \\cdot \\mathbf{v} = 0$, the subspaces are orthogonal.\n\n### Orthogonal Complements\n\nGiven a subspace $V$ of a vector space $W$, we can decompose any vector $\\mathbf{w} \\in W$ into two orthogonal components, one in $V$ and one in the orthogonal complement of $V$.\n\n:::{#def-orthogonalComplements}\nLet $V$ be a subspace of a vector space $W$. The orthogonal complement of $V$, denoted by $V^\\perp$, is the set of all vectors in $W$ that are orthogonal to every vector in $V$. That is, the orthogonal complement of $V$ is the set of vectors $\\mathbf w \\in W$ such that the inner product between $\\mathbf w$ and any vector $\\mathbf v \\in V$ is equal to zero:\n$$\nV^\\perp = \\{ \\mathbf w \\in W | \\langle \\mathbf w,\\mathbf v \\rangle = 0, \\forall \\mathbf v \\in V \\}.\n$$\n\nwhere  $V^\\perp$ represents the orthogonal complement of the subspace $V$, $w$ and $v$ are vectors in the subspaces $W$ and $V$ respectively, and $\\langle \\rangle$ denote the inner product between two vectors.\n\nIn other words, The orthogonal complement of a subspace $V$ contains every vector that is perpendicular to $V$. This orthogonal subspace is denoted by $V^\\perp$.\n:::\n\nWe can then decompose any vector $\\mathbf{w} \\in W$ into two orthogonal components as follows:\n$$\n\\mathbf w = \\mathbf w_{V} +\\mathbf w_{V^{\\perp}}  \n$$\n\nwhere $\\mathbf{w}_{V}$ is the orthogonal projection of $\\mathbf{w}$ onto $V$, and $\\mathbf{w}_{V^\\perp}$ is the orthogonal projection of $\\mathbf{w}$ onto $V^\\perp$.\n\nIf $\\mathbf v$ is orthogonal to the nullspace, it must be in the row space.\n\n:::{#thm-orthogonalComplement}\nLet $\\mathbf A$ be a matrix and let $\\mathbf W = \\operatorname{Col}(A)$. Then, $\\mathbf W^{\\perp} = \\operatorname{Null}(\\mathbf A)$.\n:::\nBy the proposition, computing the orthogonal complement of a span means solving a system of linear equations.\n\n#### Example\n\nLet $W = \\mathbb{R}^3$ and $V$ be the subspace spanned by the vectors $\\mathbf{v}_1 = (1,0,0)$ and $\\mathbf{v}_2 = (0,1,1)$. Then, we can find a basis for $V^\\perp$ by solving the system of equations\n\n$$\n\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 1  \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\n\nCompute $W^{\\perp}$, where $W=\\operatorname{Span}\\left\\{ \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\end{bmatrix}  \\right\\}$\n\nCompute $\\operatorname{Null}(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 1  \\end{bmatrix})$\n$$\n\\begin{align*}\nx &= 0 \\\\\ny + z &= 0\\\\\nW^{\\perp}&=\\operatorname{\\begin{bmatrix} 0 \\\\ -z \\\\ z\\end{bmatrix}}\n\\end{align*}\n$$,\nwhich has the unique solution $x = 0$, $y = -z$. Therefore, the subspace $V^\\perp$ is spanned by the vector $\\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}$.\n\nTo see this, note that any vector $\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$ in $V^\\perp$ must satisfy $\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = 0$ and $\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix} = 0$. These conditions can be rewritten as the equations $x = 0$ and $y = -z$, respectively. Therefore, any vector in $V^\\perp$ must have the form $\\begin{bmatrix} 0 \\\\ -z \\\\ z \\end{bmatrix}$, and it is easy to check that $\\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}$ satisfies this equation and is linearly independent from $\\mathbf{v}_1$ and $\\mathbf{v}_2$, so it is a basis for $V^\\perp$.\n\nTherefore, we have $V^\\perp = \\operatorname{span}\\left(\\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}\\right)$.\n\nGiven any vector $\\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{bmatrix} \\in \\mathbb{R}^3$, we can decompose it into two orthogonal components as\n\n$$\n\\mathbf w = \\mathbf w_{V} +\\mathbf w_{V^{\\perp}}  \n$$\n\nwhere $\\mathbf{w}_V$ is the projection of $\\mathbf{w}$ onto $V$, and $\\mathbf{w}_{V^\\perp}$ is the projection of $\\mathbf{w}$ onto $V^\\perp$.\n\nTo compute $\\mathbf{w}_V$, note that $\\mathbf{w}$ can be written as a linear combination of $\\mathbf{v}_1$ and $\\mathbf{v}_2$ as\n\n$$\n\\begin{align*}\n\\mathbf{w} &= \\frac{\\mathbf{w} \\cdot \\mathbf{v}_1}{||\\mathbf{v}_1||^2} \\mathbf{v}_1 + \\frac{\\mathbf{w} \\cdot \\mathbf{v}_2}{||\\mathbf{v}_2||^2} \\mathbf{v}_2 \\\\\n&= \\frac{1}{1^2}\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\end{bmatrix}+ \\frac{1}{2}\\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\end{bmatrix} \\\\\n&= \\begin{bmatrix}1 \\\\ \\frac{1}{2} \\\\ \\frac{1}{2}\\end{bmatrix}\n\\end{align*}\n$$\n\nTherefore, the subspace spanned by $\\mathbf{w}$ is the line passing through the point $\\begin{bmatrix}1 \\\\ \\frac{1}{2} \\\\ \\frac{1}{2}\\end{bmatrix}$ in the direction of $\\mathbf{w}$, which is $\\text{span}\\left \\{\\begin{bmatrix}1 \\\\ \\frac{1}{2} \\\\ \\frac{1}{2}\\end{bmatrix}\\right \\}$.\n\nThe projection of $\\mathbf{w}$ onto $V$ is given by\n\n$$\n\\mathbf{w}_V = \\operatorname{proj}_{V}(\\mathbf{w}) = \\frac{\\langle \\mathbf{w},\\mathbf{v}_1\\rangle}{\\|\\mathbf{v}_1\\|^2} \\mathbf{v}_1 + \\frac{\\langle \\mathbf{w},\\mathbf{v}_2\\rangle}{\\|\\mathbf{v}_2\\|^2} \\mathbf{v}_2 = \\frac{1}{1^2+0^2+0^2} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0\\end{bmatrix} =\\begin{bmatrix} 1 \\\\ 0 \\\\ 0\\end{bmatrix}\n$$\n\nThe projection of $\\mathbf{w}$ onto $V^\\perp$ is given by\n\n$$\n\\mathbf w = \\mathbf w_{V} -\\mathbf w_{V^{\\perp}}  = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\end{bmatrix}\n$$\n\nTherefore, we have decomposed $\\mathbf{w}$ into two orthogonal components as $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\end{bmatrix}$.\n\n:::{#thm-fundamentralThm}\nThe Fundamental Theorem.  \nLet $\\mathbf A$ be an $m\\times n$ matrix over $\\mathbb{R}$. Then,\n\n* The column space of $\\mathbf A$, denoted $\\operatorname{Col}(\\mathbf A)$, is a subspace of $\\mathbb{R}^m$.\n* The null space of $\\mathbf A$, denoted $\\operatorname{Null}(\\mathbf A)$, is a subspace of $\\mathbb{R}^n$.\n* The orthogonal complement of $\\operatorname{Col}(\\mathbf A)$, denoted $\\operatorname{Col}(\\mathbf A)^\\perp$, is equal to $\\operatorname{Null}(\\mathbf A)$.\n* The orthogonal complement of $\\operatorname{Null}(\\mathbf A)$, denoted $\\operatorname{Null}(\\mathbf A)^\\perp$, is equal to $\\operatorname{Col}(\\mathbf A)$.\n\nIn other words, we have the following orthogonal decomposition of $\\mathbb{R}^n$:\n$$\n\\mathbb R^n = N(\\mathbf A) \\oplus C(\\mathbf A)^\\perp\n$$\n\nand the following orthogonal decomposition of $\\mathbb{R}^m$:\n\n$$\n\\mathbb R^m = C(\\mathbf A) \\oplus N(\\mathbf A)^\\perp\n$$\n\nwhere $\\oplus$ denotes the direct sum of subspaces.\n\n:::\n\nThe Fundamental Theorem states that for a given matrix $\\mathbf A$, the column space of $\\mathbf A$ and the null space of $\\mathbf A$ are orthogonal complements of each other. In other words, every vector in the null space of $\\mathbf A$ is orthogonal to every vector in the column space of $\\mathbf A$, and vice versa. This means that any vector in the domain of $\\mathbf A$ can be uniquely decomposed as the sum of a vector in the column space and a vector in the null space.\n\nThe point of **complements** is that every $\\mathbf x$ can be split into a row space component $\\mathbf x^r$ and a nullspace component $\\mathbf x^n$. When $\\mathbf  A$ multiplies $\\mathbf x = \\mathbf x^r + \\mathbf x^n$, Figure 4.3 shows what happens:\n\n![Null Space Complement-Gilbert Strang: Introduction to Linear Algebra](../../../../../images/linear_algebra/nullspace_complement.PNG)\n\n* The nullspace component goes to zero: $\\mathbf{Ax}_n = \\mathbf{0}$.\n* The row space component goes to the column space: $\\mathbf{Ax}_r = \\mathbf{Ax}$ Ax r = Ax.\n* Every vector $\\mathbf b$ in the column space comes from one and only one vector in the row space.\n* pseudoinverse: there is an $r$ by $r$ invertible matrix hiding inside $\\mathbf A$, if we throwaway the two nullspaces. From the row space to the column space, $\\mathbf A$ is invertible.\n\n#### Exmaple\n\n**Example1**\nLet $\\mathbf A$ be an $m \\times n$ matrix with rank $r$. Then, $\\mathbb{R}^n$ can be decomposed as $\\mathbb{R}^n = N(\\mathbf A) \\oplus N(\\mathbf A)^{\\perp}$, where $N(\\mathbf A)$ is the null space of $\\mathbf A$, $N(\\mathbf A)^{\\perp}$ is its orthogonal complement, and $\\oplus$ denotes the direct sum. This means that any vector $\\mathbf{v} \\in \\mathbb{R}^n$ can be written uniquely as $\\mathbf{v} = \\mathbf{v}_1 + \\mathbf{v}_2$, where $\\mathbf{v}_1 \\in N(\\mathbf A)$ and $\\mathbf{v}_2 \\in N(\\mathbf A)^{\\perp}$.\n\n**Example2**\nLet $\\mathbf A$ be an $m \\times n$ matrix with rank $r$. Then, the column space of $\\mathbf A$, denoted $C(\\mathbf A)$, is equal to the orthogonal complement of the null space of $\\mathbf A^T$, i.e., $C(\\mathbf A) = N(\\mathbf A^T)^{\\perp}$.\n\n**Example3**\nLet $\\mathbf A$ be an $m \\times n$ matrix with rank $r$, and let $\\mathbf{b} \\in \\mathbb{R}^m$ be a vector. Then, the system of linear equations $A\\mathbf{x} = \\mathbf{b}$ has a solution if and only if $\\mathbf{b} \\in C(\\mathbf A)$. Moreover, if $\\mathbf{x}_0$ is a particular solution to $\\mathbf A\\mathbf{x} = \\mathbf{b}$, then the set of all solutions is given by ${\\mathbf{x}_0 + \\mathbf{v} : \\mathbf{v} \\in N(\\mathbf A)}$, i.e., it is the affine space consisting of $\\mathbf{x}_0$ plus the null space of $\\mathbf A$.\n\n**Example4**\nEvery diagonal matrix $\\mathbf D$ has a diagonal submatrix consisting of its first $r$ diagonal entries that is $r \\times r$ and invertible for any $r$ between $1$ and the size of $\\mathbf D$. For example, consider the diagonal matrix $\\mathbf D = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{bmatrix}$. The $2 \\times 2$ diagonal submatrix consisting of the first two diagonal entries, $\\mathbf D' = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}$, is invertible since its diagonal entries are nonzero. Similarly, the $3 \\times 3$ diagonal submatrix consisting of all the diagonal entries, $\\mathbf D'' = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{bmatrix}$, is also invertible since all of its diagonal entries are nonzero. This example illustrates the fact that every diagonal matrix has an invertible diagonal submatrix of any size between $1$ and the size of the matrix.\n\n**Example5**\nConsider the matrix $\\mathbf A=\\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$. We want to find the right bases for $\\mathbb{R}^2$ and $\\mathbb{R}^3$ such that $\\mathbf A$ becomes a diagonal matrix.\n\nWe begin by computing $\\mathbf A^T\\mathbf A$:\n\n$$\n\\mathbf A^T \\mathbf A = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}  = \\begin{bmatrix} 17 & 22 & 27 \\\\ 22 & 29 & 36 \\\\ 27 & 36 & 45\\end{bmatrix}\n$$\n\nThe eigenvalues of $\\mathbf A^T\\mathbf A$ are $\\lambda_1 = 0$, $\\lambda_2 = 1$, and $\\lambda_3 = 90$. We can find the corresponding eigenvectors as follows:\n\n- For $\\lambda_1 = 0$, we solve $(\\mathbf A^T\\mathbf A - \\lambda_1\\mathbf I)\\mathbf v = 0$, which gives us the equation $17x + 22y + 27z = 0$. One possible eigenvector is $\\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}$.\n- For $\\lambda_2 = 1$, we solve $(\\mathbf A^T\\mathbf A - \\lambda_2\\mathbf I)\\mathbf v = 0$, which gives us the equation $16x + 20y + 24z = 0$. One possible eigenvector is $\\begin{bmatrix} 3 \\\\ -2 \\\\ 0 \\end{bmatrix}$.\n- For $\\lambda_3 = 90$, we solve $(\\mathbf A^T\\mathbf A - \\lambda_3 \\mathbf I)\\mathbf v = 0$, which gives us the equation $-2x + y + z = 0$. One possible eigenvector is $\\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}$.\n\nWe normalize these eigenvectors to obtain an orthonormal basis for $\\mathbb{R}^3$:\n\n$$\n\\mathbf v_1 = \\frac{1}{\\sqrt{5}}\\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf v_2 = \\frac{1}{\\sqrt{13}}\\begin{bmatrix} 3 \\\\ -2 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf v_3 = \\frac{1}{\\sqrt{6}}\\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}\n$$\n\nNext, we compute $\\mathbf{Av}_i$ for each $i=1,2,3$:\n\n$$\n\\mathbf{Av}_1 = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\begin{bmatrix} \\frac{-2}{\\sqrt{5}} \\\\ \\frac{1}{\\sqrt{5}} \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} \\frac{-2}{\\sqrt{5}} \\\\ \\frac{8}{\\sqrt{5}} \\end{bmatrix} = \\frac{2}{\\sqrt{5}}\\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}\n$$\n\n$$\n\\mathbf{Av}_2 = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\begin{bmatrix} -0.6931 \\\\ -0.1184 \\\\ 0.7107 \\end{bmatrix} \\approx \\begin{bmatrix} -3.1623 \\\\ -7.4162 \\end{bmatrix} \\approx -3.1623v_1,\n$$\n\nand\n\n$$\n\\mathbf{Av}_3 = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\begin{bmatrix} -0.6931 \\\\ 0.6646 \\\\ -0.2774 \\end{bmatrix} \\approx \\begin{bmatrix} -4.7246 \\\\ 4.6707 \\end{bmatrix} \\approx 4.6707v_1,\n$$\n\nwhere $\\mathbf{v}_1=\\begin{bmatrix} 0.2673 \\\\ 0.5345 \\\\ 0.8018 \\end{bmatrix}$.\n\nTherefore, we can take $\\mathbf{v}_1$ as the first column of the matrix $\\mathbf{V}$, and the normalized eigenvectors $\\mathbf v_2$ and $\\mathbf v_3$ as the second and third columns of $\\mathbf V$, respectively. Then we can define $\\mathbf{U=AV\\Sigma}^{-1}$, where $\\mathbf \\Sigma$ is the diagonal matrix with the square roots of the nonzero eigenvalues of $\\mathbf A^T\\mathbf A$ as its entries.\n\n< 여기서 부터 다시 볼것>\nThus, we have\n\n$$\n\\mathbf{A} = \\mathbf{U\\Sigma V}^T = \\begin{bmatrix} -0.231 \\ \\ \\ 0.9730 \\\\ -0.5253 \\ \\ \\ 0.0806 \\\\ -0.8196 -0.9195 \\end{bmatrix} \\begin{bmatrix} 9.4868 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} -0.2673 \\ \\ \\ 0.5345 \\ \\ \\ 0.8018 \\\\ -0.6931 \\ \\ \\ -0.1184 \\ \\ \\ 0.7107 \\\\ -0.6646 \\ \\ \\ 0.7774 \\ \\ \\ -0.2774 \\end{bmatrix}.\n$$\nThis gives us the diagonalization $\\mathbf A=\\mathbf{QDQ}^{-1}$, where $\\mathbf{Q}=\\mathbf{U\\Sigma}$ and $\\mathbf{D}=\\mathbf{V}^T$. Therefore, by choosing the appropriate bases for $\\mathbb{R}^2$ and $\\mathbb{R}^3$ given by the columns of $\\mathbf Q$, we can make $\\mathbf A$ a diagonal matrix.\n\n< 여기까지>\n\n$$\n\\mathbf A = \\mathbf{U\\Sigma V}^T = \\begin{bmatrix} -0.231 & 0.973 & 0 \\\\ 0.732 & 0.182 & -0.655 \\\\ 0.641 & 0.136 & 0.755 \\end{bmatrix} \\begin{bmatrix} 3.89 & 0 & 0 \\\\ 0 & 1.27 & 0 \\\\ 0 & 0 & 0.43 \\end{bmatrix} \\begin{bmatrix} -0.227 & -0.592 & -0.773 \\\\ -0.904 & 0.275 & 0.329 \\\\ 0.361 & 0.758 & -0.541 \\end{bmatrix}\n$$\n\nThis is known as the Singular Value Decomposition (SVD) of $\\mathbf A$. The diagonal matrix $\\mathbf \\Sigma$ contains the singular values of $\\mathbf A$, which are the square roots of the eigenvalues of $\\mathbf A^T\\mathbf A$. These values represent the importance of the corresponding singular vectors in the matrix $\\mathbf A$.\n\nThe SVD can be used for a variety of applications, including data compression, dimensionality reduction, and image processing. It is also used in machine learning and data science for tasks such as collaborative filtering, recommender systems, and principal component analysis.\n\n### Combining bases from Subspaces\n\n**Any $n$ independent vectors in $\\mathbf R^n$ must span $\\mathbf R^n$. So, they are a basis.**\n\nIn $\\mathbf R^n$, a set of $n$ independent vectors is said to span $\\mathbf R^n$ if any vector in $\\mathbf R^n$ can be expressed as a linear combination of these $n$ vectors. This means that the $n$ vectors are sufficient to represent any vector in $\\mathbf R^n$.\n\nTo see why this is the case, consider that any vector in $\\mathbf R^n$ can be represented as a column vector with $n$ entries. By definition, each entry can be written as a linear combination of the entries of the $n$ independent vectors. Therefore, the entire column vector can be expressed as a linear combination of the $n$ independent vectors. Since this is true for any vector in $\\mathbf R^n$, the set of $n$ independent vectors must span $\\mathbf R^n$.\n\nMoreover, if a set of $n$ independent vectors spans $\\mathbf R^n$, then they are a basis for $\\mathbf R^n$. This means that the $n$ vectors are linearly independent and also span $\\mathbf R^n$. By definition, a basis is a set of vectors that can be used to represent any vector in a space and that is linearly independent. So, any set of $n$ independent vectors that spans $\\mathbf R^n$ is a basis for $\\mathbf R^n$.\n\n**Any $n$ vectors that span $\\mathbf R^n$ must be independent. So, they are a basis.**\n\nLet ${v_1,v_2,\\dots,v_n}$ be a set of $n$ vectors that span $\\mathbf R^n$. This means that any vector $\\mathbf x$ in $\\mathbf R^n$ can be expressed as a linear combination of the vectors in ${v_1,v_2,\\dots,v_n}$, i.e., there exist scalars $a_1,a_2,\\dots,a_n$ such that $\\mathbf x = a_1v_1+a_2v_2+\\cdots+a_nv_n$.\n\nNow suppose that the vectors in ${v_1,v_2,\\dots,v_n}$ are not independent. Then there exist scalars $b_1,b_2,\\dots,b_n$, not all zero, such that $b_1v_1+b_2v_2+\\cdots+b_nv_n=\\mathbf 0$, where $\\mathbf 0$ denotes the zero vector in $\\mathbf R^n$.\n\nWe can rewrite this equation as $a_1v_1+a_2v_2+\\cdots+a_nv_n=\\mathbf 0$, where $a_i=-b_i$ for $i=1,2,\\dots,n$. But this implies that the vector $\\mathbf x=\\mathbf 0$ can be expressed as a nontrivial linear combination of the vectors in ${v_1,v_2,\\dots,v_n}$, which contradicts the assumption that these vectors span $\\mathbf R^n$.\n\nTherefore, the vectors ${v_1,v_2,\\dots,v_n}$ must be independent. Since they span $\\mathbf R^n$, they form a basis for $\\mathbf R^n$.\n\n**If the $n$ columns of $\\mathbf A$ are independent, they span $\\mathbf R^n$. So Ax=b is solvable**\n\nIf the $n$ columns of a matrix $\\mathbf A$ are independent, then they span $\\mathbf R^n$, which means that any vector $\\mathbf b$ in $\\mathbf R^n$ can be expressed as a linear combination of the columns of $\\mathbf A$.\n\nSuppose we have a system of linear equations $\\mathbf{Ax} = \\mathbf{b}$. If the columns of $\\mathbf A$ are independent, then we can find a unique linear combination of the columns that equals $\\mathbf b$. In other words, we can solve the system of equations for $\\mathbf x$. This means that $\\mathbf{Ax} = \\mathbf{b}$ is solvable for any vector $\\mathbf b$ in $\\mathbf R^n$.\n\nTherefore, if the columns of $\\mathbf A$ are independent, the equation $\\mathbf{Ax} = \\mathbf{b}$ is solvable for any $\\mathbf b \\in \\mathbf R^n$, and the columns of $\\mathbf A$ form a basis for $\\mathbf R^n$.\n\n**If the $n$ columns span $\\mathbf R^n$, they are independent. So $\\mathbf{Ax=b}$ has only one solution.**\n\nIf the $n$ columns of $\\mathbf A$ span $\\mathbf R^n$, it means that any vector in $\\mathbf R^n$ can be expressed as a linear combination of those columns. Mathematically, if we denote the $n$ columns of $\\mathbf A$ as $\\mathbf a_1, \\mathbf a_2, \\dots, \\mathbf a_n$, then for any vector $\\mathbf b \\in \\mathbf R^n$, there exist scalars $x_1, x_2, \\dots, x_n$ such that:\n\n$$\n\\mathbf{b} = x_1 \\mathbf{a}_1 + x_2 \\mathbf{a}_2 + \\cdots + x_n \\mathbf{a}_n\n$$\n\n\nNow, let's assume that the columns of $\\mathbf A$ are not independent. This means that there exist scalars $x_1, x_2, \\dots, x_n$, not all zero, such that:\n\n$$\nx_1 \\mathbf{a}_1 + x_2 \\mathbf{a}_2 + \\cdots + x_n \\mathbf{a}_n = \\mathbf{0}\n$$\n\nThis implies that the homogeneous system $\\mathbf{Ax}=\\mathbf 0$ has a nontrivial solution, since we can choose $\\mathbf x = \\begin{bmatrix} x_1 \\ x_2 \\ \\vdots \\ x_n \\end{bmatrix} \\neq \\mathbf 0$ as a solution.\n\nHowever, this contradicts the assumption that the columns of $\\mathbf A$ span $\\mathbf R^n$. If there exists a nontrivial solution $\\mathbf x$ to $\\mathbf{Ax}=\\mathbf 0$, it means that the columns of $\\mathbf A$ do not span the entire $\\mathbf R^n$ space, because they are not able to generate the zero vector. Therefore, the assumption that the columns of $\\mathbf A$ are not independent leads to a contradiction.\n\nHence, we conclude that the columns of $\\mathbf A$ must be independent if they span $\\mathbf R^n$. This also implies that $\\mathbf A$ is invertible, since the equation $\\mathbf{Ax}=\\mathbf b$ has a unique solution for any $\\mathbf b \\in \\mathbf R^n$.\n\n### Example\n\n$$\n\\mathbf v_1=\\begin{bmatrix} 1 \\\\0\\\\1 \\end{bmatrix} \\quad \\mathbf v_2=\\begin{bmatrix} 0 \\\\1\\\\1 \\end{bmatrix} \\quad  \\mathbf v_3=\\begin{bmatrix} 1 \\\\1\\\\0 \\end{bmatrix}\n$$\n\n$$\n\\begin{align*}\nq_1 &= \\frac{v_1}{|v_1|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\\\\nu_2 &= v_2 - \\langle v_2, q_1 \\rangle q_1 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix} - \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -\\frac{1}{\\sqrt{2}} \\\\ 1 \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix}, \\\\\nq_2 &= \\frac{u_2}{|u_2|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} -1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\\\\nu_3 &= v_3 - \\langle v_3, q_1 \\rangle q_1 - \\langle v_3, q_2 \\rangle q_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} - \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} - \\frac{1}{\\sqrt{6}}\\begin{bmatrix} -1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\ -\\frac{1}{\\sqrt{3}} \\end{bmatrix}, \\\\\nq_3 &= \\frac{u_3}{|u_3|} = \\frac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\end{bmatrix}.\n\\end{align*}\n$$\n\nTherefore, the orthogonal matrix $\\mathbf{Q}$ is:\n\n$$\n\\mathbf{Q}=\n\\begin{bmatrix} \n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0\\\\ \n0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}}\\\\ \n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{3}}\n\\end{bmatrix}\n$$\n\n\n### Orthogonal Matrices\n\n\n:::{#def-orthogonalMatrix}\nAn $n\\times n$ matrix $\\mathbf{Q}$ is orthogonal if its columns $\\mathbf q$ form an orthonormal set. That is, the columns $\\mathbf q$ of $\\mathbf{Q}$ satisfy \n\n$$\n\\langle\\mathbf{q}_i,\\mathbf{q}_j\\rangle =\n\\begin{cases}\n0 \\text{ if } i \\ne j \\\\\n1 \\text{ if } i = j \n\\end{cases}\n$$\n\nWe can organize all of the dot products amongst all pairs of columns by premultiplying the matrix by its transpose. Since matrix multiplication is defined as dot products between all rows of the left matrix with all columns of the right matrix, \n\n$$\n\\mathbf{Q}\\mathbf{Q}^T=\\mathbf{Q}^T\\mathbf{Q}=\\mathbf{I}\n$$\n\nwhere $\\mathbf{I}$ is the $n\\times n$ identity matrix.\n:::\n\n### Properties\n\n* Orthogonal columns: all columns are pair-wise orthogonal\n* Unit-norm columns: the norm (geometric length) of each column is exactly 1.\n* $\\mathbf{Q}^T\\mathbf{Q}=\\mathbf{Q}\\mathbf{Q}^T=\\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix of appropriate size.\n* $\\mathbf{Q}^T=\\mathbf{Q}^{-1}$\n  * Great propoerty because the matrix inverse is tedious and prone to numerical inaccuracies, whereas the matrix transpose is fast and accurate.\n* The determinant of an orthogonal matrix is either $1$ or $-1$.\n* If $\\mathbf{Q}$ is orthogonal, then its columns form an orthonormal set, i.e., the columns are pairwise orthogonal and each column has unit length.\n* Orthogonal matrices preserve lengths and angles. If $\\mathbf{x}$ and $\\mathbf{y}$ are two vectors, then $||\\mathbf{Qx}||=||\\mathbf{x}||$ and $\\mathbf{x}^T\\mathbf{y}=(\\mathbf{Qx})^T(\\mathbf{Qy})$.\n\n### Example\n\n**Example1** \nOrthogonal matrices include rotation matrices and reflection matrices. \n\nthe $2\\times 2$ matrix and the $3\\times 3$ matrix:\n$$\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{bmatrix}\n\\quad\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta & 0 \\\\\n\\sin \\theta & \\cos \\theta & 0 \\\\\n0 & 0 & 1 \n\\end{bmatrix}\n$$\n\nis an orthogonal matrix that rotates a vector counterclockwise by an angle $\\theta$ regardless of the\nrotation angle (as long as the same rotation angle is used in all matrix elements). \n\n```{python}\n# Pure rotation matrix\n\n# angle to rotate by\nth = np.pi/5\n\n# transformation matrix\nT = np.array([ \n              [ np.cos(th),np.sin(th)],\n              [-np.sin(th),np.cos(th)]\n            ])\n\n\n# original dots are a vertical line\nx = np.linspace(-1,1,20)\norigPoints = np.vstack( (np.zeros(x.shape),x) )\n\n\n# apply the transformation\ntransformedPoints = T @ origPoints\n\n\n# plot the points\nplt.figure(figsize=(6,6))\nplt.plot(origPoints[0,:],origPoints[1,:],'ko',label='Original')\nplt.plot(transformedPoints[0,:],transformedPoints[1,:],'s',color=[.7,.7,.7],label='Transformed')\n\nplt.axis('square')\nplt.xlim([-1.2,1.2])\nplt.ylim([-1.2,1.2])\nplt.legend()\nplt.title(f'Rotation by {np.rad2deg(th):.0f} degrees.')\nplt.show()\n\n# Animating transformations\n# function to update the axis on each iteration\ndef aframe(ph):\n\n  # create the transformation matrix\n  T = np.array([\n                 [  1, 1-ph ],\n                 [  0, 1    ]\n                ])\n  \n  # apply the transformation to the points using matrix multiplication\n  P = T@points\n\n  # update the dots\n  plth.set_xdata(P[0,:])\n  plth.set_ydata(P[1,:])\n\n  # export the plot handles\n  return plth\n\n\n# define XY points\ntheta  = np.linspace(0,2*np.pi,100)\npoints = np.vstack((np.sin(theta),np.cos(theta)))\n\n\n# setup figure\nfig,ax = plt.subplots(1,figsize=(12,6))\nplth,  = ax.plot(np.cos(x),np.sin(x),'ko')\nax.set_aspect('equal')\nax.set_xlim([-2,2])\nax.set_ylim([-2,2])\n\n# define values for transformation (note: clip off the final point for a smooth animation loop)\nphi = np.linspace(-1,1-1/40,40)**2\n\n# run animation!\nanimation.FuncAnimation(fig, aframe, phi, interval=100, repeat=True)\n\n```\n\n$$\n\\begin{align*}\n\\mathbf{Q}&=\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\\\\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\end{align*}\n$$\n\n$\\mathbf{Q}$ is orthogonal by computing $\\mathbf{Q}^T\\mathbf{Q}$:\n$$\n\\begin{align*}\n\\mathbf{Q}^T\\mathbf{Q}&=\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\\\\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\n\\frac{1}{2}+\\frac{1}{2} & -\\frac{1}{2}+\\frac{1}{2}\\\\\n-\\frac{1}{2}+\\frac{1}{2} & \\frac{1}{2}+\\frac{1}{2}\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\n\\end{bmatrix}\\\\\n&=\\mathbf{I}\n\\end{align*}\n$$\n\nTherefore, $\\mathbf{Q}$ is an orthogonal matrix.\n\n**Example 2**\n$$\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & -1 \n\\end{bmatrix}\n$$,\n\nwhich is an orthogonal matrix that reflects a vector across the $x$-axis.\n\n**Example 3**\nthe identity matrix is an example of an orthogonal matrix\n\n**Exmaple 4**\nPermutation matrices are also orthogonal. Permutation matrices are used to exchange rows of a matrix.\n\n\n## Orthogonal Bases\n\nAn orthogonal basis is a set of vectors that are pairwise orthogonal (perpendicular) and each vector is non-zero.\n\n:::{#def-orthogonalBases}\nA set of vectors ${\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n}$ is said to be an orthogonal basis of a vector space $V$ if:\n1. Each vector $\\mathbf{v}_i$ is non-zero.\n2. Each vector $\\mathbf{v}_i$ is orthogonal (perpendicular) to every other vector $\\mathbf{v}_j$, $i \\neq j$. In other words, $\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0$ for $i \\neq j$.\n:::\n\n### Example\n\n**Example1**\nLet $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$ and $\\mathbf{u} = \\begin{bmatrix} -1 \\ 0 \\ 1 \\end{bmatrix}$. We can check if these vectors form an orthogonal basis by computing their dot product:\n\n$$\n\\mathbf{v} \\cdot \\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\cdot \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix} = (-1) \\cdot 1 + (0) \\cdot 2 + (1) \\cdot 3 = 2\n$$\n\nSince the dot product is not zero, we can conclude that $\\mathbf{v}$ and $\\mathbf{u}$ do not form an orthogonal basis.\n\n**Example2**\n\nLet $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}$ and $\\mathbf{u} = \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\end{bmatrix}$. To check if they form an orthogonal basis, we need to compute their dot product:\n\n$$\n\\mathbf{v} \\cdot \\mathbf{u} = (1)(-2) + (2)(1) + (0)(1) = -2 + 2 + 0 = 0\n$$\n\nSince the dot product is zero, we know that $\\mathbf{v}$ and $\\mathbf{u}$ are orthogonal. We can also check that they are both nonzero and linearly independent by computing their norms:\n\n$$\n||\\mathbf{v}|| = \\sqrt{1^2 + 2^2 + 0^2} = \\sqrt{5} \\ne 0\n$$\n\n$$\n||\\mathbf{w}|| = \\sqrt{(-2)^2 + 1^2 + 1^2} = \\sqrt{6} \\ne 0\n$$\n\nTherefore, $\\mathbf{v}$ and $\\mathbf{w}$ form an orthogonal basis for $\\mathbb{R}^3$.\n\n## Gram-Schmidt (GS or G-S)\n\nThe Gram-Schmidt procedure is a method of transforming a nonorthogonal matrix into an orthogonal matrix by orthonormalizing a set of linearly independent vectors in an inner product space, usually the Euclidean space $\\mathbb{R}^n$. The process takes a sequence of vectors $\\mathbf v_1, \\mathbf v_2, \\dots, \\mathbf v_n$ and constructs an orthonormal sequence $\\mathbf q_1, \\mathbf q_2, \\dots, \\mathbf q_n$ that spans the same subspace as the original sequence.\n\nThe Gram-Schmidt procedure is useful for understanding orthogonal vector decomposition when programming and implementing the QR decomposition algorithm, and GS is the right way to conceptualize how and why QR decomposition works even if the low-level implementation is slightly different.\n\n:::{#def-gramSchmidt}\nLet $\\mathbf v_1, \\mathbf v_2, \\dots, \\mathbf v_n$ be a sequence of linearly independent vectors in $\\mathbb{R}^n$. Define $\\mathbf q_1$ to be the unit vector in the direction of $\\mathbf v_1$, i.e., $\\mathbf q_1 = \\frac{\\mathbf v_1}{|\\mathbf v_1|}$. For $k = 2, 3, \\dots, n$, define $\\mathbf q_k$ as follows:\n\n$$\n\\mathbf q_k = \\frac{\\mathbf u_k}{|\\mathbf u_k|}\n$$\n\nwhere $\\mathbf u_k=\\mathbf v_k-\\sum_{j=1}^{k-1}\\langle \\mathbf v_k,\\mathbf q_j \\rangle\\mathbf q_j$ \n:::\n\nThe vector $\\mathbf u_k$ is the projection of $\\mathbf v_k$ onto the subspace orthogonal to $\\text{span}{\\mathbf q_1, \\mathbf q_2, \\dots, \\mathbf q_{k-1}}$.\n\nThe Gram-Schmidt process produces an orthonormal basis $\\mathbf q_1, \\mathbf q_2, \\dots, \\mathbf q_n$ for $\\text{span}{\\mathbf v_1, \\mathbf v_2, \\dots, \\mathbf v_n}$. The matrix whose columns are $\\mathbf q_1, \\mathbf q_2, \\dots, \\mathbf q_n$ is an orthogonal matrix $\\mathbf{Q}$.\n\n$\\mathbf{V}$ is transformed into $\\mathbf{Q}$ according to the following algorithm:\n\nFor all column vectors $\\mathbf{v} \\in V$ starting from the first (leftmost) and moving systematically to the last (rightmost):\n\n1. Orthogonalize $\\mathbf v_k$ to all previous columns in matrix $\\mathbf Q$ using orthogonal vector decomposition. That is, compute the component of $\\mathbf v_k$ that is perpendicular to $\\mathbf q_{k-1}, \\mathbf q_{k-2}$, and so on down to $\\mathbf q_{1}$. The orthogonalized vector is called $\\mathbf v^{*}_k$.\n:::{.callout-note}\nThe first column vector is not orthogonalized because there are no preceeding vectors; therefore, you begin\nwith the following normalization step.\n:::\n2. Normalize $\\mathbf v^{*}_k$ to unit length. This is now $\\mathbf q_{k}$, the $k$ th column in matrix $\\mathbf Q$.\n\n```{python}\n\n# # Define a 4x4 random matrix\n# A = np.random.rand(4,4)\n# \n# # Gram-Schmidt procedure\n# Q = np.zeros_like(A)\n# for j in range(A.shape[1]):\n#     v = A[:,j]\n#     for i in range(j):\n#         q = Q[:,i]\n#         R[i,j] = np.dot(q,v)\n#         v -= R[i,j]*q\n#     R[j,j] = np.linalg.norm(v)\n#     Q[:,j] = v/R[j,j]\n# \n# # Check answer against Q from np.linalg.qr\n# Q_np, R_np = np.linalg.qr(A)\n# diff = Q - Q_np\n# sum_Q = Q + Q_np\n# \n# print(\"Q matrix (Gram-Schmidt):\\n\", Q)\n# print(\"Q matrix (numpy):\\n\", Q_np)\n# print(\"Difference between Q and Q_np:\\n\", diff)\n# print(\"Sum of Q and Q_np:\\n\", sum_Q)\n\n\n# create the matrix \nm = 4\nn = 4\nA = np.random.randn(m,n)\n\n# initialize\nQ = np.zeros((m,n))\n\n\n# the GS algo\nfor i in range(n):\n    \n    # initialize\n    Q[:,i] = A[:,i]\n    \n    # orthogonalize\n    a = A[:,i] # convenience\n    for j in range(i): # only to earlier cols\n        q = Q[:,j] # convenience\n        Q[:,i]=Q[:,i]-np.dot(a,q)/np.dot(q,q)*q\n    \n    # normalize\n    Q[:,i] = Q[:,i] / np.linalg.norm(Q[:,i])\n\n    \n# \"real\" QR decomposition for comparison\nQ_np,R = np.linalg.qr(A)\n\n# note the possible sign differences.\n# seemingly non-zero columns will be 0 when adding\nprint(\"Q matrix (Gram-Schmidt):\\n\", Q)\nprint(\"Q matrix (numpy):\\n\", Q_np)\nprint(\"Difference between Q and Q_np:\\n\", np.round( Q-Q_np ,10) ), print(' ')\nprint(\"Sum of Q and Q_np:\\n\", np.round( Q+Q_np ,10) )\n\n```\n\n## QR Decomposition\n\nQR decomposition is a factorization of a matrix $\\mathbf{A}$ into the product of an orthogonal matrix $\\mathbf{Q}$ and an upper triangular matrix $\\mathbf{R}$:\n\n$$\n\\mathbf{A}=\\mathbf{QR}\n$$ \n\nwhere $\\mathbf{Q}$ has orthonormal columns and $\\mathbf{R}$ is an upper triangular matrix.\n\nThe process of finding the QR decomposition of $\\mathbf{A}$ involves the Gram-Schmidt orthogonalization process, which produces an orthonormal basis for the columns of $\\mathbf{A}$ as mentioned earlier. The columns of $\\mathbf{Q}$ are the orthonormal basis vectors, and $\\mathbf{R}$ is the matrix that expresses the columns of $\\mathbf{A}$ in terms of the orthonormal basis vectors.\n\n$$\n\\begin{align*}\n\\mathbf{A}&=\\mathbf{QR}\\\\\n\\mathbf{Q}^T \\mathbf{A}&=\\mathbf{Q}^T\\mathbf{QR}\\\\\n\\mathbf{Q}^T\\mathbf{A}&=\\mathbf{R}\n\\end{align*}\n$$\n\nThe algorithm for computing the QR decomposition of a matrix $\\mathbf{A}$ is as follows:\n\n1. Apply the Gram-Schmidt orthogonalization process to the columns of $\\mathbf{A}$ to obtain an orthonormal basis for the column space of $\\mathbf{A}$. Let the resulting matrix be denoted by $\\mathbf{Q}$.\n2. Compute the matrix $\\mathbf{R}$ such that $\\mathbf{A} = \\mathbf{Q}\\mathbf{R}$. This can be done by solving the linear system $\\mathbf{R} = \\mathbf{Q}^T\\mathbf{A}$, which expresses the columns of $\\mathbf{A}$ in terms of the orthonormal basis vectors.\n```{python}\n\n\n# create a random matrix\nA = np.random.randn(6,6)\n\n# QR decomposition\nQ,R = np.linalg.qr(A)\n\n\n\n# show the matrices\nfig = plt.figure(figsize=(10,6))\naxs = [0]*5\nc = 1.5 # color limits\n\ngs1 = gridspec.GridSpec(2,6)\naxs[0] = plt.subplot(gs1[0,:2])\naxs[0].imshow(A,vmin=-c,vmax=c,cmap='gray')\naxs[0].set_title('A',fontweight='bold')\n\naxs[1] = plt.subplot(gs1[0,2:4])\naxs[1].imshow(Q,vmin=-c,vmax=c,cmap='gray')\naxs[1].set_title('Q',fontweight='bold')\n\naxs[2] = plt.subplot(gs1[0,4:6])\naxs[2].imshow(R,vmin=-c,vmax=c,cmap='gray')\naxs[2].set_title('R',fontweight='bold')\n\naxs[3] = plt.subplot(gs1[1,1:3])\naxs[3].imshow(A - Q@R,vmin=-c,vmax=c,cmap='gray')\naxs[3].set_title('A - QR',fontweight='bold')\n\naxs[4] = plt.subplot(gs1[1,3:5])\naxs[4].imshow(Q.T@Q,cmap='gray')\naxs[4].set_title(r'$\\mathbf{Q}^{T}\\mathbf{Q}$',fontweight='bold')\n\n# remove ticks from all axes\nfor a in axs:\n  a.set_xticks([])\n  a.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n```\n\n\n\n### Examples\n\n1. Consider the matrix $\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 2 & 3 \\\\ 3 & 4 \\end{bmatrix}$. To find its QR decomposition, we first apply the Gram-Schmidt process to obtain an orthonormal basis for its column space:\n\n$$\n\\mathbf q_1 = \\frac{1}{\\sqrt{14}}\\begin{bmatrix} 1 \\\\ 2 \\\\ 3\\end{bmatrix} \\quad \\mathbf q_2 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} -2 \\\\ 1 \\\\ 0\\end{bmatrix}\n$$\n\nThe resulting orthogonal matrix is \n$$\\begin{align*}\n\\mathbf{Q} = \\begin{bmatrix} \n\\frac{1}{\\sqrt{14}} & -\\frac{2}{\\sqrt{28}} \\\\\n\\frac{2}{\\sqrt{14}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{3}{\\sqrt{14}} & 0 \n\\end{bmatrix}\n\\end{align*}\n$$.\n\nNext, we compute the upper triangular matrix $\\mathbf{R}$ by solving $\\mathbf{R} = \\mathbf{Q}^T\\mathbf{A}$. This gives \n$$\n\\mathbf{R} = \\begin{bmatrix}\n\\sqrt{14} & \\frac{11}{\\sqrt{14}} \\\\\n0 & \\frac{\\sqrt{2}}{\\sqrt{7}}\n\\end{bmatrix}\n$$\n\n\nTherefore, the QR decomposition of $\\mathbf{A}$ is given by $\\mathbf{A} = \\mathbf{Q}\\mathbf{R}$.\n\n2. Consider the matrix $\\mathbf{A} = \\begin{bmatrix} 1 & -1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}$. Applying the Gram-Schmidt process to its columns yields the orthonormal basis vectors:\n\n$$\n\\begin{align*}\n\\mathbf q_1 &= \\frac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\\\\n\\mathbf u_2 &= \\mathbf v_2 - \\langle \\mathbf v_2, \\mathbf q_1 \\rangle \\mathbf q_1 = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix} - \\frac{1}{3}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -\\frac{4}{3} \\\\ -\\frac{1}{3} \\\\ \\frac{2}{3} \\end{bmatrix}, \\\\\n\\mathbf q_2 &= \\frac{u_2}{|u_2|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} -2 \\\\ -1 \\\\ 1 \\end{bmatrix}, \\\\\n\\mathbf u_3 &= \\mathbf v_3 - \\langle \\mathbf v_3, \\mathbf q_1 \\rangle \\mathbf q_1 - \\langle \\mathbf v_3, \\mathbf q_2 \\rangle \\mathbf q_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} - \\frac{1}{3}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} - \\frac{1}{2}\\begin{bmatrix} -2 \\\\ -1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{3} \\\\ -\\frac{1}{3} \\\\ -\\frac{1}{3} \\end{bmatrix}, \\\\\n\\mathbf q_3 &= \\frac{\\mathbf u_3}{|\\mathbf u_3|} = \\frac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\end{bmatrix}.\n\\end{align*}\n$$\n\nSo the QR decomposition of $\\mathbf{A}$ is $\\mathbf{A} = \\mathbf{QR}$ where\n\n$$\n\\mathbf{A=QR} = \\begin{bmatrix}\n\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & \\frac{2}{\\sqrt{6}} & 0 \\\\\n\\end{bmatrix}\n$$\n\nand\n\n$$\n\\mathbf{R} = \\begin{bmatrix}\n\\sqrt{3} & \\frac{1}{\\sqrt{3}} & \\sqrt{3} \\\\\n0 & \\frac{2}{\\sqrt{6}} & -\\frac{1}{\\sqrt{2}} \\\\\n0 & 0 & \\frac{1}{\\sqrt{2}} \\\\\n\\end{bmatrix}\n$$.\n\nSo, \n$$\n\\mathbf{A} = \n\\begin{bmatrix}\n 1 & -1 & 0 \\\\\n 1 & 0  & 1 \\\\\n 1 & 1  & 0 \n\\end{bmatrix} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & \\frac{2}{\\sqrt{6}} & 0 \\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sqrt{3} & \\frac{1}{\\sqrt{3}} & \\sqrt{3} \\\\\n0 & \\frac{2}{\\sqrt{6}} & -\\frac{1}{\\sqrt{2}} \\\\\n0 & 0 & \\frac{1}{\\sqrt{2}} \n\\end{bmatrix}\n=\n\\mathbf{QR}\n$$\n\n\nIn Python,\n\n```{python}\n\nA = np.array([[1, -1, 0], [1, 0, 1], [1, 1, 0]])\nQ, R = np.linalg.qr(A)\n\nprint(\"Q =\\n\", Q)\nprint(\"R =\\n\", R)\nprint(\"QR =\\n\", Q @ R)\n\n```\n\nThe QR decomposition of a matrix is not unique, So the $\\mathbf Q$ and $\\mathbf R$ could be different from the latex ones.\n\n### Sizes of $\\mathbf Q$ and $\\mathbf R$\n\nThe sizes of $\\mathbf Q$ and $\\mathbf R$ depend on the size of the matrix $\\mathbf A$ and on whether the QR decomposition is *reduced* or *full*.\n\nFor a tall matrix $(m > n)$, do we create a $\\mathbf Q$ matrix with n columns or m columns? \n\n* Economy or Reduced: $\\mathbf Q_{m\\times n}$ (tall $\\mathbf Q$)\n* Full or Complete: $\\mathbf Q_{m\\times m}$ (square $\\mathbf Q$) \n  * $\\mathbf Q$ can be square when $\\mathbf A$ is tall ($\\mathbf Q$ can have more columns than $\\mathbf A$)\n  * In python, the option of `np.linalg.qr(A,'complete')` is 'complete', which produces a full QR decomposition.\n  * The option of `np.linalg.qr(A,'reduced')` is 'reduced', which is the default, gives the economy-mode QR decomposition, in which $\\mathbf Q$ is the same size as $\\mathbf A$.\n* Likewise, the rank of $\\mathbf Q$ is always the maximum possible rank, which is $m$ for all square $\\mathbf Q$ matrices and $n$ for the economy $\\mathbf Q$. The rank of $\\mathbf R$ is the same as the rank of $\\mathbf A$.\n  * the difference of $\\operatorname{rank}(\\mathbf A) and \\operatorname{rank}(\\mathbf Q)$ means $\\mathbf Q$ spans all of $\\mathbb R^m$ even if the $\\operatorname{col}(\\mathbf A)$ is only a lower-dimensional subspace of $\\mathbb R^m$.\n* **non-uniqueness**: QR decomposition is not unique for all matrix sizes and ranks. ($\\mathbf A = \\mathbf Q_1 \\mathbf R_1$ and $\\mathbf A = \\mathbf Q_2\\mathbf R_2$ where $\\mathbf Q_1 \\ne \\mathbf Q_2$).\n* **uniqueness with constraints**: QR decomposition can be made unique given additional constraints (e.g., positive values on the diagonals of $\\mathbf R$)\n\n```{python}\nA = np.array([ [1,-1] ]).T\nQ,R = np.linalg.qr(A,'complete')\nQ*np.sqrt(2) # scaled by sqrt(2) to get integers\nprint(\"A =\\n\", A)\nprint(\"Q =\\n\", Q)\nprint(\"R =\\n\", R)\nprint(\"Q*np.sqrt(2)=\\n\", Q*np.sqrt(2))\n```\n\n### Upper-triangle of $\\mathbf R$\n\n* $\\mathbf R$ comes from the formula $\\mathbf{Q}^T\\mathbf{A = R}$.\n* The lower triangle of a product matrix comprises dot products between later rows of the left matrix and earlier columns of the right matrix.\n* The rows of $\\mathbf{Q}^T$ are the columns of $\\mathbf{Q}$.\n\n```{python}\n# Define matrix A\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Compute QR decomposition of A\nQ, R = np.linalg.qr(A)\n\n# Compute Q^T * A\nQtA = np.matmul(Q.T, A)\n\n# Extract upper triangle of R\nupper_R = np.triu(R)\n\n# Compute dot products between later rows of Q^T and earlier columns of A\ndot_products = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(i):\n        dot_products[i, j] = np.dot(Q.T[i], A[:, j])\n\n# Print results\nprint(\"A = \")\nprint(A)\nprint(\"Q = \")\nprint(Q)\nprint(\"R = \")\nprint(R)\nprint(r\"$Q^T * A = $\")\nprint(QtA)\nprint(\"Upper-triangle of R = \")\nprint(upper_R)\nprint(\"Dot products between later rows of Q^T and earlier columns of A = \")\nprint(dot_products)\n```\n\nThe lower triangle of $\\mathbf{R}$ comprises dot products of $\\mathbf Q^T\\mathbf A$ (between later rows of $\\mathbf{Q}^T$ and earlier columns of $\\mathbf{A}$)\n:::{.callout-note}\nNote that the lower triangle of $\\mathbf{R}$ is zero, because the first column of $\\mathbf{A}$ is orthogonal to the remaining columns of $\\mathbf{A}$. Thus, the pairs of vectors used to form the lower triangle of $\\mathbf{R}$ are orthogonal. On the other hand, the upper triangle of $\\mathbf{R}$ comes from the dot product of later rows of $\\mathbf{Q}$ and earlier columns of $\\mathbf{A}$. Specifically, the $(2,1)$ entry of $\\mathbf{R}$ is the dot product of the second row of $\\mathbf{Q}$ with the first column of $\\mathbf{A}$\n\nIf columns $i$ and $j$ of $\\mathbf A$ were already orthogonal, then the corresponding $(i,j)$ th element in $\\mathbf R$ would be zero. In fact, if you compute the QR decomposition of an orthogonal matrix, then $\\mathbf R$ will be a diagonal matrix in which the diagonal elements are the norms of each column in $\\mathbf A$. That means that if $\\mathbf{A = Q}$, then $\\mathbf{R = I}$, which is obvious from the equation solved for $\\mathbf{R}$ \n:::\n\n### QR and Inverses\n\nQR decomposition provides a more numerically stable way to compute the matrix inverse, $\\mathbf A^{-1}$ of $\\mathbf A$ because $\\mathbf Q$ is numerically stable due to the Householder reflection algorithm, and $\\mathbf R$ is numerically stable because it simply results from matrix multiplication.\n\n$$\n\\begin{align*}\n  \\mathbf{A}&=\\mathbf{QR}\\\\\n  \\mathbf{A}^{-1}&=(\\mathbf{QR})^{-1}\\\\\n  \\mathbf{A}^{-1}&=\\mathbf{R}^{-1}\\mathbf{Q}^{-1}\\\\\n  \\mathbf{A}^{-1}&=\\mathbf{R}^{-1}\\mathbf{Q}^{T}\n\\end{align*}\n$$\n\n\n## Projections\n\n## Least Squares Approximations\n\n## Orthogonal Bases and Gram-Schmidt\n","srcMarkdownNoYaml":"\n\n```{python}\nimport numpy as np\nimport matplotlib.animation as animation\nimport matplotlib_inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport sympy as sym # for RREF\nimport scipy.linalg # for LU\nimport matplotlib.gridspec as gridspec # used to create non-regular subplots\nfrom scipy.linalg import lstsq # for least square example\n```\n\n# Orthogonality\n\n* Orthogonality of the Four Subspaces\n  * Orthogonal Vectors\n  * Orthogonal Subspaces\n    * Orthogonal Components\n  * Orthogonal Bases\n  * Orthogonal Matrices\n* Orthogonal Vector Decomposition,\n* QR decomposition\n  * 'Q' stands for an orthogonal matrix, and \n  * 'R' stands for an upper triangular matrix. \n* Gram-Schmidt Decomposition, \n* Eigen Decomposition, and \n* Singular Value Decomposition\n\n## Orthogonality of the Four Subspaces\n\nThe four subspaces: vectors, subspaces, orthogonal bases, and orthogonal matrices\n\n### Orthogonal Vectors\n\n:::{#def-orthogonalVec}\nTwo vectors $\\mathbf{v}$ and $\\mathbf{w}$ in $\\mathbb{R}^n$ are said to be orthogonal if their dot product is zero:\n\n$$\n\\mathbf{v} \\cdot \\mathbf{w} = \\sum_{i=1}^n v_i w_i = 0 \\text{ and }||\\mathbf{v}||^2+||\\mathbf{w}||^2=||\\mathbf{v}+\\mathbf{w}||^2\n$$\n:::\n\nGeometrically, two vectors are orthogonal if they are perpendicular to each other.\n\nOrthogonality is an important concept in linear algebra and has many applications, including in the construction of orthonormal bases and in least-squares regression.\n\n#### Examples\n\n**Example1**\n$\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$ and $\\mathbf{w} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$ are orthogonal because $\\mathbf{v} \\cdot \\mathbf{w} = 1 \\cdot 0 + 0 \\cdot 1 + 0 \\cdot 0 = 0$. These vectors are also perpendicular to each other in 3D space.\n\n**Example2**\n$\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$ and $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ -2 \\\\ 1 \\end{bmatrix}$ are orthogonal because $\\mathbf{v} \\cdot \\mathbf{w} = 1 \\cdot 1 + 1 \\cdot (-2) + 1 \\cdot 1 = 0$. These vectors are also perpendicular to each other in 3D space.\n\n**Example3**\n$\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$ and $\\mathbf{w} = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}$ are orthogonal because $\\mathbf{v} \\cdot \\mathbf{w} = 1 \\cdot (-2) + 2 \\cdot 1 = 0$. These vectors are also perpendicular to each other in 2D space.\n\n```{python}\n# Example 1\nv1 = np.array([1, 0, 0])\nw1 = np.array([0, 1, 0])\nprint(np.dot(v1, w1))  # Output: 0\n\n# Example 2\nv2 = np.array([1, 1, 1])\nw2 = np.array([1, -2, 1])\nprint(np.dot(v2, w2))  # Output: 0\n\n# Example 3\nv3 = np.array([1, 2])\nw3 = np.array([-2, 1])\nprint(np.dot(v3, w3))  # Output: 0\n\n# Example 1\nv2 = np.array([1, 1, 1])\nw2 = np.array([1, -2, 1])\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.quiver(0, 0, 0, v2[0], v2[1], v2[2], colors='b', arrow_length_ratio=0.1)\nax.quiver(0, 0, 0, w2[0], w2[1], w2[2], colors='r', arrow_length_ratio=0.1)\nax.set_xlim([-1, 2])\nax.set_ylim([-1, 2])\nax.set_zlim([-1, 2])\nax.set_title(\"Example 1: Orthogonal Vectors\")\nax.legend([\"v\", \"w\"])\nplt.show()\n\n# Example 2\nv1 = np.array([1, 0])\nw1 = np.array([0, 1])\nplt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='b')\nplt.quiver(0, 0, w1[0], w1[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.xlim(-1, 2)\nplt.ylim(-1, 2)\nplt.title(\"Example 2: Orthogonal Vectors\")\nplt.legend([\"v\", \"w\"])\nplt.grid(True)\nplt.show()\n\n\n# Example 3\nv3 = np.array([1, 2])\nw3 = np.array([-2, 1])\nplt.quiver(0, 0, v3[0], v3[1], angles='xy', scale_units='xy', scale=1, color='b')\nplt.quiver(0, 0, w3[0], w3[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nplt.title(\"Example 3: Orthogonal Vectors\")\nplt.legend([\"v\", \"w\"])\nplt.grid(True)\nplt.show()\n```\n\n#### Properties\n\n* Orthogonal vectors have a dot product of zero:\n$$\n\\mathbf{v} \\cdot \\mathbf{w} = 0\n$$\n\n* The magnitude (length) of the projection of a vector onto an orthogonal vector is given by:\n$$\n\\text{proj}_{\\mathbf{w}}(\\mathbf{v}) = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{\\|\\mathbf{w}\\|^2} \\mathbf{w} = 0\n$$\n\n* The Pythagorean theorem holds for orthogonal vectors:\n$$\n\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\n$$\n\n* The angle between two orthogonal vectors is $\\frac{\\pi}{2}$ radians or $90$ degrees:\n$$\n\\theta = \\frac{\\pi}{2}\n$$\n\n* Orthogonal vectors are linearly independent, which means that no vector in the span of one vector can be expressed as a linear combination of the other vector:\n$$\n\\text{span}\\{\\mathbf{v}\\} \\cap \\text{span}\\{\\mathbf{w}\\} = \\{\\mathbf{0}\\}\n$$\n\n![Orthogonal Space-Gilbert Strang: Introduction to Linear Algebra](../../../../../images/linear_algebra/orthogonal_space.PNG)\n\n* The row space is perpendicular to the nullspace\n* The column space is perpendicular to the nullspace of $\\mathbf{A}^T$.\n  * This peroperty $\\mathbf{A}$ plays a key role in solving the equation $\\mathbf{Ax=b}$ but $\\mathbf{b}$ is outside the column space (meaning we can't solve the equation directly). In this case, we use the nullspace of $\\mathbf{A}^T$ to find the \"least-squares\" solution, which gives us the smallest possible error $\\mathbf{e = b - Ax}$ in the solution.\n\n:::{.callout-note}\nWhen $\\mathbf{b}$ is outside the column space of $\\mathbf{A}$, there is no exact solution to the equation $\\mathbf{Ax = b}$. Instead, we seek a solution that minimizes the error $\\mathbf{e = b - Ax}$. The least-squares solution achieves this by finding the projection of $\\mathbf{b}$ onto the column space of $\\mathbf{A}$. It turns out that the projection of $\\mathbf{b}$ onto the column space of $\\mathbf{A}$ is exactly equal to the solution of the equation $\\mathbf{A}^T\\mathbf{Ax} = \\mathbf{A}^T\\mathbf{b}$, which can be solved using the nullspace of $\\mathbf{A}^T$.\n\nIn summary, the statement \"the column space is perpendicular to the nullspace of $\\mathbf{A}^T$\" tells us that the column space and nullspace are orthogonal (i.e., perpendicular) subspaces, and this fact allows us to use the nullspace of $\\mathbf{A}^T$ to find the least-squares solution to $\\mathbf{Ax = b}$.\n:::\n\n##### Least Square Example\n\nSuppose we have a system of equations $\\mathbf{Ax} = \\mathbf{b}$ where $\\mathbf{A}$ is an $m \\times n$ matrix and $\\mathbf{b}$ is an $m \\times 1$ vector, and we want to find the least squares solution to this system (i.e., the solution that minimizes the residual $|\\mathbf{Ax} - \\mathbf{b}|$). If $\\mathbf{A}$ has linearly independent columns, then we can solve for $\\mathbf{x}$ using the formula $\\mathbf{x} = (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{b}$. \n\nHowever, if $\\mathbf{A}$ does not have linearly independent columns, then we can use the fact that the column space of $\\mathbf{A}$ is perpendicular to the nullspace of $\\mathbf{A}^T$ to find the least squares solution.\n\nTo do this, we first find a basis for the column space of $\\mathbf{A}$ and a basis for the nullspace of $\\mathbf{A}^T$. Let $\\mathbf{P}$ be the projection matrix onto the column space of $\\mathbf{A}$, given by $\\mathbf{P} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T$. Then the least squares solution to $\\mathbf{Ax} = \\mathbf{b}$ is given by $\\mathbf{x} = \\mathbf{P}\\mathbf{b}$, and the residual $\\mathbf{e} = \\mathbf{b} - \\mathbf{Ax}$ is in the nullspace of $\\mathbf{A}^T$.\n\n\n```{python}\n#| echo: false\n#| eval: false\n\n# Define the matrix A and the vector b\nA = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9],\n              [10, 11, 12]])\nb = np.array([1, 2, 3, 4])\n\n# Compute the least-squares solution using lstsq from SciPy\nx, res, rank, s = lstsq(A, b)\n\n# Compute the error e = b - Ax\ne = b - A @ x # matrix multiplication\n\n# Compute the projection of b onto the column space of A\nProjected_b = A @ x\n\n# Compute the projection of b onto the orthogonal complement of the column space of A\nProjected_b_perp = b - Projected_b\n\n# Compute the projection of e onto the nullspace of A^T\nProjected_e = np.linalg.pinv(A.T).T @ e\n\n'''\nThe expected value of Projected_e = np.linalg.pinv(A.T) @ e is the projection of the true value x \nonto the column space of A. \nThis is because the least squares solution x_hat is the orthogonal projection of the vector b onto the column space of A, \nwhich is given by x_hat = A @ np.linalg.lstsq(A, b)[0].\n\nSince e = b - Ax_hat is the error in the least squares solution, \nnp.linalg.pinv(A.T) @ e computes the projection of this error vector onto the nullspace of A^T. \nTherefore, the expected value of Projected_e is zero, since the error e is orthogonal to the column space of A and \nits projection onto the nullspace of A^T is also orthogonal to the column space of A.\n\nIn other words, Projected_e is the component of the error e that lies in the nullspace of A^T, and \nsince the nullspace of A^T is orthogonal to the column space of A, the expected value of Projected_e is zero.\n'''\n\n# Compute the projection of e onto the orthogonal complement of the nullspace of A^T\nProjected_e_perp = e - A @ Projected_e\n\n# Verify that the column space is perpendicular to the nullspace of A^T\nassert np.allclose(A @ Projected_e, np.zeros((A.shape[0],)))\n#assert np.allclose(Projected_e_perp @ x, np.zeros((x.shape[0],)))\n\n'''\nIn this example, we define the matrix `A` and the vector `b`, and use the lstsq function from SciPy to compute the least-squares solution `x`. We then compute the error `e = b - Ax`, and project `b` onto the column space of `A` to obtain `Projected_b`, and onto the orthogonal complement of the column space of `A` to obtain `Projected_b_perp`. We also project `e` onto the nullspace of `A^T` to obtain `Projected_e`, and onto the orthogonal complement of the nullspace of `A^T` to obtain `Projected_e_perp`. Finally, we verify that the column space of `A` is perpendicular to the nullspace of `A^T` by checking that `A^T` `Projected_e = 0` and `Projected_e_perp @ x = 0`.\n'''\n```\n\n\n\n```{python}\n#| eval: false\n#| echo: false\n\n# define the matrix A and vector b\nA = np.array([[1, 1], [1, -1], [2, 1], [2, -1]])\nb = np.array([3, 1, 5, 3])\n\n# calculate the least-squares solution\nx = np.linalg.lstsq(A, b, rcond=None)[0]\n\n# create a figure with a 3D axes object\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# plot the points in the column space of A\nx1 = np.linspace(-1, 3, 10)\nx2 = np.linspace(-1, 3, 10)\nX1, X2 = np.meshgrid(x1, x2)\nY = x[0]*X1 + x[1]*X2\nax.plot_surface(X1, X2, Y, alpha=0.2)\n\n# plot the points in the nullspace of A^T\nx1 = np.linspace(-1, 3, 10)\nx3 = np.linspace(-1, 3, 10)\nX1, X3 = np.meshgrid(x1, x3)\nY = np.zeros_like(X1)\nZ = x[0]*X1 + x[1]*Y + x[2]*X3\nax.plot_surface(X1, Y, X3, color='red', alpha=0.2)\n\n# plot the data points and the least-squares solution\nax.scatter(A[:,0], A[:,1], b, color='blue', marker='o', s=50)\nax.scatter(x[0], x[1], x[2], color='green', marker='*', s=100)\n\n# set the axis labels and limits\nax.set_xlabel('x1')\nax.set_ylabel('x2')\nax.set_zlabel('b')\nax.set_xlim(-1, 3)\nax.set_ylim(-1, 3)\nax.set_zlim(0, 6)\n\n# show the plot\nplt.show()\n```\n\n### Orthogonal Subspaces\n\n:::{#def-orthogonalSubspaces}\nTwo subspaces $U$ and $V$ of a vector space $W$ are said to be orthogonal subspaces if every vector in $U$ is orthogonal to every vector in $V$. Symbolically, we write $U \\perp V$ if and only if $\\mathbf{u} \\cdot \\mathbf{v} = 0$ for all $\\mathbf{u} \\in U$ and $\\mathbf{v} \\in V$.\n\n$$\n\\mathbf u^T \\mathbf v = 0\n$$\n:::\n\nEvery vector $\\mathbf{x}$ in the nullspace is perpendicular to every row of $\\mathbf{A}$, because $\\mathbf{Ax=0}$. \nThe $\\operatorname{null}(\\mathbf{A})$ and the row space $\\operatorname{Col}(\\mathbf{A}^T)$ are orthogonal subspaces of $\\mathbb{R}^n$\n\nEvery vector $\\mathbf{y}$ in the nullspace of $\\mathbf{A}^T$ is perpendicular to every column of $\\mathbf{A}$. The left $\\operatorname{null}(\\mathbf{A}^T)$ and the column space $\\operatorname{Col}(\\mathbf{A})$ are orthogonal subspaces in $\\mathbb{R}^n$\n\n![Null(A) $\\perp$ Col(A^T)-Gilbert Strang: Introduction to Linear Algebra](../../../../../images/linear_algebra/Null(A)_Col(A%5ET)_orthogonal.PNG)\n\n\n#### Examples\n\n**Example1**\nLet $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$ and $\\mathbf{u} = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}$ be two vectors in $\\mathbb{R}^3$. Then the subspaces $U = \\text{span}\\{\\mathbf{u}\\}$ and $V = \\text{span}\\{\\mathbf{v}\\}$ are orthogonal subspaces, since $\\mathbf{u} \\cdot \\mathbf{v} = 0$.\n\n$$\nU = \\text{span}\\{\\mathbf{u}\\} = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\right\\}, \\quad\nV = \\text{span}\\{\\mathbf{v}\\} = \\text{span}\\left\\{\\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}\\right\\}\n$$\n\n**Example2**\nLet $U$ be the subspace of $\\mathbb{R}^3$ spanned by the vectors $\\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}$ and $\\begin{bmatrix} 1 \\\\ 0 \\\\ -2 \\end{bmatrix}$, and let $V$ be the subspace spanned by the vector $\\begin{bmatrix} 2 \\\\ -1 \\\\ 1 \\end{bmatrix}$. Then $U$ and $V$ are orthogonal subspaces, since every vector in $U$ is orthogonal to every vector in $V$.\n\n$$\nU = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\\\ -2 \\end{bmatrix}\\right\\}, \\quad\nV = \\text{span}\\left\\{\\begin{bmatrix} 2 \\\\ -1 \\\\ 1 \\end{bmatrix}\\right\\}\n$$\n\n**Example3**\nLet $U$ and $V$ be the subspaces of $\\mathbb{R}^2$ spanned by the vectors $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ and $\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$, respectively. Then $U$ and $V$ are orthogonal subspaces, since $\\begin{bmatrix} 1 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = 0$.\n\n$$\nU = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\right\\}, \\quad\nV = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\right\\}\n$$\n\n```{python}\nu = np.array([1, 1])\nv = np.array([1, -1])\n\n# plot vectors\nplt.figure()\nplt.plot([0, u[0]], [0, u[1]], 'b', label=r'$\\mathbf{u}$')\nplt.plot([0, v[0]], [0, v[1]], 'r', label=r'$\\mathbf{v}$')\nplt.legend()\n\n# plot subspaces\nplt.axline((0, 0), slope=u[1]/u[0], color='b', linestyle='--', label=r'$U$')\nplt.axline((0, 0), slope=v[1]/v[0], color='r', linestyle='--', label=r'$V$')\n\nplt.xlim(-1.5, 1.5)\nplt.ylim(-1.5, 1.5)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n```\n\nThe vectors $\\mathbf{u}$ and $\\mathbf{v}$ are in blue and red, respectively, and the subspaces $U$ and $V$ as dashed lines with corresponding colors. Since $\\mathbf{u} \\cdot \\mathbf{v} = 0$, the subspaces are orthogonal.\n\n### Orthogonal Complements\n\nGiven a subspace $V$ of a vector space $W$, we can decompose any vector $\\mathbf{w} \\in W$ into two orthogonal components, one in $V$ and one in the orthogonal complement of $V$.\n\n:::{#def-orthogonalComplements}\nLet $V$ be a subspace of a vector space $W$. The orthogonal complement of $V$, denoted by $V^\\perp$, is the set of all vectors in $W$ that are orthogonal to every vector in $V$. That is, the orthogonal complement of $V$ is the set of vectors $\\mathbf w \\in W$ such that the inner product between $\\mathbf w$ and any vector $\\mathbf v \\in V$ is equal to zero:\n$$\nV^\\perp = \\{ \\mathbf w \\in W | \\langle \\mathbf w,\\mathbf v \\rangle = 0, \\forall \\mathbf v \\in V \\}.\n$$\n\nwhere  $V^\\perp$ represents the orthogonal complement of the subspace $V$, $w$ and $v$ are vectors in the subspaces $W$ and $V$ respectively, and $\\langle \\rangle$ denote the inner product between two vectors.\n\nIn other words, The orthogonal complement of a subspace $V$ contains every vector that is perpendicular to $V$. This orthogonal subspace is denoted by $V^\\perp$.\n:::\n\nWe can then decompose any vector $\\mathbf{w} \\in W$ into two orthogonal components as follows:\n$$\n\\mathbf w = \\mathbf w_{V} +\\mathbf w_{V^{\\perp}}  \n$$\n\nwhere $\\mathbf{w}_{V}$ is the orthogonal projection of $\\mathbf{w}$ onto $V$, and $\\mathbf{w}_{V^\\perp}$ is the orthogonal projection of $\\mathbf{w}$ onto $V^\\perp$.\n\nIf $\\mathbf v$ is orthogonal to the nullspace, it must be in the row space.\n\n:::{#thm-orthogonalComplement}\nLet $\\mathbf A$ be a matrix and let $\\mathbf W = \\operatorname{Col}(A)$. Then, $\\mathbf W^{\\perp} = \\operatorname{Null}(\\mathbf A)$.\n:::\nBy the proposition, computing the orthogonal complement of a span means solving a system of linear equations.\n\n#### Example\n\nLet $W = \\mathbb{R}^3$ and $V$ be the subspace spanned by the vectors $\\mathbf{v}_1 = (1,0,0)$ and $\\mathbf{v}_2 = (0,1,1)$. Then, we can find a basis for $V^\\perp$ by solving the system of equations\n\n$$\n\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 1  \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\n\nCompute $W^{\\perp}$, where $W=\\operatorname{Span}\\left\\{ \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\end{bmatrix}  \\right\\}$\n\nCompute $\\operatorname{Null}(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 1  \\end{bmatrix})$\n$$\n\\begin{align*}\nx &= 0 \\\\\ny + z &= 0\\\\\nW^{\\perp}&=\\operatorname{\\begin{bmatrix} 0 \\\\ -z \\\\ z\\end{bmatrix}}\n\\end{align*}\n$$,\nwhich has the unique solution $x = 0$, $y = -z$. Therefore, the subspace $V^\\perp$ is spanned by the vector $\\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}$.\n\nTo see this, note that any vector $\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$ in $V^\\perp$ must satisfy $\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = 0$ and $\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix} = 0$. These conditions can be rewritten as the equations $x = 0$ and $y = -z$, respectively. Therefore, any vector in $V^\\perp$ must have the form $\\begin{bmatrix} 0 \\\\ -z \\\\ z \\end{bmatrix}$, and it is easy to check that $\\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}$ satisfies this equation and is linearly independent from $\\mathbf{v}_1$ and $\\mathbf{v}_2$, so it is a basis for $V^\\perp$.\n\nTherefore, we have $V^\\perp = \\operatorname{span}\\left(\\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}\\right)$.\n\nGiven any vector $\\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{bmatrix} \\in \\mathbb{R}^3$, we can decompose it into two orthogonal components as\n\n$$\n\\mathbf w = \\mathbf w_{V} +\\mathbf w_{V^{\\perp}}  \n$$\n\nwhere $\\mathbf{w}_V$ is the projection of $\\mathbf{w}$ onto $V$, and $\\mathbf{w}_{V^\\perp}$ is the projection of $\\mathbf{w}$ onto $V^\\perp$.\n\nTo compute $\\mathbf{w}_V$, note that $\\mathbf{w}$ can be written as a linear combination of $\\mathbf{v}_1$ and $\\mathbf{v}_2$ as\n\n$$\n\\begin{align*}\n\\mathbf{w} &= \\frac{\\mathbf{w} \\cdot \\mathbf{v}_1}{||\\mathbf{v}_1||^2} \\mathbf{v}_1 + \\frac{\\mathbf{w} \\cdot \\mathbf{v}_2}{||\\mathbf{v}_2||^2} \\mathbf{v}_2 \\\\\n&= \\frac{1}{1^2}\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\end{bmatrix}+ \\frac{1}{2}\\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\end{bmatrix} \\\\\n&= \\begin{bmatrix}1 \\\\ \\frac{1}{2} \\\\ \\frac{1}{2}\\end{bmatrix}\n\\end{align*}\n$$\n\nTherefore, the subspace spanned by $\\mathbf{w}$ is the line passing through the point $\\begin{bmatrix}1 \\\\ \\frac{1}{2} \\\\ \\frac{1}{2}\\end{bmatrix}$ in the direction of $\\mathbf{w}$, which is $\\text{span}\\left \\{\\begin{bmatrix}1 \\\\ \\frac{1}{2} \\\\ \\frac{1}{2}\\end{bmatrix}\\right \\}$.\n\nThe projection of $\\mathbf{w}$ onto $V$ is given by\n\n$$\n\\mathbf{w}_V = \\operatorname{proj}_{V}(\\mathbf{w}) = \\frac{\\langle \\mathbf{w},\\mathbf{v}_1\\rangle}{\\|\\mathbf{v}_1\\|^2} \\mathbf{v}_1 + \\frac{\\langle \\mathbf{w},\\mathbf{v}_2\\rangle}{\\|\\mathbf{v}_2\\|^2} \\mathbf{v}_2 = \\frac{1}{1^2+0^2+0^2} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0\\end{bmatrix} =\\begin{bmatrix} 1 \\\\ 0 \\\\ 0\\end{bmatrix}\n$$\n\nThe projection of $\\mathbf{w}$ onto $V^\\perp$ is given by\n\n$$\n\\mathbf w = \\mathbf w_{V} -\\mathbf w_{V^{\\perp}}  = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\end{bmatrix}\n$$\n\nTherefore, we have decomposed $\\mathbf{w}$ into two orthogonal components as $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\end{bmatrix}$.\n\n:::{#thm-fundamentralThm}\nThe Fundamental Theorem.  \nLet $\\mathbf A$ be an $m\\times n$ matrix over $\\mathbb{R}$. Then,\n\n* The column space of $\\mathbf A$, denoted $\\operatorname{Col}(\\mathbf A)$, is a subspace of $\\mathbb{R}^m$.\n* The null space of $\\mathbf A$, denoted $\\operatorname{Null}(\\mathbf A)$, is a subspace of $\\mathbb{R}^n$.\n* The orthogonal complement of $\\operatorname{Col}(\\mathbf A)$, denoted $\\operatorname{Col}(\\mathbf A)^\\perp$, is equal to $\\operatorname{Null}(\\mathbf A)$.\n* The orthogonal complement of $\\operatorname{Null}(\\mathbf A)$, denoted $\\operatorname{Null}(\\mathbf A)^\\perp$, is equal to $\\operatorname{Col}(\\mathbf A)$.\n\nIn other words, we have the following orthogonal decomposition of $\\mathbb{R}^n$:\n$$\n\\mathbb R^n = N(\\mathbf A) \\oplus C(\\mathbf A)^\\perp\n$$\n\nand the following orthogonal decomposition of $\\mathbb{R}^m$:\n\n$$\n\\mathbb R^m = C(\\mathbf A) \\oplus N(\\mathbf A)^\\perp\n$$\n\nwhere $\\oplus$ denotes the direct sum of subspaces.\n\n:::\n\nThe Fundamental Theorem states that for a given matrix $\\mathbf A$, the column space of $\\mathbf A$ and the null space of $\\mathbf A$ are orthogonal complements of each other. In other words, every vector in the null space of $\\mathbf A$ is orthogonal to every vector in the column space of $\\mathbf A$, and vice versa. This means that any vector in the domain of $\\mathbf A$ can be uniquely decomposed as the sum of a vector in the column space and a vector in the null space.\n\nThe point of **complements** is that every $\\mathbf x$ can be split into a row space component $\\mathbf x^r$ and a nullspace component $\\mathbf x^n$. When $\\mathbf  A$ multiplies $\\mathbf x = \\mathbf x^r + \\mathbf x^n$, Figure 4.3 shows what happens:\n\n![Null Space Complement-Gilbert Strang: Introduction to Linear Algebra](../../../../../images/linear_algebra/nullspace_complement.PNG)\n\n* The nullspace component goes to zero: $\\mathbf{Ax}_n = \\mathbf{0}$.\n* The row space component goes to the column space: $\\mathbf{Ax}_r = \\mathbf{Ax}$ Ax r = Ax.\n* Every vector $\\mathbf b$ in the column space comes from one and only one vector in the row space.\n* pseudoinverse: there is an $r$ by $r$ invertible matrix hiding inside $\\mathbf A$, if we throwaway the two nullspaces. From the row space to the column space, $\\mathbf A$ is invertible.\n\n#### Exmaple\n\n**Example1**\nLet $\\mathbf A$ be an $m \\times n$ matrix with rank $r$. Then, $\\mathbb{R}^n$ can be decomposed as $\\mathbb{R}^n = N(\\mathbf A) \\oplus N(\\mathbf A)^{\\perp}$, where $N(\\mathbf A)$ is the null space of $\\mathbf A$, $N(\\mathbf A)^{\\perp}$ is its orthogonal complement, and $\\oplus$ denotes the direct sum. This means that any vector $\\mathbf{v} \\in \\mathbb{R}^n$ can be written uniquely as $\\mathbf{v} = \\mathbf{v}_1 + \\mathbf{v}_2$, where $\\mathbf{v}_1 \\in N(\\mathbf A)$ and $\\mathbf{v}_2 \\in N(\\mathbf A)^{\\perp}$.\n\n**Example2**\nLet $\\mathbf A$ be an $m \\times n$ matrix with rank $r$. Then, the column space of $\\mathbf A$, denoted $C(\\mathbf A)$, is equal to the orthogonal complement of the null space of $\\mathbf A^T$, i.e., $C(\\mathbf A) = N(\\mathbf A^T)^{\\perp}$.\n\n**Example3**\nLet $\\mathbf A$ be an $m \\times n$ matrix with rank $r$, and let $\\mathbf{b} \\in \\mathbb{R}^m$ be a vector. Then, the system of linear equations $A\\mathbf{x} = \\mathbf{b}$ has a solution if and only if $\\mathbf{b} \\in C(\\mathbf A)$. Moreover, if $\\mathbf{x}_0$ is a particular solution to $\\mathbf A\\mathbf{x} = \\mathbf{b}$, then the set of all solutions is given by ${\\mathbf{x}_0 + \\mathbf{v} : \\mathbf{v} \\in N(\\mathbf A)}$, i.e., it is the affine space consisting of $\\mathbf{x}_0$ plus the null space of $\\mathbf A$.\n\n**Example4**\nEvery diagonal matrix $\\mathbf D$ has a diagonal submatrix consisting of its first $r$ diagonal entries that is $r \\times r$ and invertible for any $r$ between $1$ and the size of $\\mathbf D$. For example, consider the diagonal matrix $\\mathbf D = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{bmatrix}$. The $2 \\times 2$ diagonal submatrix consisting of the first two diagonal entries, $\\mathbf D' = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}$, is invertible since its diagonal entries are nonzero. Similarly, the $3 \\times 3$ diagonal submatrix consisting of all the diagonal entries, $\\mathbf D'' = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{bmatrix}$, is also invertible since all of its diagonal entries are nonzero. This example illustrates the fact that every diagonal matrix has an invertible diagonal submatrix of any size between $1$ and the size of the matrix.\n\n**Example5**\nConsider the matrix $\\mathbf A=\\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$. We want to find the right bases for $\\mathbb{R}^2$ and $\\mathbb{R}^3$ such that $\\mathbf A$ becomes a diagonal matrix.\n\nWe begin by computing $\\mathbf A^T\\mathbf A$:\n\n$$\n\\mathbf A^T \\mathbf A = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}  = \\begin{bmatrix} 17 & 22 & 27 \\\\ 22 & 29 & 36 \\\\ 27 & 36 & 45\\end{bmatrix}\n$$\n\nThe eigenvalues of $\\mathbf A^T\\mathbf A$ are $\\lambda_1 = 0$, $\\lambda_2 = 1$, and $\\lambda_3 = 90$. We can find the corresponding eigenvectors as follows:\n\n- For $\\lambda_1 = 0$, we solve $(\\mathbf A^T\\mathbf A - \\lambda_1\\mathbf I)\\mathbf v = 0$, which gives us the equation $17x + 22y + 27z = 0$. One possible eigenvector is $\\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}$.\n- For $\\lambda_2 = 1$, we solve $(\\mathbf A^T\\mathbf A - \\lambda_2\\mathbf I)\\mathbf v = 0$, which gives us the equation $16x + 20y + 24z = 0$. One possible eigenvector is $\\begin{bmatrix} 3 \\\\ -2 \\\\ 0 \\end{bmatrix}$.\n- For $\\lambda_3 = 90$, we solve $(\\mathbf A^T\\mathbf A - \\lambda_3 \\mathbf I)\\mathbf v = 0$, which gives us the equation $-2x + y + z = 0$. One possible eigenvector is $\\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}$.\n\nWe normalize these eigenvectors to obtain an orthonormal basis for $\\mathbb{R}^3$:\n\n$$\n\\mathbf v_1 = \\frac{1}{\\sqrt{5}}\\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf v_2 = \\frac{1}{\\sqrt{13}}\\begin{bmatrix} 3 \\\\ -2 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf v_3 = \\frac{1}{\\sqrt{6}}\\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}\n$$\n\nNext, we compute $\\mathbf{Av}_i$ for each $i=1,2,3$:\n\n$$\n\\mathbf{Av}_1 = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\begin{bmatrix} \\frac{-2}{\\sqrt{5}} \\\\ \\frac{1}{\\sqrt{5}} \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} \\frac{-2}{\\sqrt{5}} \\\\ \\frac{8}{\\sqrt{5}} \\end{bmatrix} = \\frac{2}{\\sqrt{5}}\\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}\n$$\n\n$$\n\\mathbf{Av}_2 = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\begin{bmatrix} -0.6931 \\\\ -0.1184 \\\\ 0.7107 \\end{bmatrix} \\approx \\begin{bmatrix} -3.1623 \\\\ -7.4162 \\end{bmatrix} \\approx -3.1623v_1,\n$$\n\nand\n\n$$\n\\mathbf{Av}_3 = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\begin{bmatrix} -0.6931 \\\\ 0.6646 \\\\ -0.2774 \\end{bmatrix} \\approx \\begin{bmatrix} -4.7246 \\\\ 4.6707 \\end{bmatrix} \\approx 4.6707v_1,\n$$\n\nwhere $\\mathbf{v}_1=\\begin{bmatrix} 0.2673 \\\\ 0.5345 \\\\ 0.8018 \\end{bmatrix}$.\n\nTherefore, we can take $\\mathbf{v}_1$ as the first column of the matrix $\\mathbf{V}$, and the normalized eigenvectors $\\mathbf v_2$ and $\\mathbf v_3$ as the second and third columns of $\\mathbf V$, respectively. Then we can define $\\mathbf{U=AV\\Sigma}^{-1}$, where $\\mathbf \\Sigma$ is the diagonal matrix with the square roots of the nonzero eigenvalues of $\\mathbf A^T\\mathbf A$ as its entries.\n\n< 여기서 부터 다시 볼것>\nThus, we have\n\n$$\n\\mathbf{A} = \\mathbf{U\\Sigma V}^T = \\begin{bmatrix} -0.231 \\ \\ \\ 0.9730 \\\\ -0.5253 \\ \\ \\ 0.0806 \\\\ -0.8196 -0.9195 \\end{bmatrix} \\begin{bmatrix} 9.4868 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} -0.2673 \\ \\ \\ 0.5345 \\ \\ \\ 0.8018 \\\\ -0.6931 \\ \\ \\ -0.1184 \\ \\ \\ 0.7107 \\\\ -0.6646 \\ \\ \\ 0.7774 \\ \\ \\ -0.2774 \\end{bmatrix}.\n$$\nThis gives us the diagonalization $\\mathbf A=\\mathbf{QDQ}^{-1}$, where $\\mathbf{Q}=\\mathbf{U\\Sigma}$ and $\\mathbf{D}=\\mathbf{V}^T$. Therefore, by choosing the appropriate bases for $\\mathbb{R}^2$ and $\\mathbb{R}^3$ given by the columns of $\\mathbf Q$, we can make $\\mathbf A$ a diagonal matrix.\n\n< 여기까지>\n\n$$\n\\mathbf A = \\mathbf{U\\Sigma V}^T = \\begin{bmatrix} -0.231 & 0.973 & 0 \\\\ 0.732 & 0.182 & -0.655 \\\\ 0.641 & 0.136 & 0.755 \\end{bmatrix} \\begin{bmatrix} 3.89 & 0 & 0 \\\\ 0 & 1.27 & 0 \\\\ 0 & 0 & 0.43 \\end{bmatrix} \\begin{bmatrix} -0.227 & -0.592 & -0.773 \\\\ -0.904 & 0.275 & 0.329 \\\\ 0.361 & 0.758 & -0.541 \\end{bmatrix}\n$$\n\nThis is known as the Singular Value Decomposition (SVD) of $\\mathbf A$. The diagonal matrix $\\mathbf \\Sigma$ contains the singular values of $\\mathbf A$, which are the square roots of the eigenvalues of $\\mathbf A^T\\mathbf A$. These values represent the importance of the corresponding singular vectors in the matrix $\\mathbf A$.\n\nThe SVD can be used for a variety of applications, including data compression, dimensionality reduction, and image processing. It is also used in machine learning and data science for tasks such as collaborative filtering, recommender systems, and principal component analysis.\n\n### Combining bases from Subspaces\n\n**Any $n$ independent vectors in $\\mathbf R^n$ must span $\\mathbf R^n$. So, they are a basis.**\n\nIn $\\mathbf R^n$, a set of $n$ independent vectors is said to span $\\mathbf R^n$ if any vector in $\\mathbf R^n$ can be expressed as a linear combination of these $n$ vectors. This means that the $n$ vectors are sufficient to represent any vector in $\\mathbf R^n$.\n\nTo see why this is the case, consider that any vector in $\\mathbf R^n$ can be represented as a column vector with $n$ entries. By definition, each entry can be written as a linear combination of the entries of the $n$ independent vectors. Therefore, the entire column vector can be expressed as a linear combination of the $n$ independent vectors. Since this is true for any vector in $\\mathbf R^n$, the set of $n$ independent vectors must span $\\mathbf R^n$.\n\nMoreover, if a set of $n$ independent vectors spans $\\mathbf R^n$, then they are a basis for $\\mathbf R^n$. This means that the $n$ vectors are linearly independent and also span $\\mathbf R^n$. By definition, a basis is a set of vectors that can be used to represent any vector in a space and that is linearly independent. So, any set of $n$ independent vectors that spans $\\mathbf R^n$ is a basis for $\\mathbf R^n$.\n\n**Any $n$ vectors that span $\\mathbf R^n$ must be independent. So, they are a basis.**\n\nLet ${v_1,v_2,\\dots,v_n}$ be a set of $n$ vectors that span $\\mathbf R^n$. This means that any vector $\\mathbf x$ in $\\mathbf R^n$ can be expressed as a linear combination of the vectors in ${v_1,v_2,\\dots,v_n}$, i.e., there exist scalars $a_1,a_2,\\dots,a_n$ such that $\\mathbf x = a_1v_1+a_2v_2+\\cdots+a_nv_n$.\n\nNow suppose that the vectors in ${v_1,v_2,\\dots,v_n}$ are not independent. Then there exist scalars $b_1,b_2,\\dots,b_n$, not all zero, such that $b_1v_1+b_2v_2+\\cdots+b_nv_n=\\mathbf 0$, where $\\mathbf 0$ denotes the zero vector in $\\mathbf R^n$.\n\nWe can rewrite this equation as $a_1v_1+a_2v_2+\\cdots+a_nv_n=\\mathbf 0$, where $a_i=-b_i$ for $i=1,2,\\dots,n$. But this implies that the vector $\\mathbf x=\\mathbf 0$ can be expressed as a nontrivial linear combination of the vectors in ${v_1,v_2,\\dots,v_n}$, which contradicts the assumption that these vectors span $\\mathbf R^n$.\n\nTherefore, the vectors ${v_1,v_2,\\dots,v_n}$ must be independent. Since they span $\\mathbf R^n$, they form a basis for $\\mathbf R^n$.\n\n**If the $n$ columns of $\\mathbf A$ are independent, they span $\\mathbf R^n$. So Ax=b is solvable**\n\nIf the $n$ columns of a matrix $\\mathbf A$ are independent, then they span $\\mathbf R^n$, which means that any vector $\\mathbf b$ in $\\mathbf R^n$ can be expressed as a linear combination of the columns of $\\mathbf A$.\n\nSuppose we have a system of linear equations $\\mathbf{Ax} = \\mathbf{b}$. If the columns of $\\mathbf A$ are independent, then we can find a unique linear combination of the columns that equals $\\mathbf b$. In other words, we can solve the system of equations for $\\mathbf x$. This means that $\\mathbf{Ax} = \\mathbf{b}$ is solvable for any vector $\\mathbf b$ in $\\mathbf R^n$.\n\nTherefore, if the columns of $\\mathbf A$ are independent, the equation $\\mathbf{Ax} = \\mathbf{b}$ is solvable for any $\\mathbf b \\in \\mathbf R^n$, and the columns of $\\mathbf A$ form a basis for $\\mathbf R^n$.\n\n**If the $n$ columns span $\\mathbf R^n$, they are independent. So $\\mathbf{Ax=b}$ has only one solution.**\n\nIf the $n$ columns of $\\mathbf A$ span $\\mathbf R^n$, it means that any vector in $\\mathbf R^n$ can be expressed as a linear combination of those columns. Mathematically, if we denote the $n$ columns of $\\mathbf A$ as $\\mathbf a_1, \\mathbf a_2, \\dots, \\mathbf a_n$, then for any vector $\\mathbf b \\in \\mathbf R^n$, there exist scalars $x_1, x_2, \\dots, x_n$ such that:\n\n$$\n\\mathbf{b} = x_1 \\mathbf{a}_1 + x_2 \\mathbf{a}_2 + \\cdots + x_n \\mathbf{a}_n\n$$\n\n\nNow, let's assume that the columns of $\\mathbf A$ are not independent. This means that there exist scalars $x_1, x_2, \\dots, x_n$, not all zero, such that:\n\n$$\nx_1 \\mathbf{a}_1 + x_2 \\mathbf{a}_2 + \\cdots + x_n \\mathbf{a}_n = \\mathbf{0}\n$$\n\nThis implies that the homogeneous system $\\mathbf{Ax}=\\mathbf 0$ has a nontrivial solution, since we can choose $\\mathbf x = \\begin{bmatrix} x_1 \\ x_2 \\ \\vdots \\ x_n \\end{bmatrix} \\neq \\mathbf 0$ as a solution.\n\nHowever, this contradicts the assumption that the columns of $\\mathbf A$ span $\\mathbf R^n$. If there exists a nontrivial solution $\\mathbf x$ to $\\mathbf{Ax}=\\mathbf 0$, it means that the columns of $\\mathbf A$ do not span the entire $\\mathbf R^n$ space, because they are not able to generate the zero vector. Therefore, the assumption that the columns of $\\mathbf A$ are not independent leads to a contradiction.\n\nHence, we conclude that the columns of $\\mathbf A$ must be independent if they span $\\mathbf R^n$. This also implies that $\\mathbf A$ is invertible, since the equation $\\mathbf{Ax}=\\mathbf b$ has a unique solution for any $\\mathbf b \\in \\mathbf R^n$.\n\n### Example\n\n$$\n\\mathbf v_1=\\begin{bmatrix} 1 \\\\0\\\\1 \\end{bmatrix} \\quad \\mathbf v_2=\\begin{bmatrix} 0 \\\\1\\\\1 \\end{bmatrix} \\quad  \\mathbf v_3=\\begin{bmatrix} 1 \\\\1\\\\0 \\end{bmatrix}\n$$\n\n$$\n\\begin{align*}\nq_1 &= \\frac{v_1}{|v_1|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\\\\nu_2 &= v_2 - \\langle v_2, q_1 \\rangle q_1 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix} - \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -\\frac{1}{\\sqrt{2}} \\\\ 1 \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix}, \\\\\nq_2 &= \\frac{u_2}{|u_2|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} -1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\\\\nu_3 &= v_3 - \\langle v_3, q_1 \\rangle q_1 - \\langle v_3, q_2 \\rangle q_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} - \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} - \\frac{1}{\\sqrt{6}}\\begin{bmatrix} -1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\ -\\frac{1}{\\sqrt{3}} \\end{bmatrix}, \\\\\nq_3 &= \\frac{u_3}{|u_3|} = \\frac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\end{bmatrix}.\n\\end{align*}\n$$\n\nTherefore, the orthogonal matrix $\\mathbf{Q}$ is:\n\n$$\n\\mathbf{Q}=\n\\begin{bmatrix} \n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0\\\\ \n0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}}\\\\ \n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{3}}\n\\end{bmatrix}\n$$\n\n\n### Orthogonal Matrices\n\n\n:::{#def-orthogonalMatrix}\nAn $n\\times n$ matrix $\\mathbf{Q}$ is orthogonal if its columns $\\mathbf q$ form an orthonormal set. That is, the columns $\\mathbf q$ of $\\mathbf{Q}$ satisfy \n\n$$\n\\langle\\mathbf{q}_i,\\mathbf{q}_j\\rangle =\n\\begin{cases}\n0 \\text{ if } i \\ne j \\\\\n1 \\text{ if } i = j \n\\end{cases}\n$$\n\nWe can organize all of the dot products amongst all pairs of columns by premultiplying the matrix by its transpose. Since matrix multiplication is defined as dot products between all rows of the left matrix with all columns of the right matrix, \n\n$$\n\\mathbf{Q}\\mathbf{Q}^T=\\mathbf{Q}^T\\mathbf{Q}=\\mathbf{I}\n$$\n\nwhere $\\mathbf{I}$ is the $n\\times n$ identity matrix.\n:::\n\n### Properties\n\n* Orthogonal columns: all columns are pair-wise orthogonal\n* Unit-norm columns: the norm (geometric length) of each column is exactly 1.\n* $\\mathbf{Q}^T\\mathbf{Q}=\\mathbf{Q}\\mathbf{Q}^T=\\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix of appropriate size.\n* $\\mathbf{Q}^T=\\mathbf{Q}^{-1}$\n  * Great propoerty because the matrix inverse is tedious and prone to numerical inaccuracies, whereas the matrix transpose is fast and accurate.\n* The determinant of an orthogonal matrix is either $1$ or $-1$.\n* If $\\mathbf{Q}$ is orthogonal, then its columns form an orthonormal set, i.e., the columns are pairwise orthogonal and each column has unit length.\n* Orthogonal matrices preserve lengths and angles. If $\\mathbf{x}$ and $\\mathbf{y}$ are two vectors, then $||\\mathbf{Qx}||=||\\mathbf{x}||$ and $\\mathbf{x}^T\\mathbf{y}=(\\mathbf{Qx})^T(\\mathbf{Qy})$.\n\n### Example\n\n**Example1** \nOrthogonal matrices include rotation matrices and reflection matrices. \n\nthe $2\\times 2$ matrix and the $3\\times 3$ matrix:\n$$\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{bmatrix}\n\\quad\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta & 0 \\\\\n\\sin \\theta & \\cos \\theta & 0 \\\\\n0 & 0 & 1 \n\\end{bmatrix}\n$$\n\nis an orthogonal matrix that rotates a vector counterclockwise by an angle $\\theta$ regardless of the\nrotation angle (as long as the same rotation angle is used in all matrix elements). \n\n```{python}\n# Pure rotation matrix\n\n# angle to rotate by\nth = np.pi/5\n\n# transformation matrix\nT = np.array([ \n              [ np.cos(th),np.sin(th)],\n              [-np.sin(th),np.cos(th)]\n            ])\n\n\n# original dots are a vertical line\nx = np.linspace(-1,1,20)\norigPoints = np.vstack( (np.zeros(x.shape),x) )\n\n\n# apply the transformation\ntransformedPoints = T @ origPoints\n\n\n# plot the points\nplt.figure(figsize=(6,6))\nplt.plot(origPoints[0,:],origPoints[1,:],'ko',label='Original')\nplt.plot(transformedPoints[0,:],transformedPoints[1,:],'s',color=[.7,.7,.7],label='Transformed')\n\nplt.axis('square')\nplt.xlim([-1.2,1.2])\nplt.ylim([-1.2,1.2])\nplt.legend()\nplt.title(f'Rotation by {np.rad2deg(th):.0f} degrees.')\nplt.show()\n\n# Animating transformations\n# function to update the axis on each iteration\ndef aframe(ph):\n\n  # create the transformation matrix\n  T = np.array([\n                 [  1, 1-ph ],\n                 [  0, 1    ]\n                ])\n  \n  # apply the transformation to the points using matrix multiplication\n  P = T@points\n\n  # update the dots\n  plth.set_xdata(P[0,:])\n  plth.set_ydata(P[1,:])\n\n  # export the plot handles\n  return plth\n\n\n# define XY points\ntheta  = np.linspace(0,2*np.pi,100)\npoints = np.vstack((np.sin(theta),np.cos(theta)))\n\n\n# setup figure\nfig,ax = plt.subplots(1,figsize=(12,6))\nplth,  = ax.plot(np.cos(x),np.sin(x),'ko')\nax.set_aspect('equal')\nax.set_xlim([-2,2])\nax.set_ylim([-2,2])\n\n# define values for transformation (note: clip off the final point for a smooth animation loop)\nphi = np.linspace(-1,1-1/40,40)**2\n\n# run animation!\nanimation.FuncAnimation(fig, aframe, phi, interval=100, repeat=True)\n\n```\n\n$$\n\\begin{align*}\n\\mathbf{Q}&=\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\\\\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n\\end{align*}\n$$\n\n$\\mathbf{Q}$ is orthogonal by computing $\\mathbf{Q}^T\\mathbf{Q}$:\n$$\n\\begin{align*}\n\\mathbf{Q}^T\\mathbf{Q}&=\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\\\\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\n\\frac{1}{2}+\\frac{1}{2} & -\\frac{1}{2}+\\frac{1}{2}\\\\\n-\\frac{1}{2}+\\frac{1}{2} & \\frac{1}{2}+\\frac{1}{2}\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\n\\end{bmatrix}\\\\\n&=\\mathbf{I}\n\\end{align*}\n$$\n\nTherefore, $\\mathbf{Q}$ is an orthogonal matrix.\n\n**Example 2**\n$$\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & -1 \n\\end{bmatrix}\n$$,\n\nwhich is an orthogonal matrix that reflects a vector across the $x$-axis.\n\n**Example 3**\nthe identity matrix is an example of an orthogonal matrix\n\n**Exmaple 4**\nPermutation matrices are also orthogonal. Permutation matrices are used to exchange rows of a matrix.\n\n\n## Orthogonal Bases\n\nAn orthogonal basis is a set of vectors that are pairwise orthogonal (perpendicular) and each vector is non-zero.\n\n:::{#def-orthogonalBases}\nA set of vectors ${\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n}$ is said to be an orthogonal basis of a vector space $V$ if:\n1. Each vector $\\mathbf{v}_i$ is non-zero.\n2. Each vector $\\mathbf{v}_i$ is orthogonal (perpendicular) to every other vector $\\mathbf{v}_j$, $i \\neq j$. In other words, $\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0$ for $i \\neq j$.\n:::\n\n### Example\n\n**Example1**\nLet $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$ and $\\mathbf{u} = \\begin{bmatrix} -1 \\ 0 \\ 1 \\end{bmatrix}$. We can check if these vectors form an orthogonal basis by computing their dot product:\n\n$$\n\\mathbf{v} \\cdot \\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\cdot \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix} = (-1) \\cdot 1 + (0) \\cdot 2 + (1) \\cdot 3 = 2\n$$\n\nSince the dot product is not zero, we can conclude that $\\mathbf{v}$ and $\\mathbf{u}$ do not form an orthogonal basis.\n\n**Example2**\n\nLet $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}$ and $\\mathbf{u} = \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\end{bmatrix}$. To check if they form an orthogonal basis, we need to compute their dot product:\n\n$$\n\\mathbf{v} \\cdot \\mathbf{u} = (1)(-2) + (2)(1) + (0)(1) = -2 + 2 + 0 = 0\n$$\n\nSince the dot product is zero, we know that $\\mathbf{v}$ and $\\mathbf{u}$ are orthogonal. We can also check that they are both nonzero and linearly independent by computing their norms:\n\n$$\n||\\mathbf{v}|| = \\sqrt{1^2 + 2^2 + 0^2} = \\sqrt{5} \\ne 0\n$$\n\n$$\n||\\mathbf{w}|| = \\sqrt{(-2)^2 + 1^2 + 1^2} = \\sqrt{6} \\ne 0\n$$\n\nTherefore, $\\mathbf{v}$ and $\\mathbf{w}$ form an orthogonal basis for $\\mathbb{R}^3$.\n\n## Gram-Schmidt (GS or G-S)\n\nThe Gram-Schmidt procedure is a method of transforming a nonorthogonal matrix into an orthogonal matrix by orthonormalizing a set of linearly independent vectors in an inner product space, usually the Euclidean space $\\mathbb{R}^n$. The process takes a sequence of vectors $\\mathbf v_1, \\mathbf v_2, \\dots, \\mathbf v_n$ and constructs an orthonormal sequence $\\mathbf q_1, \\mathbf q_2, \\dots, \\mathbf q_n$ that spans the same subspace as the original sequence.\n\nThe Gram-Schmidt procedure is useful for understanding orthogonal vector decomposition when programming and implementing the QR decomposition algorithm, and GS is the right way to conceptualize how and why QR decomposition works even if the low-level implementation is slightly different.\n\n:::{#def-gramSchmidt}\nLet $\\mathbf v_1, \\mathbf v_2, \\dots, \\mathbf v_n$ be a sequence of linearly independent vectors in $\\mathbb{R}^n$. Define $\\mathbf q_1$ to be the unit vector in the direction of $\\mathbf v_1$, i.e., $\\mathbf q_1 = \\frac{\\mathbf v_1}{|\\mathbf v_1|}$. For $k = 2, 3, \\dots, n$, define $\\mathbf q_k$ as follows:\n\n$$\n\\mathbf q_k = \\frac{\\mathbf u_k}{|\\mathbf u_k|}\n$$\n\nwhere $\\mathbf u_k=\\mathbf v_k-\\sum_{j=1}^{k-1}\\langle \\mathbf v_k,\\mathbf q_j \\rangle\\mathbf q_j$ \n:::\n\nThe vector $\\mathbf u_k$ is the projection of $\\mathbf v_k$ onto the subspace orthogonal to $\\text{span}{\\mathbf q_1, \\mathbf q_2, \\dots, \\mathbf q_{k-1}}$.\n\nThe Gram-Schmidt process produces an orthonormal basis $\\mathbf q_1, \\mathbf q_2, \\dots, \\mathbf q_n$ for $\\text{span}{\\mathbf v_1, \\mathbf v_2, \\dots, \\mathbf v_n}$. The matrix whose columns are $\\mathbf q_1, \\mathbf q_2, \\dots, \\mathbf q_n$ is an orthogonal matrix $\\mathbf{Q}$.\n\n$\\mathbf{V}$ is transformed into $\\mathbf{Q}$ according to the following algorithm:\n\nFor all column vectors $\\mathbf{v} \\in V$ starting from the first (leftmost) and moving systematically to the last (rightmost):\n\n1. Orthogonalize $\\mathbf v_k$ to all previous columns in matrix $\\mathbf Q$ using orthogonal vector decomposition. That is, compute the component of $\\mathbf v_k$ that is perpendicular to $\\mathbf q_{k-1}, \\mathbf q_{k-2}$, and so on down to $\\mathbf q_{1}$. The orthogonalized vector is called $\\mathbf v^{*}_k$.\n:::{.callout-note}\nThe first column vector is not orthogonalized because there are no preceeding vectors; therefore, you begin\nwith the following normalization step.\n:::\n2. Normalize $\\mathbf v^{*}_k$ to unit length. This is now $\\mathbf q_{k}$, the $k$ th column in matrix $\\mathbf Q$.\n\n```{python}\n\n# # Define a 4x4 random matrix\n# A = np.random.rand(4,4)\n# \n# # Gram-Schmidt procedure\n# Q = np.zeros_like(A)\n# for j in range(A.shape[1]):\n#     v = A[:,j]\n#     for i in range(j):\n#         q = Q[:,i]\n#         R[i,j] = np.dot(q,v)\n#         v -= R[i,j]*q\n#     R[j,j] = np.linalg.norm(v)\n#     Q[:,j] = v/R[j,j]\n# \n# # Check answer against Q from np.linalg.qr\n# Q_np, R_np = np.linalg.qr(A)\n# diff = Q - Q_np\n# sum_Q = Q + Q_np\n# \n# print(\"Q matrix (Gram-Schmidt):\\n\", Q)\n# print(\"Q matrix (numpy):\\n\", Q_np)\n# print(\"Difference between Q and Q_np:\\n\", diff)\n# print(\"Sum of Q and Q_np:\\n\", sum_Q)\n\n\n# create the matrix \nm = 4\nn = 4\nA = np.random.randn(m,n)\n\n# initialize\nQ = np.zeros((m,n))\n\n\n# the GS algo\nfor i in range(n):\n    \n    # initialize\n    Q[:,i] = A[:,i]\n    \n    # orthogonalize\n    a = A[:,i] # convenience\n    for j in range(i): # only to earlier cols\n        q = Q[:,j] # convenience\n        Q[:,i]=Q[:,i]-np.dot(a,q)/np.dot(q,q)*q\n    \n    # normalize\n    Q[:,i] = Q[:,i] / np.linalg.norm(Q[:,i])\n\n    \n# \"real\" QR decomposition for comparison\nQ_np,R = np.linalg.qr(A)\n\n# note the possible sign differences.\n# seemingly non-zero columns will be 0 when adding\nprint(\"Q matrix (Gram-Schmidt):\\n\", Q)\nprint(\"Q matrix (numpy):\\n\", Q_np)\nprint(\"Difference between Q and Q_np:\\n\", np.round( Q-Q_np ,10) ), print(' ')\nprint(\"Sum of Q and Q_np:\\n\", np.round( Q+Q_np ,10) )\n\n```\n\n## QR Decomposition\n\nQR decomposition is a factorization of a matrix $\\mathbf{A}$ into the product of an orthogonal matrix $\\mathbf{Q}$ and an upper triangular matrix $\\mathbf{R}$:\n\n$$\n\\mathbf{A}=\\mathbf{QR}\n$$ \n\nwhere $\\mathbf{Q}$ has orthonormal columns and $\\mathbf{R}$ is an upper triangular matrix.\n\nThe process of finding the QR decomposition of $\\mathbf{A}$ involves the Gram-Schmidt orthogonalization process, which produces an orthonormal basis for the columns of $\\mathbf{A}$ as mentioned earlier. The columns of $\\mathbf{Q}$ are the orthonormal basis vectors, and $\\mathbf{R}$ is the matrix that expresses the columns of $\\mathbf{A}$ in terms of the orthonormal basis vectors.\n\n$$\n\\begin{align*}\n\\mathbf{A}&=\\mathbf{QR}\\\\\n\\mathbf{Q}^T \\mathbf{A}&=\\mathbf{Q}^T\\mathbf{QR}\\\\\n\\mathbf{Q}^T\\mathbf{A}&=\\mathbf{R}\n\\end{align*}\n$$\n\nThe algorithm for computing the QR decomposition of a matrix $\\mathbf{A}$ is as follows:\n\n1. Apply the Gram-Schmidt orthogonalization process to the columns of $\\mathbf{A}$ to obtain an orthonormal basis for the column space of $\\mathbf{A}$. Let the resulting matrix be denoted by $\\mathbf{Q}$.\n2. Compute the matrix $\\mathbf{R}$ such that $\\mathbf{A} = \\mathbf{Q}\\mathbf{R}$. This can be done by solving the linear system $\\mathbf{R} = \\mathbf{Q}^T\\mathbf{A}$, which expresses the columns of $\\mathbf{A}$ in terms of the orthonormal basis vectors.\n```{python}\n\n\n# create a random matrix\nA = np.random.randn(6,6)\n\n# QR decomposition\nQ,R = np.linalg.qr(A)\n\n\n\n# show the matrices\nfig = plt.figure(figsize=(10,6))\naxs = [0]*5\nc = 1.5 # color limits\n\ngs1 = gridspec.GridSpec(2,6)\naxs[0] = plt.subplot(gs1[0,:2])\naxs[0].imshow(A,vmin=-c,vmax=c,cmap='gray')\naxs[0].set_title('A',fontweight='bold')\n\naxs[1] = plt.subplot(gs1[0,2:4])\naxs[1].imshow(Q,vmin=-c,vmax=c,cmap='gray')\naxs[1].set_title('Q',fontweight='bold')\n\naxs[2] = plt.subplot(gs1[0,4:6])\naxs[2].imshow(R,vmin=-c,vmax=c,cmap='gray')\naxs[2].set_title('R',fontweight='bold')\n\naxs[3] = plt.subplot(gs1[1,1:3])\naxs[3].imshow(A - Q@R,vmin=-c,vmax=c,cmap='gray')\naxs[3].set_title('A - QR',fontweight='bold')\n\naxs[4] = plt.subplot(gs1[1,3:5])\naxs[4].imshow(Q.T@Q,cmap='gray')\naxs[4].set_title(r'$\\mathbf{Q}^{T}\\mathbf{Q}$',fontweight='bold')\n\n# remove ticks from all axes\nfor a in axs:\n  a.set_xticks([])\n  a.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n```\n\n\n\n### Examples\n\n1. Consider the matrix $\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 2 & 3 \\\\ 3 & 4 \\end{bmatrix}$. To find its QR decomposition, we first apply the Gram-Schmidt process to obtain an orthonormal basis for its column space:\n\n$$\n\\mathbf q_1 = \\frac{1}{\\sqrt{14}}\\begin{bmatrix} 1 \\\\ 2 \\\\ 3\\end{bmatrix} \\quad \\mathbf q_2 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} -2 \\\\ 1 \\\\ 0\\end{bmatrix}\n$$\n\nThe resulting orthogonal matrix is \n$$\\begin{align*}\n\\mathbf{Q} = \\begin{bmatrix} \n\\frac{1}{\\sqrt{14}} & -\\frac{2}{\\sqrt{28}} \\\\\n\\frac{2}{\\sqrt{14}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{3}{\\sqrt{14}} & 0 \n\\end{bmatrix}\n\\end{align*}\n$$.\n\nNext, we compute the upper triangular matrix $\\mathbf{R}$ by solving $\\mathbf{R} = \\mathbf{Q}^T\\mathbf{A}$. This gives \n$$\n\\mathbf{R} = \\begin{bmatrix}\n\\sqrt{14} & \\frac{11}{\\sqrt{14}} \\\\\n0 & \\frac{\\sqrt{2}}{\\sqrt{7}}\n\\end{bmatrix}\n$$\n\n\nTherefore, the QR decomposition of $\\mathbf{A}$ is given by $\\mathbf{A} = \\mathbf{Q}\\mathbf{R}$.\n\n2. Consider the matrix $\\mathbf{A} = \\begin{bmatrix} 1 & -1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}$. Applying the Gram-Schmidt process to its columns yields the orthonormal basis vectors:\n\n$$\n\\begin{align*}\n\\mathbf q_1 &= \\frac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\\\\n\\mathbf u_2 &= \\mathbf v_2 - \\langle \\mathbf v_2, \\mathbf q_1 \\rangle \\mathbf q_1 = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix} - \\frac{1}{3}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -\\frac{4}{3} \\\\ -\\frac{1}{3} \\\\ \\frac{2}{3} \\end{bmatrix}, \\\\\n\\mathbf q_2 &= \\frac{u_2}{|u_2|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} -2 \\\\ -1 \\\\ 1 \\end{bmatrix}, \\\\\n\\mathbf u_3 &= \\mathbf v_3 - \\langle \\mathbf v_3, \\mathbf q_1 \\rangle \\mathbf q_1 - \\langle \\mathbf v_3, \\mathbf q_2 \\rangle \\mathbf q_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} - \\frac{1}{3}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} - \\frac{1}{2}\\begin{bmatrix} -2 \\\\ -1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{3} \\\\ -\\frac{1}{3} \\\\ -\\frac{1}{3} \\end{bmatrix}, \\\\\n\\mathbf q_3 &= \\frac{\\mathbf u_3}{|\\mathbf u_3|} = \\frac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\end{bmatrix}.\n\\end{align*}\n$$\n\nSo the QR decomposition of $\\mathbf{A}$ is $\\mathbf{A} = \\mathbf{QR}$ where\n\n$$\n\\mathbf{A=QR} = \\begin{bmatrix}\n\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & \\frac{2}{\\sqrt{6}} & 0 \\\\\n\\end{bmatrix}\n$$\n\nand\n\n$$\n\\mathbf{R} = \\begin{bmatrix}\n\\sqrt{3} & \\frac{1}{\\sqrt{3}} & \\sqrt{3} \\\\\n0 & \\frac{2}{\\sqrt{6}} & -\\frac{1}{\\sqrt{2}} \\\\\n0 & 0 & \\frac{1}{\\sqrt{2}} \\\\\n\\end{bmatrix}\n$$.\n\nSo, \n$$\n\\mathbf{A} = \n\\begin{bmatrix}\n 1 & -1 & 0 \\\\\n 1 & 0  & 1 \\\\\n 1 & 1  & 0 \n\\end{bmatrix} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & \\frac{2}{\\sqrt{6}} & 0 \\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sqrt{3} & \\frac{1}{\\sqrt{3}} & \\sqrt{3} \\\\\n0 & \\frac{2}{\\sqrt{6}} & -\\frac{1}{\\sqrt{2}} \\\\\n0 & 0 & \\frac{1}{\\sqrt{2}} \n\\end{bmatrix}\n=\n\\mathbf{QR}\n$$\n\n\nIn Python,\n\n```{python}\n\nA = np.array([[1, -1, 0], [1, 0, 1], [1, 1, 0]])\nQ, R = np.linalg.qr(A)\n\nprint(\"Q =\\n\", Q)\nprint(\"R =\\n\", R)\nprint(\"QR =\\n\", Q @ R)\n\n```\n\nThe QR decomposition of a matrix is not unique, So the $\\mathbf Q$ and $\\mathbf R$ could be different from the latex ones.\n\n### Sizes of $\\mathbf Q$ and $\\mathbf R$\n\nThe sizes of $\\mathbf Q$ and $\\mathbf R$ depend on the size of the matrix $\\mathbf A$ and on whether the QR decomposition is *reduced* or *full*.\n\nFor a tall matrix $(m > n)$, do we create a $\\mathbf Q$ matrix with n columns or m columns? \n\n* Economy or Reduced: $\\mathbf Q_{m\\times n}$ (tall $\\mathbf Q$)\n* Full or Complete: $\\mathbf Q_{m\\times m}$ (square $\\mathbf Q$) \n  * $\\mathbf Q$ can be square when $\\mathbf A$ is tall ($\\mathbf Q$ can have more columns than $\\mathbf A$)\n  * In python, the option of `np.linalg.qr(A,'complete')` is 'complete', which produces a full QR decomposition.\n  * The option of `np.linalg.qr(A,'reduced')` is 'reduced', which is the default, gives the economy-mode QR decomposition, in which $\\mathbf Q$ is the same size as $\\mathbf A$.\n* Likewise, the rank of $\\mathbf Q$ is always the maximum possible rank, which is $m$ for all square $\\mathbf Q$ matrices and $n$ for the economy $\\mathbf Q$. The rank of $\\mathbf R$ is the same as the rank of $\\mathbf A$.\n  * the difference of $\\operatorname{rank}(\\mathbf A) and \\operatorname{rank}(\\mathbf Q)$ means $\\mathbf Q$ spans all of $\\mathbb R^m$ even if the $\\operatorname{col}(\\mathbf A)$ is only a lower-dimensional subspace of $\\mathbb R^m$.\n* **non-uniqueness**: QR decomposition is not unique for all matrix sizes and ranks. ($\\mathbf A = \\mathbf Q_1 \\mathbf R_1$ and $\\mathbf A = \\mathbf Q_2\\mathbf R_2$ where $\\mathbf Q_1 \\ne \\mathbf Q_2$).\n* **uniqueness with constraints**: QR decomposition can be made unique given additional constraints (e.g., positive values on the diagonals of $\\mathbf R$)\n\n```{python}\nA = np.array([ [1,-1] ]).T\nQ,R = np.linalg.qr(A,'complete')\nQ*np.sqrt(2) # scaled by sqrt(2) to get integers\nprint(\"A =\\n\", A)\nprint(\"Q =\\n\", Q)\nprint(\"R =\\n\", R)\nprint(\"Q*np.sqrt(2)=\\n\", Q*np.sqrt(2))\n```\n\n### Upper-triangle of $\\mathbf R$\n\n* $\\mathbf R$ comes from the formula $\\mathbf{Q}^T\\mathbf{A = R}$.\n* The lower triangle of a product matrix comprises dot products between later rows of the left matrix and earlier columns of the right matrix.\n* The rows of $\\mathbf{Q}^T$ are the columns of $\\mathbf{Q}$.\n\n```{python}\n# Define matrix A\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Compute QR decomposition of A\nQ, R = np.linalg.qr(A)\n\n# Compute Q^T * A\nQtA = np.matmul(Q.T, A)\n\n# Extract upper triangle of R\nupper_R = np.triu(R)\n\n# Compute dot products between later rows of Q^T and earlier columns of A\ndot_products = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(i):\n        dot_products[i, j] = np.dot(Q.T[i], A[:, j])\n\n# Print results\nprint(\"A = \")\nprint(A)\nprint(\"Q = \")\nprint(Q)\nprint(\"R = \")\nprint(R)\nprint(r\"$Q^T * A = $\")\nprint(QtA)\nprint(\"Upper-triangle of R = \")\nprint(upper_R)\nprint(\"Dot products between later rows of Q^T and earlier columns of A = \")\nprint(dot_products)\n```\n\nThe lower triangle of $\\mathbf{R}$ comprises dot products of $\\mathbf Q^T\\mathbf A$ (between later rows of $\\mathbf{Q}^T$ and earlier columns of $\\mathbf{A}$)\n:::{.callout-note}\nNote that the lower triangle of $\\mathbf{R}$ is zero, because the first column of $\\mathbf{A}$ is orthogonal to the remaining columns of $\\mathbf{A}$. Thus, the pairs of vectors used to form the lower triangle of $\\mathbf{R}$ are orthogonal. On the other hand, the upper triangle of $\\mathbf{R}$ comes from the dot product of later rows of $\\mathbf{Q}$ and earlier columns of $\\mathbf{A}$. Specifically, the $(2,1)$ entry of $\\mathbf{R}$ is the dot product of the second row of $\\mathbf{Q}$ with the first column of $\\mathbf{A}$\n\nIf columns $i$ and $j$ of $\\mathbf A$ were already orthogonal, then the corresponding $(i,j)$ th element in $\\mathbf R$ would be zero. In fact, if you compute the QR decomposition of an orthogonal matrix, then $\\mathbf R$ will be a diagonal matrix in which the diagonal elements are the norms of each column in $\\mathbf A$. That means that if $\\mathbf{A = Q}$, then $\\mathbf{R = I}$, which is obvious from the equation solved for $\\mathbf{R}$ \n:::\n\n### QR and Inverses\n\nQR decomposition provides a more numerically stable way to compute the matrix inverse, $\\mathbf A^{-1}$ of $\\mathbf A$ because $\\mathbf Q$ is numerically stable due to the Householder reflection algorithm, and $\\mathbf R$ is numerically stable because it simply results from matrix multiplication.\n\n$$\n\\begin{align*}\n  \\mathbf{A}&=\\mathbf{QR}\\\\\n  \\mathbf{A}^{-1}&=(\\mathbf{QR})^{-1}\\\\\n  \\mathbf{A}^{-1}&=\\mathbf{R}^{-1}\\mathbf{Q}^{-1}\\\\\n  \\mathbf{A}^{-1}&=\\mathbf{R}^{-1}\\mathbf{Q}^{T}\n\\end{align*}\n$$\n\n\n## Projections\n\n## Least Squares Approximations\n\n## Orthogonal Bases and Gram-Schmidt\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"09.orthogonality.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","appendix-view-license":"라이센스 보기","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어","listing-page-filter":"필터","draft":"초안"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.5.56","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Orthogonality","subtitle":"Orthogonality of the Four Subspaces, Gram-Schmidt, QR Decomposition,","description":"template\n","categories":["Mathematics"],"author":"Kwangmin Kim","date":"04/21/2023","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"draft":false,"projectFormats":["html"]}