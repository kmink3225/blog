{"title":"Vector Space","markdown":{"yaml":{"title":"Vector Space","subtitle":"vector space, basis vector, susbspace, dimension, rank, column space, row space, null space","description":"template\n","categories":["Mathematics"],"author":"Kwangmin Kim","date":"04/10/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false},"headingText":"Vector Space","containsRefs":false,"markdown":"\n\n```{python}\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n```\n\n\nThe perspective of looking into linear algebra change from 'numbers' to 'vector space' through 'vectors'. \nHere, instead of looking at individual columns, we will observe **spaces of vecctors** and **their subspace** to better understand $\\mathbf{Ax}=\\mathbf{b}$.\nFrom this part, the fundamental theorem of linear algebra appears.\n\n## Definition\n\n::: {#def-VSnotation}\nThe space $\\mathbb{R}^{n}$ consists of all column vectors $\\mathbf v$ with $n$ components. \nThe space $\\mathbb{C}^{n}$ consists of all column vectors $\\mathbf v$ with $n$ components.\nwhere $n=1,2,3,\\ldots$, $\\mathbb{R}$ stands for a set of real numbers, and $\\mathbb{C}$ stands for a set of complex numbers.\n:::\n\nFor example, $\\mathbb{R}^{5}$ contains all column vectors $\\mathbf v$ with $5$ components, which is called '5-dimensional space'\n\n$$\n\\begin{bmatrix}2 \\\\ 4 \\end{bmatrix} \\in \\mathbb{R}^{2} \\quad \\begin{bmatrix}2 & 2 & 43 & 56 & 4 \\end{bmatrix} \\in \\mathbb{R}^{5} \\quad \\begin{bmatrix}2+4i \\\\ 4-21i \\end{bmatrix} \\in \\mathbb{C}^{2}\n$$\n\nA vector space is a set $\\mathbf{V}$ equipped with two operations: \n\n* vector addition and \n* scalar multiplication\n\n,which satisfy the following properties for all vectors $\\mathbf{u}, \\mathbf{v}, \\mathbf{w}$ in $V$ and all scalars $c, d$:\n\n1. Closure under vector addition: $\\mathbf{u} + \\mathbf{v} \\in V$\n2. Associativity of vector addition: $(\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})$\n3. Identity element of vector addition: There exists a vector $\\mathbf{0}$ in $V$ such that $\\mathbf{u} + \\mathbf{0} = \\mathbf{u}$ for all $\\mathbf{u}$ in $V$\n4. Existence of additive inverse: For each $\\mathbf{u}$ in $V$, there exists a vector $-\\mathbf{u}$ in $\\mathbf{V}$ such that $\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}$\n5. Closure under scalar multiplication: $c\\mathbf{u} \\in V$\n6. Distributive law of scalar multiplication with respect to scalar addition: $(c + d)\\mathbf{u} = c\\mathbf{u} + d\\mathbf{u}$\n7. Distributive law of scalar multiplication with respect to vector addition: $c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}$\n8. Associativity of scalar multiplication: $(cd)\\mathbf{u} = c(d\\mathbf{u})$\n9. Identity element of scalar multiplication: $1\\mathbf{u} = \\mathbf{u}$\n\nThe **closure** or **inside the vector space** mean that the result of the properties stays in the space.\n\n### Example\n\nLet $V = \\mathbb{R}^3$, the set of all 3-dimensional real vectors. The vector space $V$ is equipped with vector addition and scalar multiplication defined as usual component-wise. Vector addition and scalar multiplication satisfy all the properties listed in the definition of a vector space.\n\n### Three Special Vector Spaces\n\n* $\\mathbb{M}$ the vector space of all real 3 by 2 matrices\n  * the vectors $\\in \\mathbb{M}$ are really matrices.\n* $\\mathbb{F}$ the vector space of all real functions f(x)\n  * the vectors $\\in \\mathbb{F}$ are really functions.\n* $\\mathbb{Z}$ the vector space that consists only of a zero vectors.\n  * the vectors $\\in \\mathbb{Z}$ are used for the addition $\\mathbf{0}+\\mathbf{0}=\\mathbf{0}$\n\n## Subspaces\n\n### Definition\n\n::: {#def-subspace}\nA subset $W$ of a vector space $V$ over a field $F$ is called a subspace of $V$ if $W$ is also a vector space over $F$ under the same vector addition and scalar multiplication operations defined on $V$.\nA subspace of a vector space is a set of vectors (including $\\mathbf 0$) that satisifies two requirements: if $\\mathbf u$ and $\\mathbf v$ in the subspace and $c$ is any scalar, then \n\n1. $\\mathbf v + \\mathbf w$ is in the subspace\n2. $c \\mathbf v$ is in the subspace.\n:::\n\n:::{.callout-tip}\n\n### Field\n\nA field is a set $F$ along with two operations, addition ($+$) and multiplication ($\\cdot$), satisfying the following properties:\n\n* Closure: For all $a, b \\in F$, both $a + b$ and $a \\cdot b$ are in $F$.\n* Associativity: For all $a, b, c \\in F$, $(a + b) + c = a + (b + c)$ and $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n* Commutativity: For all $a, b \\in F$, $a + b = b + a$ and $a \\cdot b = b \\cdot a$.\n* Identity: There exists an element $0 \\in F$ such that for all $a \\in F$, $a + 0 = a$. There exists an element $1 \\in F$ such that for all $a \\in F$, $a \\cdot 1 = a$.\n* Inverse: For every $a \\in F$ with $a \\neq 0$, there exists an element $b \\in F$ such that $a + b = 0$. For every $a \\in F$ with $a \\neq 0$, there exists an element $c \\in F$ such that $a \\cdot c = 1$.\n\n### Example\n\n* The set of real numbers $\\mathbb{R}$ with addition and multiplication.\n* The set of complex numbers $\\mathbb{C}$ with addition and multiplication.\n* The set of rational numbers $\\mathbb{Q}$ with addition and multiplication.\n* The set of integers modulo a prime number $p$, denoted as $\\mathbb{Z}_p$, with addition and multiplication modulo $p$.\n\n:::\n\n### Properties\n\nA subset $W$ of a vector space $V$ is called a subspace of $V$ if it satisfies the following properties:\n\n* Closure under Addition: For all $\\mathbf{u}, \\mathbf{v} \\in W$, $\\mathbf{u} + \\mathbf{v} \\in W$.\n* Closure under Scalar Multiplication: For all $\\mathbf{u} \\in W$ and $c \\in \\mathbb{R}$ (or any field), $c\\mathbf{u} \\in W$.\n* Contains the Zero Vector: The zero vector $\\mathbf{0}$ of $V$ is in $W$.\n* Every subspace contains the zero vector\n* Lines through the origin are also subspaces.\n  (If we try to keep only part of a plane or line, the requirements for a subspace don't hold.)\n* A subspace containing $\\mathbf u$ and $\\mathbf v$ must contain all linear combinations $c\\mathbf v +d\\mathbf w$\n\n### Example\n\n1. The set of all real-valued $n$-dimensional column vectors, denoted as $\\mathbb{R}^n$, is a subspace of the vector space of all $n$-dimensional column vectors, denoted as $\\mathbb{R}^{n \\times 1}$, with closure under addition and scalar multiplication.\n  * Subset $V_3 = {(x, y, z) \\in \\mathbb{R}^3 : x + y + z = 0}$ of vector space $\\mathbb{R}^3$: $V_3 = \\{(x, y, z) \\in \\mathbb{R}^3 : x + y + z = 0\\}$\n2. The set of all $2 \\times 2$ symmetric matrices, denoted as $\\mathcal{S}^2$, is a subspace of the vector space of all $2 \\times 2$ matrices, denoted as $\\mathbb{R}^{2 \\times 2}$, with closure under addition and scalar multiplication.\n  * Subset $V_1 = {(x, y) \\in \\mathbb{R}^2 : x + y = 0}$ of vector space $\\mathbb{R}^2$: $V_1 = \\{(x, y) \\in \\mathbb{R}^2 : x + y = 0\\}$\n3. The set of all polynomials of degree at most $n$, denoted as $P_n$, is a subspace of the vector space of all polynomials, denoted as $P$, with closure under addition and scalar multiplication.\n  * Subset $V_2 = {p(x) \\in \\mathbb{R}[x] : \\text{deg}(p(x)) \\leq 3}$ of vector space $\\mathbb{R}[x]$: $V_2 = \\{p(x) \\in \\mathbb{R}[x] : \\text{deg}(p(x)) \\leq 3\\}$ \n  * \"deg\" stands for \"degree\" and refers to the degree of a polynomial. In the example, $V_2 = {p(x) \\in \\mathbb{R}[x] : \\text{deg}(p(x)) \\leq 3}$, $p(x)$ represents a polynomial, and $\\text{deg}(p(x))$ represents the degree of the polynomial $p(x)$. The condition $\\text{deg}(p(x)) \\leq 3$ specifies that the subset $V_2$ includes all polynomials of degree 3 or lower in the vector space $\\mathbb{R}[x]$ of polynomials with real coefficients.\n4. All upper triangular matrices $\\begin{bmatrix}a&b\\\\0&d\\end{bmatrix} \\in \\mathbb M$ are a subspace\n5. All diagonal matrices $\\begin{bmatrix}a&0\\\\0&d\\end{bmatrix} \\in \\mathbb M$ are a subspace\n\n## The Column Space of $\\mathbf A$\n\n### Definition\n\n::: {#def-subspace}\nGiven a matrix $\\mathbf A$ with columns $\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n$, the column space of $\\mathbf A$, denoted as $\\text{col}(\\mathbf A)$, is the set of all possible linear combinations of the columns of $\\mathbf A$. The combinations are all possible vectors $\\mathbf{Ax}$. In other words,\n$$\n\\begin{align*}\n\\text{col}(\\mathbf A) &= \\text{span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\} \\\\\nS &= \\text{ set of vectors in } V (\\text{ probably not a subspace }) \\\\\nSS &= \\text{ all combinations of vectors in } S \\\\\nSS &= \\text{ all } c_1\\mathbf{v}_1+\\ldots+c_n\\mathbf{v}_n \\\\\n  &= \\text{ the subspace of } V \\text{ spanned by } S\n\\end{align*}\n$$\n:::\n\n:::{.callout-important}\n* To solve $\\mathbf Ax=\\mathbf b$ is to express $\\mathbf b$ as a combination of the columns $\\mathbf a$. $\\mathbf b$ has to be in $\\text{col}(\\mathbf A)$ or no solution.\n  * The system $\\mathbf Ax=\\mathbf b$ is solvable if and only if $\\mathbf b$ is in $\\text{col}(\\mathbf A)$.\n* The column space of $\\mathbf A$ or $\\text{col}(\\mathbf A)$ is a subspace of $\\mathbb R^{m}$ not $\\mathbb R^{n}$.\n\n![Gilbert Strang - Introduction to Linear Algebra](../../../../../images/linear_algebra/column_space.PNG)\n:::\n\n\n\n### Example\n\n1. Consider the matrix\n$\\mathbf A = \\begin{bmatrix}1 & 2 & 3 \\\\4 & 5 & 6 \\\\7 & 8 & 9 \\end{bmatrix}$\n\nThe column space of $\\mathbf A$ is the span of its columns:\n$\\text{col}(\\mathbf A) = \\text{span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\mathbf{a}_3\\}$\n\n2. Describe the column spaces for the following matrices\n  * $\\mathbf I = \\begin{bmatrix}1 & 0 \\\\0 & 1 \\end{bmatrix}$\n    * The column space of the identity matrix $\\mathbf{I}$, denoted as $\\text{Col}(\\mathbf{I})$, spans the entire space $\\mathbb{R}^2$, as it includes all possible linear combinations of the standard unit vectors $\\mathbf{e}_1$ and $\\mathbf{e}_2$. Thus, $\\text{col}(\\mathbf A)$ is $\\mathbb R^2$\n  * $\\mathbf A = \\begin{bmatrix}1 & 2 \\\\2 & 4 \\end{bmatrix}$\n    * $\\text{Col}(\\mathbf{A})$, is the subspace spanned by the columns of $\\mathbf{A}$. Since the second column of $\\mathbf{A}$ is a scalar multiple (twice) of the first column, the column space of $\\mathbf{A}$ is a one-dimensional subspace, which is the line in $\\mathbb{R}^2$ that lies along the direction of the first column of $\\mathbf{A}$, $\\text{col}(\\mathbf A) = \\text{span}\\left\\{\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}\\right\\}$\n    * The equation $\\mathbf{Ax = b}$ is only solvable when $\\mathbf{b}$ is on the line.\n  * $\\mathbf B = \\begin{bmatrix}1 & 2 & 4\\\\0 & 0 & 4 \\end{bmatrix}$\n    * These column vectors span the column space of $\\mathbf{B}$. Since column 2 is a scalar multiple of column 1, and column 3 is a linear combination of columns 1 and 2, the column space of $\\mathbf{B}$ is the span of the first two column vectors, which is a two-dimensional subspace of $\\mathbb{R}^2$, $\\text{col}(\\mathbf B) = \\text{span}\\{\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3\\}$ is $\\mathbb R^2$.\n\n### Properties\n\nThe column space of a matrix $\\mathbf{A}$, denoted as $\\text{col}(\\mathbf{A})$ or $\\text{span}{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n}$, where $\\mathbf{a}_i$ represents the columns of $\\mathbf{A}$, has several important properties:\n\nConsists of all possible linear combinations: The column space consists of all possible linear combinations of the columns of $\\mathbf{A}$. In other words, any vector in $\\text{col}(\\mathbf{A})$ can be expressed as a linear combination of the columns of $\\mathbf{A}$.\n\n#### Is a subspace\n\nThe column space is a subspace of the vector space $\\mathbb{R}^m$, meaning it is closed under vector addition and scalar multiplication. This property allows for the column space to be used in linear combinations and as a solution space for non-homogeneous linear equations.\n\n#### Is spanned by the columns of $\\mathbf{A}$ \n\nThe column space is spanned by the columns of $\\mathbf{A}$, meaning any vector in $\\text{col}(\\mathbf{A})$ can be expressed as a linear combination of the columns of $\\mathbf{A}$.\n\n#### Has the same dimensionality as the row space\n\nThe dimensionality of the column space, denoted as $\\text{dim}(\\text{col}(\\mathbf{A}))$, is equal to the dimensionality of the row space of $\\mathbf{A}$, denoted as $\\text{dim}(\\text{row}(\\mathbf{A}))$. This property is known as the rank-nullity theorem.\n\n#### Has the same dimensionality as the row space\n\nThe dimensionality of the column space, denoted as $\\text{dim}(\\text{col}(\\mathbf{A}))$, is equal to the dimensionality of the row space of $\\mathbf{A}$, denoted as $\\text{dim}(\\text{row}(\\mathbf{A}))$. This property is known as the rank-nullity theorem.\nIs orthogonal to the nullspace: The column space is orthogonal to the nullspace of $\\mathbf{A}$, meaning any vector in the column space is orthogonal to any vector in the nullspace of $\\mathbf{A}$. This property is known as the fundamental theorem of linear algebra.\n\n## Nullspace of \\mathbf{A}: Solving \\mathbf{Ax = 0}\n\n### Definition\n\n::: {#def-nullspace}\nThe nullspace of a matrix refers to the set of all vectors that, when multiplied by the matrix, result in the zero vector $\\mathbf{0}$. For a matrix $\\mathbf{A}$, the nullspace is denoted as $\\text{null}(\\mathbf{A})$ and defined as follows:\n$$\n\\text{null}(\\mathbf{A}) = \\{\\mathbf{x} \\in \\mathbb{R}^n \\mid \\mathbf{Ax} = \\mathbf{0}\\}\n$$\n\nThe nullspace of $\\mathbf{A}$ is the set of all solutions $\\mathbf{x}$ to the homogeneous equation $\\mathbf{Ax} = \\mathbf{0}$.\n:::\n\nAll the solutions $\\mathbf{x} \\in \\mathbb R^n$, so the $\\text{null}(\\mathbf{A})$ is a subspace of $\\mathbb R^n$, and $\\operatorname{Col}(\\mathbf A)$ is a subspace of $\\mathbb R^m$ (from Gilbere Strang)\nIf the right side $\\mathbf{b}$ is not $\\mathbf{0}$, the solutions of $\\mathbf{Ax=b}$ do not form a subspace. The vector $\\mathbf{x=0}$ is only a solution if $\\mathbf{b=0}$. When the set of solutions does not include $\\mathbf{x=0}$, it cannot be a subspace. Section 3.4 will show how the solutions to $\\mathbf{Ax=b}$ (if there are any solutions) are shifted away from the origin by one particular solution.\n\n### Example\n\n#### Example 1\n\n$$\n\\begin{align*}\n\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix} \\quad \\mathbf{Ax=0} \\\\\n\\text{null}(\\mathbf{A}) = \\{\\mathbf{x} \\in \\mathbb{R}^n \\mid \\mathbf{Ax} = \\mathbf{0}\\} \\text{ is a subspace of }\\mathbb R^3\n\\end{align*}\n$$\n\n```{python}\n# Define the coefficients of the planes\na1, b1, c1, d1 = 1, 2, 3, 0\na2, b2, c2, d2 = 1, 2, 3, 6\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\n# Compute the corresponding z values for each x, y pair for the first plane\nZ1 = (-a1*X - b1*Y - d1) / c1\n\n# Compute the corresponding z values for each x, y pair for the second plane\nZ2 = (-a2*X - b2*Y - d2) / c2\n\n# Check if the origin is in the blue plane\norigin_in_blue_plane = a1*0 + b1*0 + c1*0 + d1 == 0\n\n# Check if the origin is in the red plane\norigin_in_red_plane = a2*0 + b2*0 + c2*0 + d2 == 0\n\n# Create a 3D plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the first plane\nax.plot_surface(X, Y, Z1, color='blue', alpha=0.5)\n\n# Plot the second plane\nax.plot_surface(X, Y, Z2, color='red', alpha=0.5)\n\n# Plot the origin as a black point\nax.scatter(0, 0, 0, color='black', s=50)\n\n\n# Highlight the origin in the blue plane\nif origin_in_blue_plane:\n    ax.scatter(0, 0, 0, color='blue', s=50, label=r'Origin in the $x+2y+3z=0$ Plane')\n\n# Highlight the origin in the red plane\nif origin_in_red_plane:\n    ax.scatter(0, 0, 0, color='red', s=50, label=r'Origin in the $x+2y+3z=6$ Plane')\n\n# Set labels and title\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title(r'Example of Nullspace: $x+2y+3z=0$ vs $x+2y+3z=6$')\n\n# Add a legend\nax.legend()\n\n# Show the plot\nplt.show()\n\n```\n\n#### Example 2\n\n$$\n\\begin{align*}\n\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\\\ 2 & 4 \\end{bmatrix} \\quad\n\\text{null}(\\mathbf{A}) = \\left\\{ \\begin{bmatrix} -2t \\\\ t \\end{bmatrix} \\mid t \\in \\mathbb{R} \\right\\}\n\\end{align*}\n$$\n\nThe null space of a matrix $\\mathbf{A}$, denoted as $\\text{null}(\\mathbf{A})$, is the set of all vectors $\\mathbf{x}$ that satisfy the equation $\\mathbf{Ax} = \\mathbf{0}$, where $\\mathbf{0}$ is the zero vector.\n\nFor the given matrix $\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\\\ 2 & 4 \\end{bmatrix}$, we can find the null space by solving the equation $\\mathbf{Ax} = \\mathbf{0}$.\n\nLet's set up the augmented matrix $[\\mathbf{A} | \\mathbf{0}]$ and perform Gaussian elimination to find the row-reduced echelon form (RREF):\n\n$$\n\\left[ \\begin{array}{ccc|c}\n1 & 2 & 0 & 0 \\\\\n3 & 6 & 0 & 0 \\\\\n2 & 4 & 0 & 0 \\\\\n\\end{array} \\right]\n$$\n\nRow 2 is equal to Row 1 multiplied by 3, and Row 3 is equal to Row 1 multiplied by 2. Thus, Row 3 is redundant and can be eliminated.\n\n$$\n\\begin{bmatrix}{ccc|c}\n1 & 2 & 0 & 0 \\\\\n3 & 6 & 0 & 0 \\\\\n2 & 4 & 0 & 0 \\\\\n\\end{bmatrix}\n$$\n\n\n$$\n\\begin{align*}\n\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\\\ 2 & 4 \\end{bmatrix} \\quad\n\\text{null}(\\mathbf{A}) = \\left\\{ \\begin{bmatrix} -2t \\\\ t \\end{bmatrix} \\mid t \\in \\mathbb{R} \\right\\}\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n\\mathbf{B} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\quad\n\\text{null}(\\mathbf{B}) = \\left\\{ \\begin{bmatrix} 0 \\\\ t \\\\ 0 \\end{bmatrix} \\mid t \\in \\mathbb{R} \\right\\}\n\\end{align*}\n$$\n\n### Properties\n\n#### Nullspace Contains the Zero Vector\n\nThe nullspace always contains the zero vector $\\mathbf{0}$, since $\\mathbf{A}\\mathbf{0} = \\mathbf{0}$.\n\n#### Nullspace Is a Subspace\n\nThe nullspace is a subspace of the vector space $\\mathbb{R}^n$, meaning it is closed under vector addition and scalar multiplication. This property allows for the nullspace to be used in linear combinations and as a solution space for homogeneous linear equations.\n\n#### Nullspace Is Orthogonal to Row Space\n\nThe nullspace is orthogonal to the row space of $\\mathbf{A}$, meaning any vector in the nullspace is orthogonal to all vectors in the row space of $\\mathbf{A}$.\n\n#### Nullspace Can Be Non-mpty\n\nThe nullspace can be non-empty, meaning there can be non-trivial solutions to the homogeneous equation $\\mathbf{Ax} = \\mathbf{0}$.\n\n#### Dimensionality\n\nThe dimensionality of the nullspace, denoted as $\\text{dim}(\\text{null}(\\mathbf{A}))$, is also known as the nullity of $\\mathbf{A}$. The nullity of $\\mathbf{A}$ is equal to the number of linearly independent columns or the number of free variables in the row-reduced echelon form (RREF) of $\\mathbf{A}$.\n\n## Rank and Row Reduced Form\n## Complete Solution\n## Independence, Basis, and Dimension\n## Dimensions of Four Subspaces\n\n\n\n## Basis Vector\n\nA basis for a vector space $V$ is a set of vectors ${v_1, v_2, \\ldots, v_n}$ that spans $V$ and is linearly independent. In other words, every vector in $V$ can be expressed as a linear combination of the basis vectors, and the basis vectors are linearly independent, meaning that no basis vector can be written as a linear combination of the other basis vectors.\n\nMathematically, a set of vectors ${v_1, v_2, \\ldots, v_n}$ is a basis for a vector space $V$ if it satisfies the following conditions:\n\nThe vectors ${v_1, v_2, \\ldots, v_n}$ span $V$, which means that for any vector $v$ in $V$, there exist scalars $c_1, c_2, \\ldots, c_n$ such that $v = c_1 v_1 + c_2 v_2 + \\ldots + c_n v_n$.\n\nThe vectors ${v_1, v_2, \\ldots, v_n}$ are linearly independent, which means that the only solution to the equation $c_1 v_1 + c_2 v_2 + \\ldots + c_n v_n = 0$ is $c_1 = c_2 = \\ldots = c_n = 0$.\n\n### Example\n\nConsider the vector space $V = \\mathbb{R}^3$, the set of all real-valued vectors with three components. A basis for $V$ can be given by the following set of vectors:\n\n$$\nv_1 = \\begin{bmatrix}\n1 \\\\ 0 \\\\ 0\n\\end{bmatrix}, \\quad\nv_2 = \\begin{bmatrix}\n0 \\\\ 1 \\\\ 0\n\\end{bmatrix}, \\quad\nv_3 = \\begin{bmatrix}\n0 \\\\ 0 \\\\ 1\n\\end{bmatrix}\n$$\n\nThese three vectors form a basis for $\\mathbb{R}^3$, as they span $\\mathbb{R}^3$ (any vector in $\\mathbb{R}^3$ can be expressed as a linear combination of these vectors) and they are linearly independent (the only solution to the equation $c_1 v_1 + c_2 v_2 + c_3 v_3 = 0$ is $c_1 = c_2 = c_3 = 0$).\n\nBasis vectors are fundamental in defining and understanding vector spaces because they provide a way to express any vector in the vector space as a linear combination of basis vectors. The properties of basis vectors, such as linear independence, spanning set, minimal set, and unique representation, are essential in establishing the foundational concepts of vector spaces and their properties. By using basis vectors, we can represent vectors in a vector space in a concise and systematic way, and they provide a basis for studying and analyzing vector spaces in various mathematical and practical applications.\n\n### Properties\n\n* Linear independence: A set of basis vectors is linearly independent, meaning that no vector in the set can be expressed as a linear combination of the others. In other words, the coefficients of the basis vectors in any linear combination that equals the zero vector are all zero.\n* Spanning set: The set of basis vectors spans the entire vector space, meaning that any vector in the vector space can be expressed as a linear combination of the basis vectors. In other words, the basis vectors \"span\" the vector space by forming a basis for it.\n* Minimal set: The set of basis vectors is minimal, meaning that no vector can be removed from the set without changing the span of the vector space. In other words, the basis vectors form the smallest possible set that can generate the entire vector space.\n* Unique representation: Any vector in the vector space can be uniquely represented as a linear combination of the basis vectors. This means that there is only one way to express a vector as a linear combination of the basis vectors, ensuring that the representation is unique.\n\n## Subspace\n\n### Definition of a subspace\n\nA subset $W$ of $V$ is called a subspace of $V$ if it satisfies the following three conditions:\n1. $W$ contains the zero vector $\\mathbf{0}$ of $V$.\n2. $W$ is closed under vector addition, i.e., for any vectors $\\mathbf{u}, \\mathbf{v} \\in W$, their sum $\\mathbf{u} + \\mathbf{v}$ is also in $W$.\n3. $W$ is closed under scalar multiplication, i.e., for any vector $\\mathbf{u} \\in W$ and any scalar $c$, their product $c\\mathbf{u}$ is also in $W$.\n\n### Example\n\nConsider the vector space $V = \\mathbb{R}^3$ with standard vector addition and scalar multiplication. Let $W$ be the subset of $V$ consisting of all vectors of the form $\\begin{bmatrix} x \\\\ y \\\\ 0 \\end{bmatrix}$, where $x, y$ are real numbers. We can express $W$ as:\n$W = \\left\\{ \\begin{bmatrix} x \\\\ y \\\\ 0 \\end{bmatrix} \\, \\middle| \\, x, y \\in \\mathbb{R} \\right\\}$\nThen $W$ is a subspace of $V$ because it satisfies the three conditions mentioned above.\n\n### Properties\n\nA subspace is a subset of a vector space that retains the structure of a vector space. Here are some properties of a subspace in relation to a vector space:\n\n1. Contains the zero vector: A subspace must contain the zero vector (denoted as $\\mathbf{0}$) of the vector space it is a subset of. This is because the zero vector is required for closure under vector addition and scalar multiplication.\n2. Closed under vector addition: If $\\mathbf{u}, \\mathbf{v} \\in W$, then $\\mathbf{u} + \\mathbf{v} \\in W$.\n3. Closed under scalar multiplication: If $\\mathbf{u} \\in W$ and $c$ is a scalar, then $c\\mathbf{u} \\in W$.\n\n\nThese properties provide a way to describe subsets of vector spaces that inherit certain properties from the parent vector space. Subspaces are useful for understanding the structure, properties, and behavior of vector spaces in a more focused and simplified manner. Some reasons why we need subspaces are:\n\n* Simplification and abstraction: Subspaces allow us to simplify the study of vector spaces by focusing on smaller, more manageable subsets that share similar properties. This abstraction can help us understand the fundamental structure and behavior of vector spaces without getting bogged down by the complexity of the entire vector space.\n* Study of special cases: Subspaces can represent special cases or special structures within a vector space that are of particular interest. \n  * For example, subspaces can represent sub-spaces of solutions to linear systems of equations, eigenspaces associated with eigenvalues of matrices, or orthogonal subspaces related to orthogonality and projections.\n* Applications in various fields: Subspaces have numerous applications in various fields, such as physics, computer graphics, data analysis, signal processing, and optimization, among others. Subspaces provide a mathematical framework for modeling, analyzing, and solving problems in these fields.\n* Computational efficiency: Subspaces can be used in algorithms and techniques for solving problems involving vector spaces in a computationally efficient manner. Techniques such as subspace methods, subspace approximation, and subspace projection can be employed to reduce the computational complexity of certain problems by working within lower-dimensional subspaces.\n\n### Span\n\n### Definition\n\nThe span of a set of vectors ${v_1, v_2, \\ldots, v_n}$ in a vector space $V$, denoted by $\\text{span}{v_1, v_2, \\ldots, v_n}$, is the set of all possible linear combinations of these vectors. Mathematically, it is defined as:\n\n$$\n\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\} = \\left\\{ c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\ldots + c_n\\mathbf{v}_n | c_1, c_2, \\ldots, c_n \\in \\mathbb{R} \\right\\}\n$$\n\n\nwhere $c_1, c_2, \\ldots, c_n$ are scalar coefficients and $\\mathbb{R}$ represents the set of real numbers.\n\nExample:\nLet's consider the set of vectors $v_1$, $v_2$, and $v_3$ defined as:\n\n$\\mathbf{v}_1 = \\begin{bmatrix} 1 \\ 0 \\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\ 1 \\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 0 \\ 0 \\ 1 \\end{bmatrix}$\n\nThe span of these vectors, denoted by $\\text{span}{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3}$, is the set of all possible linear combinations of these vectors, which in this case is the entire three-dimensional vector space $\\mathbb{R}^3$, since any vector in $\\mathbb{R}^3$ can be expressed as a linear combination of $\\mathbf{v}_1$, $\\mathbf{v}_2$, and $\\mathbf{v}_3$.\n\n### Proporties\n\n* Span is a subspace: The span of a set of vectors is always a subspace of the vector space in which the vectors belong. This means that it satisfies all the properties of a vector space, including closure under vector addition and scalar multiplication.\n* Span is the smallest subspace: The span of a set of vectors is the smallest subspace that contains all the vectors in the set. It is the intersection of all subspaces that contain the vectors, and thus it forms the smallest subspace that spans the set of vectors.\n* Span is closed under linear combinations: The span of a set of vectors is closed under linear combinations of the vectors. This means that any linear combination of the vectors in the set will also be in the span.\n* Span generates the entire vector space: The span of a set of vectors is capable of generating the entire vector space to which the vectors belong. This means that any vector in the vector space can be expressed as a linear combination of the vectors in the span.\n* Span is unique: The span of a set of vectors is unique, meaning that it is uniquely determined by the set of vectors being spanned. However, the span may be different for different sets of vectors.\n\nThese properties make the concept of span important in linear algebra, as it allows us to understand the space spanned by a set of vectors and the relationships between vectors within a vector space.\n\nThe concept of span is important in linear algebra because of:\n\n* Understanding vector space: The span of a set of vectors helps us understand the space that can be generated by those vectors within a vector space. It provides insight into the range of possible values and combinations that can be obtained using the given set of vectors.\n* Solving systems of linear equations: Span is closely related to solving systems of linear equations. The solutions to a system of linear equations can be expressed as linear combinations of the vectors in the span of the coefficient matrix. By finding the span of a set of vectors, we can determine the possible solutions to a system of linear equations and understand the relationship between different solutions.\n* Basis for vector spaces: The span of a set of vectors can form a basis for a vector space. A basis is a set of linearly independent vectors that span the entire vector space. By finding the span of a set of vectors, we can determine if they form a basis for a vector space, and if so, use them as a foundation for understanding and manipulating vectors within that space.\n* Vector space operations: Span is closed under vector space operations, such as vector addition and scalar multiplication. This property allows us to perform operations on vectors within the span and obtain new vectors that are still within the span. It also allows us to express any vector in the vector space as a linear combination of vectors in the span.\n* Dimensionality and rank: The span of a set of vectors is closely related to the dimensionality and rank of a vector space. The dimension of a vector space is the number of linearly independent vectors in a basis for that space, and the rank of a matrix is the dimension of the span of its column vectors. Understanding the span of vectors can help us determine the dimensionality and rank of a vector space, which has applications in areas such as data analysis, machine learning, and signal processing.\n\nSpan provides insights into the space spanned by a set of vectors, the relationships between vectors within a vector space, and the possible solutions to systems of linear equations. It also serves as a basis for vector spaces and facilitates vector space operations, and is closely related to the dimensionality and rank of a vector space.\n\n## Dimensionality and Rank\n\n### Definition\n\n* The dimensionality of a vector space is defined as the number of linearly independent vectors in its basis.\n* The rank of a matrix is defined as the maximum number of linearly independent rows or columns in the matrix.\n\n### Example \n\nLet $V$ be a vector space with a basis ${ \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n }$. The dimensionality of $V$, denoted as $\\text{dim}(V)$, is the number of linearly independent vectors in its basis, which is equal to $n$.\nLet $\\mathbf A$ be an $m \\times n$ matrix. The rank of $\\mathbf A$, denoted as $\\text{rank}(\\mathbf A)$, is the maximum number of linearly independent rows or columns in $\\mathbf A$. It can also be defined as the dimensionality of the column space or row space of $\\mathbf A$.\n\n### Properties\n\n#### Dimensionality\n\n* Dimensionality refers to the number of elements or components in a vector or the size of a vector space.\n* The dimensionality of a vector space is always a positive integer.\n* Dimensionality is additive, meaning that the dimensionality of the direct sum of vector spaces is equal to the sum of their individual dimensionality.\n* A set of vectors is linearly independent if and only if the dimensionality of the vector space they span is equal to the number of vectors in the set.\n\n#### Rank\n\n* The rank of a matrix is the maximum number of linearly independent rows or columns in the matrix.\n* The rank of a matrix is always a non-negative integer.\n* The rank of a matrix is equal to the dimensionality of its column space and row space.\n* The rank of a matrix is invariant under elementary row and column operations.\n* The rank of a matrix is less than or equal to the minimum of the number of rows and columns in the matrix.\n\n:::{.callout-note}\nIn some contexts, the term \"dimensionality\" may also refer to the dimensionality of the column space or row space of a matrix, which is equivalent to the rank of the matrix.\n:::\n\n\n## Column Space\n\n### Definition\n\nThe column space of a matrix is the subspace spanned by its column vectors. It is denoted by $\\text{Col}(\\mathbf A)$ or $\\text{span}{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n}$, where $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n$ are the column vectors of the matrix $\\mathbf A$.\n\n### Example\n\n$$\n\\mathbf A=\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \n\\end{bmatrix}\n$$\n\nThe column space of matrix $\\mathbf A$, denoted by $\\text{Col}(\\mathbf A)$ or $\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}$, is the subspace spanned by its column vectors $\\mathbf{v}_1 = [1, 4, 7]^T$, $\\mathbf{v}_2 = [2, 5, 8]^T$, and $\\mathbf{v}_3 = [3, 6, 9]^T$.\n\n$$\n\\text{Col}(\\mathbf{A}) = \\text{span}\\left\\{\n\\begin{bmatrix}\n1 \\\\ 4 \\\\ 7 \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n2 \\\\ 5 \\\\ 8 \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n3 \\\\ 6 \\\\ 9 \\\\\n\\end{bmatrix}\n\\right\\}\n$$\n\n\n### Properties\n\n* It is a subspace: The column space of a matrix is a subspace of the vector space in which the matrix's columns reside. This means it is closed under vector addition and scalar multiplication, and contains the zero vector.\n* It is spanned by the columns of the matrix: The column space of a matrix is the span of its column vectors. In other words, it is the smallest subspace that contains all the column vectors of the matrix.\n* It is the range of the corresponding linear transformation: The column space of a matrix is the range of the linear transformation associated with that matrix. This means it contains all possible output vectors that can be obtained by applying the linear transformation to input vectors.\n* It has the same dimension as the rank of the matrix: The dimension of the column space of a matrix is equal to the rank of the matrix. This is known as the column space's dimensionality property.\n* It provides a basis for the row space and null space: The column space of a matrix provides a basis for both the row space and the null space of the matrix. The row space is the orthogonal complement of the null space, and the column space is the orthogonal complement of the left null space.\n* It determines the solvability of linear systems: The column space of a coefficient matrix in a system of linear equations determines whether the system has a unique solution, infinitely many solutions, or no solution at all. If the column space spans the entire vector space, the system has a unique solution. If the column space does not span the entire vector space, the system has either infinitely many solutions (if the rank of the matrix is less than the number of variables) or no solution (if the rank of the matrix is less than the number of equations).\n\n## Row Space\n\n### Definition\n\nThe row space of a matrix $\\mathbf{A}$, denoted as $\\text{Row}(\\mathbf{A})$, is the subspace spanned by the rows of $\\mathbf{A}$.\n\n$$\n\\text{Row}(\\mathbf{A}) = \\text{span}\\{\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_m\\}\n$$\n\nwhere $\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_m$ are the rows of $\\mathbf{A}$.\n\n### Example\n\nThe row space of $\\mathbf{A}$ is the subspace spanned by the rows of $\\mathbf{A}$, which can be expressed as:\n\n$$\n\\text{Row}(\\mathbf{A}) = \\text{span}\\left\\{\n\\begin{bmatrix}\n1 \\\\ 2 \\\\ 3 \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n4 \\\\ 5 \\\\ 6 \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n7 \\\\ 8 \\\\ 9 \\\\\n\\end{bmatrix}\n\\right\\}\n$$\n\n### Properties\n\n* It is a subspace: The row space of a matrix is a subspace of the vector space in which the matrix's rows reside. This means it is closed under vector addition and scalar multiplication, and contains the zero vector.\n* It is spanned by the rows of the matrix: The row space of a matrix is the span of its row vectors. In other words, it is the smallest subspace that contains all the row vectors of the matrix.\n* It is the range of the corresponding linear transformation: The row space of a matrix is the range of the linear transformation associated with the transpose of that matrix. This means it contains all possible output vectors that can be obtained by applying the transpose of the linear transformation to input vectors.\n* It has the same dimension as the rank of the matrix: The dimension of the row space of a matrix is equal to the rank of the matrix. This is known as the row space's dimensionality property.\n* It provides a basis for the null space and left null space: The row space of a matrix provides a basis for both the null space (kernel) and the left null space (cokernel) of the matrix. The null space is the orthogonal complement of the row space, and the left null space is the orthogonal complement of the column space.\n* It determines the row-rank and column-rank equality: The row space of a matrix determines the row-rank and column-rank equality property, which states that the number of linearly independent rows (the row-rank) is equal to the number of linearly independent columns (the column-rank) of the matrix.\n* It determines the solvability of linear systems: The row space of a coefficient matrix in a system of linear equations determines whether the system has a unique solution, infinitely many solutions, or no solution at all. If the row space spans the entire vector space, the system has a unique solution. If the row space does not span the entire vector space, the system has either infinitely many solutions (if the rank of the matrix is less than the number of variables) or no solution (if the rank of the matrix is less than the number of equations).\n\n\n## Null Space\n\n$$\n\\text{Null}(\\mathbf{A}) = \\left\\{ \\mathbf{x} \\in \\mathbb{R}^n \\, \\middle| \\, \\mathbf{A}\\mathbf{x} = \\mathbf{0} \\right\\}\n$$\n\nwhere:\n\n* $\\text{Null}$ represents the null space of a matrix,\n* $\\mathbf{A}$ represents the given matrix,\n* $\\mathbf{x}$ represents a vector in the null space of \\mathbf{A},\n* $\\mathbb{R}^n$ represents the n-dimensional real vector space, and\n* $\\mathbf{0}$ represents the zero vector.\n\n### Example\n\n$$\n\\mathbf A=\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \n\\end{bmatrix}\n$$\n\nThe null space of $\\mathbf{A}$ is the set of all vectors $\\mathbf{x} \\in \\mathbb{R}^3$ that satisfy $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$.\n\n### Properties\n\nThe null space of a matrix, also known as the kernel space, is the set of all vectors that are mapped to the zero vector by the linear transformation associated with the matrix.\n\n* Contains the zero vector: The null space always contains the zero vector, as any matrix multiplied by the zero vector results in the zero vector.\n* Subspace property: The null space is a subspace of the vector space from which the vectors are drawn. This means that it is closed under vector addition and scalar multiplication. In other words, if two vectors are in the null space, their sum and any scalar multiple of them will also be in the null space.\n* Dimensionality: The dimension of the null space is equal to the number of linearly independent solutions to the homogeneous linear system associated with the matrix. This is known as the nullity of the matrix.\n* Basis: The null space has a basis consisting of linearly independent vectors that span the entire null space. This basis is used to represent all vectors in the null space as linear combinations of the basis vectors.\n* Relationship to solvability: The null space is directly related to the solvability of a system of linear equations. If the null space contains only the zero vector, the system has a unique solution. If the null space contains non-zero vectors, the system has infinitely many solutions, and the null space vectors represent the general solutions.\n* Orthogonal complement of row space: The null space is orthogonal to the row space of the matrix. This means that any vector in the null space is orthogonal to every vector in the row space, and vice versa.\n* Relationship to matrix rank: The dimension of the null space, also known as the nullity, is related to the rank of the matrix through the rank-nullity theorem. The rank of the matrix plus the nullity of the matrix is equal to the number of columns in the matrix.\n* Computation: The null space can be computed by finding the solutions to the homogeneous linear system $\\mathbf{Ax} = \\mathbf{0}$, where $\\mathbf{A}$ is the coefficient matrix of the system of linear equations.\n\n\n## Column Space vs Row Space vs Null Space\n\nWhile both column space and row space are subspaces associated with a matrix, they serve different roles in linear algebra.\n\n### Column Space vs Row Space\n\n* Dimensionality: The dimensionality of the column space and row space can differ. The dimension of the column space is determined by the number of linearly independent columns, which is also known as the rank of the matrix. On the other hand, the dimension of the row space is determined by the number of linearly independent rows of the matrix, which may not necessarily be the same as the rank of the matrix.\n* Basis and Span: The column space is typically used to find a basis for the range (output space) of a linear transformation associated with the matrix, while the row space is used to find a basis for the null space (kernel) and left null space (cokernel) of the matrix. The column space spans the range of the linear transformation, while the row space spans the orthogonal complement of the null space and left null space.\n* Solvability of linear systems: The row space of the coefficient matrix in a system of linear equations determines the solvability of the system, while the column space is not directly related to the solvability. Specifically, if the row space spans the entire vector space, the system has a unique solution. If the row space does not span the entire vector space, the system may have infinitely many solutions or no solution. The column space, on the other hand, does not directly determine the solvability of the system.\n* Transpose relationship: The row space of a matrix is related to the range of the linear transformation associated with the transpose of the matrix, while the column space is directly related to the range of the original matrix. This means that the row space and column space are related through the transpose operation, but they are not identical.\n\n### Null Space\n\nNull space: The null space of a matrix is the set of all vectors that are mapped to the zero vector by the linear transformation associated with the matrix. It represents the subspace of the vector space that consists of solutions to the homogeneous linear system  $\\mathbf{Ax} = \\mathbf{0}$, where $\\mathbf{A}$ is the coefficient matrix of the system of linear equations.\n\n### The relationships between these spaces\n\n* The column space and row space are related, as they are orthogonal complements of each other. This means that any vector in the row space is orthogonal to any vector in the null space, and vice versa. This relationship is known as the row-space-null-space decomposition.\n* The dimension of the column space is equal to the rank of the matrix, which is the maximum number of linearly independent columns or rows. Similarly, the dimension of the row space is also equal to the rank of the matrix.\n* The null space is orthogonal to both the column space and the row space. This means that any vector in the null space is orthogonal to any vector in the column space and the row space.\n* The null space is useful in determining the solvability of a system of linear equations. If the null space contains only the zero vector, the system has a unique solution. If the null space contains non-zero vectors, the system has infinitely many solutions.","srcMarkdownNoYaml":"\n\n```{python}\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n```\n\n# Vector Space\n\nThe perspective of looking into linear algebra change from 'numbers' to 'vector space' through 'vectors'. \nHere, instead of looking at individual columns, we will observe **spaces of vecctors** and **their subspace** to better understand $\\mathbf{Ax}=\\mathbf{b}$.\nFrom this part, the fundamental theorem of linear algebra appears.\n\n## Definition\n\n::: {#def-VSnotation}\nThe space $\\mathbb{R}^{n}$ consists of all column vectors $\\mathbf v$ with $n$ components. \nThe space $\\mathbb{C}^{n}$ consists of all column vectors $\\mathbf v$ with $n$ components.\nwhere $n=1,2,3,\\ldots$, $\\mathbb{R}$ stands for a set of real numbers, and $\\mathbb{C}$ stands for a set of complex numbers.\n:::\n\nFor example, $\\mathbb{R}^{5}$ contains all column vectors $\\mathbf v$ with $5$ components, which is called '5-dimensional space'\n\n$$\n\\begin{bmatrix}2 \\\\ 4 \\end{bmatrix} \\in \\mathbb{R}^{2} \\quad \\begin{bmatrix}2 & 2 & 43 & 56 & 4 \\end{bmatrix} \\in \\mathbb{R}^{5} \\quad \\begin{bmatrix}2+4i \\\\ 4-21i \\end{bmatrix} \\in \\mathbb{C}^{2}\n$$\n\nA vector space is a set $\\mathbf{V}$ equipped with two operations: \n\n* vector addition and \n* scalar multiplication\n\n,which satisfy the following properties for all vectors $\\mathbf{u}, \\mathbf{v}, \\mathbf{w}$ in $V$ and all scalars $c, d$:\n\n1. Closure under vector addition: $\\mathbf{u} + \\mathbf{v} \\in V$\n2. Associativity of vector addition: $(\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})$\n3. Identity element of vector addition: There exists a vector $\\mathbf{0}$ in $V$ such that $\\mathbf{u} + \\mathbf{0} = \\mathbf{u}$ for all $\\mathbf{u}$ in $V$\n4. Existence of additive inverse: For each $\\mathbf{u}$ in $V$, there exists a vector $-\\mathbf{u}$ in $\\mathbf{V}$ such that $\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}$\n5. Closure under scalar multiplication: $c\\mathbf{u} \\in V$\n6. Distributive law of scalar multiplication with respect to scalar addition: $(c + d)\\mathbf{u} = c\\mathbf{u} + d\\mathbf{u}$\n7. Distributive law of scalar multiplication with respect to vector addition: $c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}$\n8. Associativity of scalar multiplication: $(cd)\\mathbf{u} = c(d\\mathbf{u})$\n9. Identity element of scalar multiplication: $1\\mathbf{u} = \\mathbf{u}$\n\nThe **closure** or **inside the vector space** mean that the result of the properties stays in the space.\n\n### Example\n\nLet $V = \\mathbb{R}^3$, the set of all 3-dimensional real vectors. The vector space $V$ is equipped with vector addition and scalar multiplication defined as usual component-wise. Vector addition and scalar multiplication satisfy all the properties listed in the definition of a vector space.\n\n### Three Special Vector Spaces\n\n* $\\mathbb{M}$ the vector space of all real 3 by 2 matrices\n  * the vectors $\\in \\mathbb{M}$ are really matrices.\n* $\\mathbb{F}$ the vector space of all real functions f(x)\n  * the vectors $\\in \\mathbb{F}$ are really functions.\n* $\\mathbb{Z}$ the vector space that consists only of a zero vectors.\n  * the vectors $\\in \\mathbb{Z}$ are used for the addition $\\mathbf{0}+\\mathbf{0}=\\mathbf{0}$\n\n## Subspaces\n\n### Definition\n\n::: {#def-subspace}\nA subset $W$ of a vector space $V$ over a field $F$ is called a subspace of $V$ if $W$ is also a vector space over $F$ under the same vector addition and scalar multiplication operations defined on $V$.\nA subspace of a vector space is a set of vectors (including $\\mathbf 0$) that satisifies two requirements: if $\\mathbf u$ and $\\mathbf v$ in the subspace and $c$ is any scalar, then \n\n1. $\\mathbf v + \\mathbf w$ is in the subspace\n2. $c \\mathbf v$ is in the subspace.\n:::\n\n:::{.callout-tip}\n\n### Field\n\nA field is a set $F$ along with two operations, addition ($+$) and multiplication ($\\cdot$), satisfying the following properties:\n\n* Closure: For all $a, b \\in F$, both $a + b$ and $a \\cdot b$ are in $F$.\n* Associativity: For all $a, b, c \\in F$, $(a + b) + c = a + (b + c)$ and $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$.\n* Commutativity: For all $a, b \\in F$, $a + b = b + a$ and $a \\cdot b = b \\cdot a$.\n* Identity: There exists an element $0 \\in F$ such that for all $a \\in F$, $a + 0 = a$. There exists an element $1 \\in F$ such that for all $a \\in F$, $a \\cdot 1 = a$.\n* Inverse: For every $a \\in F$ with $a \\neq 0$, there exists an element $b \\in F$ such that $a + b = 0$. For every $a \\in F$ with $a \\neq 0$, there exists an element $c \\in F$ such that $a \\cdot c = 1$.\n\n### Example\n\n* The set of real numbers $\\mathbb{R}$ with addition and multiplication.\n* The set of complex numbers $\\mathbb{C}$ with addition and multiplication.\n* The set of rational numbers $\\mathbb{Q}$ with addition and multiplication.\n* The set of integers modulo a prime number $p$, denoted as $\\mathbb{Z}_p$, with addition and multiplication modulo $p$.\n\n:::\n\n### Properties\n\nA subset $W$ of a vector space $V$ is called a subspace of $V$ if it satisfies the following properties:\n\n* Closure under Addition: For all $\\mathbf{u}, \\mathbf{v} \\in W$, $\\mathbf{u} + \\mathbf{v} \\in W$.\n* Closure under Scalar Multiplication: For all $\\mathbf{u} \\in W$ and $c \\in \\mathbb{R}$ (or any field), $c\\mathbf{u} \\in W$.\n* Contains the Zero Vector: The zero vector $\\mathbf{0}$ of $V$ is in $W$.\n* Every subspace contains the zero vector\n* Lines through the origin are also subspaces.\n  (If we try to keep only part of a plane or line, the requirements for a subspace don't hold.)\n* A subspace containing $\\mathbf u$ and $\\mathbf v$ must contain all linear combinations $c\\mathbf v +d\\mathbf w$\n\n### Example\n\n1. The set of all real-valued $n$-dimensional column vectors, denoted as $\\mathbb{R}^n$, is a subspace of the vector space of all $n$-dimensional column vectors, denoted as $\\mathbb{R}^{n \\times 1}$, with closure under addition and scalar multiplication.\n  * Subset $V_3 = {(x, y, z) \\in \\mathbb{R}^3 : x + y + z = 0}$ of vector space $\\mathbb{R}^3$: $V_3 = \\{(x, y, z) \\in \\mathbb{R}^3 : x + y + z = 0\\}$\n2. The set of all $2 \\times 2$ symmetric matrices, denoted as $\\mathcal{S}^2$, is a subspace of the vector space of all $2 \\times 2$ matrices, denoted as $\\mathbb{R}^{2 \\times 2}$, with closure under addition and scalar multiplication.\n  * Subset $V_1 = {(x, y) \\in \\mathbb{R}^2 : x + y = 0}$ of vector space $\\mathbb{R}^2$: $V_1 = \\{(x, y) \\in \\mathbb{R}^2 : x + y = 0\\}$\n3. The set of all polynomials of degree at most $n$, denoted as $P_n$, is a subspace of the vector space of all polynomials, denoted as $P$, with closure under addition and scalar multiplication.\n  * Subset $V_2 = {p(x) \\in \\mathbb{R}[x] : \\text{deg}(p(x)) \\leq 3}$ of vector space $\\mathbb{R}[x]$: $V_2 = \\{p(x) \\in \\mathbb{R}[x] : \\text{deg}(p(x)) \\leq 3\\}$ \n  * \"deg\" stands for \"degree\" and refers to the degree of a polynomial. In the example, $V_2 = {p(x) \\in \\mathbb{R}[x] : \\text{deg}(p(x)) \\leq 3}$, $p(x)$ represents a polynomial, and $\\text{deg}(p(x))$ represents the degree of the polynomial $p(x)$. The condition $\\text{deg}(p(x)) \\leq 3$ specifies that the subset $V_2$ includes all polynomials of degree 3 or lower in the vector space $\\mathbb{R}[x]$ of polynomials with real coefficients.\n4. All upper triangular matrices $\\begin{bmatrix}a&b\\\\0&d\\end{bmatrix} \\in \\mathbb M$ are a subspace\n5. All diagonal matrices $\\begin{bmatrix}a&0\\\\0&d\\end{bmatrix} \\in \\mathbb M$ are a subspace\n\n## The Column Space of $\\mathbf A$\n\n### Definition\n\n::: {#def-subspace}\nGiven a matrix $\\mathbf A$ with columns $\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n$, the column space of $\\mathbf A$, denoted as $\\text{col}(\\mathbf A)$, is the set of all possible linear combinations of the columns of $\\mathbf A$. The combinations are all possible vectors $\\mathbf{Ax}$. In other words,\n$$\n\\begin{align*}\n\\text{col}(\\mathbf A) &= \\text{span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\} \\\\\nS &= \\text{ set of vectors in } V (\\text{ probably not a subspace }) \\\\\nSS &= \\text{ all combinations of vectors in } S \\\\\nSS &= \\text{ all } c_1\\mathbf{v}_1+\\ldots+c_n\\mathbf{v}_n \\\\\n  &= \\text{ the subspace of } V \\text{ spanned by } S\n\\end{align*}\n$$\n:::\n\n:::{.callout-important}\n* To solve $\\mathbf Ax=\\mathbf b$ is to express $\\mathbf b$ as a combination of the columns $\\mathbf a$. $\\mathbf b$ has to be in $\\text{col}(\\mathbf A)$ or no solution.\n  * The system $\\mathbf Ax=\\mathbf b$ is solvable if and only if $\\mathbf b$ is in $\\text{col}(\\mathbf A)$.\n* The column space of $\\mathbf A$ or $\\text{col}(\\mathbf A)$ is a subspace of $\\mathbb R^{m}$ not $\\mathbb R^{n}$.\n\n![Gilbert Strang - Introduction to Linear Algebra](../../../../../images/linear_algebra/column_space.PNG)\n:::\n\n\n\n### Example\n\n1. Consider the matrix\n$\\mathbf A = \\begin{bmatrix}1 & 2 & 3 \\\\4 & 5 & 6 \\\\7 & 8 & 9 \\end{bmatrix}$\n\nThe column space of $\\mathbf A$ is the span of its columns:\n$\\text{col}(\\mathbf A) = \\text{span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\mathbf{a}_3\\}$\n\n2. Describe the column spaces for the following matrices\n  * $\\mathbf I = \\begin{bmatrix}1 & 0 \\\\0 & 1 \\end{bmatrix}$\n    * The column space of the identity matrix $\\mathbf{I}$, denoted as $\\text{Col}(\\mathbf{I})$, spans the entire space $\\mathbb{R}^2$, as it includes all possible linear combinations of the standard unit vectors $\\mathbf{e}_1$ and $\\mathbf{e}_2$. Thus, $\\text{col}(\\mathbf A)$ is $\\mathbb R^2$\n  * $\\mathbf A = \\begin{bmatrix}1 & 2 \\\\2 & 4 \\end{bmatrix}$\n    * $\\text{Col}(\\mathbf{A})$, is the subspace spanned by the columns of $\\mathbf{A}$. Since the second column of $\\mathbf{A}$ is a scalar multiple (twice) of the first column, the column space of $\\mathbf{A}$ is a one-dimensional subspace, which is the line in $\\mathbb{R}^2$ that lies along the direction of the first column of $\\mathbf{A}$, $\\text{col}(\\mathbf A) = \\text{span}\\left\\{\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}\\right\\}$\n    * The equation $\\mathbf{Ax = b}$ is only solvable when $\\mathbf{b}$ is on the line.\n  * $\\mathbf B = \\begin{bmatrix}1 & 2 & 4\\\\0 & 0 & 4 \\end{bmatrix}$\n    * These column vectors span the column space of $\\mathbf{B}$. Since column 2 is a scalar multiple of column 1, and column 3 is a linear combination of columns 1 and 2, the column space of $\\mathbf{B}$ is the span of the first two column vectors, which is a two-dimensional subspace of $\\mathbb{R}^2$, $\\text{col}(\\mathbf B) = \\text{span}\\{\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3\\}$ is $\\mathbb R^2$.\n\n### Properties\n\nThe column space of a matrix $\\mathbf{A}$, denoted as $\\text{col}(\\mathbf{A})$ or $\\text{span}{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n}$, where $\\mathbf{a}_i$ represents the columns of $\\mathbf{A}$, has several important properties:\n\nConsists of all possible linear combinations: The column space consists of all possible linear combinations of the columns of $\\mathbf{A}$. In other words, any vector in $\\text{col}(\\mathbf{A})$ can be expressed as a linear combination of the columns of $\\mathbf{A}$.\n\n#### Is a subspace\n\nThe column space is a subspace of the vector space $\\mathbb{R}^m$, meaning it is closed under vector addition and scalar multiplication. This property allows for the column space to be used in linear combinations and as a solution space for non-homogeneous linear equations.\n\n#### Is spanned by the columns of $\\mathbf{A}$ \n\nThe column space is spanned by the columns of $\\mathbf{A}$, meaning any vector in $\\text{col}(\\mathbf{A})$ can be expressed as a linear combination of the columns of $\\mathbf{A}$.\n\n#### Has the same dimensionality as the row space\n\nThe dimensionality of the column space, denoted as $\\text{dim}(\\text{col}(\\mathbf{A}))$, is equal to the dimensionality of the row space of $\\mathbf{A}$, denoted as $\\text{dim}(\\text{row}(\\mathbf{A}))$. This property is known as the rank-nullity theorem.\n\n#### Has the same dimensionality as the row space\n\nThe dimensionality of the column space, denoted as $\\text{dim}(\\text{col}(\\mathbf{A}))$, is equal to the dimensionality of the row space of $\\mathbf{A}$, denoted as $\\text{dim}(\\text{row}(\\mathbf{A}))$. This property is known as the rank-nullity theorem.\nIs orthogonal to the nullspace: The column space is orthogonal to the nullspace of $\\mathbf{A}$, meaning any vector in the column space is orthogonal to any vector in the nullspace of $\\mathbf{A}$. This property is known as the fundamental theorem of linear algebra.\n\n## Nullspace of \\mathbf{A}: Solving \\mathbf{Ax = 0}\n\n### Definition\n\n::: {#def-nullspace}\nThe nullspace of a matrix refers to the set of all vectors that, when multiplied by the matrix, result in the zero vector $\\mathbf{0}$. For a matrix $\\mathbf{A}$, the nullspace is denoted as $\\text{null}(\\mathbf{A})$ and defined as follows:\n$$\n\\text{null}(\\mathbf{A}) = \\{\\mathbf{x} \\in \\mathbb{R}^n \\mid \\mathbf{Ax} = \\mathbf{0}\\}\n$$\n\nThe nullspace of $\\mathbf{A}$ is the set of all solutions $\\mathbf{x}$ to the homogeneous equation $\\mathbf{Ax} = \\mathbf{0}$.\n:::\n\nAll the solutions $\\mathbf{x} \\in \\mathbb R^n$, so the $\\text{null}(\\mathbf{A})$ is a subspace of $\\mathbb R^n$, and $\\operatorname{Col}(\\mathbf A)$ is a subspace of $\\mathbb R^m$ (from Gilbere Strang)\nIf the right side $\\mathbf{b}$ is not $\\mathbf{0}$, the solutions of $\\mathbf{Ax=b}$ do not form a subspace. The vector $\\mathbf{x=0}$ is only a solution if $\\mathbf{b=0}$. When the set of solutions does not include $\\mathbf{x=0}$, it cannot be a subspace. Section 3.4 will show how the solutions to $\\mathbf{Ax=b}$ (if there are any solutions) are shifted away from the origin by one particular solution.\n\n### Example\n\n#### Example 1\n\n$$\n\\begin{align*}\n\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix} \\quad \\mathbf{Ax=0} \\\\\n\\text{null}(\\mathbf{A}) = \\{\\mathbf{x} \\in \\mathbb{R}^n \\mid \\mathbf{Ax} = \\mathbf{0}\\} \\text{ is a subspace of }\\mathbb R^3\n\\end{align*}\n$$\n\n```{python}\n# Define the coefficients of the planes\na1, b1, c1, d1 = 1, 2, 3, 0\na2, b2, c2, d2 = 1, 2, 3, 6\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\n# Compute the corresponding z values for each x, y pair for the first plane\nZ1 = (-a1*X - b1*Y - d1) / c1\n\n# Compute the corresponding z values for each x, y pair for the second plane\nZ2 = (-a2*X - b2*Y - d2) / c2\n\n# Check if the origin is in the blue plane\norigin_in_blue_plane = a1*0 + b1*0 + c1*0 + d1 == 0\n\n# Check if the origin is in the red plane\norigin_in_red_plane = a2*0 + b2*0 + c2*0 + d2 == 0\n\n# Create a 3D plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the first plane\nax.plot_surface(X, Y, Z1, color='blue', alpha=0.5)\n\n# Plot the second plane\nax.plot_surface(X, Y, Z2, color='red', alpha=0.5)\n\n# Plot the origin as a black point\nax.scatter(0, 0, 0, color='black', s=50)\n\n\n# Highlight the origin in the blue plane\nif origin_in_blue_plane:\n    ax.scatter(0, 0, 0, color='blue', s=50, label=r'Origin in the $x+2y+3z=0$ Plane')\n\n# Highlight the origin in the red plane\nif origin_in_red_plane:\n    ax.scatter(0, 0, 0, color='red', s=50, label=r'Origin in the $x+2y+3z=6$ Plane')\n\n# Set labels and title\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title(r'Example of Nullspace: $x+2y+3z=0$ vs $x+2y+3z=6$')\n\n# Add a legend\nax.legend()\n\n# Show the plot\nplt.show()\n\n```\n\n#### Example 2\n\n$$\n\\begin{align*}\n\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\\\ 2 & 4 \\end{bmatrix} \\quad\n\\text{null}(\\mathbf{A}) = \\left\\{ \\begin{bmatrix} -2t \\\\ t \\end{bmatrix} \\mid t \\in \\mathbb{R} \\right\\}\n\\end{align*}\n$$\n\nThe null space of a matrix $\\mathbf{A}$, denoted as $\\text{null}(\\mathbf{A})$, is the set of all vectors $\\mathbf{x}$ that satisfy the equation $\\mathbf{Ax} = \\mathbf{0}$, where $\\mathbf{0}$ is the zero vector.\n\nFor the given matrix $\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\\\ 2 & 4 \\end{bmatrix}$, we can find the null space by solving the equation $\\mathbf{Ax} = \\mathbf{0}$.\n\nLet's set up the augmented matrix $[\\mathbf{A} | \\mathbf{0}]$ and perform Gaussian elimination to find the row-reduced echelon form (RREF):\n\n$$\n\\left[ \\begin{array}{ccc|c}\n1 & 2 & 0 & 0 \\\\\n3 & 6 & 0 & 0 \\\\\n2 & 4 & 0 & 0 \\\\\n\\end{array} \\right]\n$$\n\nRow 2 is equal to Row 1 multiplied by 3, and Row 3 is equal to Row 1 multiplied by 2. Thus, Row 3 is redundant and can be eliminated.\n\n$$\n\\begin{bmatrix}{ccc|c}\n1 & 2 & 0 & 0 \\\\\n3 & 6 & 0 & 0 \\\\\n2 & 4 & 0 & 0 \\\\\n\\end{bmatrix}\n$$\n\n\n$$\n\\begin{align*}\n\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\\\ 2 & 4 \\end{bmatrix} \\quad\n\\text{null}(\\mathbf{A}) = \\left\\{ \\begin{bmatrix} -2t \\\\ t \\end{bmatrix} \\mid t \\in \\mathbb{R} \\right\\}\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n\\mathbf{B} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\quad\n\\text{null}(\\mathbf{B}) = \\left\\{ \\begin{bmatrix} 0 \\\\ t \\\\ 0 \\end{bmatrix} \\mid t \\in \\mathbb{R} \\right\\}\n\\end{align*}\n$$\n\n### Properties\n\n#### Nullspace Contains the Zero Vector\n\nThe nullspace always contains the zero vector $\\mathbf{0}$, since $\\mathbf{A}\\mathbf{0} = \\mathbf{0}$.\n\n#### Nullspace Is a Subspace\n\nThe nullspace is a subspace of the vector space $\\mathbb{R}^n$, meaning it is closed under vector addition and scalar multiplication. This property allows for the nullspace to be used in linear combinations and as a solution space for homogeneous linear equations.\n\n#### Nullspace Is Orthogonal to Row Space\n\nThe nullspace is orthogonal to the row space of $\\mathbf{A}$, meaning any vector in the nullspace is orthogonal to all vectors in the row space of $\\mathbf{A}$.\n\n#### Nullspace Can Be Non-mpty\n\nThe nullspace can be non-empty, meaning there can be non-trivial solutions to the homogeneous equation $\\mathbf{Ax} = \\mathbf{0}$.\n\n#### Dimensionality\n\nThe dimensionality of the nullspace, denoted as $\\text{dim}(\\text{null}(\\mathbf{A}))$, is also known as the nullity of $\\mathbf{A}$. The nullity of $\\mathbf{A}$ is equal to the number of linearly independent columns or the number of free variables in the row-reduced echelon form (RREF) of $\\mathbf{A}$.\n\n## Rank and Row Reduced Form\n## Complete Solution\n## Independence, Basis, and Dimension\n## Dimensions of Four Subspaces\n\n\n\n## Basis Vector\n\nA basis for a vector space $V$ is a set of vectors ${v_1, v_2, \\ldots, v_n}$ that spans $V$ and is linearly independent. In other words, every vector in $V$ can be expressed as a linear combination of the basis vectors, and the basis vectors are linearly independent, meaning that no basis vector can be written as a linear combination of the other basis vectors.\n\nMathematically, a set of vectors ${v_1, v_2, \\ldots, v_n}$ is a basis for a vector space $V$ if it satisfies the following conditions:\n\nThe vectors ${v_1, v_2, \\ldots, v_n}$ span $V$, which means that for any vector $v$ in $V$, there exist scalars $c_1, c_2, \\ldots, c_n$ such that $v = c_1 v_1 + c_2 v_2 + \\ldots + c_n v_n$.\n\nThe vectors ${v_1, v_2, \\ldots, v_n}$ are linearly independent, which means that the only solution to the equation $c_1 v_1 + c_2 v_2 + \\ldots + c_n v_n = 0$ is $c_1 = c_2 = \\ldots = c_n = 0$.\n\n### Example\n\nConsider the vector space $V = \\mathbb{R}^3$, the set of all real-valued vectors with three components. A basis for $V$ can be given by the following set of vectors:\n\n$$\nv_1 = \\begin{bmatrix}\n1 \\\\ 0 \\\\ 0\n\\end{bmatrix}, \\quad\nv_2 = \\begin{bmatrix}\n0 \\\\ 1 \\\\ 0\n\\end{bmatrix}, \\quad\nv_3 = \\begin{bmatrix}\n0 \\\\ 0 \\\\ 1\n\\end{bmatrix}\n$$\n\nThese three vectors form a basis for $\\mathbb{R}^3$, as they span $\\mathbb{R}^3$ (any vector in $\\mathbb{R}^3$ can be expressed as a linear combination of these vectors) and they are linearly independent (the only solution to the equation $c_1 v_1 + c_2 v_2 + c_3 v_3 = 0$ is $c_1 = c_2 = c_3 = 0$).\n\nBasis vectors are fundamental in defining and understanding vector spaces because they provide a way to express any vector in the vector space as a linear combination of basis vectors. The properties of basis vectors, such as linear independence, spanning set, minimal set, and unique representation, are essential in establishing the foundational concepts of vector spaces and their properties. By using basis vectors, we can represent vectors in a vector space in a concise and systematic way, and they provide a basis for studying and analyzing vector spaces in various mathematical and practical applications.\n\n### Properties\n\n* Linear independence: A set of basis vectors is linearly independent, meaning that no vector in the set can be expressed as a linear combination of the others. In other words, the coefficients of the basis vectors in any linear combination that equals the zero vector are all zero.\n* Spanning set: The set of basis vectors spans the entire vector space, meaning that any vector in the vector space can be expressed as a linear combination of the basis vectors. In other words, the basis vectors \"span\" the vector space by forming a basis for it.\n* Minimal set: The set of basis vectors is minimal, meaning that no vector can be removed from the set without changing the span of the vector space. In other words, the basis vectors form the smallest possible set that can generate the entire vector space.\n* Unique representation: Any vector in the vector space can be uniquely represented as a linear combination of the basis vectors. This means that there is only one way to express a vector as a linear combination of the basis vectors, ensuring that the representation is unique.\n\n## Subspace\n\n### Definition of a subspace\n\nA subset $W$ of $V$ is called a subspace of $V$ if it satisfies the following three conditions:\n1. $W$ contains the zero vector $\\mathbf{0}$ of $V$.\n2. $W$ is closed under vector addition, i.e., for any vectors $\\mathbf{u}, \\mathbf{v} \\in W$, their sum $\\mathbf{u} + \\mathbf{v}$ is also in $W$.\n3. $W$ is closed under scalar multiplication, i.e., for any vector $\\mathbf{u} \\in W$ and any scalar $c$, their product $c\\mathbf{u}$ is also in $W$.\n\n### Example\n\nConsider the vector space $V = \\mathbb{R}^3$ with standard vector addition and scalar multiplication. Let $W$ be the subset of $V$ consisting of all vectors of the form $\\begin{bmatrix} x \\\\ y \\\\ 0 \\end{bmatrix}$, where $x, y$ are real numbers. We can express $W$ as:\n$W = \\left\\{ \\begin{bmatrix} x \\\\ y \\\\ 0 \\end{bmatrix} \\, \\middle| \\, x, y \\in \\mathbb{R} \\right\\}$\nThen $W$ is a subspace of $V$ because it satisfies the three conditions mentioned above.\n\n### Properties\n\nA subspace is a subset of a vector space that retains the structure of a vector space. Here are some properties of a subspace in relation to a vector space:\n\n1. Contains the zero vector: A subspace must contain the zero vector (denoted as $\\mathbf{0}$) of the vector space it is a subset of. This is because the zero vector is required for closure under vector addition and scalar multiplication.\n2. Closed under vector addition: If $\\mathbf{u}, \\mathbf{v} \\in W$, then $\\mathbf{u} + \\mathbf{v} \\in W$.\n3. Closed under scalar multiplication: If $\\mathbf{u} \\in W$ and $c$ is a scalar, then $c\\mathbf{u} \\in W$.\n\n\nThese properties provide a way to describe subsets of vector spaces that inherit certain properties from the parent vector space. Subspaces are useful for understanding the structure, properties, and behavior of vector spaces in a more focused and simplified manner. Some reasons why we need subspaces are:\n\n* Simplification and abstraction: Subspaces allow us to simplify the study of vector spaces by focusing on smaller, more manageable subsets that share similar properties. This abstraction can help us understand the fundamental structure and behavior of vector spaces without getting bogged down by the complexity of the entire vector space.\n* Study of special cases: Subspaces can represent special cases or special structures within a vector space that are of particular interest. \n  * For example, subspaces can represent sub-spaces of solutions to linear systems of equations, eigenspaces associated with eigenvalues of matrices, or orthogonal subspaces related to orthogonality and projections.\n* Applications in various fields: Subspaces have numerous applications in various fields, such as physics, computer graphics, data analysis, signal processing, and optimization, among others. Subspaces provide a mathematical framework for modeling, analyzing, and solving problems in these fields.\n* Computational efficiency: Subspaces can be used in algorithms and techniques for solving problems involving vector spaces in a computationally efficient manner. Techniques such as subspace methods, subspace approximation, and subspace projection can be employed to reduce the computational complexity of certain problems by working within lower-dimensional subspaces.\n\n### Span\n\n### Definition\n\nThe span of a set of vectors ${v_1, v_2, \\ldots, v_n}$ in a vector space $V$, denoted by $\\text{span}{v_1, v_2, \\ldots, v_n}$, is the set of all possible linear combinations of these vectors. Mathematically, it is defined as:\n\n$$\n\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\} = \\left\\{ c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\ldots + c_n\\mathbf{v}_n | c_1, c_2, \\ldots, c_n \\in \\mathbb{R} \\right\\}\n$$\n\n\nwhere $c_1, c_2, \\ldots, c_n$ are scalar coefficients and $\\mathbb{R}$ represents the set of real numbers.\n\nExample:\nLet's consider the set of vectors $v_1$, $v_2$, and $v_3$ defined as:\n\n$\\mathbf{v}_1 = \\begin{bmatrix} 1 \\ 0 \\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\ 1 \\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 0 \\ 0 \\ 1 \\end{bmatrix}$\n\nThe span of these vectors, denoted by $\\text{span}{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3}$, is the set of all possible linear combinations of these vectors, which in this case is the entire three-dimensional vector space $\\mathbb{R}^3$, since any vector in $\\mathbb{R}^3$ can be expressed as a linear combination of $\\mathbf{v}_1$, $\\mathbf{v}_2$, and $\\mathbf{v}_3$.\n\n### Proporties\n\n* Span is a subspace: The span of a set of vectors is always a subspace of the vector space in which the vectors belong. This means that it satisfies all the properties of a vector space, including closure under vector addition and scalar multiplication.\n* Span is the smallest subspace: The span of a set of vectors is the smallest subspace that contains all the vectors in the set. It is the intersection of all subspaces that contain the vectors, and thus it forms the smallest subspace that spans the set of vectors.\n* Span is closed under linear combinations: The span of a set of vectors is closed under linear combinations of the vectors. This means that any linear combination of the vectors in the set will also be in the span.\n* Span generates the entire vector space: The span of a set of vectors is capable of generating the entire vector space to which the vectors belong. This means that any vector in the vector space can be expressed as a linear combination of the vectors in the span.\n* Span is unique: The span of a set of vectors is unique, meaning that it is uniquely determined by the set of vectors being spanned. However, the span may be different for different sets of vectors.\n\nThese properties make the concept of span important in linear algebra, as it allows us to understand the space spanned by a set of vectors and the relationships between vectors within a vector space.\n\nThe concept of span is important in linear algebra because of:\n\n* Understanding vector space: The span of a set of vectors helps us understand the space that can be generated by those vectors within a vector space. It provides insight into the range of possible values and combinations that can be obtained using the given set of vectors.\n* Solving systems of linear equations: Span is closely related to solving systems of linear equations. The solutions to a system of linear equations can be expressed as linear combinations of the vectors in the span of the coefficient matrix. By finding the span of a set of vectors, we can determine the possible solutions to a system of linear equations and understand the relationship between different solutions.\n* Basis for vector spaces: The span of a set of vectors can form a basis for a vector space. A basis is a set of linearly independent vectors that span the entire vector space. By finding the span of a set of vectors, we can determine if they form a basis for a vector space, and if so, use them as a foundation for understanding and manipulating vectors within that space.\n* Vector space operations: Span is closed under vector space operations, such as vector addition and scalar multiplication. This property allows us to perform operations on vectors within the span and obtain new vectors that are still within the span. It also allows us to express any vector in the vector space as a linear combination of vectors in the span.\n* Dimensionality and rank: The span of a set of vectors is closely related to the dimensionality and rank of a vector space. The dimension of a vector space is the number of linearly independent vectors in a basis for that space, and the rank of a matrix is the dimension of the span of its column vectors. Understanding the span of vectors can help us determine the dimensionality and rank of a vector space, which has applications in areas such as data analysis, machine learning, and signal processing.\n\nSpan provides insights into the space spanned by a set of vectors, the relationships between vectors within a vector space, and the possible solutions to systems of linear equations. It also serves as a basis for vector spaces and facilitates vector space operations, and is closely related to the dimensionality and rank of a vector space.\n\n## Dimensionality and Rank\n\n### Definition\n\n* The dimensionality of a vector space is defined as the number of linearly independent vectors in its basis.\n* The rank of a matrix is defined as the maximum number of linearly independent rows or columns in the matrix.\n\n### Example \n\nLet $V$ be a vector space with a basis ${ \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n }$. The dimensionality of $V$, denoted as $\\text{dim}(V)$, is the number of linearly independent vectors in its basis, which is equal to $n$.\nLet $\\mathbf A$ be an $m \\times n$ matrix. The rank of $\\mathbf A$, denoted as $\\text{rank}(\\mathbf A)$, is the maximum number of linearly independent rows or columns in $\\mathbf A$. It can also be defined as the dimensionality of the column space or row space of $\\mathbf A$.\n\n### Properties\n\n#### Dimensionality\n\n* Dimensionality refers to the number of elements or components in a vector or the size of a vector space.\n* The dimensionality of a vector space is always a positive integer.\n* Dimensionality is additive, meaning that the dimensionality of the direct sum of vector spaces is equal to the sum of their individual dimensionality.\n* A set of vectors is linearly independent if and only if the dimensionality of the vector space they span is equal to the number of vectors in the set.\n\n#### Rank\n\n* The rank of a matrix is the maximum number of linearly independent rows or columns in the matrix.\n* The rank of a matrix is always a non-negative integer.\n* The rank of a matrix is equal to the dimensionality of its column space and row space.\n* The rank of a matrix is invariant under elementary row and column operations.\n* The rank of a matrix is less than or equal to the minimum of the number of rows and columns in the matrix.\n\n:::{.callout-note}\nIn some contexts, the term \"dimensionality\" may also refer to the dimensionality of the column space or row space of a matrix, which is equivalent to the rank of the matrix.\n:::\n\n\n## Column Space\n\n### Definition\n\nThe column space of a matrix is the subspace spanned by its column vectors. It is denoted by $\\text{Col}(\\mathbf A)$ or $\\text{span}{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n}$, where $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n$ are the column vectors of the matrix $\\mathbf A$.\n\n### Example\n\n$$\n\\mathbf A=\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \n\\end{bmatrix}\n$$\n\nThe column space of matrix $\\mathbf A$, denoted by $\\text{Col}(\\mathbf A)$ or $\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}$, is the subspace spanned by its column vectors $\\mathbf{v}_1 = [1, 4, 7]^T$, $\\mathbf{v}_2 = [2, 5, 8]^T$, and $\\mathbf{v}_3 = [3, 6, 9]^T$.\n\n$$\n\\text{Col}(\\mathbf{A}) = \\text{span}\\left\\{\n\\begin{bmatrix}\n1 \\\\ 4 \\\\ 7 \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n2 \\\\ 5 \\\\ 8 \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n3 \\\\ 6 \\\\ 9 \\\\\n\\end{bmatrix}\n\\right\\}\n$$\n\n\n### Properties\n\n* It is a subspace: The column space of a matrix is a subspace of the vector space in which the matrix's columns reside. This means it is closed under vector addition and scalar multiplication, and contains the zero vector.\n* It is spanned by the columns of the matrix: The column space of a matrix is the span of its column vectors. In other words, it is the smallest subspace that contains all the column vectors of the matrix.\n* It is the range of the corresponding linear transformation: The column space of a matrix is the range of the linear transformation associated with that matrix. This means it contains all possible output vectors that can be obtained by applying the linear transformation to input vectors.\n* It has the same dimension as the rank of the matrix: The dimension of the column space of a matrix is equal to the rank of the matrix. This is known as the column space's dimensionality property.\n* It provides a basis for the row space and null space: The column space of a matrix provides a basis for both the row space and the null space of the matrix. The row space is the orthogonal complement of the null space, and the column space is the orthogonal complement of the left null space.\n* It determines the solvability of linear systems: The column space of a coefficient matrix in a system of linear equations determines whether the system has a unique solution, infinitely many solutions, or no solution at all. If the column space spans the entire vector space, the system has a unique solution. If the column space does not span the entire vector space, the system has either infinitely many solutions (if the rank of the matrix is less than the number of variables) or no solution (if the rank of the matrix is less than the number of equations).\n\n## Row Space\n\n### Definition\n\nThe row space of a matrix $\\mathbf{A}$, denoted as $\\text{Row}(\\mathbf{A})$, is the subspace spanned by the rows of $\\mathbf{A}$.\n\n$$\n\\text{Row}(\\mathbf{A}) = \\text{span}\\{\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_m\\}\n$$\n\nwhere $\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_m$ are the rows of $\\mathbf{A}$.\n\n### Example\n\nThe row space of $\\mathbf{A}$ is the subspace spanned by the rows of $\\mathbf{A}$, which can be expressed as:\n\n$$\n\\text{Row}(\\mathbf{A}) = \\text{span}\\left\\{\n\\begin{bmatrix}\n1 \\\\ 2 \\\\ 3 \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n4 \\\\ 5 \\\\ 6 \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n7 \\\\ 8 \\\\ 9 \\\\\n\\end{bmatrix}\n\\right\\}\n$$\n\n### Properties\n\n* It is a subspace: The row space of a matrix is a subspace of the vector space in which the matrix's rows reside. This means it is closed under vector addition and scalar multiplication, and contains the zero vector.\n* It is spanned by the rows of the matrix: The row space of a matrix is the span of its row vectors. In other words, it is the smallest subspace that contains all the row vectors of the matrix.\n* It is the range of the corresponding linear transformation: The row space of a matrix is the range of the linear transformation associated with the transpose of that matrix. This means it contains all possible output vectors that can be obtained by applying the transpose of the linear transformation to input vectors.\n* It has the same dimension as the rank of the matrix: The dimension of the row space of a matrix is equal to the rank of the matrix. This is known as the row space's dimensionality property.\n* It provides a basis for the null space and left null space: The row space of a matrix provides a basis for both the null space (kernel) and the left null space (cokernel) of the matrix. The null space is the orthogonal complement of the row space, and the left null space is the orthogonal complement of the column space.\n* It determines the row-rank and column-rank equality: The row space of a matrix determines the row-rank and column-rank equality property, which states that the number of linearly independent rows (the row-rank) is equal to the number of linearly independent columns (the column-rank) of the matrix.\n* It determines the solvability of linear systems: The row space of a coefficient matrix in a system of linear equations determines whether the system has a unique solution, infinitely many solutions, or no solution at all. If the row space spans the entire vector space, the system has a unique solution. If the row space does not span the entire vector space, the system has either infinitely many solutions (if the rank of the matrix is less than the number of variables) or no solution (if the rank of the matrix is less than the number of equations).\n\n\n## Null Space\n\n$$\n\\text{Null}(\\mathbf{A}) = \\left\\{ \\mathbf{x} \\in \\mathbb{R}^n \\, \\middle| \\, \\mathbf{A}\\mathbf{x} = \\mathbf{0} \\right\\}\n$$\n\nwhere:\n\n* $\\text{Null}$ represents the null space of a matrix,\n* $\\mathbf{A}$ represents the given matrix,\n* $\\mathbf{x}$ represents a vector in the null space of \\mathbf{A},\n* $\\mathbb{R}^n$ represents the n-dimensional real vector space, and\n* $\\mathbf{0}$ represents the zero vector.\n\n### Example\n\n$$\n\\mathbf A=\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \n\\end{bmatrix}\n$$\n\nThe null space of $\\mathbf{A}$ is the set of all vectors $\\mathbf{x} \\in \\mathbb{R}^3$ that satisfy $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$.\n\n### Properties\n\nThe null space of a matrix, also known as the kernel space, is the set of all vectors that are mapped to the zero vector by the linear transformation associated with the matrix.\n\n* Contains the zero vector: The null space always contains the zero vector, as any matrix multiplied by the zero vector results in the zero vector.\n* Subspace property: The null space is a subspace of the vector space from which the vectors are drawn. This means that it is closed under vector addition and scalar multiplication. In other words, if two vectors are in the null space, their sum and any scalar multiple of them will also be in the null space.\n* Dimensionality: The dimension of the null space is equal to the number of linearly independent solutions to the homogeneous linear system associated with the matrix. This is known as the nullity of the matrix.\n* Basis: The null space has a basis consisting of linearly independent vectors that span the entire null space. This basis is used to represent all vectors in the null space as linear combinations of the basis vectors.\n* Relationship to solvability: The null space is directly related to the solvability of a system of linear equations. If the null space contains only the zero vector, the system has a unique solution. If the null space contains non-zero vectors, the system has infinitely many solutions, and the null space vectors represent the general solutions.\n* Orthogonal complement of row space: The null space is orthogonal to the row space of the matrix. This means that any vector in the null space is orthogonal to every vector in the row space, and vice versa.\n* Relationship to matrix rank: The dimension of the null space, also known as the nullity, is related to the rank of the matrix through the rank-nullity theorem. The rank of the matrix plus the nullity of the matrix is equal to the number of columns in the matrix.\n* Computation: The null space can be computed by finding the solutions to the homogeneous linear system $\\mathbf{Ax} = \\mathbf{0}$, where $\\mathbf{A}$ is the coefficient matrix of the system of linear equations.\n\n\n## Column Space vs Row Space vs Null Space\n\nWhile both column space and row space are subspaces associated with a matrix, they serve different roles in linear algebra.\n\n### Column Space vs Row Space\n\n* Dimensionality: The dimensionality of the column space and row space can differ. The dimension of the column space is determined by the number of linearly independent columns, which is also known as the rank of the matrix. On the other hand, the dimension of the row space is determined by the number of linearly independent rows of the matrix, which may not necessarily be the same as the rank of the matrix.\n* Basis and Span: The column space is typically used to find a basis for the range (output space) of a linear transformation associated with the matrix, while the row space is used to find a basis for the null space (kernel) and left null space (cokernel) of the matrix. The column space spans the range of the linear transformation, while the row space spans the orthogonal complement of the null space and left null space.\n* Solvability of linear systems: The row space of the coefficient matrix in a system of linear equations determines the solvability of the system, while the column space is not directly related to the solvability. Specifically, if the row space spans the entire vector space, the system has a unique solution. If the row space does not span the entire vector space, the system may have infinitely many solutions or no solution. The column space, on the other hand, does not directly determine the solvability of the system.\n* Transpose relationship: The row space of a matrix is related to the range of the linear transformation associated with the transpose of the matrix, while the column space is directly related to the range of the original matrix. This means that the row space and column space are related through the transpose operation, but they are not identical.\n\n### Null Space\n\nNull space: The null space of a matrix is the set of all vectors that are mapped to the zero vector by the linear transformation associated with the matrix. It represents the subspace of the vector space that consists of solutions to the homogeneous linear system  $\\mathbf{Ax} = \\mathbf{0}$, where $\\mathbf{A}$ is the coefficient matrix of the system of linear equations.\n\n### The relationships between these spaces\n\n* The column space and row space are related, as they are orthogonal complements of each other. This means that any vector in the row space is orthogonal to any vector in the null space, and vice versa. This relationship is known as the row-space-null-space decomposition.\n* The dimension of the column space is equal to the rank of the matrix, which is the maximum number of linearly independent columns or rows. Similarly, the dimension of the row space is also equal to the rank of the matrix.\n* The null space is orthogonal to both the column space and the row space. This means that any vector in the null space is orthogonal to any vector in the column space and the row space.\n* The null space is useful in determining the solvability of a system of linear equations. If the null space contains only the zero vector, the system has a unique solution. If the null space contains non-zero vectors, the system has infinitely many solutions."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"09.vector_space.html"},"language":{"toc-title-document":"","toc-title-website":"","related-formats-title":" ","related-notebooks-title":"Notebooks","source-notebooks-prefix":"","other-links-title":" ","code-links-title":" ","launch-dev-container-title":"Dev  ","launch-binder-title":" Binder","article-notebook-label":" ","notebook-preview-download":" ","notebook-preview-download-src":" ","notebook-preview-back":" ","manuscript-meca-bundle":"MECA ","section-title-abstract":"","section-title-appendices":"","section-title-footnotes":"","section-title-references":"","section-title-reuse":"","section-title-copyright":"","section-title-citation":"","appendix-attribution-cite-as":"","appendix-attribution-bibtex":"BibTeX :","title-block-author-single":"","title-block-author-plural":"","title-block-affiliation-single":"","title-block-affiliation-plural":"","title-block-published":"","title-block-modified":"Modified","title-block-keywords":"","callout-tip-title":"","callout-note-title":"","callout-warning-title":"","callout-important-title":"","callout-caution-title":"","code-summary":"","code-tools-menu-caption":"","code-tools-show-all-code":"  ","code-tools-hide-all-code":"  ","code-tools-view-source":"  ","code-tools-source-code":" ","tools-share":"Share","tools-download":"Download","code-line":"","code-lines":"","copy-button-tooltip":" ","copy-button-tooltip-success":"!","repo-action-links-edit":"","repo-action-links-source":" ","repo-action-links-issue":" ","back-to-top":" ","search-no-results-text":" ","search-matching-documents-text":" ","search-copy-link-title":"  ","search-hide-matches-text":"   ","search-more-match-text":" ","search-more-matches-text":" ","search-clear-button-title":"","search-text-placeholder":"","search-detached-cancel-button-title":"","search-submit-button-title":"","search-label":"","toggle-section":" ","toggle-sidebar":" ","toggle-dark-mode":"  ","toggle-reader-mode":"  ","toggle-navigation":" ","crossref-fig-title":"","crossref-tbl-title":"","crossref-lst-title":"","crossref-thm-title":"","crossref-lem-title":"","crossref-cor-title":"","crossref-prp-title":"","crossref-cnj-title":"","crossref-def-title":"","crossref-exm-title":"","crossref-exr-title":"","crossref-ch-prefix":"","crossref-apx-prefix":"","crossref-sec-prefix":"","crossref-eq-prefix":"","crossref-lof-title":" ","crossref-lot-title":" ","crossref-lol-title":" ","environment-proof-title":"","environment-remark-title":"","environment-solution-title":"","listing-page-order-by":"","listing-page-order-by-default":"","listing-page-order-by-date-asc":"()","listing-page-order-by-date-desc":"()","listing-page-order-by-number-desc":" ()","listing-page-order-by-number-asc":" ()","listing-page-field-date":"","listing-page-field-title":"","listing-page-field-description":"","listing-page-field-author":"","listing-page-field-filename":"","listing-page-field-filemodified":"","listing-page-field-subtitle":"","listing-page-field-readingtime":" ","listing-page-field-wordcount":" ","listing-page-field-categories":"","listing-page-minutes-compact":"{0} ","listing-page-category-all":"","listing-page-no-matches":" ","listing-page-words":"{0} "},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY MM DD","title":"Vector Space","subtitle":"vector space, basis vector, susbspace, dimension, rank, column space, row space, null space","description":"template\n","categories":["Mathematics"],"author":"Kwangmin Kim","date":"04/10/2023","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}