{"title":"Matrix Transformation (4) - Biinear Form","markdown":{"yaml":{"title":"Matrix Transformation (4) - Biinear Form","subtitle":"Linear Regression, Fully Connected layers, Neural Networks, Linear Classifiers","description":"template\n","categories":["Mathematics"],"author":"Kwangmin Kim","date":"04/02/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":false},"headingText":"Binear Form","containsRefs":false,"markdown":"\n\n\nA bilinear form of a matrix is a function that extends the linear form and takes two vectors as inputs and produces a scalar as output. It is linear in both of its arguments, meaning that it satisfies the following properties:\n\n$$\n\\begin{aligned}\nB(\\mathbf u+\\mathbf v)&=B(\\mathbf u+\\mathbf w)+B(\\mathbf v+\\mathbf w)\\\\\nB(\\mathbf u,\\alpha \\mathbf v)&=\\alpha B(\\mathbf u,\\mathbf v)\\\\\nB(\\alpha\\mathbf u,\\mathbf v)&=\\alpha B(\\mathbf u,\\mathbf v)\n\\end{aligned}\n$$\n\nfor all vectors $u$, $v$, $w$ and scalars $\\alpha$.\n\nA bilinear form can be represented by a matrix $B$ such that $B_{i,j}$ is the coefficient of the product $u_i v_j$ in the expansion of $B(u,v)$. The bilinear form can then be written as:\n\n$$\nB(\\mathbf u,\\mathbf v)=\\mathbf u^T B \\mathbf v\n$$\n\nwhere $\\mathbf u$ and $\\mathbf v$ are column vectors and $B$ is a matrix.\n\nFor example, consider the bilinear form $B(\\mathbf u,\\mathbf v) = u_1 v_1 + u_2 v_2$. This bilinear form can be represented by the matrix:\n\n$$\nB=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\n$$\n\nand written as:\n\n$$\nB(\\mathbf u,\\mathbf v)=\\begin{bmatrix}u_1& u_2\\end{bmatrix}\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\end{bmatrix}=u_1v_1+u_2v_2\n$$\n\nThis bilinear form computes the dot product of $u$ and $v$, which measures the similarity between the two vectors. Bilinear forms are commonly used in applications such as optimization, geometry, and physics, where they capture the interaction between two quantities or variables.\n\nThe covariance matrix can be represented as a bilinear form using matrix multiplication. Let's say we have a random vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^T$ with mean vector $\\mathbf{\\mu} = [\\mu_1, \\mu_2, \\ldots, \\mu_n]^T$ and covariance matrix $\\mathbf{\\Sigma}$. Then, we can represent the covariance matrix as a bilinear form in the following way:\n\n$$\n\\begin{aligned}\n\\Sigma&=\\operatorname{E}[(\\mathbf x-\\mathbf \\mu)(\\mathbf x-\\mathbf \\mu)^T]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})(x_i-\\bar{x})^T\n\\end{aligned}\n$$\n\nwhere $\\mathbf{E}$ denotes the expectation operator. We can expand this expression as:\n\n$$\n\\begin{aligned}\n\\Sigma&=\\operatorname{E}[\\mathbf x\\mathbf x^T]-\\mathbf \\mu\\mathbf \\mu^T\n\\end{aligned}\n$$\n\nNow, we can represent the covariance matrix as a bilinear form using matrix multiplication as:\n\n$$\n\\begin{aligned}\n\\Sigma&=\\operatorname{E}[(\\mathbf x-\\mathbf \\mu)(\\mathbf x-\\mathbf \\mu)^T]\\\\\n&=\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf x_i-\\mathbf\\mu_i)(\\mathbf x_i-\\mathbf \\mu_j)\n\\end{aligned}\n$$\n\nwhere $[\\mathbf{x}-\\mathbf{\\mu}]$ is the deviation of the random vector $\\mathbf{x}$ from its mean vector $\\mathbf{\\mu}$.\n\ncovariance matrix, and correlation matrix\n\nOne of the most famous examples is the use of bilinear forms in convolutional neural networks (CNNs), which are a type of deep learning model used for image and video recognition tasks.\n\nIn a CNN, a bilinear form is used to compute the similarity between a filter and a local region of an input image. This similarity measure is used to determine how much the filter \"matches\" the local region of the image, and is used to produce an output feature map.\n\nMore specifically, the bilinear form used in a CNN takes the form:\n\n$$\nz_{i,j} = \\sum_{m=1}^{M}\\sum_{n=1}^{N} w_{m,n}x_{i+m-1,j+n-1}\n$$\n\nwhere $z_{i,j}$ is the output feature map at location $(i,j)$, $x_{i+m-1,j+n-1}$ is the input image pixel at location $(i+m-1,j+n-1)$, and $w_{m,n}$ is the weight of the filter at position $(m,n)$. This computation is performed for each location $(i,j)$ in the output feature map.\n\nThe bilinear form used in CNNs is a type of convolution operation, and is used to learn features such as edges, corners, and other patterns in the input image. CNNs with bilinear forms have achieved state-of-the-art performance on many image recognition tasks, including object detection, face recognition, and scene classification.\n\nBilinear forms also have applications in other areas of machine learning, such as natural language processing (NLP). In NLP, bilinear forms can be used to compute the similarity between two word embeddings, which are vector representations of words. This similarity measure can be used for tasks such as sentiment analysis, text classification, and machine translation.\n\n$$\nB(\\mathbf{u},\\mathbf{v})=\\mathbf{u}^T \\mathbf{W}\\mathbf{v}\n$$\n\nwhere $\\mathbf{u}$ and $\\mathbf{v}$ are word embeddings, $\\mathbf{W}$ is a weight matrix, and $B(\\mathbf{u},\\mathbf{v})$ represents the bilinear form used to compute the similarity between the two embeddings.\n\nOverall, bilinear forms are a powerful tool for learning features from complex data such as images and text, and have many applications in deep learning and machine learning.","srcMarkdownNoYaml":"\n\n## Binear Form\n\nA bilinear form of a matrix is a function that extends the linear form and takes two vectors as inputs and produces a scalar as output. It is linear in both of its arguments, meaning that it satisfies the following properties:\n\n$$\n\\begin{aligned}\nB(\\mathbf u+\\mathbf v)&=B(\\mathbf u+\\mathbf w)+B(\\mathbf v+\\mathbf w)\\\\\nB(\\mathbf u,\\alpha \\mathbf v)&=\\alpha B(\\mathbf u,\\mathbf v)\\\\\nB(\\alpha\\mathbf u,\\mathbf v)&=\\alpha B(\\mathbf u,\\mathbf v)\n\\end{aligned}\n$$\n\nfor all vectors $u$, $v$, $w$ and scalars $\\alpha$.\n\nA bilinear form can be represented by a matrix $B$ such that $B_{i,j}$ is the coefficient of the product $u_i v_j$ in the expansion of $B(u,v)$. The bilinear form can then be written as:\n\n$$\nB(\\mathbf u,\\mathbf v)=\\mathbf u^T B \\mathbf v\n$$\n\nwhere $\\mathbf u$ and $\\mathbf v$ are column vectors and $B$ is a matrix.\n\nFor example, consider the bilinear form $B(\\mathbf u,\\mathbf v) = u_1 v_1 + u_2 v_2$. This bilinear form can be represented by the matrix:\n\n$$\nB=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\n$$\n\nand written as:\n\n$$\nB(\\mathbf u,\\mathbf v)=\\begin{bmatrix}u_1& u_2\\end{bmatrix}\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\end{bmatrix}=u_1v_1+u_2v_2\n$$\n\nThis bilinear form computes the dot product of $u$ and $v$, which measures the similarity between the two vectors. Bilinear forms are commonly used in applications such as optimization, geometry, and physics, where they capture the interaction between two quantities or variables.\n\nThe covariance matrix can be represented as a bilinear form using matrix multiplication. Let's say we have a random vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^T$ with mean vector $\\mathbf{\\mu} = [\\mu_1, \\mu_2, \\ldots, \\mu_n]^T$ and covariance matrix $\\mathbf{\\Sigma}$. Then, we can represent the covariance matrix as a bilinear form in the following way:\n\n$$\n\\begin{aligned}\n\\Sigma&=\\operatorname{E}[(\\mathbf x-\\mathbf \\mu)(\\mathbf x-\\mathbf \\mu)^T]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})(x_i-\\bar{x})^T\n\\end{aligned}\n$$\n\nwhere $\\mathbf{E}$ denotes the expectation operator. We can expand this expression as:\n\n$$\n\\begin{aligned}\n\\Sigma&=\\operatorname{E}[\\mathbf x\\mathbf x^T]-\\mathbf \\mu\\mathbf \\mu^T\n\\end{aligned}\n$$\n\nNow, we can represent the covariance matrix as a bilinear form using matrix multiplication as:\n\n$$\n\\begin{aligned}\n\\Sigma&=\\operatorname{E}[(\\mathbf x-\\mathbf \\mu)(\\mathbf x-\\mathbf \\mu)^T]\\\\\n&=\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf x_i-\\mathbf\\mu_i)(\\mathbf x_i-\\mathbf \\mu_j)\n\\end{aligned}\n$$\n\nwhere $[\\mathbf{x}-\\mathbf{\\mu}]$ is the deviation of the random vector $\\mathbf{x}$ from its mean vector $\\mathbf{\\mu}$.\n\ncovariance matrix, and correlation matrix\n\nOne of the most famous examples is the use of bilinear forms in convolutional neural networks (CNNs), which are a type of deep learning model used for image and video recognition tasks.\n\nIn a CNN, a bilinear form is used to compute the similarity between a filter and a local region of an input image. This similarity measure is used to determine how much the filter \"matches\" the local region of the image, and is used to produce an output feature map.\n\nMore specifically, the bilinear form used in a CNN takes the form:\n\n$$\nz_{i,j} = \\sum_{m=1}^{M}\\sum_{n=1}^{N} w_{m,n}x_{i+m-1,j+n-1}\n$$\n\nwhere $z_{i,j}$ is the output feature map at location $(i,j)$, $x_{i+m-1,j+n-1}$ is the input image pixel at location $(i+m-1,j+n-1)$, and $w_{m,n}$ is the weight of the filter at position $(m,n)$. This computation is performed for each location $(i,j)$ in the output feature map.\n\nThe bilinear form used in CNNs is a type of convolution operation, and is used to learn features such as edges, corners, and other patterns in the input image. CNNs with bilinear forms have achieved state-of-the-art performance on many image recognition tasks, including object detection, face recognition, and scene classification.\n\nBilinear forms also have applications in other areas of machine learning, such as natural language processing (NLP). In NLP, bilinear forms can be used to compute the similarity between two word embeddings, which are vector representations of words. This similarity measure can be used for tasks such as sentiment analysis, text classification, and machine translation.\n\n$$\nB(\\mathbf{u},\\mathbf{v})=\\mathbf{u}^T \\mathbf{W}\\mathbf{v}\n$$\n\nwhere $\\mathbf{u}$ and $\\mathbf{v}$ are word embeddings, $\\mathbf{W}$ is a weight matrix, and $B(\\mathbf{u},\\mathbf{v})$ represents the bilinear form used to compute the similarity between the two embeddings.\n\nOverall, bilinear forms are a powerful tool for learning features from complex data such as images and text, and have many applications in deep learning and machine learning."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"11.bilinear_form.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Matrix Transformation (4) - Biinear Form","subtitle":"Linear Regression, Fully Connected layers, Neural Networks, Linear Classifiers","description":"template\n","categories":["Mathematics"],"author":"Kwangmin Kim","date":"04/02/2023","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}