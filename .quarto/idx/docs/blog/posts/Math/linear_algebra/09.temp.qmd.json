{"title":"Basic Matrix (1) - Matrix Operations","markdown":{"yaml":{"title":"Basic Matrix (1) - Matrix Operations","subtitle":"template","description":"template\n","categories":["Mathematics"],"author":"Kwangmin Kim","date":"03/31/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"execute":{"warning":false,"message":false,"eval":false},"draft":true},"headingText":"NOTE: these lines define global figure properties used for publication.","containsRefs":false,"markdown":"\n\n```{r}\n#| eval: false\n\nlibrary(tidyverse)\nset.seed(100)\n```\n\n\n```{python}\n#| echo: false\n#| eval: false\n\nimport numpy as np\nimport matplotlib_inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport sympy as sym # for RREF\nimport scipy.linalg # for LU\nimport matplotlib.gridspec as gridspec # used to create non-regular subplots\n\nfrom IPython import display\nmatplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n#display.set_matplotlib_formats('svg') # display figures in vector format\nplt.rcParams.update({'font.size':14}) # set global font size\n     \n```\n\n## Block Matrices\n\nA block matrix is a matrix that is partitioned into smaller matrices, or blocks, arranged in a rectangular grid. The blocks can be of any size, and the resulting matrix is used to represent a system of linear equations with multiple variables or equations.\n\n### Example\n\nLet $\\mathbf A$  be a block matrix with four blocks, $\\mathbf A_{11}$, $\\mathbf A_{12}$, $\\mathbf A_{21}$, and $\\mathbf A_{22}$, as shown below:\n\n$$\n\\mathbf A = \\begin{bmatrix}\n\\mathbf A_{11} & \\mathbf A_{12} \\\\\n\\mathbf A_{21} & \\mathbf A_{22}\n\\end{bmatrix}\n$$\n\nwhere $\\mathbf A_{11}, \\mathbf A_{12}, \\mathbf A_{21},\\text{ and }\\mathbf A_{22}$ are individual matrices. This block matrix can be used to represent a system of linear equations with four variables or equations, where the blocks $\\mathbf A_{11}, \\mathbf A_{12}, \\mathbf A_{21},\\text{ and }\\mathbf A_{22}$ represent the coefficients of the variables in the linear equations.\n\n### Block Multiplication\n\nBlock multiplication is a matrix operation used with block matrices, where a matrix is partitioned into smaller matrices, or blocks, and the blocks are multiplied according to certain rules.\n\n#### Example\n\n$$\n\\mathbf A = \\begin{bmatrix}\n\\mathbf A_{11} & \\mathbf A_{12} \\\\\n\\mathbf A_{21} & \\mathbf A_{22}\n\\end{bmatrix}\n\\quad\n\\mathbf B = \\begin{bmatrix}\n\\mathbf B_{11} & \\mathbf B_{12} \\\\\n\\mathbf B_{21} & \\mathbf B_{22}\n\\end{bmatrix}\n$$\n\n\nThe block multiplication of $\\mathbf A$ and $\\mathbf B$, denoted as $\\mathbf{AB}$, can be computed as:\n\n$$\n\\mathbf{AB} = \\begin{bmatrix}\n\\mathbf A_{11} & \\mathbf A_{12} \\\\\n\\mathbf A_{21} & \\mathbf A_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf B_{11} & \\mathbf B_{12} \\\\\n\\mathbf B_{21} & \\mathbf B_{22}\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\mathbf A_{11}\\mathbf B_{11} + \\mathbf A_{12}\\mathbf B_{21} & \\mathbf A_{11}\\mathbf B_{12} + \\mathbf A_{12}\\mathbf B_{22} \\\\\n\\mathbf A_{21}\\mathbf B_{11} + \\mathbf A_{22}\\mathbf B_{21} & \\mathbf A_{21}\\mathbf B_{12} + \\mathbf A_{22}\\mathbf B_{22}\n\\end{bmatrix}\n$$\n\nwhere $\\mathbf{A}_{11}\\mathbf{B}_{11}, \\mathbf{A}_{12}\\mathbf{B}_{21}, \\mathbf{A}_{11}\\mathbf{B}_{12}, \\mathbf{A}_{12}\\mathbf{B}_{22}, \\mathbf{A}_{21}\\mathbf{B}_{11}, \\mathbf{A}_{22}\\mathbf{B}_{21}, \\mathbf{A}_{21}\\mathbf{B}_{12},\\text{ and }\\mathbf{A}_{22}\\mathbf{B}_{22}$ are block multiplications of the corresponding blocks.\n\nWhen matrices split into blocks, it is often simpler to see how they act. \n\nThis block unit can be reduced to the vector:\n\n$$\n\\mathbf{AB} = \\begin{bmatrix}\n\\mathbf a_{1} & \\dots & \\mathbf a_{n} \n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf b_{1} \\\\\n\\vdots\\\\\n\\mathbf b_{n} \n\\end{bmatrix}\n= \\begin{bmatrix}\n\\mathbf a_{1}\\mathbf b_{1} + \\dots + \\mathbf a_{n}\\mathbf b_{n}\n\\end{bmatrix}\n$$\n\n### Block Elimination\n\nBlock elimination is a technique used to solve systems of linear equations by reducing the system to a smaller set of equations. The process involves breaking the system down into smaller sub-systems or blocks, then eliminating one set of variables by expressing them in terms of the remaining variables. \n\n#### Schur Complement\n\nThe Schur complement is a matrix obtained by block elimination, where a large matrix $\\mathbf{A}$ is partitioned into blocks:\n\n$$\n\\begin{align*}\n\\mathbf{A} =\n\\begin{bmatrix}\n\\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\\n\\mathbf{A}_{21} & \\mathbf{A}_{22}\n\\end{bmatrix}\n\\end{align*}\n$$\n\nwhere $\\mathbf{A}_{11}$ is a square sub-matrix of $\\mathbf{A}$. The Schur complement of $\\mathbf{A}_{22}$ with respect to $\\mathbf{A}_{11}$ is defined as:\n\n$$\n\\begin{align*}\n\\mathbf{S} = \\mathbf{A}_{22} - \\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12}\n\\end{align*}\n$$\n\nThe Schur complement is useful in many areas of mathematics and engineering, including control theory, optimization, and signal processing.\n\nAs an example, consider the following system of linear equations:\n\n$$\n\\begin{align*}\n\\begin{bmatrix}\n\\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\\n\\mathbf{A}_{21} & \\mathbf{A}_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{x}_1 \\\\\n\\mathbf{x}_2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{b}_1 \\\\\n\\mathbf{b}_2\n\\end{bmatrix}\n\\end{align*}\n$$\n\nWe can eliminate the variables $\\mathbf{x}_2$ by solving for them in terms of $\\mathbf{x}_1$:\n\n$$\n\\begin{align*}\n\\mathbf{A}_{22} \\mathbf{x}_2 = \\mathbf{b}_2 - \\mathbf{A}_{21} \\mathbf{x}_1 \\\\\n\\mathbf{x}_2 = \\mathbf{A}_{22}^{-1} (\\mathbf{b}_2 - \\mathbf{A}_{21} \\mathbf{x}_1)\n\\end{align*}\n$$\n\nSubstituting this into the first equation, we obtain:\n\n$$\n\\begin{align*}\n\\mathbf{A}_{11} \\mathbf{x}_1 + \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} (\\mathbf{b}_2 - \\mathbf{A}_{21} \\mathbf{x}_1) = \\mathbf{b}_1\n\\end{align*}\n$$\n\nRearranging terms, we obtain an equation in the form $\\mathbf{B} \\mathbf{x}_1 = \\mathbf{c}$, where:\n\n$$\n\\begin{align*}\n\\mathbf{B} &= \\mathbf{A}_{11} - \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\mathbf{A}_{21} \\\\\n\\mathbf{c} &= \\mathbf{b}_1 - \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\mathbf{b}_2\n\\end{align*}\n$$\n\nThus, we have obtained a smaller system.\n\n## Inverse Matrices\n\n::: {#def-inverse}\n\nThe inverse of a square matrix $A$ of size $n$ is a matrix $A^{-1}$ such that the product of $A$ and $A^{-1}$ is the identity matrix $I_n$, i.e. $A \\times A^{-1} = I_n$. If such a matrix exists, then $A$ is said to be **invertible or non-singular**.\n\nThe inverse of a square matrix $\\mathbf{A}$ is denoted by $\\mathbf{A}^{-1}$ and is defined as the unique matrix that satisfies the following equation:\n$$\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n$$\n\nwhere $\\mathbf{I}$ is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\n:::\n\n### Properties about $\\mathbf A^{-1}$\n\n1. Existence: The inverse of the matrix $\\mathbf A$ exists if and only if elimination produces $n$ pivots, where $n$ is the number of rows (or columns) of $\\mathbf A$. Elimination solves $\\mathbf{Ax}=\\mathbf{b}$ without explicitly using the matrix \n  * Pivots are the non-zero elements that are selected during the elimination process and used as the basis for row operations. If $n$ pivots are obtained, then the matrix $\\mathbf A$ is said to be full rank, and its inverse exists. If fewer than $n$ pivots are obtained, then the matrix $\\mathbf A$ is singular, and its inverse does not exist\n1. Unique Inverse: the matrix $\\mathbf A$ cannot have two different inverses\n1. If $\\mathbf A$ is invertible, the one and only solution to  $\\mathbf{Ax}=\\mathbf{b}$ is $\\mathbf{x}=\\mathbf{A^{-1}b}$\n1. (Important) Suppose there is a nonzero vector $\\mathbf A$ such that $\\mathbf{Ax}=\\mathbf{0}$. Then $\\mathbf A$ cannot have an inverse. No matrix can bring  $\\mathbf 0$ back to $\\mathbf x$.\n  * If $\\mathbf A$ is invertible, then $\\mathbf{Ax}=\\mathbf{0}$ can only have the zero solution $\\mathbf{x}=\\mathbf{A^{-1}0=0}$.\n4. A 2 by 2 matrix is invertible if and only if $ad - bc$ is not zero:\n$$\nA = \\begin{bmatrix}\na & b \\\\\nc & d \\\\\n\\end{bmatrix}\n\\quad\nA^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix}\nd & -b \\\\\n-c & a \\\\\n\\end{bmatrix}\n$$\n5. A diagonal matrix has an inverse provided no diagonal entries are zero\n6. Inverse of Inverse: $(\\mathbf A^{-1})^{-1} = \\mathbf A$ \n7. Inverse of Product: If $\\mathbf{AB = I}$, where $\\mathbf I$ is the identity matrix, then $\\mathbf{B = A^{-1}}$.\n8. Scalar Multiple: If $c$ is a scalar, then $(c\\mathbf A)^{-1} = \\frac{1}{c}\\mathbf A^{-1}$ (if $c \\neq 0$).\n9. Product of Inverses: If $\\mathbf A^{-1}$ and $\\mathbf B^{-1}$ both exist, then $(\\mathbf{AB})^{-1} = \\mathbf B^{-1}\\mathbf A^{-1}$ (if $\\mathbf{AB}$ is invertible).\n10. Reverse Order: $(\\mathbf{ABC})^{-1}$=$\\mathbf C^{-1}$ $\\mathbf B^{-1}$ $\\mathbf A^{-1}$\n\n### Inverse by Gauss Jordan Elimination \n\nGiven a square matrix $\\mathbf A$, to find its inverse $\\mathbf A^{-1}$:\n\nStep 1: Augment the matrix $\\mathbf A$ with an identity matrix $\\mathbf I$ of the same size:\n$[\\mathbf A | \\mathbf I]$\n\nStep 2: Perform elementary row operations to transform the left half $\\mathbf A$ into the identity matrix $\\mathbf I$:\n- Interchange rows\n- Multiply a row by a scalar\n- Add a multiple of one row to another row\n\nStep 3: Apply the same row operations to the right half $\\mathbf I$ to obtain $\\mathbf A^{-1}$.\n\nStep 4: If $\\mathbf A$ is not invertible, the augmented matrix $[\\mathbf A | \\mathbf I]$ will not result in an identity matrix on the left half. \n\n#### Example \n\n$$\n\\mathbf A= \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \\\\\n\\end{bmatrix}\n$$\n\nStep 1: Augment the matrix $\\mathbf A$ with an identity matrix $\\mathbf I$ of the same size:\n$$\n[\\mathbf A | \\mathbf I] = \\begin{bmatrix}\n1 & 2 & 3 & | & 1 & 0 & 0 \\\\\n4 & 5 & 6 & | & 0 & 1 & 0 \\\\\n7 & 8 & 9 & | & 0 & 0 & 1 \n\\end{bmatrix}\n$$\n\nStep 2: Perform elementary row operations to transform the left half $\\mathbf A$ into the identity matrix $\\mathbf I$:\n- Interchange rows\n- Multiply a row by a scalar\n- Add a multiple of one row to another row\n\nStep 3: Apply the same row operations to the right half $\\mathbf I$ to obtain $\\mathbf A^{-1}$.\n\nStep 4: If $\\mathbf A$ is not invertible, the augmented matrix $[\\mathbf A | \\mathbf I]$ will not result in an identity matrix on the left half. In this case, $\\mathbf A$ does not have an inverse because $\\text{det}(\\mathbf A) = 1(5 \\cdot 9 - 6 \\cdot 8) - 2(4 \\cdot 9 - 6 \\cdot 7) + 3(4 \\cdot 8 - 5 \\cdot 7) = 0$\n\n::: {#def-elementary_matrix}\nAn elementary matrix is a square matrix obtained by performing a single elementary row operation on the identity matrix $\\mathbf{I}$.\n\nThere are three types of elementary row operations:\n\n* Swapping two rows: The elementary matrix obtained by swapping two rows of the identity matrix is denoted by $\\mathbf{E}_i$, where $i$ indicates the row numbers to be swapped. $\\mathbf{E}_1 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$ \n* Scaling a row by a nonzero scalar: The elementary matrix obtained by scaling a row of the identity matrix by a nonzero scalar $c$ is denoted by $\\mathbf{E}_i(c)$, where $i$ indicates the row number to be scaled and $c$ is the scalar. $\\mathbf{E}_2(2) = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2 \\end{bmatrix}$\n\n* Adding a multiple of one row to another row: The elementary matrix obtained by adding a multiple of one row of the identity matrix to another row is denoted by $\\mathbf{E}_{ij}(c)$, where $i$ indicates the row number from which a multiple is added, $j$ indicates the row number to which the multiple is added, and $c$ is the scalar multiple. $\\mathbf{E}_{12}(3) = \\begin{bmatrix} 1 & 3 \\\\ 0 & 1 \\end{bmatrix}$\n:::\n\n\n\n::: {#thm-elementary_matrix}\n**Elementary Matrix Theorem** \n\nLet $\\mathbf{A}$ be an invertible $n \\times n$ matrix. Then $\\mathbf{A}$ can be represented as the product of elementary matrices $\\mathbf{E}_1, \\mathbf{E}_2, \\ldots, \\mathbf{E}_k$, where each $\\mathbf{E}_i$ is an elementary matrix corresponding to a single elementary row operation.\n:::\n\nAny invertible matrix can be obtained by performing a sequence of elementary row operations on the identity matrix.\n\nLet $\\mathbf{A} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 5 \\end{bmatrix}$ be an invertible matrix. We can represent $\\mathbf{A}$ as the product of elementary matrices $\\mathbf{E}_1$ and $\\mathbf{E}_2$ as follows:\n\n$$\n\\begin{align*}\n\\mathbf{E}_1 = \\begin{bmatrix} 1 & 0 \\\\ -2 & 1 \\end{bmatrix}, \\quad\n\\mathbf{E}_2 = \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\end{align*}\n$$\n\nsuch that $\\mathbf{A} = \\mathbf{E}_2 \\mathbf{E}_1 \\mathbf{I}$.\n\nLet $\\mathbf{B} = \\begin{bmatrix} 3 & 2 & 1 \\\\ 1 & 1 & 1 \\\\ 2 & 3 & 4 \\end{bmatrix}$ be an invertible matrix. We can represent $\\mathbf{B}$ as the product of elementary matrices $\\mathbf{E}_1$, $\\mathbf{E}_2$, and $\\mathbf{E}_3$ as follows:\n\n$$\n\\begin{align*}\n\\mathbf{E}_1 = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad\n\\mathbf{E}_2 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}, \\quad\n\\mathbf{E}_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & \\frac{1}{2} \\end{bmatrix}\n\\end{align*}\n$$\n\nsuch that $\\mathbf{B} = \\mathbf{E}_3 \\mathbf{E}_2 \\mathbf{E}_1 \\mathbf{I}$.\n\n## Factorization\n\nFactorization is one of the computational backbones underlying data-science algorithms, including least squares model fitting and the matrix inverse. \n\n**Prerequisites**\n\n* systems of equations, \n* row reduction or elmentary row operations, and\n* Gaussian elimination\n* echelon matrices and \n* permutation matrices\n\n::: {#def-factorization}\nThe LU factorization, also known as the LU decomposition, is a matrix factorization method that expresses a given matrix $\\mathbf{A}$ as the product of two matrices: a lower triangular matrix $\\mathbf{L}$ and an upper triangular matrix $\\mathbf{U}$:\n$$\n\\mathbf{A} = \\mathbf{LU}\n$$\n\nwhere\n\n* $\\mathbf{A}$ is the given matrix,\n* $\\mathbf{L}$ is the lower triangular matrix with ones on the diagonal, and\n* $\\mathbf{U}$ is the upper triangular matrix.\n:::\n\nIt decomposes a given square matrix into the product of two matrices, a lower triangular matrix ($\\mathbf{L}$) and an upper triangular matrix ($\\mathbf{U}$).\n\nrow reduction can be expressed as $\\mathbf{L}^{-1}\\mathbf{A} = \\mathbf{U}$, where $\\mathbf{L}^{-1}$ contains the set of row manipulations that transforms the dense $\\mathbf{A}$ into upper-triangular (echelon) $\\mathbf{U}$. Because the echelon form is not unique, LU decomposition is not necessarily unique. Thus, there is an infinite pairing of lower- and upper-triangular matrices that could multiply to produce matrix $\\mathbf{A}$. \n\nHowever, adding the constraint that the diagonals of L equal 1 ensures that LU decomposition is unique for a full-rank square matrix $\\mathbf{A}$. \n\n* Efficient Solution of Linear Systems: Once a matrix is factorized into its LU form, it can be used to efficiently solve systems of linear equations. This is because solving a system of equations involving triangular matrices (such as L and U) is computationally more efficient compared to directly solving the original system of equations involving a general matrix.\n* Matrix Inversion: LU decomposition can also be used to efficiently calculate the inverse of a matrix. Once a matrix is factorized into its LU form, the inverse can be obtained by solving two triangular systems of equations, which is computationally more efficient compared to direct methods for matrix inversion.\n* Numerical Stability: LU decomposition can be used as a more numerically stable method for solving linear systems compared to direct methods, such as Gaussian elimination, because it avoids the issues of division by small or zero pivots.\n\n### Properties\n\n* The LU decomposition of a matrix is not unique. There can be multiple factorizations of the same matrix into different combinations of L and U matrices.\n* If the original matrix has a determinant of zero, it is singular and does not have a unique LU decomposition.\n* The LU decomposition can be used for square as well as rectangular matrices, although in the case of rectangular matrices, it may not be unique and may involve additional techniques such as pivoting.\n* The LU decomposition can be calculated using various algorithms, such as Gaussian elimination, Crout's method, and Doolittle's method, among others, with different advantages and disadvantages in terms of computational complexity and numerical stability.\n\n\n### Example \n\n$$\n\\begin{align*} \n\\mathbf{A} &= \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nn} \\\\\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\nl_{21} & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nl_{n1} & l_{n2} & \\cdots & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nu_{11} & u_{12} & \\cdots & u_{1n} \\\\\n0 & u_{22} & \\cdots & u_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & u_{nn} \\\\\n\\end{bmatrix}\n\\end{align*} \n$$\n\n$$\n\\begin{align*}\n\\mathbf{A} &= \\begin{bmatrix}\n2 & 3 & 1 \\\\\n4 & 9 & 5 \\\\\n6 & 15 & 9\n\\end{bmatrix} \\\\\n\\mathbf{A} &= \\mathbf{LU} \\\\\n\\mathbf{L} &= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n3 & 5 & 1\n\\end{bmatrix}, \\quad\n\\mathbf{U} = \\begin{bmatrix}\n2 & 3 & 1 \\\\\n0 & 3 & 3 \\\\\n0 & 0 & 2\n\\end{bmatrix}\n\\end{align*}\n$$\n\n```{python}\n#| eval: false\n\n\nA = np.array([[2, 3, 1],\n              [0, 4, -2],\n              [0, 0, 3]])\n           \n# its LU decomposition via scipy (please ignore the first output for now)\n_,L,U = scipy.linalg.lu(A)\n# print them out\nprint('A: ')\nprint(A), print(' ')\n\nprint('L: ')\nprint(L), print(' ')\n\nprint('U: ')\nprint(U), print(' ')\n\nprint('A - LU: ')\nprint(A - L@U) # should be zeros\n```\n\n## Transposes and Permutations\n\n### Transpose\n\n[Read the previous blog: the basic matrix operations](./02.basic_matrix.qmd)\n\n### Permutations\n\nA permutation is a reordering of a finite sequence of elements. In the context of solving $\\mathbf{Ax=b}$, a permutation can be used to reorder the rows of the augmented matrix $\\begin{bmatrix} \\mathbf{A} & \\mathbf{b} \\end{bmatrix}$ to simplify the process of finding the row echelon form.\n\nSome matrices do not easily transform into an upper-triangular form, which can be transformed into upper-triangular form through a permutation matrix. Consider the following matrix:\n\n$$\n\\begin{align*}\n&\\mathbf{A} = \\begin{bmatrix}\n3 & 2 & 1 \\\\\n0 & 0 & 5 \\\\\n0 & 7 & 2\n\\end{bmatrix} \\rightarrow\n\n\\mathbf{A}' = \\begin{bmatrix}\n3 & 2 & 1 \\\\\n0 & 7 & 2 \\\\\n0 & 0 & 5\n\\end{bmatrix} \\\\ \\\\\n&\\text{through a permutation matrix}\n\\end{align*}\n$$\n\n\n::: {#def-permutation}\n\nA permutation of a set of size $n$ is a bijective function $\\sigma: {1, 2, \\ldots, n} \\to {1, 2, \\ldots, n}$, which means that every element in the set is mapped to a unique element in the set and vice versa. A common way to represent a permutation is by using a permutation matrix, which is a square matrix with a single 1 in each row and each column and 0s elsewhere. The location of the 1 in each row represents the new position of that row after the permutation.\n:::\n\n#### Example\n\nConsider the permutation $\\sigma$ of the set ${1, 2, 3}$ defined by $\\sigma(1) = 2$, $\\sigma(2) = 3$, and $\\sigma(3) = 1$. The corresponding permutation matrix is:\n\n$$\n\\begin{align*}\n\\mathbf{P} = \\begin{bmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n\\end{align*}\n$$\n\nTo apply this permutation to the matrix $\\mathbf{A}$, we multiply $\\mathbf{A}$ on the left by $\\mathbf{P}$, i.e., $P\\mathbf{A}$. Similarly, to apply the permutation to the vector $\\mathbf{b}$, we multiply $\\mathbf{b}$ on the left by $\\mathbf{P}$, i.e., $P\\mathbf{b}$. This gives us the reordered augmented matrix:\n\n$$\n\\begin{align*}\n\\mathbf{P}\\begin{bmatrix} \\mathbf{A} & \\mathbf{b} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{P}\\mathbf{A} & \\mathbf{P}\\mathbf{b} \\end{bmatrix}\n\\end{align*}\n$$\n\nBy permuting the rows of the augmented matrix, we can obtain a row echelon form that is easier to work with and may lead to simpler solutions for $\\mathbf{x}$.\n\nThe full LU decomposition actually takes the following form:\n\n$$\n\\begin{align*}\n\\mathbf{P}\\mathbf{A} &= \\mathbf{L}\\mathbf{U} \\\\\n\\mathbf{A} &= \\mathbf{P}^{T}\\mathbf{L}\\mathbf{U} \\because \\mathbf{P} \\text{ is orthogonal, } \\mathbf{P}^{-1}=\\mathbf{P}^{T}\n\\end{align*}\n$$\n\nAll elements of a permutation matrix are either 0 or 1, and rows are swapped only once, each column has exactly one nonzero element (indeed, all permutation matrices are identity matrices with row swaps). Therefore, the dot\nproduct of any two columns is 0 while the dot product of a column with itself is 1, meaning $\\mathbf{P}^{T}\\mathbf{P}=\\mathbf{I}$\n\n\n#### Properties\n\nPermutations are used in solving linear systems of equations using Gaussian elimination, which is a common method for finding solutions to $\\mathbf{Ax=b}$. Here are some formal properties of permutations:\n\n* The LU decomposition with permutations is used in several applications, including computing the determinant and the matrix inverse.\n* A permutation matrix $\\mathbf{P}$ is a square matrix obtained by permuting the rows of the identity matrix $\\mathbf{I}$.\n* The product of two permutation matrices is also a permutation matrix.\n* The inverse of a permutation matrix is its transpose.\n* Permutation matrices can be used to interchange rows of a matrix.\n\n$$\n\\begin{align*}\nx_1 + 2x_2 + 3x_3 &= 7 \\\\\n4x_1 + 5x_2 + 6x_3 &= 8 \\\\\n7x_1 + 8x_2 + 9x_3 &= 10\n\\end{align*}\n$$\n\nThe augmented matrix for this system is:\n\n$$\n\\begin{align*}\n\\left[\\begin{array}{ccc|c}\n1 & 2 & 3 & 7 \\\\\n4 & 5 & 6 & 8 \\\\\n7 & 8 & 9 & 10\n\\end{array}\\right]\n\\end{align*}\n$$\n\nTo perform Gaussian elimination, we might want to interchange the first and second rows of the matrix. We can do this by multiplying the matrix by the permutation matrix:\n\n$$\n\\begin{align*}\n\\mathbf{P}_1 = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\end{align*}\n$$\n\nWe can verify that $\\mathbf{P}_1$ is a permutation matrix, and we can apply it to the augmented matrix:\n\n$$\n\\begin{align*}\n\\mathbf{P}_1\\left[\\begin{array}{ccc|c}\n1 & 2 & 3 & 7 \\\\\n4 & 5 & 6 & 8 \\\\\n7 & 8 & 9 & 10\n\\end{array}\\right] &= \\left[\\begin{array}{ccc|c}\n4 & 5 & 6 & 8 \\\\\n1 & 2 & 3 & 7 \\\\\n7 & 8 & 9 & 10\n\\end{array}\\right]\n\\end{align*}\n$$\n\nWe can continue with the Gaussian elimination process on the new augmented matrix. If we want to interchange the second and third rows, we can use the permutation matrix:\n\n$$\n\\begin{align*}\n\\mathbf{P}_2 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n\\end{align*}\n$$\n\nWe can verify that $\\mathbf{P}_2$ is a permutation matrix, and we can apply it to the augmented matrix:\n\n$$\n\\begin{align*}\n\\mathbf{P}_2\\left[\\begin{array}{ccc|c}\n4 & 5 & 6 & 8 \\\\\n1 & 2 & 3 & 7 \\\\\n7 & 8 & 9 & 10\n\\end{array}\\right] &= \\left[\\begin{array}{ccc|c}\n4 & 5 & 6 & 8 \\\\\n7 & 8 & 9 & 10 \\\\\n1 & 2 & 3 & 7\n\\end{array}\\right]\n\\end{align*}\n$$\n\nWe can now continue with Gaussian elimination on this new augmented matrix, which is in row echelon form.\n\n#### Example\n\nThe Scipy package actually returns $\\mathbf{A = PLU}$, which we could also write as $\\mathbf{P^{T}A = LU}$.\n\n```{python}\n#| eval: false\n\n\n# matrix sizes\nm = 4\nn = 6\n\nA = np.random.randn(m,n)\n\nP,L,U = scipy.linalg.lu(A)\n\n# show the matrices\nfig,axs = plt.subplots(1,5,figsize=(13,4))\n\naxs[0].imshow(A,vmin=-1,vmax=1)\naxs[0].set_title('A')\n\naxs[1].imshow(np.ones((m,n)),cmap='gray',vmin=-1,vmax=1)\naxs[1].text(n/2,m/2,'=',ha='center',fontsize=30,fontweight='bold')\n# axs[1].axis('off')\n\naxs[2].imshow(P.T,vmin=-1,vmax=1)\naxs[2].set_title(r'P')\n\naxs[3].imshow(L,vmin=-1,vmax=1)\naxs[3].set_title('L')\n\nh = axs[4].imshow(U,vmin=-1,vmax=1)\naxs[4].set_title('U')\n\nfor a in axs:\n  a.axis('off')\n  a.set_xlim([-.5,n-.5])\n  a.set_ylim([m-.5,-.5])\n\n\nfig.colorbar(h,ax=axs[-1],fraction=.05)\nplt.tight_layout()\nplt.savefig('Figure_10_01.png',dpi=300)\nplt.show()\n     \n```\n\n```{python}\n#| eval: false\n\n\n\nA = np.array([[1, 2, 3, 7],\n              [4, 5, 6, 8],\n              [7, 8, 9, 10]])\n           \n# its LU decomposition via scipy (please ignore the first output for now)\nP,L,U = scipy.linalg.lu(A)\n# print them out\nprint('A: ')\nprint(A), print(' ')\n\nprint('P: ')\nprint(P), print(' ')\n\n\nprint('L: ')\nprint(L), print(' ')\n\nprint('U: ')\nprint(U), print(' ')\n\nprint('A - LU: ')\nprint(A - P@L@U) # should be zeros\n```\n\n\n# 정리할 것 end\n\n\n\n\n\n\n\n### Matrix with Combinations of Vectors\n\nA matrix can be written as combinations of vectors. \n\nLet's see and apply the concept of **a matrix with combinations of vectors** to a linear combination of vectors. We can represent a linear combination of vectors as a matrix form with combinations of vectors. \n\nGiven vectors $\\mathbf{a}_1, \\mathbf{a}_2, \\dots, \\mathbf{a}_n \\in \\mathbb{R}^m$ and the vector $\\mathbf x$ whose entries are scalars $x_1, x_2, \\dots, x_n \\in \\mathbb{R}$, a linear combination of the vectors and scalars is written as:\n\n$$\nx_1\\mathbf a_1 +x_2\\mathbf a_2 +\\dots+x_n \\mathbf a_n\n$$\n\nThe combinations of the $\\mathbf{a}$ vectors is represented as a matrix that can be written as combinations of column vectors with the $\\mathbf{a}$ vectors \n\n$$\n\\begin{aligned}\n\\mathbf{A}_{m\\times n} &=\\begin{bmatrix} \\mathbf{a_1}&\\mathbf{a_2}& \\dots &\\mathbf{a_n} \\end{bmatrix} \\quad \\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n\\end{aligned}\n$$\n\nThus, the linear combination is simply written as:\n\n$$\n\\begin{aligned}\n&x_1\\mathbf a_1 +x_2\\mathbf a_2 +\\dots+x_n \\mathbf a_n \\\\\n&=\\begin{bmatrix} \\mathbf{a_1}&\\mathbf{a_2}& \\dots &\\mathbf{a_n} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\\\\n&=\\begin{bmatrix} \\mathbf{a_1}&\\mathbf{a_2}& \\dots &\\mathbf{a_n} \\end{bmatrix} \\mathbf x \\\\\n&=\\mathbf{Ax} \n\\end{aligned}\n$$\n\nwhere $\\mathbf{x} = \\begin{bmatrix} x_1 & x_2 & \\dots & x_n \\end{bmatrix}^T$ is a column vector of scalars.\n\n:::{.callout-tip}\nIt is a different example of a matrix with combinations of the product of vectors from that of matrix combination of vectors.\nA matrix can be represented as the outer product of column vectors and standard basis vectors, and their sum like the following example:\n\n$$\n\\mathbf A =\n\\begin{bmatrix} \n1&4&7\\\\\n2&5&8\\\\\n3&6&9\\\\\n\\end{bmatrix} =\n\\begin{bmatrix} \n1\\\\\n2\\\\\n3\\\\\n\\end{bmatrix}\\mathbf{e}_1^\\text{T}+\n\\begin{bmatrix} \n4\\\\\n5\\\\\n6\\\\\n\\end{bmatrix}\\mathbf{e}_2^\\text{T}+\n\\begin{bmatrix} \n7\\\\\n8\\\\\n9\\\\\n\\end{bmatrix}\\mathbf{e}_3^\\text{T}\n$$\nwhere $\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3$ are the standard basis vectors of $\\mathbb{R}^3$.\n:::\n\n### Matrix Multiplication with a Vector\n\nLet $\\mathbf{A}$ be an $m \\times n$ matrix and $\\mathbf{x}$ be a $n \\times 1$ column vector. The matrix-vector product $\\mathbf{Ax}$ is defined as:\n\n$$\n\\begin{aligned}\n\\mathbf{Ax}&=\\mathbf{b}\\\\\n\\mathbf{Ax}&=\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn} \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\na_{11}x_1 + a_{12}x_2 + \\dots + a_{1n}x_n \\\\\na_{21}x_1 + a_{22}x_2 + \\dots + a_{2n}x_n \\\\\n\\vdots  \\\\\na_{m1}x_1 + a_{m2}x_2 + \\dots + a_{mn}x_n \n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \n\\end{bmatrix}\\\\\n&=\\mathbf{b}\n\\end{aligned}\n$$\n\nIn other words, each entry of the resulting column vector is the dot product of the corresponding row of the matrix $\\mathbf{A}$ and the column vector $\\mathbf{x}$.\n\nThe matrix $\\mathbf{A}$ acts on the vector $\\mathbf{x}$. The result $\\mathbf{Ax}$ is a combination $\\mathbf{b}$ of the columns of $\\mathbf{A}$. The input is $\\mathbf{x}$ and the output is $\\mathbf{b}=\\mathbf{Ax}$\n\nFor example, let\n\n$$\n\\mathbf A =\n\\begin{bmatrix}\n2 & 1 \\\\\n3 & 4 \\\\\n1 & 2\n\\end{bmatrix} \\quad\n\\mathbf x=\n\\begin{bmatrix}\n  x_1\\\\x_2\n\\end{bmatrix} \\quad\n\\mathbf b=\n\\begin{bmatrix}\n  1\\\\-5\\\\-1\n\\end{bmatrix} \n$$\n\nThen we have\n\n$$\n\\begin{aligned}\n\\mathbf{Ax} &= \\mathbf{b} \\\\\n\\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix} \n\\begin{bmatrix}\n  x_1\\\\x_2\n\\end{bmatrix} \n&=\n\\begin{bmatrix}\n  1\\\\-1\n\\end{bmatrix} \\\\\n\\begin{bmatrix}\n2x_1 + x_2 \\\\\nx_1 + 2x_2\n\\end{bmatrix} \n&=\n\\begin{bmatrix}\n  1\\\\-1\n\\end{bmatrix} \n\\end{aligned}\n$$\n\nThe solution to the above system of equation is $x_1=1, x_2=-1$.\n\n### Linear Equations of a Matrix\n\nBefore the introduction of a matrix to the solution to $x_1\\mathbf a_1 +x_2\\mathbf a_2 +\\dots+x_n \\mathbf a_n=\\mathbf b$, we used the concept of a linear combination of $\\mathbf a$ vectors to find $\\mathbf{x}$ for $\\mathbf{b}$.\n\nBut after the introduction of a matrix, the viewpoint can changes:\n\n* **As-Is**: Compute the linear combination $x_1\\mathbf a_1 +x_2\\mathbf a_2 +\\dots+x_n \\mathbf a_n$ to find $\\mathbf b$.\n* **To-Be**: Which combination of $\\mathbf a$ vectors produces a particular vector $\\mathbf b$?\n\nAnwering the two questions means looking for $\\mathbf{x}$ for $\\mathbf{b}$. To do so, we have two ways:\n\n* to solve a system of linear equations and\n* to find an inverse of $\\mathbf{A}$\n\n#### Solving a System of Linear Equations\n\nGiven an $m \\times n$ matrix $\\mathbf A$ and an $n \\times 1$ vector $\\mathbf{x}$, the matrix-vector product $\\mathbf A\\mathbf{x}$ is a linear combination of the columns of $\\mathbf A$ with coefficients given by the entries of $\\mathbf{x}$. The system of linear equations represented by $\\mathbf A\\mathbf{x}=\\mathbf{b}$ has a unique solution if and only if the columns of $\\mathbf A$ are linearly independent.\n\nA system of linear equations can be written in matrix form as follows:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}=\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_m\n\\end{bmatrix}\n\\end{aligned}\n$$\n\nwhere $a_{ij}$ are the coefficients of the system, $x_i$ are the variables, and $b_j$ are the constants.\n\nWe call the above $\\mathbf{A}$ matrix is a **coefficient matrix** from the point of view of a system of lineqr equations and the above $\\mathbf{Ax}=\\mathbf{b}$ a **matrix equation**.\n\nHere's an example of a system of linear equations represented by a matrix:\n$$\n\\begin{align*}\n2x_1 + 3x_2 &= 8 \\\\\n4x_1 + 5x_2 &= 13\n\\end{align*}\n$$\n\nThis can be written as the matrix equation $A\\mathbf{x}=\\mathbf{b}$, where\n\n$$\n\\begin{equation}\n  \\mathbf{A} = \n    \\begin{bmatrix}\n    2 & 3\\\\\n    4 & 5\n    \\end{bmatrix} \\quad\n  \\mathbf{x} = \n    \\begin{bmatrix}\n    x_1\\\\\n    x_2\n    \\end{bmatrix}\\quad\n  \\mathbf{b} = \n    \\begin{bmatrix}\n    8\\\\\n    13\n    \\end{bmatrix}\n\\end{equation}\n$$\n\nThis can be written in matrix form as:\n$$\n\\begin{equation}\n\\begin{bmatrix}\n2 & 3 \\\\\n4 & -1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}=\n\\begin{bmatrix}\n5 \\\\\n2\n\\end{bmatrix}\n\\end{equation}\n$$\n\nConsider the following system of equations:\n\n$$\n\\begin{aligned}\n2x_1 + 3x_2 &= 5 \\\\\n4x_1 - x_2 &= 2\n\\end{aligned}\n$$\n\n#### Finding $\\mathbf{A}^{-1}$\n\nThe solution to this system can be found by computing the inverse of $\\mathbf A$ (if it exists) and multiplying both sides of the equation by it:\n\n$$\n\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b} \n$$\n\nIf $\\mathbf A$ does not have an inverse, then the system of equations may have either no solutions or infinitely many solutions.\n\n### Inverse\n\nThe inverse of a **square** matrix $\\mathbf A$ of size $n$ is a matrix $A^{-1}$ such that the product of $\\mathbf A$ and $\\mathbf A^{-1}$ is the identity matrix $\\mathbf I_n$, i.e. $\\mathbf A \\times \\mathbf A^{-1} = I_n$. If such a matrix exists, then $\\mathbf A$ is said to be invertible or non-singular.\n\nThe inverse of a square matrix $\\mathbf{A}$ is denoted by $\\mathbf{A}^{-1}$ and is defined as the unique matrix that satisfies the following equation:\n$$\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n$$\n\nwhere $\\mathbf{I}$ is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\n\n#### Examples\n\n**Example1** consider the $2\\times 2$ matrix \n$$\n\\mathbf{A} = \n\\begin{bmatrix} \n1 & 2 \\\\ \n3 & 4 \n\\end{bmatrix}\n$$\n\nThe inverse of $\\mathbf{A}$ or $\\mathbf{A}^{-1}$ is given by:\n\n$$\n\\mathbf{A}^{-1} = \n\\frac{1}{-2}\n\\begin{bmatrix} \n4 & -2 \\\\ \n-3 & 1 \n\\end{bmatrix} = \n\\begin{bmatrix} \n-2 & 1 \\\\ \n\\frac{3}{2} & -\\frac{1}{2} \n\\end{bmatrix}\n$$\n\nWe can verify that $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$ by computing:\n\n$$\n\\mathbf{A}\\mathbf{A}^{-1} = \n\\begin{bmatrix} \n1 & 2 \\\\ \n3 & 4 \n\\end{bmatrix}\n\\begin{bmatrix} \n-2 & 1 \\\\ \n\\frac{3}{2} & -\\frac{1}{2} \n\\end{bmatrix} = \n\\begin{bmatrix} \n1 & 0 \\\\ \n0 & 1 \n\\end{bmatrix} = \\mathbf{I}\n$$\n\n$$\n\\mathbf{A}^{-1}\\mathbf{A} = \n\\begin{bmatrix} -2 & 1 \\\\ \n\\frac{3}{2} & -\\frac{1}{2} \n\\end{bmatrix}\n\\begin{bmatrix} \n1 & 2 \\\\ \n3 & 4 \n\\end{bmatrix} = \n\\begin{bmatrix} \n1 & 0 \\\\ \n0 & 1 \n\\end{bmatrix} = \\mathbf{I}\n$$\n\n**Example2** consider the $3\\times 3$ matrix \n```{r}\n#| eval: false\n\n\nA<-matrix(sample(1:100,9,replace = TRUE),ncol=3,byrow = TRUE)\nprint('matrix A= ')\nprint(A)\nprint('inverse of A= ')\ninverse_A<-solve(A)\ninverse_A\nprint('AA^{-1}=A^{-1}A=I')\nA%*%inverse_A\ninverse_A%*%A\n```\n\n#### Properties\n\nThe inverse of a matrix is unique, if it exists.\n\n* uniqueness\n* If $\\mathbf{A}$ and $\\mathbf{B}$ are invertible matrices of the same size, then $(\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}$.\n* If $\\mathbf{A}$ is an invertible matrix, then $(\\mathbf{A}^{-1})^{-1} = \\mathbf{A}$.\n* If $\\mathbf{A}$ is an invertible matrix, then $\\text{det}(\\mathbf{A}) \\neq 0$.\n* If $\\mathbf{A}$ is an invertible matrix, then $\\mathbf{A}^T$ is invertible, and $(\\mathbf{A}^T)^{-1} = (\\mathbf{A}^{-1})^T$.\n* If $\\mathbf{A}$ is an invertible matrix, then for any scalar $c \\neq 0$, the matrix $c\\mathbf{A}$ is invertible, and $(c\\mathbf{A})^{-1} = \\frac{1}{c}\\mathbf{A}^{-1}$.\n* If $\\mathbf{A}$ is an invertible matrix, then for any positive integer $n$, the matrix $\\mathbf{A}^n$ is invertible, and $(\\mathbf{A}^n)^{-1} = (\\mathbf{A}^{-1})^n$.\n* If $\\mathbf{A}$ is an invertible matrix, then for any non-zero vector $\\mathbf{v}$, the matrix $\\mathbf{A}+\\mathbf{v}\\mathbf{v}^T/\\mathbf{v}^T\\mathbf{A}^{-1}\\mathbf{v}$ is invertible, and $(\\mathbf{A}+\\mathbf{v}\\mathbf{v}^T/\\mathbf{v}^T\\mathbf{A}^{-1}\\mathbf{v})^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1}\\mathbf{v}\\mathbf{v}^T\\mathbf{A}^{-1}}{1+\\mathbf{v}^T\\mathbf{A}^{-1}\\mathbf{v}}$.\n\n### Determinant\n\nLet $\\mathbf{A}$ be an $n \\times n$ square matrix. The determinant of $\\mathbf{A}$, denoted by $|\\mathbf{A}|$ or $\\det(\\mathbf{A})$, is a scalar value calculated as the sum of the products of the elements in any row or column of $\\mathbf{A}$ with their corresponding cofactors, that is,\n\n$$\n|\\mathbf{A}|=\\operatorname{det}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n$$\n\nwhere $a_{ij}$ is the element of $\\mathbf{A}$ in the $i$-th row and $j$-th column, and $C_{ij}$ is the cofactor (a signed minor matrix) of $a_{ij}$. The cofactor of $a_{ij}$ is a scalar value given by $(-1)^{i+j}$ times the determinant of the $(n-1) \\times (n-1)$ matrix obtained by deleting the $i$-th row and $j$-th column of $\\mathbf{A}$.\n\n#### Computing Cofactor\n\n**(Laplace Formula)**\nTo compute the cofactor of a matrix entry $a_{ij}$, you need to first remove the $i$-th row and $j$-th column of the matrix to obtain a $(n-1) \\times (n-1)$ submatrix. The cofactor of $a_{ij}$ is then defined as the product of $(-1)^{i+j}$ and the determinant of this submatrix.\n\n$\\mathbf A$ is an $n \\times n$ matrix and $\\mathbf A_{ij}$ denotes the submatrix obtained by deleting the $i$-th row and $j$-th column of $\\mathbf A$, then the cofactor of $a_{ij}$ is given by\n\n$$\nC_{ij}=\\operatorname{adj}(\\mathbf A)_{ij}=(-1)^{i+j}\\operatorname{det}(\\mathbf{A}_{ij})\n$$\n\nwhere $\\operatorname{adj}(\\mathbf A)$ is an adjugate matrix, which is the transpose of the matrix of cofactors of $\\mathbf A$\n\nOnce you have computed the cofactor of each entry, you can use them to compute the determinant of $\\mathbf{A}$ using the following formula:\n\n$$\n|\\mathbf{A}|=\\operatorname{det}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n$$\n\nwhere $i$ can be any fixed row or column of $\\mathbf{A}$. This formula is called **the Laplace expansion** of the determinant along the $i$-th row (or column). \n\n\nIt is defined as the sum of all possible products of $n$ elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. \n\nThe determinant is used to check whether a square matrix is invertible or not, which is a necessary step for computing an inverse of a square matrix. \n\n#### Examples\n\n**Example1** the inverse of $2\\times 2$ matrix.\n$$\n\\begin{align*}\n\\mathbf{A}&=\n\\begin{bmatrix}\na & b\\\\\nc & d\n\\end{bmatrix}\\\\\n\\mathbf{A}^{-1}&=\n\\frac{1}{\\operatorname{det}(\\mathbf A)}\\begin{bmatrix}\nd & -b\\\\\n-c & a\n\\end{bmatrix}\n\\end{align*}\n$$\n\nwhere $\\operatorname{det}(\\mathbf A)=ad-bc$\n\n$\\operatorname{det}(\\mathbf A)=ad-bc\\ne 0$ in order that $\\mathbf A$ is invertible.\n\n```{r}\n#| eval: false\n\n\nmat_A<-A[-3,-3]\nprint('2 by 2 matrix:')\nmat_A\npaste0('det(mat_A)=',det(mat_A))\nprint('the inverse of mat_A:')\ndet(mat_A)^-1*matrix(c(14,-65,-20,63),ncol=2,byrow = TRUE)\nprint('the inverse of solve(mat_A):')\nsolve(mat_A)\n```\n\n**Example2** the determinant of a $3 \\times 3$ matrix $\\mathbf{A}$, \n$$\n\\mathbf{A}=\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n$$\n\nis given by:\n\n$$\n|\\mathbf{A}| = \\operatorname{det}(\\mathbf{A})= a_{11} \n\\begin{vmatrix} \na_{22} & a_{23} \\\\ \na_{32} & a_{33} \n\\end{vmatrix} \n- a_{12} \n\\begin{vmatrix} \na_{21} & a_{23} \\\\ \na_{31} & a_{33} \n\\end{vmatrix} \n+ a_{13} \n\\begin{vmatrix} \na_{21} & a_{22} \\\\ \na_{31} & a_{32} \n\\end{vmatrix}\n$$\n\nThen, $\\mathbf{A}^{-1}$ is :\n\n$$\n\\begin{align*}\n\\mathbf{A}^{-1} &= \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\\\\\n&= \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\nC_{11} & C_{12} & C_{13} \\\\\nC_{21} & C_{22} & C_{23} \\\\\nC_{31} & C_{32} & C_{33} \n\\end{bmatrix}^{T} \\\\\n&= \\frac{1}{\\text{det}(\\mathbf{A})}\\operatorname{adj}(\\mathbf{A})\n\\end{align*}\n$$\n\nwhere $\\operatorname{det}(\\mathbf{A})$ is the determinant of $\\mathbf{A}$ and $\\operatorname{adj}(\\mathbf A)_{ij}=(-1)^{i+j}\\operatorname{det}(\\mathbf{A}_{ij})$.\n\nConsider the $3 \\times 3$ matrix $\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$. We can calculate the determinant of $\\mathbf{A}$ using any row or column. Let's use the first column:\n\n$$\n|\\mathbf{A}| = 1 \n\\begin{vmatrix} \n5 & 6 \\\\ \n8 & 9 \n\\end{vmatrix} \n- 4 \n\\begin{vmatrix} \n2 & 3\\\\ \n8 & 9\n\\end{vmatrix} \n+ 7 \n\\begin{vmatrix} \n2 & 5 \\\\ \n3 & 6 \n\\end{vmatrix} = 0\n$$\n\nTherefore, the determinant of $\\mathbf{A}$ is zero. So, $\\mathbf{A}$ is not invertible or singular, which means its inverse does not exist.\n\nConsider another $3 \\times 3$ matrix $\\mathbf A$:\n\n$$\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \n\\end{bmatrix}\n\\end{equation*}\n$$\nThen, $\\operatorname{det}(A)$ = -57, and the inverse of $\\mathbf{A}$ is:\n\n$$\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \n\\end{bmatrix}\n\\end{equation*}\n$$\n\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer's rule, Inverse formula, etc. \n\n```{r}\n#| eval: false\n\n\ncofactor<-function(mat,i,j){\n  mat_sub<-mat[-i,-j]\n  return((-1)^(i+j)*det(mat_sub))\n}\ncofactor_matrix<-function(mat){\n  n<-nrow(mat)\n  if(n!=ncol(mat)){\n    stop('the matrix is not a square matrix')\n  }\n  cofactors<-matrix(nrow=n,ncol=n)\n  for (i in 1:n){\n    for (j in 1:n){\n      cofactors[i,j]<-cofactor(mat,i,j)\n    }\n  }\n  return(cofactors)\n}\n\ncofactor_matrix2<-function(mat){\n  n<-nrow(mat)\n  if(n!=ncol(mat)){\n    stop('the matrix is not a square matrix')\n  }\n  coordinate_set<-expand.grid(1:n,1:n)\n  cofactors<-mapply(cofactor,\n  i=coordinate_set[,1],\n  j=coordinate_set[,2],\n  MoreArgs=list(mat=A))\n  return(\n  matrix(cofactors,ncol=n)\n  )\n}\n\n\nA=matrix(c(1,2,3,0,1,4,5,6,0), ncol=3, byrow=TRUE)\ncofactor(A,1,1)\ncofactor_matrix(A)\ncofactor_matrix2(A)\n(1/det(A))*t(cofactor_matrix2(A))\n(1/det(A))*t(cofactor_matrix(A))\nsolve(A)\n```\n\n#### Properties\n\n* If we multiply any row or column of a matrix by a scalar $c$, then the determinant of the resulting matrix is $c$ times the determinant of the original matrix.\n* If we interchange any two rows or columns of a matrix, then the determinant of the resulting matrix is the negative of the determinant of the original matrix.\n* If we add a multiple of one row or column to another row or column of a matrix, then the determinant of the resulting matrix is the same as the determinant of the original matrix.\n* If a matrix has a row or column of zeros, then its determinant is zero.\n* If a matrix is upper triangular or lower triangular, then its determinant is equal to the product of its diagonal entries.\n* If a matrix is a diagonal matrix, then its determinant is equal to the product of its diagonal entries.\n* If a matrix $\\mathbf{A}$ is invertible, then its determinant is nonzero, and $\\det(\\mathbf{A}^{-1}) = 1/\\det(\\mathbf{A})$.\n* If a matrix $\\mathbf{A}$ is invertible, then its determinant is nonzero, and $\\det(\\mathbf{A}^{T}) = \\det(\\mathbf{A})$.\n* If $\\mathbf{A}$ and $\\mathbf{B}$ are $n\\times n$ matrices, then $\\det(\\mathbf{AB}) = \\det(\\mathbf{A})\\det(\\mathbf{B})$.\n* $\\det(k\\mathbf{A}) = k^n\\det(\\mathbf{A})$ where $\\mathbf{A}$ is a $n\\times n$ matrix.\n* determinant and linear independence: if $\\det(\\mathbf{A}) \\ne 0$, the row vectors and columns vectors of $\\mathbf{A}$ are linear independent. Otherwise, they are linearly dependent.\n\n\n### Linear Equations\n\n#### Unique Solution\n\nA linear system of equations has a unique solution if and only if the coefficient matrix is non-singular (i.e., its determinant is nonzero).\n\n$$\n\\begin{aligned}\n2x_1 + 3x_2 &= 10 \\\\\n4x_1 + 5x_2 &= 20 \\\\\n\n\\mathbf{A} \\mathbf{x} &= \\mathbf{b} \\\\\n\\begin{bmatrix}\n2 & 3 \\\\\n4 & 5 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n10 \\\\\n20 \\\\\n\\end{bmatrix} \\\\\n\\text{det}(\\mathbf{A}) &= (a_{11}a_{22}) - (a_{12}a_{21}) \\text{ where } a_{ij} \\text{ is the element of } \\mathbf{A}\\\\\n\\text{det}(\\mathbf{A}) &= (2 \\cdot 5) - (3 \\cdot 4) = -2\\\\\n\\mathbf{A}^{-1} &= \\frac{1}{\\text{det}(\\mathbf{A})} \\begin{bmatrix}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{bmatrix} \\\\\n\n\\mathbf{A}^{-1} &=\n\n\\begin{bmatrix}\n-5/2 & 3/2 \\\\\n2 & -1 \\\\\n\\end{bmatrix} \\\\\n\\mathbf{A}^{-1}\\mathbf{A} \\mathbf{x} &= \\mathbf{A}^{-1}\\mathbf{b}\\\\\n\\mathbf{x} &= \\mathbf{A}^{-1}\\mathbf{b}\\\\\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}&=\n\\begin{bmatrix}\n-5/2 & 3/2 \\\\\n2 & -1 \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\n10 \\\\\n20 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n5 \\\\\n0 \\\\\n\\end{bmatrix} \n\\end{aligned}\n$$\n\nThe unique solution is $(x_1,x_2) = (5,0)$.\n\n#### Infinitely Many Solutions\n\nA linear system of equations has infinitely many solutions if and only if the system has at least one solution and the coefficient matrix is singular (i.e., its determinant is zero), and the system has more unknowns variables than linearly independent equations.\n\n$$\n\\begin{align*}\n\n2x_1 + 3x_2 &= 10 \\\\\n4x_1 + 6x_2 &= 20 \\\\\n\n\\mathbf{A} \\mathbf{x} &= \\mathbf{b} \\\\\n\\begin{bmatrix}\n2 & 3 \\\\\n4 & 6 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n&=\\begin{bmatrix}\n10 \\\\\n20 \\\\\n\\end{bmatrix} \\\\\n\\text{det}(\\mathbf{A}) &= (2 \\cdot 6) - (3 \\cdot 4) = 0\n\n\\end{align*}\n$$\n\nSince the determinant of $\\mathbf{A}$ is $0$, the matrix $\\mathbf{A}$ is singular and does not have an inverse.\nThis implies that the system of linear equations has infinitely many solutions, as the determinant of the coefficient matrix is $0$. \n\n#### No Solution\n\nA linear system of equations has no solution if the coefficient matrix is singular (i.e., its determinant is zero) and it has more linearly independent equations than unknowns variables.\n\n$$\n\\begin{align*}\n\n3x_1 + 4x_2 &= 10 \\\\\n6x_1 + 8x_2 &= 20 \\\\\n\n\\mathbf{A} \\mathbf{x} &= \\mathbf{b} \\\\\n\\begin{bmatrix}\n3 & 4 \\\\\n6 & 8 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n10 \\\\\n20 \\\\\n\\end{bmatrix} \\\\\n\\text{det}(\\mathbf{A}) &= (a_{11}a_{22}) - (a_{12}a_{21}) \\text{ where } a_{ij} \\text{ is the element of } \\mathbf{A}\\\\\n\\text{det}(\\mathbf{A}) &= (3 \\cdot 8) - (4 \\cdot 6) = 0\\\\\n\\end{align*}\n$$\n\nthe determinant of $\\mathbf A$ is $0$, which means that $\\mathbf A$ is a singular matrix and does not have an inverse.\n\n```{r}\n#| echo: false\n#| eval: false\n\n\nhttps://chbe241.github.io/Module-0-Introduction/MATH-152/Unique%20Solution,%20No%20Solution,%20or%20Infinite%20Solutions.html\n\n```\n\n### Linear Independence\n:::{#def-linear_independence}\n## Linear Independence\n\n#### Independence\n\nA set of vectors is said to be linearly independent if none of the vectors in the set can be written as a linear combination of the other vectors in the set. In other words, a set of vectors ${\\mathbf{v_1},\\mathbf{v_2},\\dots,\\mathbf{v_n}}$ is linearly independent if the only solution to the linear equation $c_1\\mathbf{v_1}+c_2\\mathbf{v_2}+\\dots+c_n\\mathbf{v_n}=0$ is $c_1=c_2=\\dots=c_n=0$, where $c_i$ is a scalar.\n\n#### Dependence\n\nA set of vectors is said to be linearly dependent if at least one vector in the set can be written as a linear combination of the other vectors in the set. In other words, a set of vectors $\\{v_1, v_2, \\ldots, v_n\\}$ is linearly dependent if there exist scalars $c_1, c_2, \\ldots, c_n$ (not all zero) such that $c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\ldots + c_n\\mathbf{v}_n = 0$\n\n:::{.callout-important}\n* Independent columns: $\\mathbf{Ax} = \\mathbf{0}$ has one solution. $\\mathbf{A}$ is an invertible matrix.\n* Dependent columns: $\\mathbf{Ax} = \\mathbf{0}$ has many solutions. $\\mathbf{A}$ is a singular matrix.\n:::\n\n:::\n\nIf a set of columns are dependent, $\\mathbf{v_i}$ can be represented as a linear combination of the other vectors:\n$$\n\\mathbf{v_i}=\\frac{-c_1}{c_i}\\mathbf{v_1}+\\frac{-c_2}{c_i}\\mathbf{v_2}+\\dots+\\frac{-c_n}{c_i}\\mathbf{v_n}\n$$\n\n#### Properties\n\n* **Non-Trivial Linear Combination**: a set of vectors is linearly independent if the only way to obtain the zero vector as a linear combination of these vectors is by setting all the scalar or coefficients to zero.\n* **No Redundancy**: If a vector can be expressed as a linear combination of other vectors in a set, then it is redundant and does not contribute to the linear independence of the set. Thus, a set of vectors is linearly independent if and only if no vector in the set can be written as a linear combination of the others.\n* **Minimal Set**: A linearly independent set of vectors is a minimal set, meaning that removing any vector from the set would result in a linearly dependent set.\n* **Size**: If a set of vectors contains more vectors than the dimension of the vector space in which they reside, then the set is necessarily linearly dependent. Conversely, if a set of vectors is linearly independent and contains exactly as many vectors as the dimension of the vector space, then it forms a basis for that vector space.\n\n\n#### Example\n\n**Example1** Let $\\mathbf{v}_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}, \\quad \\mathbf{v}_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}$. These two vectors are linearly independent because no linear combination of and $\\mathbf{v}_1,\\quad \\mathbf{v}_2$ can yield the zero vector, except when all the coefficients are zero. In other words, the only solution to the linear equation\n$$\nc_1\\mathbf v_1​ + c_2\\mathbf v_2 = 0\n$$  \n\nis $c_1=c_2=0$.\n\n**Example2** Computation example with dependent columns.\n\n```{r}\n#| eval: false\n\n\nv1 <- c(-4,2,0,6)\nv2 <- c(0,1,-2,-1)\nv3 <- c(-8,1,6,15)\n\ndependent_matrix<-cbind(v1,v2,v3)\n-2*v1+ 3*v2 + v3 # the coeffecients that makes the linear combiniation of the vectors zero are c(-2,3,1)\n\n# then, how many possible to such coefficients are there?\n\ndependent_function <- function(input_matrix, vector1){\n  result <- rowSums(sweep(input_matrix,2,vector1, '*'))\n  sqrt(sum(result^2))\n}\n\n# find a=c(a1,a2,a3) such a != c(0,0,0) \noptimal_a <- optim(rep(0,3),\n dependent_function,\n NULL, \n method='L-BFGS-B',\n input_matrix=dependent_matrix,\n lower=c(1,-5,-5), # c(a1_lower_bound,a2_lower_bound,a3_lower_bound)\n upper=c(5,5,5)) # c(a1_upper_bound,a2_upper_bound,a3_upper_bound)\n\noptimal_a$par\nrowSums(sweep(dependent_matrix,2,optimal_a$par, '*')) \n```\n\n**Example3** rectangular data with dependent columns\n\n```{r}\n#| eval: false\n\n\ntemperature_data <- tibble(celcius=c(5,36,30,-3,26,33),\nvelocity=c(2.35,11.5,8.62,5.37,11.8,12.20))%>%\nmutate(fahrenheit=celcius*9/5+32,after=celcius)\n\ntemperature_data\n\nThe temperature data include 3 columns: '`r names(temperature_data)`. Since the fahrenheit column can be represented as $\\frac{9}{5}\\text{celcius}+32$, the columns are not linearly independent. So, it is actually enought to reproduce the data with two columns: celcius and velocity. From the analysis perspective, the temperature data has a redundant column, `fahrenheit`.\n\n```\n\n\nSince the fahrenheit and celcius are linearly dependent, the scattor plot is represented as a line like the below. \n\n```{r}\n#| eval: false\n\n\nmosaic::plotPoints(fahrenheit~celcius,data=temperature_data)\n```\nIt indicates that to fit velocity on the temperature data, adding the fahrenheit columns does not influence and explain the variations of the velocity variable at all. \n\n```{r}\n#| eval: false\n\n\nresult1 <- lm(velocity~celcius, data= temperature_data)\nresult2 <- lm(velocity~celcius+fahrenheit, data= temperature_data)\n\nsum(result1$residuals^2)\nsum(result2$residuals^2)\n```\n\n**Example4**\n\n```{r}\n#| eval: false\n\n\na<-sample(seq(-10,10,by=0.01),3,replace=T)\n\nrowSums(sweep(dependent_matrix,2,a,'*')) # a[1]*v1+a[2]*v2+a[3]*v3 \n\nindependent_function<-function(matrix,vector1){\n  a<-sample(seq(-5,5,by=0.01),3,replace=T)\n  result <- rowSums(sweep(matrix,2,a,'*'))\n  result <- sum(result^2)%>%sqrt() # to get a zero vector of result, sum(result^2)%>%squrt() must be 0, which means the rowSums(sweep(matrix,2,a,'*')) must become c(0,0,0).\n  list(coef=a,vector_norm=result)\n}\n\nset.seed(2023)\n\nresult <-replicate(5000, independent_function(dependent_matrix), simplify=FALSE)\nlinear_combo_sum<-result %>% tibble() %>% unnest_auto(col=1) %>% unnest_wider(coef,names_sep='_')\nlinear_combo_sum%>%slice_min(abs(vector_norm), n=10) # ceffecient converges to zero as vector_norm goes to zero\n\nrowSums(sweep(dependent_matrix,2,c(0,0,0),'*'))^2%>%sum()%>%sqrt() # thus, we can know the scalars or coefficients that make vector_norm zero are c(0,0,0)\n```\n\n### Gram Matrix\n\nHow to check whether the columns vectors are linear independent or not? : Gram Matrix\n\n:::{#def-gram}\nThe Gram matrix, also known as the covariance matrix or the autocovariance matrix, is a matrix that captures the inner products between vectors in a set. It is defined as follows:\n\nGiven a set of vectors $\\{ \\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n \\}$, for the matrix, $\\mathbf{V}=\\begin{bmatrix} \\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n \\end{bmatrix}$, the Gram matrix $\\mathbf{G}=\\mathbf{V}^{T}\\mathbf{V}$ is an $n \\times n$ matrix whose entries are given by $\\mathbf{G}_{ij}= \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle$, where $\\langle , \\rangle$ represents the inner product between vectors.\n\n$$\n\\mathbf{G} = \\mathbf{V}^{T}\\mathbf{V} = \\begin{bmatrix}\n\\langle \\mathbf{v}_1, \\mathbf{v}_1 \\rangle & \\langle \\mathbf{v}_1, \\mathbf{v}_2 \\rangle & \\cdots & \\langle \\mathbf{v}_1, \\mathbf{v}_n \\rangle \\\\\n\\langle \\mathbf{v}_2, \\mathbf{v}_1 \\rangle & \\langle \\mathbf{v}_2, \\mathbf{v}_2 \\rangle & \\cdots & \\langle \\mathbf{v}_2, \\mathbf{v}_n \\rangle \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\langle \\mathbf{v}_n, \\mathbf{v}_1 \\rangle & \\langle \\mathbf{v}_n, \\mathbf{v}_2 \\rangle & \\cdots & \\langle \\mathbf{v}_n, \\mathbf{v}_n \\rangle \\\\\n\\end{bmatrix}\n$$\n:::\n\n#### Properties\n\n* Symmetry: The Gram matrix is always symmetric, meaning that $\\mathbf{G}_{ij}=\\mathbf{G}_{ji}$ for all $i$ and $j$\n* The sample covariance matrix is ​​expressed in the form of the Gram matrix of the centering matrix because the sample covariance matrix is computed using $\\mathbf{X}^{T}\\mathbf{X}$. \n* Covariance interpretation: In statistics, the Gram matrix can represent the covariance matrix of a set of random variables. Each entry ${G}_{ij}$ represents the covariance between the $i$ th and $j$ th random variables.\n* Positive semi-definiteness: The Gram matrix is positive semi-definite, which means that all of its eigenvalues are non-negative. This property ensures that the Gram matrix is non-negative and preserves the positive definite inner product structure.\n  $$\n  \\mathbf{x}^{T}\\mathbf{G}\\mathbf{x} = \\mathbf{x}^{T}\\mathbf{V}^{T}\\mathbf{V}\\mathbf{x} \\ge 0 \\text{ for all }  \\mathbf{x} \n  $$\n* Inner products: The entries of the Gram matrix represent the inner products between vectors.\n* **Linear independence**: The column vectors in $\\mathbf{V}$ are linearly independent if and only if the Gram matrix is positive definite, meaning that all of its eigenvalues are strictly positive. \n  $$\n  \\mathbf{x}^{T}\\mathbf{G}\\mathbf{x} = \\mathbf{x}^{T}\\mathbf{V}^{T}\\mathbf{V}\\mathbf{x} > 0 \\text{ for all } \\mathbf{x} \\ne \\mathbf{0}\n  $$\n  * This property allows to determine linear independence of the column vectors of $\\mathbf{V}$ by examining the eigenvalues of the Gram matrix.\n* When $\\mathbf{V}$ has independent columns, its Gram matrix is invertible. When $\\mathbf{V}$ has dependent columns, its Gram matrix is not invertible because the trace of $\\mathbf{V}$ can be computed using the product of the eigenvalues. If there is at least one zero in the list of the eigenvalues, the product becomes zero and the determinant of the gram matrix is zero, which results in the gram marix corresponding to $\\mathbf{V}$ with the dependent columns is not invertible\n\n:::{.callout-tip}\n## Multicolinearity Problem\n\n$$\n\\hat{\\beta}=(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\n$$\n\nwhere $\\mathbf{X}^{T}\\mathbf{X}$ is a Gram matrix.\n\nIf there are linearly dependent columns, $\\operatorname{det}(\\mathbf{X}^{T}\\mathbf{X})$ is zero. So, there is no inverse of $\\mathbf{X}^{T}\\mathbf{X}$. If a scatter plot of two columns show not a perfect line but an almost perfect line, at least one of the eigenvalues is close to zero. Then, their product become so small that $\\frac{1}{\\operatorname{det}(\\mathbf{X}^{T}\\mathbf{X})}$ becomes very large and unstably computed, which causes unreliable values of $\\hat{\\beta}$ to be computed. The meaning of 'unstable' is that the estimates of $\\hat{\\beta}$ are very large or very small or greatly fluctuated to such an extent that it does not make sense whenever the number of the columns or variables change in the regression model.\n\n:::\n\n#### Example\n\n**Example1** Simple Gram Matrix Example\n```{r}\n#| eval: false\n\n\n# Define the set of vectors\nv1 <- c(1, 2, 3)\nv2 <- c(4, 5, 6)\nv3 <- c(7, 8, 9)\n\n# Compute the Gram matrix\ncolumn_vector_matrix <- t(matrix(c(\n  crossprod(v1), crossprod(v1, v2), crossprod(v1, v3),\n  crossprod(v2, v1), crossprod(v2), crossprod(v2, v3),\n  crossprod(v3, v1), crossprod(v3, v2), crossprod(v3)\n), nrow = 3))\n\n# Print the Gram matrix\nprint(column_vector_matrix)\n```\n\n**Example2** Check if $\\mathbf{V}$ is linearly independent using the linear independence property.\n```{r}\n#| eval: false\n\n\ngram_matrix <- t(column_vector_matrix) %*% column_vector_matrix\neigen(gram_matrix)$values %>% round(1)\n```\n\nIf the result eigenvalues of eigendecomposition of gram matrix are all positive, the columns of $\\mathbf{V}$ are linearly independent. The number of eigen values $>0$ is the number of the eigenvectors.\n\n### Transpose\n\nThe transpose of an $m \\times n$ matrix $\\mathbf{A}$, denoted by $\\mathbf{A}^T$, is the $n \\times m$ matrix obtained by interchanging the rows and columns of $\\mathbf{A}$. Formally, if $\\mathbf{A} = [a_{ij}]$ is an $m \\times n$ matrix, then its transpose $\\mathbf{A}^T = [b_{ij}]$ is an $n x m$ matrix where $b_{ij}$ = $a_{ji}$ for all $i$ and $j$. In other words, the element in the $i$ th row and $j$ th column of $A^T$ is equal to the element in the $j$ th row and ith column of $\\mathbf{A}$.\n\nGiven an $m \\times n$ matrix $\\mathbf{A}$, its transpose $\\mathbf{A}^T$ is an $n \\times m$ matrix defined by:\n$$\n(\\mathbf{A^T})_{i,j} = \\mathbf{A}_{j,i}\n$$\n\n* When $\\mathbf{A}$ is transposed, diagnoal entries($a_{ii}$) do not change but off-diagnoal elements($a_{ij} \\; i \\neq j$) change.\n* A column vector is tranposed into a row vector, and vice versa.\n* symmetric matrix: $\\mathbf{A} = \\mathbf{A}^T$\n\nExample:\n\nLet $\\mathbf A$ be the matrix\n$$\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n$$\nThe transpose of $\\mathbf A$, denoted by $\\mathbf A^T$, is the matrix\n$$\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n$$\n\n#### Properties\n\nLet $\\mathbf A$ be an $m \\times n$ matrix and $\\mathbf B$ be an $n \\times p$ matrix, and let $c$ be a scalar. Then:\n\n* $(\\mathbf{A}^T)^T = \\mathbf{A}$\n* $(\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T$\n* $(c\\mathbf{A})^T = c\\mathbf{A}^T$\n* $(\\mathbf{AB})^T = \\mathbf{B}^T \\mathbf{A}^T$\n* $(\\mathbf{Ax})^T = \\mathbf{x}^T \\mathbf{A}^T$\n  * $(\\mathbf{Ax})$ combies the columns of $\\mathbf A$ while $\\mathbf{x}^T \\mathbf{A}^T$ combines the rows of $\\mathbf{A}^T$\n* $(\\mathbf{ABC})^T = \\mathbf{C}^T\\mathbf{B}^T \\mathbf{A}^T$ , (cyclic properties)\n  * If $\\mathbf A=(\\mathbf{LDU})^T$ then,$\\mathbf{A}^T=\\mathbf{U}^T\\mathbf{D}^T \\mathbf{L}^T$ where $\\mathbf{D} = \\mathbf{D}^T$\n* $(\\mathbf{A}^{-1})^T = (\\mathbf{A}^{T})^{-1}$\n  * $\\mathbf{A}^{T}(\\mathbf{A}^{-1})^T = \\mathbf{A}^{T}(\\mathbf{A}^{T})^{-1}=\\mathbf{I}$\n\n### Trace\n\nThe trace of a square matrix $\\mathbf{A}$, denoted by $\\mathrm{tr}(\\mathbf{A})$, is defined as the sum of the diagonal elements of $\\mathbf{A}$. In other words, if $\\mathbf{A}$ is an $n \\times n$ matrix, then its trace is given by:\n\n$$\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ii}\n$$\n\nwhere $a_{ii}$ denotes the $i$ th diagonal element of $\\mathbf{A}$.\n\nFor example, let\n$$\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n$$\n\nThen, the trace of $\\mathbf{A}$ is $\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11$\n\n#### Properties\n\n* $\\operatorname{tr}(k\\mathbf A) =k \\operatorname{tr}(\\mathbf A)$\n* $\\operatorname{tr}(\\mathbf A+\\mathbf B) =\\operatorname{tr}(\\mathbf A)+\\operatorname{tr}(\\mathbf B)$\n* $\\operatorname{tr}(\\mathbf{AB}) =\\operatorname{tr}(\\mathbf{BA})$\n* Cyclic Property: $\\operatorname{tr}(\\mathbf{ABC}) =\\operatorname{tr}(\\mathbf{BCA})=\\operatorname{tr}(\\mathbf{CAB})\\ne \\operatorname{tr}(\\mathbf{CBA})$\n* Distributive Law: $\\operatorname{tr}(\\mathbf{A(B+C)}) =\\operatorname{tr}(\\mathbf{AB})+\\operatorname{tr}(\\mathbf{AC})$\n* $\\operatorname{tr}(\\mathbf{A}^{T}) =\\operatorname{tr}(\\mathbf{A})$\n\n```{r}\n#| eval: false\n\n\ntr<-function(mat){\n  return(sum(diag(mat)))\n}\n\nA<-matrix(sample(1:100,9,replace=TRUE),ncol=3,byrow=TRUE)\nB<-matrix(sample(1:100,9,replace=TRUE),ncol=3,byrow=TRUE)\nC<-matrix(sample(1:100,9,replace=TRUE),ncol=3,byrow=TRUE)\nk<-3\npaste0('k=', k)\nprint('matrix A:')\nprint(A)\nprint('matrix B:')\nprint(B)\nprint('matrix C:')\nprint(C)\nprint('Diagonal Elements of the matrix A:')\nprint(diag(A))\n\npaste0('trace(A) or tr(A):',tr(A))\npaste('tr(kA)=ktr(A)',tr(k*A),k*tr(A),sep=', ')\npaste('tr(A+B)=tr(A)+tr(B)',tr(A+B),tr(A)+tr(B),sep=', ')\npaste('tr(A+B)=tr(B+A)',tr(A+B),tr(B+A),sep=', ')\npaste('tr(AB)=tr(BA)',tr(A%*%B),tr(B%*%A),sep=', ')\npaste('tr(ABC)=tr(BCA)=tr(CAB)!=tr(CBA)',tr(A%*%B%*%C),tr(B%*%C%*%A),tr(C%*%A%*%B),tr(C%*%B%*%A),sep=', ')\npaste('tr(A(B+C))=tr(AB)+tr(AC)',tr(A%*%(B+C)),tr(A%*%B)+tr(A%*%C),sep=', ')\npaste('tr(A^T)=tr(A)',tr(t(A)),tr(A),sep=', ')\n\n```\n\n\n### Rank\n\n\n:::{#def-rank}\nThe rank of a matrix is the maximum number of linearly independent rows or columns in the matrix. It is denoted by $\\text{rank}(\\mathbf{A})$.\n:::\n\nThe rank of matrix $\\mathbf{A}$ is the dimension of the space that matrix $\\mathbf{A}$ can create.\nSince the variables of real high-dimensional data have high correlation with each other, the number of variables and the number of ranks can be different.\nRank is one of the important measures of the amount of information a matrix contains. In practice, the concept of rank is widely used in various computational techniques in data analysis.\n\n:::{.callout-tip}\n1. Determinant: The determinant of a square matrix provides information about the scaling factor of the linear transformation represented by the matrix. It can indicate whether the transformation stretches or compresses space and whether it changes orientation.\n2. Trace: It provides information about the sum of the eigenvalues of the matrix and is often used in various matrix decompositions and calculations.\n3. Eigenvalues and Eigenvectors: The eigenvalues and eigenvectors of a matrix represent the characteristic values and corresponding vectors that describe the matrix's behavior under linear transformations. They can provide insights into the stretching, shearing, or rotating effects of the matrix.\n4. Singular Value Decomposition (SVD): SVD decomposes a matrix into three components: U, Σ, and V. The singular values in Σ represent the scaling factors, and the matrices U and V describe the transformations. The singular values indicate the amount of information carried by each dimension of the matrix.\n5. Principal Component Analysis (PCA): It identifies the principal components, which are linear combinations of the original variables that capture the maximum variance in the data.\n\n:::\nKnowing the rank can help us in various data analysis tasks, such as:\n\n* Feature selection: We can prioritize the selection of the most informative variables by considering their contribution to the rank. Variables with a higher rank tend to carry more unique information and are less likely to be redundant.\n* Dimensionality reduction: If we find that the rank is lower than the total number of variables, we can use dimensionality reduction techniques, such as Principal Component Analysis (PCA), to transform the data into a lower-dimensional space while preserving the most important information.\n* Linear regression: The rank of the design matrix is essential in linear regression modeling. If the rank is less than the number of variables, it indicates the presence of multicollinearity, which can affect the model's stability and interpretation.\n\n#### Properties\n\n1. The rank of a matrix is always less than or equal to the minimum of the number of rows and the number of columns:\n\n$$\n\\text{rank}(\\mathbf{A}) \\leq \\min(\\text{rows}(\\mathbf{A}), \\text{cols}(\\mathbf{A}))\n$$\n\n$\\mathbf{A}$ is said to be a full rank matrix if $\\text{rank}(\\mathbf{A}) = \\min(\\text{rows}(\\mathbf{A}), \\text{cols}(\\mathbf{A}))$. \n\nIt is said that the vectors are linearly indpendent if $\\mathbf{A}$ is said to be a full rank matrix.\n\n```{r}\n#| eval: false\n\n\nlibrary(Matrix)\nA <- matrix(1:6,nrow=2)\nrankMatrix(A)\n```\n\n1. The rank of a matrix is equal to the rank of its transpose\n\n$$\n\\text{rank}(\\mathbf{A}) =\\text{rank}(\\mathbf{A}^T)\n$$\n\n```{r}\n#| eval: false\n\n\nrank_A <-rankMatrix(A)\nrank_A_transpose <-rankMatrix(t(A))\nrank_A == rank_A_transpose\n```\n\n1. The determinant of a square matrix is zero if and only if the rank of the matrix is less than the number of columns (or rows) in the matrix.\n$$\n|\\mathbf{A_{n \\times n}}|=0 \\text{ if and only if } \\text{rank}(\\mathbf{A}) < n\n$$\n\n```{r}\nA<-matrix(1:9,nrow=3)\ndet_A <- det(A)\nrank_A <- rankMatrix(A)\nn<-nrow(A)\n\nrank_A < n\ndet_A == 0\n```\n\n\n1. For $\\mathbf{C}_{m\\times n}$, invertible $\\mathbf{A}_{m\\times m}$, and invertible $\\mathbf{B}_{n\\times n}$, $\\text{rank}(\\mathbf{C})$ is preserved. This property highlights that the rank of a matrix remains unchanged when multiplied by an invertible matrix on either side.\n\n$$\n\\text{rank}(\\mathbf{C}) =\\text{rank}(\\mathbf{AC})=\\text{rank}(\\mathbf{CB}) =\\text{rank}(\\mathbf{ACB})\n$$\n\n```{r}\n#| eval: false\n\n\nA<-matrix(c(2,0,0,1), nrow=2)\nB<-matrix(c(1,4,3,3,1,0,0,0,1), nrow=3)\nC<-matrix(c(1:3,2,4,6), nrow=2, byrow=T)\n\ndet(A)\ndet(B)\n\nrank_C<-rankMatrix(C)\nrank_AC<-rankMatrix(A%*%C)\nrank_CB<-rankMatrix(C%*%B)\nrank_ACB<-rankMatrix(A%*%C%*%B)\n\n(rank_C==rank_AC)&(rank_C==rank_CB)&(rank_C==rank_ACB)\n\n```\n\n1. For $\\text{rank}(\\mathbf{A}_{m\\times n}) =r$, there exists $\\mathbf{P}_{m\\times m}$ and $\\mathbf{Q}_{n\\times n}$ such that\n\n$$\n\\mathbf{PAQ}= \\begin{bmatrix} \\mathbf{I_{r\\times r}} & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{0} \\end{bmatrix}\n$$\n\nThis property, known as the rank factorization theorem, states that any matrix with a given rank $r$ can be transformed into a canonical form using suitable row and column operations represented by the invertible matrices $\\mathbf{P}$ and $\\mathbf{Q}$, respectively. In this canonical form, the non-zero elements are confined to the top-left $r \\times r$ submatrix, while the remaining elements are all zeros. ( Canonical forms are often used to simplify the analysis and study of matrices by providing a standard or standardized form that exhibits specific characteristics.)\n\n:::{.callout-tip}\n## Generalized Inverse (Pseudoinverse)\n\nFor a given matrix $\\mathbf{A}$, its generalized inverse is denoted as $\\mathbf{A}^\\dagger$, and it satisfies the following properties:\n\n* $\\mathbf{A}\\mathbf{A}^\\dagger\\mathbf{A} = \\mathbf{A}$: The product of $\\mathbf{A}$ with its generalized inverse from the left is equal to $\\mathbf{A}$ itself.\n* $\\mathbf{A}^\\dagger\\mathbf{A}\\mathbf{A}^\\dagger = \\mathbf{A}^\\dagger$: The product of $\\mathbf{A}^\\dagger$ with $\\mathbf{A}$ from the right is equal to $\\mathbf{A}^\\dagger$ itself.\n\nA generalized inverse, also known as a pseudoinverse, is a concept in linear algebra that extends the notion of inverse to matrices that may not have a unique or traditional inverse. It is used to solve systems of equations involving matrices that are not necessarily square or invertible. It provides a way to \"undo\" the effect of a non-invertible or underdetermined matrix.\n\nThere are several methods to compute the generalized inverse of a matrix, such as the Moore-Penrose pseudoinverse, the Drazin inverse, and the singular value decomposition (SVD) method.\n\n:::\n\n1. For any $\\mathbf{A}$, the rank of a gram matrix $\\mathbf{A}^T\\mathbf{A}$ is the same as the rank of $\\mathbf{A}$\n\n$$\n\\text{rank}(\\mathbf{A}^T\\mathbf{A}) =\\text{rank}(\\mathbf{A})\n$$\n\n```{r}\n#| eval: false\n\n\nA<-matrix(1:6,nrow=2)\nrankMatrix(A)\n```\n\nWhen performing hypothesis tests or fitting models, the $\\chi^2$ distribution is commonly used to assess the goodness of fit or test the significance of certain parameters. The test statistic follows a $\\chi^2$ distribution under the null hypothesis.\n\nIn the case of linear regression, for example, the degrees of freedom associated with the $\\chi^2$ distribution can be related to the difference between the number of observations and the rank of the design matrix. Specifically, if you have a linear regression model with $n$ observations and $p$ predictors, the degrees of freedom for the $\\chi^2$ distribution would be $n - p$. Here, $p$ represents the rank of the design matrix.\n\nThis relationship arises because the rank of the design matrix determines the effective number of parameters or independent components in the model. Subtracting the rank from the total number of observations yields the degrees of freedom associated with the $\\chi^2$ distribution in this specific context.\n\n1. The rank of a matrix is equal to the number of non-zero singular values in its singular value decomposition (SVD):\n\n$$\n\\text{rank}(\\mathbf{A}) = \\text{number of non-zero singular values}\n$$\n\n1. The rank of a matrix is equal to the dimension of the column space (or row space) of the matrix: \n\n$$\n\\text{rank}(\\mathbf{A}) = \\text{dimension of column space} = \\text{dimension of row space}\n$$\n\n1. For two matrices $\\mathbf{A}$ and $\\mathbf{B}$ of compatible sizes, the rank of their matrix product is at most the minimum of their individual ranks:\n\n$$\n\\text{rank}(\\mathbf{AB}) \\leq \\min(\\text{rank}(\\mathbf{A}), \\text{rank}(\\mathbf{B}))\\\\\n$$\n\n```{r}\n#| eval: false\n\n\nA<-matrix(c(1,2,3,6,5,10),nrow=2)\nB<-matrix(c(7,8,9,10,11,12),ncol=2)\n\nrank_A<-rankMatrix(A)\nrank_B<-rankMatrix(B)\nrank_AB<-rankMatrix(A%*%B)\nrank_AB<=min(rank_A,rank_B)\n```\n\n$$\n\\text{rank}(\\mathbf{A+B}) \\leq \\text{rank}(\\mathbf{A}) + \\text{rank}(\\mathbf{B})\\\\\n$$\n\n```{r}\n#| eval: false\n\n\nA<-matrix(c(1,2,3,6,5,10),nrow=2)\nB<-matrix(c(7,8,9,10,11,12),nrow=2)\n\nrank_A<-rankMatrix(A)\nrank_B<-rankMatrix(B)\nrank_A_plus_B<-rankMatrix(A+B)\nrank_A_plus_B<=rank_A+rank_B\n```\n\n\n1. If a matrix is invertible (i.e., has full rank), its rank is equal to its number of rows (or columns):\n\n$$\n\\text{rank}(\\mathbf{A}) = \\text{rows}(\\mathbf{A}) = \\text{cols}(\\mathbf{A})\n$$\n\n\n#### Examples\n\n**Example1**\nConsider the following matrix:\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \n\\end{bmatrix}\n$$\n\n\nTo find the rank of $\\mathbf{A}$, we can row reduce the matrix to its echelon form or perform other operations to determine the linearly independent rows or columns. Let's compute the rank of $\\mathbf{A}$:\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \\\\\n\\end{bmatrix} \\sim \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & -6 & -12 \\\\\n\\end{bmatrix} \\sim \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix} \\sim \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}\n$$\n\nApplying the row operations $(R2\\leftarrow R2-4R1)$ and $(R3\\leftarrow R3-7R1)$, we get:\n\n$$\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix} \\sim \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}\n$$\n\nFurther applying the row operation $(R3\\leftarrow R3-2R2)$, we obtain:\n\n$$\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix} \\sim \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}\n$$\n\nWe have reached the echelon form of $\\mathbf{A}$. Notice that there are two linearly independent rows (row 1 and row 2) and one zero row. Therefore, the rank of $\\mathbf{A}$ is 2.\n\nHence, the rank of $\\mathbf{A}$ is 2.\n\n```{python}\n#| eval: false\n\n\nimport numpy as np\n\n# Example data matrix\ndata = np.array([[1, 2, 3, 4, 5],\n                 [2, 4, 6, 8, 10],\n                 [3, 6, 9, 12, 15]])\n\n# Compute the rank of the data matrix\nrank = np.linalg.matrix_rank(data)\n\nprint(\"Rank of the data matrix:\", rank)\n\n```\n\n## Expectation\n\n**Expectation**\n\nThe expectation (also known as the expected value) of a random variable is a measure of its central tendency. For a discrete random variable $X$ with probability mass function $P(X)$, the expectation is defined as:\n\n$$\nE(X) = \\sum_{x} x \\cdot P(X = x)\n$$\n\nwhere $x$ represents the possible values of $X$.\n\nFor a continuous random variable $Y$ with probability density function $f(Y)$, the expectation is defined as:\n\n$$\nE(Y) = \\int_{-\\infty}^{\\infty} y \\cdot f(Y = y) \\, dy\n$$\n\nwhere $y$ represents the possible values of $Y$.\n\n\n### Properties\n\nFor constants $a$ and $b$, matrix $\\mathbf{A}_{m\\times n}$, vector $\\mathbf{b}_{m \\times 1}$, random variables $X$ and $Y$, and random vector $\\mathbf{x}_{m\\times 1}$ and $\\mathbf{y}_{m\\times 1}$\n\n**Linearity of Expectation**\n\nThe expectation is a linear operator, which means it follows the rules of linearity. \n\n$$\nE(aX + bY) = aE(X) + bE(Y)\n$$\n\n$$\nE(\\mathbf{Ax} + \\mathbf{b}) = \\mathbf{A}_{m \\times n}E(\\mathbf{x}_{m\\times 1})_{n\\times 1} + \\mathbf{b}_{m\\times 1}\n$$\n\n\n**Example:**\n\nLet's consider two random variables $X$ and $Y$ with the following probability distributions:\n\n$$\nP(X = 1) = \\frac{1}{3}, \\quad P(X = 2) = \\frac{1}{3}, \\quad P(X = 3) = \\frac{1}{3}\n$$\n\n$$\nP(Y = 0) = \\frac{1}{2}, \\quad P(Y = 1) = \\frac{1}{2}\n$$\n\nWe want to find $E(2X + 3Y)$, the expectation of $2X + 3Y$.\n\nUsing the linearity property of expectation, we can calculate it as:\n\n$$\n\\begin{aligned}\n  E(2X + 3Y) &= 2E(X) + 3E(Y) \\\\\n  E(X) &= 1 \\cdot \\frac{1}{3} + 2 \\cdot \\frac{1}{3} + 3 \\cdot \\frac{1}{3} = 2 \\\\\n  E(Y) &= 0 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2} = \\frac{1}{2} \\\\\n  E(2X + 3Y) &= 2 \\cdot 2 + 3 \\cdot \\frac{1}{2} = 5\n\\end{aligned}\n$$\n\n### Random Vector\n\nLet $X,Y$ be $X_1,X_2$, independent with each other, and $\\mathbf{x}=\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}$.\n\n$$\n\\begin{aligned}\n  E(\\mathbf{x}) &= \\mathbf{\\mu} = \\begin{bmatrix} E( X_1) \\\\ E(X_2) \\end{bmatrix}= \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\\\\n  E(2X+3Y) &= E(\\mathbf{A}\\mathbf{x}+\\mathbf{b}) = E(\\begin{bmatrix} 2 & 3 \\end{bmatrix}\\mathbf{x} + \\mathbf{0}) \\\\\n  &= \\begin{bmatrix} 2 & 3 \\end{bmatrix}\\mathbf{\\mu} + \\mathbf{0} \\\\\n  &= \\begin{bmatrix} 2 & 3 \\end{bmatrix}\\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\end{bmatrix} \\\\\n  &= \\begin{bmatrix} 2 & 3 \\end{bmatrix}\\begin{bmatrix} 2 \\\\ \\frac{1}{2} \\end{bmatrix} = 1\n\\end{aligned}\n$$\n\nLet $Cov(X,Y)=1$.\nLet $X,Y$ be $X_1,X_2$, dependent with each other.\nLet $\\mathbf{x}=\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}$. \nThe expected value is the same as that of the independent one.\n\n$$\n\\begin{aligned}\n  Var(\\mathbf{x}) &= \\mathbf{\\Sigma} = \\begin{bmatrix} Var(X_1) & Cov(X_1,X_2) \\\\ Cov(X_2,X_1) & Var(X_2)\\end{bmatrix} =  \\begin{bmatrix} Var(X_1) & 1 \\\\ 1 & Var(X_2)\\end{bmatrix} \\\\\n  Var(2X_1-3X_2+3) &= Var(\\begin{bmatrix} 2 & -3 \\end{bmatrix} \\mathbf{X} +3) = \\begin{bmatrix} 2 & -3 \\end{bmatrix} Var( \\mathbf{X}) \\begin{bmatrix} 2 \\\\ -3 \\end{bmatrix} \\\\\n  &= \\begin{bmatrix} 2 & -3 \\end{bmatrix} \\begin{bmatrix} Var(X_1) & 1 \\\\ 1 & Var(X_2)\\end{bmatrix} \\begin{bmatrix} 2 \\\\ -3 \\end{bmatrix}\n\\end{aligned}\n$$\n\n* Monotonicity of Expectation\n\nIf $X$ and $Y$ are random variables such that $X \\leq Y$ almost surely, then:\n\n$$\nE(X) \\leq E(Y)\n$$\n\nIn other words, if one random variable is always less than or equal to another, then their expectations follow the same order.\n\n**Example:**\n\nLet's consider two random variables $X$ and $Y$ with the following probability distribution:\n\n$$\nP(X = 1) = \\frac{1}{2}, \\quad P(X = 2) = \\frac{1}{2}\n$$\n\n$$\nP(Y = 2) = \\frac{1}{2}, \\quad P(Y = 3) = \\frac{1}{2}\n$$\n\nWe can observe that $X \\leq Y$ almost surely since the largest value of $X$ is 2, which is less than the smallest value of $Y$ which is 3.\n\nTo compare their expectations, we calculate:\n\n$$\nE(X) = 1 \\cdot \\frac{1}{2} + 2 \\cdot \\frac{1}{2} = \\frac{3}{2}\n$$\n\n$$\nE(Y) = 2 \\cdot \\frac{1}{2} + 3 \\cdot \\frac{1}{2} = \\frac{5}{2}\n$$\n\nTherefore, we have $E(X) = \\frac{3}{2} \\leq E(Y) = \\frac{5}{2}$, confirming the monotonicity property of expectation.\n\n* Linearity of Expectation of random variables\n\nFor any two random variables $X$ and $Y$, we have:\n\n$E(X + Y) = E(X) + E(Y)$\n\nIn other words, the expectation of the sum of two random variables is equal to the sum of their individual expectations.\n\n**Example:**\n\nConsider two fair six-sided dice. Let $X$ be the outcome of the first die, and $Y$ be the outcome of the second die.\n\nThe probability distribution of each die roll is:\n\n$$\nP(X = i) = \\frac{1}{6} \\quad \\text{for } i = 1, 2, 3, 4, 5, 6\n$$\n\n$$\nP(Y = j) = \\frac{1}{6} \\quad \\text{for } j = 1, 2, 3, 4, 5, 6\n$$\n\nWe want to calculate the expected value of the sum $X + Y$.\n\nUsing the linearity of expectation property, we can compute:\n\n$$\nE(X) = \\sum_{i=1}^{6} i \\cdot P(X = i) = \\frac{1}{6} \\left(1 + 2 + 3 + 4 + 5 + 6\\right) = \\frac{7}{2}\n$$\n\n$$\nE(Y) = \\sum_{j=1}^{6} j \\cdot P(Y = j) = \\frac{1}{6} \\left(1 + 2 + 3 + 4 + 5 + 6\\right) = \\frac{7}{2}\n$$\n\nNow, we can calculate the expectation of their sum:\n\n$$\nE(X + Y) = E(X) + E(Y) = \\frac{7}{2} + \\frac{7}{2} = 7\n$$\n\nTherefore, we have $E(X + Y) = 7$.\n\nThe linearity of expectation property holds, and we see that the expectation of the sum $X + Y$ is equal to the sum of their individual expectations.\n\n## Variance\n\n**Definition: Variance**\n\nThe variance of a random variable $X$ is a measure of the spread or dispersion of the values of $X$. It quantifies how much the values of $X$ deviate from their expected value.\n\nFor a random variable $X$ with expected value $E(X)$, the variance is denoted by $\\text{Var}(X)$ and is defined as the average of the squared differences between each value $x$ of $X$ and the expected value, weighted by their probabilities:\n\n$$\n\\text{Var}(X) = E\\left[(X - E(X))^2\\right]\n$$\n\nAlternatively, the variance can be expressed as:\n\n$$\n\\text{Var}(X) = E(X^2) - (E(X))^2\n$$\n\n**Example:**\n\nConsider a random variable $X$ representing the number obtained when rolling a fair six-sided die. The possible values of $X$ are $1, 2, 3, 4, 5, \\text{ and } 6$, each with a probability of $\\frac{1}{6}$.\n\nTo find the variance $\\text{Var}(X)$, we need to compute the expected value $E(X)$ and the expected value of the square $E(X^2)$.\n\nThe expected value $E(X)$ is the average value of $X$ and can be calculated as:\n\n$$\nE(X) = 1 \\cdot P(X = 1) + 2 \\cdot P(X = 2) + 3 \\cdot P(X = 3) + 4 \\cdot P(X = 4) + 5 \\cdot P(X = 5) + 6 \\cdot P(X = 6)\n$$\n\nSubstituting the probabilities, we have:\n\n$$\nE(X) = 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6}\n$$\n\nSimplifying the expression gives us:\n\n$$\nE(X) = \\frac{21}{6} = \\frac{7}{2}\n$$\n\nNext, we need to compute the expected value of the square $$E(X^2)$$:\n\n$$\nE(X^2) = 1^2 \\cdot P(X = 1) + 2^2 \\cdot P(X = 2) + 3^2 \\cdot P(X = 3) + 4^2 \\cdot P(X = 4) + 5^2 \\cdot P(X = 5) + 6^2 \\cdot P(X = 6)\n$$\n\nSubstituting the probabilities, we have:\n\n$$\nE(X^2) = 1^2 \\cdot \\frac{1}{6} + 2^2 \\cdot \\frac{1}{6} + 3^2 \\cdot \\frac{1}{6} + 4^2 \\cdot \\frac{1}{6} + 5^2 \\cdot \\frac{1}{6} + 6^2 \\cdot \\frac{1}{6}\n$$\n\nSimplifying the expression gives us:\n\n$$\nE(X^2) = \\frac{91}{6}\n$$\n\nFinally, we can compute the variance $\\text{Var}(X)$ using the formula:\n\n$$\n\\text{Var}(X) = E(X^2) - (E(X))^2\n$$\n\nSubstituting the values, we have:\n\n$$\n\\text{Var}(X) = \\frac{91}{6} - \\left(\\frac{7}{2}\\right)^2\n$$\n\nSimplifying the expression gives us:\n\n$$\n\\text{Var}(X) = \\frac{35}{12}\n$$\n\nTherefore, the variance $\\text{Var}(X)$ of the random variable $X$ is $\\frac{35}{12}$.\n\n### Properties\nFor constants $a$ and $b$, matrix $\\mathbf{A}_{m\\times n}$, vector $\\mathbf{b}_{m \\times 1}$, random variables $X$ and $Y$, and random vector $\\mathbf{x}_{m\\times 1}$ and $\\mathbf{y}_{m\\times 1}$\n\n1. Non-Negativity: The variance is always non-negative.\n   $$\n   \\text{Var}(X) \\geq 0\n   $$\n\n2. Variance of a Constant: The variance of a constant is zero.\n   $$\n   \\text{Var}(c) = 0\n   $$\n   where $c$ is a constant.\n\n3. Linearity: The variance of a linear transformation of a random variable can be obtained by multiplying the variance of the original random variable by the square of the constant factor.\n   $$\n   \\text{Var}(aX) = a^2\\text{Var}(X)\n   $$\n\n   where $a$ is a constant. For a random vector $\\mathbf{x}$,\n   \n   $$\n   \\begin{aligned}\n      \\text{Var}(\\mathbf{A}\\mathbf{x}+\\mathbf{b}) &= \\mathbf{A}\\text{Var}(\\mathbf{x})\\mathbf{A}^T \\\\\n      \\text{Var}(\\mathbf{x}) &= \\mathbf{\\Sigma} = \\begin{bmatrix} \\text{Var}(X_1) & \\text{Cov}(X_1,X_2) \\\\ \\text{Cov}(X_2,X_1) & \\text{Var}(X_2)\\end{bmatrix} \\\\\n      &=\\begin{bmatrix} \\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2\\end{bmatrix} \\\\\n      &=\\begin{bmatrix} \\text{Var}(X_1) & 0 \\\\ 0 & \\text{Var}(X_2)\\end{bmatrix} \\text{when the two random variables are independent}\n    \\end{aligned}\n   $$\n\n\n4. Additivity of Independent Variables: The variance of the sum (or difference) of independent random variables is equal to the sum of their variances.\n   $$\n   \\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\n   $$\n   where $X$ and $Y$ are independent random variables.\n\n   $$\n   \\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X,Y)\n   $$\n   where $X$ and $Y$ are dependent random variables.\n\n**Example:**\n\nConsider two independent random variables $X$ and $Y$ with variances $\\text{Var}(X) = 4$ and $\\text{Var}(Y) = 9$.\n\n1. Non-Negativity:\n   $\\text{Var}(X) = 4 \\geq 0$, $\\text{Var}(Y) = 9 \\geq 0$\n\n2. Variance of a Constant:\n   Let $c = 5$ be a constant. $\\text{Var}(c) = \\text{Var}(5) = 0$\n\n3. Linearity:\n   Let $a = 2$ be a constant. $\\text{Var}(aX) = \\text{Var}(2X) = 4(2^2) = 16$\n\n  $$\n  \\begin{aligned}\n    Var(2X_1+3X_2+3) &= Var(\\begin{bmatrix} 2 & 3 \\end{bmatrix} \\mathbf{X} +3) = \\begin{bmatrix} 2 & 3 \\end{bmatrix} Var( \\mathbf{X}) \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\\\\n      &= \\begin{bmatrix} 2 & 3 \\end{bmatrix} \\begin{bmatrix} Var(X_1) & 0 \\\\ 0 & Var(X_2)\\end{bmatrix} \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \n  \\end{aligned}\n  $$\n\n4. Additivity of Independent Variables:\n   $\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) = 4 + 9 = 13$\n\n## Covariance\n\n**Definition:**\nThe covariance between two random variables $X$ and $Y$ is a measure of how they vary together. It is defined as the expected value of the product of the deviations of $X$ and $Y$ from their respective means.\n\nThe covariance between $X$ and $Y$ is denoted as $Cov(X, Y)$ and can be calculated as:\n$$\n\\text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))]\n$$\n\n**Example:**\nLet's consider two random variables $X$ and $Y$ with the following data:\n\n$$\n\\begin{align*}\nX & : 1, 2, 3, 4, 5 \\\\\nY & : 2, 4, 6, 8, 10 \\\\\n\\end{align*}\n$$\n\nWe first calculate the means of $X$ and $Y$:\n$$\n\\begin{align*}\nE(X) &= \\frac{1 + 2 + 3 + 4 + 5}{5} = 3 \\\\\nE(Y) &= \\frac{2 + 4 + 6 + 8 + 10}{5} = 6 \\\\\n\\end{align*}\n$$\n\nNext, we calculate the deviations of $X$ and $Y$ from their means:\n$$\n\\begin{align*}\nX - E(X) & : -2, -1, 0, 1, 2 \\\\\nY - E(Y) & : -4, -2, 0, 2, 4 \\\\\n\\end{align*}\n$$\n\nThen, we calculate the product of these deviations and take their expected value:\n$$\n\\text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))]\n$$\n\nFinally, we substitute the values and calculate the covariance.\n\nThe resulting covariance will give us an indication of the relationship between the two variables.\n\n### Properties\n\n1. **Bilinearity:** Covariance is a bilinear function, meaning it satisfies the following properties:\n   - Linearity in the first argument: $Cov(aX, Y) = a * Cov(X, Y)$\n   - Linearity in the second argument: $Cov(X, bY) = b * Cov(X, Y)$\n   - Additivity: $Cov(X1 + X2, Y) = Cov(X1, Y) + Cov(X2, Y)$\n   - $Cov(aX+b+cY+d)=acCov(X,Y)$\n2. **Symmetry:** Covariance is symmetric, which means $Cov(X, Y) = Cov(Y, X)$.\n3. **Covariance with Constant:** $Cov(X, c) = 0$, where $c$ is a constant.\n4.  $Cov(X,Y)=0$ if the two random variables are independent.\n\n**Example:**\nLet's consider two random variables $X$ and $Y$ with the following data:\n\n$$\n\\begin{align*}\nX & : 1, 2, 3, 4, 5 \\\\\nY & : 2, 4, 6, 8, 10 \\\\\n\\end{align*}\n$$\n\n1. **Bilinearity:**\n   - Linearity in the first argument:\n   $$\n   \\text{Cov}(2X, Y) = 2 \\cdot \\text{Cov}(X, Y)\n   $$\n   \n   - Linearity in the second argument:\n   $$\n   \\text{Cov}(X, 3Y) = 3 \\cdot \\text{Cov}(X, Y)\n   $$\n   \n   - Additivity:\n   $$\n   \\text{Cov}(X_1 + X_2, Y) = \\text{Cov}(X_1, Y) + \\text{Cov}(X_2, Y)\n   $$\n\n2. **Symmetry:**\n   $$\n   \\text{Cov}(X, Y) = \\text{Cov}(Y, X)\n   $$\n\n3. **Covariance with Constant:**\n   $$\n   \\text{Cov}(X, c) = 0\n   $$\n\n\n\n```{r}\n#| echo: false\n#| eval: false\n\n### Singular value and Singluar Vectors\n\nThe singular value decomposition (SVD) of a matrix $\\mathbf A$ is a factorization of $\\mathbf A$ into the product of three matrices as follows:\n\n$$\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n$$\nwhere $\\mathbf{U}$ is an $m \\times m$ orthogonal matrix, $\\mathbf{\\Sigma}$ is an $m \\times n$ rectangular diagonal matrix with non-negative real numbers on the diagonal, and $\\mathbf{V}$ is an $n \\times n$ orthogonal matrix.\n\nThe diagonal entries of $\\mathbf{\\Sigma}$ are called the singular values of $\\mathbf{A}$, denoted as $\\sigma_1, \\sigma_2, \\ldots, \\sigma_r$ (where $r$ is the rank of $\\mathbf{A}$), and are arranged in descending order. The columns of $\\mathbf{U}$ and $\\mathbf{V}$ are called the left and right singular vectors of $\\mathbf{A}$, respectively, and are orthonormal vectors.\n\nFor example, let $\\mathbf{A}$ be a 3 by 2 matrix given by:\n\n$$\n\\begin{equation*}\n\\mathbf{A} = \n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n$$\n\nThe SVD of $\\mathbf{A}$ is given by:\n\n$$\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n$$\n\nwhere the left singular vectors of $\\mathbf{A}$ are the columns of $\\mathbf{U}$, the right singular vectors of $\\mathbf{A}$ are the columns of $\\mathbf{V}$, and the singular values of $\\mathbf{A}$ are the diagonal entries of $\\boldsymbol{\\Sigma}$.\n\n\n\n* 연립 방정식을 행렬의 곱으로 나타내보기\n  $$\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}$$ \n  $$ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}$$\n* 중요한 사실(....당연한 사실?)\n  * 곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n    * $A_{m \\times n} \\times B_{o \\times p}$ 에서 $n = o$ 여야 곱셈 성립\n  * 곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 $\\times$ 곱셈의 오른쪽 행렬의 열 수\n    * $A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}$\n  * 교환법칙(Commutative property)이 성립하지 않음\n    * $AB \\neq BA$\n* 행렬 곱셈의 여러가지 관점\n  * 내적으로 바라보기\n    $$ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} $$\n    $$ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}$$\n  * rank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중.....-_-)\n    $$AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}$$\n  * column space로 바라보기\n    $$A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m $$ (스칼라배의 합)\n    * $A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$ 는 2차원 좌표평면의 모든 점을, $A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$은 3차원 좌표평면의 모든 점 표현 가능\n    * $AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}$\n    * column space: A의 column vector로 만들 수 있는 부분 공간\n  * row space로 바라보기\n    $$\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} $$\n\n\n## 열공간(Column Space)\n{{< video https://youtu.be/g0eaDeVRdZk >}}\n* column space: column vector 들이 span 하는 space\n  * $A$의 column space = $C(A)$ 또는 $range(A)$\n* span: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n  * vector에 따라, 점일수도 선일수도 평면일 수도 있음\n  * vector space를 이 vector들이 span하는 space &rarr; column space는 행렬의 열들이 span하는 space\n* vector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n  * $\\mathbf{v_1}$ 과 $\\mathbf{v_2}$의 linear combination으로 2차원 좌표평면 나타내기\n  ![](images/chap02_09.PNG)\n\n\n\n## 선형 독립(Linear Independent)\n{{< video https://youtu.be/mOOI4-BfjGQ >}}\n\n...and also see\n{{< video https://youtu.be/9F4PZ_1orF0 >}}\n\n* 선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\n* orthogonal 하면 independent\n  * but independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n* definition: $a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}$ 를 만족하는 $a_1, a_2, a_3, \\cdots a_n$ 이 $a_1 = a_2 = a_3 = \\cdots = a_n = 0$ 밖에 없을때\n  * $\\mathbf{0}$는 모든 elements가 $0$인 벡터\n  * 예: $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, $\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}$ 는 $-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ 이 되므로, linearly independent 하지 않음\n  * independent한 vector 들의 수 = 표현할 수 있는 차원의 dimension\n\n## 기저(basis)\n* 주어진 vector space를 span하는 linearly independent한 vectors\n* 어떤 공간을 이루는 필수적인 구성요소\n* orthogonal 하면 orthogonal basis\n* 예: 2차원 좌표평면에 대해\n  * $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ : orthogonal basis\n  * $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ : orthogonal 하지 않은 basis\n  * $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$ : linearly independent 하지 않으므로 basis 아님\n\n## 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n{{< video https://youtu.be/XqOvyfMUAwA >}}\n\n### Identity matrix(항등행렬)\n* 항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n  * 실수에서 곱셈의 항등원은 1\n* 행렬의 항등원: 항등행렬($I$)\n  $$I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}$$\n  * $A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}$\n  * $I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}$ \n\n### Inverse matrix(역행렬) {#sec-inv}\n* 역원: 연산 결과 항등원이 나오게 하는 연소\n  * 실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): $a \\times a^{-1} = 1$\n* 행렬의 역원: 역행렬($A^{-1}$)\n  $$A \\times A^{-1} = I , A^{-1} \\times A = I$$\n  * 존재하지 않는 경우도 있음\n  * 존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n    * 존재하지 않으면 singular, degenerate라고 불림\n  * square matrix(정사각행렬, $m = n$)은 특수한 경우를 제외하면 역행렬이 항상 존재\n    * 역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n  * $m \\neq n$인 행렬의 경우에는 역행렬이 존재하지 않음\n    * 다만, 경우에 따라 $A \\times A^{-1} = I$ 를 만족하거나(right inverse), $A^{-1} \\times A = I$를 만족하는(left inverse)는 $A^{-1}$이 존재함\n  * 연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음\n    $$A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}$$ \n\n### Diagonal Matrix(대각행렬)\n* diagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix\n  $$ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}$$\n  * identity matrix는 diagonal matrix\n  * diagnomal matrix는 symmetric matrix 이기도 함\n  * 보통은 square matrix에서 주로 사용됨\n    * square matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n  \n\n### Orthogonal matrix(직교행렬, orthonomal matrix)\n* 행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors)\n  $$A A^T = A^T A = I$$\n* identity matrix는 orthogonal matrix\n* square matrix에서만 정의됨\n* Orthogonal matrix인 $A$이면 $A^{-1} = A^{T}$\n  * 각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n* complex matrix(복소수 행렬)에서는 unitary matrix라고 부름\n\n## 계수(Rank)\n\n{{< video https://youtu.be/HMST0Yc7EXE >}}\n\n* rank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\n* independent한 column의 수 = independent한 행의 수: $rank(A) = rank(A^T)$\n  * proof: [Wikipedia](https://en.wikipedia.org/wiki/Rank_%28linear_algebra%29#Proofs_that_column_rank_=_row_rank)\n* 예:\n  $$\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1$$\n  $$\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2$$\n* $A_{m \\times n}$ 의 최대 랭크는 $min\\{m,n\\}$\n  * $rank(A) < min\\{m,n\\}$ 면 rank-deficient, $rank(A) = min\\{m,n\\}$면 full (row/column) rank\n\n## 영공간(Null space)\n\n{{< video https://youtu.be/Eizc9TSRYMQ >}}\n\n* $A\\mathbf{x}= \\mathbf{0}$ 을 만족하는 $\\mathbf{x}$의 집합\n  * column space 관점에서 보기: $A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}$\n  * null space에 항상 들어가는 $\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$ : trivial solution\n    * 모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 $\\mathbf{x}=\\mathbf{0}$하나 밖에 없음\n* $\\mathbf{x}=\\mathbf{0}$ 가 아닌 vector가 null space에 있으면, 스칼라배(constant $c$에 대해 $c \\mathbf{x}$) 역시 null space에 포함됨\n* 혼동 주의! null space는 column space의 일부가 아님\n    * row vector의 차원이 null space가 존재하는 공간\n* rank와 null space의 dimension의 합은 항상 matrix의 column의 수\n    * $A_{m \\times n}$에 대해, $dim(N(A)) = n - r$\n      * 모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n  * null space는 row space와 수직한 space\n    * $A\\mathbf{x}= \\mathbf{0}$ : 각각 모든 행과 내적해서 0, &rarr; 행들의 linear combination과 내적해도 0\n    * rank는 row space의 dimension &rarr; row space의 dimension($dim(R(A))$)과 null space의 dimension($dim(N(A))$)의 합이 $n$\n    * $\\mathbb{R^n}$ 공간에 표현: \n      ![](images/chap02_10.PNG)\n      * 겹친 점: 영벡터\n* left null space: $\\mathbf{x^T} A = \\mathbf{0^T}$ 인 $\\mathbf{x}$\n  * 위의 성질을 row에 대해 적용\n    * m 차원에 놓인 벡터\n    * dimension:  $dim(N_L(A)) = m - r$\n    * column space와 수직: $dim(N_L(A)) +dim(C(A)) = m$\n* $R(A)$에 있는 vector $\\mathbf{x_r}$ 와 $N(A)$에 있는 vector $\\mathbf{x_n}$에 대해:\n  * $\\mathbf{x_r}$에 $A$를 곱하면 column space로 감\n  * $\\mathbf{x_n}$에 $A$를 곱하면 \\mathbf{x_0}$\n  * $A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} +  A\\mathbf{x_n} =  A\\mathbf{x_r} = \\mathbf{b}$\n\n## Ax = b의 해의 수\n\n{{< video https://youtu.be/nNI2TlD598c >}}\n\n* full column rank 일때\n  * $\\mathbf{b}$가 column space($C(A)$)안에 있으면 해가 하나\n  * $\\mathbf{b}$가 column space($C(A)$)안에 없으면 해가 없음\n  ![](images/chap02_11.PNG)\n\n* full row rank 일때\n  * $\\mathbf{b}$는 항상 column space 안에 있음: 무한의 해를 가짐\n  * 임의의 특정한 해(particular solution) $\\mathbf{x_p}$와 null space의 vector $\\mathbf{x_n}$에 대해, $A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}$\n    * 즉, $\\mathbf{x_p} +\\mathbf{x_n}$ 도 해가 됨: complete solution\n      * null space는 무한하므로, 해도 무한함\n\n* full rank 일때(square matrix): 해가 하나 존재 ($\\mathbf{x} = A^{-1}$\\mathbf{b}$)\n\n* rank-deficient 일때\n  * $\\mathbf{b}$가 column space($C(A)$)안에 있으면 무한한 해를 가짐\n  * $\\mathbf{b}$가 column space($C(A)$)안에 없으면 해가 없음\n\n```\n\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| eval: false\n\nlibrary(tidyverse)\nset.seed(100)\n```\n\n\n```{python}\n#| echo: false\n#| eval: false\n\nimport numpy as np\nimport matplotlib_inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport sympy as sym # for RREF\nimport scipy.linalg # for LU\nimport matplotlib.gridspec as gridspec # used to create non-regular subplots\n\n# NOTE: these lines define global figure properties used for publication.\nfrom IPython import display\nmatplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n#display.set_matplotlib_formats('svg') # display figures in vector format\nplt.rcParams.update({'font.size':14}) # set global font size\n     \n```\n\n## Block Matrices\n\nA block matrix is a matrix that is partitioned into smaller matrices, or blocks, arranged in a rectangular grid. The blocks can be of any size, and the resulting matrix is used to represent a system of linear equations with multiple variables or equations.\n\n### Example\n\nLet $\\mathbf A$  be a block matrix with four blocks, $\\mathbf A_{11}$, $\\mathbf A_{12}$, $\\mathbf A_{21}$, and $\\mathbf A_{22}$, as shown below:\n\n$$\n\\mathbf A = \\begin{bmatrix}\n\\mathbf A_{11} & \\mathbf A_{12} \\\\\n\\mathbf A_{21} & \\mathbf A_{22}\n\\end{bmatrix}\n$$\n\nwhere $\\mathbf A_{11}, \\mathbf A_{12}, \\mathbf A_{21},\\text{ and }\\mathbf A_{22}$ are individual matrices. This block matrix can be used to represent a system of linear equations with four variables or equations, where the blocks $\\mathbf A_{11}, \\mathbf A_{12}, \\mathbf A_{21},\\text{ and }\\mathbf A_{22}$ represent the coefficients of the variables in the linear equations.\n\n### Block Multiplication\n\nBlock multiplication is a matrix operation used with block matrices, where a matrix is partitioned into smaller matrices, or blocks, and the blocks are multiplied according to certain rules.\n\n#### Example\n\n$$\n\\mathbf A = \\begin{bmatrix}\n\\mathbf A_{11} & \\mathbf A_{12} \\\\\n\\mathbf A_{21} & \\mathbf A_{22}\n\\end{bmatrix}\n\\quad\n\\mathbf B = \\begin{bmatrix}\n\\mathbf B_{11} & \\mathbf B_{12} \\\\\n\\mathbf B_{21} & \\mathbf B_{22}\n\\end{bmatrix}\n$$\n\n\nThe block multiplication of $\\mathbf A$ and $\\mathbf B$, denoted as $\\mathbf{AB}$, can be computed as:\n\n$$\n\\mathbf{AB} = \\begin{bmatrix}\n\\mathbf A_{11} & \\mathbf A_{12} \\\\\n\\mathbf A_{21} & \\mathbf A_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf B_{11} & \\mathbf B_{12} \\\\\n\\mathbf B_{21} & \\mathbf B_{22}\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\mathbf A_{11}\\mathbf B_{11} + \\mathbf A_{12}\\mathbf B_{21} & \\mathbf A_{11}\\mathbf B_{12} + \\mathbf A_{12}\\mathbf B_{22} \\\\\n\\mathbf A_{21}\\mathbf B_{11} + \\mathbf A_{22}\\mathbf B_{21} & \\mathbf A_{21}\\mathbf B_{12} + \\mathbf A_{22}\\mathbf B_{22}\n\\end{bmatrix}\n$$\n\nwhere $\\mathbf{A}_{11}\\mathbf{B}_{11}, \\mathbf{A}_{12}\\mathbf{B}_{21}, \\mathbf{A}_{11}\\mathbf{B}_{12}, \\mathbf{A}_{12}\\mathbf{B}_{22}, \\mathbf{A}_{21}\\mathbf{B}_{11}, \\mathbf{A}_{22}\\mathbf{B}_{21}, \\mathbf{A}_{21}\\mathbf{B}_{12},\\text{ and }\\mathbf{A}_{22}\\mathbf{B}_{22}$ are block multiplications of the corresponding blocks.\n\nWhen matrices split into blocks, it is often simpler to see how they act. \n\nThis block unit can be reduced to the vector:\n\n$$\n\\mathbf{AB} = \\begin{bmatrix}\n\\mathbf a_{1} & \\dots & \\mathbf a_{n} \n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf b_{1} \\\\\n\\vdots\\\\\n\\mathbf b_{n} \n\\end{bmatrix}\n= \\begin{bmatrix}\n\\mathbf a_{1}\\mathbf b_{1} + \\dots + \\mathbf a_{n}\\mathbf b_{n}\n\\end{bmatrix}\n$$\n\n### Block Elimination\n\nBlock elimination is a technique used to solve systems of linear equations by reducing the system to a smaller set of equations. The process involves breaking the system down into smaller sub-systems or blocks, then eliminating one set of variables by expressing them in terms of the remaining variables. \n\n#### Schur Complement\n\nThe Schur complement is a matrix obtained by block elimination, where a large matrix $\\mathbf{A}$ is partitioned into blocks:\n\n$$\n\\begin{align*}\n\\mathbf{A} =\n\\begin{bmatrix}\n\\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\\n\\mathbf{A}_{21} & \\mathbf{A}_{22}\n\\end{bmatrix}\n\\end{align*}\n$$\n\nwhere $\\mathbf{A}_{11}$ is a square sub-matrix of $\\mathbf{A}$. The Schur complement of $\\mathbf{A}_{22}$ with respect to $\\mathbf{A}_{11}$ is defined as:\n\n$$\n\\begin{align*}\n\\mathbf{S} = \\mathbf{A}_{22} - \\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12}\n\\end{align*}\n$$\n\nThe Schur complement is useful in many areas of mathematics and engineering, including control theory, optimization, and signal processing.\n\nAs an example, consider the following system of linear equations:\n\n$$\n\\begin{align*}\n\\begin{bmatrix}\n\\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\\n\\mathbf{A}_{21} & \\mathbf{A}_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{x}_1 \\\\\n\\mathbf{x}_2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{b}_1 \\\\\n\\mathbf{b}_2\n\\end{bmatrix}\n\\end{align*}\n$$\n\nWe can eliminate the variables $\\mathbf{x}_2$ by solving for them in terms of $\\mathbf{x}_1$:\n\n$$\n\\begin{align*}\n\\mathbf{A}_{22} \\mathbf{x}_2 = \\mathbf{b}_2 - \\mathbf{A}_{21} \\mathbf{x}_1 \\\\\n\\mathbf{x}_2 = \\mathbf{A}_{22}^{-1} (\\mathbf{b}_2 - \\mathbf{A}_{21} \\mathbf{x}_1)\n\\end{align*}\n$$\n\nSubstituting this into the first equation, we obtain:\n\n$$\n\\begin{align*}\n\\mathbf{A}_{11} \\mathbf{x}_1 + \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} (\\mathbf{b}_2 - \\mathbf{A}_{21} \\mathbf{x}_1) = \\mathbf{b}_1\n\\end{align*}\n$$\n\nRearranging terms, we obtain an equation in the form $\\mathbf{B} \\mathbf{x}_1 = \\mathbf{c}$, where:\n\n$$\n\\begin{align*}\n\\mathbf{B} &= \\mathbf{A}_{11} - \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\mathbf{A}_{21} \\\\\n\\mathbf{c} &= \\mathbf{b}_1 - \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\mathbf{b}_2\n\\end{align*}\n$$\n\nThus, we have obtained a smaller system.\n\n## Inverse Matrices\n\n::: {#def-inverse}\n\nThe inverse of a square matrix $A$ of size $n$ is a matrix $A^{-1}$ such that the product of $A$ and $A^{-1}$ is the identity matrix $I_n$, i.e. $A \\times A^{-1} = I_n$. If such a matrix exists, then $A$ is said to be **invertible or non-singular**.\n\nThe inverse of a square matrix $\\mathbf{A}$ is denoted by $\\mathbf{A}^{-1}$ and is defined as the unique matrix that satisfies the following equation:\n$$\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n$$\n\nwhere $\\mathbf{I}$ is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\n:::\n\n### Properties about $\\mathbf A^{-1}$\n\n1. Existence: The inverse of the matrix $\\mathbf A$ exists if and only if elimination produces $n$ pivots, where $n$ is the number of rows (or columns) of $\\mathbf A$. Elimination solves $\\mathbf{Ax}=\\mathbf{b}$ without explicitly using the matrix \n  * Pivots are the non-zero elements that are selected during the elimination process and used as the basis for row operations. If $n$ pivots are obtained, then the matrix $\\mathbf A$ is said to be full rank, and its inverse exists. If fewer than $n$ pivots are obtained, then the matrix $\\mathbf A$ is singular, and its inverse does not exist\n1. Unique Inverse: the matrix $\\mathbf A$ cannot have two different inverses\n1. If $\\mathbf A$ is invertible, the one and only solution to  $\\mathbf{Ax}=\\mathbf{b}$ is $\\mathbf{x}=\\mathbf{A^{-1}b}$\n1. (Important) Suppose there is a nonzero vector $\\mathbf A$ such that $\\mathbf{Ax}=\\mathbf{0}$. Then $\\mathbf A$ cannot have an inverse. No matrix can bring  $\\mathbf 0$ back to $\\mathbf x$.\n  * If $\\mathbf A$ is invertible, then $\\mathbf{Ax}=\\mathbf{0}$ can only have the zero solution $\\mathbf{x}=\\mathbf{A^{-1}0=0}$.\n4. A 2 by 2 matrix is invertible if and only if $ad - bc$ is not zero:\n$$\nA = \\begin{bmatrix}\na & b \\\\\nc & d \\\\\n\\end{bmatrix}\n\\quad\nA^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix}\nd & -b \\\\\n-c & a \\\\\n\\end{bmatrix}\n$$\n5. A diagonal matrix has an inverse provided no diagonal entries are zero\n6. Inverse of Inverse: $(\\mathbf A^{-1})^{-1} = \\mathbf A$ \n7. Inverse of Product: If $\\mathbf{AB = I}$, where $\\mathbf I$ is the identity matrix, then $\\mathbf{B = A^{-1}}$.\n8. Scalar Multiple: If $c$ is a scalar, then $(c\\mathbf A)^{-1} = \\frac{1}{c}\\mathbf A^{-1}$ (if $c \\neq 0$).\n9. Product of Inverses: If $\\mathbf A^{-1}$ and $\\mathbf B^{-1}$ both exist, then $(\\mathbf{AB})^{-1} = \\mathbf B^{-1}\\mathbf A^{-1}$ (if $\\mathbf{AB}$ is invertible).\n10. Reverse Order: $(\\mathbf{ABC})^{-1}$=$\\mathbf C^{-1}$ $\\mathbf B^{-1}$ $\\mathbf A^{-1}$\n\n### Inverse by Gauss Jordan Elimination \n\nGiven a square matrix $\\mathbf A$, to find its inverse $\\mathbf A^{-1}$:\n\nStep 1: Augment the matrix $\\mathbf A$ with an identity matrix $\\mathbf I$ of the same size:\n$[\\mathbf A | \\mathbf I]$\n\nStep 2: Perform elementary row operations to transform the left half $\\mathbf A$ into the identity matrix $\\mathbf I$:\n- Interchange rows\n- Multiply a row by a scalar\n- Add a multiple of one row to another row\n\nStep 3: Apply the same row operations to the right half $\\mathbf I$ to obtain $\\mathbf A^{-1}$.\n\nStep 4: If $\\mathbf A$ is not invertible, the augmented matrix $[\\mathbf A | \\mathbf I]$ will not result in an identity matrix on the left half. \n\n#### Example \n\n$$\n\\mathbf A= \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \\\\\n\\end{bmatrix}\n$$\n\nStep 1: Augment the matrix $\\mathbf A$ with an identity matrix $\\mathbf I$ of the same size:\n$$\n[\\mathbf A | \\mathbf I] = \\begin{bmatrix}\n1 & 2 & 3 & | & 1 & 0 & 0 \\\\\n4 & 5 & 6 & | & 0 & 1 & 0 \\\\\n7 & 8 & 9 & | & 0 & 0 & 1 \n\\end{bmatrix}\n$$\n\nStep 2: Perform elementary row operations to transform the left half $\\mathbf A$ into the identity matrix $\\mathbf I$:\n- Interchange rows\n- Multiply a row by a scalar\n- Add a multiple of one row to another row\n\nStep 3: Apply the same row operations to the right half $\\mathbf I$ to obtain $\\mathbf A^{-1}$.\n\nStep 4: If $\\mathbf A$ is not invertible, the augmented matrix $[\\mathbf A | \\mathbf I]$ will not result in an identity matrix on the left half. In this case, $\\mathbf A$ does not have an inverse because $\\text{det}(\\mathbf A) = 1(5 \\cdot 9 - 6 \\cdot 8) - 2(4 \\cdot 9 - 6 \\cdot 7) + 3(4 \\cdot 8 - 5 \\cdot 7) = 0$\n\n::: {#def-elementary_matrix}\nAn elementary matrix is a square matrix obtained by performing a single elementary row operation on the identity matrix $\\mathbf{I}$.\n\nThere are three types of elementary row operations:\n\n* Swapping two rows: The elementary matrix obtained by swapping two rows of the identity matrix is denoted by $\\mathbf{E}_i$, where $i$ indicates the row numbers to be swapped. $\\mathbf{E}_1 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$ \n* Scaling a row by a nonzero scalar: The elementary matrix obtained by scaling a row of the identity matrix by a nonzero scalar $c$ is denoted by $\\mathbf{E}_i(c)$, where $i$ indicates the row number to be scaled and $c$ is the scalar. $\\mathbf{E}_2(2) = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2 \\end{bmatrix}$\n\n* Adding a multiple of one row to another row: The elementary matrix obtained by adding a multiple of one row of the identity matrix to another row is denoted by $\\mathbf{E}_{ij}(c)$, where $i$ indicates the row number from which a multiple is added, $j$ indicates the row number to which the multiple is added, and $c$ is the scalar multiple. $\\mathbf{E}_{12}(3) = \\begin{bmatrix} 1 & 3 \\\\ 0 & 1 \\end{bmatrix}$\n:::\n\n\n\n::: {#thm-elementary_matrix}\n**Elementary Matrix Theorem** \n\nLet $\\mathbf{A}$ be an invertible $n \\times n$ matrix. Then $\\mathbf{A}$ can be represented as the product of elementary matrices $\\mathbf{E}_1, \\mathbf{E}_2, \\ldots, \\mathbf{E}_k$, where each $\\mathbf{E}_i$ is an elementary matrix corresponding to a single elementary row operation.\n:::\n\nAny invertible matrix can be obtained by performing a sequence of elementary row operations on the identity matrix.\n\nLet $\\mathbf{A} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 5 \\end{bmatrix}$ be an invertible matrix. We can represent $\\mathbf{A}$ as the product of elementary matrices $\\mathbf{E}_1$ and $\\mathbf{E}_2$ as follows:\n\n$$\n\\begin{align*}\n\\mathbf{E}_1 = \\begin{bmatrix} 1 & 0 \\\\ -2 & 1 \\end{bmatrix}, \\quad\n\\mathbf{E}_2 = \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\end{align*}\n$$\n\nsuch that $\\mathbf{A} = \\mathbf{E}_2 \\mathbf{E}_1 \\mathbf{I}$.\n\nLet $\\mathbf{B} = \\begin{bmatrix} 3 & 2 & 1 \\\\ 1 & 1 & 1 \\\\ 2 & 3 & 4 \\end{bmatrix}$ be an invertible matrix. We can represent $\\mathbf{B}$ as the product of elementary matrices $\\mathbf{E}_1$, $\\mathbf{E}_2$, and $\\mathbf{E}_3$ as follows:\n\n$$\n\\begin{align*}\n\\mathbf{E}_1 = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad\n\\mathbf{E}_2 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}, \\quad\n\\mathbf{E}_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & \\frac{1}{2} \\end{bmatrix}\n\\end{align*}\n$$\n\nsuch that $\\mathbf{B} = \\mathbf{E}_3 \\mathbf{E}_2 \\mathbf{E}_1 \\mathbf{I}$.\n\n## Factorization\n\nFactorization is one of the computational backbones underlying data-science algorithms, including least squares model fitting and the matrix inverse. \n\n**Prerequisites**\n\n* systems of equations, \n* row reduction or elmentary row operations, and\n* Gaussian elimination\n* echelon matrices and \n* permutation matrices\n\n::: {#def-factorization}\nThe LU factorization, also known as the LU decomposition, is a matrix factorization method that expresses a given matrix $\\mathbf{A}$ as the product of two matrices: a lower triangular matrix $\\mathbf{L}$ and an upper triangular matrix $\\mathbf{U}$:\n$$\n\\mathbf{A} = \\mathbf{LU}\n$$\n\nwhere\n\n* $\\mathbf{A}$ is the given matrix,\n* $\\mathbf{L}$ is the lower triangular matrix with ones on the diagonal, and\n* $\\mathbf{U}$ is the upper triangular matrix.\n:::\n\nIt decomposes a given square matrix into the product of two matrices, a lower triangular matrix ($\\mathbf{L}$) and an upper triangular matrix ($\\mathbf{U}$).\n\nrow reduction can be expressed as $\\mathbf{L}^{-1}\\mathbf{A} = \\mathbf{U}$, where $\\mathbf{L}^{-1}$ contains the set of row manipulations that transforms the dense $\\mathbf{A}$ into upper-triangular (echelon) $\\mathbf{U}$. Because the echelon form is not unique, LU decomposition is not necessarily unique. Thus, there is an infinite pairing of lower- and upper-triangular matrices that could multiply to produce matrix $\\mathbf{A}$. \n\nHowever, adding the constraint that the diagonals of L equal 1 ensures that LU decomposition is unique for a full-rank square matrix $\\mathbf{A}$. \n\n* Efficient Solution of Linear Systems: Once a matrix is factorized into its LU form, it can be used to efficiently solve systems of linear equations. This is because solving a system of equations involving triangular matrices (such as L and U) is computationally more efficient compared to directly solving the original system of equations involving a general matrix.\n* Matrix Inversion: LU decomposition can also be used to efficiently calculate the inverse of a matrix. Once a matrix is factorized into its LU form, the inverse can be obtained by solving two triangular systems of equations, which is computationally more efficient compared to direct methods for matrix inversion.\n* Numerical Stability: LU decomposition can be used as a more numerically stable method for solving linear systems compared to direct methods, such as Gaussian elimination, because it avoids the issues of division by small or zero pivots.\n\n### Properties\n\n* The LU decomposition of a matrix is not unique. There can be multiple factorizations of the same matrix into different combinations of L and U matrices.\n* If the original matrix has a determinant of zero, it is singular and does not have a unique LU decomposition.\n* The LU decomposition can be used for square as well as rectangular matrices, although in the case of rectangular matrices, it may not be unique and may involve additional techniques such as pivoting.\n* The LU decomposition can be calculated using various algorithms, such as Gaussian elimination, Crout's method, and Doolittle's method, among others, with different advantages and disadvantages in terms of computational complexity and numerical stability.\n\n\n### Example \n\n$$\n\\begin{align*} \n\\mathbf{A} &= \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nn} \\\\\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\nl_{21} & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nl_{n1} & l_{n2} & \\cdots & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nu_{11} & u_{12} & \\cdots & u_{1n} \\\\\n0 & u_{22} & \\cdots & u_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & u_{nn} \\\\\n\\end{bmatrix}\n\\end{align*} \n$$\n\n$$\n\\begin{align*}\n\\mathbf{A} &= \\begin{bmatrix}\n2 & 3 & 1 \\\\\n4 & 9 & 5 \\\\\n6 & 15 & 9\n\\end{bmatrix} \\\\\n\\mathbf{A} &= \\mathbf{LU} \\\\\n\\mathbf{L} &= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n3 & 5 & 1\n\\end{bmatrix}, \\quad\n\\mathbf{U} = \\begin{bmatrix}\n2 & 3 & 1 \\\\\n0 & 3 & 3 \\\\\n0 & 0 & 2\n\\end{bmatrix}\n\\end{align*}\n$$\n\n```{python}\n#| eval: false\n\n\nA = np.array([[2, 3, 1],\n              [0, 4, -2],\n              [0, 0, 3]])\n           \n# its LU decomposition via scipy (please ignore the first output for now)\n_,L,U = scipy.linalg.lu(A)\n# print them out\nprint('A: ')\nprint(A), print(' ')\n\nprint('L: ')\nprint(L), print(' ')\n\nprint('U: ')\nprint(U), print(' ')\n\nprint('A - LU: ')\nprint(A - L@U) # should be zeros\n```\n\n## Transposes and Permutations\n\n### Transpose\n\n[Read the previous blog: the basic matrix operations](./02.basic_matrix.qmd)\n\n### Permutations\n\nA permutation is a reordering of a finite sequence of elements. In the context of solving $\\mathbf{Ax=b}$, a permutation can be used to reorder the rows of the augmented matrix $\\begin{bmatrix} \\mathbf{A} & \\mathbf{b} \\end{bmatrix}$ to simplify the process of finding the row echelon form.\n\nSome matrices do not easily transform into an upper-triangular form, which can be transformed into upper-triangular form through a permutation matrix. Consider the following matrix:\n\n$$\n\\begin{align*}\n&\\mathbf{A} = \\begin{bmatrix}\n3 & 2 & 1 \\\\\n0 & 0 & 5 \\\\\n0 & 7 & 2\n\\end{bmatrix} \\rightarrow\n\n\\mathbf{A}' = \\begin{bmatrix}\n3 & 2 & 1 \\\\\n0 & 7 & 2 \\\\\n0 & 0 & 5\n\\end{bmatrix} \\\\ \\\\\n&\\text{through a permutation matrix}\n\\end{align*}\n$$\n\n\n::: {#def-permutation}\n\nA permutation of a set of size $n$ is a bijective function $\\sigma: {1, 2, \\ldots, n} \\to {1, 2, \\ldots, n}$, which means that every element in the set is mapped to a unique element in the set and vice versa. A common way to represent a permutation is by using a permutation matrix, which is a square matrix with a single 1 in each row and each column and 0s elsewhere. The location of the 1 in each row represents the new position of that row after the permutation.\n:::\n\n#### Example\n\nConsider the permutation $\\sigma$ of the set ${1, 2, 3}$ defined by $\\sigma(1) = 2$, $\\sigma(2) = 3$, and $\\sigma(3) = 1$. The corresponding permutation matrix is:\n\n$$\n\\begin{align*}\n\\mathbf{P} = \\begin{bmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n\\end{align*}\n$$\n\nTo apply this permutation to the matrix $\\mathbf{A}$, we multiply $\\mathbf{A}$ on the left by $\\mathbf{P}$, i.e., $P\\mathbf{A}$. Similarly, to apply the permutation to the vector $\\mathbf{b}$, we multiply $\\mathbf{b}$ on the left by $\\mathbf{P}$, i.e., $P\\mathbf{b}$. This gives us the reordered augmented matrix:\n\n$$\n\\begin{align*}\n\\mathbf{P}\\begin{bmatrix} \\mathbf{A} & \\mathbf{b} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{P}\\mathbf{A} & \\mathbf{P}\\mathbf{b} \\end{bmatrix}\n\\end{align*}\n$$\n\nBy permuting the rows of the augmented matrix, we can obtain a row echelon form that is easier to work with and may lead to simpler solutions for $\\mathbf{x}$.\n\nThe full LU decomposition actually takes the following form:\n\n$$\n\\begin{align*}\n\\mathbf{P}\\mathbf{A} &= \\mathbf{L}\\mathbf{U} \\\\\n\\mathbf{A} &= \\mathbf{P}^{T}\\mathbf{L}\\mathbf{U} \\because \\mathbf{P} \\text{ is orthogonal, } \\mathbf{P}^{-1}=\\mathbf{P}^{T}\n\\end{align*}\n$$\n\nAll elements of a permutation matrix are either 0 or 1, and rows are swapped only once, each column has exactly one nonzero element (indeed, all permutation matrices are identity matrices with row swaps). Therefore, the dot\nproduct of any two columns is 0 while the dot product of a column with itself is 1, meaning $\\mathbf{P}^{T}\\mathbf{P}=\\mathbf{I}$\n\n\n#### Properties\n\nPermutations are used in solving linear systems of equations using Gaussian elimination, which is a common method for finding solutions to $\\mathbf{Ax=b}$. Here are some formal properties of permutations:\n\n* The LU decomposition with permutations is used in several applications, including computing the determinant and the matrix inverse.\n* A permutation matrix $\\mathbf{P}$ is a square matrix obtained by permuting the rows of the identity matrix $\\mathbf{I}$.\n* The product of two permutation matrices is also a permutation matrix.\n* The inverse of a permutation matrix is its transpose.\n* Permutation matrices can be used to interchange rows of a matrix.\n\n$$\n\\begin{align*}\nx_1 + 2x_2 + 3x_3 &= 7 \\\\\n4x_1 + 5x_2 + 6x_3 &= 8 \\\\\n7x_1 + 8x_2 + 9x_3 &= 10\n\\end{align*}\n$$\n\nThe augmented matrix for this system is:\n\n$$\n\\begin{align*}\n\\left[\\begin{array}{ccc|c}\n1 & 2 & 3 & 7 \\\\\n4 & 5 & 6 & 8 \\\\\n7 & 8 & 9 & 10\n\\end{array}\\right]\n\\end{align*}\n$$\n\nTo perform Gaussian elimination, we might want to interchange the first and second rows of the matrix. We can do this by multiplying the matrix by the permutation matrix:\n\n$$\n\\begin{align*}\n\\mathbf{P}_1 = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\end{align*}\n$$\n\nWe can verify that $\\mathbf{P}_1$ is a permutation matrix, and we can apply it to the augmented matrix:\n\n$$\n\\begin{align*}\n\\mathbf{P}_1\\left[\\begin{array}{ccc|c}\n1 & 2 & 3 & 7 \\\\\n4 & 5 & 6 & 8 \\\\\n7 & 8 & 9 & 10\n\\end{array}\\right] &= \\left[\\begin{array}{ccc|c}\n4 & 5 & 6 & 8 \\\\\n1 & 2 & 3 & 7 \\\\\n7 & 8 & 9 & 10\n\\end{array}\\right]\n\\end{align*}\n$$\n\nWe can continue with the Gaussian elimination process on the new augmented matrix. If we want to interchange the second and third rows, we can use the permutation matrix:\n\n$$\n\\begin{align*}\n\\mathbf{P}_2 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n\\end{align*}\n$$\n\nWe can verify that $\\mathbf{P}_2$ is a permutation matrix, and we can apply it to the augmented matrix:\n\n$$\n\\begin{align*}\n\\mathbf{P}_2\\left[\\begin{array}{ccc|c}\n4 & 5 & 6 & 8 \\\\\n1 & 2 & 3 & 7 \\\\\n7 & 8 & 9 & 10\n\\end{array}\\right] &= \\left[\\begin{array}{ccc|c}\n4 & 5 & 6 & 8 \\\\\n7 & 8 & 9 & 10 \\\\\n1 & 2 & 3 & 7\n\\end{array}\\right]\n\\end{align*}\n$$\n\nWe can now continue with Gaussian elimination on this new augmented matrix, which is in row echelon form.\n\n#### Example\n\nThe Scipy package actually returns $\\mathbf{A = PLU}$, which we could also write as $\\mathbf{P^{T}A = LU}$.\n\n```{python}\n#| eval: false\n\n\n# matrix sizes\nm = 4\nn = 6\n\nA = np.random.randn(m,n)\n\nP,L,U = scipy.linalg.lu(A)\n\n# show the matrices\nfig,axs = plt.subplots(1,5,figsize=(13,4))\n\naxs[0].imshow(A,vmin=-1,vmax=1)\naxs[0].set_title('A')\n\naxs[1].imshow(np.ones((m,n)),cmap='gray',vmin=-1,vmax=1)\naxs[1].text(n/2,m/2,'=',ha='center',fontsize=30,fontweight='bold')\n# axs[1].axis('off')\n\naxs[2].imshow(P.T,vmin=-1,vmax=1)\naxs[2].set_title(r'P')\n\naxs[3].imshow(L,vmin=-1,vmax=1)\naxs[3].set_title('L')\n\nh = axs[4].imshow(U,vmin=-1,vmax=1)\naxs[4].set_title('U')\n\nfor a in axs:\n  a.axis('off')\n  a.set_xlim([-.5,n-.5])\n  a.set_ylim([m-.5,-.5])\n\n\nfig.colorbar(h,ax=axs[-1],fraction=.05)\nplt.tight_layout()\nplt.savefig('Figure_10_01.png',dpi=300)\nplt.show()\n     \n```\n\n```{python}\n#| eval: false\n\n\n\nA = np.array([[1, 2, 3, 7],\n              [4, 5, 6, 8],\n              [7, 8, 9, 10]])\n           \n# its LU decomposition via scipy (please ignore the first output for now)\nP,L,U = scipy.linalg.lu(A)\n# print them out\nprint('A: ')\nprint(A), print(' ')\n\nprint('P: ')\nprint(P), print(' ')\n\n\nprint('L: ')\nprint(L), print(' ')\n\nprint('U: ')\nprint(U), print(' ')\n\nprint('A - LU: ')\nprint(A - P@L@U) # should be zeros\n```\n\n\n# 정리할 것 end\n\n\n\n\n\n\n\n### Matrix with Combinations of Vectors\n\nA matrix can be written as combinations of vectors. \n\nLet's see and apply the concept of **a matrix with combinations of vectors** to a linear combination of vectors. We can represent a linear combination of vectors as a matrix form with combinations of vectors. \n\nGiven vectors $\\mathbf{a}_1, \\mathbf{a}_2, \\dots, \\mathbf{a}_n \\in \\mathbb{R}^m$ and the vector $\\mathbf x$ whose entries are scalars $x_1, x_2, \\dots, x_n \\in \\mathbb{R}$, a linear combination of the vectors and scalars is written as:\n\n$$\nx_1\\mathbf a_1 +x_2\\mathbf a_2 +\\dots+x_n \\mathbf a_n\n$$\n\nThe combinations of the $\\mathbf{a}$ vectors is represented as a matrix that can be written as combinations of column vectors with the $\\mathbf{a}$ vectors \n\n$$\n\\begin{aligned}\n\\mathbf{A}_{m\\times n} &=\\begin{bmatrix} \\mathbf{a_1}&\\mathbf{a_2}& \\dots &\\mathbf{a_n} \\end{bmatrix} \\quad \\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n\\end{aligned}\n$$\n\nThus, the linear combination is simply written as:\n\n$$\n\\begin{aligned}\n&x_1\\mathbf a_1 +x_2\\mathbf a_2 +\\dots+x_n \\mathbf a_n \\\\\n&=\\begin{bmatrix} \\mathbf{a_1}&\\mathbf{a_2}& \\dots &\\mathbf{a_n} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\\\\n&=\\begin{bmatrix} \\mathbf{a_1}&\\mathbf{a_2}& \\dots &\\mathbf{a_n} \\end{bmatrix} \\mathbf x \\\\\n&=\\mathbf{Ax} \n\\end{aligned}\n$$\n\nwhere $\\mathbf{x} = \\begin{bmatrix} x_1 & x_2 & \\dots & x_n \\end{bmatrix}^T$ is a column vector of scalars.\n\n:::{.callout-tip}\nIt is a different example of a matrix with combinations of the product of vectors from that of matrix combination of vectors.\nA matrix can be represented as the outer product of column vectors and standard basis vectors, and their sum like the following example:\n\n$$\n\\mathbf A =\n\\begin{bmatrix} \n1&4&7\\\\\n2&5&8\\\\\n3&6&9\\\\\n\\end{bmatrix} =\n\\begin{bmatrix} \n1\\\\\n2\\\\\n3\\\\\n\\end{bmatrix}\\mathbf{e}_1^\\text{T}+\n\\begin{bmatrix} \n4\\\\\n5\\\\\n6\\\\\n\\end{bmatrix}\\mathbf{e}_2^\\text{T}+\n\\begin{bmatrix} \n7\\\\\n8\\\\\n9\\\\\n\\end{bmatrix}\\mathbf{e}_3^\\text{T}\n$$\nwhere $\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3$ are the standard basis vectors of $\\mathbb{R}^3$.\n:::\n\n### Matrix Multiplication with a Vector\n\nLet $\\mathbf{A}$ be an $m \\times n$ matrix and $\\mathbf{x}$ be a $n \\times 1$ column vector. The matrix-vector product $\\mathbf{Ax}$ is defined as:\n\n$$\n\\begin{aligned}\n\\mathbf{Ax}&=\\mathbf{b}\\\\\n\\mathbf{Ax}&=\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn} \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\na_{11}x_1 + a_{12}x_2 + \\dots + a_{1n}x_n \\\\\na_{21}x_1 + a_{22}x_2 + \\dots + a_{2n}x_n \\\\\n\\vdots  \\\\\na_{m1}x_1 + a_{m2}x_2 + \\dots + a_{mn}x_n \n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \n\\end{bmatrix}\\\\\n&=\\mathbf{b}\n\\end{aligned}\n$$\n\nIn other words, each entry of the resulting column vector is the dot product of the corresponding row of the matrix $\\mathbf{A}$ and the column vector $\\mathbf{x}$.\n\nThe matrix $\\mathbf{A}$ acts on the vector $\\mathbf{x}$. The result $\\mathbf{Ax}$ is a combination $\\mathbf{b}$ of the columns of $\\mathbf{A}$. The input is $\\mathbf{x}$ and the output is $\\mathbf{b}=\\mathbf{Ax}$\n\nFor example, let\n\n$$\n\\mathbf A =\n\\begin{bmatrix}\n2 & 1 \\\\\n3 & 4 \\\\\n1 & 2\n\\end{bmatrix} \\quad\n\\mathbf x=\n\\begin{bmatrix}\n  x_1\\\\x_2\n\\end{bmatrix} \\quad\n\\mathbf b=\n\\begin{bmatrix}\n  1\\\\-5\\\\-1\n\\end{bmatrix} \n$$\n\nThen we have\n\n$$\n\\begin{aligned}\n\\mathbf{Ax} &= \\mathbf{b} \\\\\n\\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix} \n\\begin{bmatrix}\n  x_1\\\\x_2\n\\end{bmatrix} \n&=\n\\begin{bmatrix}\n  1\\\\-1\n\\end{bmatrix} \\\\\n\\begin{bmatrix}\n2x_1 + x_2 \\\\\nx_1 + 2x_2\n\\end{bmatrix} \n&=\n\\begin{bmatrix}\n  1\\\\-1\n\\end{bmatrix} \n\\end{aligned}\n$$\n\nThe solution to the above system of equation is $x_1=1, x_2=-1$.\n\n### Linear Equations of a Matrix\n\nBefore the introduction of a matrix to the solution to $x_1\\mathbf a_1 +x_2\\mathbf a_2 +\\dots+x_n \\mathbf a_n=\\mathbf b$, we used the concept of a linear combination of $\\mathbf a$ vectors to find $\\mathbf{x}$ for $\\mathbf{b}$.\n\nBut after the introduction of a matrix, the viewpoint can changes:\n\n* **As-Is**: Compute the linear combination $x_1\\mathbf a_1 +x_2\\mathbf a_2 +\\dots+x_n \\mathbf a_n$ to find $\\mathbf b$.\n* **To-Be**: Which combination of $\\mathbf a$ vectors produces a particular vector $\\mathbf b$?\n\nAnwering the two questions means looking for $\\mathbf{x}$ for $\\mathbf{b}$. To do so, we have two ways:\n\n* to solve a system of linear equations and\n* to find an inverse of $\\mathbf{A}$\n\n#### Solving a System of Linear Equations\n\nGiven an $m \\times n$ matrix $\\mathbf A$ and an $n \\times 1$ vector $\\mathbf{x}$, the matrix-vector product $\\mathbf A\\mathbf{x}$ is a linear combination of the columns of $\\mathbf A$ with coefficients given by the entries of $\\mathbf{x}$. The system of linear equations represented by $\\mathbf A\\mathbf{x}=\\mathbf{b}$ has a unique solution if and only if the columns of $\\mathbf A$ are linearly independent.\n\nA system of linear equations can be written in matrix form as follows:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}=\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_m\n\\end{bmatrix}\n\\end{aligned}\n$$\n\nwhere $a_{ij}$ are the coefficients of the system, $x_i$ are the variables, and $b_j$ are the constants.\n\nWe call the above $\\mathbf{A}$ matrix is a **coefficient matrix** from the point of view of a system of lineqr equations and the above $\\mathbf{Ax}=\\mathbf{b}$ a **matrix equation**.\n\nHere's an example of a system of linear equations represented by a matrix:\n$$\n\\begin{align*}\n2x_1 + 3x_2 &= 8 \\\\\n4x_1 + 5x_2 &= 13\n\\end{align*}\n$$\n\nThis can be written as the matrix equation $A\\mathbf{x}=\\mathbf{b}$, where\n\n$$\n\\begin{equation}\n  \\mathbf{A} = \n    \\begin{bmatrix}\n    2 & 3\\\\\n    4 & 5\n    \\end{bmatrix} \\quad\n  \\mathbf{x} = \n    \\begin{bmatrix}\n    x_1\\\\\n    x_2\n    \\end{bmatrix}\\quad\n  \\mathbf{b} = \n    \\begin{bmatrix}\n    8\\\\\n    13\n    \\end{bmatrix}\n\\end{equation}\n$$\n\nThis can be written in matrix form as:\n$$\n\\begin{equation}\n\\begin{bmatrix}\n2 & 3 \\\\\n4 & -1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}=\n\\begin{bmatrix}\n5 \\\\\n2\n\\end{bmatrix}\n\\end{equation}\n$$\n\nConsider the following system of equations:\n\n$$\n\\begin{aligned}\n2x_1 + 3x_2 &= 5 \\\\\n4x_1 - x_2 &= 2\n\\end{aligned}\n$$\n\n#### Finding $\\mathbf{A}^{-1}$\n\nThe solution to this system can be found by computing the inverse of $\\mathbf A$ (if it exists) and multiplying both sides of the equation by it:\n\n$$\n\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b} \n$$\n\nIf $\\mathbf A$ does not have an inverse, then the system of equations may have either no solutions or infinitely many solutions.\n\n### Inverse\n\nThe inverse of a **square** matrix $\\mathbf A$ of size $n$ is a matrix $A^{-1}$ such that the product of $\\mathbf A$ and $\\mathbf A^{-1}$ is the identity matrix $\\mathbf I_n$, i.e. $\\mathbf A \\times \\mathbf A^{-1} = I_n$. If such a matrix exists, then $\\mathbf A$ is said to be invertible or non-singular.\n\nThe inverse of a square matrix $\\mathbf{A}$ is denoted by $\\mathbf{A}^{-1}$ and is defined as the unique matrix that satisfies the following equation:\n$$\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n$$\n\nwhere $\\mathbf{I}$ is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\n\n#### Examples\n\n**Example1** consider the $2\\times 2$ matrix \n$$\n\\mathbf{A} = \n\\begin{bmatrix} \n1 & 2 \\\\ \n3 & 4 \n\\end{bmatrix}\n$$\n\nThe inverse of $\\mathbf{A}$ or $\\mathbf{A}^{-1}$ is given by:\n\n$$\n\\mathbf{A}^{-1} = \n\\frac{1}{-2}\n\\begin{bmatrix} \n4 & -2 \\\\ \n-3 & 1 \n\\end{bmatrix} = \n\\begin{bmatrix} \n-2 & 1 \\\\ \n\\frac{3}{2} & -\\frac{1}{2} \n\\end{bmatrix}\n$$\n\nWe can verify that $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$ by computing:\n\n$$\n\\mathbf{A}\\mathbf{A}^{-1} = \n\\begin{bmatrix} \n1 & 2 \\\\ \n3 & 4 \n\\end{bmatrix}\n\\begin{bmatrix} \n-2 & 1 \\\\ \n\\frac{3}{2} & -\\frac{1}{2} \n\\end{bmatrix} = \n\\begin{bmatrix} \n1 & 0 \\\\ \n0 & 1 \n\\end{bmatrix} = \\mathbf{I}\n$$\n\n$$\n\\mathbf{A}^{-1}\\mathbf{A} = \n\\begin{bmatrix} -2 & 1 \\\\ \n\\frac{3}{2} & -\\frac{1}{2} \n\\end{bmatrix}\n\\begin{bmatrix} \n1 & 2 \\\\ \n3 & 4 \n\\end{bmatrix} = \n\\begin{bmatrix} \n1 & 0 \\\\ \n0 & 1 \n\\end{bmatrix} = \\mathbf{I}\n$$\n\n**Example2** consider the $3\\times 3$ matrix \n```{r}\n#| eval: false\n\n\nA<-matrix(sample(1:100,9,replace = TRUE),ncol=3,byrow = TRUE)\nprint('matrix A= ')\nprint(A)\nprint('inverse of A= ')\ninverse_A<-solve(A)\ninverse_A\nprint('AA^{-1}=A^{-1}A=I')\nA%*%inverse_A\ninverse_A%*%A\n```\n\n#### Properties\n\nThe inverse of a matrix is unique, if it exists.\n\n* uniqueness\n* If $\\mathbf{A}$ and $\\mathbf{B}$ are invertible matrices of the same size, then $(\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}$.\n* If $\\mathbf{A}$ is an invertible matrix, then $(\\mathbf{A}^{-1})^{-1} = \\mathbf{A}$.\n* If $\\mathbf{A}$ is an invertible matrix, then $\\text{det}(\\mathbf{A}) \\neq 0$.\n* If $\\mathbf{A}$ is an invertible matrix, then $\\mathbf{A}^T$ is invertible, and $(\\mathbf{A}^T)^{-1} = (\\mathbf{A}^{-1})^T$.\n* If $\\mathbf{A}$ is an invertible matrix, then for any scalar $c \\neq 0$, the matrix $c\\mathbf{A}$ is invertible, and $(c\\mathbf{A})^{-1} = \\frac{1}{c}\\mathbf{A}^{-1}$.\n* If $\\mathbf{A}$ is an invertible matrix, then for any positive integer $n$, the matrix $\\mathbf{A}^n$ is invertible, and $(\\mathbf{A}^n)^{-1} = (\\mathbf{A}^{-1})^n$.\n* If $\\mathbf{A}$ is an invertible matrix, then for any non-zero vector $\\mathbf{v}$, the matrix $\\mathbf{A}+\\mathbf{v}\\mathbf{v}^T/\\mathbf{v}^T\\mathbf{A}^{-1}\\mathbf{v}$ is invertible, and $(\\mathbf{A}+\\mathbf{v}\\mathbf{v}^T/\\mathbf{v}^T\\mathbf{A}^{-1}\\mathbf{v})^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1}\\mathbf{v}\\mathbf{v}^T\\mathbf{A}^{-1}}{1+\\mathbf{v}^T\\mathbf{A}^{-1}\\mathbf{v}}$.\n\n### Determinant\n\nLet $\\mathbf{A}$ be an $n \\times n$ square matrix. The determinant of $\\mathbf{A}$, denoted by $|\\mathbf{A}|$ or $\\det(\\mathbf{A})$, is a scalar value calculated as the sum of the products of the elements in any row or column of $\\mathbf{A}$ with their corresponding cofactors, that is,\n\n$$\n|\\mathbf{A}|=\\operatorname{det}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n$$\n\nwhere $a_{ij}$ is the element of $\\mathbf{A}$ in the $i$-th row and $j$-th column, and $C_{ij}$ is the cofactor (a signed minor matrix) of $a_{ij}$. The cofactor of $a_{ij}$ is a scalar value given by $(-1)^{i+j}$ times the determinant of the $(n-1) \\times (n-1)$ matrix obtained by deleting the $i$-th row and $j$-th column of $\\mathbf{A}$.\n\n#### Computing Cofactor\n\n**(Laplace Formula)**\nTo compute the cofactor of a matrix entry $a_{ij}$, you need to first remove the $i$-th row and $j$-th column of the matrix to obtain a $(n-1) \\times (n-1)$ submatrix. The cofactor of $a_{ij}$ is then defined as the product of $(-1)^{i+j}$ and the determinant of this submatrix.\n\n$\\mathbf A$ is an $n \\times n$ matrix and $\\mathbf A_{ij}$ denotes the submatrix obtained by deleting the $i$-th row and $j$-th column of $\\mathbf A$, then the cofactor of $a_{ij}$ is given by\n\n$$\nC_{ij}=\\operatorname{adj}(\\mathbf A)_{ij}=(-1)^{i+j}\\operatorname{det}(\\mathbf{A}_{ij})\n$$\n\nwhere $\\operatorname{adj}(\\mathbf A)$ is an adjugate matrix, which is the transpose of the matrix of cofactors of $\\mathbf A$\n\nOnce you have computed the cofactor of each entry, you can use them to compute the determinant of $\\mathbf{A}$ using the following formula:\n\n$$\n|\\mathbf{A}|=\\operatorname{det}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n$$\n\nwhere $i$ can be any fixed row or column of $\\mathbf{A}$. This formula is called **the Laplace expansion** of the determinant along the $i$-th row (or column). \n\n\nIt is defined as the sum of all possible products of $n$ elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. \n\nThe determinant is used to check whether a square matrix is invertible or not, which is a necessary step for computing an inverse of a square matrix. \n\n#### Examples\n\n**Example1** the inverse of $2\\times 2$ matrix.\n$$\n\\begin{align*}\n\\mathbf{A}&=\n\\begin{bmatrix}\na & b\\\\\nc & d\n\\end{bmatrix}\\\\\n\\mathbf{A}^{-1}&=\n\\frac{1}{\\operatorname{det}(\\mathbf A)}\\begin{bmatrix}\nd & -b\\\\\n-c & a\n\\end{bmatrix}\n\\end{align*}\n$$\n\nwhere $\\operatorname{det}(\\mathbf A)=ad-bc$\n\n$\\operatorname{det}(\\mathbf A)=ad-bc\\ne 0$ in order that $\\mathbf A$ is invertible.\n\n```{r}\n#| eval: false\n\n\nmat_A<-A[-3,-3]\nprint('2 by 2 matrix:')\nmat_A\npaste0('det(mat_A)=',det(mat_A))\nprint('the inverse of mat_A:')\ndet(mat_A)^-1*matrix(c(14,-65,-20,63),ncol=2,byrow = TRUE)\nprint('the inverse of solve(mat_A):')\nsolve(mat_A)\n```\n\n**Example2** the determinant of a $3 \\times 3$ matrix $\\mathbf{A}$, \n$$\n\\mathbf{A}=\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n$$\n\nis given by:\n\n$$\n|\\mathbf{A}| = \\operatorname{det}(\\mathbf{A})= a_{11} \n\\begin{vmatrix} \na_{22} & a_{23} \\\\ \na_{32} & a_{33} \n\\end{vmatrix} \n- a_{12} \n\\begin{vmatrix} \na_{21} & a_{23} \\\\ \na_{31} & a_{33} \n\\end{vmatrix} \n+ a_{13} \n\\begin{vmatrix} \na_{21} & a_{22} \\\\ \na_{31} & a_{32} \n\\end{vmatrix}\n$$\n\nThen, $\\mathbf{A}^{-1}$ is :\n\n$$\n\\begin{align*}\n\\mathbf{A}^{-1} &= \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\\\\\n&= \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\nC_{11} & C_{12} & C_{13} \\\\\nC_{21} & C_{22} & C_{23} \\\\\nC_{31} & C_{32} & C_{33} \n\\end{bmatrix}^{T} \\\\\n&= \\frac{1}{\\text{det}(\\mathbf{A})}\\operatorname{adj}(\\mathbf{A})\n\\end{align*}\n$$\n\nwhere $\\operatorname{det}(\\mathbf{A})$ is the determinant of $\\mathbf{A}$ and $\\operatorname{adj}(\\mathbf A)_{ij}=(-1)^{i+j}\\operatorname{det}(\\mathbf{A}_{ij})$.\n\nConsider the $3 \\times 3$ matrix $\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$. We can calculate the determinant of $\\mathbf{A}$ using any row or column. Let's use the first column:\n\n$$\n|\\mathbf{A}| = 1 \n\\begin{vmatrix} \n5 & 6 \\\\ \n8 & 9 \n\\end{vmatrix} \n- 4 \n\\begin{vmatrix} \n2 & 3\\\\ \n8 & 9\n\\end{vmatrix} \n+ 7 \n\\begin{vmatrix} \n2 & 5 \\\\ \n3 & 6 \n\\end{vmatrix} = 0\n$$\n\nTherefore, the determinant of $\\mathbf{A}$ is zero. So, $\\mathbf{A}$ is not invertible or singular, which means its inverse does not exist.\n\nConsider another $3 \\times 3$ matrix $\\mathbf A$:\n\n$$\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \n\\end{bmatrix}\n\\end{equation*}\n$$\nThen, $\\operatorname{det}(A)$ = -57, and the inverse of $\\mathbf{A}$ is:\n\n$$\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \n\\end{bmatrix}\n\\end{equation*}\n$$\n\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer's rule, Inverse formula, etc. \n\n```{r}\n#| eval: false\n\n\ncofactor<-function(mat,i,j){\n  mat_sub<-mat[-i,-j]\n  return((-1)^(i+j)*det(mat_sub))\n}\ncofactor_matrix<-function(mat){\n  n<-nrow(mat)\n  if(n!=ncol(mat)){\n    stop('the matrix is not a square matrix')\n  }\n  cofactors<-matrix(nrow=n,ncol=n)\n  for (i in 1:n){\n    for (j in 1:n){\n      cofactors[i,j]<-cofactor(mat,i,j)\n    }\n  }\n  return(cofactors)\n}\n\ncofactor_matrix2<-function(mat){\n  n<-nrow(mat)\n  if(n!=ncol(mat)){\n    stop('the matrix is not a square matrix')\n  }\n  coordinate_set<-expand.grid(1:n,1:n)\n  cofactors<-mapply(cofactor,\n  i=coordinate_set[,1],\n  j=coordinate_set[,2],\n  MoreArgs=list(mat=A))\n  return(\n  matrix(cofactors,ncol=n)\n  )\n}\n\n\nA=matrix(c(1,2,3,0,1,4,5,6,0), ncol=3, byrow=TRUE)\ncofactor(A,1,1)\ncofactor_matrix(A)\ncofactor_matrix2(A)\n(1/det(A))*t(cofactor_matrix2(A))\n(1/det(A))*t(cofactor_matrix(A))\nsolve(A)\n```\n\n#### Properties\n\n* If we multiply any row or column of a matrix by a scalar $c$, then the determinant of the resulting matrix is $c$ times the determinant of the original matrix.\n* If we interchange any two rows or columns of a matrix, then the determinant of the resulting matrix is the negative of the determinant of the original matrix.\n* If we add a multiple of one row or column to another row or column of a matrix, then the determinant of the resulting matrix is the same as the determinant of the original matrix.\n* If a matrix has a row or column of zeros, then its determinant is zero.\n* If a matrix is upper triangular or lower triangular, then its determinant is equal to the product of its diagonal entries.\n* If a matrix is a diagonal matrix, then its determinant is equal to the product of its diagonal entries.\n* If a matrix $\\mathbf{A}$ is invertible, then its determinant is nonzero, and $\\det(\\mathbf{A}^{-1}) = 1/\\det(\\mathbf{A})$.\n* If a matrix $\\mathbf{A}$ is invertible, then its determinant is nonzero, and $\\det(\\mathbf{A}^{T}) = \\det(\\mathbf{A})$.\n* If $\\mathbf{A}$ and $\\mathbf{B}$ are $n\\times n$ matrices, then $\\det(\\mathbf{AB}) = \\det(\\mathbf{A})\\det(\\mathbf{B})$.\n* $\\det(k\\mathbf{A}) = k^n\\det(\\mathbf{A})$ where $\\mathbf{A}$ is a $n\\times n$ matrix.\n* determinant and linear independence: if $\\det(\\mathbf{A}) \\ne 0$, the row vectors and columns vectors of $\\mathbf{A}$ are linear independent. Otherwise, they are linearly dependent.\n\n\n### Linear Equations\n\n#### Unique Solution\n\nA linear system of equations has a unique solution if and only if the coefficient matrix is non-singular (i.e., its determinant is nonzero).\n\n$$\n\\begin{aligned}\n2x_1 + 3x_2 &= 10 \\\\\n4x_1 + 5x_2 &= 20 \\\\\n\n\\mathbf{A} \\mathbf{x} &= \\mathbf{b} \\\\\n\\begin{bmatrix}\n2 & 3 \\\\\n4 & 5 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n10 \\\\\n20 \\\\\n\\end{bmatrix} \\\\\n\\text{det}(\\mathbf{A}) &= (a_{11}a_{22}) - (a_{12}a_{21}) \\text{ where } a_{ij} \\text{ is the element of } \\mathbf{A}\\\\\n\\text{det}(\\mathbf{A}) &= (2 \\cdot 5) - (3 \\cdot 4) = -2\\\\\n\\mathbf{A}^{-1} &= \\frac{1}{\\text{det}(\\mathbf{A})} \\begin{bmatrix}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{bmatrix} \\\\\n\n\\mathbf{A}^{-1} &=\n\n\\begin{bmatrix}\n-5/2 & 3/2 \\\\\n2 & -1 \\\\\n\\end{bmatrix} \\\\\n\\mathbf{A}^{-1}\\mathbf{A} \\mathbf{x} &= \\mathbf{A}^{-1}\\mathbf{b}\\\\\n\\mathbf{x} &= \\mathbf{A}^{-1}\\mathbf{b}\\\\\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}&=\n\\begin{bmatrix}\n-5/2 & 3/2 \\\\\n2 & -1 \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\n10 \\\\\n20 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n5 \\\\\n0 \\\\\n\\end{bmatrix} \n\\end{aligned}\n$$\n\nThe unique solution is $(x_1,x_2) = (5,0)$.\n\n#### Infinitely Many Solutions\n\nA linear system of equations has infinitely many solutions if and only if the system has at least one solution and the coefficient matrix is singular (i.e., its determinant is zero), and the system has more unknowns variables than linearly independent equations.\n\n$$\n\\begin{align*}\n\n2x_1 + 3x_2 &= 10 \\\\\n4x_1 + 6x_2 &= 20 \\\\\n\n\\mathbf{A} \\mathbf{x} &= \\mathbf{b} \\\\\n\\begin{bmatrix}\n2 & 3 \\\\\n4 & 6 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n&=\\begin{bmatrix}\n10 \\\\\n20 \\\\\n\\end{bmatrix} \\\\\n\\text{det}(\\mathbf{A}) &= (2 \\cdot 6) - (3 \\cdot 4) = 0\n\n\\end{align*}\n$$\n\nSince the determinant of $\\mathbf{A}$ is $0$, the matrix $\\mathbf{A}$ is singular and does not have an inverse.\nThis implies that the system of linear equations has infinitely many solutions, as the determinant of the coefficient matrix is $0$. \n\n#### No Solution\n\nA linear system of equations has no solution if the coefficient matrix is singular (i.e., its determinant is zero) and it has more linearly independent equations than unknowns variables.\n\n$$\n\\begin{align*}\n\n3x_1 + 4x_2 &= 10 \\\\\n6x_1 + 8x_2 &= 20 \\\\\n\n\\mathbf{A} \\mathbf{x} &= \\mathbf{b} \\\\\n\\begin{bmatrix}\n3 & 4 \\\\\n6 & 8 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n10 \\\\\n20 \\\\\n\\end{bmatrix} \\\\\n\\text{det}(\\mathbf{A}) &= (a_{11}a_{22}) - (a_{12}a_{21}) \\text{ where } a_{ij} \\text{ is the element of } \\mathbf{A}\\\\\n\\text{det}(\\mathbf{A}) &= (3 \\cdot 8) - (4 \\cdot 6) = 0\\\\\n\\end{align*}\n$$\n\nthe determinant of $\\mathbf A$ is $0$, which means that $\\mathbf A$ is a singular matrix and does not have an inverse.\n\n```{r}\n#| echo: false\n#| eval: false\n\n\nhttps://chbe241.github.io/Module-0-Introduction/MATH-152/Unique%20Solution,%20No%20Solution,%20or%20Infinite%20Solutions.html\n\n```\n\n### Linear Independence\n:::{#def-linear_independence}\n## Linear Independence\n\n#### Independence\n\nA set of vectors is said to be linearly independent if none of the vectors in the set can be written as a linear combination of the other vectors in the set. In other words, a set of vectors ${\\mathbf{v_1},\\mathbf{v_2},\\dots,\\mathbf{v_n}}$ is linearly independent if the only solution to the linear equation $c_1\\mathbf{v_1}+c_2\\mathbf{v_2}+\\dots+c_n\\mathbf{v_n}=0$ is $c_1=c_2=\\dots=c_n=0$, where $c_i$ is a scalar.\n\n#### Dependence\n\nA set of vectors is said to be linearly dependent if at least one vector in the set can be written as a linear combination of the other vectors in the set. In other words, a set of vectors $\\{v_1, v_2, \\ldots, v_n\\}$ is linearly dependent if there exist scalars $c_1, c_2, \\ldots, c_n$ (not all zero) such that $c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\ldots + c_n\\mathbf{v}_n = 0$\n\n:::{.callout-important}\n* Independent columns: $\\mathbf{Ax} = \\mathbf{0}$ has one solution. $\\mathbf{A}$ is an invertible matrix.\n* Dependent columns: $\\mathbf{Ax} = \\mathbf{0}$ has many solutions. $\\mathbf{A}$ is a singular matrix.\n:::\n\n:::\n\nIf a set of columns are dependent, $\\mathbf{v_i}$ can be represented as a linear combination of the other vectors:\n$$\n\\mathbf{v_i}=\\frac{-c_1}{c_i}\\mathbf{v_1}+\\frac{-c_2}{c_i}\\mathbf{v_2}+\\dots+\\frac{-c_n}{c_i}\\mathbf{v_n}\n$$\n\n#### Properties\n\n* **Non-Trivial Linear Combination**: a set of vectors is linearly independent if the only way to obtain the zero vector as a linear combination of these vectors is by setting all the scalar or coefficients to zero.\n* **No Redundancy**: If a vector can be expressed as a linear combination of other vectors in a set, then it is redundant and does not contribute to the linear independence of the set. Thus, a set of vectors is linearly independent if and only if no vector in the set can be written as a linear combination of the others.\n* **Minimal Set**: A linearly independent set of vectors is a minimal set, meaning that removing any vector from the set would result in a linearly dependent set.\n* **Size**: If a set of vectors contains more vectors than the dimension of the vector space in which they reside, then the set is necessarily linearly dependent. Conversely, if a set of vectors is linearly independent and contains exactly as many vectors as the dimension of the vector space, then it forms a basis for that vector space.\n\n\n#### Example\n\n**Example1** Let $\\mathbf{v}_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}, \\quad \\mathbf{v}_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}$. These two vectors are linearly independent because no linear combination of and $\\mathbf{v}_1,\\quad \\mathbf{v}_2$ can yield the zero vector, except when all the coefficients are zero. In other words, the only solution to the linear equation\n$$\nc_1\\mathbf v_1​ + c_2\\mathbf v_2 = 0\n$$  \n\nis $c_1=c_2=0$.\n\n**Example2** Computation example with dependent columns.\n\n```{r}\n#| eval: false\n\n\nv1 <- c(-4,2,0,6)\nv2 <- c(0,1,-2,-1)\nv3 <- c(-8,1,6,15)\n\ndependent_matrix<-cbind(v1,v2,v3)\n-2*v1+ 3*v2 + v3 # the coeffecients that makes the linear combiniation of the vectors zero are c(-2,3,1)\n\n# then, how many possible to such coefficients are there?\n\ndependent_function <- function(input_matrix, vector1){\n  result <- rowSums(sweep(input_matrix,2,vector1, '*'))\n  sqrt(sum(result^2))\n}\n\n# find a=c(a1,a2,a3) such a != c(0,0,0) \noptimal_a <- optim(rep(0,3),\n dependent_function,\n NULL, \n method='L-BFGS-B',\n input_matrix=dependent_matrix,\n lower=c(1,-5,-5), # c(a1_lower_bound,a2_lower_bound,a3_lower_bound)\n upper=c(5,5,5)) # c(a1_upper_bound,a2_upper_bound,a3_upper_bound)\n\noptimal_a$par\nrowSums(sweep(dependent_matrix,2,optimal_a$par, '*')) \n```\n\n**Example3** rectangular data with dependent columns\n\n```{r}\n#| eval: false\n\n\ntemperature_data <- tibble(celcius=c(5,36,30,-3,26,33),\nvelocity=c(2.35,11.5,8.62,5.37,11.8,12.20))%>%\nmutate(fahrenheit=celcius*9/5+32,after=celcius)\n\ntemperature_data\n\nThe temperature data include 3 columns: '`r names(temperature_data)`. Since the fahrenheit column can be represented as $\\frac{9}{5}\\text{celcius}+32$, the columns are not linearly independent. So, it is actually enought to reproduce the data with two columns: celcius and velocity. From the analysis perspective, the temperature data has a redundant column, `fahrenheit`.\n\n```\n\n\nSince the fahrenheit and celcius are linearly dependent, the scattor plot is represented as a line like the below. \n\n```{r}\n#| eval: false\n\n\nmosaic::plotPoints(fahrenheit~celcius,data=temperature_data)\n```\nIt indicates that to fit velocity on the temperature data, adding the fahrenheit columns does not influence and explain the variations of the velocity variable at all. \n\n```{r}\n#| eval: false\n\n\nresult1 <- lm(velocity~celcius, data= temperature_data)\nresult2 <- lm(velocity~celcius+fahrenheit, data= temperature_data)\n\nsum(result1$residuals^2)\nsum(result2$residuals^2)\n```\n\n**Example4**\n\n```{r}\n#| eval: false\n\n\na<-sample(seq(-10,10,by=0.01),3,replace=T)\n\nrowSums(sweep(dependent_matrix,2,a,'*')) # a[1]*v1+a[2]*v2+a[3]*v3 \n\nindependent_function<-function(matrix,vector1){\n  a<-sample(seq(-5,5,by=0.01),3,replace=T)\n  result <- rowSums(sweep(matrix,2,a,'*'))\n  result <- sum(result^2)%>%sqrt() # to get a zero vector of result, sum(result^2)%>%squrt() must be 0, which means the rowSums(sweep(matrix,2,a,'*')) must become c(0,0,0).\n  list(coef=a,vector_norm=result)\n}\n\nset.seed(2023)\n\nresult <-replicate(5000, independent_function(dependent_matrix), simplify=FALSE)\nlinear_combo_sum<-result %>% tibble() %>% unnest_auto(col=1) %>% unnest_wider(coef,names_sep='_')\nlinear_combo_sum%>%slice_min(abs(vector_norm), n=10) # ceffecient converges to zero as vector_norm goes to zero\n\nrowSums(sweep(dependent_matrix,2,c(0,0,0),'*'))^2%>%sum()%>%sqrt() # thus, we can know the scalars or coefficients that make vector_norm zero are c(0,0,0)\n```\n\n### Gram Matrix\n\nHow to check whether the columns vectors are linear independent or not? : Gram Matrix\n\n:::{#def-gram}\nThe Gram matrix, also known as the covariance matrix or the autocovariance matrix, is a matrix that captures the inner products between vectors in a set. It is defined as follows:\n\nGiven a set of vectors $\\{ \\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n \\}$, for the matrix, $\\mathbf{V}=\\begin{bmatrix} \\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n \\end{bmatrix}$, the Gram matrix $\\mathbf{G}=\\mathbf{V}^{T}\\mathbf{V}$ is an $n \\times n$ matrix whose entries are given by $\\mathbf{G}_{ij}= \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle$, where $\\langle , \\rangle$ represents the inner product between vectors.\n\n$$\n\\mathbf{G} = \\mathbf{V}^{T}\\mathbf{V} = \\begin{bmatrix}\n\\langle \\mathbf{v}_1, \\mathbf{v}_1 \\rangle & \\langle \\mathbf{v}_1, \\mathbf{v}_2 \\rangle & \\cdots & \\langle \\mathbf{v}_1, \\mathbf{v}_n \\rangle \\\\\n\\langle \\mathbf{v}_2, \\mathbf{v}_1 \\rangle & \\langle \\mathbf{v}_2, \\mathbf{v}_2 \\rangle & \\cdots & \\langle \\mathbf{v}_2, \\mathbf{v}_n \\rangle \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\langle \\mathbf{v}_n, \\mathbf{v}_1 \\rangle & \\langle \\mathbf{v}_n, \\mathbf{v}_2 \\rangle & \\cdots & \\langle \\mathbf{v}_n, \\mathbf{v}_n \\rangle \\\\\n\\end{bmatrix}\n$$\n:::\n\n#### Properties\n\n* Symmetry: The Gram matrix is always symmetric, meaning that $\\mathbf{G}_{ij}=\\mathbf{G}_{ji}$ for all $i$ and $j$\n* The sample covariance matrix is ​​expressed in the form of the Gram matrix of the centering matrix because the sample covariance matrix is computed using $\\mathbf{X}^{T}\\mathbf{X}$. \n* Covariance interpretation: In statistics, the Gram matrix can represent the covariance matrix of a set of random variables. Each entry ${G}_{ij}$ represents the covariance between the $i$ th and $j$ th random variables.\n* Positive semi-definiteness: The Gram matrix is positive semi-definite, which means that all of its eigenvalues are non-negative. This property ensures that the Gram matrix is non-negative and preserves the positive definite inner product structure.\n  $$\n  \\mathbf{x}^{T}\\mathbf{G}\\mathbf{x} = \\mathbf{x}^{T}\\mathbf{V}^{T}\\mathbf{V}\\mathbf{x} \\ge 0 \\text{ for all }  \\mathbf{x} \n  $$\n* Inner products: The entries of the Gram matrix represent the inner products between vectors.\n* **Linear independence**: The column vectors in $\\mathbf{V}$ are linearly independent if and only if the Gram matrix is positive definite, meaning that all of its eigenvalues are strictly positive. \n  $$\n  \\mathbf{x}^{T}\\mathbf{G}\\mathbf{x} = \\mathbf{x}^{T}\\mathbf{V}^{T}\\mathbf{V}\\mathbf{x} > 0 \\text{ for all } \\mathbf{x} \\ne \\mathbf{0}\n  $$\n  * This property allows to determine linear independence of the column vectors of $\\mathbf{V}$ by examining the eigenvalues of the Gram matrix.\n* When $\\mathbf{V}$ has independent columns, its Gram matrix is invertible. When $\\mathbf{V}$ has dependent columns, its Gram matrix is not invertible because the trace of $\\mathbf{V}$ can be computed using the product of the eigenvalues. If there is at least one zero in the list of the eigenvalues, the product becomes zero and the determinant of the gram matrix is zero, which results in the gram marix corresponding to $\\mathbf{V}$ with the dependent columns is not invertible\n\n:::{.callout-tip}\n## Multicolinearity Problem\n\n$$\n\\hat{\\beta}=(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\n$$\n\nwhere $\\mathbf{X}^{T}\\mathbf{X}$ is a Gram matrix.\n\nIf there are linearly dependent columns, $\\operatorname{det}(\\mathbf{X}^{T}\\mathbf{X})$ is zero. So, there is no inverse of $\\mathbf{X}^{T}\\mathbf{X}$. If a scatter plot of two columns show not a perfect line but an almost perfect line, at least one of the eigenvalues is close to zero. Then, their product become so small that $\\frac{1}{\\operatorname{det}(\\mathbf{X}^{T}\\mathbf{X})}$ becomes very large and unstably computed, which causes unreliable values of $\\hat{\\beta}$ to be computed. The meaning of 'unstable' is that the estimates of $\\hat{\\beta}$ are very large or very small or greatly fluctuated to such an extent that it does not make sense whenever the number of the columns or variables change in the regression model.\n\n:::\n\n#### Example\n\n**Example1** Simple Gram Matrix Example\n```{r}\n#| eval: false\n\n\n# Define the set of vectors\nv1 <- c(1, 2, 3)\nv2 <- c(4, 5, 6)\nv3 <- c(7, 8, 9)\n\n# Compute the Gram matrix\ncolumn_vector_matrix <- t(matrix(c(\n  crossprod(v1), crossprod(v1, v2), crossprod(v1, v3),\n  crossprod(v2, v1), crossprod(v2), crossprod(v2, v3),\n  crossprod(v3, v1), crossprod(v3, v2), crossprod(v3)\n), nrow = 3))\n\n# Print the Gram matrix\nprint(column_vector_matrix)\n```\n\n**Example2** Check if $\\mathbf{V}$ is linearly independent using the linear independence property.\n```{r}\n#| eval: false\n\n\ngram_matrix <- t(column_vector_matrix) %*% column_vector_matrix\neigen(gram_matrix)$values %>% round(1)\n```\n\nIf the result eigenvalues of eigendecomposition of gram matrix are all positive, the columns of $\\mathbf{V}$ are linearly independent. The number of eigen values $>0$ is the number of the eigenvectors.\n\n### Transpose\n\nThe transpose of an $m \\times n$ matrix $\\mathbf{A}$, denoted by $\\mathbf{A}^T$, is the $n \\times m$ matrix obtained by interchanging the rows and columns of $\\mathbf{A}$. Formally, if $\\mathbf{A} = [a_{ij}]$ is an $m \\times n$ matrix, then its transpose $\\mathbf{A}^T = [b_{ij}]$ is an $n x m$ matrix where $b_{ij}$ = $a_{ji}$ for all $i$ and $j$. In other words, the element in the $i$ th row and $j$ th column of $A^T$ is equal to the element in the $j$ th row and ith column of $\\mathbf{A}$.\n\nGiven an $m \\times n$ matrix $\\mathbf{A}$, its transpose $\\mathbf{A}^T$ is an $n \\times m$ matrix defined by:\n$$\n(\\mathbf{A^T})_{i,j} = \\mathbf{A}_{j,i}\n$$\n\n* When $\\mathbf{A}$ is transposed, diagnoal entries($a_{ii}$) do not change but off-diagnoal elements($a_{ij} \\; i \\neq j$) change.\n* A column vector is tranposed into a row vector, and vice versa.\n* symmetric matrix: $\\mathbf{A} = \\mathbf{A}^T$\n\nExample:\n\nLet $\\mathbf A$ be the matrix\n$$\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n$$\nThe transpose of $\\mathbf A$, denoted by $\\mathbf A^T$, is the matrix\n$$\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n$$\n\n#### Properties\n\nLet $\\mathbf A$ be an $m \\times n$ matrix and $\\mathbf B$ be an $n \\times p$ matrix, and let $c$ be a scalar. Then:\n\n* $(\\mathbf{A}^T)^T = \\mathbf{A}$\n* $(\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T$\n* $(c\\mathbf{A})^T = c\\mathbf{A}^T$\n* $(\\mathbf{AB})^T = \\mathbf{B}^T \\mathbf{A}^T$\n* $(\\mathbf{Ax})^T = \\mathbf{x}^T \\mathbf{A}^T$\n  * $(\\mathbf{Ax})$ combies the columns of $\\mathbf A$ while $\\mathbf{x}^T \\mathbf{A}^T$ combines the rows of $\\mathbf{A}^T$\n* $(\\mathbf{ABC})^T = \\mathbf{C}^T\\mathbf{B}^T \\mathbf{A}^T$ , (cyclic properties)\n  * If $\\mathbf A=(\\mathbf{LDU})^T$ then,$\\mathbf{A}^T=\\mathbf{U}^T\\mathbf{D}^T \\mathbf{L}^T$ where $\\mathbf{D} = \\mathbf{D}^T$\n* $(\\mathbf{A}^{-1})^T = (\\mathbf{A}^{T})^{-1}$\n  * $\\mathbf{A}^{T}(\\mathbf{A}^{-1})^T = \\mathbf{A}^{T}(\\mathbf{A}^{T})^{-1}=\\mathbf{I}$\n\n### Trace\n\nThe trace of a square matrix $\\mathbf{A}$, denoted by $\\mathrm{tr}(\\mathbf{A})$, is defined as the sum of the diagonal elements of $\\mathbf{A}$. In other words, if $\\mathbf{A}$ is an $n \\times n$ matrix, then its trace is given by:\n\n$$\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ii}\n$$\n\nwhere $a_{ii}$ denotes the $i$ th diagonal element of $\\mathbf{A}$.\n\nFor example, let\n$$\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n$$\n\nThen, the trace of $\\mathbf{A}$ is $\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11$\n\n#### Properties\n\n* $\\operatorname{tr}(k\\mathbf A) =k \\operatorname{tr}(\\mathbf A)$\n* $\\operatorname{tr}(\\mathbf A+\\mathbf B) =\\operatorname{tr}(\\mathbf A)+\\operatorname{tr}(\\mathbf B)$\n* $\\operatorname{tr}(\\mathbf{AB}) =\\operatorname{tr}(\\mathbf{BA})$\n* Cyclic Property: $\\operatorname{tr}(\\mathbf{ABC}) =\\operatorname{tr}(\\mathbf{BCA})=\\operatorname{tr}(\\mathbf{CAB})\\ne \\operatorname{tr}(\\mathbf{CBA})$\n* Distributive Law: $\\operatorname{tr}(\\mathbf{A(B+C)}) =\\operatorname{tr}(\\mathbf{AB})+\\operatorname{tr}(\\mathbf{AC})$\n* $\\operatorname{tr}(\\mathbf{A}^{T}) =\\operatorname{tr}(\\mathbf{A})$\n\n```{r}\n#| eval: false\n\n\ntr<-function(mat){\n  return(sum(diag(mat)))\n}\n\nA<-matrix(sample(1:100,9,replace=TRUE),ncol=3,byrow=TRUE)\nB<-matrix(sample(1:100,9,replace=TRUE),ncol=3,byrow=TRUE)\nC<-matrix(sample(1:100,9,replace=TRUE),ncol=3,byrow=TRUE)\nk<-3\npaste0('k=', k)\nprint('matrix A:')\nprint(A)\nprint('matrix B:')\nprint(B)\nprint('matrix C:')\nprint(C)\nprint('Diagonal Elements of the matrix A:')\nprint(diag(A))\n\npaste0('trace(A) or tr(A):',tr(A))\npaste('tr(kA)=ktr(A)',tr(k*A),k*tr(A),sep=', ')\npaste('tr(A+B)=tr(A)+tr(B)',tr(A+B),tr(A)+tr(B),sep=', ')\npaste('tr(A+B)=tr(B+A)',tr(A+B),tr(B+A),sep=', ')\npaste('tr(AB)=tr(BA)',tr(A%*%B),tr(B%*%A),sep=', ')\npaste('tr(ABC)=tr(BCA)=tr(CAB)!=tr(CBA)',tr(A%*%B%*%C),tr(B%*%C%*%A),tr(C%*%A%*%B),tr(C%*%B%*%A),sep=', ')\npaste('tr(A(B+C))=tr(AB)+tr(AC)',tr(A%*%(B+C)),tr(A%*%B)+tr(A%*%C),sep=', ')\npaste('tr(A^T)=tr(A)',tr(t(A)),tr(A),sep=', ')\n\n```\n\n\n### Rank\n\n\n:::{#def-rank}\nThe rank of a matrix is the maximum number of linearly independent rows or columns in the matrix. It is denoted by $\\text{rank}(\\mathbf{A})$.\n:::\n\nThe rank of matrix $\\mathbf{A}$ is the dimension of the space that matrix $\\mathbf{A}$ can create.\nSince the variables of real high-dimensional data have high correlation with each other, the number of variables and the number of ranks can be different.\nRank is one of the important measures of the amount of information a matrix contains. In practice, the concept of rank is widely used in various computational techniques in data analysis.\n\n:::{.callout-tip}\n1. Determinant: The determinant of a square matrix provides information about the scaling factor of the linear transformation represented by the matrix. It can indicate whether the transformation stretches or compresses space and whether it changes orientation.\n2. Trace: It provides information about the sum of the eigenvalues of the matrix and is often used in various matrix decompositions and calculations.\n3. Eigenvalues and Eigenvectors: The eigenvalues and eigenvectors of a matrix represent the characteristic values and corresponding vectors that describe the matrix's behavior under linear transformations. They can provide insights into the stretching, shearing, or rotating effects of the matrix.\n4. Singular Value Decomposition (SVD): SVD decomposes a matrix into three components: U, Σ, and V. The singular values in Σ represent the scaling factors, and the matrices U and V describe the transformations. The singular values indicate the amount of information carried by each dimension of the matrix.\n5. Principal Component Analysis (PCA): It identifies the principal components, which are linear combinations of the original variables that capture the maximum variance in the data.\n\n:::\nKnowing the rank can help us in various data analysis tasks, such as:\n\n* Feature selection: We can prioritize the selection of the most informative variables by considering their contribution to the rank. Variables with a higher rank tend to carry more unique information and are less likely to be redundant.\n* Dimensionality reduction: If we find that the rank is lower than the total number of variables, we can use dimensionality reduction techniques, such as Principal Component Analysis (PCA), to transform the data into a lower-dimensional space while preserving the most important information.\n* Linear regression: The rank of the design matrix is essential in linear regression modeling. If the rank is less than the number of variables, it indicates the presence of multicollinearity, which can affect the model's stability and interpretation.\n\n#### Properties\n\n1. The rank of a matrix is always less than or equal to the minimum of the number of rows and the number of columns:\n\n$$\n\\text{rank}(\\mathbf{A}) \\leq \\min(\\text{rows}(\\mathbf{A}), \\text{cols}(\\mathbf{A}))\n$$\n\n$\\mathbf{A}$ is said to be a full rank matrix if $\\text{rank}(\\mathbf{A}) = \\min(\\text{rows}(\\mathbf{A}), \\text{cols}(\\mathbf{A}))$. \n\nIt is said that the vectors are linearly indpendent if $\\mathbf{A}$ is said to be a full rank matrix.\n\n```{r}\n#| eval: false\n\n\nlibrary(Matrix)\nA <- matrix(1:6,nrow=2)\nrankMatrix(A)\n```\n\n1. The rank of a matrix is equal to the rank of its transpose\n\n$$\n\\text{rank}(\\mathbf{A}) =\\text{rank}(\\mathbf{A}^T)\n$$\n\n```{r}\n#| eval: false\n\n\nrank_A <-rankMatrix(A)\nrank_A_transpose <-rankMatrix(t(A))\nrank_A == rank_A_transpose\n```\n\n1. The determinant of a square matrix is zero if and only if the rank of the matrix is less than the number of columns (or rows) in the matrix.\n$$\n|\\mathbf{A_{n \\times n}}|=0 \\text{ if and only if } \\text{rank}(\\mathbf{A}) < n\n$$\n\n```{r}\nA<-matrix(1:9,nrow=3)\ndet_A <- det(A)\nrank_A <- rankMatrix(A)\nn<-nrow(A)\n\nrank_A < n\ndet_A == 0\n```\n\n\n1. For $\\mathbf{C}_{m\\times n}$, invertible $\\mathbf{A}_{m\\times m}$, and invertible $\\mathbf{B}_{n\\times n}$, $\\text{rank}(\\mathbf{C})$ is preserved. This property highlights that the rank of a matrix remains unchanged when multiplied by an invertible matrix on either side.\n\n$$\n\\text{rank}(\\mathbf{C}) =\\text{rank}(\\mathbf{AC})=\\text{rank}(\\mathbf{CB}) =\\text{rank}(\\mathbf{ACB})\n$$\n\n```{r}\n#| eval: false\n\n\nA<-matrix(c(2,0,0,1), nrow=2)\nB<-matrix(c(1,4,3,3,1,0,0,0,1), nrow=3)\nC<-matrix(c(1:3,2,4,6), nrow=2, byrow=T)\n\ndet(A)\ndet(B)\n\nrank_C<-rankMatrix(C)\nrank_AC<-rankMatrix(A%*%C)\nrank_CB<-rankMatrix(C%*%B)\nrank_ACB<-rankMatrix(A%*%C%*%B)\n\n(rank_C==rank_AC)&(rank_C==rank_CB)&(rank_C==rank_ACB)\n\n```\n\n1. For $\\text{rank}(\\mathbf{A}_{m\\times n}) =r$, there exists $\\mathbf{P}_{m\\times m}$ and $\\mathbf{Q}_{n\\times n}$ such that\n\n$$\n\\mathbf{PAQ}= \\begin{bmatrix} \\mathbf{I_{r\\times r}} & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{0} \\end{bmatrix}\n$$\n\nThis property, known as the rank factorization theorem, states that any matrix with a given rank $r$ can be transformed into a canonical form using suitable row and column operations represented by the invertible matrices $\\mathbf{P}$ and $\\mathbf{Q}$, respectively. In this canonical form, the non-zero elements are confined to the top-left $r \\times r$ submatrix, while the remaining elements are all zeros. ( Canonical forms are often used to simplify the analysis and study of matrices by providing a standard or standardized form that exhibits specific characteristics.)\n\n:::{.callout-tip}\n## Generalized Inverse (Pseudoinverse)\n\nFor a given matrix $\\mathbf{A}$, its generalized inverse is denoted as $\\mathbf{A}^\\dagger$, and it satisfies the following properties:\n\n* $\\mathbf{A}\\mathbf{A}^\\dagger\\mathbf{A} = \\mathbf{A}$: The product of $\\mathbf{A}$ with its generalized inverse from the left is equal to $\\mathbf{A}$ itself.\n* $\\mathbf{A}^\\dagger\\mathbf{A}\\mathbf{A}^\\dagger = \\mathbf{A}^\\dagger$: The product of $\\mathbf{A}^\\dagger$ with $\\mathbf{A}$ from the right is equal to $\\mathbf{A}^\\dagger$ itself.\n\nA generalized inverse, also known as a pseudoinverse, is a concept in linear algebra that extends the notion of inverse to matrices that may not have a unique or traditional inverse. It is used to solve systems of equations involving matrices that are not necessarily square or invertible. It provides a way to \"undo\" the effect of a non-invertible or underdetermined matrix.\n\nThere are several methods to compute the generalized inverse of a matrix, such as the Moore-Penrose pseudoinverse, the Drazin inverse, and the singular value decomposition (SVD) method.\n\n:::\n\n1. For any $\\mathbf{A}$, the rank of a gram matrix $\\mathbf{A}^T\\mathbf{A}$ is the same as the rank of $\\mathbf{A}$\n\n$$\n\\text{rank}(\\mathbf{A}^T\\mathbf{A}) =\\text{rank}(\\mathbf{A})\n$$\n\n```{r}\n#| eval: false\n\n\nA<-matrix(1:6,nrow=2)\nrankMatrix(A)\n```\n\nWhen performing hypothesis tests or fitting models, the $\\chi^2$ distribution is commonly used to assess the goodness of fit or test the significance of certain parameters. The test statistic follows a $\\chi^2$ distribution under the null hypothesis.\n\nIn the case of linear regression, for example, the degrees of freedom associated with the $\\chi^2$ distribution can be related to the difference between the number of observations and the rank of the design matrix. Specifically, if you have a linear regression model with $n$ observations and $p$ predictors, the degrees of freedom for the $\\chi^2$ distribution would be $n - p$. Here, $p$ represents the rank of the design matrix.\n\nThis relationship arises because the rank of the design matrix determines the effective number of parameters or independent components in the model. Subtracting the rank from the total number of observations yields the degrees of freedom associated with the $\\chi^2$ distribution in this specific context.\n\n1. The rank of a matrix is equal to the number of non-zero singular values in its singular value decomposition (SVD):\n\n$$\n\\text{rank}(\\mathbf{A}) = \\text{number of non-zero singular values}\n$$\n\n1. The rank of a matrix is equal to the dimension of the column space (or row space) of the matrix: \n\n$$\n\\text{rank}(\\mathbf{A}) = \\text{dimension of column space} = \\text{dimension of row space}\n$$\n\n1. For two matrices $\\mathbf{A}$ and $\\mathbf{B}$ of compatible sizes, the rank of their matrix product is at most the minimum of their individual ranks:\n\n$$\n\\text{rank}(\\mathbf{AB}) \\leq \\min(\\text{rank}(\\mathbf{A}), \\text{rank}(\\mathbf{B}))\\\\\n$$\n\n```{r}\n#| eval: false\n\n\nA<-matrix(c(1,2,3,6,5,10),nrow=2)\nB<-matrix(c(7,8,9,10,11,12),ncol=2)\n\nrank_A<-rankMatrix(A)\nrank_B<-rankMatrix(B)\nrank_AB<-rankMatrix(A%*%B)\nrank_AB<=min(rank_A,rank_B)\n```\n\n$$\n\\text{rank}(\\mathbf{A+B}) \\leq \\text{rank}(\\mathbf{A}) + \\text{rank}(\\mathbf{B})\\\\\n$$\n\n```{r}\n#| eval: false\n\n\nA<-matrix(c(1,2,3,6,5,10),nrow=2)\nB<-matrix(c(7,8,9,10,11,12),nrow=2)\n\nrank_A<-rankMatrix(A)\nrank_B<-rankMatrix(B)\nrank_A_plus_B<-rankMatrix(A+B)\nrank_A_plus_B<=rank_A+rank_B\n```\n\n\n1. If a matrix is invertible (i.e., has full rank), its rank is equal to its number of rows (or columns):\n\n$$\n\\text{rank}(\\mathbf{A}) = \\text{rows}(\\mathbf{A}) = \\text{cols}(\\mathbf{A})\n$$\n\n\n#### Examples\n\n**Example1**\nConsider the following matrix:\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \n\\end{bmatrix}\n$$\n\n\nTo find the rank of $\\mathbf{A}$, we can row reduce the matrix to its echelon form or perform other operations to determine the linearly independent rows or columns. Let's compute the rank of $\\mathbf{A}$:\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \\\\\n\\end{bmatrix} \\sim \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & -6 & -12 \\\\\n\\end{bmatrix} \\sim \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix} \\sim \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}\n$$\n\nApplying the row operations $(R2\\leftarrow R2-4R1)$ and $(R3\\leftarrow R3-7R1)$, we get:\n\n$$\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix} \\sim \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}\n$$\n\nFurther applying the row operation $(R3\\leftarrow R3-2R2)$, we obtain:\n\n$$\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix} \\sim \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}\n$$\n\nWe have reached the echelon form of $\\mathbf{A}$. Notice that there are two linearly independent rows (row 1 and row 2) and one zero row. Therefore, the rank of $\\mathbf{A}$ is 2.\n\nHence, the rank of $\\mathbf{A}$ is 2.\n\n```{python}\n#| eval: false\n\n\nimport numpy as np\n\n# Example data matrix\ndata = np.array([[1, 2, 3, 4, 5],\n                 [2, 4, 6, 8, 10],\n                 [3, 6, 9, 12, 15]])\n\n# Compute the rank of the data matrix\nrank = np.linalg.matrix_rank(data)\n\nprint(\"Rank of the data matrix:\", rank)\n\n```\n\n## Expectation\n\n**Expectation**\n\nThe expectation (also known as the expected value) of a random variable is a measure of its central tendency. For a discrete random variable $X$ with probability mass function $P(X)$, the expectation is defined as:\n\n$$\nE(X) = \\sum_{x} x \\cdot P(X = x)\n$$\n\nwhere $x$ represents the possible values of $X$.\n\nFor a continuous random variable $Y$ with probability density function $f(Y)$, the expectation is defined as:\n\n$$\nE(Y) = \\int_{-\\infty}^{\\infty} y \\cdot f(Y = y) \\, dy\n$$\n\nwhere $y$ represents the possible values of $Y$.\n\n\n### Properties\n\nFor constants $a$ and $b$, matrix $\\mathbf{A}_{m\\times n}$, vector $\\mathbf{b}_{m \\times 1}$, random variables $X$ and $Y$, and random vector $\\mathbf{x}_{m\\times 1}$ and $\\mathbf{y}_{m\\times 1}$\n\n**Linearity of Expectation**\n\nThe expectation is a linear operator, which means it follows the rules of linearity. \n\n$$\nE(aX + bY) = aE(X) + bE(Y)\n$$\n\n$$\nE(\\mathbf{Ax} + \\mathbf{b}) = \\mathbf{A}_{m \\times n}E(\\mathbf{x}_{m\\times 1})_{n\\times 1} + \\mathbf{b}_{m\\times 1}\n$$\n\n\n**Example:**\n\nLet's consider two random variables $X$ and $Y$ with the following probability distributions:\n\n$$\nP(X = 1) = \\frac{1}{3}, \\quad P(X = 2) = \\frac{1}{3}, \\quad P(X = 3) = \\frac{1}{3}\n$$\n\n$$\nP(Y = 0) = \\frac{1}{2}, \\quad P(Y = 1) = \\frac{1}{2}\n$$\n\nWe want to find $E(2X + 3Y)$, the expectation of $2X + 3Y$.\n\nUsing the linearity property of expectation, we can calculate it as:\n\n$$\n\\begin{aligned}\n  E(2X + 3Y) &= 2E(X) + 3E(Y) \\\\\n  E(X) &= 1 \\cdot \\frac{1}{3} + 2 \\cdot \\frac{1}{3} + 3 \\cdot \\frac{1}{3} = 2 \\\\\n  E(Y) &= 0 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2} = \\frac{1}{2} \\\\\n  E(2X + 3Y) &= 2 \\cdot 2 + 3 \\cdot \\frac{1}{2} = 5\n\\end{aligned}\n$$\n\n### Random Vector\n\nLet $X,Y$ be $X_1,X_2$, independent with each other, and $\\mathbf{x}=\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}$.\n\n$$\n\\begin{aligned}\n  E(\\mathbf{x}) &= \\mathbf{\\mu} = \\begin{bmatrix} E( X_1) \\\\ E(X_2) \\end{bmatrix}= \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\\\\n  E(2X+3Y) &= E(\\mathbf{A}\\mathbf{x}+\\mathbf{b}) = E(\\begin{bmatrix} 2 & 3 \\end{bmatrix}\\mathbf{x} + \\mathbf{0}) \\\\\n  &= \\begin{bmatrix} 2 & 3 \\end{bmatrix}\\mathbf{\\mu} + \\mathbf{0} \\\\\n  &= \\begin{bmatrix} 2 & 3 \\end{bmatrix}\\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\end{bmatrix} \\\\\n  &= \\begin{bmatrix} 2 & 3 \\end{bmatrix}\\begin{bmatrix} 2 \\\\ \\frac{1}{2} \\end{bmatrix} = 1\n\\end{aligned}\n$$\n\nLet $Cov(X,Y)=1$.\nLet $X,Y$ be $X_1,X_2$, dependent with each other.\nLet $\\mathbf{x}=\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}$. \nThe expected value is the same as that of the independent one.\n\n$$\n\\begin{aligned}\n  Var(\\mathbf{x}) &= \\mathbf{\\Sigma} = \\begin{bmatrix} Var(X_1) & Cov(X_1,X_2) \\\\ Cov(X_2,X_1) & Var(X_2)\\end{bmatrix} =  \\begin{bmatrix} Var(X_1) & 1 \\\\ 1 & Var(X_2)\\end{bmatrix} \\\\\n  Var(2X_1-3X_2+3) &= Var(\\begin{bmatrix} 2 & -3 \\end{bmatrix} \\mathbf{X} +3) = \\begin{bmatrix} 2 & -3 \\end{bmatrix} Var( \\mathbf{X}) \\begin{bmatrix} 2 \\\\ -3 \\end{bmatrix} \\\\\n  &= \\begin{bmatrix} 2 & -3 \\end{bmatrix} \\begin{bmatrix} Var(X_1) & 1 \\\\ 1 & Var(X_2)\\end{bmatrix} \\begin{bmatrix} 2 \\\\ -3 \\end{bmatrix}\n\\end{aligned}\n$$\n\n* Monotonicity of Expectation\n\nIf $X$ and $Y$ are random variables such that $X \\leq Y$ almost surely, then:\n\n$$\nE(X) \\leq E(Y)\n$$\n\nIn other words, if one random variable is always less than or equal to another, then their expectations follow the same order.\n\n**Example:**\n\nLet's consider two random variables $X$ and $Y$ with the following probability distribution:\n\n$$\nP(X = 1) = \\frac{1}{2}, \\quad P(X = 2) = \\frac{1}{2}\n$$\n\n$$\nP(Y = 2) = \\frac{1}{2}, \\quad P(Y = 3) = \\frac{1}{2}\n$$\n\nWe can observe that $X \\leq Y$ almost surely since the largest value of $X$ is 2, which is less than the smallest value of $Y$ which is 3.\n\nTo compare their expectations, we calculate:\n\n$$\nE(X) = 1 \\cdot \\frac{1}{2} + 2 \\cdot \\frac{1}{2} = \\frac{3}{2}\n$$\n\n$$\nE(Y) = 2 \\cdot \\frac{1}{2} + 3 \\cdot \\frac{1}{2} = \\frac{5}{2}\n$$\n\nTherefore, we have $E(X) = \\frac{3}{2} \\leq E(Y) = \\frac{5}{2}$, confirming the monotonicity property of expectation.\n\n* Linearity of Expectation of random variables\n\nFor any two random variables $X$ and $Y$, we have:\n\n$E(X + Y) = E(X) + E(Y)$\n\nIn other words, the expectation of the sum of two random variables is equal to the sum of their individual expectations.\n\n**Example:**\n\nConsider two fair six-sided dice. Let $X$ be the outcome of the first die, and $Y$ be the outcome of the second die.\n\nThe probability distribution of each die roll is:\n\n$$\nP(X = i) = \\frac{1}{6} \\quad \\text{for } i = 1, 2, 3, 4, 5, 6\n$$\n\n$$\nP(Y = j) = \\frac{1}{6} \\quad \\text{for } j = 1, 2, 3, 4, 5, 6\n$$\n\nWe want to calculate the expected value of the sum $X + Y$.\n\nUsing the linearity of expectation property, we can compute:\n\n$$\nE(X) = \\sum_{i=1}^{6} i \\cdot P(X = i) = \\frac{1}{6} \\left(1 + 2 + 3 + 4 + 5 + 6\\right) = \\frac{7}{2}\n$$\n\n$$\nE(Y) = \\sum_{j=1}^{6} j \\cdot P(Y = j) = \\frac{1}{6} \\left(1 + 2 + 3 + 4 + 5 + 6\\right) = \\frac{7}{2}\n$$\n\nNow, we can calculate the expectation of their sum:\n\n$$\nE(X + Y) = E(X) + E(Y) = \\frac{7}{2} + \\frac{7}{2} = 7\n$$\n\nTherefore, we have $E(X + Y) = 7$.\n\nThe linearity of expectation property holds, and we see that the expectation of the sum $X + Y$ is equal to the sum of their individual expectations.\n\n## Variance\n\n**Definition: Variance**\n\nThe variance of a random variable $X$ is a measure of the spread or dispersion of the values of $X$. It quantifies how much the values of $X$ deviate from their expected value.\n\nFor a random variable $X$ with expected value $E(X)$, the variance is denoted by $\\text{Var}(X)$ and is defined as the average of the squared differences between each value $x$ of $X$ and the expected value, weighted by their probabilities:\n\n$$\n\\text{Var}(X) = E\\left[(X - E(X))^2\\right]\n$$\n\nAlternatively, the variance can be expressed as:\n\n$$\n\\text{Var}(X) = E(X^2) - (E(X))^2\n$$\n\n**Example:**\n\nConsider a random variable $X$ representing the number obtained when rolling a fair six-sided die. The possible values of $X$ are $1, 2, 3, 4, 5, \\text{ and } 6$, each with a probability of $\\frac{1}{6}$.\n\nTo find the variance $\\text{Var}(X)$, we need to compute the expected value $E(X)$ and the expected value of the square $E(X^2)$.\n\nThe expected value $E(X)$ is the average value of $X$ and can be calculated as:\n\n$$\nE(X) = 1 \\cdot P(X = 1) + 2 \\cdot P(X = 2) + 3 \\cdot P(X = 3) + 4 \\cdot P(X = 4) + 5 \\cdot P(X = 5) + 6 \\cdot P(X = 6)\n$$\n\nSubstituting the probabilities, we have:\n\n$$\nE(X) = 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6}\n$$\n\nSimplifying the expression gives us:\n\n$$\nE(X) = \\frac{21}{6} = \\frac{7}{2}\n$$\n\nNext, we need to compute the expected value of the square $$E(X^2)$$:\n\n$$\nE(X^2) = 1^2 \\cdot P(X = 1) + 2^2 \\cdot P(X = 2) + 3^2 \\cdot P(X = 3) + 4^2 \\cdot P(X = 4) + 5^2 \\cdot P(X = 5) + 6^2 \\cdot P(X = 6)\n$$\n\nSubstituting the probabilities, we have:\n\n$$\nE(X^2) = 1^2 \\cdot \\frac{1}{6} + 2^2 \\cdot \\frac{1}{6} + 3^2 \\cdot \\frac{1}{6} + 4^2 \\cdot \\frac{1}{6} + 5^2 \\cdot \\frac{1}{6} + 6^2 \\cdot \\frac{1}{6}\n$$\n\nSimplifying the expression gives us:\n\n$$\nE(X^2) = \\frac{91}{6}\n$$\n\nFinally, we can compute the variance $\\text{Var}(X)$ using the formula:\n\n$$\n\\text{Var}(X) = E(X^2) - (E(X))^2\n$$\n\nSubstituting the values, we have:\n\n$$\n\\text{Var}(X) = \\frac{91}{6} - \\left(\\frac{7}{2}\\right)^2\n$$\n\nSimplifying the expression gives us:\n\n$$\n\\text{Var}(X) = \\frac{35}{12}\n$$\n\nTherefore, the variance $\\text{Var}(X)$ of the random variable $X$ is $\\frac{35}{12}$.\n\n### Properties\nFor constants $a$ and $b$, matrix $\\mathbf{A}_{m\\times n}$, vector $\\mathbf{b}_{m \\times 1}$, random variables $X$ and $Y$, and random vector $\\mathbf{x}_{m\\times 1}$ and $\\mathbf{y}_{m\\times 1}$\n\n1. Non-Negativity: The variance is always non-negative.\n   $$\n   \\text{Var}(X) \\geq 0\n   $$\n\n2. Variance of a Constant: The variance of a constant is zero.\n   $$\n   \\text{Var}(c) = 0\n   $$\n   where $c$ is a constant.\n\n3. Linearity: The variance of a linear transformation of a random variable can be obtained by multiplying the variance of the original random variable by the square of the constant factor.\n   $$\n   \\text{Var}(aX) = a^2\\text{Var}(X)\n   $$\n\n   where $a$ is a constant. For a random vector $\\mathbf{x}$,\n   \n   $$\n   \\begin{aligned}\n      \\text{Var}(\\mathbf{A}\\mathbf{x}+\\mathbf{b}) &= \\mathbf{A}\\text{Var}(\\mathbf{x})\\mathbf{A}^T \\\\\n      \\text{Var}(\\mathbf{x}) &= \\mathbf{\\Sigma} = \\begin{bmatrix} \\text{Var}(X_1) & \\text{Cov}(X_1,X_2) \\\\ \\text{Cov}(X_2,X_1) & \\text{Var}(X_2)\\end{bmatrix} \\\\\n      &=\\begin{bmatrix} \\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2\\end{bmatrix} \\\\\n      &=\\begin{bmatrix} \\text{Var}(X_1) & 0 \\\\ 0 & \\text{Var}(X_2)\\end{bmatrix} \\text{when the two random variables are independent}\n    \\end{aligned}\n   $$\n\n\n4. Additivity of Independent Variables: The variance of the sum (or difference) of independent random variables is equal to the sum of their variances.\n   $$\n   \\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\n   $$\n   where $X$ and $Y$ are independent random variables.\n\n   $$\n   \\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X,Y)\n   $$\n   where $X$ and $Y$ are dependent random variables.\n\n**Example:**\n\nConsider two independent random variables $X$ and $Y$ with variances $\\text{Var}(X) = 4$ and $\\text{Var}(Y) = 9$.\n\n1. Non-Negativity:\n   $\\text{Var}(X) = 4 \\geq 0$, $\\text{Var}(Y) = 9 \\geq 0$\n\n2. Variance of a Constant:\n   Let $c = 5$ be a constant. $\\text{Var}(c) = \\text{Var}(5) = 0$\n\n3. Linearity:\n   Let $a = 2$ be a constant. $\\text{Var}(aX) = \\text{Var}(2X) = 4(2^2) = 16$\n\n  $$\n  \\begin{aligned}\n    Var(2X_1+3X_2+3) &= Var(\\begin{bmatrix} 2 & 3 \\end{bmatrix} \\mathbf{X} +3) = \\begin{bmatrix} 2 & 3 \\end{bmatrix} Var( \\mathbf{X}) \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\\\\n      &= \\begin{bmatrix} 2 & 3 \\end{bmatrix} \\begin{bmatrix} Var(X_1) & 0 \\\\ 0 & Var(X_2)\\end{bmatrix} \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \n  \\end{aligned}\n  $$\n\n4. Additivity of Independent Variables:\n   $\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) = 4 + 9 = 13$\n\n## Covariance\n\n**Definition:**\nThe covariance between two random variables $X$ and $Y$ is a measure of how they vary together. It is defined as the expected value of the product of the deviations of $X$ and $Y$ from their respective means.\n\nThe covariance between $X$ and $Y$ is denoted as $Cov(X, Y)$ and can be calculated as:\n$$\n\\text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))]\n$$\n\n**Example:**\nLet's consider two random variables $X$ and $Y$ with the following data:\n\n$$\n\\begin{align*}\nX & : 1, 2, 3, 4, 5 \\\\\nY & : 2, 4, 6, 8, 10 \\\\\n\\end{align*}\n$$\n\nWe first calculate the means of $X$ and $Y$:\n$$\n\\begin{align*}\nE(X) &= \\frac{1 + 2 + 3 + 4 + 5}{5} = 3 \\\\\nE(Y) &= \\frac{2 + 4 + 6 + 8 + 10}{5} = 6 \\\\\n\\end{align*}\n$$\n\nNext, we calculate the deviations of $X$ and $Y$ from their means:\n$$\n\\begin{align*}\nX - E(X) & : -2, -1, 0, 1, 2 \\\\\nY - E(Y) & : -4, -2, 0, 2, 4 \\\\\n\\end{align*}\n$$\n\nThen, we calculate the product of these deviations and take their expected value:\n$$\n\\text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))]\n$$\n\nFinally, we substitute the values and calculate the covariance.\n\nThe resulting covariance will give us an indication of the relationship between the two variables.\n\n### Properties\n\n1. **Bilinearity:** Covariance is a bilinear function, meaning it satisfies the following properties:\n   - Linearity in the first argument: $Cov(aX, Y) = a * Cov(X, Y)$\n   - Linearity in the second argument: $Cov(X, bY) = b * Cov(X, Y)$\n   - Additivity: $Cov(X1 + X2, Y) = Cov(X1, Y) + Cov(X2, Y)$\n   - $Cov(aX+b+cY+d)=acCov(X,Y)$\n2. **Symmetry:** Covariance is symmetric, which means $Cov(X, Y) = Cov(Y, X)$.\n3. **Covariance with Constant:** $Cov(X, c) = 0$, where $c$ is a constant.\n4.  $Cov(X,Y)=0$ if the two random variables are independent.\n\n**Example:**\nLet's consider two random variables $X$ and $Y$ with the following data:\n\n$$\n\\begin{align*}\nX & : 1, 2, 3, 4, 5 \\\\\nY & : 2, 4, 6, 8, 10 \\\\\n\\end{align*}\n$$\n\n1. **Bilinearity:**\n   - Linearity in the first argument:\n   $$\n   \\text{Cov}(2X, Y) = 2 \\cdot \\text{Cov}(X, Y)\n   $$\n   \n   - Linearity in the second argument:\n   $$\n   \\text{Cov}(X, 3Y) = 3 \\cdot \\text{Cov}(X, Y)\n   $$\n   \n   - Additivity:\n   $$\n   \\text{Cov}(X_1 + X_2, Y) = \\text{Cov}(X_1, Y) + \\text{Cov}(X_2, Y)\n   $$\n\n2. **Symmetry:**\n   $$\n   \\text{Cov}(X, Y) = \\text{Cov}(Y, X)\n   $$\n\n3. **Covariance with Constant:**\n   $$\n   \\text{Cov}(X, c) = 0\n   $$\n\n\n\n```{r}\n#| echo: false\n#| eval: false\n\n### Singular value and Singluar Vectors\n\nThe singular value decomposition (SVD) of a matrix $\\mathbf A$ is a factorization of $\\mathbf A$ into the product of three matrices as follows:\n\n$$\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n$$\nwhere $\\mathbf{U}$ is an $m \\times m$ orthogonal matrix, $\\mathbf{\\Sigma}$ is an $m \\times n$ rectangular diagonal matrix with non-negative real numbers on the diagonal, and $\\mathbf{V}$ is an $n \\times n$ orthogonal matrix.\n\nThe diagonal entries of $\\mathbf{\\Sigma}$ are called the singular values of $\\mathbf{A}$, denoted as $\\sigma_1, \\sigma_2, \\ldots, \\sigma_r$ (where $r$ is the rank of $\\mathbf{A}$), and are arranged in descending order. The columns of $\\mathbf{U}$ and $\\mathbf{V}$ are called the left and right singular vectors of $\\mathbf{A}$, respectively, and are orthonormal vectors.\n\nFor example, let $\\mathbf{A}$ be a 3 by 2 matrix given by:\n\n$$\n\\begin{equation*}\n\\mathbf{A} = \n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n$$\n\nThe SVD of $\\mathbf{A}$ is given by:\n\n$$\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n$$\n\nwhere the left singular vectors of $\\mathbf{A}$ are the columns of $\\mathbf{U}$, the right singular vectors of $\\mathbf{A}$ are the columns of $\\mathbf{V}$, and the singular values of $\\mathbf{A}$ are the diagonal entries of $\\boldsymbol{\\Sigma}$.\n\n\n\n* 연립 방정식을 행렬의 곱으로 나타내보기\n  $$\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}$$ \n  $$ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}$$\n* 중요한 사실(....당연한 사실?)\n  * 곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n    * $A_{m \\times n} \\times B_{o \\times p}$ 에서 $n = o$ 여야 곱셈 성립\n  * 곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 $\\times$ 곱셈의 오른쪽 행렬의 열 수\n    * $A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}$\n  * 교환법칙(Commutative property)이 성립하지 않음\n    * $AB \\neq BA$\n* 행렬 곱셈의 여러가지 관점\n  * 내적으로 바라보기\n    $$ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} $$\n    $$ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}$$\n  * rank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중.....-_-)\n    $$AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}$$\n  * column space로 바라보기\n    $$A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m $$ (스칼라배의 합)\n    * $A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$ 는 2차원 좌표평면의 모든 점을, $A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$은 3차원 좌표평면의 모든 점 표현 가능\n    * $AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}$\n    * column space: A의 column vector로 만들 수 있는 부분 공간\n  * row space로 바라보기\n    $$\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} $$\n\n\n## 열공간(Column Space)\n{{< video https://youtu.be/g0eaDeVRdZk >}}\n* column space: column vector 들이 span 하는 space\n  * $A$의 column space = $C(A)$ 또는 $range(A)$\n* span: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n  * vector에 따라, 점일수도 선일수도 평면일 수도 있음\n  * vector space를 이 vector들이 span하는 space &rarr; column space는 행렬의 열들이 span하는 space\n* vector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n  * $\\mathbf{v_1}$ 과 $\\mathbf{v_2}$의 linear combination으로 2차원 좌표평면 나타내기\n  ![](images/chap02_09.PNG)\n\n\n\n## 선형 독립(Linear Independent)\n{{< video https://youtu.be/mOOI4-BfjGQ >}}\n\n...and also see\n{{< video https://youtu.be/9F4PZ_1orF0 >}}\n\n* 선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\n* orthogonal 하면 independent\n  * but independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n* definition: $a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}$ 를 만족하는 $a_1, a_2, a_3, \\cdots a_n$ 이 $a_1 = a_2 = a_3 = \\cdots = a_n = 0$ 밖에 없을때\n  * $\\mathbf{0}$는 모든 elements가 $0$인 벡터\n  * 예: $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, $\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}$ 는 $-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ 이 되므로, linearly independent 하지 않음\n  * independent한 vector 들의 수 = 표현할 수 있는 차원의 dimension\n\n## 기저(basis)\n* 주어진 vector space를 span하는 linearly independent한 vectors\n* 어떤 공간을 이루는 필수적인 구성요소\n* orthogonal 하면 orthogonal basis\n* 예: 2차원 좌표평면에 대해\n  * $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ : orthogonal basis\n  * $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ : orthogonal 하지 않은 basis\n  * $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$ : linearly independent 하지 않으므로 basis 아님\n\n## 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n{{< video https://youtu.be/XqOvyfMUAwA >}}\n\n### Identity matrix(항등행렬)\n* 항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n  * 실수에서 곱셈의 항등원은 1\n* 행렬의 항등원: 항등행렬($I$)\n  $$I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}$$\n  * $A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}$\n  * $I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}$ \n\n### Inverse matrix(역행렬) {#sec-inv}\n* 역원: 연산 결과 항등원이 나오게 하는 연소\n  * 실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): $a \\times a^{-1} = 1$\n* 행렬의 역원: 역행렬($A^{-1}$)\n  $$A \\times A^{-1} = I , A^{-1} \\times A = I$$\n  * 존재하지 않는 경우도 있음\n  * 존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n    * 존재하지 않으면 singular, degenerate라고 불림\n  * square matrix(정사각행렬, $m = n$)은 특수한 경우를 제외하면 역행렬이 항상 존재\n    * 역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n  * $m \\neq n$인 행렬의 경우에는 역행렬이 존재하지 않음\n    * 다만, 경우에 따라 $A \\times A^{-1} = I$ 를 만족하거나(right inverse), $A^{-1} \\times A = I$를 만족하는(left inverse)는 $A^{-1}$이 존재함\n  * 연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음\n    $$A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}$$ \n\n### Diagonal Matrix(대각행렬)\n* diagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix\n  $$ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}$$\n  * identity matrix는 diagonal matrix\n  * diagnomal matrix는 symmetric matrix 이기도 함\n  * 보통은 square matrix에서 주로 사용됨\n    * square matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n  \n\n### Orthogonal matrix(직교행렬, orthonomal matrix)\n* 행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors)\n  $$A A^T = A^T A = I$$\n* identity matrix는 orthogonal matrix\n* square matrix에서만 정의됨\n* Orthogonal matrix인 $A$이면 $A^{-1} = A^{T}$\n  * 각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n* complex matrix(복소수 행렬)에서는 unitary matrix라고 부름\n\n## 계수(Rank)\n\n{{< video https://youtu.be/HMST0Yc7EXE >}}\n\n* rank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\n* independent한 column의 수 = independent한 행의 수: $rank(A) = rank(A^T)$\n  * proof: [Wikipedia](https://en.wikipedia.org/wiki/Rank_%28linear_algebra%29#Proofs_that_column_rank_=_row_rank)\n* 예:\n  $$\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1$$\n  $$\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2$$\n* $A_{m \\times n}$ 의 최대 랭크는 $min\\{m,n\\}$\n  * $rank(A) < min\\{m,n\\}$ 면 rank-deficient, $rank(A) = min\\{m,n\\}$면 full (row/column) rank\n\n## 영공간(Null space)\n\n{{< video https://youtu.be/Eizc9TSRYMQ >}}\n\n* $A\\mathbf{x}= \\mathbf{0}$ 을 만족하는 $\\mathbf{x}$의 집합\n  * column space 관점에서 보기: $A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}$\n  * null space에 항상 들어가는 $\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$ : trivial solution\n    * 모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 $\\mathbf{x}=\\mathbf{0}$하나 밖에 없음\n* $\\mathbf{x}=\\mathbf{0}$ 가 아닌 vector가 null space에 있으면, 스칼라배(constant $c$에 대해 $c \\mathbf{x}$) 역시 null space에 포함됨\n* 혼동 주의! null space는 column space의 일부가 아님\n    * row vector의 차원이 null space가 존재하는 공간\n* rank와 null space의 dimension의 합은 항상 matrix의 column의 수\n    * $A_{m \\times n}$에 대해, $dim(N(A)) = n - r$\n      * 모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n  * null space는 row space와 수직한 space\n    * $A\\mathbf{x}= \\mathbf{0}$ : 각각 모든 행과 내적해서 0, &rarr; 행들의 linear combination과 내적해도 0\n    * rank는 row space의 dimension &rarr; row space의 dimension($dim(R(A))$)과 null space의 dimension($dim(N(A))$)의 합이 $n$\n    * $\\mathbb{R^n}$ 공간에 표현: \n      ![](images/chap02_10.PNG)\n      * 겹친 점: 영벡터\n* left null space: $\\mathbf{x^T} A = \\mathbf{0^T}$ 인 $\\mathbf{x}$\n  * 위의 성질을 row에 대해 적용\n    * m 차원에 놓인 벡터\n    * dimension:  $dim(N_L(A)) = m - r$\n    * column space와 수직: $dim(N_L(A)) +dim(C(A)) = m$\n* $R(A)$에 있는 vector $\\mathbf{x_r}$ 와 $N(A)$에 있는 vector $\\mathbf{x_n}$에 대해:\n  * $\\mathbf{x_r}$에 $A$를 곱하면 column space로 감\n  * $\\mathbf{x_n}$에 $A$를 곱하면 \\mathbf{x_0}$\n  * $A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} +  A\\mathbf{x_n} =  A\\mathbf{x_r} = \\mathbf{b}$\n\n## Ax = b의 해의 수\n\n{{< video https://youtu.be/nNI2TlD598c >}}\n\n* full column rank 일때\n  * $\\mathbf{b}$가 column space($C(A)$)안에 있으면 해가 하나\n  * $\\mathbf{b}$가 column space($C(A)$)안에 없으면 해가 없음\n  ![](images/chap02_11.PNG)\n\n* full row rank 일때\n  * $\\mathbf{b}$는 항상 column space 안에 있음: 무한의 해를 가짐\n  * 임의의 특정한 해(particular solution) $\\mathbf{x_p}$와 null space의 vector $\\mathbf{x_n}$에 대해, $A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}$\n    * 즉, $\\mathbf{x_p} +\\mathbf{x_n}$ 도 해가 됨: complete solution\n      * null space는 무한하므로, 해도 무한함\n\n* full rank 일때(square matrix): 해가 하나 존재 ($\\mathbf{x} = A^{-1}$\\mathbf{b}$)\n\n* rank-deficient 일때\n  * $\\mathbf{b}$가 column space($C(A)$)안에 있으면 무한한 해를 가짐\n  * $\\mathbf{b}$가 column space($C(A)$)안에 없으면 해가 없음\n\n```\n\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":true,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"09.temp.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Basic Matrix (1) - Matrix Operations","subtitle":"template","description":"template\n","categories":["Mathematics"],"author":"Kwangmin Kim","date":"03/31/2023","draft":true,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}