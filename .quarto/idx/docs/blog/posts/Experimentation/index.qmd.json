{"title":"Experimentation","markdown":{"yaml":{"title":"Experimentation","subtitle":"Comprehensive Guide to Online Experimentation and Causal Inference","description":"From epidemiological foundations to modern adaptive algorithms -\nA systematic path to mastering A/B testing, Multi-Armed Bandits, and causal inference\n","categories":["Experimentation"],"author":"Kwangmin Kim","date":"11/10/2025","format":{"html":{"page-layout":"full","code-fold":true}}},"headingText":"Experimentation","containsRefs":false,"markdown":"\n\n\n* 실험 설계와 인과추론의 체계적 이해를 위한 구조화된 학습 경로를 제시\n* Epidemiology 기반 지식을 활용하여 A/B Testing과 Multi-Armed Bandit을 포함한 Experimentation 전반을 학습할 수 있도록 구성\n\n## Experimentation 하위구조\n\n* Experimentation\n  * Epidemiolog\n  * AB_test\n  * Fundamentals\n  * MAB (Multi-Armed Badit)\n  * Causal_inference\n  * Advanced\n  * Platform\n\n## Learning Path\n\n이 학습 경로는 **Epidemiology → Classical A/B Testing → Multi-Armed Bandits → Advanced Methods** 순서로 \n실험 설계와 인과추론을 체계적으로 학습하도록 설계\n\n### Core Concepts Connection Map\n\n```\nEpidemiology RCT → A/B Testing → Sequential Testing → Adaptive Testing → MAB\n       ↓              ↓                                        ↓\nCausal Inference → DAG/SUTVA ────────────────────→ Interference Handling\n       ↓              ↓                                        ↓\nEffect Measures → Lift/Uplift ──────────────────→ Heterogeneous TE\n       ↓\nSample Size → Power Analysis → Sequential Monitoring → Variance Reduction\n```\n\n### Key Mathematical Connections\n\n| Epidemiology | Experimentation | 수식 |\n|-------------|-----------------|------|\n| Relative Risk | Lift | $\\frac{P(Y=1|T=1)}{P(Y=1|T=0)}$ |\n| Risk Difference | Absolute Uplift | $P(Y=1|T=1) - P(Y=1|T=0)$ |\n| NNT | Number to Convert | $\\frac{1}{RD}$ |\n| Effect Modification | HTE | $E[Y(1)-Y(0)|X]$ |\n\n### Recommended Study Time\n\n```\nPhase 1: 1-2개월 (주 10-15시간)\n  - Epidemiology: 2-3주\n  - Statistics: 1주\n  - Classical A/B: 2-3주\n\nPhase 2: 2-3개월 (주 10-15시간)\n  - Advanced A/B: 2-3주\n  - Causal Inference: 3-4주 ⭐ (가장 중요)\n  - MAB Fundamentals: 2주\n  - Classical Algorithms: 2-3주\n\nPhase 3: 2-3개월 (주 8-12시간)\n  - HTE: 2-3주\n  - Variance Reduction: 2주\n  - Advanced Bandits: 3-4주\n  - Practical Challenges: 2-3주\n\nPhase 4: 지속적 (프로젝트 기반)\n  - Platform: 필요시\n  - Applications: 도메인별\n  - Research: 관심분야별\n```\n\n## Foundations (1-2개월)\n\n### Epidemiology Foundations\n\nRCT(무작위 대조 시험)는 A/B Testing의 직접적 원형이며, 인과추론의 수학적 프레임워크(Potential Outcomes, SUTVA)가 실험 설계의 이론적 기초를 제공한다. 역학의 effect measures(RR, RD, NNT)는 디지털 실험의 lift와 uplift 지표로 직접 대응되며, bias와 confounding 개념은 실험의 타당도를 판단하는 핵심 도구다. Sample size와 power 계산은 모든 실험 설계의 출발점이다.\n\n* [Epidemiology Overview](./Epidemiology/index.qmd) ⭐\n  * Study Design Framework\n    * 1111-11-11, [Randomized Controlled Trial (무작위 대조 시험)](./Epidemiology/experimental/rct.qmd) ⭐\n    * 1111-11-11, [Quasi-experimental Studies (준실험 연구)](./Epidemiology/experimental/quasi_experimental.qmd)\n    * 1111-11-11, [Factorial Design (요인 설계)](./Epidemiology/experimental/factorial_design.qmd)\n  * Measures of Association and Impact\n    * 1111-11-11, [Relative Risk and Risk Difference](./Epidemiology/measures/relative_risk.qmd)\n    * 1111-11-11, [Effect Size Measures](./Epidemiology/measures/effect_size.qmd)\n    * 1111-11-11, [Number Needed to Treat](./Epidemiology/measures/nnt.qmd)\n  * Bias and Confounding\n    * 1111-11-11, [Selection Bias](./Epidemiology/bias/selection_bias.qmd)\n    * 1111-11-11, [Information Bias](./Epidemiology/bias/information_bias.qmd)\n    * 1111-11-11, [Confounding and Control](./Epidemiology/confounding/index.qmd)\n  * Study Design Fundamentals\n    * 1111-11-11, [Sample Size and Power](./Epidemiology/fundamentals/sample_size_power.qmd) ⭐\n    * 1111-11-11, [Randomization Methods](./Epidemiology/fundamentals/randomization.qmd) ⭐\n\n### Statistical Foundations\n\n가설 검정의 Type I/II 오류와 statistical power 개념은 실험 결과 해석의 필수 요소다. Multiple testing problem은 여러 지표를 동시에 평가하거나 중간 결과를 확인할 때 발생하는 false positive를 통제하는 방법을 제공한다. Effect size와 MDE(최소감지효과) 이해 없이는 실무적으로 의미 있는 실험 설계가 불가능하다.\n\n\n* [Statistical Foundations for Experimentation](./Fundamentals/statistics.qmd)\n  * 1111-11-11, [Hypothesis Testing Framework](./Fundamentals/hypothesis_testing.qmd)\n    * 1111-11-11, Type I and Type II Errors (1종·2종 오류)\n    * 1111-11-11, Statistical Power (검정력)\n    * 1111-11-11, p-values and Significance (p-값과 유의성)\n  * 1111-11-11, [Effect Size and Practical Significance](./Fundamentals/effect_size.qmd)\n    * 1111-11-11, Minimum Detectable Effect (MDE, 최소감지효과)\n    * 1111-11-11, Cohen's d and Standardized Effect Sizes\n  * 1111-11-11, [Multiple Testing Problem](./Fundamentals/multiple_testing.qmd)\n    * 1111-11-11, Family-wise Error Rate (FWER)\n    * 1111-11-11, False Discovery Rate (FDR)\n    * 1111-11-11, Bonferroni and Holm Corrections\n  * 1111-11-11, [Confidence Intervals](./Fundamentals/confidence_intervals.qmd)\n    * 1111-11-11, Interpretation and Common Mistakes\n    * 1111-11-11, Bootstrap Methods\n\n### Classical A/B Testing Fundamentals\n\n디지털 제품에서 RCT 원리를 적용하는 구체적 방법론이다. Randomization unit 선택, traffic allocation, metric selection 등 실무적 의사결정 프레임워크를 제공한다. Fixed-horizon testing의 원칙과 \"peeking problem\"을 이해해야 sequential testing과 MAB의 필요성을 정확히 파악할 수 있다. 대부분의 실무 실험이 이 방법으로 진행되므로 가장 실용적이다.\n\n\n* [Classical A/B Testing](./AB_test/index.qmd) ⭐\n  * 1111-11-11, [A/B Test Design Principles](./AB_test/design_principles.qmd)\n    * 1111-11-11, Hypothesis Formulation (가설 수립)\n    * 1111-11-11, Metric Selection (지표 선택)\n    * 1111-11-11, Randomization Unit (무작위 배정 단위)\n    * 1111-11-11, Traffic Allocation (트래픽 할당)\n  * 1111-11-11, [Sample Size Calculation](./AB_test/sample_size.qmd) ⭐\n    * 1111-11-11, Power Analysis for A/B Tests\n    * 1111-11-11, Continuous vs. Binary Metrics\n    * 1111-11-11, Unequal Allocation\n  * 1111-11-11, [Randomization Strategies](./AB_test/randomization.qmd)\n    * 1111-11-11, Simple Randomization\n    * 1111-11-11, Stratified Randomization\n    * 1111-11-11, Consistent Hashing\n  * 1111-11-11, [Analysis Methods](./AB_test/analysis.qmd)\n    * 1111-11-11, t-tests and z-tests\n    * 1111-11-11, Chi-square Tests\n    * 1111-11-11, Non-parametric Tests (Mann-Whitney)\n  * 1111-11-11, [Duration and Stopping Rules](./AB_test/duration.qmd)\n    * 1111-11-11, Fixed Horizon Testing\n    * 1111-11-11, When to Stop (언제 중단할 것인가)\n    * 1111-11-11, Peeking Problem (중간확인 문제)\n\n## Core Methods (2-3개월)\n\n### Advanced A/B Testing\n\nSequential testing은 실험 진행 중 중간 결과를 확인하면서도 Type I 오류를 통제하는 방법으로, 실무에서 가장 많이 요구되는 기술이다. Bayesian A/B testing은 \"승률 확률\" 같은 비즈니스 친화적 해석을 제공한다. Multi-variate testing은 여러 요소의 상호작용 효과를 동시에 평가할 수 있어 복잡한 제품 최적화에 필수적이다.\n\n\n* [Advanced A/B Testing Techniques](./AB_test/advanced/index.qmd)\n  * 1111-11-11, [Sequential Testing](./AB_test/advanced/sequential.qmd) ⭐\n    * 1111-11-11, Group Sequential Designs (그룹순차설계)\n    * 1111-11-11, Sequential Probability Ratio Test (SPRT)\n    * 1111-11-11, Alpha Spending Functions\n    * 1111-11-11, Always-Valid Inference\n  * 1111-11-11, [Bayesian A/B Testing](./AB_test/advanced/bayesian.qmd)\n    * 1111-11-11, Prior Distribution Selection (사전분포 선택)\n    * 1111-11-11, Posterior Probability (사후확률)\n    * 1111-11-11, Credible Intervals (신용구간)\n    * 1111-11-11, Probability of Being Best\n  * 1111-11-11, [Multi-variate Testing](./AB_test/advanced/multivariate.qmd)\n    * 1111-11-11, Full Factorial Designs (완전요인설계)\n    * 1111-11-11, Fractional Factorial Designs (부분요인설계)\n    * 1111-11-11, Interaction Effects (상호작용효과)\n  * 1111-11-11, [A/A Testing](./AB_test/advanced/aa_testing.qmd)\n    * 1111-11-11, Platform Validation\n    * 1111-11-11, Sample Ratio Mismatch (SRM) Detection\n\n### Causal Inference Framework\n\nPotential outcomes와 DAG는 \"왜 무작위 배정이 중요한가\", \"어떤 변수를 통제해야 하는가\"에 대한 수학적으로 엄밀한 답을 제공한다. SUTVA 위반(network effects, spillover)은 디지털 실험에서 흔히 발생하며, 이를 감지하고 처리하는 방법이 필요하다. HTE(heterogeneous treatment effects) 분석의 이론적 기초가 되어 \"어떤 사용자에게 효과적인가\"를 과학적으로 답할 수 있게 한다.\n\n* [Causal Inference for Experimentation](./Causal_inference/index.qmd) ⭐\n  * 1111-11-11, [Potential Outcomes Framework](./Causal_inference/potential_outcomes.qmd) ⭐\n    * 1111-11-11, Rubin Causal Model (루빈 인과 모형)\n    * 1111-11-11, Average Treatment Effect (ATE, 평균처치효과)\n    * 1111-11-11, CATE and ITE (조건부·개별 처치효과)\n    * 1111-11-11, SUTVA (안정단위처치값 가정)\n  * 1111-11-11, [Directed Acyclic Graphs](./Causal_inference/dag.qmd) ⭐\n    * 1111-11-11, Causal Pathways (인과경로)\n    * 1111-11-11, Confounding Paths (교란경로)\n    * 1111-11-11, Colliders and Selection Bias (충돌자와 선택편향)\n    * 1111-11-11, d-separation (d-분리)\n    * 1111-11-11, Backdoor and Frontdoor Criteria\n  * 1111-11-11, [Assignment Mechanisms](./Causal_inference/assignment.qmd)\n    * 1111-11-11, Intent-to-Treat (ITT) Analysis\n    * 1111-11-11, Per-Protocol Analysis\n    * 1111-11-11, As-Treated Analysis\n    * 1111-11-11, Complier Average Causal Effect (CACE)\n  * 1111-11-11, [Interference and Spillover](./Causal_inference/interference.qmd)\n    * 1111-11-11, Network Effects (네트워크 효과)\n    * 1111-11-11, Cluster Randomization (군집 무작위배정)\n    * 1111-11-11, Switchback Experiments\n    * 1111-11-11, Geo-experiments\n\n### Multi-Armed Bandit Fundamentals\n\nA/B Testing이 \"학습 후 의사결정\"이라면, MAB는 \"학습과 최적화를 동시에\" 수행한다. Exploration-exploitation trade-off는 제한된 자원(트래픽)으로 최대 성과를 내는 전략의 핵심이다. Regret 개념은 \"실험 비용\"을 수학적으로 정량화하여 알고리즘 간 비교를 가능하게 한다.\n\n* [Multi-Armed Bandits Overview](./MAB/index.qmd) ⭐\n  * 1111-11-11, [MAB Problem Formulation](./MAB/formulation.qmd)\n    * 1111-11-11, Exploration vs. Exploitation Trade-off\n    * 1111-11-11, Regret Definition (후회 정의)\n      * Cumulative Regret (누적 후회)\n      * Simple Regret (단순 후회)\n    * 1111-11-11, Reward Distributions (보상 분포)\n    * 1111-11-11, Stationarity Assumptions (정상성 가정)\n  * 1111-11-11, [Performance Metrics](./MAB/metrics.qmd)\n    * 1111-11-11, Expected Cumulative Regret\n    * 1111-11-11, Best Arm Identification\n    * 1111-11-11, Sample Complexity\n    * 1111-11-11, Convergence Rates\n\n### Classical MAB Algorithms\n\nEpsilon-greedy는 가장 단순하지만 exploration 비율을 수동으로 조정해야 하는 한계가 있다. UCB는 불확실성을 자동으로 정량화하여 exploration을 관리하며, 이론적 regret bound가 증명되었다. Thompson Sampling은 1933년 의학 실험을 위해 개발된 알고리즘으로, 실무에서 가장 성능이 좋고 구현이 간단하여 광범위하게 사용된다.\n\n\n* [Classical Bandit Algorithms](./MAB/classical/index.qmd)\n  * 1111-11-11, [Epsilon-Greedy Methods](./MAB/classical/epsilon_greedy.qmd)\n    * 1111-11-11, Fixed Epsilon Strategy\n    * 1111-11-11, Decaying Epsilon Strategy\n    * 1111-11-11, Theoretical Regret Bounds\n  * 1111-11-11, [Upper Confidence Bound (UCB)](./MAB/classical/ucb.qmd)\n    * 1111-11-11, UCB1 Algorithm\n    * 1111-11-11, UCB-Tuned\n    * 1111-11-11, Bayesian UCB\n    * 1111-11-11, KL-UCB\n  * 1111-11-11, [Thompson Sampling](./MAB/classical/thompson_sampling.qmd) ⭐\n    * 1111-11-11, Beta-Bernoulli Thompson Sampling\n    * 1111-11-11, Gaussian Thompson Sampling\n    * 1111-11-11, Theoretical Properties\n    * 1111-11-11, Historical Context (의학 실험 기원)\n\n### MAB vs. A/B Testing\n\n두 방법의 trade-off를 이해해야 상황에 맞는 선택이 가능하다. A/B Testing은 statistical validity가 명확하고 효과 크기를 정확히 추정하지만, MAB는 실험 중 기회비용을 최소화한다. 비즈니스 목표(정확한 측정 vs. 빠른 최적화), 트래픽 규모, 의사결정 맥락에 따라 최적 방법이 달라진다.\n\n## Advanced Applications (2-3개월)\n\n### Heterogeneous Treatment Effects\n\n\"평균적으로 효과가 있다\"는 것과 \"모든 사용자에게 효과가 있다\"는 것은 다르다. HTE 분석을 통해 어떤 사용자 세그먼트에서 효과가 크고 작은지 파악할 수 있다. Causal forests와 meta-learners 같은 ML 방법은 수백 개의 특성 조합에서 효과 패턴을 자동으로 발견한다. 개인화 전략 수립의 과학적 기반이 된다.\n\n\n* 1111-11-11, [When to Use MAB vs. A/B Testing](./MAB/comparison.qmd) ⭐\n  * 1111-11-11, Trade-offs and Decision Framework\n  * 1111-11-11, Statistical Validity Considerations\n  * 1111-11-11, Business Context and Goals\n  * 1111-11-11, Hybrid Approaches\n\n\n* [Heterogeneous Treatment Effects](./Causal_inference/hte/index.qmd) ⭐\n  * 1111-11-11, [Subgroup Analysis](./Causal_inference/hte/subgroup.qmd)\n    * 1111-11-11, Pre-specified Subgroups\n    * 1111-11-11, Multiple Comparison Corrections\n    * 1111-11-11, Statistical vs. Practical Significance\n  * 1111-11-11, [Effect Modification Analysis](./Causal_inference/hte/effect_modification.qmd)\n    * 1111-11-11, Interaction Terms (상호작용항)\n    * 1111-11-11, Stratified Analysis (층화분석)\n  * 1111-11-11, [Machine Learning Methods](./Causal_inference/hte/ml_methods.qmd)\n    * 1111-11-11, Causal Forests\n    * 1111-11-11, Meta-learners (S-, T-, X-learner)\n    * 1111-11-11, Double Machine Learning (DML)\n    * 1111-11-11, BART (Bayesian Additive Regression Trees)\n\n### Variance Reduction Techniques\n\n동일한 sample size로 더 정확한 추정이 가능하면, 실험 기간을 단축하거나 더 작은 효과를 감지할 수 있다. CUPED는 실험 전 데이터(baseline)를 활용해 분산을 최대 50% 이상 줄일 수 있어 실무에서 표준 기법이 되었다. Stratification과 regression adjustment는 역학에서 검증된 방법으로, 디지털 실험에 직접 적용 가능하다.\n\n* [Variance Reduction Methods](./Advanced/variance_reduction/index.qmd) ⭐\n  * 1111-11-11, [Pre-experiment Methods](./Advanced/variance_reduction/pre_experiment.qmd)\n    * 1111-11-11, Stratification (층화)\n    * 1111-11-11, Matched Pair Design (대응설계)\n    * 1111-11-11, Blocking (블록화)\n  * 1111-11-11, [Post-experiment Methods](./Advanced/variance_reduction/post_experiment.qmd)\n    * 1111-11-11, CUPED (Controlled-experiment Using Pre-Experiment Data) ⭐\n    * 1111-11-11, CUPAC (CUPED with Asymptotic Confidence)\n    * 1111-11-11, Regression Adjustment (회귀조정)\n    * 1111-11-11, Difference-in-Differences (이중차분법)\n\n### Contextual and Advanced Bandits\n\nContextual bandits는 사용자 특성(context)을 고려해 개인화된 의사결정을 내린다. Non-stationary bandits는 시간에 따라 최적 선택지가 변하는 현실을 반영한다. 추천 시스템, 동적 가격 책정, 개인화 마케팅 등 복잡한 실무 문제에 적용된다.\n\n* [Advanced Bandit Methods](./MAB/advanced/index.qmd)\n  * 1111-11-11, [Contextual Bandits](./MAB/advanced/contextual.qmd)\n    * 1111-11-11, Linear Contextual Bandits\n    * 1111-11-11, LinUCB Algorithm\n    * 1111-11-11, Neural Bandits\n    * 1111-11-11, Policy Gradient Methods\n  * 1111-11-11, [Non-stationary Bandits](./MAB/advanced/non_stationary.qmd)\n    * 1111-11-11, Sliding Window Approaches\n    * 1111-11-11, Discounted Rewards\n    * 1111-11-11, Change Detection Methods\n    * 1111-11-11, Switching Bandits\n  * 1111-11-11, [Structured Bandits](./MAB/advanced/structured.qmd)\n    * 1111-11-11, Combinatorial Bandits\n    * 1111-11-11, Dueling Bandits\n    * 1111-11-11, Ranking Bandits\n\n### Practical Implementation Challenges\n\n이론적으로 완벽한 실험도 실무에서는 metric 정의, novelty effects, network interference, SRM 등 다양한 문제에 직면한다. 이러한 문제들을 감지하고 완화하는 방법을 모르면 잘못된 의사결정으로 이어진다. North star metric과 guardrail metric 설정은 실험 프로그램의 성공을 좌우한다.\n\n\n* [Practical Experimentation Challenges](./Advanced/practical/index.qmd)\n  * 1111-11-11, [Metric Design](./Advanced/practical/metrics.qmd)\n    * 1111-11-11, North Star Metrics (핵심지표)\n    * 1111-11-11, Proxy Metrics (대리지표)\n    * 1111-11-11, Guardrail Metrics (가드레일지표)\n    * 1111-11-11, Long-term vs. Short-term Metrics\n  * 1111-11-11, [Novelty and Primacy Effects](./Advanced/practical/novelty_effects.qmd)\n    * 1111-11-11, Detection Methods\n    * 1111-11-11, Mitigation Strategies\n  * 1111-11-11, [Network Effects and Interference](./Advanced/practical/interference.qmd)\n    * 1111-11-11, Detection of SUTVA Violations\n    * 1111-11-11, Cluster-based Approaches\n    * 1111-11-11, Graph Cluster Randomization\n  * 1111-11-11, [Sample Ratio Mismatch](./Advanced/practical/srm.qmd)\n    * 1111-11-11, Detection Methods\n    * 1111-11-11, Root Cause Analysis\n    * 1111-11-11, Prevention Strategies\n\n## Platform and Specialization (지속적)\n\n#### Experimentation Platform Architecture\n\n실험이 일회성이 아닌 조직의 표준 프로세스가 되려면 확장 가능한 인프라가 필요하다. Assignment service, logging, analysis engine의 설계는 실험의 신뢰성과 효율성을 결정한다. Feature flag integration과 auto-stopping 같은 자동화는 실험 운영 비용을 획기적으로 줄인다.\n\n* [Platform Design and Infrastructure](./Platform/index.qmd)\n  * 1111-11-11, [Core Components](./Platform/components.qmd)\n    * 1111-11-11, Assignment Service (배정 서비스)\n    * 1111-11-11, Logging and Tracking (로깅과 추적)\n    * 1111-11-11, Analysis Engine (분석 엔진)\n    * 1111-11-11, Reporting Dashboard (보고 대시보드)\n  * 1111-11-11, [Technical Considerations](./Platform/technical.qmd)\n    * 1111-11-11, Consistent Hashing for Assignment\n    * 1111-11-11, Experiment Overlap and Orthogonality\n    * 1111-11-11, Feature Flag Integration\n    * 1111-11-11, A/A Testing for Validation\n  * 1111-11-11, [Scale and Automation](./Platform/automation.qmd)\n    * 1111-11-11, Auto-stopping Rules (자동중단규칙)\n    * 1111-11-11, Winner Selection Algorithms\n    * 1111-11-11, Multi-objective Optimization\n\n### Domain-Specific Applications\n\n제품 최적화, 마케팅, 의료 등 도메인마다 고유한 제약과 요구사항이 있다. Healthcare의 경우 FDA guidance를 따라야 하며, marketplace 실험은 양면 시장의 특성을 고려해야 한다. 도메인 특화 지식이 실험 설계의 성공 여부를 결정한다.\n\n* [Industry Applications](./Advanced/applications/index.qmd)\n  * 1111-11-11, [Product Optimization](./Advanced/applications/product.qmd)\n    * 1111-11-11, UI/UX Experiments\n    * 1111-11-11, Recommendation System Testing\n    * 1111-11-11, Search Ranking Experiments\n  * 1111-11-11, [Growth and Marketing](./Advanced/applications/growth.qmd)\n    * 1111-11-11, Conversion Funnel Optimization\n    * 1111-11-11, Pricing Experiments\n    * 1111-11-11, Email and Notification Testing\n  * 1111-11-11, [Healthcare Applications](./Advanced/applications/healthcare.qmd)\n    * 1111-11-11, Adaptive Clinical Trials\n    * 1111-11-11, Response-Adaptive Randomization\n    * 1111-11-11, Platform Trials\n    * 1111-11-11, Regulatory Considerations (FDA Guidance)\n\n### Research Frontiers\n\nRL과의 통합, differential privacy, causal discovery 등은 차세대 실험 방법론이다. Off-policy evaluation은 과거 실험 데이터로 새로운 정책을 평가할 수 있게 하여 실험 비용을 줄인다. 이 분야의 최신 연구를 추적하면 경쟁 우위를 확보할 수 있다.\n\n* [Emerging Topics and Research](./Advanced/research/index.qmd)\n  * 1111-11-11, [Reinforcement Learning Integration](./Advanced/research/rl.qmd)\n    * 1111-11-11, Bandits as RL Problems\n    * 1111-11-11, Deep RL for Experimentation\n    * 1111-11-11, Off-policy Evaluation\n  * 1111-11-11, [Privacy-Preserving Experiments](./Advanced/research/privacy.qmd)\n    * 1111-11-11, Differential Privacy in Experiments\n    * 1111-11-11, Federated A/B Testing\n  * 1111-11-11, [Causal Discovery](./Advanced/research/causal_discovery.qmd)\n    * 1111-11-11, Constraint-based Methods\n    * 1111-11-11, Score-based Methods\n    * 1111-11-11, Experimentation for Graph Learning\n\n## References\n\n**Books:**\n- Kohavi, Tang, and Xu (2020). \"Trustworthy Online Controlled Experiments\"\n- Lattimore and Szepesvári (2020). \"Bandit Algorithms\"\n- Imbens and Rubin (2015). \"Causal Inference for Statistics, Social, and Biomedical Sciences\"\n- Pearl and Mackenzie (2018). \"The Book of Why\"\n\n**Papers:**\n- Thompson (1933). \"On the likelihood that one unknown probability exceeds another\"\n- Auer et al. (2002). \"Finite-time analysis of the multiarmed bandit problem\"\n- Deng et al. (2013). \"Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data\" (CUPED)\n\n**Online Courses:**\n- Stanford CS234: Reinforcement Learning\n- MIT 6.S897: Machine Learning for Healthcare\n","srcMarkdownNoYaml":"\n\n# Experimentation\n\n* 실험 설계와 인과추론의 체계적 이해를 위한 구조화된 학습 경로를 제시\n* Epidemiology 기반 지식을 활용하여 A/B Testing과 Multi-Armed Bandit을 포함한 Experimentation 전반을 학습할 수 있도록 구성\n\n## Experimentation 하위구조\n\n* Experimentation\n  * Epidemiolog\n  * AB_test\n  * Fundamentals\n  * MAB (Multi-Armed Badit)\n  * Causal_inference\n  * Advanced\n  * Platform\n\n## Learning Path\n\n이 학습 경로는 **Epidemiology → Classical A/B Testing → Multi-Armed Bandits → Advanced Methods** 순서로 \n실험 설계와 인과추론을 체계적으로 학습하도록 설계\n\n### Core Concepts Connection Map\n\n```\nEpidemiology RCT → A/B Testing → Sequential Testing → Adaptive Testing → MAB\n       ↓              ↓                                        ↓\nCausal Inference → DAG/SUTVA ────────────────────→ Interference Handling\n       ↓              ↓                                        ↓\nEffect Measures → Lift/Uplift ──────────────────→ Heterogeneous TE\n       ↓\nSample Size → Power Analysis → Sequential Monitoring → Variance Reduction\n```\n\n### Key Mathematical Connections\n\n| Epidemiology | Experimentation | 수식 |\n|-------------|-----------------|------|\n| Relative Risk | Lift | $\\frac{P(Y=1|T=1)}{P(Y=1|T=0)}$ |\n| Risk Difference | Absolute Uplift | $P(Y=1|T=1) - P(Y=1|T=0)$ |\n| NNT | Number to Convert | $\\frac{1}{RD}$ |\n| Effect Modification | HTE | $E[Y(1)-Y(0)|X]$ |\n\n### Recommended Study Time\n\n```\nPhase 1: 1-2개월 (주 10-15시간)\n  - Epidemiology: 2-3주\n  - Statistics: 1주\n  - Classical A/B: 2-3주\n\nPhase 2: 2-3개월 (주 10-15시간)\n  - Advanced A/B: 2-3주\n  - Causal Inference: 3-4주 ⭐ (가장 중요)\n  - MAB Fundamentals: 2주\n  - Classical Algorithms: 2-3주\n\nPhase 3: 2-3개월 (주 8-12시간)\n  - HTE: 2-3주\n  - Variance Reduction: 2주\n  - Advanced Bandits: 3-4주\n  - Practical Challenges: 2-3주\n\nPhase 4: 지속적 (프로젝트 기반)\n  - Platform: 필요시\n  - Applications: 도메인별\n  - Research: 관심분야별\n```\n\n## Foundations (1-2개월)\n\n### Epidemiology Foundations\n\nRCT(무작위 대조 시험)는 A/B Testing의 직접적 원형이며, 인과추론의 수학적 프레임워크(Potential Outcomes, SUTVA)가 실험 설계의 이론적 기초를 제공한다. 역학의 effect measures(RR, RD, NNT)는 디지털 실험의 lift와 uplift 지표로 직접 대응되며, bias와 confounding 개념은 실험의 타당도를 판단하는 핵심 도구다. Sample size와 power 계산은 모든 실험 설계의 출발점이다.\n\n* [Epidemiology Overview](./Epidemiology/index.qmd) ⭐\n  * Study Design Framework\n    * 1111-11-11, [Randomized Controlled Trial (무작위 대조 시험)](./Epidemiology/experimental/rct.qmd) ⭐\n    * 1111-11-11, [Quasi-experimental Studies (준실험 연구)](./Epidemiology/experimental/quasi_experimental.qmd)\n    * 1111-11-11, [Factorial Design (요인 설계)](./Epidemiology/experimental/factorial_design.qmd)\n  * Measures of Association and Impact\n    * 1111-11-11, [Relative Risk and Risk Difference](./Epidemiology/measures/relative_risk.qmd)\n    * 1111-11-11, [Effect Size Measures](./Epidemiology/measures/effect_size.qmd)\n    * 1111-11-11, [Number Needed to Treat](./Epidemiology/measures/nnt.qmd)\n  * Bias and Confounding\n    * 1111-11-11, [Selection Bias](./Epidemiology/bias/selection_bias.qmd)\n    * 1111-11-11, [Information Bias](./Epidemiology/bias/information_bias.qmd)\n    * 1111-11-11, [Confounding and Control](./Epidemiology/confounding/index.qmd)\n  * Study Design Fundamentals\n    * 1111-11-11, [Sample Size and Power](./Epidemiology/fundamentals/sample_size_power.qmd) ⭐\n    * 1111-11-11, [Randomization Methods](./Epidemiology/fundamentals/randomization.qmd) ⭐\n\n### Statistical Foundations\n\n가설 검정의 Type I/II 오류와 statistical power 개념은 실험 결과 해석의 필수 요소다. Multiple testing problem은 여러 지표를 동시에 평가하거나 중간 결과를 확인할 때 발생하는 false positive를 통제하는 방법을 제공한다. Effect size와 MDE(최소감지효과) 이해 없이는 실무적으로 의미 있는 실험 설계가 불가능하다.\n\n\n* [Statistical Foundations for Experimentation](./Fundamentals/statistics.qmd)\n  * 1111-11-11, [Hypothesis Testing Framework](./Fundamentals/hypothesis_testing.qmd)\n    * 1111-11-11, Type I and Type II Errors (1종·2종 오류)\n    * 1111-11-11, Statistical Power (검정력)\n    * 1111-11-11, p-values and Significance (p-값과 유의성)\n  * 1111-11-11, [Effect Size and Practical Significance](./Fundamentals/effect_size.qmd)\n    * 1111-11-11, Minimum Detectable Effect (MDE, 최소감지효과)\n    * 1111-11-11, Cohen's d and Standardized Effect Sizes\n  * 1111-11-11, [Multiple Testing Problem](./Fundamentals/multiple_testing.qmd)\n    * 1111-11-11, Family-wise Error Rate (FWER)\n    * 1111-11-11, False Discovery Rate (FDR)\n    * 1111-11-11, Bonferroni and Holm Corrections\n  * 1111-11-11, [Confidence Intervals](./Fundamentals/confidence_intervals.qmd)\n    * 1111-11-11, Interpretation and Common Mistakes\n    * 1111-11-11, Bootstrap Methods\n\n### Classical A/B Testing Fundamentals\n\n디지털 제품에서 RCT 원리를 적용하는 구체적 방법론이다. Randomization unit 선택, traffic allocation, metric selection 등 실무적 의사결정 프레임워크를 제공한다. Fixed-horizon testing의 원칙과 \"peeking problem\"을 이해해야 sequential testing과 MAB의 필요성을 정확히 파악할 수 있다. 대부분의 실무 실험이 이 방법으로 진행되므로 가장 실용적이다.\n\n\n* [Classical A/B Testing](./AB_test/index.qmd) ⭐\n  * 1111-11-11, [A/B Test Design Principles](./AB_test/design_principles.qmd)\n    * 1111-11-11, Hypothesis Formulation (가설 수립)\n    * 1111-11-11, Metric Selection (지표 선택)\n    * 1111-11-11, Randomization Unit (무작위 배정 단위)\n    * 1111-11-11, Traffic Allocation (트래픽 할당)\n  * 1111-11-11, [Sample Size Calculation](./AB_test/sample_size.qmd) ⭐\n    * 1111-11-11, Power Analysis for A/B Tests\n    * 1111-11-11, Continuous vs. Binary Metrics\n    * 1111-11-11, Unequal Allocation\n  * 1111-11-11, [Randomization Strategies](./AB_test/randomization.qmd)\n    * 1111-11-11, Simple Randomization\n    * 1111-11-11, Stratified Randomization\n    * 1111-11-11, Consistent Hashing\n  * 1111-11-11, [Analysis Methods](./AB_test/analysis.qmd)\n    * 1111-11-11, t-tests and z-tests\n    * 1111-11-11, Chi-square Tests\n    * 1111-11-11, Non-parametric Tests (Mann-Whitney)\n  * 1111-11-11, [Duration and Stopping Rules](./AB_test/duration.qmd)\n    * 1111-11-11, Fixed Horizon Testing\n    * 1111-11-11, When to Stop (언제 중단할 것인가)\n    * 1111-11-11, Peeking Problem (중간확인 문제)\n\n## Core Methods (2-3개월)\n\n### Advanced A/B Testing\n\nSequential testing은 실험 진행 중 중간 결과를 확인하면서도 Type I 오류를 통제하는 방법으로, 실무에서 가장 많이 요구되는 기술이다. Bayesian A/B testing은 \"승률 확률\" 같은 비즈니스 친화적 해석을 제공한다. Multi-variate testing은 여러 요소의 상호작용 효과를 동시에 평가할 수 있어 복잡한 제품 최적화에 필수적이다.\n\n\n* [Advanced A/B Testing Techniques](./AB_test/advanced/index.qmd)\n  * 1111-11-11, [Sequential Testing](./AB_test/advanced/sequential.qmd) ⭐\n    * 1111-11-11, Group Sequential Designs (그룹순차설계)\n    * 1111-11-11, Sequential Probability Ratio Test (SPRT)\n    * 1111-11-11, Alpha Spending Functions\n    * 1111-11-11, Always-Valid Inference\n  * 1111-11-11, [Bayesian A/B Testing](./AB_test/advanced/bayesian.qmd)\n    * 1111-11-11, Prior Distribution Selection (사전분포 선택)\n    * 1111-11-11, Posterior Probability (사후확률)\n    * 1111-11-11, Credible Intervals (신용구간)\n    * 1111-11-11, Probability of Being Best\n  * 1111-11-11, [Multi-variate Testing](./AB_test/advanced/multivariate.qmd)\n    * 1111-11-11, Full Factorial Designs (완전요인설계)\n    * 1111-11-11, Fractional Factorial Designs (부분요인설계)\n    * 1111-11-11, Interaction Effects (상호작용효과)\n  * 1111-11-11, [A/A Testing](./AB_test/advanced/aa_testing.qmd)\n    * 1111-11-11, Platform Validation\n    * 1111-11-11, Sample Ratio Mismatch (SRM) Detection\n\n### Causal Inference Framework\n\nPotential outcomes와 DAG는 \"왜 무작위 배정이 중요한가\", \"어떤 변수를 통제해야 하는가\"에 대한 수학적으로 엄밀한 답을 제공한다. SUTVA 위반(network effects, spillover)은 디지털 실험에서 흔히 발생하며, 이를 감지하고 처리하는 방법이 필요하다. HTE(heterogeneous treatment effects) 분석의 이론적 기초가 되어 \"어떤 사용자에게 효과적인가\"를 과학적으로 답할 수 있게 한다.\n\n* [Causal Inference for Experimentation](./Causal_inference/index.qmd) ⭐\n  * 1111-11-11, [Potential Outcomes Framework](./Causal_inference/potential_outcomes.qmd) ⭐\n    * 1111-11-11, Rubin Causal Model (루빈 인과 모형)\n    * 1111-11-11, Average Treatment Effect (ATE, 평균처치효과)\n    * 1111-11-11, CATE and ITE (조건부·개별 처치효과)\n    * 1111-11-11, SUTVA (안정단위처치값 가정)\n  * 1111-11-11, [Directed Acyclic Graphs](./Causal_inference/dag.qmd) ⭐\n    * 1111-11-11, Causal Pathways (인과경로)\n    * 1111-11-11, Confounding Paths (교란경로)\n    * 1111-11-11, Colliders and Selection Bias (충돌자와 선택편향)\n    * 1111-11-11, d-separation (d-분리)\n    * 1111-11-11, Backdoor and Frontdoor Criteria\n  * 1111-11-11, [Assignment Mechanisms](./Causal_inference/assignment.qmd)\n    * 1111-11-11, Intent-to-Treat (ITT) Analysis\n    * 1111-11-11, Per-Protocol Analysis\n    * 1111-11-11, As-Treated Analysis\n    * 1111-11-11, Complier Average Causal Effect (CACE)\n  * 1111-11-11, [Interference and Spillover](./Causal_inference/interference.qmd)\n    * 1111-11-11, Network Effects (네트워크 효과)\n    * 1111-11-11, Cluster Randomization (군집 무작위배정)\n    * 1111-11-11, Switchback Experiments\n    * 1111-11-11, Geo-experiments\n\n### Multi-Armed Bandit Fundamentals\n\nA/B Testing이 \"학습 후 의사결정\"이라면, MAB는 \"학습과 최적화를 동시에\" 수행한다. Exploration-exploitation trade-off는 제한된 자원(트래픽)으로 최대 성과를 내는 전략의 핵심이다. Regret 개념은 \"실험 비용\"을 수학적으로 정량화하여 알고리즘 간 비교를 가능하게 한다.\n\n* [Multi-Armed Bandits Overview](./MAB/index.qmd) ⭐\n  * 1111-11-11, [MAB Problem Formulation](./MAB/formulation.qmd)\n    * 1111-11-11, Exploration vs. Exploitation Trade-off\n    * 1111-11-11, Regret Definition (후회 정의)\n      * Cumulative Regret (누적 후회)\n      * Simple Regret (단순 후회)\n    * 1111-11-11, Reward Distributions (보상 분포)\n    * 1111-11-11, Stationarity Assumptions (정상성 가정)\n  * 1111-11-11, [Performance Metrics](./MAB/metrics.qmd)\n    * 1111-11-11, Expected Cumulative Regret\n    * 1111-11-11, Best Arm Identification\n    * 1111-11-11, Sample Complexity\n    * 1111-11-11, Convergence Rates\n\n### Classical MAB Algorithms\n\nEpsilon-greedy는 가장 단순하지만 exploration 비율을 수동으로 조정해야 하는 한계가 있다. UCB는 불확실성을 자동으로 정량화하여 exploration을 관리하며, 이론적 regret bound가 증명되었다. Thompson Sampling은 1933년 의학 실험을 위해 개발된 알고리즘으로, 실무에서 가장 성능이 좋고 구현이 간단하여 광범위하게 사용된다.\n\n\n* [Classical Bandit Algorithms](./MAB/classical/index.qmd)\n  * 1111-11-11, [Epsilon-Greedy Methods](./MAB/classical/epsilon_greedy.qmd)\n    * 1111-11-11, Fixed Epsilon Strategy\n    * 1111-11-11, Decaying Epsilon Strategy\n    * 1111-11-11, Theoretical Regret Bounds\n  * 1111-11-11, [Upper Confidence Bound (UCB)](./MAB/classical/ucb.qmd)\n    * 1111-11-11, UCB1 Algorithm\n    * 1111-11-11, UCB-Tuned\n    * 1111-11-11, Bayesian UCB\n    * 1111-11-11, KL-UCB\n  * 1111-11-11, [Thompson Sampling](./MAB/classical/thompson_sampling.qmd) ⭐\n    * 1111-11-11, Beta-Bernoulli Thompson Sampling\n    * 1111-11-11, Gaussian Thompson Sampling\n    * 1111-11-11, Theoretical Properties\n    * 1111-11-11, Historical Context (의학 실험 기원)\n\n### MAB vs. A/B Testing\n\n두 방법의 trade-off를 이해해야 상황에 맞는 선택이 가능하다. A/B Testing은 statistical validity가 명확하고 효과 크기를 정확히 추정하지만, MAB는 실험 중 기회비용을 최소화한다. 비즈니스 목표(정확한 측정 vs. 빠른 최적화), 트래픽 규모, 의사결정 맥락에 따라 최적 방법이 달라진다.\n\n## Advanced Applications (2-3개월)\n\n### Heterogeneous Treatment Effects\n\n\"평균적으로 효과가 있다\"는 것과 \"모든 사용자에게 효과가 있다\"는 것은 다르다. HTE 분석을 통해 어떤 사용자 세그먼트에서 효과가 크고 작은지 파악할 수 있다. Causal forests와 meta-learners 같은 ML 방법은 수백 개의 특성 조합에서 효과 패턴을 자동으로 발견한다. 개인화 전략 수립의 과학적 기반이 된다.\n\n\n* 1111-11-11, [When to Use MAB vs. A/B Testing](./MAB/comparison.qmd) ⭐\n  * 1111-11-11, Trade-offs and Decision Framework\n  * 1111-11-11, Statistical Validity Considerations\n  * 1111-11-11, Business Context and Goals\n  * 1111-11-11, Hybrid Approaches\n\n\n* [Heterogeneous Treatment Effects](./Causal_inference/hte/index.qmd) ⭐\n  * 1111-11-11, [Subgroup Analysis](./Causal_inference/hte/subgroup.qmd)\n    * 1111-11-11, Pre-specified Subgroups\n    * 1111-11-11, Multiple Comparison Corrections\n    * 1111-11-11, Statistical vs. Practical Significance\n  * 1111-11-11, [Effect Modification Analysis](./Causal_inference/hte/effect_modification.qmd)\n    * 1111-11-11, Interaction Terms (상호작용항)\n    * 1111-11-11, Stratified Analysis (층화분석)\n  * 1111-11-11, [Machine Learning Methods](./Causal_inference/hte/ml_methods.qmd)\n    * 1111-11-11, Causal Forests\n    * 1111-11-11, Meta-learners (S-, T-, X-learner)\n    * 1111-11-11, Double Machine Learning (DML)\n    * 1111-11-11, BART (Bayesian Additive Regression Trees)\n\n### Variance Reduction Techniques\n\n동일한 sample size로 더 정확한 추정이 가능하면, 실험 기간을 단축하거나 더 작은 효과를 감지할 수 있다. CUPED는 실험 전 데이터(baseline)를 활용해 분산을 최대 50% 이상 줄일 수 있어 실무에서 표준 기법이 되었다. Stratification과 regression adjustment는 역학에서 검증된 방법으로, 디지털 실험에 직접 적용 가능하다.\n\n* [Variance Reduction Methods](./Advanced/variance_reduction/index.qmd) ⭐\n  * 1111-11-11, [Pre-experiment Methods](./Advanced/variance_reduction/pre_experiment.qmd)\n    * 1111-11-11, Stratification (층화)\n    * 1111-11-11, Matched Pair Design (대응설계)\n    * 1111-11-11, Blocking (블록화)\n  * 1111-11-11, [Post-experiment Methods](./Advanced/variance_reduction/post_experiment.qmd)\n    * 1111-11-11, CUPED (Controlled-experiment Using Pre-Experiment Data) ⭐\n    * 1111-11-11, CUPAC (CUPED with Asymptotic Confidence)\n    * 1111-11-11, Regression Adjustment (회귀조정)\n    * 1111-11-11, Difference-in-Differences (이중차분법)\n\n### Contextual and Advanced Bandits\n\nContextual bandits는 사용자 특성(context)을 고려해 개인화된 의사결정을 내린다. Non-stationary bandits는 시간에 따라 최적 선택지가 변하는 현실을 반영한다. 추천 시스템, 동적 가격 책정, 개인화 마케팅 등 복잡한 실무 문제에 적용된다.\n\n* [Advanced Bandit Methods](./MAB/advanced/index.qmd)\n  * 1111-11-11, [Contextual Bandits](./MAB/advanced/contextual.qmd)\n    * 1111-11-11, Linear Contextual Bandits\n    * 1111-11-11, LinUCB Algorithm\n    * 1111-11-11, Neural Bandits\n    * 1111-11-11, Policy Gradient Methods\n  * 1111-11-11, [Non-stationary Bandits](./MAB/advanced/non_stationary.qmd)\n    * 1111-11-11, Sliding Window Approaches\n    * 1111-11-11, Discounted Rewards\n    * 1111-11-11, Change Detection Methods\n    * 1111-11-11, Switching Bandits\n  * 1111-11-11, [Structured Bandits](./MAB/advanced/structured.qmd)\n    * 1111-11-11, Combinatorial Bandits\n    * 1111-11-11, Dueling Bandits\n    * 1111-11-11, Ranking Bandits\n\n### Practical Implementation Challenges\n\n이론적으로 완벽한 실험도 실무에서는 metric 정의, novelty effects, network interference, SRM 등 다양한 문제에 직면한다. 이러한 문제들을 감지하고 완화하는 방법을 모르면 잘못된 의사결정으로 이어진다. North star metric과 guardrail metric 설정은 실험 프로그램의 성공을 좌우한다.\n\n\n* [Practical Experimentation Challenges](./Advanced/practical/index.qmd)\n  * 1111-11-11, [Metric Design](./Advanced/practical/metrics.qmd)\n    * 1111-11-11, North Star Metrics (핵심지표)\n    * 1111-11-11, Proxy Metrics (대리지표)\n    * 1111-11-11, Guardrail Metrics (가드레일지표)\n    * 1111-11-11, Long-term vs. Short-term Metrics\n  * 1111-11-11, [Novelty and Primacy Effects](./Advanced/practical/novelty_effects.qmd)\n    * 1111-11-11, Detection Methods\n    * 1111-11-11, Mitigation Strategies\n  * 1111-11-11, [Network Effects and Interference](./Advanced/practical/interference.qmd)\n    * 1111-11-11, Detection of SUTVA Violations\n    * 1111-11-11, Cluster-based Approaches\n    * 1111-11-11, Graph Cluster Randomization\n  * 1111-11-11, [Sample Ratio Mismatch](./Advanced/practical/srm.qmd)\n    * 1111-11-11, Detection Methods\n    * 1111-11-11, Root Cause Analysis\n    * 1111-11-11, Prevention Strategies\n\n## Platform and Specialization (지속적)\n\n#### Experimentation Platform Architecture\n\n실험이 일회성이 아닌 조직의 표준 프로세스가 되려면 확장 가능한 인프라가 필요하다. Assignment service, logging, analysis engine의 설계는 실험의 신뢰성과 효율성을 결정한다. Feature flag integration과 auto-stopping 같은 자동화는 실험 운영 비용을 획기적으로 줄인다.\n\n* [Platform Design and Infrastructure](./Platform/index.qmd)\n  * 1111-11-11, [Core Components](./Platform/components.qmd)\n    * 1111-11-11, Assignment Service (배정 서비스)\n    * 1111-11-11, Logging and Tracking (로깅과 추적)\n    * 1111-11-11, Analysis Engine (분석 엔진)\n    * 1111-11-11, Reporting Dashboard (보고 대시보드)\n  * 1111-11-11, [Technical Considerations](./Platform/technical.qmd)\n    * 1111-11-11, Consistent Hashing for Assignment\n    * 1111-11-11, Experiment Overlap and Orthogonality\n    * 1111-11-11, Feature Flag Integration\n    * 1111-11-11, A/A Testing for Validation\n  * 1111-11-11, [Scale and Automation](./Platform/automation.qmd)\n    * 1111-11-11, Auto-stopping Rules (자동중단규칙)\n    * 1111-11-11, Winner Selection Algorithms\n    * 1111-11-11, Multi-objective Optimization\n\n### Domain-Specific Applications\n\n제품 최적화, 마케팅, 의료 등 도메인마다 고유한 제약과 요구사항이 있다. Healthcare의 경우 FDA guidance를 따라야 하며, marketplace 실험은 양면 시장의 특성을 고려해야 한다. 도메인 특화 지식이 실험 설계의 성공 여부를 결정한다.\n\n* [Industry Applications](./Advanced/applications/index.qmd)\n  * 1111-11-11, [Product Optimization](./Advanced/applications/product.qmd)\n    * 1111-11-11, UI/UX Experiments\n    * 1111-11-11, Recommendation System Testing\n    * 1111-11-11, Search Ranking Experiments\n  * 1111-11-11, [Growth and Marketing](./Advanced/applications/growth.qmd)\n    * 1111-11-11, Conversion Funnel Optimization\n    * 1111-11-11, Pricing Experiments\n    * 1111-11-11, Email and Notification Testing\n  * 1111-11-11, [Healthcare Applications](./Advanced/applications/healthcare.qmd)\n    * 1111-11-11, Adaptive Clinical Trials\n    * 1111-11-11, Response-Adaptive Randomization\n    * 1111-11-11, Platform Trials\n    * 1111-11-11, Regulatory Considerations (FDA Guidance)\n\n### Research Frontiers\n\nRL과의 통합, differential privacy, causal discovery 등은 차세대 실험 방법론이다. Off-policy evaluation은 과거 실험 데이터로 새로운 정책을 평가할 수 있게 하여 실험 비용을 줄인다. 이 분야의 최신 연구를 추적하면 경쟁 우위를 확보할 수 있다.\n\n* [Emerging Topics and Research](./Advanced/research/index.qmd)\n  * 1111-11-11, [Reinforcement Learning Integration](./Advanced/research/rl.qmd)\n    * 1111-11-11, Bandits as RL Problems\n    * 1111-11-11, Deep RL for Experimentation\n    * 1111-11-11, Off-policy Evaluation\n  * 1111-11-11, [Privacy-Preserving Experiments](./Advanced/research/privacy.qmd)\n    * 1111-11-11, Differential Privacy in Experiments\n    * 1111-11-11, Federated A/B Testing\n  * 1111-11-11, [Causal Discovery](./Advanced/research/causal_discovery.qmd)\n    * 1111-11-11, Constraint-based Methods\n    * 1111-11-11, Score-based Methods\n    * 1111-11-11, Experimentation for Graph Learning\n\n## References\n\n**Books:**\n- Kohavi, Tang, and Xu (2020). \"Trustworthy Online Controlled Experiments\"\n- Lattimore and Szepesvári (2020). \"Bandit Algorithms\"\n- Imbens and Rubin (2015). \"Causal Inference for Statistics, Social, and Biomedical Sciences\"\n- Pearl and Mackenzie (2018). \"The Book of Why\"\n\n**Papers:**\n- Thompson (1933). \"On the likelihood that one unknown probability exceeds another\"\n- Auer et al. (2002). \"Finite-time analysis of the multiarmed bandit problem\"\n- Deng et al. (2013). \"Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data\" (CUPED)\n\n**Online Courses:**\n- Stanford CS234: Reinforcement Learning\n- MIT 6.S897: Machine Learning for Healthcare\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../js.html","../../signup.html"],"output-file":"index.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../theme.scss"],"dark":["cosmo","../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Experimentation","subtitle":"Comprehensive Guide to Online Experimentation and Causal Inference","description":"From epidemiological foundations to modern adaptive algorithms -\nA systematic path to mastering A/B testing, Multi-Armed Bandits, and causal inference\n","categories":["Experimentation"],"author":"Kwangmin Kim","date":"11/10/2025","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}