{"title":"TokenTextSplitter","markdown":{"yaml":{"title":"TokenTextSplitter"},"headingText":"tiktoken","containsRefs":false,"markdown":"\n\n\n\n\n언어 모델에는 토큰 제한이 있습니다. 따라서 토큰 제한을 초과하지 않아야 합니다.\n\n`TokenTextSplitter` 는 텍스트를 토큰 수를 기반으로 청크를 생성할 때 유용합니다.\n\n\n\n\n`tiktoken` 은 OpenAI에서 만든 빠른 `BPE Tokenizer` 입니다.\n\n\n- `./data/appendix-keywords.txt` 파일을 열어 내용을 읽어들입니다.\n- 읽어들인 내용을 `file` 변수에 저장합니다.\n\n\n파일로부터 읽은 파일의 일부 내용을 출력합니다.\n\n\n`CharacterTextSplitter`를 사용하여 텍스트를 분할합니다.\n\n- `from_tiktoken_encoder` 메서드를 사용하여 Tiktoken 인코더 기반의 텍스트 분할기를 초기화합니다.\n\n\n분할된 청크의 개수를 출력합니다.\n\n\ntexts 리스트의 첫 번째 요소를 출력합니다.\n\n\n**참고**\n\n- `CharacterTextSplitter.from_tiktoken_encoder`를 사용하는 경우, 텍스트는 `CharacterTextSplitter`에 의해서만 분할되고 `tiktoken` 토크나이저는 분할된 텍스트를 병합하는 데 사용됩니다. (이는 분할된 텍스트가 `tiktoken` 토크나이저로 측정한 청크 크기보다 클 수 있음을 의미합니다.)\n\n- `RecursiveCharacterTextSplitter.from_tiktoken_encoder`를 사용하면 분할된 텍스트가 언어 모델에서 허용하는 토큰의 청크 크기보다 크지 않도록 할 수 있으며, 각 분할은 크기가 더 큰 경우 재귀적으로 분할됩니다. 또한 tiktoken 분할기를 직접 로드할 수 있으며, 이는 각 분할이 청크 크기보다 작음을 보장합니다.\n\n\n## TokenTextSplitter\n\n- `TokenTextSplitter` 클래스를 사용하여 텍스트를 토큰 단위로 분할합니다.\n\n\n## spaCy\n\nspaCy는 Python과 Cython 프로그래밍 언어로 작성된 고급 자연어 처리를 위한 오픈 소스 소프트웨어 라이브러리입니다.\n\nNLTK의 또 다른 대안은 spaCy tokenizer를 사용하는 것입니다.\n\n1. 텍스트가 분할되는 방식: **spaCy tokenizer**에 의해 분할됩니다.\n\n2. chunk size가 측정되는 방법: **문자 수**로 측정됩니다.\n\n\nspaCy 라이브러리를 최신 버전으로 업그레이드하는 pip 명령어입니다.\n\n\n`en_core_web_sm` 모델을 다운로드합니다.\n\n\n- `appendix-keywords.txt` 파일을 열어 내용을 읽어들입니다.\n\n\n일부 내용을 출력하여 확인합니다.\n\n\n- `SpacyTextSplitter` 클래스를 사용하여 텍스트 분할기를 생성합니다.\n\n\n- `text_splitter` 객체의 `split_text` 메서드를 사용하여 `file` 텍스트를 분할합니다.\n\n\n## SentenceTransformers\n\n`SentenceTransformersTokenTextSplitter`는 `sentence-transformer` 모델에 특화된 텍스트 분할기입니다.\n\n기본 동작은 사용하고자 하는 sentence transformer 모델의 토큰 윈도우에 맞게 텍스트를 청크로 분할하는 것입니다.\n\n\n샘플 텍스트를 확인합니다.\n\n\n다음은 `file` 변수에 담긴 텍스트의 토큰의 개수를 세는 코드입니다. 시작과 종료 토큰의 개수를 제외한 후 출력합니다.\n\n\n`splitter.split_text()` 함수를 사용하여 `text_to_split` 변수에 저장된 텍스트를 청크(chunk) 단위로 분할합니다.\n\n\n첫 번째 청크를 출력하여 내용을 확인합니다.\n\n\n## NLTK\n\nNatural Language Toolkit (NLTK)은 Python 프로그래밍 언어로 작성된 영어 자연어 처리(NLP)를 위한 라이브러리와 프로그램 모음입니다.\n\n단순히 \"\\n\\n\"으로 분할하는 대신, NLTK tokenizers를 기반으로 텍스트를 분할하는 데 NLTK를 사용할 수 있습니다.\n\n1. 텍스트 분할 방법: NLTK tokenizer에 의해 분할됩니다.\n2. chunk 크기 측정 방법: 문자 수에 의해 측정됩니다.\n\n\n- `nltk` 라이브러리를 설치하는 pip 명령어입니다.\n- NLTK(Natural Language Toolkit)는 자연어 처리를 위한 파이썬 라이브러리입니다.\n- 텍스트 데이터의 전처리, 토큰화, 형태소 분석, 품사 태깅 등 다양한 NLP 작업을 수행할 수 있습니다.\n\n\nNLTK는 기본 설치 시 모든 데이터를 포함하지 않습니다. 이는 초기 설치 크기를 줄이고, 사용자가 필요한 데이터만 선택적으로 다운로드할 수 있게 합니다. \nNLTK에서 사용할 데이터를 다운로드 받습니다. 다운로드는 \"~/nltk_data\"에 설치됩니다.\n\n샘플 텍스트를 확인합니다.\n\n\n`NLTKTextSplitter` 클래스를 사용하여 텍스트 분할기를 생성합니다.\n\n\n`text_splitter` 객체의 `split_text` 메서드를 사용하여 `file` 텍스트를 분할합니다.\n\n\n## KoNLPy\n\nKoNLPy(Korean NLP in Python)는 한국어 자연어 처리(NLP)를 위한 파이썬 패키지입니다.\n\n토큰 분할은 텍스트를 토큰이라고 하는 더 작고 관리하기 쉬운 단위로 분할하는 과정을 포함합니다.\n\n이러한 토큰은 종종 단어, 구, 기호 또는 추가 처리 및 분석에 중요한 다른 의미 있는 요소입니다.\n\n영어와 같은 언어에서 토큰 분할은 일반적으로 공백과 구두점으로 단어를 분리하는 것을 포함합니다.\n\n토큰 분할의 효과는 언어 구조에 대한 토크나이저의 이해에 크게 의존하며, 이는 의미 있는 토큰 생성을 보장합니다.\n\n영어를 위해 설계된 토크나이저는 한국어와 같은 다른 언어의 고유한 의미 구조를 이해할 수 있는 능력이 없기 때문에 한국어 처리에 효과적으로 사용될 수 없습니다.\n\n### KoNLPy의 Kkma 분석기를 사용한 한국어 토큰 분할\n\n한국어 텍스트의 경우 KoNLPY에는 `Kkma`(Korean Knowledge Morpheme Analyzer)라는 형태소 분석기가 포함되어 있습니다.\n\n`Kkma`는 한국어 텍스트에 대한 상세한 형태소 분석을 제공합니다.\n\n문장을 단어로, 단어를 각각의 형태소로 분해하고 각 토큰에 대한 품사를 식별합니다.\n\n텍스트 블록을 개별 문장으로 분할할 수 있어 긴 텍스트 처리에 특히 유용합니다.\n\n### 사용시 고려사항\n\n`Kkma`는 상세한 분석으로 유명하지만, 이러한 정밀성이 처리 속도에 영향을 미칠 수 있다는 점에 유의해야 합니다. 따라서 `Kkma`는 신속한 텍스트 처리보다 분석적 깊이가 우선시되는 애플리케이션에 가장 적합합니다.\n\n\n- KoNLPy 라이브러리를 설치하는 pip 명령어입니다.\n- KoNLPy는 한국어 자연어 처리를 위한 파이썬 패키지로, 형태소 분석, 품사 태깅, 구문 분석 등의 기능을 제공합니다.\n\n\n샘플 텍스트를 확인합니다.\n\n\nKonlpyTextSplitter를 사용하여 한국어 텍스트를 분할하는 예제입니다.\n\n\n`text_splitter`를 사용하여 `file`를 문장 단위로 분할합니다.\n\n\n## Hugging Face tokenizer\n\nHugging Face는 다양한 토크나이저를 제공합니다.\n\n이 코드에서는 Hugging Face의 토크나이저 중 하나인 GPT2TokenizerFast를 사용하여 텍스트의 토큰 길이를 계산합니다.\n\n텍스트 분할 방식은 다음과 같습니다:\n\n- 전달된 문자 단위로 분할됩니다.\n\n청크 크기 측정 방식은 다음과 같습니다:\n\n- Hugging Face 토크나이저에 의해 계산된 토큰 수를 기준으로 합니다.\n\n\n- `GPT2TokenizerFast` 클래스를 사용하여 `tokenizer` 객체를 생성합니다.\n- `from_pretrained` 메서드를 호출하여 사전 학습된 \"gpt2\" 토크나이저 모델을 로드합니다.\n\n\n샘플 텍스트를 확인합니다.\n\n\n`from_huggingface_tokenizer` 메서드를 통해 허깅페이스 토크나이저(`tokenizer`)를 사용하여 텍스트 분할기를 초기화합니다.\n\n\n1 번째 요소의 분할 결과를 확인합니다.\n\n","srcMarkdownNoYaml":"\n\n\n\n\n언어 모델에는 토큰 제한이 있습니다. 따라서 토큰 제한을 초과하지 않아야 합니다.\n\n`TokenTextSplitter` 는 텍스트를 토큰 수를 기반으로 청크를 생성할 때 유용합니다.\n\n\n\n## tiktoken\n\n`tiktoken` 은 OpenAI에서 만든 빠른 `BPE Tokenizer` 입니다.\n\n\n- `./data/appendix-keywords.txt` 파일을 열어 내용을 읽어들입니다.\n- 읽어들인 내용을 `file` 변수에 저장합니다.\n\n\n파일로부터 읽은 파일의 일부 내용을 출력합니다.\n\n\n`CharacterTextSplitter`를 사용하여 텍스트를 분할합니다.\n\n- `from_tiktoken_encoder` 메서드를 사용하여 Tiktoken 인코더 기반의 텍스트 분할기를 초기화합니다.\n\n\n분할된 청크의 개수를 출력합니다.\n\n\ntexts 리스트의 첫 번째 요소를 출력합니다.\n\n\n**참고**\n\n- `CharacterTextSplitter.from_tiktoken_encoder`를 사용하는 경우, 텍스트는 `CharacterTextSplitter`에 의해서만 분할되고 `tiktoken` 토크나이저는 분할된 텍스트를 병합하는 데 사용됩니다. (이는 분할된 텍스트가 `tiktoken` 토크나이저로 측정한 청크 크기보다 클 수 있음을 의미합니다.)\n\n- `RecursiveCharacterTextSplitter.from_tiktoken_encoder`를 사용하면 분할된 텍스트가 언어 모델에서 허용하는 토큰의 청크 크기보다 크지 않도록 할 수 있으며, 각 분할은 크기가 더 큰 경우 재귀적으로 분할됩니다. 또한 tiktoken 분할기를 직접 로드할 수 있으며, 이는 각 분할이 청크 크기보다 작음을 보장합니다.\n\n\n## TokenTextSplitter\n\n- `TokenTextSplitter` 클래스를 사용하여 텍스트를 토큰 단위로 분할합니다.\n\n\n## spaCy\n\nspaCy는 Python과 Cython 프로그래밍 언어로 작성된 고급 자연어 처리를 위한 오픈 소스 소프트웨어 라이브러리입니다.\n\nNLTK의 또 다른 대안은 spaCy tokenizer를 사용하는 것입니다.\n\n1. 텍스트가 분할되는 방식: **spaCy tokenizer**에 의해 분할됩니다.\n\n2. chunk size가 측정되는 방법: **문자 수**로 측정됩니다.\n\n\nspaCy 라이브러리를 최신 버전으로 업그레이드하는 pip 명령어입니다.\n\n\n`en_core_web_sm` 모델을 다운로드합니다.\n\n\n- `appendix-keywords.txt` 파일을 열어 내용을 읽어들입니다.\n\n\n일부 내용을 출력하여 확인합니다.\n\n\n- `SpacyTextSplitter` 클래스를 사용하여 텍스트 분할기를 생성합니다.\n\n\n- `text_splitter` 객체의 `split_text` 메서드를 사용하여 `file` 텍스트를 분할합니다.\n\n\n## SentenceTransformers\n\n`SentenceTransformersTokenTextSplitter`는 `sentence-transformer` 모델에 특화된 텍스트 분할기입니다.\n\n기본 동작은 사용하고자 하는 sentence transformer 모델의 토큰 윈도우에 맞게 텍스트를 청크로 분할하는 것입니다.\n\n\n샘플 텍스트를 확인합니다.\n\n\n다음은 `file` 변수에 담긴 텍스트의 토큰의 개수를 세는 코드입니다. 시작과 종료 토큰의 개수를 제외한 후 출력합니다.\n\n\n`splitter.split_text()` 함수를 사용하여 `text_to_split` 변수에 저장된 텍스트를 청크(chunk) 단위로 분할합니다.\n\n\n첫 번째 청크를 출력하여 내용을 확인합니다.\n\n\n## NLTK\n\nNatural Language Toolkit (NLTK)은 Python 프로그래밍 언어로 작성된 영어 자연어 처리(NLP)를 위한 라이브러리와 프로그램 모음입니다.\n\n단순히 \"\\n\\n\"으로 분할하는 대신, NLTK tokenizers를 기반으로 텍스트를 분할하는 데 NLTK를 사용할 수 있습니다.\n\n1. 텍스트 분할 방법: NLTK tokenizer에 의해 분할됩니다.\n2. chunk 크기 측정 방법: 문자 수에 의해 측정됩니다.\n\n\n- `nltk` 라이브러리를 설치하는 pip 명령어입니다.\n- NLTK(Natural Language Toolkit)는 자연어 처리를 위한 파이썬 라이브러리입니다.\n- 텍스트 데이터의 전처리, 토큰화, 형태소 분석, 품사 태깅 등 다양한 NLP 작업을 수행할 수 있습니다.\n\n\nNLTK는 기본 설치 시 모든 데이터를 포함하지 않습니다. 이는 초기 설치 크기를 줄이고, 사용자가 필요한 데이터만 선택적으로 다운로드할 수 있게 합니다. \nNLTK에서 사용할 데이터를 다운로드 받습니다. 다운로드는 \"~/nltk_data\"에 설치됩니다.\n\n샘플 텍스트를 확인합니다.\n\n\n`NLTKTextSplitter` 클래스를 사용하여 텍스트 분할기를 생성합니다.\n\n\n`text_splitter` 객체의 `split_text` 메서드를 사용하여 `file` 텍스트를 분할합니다.\n\n\n## KoNLPy\n\nKoNLPy(Korean NLP in Python)는 한국어 자연어 처리(NLP)를 위한 파이썬 패키지입니다.\n\n토큰 분할은 텍스트를 토큰이라고 하는 더 작고 관리하기 쉬운 단위로 분할하는 과정을 포함합니다.\n\n이러한 토큰은 종종 단어, 구, 기호 또는 추가 처리 및 분석에 중요한 다른 의미 있는 요소입니다.\n\n영어와 같은 언어에서 토큰 분할은 일반적으로 공백과 구두점으로 단어를 분리하는 것을 포함합니다.\n\n토큰 분할의 효과는 언어 구조에 대한 토크나이저의 이해에 크게 의존하며, 이는 의미 있는 토큰 생성을 보장합니다.\n\n영어를 위해 설계된 토크나이저는 한국어와 같은 다른 언어의 고유한 의미 구조를 이해할 수 있는 능력이 없기 때문에 한국어 처리에 효과적으로 사용될 수 없습니다.\n\n### KoNLPy의 Kkma 분석기를 사용한 한국어 토큰 분할\n\n한국어 텍스트의 경우 KoNLPY에는 `Kkma`(Korean Knowledge Morpheme Analyzer)라는 형태소 분석기가 포함되어 있습니다.\n\n`Kkma`는 한국어 텍스트에 대한 상세한 형태소 분석을 제공합니다.\n\n문장을 단어로, 단어를 각각의 형태소로 분해하고 각 토큰에 대한 품사를 식별합니다.\n\n텍스트 블록을 개별 문장으로 분할할 수 있어 긴 텍스트 처리에 특히 유용합니다.\n\n### 사용시 고려사항\n\n`Kkma`는 상세한 분석으로 유명하지만, 이러한 정밀성이 처리 속도에 영향을 미칠 수 있다는 점에 유의해야 합니다. 따라서 `Kkma`는 신속한 텍스트 처리보다 분석적 깊이가 우선시되는 애플리케이션에 가장 적합합니다.\n\n\n- KoNLPy 라이브러리를 설치하는 pip 명령어입니다.\n- KoNLPy는 한국어 자연어 처리를 위한 파이썬 패키지로, 형태소 분석, 품사 태깅, 구문 분석 등의 기능을 제공합니다.\n\n\n샘플 텍스트를 확인합니다.\n\n\nKonlpyTextSplitter를 사용하여 한국어 텍스트를 분할하는 예제입니다.\n\n\n`text_splitter`를 사용하여 `file`를 문장 단위로 분할합니다.\n\n\n## Hugging Face tokenizer\n\nHugging Face는 다양한 토크나이저를 제공합니다.\n\n이 코드에서는 Hugging Face의 토크나이저 중 하나인 GPT2TokenizerFast를 사용하여 텍스트의 토큰 길이를 계산합니다.\n\n텍스트 분할 방식은 다음과 같습니다:\n\n- 전달된 문자 단위로 분할됩니다.\n\n청크 크기 측정 방식은 다음과 같습니다:\n\n- Hugging Face 토크나이저에 의해 계산된 토큰 수를 기준으로 합니다.\n\n\n- `GPT2TokenizerFast` 클래스를 사용하여 `tokenizer` 객체를 생성합니다.\n- `from_pretrained` 메서드를 호출하여 사전 학습된 \"gpt2\" 토크나이저 모델을 로드합니다.\n\n\n샘플 텍스트를 확인합니다.\n\n\n`from_huggingface_tokenizer` 메서드를 통해 허깅페이스 토크나이저(`tokenizer`)를 사용하여 텍스트 분할기를 초기화합니다.\n\n\n1 번째 요소의 분할 결과를 확인합니다.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"03-TokenTextSplitter.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"TokenTextSplitter"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}