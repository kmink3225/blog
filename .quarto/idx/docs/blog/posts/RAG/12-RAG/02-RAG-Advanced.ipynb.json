{"title":"LangChain의 RAG 파헤치기","markdown":{"yaml":{"title":"LangChain의 RAG 파헤치기"},"headingText":"1. 질문 처리","containsRefs":false,"markdown":"\n\n\n\n\n![rag-1.png](./assets/rag-1.png)\n\n![rag-2.png](./assets/rag-2.png)\n\n\n질문 처리 단계에서는 사용자의 질문을 받아 이를 처리하고, 관련 데이터를 찾는 작업이 이루어집니다. 이를 위해 다음과 같은 구성 요소들이 필요합니다:\n\n- **데이터 소스 연결**: 질문에 대한 답변을 찾기 위해 다양한 텍스트 데이터 소스에 연결해야 합니다. LangChain은 다양한 데이터 소스와의 연결을 간편하게 설정할 수 있도록 돕습니다.\n- **데이터 인덱싱 및 검색**: 데이터 소스에서 관련 정보를 효율적으로 찾기 위해, 데이터는 인덱싱되어야 합니다. LangChain은 인덱싱 과정을 자동화하고, 사용자의 질문과 관련된 데이터를 검색하는 데 필요한 도구를 제공합니다.\n\n### 2. 답변 생성\n\n관련 데이터를 찾은 후에는 이를 기반으로 사용자의 질문에 답변을 생성해야 합니다. 이 단계에서는 다음 구성 요소가 중요합니다:\n\n- **답변 생성 모델**: LangChain은 고급 자연어 처리(NLP) 모델을 사용하여 검색된 데이터로부터 답변을 생성할 수 있는 기능을 제공합니다. 이러한 모델은 사용자의 질문과 검색된 데이터를 입력으로 받아, 적절한 답변을 생성합니다.\n\n\n\n## 아키텍처\n\n우리는 [Q&A 소개](https://python.langchain.comhttps://python.langchain.com/docs/use_cases/question_answering/)에서 개요한 대로 전형적인 RAG 애플리케이션을 만들 것입니다. 이것은 두 가지 주요 구성 요소를 가지고 있습니다:\n\n- **인덱싱**: 소스에서 데이터를 수집하고 인덱싱하는 파이프라인입니다. _이 작업은 보통 오프라인에서 발생합니다._\n\n- **검색 및 생성**: 실제 RAG 체인으로, 사용자 쿼리를 실행 시간에 받아 인덱스에서 관련 데이터를 검색한 다음, 그 데이터를 모델에 전달합니다.\n\nRAW 데이터에서 답변을 받기까지의 전체 순서는 다음과 같습니다.\n\n### 인덱싱\n\n![](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)\n\n1. **로드**: 먼저 데이터를 로드해야 합니다. 이를 위해 [DocumentLoaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)를 사용할 것입니다.\n2. **분할**: [Text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)는 큰 `Documents`를 더 작은 청크로 나눕니다. 이는 데이터를 인덱싱하고 모델에 전달하는 데 유용하며, 큰 청크는 검색하기 어렵고 모델의 유한한 컨텍스트 창에 맞지 않습니다.\n3. **저장**: 나중에 검색할 수 있도록 분할을 저장하고 인덱싱할 장소가 필요합니다. 이는 종종 [VectorStore](https://python.langchain.com/docs/modules/data_connection/vectorstores/)와 [Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/) 모델을 사용하여 수행됩니다.\n\n### 검색 및 생성\n\n![](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)\n\n1. **검색**: 사용자 입력이 주어지면 [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/)를 사용하여 저장소에서 관련 분할을 검색합니다.\n2. **생성**: [ChatModel](https://python.langchain.com/docs/modules/model_io/chat/) / [LLM](https://python.langchain.com/docs/modules/model_io/llms/)은 질문과 검색된 데이터를 포함한 프롬프트를 사용하여 답변을 생성합니다\n\n\n## 실습에 활용한 문서\n\n소프트웨어정책연구소(SPRi) - 2023년 12월호\n\n- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n- 링크: https://spri.kr/posts/view/23669\n- 파일명: `SPRI_AI_Brief_2023년12월호_F.pdf`\n\n_실습을 위해 다운로드 받은 파일을 `data` 폴더로 복사해 주시기 바랍니다_\n\n\n## 환경설정\n\n\nAPI KEY 를 설정합니다.\n\n\nLangChain으로 구축한 애플리케이션은 여러 단계에 걸쳐 LLM 호출을 여러 번 사용하게 됩니다. 이러한 애플리케이션이 점점 더 복잡해짐에 따라, 체인이나 에이전트 내부에서 정확히 무슨 일이 일어나고 있는지 조사할 수 있는 능력이 매우 중요해집니다. 이를 위한 최선의 방법은 [LangSmith](https://smith.langchain.com)를 사용하는 것입니다.\n\nLangSmith가 필수는 아니지만, 유용합니다. LangSmith를 사용하고 싶다면, 위의 링크에서 가입한 후, 로깅 추적을 시작하기 위해 환경 변수를 설정해야 합니다.\n\n\n## 모듈별로 자세히 살펴보기\n\n\n아래는 [](https://teddylee777.github.io/langchain/rag-naver-news-qa/) 에서 다뤘던 기본적인 RAG 모델을 사용하는 예제입니다.\n\n여기서 각 단계별로 다양한 옵션을 설정하거나 새로운 기법을 적용할 수 있습니다.\n\n\n## 단계 1: 문서 로드(Load Documents)\n\n- [공식문서 링크 - Document loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n\n\n### 웹페이지\n\n`WebBaseLoader`는 지정된 웹 페이지에서 필요한 부분만을 파싱하기 위해 `bs4.SoupStrainer`를 사용합니다.\n\n[참고]\n\n- `bs4.SoupStrainer` 는 편리하게 웹에서 원하는 요소를 가져올 수 있도록 해줍니다.\n\n(예시)\n\n```python\nbs4.SoupStrainer(\n    \"div\",\n    attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]}, # 클래스 명을 입력\n)\n\nbs4.SoupStrainer(\n    \"article\",\n    attrs={\"id\": [\"dic_area\"]}, # 클래스 명을 입력\n)\n```\n\n\n아래의 BBC 뉴스 기사입니다. 영문으로 작성된 기사로 시험해 보고 싶다면, 아래의 주석을 해제하고 실행해 보세요.\n\n\n### PDF\n\n\n### CSV\n\nCSV 는 페이지 번호 대신 행번호로 데이터를 조회합니다.\n\n\n### TXT 파일\n\n\n### 폴더 내의 모든 파일 로드\n\n\n아래는 폴더 내 모든 `.txt` 파일을 로드하는 예시입니다.\n\n\n다음은 폴더내 모든 `.pdf` 파일을 로드하는 예제입니다.\n\n\n### Python\n\n다음은 `.py` 파일을 로드하는 예제입니다.\n\n\n---\n\n\n## 단계 2: 문서 분할(Split Documents)\n\n\n### CharacterTextSplitter\n\n이것은 가장 간단한 방법입니다. 이 방법은 문자를 기준으로 분할합니다(기본값은 \"\\n\\n\") 그리고 청크의 길이를 문자의 수로 측정합니다.\n\n1. 텍스트가 어떻게 분할되는지: 단일 문자 단위\n2. 청크 크기가 어떻게 측정되는지: `len` of characters.\n\n시각화 예제: https://chunkviz.up.railway.app/\n\n\n`CharacterTextSplitter` 클래스는 텍스트를 특정 크기의 청크로 분할하는 기능을 제공합니다.\n\n- `separator` 매개변수는 청크를 구분하는 데 사용되는 문자열을 지정하며, 여기서는 두 개의 개행 문자(`\"\\n\\n\"`)를 사용합니다\n- `chunk_size`는 각 청크의 최대 길이를 결정합니다\n- `chunk_overlap`은 인접한 청크 간에 겹치는 문자의 수를 지정합니다.\n- `length_function`은 청크의 길이를 계산하는 데 사용되는 함수를 결정하며, 기본적으로 문자열의 길이를 반환하는 `len` 함수가 사용됩니다.\n- `is_separator_regex`는 `separator`가 정규 표현식으로 해석될지 여부를 결정하는 불리언 값입니다.\n\n\n이 함수는 `text_splitter` 객체의 `create_documents` 메소드를 사용하여 주어진 텍스트(`state_of_the_union`)를 여러 문서로 분할하고, 그 결과를 `texts` 변수에 저장합니다. 이후 `texts`의 첫 번째 문서를 출력합니다. 이 과정은 텍스트 데이터를 처리하고 분석하기 위한 초기 단계로 볼 수 있으며, 특히 큰 텍스트 데이터를 관리 가능한 크기의 단위로 나누는 데 유용합니다.\n\n\n### RecursiveTextSplitter\n\n이 텍스트 분할기는 일반 텍스트에 권장되는 텍스트 분할기입니다.\n\n1. 텍스트가 어떻게 분할 규칙: list of `separators`\n2. 청크 크기가 어떻게 측정되는가: `len` of characters\n\n\n`RecursiveCharacterTextSplitter` 클래스는 텍스트를 재귀적으로 분할하는 기능을 제공합니다. 이 클래스는 `chunk_size`로 분할할 청크의 크기, `chunk_overlap`으로 인접 청크 간의 겹침 크기, `length_function`으로 청크의 길이를 계산하는 함수, 그리고 `is_separator_regex`로 구분자가 정규 표현식인지 여부를 지정하는 매개변수를 받습니다. 예시에서는 청크 크기를 100, 겹침 크기를 20으로 설정하고, 길이 계산 함수로 `len`을 사용하며, 구분자가 정규 표현식이 아님을 나타내기 위해 `is_separator_regex`를 `False`로 설정합니다.\n\n\n- 지정한 separators 리스트를 순차적으로 시도하며 주어진 문서를 분할합니다.\n- 청크가 충분히 작아질 때까지 순서대로 분할을 시도합니다. 기본 목록은 [\"\\n\\n\", \"\\n\", \" \", \"\"]입니다.\n- 이는 일반적으로 의미적으로 가장 연관성이 강한 텍스트 조각인 것처럼 보이는 모든 단락(그리고 문장, 단어)을 가능한 한 길게 유지하려는 효과가 있습니다.\n\n\n### Semantic Similarity\n\n\n의미적 유사성을 기준으로 텍스트를 분할합니다.\n\n출처: [Greg Kamradt’s Notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb)\n\n높은 수준(high level)에서 문장으로 분할한 다음 3개 문장으로 그룹화한 다음 임베딩 공간에서 유사한 문장을 병합하는 방식입니다.\n\n\n## 3 단계: 임베딩\n\n참고: https://python.langchain.com/docs/integrations/text_embedding\n\n\n### 유료 과금 임베딩(OpenAI)\n\n\n다음은 `OpenAI` 의 지원되는 Embedding 모델들의 목록입니다.\n\n- 기본 값은 `text-embeding-ada-002` 입니다.\n\n\n| MODEL                  | ROUGH PAGES PER DOLLAR | EXAMPLE PERFORMANCE ON MTEB EVAL |\n| ---------------------- | ---------------------- | -------------------------------- |\n| text-embedding-3-small | 62,500                 | 62.3%                            |\n| text-embedding-3-large | 9,615                  | 64.6%                            |\n| text-embedding-ada-002 | 12,500                 | 61.0%                            |\n\n\n### 무료 Open Source 기반 임베딩\n\n\n## 4단계: 벡터스토어 생성(Create Vectorstore)\n\n\n## 5단계: Retriever 생성\n\n리트리버는 구조화되지 않은 쿼리가 주어지면 문서를 반환하는 인터페이스입니다.\n\n리트리버는 문서를 저장할 필요 없이 문서를 반환(또는 검색)하기만 합니다.\n\n- [공식 도큐먼트](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n\n생성된 VectorStore 에 `as_retriver()` 로 가져와서 **Retriever** 를 생성합니다.\n\n\n### 유사도 기반 검색\n\n- 기본값은 코사인 유사도인 `similarity` 가 적용되어 있습니다.\n\n\n`similarity_score_threshold` 는 유사도 기반 검색에서 `score_threshold` 이상인 결과만 반환합니다.\n\n\n`maximum marginal search result` 를 사용하여 검색합니다.\n\n\n### 다양한 쿼리 생성\n\n\n### Ensemble Retriever\n\n\n## 6단계: 프롬프트 생성(Create Prompt)\n\n\n프롬프트 엔지니어링은 주어진 데이터(`context`)를 토대로 우리가 원하는 결과를 도출할 때 중요한 역할을 합니다.\n\n[TIP1]\n\n1. 만약, `retriever` 에서 도출한 결과에서 중요한 정보가 누락된다면 `retriever` 의 로직을 수정해야 합니다.\n2. 만약, `retriever` 에서 도출한 결과가 많은 정보를 포함하고 있지만, `llm` 이 그 중에서 중요한 정보를 찾지 못한거나 원하는 형태로 출력하지 않는다면 프롬프트를 수정해야 합니다.\n\n[TIP2]\n\n1. LangSmith 의 **hub** 에는 검증된 프롬프트가 많이 업로드 되어 있습니다.\n2. 검증된 프롬프트를 활용하거나 약간 수정한다면 비용과 시간을 절약할 수 있습니다.\n\n- https://smith.langchain.com/hub/search?q=rag\n\n\n## 7단계: 언어모델 생성(Create LLM)\n\n\nOpenAI 모델 중 하나를 선택합니다.\n\n- `gpt-3.5-turbo` : OpenAI의 GPT-3.5-turbo 모델\n- `gpt-4-turbo-preview` : OpenAI의 GPT-4-turbo-preview 모델\n\n자세한 비용 체계는 [OpenAI API 모델 리스트 / 요금표](https://teddylee777.github.io/openai/openai-models/)에서 확인할 수 있습니다.\n\n\n다음의 방식으로 토큰 사용량을 확인할 수 있습니다.\n\n\nHuggingFace 에 업로드 되어 있는 오픈소스 모델을 손쉽게 다운로드 받아 사용할 수 있습니다.\n\n아래의 리더보드에서 날마다 성능을 개선하는 오픈소스 리더보드를 확인할 수 있습니다.\n\n- [HuggingFace LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n\n\n## RAG 템플릿 실험\n\n\n> 문서: data/SPRI_AI_Brief_2023년12월호\\_F.pdf (페이지 10)\n\n- LangSmith: https://smith.langchain.com/public/4449e744-f0a0-42d2-a3df-855bd7f41652/r\n\n\n> 문서: data/SPRI_AI_Brief_2023년12월호\\_F.pdf (페이지 12)\n\n- LangSmith: https://smith.langchain.com/public/2b2913c9-6b9c-4a19-bb16-dc2256e2fdbf/r\n\n\n> 문서: data/SPRI_AI_Brief_2023년12월호\\_F.pdf (페이지 14)\n\n- LangSmith: https://smith.langchain.com/public/17ef6df2-b012-4f8e-b0a8-62894d82c097/r\n\n","srcMarkdownNoYaml":"\n\n\n\n\n![rag-1.png](./assets/rag-1.png)\n\n![rag-2.png](./assets/rag-2.png)\n\n### 1. 질문 처리\n\n질문 처리 단계에서는 사용자의 질문을 받아 이를 처리하고, 관련 데이터를 찾는 작업이 이루어집니다. 이를 위해 다음과 같은 구성 요소들이 필요합니다:\n\n- **데이터 소스 연결**: 질문에 대한 답변을 찾기 위해 다양한 텍스트 데이터 소스에 연결해야 합니다. LangChain은 다양한 데이터 소스와의 연결을 간편하게 설정할 수 있도록 돕습니다.\n- **데이터 인덱싱 및 검색**: 데이터 소스에서 관련 정보를 효율적으로 찾기 위해, 데이터는 인덱싱되어야 합니다. LangChain은 인덱싱 과정을 자동화하고, 사용자의 질문과 관련된 데이터를 검색하는 데 필요한 도구를 제공합니다.\n\n### 2. 답변 생성\n\n관련 데이터를 찾은 후에는 이를 기반으로 사용자의 질문에 답변을 생성해야 합니다. 이 단계에서는 다음 구성 요소가 중요합니다:\n\n- **답변 생성 모델**: LangChain은 고급 자연어 처리(NLP) 모델을 사용하여 검색된 데이터로부터 답변을 생성할 수 있는 기능을 제공합니다. 이러한 모델은 사용자의 질문과 검색된 데이터를 입력으로 받아, 적절한 답변을 생성합니다.\n\n\n\n## 아키텍처\n\n우리는 [Q&A 소개](https://python.langchain.comhttps://python.langchain.com/docs/use_cases/question_answering/)에서 개요한 대로 전형적인 RAG 애플리케이션을 만들 것입니다. 이것은 두 가지 주요 구성 요소를 가지고 있습니다:\n\n- **인덱싱**: 소스에서 데이터를 수집하고 인덱싱하는 파이프라인입니다. _이 작업은 보통 오프라인에서 발생합니다._\n\n- **검색 및 생성**: 실제 RAG 체인으로, 사용자 쿼리를 실행 시간에 받아 인덱스에서 관련 데이터를 검색한 다음, 그 데이터를 모델에 전달합니다.\n\nRAW 데이터에서 답변을 받기까지의 전체 순서는 다음과 같습니다.\n\n### 인덱싱\n\n![](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)\n\n1. **로드**: 먼저 데이터를 로드해야 합니다. 이를 위해 [DocumentLoaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)를 사용할 것입니다.\n2. **분할**: [Text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)는 큰 `Documents`를 더 작은 청크로 나눕니다. 이는 데이터를 인덱싱하고 모델에 전달하는 데 유용하며, 큰 청크는 검색하기 어렵고 모델의 유한한 컨텍스트 창에 맞지 않습니다.\n3. **저장**: 나중에 검색할 수 있도록 분할을 저장하고 인덱싱할 장소가 필요합니다. 이는 종종 [VectorStore](https://python.langchain.com/docs/modules/data_connection/vectorstores/)와 [Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/) 모델을 사용하여 수행됩니다.\n\n### 검색 및 생성\n\n![](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)\n\n1. **검색**: 사용자 입력이 주어지면 [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/)를 사용하여 저장소에서 관련 분할을 검색합니다.\n2. **생성**: [ChatModel](https://python.langchain.com/docs/modules/model_io/chat/) / [LLM](https://python.langchain.com/docs/modules/model_io/llms/)은 질문과 검색된 데이터를 포함한 프롬프트를 사용하여 답변을 생성합니다\n\n\n## 실습에 활용한 문서\n\n소프트웨어정책연구소(SPRi) - 2023년 12월호\n\n- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n- 링크: https://spri.kr/posts/view/23669\n- 파일명: `SPRI_AI_Brief_2023년12월호_F.pdf`\n\n_실습을 위해 다운로드 받은 파일을 `data` 폴더로 복사해 주시기 바랍니다_\n\n\n## 환경설정\n\n\nAPI KEY 를 설정합니다.\n\n\nLangChain으로 구축한 애플리케이션은 여러 단계에 걸쳐 LLM 호출을 여러 번 사용하게 됩니다. 이러한 애플리케이션이 점점 더 복잡해짐에 따라, 체인이나 에이전트 내부에서 정확히 무슨 일이 일어나고 있는지 조사할 수 있는 능력이 매우 중요해집니다. 이를 위한 최선의 방법은 [LangSmith](https://smith.langchain.com)를 사용하는 것입니다.\n\nLangSmith가 필수는 아니지만, 유용합니다. LangSmith를 사용하고 싶다면, 위의 링크에서 가입한 후, 로깅 추적을 시작하기 위해 환경 변수를 설정해야 합니다.\n\n\n## 모듈별로 자세히 살펴보기\n\n\n아래는 [](https://teddylee777.github.io/langchain/rag-naver-news-qa/) 에서 다뤘던 기본적인 RAG 모델을 사용하는 예제입니다.\n\n여기서 각 단계별로 다양한 옵션을 설정하거나 새로운 기법을 적용할 수 있습니다.\n\n\n## 단계 1: 문서 로드(Load Documents)\n\n- [공식문서 링크 - Document loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n\n\n### 웹페이지\n\n`WebBaseLoader`는 지정된 웹 페이지에서 필요한 부분만을 파싱하기 위해 `bs4.SoupStrainer`를 사용합니다.\n\n[참고]\n\n- `bs4.SoupStrainer` 는 편리하게 웹에서 원하는 요소를 가져올 수 있도록 해줍니다.\n\n(예시)\n\n```python\nbs4.SoupStrainer(\n    \"div\",\n    attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]}, # 클래스 명을 입력\n)\n\nbs4.SoupStrainer(\n    \"article\",\n    attrs={\"id\": [\"dic_area\"]}, # 클래스 명을 입력\n)\n```\n\n\n아래의 BBC 뉴스 기사입니다. 영문으로 작성된 기사로 시험해 보고 싶다면, 아래의 주석을 해제하고 실행해 보세요.\n\n\n### PDF\n\n\n### CSV\n\nCSV 는 페이지 번호 대신 행번호로 데이터를 조회합니다.\n\n\n### TXT 파일\n\n\n### 폴더 내의 모든 파일 로드\n\n\n아래는 폴더 내 모든 `.txt` 파일을 로드하는 예시입니다.\n\n\n다음은 폴더내 모든 `.pdf` 파일을 로드하는 예제입니다.\n\n\n### Python\n\n다음은 `.py` 파일을 로드하는 예제입니다.\n\n\n---\n\n\n## 단계 2: 문서 분할(Split Documents)\n\n\n### CharacterTextSplitter\n\n이것은 가장 간단한 방법입니다. 이 방법은 문자를 기준으로 분할합니다(기본값은 \"\\n\\n\") 그리고 청크의 길이를 문자의 수로 측정합니다.\n\n1. 텍스트가 어떻게 분할되는지: 단일 문자 단위\n2. 청크 크기가 어떻게 측정되는지: `len` of characters.\n\n시각화 예제: https://chunkviz.up.railway.app/\n\n\n`CharacterTextSplitter` 클래스는 텍스트를 특정 크기의 청크로 분할하는 기능을 제공합니다.\n\n- `separator` 매개변수는 청크를 구분하는 데 사용되는 문자열을 지정하며, 여기서는 두 개의 개행 문자(`\"\\n\\n\"`)를 사용합니다\n- `chunk_size`는 각 청크의 최대 길이를 결정합니다\n- `chunk_overlap`은 인접한 청크 간에 겹치는 문자의 수를 지정합니다.\n- `length_function`은 청크의 길이를 계산하는 데 사용되는 함수를 결정하며, 기본적으로 문자열의 길이를 반환하는 `len` 함수가 사용됩니다.\n- `is_separator_regex`는 `separator`가 정규 표현식으로 해석될지 여부를 결정하는 불리언 값입니다.\n\n\n이 함수는 `text_splitter` 객체의 `create_documents` 메소드를 사용하여 주어진 텍스트(`state_of_the_union`)를 여러 문서로 분할하고, 그 결과를 `texts` 변수에 저장합니다. 이후 `texts`의 첫 번째 문서를 출력합니다. 이 과정은 텍스트 데이터를 처리하고 분석하기 위한 초기 단계로 볼 수 있으며, 특히 큰 텍스트 데이터를 관리 가능한 크기의 단위로 나누는 데 유용합니다.\n\n\n### RecursiveTextSplitter\n\n이 텍스트 분할기는 일반 텍스트에 권장되는 텍스트 분할기입니다.\n\n1. 텍스트가 어떻게 분할 규칙: list of `separators`\n2. 청크 크기가 어떻게 측정되는가: `len` of characters\n\n\n`RecursiveCharacterTextSplitter` 클래스는 텍스트를 재귀적으로 분할하는 기능을 제공합니다. 이 클래스는 `chunk_size`로 분할할 청크의 크기, `chunk_overlap`으로 인접 청크 간의 겹침 크기, `length_function`으로 청크의 길이를 계산하는 함수, 그리고 `is_separator_regex`로 구분자가 정규 표현식인지 여부를 지정하는 매개변수를 받습니다. 예시에서는 청크 크기를 100, 겹침 크기를 20으로 설정하고, 길이 계산 함수로 `len`을 사용하며, 구분자가 정규 표현식이 아님을 나타내기 위해 `is_separator_regex`를 `False`로 설정합니다.\n\n\n- 지정한 separators 리스트를 순차적으로 시도하며 주어진 문서를 분할합니다.\n- 청크가 충분히 작아질 때까지 순서대로 분할을 시도합니다. 기본 목록은 [\"\\n\\n\", \"\\n\", \" \", \"\"]입니다.\n- 이는 일반적으로 의미적으로 가장 연관성이 강한 텍스트 조각인 것처럼 보이는 모든 단락(그리고 문장, 단어)을 가능한 한 길게 유지하려는 효과가 있습니다.\n\n\n### Semantic Similarity\n\n\n의미적 유사성을 기준으로 텍스트를 분할합니다.\n\n출처: [Greg Kamradt’s Notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb)\n\n높은 수준(high level)에서 문장으로 분할한 다음 3개 문장으로 그룹화한 다음 임베딩 공간에서 유사한 문장을 병합하는 방식입니다.\n\n\n## 3 단계: 임베딩\n\n참고: https://python.langchain.com/docs/integrations/text_embedding\n\n\n### 유료 과금 임베딩(OpenAI)\n\n\n다음은 `OpenAI` 의 지원되는 Embedding 모델들의 목록입니다.\n\n- 기본 값은 `text-embeding-ada-002` 입니다.\n\n\n| MODEL                  | ROUGH PAGES PER DOLLAR | EXAMPLE PERFORMANCE ON MTEB EVAL |\n| ---------------------- | ---------------------- | -------------------------------- |\n| text-embedding-3-small | 62,500                 | 62.3%                            |\n| text-embedding-3-large | 9,615                  | 64.6%                            |\n| text-embedding-ada-002 | 12,500                 | 61.0%                            |\n\n\n### 무료 Open Source 기반 임베딩\n\n\n## 4단계: 벡터스토어 생성(Create Vectorstore)\n\n\n## 5단계: Retriever 생성\n\n리트리버는 구조화되지 않은 쿼리가 주어지면 문서를 반환하는 인터페이스입니다.\n\n리트리버는 문서를 저장할 필요 없이 문서를 반환(또는 검색)하기만 합니다.\n\n- [공식 도큐먼트](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n\n생성된 VectorStore 에 `as_retriver()` 로 가져와서 **Retriever** 를 생성합니다.\n\n\n### 유사도 기반 검색\n\n- 기본값은 코사인 유사도인 `similarity` 가 적용되어 있습니다.\n\n\n`similarity_score_threshold` 는 유사도 기반 검색에서 `score_threshold` 이상인 결과만 반환합니다.\n\n\n`maximum marginal search result` 를 사용하여 검색합니다.\n\n\n### 다양한 쿼리 생성\n\n\n### Ensemble Retriever\n\n\n## 6단계: 프롬프트 생성(Create Prompt)\n\n\n프롬프트 엔지니어링은 주어진 데이터(`context`)를 토대로 우리가 원하는 결과를 도출할 때 중요한 역할을 합니다.\n\n[TIP1]\n\n1. 만약, `retriever` 에서 도출한 결과에서 중요한 정보가 누락된다면 `retriever` 의 로직을 수정해야 합니다.\n2. 만약, `retriever` 에서 도출한 결과가 많은 정보를 포함하고 있지만, `llm` 이 그 중에서 중요한 정보를 찾지 못한거나 원하는 형태로 출력하지 않는다면 프롬프트를 수정해야 합니다.\n\n[TIP2]\n\n1. LangSmith 의 **hub** 에는 검증된 프롬프트가 많이 업로드 되어 있습니다.\n2. 검증된 프롬프트를 활용하거나 약간 수정한다면 비용과 시간을 절약할 수 있습니다.\n\n- https://smith.langchain.com/hub/search?q=rag\n\n\n## 7단계: 언어모델 생성(Create LLM)\n\n\nOpenAI 모델 중 하나를 선택합니다.\n\n- `gpt-3.5-turbo` : OpenAI의 GPT-3.5-turbo 모델\n- `gpt-4-turbo-preview` : OpenAI의 GPT-4-turbo-preview 모델\n\n자세한 비용 체계는 [OpenAI API 모델 리스트 / 요금표](https://teddylee777.github.io/openai/openai-models/)에서 확인할 수 있습니다.\n\n\n다음의 방식으로 토큰 사용량을 확인할 수 있습니다.\n\n\nHuggingFace 에 업로드 되어 있는 오픈소스 모델을 손쉽게 다운로드 받아 사용할 수 있습니다.\n\n아래의 리더보드에서 날마다 성능을 개선하는 오픈소스 리더보드를 확인할 수 있습니다.\n\n- [HuggingFace LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n\n\n## RAG 템플릿 실험\n\n\n> 문서: data/SPRI_AI_Brief_2023년12월호\\_F.pdf (페이지 10)\n\n- LangSmith: https://smith.langchain.com/public/4449e744-f0a0-42d2-a3df-855bd7f41652/r\n\n\n> 문서: data/SPRI_AI_Brief_2023년12월호\\_F.pdf (페이지 12)\n\n- LangSmith: https://smith.langchain.com/public/2b2913c9-6b9c-4a19-bb16-dc2256e2fdbf/r\n\n\n> 문서: data/SPRI_AI_Brief_2023년12월호\\_F.pdf (페이지 14)\n\n- LangSmith: https://smith.langchain.com/public/17ef6df2-b012-4f8e-b0a8-62894d82c097/r\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"02-RAG-Advanced.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"LangChain의 RAG 파헤치기"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}