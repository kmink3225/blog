{"title":"ChatOllama","markdown":{"yaml":{"title":"ChatOllama"},"headingText":"설치","containsRefs":false,"markdown":"\n\n\n\n\nOllama를 사용하면 Llama 2와 같은 오픈 소스 대규모 언어 모델을 로컬에서 실행할 수 있습니다. Ollama는 모델 가중치, 구성 및 데이터를 Modelfile로 정의된 단일 패키지로 번들링합니다. GPU 사용을 포함하여 설정 및 구성 세부 정보를 최적화합니다. 지원되는 모델 및 모델 변형의 전체 목록은 [Ollama model library](https://ollama.com/library)를 참조하세요.\n\n\n### 프로그램 설치\nOllama를 지원되는 플랫폼(Mac / Linux / Windows)에 다운로드하고 설치하세요.\n\n- 설치주소: [https://ollama.com/](https://ollama.com/)\n\n### 모델 다운로드\n\n#### 허깅페이스\n\n허깅페이스(HuggingFace) 에서 오픈모델을 다운로드 받습니다 (.gguf 확장자)\n\n- GGUF: https://huggingface.co/teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf\n\n#### Ollama 제공하는 모델\n\n`ollama pull <name-of-model>` 명령을 사용하여 사용 가능한 LLM 모델을 가져오세요.\n- 예: `ollama pull gemma:7b`\n\n아래의 경로에 모델의 기본 태그 버전이 다운로드됩니다.\n\n- Mac: `~/.ollama/models`\n- Linux/WSL: `/usr/share/ollama/.ollama/models`\n\n`ollama list`로 가져온 모든 모델을 확인하세요.\n\n`ollama run <name-of-model>`로 명령줄에서 모델과 직접 채팅하세요.\n\n### Modelfile 로부터 커스텀 모델 생성하기\n\n모델을 임포트하기 위해 ModelFile을 먼저 생성해야 합니다. 자세한 정보는 [ModelFile 관련 공식 문서](https://github.com/ollama/ollama/blob/69f392c9b7ea7c5cc3d46c29774e37fdef51abd8/docs/modelfile.md)에서 확인할 수 있습니다.\n\n> 샘플 모델파일 예시\n\n```\nFROM ggml-model-Q5_K_M.gguf\n\nTEMPLATE \"\"\"{{- if .System }}\n<s>{{ .System }}</s>\n{{- end }}\n<s>Human:\n{{ .Prompt }}</s>\n<s>Assistant:\n\"\"\"\n\nSYSTEM \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\"\"\n\nPARAMETER stop <s>\nPARAMETER stop </s>\n```\n\n### Chat 모델\n\nLlama `chat` 모델(예: `ollama pull llama2:7b-chat`)을 사용하는 경우 `ChatOllama` 인터페이스를 사용할 수 있습니다. 여기에는 시스템 메시지 및 사용자 입력을 위한 special tokens이 포함됩니다.\n\n### Ollama 모델 활용\n\n- 모든 로컬 모델은 `localhost:11434`에서 제공됩니다.\n- Command 창에서 직접 상호 작용하려면 `ollama run <name-of-model>`을 실행하세요.\n\n\n\n비동기 스트리밍(`astream()`) 지원을 위한 예시입니다. 위에서 생성한 단일 chain을 통해 모든 것이 가능합니다.\n\n\n## 출력형식: JSON\n\nOllama의 최신 버전을 사용하고 [`format`](https://github.com/jmorganca/ollama/blob/main/docs/api.md#json-mode) 플래그를 제공하세요.\n\n`format` 플래그는 모델이 JSON 형식으로 응답을 생성하도록 강제합니다.\n\nJSON 형식의 답변을 받기 위해서는 `\"resonse in JSON format.\"` 이 프롬프트에 포함되어야 합니다.\n\n## 멀티모달(Multimodal) 지원\n\nOllama는 [bakllava](https://ollama.ai/library/bakllava)와 [llava](https://ollama.ai/library/llava)와 같은 멀티모달 LLM을 지원합니다.\n\n`tags`를 사용하여 [Llava](https://ollama.ai/library/llava/tags)와 같은 모델의 전체 버전 세트를 탐색할 수 있습니다.\n\n`ollama pull llava:7b` 혹은 `ollama pull bakllava` 명령어를 통해 멀티모달 LLM을 다운로드하세요.\n\n**참고**\n- 멀티모달을 지원하는 최신 버전을 사용하려면 Ollama를 업데이트해야 합니다.\n\nPIL 이미지를 Base64 인코딩된 문자열로 변환하고 이를 HTML에 포함하여 이미지를 표시하는 함수를 제공합니다.\n\n- `convert_to_base64` 함수:\n\n  - PIL 이미지를 입력으로 받습니다.\n  - 이미지를 JPEG 형식으로 BytesIO 버퍼에 저장합니다.\n  - 버퍼의 값을 Base64로 인코딩하고 문자열로 반환합니다.\n\n- `plt_img_base64` 함수:\n\n  - Base64 인코딩된 문자열을 입력으로 받습니다.\n  - Base64 문자열을 소스로 사용하는 HTML `<img>` 태그를 생성합니다.\n  - HTML을 렌더링하여 이미지를 표시합니다.\n\n- 사용 예시:\n  - 지정된 파일 경로에서 PIL 이미지를 열어 `pil_image`에 저장합니다.\n  - `convert_to_base64` 함수를 사용하여 `pil_image`를 Base64 인코딩된 문자열로 변환합니다.\n  - `plt_img_base64` 함수를 사용하여 Base64 인코딩된 문자열을 이미지로 표시합니다.\n\n\n- `ChatOllama` 언어 모델을 사용하여 이미지와 텍스트 기반 질의에 대한 답변을 생성하는 체인을 구현합니다.\n- `prompt_func` 함수는 이미지와 텍스트 데이터를 입력으로 받아 `HumanMessage` 형식으로 변환합니다.\n  - 이미지 데이터는 Base64 인코딩된 JPEG 형식으로 전달됩니다.\n  - 텍스트 데이터는 일반 텍스트로 전달됩니다.\n- `StrOutputParser`를 사용하여 언어 모델의 출력을 문자열로 파싱합니다.\n- `prompt_func`, `llm`, `StrOutputParser`를 파이프라인으로 연결하여 `chain`을 생성합니다.\n- `chain.invoke` 메서드를 호출하여 이미지와 텍스트 질의를 전달하고 답변을 생성합니다.\n- 생성된 답변을 출력합니다.\n\n","srcMarkdownNoYaml":"\n\n\n\n\nOllama를 사용하면 Llama 2와 같은 오픈 소스 대규모 언어 모델을 로컬에서 실행할 수 있습니다. Ollama는 모델 가중치, 구성 및 데이터를 Modelfile로 정의된 단일 패키지로 번들링합니다. GPU 사용을 포함하여 설정 및 구성 세부 정보를 최적화합니다. 지원되는 모델 및 모델 변형의 전체 목록은 [Ollama model library](https://ollama.com/library)를 참조하세요.\n\n## 설치\n\n### 프로그램 설치\nOllama를 지원되는 플랫폼(Mac / Linux / Windows)에 다운로드하고 설치하세요.\n\n- 설치주소: [https://ollama.com/](https://ollama.com/)\n\n### 모델 다운로드\n\n#### 허깅페이스\n\n허깅페이스(HuggingFace) 에서 오픈모델을 다운로드 받습니다 (.gguf 확장자)\n\n- GGUF: https://huggingface.co/teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf\n\n#### Ollama 제공하는 모델\n\n`ollama pull <name-of-model>` 명령을 사용하여 사용 가능한 LLM 모델을 가져오세요.\n- 예: `ollama pull gemma:7b`\n\n아래의 경로에 모델의 기본 태그 버전이 다운로드됩니다.\n\n- Mac: `~/.ollama/models`\n- Linux/WSL: `/usr/share/ollama/.ollama/models`\n\n`ollama list`로 가져온 모든 모델을 확인하세요.\n\n`ollama run <name-of-model>`로 명령줄에서 모델과 직접 채팅하세요.\n\n### Modelfile 로부터 커스텀 모델 생성하기\n\n모델을 임포트하기 위해 ModelFile을 먼저 생성해야 합니다. 자세한 정보는 [ModelFile 관련 공식 문서](https://github.com/ollama/ollama/blob/69f392c9b7ea7c5cc3d46c29774e37fdef51abd8/docs/modelfile.md)에서 확인할 수 있습니다.\n\n> 샘플 모델파일 예시\n\n```\nFROM ggml-model-Q5_K_M.gguf\n\nTEMPLATE \"\"\"{{- if .System }}\n<s>{{ .System }}</s>\n{{- end }}\n<s>Human:\n{{ .Prompt }}</s>\n<s>Assistant:\n\"\"\"\n\nSYSTEM \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\"\"\n\nPARAMETER stop <s>\nPARAMETER stop </s>\n```\n\n### Chat 모델\n\nLlama `chat` 모델(예: `ollama pull llama2:7b-chat`)을 사용하는 경우 `ChatOllama` 인터페이스를 사용할 수 있습니다. 여기에는 시스템 메시지 및 사용자 입력을 위한 special tokens이 포함됩니다.\n\n### Ollama 모델 활용\n\n- 모든 로컬 모델은 `localhost:11434`에서 제공됩니다.\n- Command 창에서 직접 상호 작용하려면 `ollama run <name-of-model>`을 실행하세요.\n\n\n\n비동기 스트리밍(`astream()`) 지원을 위한 예시입니다. 위에서 생성한 단일 chain을 통해 모든 것이 가능합니다.\n\n\n## 출력형식: JSON\n\nOllama의 최신 버전을 사용하고 [`format`](https://github.com/jmorganca/ollama/blob/main/docs/api.md#json-mode) 플래그를 제공하세요.\n\n`format` 플래그는 모델이 JSON 형식으로 응답을 생성하도록 강제합니다.\n\nJSON 형식의 답변을 받기 위해서는 `\"resonse in JSON format.\"` 이 프롬프트에 포함되어야 합니다.\n\n## 멀티모달(Multimodal) 지원\n\nOllama는 [bakllava](https://ollama.ai/library/bakllava)와 [llava](https://ollama.ai/library/llava)와 같은 멀티모달 LLM을 지원합니다.\n\n`tags`를 사용하여 [Llava](https://ollama.ai/library/llava/tags)와 같은 모델의 전체 버전 세트를 탐색할 수 있습니다.\n\n`ollama pull llava:7b` 혹은 `ollama pull bakllava` 명령어를 통해 멀티모달 LLM을 다운로드하세요.\n\n**참고**\n- 멀티모달을 지원하는 최신 버전을 사용하려면 Ollama를 업데이트해야 합니다.\n\nPIL 이미지를 Base64 인코딩된 문자열로 변환하고 이를 HTML에 포함하여 이미지를 표시하는 함수를 제공합니다.\n\n- `convert_to_base64` 함수:\n\n  - PIL 이미지를 입력으로 받습니다.\n  - 이미지를 JPEG 형식으로 BytesIO 버퍼에 저장합니다.\n  - 버퍼의 값을 Base64로 인코딩하고 문자열로 반환합니다.\n\n- `plt_img_base64` 함수:\n\n  - Base64 인코딩된 문자열을 입력으로 받습니다.\n  - Base64 문자열을 소스로 사용하는 HTML `<img>` 태그를 생성합니다.\n  - HTML을 렌더링하여 이미지를 표시합니다.\n\n- 사용 예시:\n  - 지정된 파일 경로에서 PIL 이미지를 열어 `pil_image`에 저장합니다.\n  - `convert_to_base64` 함수를 사용하여 `pil_image`를 Base64 인코딩된 문자열로 변환합니다.\n  - `plt_img_base64` 함수를 사용하여 Base64 인코딩된 문자열을 이미지로 표시합니다.\n\n\n- `ChatOllama` 언어 모델을 사용하여 이미지와 텍스트 기반 질의에 대한 답변을 생성하는 체인을 구현합니다.\n- `prompt_func` 함수는 이미지와 텍스트 데이터를 입력으로 받아 `HumanMessage` 형식으로 변환합니다.\n  - 이미지 데이터는 Base64 인코딩된 JPEG 형식으로 전달됩니다.\n  - 텍스트 데이터는 일반 텍스트로 전달됩니다.\n- `StrOutputParser`를 사용하여 언어 모델의 출력을 문자열로 파싱합니다.\n- `prompt_func`, `llm`, `StrOutputParser`를 파이프라인으로 연결하여 `chain`을 생성합니다.\n- `chain.invoke` 메서드를 호출하여 이미지와 텍스트 질의를 전달하고 답변을 생성합니다.\n- 생성된 답변을 출력합니다.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"09-Ollama.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"ChatOllama"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}