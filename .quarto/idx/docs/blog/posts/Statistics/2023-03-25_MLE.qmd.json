{"title":"Maximum Likelihood Estimation, Statistical Bias, and Point Estimation","markdown":{"yaml":{"title":"Maximum Likelihood Estimation, Statistical Bias, and Point Estimation","subtitle":"Overview","description":"template\n","categories":["Statistics"],"author":"Kwangmin Kim","date":"03/29/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"execute":{"warning":false},"draft":false},"headingText":"Definition","containsRefs":false,"markdown":"\n\n\nTo talk about MLE (Maximum Likelihood Estimation), we need to recap the concepts and definitions of probability and likelihood. They are related but distinct concepts.\n\n* probability is a measure of the chance that an event will occur, given some prior knowledge or assumptions.\n* likelihood is a measure of the plausibility or compatibility of a particular set of model parameters, given the observed data.\n\n::: {.callout-tip}\n#### Chance vs Plausibility (personal opinion)\n  * chance is a general term but closer term to statistics, which is used in the situation of the probability or likelihood of an event occurring based on randomness or uncertainty.  \n  * plausibility chance is a term more often used in everyday life, which refers to the degree to which something is believable based on the available evidence or information.\n:::\n\n:::{#def-likelihood}\n\n Let $X_1, X_2, ..., X_n$ be a set of iid random variables with pdf or pmf $f(x_i | \\theta)$, where $\\theta$ is a vector of unknown parameters. Then, \n **the likelihood function is defined as the joint pdf or pmf of the observed data, given the values of the parameters**:\n\n$$\nL(\\theta | x_1, x_2, ..., x_n) = L(\\theta | \\mathbf x) = \\prod_{i=1}^n f(x_i | \\theta)\n$$\n:::\n\nThe likelihood function is a function of the parameters $\\theta$. The function measures the probability of observing a set of data, given the values of the parameters of a statistical model in order to estimate the values of the parameters by finding the values that maximize the likelihood function. The likelihood function is often used in the maximum likelihood estimation (MLE) method, where the MLE estimator is the set of parameter values that maximize the likelihood function.\n\nThe likelihood function is related to the concept of conditional probability. Given a set of observed data $x_1, x_2, ..., x_n$, the likelihood function measures the probability of observing these data, assuming a particular set of parameter values. The likelihood function is not a probability distribution, but it can be used to derive a probability distribution for the parameters, known as the posterior distribution, using Bayes' theorem.\n\n::: {.callout-tip}\n\n#### Probability\n\nProbability is a measure of the likelihood or chance that an event will occur, which is used to quantify uncertainty and randomness.  \nprobability is a function that maps a real number mapped from a random variable into $[0,1]$. The probability function, denoted by $P$, satisfies the following axioms:\n\n* Non-negativity: For any event $A \\in \\Omega$, $P(A) \\geq 0$.\n* Normalization: The probability of the entire sample space is 1, i.e., $P(\\Omega) = 1$.\n* Additivity: For any two disjoint events $A, B \\in \\Omega$, or $A \\cap B = \\emptyset$, the probability of their union is equal to the sum of their individual probabilities, i.e., $P(A \\cup B) = P(A) + P(B)$.\n:::\n\n#### Probability vs Likelihood\n\nProbability and likelihood are related but distinct concepts in statistics.\n\n* Probability refers to the measure of the likelihood that a particular event will occur, scaled on $[0,1]$. It is calculated based on a known probability distribution (= some prior knowledge or assumptions) before the data is observed. \n* On the other hand, likelihood refers to the probability of observing a set of data given a particular set of parameter values in a statistical model. It is calculated based on the unknown parameters after the data is observed. \n\nThe likelihood function is used to estimate the values of the parameters by finding the parameter values that maximize the likelihood function.\n\nFor instance of a coin flip, the **probability** of getting heads on a coin flip is 0.5, regardless of whether the coin has been flipped or not (i.e., without data). In contrast, the **likelihood** of observing heads after a coin has been flipped depends on the parameter of interest. We need to find the parameter given data, the results of multiple coin flips. \n\nIf we want to estimate the probability of heads, we can use the maximum likelihood estimation (MLE) approach, which involves finding the value of the coin bias that maximizes the likelihood of observing the observed sequence of heads and tails. The likelihood of the data is calculated using the binomial distribution, which gives the probability of observing a certain number of heads, given the number of tosses and the coin bias. In this case, the likelihood function is a function of the coin bias, and the probability of heads is the value of the coin bias that maximizes the likelihood function.\n\n#### Relation between Likelihood and PMF or PDF\n\nThe likelihood function is closely related to pmf or pdf of the data, which is a function that describes the probability of observing a particular value or range of values for the data, given the model parameters. The pmf or pdf is a function of the data, not the parameters, and is often written as $f(x|\\theta)$. The likelihood function is proportional to the pdf because is a product of pdfs when $X_i$ is independent, but with the data fixed and the parameter values treated as variables.\n\n```{r}\nlibrary(tidyverse)\n\n# Probability of getting heads on a fair coin flip\nprob <- 0.5\n\n# likelihood\n## Simulate a coin flip with a biased coin\nset.seed(123) # Set random seed for reproducibility\nn <- 100 # flipping numbers\np <- 0.2 # Probability of getting heads\nx <- rbinom(n, size = 1, prob = p) # Simulate n coin flips\nx%>%head(10)\n\n## Calculate the likelihood of observing the data given the parameter value p\nlikelihood <- prod(dbinom(x, size = 1, prob = p))\nlikelihood%>%round()\n\n```\n\nIn this example, `prob` is the assumed probability of getting heads on a fair coin flip. The probability of getting heads on a fair coin flip is 0.5, which is a fixed value that does not depend on any specific data. On the other hand, `p` is the probability of getting heads for the biased coin that we are simulating. The likelihood of observing a set of coin flips depends on the parameter value, which is unknown (actually, we know that it was $p=0.2$), and the observed data. We simulate a set of 100 coin flips with a biased coin that has a probability of 0.2 of getting heads. We then calculate the likelihood of observing this data given the parameter value of 0.2, which is the product of the probability mass function for each flip. \n\nHowever, in a real-world scenario, we would not know the true value of `p` and we would need to estimate it based on the observed data. By finding the Maximum Likelihood Estimation (MLE) of p, we are estimating the value of p that is most likely to have generated the observed data. To find MLE of `p` that maximizes the likelihood function, we can use numerical optimization methods.\n\nAs n increases, the product term in the likelihood function $\\prod_{i=1}^{n} f(x_i; \\theta)$, where $f$ is the pdf or pmf of the distribution being used, can become very small (since it is a product of values less than 1) and may result in numerical underflow (i.e., the product becomes so small that it rounds down to 0 in computer calculations). In practice, we typically take the logarithm of the likelihood function, called the log-likelihood, to avoid this issue:\n\n$$\n\\log L(\\theta|x_1, x_2, ..., x_n) = \\sum_{i=1}^{n} \\log f(x_i; \\theta)\n$$\n\nUsing the logarithm allows us to convert the product of small probabilities into a sum of log-probabilities, which are typically easier to work with numerically and mathematically. In this case, as n increases, the sum term in the log-likelihood can decrease (since it is a sum of negative values), but the decrease may not be as severe as in the product term of the likelihood function because of the log scale converting very small or large values into larger or smaller values .\n\n:::{#def-MLE}\nThe MLE estimator $\\hat{\\theta}$ is the value of the parameter vector that maximizes the likelihood function:\n\n$$\n\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} L(\\theta | x_1, x_2, ..., x_n)\n$$\n\nor equivalently, maximizes the log-likelihood function:\n\n$$\n\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\log L(\\theta | x_1, x_2, ..., x_n)\n$$\n\n\n:::\n\n\nThe Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a statistical model by finding the values of the parameters that maximize the likelihood function. The likelihood function is the probability of observing the data, given the parameters of the model. The MLE estimator is the set of parameter values that maximize the likelihood function. In other words, the MLE is the set of parameter values that make the observed data most probable, given the assumed probability distribution. The likelihood function is typically the product or the sum (depending on whether the observations are assumed to be independent or not) of the probabilities or probability densities of the observations, evaluated at the values of the parameters.\n\nThe MLE estimator has desirable statistical properties, such as consistency, efficiency, and asymptotic normality, under certain regularity conditions on the likelihood function and the parameter space. However, it is important to note that the MLE is not always the best estimator for a given problem, and other estimation methods may be more appropriate depending on the specific characteristics of the data and the model.\n\nThe likelihood function is the joint probability density (or mass) function of the data, viewed as a function of the parameters, and we find the maximum of this function **by differentiating** it with respect to the parameters and setting the derivative to zero.\n\n\n\n### MLE of OLS{#sec-mle_ols}\n\nIn a linear regression, the maximum likelihood estimate of the ordinary least squares (OLS) coefficients is equivalent to the least squares estimate. To derive this, we assume that $\\epsilon_i \\sim N(0,\\sigma^2)$ and that the observations are independent. Then, the likelihood function for the data $Y = (Y_1, Y_2, \\dots, Y_n)$ is:\n\n$$\nL(Y|\\theta) =L(Y|\\beta,\\sigma^2) = (2\\pi\\sigma^2)^{-\\frac{n}{2}} e^{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\beta)^2}\n$$\n\nwhere $X_i$ is the $i$ th row of the design matrix $X$ and $\\beta$ is the vector of regression coefficients.\n\nTo find the maximum likelihood estimates of $\\beta$ and $\\sigma^2$, we maximize the likelihood function with respect to these parameters. Taking the log of the likelihood function and simplifying, we obtain:\n\n$$\n\\log L(Y|\\theta) = \\log L(Y|\\beta,\\sigma^2) = -\\frac{n}{2} \\log (2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\beta)^2\n$$\n\nTo maximize this function with respect to $\\beta$, we differentiate with respect to $\\beta$ and set the derivative to zero, $\\frac{\\partial}{\\partial \\theta} \\log L(Y|\\theta) = 0$:\n\nSolving for $\\beta$, we obtain:\n\n$$\n \\frac{\\partial}{\\partial \\theta} \\log L(Y|\\beta,\\sigma^2) = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n 2X_i(Y_i - X_i\\beta) = 0 \n$$\n\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n$$\n, which is the OLS estimate of $\\beta$.\n\nSolving for $\\sigma^2$, we differentiate with respect to $sigma^2$ and set the derivative to zero:\n\n$$\n\\frac{\\partial}{\\partial \\sigma^2} log L(Y|\\beta,\\sigma^2) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^n (Y_i - X_i\\beta)^2 = 0\n$$\n$$\n\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (Y_i - X_i\\beta)^2}{n}\n$$\n\n\nwhich is the OLS estimate of $\\sigma^2$.\n\nTherefore, we see that the maximum likelihood estimates of $\\beta$ and $\\sigma^2$ in linear regression with normally distributed errors are equivalent to the OLS estimates of these parameters.\n\n### Statistical Bias\n\nStatistical bias refers to a systematic error or deviation in the results of a statistical analysis that is caused by factors other than chance. A biased estimator is one that consistently produces estimates that are systematically different from the true value of the parameter being estimated.\n\n::: {.callout-tip}\n\n#### There are 5 Types of bias \n\nThe above article discusses five types of statistical bias that analysts, data scientists, and other business professionals should be aware of to minimize their effects on the final results. \n\n1. selection bias: data selection methods are not truly random, leading to unequal representation of the population.\n2. bias in assignment: pre-existing differences between groups in an experiment can affect the outcome, a.k.a  allocation bias, treatment assignment bias, or exposure assignment bias. \n3. confounders: additional variables not accounted for in the experimental design can impact the results. \n4. self-serving bias: individuals tend to downplay undesirable qualities and overemphasize desirable ones a.k.a cognitive bias. In other words, people tend to take credit for their successes and blame outside factors for their failures.\n5. experimenter expectations: researchers can unconsciously influence the data through verbal or non-verbal cues. \n\nBeing aware of these biases can lead to better models and more reliable insights for data-backed business decisions.\n\n[Source: Article Written by Jenny Gutbezahl](https://online.hbs.edu/blog/post/types-of-statistical-bias)\n:::\n\nIt is important to detect and correct for bias in statistical analyses, as biased estimates can lead to incorrect conclusions and decisions. One way to correct for bias is to use an unbiased estimator, which is one that has a zero bias, i.e., its expected value is equal to the true value of the parameter being estimated.\n\n\n:::{#def-bias}\nAn estimator $\\hat{\\theta}$ is said to be biased if\n\n$$\n\\operatorname{E}(\\hat{\\theta})\\ne \\theta\n$$\n\nwhere $\\operatorname{E}(\\hat{\\theta})$ is the expected value of the estimator $\\hat{\\theta}$, and $\\theta$ is the true value of the parameter being estimated.\n:::\n\nAn estimator is said to be unbiased if **its expected value is equal to the true value of the parameter being estimated**. In other words, an estimator is unbiased if, on average, it gives an estimate that is equal to the true value of the parameter.\n\n\n\n\n```{markdown}\n#| echo: false\n#| evale: false\n\nFor example, suppose we want to estimate the mean $\\mu$ of a normal distribution with unknown variance $\\sigma^2$.\nWe can use the sample mean $\\bar{X}$ as an estimator of $\\mu$. It can be shown that the expected value of the sample mean is equal to the true value of the population mean,\n\n\nTo show that the sample mean $\\bar{X}$ is an unbiased estimator of the population mean $\\mu$, we need to show that the expected value of $\\bar{X}$ is equal to $\\mu$.\n\nRecall that the sample mean is defined as:\n\n$$\n\\begin{align*}\n \\bar{X}&=\\frac{1}{n} \\sum_{i=1}^{n} X_i \\\\\n\\operatorname{E}(\\bar{X}) \n&= \\operatorname{E} \\left( \\frac{1}{n} \\sum_{i=1}^{n} X_i \\right) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{E}(X_i) & \\text{(linearity of expectation)} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} \\mu & \\text{(since } X_i \\text{ has mean } \\mu) \\\\\n&= \\frac{n\\mu}{n} \\\\\n&= \\mu\n\\end{align*}\n$$\n\nwhere $X_1, X_2, \\dots, X_n$ are independent and identically distributed (i.i.d.) random variables from a normal distribution with mean $\\mu$ and variance $\\sigma^2$.\n\nTherefore, the sample mean is an unbiased estimator of the population mean.\n\n$$\n\\begin{align*}\n\\operatorname{E}[\\hat{\\sigma}^2] \n&= \\operatorname{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right] \\\\\n&= \\frac{1}{n}\\sum_{i=1}^{n}\\operatorname{E}[(X_i - \\bar{X})^2] \\\\\n&= \\frac{1}{n}\\sum_{i=1}^{n}\\operatorname{E}[X_i^2 - 2X_i\\bar{X} + \\bar{X}^2]\\\\\n&=\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname{E}[X_i^2 - 2X_i\\frac{1}{n}\\sum_{j=1}^{n}X_j + \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1}^{n}X_jX_k]\\\\\n&=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\operatorname{E}[X_i^2] - 2\\operatorname{E}[X_i]\\frac{1}{n}\\sum_{j=1}^{n}\\operatorname{E}[X_j] + \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\operatorname{E}[X_jX_k]\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\mu^2 + \\sigma^2 - 2\\mu\\frac{1}{n}\\sum_{j=1}^{n}\\mu + \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\mu^2\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\sigma^2 - \\frac{\\sigma^2}{n}\\right)\\\\\n&= \\frac{n-1}{n}\\sigma^2\n\\end{align*}\n$$\n\nwhere   \n$\\operatorname{E}[\\bar{X}] = \\mu$,  \n$\\operatorname{E}[X_i] = \\mu$ for all $i$,   \n$\\operatorname{Var}(\\bar{X}) = \\frac{\\sigma^2}{n}$,  \n$\\operatorname{E}[X_iX_j] = \\mu^2 + \\sigma^2$ for $i=j$, and  \n$\\operatorname{E}[X_iX_j] = \\mu^2$ for $i \\neq j$\n\n\n$\\operatorname{E}(\\hat{\\sigma}^2) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\operatorname{E}[X_i^2] - 2\\operatorname{E}[X_i]\\frac{1}{n}\\sum_{j=1}^{n}\\operatorname{E}[X_j] + \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\operatorname{E}[X_jX_k]\\right)$\n\nSince $X_i$ are independent and identically distributed, we have:\n\n$\\operatorname{E}[X_i] = \\mu$ for all $i$\n\n$\\operatorname{E}[X_iX_j] = \\mu^2 + \\sigma^2$ for $i=j$\n\n$\\operatorname{E}[X_iX_j] = \\mu^2$ for $i \\neq j$\n\nSubstituting these in the equation above:\n\n$\\operatorname{E}(\\hat{\\sigma}^2) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\mu^2 + \\sigma^2 - 2\\mu\\frac{1}{n}\\sum_{j=1}^{n}\\mu + \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\mu^2\\right)$\n\n\n\nTherefore, we can see that the expected value of the MLE for $\\sigma^2$ is $\\frac{n-1}{n}\\sigma^2$, which is not equal to the true value $\\sigma^2$ unless $n=1$. This shows that the MLE for $\\sigma^2$ is a biased estimator.\n\n### $\\sigma^2$\n\nThe sample variance $S^2$ can be used as an estimator of the population variance $\\sigma^2$. It can be shown that the expected value of the sample variance is equal to the true value of the population variance $\\mathbb{E}(S^2)\\sigma^2$\n\nTo show that $S^2$ is an unbiased estimator of $\\sigma^2$, we can use the definition of expected value:\n$$\n\\operatorname{E}(S^2)=\\int_{-\\infty}^{\\infty}s^2f_S(s)ds\n$$\nwhere $f_S(s)$ is the probability density function of the sample variance $S^2$. Since $S^2$ follows a chi-squared distribution with $n-1$ degrees of freedom, we have:\n\n$$\nf_S(s)=\\frac{1}{\\Gamma(\\frac{n-1}{2}) 2^{\\frac{n-1}{2}}},s^{\\frac{n-1}{2}-1} e^{-\\frac{s}{2}} \\text{ for } s\\ge 0\n$$\n\nwhere $\\Gamma$ is the gamma function. Substituting this into the integral, we get:\n\n$$\n\\begin{align*}\n\\operatorname{E}(S^2) \n&= \\int_{0}^{\\infty} s \\frac{1}{\\Gamma(\\frac{n-1}{2}) 2^{\\frac{n-1}{2}}},s^{\\frac{n-1}{2}-1} e^{-\\frac{s}{2}} ds \\\\\n&= \\frac{2}{n-1} \\int_{0}^{\\infty} \\frac{(u/2)^{\\frac{n}{2}-1}}{\\Gamma(\\frac{n}{2})} e^{-u/2} du \\quad \\text{(substituting } u = 2s\\text{)} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{\\Gamma(\\frac{n}{2})}{\\Gamma(\\frac{n}{2})} \\quad \\text{(using the definition of gamma function)} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{(n/2-1)!}{(n/2)!} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{(n/2-1)!}{(n/2-1)! \\cdot (n/2-1)} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{(n/2-1)!}{(n/2-2)! \\cdot (n/2-1) \\cdot (n/2-2)!} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{(n/2)!}{(n/2-2)! \\cdot (n/2)!} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{(n/2)(n/2-1)}{n(n/2-1)} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{n}{2(n/2-1)} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{n}{n-2} \\\\\n&= \\sigma^2\n\\end{align*}\n$$\n\nTherefore, $S^2$ is an unbiased estimator of $\\sigma^2$.\n\nThen, let's check whether the estimators from the example of @sec-mle_ols are biased for $\\hat{\\beta}$ and $\\hat{\\sigma^2}$ or not.\n\n\n\n### Point Estimator and MLE\n\nMaximum likelihood estimator (MLE) is a method used to estimate the parameters of a statistical model by finding the parameter values that maximize the likelihood function of the observed data. MLE provides a way to obtain point estimates of the parameters in a statistical model.\n\nA point estimator is a statistic used to estimate the value of an unknown parameter in a statistical model. For example, in linear regression, the slope parameter $\\beta_1$ can be estimated using the method of least squares, which provides a point estimate of $\\beta_1$ that minimizes the sum of squared residuals.\n\nThe MLE provides a way to obtain a point estimate of a parameter by finding the value that maximizes the likelihood function. The resulting estimate is often considered to be the \"most likely\" value of the parameter given the observed data.\n\nFor example, consider a simple coin-flipping experiment where we flip a coin 10 times and observe 7 heads and 3 tails. We assume that the coin is biased and we want to estimate the probability $p$ of getting heads. The likelihood function for this experiment is given by the binomial distribution, which can be written as:\n\n$L(p)=\\choose(10,7) p^7(1-p)^3$\n\nWe can use MLE to estimate the parameter $p$ by finding the value that maximizes the likelihood function. Taking the derivative of the likelihood function with respect to $p$ and setting it equal to zero, we obtain:\n\nd/dp L(p)=0⇒p= 7/10\n​\n \n\nThis value of $p$ maximizes the likelihood function and is therefore the MLE of the parameter $p$. It also provides a point estimate of $p$ that we can use to make inferences about the true value of $p$.\n\nIn summary, MLE is a method used to find the parameter values that maximize the likelihood function of the observed data, providing a point estimate of the parameters in a statistical model. The resulting estimate is often considered to be the \"most likely\" value of the parameter given the observed data.\n\n```\n","srcMarkdownNoYaml":"\n\n### Definition\n\nTo talk about MLE (Maximum Likelihood Estimation), we need to recap the concepts and definitions of probability and likelihood. They are related but distinct concepts.\n\n* probability is a measure of the chance that an event will occur, given some prior knowledge or assumptions.\n* likelihood is a measure of the plausibility or compatibility of a particular set of model parameters, given the observed data.\n\n::: {.callout-tip}\n#### Chance vs Plausibility (personal opinion)\n  * chance is a general term but closer term to statistics, which is used in the situation of the probability or likelihood of an event occurring based on randomness or uncertainty.  \n  * plausibility chance is a term more often used in everyday life, which refers to the degree to which something is believable based on the available evidence or information.\n:::\n\n:::{#def-likelihood}\n\n Let $X_1, X_2, ..., X_n$ be a set of iid random variables with pdf or pmf $f(x_i | \\theta)$, where $\\theta$ is a vector of unknown parameters. Then, \n **the likelihood function is defined as the joint pdf or pmf of the observed data, given the values of the parameters**:\n\n$$\nL(\\theta | x_1, x_2, ..., x_n) = L(\\theta | \\mathbf x) = \\prod_{i=1}^n f(x_i | \\theta)\n$$\n:::\n\nThe likelihood function is a function of the parameters $\\theta$. The function measures the probability of observing a set of data, given the values of the parameters of a statistical model in order to estimate the values of the parameters by finding the values that maximize the likelihood function. The likelihood function is often used in the maximum likelihood estimation (MLE) method, where the MLE estimator is the set of parameter values that maximize the likelihood function.\n\nThe likelihood function is related to the concept of conditional probability. Given a set of observed data $x_1, x_2, ..., x_n$, the likelihood function measures the probability of observing these data, assuming a particular set of parameter values. The likelihood function is not a probability distribution, but it can be used to derive a probability distribution for the parameters, known as the posterior distribution, using Bayes' theorem.\n\n::: {.callout-tip}\n\n#### Probability\n\nProbability is a measure of the likelihood or chance that an event will occur, which is used to quantify uncertainty and randomness.  \nprobability is a function that maps a real number mapped from a random variable into $[0,1]$. The probability function, denoted by $P$, satisfies the following axioms:\n\n* Non-negativity: For any event $A \\in \\Omega$, $P(A) \\geq 0$.\n* Normalization: The probability of the entire sample space is 1, i.e., $P(\\Omega) = 1$.\n* Additivity: For any two disjoint events $A, B \\in \\Omega$, or $A \\cap B = \\emptyset$, the probability of their union is equal to the sum of their individual probabilities, i.e., $P(A \\cup B) = P(A) + P(B)$.\n:::\n\n#### Probability vs Likelihood\n\nProbability and likelihood are related but distinct concepts in statistics.\n\n* Probability refers to the measure of the likelihood that a particular event will occur, scaled on $[0,1]$. It is calculated based on a known probability distribution (= some prior knowledge or assumptions) before the data is observed. \n* On the other hand, likelihood refers to the probability of observing a set of data given a particular set of parameter values in a statistical model. It is calculated based on the unknown parameters after the data is observed. \n\nThe likelihood function is used to estimate the values of the parameters by finding the parameter values that maximize the likelihood function.\n\nFor instance of a coin flip, the **probability** of getting heads on a coin flip is 0.5, regardless of whether the coin has been flipped or not (i.e., without data). In contrast, the **likelihood** of observing heads after a coin has been flipped depends on the parameter of interest. We need to find the parameter given data, the results of multiple coin flips. \n\nIf we want to estimate the probability of heads, we can use the maximum likelihood estimation (MLE) approach, which involves finding the value of the coin bias that maximizes the likelihood of observing the observed sequence of heads and tails. The likelihood of the data is calculated using the binomial distribution, which gives the probability of observing a certain number of heads, given the number of tosses and the coin bias. In this case, the likelihood function is a function of the coin bias, and the probability of heads is the value of the coin bias that maximizes the likelihood function.\n\n#### Relation between Likelihood and PMF or PDF\n\nThe likelihood function is closely related to pmf or pdf of the data, which is a function that describes the probability of observing a particular value or range of values for the data, given the model parameters. The pmf or pdf is a function of the data, not the parameters, and is often written as $f(x|\\theta)$. The likelihood function is proportional to the pdf because is a product of pdfs when $X_i$ is independent, but with the data fixed and the parameter values treated as variables.\n\n```{r}\nlibrary(tidyverse)\n\n# Probability of getting heads on a fair coin flip\nprob <- 0.5\n\n# likelihood\n## Simulate a coin flip with a biased coin\nset.seed(123) # Set random seed for reproducibility\nn <- 100 # flipping numbers\np <- 0.2 # Probability of getting heads\nx <- rbinom(n, size = 1, prob = p) # Simulate n coin flips\nx%>%head(10)\n\n## Calculate the likelihood of observing the data given the parameter value p\nlikelihood <- prod(dbinom(x, size = 1, prob = p))\nlikelihood%>%round()\n\n```\n\nIn this example, `prob` is the assumed probability of getting heads on a fair coin flip. The probability of getting heads on a fair coin flip is 0.5, which is a fixed value that does not depend on any specific data. On the other hand, `p` is the probability of getting heads for the biased coin that we are simulating. The likelihood of observing a set of coin flips depends on the parameter value, which is unknown (actually, we know that it was $p=0.2$), and the observed data. We simulate a set of 100 coin flips with a biased coin that has a probability of 0.2 of getting heads. We then calculate the likelihood of observing this data given the parameter value of 0.2, which is the product of the probability mass function for each flip. \n\nHowever, in a real-world scenario, we would not know the true value of `p` and we would need to estimate it based on the observed data. By finding the Maximum Likelihood Estimation (MLE) of p, we are estimating the value of p that is most likely to have generated the observed data. To find MLE of `p` that maximizes the likelihood function, we can use numerical optimization methods.\n\nAs n increases, the product term in the likelihood function $\\prod_{i=1}^{n} f(x_i; \\theta)$, where $f$ is the pdf or pmf of the distribution being used, can become very small (since it is a product of values less than 1) and may result in numerical underflow (i.e., the product becomes so small that it rounds down to 0 in computer calculations). In practice, we typically take the logarithm of the likelihood function, called the log-likelihood, to avoid this issue:\n\n$$\n\\log L(\\theta|x_1, x_2, ..., x_n) = \\sum_{i=1}^{n} \\log f(x_i; \\theta)\n$$\n\nUsing the logarithm allows us to convert the product of small probabilities into a sum of log-probabilities, which are typically easier to work with numerically and mathematically. In this case, as n increases, the sum term in the log-likelihood can decrease (since it is a sum of negative values), but the decrease may not be as severe as in the product term of the likelihood function because of the log scale converting very small or large values into larger or smaller values .\n\n:::{#def-MLE}\nThe MLE estimator $\\hat{\\theta}$ is the value of the parameter vector that maximizes the likelihood function:\n\n$$\n\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} L(\\theta | x_1, x_2, ..., x_n)\n$$\n\nor equivalently, maximizes the log-likelihood function:\n\n$$\n\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\log L(\\theta | x_1, x_2, ..., x_n)\n$$\n\n\n:::\n\n\nThe Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a statistical model by finding the values of the parameters that maximize the likelihood function. The likelihood function is the probability of observing the data, given the parameters of the model. The MLE estimator is the set of parameter values that maximize the likelihood function. In other words, the MLE is the set of parameter values that make the observed data most probable, given the assumed probability distribution. The likelihood function is typically the product or the sum (depending on whether the observations are assumed to be independent or not) of the probabilities or probability densities of the observations, evaluated at the values of the parameters.\n\nThe MLE estimator has desirable statistical properties, such as consistency, efficiency, and asymptotic normality, under certain regularity conditions on the likelihood function and the parameter space. However, it is important to note that the MLE is not always the best estimator for a given problem, and other estimation methods may be more appropriate depending on the specific characteristics of the data and the model.\n\nThe likelihood function is the joint probability density (or mass) function of the data, viewed as a function of the parameters, and we find the maximum of this function **by differentiating** it with respect to the parameters and setting the derivative to zero.\n\n\n\n### MLE of OLS{#sec-mle_ols}\n\nIn a linear regression, the maximum likelihood estimate of the ordinary least squares (OLS) coefficients is equivalent to the least squares estimate. To derive this, we assume that $\\epsilon_i \\sim N(0,\\sigma^2)$ and that the observations are independent. Then, the likelihood function for the data $Y = (Y_1, Y_2, \\dots, Y_n)$ is:\n\n$$\nL(Y|\\theta) =L(Y|\\beta,\\sigma^2) = (2\\pi\\sigma^2)^{-\\frac{n}{2}} e^{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\beta)^2}\n$$\n\nwhere $X_i$ is the $i$ th row of the design matrix $X$ and $\\beta$ is the vector of regression coefficients.\n\nTo find the maximum likelihood estimates of $\\beta$ and $\\sigma^2$, we maximize the likelihood function with respect to these parameters. Taking the log of the likelihood function and simplifying, we obtain:\n\n$$\n\\log L(Y|\\theta) = \\log L(Y|\\beta,\\sigma^2) = -\\frac{n}{2} \\log (2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\beta)^2\n$$\n\nTo maximize this function with respect to $\\beta$, we differentiate with respect to $\\beta$ and set the derivative to zero, $\\frac{\\partial}{\\partial \\theta} \\log L(Y|\\theta) = 0$:\n\nSolving for $\\beta$, we obtain:\n\n$$\n \\frac{\\partial}{\\partial \\theta} \\log L(Y|\\beta,\\sigma^2) = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n 2X_i(Y_i - X_i\\beta) = 0 \n$$\n\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n$$\n, which is the OLS estimate of $\\beta$.\n\nSolving for $\\sigma^2$, we differentiate with respect to $sigma^2$ and set the derivative to zero:\n\n$$\n\\frac{\\partial}{\\partial \\sigma^2} log L(Y|\\beta,\\sigma^2) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^n (Y_i - X_i\\beta)^2 = 0\n$$\n$$\n\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (Y_i - X_i\\beta)^2}{n}\n$$\n\n\nwhich is the OLS estimate of $\\sigma^2$.\n\nTherefore, we see that the maximum likelihood estimates of $\\beta$ and $\\sigma^2$ in linear regression with normally distributed errors are equivalent to the OLS estimates of these parameters.\n\n### Statistical Bias\n\nStatistical bias refers to a systematic error or deviation in the results of a statistical analysis that is caused by factors other than chance. A biased estimator is one that consistently produces estimates that are systematically different from the true value of the parameter being estimated.\n\n::: {.callout-tip}\n\n#### There are 5 Types of bias \n\nThe above article discusses five types of statistical bias that analysts, data scientists, and other business professionals should be aware of to minimize their effects on the final results. \n\n1. selection bias: data selection methods are not truly random, leading to unequal representation of the population.\n2. bias in assignment: pre-existing differences between groups in an experiment can affect the outcome, a.k.a  allocation bias, treatment assignment bias, or exposure assignment bias. \n3. confounders: additional variables not accounted for in the experimental design can impact the results. \n4. self-serving bias: individuals tend to downplay undesirable qualities and overemphasize desirable ones a.k.a cognitive bias. In other words, people tend to take credit for their successes and blame outside factors for their failures.\n5. experimenter expectations: researchers can unconsciously influence the data through verbal or non-verbal cues. \n\nBeing aware of these biases can lead to better models and more reliable insights for data-backed business decisions.\n\n[Source: Article Written by Jenny Gutbezahl](https://online.hbs.edu/blog/post/types-of-statistical-bias)\n:::\n\nIt is important to detect and correct for bias in statistical analyses, as biased estimates can lead to incorrect conclusions and decisions. One way to correct for bias is to use an unbiased estimator, which is one that has a zero bias, i.e., its expected value is equal to the true value of the parameter being estimated.\n\n\n:::{#def-bias}\nAn estimator $\\hat{\\theta}$ is said to be biased if\n\n$$\n\\operatorname{E}(\\hat{\\theta})\\ne \\theta\n$$\n\nwhere $\\operatorname{E}(\\hat{\\theta})$ is the expected value of the estimator $\\hat{\\theta}$, and $\\theta$ is the true value of the parameter being estimated.\n:::\n\nAn estimator is said to be unbiased if **its expected value is equal to the true value of the parameter being estimated**. In other words, an estimator is unbiased if, on average, it gives an estimate that is equal to the true value of the parameter.\n\n\n\n\n```{markdown}\n#| echo: false\n#| evale: false\n\nFor example, suppose we want to estimate the mean $\\mu$ of a normal distribution with unknown variance $\\sigma^2$.\nWe can use the sample mean $\\bar{X}$ as an estimator of $\\mu$. It can be shown that the expected value of the sample mean is equal to the true value of the population mean,\n\n\nTo show that the sample mean $\\bar{X}$ is an unbiased estimator of the population mean $\\mu$, we need to show that the expected value of $\\bar{X}$ is equal to $\\mu$.\n\nRecall that the sample mean is defined as:\n\n$$\n\\begin{align*}\n \\bar{X}&=\\frac{1}{n} \\sum_{i=1}^{n} X_i \\\\\n\\operatorname{E}(\\bar{X}) \n&= \\operatorname{E} \\left( \\frac{1}{n} \\sum_{i=1}^{n} X_i \\right) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{E}(X_i) & \\text{(linearity of expectation)} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} \\mu & \\text{(since } X_i \\text{ has mean } \\mu) \\\\\n&= \\frac{n\\mu}{n} \\\\\n&= \\mu\n\\end{align*}\n$$\n\nwhere $X_1, X_2, \\dots, X_n$ are independent and identically distributed (i.i.d.) random variables from a normal distribution with mean $\\mu$ and variance $\\sigma^2$.\n\nTherefore, the sample mean is an unbiased estimator of the population mean.\n\n$$\n\\begin{align*}\n\\operatorname{E}[\\hat{\\sigma}^2] \n&= \\operatorname{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right] \\\\\n&= \\frac{1}{n}\\sum_{i=1}^{n}\\operatorname{E}[(X_i - \\bar{X})^2] \\\\\n&= \\frac{1}{n}\\sum_{i=1}^{n}\\operatorname{E}[X_i^2 - 2X_i\\bar{X} + \\bar{X}^2]\\\\\n&=\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname{E}[X_i^2 - 2X_i\\frac{1}{n}\\sum_{j=1}^{n}X_j + \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1}^{n}X_jX_k]\\\\\n&=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\operatorname{E}[X_i^2] - 2\\operatorname{E}[X_i]\\frac{1}{n}\\sum_{j=1}^{n}\\operatorname{E}[X_j] + \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\operatorname{E}[X_jX_k]\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\mu^2 + \\sigma^2 - 2\\mu\\frac{1}{n}\\sum_{j=1}^{n}\\mu + \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\mu^2\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\sigma^2 - \\frac{\\sigma^2}{n}\\right)\\\\\n&= \\frac{n-1}{n}\\sigma^2\n\\end{align*}\n$$\n\nwhere   \n$\\operatorname{E}[\\bar{X}] = \\mu$,  \n$\\operatorname{E}[X_i] = \\mu$ for all $i$,   \n$\\operatorname{Var}(\\bar{X}) = \\frac{\\sigma^2}{n}$,  \n$\\operatorname{E}[X_iX_j] = \\mu^2 + \\sigma^2$ for $i=j$, and  \n$\\operatorname{E}[X_iX_j] = \\mu^2$ for $i \\neq j$\n\n\n$\\operatorname{E}(\\hat{\\sigma}^2) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\operatorname{E}[X_i^2] - 2\\operatorname{E}[X_i]\\frac{1}{n}\\sum_{j=1}^{n}\\operatorname{E}[X_j] + \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\operatorname{E}[X_jX_k]\\right)$\n\nSince $X_i$ are independent and identically distributed, we have:\n\n$\\operatorname{E}[X_i] = \\mu$ for all $i$\n\n$\\operatorname{E}[X_iX_j] = \\mu^2 + \\sigma^2$ for $i=j$\n\n$\\operatorname{E}[X_iX_j] = \\mu^2$ for $i \\neq j$\n\nSubstituting these in the equation above:\n\n$\\operatorname{E}(\\hat{\\sigma}^2) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\mu^2 + \\sigma^2 - 2\\mu\\frac{1}{n}\\sum_{j=1}^{n}\\mu + \\frac{1}{n^2}\\sum_{j=1}^{n}\\sum_{k=1}^{n}\\mu^2\\right)$\n\n\n\nTherefore, we can see that the expected value of the MLE for $\\sigma^2$ is $\\frac{n-1}{n}\\sigma^2$, which is not equal to the true value $\\sigma^2$ unless $n=1$. This shows that the MLE for $\\sigma^2$ is a biased estimator.\n\n### $\\sigma^2$\n\nThe sample variance $S^2$ can be used as an estimator of the population variance $\\sigma^2$. It can be shown that the expected value of the sample variance is equal to the true value of the population variance $\\mathbb{E}(S^2)\\sigma^2$\n\nTo show that $S^2$ is an unbiased estimator of $\\sigma^2$, we can use the definition of expected value:\n$$\n\\operatorname{E}(S^2)=\\int_{-\\infty}^{\\infty}s^2f_S(s)ds\n$$\nwhere $f_S(s)$ is the probability density function of the sample variance $S^2$. Since $S^2$ follows a chi-squared distribution with $n-1$ degrees of freedom, we have:\n\n$$\nf_S(s)=\\frac{1}{\\Gamma(\\frac{n-1}{2}) 2^{\\frac{n-1}{2}}},s^{\\frac{n-1}{2}-1} e^{-\\frac{s}{2}} \\text{ for } s\\ge 0\n$$\n\nwhere $\\Gamma$ is the gamma function. Substituting this into the integral, we get:\n\n$$\n\\begin{align*}\n\\operatorname{E}(S^2) \n&= \\int_{0}^{\\infty} s \\frac{1}{\\Gamma(\\frac{n-1}{2}) 2^{\\frac{n-1}{2}}},s^{\\frac{n-1}{2}-1} e^{-\\frac{s}{2}} ds \\\\\n&= \\frac{2}{n-1} \\int_{0}^{\\infty} \\frac{(u/2)^{\\frac{n}{2}-1}}{\\Gamma(\\frac{n}{2})} e^{-u/2} du \\quad \\text{(substituting } u = 2s\\text{)} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{\\Gamma(\\frac{n}{2})}{\\Gamma(\\frac{n}{2})} \\quad \\text{(using the definition of gamma function)} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{(n/2-1)!}{(n/2)!} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{(n/2-1)!}{(n/2-1)! \\cdot (n/2-1)} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{(n/2-1)!}{(n/2-2)! \\cdot (n/2-1) \\cdot (n/2-2)!} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{(n/2)!}{(n/2-2)! \\cdot (n/2)!} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{(n/2)(n/2-1)}{n(n/2-1)} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{n}{2(n/2-1)} \\\\\n&= \\frac{2}{n-1} \\cdot \\frac{n}{n-2} \\\\\n&= \\sigma^2\n\\end{align*}\n$$\n\nTherefore, $S^2$ is an unbiased estimator of $\\sigma^2$.\n\nThen, let's check whether the estimators from the example of @sec-mle_ols are biased for $\\hat{\\beta}$ and $\\hat{\\sigma^2}$ or not.\n\n\n\n### Point Estimator and MLE\n\nMaximum likelihood estimator (MLE) is a method used to estimate the parameters of a statistical model by finding the parameter values that maximize the likelihood function of the observed data. MLE provides a way to obtain point estimates of the parameters in a statistical model.\n\nA point estimator is a statistic used to estimate the value of an unknown parameter in a statistical model. For example, in linear regression, the slope parameter $\\beta_1$ can be estimated using the method of least squares, which provides a point estimate of $\\beta_1$ that minimizes the sum of squared residuals.\n\nThe MLE provides a way to obtain a point estimate of a parameter by finding the value that maximizes the likelihood function. The resulting estimate is often considered to be the \"most likely\" value of the parameter given the observed data.\n\nFor example, consider a simple coin-flipping experiment where we flip a coin 10 times and observe 7 heads and 3 tails. We assume that the coin is biased and we want to estimate the probability $p$ of getting heads. The likelihood function for this experiment is given by the binomial distribution, which can be written as:\n\n$L(p)=\\choose(10,7) p^7(1-p)^3$\n\nWe can use MLE to estimate the parameter $p$ by finding the value that maximizes the likelihood function. Taking the derivative of the likelihood function with respect to $p$ and setting it equal to zero, we obtain:\n\nd/dp L(p)=0⇒p= 7/10\n​\n \n\nThis value of $p$ maximizes the likelihood function and is therefore the MLE of the parameter $p$. It also provides a point estimate of $p$ that we can use to make inferences about the true value of $p$.\n\nIn summary, MLE is a method used to find the parameter values that maximize the likelihood function of the observed data, providing a point estimate of the parameters in a statistical model. The resulting estimate is often considered to be the \"most likely\" value of the parameter given the observed data.\n\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../js.html","../../signup.html"],"output-file":"2023-03-25_MLE.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../theme.scss"],"dark":["cosmo","../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Maximum Likelihood Estimation, Statistical Bias, and Point Estimation","subtitle":"Overview","description":"template\n","categories":["Statistics"],"author":"Kwangmin Kim","date":"03/29/2023","draft":false,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}