{"title":"Basic (1) - Stationay Checking","markdown":{"yaml":{"title":"Basic (1) - Stationay Checking","subtitle":"Overview","description":"template\n","categories":["Statistics"],"author":"Kwangmin Kim","date":"03/23/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":true,"execute":{"echo":false,"warning":false}},"headingText":"Load required libraries","containsRefs":false,"markdown":"\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n```{r}\nlibrary(tseries)\nlibrary(tidyverse)\nlibrary(timetk) #for lag_vec()\nlibrary(stats) #for acf()\nlibrary(forecast) # Ljung-box test\nrm(list=ls())\n```\n\n::: {#Korean .tab-pane .fade .show .active role=\"tabpanel\" aria-labelledby=\"Korean-tab\"}\n\n# Definition\n\n:::{#def-Stationary}\nA time series is said to be stationary if its statistical properties such as mean, variance, and autocorrelation remain constant over time. In other words, a stationary time series does not exhibit any trend, seasonality, or change in statistical properties over time.\n:::\n\ntime series modeling is the process of converting from non-stationary data into stationary data.\n\n## Stationary Data\n\n시계열적인 특성이 없는 데이터\n\n### Constant Mean\n\nA stationary time series has a mean that remains constant over time, which means that the average value of the time series does not change over time. A smoothing line of moving average with a certain window should show a constant trend.\n\n### Constant Variance\n\nA stationary time series has a variance that remains constant over time, which means that the variability or spread of the time series data around its mean does not change over time. A moving variance with a certain window should show a constant trend.\n\n### Constant Autocorrelation\n\nA stationary time series has autocorrelation that remains constant over time. Autocorrelation refers to the relationship between the values of a time series at different time lags. In a stationary time series, the strength and direction of autocorrelation do not change over time.\n\n### Absence of Trend \n\nA stationary time series does not exhibit any trend, which means that there is no systematic upward or downward movement in the mean of the time series over time.\n\n### Absence of Seasonality\n\nA stationary time series does not exhibit any seasonality, which means that there are no regular, repeating patterns or cycles in the data over time.\n\n### Statistical Properties are Time-Invariant\n\nThe statistical properties of a stationary time series, such as mean, variance, and autocorrelation, do not change with time. This property allows for the use of statistical techniques and models that assume constant statistical properties over time.\n\n### Example\n\n* white noise : no pattern about the time independent variable\n\n```{r}\n\n# Generate a random time series data\nset.seed(123)\nts_data <- rnorm(100)\nplot(ts_data)\n# Perform ADF test to check for stationarity\nadf_test <- adf.test(ts_data)\n\n# Print the results\ncat(\"ADF Test Results:\\n\")\ncat(\"Test statistic:\", adf_test$statistic, \"\\n\")\ncat(\"P-value:\", adf_test$p.value, \"\\n\")\n\n# Check if the time series is stationary\nif (adf_test$p.value <= 0.05) {\n  cat(\"Conclusion: The time series is stationary.\\n\")\n} else {\n  cat(\"Conclusion: The time series is not stationary.\\n\")\n}\n```\n\n:::{.callout-note}\nstationary data could have a trend and seasonality, but its period is not constant and easy to be predicted.\n\n```{r}\n# Set random seed for reproducibility\nset.seed(123)\n\n# Generate time series data with irregular trend and seasonality\nn <- 100\nt <- 1:n\ntrend <- 0.1 * t + 2 * sin(t * 0.05) * rnorm(n)\nseasonality <- 2 * sin(t * 0.2 + 1) * rnorm(n)\nirregular <- rnorm(n)\nts_data <- trend + seasonality + irregular\n\n# Plot the time series data\nggplot(data = data.frame(t = t, ts_data = ts_data), aes(x = t, y = ts_data)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Value\", title = \"Time Series Data with Irregular Trend and Seasonality\")\n```\n\nstationary data with an irregular trend and seasonality using a combination of a linear trend (small coefficient), sinusoidal pattern(for varying amplitude), and random noise (irregular trend and seasonality).\n:::\n\n## Non-stationary Data\n\n분석대상으로 시간 축에 대하여 분산(=정보)이 있음\n\n### Example\n\n* 심장 박동 수 : 일정한 주기를 반복해야 건강한 상태\n\n## Conversion Process from Non-tationary to Stationary\n\n### Lag\n\nA lag refers to the time interval between observations in a time series. It represents the number of time units (e.g., time periods, days, months) by which a variable is shifted or delayed in time.\n\n:::{#def-lag}\n\nIn time series analysis, a lag refers to the time interval between observations in a time series. Let $Y_t$ denote the value of a variable at time $t$, and $Y_{t-k}$ denote the value of the same variable at time $t$ lagged by $k$ time units. The lagged value $Y_{t-k}$ is defined as:\n\n$$\nY_{t-k} = Y_{t-k}\n$$\nwhere $k$ is the lag.\n\n:::\n\nThe lag can be positive, indicating a forward shift in time, or negative, indicating a backward shift in time.\n\nFor example, suppose we have a time series of daily temperature data, and we want to examine the relationship between the temperature at a given day and the temperature on the same day one week ago (i.e., $k = 7$). In this case, $Y_t$ represents the temperature at time $t$, and $Y_{t-7}$ represents the temperature on the same day one week ago. By examining the lagged relationship between $Y_t$ and $Y_{t-7}$, we can analyze any patterns or trends in the temperature data over a one-week period.\n\n\n#### Properties\n\n##### Time Shifting\n\nLags allow for time shifting of a time series variable, where the value of the variable at a given time step is compared to its value at a previous time step. This allows for analyzing the temporal relationship and dependencies between values of a time series over different time intervals.\n\n##### Autocorrelation\n\nLags are used to calculate autocorrelation, which is the correlation between a time series variable and its lagged values. Autocorrelation helps in understanding the persistence or pattern of the variable over time, and can be used to detect seasonality, trends, or other patterns in the data.\n\n##### Trend Analysis\n\nLags can be used to analyze trends in time series data. By comparing a time series variable with its lagged values, trends can be identified and analyzed to understand the direction and magnitude of changes in the variable over time.\n\n##### Seasonality Detection\n\nLags can be used to detect seasonality in time series data. By analyzing the relationship between a time series variable and its lagged values, patterns that repeat at regular intervals (e.g., daily, monthly, yearly) can be identified, indicating seasonality in the data.\n\n##### Forecasting\n\nLags are used in time series forecasting models to make predictions about future values of a time series variable. By using lagged values of the variable as predictors, forecasting models can capture the historical patterns and trends in the data to make future predictions.\n\n##### Data Transformation\n\nLags can be used to transform time series data into a different format, such as creating lagged variables or lagged differences, which can be used in various statistical techniques for analysis, modeling, and forecasting of time series data.\n\n```{r}\n# Create an example time series data\nset.seed(123)\n\nn <- 100 # Number of observations\nt <- 1:n\ntrend <- 0.5 * t # Linear trend component\nseasonality <- 10 * sin(2 * pi * t/12) # Seasonal component\nerror <- rnorm(n, mean = 0, sd = 5) # Error component\nts_data <- trend + seasonality + error # Combine components to create time series data\n\n# Create a data frame with lagged variables\nlagged <- data.frame(\n  Value = ts_data)%>%\n  mutate(\n  Lag1 = lag_vec(ts_data, lag = 1),\n  Lag2 = lag_vec(ts_data, lag = 6),\n  Lag3 = lag_vec(ts_data, lag = 12),\n  n=1:n()\n)\nlagged_data<-lagged%>%\ngather(key=lag,value=value,Value:Lag3)\n\nknitr::kable(lagged%>%head(20))\n\n# Plot the time series data and its lagged variables\nggplot(lagged_data, aes(x = n,y = value,color=lag)) +\n  geom_line( size = 1.5, linetype = \"solid\")+\n  labs(title = \"Time Series Data and Lagged Variables\",\n       x = \"Time Step\", y = \"Value\") +\n  scale_color_manual(values=c('darkred','darkgreen','darkblue','black'))+\n  theme(legend.position = \"right\")\n```\n\n### Difference\n\nDifferencing is a common technique used in time series analysis to transform a non-stationary time series into a stationary time series. It involves computing the difference between consecutive observations in the time series to remove trends or seasonality, and create a stationary time series that can be easier to analyze and model.\n\n:::{#def-difference}\nThe differenced time series $Y_t$ of an original time series $X_t$ of order $d$ can be defined as:\n\n$Y_t = X_t - X_{t-d}$\n\nwhere $X_t$ is the original time series value at time $t$, $ X_{t-d}$ is the original time series value at time $t-d$, and $d$ is the order of differencing.\n:::\n\nThe difference in data at a specific time interval could be used to represent time interval data such as year-on-year growth and month-on-month growth\n\n```{r}\n\n# Create a data frame with lagged variables\nlagged <- data.frame(\n  Value = ts_data)%>%\n  mutate(\n  Lag1 = lag_vec(ts_data, lag = 1),\n  Lag2 = lag_vec(ts_data, lag = 6),\n  Lag3 = lag_vec(ts_data, lag = 12),\n  n=1:n(),\n  diff1= c(diff_vec(Value,lag=1)),\n  diff2= c(diff_vec(Value,lag=6)),\n  diff3= c(diff_vec(Value,lag=12)))%>%\n  dplyr::select(n,everything())\n\nknitr::kable(lagged%>%round(3)%>%head(20))\n\nlagged_data<-lagged%>%\ngather(key=process,value=value,Value:diff3)\n\n# Plot the time series data and its lagged variables\n\nggplot(lagged_data%>%filter(!grepl('Lag',process)), aes(x = n,y = value,color=process)) +\n  geom_line(size =1, linetype = \"solid\")+\n  labs(title = \"Time Series Data and Lagged Variables\",\n       x = \"Time Step\", y = \"Value\") +\n  scale_color_manual(values=c('red','green','blue','black'))+\n  theme(legend.position = \"right\")\n\n```\n\n## ACF\n\nACF stands for autocorrelation function. ACF is a statistical tool used in time series analysis to measure the correlation between a time series and its lagged values. It helps to identify the presence of autocorrelation, which is the tendency of a time series to exhibit similar patterns or trends at different time points.\n\n:::{#def-acf}\nThe autocorrelation function $ \\rho(k) $ of a time series $ X_t $ at lag $ k $ can be defined as:\n\n$$ \n\\rho(k) = \\frac{\\text{Cov}(X_t, X_{t-k})}{\\sqrt{\\text{Var}(X_t) \\cdot \\text{Var}(X_{t-k})}}\n$$\n\nwhere $X_t$ is the value of the time series at time $t$, $X_{t-k}$ is the value of the time series at time $t-k$, and $\\text{Cov}(X_t, X_{t-k})$ and $\\text{Var}(X_t)$ are the covariance and variance of the time series, respectively.\n:::\n\nWe can calculate the ACF of this time series to check for autocorrelation using R. Here's an example code:\n\n```{r}\n# Calculate autocorrelation function\nacf_sales<-acf(ts_data)\nacf_sales\n```\n\n### ACF Interpretation\n\n* The lag on the x-axis represents the time lag between the current observation and the lagged observation for which the autocorrelation coefficient is calculated. Lags closer to 0 represent autocorrelation between neighboring observations, while larger lags represent autocorrelation between more distant observations.\n* The height of the autocorrelation coefficients on the y-axis indicates the strength of autocorrelation at different lags. Larger coefficients indicate stronger autocorrelation, while smaller coefficients indicate weaker autocorrelation.\n  * The ACF of the lag $1$ on the $x$ axis means the autocorrelation between the original data, Value and the Value lagged by $k = 1$, and the ACF of the lag $6$ on the $x$ axis means the autocorrelation between the original data, Value and the Value lagged by $k = 6$\n* The sign of the autocorrelation coefficient indicates the direction of autocorrelation. Positive coefficients indicate values tend to be similar at neighboring lags, while negative coefficients indicate values tend to be dissimilar at neighboring lags.\n* The horizontal dashed lines on the ACF plot represent the confidence intervals. Autocorrelation coefficients that fall outside these confidence intervals are considered statistically significant, indicating a high likelihood that the observed autocorrelation is not due to random chance.\n  * the line means the confidence interval = $\\left[ \\text{ACF}(k) \\pm \\frac{z_{\\alpha/2}}{\\sqrt{n}}\\right]$ where $\\operatorname{ACF}(k)$ is the autocorrelation coefficient at lag $k$, $z_{\\alpha/2}$ is the critical value from the standard normal distribution for the desired confidence level (e.g., 1.96 for a 95% confidence level),\n$n$ is the sample size.\n* The pattern of autocorrelation coefficients can provide insights into the presence of trend, seasonality, or other underlying patterns in the time series data. For instance, a repeating pattern of positive and negative autocorrelation coefficients may indicate the presence of seasonality, while a gradual decline in autocorrelation coefficients may indicate the presence of a trend. For our example, we can see the decline and increasing pattern both in ACF and the original plot.\n\n### ACF's Weakness\n\nACF has a weakness in that it can sometimes show spurious correlations due to the effect of earlier lags. \nThis is known as the **chain reaction** or **spillover** effect. For example, if a time series has a strong autocorrelation at lag 1, it can cause subsequent lags to also exhibit autocorrelation, even if there is no true underlying relationship. To address this issue, the Partial Autocorrelation Function (PACF) was developed. \n\n## PACF\n\nPartial Autocorrelation Function stands for PACF.\n\n:::{#def-pacf}\nThe PACF at lag $k$, denoted as $\\operatorname{PACF}(k)$, is defined as the autocorrelation between the original time series and its lagged values, with the effects of all shorter lags removed.\n\n$$ \n\\begin{align*}\n\\text{PACF}(k) &= \\phi_{kk} \\\\\n&= \\frac{\\text{cov}(Y_t, Y_{t-k} | Y_{t-1}, Y_{t-2}, \\ldots, Y_{1})}{\\sqrt{\\text{var}(Y_t | Y_{t-1}, Y_{t-2}, \\ldots, Y_{1}) \\cdot \\text{var}(Y_{t-k} | Y_{t-1}, Y_{t-2}, \\ldots, Y_{1})}}\n\\end{align*}\n$$\n\nwhere $\\phi_{kk}$ represents the partial autocorrelation coefficient at lag $k$.\n:::\n\nPACF measures the autocorrelation between the residuals of a time series after removing the effects of shorter lags. It provides a more direct measure of the linear relationship between the time series at a specific lag, while accounting for the effects of earlier lags. PACF helps to isolate the direct impact of a particular lag on the time series, without the spillover effect from earlier lags.\n\nPACF in time series analysis measures the correlation between a time series value at a specific lag (denoted as $k$) and its lagged value at a previous time step (denoted as $t−k$), after removing the linear dependence on the intermediate lags ($1,2,…,k−1$).\n\nPACF is useful in time series analysis for identifying the order of an autoregressive (AR) model, which is a common type of time series model. AR models use past values of the time series to predict future values. The PACF plot can help identify the significant lags that contribute to the prediction of the time series, and thus aid in model selection and forecasting accuracy.\n\n:::{.callout-note}\nACF(1) is equaivalent to PACF(1) because there is no in-between lags and chain reaction at lag $k = 1$\n:::\n\n```{r}\npacf_result <- stats::pacf(ts_data)\n```\n\n### PACF Interpretation\n\n* A significant positive value at lag k in the PACF plot indicates a strong positive linear relationship between the value at lag k and the current value of the time series. This suggests that the value at lag k is an important predictor for the current value.\n* A significant negative value at lag k in the PACF plot indicates a strong negative linear relationship between the value at lag k and the current value of the time series. This suggests that the value at lag k is an important predictor for the current value, but with an inverse relationship compared to positive values.\n* Non-significant values close to zero in the PACF plot indicate that there is little or no autocorrelation at those lags. This suggests that the value at those lags does not significantly impact the current value of the time series.\n\n## Time Series Model\n\nThe fitted values, denoted as $\\hat{Y}_t$, for a time series model are obtained by applying the estimated model parameters to the observed data points $Y_t$ up to time $t$, using the estimated model equations.\n\n$$\n\\hat{Y}_t = \\hat{\\alpha} + \\hat{\\beta}_1 X_{1,t} + \\hat{\\beta}_2 X_{2,t} + \\ldots + \\hat{\\beta}_p X_{p,t}\n$$\n\nwhere $\\hat{\\alpha}$ is the estimated intercept term, $\\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ are the estimated coefficients for the explanatory variables $X_1, X_2, \\ldots, X_p$ respectively, and $X_{1,t}, X_{2,t}, \\ldots, X_{p,t}$  are the observed values of the explanatory variables at time $t$.\n\n### Fitted Value\n\n```{r}\nmodel<-lm(Value~n,data=lagged)\nfitted_data<-lagged%>%\n  mutate(fit=fitted(model),\n  residual=residuals(model))%>%\n  gather(key=fit_output,value=output,c(Value,fit,residual))\n\nggplot(data=fitted_data,aes(x=n,y=output,color=fit_output))+\ngeom_line()+\ngeom_point()\n```\n\nAs you can see the residual pattern is the same as the original data, Value when you fit the data with a linear regression. If you use the time index as the explanatory variable in a linear regression to fit a time series model, and the residuals will capture the deviations of the observed values from this linear trend. As a result, the residuals will exhibit the same pattern, since they represent the discrepancies between the observed values and the linear trend estimated from the row numbers. It's important to note that using row numbers as the explanatory variable in a linear regression for time series analysis may not always be meaningful, as it does not take into account any underlying patterns, trends, or seasonality present in the data. It's generally recommended to use appropriate time-related variables or other relevant explanatory variables in time series modeling to capture the inherent dynamics of the data.\n\n## White Noise\n\n\n:::{#def-white_noise}\n\nWhite noise is a type of time series data that is characterized by random, uncorrelated, and identically distributed (i.i.d.) values.\n$$ \nY_t \\sim \\operatorname{WN}(0,\\sigma^2)\n$$\n\nwhere \n\n$$\n\\begin{align*}\nX_t & : \\text{The value of the white noise at time t} \\\\\n\\text{WN} & : \\text{Indicates that the data follows a white noise process} \\\\\n0 & : \\text{The mean of the white noise process, which is typically assumed to be 0} \\\\\n\\sigma^2 & : \\text{The variance of the white noise process, which determines the spread of the random values}\n\\end{align*}\n$$\n:::\n\nIt is often used as a reference or benchmark series to compare against other time series data for identifying patterns or structures.\n\n### Properties\n\n* Randomness: White noise is a series of random values that are not predictable or follow any pattern.\n* Independence: The values in a white noise series are uncorrelated, meaning that the value at any time point does not depend on the values at other time points.\n* Identically Distributed: The values in a white noise series are drawn from the same distribution, typically assumed to have a constant mean and variance.\n* Constant Mean: The mean of a white noise series is typically assumed to be constant and equal to zero, although this can be adjusted to a different value if necessary.\n* Constant Variance: The variance of a white noise series is typically assumed to be constant, meaning that the spread of the values remains the same over time.\n* No Autocorrelation: White noise series have no autocorrelation, meaning that the correlation between the values at different time points is close to zero.\n* No Trend: White noise series do not exhibit any trend or pattern over time, as the values are purely random and do not follow any systematic behavior.\n* Useful as a Benchmark: White noise series are often used as a benchmark or reference to compare against other time series data for identifying patterns or structures.\n\n### Ljung-box Test\n\nThe Ljung-Box test is a statistical test used to assess whether a time series data exhibits significant autocorrelation at different lags. The null hypothesis ($H_0$) of the Ljung-Box test is that there is no autocorrelation in the time series data up to a certain lag, while the alternative hypothesis ($H_a$) is that there is significant autocorrelation present. It is used to assess the goodness-of-fit of a model by testing whether the autocorrelation coefficients of the residuals (or errors) of the model are significantly different from zero.\n\n$$\nQ(m) = n(n+2) \\sum_{k=1}^{m} \\frac{\\hat{\\rho}_k^2}{n-k} \\sim \\chi^2_{1-\\alpha,h}\n$$\n\nwhere:\n- $Q(m)$ is the Ljung-Box test statistic for a given maximum lag $m$\n- $n$ is the sample size of the time series data\n- $\\hat{\\rho}_k$ is the sample autocorrelation at lag $k$\n- Under $H_0$, $Q(m)$ assymptotically follows a $\\chi^2_{1-\\alpha,h}$\n- $h$ is the number of lags being tested\n\nSuppose we have a time series data vector $x$ of length $n$, and we want to perform a Ljung-Box test up to a maximum lag of $m$.\n\nThe Q statistic follows a chi-squared distribution with degrees of freedom equal to the number of autocorrelation coefficients being tested. The p-value associated with the Q statistic can be compared to a chosen significance level (e.g., 0.05) to determine if the residuals exhibit significant autocorrelation. If the p-value is below the chosen significance level, it suggests that the model may have inadequate fit and that there may be remaining autocorrelation in the residuals.\n\n:::{.callout-tip}\nThe Box-Pierce test is a modified version of the Ljung-Box test,\n$$\nQ(m) = n \\sum_{k=1}^{m} \\hat{\\rho}_k^2\n$$\n\nLjung-Box test incorporates the sample size $n$ in the denominator. This makes the Ljung-Box test more appropriate for small sample sizes, while the Box-Pierce test is suitable for larger sample sizes.\n:::\n\n\n### Example\n\nA classic example of white noise is a series of random numbers generated from a standard normal distribution, where each value in the series is independent and identically distributed with mean 0 and variance 1.\n\n```{r}\n# Generate white noise series\nset.seed(123)\nn <- 100 # Number of observations\nwn <- rnorm(n, mean = 0, sd = 1) # Generate random values from standard normal distribution\n\n# Plot white noise series\nwn_data <- data.frame(Time = 1:n, Value = wn)\nggplot(wn_data, aes(x = Time, y = Value)) +\n  geom_line() +\n  labs(title = \"White Noise Series\", x = \"Time\", y = \"Value\")\n\nresult<-acf(wn)\nplot(result)\ncheckresiduals(wn)\n```\n\n## Time Series Decomposition\n\nThe characteristics of time series data has trend, seasonality, and autocorrelation. To check if autocorrelation exists in data, we used ACF and PACF. Then, how to check the trend and seasonality characteristics? Time series decomposition can be used to check them.\n\nThe observed time series $y_t$ can be decomposed into four components: the trend component $T_t$, the seasonal component $S_t$, the cyclical component $C_t$, and the remainder or error component $E_t$. This can be expressed as:\n\n$$\n\\begin{equation}\ny_t = T_t + S_t + C_t + E_t, \\quad \\text{where} \\quad E_t \\sim \\text{WN}(0,\\sigma^2),\n\\end{equation}\n$$\n\nwhere $T_t$ is the trend component, $S_t$ is the seasonal component, $C_t$ is the cyclical component, and $E_t$ is the error term, which is a random variable with a white noise distribution with mean 0 and variance $\\sigma^2$.\n\nThe trend component $T_t$ represents the long-term behavior of the time series, and can be either deterministic or stochastic. The seasonal component $S_t$ represents the regular and repeated variations that occur within a single year or other fixed time period. The cyclical component $C_t$ represents the irregular variations in the time series that do not follow a fixed pattern.\n\n:::{.callout-tip}\nDeterministic refers to a situation where the outcome is completely determined by the initial conditions, and there is no randomness involved. For example, if you drop a ball from a certain height, the time it takes to hit the ground can be determined exactly based on the initial height and the acceleration due to gravity. This is a deterministic process because there is no randomness involved.\n\nStochastic, on the other hand, refers to a situation where the outcome is uncertain and subject to randomness. For example, if you roll a fair six-sided die, you cannot predict with certainty what number will come up. The outcome is determined by chance, and is therefore stochastic.\n:::\n\nThere are two ways of decomposing time series data: **additive decomposition** and **multiplicative decomposition**\nThis additive deomoposition method is suitable for the time series data with a constant variation of seasonality according the trend, while multiplicative decomposition is suitable for the time series data with a inconsistent variation of seasonality according the trend. The variation magnitude could increase or decrease.\n\n### Additive Decomposition\n\nA common method for decomposing time series is the additive decomposition, which can be expressed as:\n$$\n\\begin{equation}\ny_t = T_t + S_t + E_t.\n\\end{equation}\n$$\n\n\nThe trend component $T_t$ is estimated by smoothing the data using techniques such as moving averages or exponential smoothing. The seasonal component $S_t$ is estimated by computing the seasonal indices, which are ratios of the observed values to the estimated trend component. The cyclical component $C_t$ is usually not explicitly estimated, but can be identified as the fluctuations that remain after removing the trend and seasonal components.\n\nFor example, let $X_t$ be a quarterly time series of sales data for a company, with $t$ ranging from 1 to 20. We can decompose the time series using an additive model:\n\n$$\n\\begin{equation}\nX_t = T_t + S_t + E_t,\n\\end{equation}\n$$\n\nwhere $T_t$ is the trend component, $S_t$ is the seasonal component, and $E_t$ is the error term. The trend component can be estimated using a 3-period moving average:\n$$\n\\begin{equation}\nT_t = \\frac{1}{3} (X_{t-1} + X_t + X_{t+1}),\n\\end{equation}\n$$\n\nwhere $t=2,3,\\ldots,19$. The seasonal component can be estimated by computing the average value of $X_t$ for each quarter:\n\n$$\n\\begin{equation}\nS_t = \\frac{1}{5} \\sum_{i=0}^3 X_{t+4i}.\n\\end{equation}\n$$\n\nFinally, the error term $E_t$ can be computed as:\n\n$$\n\\begin{equation}\nE_t = X_t - T_t - S_t.\n\\end{equation}\n$$\n\nThis decomposition allows us to separate the long-term trend, seasonal patterns, and irregular fluctuations in the sales data, and can help us identify any underlying patterns or trends that may be present in the data.\n\n:::\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n::: {#English .tab-pane .fade role=\"tabpanel\" aria-labelledby=\"English-tab\"}\n\n:::\n\n</div>\n\n## Go to Blog Content List\n\n[Blog Content List](../../content_list.qmd)","srcMarkdownNoYaml":"\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n```{r}\n# Load required libraries\nlibrary(tseries)\nlibrary(tidyverse)\nlibrary(timetk) #for lag_vec()\nlibrary(stats) #for acf()\nlibrary(forecast) # Ljung-box test\nrm(list=ls())\n```\n\n::: {#Korean .tab-pane .fade .show .active role=\"tabpanel\" aria-labelledby=\"Korean-tab\"}\n\n# Definition\n\n:::{#def-Stationary}\nA time series is said to be stationary if its statistical properties such as mean, variance, and autocorrelation remain constant over time. In other words, a stationary time series does not exhibit any trend, seasonality, or change in statistical properties over time.\n:::\n\ntime series modeling is the process of converting from non-stationary data into stationary data.\n\n## Stationary Data\n\n시계열적인 특성이 없는 데이터\n\n### Constant Mean\n\nA stationary time series has a mean that remains constant over time, which means that the average value of the time series does not change over time. A smoothing line of moving average with a certain window should show a constant trend.\n\n### Constant Variance\n\nA stationary time series has a variance that remains constant over time, which means that the variability or spread of the time series data around its mean does not change over time. A moving variance with a certain window should show a constant trend.\n\n### Constant Autocorrelation\n\nA stationary time series has autocorrelation that remains constant over time. Autocorrelation refers to the relationship between the values of a time series at different time lags. In a stationary time series, the strength and direction of autocorrelation do not change over time.\n\n### Absence of Trend \n\nA stationary time series does not exhibit any trend, which means that there is no systematic upward or downward movement in the mean of the time series over time.\n\n### Absence of Seasonality\n\nA stationary time series does not exhibit any seasonality, which means that there are no regular, repeating patterns or cycles in the data over time.\n\n### Statistical Properties are Time-Invariant\n\nThe statistical properties of a stationary time series, such as mean, variance, and autocorrelation, do not change with time. This property allows for the use of statistical techniques and models that assume constant statistical properties over time.\n\n### Example\n\n* white noise : no pattern about the time independent variable\n\n```{r}\n\n# Generate a random time series data\nset.seed(123)\nts_data <- rnorm(100)\nplot(ts_data)\n# Perform ADF test to check for stationarity\nadf_test <- adf.test(ts_data)\n\n# Print the results\ncat(\"ADF Test Results:\\n\")\ncat(\"Test statistic:\", adf_test$statistic, \"\\n\")\ncat(\"P-value:\", adf_test$p.value, \"\\n\")\n\n# Check if the time series is stationary\nif (adf_test$p.value <= 0.05) {\n  cat(\"Conclusion: The time series is stationary.\\n\")\n} else {\n  cat(\"Conclusion: The time series is not stationary.\\n\")\n}\n```\n\n:::{.callout-note}\nstationary data could have a trend and seasonality, but its period is not constant and easy to be predicted.\n\n```{r}\n# Set random seed for reproducibility\nset.seed(123)\n\n# Generate time series data with irregular trend and seasonality\nn <- 100\nt <- 1:n\ntrend <- 0.1 * t + 2 * sin(t * 0.05) * rnorm(n)\nseasonality <- 2 * sin(t * 0.2 + 1) * rnorm(n)\nirregular <- rnorm(n)\nts_data <- trend + seasonality + irregular\n\n# Plot the time series data\nggplot(data = data.frame(t = t, ts_data = ts_data), aes(x = t, y = ts_data)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Value\", title = \"Time Series Data with Irregular Trend and Seasonality\")\n```\n\nstationary data with an irregular trend and seasonality using a combination of a linear trend (small coefficient), sinusoidal pattern(for varying amplitude), and random noise (irregular trend and seasonality).\n:::\n\n## Non-stationary Data\n\n분석대상으로 시간 축에 대하여 분산(=정보)이 있음\n\n### Example\n\n* 심장 박동 수 : 일정한 주기를 반복해야 건강한 상태\n\n## Conversion Process from Non-tationary to Stationary\n\n### Lag\n\nA lag refers to the time interval between observations in a time series. It represents the number of time units (e.g., time periods, days, months) by which a variable is shifted or delayed in time.\n\n:::{#def-lag}\n\nIn time series analysis, a lag refers to the time interval between observations in a time series. Let $Y_t$ denote the value of a variable at time $t$, and $Y_{t-k}$ denote the value of the same variable at time $t$ lagged by $k$ time units. The lagged value $Y_{t-k}$ is defined as:\n\n$$\nY_{t-k} = Y_{t-k}\n$$\nwhere $k$ is the lag.\n\n:::\n\nThe lag can be positive, indicating a forward shift in time, or negative, indicating a backward shift in time.\n\nFor example, suppose we have a time series of daily temperature data, and we want to examine the relationship between the temperature at a given day and the temperature on the same day one week ago (i.e., $k = 7$). In this case, $Y_t$ represents the temperature at time $t$, and $Y_{t-7}$ represents the temperature on the same day one week ago. By examining the lagged relationship between $Y_t$ and $Y_{t-7}$, we can analyze any patterns or trends in the temperature data over a one-week period.\n\n\n#### Properties\n\n##### Time Shifting\n\nLags allow for time shifting of a time series variable, where the value of the variable at a given time step is compared to its value at a previous time step. This allows for analyzing the temporal relationship and dependencies between values of a time series over different time intervals.\n\n##### Autocorrelation\n\nLags are used to calculate autocorrelation, which is the correlation between a time series variable and its lagged values. Autocorrelation helps in understanding the persistence or pattern of the variable over time, and can be used to detect seasonality, trends, or other patterns in the data.\n\n##### Trend Analysis\n\nLags can be used to analyze trends in time series data. By comparing a time series variable with its lagged values, trends can be identified and analyzed to understand the direction and magnitude of changes in the variable over time.\n\n##### Seasonality Detection\n\nLags can be used to detect seasonality in time series data. By analyzing the relationship between a time series variable and its lagged values, patterns that repeat at regular intervals (e.g., daily, monthly, yearly) can be identified, indicating seasonality in the data.\n\n##### Forecasting\n\nLags are used in time series forecasting models to make predictions about future values of a time series variable. By using lagged values of the variable as predictors, forecasting models can capture the historical patterns and trends in the data to make future predictions.\n\n##### Data Transformation\n\nLags can be used to transform time series data into a different format, such as creating lagged variables or lagged differences, which can be used in various statistical techniques for analysis, modeling, and forecasting of time series data.\n\n```{r}\n# Create an example time series data\nset.seed(123)\n\nn <- 100 # Number of observations\nt <- 1:n\ntrend <- 0.5 * t # Linear trend component\nseasonality <- 10 * sin(2 * pi * t/12) # Seasonal component\nerror <- rnorm(n, mean = 0, sd = 5) # Error component\nts_data <- trend + seasonality + error # Combine components to create time series data\n\n# Create a data frame with lagged variables\nlagged <- data.frame(\n  Value = ts_data)%>%\n  mutate(\n  Lag1 = lag_vec(ts_data, lag = 1),\n  Lag2 = lag_vec(ts_data, lag = 6),\n  Lag3 = lag_vec(ts_data, lag = 12),\n  n=1:n()\n)\nlagged_data<-lagged%>%\ngather(key=lag,value=value,Value:Lag3)\n\nknitr::kable(lagged%>%head(20))\n\n# Plot the time series data and its lagged variables\nggplot(lagged_data, aes(x = n,y = value,color=lag)) +\n  geom_line( size = 1.5, linetype = \"solid\")+\n  labs(title = \"Time Series Data and Lagged Variables\",\n       x = \"Time Step\", y = \"Value\") +\n  scale_color_manual(values=c('darkred','darkgreen','darkblue','black'))+\n  theme(legend.position = \"right\")\n```\n\n### Difference\n\nDifferencing is a common technique used in time series analysis to transform a non-stationary time series into a stationary time series. It involves computing the difference between consecutive observations in the time series to remove trends or seasonality, and create a stationary time series that can be easier to analyze and model.\n\n:::{#def-difference}\nThe differenced time series $Y_t$ of an original time series $X_t$ of order $d$ can be defined as:\n\n$Y_t = X_t - X_{t-d}$\n\nwhere $X_t$ is the original time series value at time $t$, $ X_{t-d}$ is the original time series value at time $t-d$, and $d$ is the order of differencing.\n:::\n\nThe difference in data at a specific time interval could be used to represent time interval data such as year-on-year growth and month-on-month growth\n\n```{r}\n\n# Create a data frame with lagged variables\nlagged <- data.frame(\n  Value = ts_data)%>%\n  mutate(\n  Lag1 = lag_vec(ts_data, lag = 1),\n  Lag2 = lag_vec(ts_data, lag = 6),\n  Lag3 = lag_vec(ts_data, lag = 12),\n  n=1:n(),\n  diff1= c(diff_vec(Value,lag=1)),\n  diff2= c(diff_vec(Value,lag=6)),\n  diff3= c(diff_vec(Value,lag=12)))%>%\n  dplyr::select(n,everything())\n\nknitr::kable(lagged%>%round(3)%>%head(20))\n\nlagged_data<-lagged%>%\ngather(key=process,value=value,Value:diff3)\n\n# Plot the time series data and its lagged variables\n\nggplot(lagged_data%>%filter(!grepl('Lag',process)), aes(x = n,y = value,color=process)) +\n  geom_line(size =1, linetype = \"solid\")+\n  labs(title = \"Time Series Data and Lagged Variables\",\n       x = \"Time Step\", y = \"Value\") +\n  scale_color_manual(values=c('red','green','blue','black'))+\n  theme(legend.position = \"right\")\n\n```\n\n## ACF\n\nACF stands for autocorrelation function. ACF is a statistical tool used in time series analysis to measure the correlation between a time series and its lagged values. It helps to identify the presence of autocorrelation, which is the tendency of a time series to exhibit similar patterns or trends at different time points.\n\n:::{#def-acf}\nThe autocorrelation function $ \\rho(k) $ of a time series $ X_t $ at lag $ k $ can be defined as:\n\n$$ \n\\rho(k) = \\frac{\\text{Cov}(X_t, X_{t-k})}{\\sqrt{\\text{Var}(X_t) \\cdot \\text{Var}(X_{t-k})}}\n$$\n\nwhere $X_t$ is the value of the time series at time $t$, $X_{t-k}$ is the value of the time series at time $t-k$, and $\\text{Cov}(X_t, X_{t-k})$ and $\\text{Var}(X_t)$ are the covariance and variance of the time series, respectively.\n:::\n\nWe can calculate the ACF of this time series to check for autocorrelation using R. Here's an example code:\n\n```{r}\n# Calculate autocorrelation function\nacf_sales<-acf(ts_data)\nacf_sales\n```\n\n### ACF Interpretation\n\n* The lag on the x-axis represents the time lag between the current observation and the lagged observation for which the autocorrelation coefficient is calculated. Lags closer to 0 represent autocorrelation between neighboring observations, while larger lags represent autocorrelation between more distant observations.\n* The height of the autocorrelation coefficients on the y-axis indicates the strength of autocorrelation at different lags. Larger coefficients indicate stronger autocorrelation, while smaller coefficients indicate weaker autocorrelation.\n  * The ACF of the lag $1$ on the $x$ axis means the autocorrelation between the original data, Value and the Value lagged by $k = 1$, and the ACF of the lag $6$ on the $x$ axis means the autocorrelation between the original data, Value and the Value lagged by $k = 6$\n* The sign of the autocorrelation coefficient indicates the direction of autocorrelation. Positive coefficients indicate values tend to be similar at neighboring lags, while negative coefficients indicate values tend to be dissimilar at neighboring lags.\n* The horizontal dashed lines on the ACF plot represent the confidence intervals. Autocorrelation coefficients that fall outside these confidence intervals are considered statistically significant, indicating a high likelihood that the observed autocorrelation is not due to random chance.\n  * the line means the confidence interval = $\\left[ \\text{ACF}(k) \\pm \\frac{z_{\\alpha/2}}{\\sqrt{n}}\\right]$ where $\\operatorname{ACF}(k)$ is the autocorrelation coefficient at lag $k$, $z_{\\alpha/2}$ is the critical value from the standard normal distribution for the desired confidence level (e.g., 1.96 for a 95% confidence level),\n$n$ is the sample size.\n* The pattern of autocorrelation coefficients can provide insights into the presence of trend, seasonality, or other underlying patterns in the time series data. For instance, a repeating pattern of positive and negative autocorrelation coefficients may indicate the presence of seasonality, while a gradual decline in autocorrelation coefficients may indicate the presence of a trend. For our example, we can see the decline and increasing pattern both in ACF and the original plot.\n\n### ACF's Weakness\n\nACF has a weakness in that it can sometimes show spurious correlations due to the effect of earlier lags. \nThis is known as the **chain reaction** or **spillover** effect. For example, if a time series has a strong autocorrelation at lag 1, it can cause subsequent lags to also exhibit autocorrelation, even if there is no true underlying relationship. To address this issue, the Partial Autocorrelation Function (PACF) was developed. \n\n## PACF\n\nPartial Autocorrelation Function stands for PACF.\n\n:::{#def-pacf}\nThe PACF at lag $k$, denoted as $\\operatorname{PACF}(k)$, is defined as the autocorrelation between the original time series and its lagged values, with the effects of all shorter lags removed.\n\n$$ \n\\begin{align*}\n\\text{PACF}(k) &= \\phi_{kk} \\\\\n&= \\frac{\\text{cov}(Y_t, Y_{t-k} | Y_{t-1}, Y_{t-2}, \\ldots, Y_{1})}{\\sqrt{\\text{var}(Y_t | Y_{t-1}, Y_{t-2}, \\ldots, Y_{1}) \\cdot \\text{var}(Y_{t-k} | Y_{t-1}, Y_{t-2}, \\ldots, Y_{1})}}\n\\end{align*}\n$$\n\nwhere $\\phi_{kk}$ represents the partial autocorrelation coefficient at lag $k$.\n:::\n\nPACF measures the autocorrelation between the residuals of a time series after removing the effects of shorter lags. It provides a more direct measure of the linear relationship between the time series at a specific lag, while accounting for the effects of earlier lags. PACF helps to isolate the direct impact of a particular lag on the time series, without the spillover effect from earlier lags.\n\nPACF in time series analysis measures the correlation between a time series value at a specific lag (denoted as $k$) and its lagged value at a previous time step (denoted as $t−k$), after removing the linear dependence on the intermediate lags ($1,2,…,k−1$).\n\nPACF is useful in time series analysis for identifying the order of an autoregressive (AR) model, which is a common type of time series model. AR models use past values of the time series to predict future values. The PACF plot can help identify the significant lags that contribute to the prediction of the time series, and thus aid in model selection and forecasting accuracy.\n\n:::{.callout-note}\nACF(1) is equaivalent to PACF(1) because there is no in-between lags and chain reaction at lag $k = 1$\n:::\n\n```{r}\npacf_result <- stats::pacf(ts_data)\n```\n\n### PACF Interpretation\n\n* A significant positive value at lag k in the PACF plot indicates a strong positive linear relationship between the value at lag k and the current value of the time series. This suggests that the value at lag k is an important predictor for the current value.\n* A significant negative value at lag k in the PACF plot indicates a strong negative linear relationship between the value at lag k and the current value of the time series. This suggests that the value at lag k is an important predictor for the current value, but with an inverse relationship compared to positive values.\n* Non-significant values close to zero in the PACF plot indicate that there is little or no autocorrelation at those lags. This suggests that the value at those lags does not significantly impact the current value of the time series.\n\n## Time Series Model\n\nThe fitted values, denoted as $\\hat{Y}_t$, for a time series model are obtained by applying the estimated model parameters to the observed data points $Y_t$ up to time $t$, using the estimated model equations.\n\n$$\n\\hat{Y}_t = \\hat{\\alpha} + \\hat{\\beta}_1 X_{1,t} + \\hat{\\beta}_2 X_{2,t} + \\ldots + \\hat{\\beta}_p X_{p,t}\n$$\n\nwhere $\\hat{\\alpha}$ is the estimated intercept term, $\\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ are the estimated coefficients for the explanatory variables $X_1, X_2, \\ldots, X_p$ respectively, and $X_{1,t}, X_{2,t}, \\ldots, X_{p,t}$  are the observed values of the explanatory variables at time $t$.\n\n### Fitted Value\n\n```{r}\nmodel<-lm(Value~n,data=lagged)\nfitted_data<-lagged%>%\n  mutate(fit=fitted(model),\n  residual=residuals(model))%>%\n  gather(key=fit_output,value=output,c(Value,fit,residual))\n\nggplot(data=fitted_data,aes(x=n,y=output,color=fit_output))+\ngeom_line()+\ngeom_point()\n```\n\nAs you can see the residual pattern is the same as the original data, Value when you fit the data with a linear regression. If you use the time index as the explanatory variable in a linear regression to fit a time series model, and the residuals will capture the deviations of the observed values from this linear trend. As a result, the residuals will exhibit the same pattern, since they represent the discrepancies between the observed values and the linear trend estimated from the row numbers. It's important to note that using row numbers as the explanatory variable in a linear regression for time series analysis may not always be meaningful, as it does not take into account any underlying patterns, trends, or seasonality present in the data. It's generally recommended to use appropriate time-related variables or other relevant explanatory variables in time series modeling to capture the inherent dynamics of the data.\n\n## White Noise\n\n\n:::{#def-white_noise}\n\nWhite noise is a type of time series data that is characterized by random, uncorrelated, and identically distributed (i.i.d.) values.\n$$ \nY_t \\sim \\operatorname{WN}(0,\\sigma^2)\n$$\n\nwhere \n\n$$\n\\begin{align*}\nX_t & : \\text{The value of the white noise at time t} \\\\\n\\text{WN} & : \\text{Indicates that the data follows a white noise process} \\\\\n0 & : \\text{The mean of the white noise process, which is typically assumed to be 0} \\\\\n\\sigma^2 & : \\text{The variance of the white noise process, which determines the spread of the random values}\n\\end{align*}\n$$\n:::\n\nIt is often used as a reference or benchmark series to compare against other time series data for identifying patterns or structures.\n\n### Properties\n\n* Randomness: White noise is a series of random values that are not predictable or follow any pattern.\n* Independence: The values in a white noise series are uncorrelated, meaning that the value at any time point does not depend on the values at other time points.\n* Identically Distributed: The values in a white noise series are drawn from the same distribution, typically assumed to have a constant mean and variance.\n* Constant Mean: The mean of a white noise series is typically assumed to be constant and equal to zero, although this can be adjusted to a different value if necessary.\n* Constant Variance: The variance of a white noise series is typically assumed to be constant, meaning that the spread of the values remains the same over time.\n* No Autocorrelation: White noise series have no autocorrelation, meaning that the correlation between the values at different time points is close to zero.\n* No Trend: White noise series do not exhibit any trend or pattern over time, as the values are purely random and do not follow any systematic behavior.\n* Useful as a Benchmark: White noise series are often used as a benchmark or reference to compare against other time series data for identifying patterns or structures.\n\n### Ljung-box Test\n\nThe Ljung-Box test is a statistical test used to assess whether a time series data exhibits significant autocorrelation at different lags. The null hypothesis ($H_0$) of the Ljung-Box test is that there is no autocorrelation in the time series data up to a certain lag, while the alternative hypothesis ($H_a$) is that there is significant autocorrelation present. It is used to assess the goodness-of-fit of a model by testing whether the autocorrelation coefficients of the residuals (or errors) of the model are significantly different from zero.\n\n$$\nQ(m) = n(n+2) \\sum_{k=1}^{m} \\frac{\\hat{\\rho}_k^2}{n-k} \\sim \\chi^2_{1-\\alpha,h}\n$$\n\nwhere:\n- $Q(m)$ is the Ljung-Box test statistic for a given maximum lag $m$\n- $n$ is the sample size of the time series data\n- $\\hat{\\rho}_k$ is the sample autocorrelation at lag $k$\n- Under $H_0$, $Q(m)$ assymptotically follows a $\\chi^2_{1-\\alpha,h}$\n- $h$ is the number of lags being tested\n\nSuppose we have a time series data vector $x$ of length $n$, and we want to perform a Ljung-Box test up to a maximum lag of $m$.\n\nThe Q statistic follows a chi-squared distribution with degrees of freedom equal to the number of autocorrelation coefficients being tested. The p-value associated with the Q statistic can be compared to a chosen significance level (e.g., 0.05) to determine if the residuals exhibit significant autocorrelation. If the p-value is below the chosen significance level, it suggests that the model may have inadequate fit and that there may be remaining autocorrelation in the residuals.\n\n:::{.callout-tip}\nThe Box-Pierce test is a modified version of the Ljung-Box test,\n$$\nQ(m) = n \\sum_{k=1}^{m} \\hat{\\rho}_k^2\n$$\n\nLjung-Box test incorporates the sample size $n$ in the denominator. This makes the Ljung-Box test more appropriate for small sample sizes, while the Box-Pierce test is suitable for larger sample sizes.\n:::\n\n\n### Example\n\nA classic example of white noise is a series of random numbers generated from a standard normal distribution, where each value in the series is independent and identically distributed with mean 0 and variance 1.\n\n```{r}\n# Generate white noise series\nset.seed(123)\nn <- 100 # Number of observations\nwn <- rnorm(n, mean = 0, sd = 1) # Generate random values from standard normal distribution\n\n# Plot white noise series\nwn_data <- data.frame(Time = 1:n, Value = wn)\nggplot(wn_data, aes(x = Time, y = Value)) +\n  geom_line() +\n  labs(title = \"White Noise Series\", x = \"Time\", y = \"Value\")\n\nresult<-acf(wn)\nplot(result)\ncheckresiduals(wn)\n```\n\n## Time Series Decomposition\n\nThe characteristics of time series data has trend, seasonality, and autocorrelation. To check if autocorrelation exists in data, we used ACF and PACF. Then, how to check the trend and seasonality characteristics? Time series decomposition can be used to check them.\n\nThe observed time series $y_t$ can be decomposed into four components: the trend component $T_t$, the seasonal component $S_t$, the cyclical component $C_t$, and the remainder or error component $E_t$. This can be expressed as:\n\n$$\n\\begin{equation}\ny_t = T_t + S_t + C_t + E_t, \\quad \\text{where} \\quad E_t \\sim \\text{WN}(0,\\sigma^2),\n\\end{equation}\n$$\n\nwhere $T_t$ is the trend component, $S_t$ is the seasonal component, $C_t$ is the cyclical component, and $E_t$ is the error term, which is a random variable with a white noise distribution with mean 0 and variance $\\sigma^2$.\n\nThe trend component $T_t$ represents the long-term behavior of the time series, and can be either deterministic or stochastic. The seasonal component $S_t$ represents the regular and repeated variations that occur within a single year or other fixed time period. The cyclical component $C_t$ represents the irregular variations in the time series that do not follow a fixed pattern.\n\n:::{.callout-tip}\nDeterministic refers to a situation where the outcome is completely determined by the initial conditions, and there is no randomness involved. For example, if you drop a ball from a certain height, the time it takes to hit the ground can be determined exactly based on the initial height and the acceleration due to gravity. This is a deterministic process because there is no randomness involved.\n\nStochastic, on the other hand, refers to a situation where the outcome is uncertain and subject to randomness. For example, if you roll a fair six-sided die, you cannot predict with certainty what number will come up. The outcome is determined by chance, and is therefore stochastic.\n:::\n\nThere are two ways of decomposing time series data: **additive decomposition** and **multiplicative decomposition**\nThis additive deomoposition method is suitable for the time series data with a constant variation of seasonality according the trend, while multiplicative decomposition is suitable for the time series data with a inconsistent variation of seasonality according the trend. The variation magnitude could increase or decrease.\n\n### Additive Decomposition\n\nA common method for decomposing time series is the additive decomposition, which can be expressed as:\n$$\n\\begin{equation}\ny_t = T_t + S_t + E_t.\n\\end{equation}\n$$\n\n\nThe trend component $T_t$ is estimated by smoothing the data using techniques such as moving averages or exponential smoothing. The seasonal component $S_t$ is estimated by computing the seasonal indices, which are ratios of the observed values to the estimated trend component. The cyclical component $C_t$ is usually not explicitly estimated, but can be identified as the fluctuations that remain after removing the trend and seasonal components.\n\nFor example, let $X_t$ be a quarterly time series of sales data for a company, with $t$ ranging from 1 to 20. We can decompose the time series using an additive model:\n\n$$\n\\begin{equation}\nX_t = T_t + S_t + E_t,\n\\end{equation}\n$$\n\nwhere $T_t$ is the trend component, $S_t$ is the seasonal component, and $E_t$ is the error term. The trend component can be estimated using a 3-period moving average:\n$$\n\\begin{equation}\nT_t = \\frac{1}{3} (X_{t-1} + X_t + X_{t+1}),\n\\end{equation}\n$$\n\nwhere $t=2,3,\\ldots,19$. The seasonal component can be estimated by computing the average value of $X_t$ for each quarter:\n\n$$\n\\begin{equation}\nS_t = \\frac{1}{5} \\sum_{i=0}^3 X_{t+4i}.\n\\end{equation}\n$$\n\nFinally, the error term $E_t$ can be computed as:\n\n$$\n\\begin{equation}\nE_t = X_t - T_t - S_t.\n\\end{equation}\n$$\n\nThis decomposition allows us to separate the long-term trend, seasonal patterns, and irregular fluctuations in the sales data, and can help us identify any underlying patterns or trends that may be present in the data.\n\n:::\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n::: {#English .tab-pane .fade role=\"tabpanel\" aria-labelledby=\"English-tab\"}\n\n:::\n\n</div>\n\n## Go to Blog Content List\n\n[Blog Content List](../../content_list.qmd)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"01_basic.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Basic (1) - Stationay Checking","subtitle":"Overview","description":"template\n","categories":["Statistics"],"author":"Kwangmin Kim","date":"03/23/2023","draft":true,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}