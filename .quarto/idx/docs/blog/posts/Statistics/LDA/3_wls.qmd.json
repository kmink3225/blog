{"title":"LDA (3) - Weight Least Square & REML","markdown":{"yaml":{"title":"LDA (3) - Weight Least Square & REML","subtitle":"Overview","description":"template\n","categories":["Statistics"],"author":"Kwangmin Kim","date":"03/25/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"draft":true},"headingText":"Notations","containsRefs":false,"markdown":"\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n::: {#Korean .tab-pane .fade .show .active role=\"tabpanel\" aria-labelledby=\"Korean-tab\"}\n\n\n* $y_{ij}$ : the univariate response (i.e. scalar) for the $i$ th subject at the $j$ th occasion or measurement\n  * later when I use the vector case, I will re-define this notation, but focus on the scalar case for now.\n* $x_{ij}$ : the predictor at time $t_{ij}$, which is either a scalr or vector.\n  * a scalar case: $x_{ij}$ where $i$ is the $i$ th subject, and $j$ is the $j$ th measurement.\n  * a vector case: $x_{ijk}$ where $i$ is the $i$ th subject, $j$ is the $j$ th measurement, and $k \\in [1,p]$ is the $k$ th predictor.\n  * sometimes, covariate for different measurements could be the same. In this case, the notation could be written in $x_{i}$ \n    * ex) a gender does not change over time in the most cases.\n* $i=1, \\dots, m$ : i is the index for the $i$ th subject\n* $j=1, \\dots, n_i$ : j is the index for the $j$ th measurement of the $i$ th subject\n  * ${n_i}$ is the number of measurements of the $i$ th subject, each ${n_i}$ does not have to the same.\n  * balanced desgin: ${n_i}$ is the same.\n  * unbalanced desgin: ${n_i}$ is different.\n* $\\mathbf y_i$ : a vector (not a matrix), $(y_{i1},y_{i2},\\dots ,y_{in_i})$ of the $i$ th subject\n* $\\mathbf Y$ : the reponse matrix \n* $\\mathbf X$ : the predictor matrix \n* $\\text{E}(y_{ij})$ : $\\mu_{ij}$\n* $\\text{E}(\\mathbf y_i)$ : $\\mathbf \\mu_{i}$\n* $\\text{Var}(\\mathbf y_i)$ : $\\text{Var}(\\mathbf y_i)$ is a variance-covariance matrix of the different measurement for the $i$ th subject\n  * for now, we do not care of the variance covariance of the different subjects because we assume that the measurements of different subjects are indpendent.\n$$\n\\begin{bmatrix}\n    \\text{Var}(y_{i1}) & \\text{Cov}( y_{i1}, y_{i2}) & \\dots & \\text{Cov}( y_{i1}, y_{in_i}) \\\\\n                               & \\text{Var}( y_{i2}) & \\dots & \\text{Cov}( y_{i2}, y_{in_i}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{in_i}) \n\\end{bmatrix}\n$$\n\n## OLS vs GLS\n\nOLS (Ordinary Least Squares) and GLS (Generalized Least Squares) are both methods of regression analysis used to model the relationship between a dependent variable and one or more independent variables. The main difference between OLS and GLS is in the assumptions about the errors in the model. \n\n### OLS \n\nIt assumes that the errors are homoscedastic and independent of each other. OLS is a simpler and more commonly used method, but it may not be appropriate for datasets with non-constant variances or autocorrelation in the errors.\n\n#### OLS Solutions\n\n$$\n\\begin{aligned}\n  \\hat{\\beta}&=\\min_\\beta||y-X\\beta||^2 \\\\ \n  &=(X^TX)^{-1}X^Ty\n\\end{aligned}\n$$\n\n$\\hat \\beta$ is unbiased because if we assume that the errors $\\epsilon \\sim N(0,\\sigma^2I)$, \n$E(\\hat \\beta)=\\text{E}((X^TX)^{-1}X^Ty)=(X^TX)^{-1}X^T\\text{E}(y)=(X^TX)^{-1}X^TX\\beta=\\beta$\n\n### GLS \n\nIt relaxes the assumptions of OLS and allows for heteroscedasticity and autocorrelation in the data. GLS is a more flexible method that can handle heteroscedasticity and autocorrelation in the data, but it requires more complex computations and may be computationally expensive for large datasets.\n\n#### Generalized Least Squares\n\nThe estimator $\\hat{\\beta}$, which is also known as the generalized least squares estimator, is given by $\\hat{\\beta}=(X^TWX)^{-1}X^TWy$, where $W$ is a positive definite weighting matrix.\n\n$$\n\\begin{aligned}\n  \\hat{\\beta}&=\\min_\\beta||y-X\\beta||^2 \\\\ \n  &=(X^TWX)^{-1}X^TWy\n\\end{aligned}\n$$\n\n\n##### Weighted Least Square\n\nThe weighted least squares (WLS) solution can be obtained by minimizing the sum of squared weighted residuals, given by:\n\n$$\n\\hat{\\beta}=\\min_\\beta||y-X\\beta||^2 \n$$\n\nThe WLS solution is given by:\n$$\n\\beta_{WLS} = (X^TWX)^{-1}X^TWy\n$$\n\nwhere $X$ is the design matrix, $W$ is a diagonal weight matrix with $w_i$ on the $i$ th diagonal element, and $y$ is the vector of responses. The predicted response $\\hat{y}$ can be obtained as $\\hat{y} = X\\hat{\\beta}$.\n\n:::{.callout-note}\nNote that the OLS solution is a special case of WLS when all weights are equal to 1.\n:::\n\n1. Define the weighted design matrix, $\\mathbf{W}$, as a diagonal matrix of weights, where each diagonal element corresponds to the weight for the corresponding observation.\n1. Define the weighted response vector, $\\mathbf{y}_{w}$, as a vector of the response values multiplied by the square root of the corresponding weight.\n1. Define the weighted parameter estimates, $\\hat{\\beta}_{w}$, as the solution to the weighted least squares problem:\n$$\n\\hat{\\beta}_w = \\operatorname*{arg\\,min}_{\\beta} (y_w - X\\beta)^T W (y_w - X\\beta)\n$$\nwhere $\\mathbf{X}$ is the design matrix of predictor variables.\n4. The estimated model can be obtained by substituting the weighted parameter estimates, $\\hat{\\beta}_{w}$, into the regression equation:\n\n$$\n\\hat{y}=\\mathbf{X}\\hat{\\beta}_w\n$$\n\nLet's start by defining the problem: we have a set of m data points, represented as a matrix $\\mathbf{X}$ with dimensions $m \\times n$, where $n$ is the number of independent variables. We also have a corresponding vector $\\mathbf y$ with $m$ elements, representing the dependent variable. We want to fit a linear function of the form $\\mathbf y = \\mathbf{X\\beta}+ \\mathbf \\epsilon$ to the data points, where $\\mathbf \\beta$ is a vector of coefficients to be determined and $\\mathbf \\epsilon$ is the residual error.\n\nTo perform weighted least squares, we define a weight matrix $\\mathbf W$ with dimensions $m \\times m$, where the diagonal elements $w_i$ are the weights for each data point $i$. Weights are typically chosen to be proportional to the inverse of the variance of the data point, so that data points with smaller variances are given more weight.\n\nUsing this weight matrix, the objective function for weighted least squares is defined as follows:\n$$\n\\begin{aligned}\n\\text{minimize } S &= (y - X\\beta)^TW(y - X\\beta) \\\\\n&= y^TWy - \\beta^TX^TWy - y^TWX\\beta + \\beta^TX^TWX\\beta \\\\\n\\frac{\\partial S}{\\partial \\beta} &= -2X^TWy + 2X^TWX\\beta = 0 \\\\\nX^TWX\\beta &= X^TWy \\\\\n\\beta &= (X^TWX)^{-1}X^TWy\n\\end{aligned}\n$$\n\n\nUnder the assumptions of the GLS model, the GLS estimator is unbiased, which means on average, the GLS estimator will estimate the true population parameters correctly.\nTo be specific, the GLS estimator is based on the assumption that the errors or residuals follow a multivariate normal distribution with a mean vector of zeros and a covariance matrix that is known up to a scalar factor. If this assumption holds, then the GLS estimator is the Best Linear Unbiased Estimator (BLUE) of the model parameters.\n\n:::{.callout-note}\nBeing a BLUE estimator implies that the GLS estimator has the smallest possible variance among all linear unbiased estimators. Therefore, under the GLS assumptions, the GLS estimator is not only unbiased but also efficient, meaning that it achieves the lowest possible variance of all unbiased estimators.\n\nHowever, If the assumptions are violated (e.g., the errors are not normally distributed or the covariance structure is misspecified), then the GLS estimator may not be unbiased or efficient.\n:::\n\n$$\n\\hat{\\beta}_{GLS} = (X^T V^{-1} X)^{-1} X^T V^{-1} Y\n$$\n\nwhere $V$ is the known covariance matrix of the errors or residuals.\n\nTo show that $\\hat{\\beta}_{GLS}$ is unbiased, we need to show that:\n\n$$\n\\operatorname{E}(\\hat{\\beta}_{GLS}) = \\beta\n$$\n\nwhere $\\beta$ is the true population parameter.\n\nUsing the linearity of expectation, we have:\n\n$$\n\\begin{align}\n\\operatorname{E}(\\hat{\\beta}_{GLS}) \n&= \\operatorname{E}((X^T W^{-1} X)^{-1} X^T W^{-1} Y)\\\\\n&= (X^T W^{-1} X)^{-1} X^T W^{-1} \\operatorname{E}(Y)\\\\\n&= (X^T W^{-1} X)^{-1} X^T W^{-1} X \\beta\\\\\n&= (X^T W^{-1} X)^{-1} X^T W^{-1} X (X^T X)^{-1} X^T X \\beta\\\\\n&= (X^T X)^{-1} X^T W^{-1} X (X^T X)^{-1} X^T X \\beta\\\\\n&= (X^T X)^{-1} X^T W^{-1} Y\\\\\n&= \\beta\n\\end{align}\n$$\n\nwhere the second-to-last equality follows from the fact that $X^T W^{-1} X$ is a symmetric positive definite matrix, and hence its inverse can be written as $(X^T X)^{-1}$. Therefore, we have shown that the GLS estimator is unbiased, i.e., its expected value is equal to the true population parameter.\n\n\n\n\n\n\n#### GLS Solution\n\nThe Generalized Least Squares (GLS) estimator is obtained by minimizing the weighted sum of squared residuals, where the weights are based on the estimated variance-covariance matrix of the errors.\n\nTo derive the GLS estimator, we start with the linear regression model:\n\n$Y = X\\beta + \\epsilon$\n\nwhere $Y$ is the response variable, $X$ is the design matrix of predictor variables, $\\beta$ is a vector of unknown coefficients, and $\\epsilon$ is a vector of errors or residuals.\n\nThe covariance matrix of the errors is denoted by $V = \\text{Cov}(\\epsilon)$, which is assumed to be known up to a scalar factor. Specifically, we assume that $V = \\sigma^2 W$, where $\\sigma^2$ is an unknown scalar factor and $W$ is a known positive definite matrix.\n\nThe GLS estimator of $\\beta$ is obtained by minimizing the weighted sum of squared residuals:\n\n$Q(\\beta) = (\\textbf{Y} - \\textbf{X}\\beta)^T V^{-1} (\\textbf{Y} - \\textbf{X}\\beta)$\n\nwhere $\\textbf{Y}$ and $\\textbf{X}$ are the vectors of observed responses and design matrix of predictors, respectively.\n\nTaking the derivative of $Q(\\beta)$ with respect to $\\beta$, and setting it to zero, we get:\n\n$\\frac{\\partial Q(\\beta)}{\\partial \\beta} = -2 \\textbf{X}^T V^{-1} (\\textbf{Y} - \\textbf{X}\\beta) = 0$\n\nSolving for $\\beta$, we obtain the GLS estimator:\n\n$\\hat{\\beta}_{GLS} = (\\textbf{X}^T V^{-1} \\textbf{X})^{-1} \\textbf{X}^T V^{-1} \\textbf{Y}$\n\nwhere $V^{-1} = \\frac{1}{\\sigma^2} W^{-1}$.\n\nNote that the GLS estimator reduces to the OLS estimator when $V$ is a scalar multiple of the identity matrix, i.e., when the errors have constant variance and are uncorrelated. In this case, $W = I$ and $V = \\sigma^2 I$, and the GLS estimator simplifies to:\n\n$\\hat{\\beta}_{OLS} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{Y}$\n\nwhich is the usual OLS estimator.\n\n\n\n\n\n## If  W=blkdiag(\\Sigma)^{-1} in longitudinal data analysis, why more efficient than OLS?\n\nThe expression \"blkdiag(\\Sigma)^{-1}\" means the inverse of a block diagonal matrix where each block is a covariance matrix denoted by Sigma (\\Sigma). Specifically, if we have n covariance matrices \\Sigma_1, \\Sigma_2, ..., \\Sigma_n, then the block diagonal matrix is given by:\n\nblkdiag(\\Sigma_1, \\Sigma_2, ..., \\Sigma_n) = \\begin{bmatrix} \\Sigma_1 & 0 & \\cdots & 0 \\ 0 & \\Sigma_2 & \\cdots & 0 \\ \\vdots & \\vdots & \\ddots & \\vdots \\ 0 & 0 & \\cdots & \\Sigma_n \\end{bmatrix}\n\nThe inverse of this block diagonal matrix can be computed by taking the inverse of each block matrix and placing them on the diagonal, resulting in the following expression:\n\n[blkdiag(\\Sigma_1, \\Sigma_2, ..., \\Sigma_n)]^{-1} = \\begin{bmatrix} \\Sigma_1^{-1} & 0 & \\cdots & 0 \\ 0 & \\Sigma_2^{-1} & \\cdots & 0 \\ \\vdots & \\vdots & \\ddots & \\vdots \\ 0 & 0 & \\cdots & \\Sigma_n^{-1} \\end{bmatrix}\n\nTherefore, blkdiag(\\Sigma)^{-1} is a block diagonal matrix where each block is the inverse of the corresponding covariance matrix, and it can be computed by taking the inverse of each block and placing them on the diagonal.\n\nREML for estimating \\Sigma variance covrainace matrix divides the data into twp parts: irrelevant to sigma and irrelevant to beta (REML estimate)\n\n\nIn longitudinal data analysis, it is common to use a generalized linear mixed model (GLMM) to account for the correlated nature of the data. The GLMM framework includes random effects to capture the individual-level variation and allows for the specification of a covariance structure to model the correlation between the repeated measurements within each individual. When fitting a GLMM, the covariance matrix of the random effects, denoted by \\Sigma, needs to be estimated.\n\nOne way to estimate \\Sigma is to use maximum likelihood estimation (MLE), which involves maximizing the likelihood function of the GLMM with respect to the unknown parameters, including \\Sigma. However, the MLE of \\Sigma can be inefficient when the number of repeated measurements per individual is small or the correlation between the repeated measurements is weak.\n\nTo improve the efficiency of the MLE of \\Sigma, a weighted likelihood method can be used, where the likelihood function is multiplied by a weight matrix that depends on the estimated covariance matrix. Specifically, the weight matrix is given by W = blkdiag(\\Sigma)^{-1}, where blkdiag(\\Sigma) is the block diagonal matrix of the estimated covariance matrix \\Sigma. The inverse of blkdiag(\\Sigma) is taken because it is a positive definite matrix and its inverse exists.\n\nBy weighting the likelihood function with W, the resulting weighted likelihood estimator (WLE) of \\Sigma is more efficient than the MLE because it incorporates additional information about the covariance structure of the data. The WLE of \\Sigma is then used in the GLMM to estimate the other parameters, such as the fixed effects.\n\nIn summary, using W = blkdiag(\\Sigma)^{-1} as a weight matrix in the GLMM framework can improve the efficiency of the MLE of the covariance matrix \\Sigma and result in more accurate estimates of the other parameters in the model, making it more efficient than the OLS estimator, which assumes independence between the observations.\n\n### Family and Longitudinal Data\n\n\nIn longitudinal data analysis with family data, one common approach to estimating the regression coefficients is to use a linear mixed-effects model, also known as a multilevel model or a hierarchical model. The linear mixed-effects model can handle the within-subject correlation due to repeated measurements over time and the within-family correlation due to shared genetic or environmental factors among family members.\n\nThe linear mixed-effects model assumes that the outcome variable Y is a function of the fixed effects X and the random effects b, which can account for the within-subject and within-family correlation. The model can be written as:\n\nY = Xβ + Zb + ε\n\nwhere β is the vector of fixed effects coefficients, b is the vector of random effects coefficients, Z is the design matrix for the random effects, and ε is the error term.\n\nTo estimate the fixed effects coefficients β, one can use maximum likelihood estimation (MLE) or restricted maximum likelihood estimation (REML) methods. The MLE estimates the variance components for both the random effects and the error term, while the REML estimates the variance components for only the random effects and adjusts for the bias in the likelihood function.\n\nTo fit the linear mixed-effects model, one can use software packages such as R, SAS, or Stata, which have functions for fitting linear mixed-effects models with repeated measurements and random effects. In R, for example, one can use the lme4 package to fit the linear mixed-effects model using the lmer() function. The output of the function includes the estimated fixed effects coefficients β and the estimated variance components for the random effects and the error term.\n\nOverall, estimating the fixed effects coefficients β of family data with repeated measurements in longitudinal data analysis requires fitting a linear mixed-effects model that accounts for the within-subject and within-family correlation, and the choice of model depends on the research question and assumptions of the data.\n\nIn longitudinal data analysis with repeated measurements and correlated subjects, the variance-covariance matrix of the responses can be modeled using a mixed-effects model or a generalized estimating equation (GEE) model.\n\nIn the mixed-effects model, the variance-covariance matrix of the responses is decomposed into two components: the within-subject covariance and the between-subject covariance. The within-subject covariance accounts for the correlation between the repeated measurements within the same subject, while the between-subject covariance accounts for the correlation between subjects. The mixed-effects model allows for the inclusion of both fixed effects and random effects, which can model the mean and the variance structure of the responses.\n\nIn the GEE model, the variance-covariance matrix of the responses is modeled using a working correlation matrix, which specifies the correlation structure between the observations. The GEE model allows for the inclusion of fixed effects, but not random effects, and estimates the population-averaged mean and the variance structure of the responses.\n\nTo design the variance-covariance matrix of the responses in a mixed-effects model or a GEE model, one needs to specify the correlation structure between the repeated measurements within the same subject and between subjects. Common correlation structures for the within-subject covariance include the autoregressive (AR), the exchangeable, and the unstructured covariance structures, while common correlation structures for the between-subject covariance include the independent and the compound symmetry covariance structures.\n\nThe choice of correlation structure depends on the research question and assumptions of the data. For example, the AR(1) correlation structure assumes that the correlation between two measurements decreases as the time lag between them increases, while the exchangeable correlation structure assumes that all measurements within the same subject are equally correlated. The choice of correlation structure can be evaluated using statistical criteria such as the Akaike information criterion (AIC) or the Bayesian information criterion (BIC).\n\nOverall, designing the variance-covariance matrix of the responses in longitudinal data analysis with repeated measurements and correlated subjects requires specifying the correlation structure between the repeated measurements within the same subject and between subjects, and the choice of model depends on the research question and assumptions of the data.\n\n\n\n```{r}\n#| eval: false\n\n# this file contains code for linear marginal models for longitudinal data\n# We test different covariance patterns and show how to fit model with WLS and REML\n# By Gen Li, 1/1/2018\n# also check lme4\n\n\nlibrary(nlme)\nlibrary(tidyverse)\nopposites <- read.table(\"https://stats.idre.ucla.edu/stat/r/examples/alda/data/opposites_pp.txt\",header=TRUE,sep=\",\")\nhead(opposites)\n\n# spaghetti plot\np = ggplot(opposites, aes(time, opp, group=id)) + geom_line()\nprint(p)\n\n\n# Fit different cov model with REML\n###################################################\n# unstructured covariance for marginal model\nunstruct <- gls(opp~time*ccog,opposites, correlation=corSymm(form = ~ 1 |id),  weights=varIdent(form = ~ 1| wave),method=\"REML\")\n\n# corSymm(form = ~ 1 |id) : the same covariance across different measurement, same correlation matrix for different subjects\n# check ?gls ?corClasses ?corSymm\nsummary(unstruct) # focus on corr, var, (weight)\nunstruct$modelStruct$corStruct # corr\nunstruct$modelStruct$varStruct # variance:weight\nunstruct$sigma # standard deviation\ncov2cor(unstruct$varBeta)\n\n\n# compound symmetry\ncomsym <- gls(opp~time,opposites, correlation=corCompSymm(form = ~ 1|id),   weights=varIdent(form = ~ 1| wave), method=\"REML\")\nsummary(comsym)\ncorMatrix(comsym$modelStruct$corStruct)[[1]]\n\n\n\n# AR(1)\nauto1 <- gls(opp~time ,opposites, correlation=corAR1(form = ~ 1 |id), method=\"REML\")\nsummary(auto1)\ncorMatrix(auto1$modelStruct$corStruct)[[1]]\n\n\n\n```\n\n\n::: {.callout-tip}\n\n### Longitudinal Study vs Cross-Sectional Study Example\n\nA cross-sectional study found that older people smoke more.\n\nPossible explanations:\n\n* People tend to smoke more when they get older.\n* Older people grew up in an environment where the harm of smoking was less widely accepted. In other words, when older people were younger, smoking was more socially acceptable and its harmful effects were not well-known or well-publicized. As a result, they may have started smoking earlier in life and developed a stronger habit or addiction. This explanation implies that younger people today may be less likely to smoke because they are growing up in an environment where smoking is less socially acceptable and the risks are more widely known.\n\nLDA can distinguish the effect due to aging (i.e., changes over time within subject) from cohort effects (i.e., difference between subjects at baseline). Cross-sectional study cannot.\n:::\n\n## Advantages of Longitudinal Data Analysis\n\n* Each subject can serve as his/her own control. Influence of genetic make-up, environmental exposures, and maybe unmeasured characteristics tend to persist over time.\n    * in certain types of studies or experiments, individuals can be used as their own comparison group. This means that the same person is tested or measured at different points in time, and the differences observed can be attributed to changes over time rather than to differences between individuals. \n    * For example, in a study looking at the effect of a new medication on blood pressure, each participant's blood pressure before and after taking the medication would be compared to determine if there was a change. By using the same participant as their own control, the effects of genetic factors, environmental exposures, and other individual differences that might influence blood pressure would be minimized.\n    * However, even when using this approach, some individual differences that are not directly measured or controlled for may persist over time and influence the results. These could include factors such as diet, stress levels, or other lifestyle habits that could impact the outcome being measured.\n* Distinguish the degree of variation in $Y$ across time within a subject from the variation in $Y$ between subjects. With repeated values, one can borrow strength across time for the person of interest as well as across people.\n    * when you have repeated measurements of a variable (Y) for the same person over time, you can use the information from those repeated measurements to better estimate the true value of Y for that person at any given time point. This is known as borrowing strength across time.\n    * Additionally, you can also use the information from the repeated measurements of Y across different people to better estimate the true value of Y for a particular time point across the population. This is known as borrowing strength across people.\n    * By doing both, you can distinguish the degree of variation in Y across time within a subject (i.e., how much Y varies for a particular person over time) from the variation in Y between subjects (i.e., how much Y varies between different people at a particular time point).\n* Increased power, by repeated measurements. The repeated measurements from the same subject are rarely perfectly correlated. Hence, longitudinal studies are more powerful than cross-sectional studies.\n    * Longitudinal studies are more powerful than cross-sectional studies because they allow researchers to directly model and account for the within-subject correlation among repeated measurements. In other words, longitudinal studies take advantage of the fact that individuals are their own controls by measuring outcomes at multiple time points, which allows for a more accurate estimation of the true effect of an exposure or intervention.\n    * In contrast, cross-sectional studies only measure outcomes at a single time point, which makes it difficult to distinguish between within-subject variability and between-subject variability. In a cross-sectional study, any observed differences between groups may be due to differences in the underlying populations, or due to differences in the timing of the outcome measurement, or both. \n    * Furthermore, longitudinal studies can also provide information on the rate of change in the outcome over time, which can be important in understanding disease progression, treatment effects, or the impact of other time-dependent factors.\n* Therefore, because longitudinal studies allow for a more accurate estimation of the true effect of an exposure or intervention and provide more information about disease progression, they are generally considered more powerful than cross-sectional studies.\n\n### Specialty of LDA\n\nLDA requires special statistical methods because the set of observations on one subject tends to be inter-correlated.\n\n[Copied from Diggle et al. (2002, page 2).](./childhood%20readbility.PNG)\n\n* Consider the example, variation of readability of child as they get aged.\n    * Assume this is a longitudinal study with two measurements per child at two age or time points.\n    * The two measurements per subject may be highly correlated.\n    * If we use cross-sectional methods to analyze the data, we may not be able to distinguish changes over time within individual and difference among people in their baseline levels.\n        * Only plot (a) is from cross sectional study. Not using connected lines might mislead the time trend within subjects.\n* In general, repeated observations $y_{i1}, \\dots , y_{in_i}$ for subject $i$ are likely to be correlated, so the independence assumption is violated.\n* The standard regression methods (ignoring correlation) may lead to\n    * Incorrect inference\n        * the violation of the independence assumption means that the errors in the model are correlated across observations, and this correlation can bias the estimated coefficients.\n        * The errors in the model are correlated across observations when there is some form of dependence or clustering in the data. This means that the error term in one observation is related to the error terms in other observations, either through some underlying factor or because of the way the data is collected.\n        * When the errors are correlated across observations, the estimated coefficients from standard regression methods may be biased because the regression model assumes that the errors are independent. \n        * The bias in the estimated coefficients can arise in several ways:\n            * The standard errors of the estimated coefficients will be too small, which can lead to overconfidence in the results.\n            * The estimated coefficients may not reflect the true relationships between the dependent variable and the independent variables, as the correlation between the errors can distort the estimates.\n            * The estimates of the standard errors will be biased, leading to incorrect inference in hypothesis testing and confidence interval construction.\n        * To sum up, failing to account for the correlation between errors can lead to incorrect and imprecise estimates of the coefficients and standard errors in a regression model.\n    * Inefficient estimates of $\\beta$\n        * the estimates are less precise than they could be if the correlation between observations were taken into account. \n        * The standard errors of the estimates will be too large, which means that confidence intervals will be wider and hypothesis tests will have less power.\n    * Oversight of important correlation structure\n        * the standard regression methods may miss important patterns of correlation in the data that could be used to improve the accuracy and precision of the estimates. \n        * For example, if there is a time trend in the data that is not accounted for, the standard errors of the estimates may be too large, and the estimates may not capture the true effect of the independent variables. \n        * Accounting for the correlation structure in the data can lead to more accurate and precise estimates, and can also help identify interesting patterns and relationships that might otherwise be missed.\n\n:::\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n::: {#English .tab-pane .fade role=\"tabpanel\" aria-labelledby=\"English-tab\"}\n\n:::\n\n</div>\n\n\n# Go to Project Content List\n\n[Project Content List](./docs/projects/index.qmd)\n\n# Go to Blog Content List\n\n[Blog Content List](./docs/blog/posts/content_list.qmd)","srcMarkdownNoYaml":"\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n::: {#Korean .tab-pane .fade .show .active role=\"tabpanel\" aria-labelledby=\"Korean-tab\"}\n\n## Notations\n\n* $y_{ij}$ : the univariate response (i.e. scalar) for the $i$ th subject at the $j$ th occasion or measurement\n  * later when I use the vector case, I will re-define this notation, but focus on the scalar case for now.\n* $x_{ij}$ : the predictor at time $t_{ij}$, which is either a scalr or vector.\n  * a scalar case: $x_{ij}$ where $i$ is the $i$ th subject, and $j$ is the $j$ th measurement.\n  * a vector case: $x_{ijk}$ where $i$ is the $i$ th subject, $j$ is the $j$ th measurement, and $k \\in [1,p]$ is the $k$ th predictor.\n  * sometimes, covariate for different measurements could be the same. In this case, the notation could be written in $x_{i}$ \n    * ex) a gender does not change over time in the most cases.\n* $i=1, \\dots, m$ : i is the index for the $i$ th subject\n* $j=1, \\dots, n_i$ : j is the index for the $j$ th measurement of the $i$ th subject\n  * ${n_i}$ is the number of measurements of the $i$ th subject, each ${n_i}$ does not have to the same.\n  * balanced desgin: ${n_i}$ is the same.\n  * unbalanced desgin: ${n_i}$ is different.\n* $\\mathbf y_i$ : a vector (not a matrix), $(y_{i1},y_{i2},\\dots ,y_{in_i})$ of the $i$ th subject\n* $\\mathbf Y$ : the reponse matrix \n* $\\mathbf X$ : the predictor matrix \n* $\\text{E}(y_{ij})$ : $\\mu_{ij}$\n* $\\text{E}(\\mathbf y_i)$ : $\\mathbf \\mu_{i}$\n* $\\text{Var}(\\mathbf y_i)$ : $\\text{Var}(\\mathbf y_i)$ is a variance-covariance matrix of the different measurement for the $i$ th subject\n  * for now, we do not care of the variance covariance of the different subjects because we assume that the measurements of different subjects are indpendent.\n$$\n\\begin{bmatrix}\n    \\text{Var}(y_{i1}) & \\text{Cov}( y_{i1}, y_{i2}) & \\dots & \\text{Cov}( y_{i1}, y_{in_i}) \\\\\n                               & \\text{Var}( y_{i2}) & \\dots & \\text{Cov}( y_{i2}, y_{in_i}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{in_i}) \n\\end{bmatrix}\n$$\n\n## OLS vs GLS\n\nOLS (Ordinary Least Squares) and GLS (Generalized Least Squares) are both methods of regression analysis used to model the relationship between a dependent variable and one or more independent variables. The main difference between OLS and GLS is in the assumptions about the errors in the model. \n\n### OLS \n\nIt assumes that the errors are homoscedastic and independent of each other. OLS is a simpler and more commonly used method, but it may not be appropriate for datasets with non-constant variances or autocorrelation in the errors.\n\n#### OLS Solutions\n\n$$\n\\begin{aligned}\n  \\hat{\\beta}&=\\min_\\beta||y-X\\beta||^2 \\\\ \n  &=(X^TX)^{-1}X^Ty\n\\end{aligned}\n$$\n\n$\\hat \\beta$ is unbiased because if we assume that the errors $\\epsilon \\sim N(0,\\sigma^2I)$, \n$E(\\hat \\beta)=\\text{E}((X^TX)^{-1}X^Ty)=(X^TX)^{-1}X^T\\text{E}(y)=(X^TX)^{-1}X^TX\\beta=\\beta$\n\n### GLS \n\nIt relaxes the assumptions of OLS and allows for heteroscedasticity and autocorrelation in the data. GLS is a more flexible method that can handle heteroscedasticity and autocorrelation in the data, but it requires more complex computations and may be computationally expensive for large datasets.\n\n#### Generalized Least Squares\n\nThe estimator $\\hat{\\beta}$, which is also known as the generalized least squares estimator, is given by $\\hat{\\beta}=(X^TWX)^{-1}X^TWy$, where $W$ is a positive definite weighting matrix.\n\n$$\n\\begin{aligned}\n  \\hat{\\beta}&=\\min_\\beta||y-X\\beta||^2 \\\\ \n  &=(X^TWX)^{-1}X^TWy\n\\end{aligned}\n$$\n\n\n##### Weighted Least Square\n\nThe weighted least squares (WLS) solution can be obtained by minimizing the sum of squared weighted residuals, given by:\n\n$$\n\\hat{\\beta}=\\min_\\beta||y-X\\beta||^2 \n$$\n\nThe WLS solution is given by:\n$$\n\\beta_{WLS} = (X^TWX)^{-1}X^TWy\n$$\n\nwhere $X$ is the design matrix, $W$ is a diagonal weight matrix with $w_i$ on the $i$ th diagonal element, and $y$ is the vector of responses. The predicted response $\\hat{y}$ can be obtained as $\\hat{y} = X\\hat{\\beta}$.\n\n:::{.callout-note}\nNote that the OLS solution is a special case of WLS when all weights are equal to 1.\n:::\n\n1. Define the weighted design matrix, $\\mathbf{W}$, as a diagonal matrix of weights, where each diagonal element corresponds to the weight for the corresponding observation.\n1. Define the weighted response vector, $\\mathbf{y}_{w}$, as a vector of the response values multiplied by the square root of the corresponding weight.\n1. Define the weighted parameter estimates, $\\hat{\\beta}_{w}$, as the solution to the weighted least squares problem:\n$$\n\\hat{\\beta}_w = \\operatorname*{arg\\,min}_{\\beta} (y_w - X\\beta)^T W (y_w - X\\beta)\n$$\nwhere $\\mathbf{X}$ is the design matrix of predictor variables.\n4. The estimated model can be obtained by substituting the weighted parameter estimates, $\\hat{\\beta}_{w}$, into the regression equation:\n\n$$\n\\hat{y}=\\mathbf{X}\\hat{\\beta}_w\n$$\n\nLet's start by defining the problem: we have a set of m data points, represented as a matrix $\\mathbf{X}$ with dimensions $m \\times n$, where $n$ is the number of independent variables. We also have a corresponding vector $\\mathbf y$ with $m$ elements, representing the dependent variable. We want to fit a linear function of the form $\\mathbf y = \\mathbf{X\\beta}+ \\mathbf \\epsilon$ to the data points, where $\\mathbf \\beta$ is a vector of coefficients to be determined and $\\mathbf \\epsilon$ is the residual error.\n\nTo perform weighted least squares, we define a weight matrix $\\mathbf W$ with dimensions $m \\times m$, where the diagonal elements $w_i$ are the weights for each data point $i$. Weights are typically chosen to be proportional to the inverse of the variance of the data point, so that data points with smaller variances are given more weight.\n\nUsing this weight matrix, the objective function for weighted least squares is defined as follows:\n$$\n\\begin{aligned}\n\\text{minimize } S &= (y - X\\beta)^TW(y - X\\beta) \\\\\n&= y^TWy - \\beta^TX^TWy - y^TWX\\beta + \\beta^TX^TWX\\beta \\\\\n\\frac{\\partial S}{\\partial \\beta} &= -2X^TWy + 2X^TWX\\beta = 0 \\\\\nX^TWX\\beta &= X^TWy \\\\\n\\beta &= (X^TWX)^{-1}X^TWy\n\\end{aligned}\n$$\n\n\nUnder the assumptions of the GLS model, the GLS estimator is unbiased, which means on average, the GLS estimator will estimate the true population parameters correctly.\nTo be specific, the GLS estimator is based on the assumption that the errors or residuals follow a multivariate normal distribution with a mean vector of zeros and a covariance matrix that is known up to a scalar factor. If this assumption holds, then the GLS estimator is the Best Linear Unbiased Estimator (BLUE) of the model parameters.\n\n:::{.callout-note}\nBeing a BLUE estimator implies that the GLS estimator has the smallest possible variance among all linear unbiased estimators. Therefore, under the GLS assumptions, the GLS estimator is not only unbiased but also efficient, meaning that it achieves the lowest possible variance of all unbiased estimators.\n\nHowever, If the assumptions are violated (e.g., the errors are not normally distributed or the covariance structure is misspecified), then the GLS estimator may not be unbiased or efficient.\n:::\n\n$$\n\\hat{\\beta}_{GLS} = (X^T V^{-1} X)^{-1} X^T V^{-1} Y\n$$\n\nwhere $V$ is the known covariance matrix of the errors or residuals.\n\nTo show that $\\hat{\\beta}_{GLS}$ is unbiased, we need to show that:\n\n$$\n\\operatorname{E}(\\hat{\\beta}_{GLS}) = \\beta\n$$\n\nwhere $\\beta$ is the true population parameter.\n\nUsing the linearity of expectation, we have:\n\n$$\n\\begin{align}\n\\operatorname{E}(\\hat{\\beta}_{GLS}) \n&= \\operatorname{E}((X^T W^{-1} X)^{-1} X^T W^{-1} Y)\\\\\n&= (X^T W^{-1} X)^{-1} X^T W^{-1} \\operatorname{E}(Y)\\\\\n&= (X^T W^{-1} X)^{-1} X^T W^{-1} X \\beta\\\\\n&= (X^T W^{-1} X)^{-1} X^T W^{-1} X (X^T X)^{-1} X^T X \\beta\\\\\n&= (X^T X)^{-1} X^T W^{-1} X (X^T X)^{-1} X^T X \\beta\\\\\n&= (X^T X)^{-1} X^T W^{-1} Y\\\\\n&= \\beta\n\\end{align}\n$$\n\nwhere the second-to-last equality follows from the fact that $X^T W^{-1} X$ is a symmetric positive definite matrix, and hence its inverse can be written as $(X^T X)^{-1}$. Therefore, we have shown that the GLS estimator is unbiased, i.e., its expected value is equal to the true population parameter.\n\n\n\n\n\n\n#### GLS Solution\n\nThe Generalized Least Squares (GLS) estimator is obtained by minimizing the weighted sum of squared residuals, where the weights are based on the estimated variance-covariance matrix of the errors.\n\nTo derive the GLS estimator, we start with the linear regression model:\n\n$Y = X\\beta + \\epsilon$\n\nwhere $Y$ is the response variable, $X$ is the design matrix of predictor variables, $\\beta$ is a vector of unknown coefficients, and $\\epsilon$ is a vector of errors or residuals.\n\nThe covariance matrix of the errors is denoted by $V = \\text{Cov}(\\epsilon)$, which is assumed to be known up to a scalar factor. Specifically, we assume that $V = \\sigma^2 W$, where $\\sigma^2$ is an unknown scalar factor and $W$ is a known positive definite matrix.\n\nThe GLS estimator of $\\beta$ is obtained by minimizing the weighted sum of squared residuals:\n\n$Q(\\beta) = (\\textbf{Y} - \\textbf{X}\\beta)^T V^{-1} (\\textbf{Y} - \\textbf{X}\\beta)$\n\nwhere $\\textbf{Y}$ and $\\textbf{X}$ are the vectors of observed responses and design matrix of predictors, respectively.\n\nTaking the derivative of $Q(\\beta)$ with respect to $\\beta$, and setting it to zero, we get:\n\n$\\frac{\\partial Q(\\beta)}{\\partial \\beta} = -2 \\textbf{X}^T V^{-1} (\\textbf{Y} - \\textbf{X}\\beta) = 0$\n\nSolving for $\\beta$, we obtain the GLS estimator:\n\n$\\hat{\\beta}_{GLS} = (\\textbf{X}^T V^{-1} \\textbf{X})^{-1} \\textbf{X}^T V^{-1} \\textbf{Y}$\n\nwhere $V^{-1} = \\frac{1}{\\sigma^2} W^{-1}$.\n\nNote that the GLS estimator reduces to the OLS estimator when $V$ is a scalar multiple of the identity matrix, i.e., when the errors have constant variance and are uncorrelated. In this case, $W = I$ and $V = \\sigma^2 I$, and the GLS estimator simplifies to:\n\n$\\hat{\\beta}_{OLS} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{Y}$\n\nwhich is the usual OLS estimator.\n\n\n\n\n\n## If  W=blkdiag(\\Sigma)^{-1} in longitudinal data analysis, why more efficient than OLS?\n\nThe expression \"blkdiag(\\Sigma)^{-1}\" means the inverse of a block diagonal matrix where each block is a covariance matrix denoted by Sigma (\\Sigma). Specifically, if we have n covariance matrices \\Sigma_1, \\Sigma_2, ..., \\Sigma_n, then the block diagonal matrix is given by:\n\nblkdiag(\\Sigma_1, \\Sigma_2, ..., \\Sigma_n) = \\begin{bmatrix} \\Sigma_1 & 0 & \\cdots & 0 \\ 0 & \\Sigma_2 & \\cdots & 0 \\ \\vdots & \\vdots & \\ddots & \\vdots \\ 0 & 0 & \\cdots & \\Sigma_n \\end{bmatrix}\n\nThe inverse of this block diagonal matrix can be computed by taking the inverse of each block matrix and placing them on the diagonal, resulting in the following expression:\n\n[blkdiag(\\Sigma_1, \\Sigma_2, ..., \\Sigma_n)]^{-1} = \\begin{bmatrix} \\Sigma_1^{-1} & 0 & \\cdots & 0 \\ 0 & \\Sigma_2^{-1} & \\cdots & 0 \\ \\vdots & \\vdots & \\ddots & \\vdots \\ 0 & 0 & \\cdots & \\Sigma_n^{-1} \\end{bmatrix}\n\nTherefore, blkdiag(\\Sigma)^{-1} is a block diagonal matrix where each block is the inverse of the corresponding covariance matrix, and it can be computed by taking the inverse of each block and placing them on the diagonal.\n\nREML for estimating \\Sigma variance covrainace matrix divides the data into twp parts: irrelevant to sigma and irrelevant to beta (REML estimate)\n\n\nIn longitudinal data analysis, it is common to use a generalized linear mixed model (GLMM) to account for the correlated nature of the data. The GLMM framework includes random effects to capture the individual-level variation and allows for the specification of a covariance structure to model the correlation between the repeated measurements within each individual. When fitting a GLMM, the covariance matrix of the random effects, denoted by \\Sigma, needs to be estimated.\n\nOne way to estimate \\Sigma is to use maximum likelihood estimation (MLE), which involves maximizing the likelihood function of the GLMM with respect to the unknown parameters, including \\Sigma. However, the MLE of \\Sigma can be inefficient when the number of repeated measurements per individual is small or the correlation between the repeated measurements is weak.\n\nTo improve the efficiency of the MLE of \\Sigma, a weighted likelihood method can be used, where the likelihood function is multiplied by a weight matrix that depends on the estimated covariance matrix. Specifically, the weight matrix is given by W = blkdiag(\\Sigma)^{-1}, where blkdiag(\\Sigma) is the block diagonal matrix of the estimated covariance matrix \\Sigma. The inverse of blkdiag(\\Sigma) is taken because it is a positive definite matrix and its inverse exists.\n\nBy weighting the likelihood function with W, the resulting weighted likelihood estimator (WLE) of \\Sigma is more efficient than the MLE because it incorporates additional information about the covariance structure of the data. The WLE of \\Sigma is then used in the GLMM to estimate the other parameters, such as the fixed effects.\n\nIn summary, using W = blkdiag(\\Sigma)^{-1} as a weight matrix in the GLMM framework can improve the efficiency of the MLE of the covariance matrix \\Sigma and result in more accurate estimates of the other parameters in the model, making it more efficient than the OLS estimator, which assumes independence between the observations.\n\n### Family and Longitudinal Data\n\n\nIn longitudinal data analysis with family data, one common approach to estimating the regression coefficients is to use a linear mixed-effects model, also known as a multilevel model or a hierarchical model. The linear mixed-effects model can handle the within-subject correlation due to repeated measurements over time and the within-family correlation due to shared genetic or environmental factors among family members.\n\nThe linear mixed-effects model assumes that the outcome variable Y is a function of the fixed effects X and the random effects b, which can account for the within-subject and within-family correlation. The model can be written as:\n\nY = Xβ + Zb + ε\n\nwhere β is the vector of fixed effects coefficients, b is the vector of random effects coefficients, Z is the design matrix for the random effects, and ε is the error term.\n\nTo estimate the fixed effects coefficients β, one can use maximum likelihood estimation (MLE) or restricted maximum likelihood estimation (REML) methods. The MLE estimates the variance components for both the random effects and the error term, while the REML estimates the variance components for only the random effects and adjusts for the bias in the likelihood function.\n\nTo fit the linear mixed-effects model, one can use software packages such as R, SAS, or Stata, which have functions for fitting linear mixed-effects models with repeated measurements and random effects. In R, for example, one can use the lme4 package to fit the linear mixed-effects model using the lmer() function. The output of the function includes the estimated fixed effects coefficients β and the estimated variance components for the random effects and the error term.\n\nOverall, estimating the fixed effects coefficients β of family data with repeated measurements in longitudinal data analysis requires fitting a linear mixed-effects model that accounts for the within-subject and within-family correlation, and the choice of model depends on the research question and assumptions of the data.\n\nIn longitudinal data analysis with repeated measurements and correlated subjects, the variance-covariance matrix of the responses can be modeled using a mixed-effects model or a generalized estimating equation (GEE) model.\n\nIn the mixed-effects model, the variance-covariance matrix of the responses is decomposed into two components: the within-subject covariance and the between-subject covariance. The within-subject covariance accounts for the correlation between the repeated measurements within the same subject, while the between-subject covariance accounts for the correlation between subjects. The mixed-effects model allows for the inclusion of both fixed effects and random effects, which can model the mean and the variance structure of the responses.\n\nIn the GEE model, the variance-covariance matrix of the responses is modeled using a working correlation matrix, which specifies the correlation structure between the observations. The GEE model allows for the inclusion of fixed effects, but not random effects, and estimates the population-averaged mean and the variance structure of the responses.\n\nTo design the variance-covariance matrix of the responses in a mixed-effects model or a GEE model, one needs to specify the correlation structure between the repeated measurements within the same subject and between subjects. Common correlation structures for the within-subject covariance include the autoregressive (AR), the exchangeable, and the unstructured covariance structures, while common correlation structures for the between-subject covariance include the independent and the compound symmetry covariance structures.\n\nThe choice of correlation structure depends on the research question and assumptions of the data. For example, the AR(1) correlation structure assumes that the correlation between two measurements decreases as the time lag between them increases, while the exchangeable correlation structure assumes that all measurements within the same subject are equally correlated. The choice of correlation structure can be evaluated using statistical criteria such as the Akaike information criterion (AIC) or the Bayesian information criterion (BIC).\n\nOverall, designing the variance-covariance matrix of the responses in longitudinal data analysis with repeated measurements and correlated subjects requires specifying the correlation structure between the repeated measurements within the same subject and between subjects, and the choice of model depends on the research question and assumptions of the data.\n\n\n\n```{r}\n#| eval: false\n\n# this file contains code for linear marginal models for longitudinal data\n# We test different covariance patterns and show how to fit model with WLS and REML\n# By Gen Li, 1/1/2018\n# also check lme4\n\n\nlibrary(nlme)\nlibrary(tidyverse)\nopposites <- read.table(\"https://stats.idre.ucla.edu/stat/r/examples/alda/data/opposites_pp.txt\",header=TRUE,sep=\",\")\nhead(opposites)\n\n# spaghetti plot\np = ggplot(opposites, aes(time, opp, group=id)) + geom_line()\nprint(p)\n\n\n# Fit different cov model with REML\n###################################################\n# unstructured covariance for marginal model\nunstruct <- gls(opp~time*ccog,opposites, correlation=corSymm(form = ~ 1 |id),  weights=varIdent(form = ~ 1| wave),method=\"REML\")\n\n# corSymm(form = ~ 1 |id) : the same covariance across different measurement, same correlation matrix for different subjects\n# check ?gls ?corClasses ?corSymm\nsummary(unstruct) # focus on corr, var, (weight)\nunstruct$modelStruct$corStruct # corr\nunstruct$modelStruct$varStruct # variance:weight\nunstruct$sigma # standard deviation\ncov2cor(unstruct$varBeta)\n\n\n# compound symmetry\ncomsym <- gls(opp~time,opposites, correlation=corCompSymm(form = ~ 1|id),   weights=varIdent(form = ~ 1| wave), method=\"REML\")\nsummary(comsym)\ncorMatrix(comsym$modelStruct$corStruct)[[1]]\n\n\n\n# AR(1)\nauto1 <- gls(opp~time ,opposites, correlation=corAR1(form = ~ 1 |id), method=\"REML\")\nsummary(auto1)\ncorMatrix(auto1$modelStruct$corStruct)[[1]]\n\n\n\n```\n\n\n::: {.callout-tip}\n\n### Longitudinal Study vs Cross-Sectional Study Example\n\nA cross-sectional study found that older people smoke more.\n\nPossible explanations:\n\n* People tend to smoke more when they get older.\n* Older people grew up in an environment where the harm of smoking was less widely accepted. In other words, when older people were younger, smoking was more socially acceptable and its harmful effects were not well-known or well-publicized. As a result, they may have started smoking earlier in life and developed a stronger habit or addiction. This explanation implies that younger people today may be less likely to smoke because they are growing up in an environment where smoking is less socially acceptable and the risks are more widely known.\n\nLDA can distinguish the effect due to aging (i.e., changes over time within subject) from cohort effects (i.e., difference between subjects at baseline). Cross-sectional study cannot.\n:::\n\n## Advantages of Longitudinal Data Analysis\n\n* Each subject can serve as his/her own control. Influence of genetic make-up, environmental exposures, and maybe unmeasured characteristics tend to persist over time.\n    * in certain types of studies or experiments, individuals can be used as their own comparison group. This means that the same person is tested or measured at different points in time, and the differences observed can be attributed to changes over time rather than to differences between individuals. \n    * For example, in a study looking at the effect of a new medication on blood pressure, each participant's blood pressure before and after taking the medication would be compared to determine if there was a change. By using the same participant as their own control, the effects of genetic factors, environmental exposures, and other individual differences that might influence blood pressure would be minimized.\n    * However, even when using this approach, some individual differences that are not directly measured or controlled for may persist over time and influence the results. These could include factors such as diet, stress levels, or other lifestyle habits that could impact the outcome being measured.\n* Distinguish the degree of variation in $Y$ across time within a subject from the variation in $Y$ between subjects. With repeated values, one can borrow strength across time for the person of interest as well as across people.\n    * when you have repeated measurements of a variable (Y) for the same person over time, you can use the information from those repeated measurements to better estimate the true value of Y for that person at any given time point. This is known as borrowing strength across time.\n    * Additionally, you can also use the information from the repeated measurements of Y across different people to better estimate the true value of Y for a particular time point across the population. This is known as borrowing strength across people.\n    * By doing both, you can distinguish the degree of variation in Y across time within a subject (i.e., how much Y varies for a particular person over time) from the variation in Y between subjects (i.e., how much Y varies between different people at a particular time point).\n* Increased power, by repeated measurements. The repeated measurements from the same subject are rarely perfectly correlated. Hence, longitudinal studies are more powerful than cross-sectional studies.\n    * Longitudinal studies are more powerful than cross-sectional studies because they allow researchers to directly model and account for the within-subject correlation among repeated measurements. In other words, longitudinal studies take advantage of the fact that individuals are their own controls by measuring outcomes at multiple time points, which allows for a more accurate estimation of the true effect of an exposure or intervention.\n    * In contrast, cross-sectional studies only measure outcomes at a single time point, which makes it difficult to distinguish between within-subject variability and between-subject variability. In a cross-sectional study, any observed differences between groups may be due to differences in the underlying populations, or due to differences in the timing of the outcome measurement, or both. \n    * Furthermore, longitudinal studies can also provide information on the rate of change in the outcome over time, which can be important in understanding disease progression, treatment effects, or the impact of other time-dependent factors.\n* Therefore, because longitudinal studies allow for a more accurate estimation of the true effect of an exposure or intervention and provide more information about disease progression, they are generally considered more powerful than cross-sectional studies.\n\n### Specialty of LDA\n\nLDA requires special statistical methods because the set of observations on one subject tends to be inter-correlated.\n\n[Copied from Diggle et al. (2002, page 2).](./childhood%20readbility.PNG)\n\n* Consider the example, variation of readability of child as they get aged.\n    * Assume this is a longitudinal study with two measurements per child at two age or time points.\n    * The two measurements per subject may be highly correlated.\n    * If we use cross-sectional methods to analyze the data, we may not be able to distinguish changes over time within individual and difference among people in their baseline levels.\n        * Only plot (a) is from cross sectional study. Not using connected lines might mislead the time trend within subjects.\n* In general, repeated observations $y_{i1}, \\dots , y_{in_i}$ for subject $i$ are likely to be correlated, so the independence assumption is violated.\n* The standard regression methods (ignoring correlation) may lead to\n    * Incorrect inference\n        * the violation of the independence assumption means that the errors in the model are correlated across observations, and this correlation can bias the estimated coefficients.\n        * The errors in the model are correlated across observations when there is some form of dependence or clustering in the data. This means that the error term in one observation is related to the error terms in other observations, either through some underlying factor or because of the way the data is collected.\n        * When the errors are correlated across observations, the estimated coefficients from standard regression methods may be biased because the regression model assumes that the errors are independent. \n        * The bias in the estimated coefficients can arise in several ways:\n            * The standard errors of the estimated coefficients will be too small, which can lead to overconfidence in the results.\n            * The estimated coefficients may not reflect the true relationships between the dependent variable and the independent variables, as the correlation between the errors can distort the estimates.\n            * The estimates of the standard errors will be biased, leading to incorrect inference in hypothesis testing and confidence interval construction.\n        * To sum up, failing to account for the correlation between errors can lead to incorrect and imprecise estimates of the coefficients and standard errors in a regression model.\n    * Inefficient estimates of $\\beta$\n        * the estimates are less precise than they could be if the correlation between observations were taken into account. \n        * The standard errors of the estimates will be too large, which means that confidence intervals will be wider and hypothesis tests will have less power.\n    * Oversight of important correlation structure\n        * the standard regression methods may miss important patterns of correlation in the data that could be used to improve the accuracy and precision of the estimates. \n        * For example, if there is a time trend in the data that is not accounted for, the standard errors of the estimates may be too large, and the estimates may not capture the true effect of the independent variables. \n        * Accounting for the correlation structure in the data can lead to more accurate and precise estimates, and can also help identify interesting patterns and relationships that might otherwise be missed.\n\n:::\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n::: {#English .tab-pane .fade role=\"tabpanel\" aria-labelledby=\"English-tab\"}\n\n:::\n\n</div>\n\n\n# Go to Project Content List\n\n[Project Content List](./docs/projects/index.qmd)\n\n# Go to Blog Content List\n\n[Blog Content List](./docs/blog/posts/content_list.qmd)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"3_wls.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"LDA (3) - Weight Least Square & REML","subtitle":"Overview","description":"template\n","categories":["Statistics"],"author":"Kwangmin Kim","date":"03/25/2023","draft":true,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}