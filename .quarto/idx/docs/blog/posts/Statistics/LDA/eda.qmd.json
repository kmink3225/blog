{"title":"LDA - EDA","markdown":{"yaml":{"title":"LDA - EDA","subtitle":"Exploratory Data Analysis","description":"template\n","categories":["Statistics"],"author":"Kwangmin Kim","date":"04/23/2023","format":{"html":{"page-layout":"full","code-fold":true,"toc":true,"number-sections":true}},"execute":{"echo":false,"warning":false,"eval":false},"draft":true},"headingText":"Mean Function Estimation Using Smoothing Methods","containsRefs":false,"markdown":"\n```{r}\nlibrary(tidyverse)\nlibrary(lme4)\nrm(list=ls())\n#unzip(\"C:/Users/kmkim/Desktop/projects/data/LDA.zip\",list=T)\nspruce_data<-read.table(\"C:/Users/kmkim/Desktop/projects/data/LDA/spruce_data.txt\")\nmilk_data<-read.table(\"C:/Users/kmkim/Desktop/projects/data/LDA/milk_modified.tsv\")\nnames(milk_data)<-c('trt','id','time','protein')\n```\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n::: {#Korean .tab-pane .fade .show .active role=\"tabpanel\" aria-labelledby=\"Korean-tab\"}\n\nIn EDA of longitudinal data, mean function, covariance structure, and variogram are estimated and visualized to capture the characteristics of the data and to support the statistical inference results.\n\n\nTo show a trend of a response variable, smoothing methods are used to estimate the trend or mean funtion.\n\n* 시간에 따라 변화하는 반응 변수의 평균 패턴 인식\n* 시간에 따라 변화하는 개인별 패턴 인식\n* 그룹간의 반응 변수와의 관계 인식\n* 이상점 또는 특이치를 판독 \n\n### Recognition of Average Patterns in Response Variables Changing over Time\n\n#### Spaghetti Plot\n\nSpaghetti Plot: individual trends of a response variable\n\n```{r}\nggplot(data=milk_data,aes(x=time,y=protein,group=id))+\nge  om_line()\n\nggplot(data=milk_data,aes(x=time,y=protein,group=id,col=factor(trt)))+\n  geom_line()+\nfa  cet_wrap(.~trt,ncol=1)\n```\n\n\n#### Spaghetti Plot with Smoothing\n\nSpaghetti plots with mean functions are used to make them more informative.\n\n$$\nY(t)=\\mu(t)+\\epsilon\n$$\n\n##### Kernel Estimation\n\nKernel estimation is a nonparametric method used to estimate the underlying probability density function of a random variable. In kernel estimation, the density estimate is calculated at each point by placing a kernel function around that point, and the values of all kernel functions are added up to estimate the density.\n\n\nIn the case of estimating the conditional mean function $\\mu(t)=\\operatorname{E}(Y|T=t)$, we can use kernel estimation with a smoothing kernel function to estimate the mean at each point $t$. The kernel function is used to assign weights to the data points near each point $t$ based on their distance from $t$, and the weighted average of the $Y$ values for these nearby data points gives the estimated value of $\\mu(t)$.\n\nt시점을 중심으로 window에 포함된 반응변수 값에 대해 적절한 가중치를 적용하여 mean function을 추정.\n\n$$\n\\mu(t)=\\operatorname{E}(Y|T=t)=\\int y f(y|t)dy=\\int y \\frac{f(t.y)}{f_{T}(t)}dy\n$$\n\n\n$$\n\\begin{aligned}\n\\hat{\\mu}(t) &= \\frac{\\sum_{i=1}^n K\\left(\\frac{t-t_i}{h}\\right) y_i}{\\sum_{i=1}^n K\\left(\\frac{t-t_i}{h}\\right)} \\\\\n&=\\frac{\\sum\\limits_{i=1}^n y_i K_h(t-t_i)}{\\sum\\limits_{i=1}^n K_h(t-t_i)} \\\\\n&=\\frac{\\sum\\limits_{i=1}^n y_iw(t,t_i,h)}{\\sum\\limits_{i=1}^n w(t,t_i,h)} \\\\\n&=\\hat{\\mu}_{NW}(t)\n\\end{aligned}\n$$\n\nwhere \n$\\hat{\\mu}(t)$ is the estimate of the mean function at time point $t$, \n$y_i$ is the response variable for the $i$ th observation, \n$t_i$ is the time point for the $i$ th observation, \n$K_h$ is the kernel function with bandwidth parameter $h$, \n$n$ is the number of observations, \n$\\hat{\\mu}_{NW}(t)$ is the Nadarian-Watson estimator, and \n$w(t,t_i,h)=\\frac{K(t-t_i)}{h}$.\n\nThe smaller the bandwith parameter $h$, the more wiggly the smoothing line .\n\n:::{.callout-note}\nA kernel is a mathematical function that weights data points in a certain way to estimate a target function, such as a pdf or a regression function. The idea is to assign weights to neighboring data points based on their distance to the target point, with the weights determined by the kernel function. The kernel function, $K(\\cdot)$ is typically a symmetric, non-negative function that integrates to 1, such as the Gaussian or Epanechnikov kernel.\n:::\n\nThe Gassuian kernel is most commonly chosen:\n\n**Gaussian kernel**\n$$\nK(u) = \\frac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\frac{u^2}{2}\\right)}\n$$\n\n**Epanechnikov kernel**\n\n$$\nK(u) = \\begin{cases}\n\\dfrac{3}{4}(1-u^2), & \\text{if } |u|<1 \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\n\n\n```{r}\n#| eval: false\n\n# Nadaraya-Watson estimator using Gaussian kernel\ngaussian_kernel_density <- function(x, x_i, h) {\n    dnorm((x - x_i) / h) / h\n}\n\ngaussian_nadaraya_watson <- function(x, y, t, h) {\n    numerator <- sum(y * gaussian_kernel_density(t, x, h))\n    denominator <- sum(gaussian_kernel_density(t, x, h))\n    return(numerator / denominator)\n}\n\n# Nadaraya-Watson estimator using Epanechnikov kernel\nEpanechnikov_kernel_density <- function(x, x_i, h) {\n    ifelse(abs((x - x_i) / h) > 1, 0, 0.75 * (1 - ((x - x_i) / h) ^ 2) / h)\n}\n\nEpanechnikov_nadaraya_watson <- function(x, y, t, h) {\n    numerator <- sum(y * Epanechnikov_kernel_density(t, x, h))\n    denominator <- sum(Epanechnikov_kernel_density(t, x, h))\n    return(numerator / denominator)\n}\n\n# long_milk_data<-milk_data%>%\n#   mutate(gaussian_wt_h0.1=sapply(milk_data$time, \n#   function(t) gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.1)),\n#   Epanechnikov_wt_h0.1=sapply(milk_data$time, \n#   function(t) Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.1)),\n#   gaussian_wt_h0.3=sapply(milk_data$time, \n#   function(t) gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.3)),\n#   Epanechnikov_wt_h0.3=sapply(milk_data$time, \n#   function(t) Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.3)),\n#   gaussian_wt_h0.6=sapply(milk_data$time, \n#   function(t) gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.6)),\n#   Epanechnikov_wt_h0.6=sapply(milk_data$time, \n#   function(t) Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.6)),\n#   gaussian_wt_h0.9=sapply(milk_data$time, \n#   function(t) gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.9)),\n#   Epanechnikov_wt_h0.9=sapply(milk_data$time, \n#   function(t) Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.9)))%>%\n#   gather(key=kernels,value=smoothed,protein:Epanechnikov_wt_h0.9)\n\nbandwidths <- c(0.1, 0.3, 0.6, 0.9)\nkernels <- c(\"gaussian\", \"Epanechnikov\")\n\nsmoothed_data <- map_dfc(bandwidths, function(h) {\n    map_dfc(kernels, function(kernel) {\n        col_name <- paste0(kernel, \"_wt_h\", h)\n        smoothed_values <- sapply(milk_data$time, function(t) {\n            if (kernel == \"gaussian\") {\n                gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h)\n            } else {\n                Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h)\n            }\n        })\n        tibble(!!col_name := smoothed_values)\n    })\n})\n\nmilk_data <- bind_cols(milk_data, smoothed_data)\n\n# Generate fake data\nset.seed(123)\nn <- 100\nx <- seq(0, 1, length.out = n)\ny <- sin(2*pi*x) + rnorm(n, sd = 0.2)\n\n# Estimate mean function using Gaussian Nadaraya-Watson estimator\nt_grid <- seq(0, 1, length.out = 100)\nh <- 0.1\ngaussian_mu_hat_0.1 <- sapply(t_grid, function(t) gaussian_nadaraya_watson(x, y, t, 0.1))\ngaussian_mu_hat_0.3 <- sapply(t_grid, function(t) gaussian_nadaraya_watson(x, y, t, 0.3))\ngaussian_mu_hat_0.6 <- sapply(t_grid, function(t) gaussian_nadaraya_watson(x, y, t, 0.6))\ngaussian_mu_hat_0.9 <- sapply(t_grid, function(t) gaussian_nadaraya_watson(x, y, t, 0.9))\n\n# Plot results\nplot(x, y, main = \"Gaussian Nadaraya-Watson estimator\", xlab = \"x\", ylab = \"y\", ylim = c(-2, 2))\nlines(t_grid, gaussian_mu_hat_0.1 , col = \"red\", lwd = 2,lty=2)\nlines(t_grid, gaussian_mu_hat_0.3 , col = \"green\", lwd = 2,lty=3)\nlines(t_grid, gaussian_mu_hat_0.6 , col = \"blue\", lwd = 2,lty=4)\nlines(t_grid, gaussian_mu_hat_0.9 , col = \"purple\", lwd = 2,lty=5)\n\n\n```\n\n##### Tuning Hyperparameter `h`\n\nTo tune the hyperparameter `h`, we can use and estimate PSE (average predicted squared error) reflecting both bias and variance using cross-validation. \n\n$$\n\\operatorname{PSE}(h)=\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname{E}(Y_i^{*}-\\hat{\\mu}(t,h))^2\n$$\n\n$Y_i^{*}$ typically denotes a transformed version of the response variable $Y_i$. It is used to make the distribution of $Y_i^{*}$ more symmetric or more normal, which can be helpful in some statistical analyses.\n\n$$\n\\operatorname{CV}(h)=\\sum_{i=1}^{n}\\operatorname{E}(y_i-\\hat{\\mu}^{-i}(t,h))^2\n$$\n\nwhere $\\hat{\\mu}^{-i}$ is the mean estimator estimated excluding the ith observation.\n\n```{r}\n#| eval: false\n\nset.seed(123)\n\n# Generate fake data\nn <- 100\nx <- seq(0, 10, length.out = n)\ny <- rnorm(n, mean = sin(x))\nmilk_data <- data.frame(time = x, protein = y)\n\n# PSE function\nPSE <- function(h, x, y, K) {\n    n <- length(y)\n    Y_hat_star <- rep(0, n)\n    for (i in 1:n) {\n        Y_hat_star[i] <- sum(K((x - x[i]) / h) * y) / sum(K((x - x[i]) / h))\n    }\n    return(mean((Y_hat_star - y)^2))\n}\n\n# CV function\nCV <- function(h, x, y, K) {\n    n <- length(y)\n    Y_hat_minus_i <- rep(0, n)\n    for (i in 1:n) {\n        Y_hat_minus_i[i] <- sum(K((x[-i] - x[i]) / h) * y[-i]) / sum(K((x[-i] - x[i]) / h))\n    }\n    return(sum((y - Y_hat_minus_i)^2))\n}\n\n# Bandwidth tuning using PSE\ntune_bandwidth_PSE <- function(x, y, K) {\n    n <- length(y)\n    h_seq <- seq(0.01, 1, length = 100)\n    PSE_seq <- sapply(h_seq, PSE, x = x, y = y, K = K)\n    h_opt <- h_seq[which.min(PSE_seq)]\n    return(h_opt)\n}\n\n# Bandwidth tuning using CV\ntune_bandwidth_CV <- function(x, y, K) {\n    n <- length(y)\n    h_seq <- seq(0.01, 1, length = 100)\n    CV_seq <- sapply(h_seq, CV, x = x, y = y, K = K)\n    h_opt <- h_seq[which.min(CV_seq)]\n    return(h_opt)\n}\n\nh_opt_PSE <- tune_bandwidth_PSE(milk_data$time, milk_data$protein, dnorm)\n\nh_opt_CV <- tune_bandwidth_CV(milk_data$time, milk_data$protein, function(x) ifelse(abs(x) > 1, 0, 0.75 * (1 - x^2)))\n\n# Plotting CV sequence against bandwidth values\nCV_plot <- function(x, y, K) {\n    n <- length(y)\n    h_seq <- seq(0.01, 1, length = 100)\n    CV_seq <- sapply(h_seq, CV, x = x, y = y, K = K)\n    df <- data.frame(h = h_seq, CV = CV_seq)\n    ggplot(df, aes(x = h, y = CV)) +\n        geom_line() +\n        geom_vline(xintercept = tune_bandwidth_CV(x, y, K), linetype = \"dashed\", color = \"red\") +\n        labs(x = \"Bandwidth\", y = \"Cross-validation score\", title = \"Bandwidth tuning using CV\")\n}\n\nCV_plot(milk_data$time, milk_data$protein, function(x) ifelse(abs(x) > 1, 0, 0.75 * (1 - x^2)))\n\n# Bandwidth tuning using CV, returning CV sequence for all bandwidth values\ntune_bandwidth_CV_table <- function(x, y, K) {\n    n <- length(y)\n    h_seq <- seq(0.01, 1, length = 100)\n    CV_seq <- sapply(h_seq, CV, x = x, y = y, K = K)\n    df <- data.frame(h = h_seq, CV = CV_seq)\n    return(df)\n}\n\nCV_table <- tune_bandwidth_CV_table(milk_data$time, milk_data$protein, function(x) ifelse(abs(x) > 1, 0, 0.75 * (1 - x^2)))\n\nknitr::kable(CV_table)\n```\n\n\n\n###### LOESS\n\nLOESS (locally estimated scatterplot smoothing or LOcal regrESSion) is a nonparametric regression method used for modeling the relationship between a response variable $Y$ and a predictor variable $T$. The goal of LOESS is to estimate the conditional mean function $\\mu(t) = \\mathbb{E}(Y|T = t)$ using a weighted polynomial regression model.\n\nLOESS involves fitting a separate polynomial regression model to the data in each local neighborhood of the predictor variable $T$. The size of the local neighborhood is controlled by a tuning parameter called the smoothing parameter. For each observation $i$, the model is fit using a weighted least squares method, with weights given by a kernel function that assigns higher weights to observations closer to $i$ in the predictor variable $T$. The polynomial order of the regression model is chosen by the user, with a typical choice being a second-order polynomial.\n\nThe loess method first selects a subset of data points near a target point $t$ using a kernel function. A weighted linear regression model is then fit to the data points in the subset, giving more weight to points closer to the target point $t$. The degree of smoothing is controlled by a bandwidth parameter, which determines the size of the subset of data points used in the regression.\n\nThe estimated mean function $\\hat{\\mu}(t)$ is obtained by repeating this process at a large number of target points along the range of $t$ values. The final smooth function is obtained by connecting these estimated mean values.\n\nLoess is particularly useful for estimating smooth nonlinear functions and can handle heteroscedasticity (non-constant variance) and nonlinearity in the data. It is commonly used in applications such as time series analysis, epidemiology, and environmental science.\n\n:::{.callout-tip}\n**weighted least square**\n\nThe weighted least squares (WLS) solution can be obtained by minimizing the sum of squared weighted residuals, given by:\n\n$$\n\\operatorname{minimize} \\sum_{i=1}^{n} w_i(y_i - f(x_i))^2\n$$\n\nThe WLS solution is given by:\n$$\n\\beta_{WLS} = (X^TWX)^{-1}X^TWy\n\n$$\n\nwhere $X$ is the design matrix, $W$ is a diagonal weight matrix with $w_i$ on the $i$th diagonal element, and $y$ is the vector of responses. The predicted response $\\hat{y}$ can be obtained as $\\hat{y} = X\\hat{\\beta}$.\n\nNote that the OLS solution is a special case of WLS when all weights are equal to 1.\n\n\n1. Define the weighted design matrix, $\\mathbf{W}$, as a diagonal matrix of weights, where each diagonal element corresponds to the weight for the corresponding observation.\n1. Define the weighted response vector, $\\mathbf{y}_{w}$, as a vector of the response values multiplied by the square root of the corresponding weight.\n1. Define the weighted parameter estimates, $\\hat{\\beta}_{w}$, as the solution to the weighted least squares problem:\n$$\n\\hat{\\beta}_w = \\operatorname*{arg\\,min}_{\\beta} (y_w - X\\beta)^T W (y_w - X\\beta)\n$$\nwhere $\\mathbf{X}$ is the design matrix of predictor variables.\n4. The estimated model can be obtained by substituting the weighted parameter estimates, $\\hat{\\beta}_{w}$, into the regression equation:\n\n$$\n\\hat{y}=\\mathbf{X}\\hat{\\beta}_w\n$$\n\nLet's start by defining the problem: we have a set of m data points, represented as a matrix X with dimensions m x p, where p is the number of independent variables. We also have a corresponding vector y with m elements, representing the dependent variable. We want to fit a linear function of the form y = Xβ + ε to the data points, where β is a vector of coefficients to be determined and ε is the residual error.\n\nTo perform weighted least squares, we define a weight matrix W with dimensions m x m, where the diagonal elements w(i) are the weights for each data point i. Weights are typically chosen to be proportional to the inverse of the variance of the data point, so that data points with smaller variances are given more weight.\n\nUsing this weight matrix, the objective function for weighted least squares is defined as follows:\n$$\n\\begin{aligned}\n\\text{minimize } S &= (y - X\\beta)^TW(y - X\\beta) \\\\\n&= y^TWy - \\beta^TX^TWy - y^TWX\\beta + \\beta^TX^TWX\\beta \\\\\n\\frac{\\partial S}{\\partial \\beta} &= -2X^TWy + 2X^TWX\\beta = 0 \\\\\nX^TWX\\beta &= X^TWy \\\\\n\\beta &= (X^TWX)^{-1}X^TWy\n\\end{aligned}\n\n$$\n\n:::\nthe LOESS model can be expressed as:\n$$\n\\hat{\\mu}(t_i)=\\sum_{j=1}^{n}w_{ij}(t_i)y_j\n$$\n\nwhere $\\hat{\\mu}(t_i)$ is the estimated mean response at predictor value $t_i$, $y_j$ is the response value at predictor value $t_j$, and $w_{ij}(t_i)$ is the weight assigned to the $j$th observation in the local neighborhood of $t_i$. The weights are defined by a kernel function $K$, such that:\n\n$$\nw_{ij}(t_i)=K\\left(\\frac{t_i-t_j}{h}\\right)\n$$\n\nwhere $h$ is the smoothing parameter, controlling the size of the local neighborhood. A common choice for the kernel function is the tri-cube kernel:\n\n$$\nK(x) = \\begin{cases} \n         \\left(1 - |x|^3\\right)^3, & \\text{if } |x| < 1 \\\\\n         0, & \\text{otherwise}\n      \\end{cases}\n$$\n\n```{r}\n#| eval: false\n\ntri_cube_kernel <- function(x) {\n    sapply(x, function(x_i) {\n        ifelse(abs(x_i) <= 1, (1 - abs(x_i)^3)^3, 0)\n    })\n}\n\n# Define LOESS function\nloess <- function(x, y, span, degree) {\n    n <- length(x)\n    weights <- matrix(0, n, n)\n    for (i in 1:n) {\n        weights[i,] <- tri_cube_kernel((x - x[i]) / span)\n    }\n    fit <- lm(y ~ poly(x, degree), weights = weights)\n    return(fit)\n}\n\n# Generate fake data\nset.seed(123)\nn <- 100\nx <- seq(0, 10, length.out = n)\ny <- rnorm(n, mean = sin(x) + 0.1 * x)\ndf <- data.frame(x = x, y = y)\n\n# Fit LOESS\nfit <- loess(x = df$x, y = df$y, span = 0.2, degree = 2)\n# Predict on new data\nnew_x <- seq(0, 10, length.out = 1000)\npred <- predict(fit, newdata = data.frame(x = new_x))\n\n# Plot results\nlibrary(ggplot2)\nggplot(data = df, aes(x = x, y = y)) +\n    geom_point() +\n    geom_line(aes(x = new_x, y = pred), color = \"red\", size = 1) +\n    theme_minimal()\n\n\n# Fit loess model\nloess_fit <- loess(y ~ x, data = sim_data, span = 0.5)\n\n# Plot data and fitted curve\nplot(sim_data, pch = 16, col = \"blue\", main = \"Simulated Data with Loess Fit\")\nlines(sim_data$x, loess_fit$fitted, col = \"red\", lwd = 2)\n```\n\n\n```{r}\n\n\nggplot(milk_data, aes(x = time, y = protein)) +\n  geom_point()+\nge  om_smooth(method = \"loess\", formula = \"y~x\", se = FALSE, span = 0.1,color='red')+\n  geom_smooth(method = \"loess\", formula = \"y~x\", se = FALSE, span = 0.3,color='green')+\nge  om_smooth(method = \"loess\", formula = \"y~x\", se = FALSE, span = 0.6,color='blue')+\n ge om_smooth(method = \"loess\", formula = \"y~x\", se = FALSE, span = 0.9,color='purple')+\nsc  ale_color_manual(values=c('red','green','blue','purple'),\n    labels = c(\"Span = 0.1\", \"Span = 0.3\", \"Span = 0.6\", \"Span = 0.9\"))\n```    \n  \n```{r}\nggplot(data=milk_data,aes(x=time,y=protein,group=id))+\ngeom_line()+\ngeom_smooth(aes(group=1),method='loess',formula=y~x)+\n st at_summary(aes(x = 19, yintercept = ..y.., group = 1), fun = \"median\", color = \"red\", geom = \"hline\")\n  \n gg plot(data=milk_data,aes(x=time,y=protein,group=id,col=factor(trt)))+\ngeom_line()+\ngeom_smooth(aes(group=1),method='loess',formula=y~x,color='black')+\n  stat_summary(aes(x = 19, yintercept = ..y.., group = 1), fun = \"median\", color = \"red\", geom = \"hline\")+\nfa  cet_wrap(.~trt,ncol=3)\n `` `\n  \n### Recognition of Individual Patterns Changing over Time\n\n### Recognition of Relationships with Response Variables between Groups\n\n### Recognition of Outliers or Anomaly Data \n\n\n:::\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n::: {#English .tab-pane .fade role=\"tabpanel\" aria-labelledby=\"English-tab\"}\n\n:::\n\n\n</div>\n\n# Go to Project Content List\n\n[Project Content List](./docs/projects/index.qmd)\n\n# Go to Blog Content List\n\n[Blog Content List](../../content_list.qmd)","srcMarkdownNoYaml":"\n```{r}\nlibrary(tidyverse)\nlibrary(lme4)\nrm(list=ls())\n#unzip(\"C:/Users/kmkim/Desktop/projects/data/LDA.zip\",list=T)\nspruce_data<-read.table(\"C:/Users/kmkim/Desktop/projects/data/LDA/spruce_data.txt\")\nmilk_data<-read.table(\"C:/Users/kmkim/Desktop/projects/data/LDA/milk_modified.tsv\")\nnames(milk_data)<-c('trt','id','time','protein')\n```\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n::: {#Korean .tab-pane .fade .show .active role=\"tabpanel\" aria-labelledby=\"Korean-tab\"}\n\nIn EDA of longitudinal data, mean function, covariance structure, and variogram are estimated and visualized to capture the characteristics of the data and to support the statistical inference results.\n\n## Mean Function Estimation Using Smoothing Methods\n\nTo show a trend of a response variable, smoothing methods are used to estimate the trend or mean funtion.\n\n* 시간에 따라 변화하는 반응 변수의 평균 패턴 인식\n* 시간에 따라 변화하는 개인별 패턴 인식\n* 그룹간의 반응 변수와의 관계 인식\n* 이상점 또는 특이치를 판독 \n\n### Recognition of Average Patterns in Response Variables Changing over Time\n\n#### Spaghetti Plot\n\nSpaghetti Plot: individual trends of a response variable\n\n```{r}\nggplot(data=milk_data,aes(x=time,y=protein,group=id))+\nge  om_line()\n\nggplot(data=milk_data,aes(x=time,y=protein,group=id,col=factor(trt)))+\n  geom_line()+\nfa  cet_wrap(.~trt,ncol=1)\n```\n\n\n#### Spaghetti Plot with Smoothing\n\nSpaghetti plots with mean functions are used to make them more informative.\n\n$$\nY(t)=\\mu(t)+\\epsilon\n$$\n\n##### Kernel Estimation\n\nKernel estimation is a nonparametric method used to estimate the underlying probability density function of a random variable. In kernel estimation, the density estimate is calculated at each point by placing a kernel function around that point, and the values of all kernel functions are added up to estimate the density.\n\n\nIn the case of estimating the conditional mean function $\\mu(t)=\\operatorname{E}(Y|T=t)$, we can use kernel estimation with a smoothing kernel function to estimate the mean at each point $t$. The kernel function is used to assign weights to the data points near each point $t$ based on their distance from $t$, and the weighted average of the $Y$ values for these nearby data points gives the estimated value of $\\mu(t)$.\n\nt시점을 중심으로 window에 포함된 반응변수 값에 대해 적절한 가중치를 적용하여 mean function을 추정.\n\n$$\n\\mu(t)=\\operatorname{E}(Y|T=t)=\\int y f(y|t)dy=\\int y \\frac{f(t.y)}{f_{T}(t)}dy\n$$\n\n\n$$\n\\begin{aligned}\n\\hat{\\mu}(t) &= \\frac{\\sum_{i=1}^n K\\left(\\frac{t-t_i}{h}\\right) y_i}{\\sum_{i=1}^n K\\left(\\frac{t-t_i}{h}\\right)} \\\\\n&=\\frac{\\sum\\limits_{i=1}^n y_i K_h(t-t_i)}{\\sum\\limits_{i=1}^n K_h(t-t_i)} \\\\\n&=\\frac{\\sum\\limits_{i=1}^n y_iw(t,t_i,h)}{\\sum\\limits_{i=1}^n w(t,t_i,h)} \\\\\n&=\\hat{\\mu}_{NW}(t)\n\\end{aligned}\n$$\n\nwhere \n$\\hat{\\mu}(t)$ is the estimate of the mean function at time point $t$, \n$y_i$ is the response variable for the $i$ th observation, \n$t_i$ is the time point for the $i$ th observation, \n$K_h$ is the kernel function with bandwidth parameter $h$, \n$n$ is the number of observations, \n$\\hat{\\mu}_{NW}(t)$ is the Nadarian-Watson estimator, and \n$w(t,t_i,h)=\\frac{K(t-t_i)}{h}$.\n\nThe smaller the bandwith parameter $h$, the more wiggly the smoothing line .\n\n:::{.callout-note}\nA kernel is a mathematical function that weights data points in a certain way to estimate a target function, such as a pdf or a regression function. The idea is to assign weights to neighboring data points based on their distance to the target point, with the weights determined by the kernel function. The kernel function, $K(\\cdot)$ is typically a symmetric, non-negative function that integrates to 1, such as the Gaussian or Epanechnikov kernel.\n:::\n\nThe Gassuian kernel is most commonly chosen:\n\n**Gaussian kernel**\n$$\nK(u) = \\frac{1}{\\sqrt{2\\pi}}\\exp{\\left(-\\frac{u^2}{2}\\right)}\n$$\n\n**Epanechnikov kernel**\n\n$$\nK(u) = \\begin{cases}\n\\dfrac{3}{4}(1-u^2), & \\text{if } |u|<1 \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\n\n\n```{r}\n#| eval: false\n\n# Nadaraya-Watson estimator using Gaussian kernel\ngaussian_kernel_density <- function(x, x_i, h) {\n    dnorm((x - x_i) / h) / h\n}\n\ngaussian_nadaraya_watson <- function(x, y, t, h) {\n    numerator <- sum(y * gaussian_kernel_density(t, x, h))\n    denominator <- sum(gaussian_kernel_density(t, x, h))\n    return(numerator / denominator)\n}\n\n# Nadaraya-Watson estimator using Epanechnikov kernel\nEpanechnikov_kernel_density <- function(x, x_i, h) {\n    ifelse(abs((x - x_i) / h) > 1, 0, 0.75 * (1 - ((x - x_i) / h) ^ 2) / h)\n}\n\nEpanechnikov_nadaraya_watson <- function(x, y, t, h) {\n    numerator <- sum(y * Epanechnikov_kernel_density(t, x, h))\n    denominator <- sum(Epanechnikov_kernel_density(t, x, h))\n    return(numerator / denominator)\n}\n\n# long_milk_data<-milk_data%>%\n#   mutate(gaussian_wt_h0.1=sapply(milk_data$time, \n#   function(t) gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.1)),\n#   Epanechnikov_wt_h0.1=sapply(milk_data$time, \n#   function(t) Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.1)),\n#   gaussian_wt_h0.3=sapply(milk_data$time, \n#   function(t) gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.3)),\n#   Epanechnikov_wt_h0.3=sapply(milk_data$time, \n#   function(t) Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.3)),\n#   gaussian_wt_h0.6=sapply(milk_data$time, \n#   function(t) gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.6)),\n#   Epanechnikov_wt_h0.6=sapply(milk_data$time, \n#   function(t) Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.6)),\n#   gaussian_wt_h0.9=sapply(milk_data$time, \n#   function(t) gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.9)),\n#   Epanechnikov_wt_h0.9=sapply(milk_data$time, \n#   function(t) Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.9)))%>%\n#   gather(key=kernels,value=smoothed,protein:Epanechnikov_wt_h0.9)\n\nbandwidths <- c(0.1, 0.3, 0.6, 0.9)\nkernels <- c(\"gaussian\", \"Epanechnikov\")\n\nsmoothed_data <- map_dfc(bandwidths, function(h) {\n    map_dfc(kernels, function(kernel) {\n        col_name <- paste0(kernel, \"_wt_h\", h)\n        smoothed_values <- sapply(milk_data$time, function(t) {\n            if (kernel == \"gaussian\") {\n                gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h)\n            } else {\n                Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h)\n            }\n        })\n        tibble(!!col_name := smoothed_values)\n    })\n})\n\nmilk_data <- bind_cols(milk_data, smoothed_data)\n\n# Generate fake data\nset.seed(123)\nn <- 100\nx <- seq(0, 1, length.out = n)\ny <- sin(2*pi*x) + rnorm(n, sd = 0.2)\n\n# Estimate mean function using Gaussian Nadaraya-Watson estimator\nt_grid <- seq(0, 1, length.out = 100)\nh <- 0.1\ngaussian_mu_hat_0.1 <- sapply(t_grid, function(t) gaussian_nadaraya_watson(x, y, t, 0.1))\ngaussian_mu_hat_0.3 <- sapply(t_grid, function(t) gaussian_nadaraya_watson(x, y, t, 0.3))\ngaussian_mu_hat_0.6 <- sapply(t_grid, function(t) gaussian_nadaraya_watson(x, y, t, 0.6))\ngaussian_mu_hat_0.9 <- sapply(t_grid, function(t) gaussian_nadaraya_watson(x, y, t, 0.9))\n\n# Plot results\nplot(x, y, main = \"Gaussian Nadaraya-Watson estimator\", xlab = \"x\", ylab = \"y\", ylim = c(-2, 2))\nlines(t_grid, gaussian_mu_hat_0.1 , col = \"red\", lwd = 2,lty=2)\nlines(t_grid, gaussian_mu_hat_0.3 , col = \"green\", lwd = 2,lty=3)\nlines(t_grid, gaussian_mu_hat_0.6 , col = \"blue\", lwd = 2,lty=4)\nlines(t_grid, gaussian_mu_hat_0.9 , col = \"purple\", lwd = 2,lty=5)\n\n\n```\n\n##### Tuning Hyperparameter `h`\n\nTo tune the hyperparameter `h`, we can use and estimate PSE (average predicted squared error) reflecting both bias and variance using cross-validation. \n\n$$\n\\operatorname{PSE}(h)=\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname{E}(Y_i^{*}-\\hat{\\mu}(t,h))^2\n$$\n\n$Y_i^{*}$ typically denotes a transformed version of the response variable $Y_i$. It is used to make the distribution of $Y_i^{*}$ more symmetric or more normal, which can be helpful in some statistical analyses.\n\n$$\n\\operatorname{CV}(h)=\\sum_{i=1}^{n}\\operatorname{E}(y_i-\\hat{\\mu}^{-i}(t,h))^2\n$$\n\nwhere $\\hat{\\mu}^{-i}$ is the mean estimator estimated excluding the ith observation.\n\n```{r}\n#| eval: false\n\nset.seed(123)\n\n# Generate fake data\nn <- 100\nx <- seq(0, 10, length.out = n)\ny <- rnorm(n, mean = sin(x))\nmilk_data <- data.frame(time = x, protein = y)\n\n# PSE function\nPSE <- function(h, x, y, K) {\n    n <- length(y)\n    Y_hat_star <- rep(0, n)\n    for (i in 1:n) {\n        Y_hat_star[i] <- sum(K((x - x[i]) / h) * y) / sum(K((x - x[i]) / h))\n    }\n    return(mean((Y_hat_star - y)^2))\n}\n\n# CV function\nCV <- function(h, x, y, K) {\n    n <- length(y)\n    Y_hat_minus_i <- rep(0, n)\n    for (i in 1:n) {\n        Y_hat_minus_i[i] <- sum(K((x[-i] - x[i]) / h) * y[-i]) / sum(K((x[-i] - x[i]) / h))\n    }\n    return(sum((y - Y_hat_minus_i)^2))\n}\n\n# Bandwidth tuning using PSE\ntune_bandwidth_PSE <- function(x, y, K) {\n    n <- length(y)\n    h_seq <- seq(0.01, 1, length = 100)\n    PSE_seq <- sapply(h_seq, PSE, x = x, y = y, K = K)\n    h_opt <- h_seq[which.min(PSE_seq)]\n    return(h_opt)\n}\n\n# Bandwidth tuning using CV\ntune_bandwidth_CV <- function(x, y, K) {\n    n <- length(y)\n    h_seq <- seq(0.01, 1, length = 100)\n    CV_seq <- sapply(h_seq, CV, x = x, y = y, K = K)\n    h_opt <- h_seq[which.min(CV_seq)]\n    return(h_opt)\n}\n\nh_opt_PSE <- tune_bandwidth_PSE(milk_data$time, milk_data$protein, dnorm)\n\nh_opt_CV <- tune_bandwidth_CV(milk_data$time, milk_data$protein, function(x) ifelse(abs(x) > 1, 0, 0.75 * (1 - x^2)))\n\n# Plotting CV sequence against bandwidth values\nCV_plot <- function(x, y, K) {\n    n <- length(y)\n    h_seq <- seq(0.01, 1, length = 100)\n    CV_seq <- sapply(h_seq, CV, x = x, y = y, K = K)\n    df <- data.frame(h = h_seq, CV = CV_seq)\n    ggplot(df, aes(x = h, y = CV)) +\n        geom_line() +\n        geom_vline(xintercept = tune_bandwidth_CV(x, y, K), linetype = \"dashed\", color = \"red\") +\n        labs(x = \"Bandwidth\", y = \"Cross-validation score\", title = \"Bandwidth tuning using CV\")\n}\n\nCV_plot(milk_data$time, milk_data$protein, function(x) ifelse(abs(x) > 1, 0, 0.75 * (1 - x^2)))\n\n# Bandwidth tuning using CV, returning CV sequence for all bandwidth values\ntune_bandwidth_CV_table <- function(x, y, K) {\n    n <- length(y)\n    h_seq <- seq(0.01, 1, length = 100)\n    CV_seq <- sapply(h_seq, CV, x = x, y = y, K = K)\n    df <- data.frame(h = h_seq, CV = CV_seq)\n    return(df)\n}\n\nCV_table <- tune_bandwidth_CV_table(milk_data$time, milk_data$protein, function(x) ifelse(abs(x) > 1, 0, 0.75 * (1 - x^2)))\n\nknitr::kable(CV_table)\n```\n\n\n\n###### LOESS\n\nLOESS (locally estimated scatterplot smoothing or LOcal regrESSion) is a nonparametric regression method used for modeling the relationship between a response variable $Y$ and a predictor variable $T$. The goal of LOESS is to estimate the conditional mean function $\\mu(t) = \\mathbb{E}(Y|T = t)$ using a weighted polynomial regression model.\n\nLOESS involves fitting a separate polynomial regression model to the data in each local neighborhood of the predictor variable $T$. The size of the local neighborhood is controlled by a tuning parameter called the smoothing parameter. For each observation $i$, the model is fit using a weighted least squares method, with weights given by a kernel function that assigns higher weights to observations closer to $i$ in the predictor variable $T$. The polynomial order of the regression model is chosen by the user, with a typical choice being a second-order polynomial.\n\nThe loess method first selects a subset of data points near a target point $t$ using a kernel function. A weighted linear regression model is then fit to the data points in the subset, giving more weight to points closer to the target point $t$. The degree of smoothing is controlled by a bandwidth parameter, which determines the size of the subset of data points used in the regression.\n\nThe estimated mean function $\\hat{\\mu}(t)$ is obtained by repeating this process at a large number of target points along the range of $t$ values. The final smooth function is obtained by connecting these estimated mean values.\n\nLoess is particularly useful for estimating smooth nonlinear functions and can handle heteroscedasticity (non-constant variance) and nonlinearity in the data. It is commonly used in applications such as time series analysis, epidemiology, and environmental science.\n\n:::{.callout-tip}\n**weighted least square**\n\nThe weighted least squares (WLS) solution can be obtained by minimizing the sum of squared weighted residuals, given by:\n\n$$\n\\operatorname{minimize} \\sum_{i=1}^{n} w_i(y_i - f(x_i))^2\n$$\n\nThe WLS solution is given by:\n$$\n\\beta_{WLS} = (X^TWX)^{-1}X^TWy\n\n$$\n\nwhere $X$ is the design matrix, $W$ is a diagonal weight matrix with $w_i$ on the $i$th diagonal element, and $y$ is the vector of responses. The predicted response $\\hat{y}$ can be obtained as $\\hat{y} = X\\hat{\\beta}$.\n\nNote that the OLS solution is a special case of WLS when all weights are equal to 1.\n\n\n1. Define the weighted design matrix, $\\mathbf{W}$, as a diagonal matrix of weights, where each diagonal element corresponds to the weight for the corresponding observation.\n1. Define the weighted response vector, $\\mathbf{y}_{w}$, as a vector of the response values multiplied by the square root of the corresponding weight.\n1. Define the weighted parameter estimates, $\\hat{\\beta}_{w}$, as the solution to the weighted least squares problem:\n$$\n\\hat{\\beta}_w = \\operatorname*{arg\\,min}_{\\beta} (y_w - X\\beta)^T W (y_w - X\\beta)\n$$\nwhere $\\mathbf{X}$ is the design matrix of predictor variables.\n4. The estimated model can be obtained by substituting the weighted parameter estimates, $\\hat{\\beta}_{w}$, into the regression equation:\n\n$$\n\\hat{y}=\\mathbf{X}\\hat{\\beta}_w\n$$\n\nLet's start by defining the problem: we have a set of m data points, represented as a matrix X with dimensions m x p, where p is the number of independent variables. We also have a corresponding vector y with m elements, representing the dependent variable. We want to fit a linear function of the form y = Xβ + ε to the data points, where β is a vector of coefficients to be determined and ε is the residual error.\n\nTo perform weighted least squares, we define a weight matrix W with dimensions m x m, where the diagonal elements w(i) are the weights for each data point i. Weights are typically chosen to be proportional to the inverse of the variance of the data point, so that data points with smaller variances are given more weight.\n\nUsing this weight matrix, the objective function for weighted least squares is defined as follows:\n$$\n\\begin{aligned}\n\\text{minimize } S &= (y - X\\beta)^TW(y - X\\beta) \\\\\n&= y^TWy - \\beta^TX^TWy - y^TWX\\beta + \\beta^TX^TWX\\beta \\\\\n\\frac{\\partial S}{\\partial \\beta} &= -2X^TWy + 2X^TWX\\beta = 0 \\\\\nX^TWX\\beta &= X^TWy \\\\\n\\beta &= (X^TWX)^{-1}X^TWy\n\\end{aligned}\n\n$$\n\n:::\nthe LOESS model can be expressed as:\n$$\n\\hat{\\mu}(t_i)=\\sum_{j=1}^{n}w_{ij}(t_i)y_j\n$$\n\nwhere $\\hat{\\mu}(t_i)$ is the estimated mean response at predictor value $t_i$, $y_j$ is the response value at predictor value $t_j$, and $w_{ij}(t_i)$ is the weight assigned to the $j$th observation in the local neighborhood of $t_i$. The weights are defined by a kernel function $K$, such that:\n\n$$\nw_{ij}(t_i)=K\\left(\\frac{t_i-t_j}{h}\\right)\n$$\n\nwhere $h$ is the smoothing parameter, controlling the size of the local neighborhood. A common choice for the kernel function is the tri-cube kernel:\n\n$$\nK(x) = \\begin{cases} \n         \\left(1 - |x|^3\\right)^3, & \\text{if } |x| < 1 \\\\\n         0, & \\text{otherwise}\n      \\end{cases}\n$$\n\n```{r}\n#| eval: false\n\ntri_cube_kernel <- function(x) {\n    sapply(x, function(x_i) {\n        ifelse(abs(x_i) <= 1, (1 - abs(x_i)^3)^3, 0)\n    })\n}\n\n# Define LOESS function\nloess <- function(x, y, span, degree) {\n    n <- length(x)\n    weights <- matrix(0, n, n)\n    for (i in 1:n) {\n        weights[i,] <- tri_cube_kernel((x - x[i]) / span)\n    }\n    fit <- lm(y ~ poly(x, degree), weights = weights)\n    return(fit)\n}\n\n# Generate fake data\nset.seed(123)\nn <- 100\nx <- seq(0, 10, length.out = n)\ny <- rnorm(n, mean = sin(x) + 0.1 * x)\ndf <- data.frame(x = x, y = y)\n\n# Fit LOESS\nfit <- loess(x = df$x, y = df$y, span = 0.2, degree = 2)\n# Predict on new data\nnew_x <- seq(0, 10, length.out = 1000)\npred <- predict(fit, newdata = data.frame(x = new_x))\n\n# Plot results\nlibrary(ggplot2)\nggplot(data = df, aes(x = x, y = y)) +\n    geom_point() +\n    geom_line(aes(x = new_x, y = pred), color = \"red\", size = 1) +\n    theme_minimal()\n\n\n# Fit loess model\nloess_fit <- loess(y ~ x, data = sim_data, span = 0.5)\n\n# Plot data and fitted curve\nplot(sim_data, pch = 16, col = \"blue\", main = \"Simulated Data with Loess Fit\")\nlines(sim_data$x, loess_fit$fitted, col = \"red\", lwd = 2)\n```\n\n\n```{r}\n\n\nggplot(milk_data, aes(x = time, y = protein)) +\n  geom_point()+\nge  om_smooth(method = \"loess\", formula = \"y~x\", se = FALSE, span = 0.1,color='red')+\n  geom_smooth(method = \"loess\", formula = \"y~x\", se = FALSE, span = 0.3,color='green')+\nge  om_smooth(method = \"loess\", formula = \"y~x\", se = FALSE, span = 0.6,color='blue')+\n ge om_smooth(method = \"loess\", formula = \"y~x\", se = FALSE, span = 0.9,color='purple')+\nsc  ale_color_manual(values=c('red','green','blue','purple'),\n    labels = c(\"Span = 0.1\", \"Span = 0.3\", \"Span = 0.6\", \"Span = 0.9\"))\n```    \n  \n```{r}\nggplot(data=milk_data,aes(x=time,y=protein,group=id))+\ngeom_line()+\ngeom_smooth(aes(group=1),method='loess',formula=y~x)+\n st at_summary(aes(x = 19, yintercept = ..y.., group = 1), fun = \"median\", color = \"red\", geom = \"hline\")\n  \n gg plot(data=milk_data,aes(x=time,y=protein,group=id,col=factor(trt)))+\ngeom_line()+\ngeom_smooth(aes(group=1),method='loess',formula=y~x,color='black')+\n  stat_summary(aes(x = 19, yintercept = ..y.., group = 1), fun = \"median\", color = \"red\", geom = \"hline\")+\nfa  cet_wrap(.~trt,ncol=3)\n `` `\n  \n### Recognition of Individual Patterns Changing over Time\n\n### Recognition of Relationships with Response Variables between Groups\n\n### Recognition of Outliers or Anomaly Data \n\n\n:::\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n::: {#English .tab-pane .fade role=\"tabpanel\" aria-labelledby=\"English-tab\"}\n\n:::\n\n\n</div>\n\n# Go to Project Content List\n\n[Project Content List](./docs/projects/index.qmd)\n\n# Go to Blog Content List\n\n[Blog Content List](../../content_list.qmd)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../../js.html","../../../signup.html"],"output-file":"eda.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../../theme.scss"],"dark":["cosmo","../../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"LDA - EDA","subtitle":"Exploratory Data Analysis","description":"template\n","categories":["Statistics"],"author":"Kwangmin Kim","date":"04/23/2023","draft":true,"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}