{"title":"Machine Learning","markdown":{"yaml":{"title":"Machine Learning","subtitle":"Comprehensive Guide to Machine Learning Algorithms and Methods","description":"From foundations to advanced methods - A systematic path to mastering\nsupervised learning, unsupervised learning, and practical ML engineering\n","categories":["Machine Learning"],"author":"Kwangmin Kim","date":"11/10/2024"},"headingText":"Learning Path","containsRefs":false,"markdown":"\n\n\n### Phase 1: Foundations (2-3개월, 수학적 기초와 ML 핵심 개념 확립)\n\nMathematical Prerequisites\n  - Linear Algebra ⭐\n  - Calculus and Optimization ⭐\n  - Probability and Statistics ⭐\n\nML Fundamentals\n  - ML 개념과 학습 유형\n  - Bias-Variance Tradeoff ⭐\n  - Data Preprocessing\n  - Feature Engineering ⭐\n\nBasic Algorithms\n  - Linear Regression ⭐\n  - Logistic Regression ⭐\n  - Decision Trees ⭐\n\n### Historical Methods\n\n* 2022-12-09, [PCA (Principal Component Analysis)](./2022-12-09-PCA.qmd)\n* 2023-02-03, [PyTorch Introduction](./2023-02-03_pytorch_introduction.qmd)\n* 2023-02-03, [TensorFlow Introduction](./2023-02-03_tf_introduction.qmd)\n* 2023-02-06, [Naive Bayes](./2023-02-06_naive_bayes.qmd)\n\n### Phase 2: Core Algorithms (2-3개월, 주요 알고리즘)\n\nClassification & Regression\n  - SVM ⭐\n  - KNN\n  - Naive Bayes\n  - Regularization ⭐\n\nEnsemble Methods\n  - Random Forest ⭐\n  - Gradient Boosting ⭐\n  - XGBoost ⭐\n  - LightGBM ⭐\n\nUnsupervised Learning\n  - K-Means ⭐\n  - Hierarchical Clustering\n  - PCA ⭐\n  - DBSCAN\n\n\n### Phase 3: Advanced Methods (2-3개월, 고급 기법)\n\nModel Evaluation\n  - Cross-Validation ⭐\n  - Performance Metrics ⭐\n  - Hyperparameter Tuning ⭐\n\nAdvanced Topics\n  - Imbalanced Learning ⭐\n  - Gaussian Processes\n  - Model Interpretability (SHAP) ⭐\n\nSpecial Applications\n  - Time Series\n  - NLP Basics\n  - Recommender Systems\n\n### Phase 4: ML Engineering (2개월, 프로덕션 배포와 운영)\n\nProduction ML\n  - Model Serving\n  - Model Monitoring ⭐\n  - A/B Testing for Models\n\nScalability\n  - Distributed Training\n  - Model Compression\n  - Feature Stores\n\n## Implementation Notes\n\n### Python Libraries\n\nInterpretability\n- import shap ⭐\n- import lime\n\nProduction\n- import mlflow ⭐\n- import bentoml\n\n\n### Recommended Project Structure\n\nml_project/\n├── data/\n│   ├── raw/\n│   ├── processed/\n│   └── features/\n├── notebooks/\n│   ├── eda.ipynb\n│   ├── modeling.ipynb\n│   └── evaluation.ipynb\n├── src/\n│   ├── preprocessing/\n│   ├── features/\n│   ├── models/\n│   └── evaluation/\n├── tests/\n├── configs/\n└── models/\n\n\n## Foundations\n\n### Mathematical Prerequisites\n* [Linear Algebra for ML (ML을 위한 선형대수)](./foundations/linear_algebra.qmd)\n  * 1111-11-11, [Vectors and Matrices (벡터와 행렬)](./foundations/vectors_matrices.qmd)\n  * 1111-11-11, [Matrix Operations (행렬 연산)](./foundations/matrix_operations.qmd)\n  * 1111-11-11, [Eigenvalues and Eigenvectors (고유값과 고유벡터)](./foundations/eigenvalues.qmd)\n  * 1111-11-11, [Singular Value Decomposition (특이값 분해)](./foundations/svd.qmd)\n\n* [Calculus and Optimization (미적분과 최적화)](./foundations/calculus.qmd)\n  * 1111-11-11, [Derivatives and Gradients (도함수와 기울기)](./foundations/derivatives.qmd)\n  * 1111-11-11, [Chain Rule and Backpropagation (연쇄법칙과 역전파)](./foundations/chain_rule.qmd)\n  * 1111-11-11, [Convex Optimization (볼록 최적화)](./foundations/convex_optimization.qmd)\n  * 1111-11-11, [Gradient Descent Methods (경사하강법)](./foundations/gradient_descent.qmd)\n\n* [Probability and Statistics (확률과 통계)](./foundations/probability.qmd)\n  * 1111-11-11, [Probability Distributions (확률분포)](./foundations/distributions.qmd)\n  * 1111-11-11, [Maximum Likelihood Estimation (최대우도추정)](./foundations/mle.qmd)\n  * 1111-11-11, [Bayesian Inference (베이지안 추론)](./foundations/bayesian_inference.qmd)\n  * 1111-11-11, [Statistical Testing (통계적 검정)](./foundations/statistical_testing.qmd)\n\n### ML Fundamentals\n* [Introduction to Machine Learning (머신러닝 소개)](./fundamentals/intro.qmd)\n  * 1111-11-11, [What is Machine Learning? (머신러닝이란?)](./fundamentals/what_is_ml.qmd)\n  * 1111-11-11, [Types of Learning (학습의 종류)](./fundamentals/learning_types.qmd)\n    * Supervised Learning (지도학습)\n    * Unsupervised Learning (비지도학습)\n    * Semi-supervised Learning (준지도학습)\n    * Reinforcement Learning (강화학습)\n  * 1111-11-11, [Bias-Variance Tradeoff (편향-분산 트레이드오프)](./fundamentals/bias_variance.qmd) ⭐\n  * 1111-11-11, [Overfitting and Underfitting (과적합과 과소적합)](./fundamentals/overfitting.qmd)\n  * 1111-11-11, [No Free Lunch Theorem (공짜 점심 정리)](./fundamentals/no_free_lunch.qmd)\n\n* [Data Preprocessing (데이터 전처리)](./preprocessing/index.qmd)\n  * 1111-11-11, [Data Cleaning (데이터 정제)](./preprocessing/cleaning.qmd)\n    * Missing Value Handling (결측치 처리)\n    * Outlier Detection (이상치 탐지)\n    * Data Quality Assessment (데이터 품질 평가)\n  * 1111-11-11, [Feature Scaling (특성 스케일링)](./preprocessing/scaling.qmd)\n    * Standardization (표준화)\n    * Normalization (정규화)\n    * Robust Scaling (로버스트 스케일링)\n  * 1111-11-11, [Encoding Categorical Variables (범주형 변수 인코딩)](./preprocessing/encoding.qmd)\n    * One-Hot Encoding\n    * Label Encoding\n    * Target Encoding\n    * Embeddings\n\n* [Feature Engineering (특성 공학)](./feature_engineering/index.qmd) ⭐\n  * 1111-11-11, [Feature Creation (특성 생성)](./feature_engineering/creation.qmd)\n    * Polynomial Features (다항 특성)\n    * Interaction Features (상호작용 특성)\n    * Domain-specific Features (도메인 특화 특성)\n  * 1111-11-11, [Feature Selection (특성 선택)](./feature_engineering/selection.qmd)\n    * Filter Methods (필터 방법)\n    * Wrapper Methods (래퍼 방법)\n    * Embedded Methods (임베디드 방법)\n  * 1111-11-11, [Dimensionality Reduction (차원 축소)](./feature_engineering/dimensionality_reduction.qmd)\n    * Principal Component Analysis (주성분 분석)\n    * Linear Discriminant Analysis (선형판별분석)\n    * t-SNE and UMAP\n\n\n\n## Supervised Learning\n\n### Regression (회귀)\n* [Regression Analysis (회귀 분석)](./regression/index.qmd)\n  * 1111-11-11, [Linear Regression (선형 회귀)](./regression/linear_regression.qmd) ⭐\n    * Ordinary Least Squares (최소자승법)\n    * Geometric Interpretation (기하학적 해석)\n    * Assumptions and Diagnostics (가정과 진단)\n    * Coefficient Interpretation (계수 해석)\n  * 1111-11-11, [Regularization (정규화)](./regression/regularization.qmd) ⭐\n    * Ridge Regression (L2 정규화)\n    * Lasso Regression (L1 정규화)\n    * Elastic Net (탄력망)\n    * Regularization Path (정규화 경로)\n  * 1111-11-11, [Polynomial Regression (다항 회귀)](./regression/polynomial.qmd)\n  * 1111-11-11, [Generalized Linear Models (일반화 선형모형)](./regression/glm.qmd)\n    * Logistic Regression (로지스틱 회귀)\n    * Poisson Regression (포아송 회귀)\n    * Link Functions (연결함수)\n\n### Classification (분류)\n* [Classification Methods (분류 방법)](./classification/index.qmd)\n  * 1111-11-11, [Logistic Regression (로지스틱 회귀)](./classification/logistic_regression.qmd) ⭐\n    * Binary Classification (이진 분류)\n    * Multinomial Logistic Regression (다항 로지스틱)\n    * Odds Ratio Interpretation (오즈비 해석)\n  * 1111-11-11, [Naive Bayes (나이브 베이즈)](./classification/naive_bayes.qmd)\n    * Gaussian Naive Bayes\n    * Multinomial Naive Bayes\n    * Bernoulli Naive Bayes\n  * 1111-11-11, [K-Nearest Neighbors (K-최근접 이웃)](./classification/knn.qmd)\n    * Distance Metrics (거리 척도)\n    * Curse of Dimensionality (차원의 저주)\n    * Weighted KNN (가중 KNN)\n  * 1111-11-11, [Support Vector Machines (서포트 벡터 머신)](./classification/svm.qmd) ⭐\n    * Linear SVM (선형 SVM)\n    * Kernel Trick (커널 트릭)\n    * Soft Margin (소프트 마진)\n    * Multi-class SVM (다중클래스 SVM)\n  * 1111-11-11, [Decision Trees (의사결정 나무)](./classification/decision_trees.qmd) ⭐\n    * Splitting Criteria (분할 기준)\n      * Gini Impurity (지니 불순도)\n      * Information Gain (정보 이득)\n      * Gain Ratio (이득 비율)\n    * Pruning (가지치기)\n    * CART Algorithm (CART 알고리즘)\n\n### Ensemble Methods (앙상블 방법)\n* [Ensemble Learning (앙상블 학습)](./ensemble/index.qmd) ⭐\n  * 1111-11-11, [Bagging Methods (배깅 방법)](./ensemble/bagging.qmd)\n    * [Random Forest (랜덤 포레스트)](./ensemble/random_forest.qmd) ⭐\n      * Bootstrap Aggregating\n      * Feature Randomness (특성 무작위성)\n      * Out-of-Bag Error (OOB 오차)\n      * Feature Importance (특성 중요도)\n    * Extra Trees (극단적 무작위 나무)\n  * 1111-11-11, [Boosting Methods (부스팅 방법)](./ensemble/boosting.qmd) ⭐\n    * [AdaBoost (적응적 부스팅)](./ensemble/adaboost.qmd)\n      * Weak Learner Weighting (약학습기 가중치)\n      * Sample Reweighting (표본 재가중)\n    * [Gradient Boosting (경사 부스팅)](./ensemble/gradient_boosting.qmd) ⭐\n      * Gradient Boosting Machines (GBM)\n      * Loss Function Optimization (손실함수 최적화)\n      * Learning Rate and Trees (학습률과 나무 수)\n    * [XGBoost](./ensemble/xgboost.qmd) ⭐\n      * Regularization Terms (정규화 항)\n      * System Optimization (시스템 최적화)\n      * Handling Missing Values (결측치 처리)\n    * [LightGBM](./ensemble/lightgbm.qmd) ⭐\n      * Histogram-based Algorithm (히스토그램 기반)\n      * Leaf-wise Growth (리프 단위 성장)\n      * Categorical Feature Support (범주형 특성 지원)\n    * [CatBoost](./ensemble/catboost.qmd)\n      * Ordered Boosting (순서 부스팅)\n      * Native Categorical Handling (네이티브 범주형 처리)\n  * 1111-11-11, [Stacking (스태킹)](./ensemble/stacking.qmd)\n    * Meta-learner (메타학습기)\n    * Blending (블렌딩)\n  * 1111-11-11, [Voting Classifiers (투표 분류기)](./ensemble/voting.qmd)\n    * Hard Voting (하드 투표)\n    * Soft Voting (소프트 투표)\n\n\n## Unsupervised Learning\n\n### Clustering (군집화)\n* [Clustering Methods (군집화 방법)](./clustering/index.qmd)\n  * 1111-11-11, [K-Means Clustering (K-평균 군집화)](./clustering/kmeans.qmd) ⭐\n    * Lloyd's Algorithm (로이드 알고리즘)\n    * Elbow Method (엘보우 방법)\n    * K-Means++ Initialization (K-평균++ 초기화)\n    * Mini-Batch K-Means (미니배치 K-평균)\n  * 1111-11-11, [Hierarchical Clustering (계층적 군집화)](./clustering/hierarchical.qmd)\n    * Agglomerative Clustering (병합 군집화)\n    * Divisive Clustering (분할 군집화)\n    * Linkage Methods (연결 방법)\n    * Dendrogram (덴드로그램)\n  * 1111-11-11, [DBSCAN (밀도 기반 군집화)](./clustering/dbscan.qmd)\n    * Density-Based Clustering (밀도 기반)\n    * Core Points and Border Points (핵심점과 경계점)\n    * Handling Noise (잡음 처리)\n  * 1111-11-11, [Gaussian Mixture Models (가우시안 혼합 모형)](./clustering/gmm.qmd)\n    * Expectation-Maximization (EM 알고리즘)\n    * Soft Clustering (소프트 군집화)\n    * Model Selection (모형 선택)\n  * 1111-11-11, [Other Clustering Methods (기타 군집화 방법)](./clustering/others.qmd)\n    * OPTICS\n    * Mean Shift\n    * Spectral Clustering (스펙트럼 군집화)\n\n### Dimensionality Reduction (차원 축소)\n* [Dimensionality Reduction Methods (차원 축소 방법)](./dim_reduction/index.qmd)\n  * 1111-11-11, [Principal Component Analysis (주성분 분석)](./dim_reduction/pca.qmd) ⭐\n    * Eigenvalue Decomposition (고유값 분해)\n    * Variance Explained (설명된 분산)\n    * Scree Plot (스크리 플롯)\n    * Kernel PCA (커널 PCA)\n  * 1111-11-11, [Linear Discriminant Analysis (선형판별분석)](./dim_reduction/lda.qmd)\n    * Fisher's Linear Discriminant (피셔의 선형판별)\n    * Between-class vs. Within-class Variance\n  * 1111-11-11, [t-SNE (t-분포 확률적 임베딩)](./dim_reduction/tsne.qmd)\n    * Perplexity Parameter (복잡도 매개변수)\n    * KL Divergence Optimization (KL 발산 최적화)\n    * Visualization Considerations (시각화 고려사항)\n  * 1111-11-11, [UMAP (균일 다양체 근사)](./dim_reduction/umap.qmd)\n    * Topological Data Analysis (위상 데이터 분석)\n    * Comparison with t-SNE\n  * 1111-11-11, [Autoencoders (오토인코더)](./dim_reduction/autoencoders.qmd)\n    * Vanilla Autoencoder\n    * Denoising Autoencoder (잡음제거 오토인코더)\n    * Variational Autoencoder (VAE)\n\n### Anomaly Detection (이상 탐지)\n* [Anomaly Detection Methods (이상 탐지 방법)](./anomaly/index.qmd)\n  * 1111-11-11, [Statistical Methods (통계적 방법)](./anomaly/statistical.qmd)\n    * Z-score Method\n    * Interquartile Range (IQR)\n    * Mahalanobis Distance (마할라노비스 거리)\n  * 1111-11-11, [Isolation Forest (고립 숲)](./anomaly/isolation_forest.qmd)\n    * Random Partitioning (무작위 분할)\n    * Anomaly Score (이상 점수)\n  * 1111-11-11, [One-Class SVM (단일클래스 SVM)](./anomaly/one_class_svm.qmd)\n  * 1111-11-11, [Local Outlier Factor (LOF)](./anomaly/lof.qmd)\n    * Local Density Estimation (지역 밀도 추정)\n\n### Association Rule Learning (연관 규칙 학습)\n* [Association Rules (연관 규칙)](./association/index.qmd)\n  * 1111-11-11, [Apriori Algorithm (Apriori 알고리즘)](./association/apriori.qmd)\n    * Support, Confidence, Lift (지지도, 신뢰도, 향상도)\n    * Frequent Itemsets (빈발 항목집합)\n  * 1111-11-11, [FP-Growth](./association/fp_growth.qmd)\n  * 1111-11-11, [Eclat Algorithm](./association/eclat.qmd)\n\n\n\n## Model Evaluation and Selection\n\n### Performance Metrics (성능 지표)\n* [Evaluation Metrics (평가 지표)](./evaluation/metrics/index.qmd) ⭐\n  * 1111-11-11, [Classification Metrics (분류 지표)](./evaluation/metrics/classification.qmd)\n    * Confusion Matrix (혼동 행렬)\n    * Accuracy, Precision, Recall (정확도, 정밀도, 재현율)\n    * F1-Score and F-beta Score\n    * ROC Curve and AUC (ROC 곡선과 AUC)\n    * Precision-Recall Curve (정밀도-재현율 곡선)\n    * Matthews Correlation Coefficient (MCC)\n    * Cohen's Kappa (코헨의 카파)\n  * 1111-11-11, [Regression Metrics (회귀 지표)](./evaluation/metrics/regression.qmd)\n    * Mean Squared Error (MSE, 평균제곱오차)\n    * Root Mean Squared Error (RMSE)\n    * Mean Absolute Error (MAE, 평균절대오차)\n    * R-squared and Adjusted R-squared\n    * Mean Absolute Percentage Error (MAPE)\n  * 1111-11-11, [Ranking Metrics (순위 지표)](./evaluation/metrics/ranking.qmd)\n    * Mean Average Precision (MAP)\n    * Normalized Discounted Cumulative Gain (NDCG)\n  * 1111-11-11, [Clustering Metrics (군집화 지표)](./evaluation/metrics/clustering.qmd)\n    * Silhouette Score (실루엣 점수)\n    * Davies-Bouldin Index\n    * Calinski-Harabasz Index\n\n### Model Validation (모델 검증)\n* [Validation Strategies (검증 전략)](./evaluation/validation/index.qmd) ⭐\n  * 1111-11-11, [Train-Test Split (학습-테스트 분할)](./evaluation/validation/train_test_split.qmd)\n  * 1111-11-11, [Cross-Validation (교차 검증)](./evaluation/validation/cross_validation.qmd) ⭐\n    * K-Fold Cross-Validation (K-겹 교차검증)\n    * Stratified K-Fold (층화 K-겹)\n    * Leave-One-Out (LOOCV)\n    * Time Series Split (시계열 분할)\n  * 1111-11-11, [Bootstrap Methods (부트스트랩 방법)](./evaluation/validation/bootstrap.qmd)\n    * Bootstrap Sampling (부트스트랩 샘플링)\n    * Out-of-Bag Validation\n    * .632 Bootstrap\n\n### Hyperparameter Tuning (하이퍼파라미터 튜닝)\n* [Hyperparameter Optimization (하이퍼파라미터 최적화)](./tuning/index.qmd) ⭐\n  * 1111-11-11, [Grid Search (그리드 탐색)](./tuning/grid_search.qmd)\n  * 1111-11-11, [Random Search (무작위 탐색)](./tuning/random_search.qmd)\n  * 1111-11-11, [Bayesian Optimization (베이지안 최적화)](./tuning/bayesian_optimization.qmd)\n    * Gaussian Process (가우시안 과정)\n    * Acquisition Functions (획득 함수)\n  * 1111-11-11, [Hyperband and ASHA](./tuning/hyperband.qmd)\n  * 1111-11-11, [Automated Machine Learning (AutoML)](./tuning/automl.qmd)\n    * Auto-sklearn\n    * TPOT\n    * H2O AutoML\n\n### Model Selection (모델 선택)\n* [Model Selection Strategies (모델 선택 전략)](./evaluation/selection/index.qmd)\n  * 1111-11-11, [Information Criteria (정보 기준)](./evaluation/selection/criteria.qmd)\n    * AIC (Akaike Information Criterion)\n    * BIC (Bayesian Information Criterion)\n    * MDL (Minimum Description Length)\n  * 1111-11-11, [Model Comparison (모델 비교)](./evaluation/selection/comparison.qmd)\n    * Statistical Tests for Model Comparison\n    * McNemar's Test\n    * Paired t-test\n  * 1111-11-11, [Baseline Models (베이스라인 모델)](./evaluation/selection/baseline.qmd)\n\n\n\n## Advanced Topics\n\n### Imbalanced Learning (불균형 학습)\n* [Handling Imbalanced Data (불균형 데이터 처리)](./imbalanced/index.qmd) ⭐\n  * 1111-11-11, [Resampling Techniques (리샘플링 기법)](./imbalanced/resampling.qmd)\n    * Oversampling (과대표집)\n      * Random Oversampling\n      * SMOTE (Synthetic Minority Over-sampling)\n      * ADASYN\n    * Undersampling (과소표집)\n      * Random Undersampling\n      * Tomek Links\n      * NearMiss\n    * Combined Methods (결합 방법)\n  * 1111-11-11, [Cost-Sensitive Learning (비용 민감 학습)](./imbalanced/cost_sensitive.qmd)\n    * Class Weights (클래스 가중치)\n    * Cost Matrix (비용 행렬)\n  * 1111-11-11, [Ensemble Methods for Imbalanced Data](./imbalanced/ensemble.qmd)\n    * Balanced Random Forest\n    * EasyEnsemble\n    * RUSBoost\n\n### Probabilistic Models (확률 모형)\n* [Probabilistic Machine Learning (확률적 머신러닝)](./probabilistic/index.qmd)\n  * 1111-11-11, [Bayesian Methods (베이지안 방법)](./probabilistic/bayesian.qmd)\n    * Bayesian Linear Regression\n    * Bayesian Logistic Regression\n    * Prior Selection (사전분포 선택)\n  * 1111-11-11, [Gaussian Processes (가우시안 과정)](./probabilistic/gaussian_processes.qmd) ⭐\n    * Kernel Functions (커널 함수)\n    * Predictive Distribution (예측 분포)\n    * Uncertainty Quantification (불확실성 정량화)\n  * 1111-11-11, [Hidden Markov Models (은닉 마르코프 모형)](./probabilistic/hmm.qmd)\n    * Forward-Backward Algorithm\n    * Viterbi Algorithm\n  * 1111-11-11, [Probabilistic Graphical Models (확률적 그래프 모형)](./probabilistic/pgm.qmd)\n    * Bayesian Networks (베이지안 네트워크)\n    * Markov Random Fields (마르코프 무작위장)\n\n### Online Learning (온라인 학습)\n* [Online and Incremental Learning (온라인 및 증분 학습)](./online/index.qmd)\n  * 1111-11-11, [Stochastic Gradient Descent (확률적 경사하강)](./online/sgd.qmd)\n  * 1111-11-11, [Online Learning Algorithms (온라인 학습 알고리즘)](./online/algorithms.qmd)\n    * Perceptron\n    * Passive-Aggressive Algorithms\n    * Online Gradient Descent\n  * 1111-11-11, [Concept Drift Detection (개념 표류 탐지)](./online/concept_drift.qmd)\n    * ADWIN\n    * DDM (Drift Detection Method)\n  * 1111-11-11, [Streaming Algorithms (스트리밍 알고리즘)](./online/streaming.qmd)\n\n### Semi-Supervised Learning (준지도학습)\n* [Semi-Supervised Methods (준지도 방법)](./semi_supervised/index.qmd)\n  * 1111-11-11, [Self-Training (자기학습)](./semi_supervised/self_training.qmd)\n  * 1111-11-11, [Co-Training (공동학습)](./semi_supervised/co_training.qmd)\n  * 1111-11-11, [Label Propagation (레이블 전파)](./semi_supervised/label_propagation.qmd)\n  * 1111-11-11, [Pseudo-Labeling (의사 레이블링)](./semi_supervised/pseudo_labeling.qmd)\n\n### Transfer Learning (전이학습)\n* [Transfer Learning Methods (전이학습 방법)](./transfer/index.qmd)\n  * 1111-11-11, [Domain Adaptation (도메인 적응)](./transfer/domain_adaptation.qmd)\n  * 1111-11-11, [Fine-tuning (미세조정)](./transfer/fine_tuning.qmd)\n  * 1111-11-11, [Multi-task Learning (다중작업 학습)](./transfer/multi_task.qmd)\n\n### Interpretability and Explainability (해석가능성)\n* [Model Interpretability (모델 해석가능성)](./interpretability/index.qmd) ⭐\n  * 1111-11-11, [Feature Importance (특성 중요도)](./interpretability/feature_importance.qmd)\n    * Permutation Importance (순열 중요도)\n    * Drop-Column Importance\n  * 1111-11-11, [Partial Dependence Plots (부분 의존 플롯)](./interpretability/pdp.qmd)\n  * 1111-11-11, [SHAP (SHapley Additive exPlanations)](./interpretability/shap.qmd) ⭐\n    * Shapley Values (섀플리 값)\n    * TreeSHAP\n    * KernelSHAP\n  * 1111-11-11, [LIME (Local Interpretable Model-agnostic Explanations)](./interpretability/lime.qmd)\n  * 1111-11-11, [Anchors and Counterfactuals (앵커와 반사실)](./interpretability/anchors.qmd)\n\n### Fairness and Bias (공정성과 편향)\n* [Fairness in Machine Learning (머신러닝의 공정성)](./fairness/index.qmd)\n  * 1111-11-11, [Bias Detection (편향 탐지)](./fairness/bias_detection.qmd)\n    * Demographic Parity (인구통계적 동등성)\n    * Equal Opportunity (동등한 기회)\n    * Disparate Impact (차별적 영향)\n  * 1111-11-11, [Bias Mitigation (편향 완화)](./fairness/bias_mitigation.qmd)\n    * Pre-processing Methods (전처리 방법)\n    * In-processing Methods (학습 중 방법)\n    * Post-processing Methods (후처리 방법)\n  * 1111-11-11, [Fairness Metrics (공정성 지표)](./fairness/metrics.qmd)\n\n\n\n## ML Engineering\n\n### Production ML (프로덕션 ML)\n* [ML in Production (프로덕션 머신러닝)](./production/index.qmd)\n  * 1111-11-11, [Model Serialization (모델 직렬화)](./production/serialization.qmd)\n    * Pickle and Joblib\n    * ONNX\n    * PMML\n  * 1111-11-11, [Model Serving (모델 서빙)](./production/serving.qmd)\n    * REST API\n    * gRPC\n    * Batch Prediction (배치 예측)\n  * 1111-11-11, [Model Monitoring (모델 모니터링)](./production/monitoring.qmd) ⭐\n    * Performance Monitoring (성능 모니터링)\n    * Data Drift Detection (데이터 표류 탐지)\n    * Model Drift Detection (모델 표류 탐지)\n    * Feature Drift (특성 표류)\n  * 1111-11-11, [Model Versioning (모델 버전관리)](./production/versioning.qmd)\n  * 1111-11-11, [A/B Testing for Models (모델 A/B 테스트)](./production/ab_testing.qmd)\n\n### Scalability (확장성)\n* [Scaling Machine Learning (머신러닝 확장)](./scalability/index.qmd)\n  * 1111-11-11, [Distributed Training (분산 학습)](./scalability/distributed_training.qmd)\n    * Data Parallelism (데이터 병렬화)\n    * Model Parallelism (모델 병렬화)\n  * 1111-11-11, [Large-scale ML Frameworks (대규모 ML 프레임워크)](./scalability/frameworks.qmd)\n    * Apache Spark MLlib\n    * Dask-ML\n    * Ray\n  * 1111-11-11, [Feature Stores (특성 저장소)](./scalability/feature_stores.qmd)\n  * 1111-11-11, [Model Compression (모델 압축)](./scalability/compression.qmd)\n    * Quantization (양자화)\n    * Pruning (가지치기)\n    * Knowledge Distillation (지식 증류)\n\n\n\n## Special Applications\n\n### Time Series Analysis (시계열 분석)\n* [Time Series Methods (시계열 방법)](./time_series/index.qmd)\n  * 1111-11-11, [Classical Methods (고전적 방법)](./time_series/classical.qmd)\n    * ARIMA Models\n    * Exponential Smoothing (지수 평활)\n    * Seasonal Decomposition (계절 분해)\n  * 1111-11-11, [ML for Time Series (시계열 ML)](./time_series/ml_methods.qmd)\n    * Feature Engineering for Time Series\n    * Lagged Features (지연 특성)\n    * Rolling Statistics (이동 통계량)\n  * 1111-11-11, [Forecasting (예측)](./time_series/forecasting.qmd)\n    * Prophet\n    * LightGBM for Time Series\n    * XGBoost for Time Series\n\n### Natural Language Processing (자연어처리)\n* [Classical NLP with ML (머신러닝 기반 NLP)](./nlp/index.qmd)\n  * 1111-11-11, [Text Preprocessing (텍스트 전처리)](./nlp/preprocessing.qmd)\n    * Tokenization (토큰화)\n    * Stemming and Lemmatization (어간추출과 표제어추출)\n    * Stop Words Removal (불용어 제거)\n  * 1111-11-11, [Text Representation (텍스트 표현)](./nlp/representation.qmd)\n    * Bag of Words (단어 가방)\n    * TF-IDF\n    * Word Embeddings (단어 임베딩)\n      * Word2Vec\n      * GloVe\n      * FastText\n  * 1111-11-11, [Text Classification (텍스트 분류)](./nlp/classification.qmd)\n    * Naive Bayes for Text\n    * SVM for Text\n  * 1111-11-11, [Topic Modeling (토픽 모델링)](./nlp/topic_modeling.qmd)\n    * Latent Dirichlet Allocation (LDA)\n    * Non-negative Matrix Factorization (NMF)\n\n### Computer Vision (컴퓨터 비전)\n* [Classical Computer Vision with ML (머신러닝 기반 컴퓨터 비전)](./cv/index.qmd)\n  * 1111-11-11, [Image Features (이미지 특성)](./cv/features.qmd)\n    * HOG (Histogram of Oriented Gradients)\n    * SIFT (Scale-Invariant Feature Transform)\n    * SURF\n  * 1111-11-11, [Image Classification (이미지 분류)](./cv/classification.qmd)\n    * SVM for Images\n    * Random Forest for Images\n  * 1111-11-11, [Object Detection Basics (객체 탐지 기초)](./cv/detection.qmd)\n    * Sliding Window (슬라이딩 윈도우)\n    * Image Pyramids (이미지 피라미드)\n\n### Recommender Systems (추천 시스템)\n* [Recommendation Algorithms (추천 알고리즘)](./recsys/index.qmd)\n  * 1111-11-11, [Collaborative Filtering (협업 필터링)](./recsys/collaborative.qmd)\n    * User-based CF (사용자 기반)\n    * Item-based CF (아이템 기반)\n    * Matrix Factorization (행렬 분해)\n      * SVD (특이값 분해)\n      * ALS (교대 최소자승)\n  * 1111-11-11, [Content-Based Filtering (내용 기반 필터링)](./recsys/content_based.qmd)\n  * 1111-11-11, [Hybrid Methods (하이브리드 방법)](./recsys/hybrid.qmd)\n  * 1111-11-11, [Evaluation Metrics (평가 지표)](./recsys/metrics.qmd)\n    * Precision@K and Recall@K\n    * MAP and NDCG\n    * Coverage and Diversity\n\n\n\n## Key Resources\n\n### Books\n- **Foundations:**\n  - Bishop (2006). \"Pattern Recognition and Machine Learning\"\n  - Murphy (2012). \"Machine Learning: A Probabilistic Perspective\"\n  - Hastie, Tibshirani, Friedman (2009). \"The Elements of Statistical Learning\" ⭐\n\n- **Practical:**\n  - Géron (2019). \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"\n  - Kuhn and Johnson (2013). \"Applied Predictive Modeling\"\n\n- **Advanced:**\n  - Raschka and Mirjalili (2019). \"Python Machine Learning\"\n  - Chollet (2021). \"Deep Learning with Python\"\n\n### Online Courses\n- Andrew Ng - Machine Learning (Coursera) ⭐\n- Fast.ai - Practical Deep Learning for Coders\n- Stanford CS229 - Machine Learning\n\n### Papers\n- **Random Forests:** Breiman (2001). \"Random Forests\"\n- **XGBoost:** Chen and Guestrin (2016). \"XGBoost: A Scalable Tree Boosting System\"\n- **SHAP:** Lundberg and Lee (2017). \"A Unified Approach to Interpreting Model Predictions\"\n\n\n\n","srcMarkdownNoYaml":"\n\n## Learning Path\n\n### Phase 1: Foundations (2-3개월, 수학적 기초와 ML 핵심 개념 확립)\n\nMathematical Prerequisites\n  - Linear Algebra ⭐\n  - Calculus and Optimization ⭐\n  - Probability and Statistics ⭐\n\nML Fundamentals\n  - ML 개념과 학습 유형\n  - Bias-Variance Tradeoff ⭐\n  - Data Preprocessing\n  - Feature Engineering ⭐\n\nBasic Algorithms\n  - Linear Regression ⭐\n  - Logistic Regression ⭐\n  - Decision Trees ⭐\n\n### Historical Methods\n\n* 2022-12-09, [PCA (Principal Component Analysis)](./2022-12-09-PCA.qmd)\n* 2023-02-03, [PyTorch Introduction](./2023-02-03_pytorch_introduction.qmd)\n* 2023-02-03, [TensorFlow Introduction](./2023-02-03_tf_introduction.qmd)\n* 2023-02-06, [Naive Bayes](./2023-02-06_naive_bayes.qmd)\n\n### Phase 2: Core Algorithms (2-3개월, 주요 알고리즘)\n\nClassification & Regression\n  - SVM ⭐\n  - KNN\n  - Naive Bayes\n  - Regularization ⭐\n\nEnsemble Methods\n  - Random Forest ⭐\n  - Gradient Boosting ⭐\n  - XGBoost ⭐\n  - LightGBM ⭐\n\nUnsupervised Learning\n  - K-Means ⭐\n  - Hierarchical Clustering\n  - PCA ⭐\n  - DBSCAN\n\n\n### Phase 3: Advanced Methods (2-3개월, 고급 기법)\n\nModel Evaluation\n  - Cross-Validation ⭐\n  - Performance Metrics ⭐\n  - Hyperparameter Tuning ⭐\n\nAdvanced Topics\n  - Imbalanced Learning ⭐\n  - Gaussian Processes\n  - Model Interpretability (SHAP) ⭐\n\nSpecial Applications\n  - Time Series\n  - NLP Basics\n  - Recommender Systems\n\n### Phase 4: ML Engineering (2개월, 프로덕션 배포와 운영)\n\nProduction ML\n  - Model Serving\n  - Model Monitoring ⭐\n  - A/B Testing for Models\n\nScalability\n  - Distributed Training\n  - Model Compression\n  - Feature Stores\n\n## Implementation Notes\n\n### Python Libraries\n\nInterpretability\n- import shap ⭐\n- import lime\n\nProduction\n- import mlflow ⭐\n- import bentoml\n\n\n### Recommended Project Structure\n\nml_project/\n├── data/\n│   ├── raw/\n│   ├── processed/\n│   └── features/\n├── notebooks/\n│   ├── eda.ipynb\n│   ├── modeling.ipynb\n│   └── evaluation.ipynb\n├── src/\n│   ├── preprocessing/\n│   ├── features/\n│   ├── models/\n│   └── evaluation/\n├── tests/\n├── configs/\n└── models/\n\n\n## Foundations\n\n### Mathematical Prerequisites\n* [Linear Algebra for ML (ML을 위한 선형대수)](./foundations/linear_algebra.qmd)\n  * 1111-11-11, [Vectors and Matrices (벡터와 행렬)](./foundations/vectors_matrices.qmd)\n  * 1111-11-11, [Matrix Operations (행렬 연산)](./foundations/matrix_operations.qmd)\n  * 1111-11-11, [Eigenvalues and Eigenvectors (고유값과 고유벡터)](./foundations/eigenvalues.qmd)\n  * 1111-11-11, [Singular Value Decomposition (특이값 분해)](./foundations/svd.qmd)\n\n* [Calculus and Optimization (미적분과 최적화)](./foundations/calculus.qmd)\n  * 1111-11-11, [Derivatives and Gradients (도함수와 기울기)](./foundations/derivatives.qmd)\n  * 1111-11-11, [Chain Rule and Backpropagation (연쇄법칙과 역전파)](./foundations/chain_rule.qmd)\n  * 1111-11-11, [Convex Optimization (볼록 최적화)](./foundations/convex_optimization.qmd)\n  * 1111-11-11, [Gradient Descent Methods (경사하강법)](./foundations/gradient_descent.qmd)\n\n* [Probability and Statistics (확률과 통계)](./foundations/probability.qmd)\n  * 1111-11-11, [Probability Distributions (확률분포)](./foundations/distributions.qmd)\n  * 1111-11-11, [Maximum Likelihood Estimation (최대우도추정)](./foundations/mle.qmd)\n  * 1111-11-11, [Bayesian Inference (베이지안 추론)](./foundations/bayesian_inference.qmd)\n  * 1111-11-11, [Statistical Testing (통계적 검정)](./foundations/statistical_testing.qmd)\n\n### ML Fundamentals\n* [Introduction to Machine Learning (머신러닝 소개)](./fundamentals/intro.qmd)\n  * 1111-11-11, [What is Machine Learning? (머신러닝이란?)](./fundamentals/what_is_ml.qmd)\n  * 1111-11-11, [Types of Learning (학습의 종류)](./fundamentals/learning_types.qmd)\n    * Supervised Learning (지도학습)\n    * Unsupervised Learning (비지도학습)\n    * Semi-supervised Learning (준지도학습)\n    * Reinforcement Learning (강화학습)\n  * 1111-11-11, [Bias-Variance Tradeoff (편향-분산 트레이드오프)](./fundamentals/bias_variance.qmd) ⭐\n  * 1111-11-11, [Overfitting and Underfitting (과적합과 과소적합)](./fundamentals/overfitting.qmd)\n  * 1111-11-11, [No Free Lunch Theorem (공짜 점심 정리)](./fundamentals/no_free_lunch.qmd)\n\n* [Data Preprocessing (데이터 전처리)](./preprocessing/index.qmd)\n  * 1111-11-11, [Data Cleaning (데이터 정제)](./preprocessing/cleaning.qmd)\n    * Missing Value Handling (결측치 처리)\n    * Outlier Detection (이상치 탐지)\n    * Data Quality Assessment (데이터 품질 평가)\n  * 1111-11-11, [Feature Scaling (특성 스케일링)](./preprocessing/scaling.qmd)\n    * Standardization (표준화)\n    * Normalization (정규화)\n    * Robust Scaling (로버스트 스케일링)\n  * 1111-11-11, [Encoding Categorical Variables (범주형 변수 인코딩)](./preprocessing/encoding.qmd)\n    * One-Hot Encoding\n    * Label Encoding\n    * Target Encoding\n    * Embeddings\n\n* [Feature Engineering (특성 공학)](./feature_engineering/index.qmd) ⭐\n  * 1111-11-11, [Feature Creation (특성 생성)](./feature_engineering/creation.qmd)\n    * Polynomial Features (다항 특성)\n    * Interaction Features (상호작용 특성)\n    * Domain-specific Features (도메인 특화 특성)\n  * 1111-11-11, [Feature Selection (특성 선택)](./feature_engineering/selection.qmd)\n    * Filter Methods (필터 방법)\n    * Wrapper Methods (래퍼 방법)\n    * Embedded Methods (임베디드 방법)\n  * 1111-11-11, [Dimensionality Reduction (차원 축소)](./feature_engineering/dimensionality_reduction.qmd)\n    * Principal Component Analysis (주성분 분석)\n    * Linear Discriminant Analysis (선형판별분석)\n    * t-SNE and UMAP\n\n\n\n## Supervised Learning\n\n### Regression (회귀)\n* [Regression Analysis (회귀 분석)](./regression/index.qmd)\n  * 1111-11-11, [Linear Regression (선형 회귀)](./regression/linear_regression.qmd) ⭐\n    * Ordinary Least Squares (최소자승법)\n    * Geometric Interpretation (기하학적 해석)\n    * Assumptions and Diagnostics (가정과 진단)\n    * Coefficient Interpretation (계수 해석)\n  * 1111-11-11, [Regularization (정규화)](./regression/regularization.qmd) ⭐\n    * Ridge Regression (L2 정규화)\n    * Lasso Regression (L1 정규화)\n    * Elastic Net (탄력망)\n    * Regularization Path (정규화 경로)\n  * 1111-11-11, [Polynomial Regression (다항 회귀)](./regression/polynomial.qmd)\n  * 1111-11-11, [Generalized Linear Models (일반화 선형모형)](./regression/glm.qmd)\n    * Logistic Regression (로지스틱 회귀)\n    * Poisson Regression (포아송 회귀)\n    * Link Functions (연결함수)\n\n### Classification (분류)\n* [Classification Methods (분류 방법)](./classification/index.qmd)\n  * 1111-11-11, [Logistic Regression (로지스틱 회귀)](./classification/logistic_regression.qmd) ⭐\n    * Binary Classification (이진 분류)\n    * Multinomial Logistic Regression (다항 로지스틱)\n    * Odds Ratio Interpretation (오즈비 해석)\n  * 1111-11-11, [Naive Bayes (나이브 베이즈)](./classification/naive_bayes.qmd)\n    * Gaussian Naive Bayes\n    * Multinomial Naive Bayes\n    * Bernoulli Naive Bayes\n  * 1111-11-11, [K-Nearest Neighbors (K-최근접 이웃)](./classification/knn.qmd)\n    * Distance Metrics (거리 척도)\n    * Curse of Dimensionality (차원의 저주)\n    * Weighted KNN (가중 KNN)\n  * 1111-11-11, [Support Vector Machines (서포트 벡터 머신)](./classification/svm.qmd) ⭐\n    * Linear SVM (선형 SVM)\n    * Kernel Trick (커널 트릭)\n    * Soft Margin (소프트 마진)\n    * Multi-class SVM (다중클래스 SVM)\n  * 1111-11-11, [Decision Trees (의사결정 나무)](./classification/decision_trees.qmd) ⭐\n    * Splitting Criteria (분할 기준)\n      * Gini Impurity (지니 불순도)\n      * Information Gain (정보 이득)\n      * Gain Ratio (이득 비율)\n    * Pruning (가지치기)\n    * CART Algorithm (CART 알고리즘)\n\n### Ensemble Methods (앙상블 방법)\n* [Ensemble Learning (앙상블 학습)](./ensemble/index.qmd) ⭐\n  * 1111-11-11, [Bagging Methods (배깅 방법)](./ensemble/bagging.qmd)\n    * [Random Forest (랜덤 포레스트)](./ensemble/random_forest.qmd) ⭐\n      * Bootstrap Aggregating\n      * Feature Randomness (특성 무작위성)\n      * Out-of-Bag Error (OOB 오차)\n      * Feature Importance (특성 중요도)\n    * Extra Trees (극단적 무작위 나무)\n  * 1111-11-11, [Boosting Methods (부스팅 방법)](./ensemble/boosting.qmd) ⭐\n    * [AdaBoost (적응적 부스팅)](./ensemble/adaboost.qmd)\n      * Weak Learner Weighting (약학습기 가중치)\n      * Sample Reweighting (표본 재가중)\n    * [Gradient Boosting (경사 부스팅)](./ensemble/gradient_boosting.qmd) ⭐\n      * Gradient Boosting Machines (GBM)\n      * Loss Function Optimization (손실함수 최적화)\n      * Learning Rate and Trees (학습률과 나무 수)\n    * [XGBoost](./ensemble/xgboost.qmd) ⭐\n      * Regularization Terms (정규화 항)\n      * System Optimization (시스템 최적화)\n      * Handling Missing Values (결측치 처리)\n    * [LightGBM](./ensemble/lightgbm.qmd) ⭐\n      * Histogram-based Algorithm (히스토그램 기반)\n      * Leaf-wise Growth (리프 단위 성장)\n      * Categorical Feature Support (범주형 특성 지원)\n    * [CatBoost](./ensemble/catboost.qmd)\n      * Ordered Boosting (순서 부스팅)\n      * Native Categorical Handling (네이티브 범주형 처리)\n  * 1111-11-11, [Stacking (스태킹)](./ensemble/stacking.qmd)\n    * Meta-learner (메타학습기)\n    * Blending (블렌딩)\n  * 1111-11-11, [Voting Classifiers (투표 분류기)](./ensemble/voting.qmd)\n    * Hard Voting (하드 투표)\n    * Soft Voting (소프트 투표)\n\n\n## Unsupervised Learning\n\n### Clustering (군집화)\n* [Clustering Methods (군집화 방법)](./clustering/index.qmd)\n  * 1111-11-11, [K-Means Clustering (K-평균 군집화)](./clustering/kmeans.qmd) ⭐\n    * Lloyd's Algorithm (로이드 알고리즘)\n    * Elbow Method (엘보우 방법)\n    * K-Means++ Initialization (K-평균++ 초기화)\n    * Mini-Batch K-Means (미니배치 K-평균)\n  * 1111-11-11, [Hierarchical Clustering (계층적 군집화)](./clustering/hierarchical.qmd)\n    * Agglomerative Clustering (병합 군집화)\n    * Divisive Clustering (분할 군집화)\n    * Linkage Methods (연결 방법)\n    * Dendrogram (덴드로그램)\n  * 1111-11-11, [DBSCAN (밀도 기반 군집화)](./clustering/dbscan.qmd)\n    * Density-Based Clustering (밀도 기반)\n    * Core Points and Border Points (핵심점과 경계점)\n    * Handling Noise (잡음 처리)\n  * 1111-11-11, [Gaussian Mixture Models (가우시안 혼합 모형)](./clustering/gmm.qmd)\n    * Expectation-Maximization (EM 알고리즘)\n    * Soft Clustering (소프트 군집화)\n    * Model Selection (모형 선택)\n  * 1111-11-11, [Other Clustering Methods (기타 군집화 방법)](./clustering/others.qmd)\n    * OPTICS\n    * Mean Shift\n    * Spectral Clustering (스펙트럼 군집화)\n\n### Dimensionality Reduction (차원 축소)\n* [Dimensionality Reduction Methods (차원 축소 방법)](./dim_reduction/index.qmd)\n  * 1111-11-11, [Principal Component Analysis (주성분 분석)](./dim_reduction/pca.qmd) ⭐\n    * Eigenvalue Decomposition (고유값 분해)\n    * Variance Explained (설명된 분산)\n    * Scree Plot (스크리 플롯)\n    * Kernel PCA (커널 PCA)\n  * 1111-11-11, [Linear Discriminant Analysis (선형판별분석)](./dim_reduction/lda.qmd)\n    * Fisher's Linear Discriminant (피셔의 선형판별)\n    * Between-class vs. Within-class Variance\n  * 1111-11-11, [t-SNE (t-분포 확률적 임베딩)](./dim_reduction/tsne.qmd)\n    * Perplexity Parameter (복잡도 매개변수)\n    * KL Divergence Optimization (KL 발산 최적화)\n    * Visualization Considerations (시각화 고려사항)\n  * 1111-11-11, [UMAP (균일 다양체 근사)](./dim_reduction/umap.qmd)\n    * Topological Data Analysis (위상 데이터 분석)\n    * Comparison with t-SNE\n  * 1111-11-11, [Autoencoders (오토인코더)](./dim_reduction/autoencoders.qmd)\n    * Vanilla Autoencoder\n    * Denoising Autoencoder (잡음제거 오토인코더)\n    * Variational Autoencoder (VAE)\n\n### Anomaly Detection (이상 탐지)\n* [Anomaly Detection Methods (이상 탐지 방법)](./anomaly/index.qmd)\n  * 1111-11-11, [Statistical Methods (통계적 방법)](./anomaly/statistical.qmd)\n    * Z-score Method\n    * Interquartile Range (IQR)\n    * Mahalanobis Distance (마할라노비스 거리)\n  * 1111-11-11, [Isolation Forest (고립 숲)](./anomaly/isolation_forest.qmd)\n    * Random Partitioning (무작위 분할)\n    * Anomaly Score (이상 점수)\n  * 1111-11-11, [One-Class SVM (단일클래스 SVM)](./anomaly/one_class_svm.qmd)\n  * 1111-11-11, [Local Outlier Factor (LOF)](./anomaly/lof.qmd)\n    * Local Density Estimation (지역 밀도 추정)\n\n### Association Rule Learning (연관 규칙 학습)\n* [Association Rules (연관 규칙)](./association/index.qmd)\n  * 1111-11-11, [Apriori Algorithm (Apriori 알고리즘)](./association/apriori.qmd)\n    * Support, Confidence, Lift (지지도, 신뢰도, 향상도)\n    * Frequent Itemsets (빈발 항목집합)\n  * 1111-11-11, [FP-Growth](./association/fp_growth.qmd)\n  * 1111-11-11, [Eclat Algorithm](./association/eclat.qmd)\n\n\n\n## Model Evaluation and Selection\n\n### Performance Metrics (성능 지표)\n* [Evaluation Metrics (평가 지표)](./evaluation/metrics/index.qmd) ⭐\n  * 1111-11-11, [Classification Metrics (분류 지표)](./evaluation/metrics/classification.qmd)\n    * Confusion Matrix (혼동 행렬)\n    * Accuracy, Precision, Recall (정확도, 정밀도, 재현율)\n    * F1-Score and F-beta Score\n    * ROC Curve and AUC (ROC 곡선과 AUC)\n    * Precision-Recall Curve (정밀도-재현율 곡선)\n    * Matthews Correlation Coefficient (MCC)\n    * Cohen's Kappa (코헨의 카파)\n  * 1111-11-11, [Regression Metrics (회귀 지표)](./evaluation/metrics/regression.qmd)\n    * Mean Squared Error (MSE, 평균제곱오차)\n    * Root Mean Squared Error (RMSE)\n    * Mean Absolute Error (MAE, 평균절대오차)\n    * R-squared and Adjusted R-squared\n    * Mean Absolute Percentage Error (MAPE)\n  * 1111-11-11, [Ranking Metrics (순위 지표)](./evaluation/metrics/ranking.qmd)\n    * Mean Average Precision (MAP)\n    * Normalized Discounted Cumulative Gain (NDCG)\n  * 1111-11-11, [Clustering Metrics (군집화 지표)](./evaluation/metrics/clustering.qmd)\n    * Silhouette Score (실루엣 점수)\n    * Davies-Bouldin Index\n    * Calinski-Harabasz Index\n\n### Model Validation (모델 검증)\n* [Validation Strategies (검증 전략)](./evaluation/validation/index.qmd) ⭐\n  * 1111-11-11, [Train-Test Split (학습-테스트 분할)](./evaluation/validation/train_test_split.qmd)\n  * 1111-11-11, [Cross-Validation (교차 검증)](./evaluation/validation/cross_validation.qmd) ⭐\n    * K-Fold Cross-Validation (K-겹 교차검증)\n    * Stratified K-Fold (층화 K-겹)\n    * Leave-One-Out (LOOCV)\n    * Time Series Split (시계열 분할)\n  * 1111-11-11, [Bootstrap Methods (부트스트랩 방법)](./evaluation/validation/bootstrap.qmd)\n    * Bootstrap Sampling (부트스트랩 샘플링)\n    * Out-of-Bag Validation\n    * .632 Bootstrap\n\n### Hyperparameter Tuning (하이퍼파라미터 튜닝)\n* [Hyperparameter Optimization (하이퍼파라미터 최적화)](./tuning/index.qmd) ⭐\n  * 1111-11-11, [Grid Search (그리드 탐색)](./tuning/grid_search.qmd)\n  * 1111-11-11, [Random Search (무작위 탐색)](./tuning/random_search.qmd)\n  * 1111-11-11, [Bayesian Optimization (베이지안 최적화)](./tuning/bayesian_optimization.qmd)\n    * Gaussian Process (가우시안 과정)\n    * Acquisition Functions (획득 함수)\n  * 1111-11-11, [Hyperband and ASHA](./tuning/hyperband.qmd)\n  * 1111-11-11, [Automated Machine Learning (AutoML)](./tuning/automl.qmd)\n    * Auto-sklearn\n    * TPOT\n    * H2O AutoML\n\n### Model Selection (모델 선택)\n* [Model Selection Strategies (모델 선택 전략)](./evaluation/selection/index.qmd)\n  * 1111-11-11, [Information Criteria (정보 기준)](./evaluation/selection/criteria.qmd)\n    * AIC (Akaike Information Criterion)\n    * BIC (Bayesian Information Criterion)\n    * MDL (Minimum Description Length)\n  * 1111-11-11, [Model Comparison (모델 비교)](./evaluation/selection/comparison.qmd)\n    * Statistical Tests for Model Comparison\n    * McNemar's Test\n    * Paired t-test\n  * 1111-11-11, [Baseline Models (베이스라인 모델)](./evaluation/selection/baseline.qmd)\n\n\n\n## Advanced Topics\n\n### Imbalanced Learning (불균형 학습)\n* [Handling Imbalanced Data (불균형 데이터 처리)](./imbalanced/index.qmd) ⭐\n  * 1111-11-11, [Resampling Techniques (리샘플링 기법)](./imbalanced/resampling.qmd)\n    * Oversampling (과대표집)\n      * Random Oversampling\n      * SMOTE (Synthetic Minority Over-sampling)\n      * ADASYN\n    * Undersampling (과소표집)\n      * Random Undersampling\n      * Tomek Links\n      * NearMiss\n    * Combined Methods (결합 방법)\n  * 1111-11-11, [Cost-Sensitive Learning (비용 민감 학습)](./imbalanced/cost_sensitive.qmd)\n    * Class Weights (클래스 가중치)\n    * Cost Matrix (비용 행렬)\n  * 1111-11-11, [Ensemble Methods for Imbalanced Data](./imbalanced/ensemble.qmd)\n    * Balanced Random Forest\n    * EasyEnsemble\n    * RUSBoost\n\n### Probabilistic Models (확률 모형)\n* [Probabilistic Machine Learning (확률적 머신러닝)](./probabilistic/index.qmd)\n  * 1111-11-11, [Bayesian Methods (베이지안 방법)](./probabilistic/bayesian.qmd)\n    * Bayesian Linear Regression\n    * Bayesian Logistic Regression\n    * Prior Selection (사전분포 선택)\n  * 1111-11-11, [Gaussian Processes (가우시안 과정)](./probabilistic/gaussian_processes.qmd) ⭐\n    * Kernel Functions (커널 함수)\n    * Predictive Distribution (예측 분포)\n    * Uncertainty Quantification (불확실성 정량화)\n  * 1111-11-11, [Hidden Markov Models (은닉 마르코프 모형)](./probabilistic/hmm.qmd)\n    * Forward-Backward Algorithm\n    * Viterbi Algorithm\n  * 1111-11-11, [Probabilistic Graphical Models (확률적 그래프 모형)](./probabilistic/pgm.qmd)\n    * Bayesian Networks (베이지안 네트워크)\n    * Markov Random Fields (마르코프 무작위장)\n\n### Online Learning (온라인 학습)\n* [Online and Incremental Learning (온라인 및 증분 학습)](./online/index.qmd)\n  * 1111-11-11, [Stochastic Gradient Descent (확률적 경사하강)](./online/sgd.qmd)\n  * 1111-11-11, [Online Learning Algorithms (온라인 학습 알고리즘)](./online/algorithms.qmd)\n    * Perceptron\n    * Passive-Aggressive Algorithms\n    * Online Gradient Descent\n  * 1111-11-11, [Concept Drift Detection (개념 표류 탐지)](./online/concept_drift.qmd)\n    * ADWIN\n    * DDM (Drift Detection Method)\n  * 1111-11-11, [Streaming Algorithms (스트리밍 알고리즘)](./online/streaming.qmd)\n\n### Semi-Supervised Learning (준지도학습)\n* [Semi-Supervised Methods (준지도 방법)](./semi_supervised/index.qmd)\n  * 1111-11-11, [Self-Training (자기학습)](./semi_supervised/self_training.qmd)\n  * 1111-11-11, [Co-Training (공동학습)](./semi_supervised/co_training.qmd)\n  * 1111-11-11, [Label Propagation (레이블 전파)](./semi_supervised/label_propagation.qmd)\n  * 1111-11-11, [Pseudo-Labeling (의사 레이블링)](./semi_supervised/pseudo_labeling.qmd)\n\n### Transfer Learning (전이학습)\n* [Transfer Learning Methods (전이학습 방법)](./transfer/index.qmd)\n  * 1111-11-11, [Domain Adaptation (도메인 적응)](./transfer/domain_adaptation.qmd)\n  * 1111-11-11, [Fine-tuning (미세조정)](./transfer/fine_tuning.qmd)\n  * 1111-11-11, [Multi-task Learning (다중작업 학습)](./transfer/multi_task.qmd)\n\n### Interpretability and Explainability (해석가능성)\n* [Model Interpretability (모델 해석가능성)](./interpretability/index.qmd) ⭐\n  * 1111-11-11, [Feature Importance (특성 중요도)](./interpretability/feature_importance.qmd)\n    * Permutation Importance (순열 중요도)\n    * Drop-Column Importance\n  * 1111-11-11, [Partial Dependence Plots (부분 의존 플롯)](./interpretability/pdp.qmd)\n  * 1111-11-11, [SHAP (SHapley Additive exPlanations)](./interpretability/shap.qmd) ⭐\n    * Shapley Values (섀플리 값)\n    * TreeSHAP\n    * KernelSHAP\n  * 1111-11-11, [LIME (Local Interpretable Model-agnostic Explanations)](./interpretability/lime.qmd)\n  * 1111-11-11, [Anchors and Counterfactuals (앵커와 반사실)](./interpretability/anchors.qmd)\n\n### Fairness and Bias (공정성과 편향)\n* [Fairness in Machine Learning (머신러닝의 공정성)](./fairness/index.qmd)\n  * 1111-11-11, [Bias Detection (편향 탐지)](./fairness/bias_detection.qmd)\n    * Demographic Parity (인구통계적 동등성)\n    * Equal Opportunity (동등한 기회)\n    * Disparate Impact (차별적 영향)\n  * 1111-11-11, [Bias Mitigation (편향 완화)](./fairness/bias_mitigation.qmd)\n    * Pre-processing Methods (전처리 방법)\n    * In-processing Methods (학습 중 방법)\n    * Post-processing Methods (후처리 방법)\n  * 1111-11-11, [Fairness Metrics (공정성 지표)](./fairness/metrics.qmd)\n\n\n\n## ML Engineering\n\n### Production ML (프로덕션 ML)\n* [ML in Production (프로덕션 머신러닝)](./production/index.qmd)\n  * 1111-11-11, [Model Serialization (모델 직렬화)](./production/serialization.qmd)\n    * Pickle and Joblib\n    * ONNX\n    * PMML\n  * 1111-11-11, [Model Serving (모델 서빙)](./production/serving.qmd)\n    * REST API\n    * gRPC\n    * Batch Prediction (배치 예측)\n  * 1111-11-11, [Model Monitoring (모델 모니터링)](./production/monitoring.qmd) ⭐\n    * Performance Monitoring (성능 모니터링)\n    * Data Drift Detection (데이터 표류 탐지)\n    * Model Drift Detection (모델 표류 탐지)\n    * Feature Drift (특성 표류)\n  * 1111-11-11, [Model Versioning (모델 버전관리)](./production/versioning.qmd)\n  * 1111-11-11, [A/B Testing for Models (모델 A/B 테스트)](./production/ab_testing.qmd)\n\n### Scalability (확장성)\n* [Scaling Machine Learning (머신러닝 확장)](./scalability/index.qmd)\n  * 1111-11-11, [Distributed Training (분산 학습)](./scalability/distributed_training.qmd)\n    * Data Parallelism (데이터 병렬화)\n    * Model Parallelism (모델 병렬화)\n  * 1111-11-11, [Large-scale ML Frameworks (대규모 ML 프레임워크)](./scalability/frameworks.qmd)\n    * Apache Spark MLlib\n    * Dask-ML\n    * Ray\n  * 1111-11-11, [Feature Stores (특성 저장소)](./scalability/feature_stores.qmd)\n  * 1111-11-11, [Model Compression (모델 압축)](./scalability/compression.qmd)\n    * Quantization (양자화)\n    * Pruning (가지치기)\n    * Knowledge Distillation (지식 증류)\n\n\n\n## Special Applications\n\n### Time Series Analysis (시계열 분석)\n* [Time Series Methods (시계열 방법)](./time_series/index.qmd)\n  * 1111-11-11, [Classical Methods (고전적 방법)](./time_series/classical.qmd)\n    * ARIMA Models\n    * Exponential Smoothing (지수 평활)\n    * Seasonal Decomposition (계절 분해)\n  * 1111-11-11, [ML for Time Series (시계열 ML)](./time_series/ml_methods.qmd)\n    * Feature Engineering for Time Series\n    * Lagged Features (지연 특성)\n    * Rolling Statistics (이동 통계량)\n  * 1111-11-11, [Forecasting (예측)](./time_series/forecasting.qmd)\n    * Prophet\n    * LightGBM for Time Series\n    * XGBoost for Time Series\n\n### Natural Language Processing (자연어처리)\n* [Classical NLP with ML (머신러닝 기반 NLP)](./nlp/index.qmd)\n  * 1111-11-11, [Text Preprocessing (텍스트 전처리)](./nlp/preprocessing.qmd)\n    * Tokenization (토큰화)\n    * Stemming and Lemmatization (어간추출과 표제어추출)\n    * Stop Words Removal (불용어 제거)\n  * 1111-11-11, [Text Representation (텍스트 표현)](./nlp/representation.qmd)\n    * Bag of Words (단어 가방)\n    * TF-IDF\n    * Word Embeddings (단어 임베딩)\n      * Word2Vec\n      * GloVe\n      * FastText\n  * 1111-11-11, [Text Classification (텍스트 분류)](./nlp/classification.qmd)\n    * Naive Bayes for Text\n    * SVM for Text\n  * 1111-11-11, [Topic Modeling (토픽 모델링)](./nlp/topic_modeling.qmd)\n    * Latent Dirichlet Allocation (LDA)\n    * Non-negative Matrix Factorization (NMF)\n\n### Computer Vision (컴퓨터 비전)\n* [Classical Computer Vision with ML (머신러닝 기반 컴퓨터 비전)](./cv/index.qmd)\n  * 1111-11-11, [Image Features (이미지 특성)](./cv/features.qmd)\n    * HOG (Histogram of Oriented Gradients)\n    * SIFT (Scale-Invariant Feature Transform)\n    * SURF\n  * 1111-11-11, [Image Classification (이미지 분류)](./cv/classification.qmd)\n    * SVM for Images\n    * Random Forest for Images\n  * 1111-11-11, [Object Detection Basics (객체 탐지 기초)](./cv/detection.qmd)\n    * Sliding Window (슬라이딩 윈도우)\n    * Image Pyramids (이미지 피라미드)\n\n### Recommender Systems (추천 시스템)\n* [Recommendation Algorithms (추천 알고리즘)](./recsys/index.qmd)\n  * 1111-11-11, [Collaborative Filtering (협업 필터링)](./recsys/collaborative.qmd)\n    * User-based CF (사용자 기반)\n    * Item-based CF (아이템 기반)\n    * Matrix Factorization (행렬 분해)\n      * SVD (특이값 분해)\n      * ALS (교대 최소자승)\n  * 1111-11-11, [Content-Based Filtering (내용 기반 필터링)](./recsys/content_based.qmd)\n  * 1111-11-11, [Hybrid Methods (하이브리드 방법)](./recsys/hybrid.qmd)\n  * 1111-11-11, [Evaluation Metrics (평가 지표)](./recsys/metrics.qmd)\n    * Precision@K and Recall@K\n    * MAP and NDCG\n    * Coverage and Diversity\n\n\n\n## Key Resources\n\n### Books\n- **Foundations:**\n  - Bishop (2006). \"Pattern Recognition and Machine Learning\"\n  - Murphy (2012). \"Machine Learning: A Probabilistic Perspective\"\n  - Hastie, Tibshirani, Friedman (2009). \"The Elements of Statistical Learning\" ⭐\n\n- **Practical:**\n  - Géron (2019). \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"\n  - Kuhn and Johnson (2013). \"Applied Predictive Modeling\"\n\n- **Advanced:**\n  - Raschka and Mirjalili (2019). \"Python Machine Learning\"\n  - Chollet (2021). \"Deep Learning with Python\"\n\n### Online Courses\n- Andrew Ng - Machine Learning (Coursera) ⭐\n- Fast.ai - Practical Deep Learning for Coders\n- Stanford CS229 - Machine Learning\n\n### Papers\n- **Random Forests:** Breiman (2001). \"Random Forests\"\n- **XGBoost:** Chen and Guestrin (2016). \"XGBoost: A Scalable Tree Boosting System\"\n- **SHAP:** Lundberg and Lee (2017). \"A Unified Approach to Interpreting Model Predictions\"\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":true,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","css":["../../../../styles.css"],"toc":true,"toc-depth":3,"number-sections":true,"include-in-header":[{"text":"<style>\n.custom-footer { \n  text-align: center; \n  font-size: 0.8em; \n  color: #666; \n  margin-top: 2rem; \n}\n</style>\n"}],"include-after-body":["../../../../js.html","../../signup.html"],"output-file":"index.html"},"language":{"toc-title-document":"목차","toc-title-website":"목차","related-formats-title":"기타 형식","related-notebooks-title":"Notebooks","source-notebooks-prefix":"원천","other-links-title":"기타 링크","code-links-title":"코드 링크","launch-dev-container-title":"Dev 컨테이너 실행","launch-binder-title":"랜치 Binder","article-notebook-label":"기사 노트북","notebook-preview-download":"노트북 다운로드","notebook-preview-download-src":"소스 다운로드","notebook-preview-back":"기사로 돌아가기","manuscript-meca-bundle":"MECA 아카이브","section-title-abstract":"초록","section-title-appendices":"부록","section-title-footnotes":"각주","section-title-references":"참고문헌","section-title-reuse":"라이센스","section-title-copyright":"저작권","section-title-citation":"인용","appendix-attribution-cite-as":"인용방법","appendix-attribution-bibtex":"BibTeX 인용:","title-block-author-single":"저자","title-block-author-plural":"저자","title-block-affiliation-single":"소속","title-block-affiliation-plural":"소속","title-block-published":"공개","title-block-modified":"Modified","title-block-keywords":"키워드","callout-tip-title":"힌트","callout-note-title":"노트","callout-warning-title":"경고","callout-important-title":"중요","callout-caution-title":"주의","code-summary":"코드","code-tools-menu-caption":"코드","code-tools-show-all-code":"전체 코드 표시","code-tools-hide-all-code":"전체 코드 숨기기","code-tools-view-source":"소스 코드 표시","code-tools-source-code":"소스 코드","tools-share":"Share","tools-download":"Download","code-line":"선","code-lines":"윤곽","copy-button-tooltip":"클립보드 복사","copy-button-tooltip-success":"복사완료!","repo-action-links-edit":"편집","repo-action-links-source":"소스코드 보기","repo-action-links-issue":"이슈 보고","back-to-top":"맨 위로","search-no-results-text":"일치 없음","search-matching-documents-text":"일치된 문서","search-copy-link-title":"검색 링크 복사","search-hide-matches-text":"추가 검색 결과 숨기기","search-more-match-text":"추가 검색결과","search-more-matches-text":"추가 검색결과","search-clear-button-title":"제거","search-text-placeholder":"","search-detached-cancel-button-title":"취소","search-submit-button-title":"검색","search-label":"검색","toggle-section":"토글 섹션","toggle-sidebar":"사이드바 전환","toggle-dark-mode":"다크 모드 전환","toggle-reader-mode":"리더 모드 전환","toggle-navigation":"탐색 전환","crossref-fig-title":"그림","crossref-tbl-title":"표","crossref-lst-title":"목록","crossref-thm-title":"정리","crossref-lem-title":"보조정리","crossref-cor-title":"따름정리","crossref-prp-title":"명제","crossref-cnj-title":"추측","crossref-def-title":"정의","crossref-exm-title":"보기","crossref-exr-title":"예제","crossref-ch-prefix":"장","crossref-apx-prefix":"부록","crossref-sec-prefix":"섹션","crossref-eq-prefix":"방정식","crossref-lof-title":"그림 목록","crossref-lot-title":"표 목록","crossref-lol-title":"코드 목록","environment-proof-title":"증명","environment-remark-title":"주석","environment-solution-title":"해답","listing-page-order-by":"정렬","listing-page-order-by-default":"디폴트","listing-page-order-by-date-asc":"날짜(오름차순)","listing-page-order-by-date-desc":"날짜(내림차순)","listing-page-order-by-number-desc":"페이지 번호(내림차순)","listing-page-order-by-number-asc":"페이지 번호(오름차순)","listing-page-field-date":"날짜","listing-page-field-title":"제목","listing-page-field-description":"설명","listing-page-field-author":"저자","listing-page-field-filename":"파일명","listing-page-field-filemodified":"갱신일","listing-page-field-subtitle":"부제목","listing-page-field-readingtime":"읽기 시간","listing-page-field-wordcount":"단어 수","listing-page-field-categories":"분류","listing-page-minutes-compact":"{0} 분","listing-page-category-all":"전체","listing-page-no-matches":"일치 없음","listing-page-words":"{0} 단어"},"metadata":{"lang":"ko","fig-responsive":true,"quarto-version":"1.4.543","theme":{"light":["cosmo","../../../../theme.scss"],"dark":["cosmo","../../../../theme-dark.scss"]},"code-copy":true,"grid":{"sidebar-width":"200px","body-width":"1200px","margin-width":"200px"},"comments":{"giscus":{"repo":"kmink3225/blog","category":"Blog"}},"title-block-banner":"#EDF3F9","title-block-banner-color":"black","toc-location":"right","open-graph":true,"twitter-card":true,"search":true,"date-format":"YYYY년 MM월 DD일","title":"Machine Learning","subtitle":"Comprehensive Guide to Machine Learning Algorithms and Methods","description":"From foundations to advanced methods - A systematic path to mastering\nsupervised learning, unsupervised learning, and practical ML engineering\n","categories":["Machine Learning"],"author":"Kwangmin Kim","date":"11/10/2024"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}