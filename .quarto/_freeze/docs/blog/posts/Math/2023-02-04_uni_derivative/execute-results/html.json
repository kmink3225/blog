{
  "hash": "2ac56c1d822b583039e99e049c4c094a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Differentiation - Univariabe Function\nsubtitle: Univariable Differentiation\ndescription: |\n  To solve optimization problems, it is required to know about derivatives because derivatives are mostly used 최적화 문제를 풀기위해 미분이 항상 사용되기 떄문에 미분에 대해서 알 필요가 있다. \ncategories:\n  - Mathematics\nauthor: Kwangmin Kim\ndate: 02/04/2023\nformat: \n  html:\n    page-layout: full\n    code-fold: true\ndraft: False\n---\n\n<ul class=\"nav nav-pills\" id=\"language-tab\" role=\"tablist\">\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link active\" id=\"Korean-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#Korean\" type=\"button\" role=\"tab\" aria-controls=\"Korean\" aria-selected=\"true\">Korean</button>\n  </li>\n  <li class=\"nav-item\" role=\"presentation\">\n    <button class=\"nav-link\" id=\"English-tab\" data-bs-toggle=\"tab\" data-bs-target=\"#English\" type=\"button\" role=\"tab\" aria-controls=\"knitr\" aria-selected=\"false\">English</button>\n  </li>\n\n<div class=\"tab-content\" id=\"language-tabcontent\">\n\n<div class=\"tab-pane fade  show active\" id=\"Korean\" role=\"tabpanel\" aria-labelledby=\"Korean-tab\">\n\n::: {#Korean .tab-pane .fade .show .active role=\"tabpanel\" aria-labelledby=\"Korean-tab\"}\n\n\n## Overview\n\n미분은 최적화 문제를 푼다는 의미는 목적함수를 최적화한다는 말이고 그 과정에서 대부분의 경우 미분을 사용하여 상황에 맞게 목적함수의 극소값이나 극대값을 구하게 된다. 예를 들어, 목적 함수 $f(x;w)$ 가 매개변수 또는 가중치 $w$ 에 의해 그 모양이 결정되므로 $f(x;w)$ 를 최소화하는 최적화 문제를 풀고 싶을 때 $f(x;w)$ 를 cost function (=$f(x;w)$ 의 함수) 또는 loss function (=$f(x;w)$ 의 함수) 으로 잘 정의를 한 후 cost function 또는 loss function을 최소화하는 매개변수 $w$ 를 구해야한다.\n대부분의 경우, Machine Learning (Deep Learning 포함) 에서 미분은 error를 줄이기 위해 사용한다.\n\n목적 함수 $f(x;w)$ 가 정의 됐을 때 목적 함수의 극 값을 구하기 위해서 변화량을 관찰해야 한다. 비단 목적함수를 포함한 어떤 현상을 함수로 표현할 때에도 문제를 해결하기 위해 변화량을 관찰하는 경우가 빈번하다.\n\n예를 들어, 선풍기 바람의 세기를 조절할 때 입력값은 바람 세기 버튼 출력값은 바람의 세기로 가정한다면 선풍기 바람 세기 입력에 따라 적절한 출력값을 갖도록 조정하고 싶을 것이다. 이때 변화량 관찰이 요구된다. 변화량을 잘 대변하는 것이 함수의 기울기이다. 함수의 기울기는 민감도 (sensitivity)로 표현되기도 한다(wiki). 기울기는 아래와 같이 정의된다.\n\n## 미분의 종류\n\n* 직접 미분\n  * 이번 블로그에서 정리한 내용\n  * 수동 미분: 식을 알고 손으로 미분하여 도함수를 얻음\n  * symbolic 연산: 기호를 이용하여 미분 연산 $\\rightarrow$ sympy package 이용\n  * 함수 복잡하면 풀기 매우 어려움\n* 간접 미분 \n  * 수치 미분 (numerical differentiation): 도함수를 몰라도 미분계수를 구하는데 사용\n  * 함수 복잡하면 시간 오래 걸림\n* 자동 미분 (automatic differentiation)\n  * 복잡한 함수를 간단한 함수로 쪼개어 직접 미분 - Pytorch\n  * 실무에서, 정합성 검사를 위해 자동 미분의 결과를 간접 미분의 결과와 맞추기도 한다.\n  * 간접 미분이 정답이 된다.\n\n## 일 변수 함수의 미분\n\n::: {#def-Slope}\n\nThe slope of line connected with the two points $P_1(x_1,y_1)$ and $P_2(x_2,y_2)$ is\n$$\nm=\\frac{y_2-y_1}{x_2-x_1}\n$$\n\n:::\n\n::: {#thm-PointSlope}\n\nThe point slope equation of line through $P_1(x_1,y_1)$ with slope $m$ is\n$$\ny-y_1=m(x-x_1)\n$$\n\n:::\n\n::: {#thm-SlopeIntercept}\n\nThe slope intercept euation of line with slope m and y-intercept b is\n$$\ny=mx+b\n$$\n\n:::\n\n$$\n\\begin{aligned}\n\\text{기울기}(slope)&=\\frac{\\text{출력의 변화량}}{\\text{입력의 변화량}}\\\\ \n&= \\text{입력 변화량에 대한 출력 변화량} \\\\\n&=\\frac{\\Delta output}{\\Delta input}\\\\\n&= \\text{단위 입력당 출력의 변화량}\\\\ \n&= \\text{민감도, 평균 변화율 (Rates Of Change), or etc}.\\\\\n\\end{aligned}\n$$\n\n\n::: {#def-평균변화율}\n\n\n$$\n\\begin{aligned}\n\\text{평균 변화율}&=\\Delta x\\text{에 대한} \\Delta y\\text{의 비율}\\\\\n&=\\frac{\\Delta y}{\\Delta x}=\\frac{f(b)-f(a)}{b-a}\\\\\n&=\\frac{f(a+\\Delta x)-f(a)}{\\Delta x}\n\\end{aligned}\n$$\n\n\n:::\n\n* 입력에 대한 변화율을 조절하고 싶을 때 필요한 개념 \n  * ex) 시약 농도 대비 신호 증폭의 변화율을 관찰\n* 하지만 관심의 대상의 관계 그래프가 직선이 아닌 **곡선의 형태**의 경우 평균 변화율이 대략적인 추세 정보만 제공해줄뿐 변화량의 자세한 정보를 제공해주지 못함\n  * sigmoid 형태의 경우 첫 포인트와 마지막 포인트의 평균 변화율을 보는 것 보다 구간을 짧게하여 여러 군데서 관찰하는 것이 graph의 shape를 더 잘 설명하는 것\n* 이 때, 입력값의 구간을 **충분히** 짧게 만들어 출력값의 변화량을 관찰하는 것이 미분이다. (limit의 개념, $\\epsilon-\\delta$ method)\n\n::: {#def-TangentLine}\n\nThe tangent line to the curve $y=f(x)$ at the point $P(a,f(a))$ is the line through P with slope\n\n$$\nm = \\lim_{x\\to a} f(x)\n$$\n\nprovided that this limit exists.\n:::\n\n\n\n::: {#def-Derivative}\n\nWhen $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is continuous and differentiable, the derivative of a function $f$ at a number $a \\in \\mathbb R$, denoted by $f'(a)$, is\n\n$$\n\\begin{aligned}\nf'(a) &= \\lim_{h\\to 0} \\frac{f(a+h)-f(a)}{h}\\\\\n      &= \\lim_{x\\to a} \\frac{f(x)-f(a)}{x-a}\n\\end{aligned}\n$$\n\nprovided that this limit exists. 이때 위의 함수의 극한값, $f'(a)$ 라고도 표시하며 점 $a$ 에서의 $f(x)$ 의 도함수 (derivative) 라고 한다. \n:::\n\n![James Stewart - Calculus Early Transcedentals, 7th Eidition, P.143](derivative.PNG)\n\n::: {#def-Differentiable}\nA function $f$ is differentiable at $a$ if $f'(a)$ exists. It is differentiable on an open interval (a,b) [or (a,$\\infty$), (-$\\infty$,a) or (-$\\infty$,$\\infty$)] if it is differentiable at every number in the interval.\n:::\n\n::: {#thm-Continuous}\nIf $f$ is differentiable at $a$, then $f$ is continuous at $a$.\n:::\n\n* 순간 변화율 = 미분 계수 = 접선의 기울기\n* 미분 (differentiation) : 순간 변화율 구하는 행위\n* 도함수 (derivative) : 도함수 자체는 equation 으로, 특정 포인트에서의 순간 변화율 (값)을 출력하는 함수\n* 문제를 풀때 도함수를 구하는 것인지, 미분계수를 계산하는 것인지를 구별해야함\n* 전체 도함수를 구하는것은 보통 굉장히 어려움. 하지만 한점에서의 순간변화율 즉, 미분계수를 구하는 것은 가능\n* 에러를 줄이는 데에는 값으로 나오는 순간 변화율을 구하는 것이 일반적으로 실현성이 있는 문제\n\n::: {#def-NaturalNumber}\nThe natural number, $e$ is the number such that $\\lim_{h\\to 0} \\frac{e^h-1}{h}=1$.\n:::\n\n모든 지수 함수 $f(x)=a^x$ 중에서 $f(x)=e^x$ 가 점 (0.1) 에서의 접선의 기울기가 $f'(0)=1$ 이 되는 수를 $e=2.71828...$ 라고 정의한다.\n\n### Notation\n\n$f'(x)$ 는 다음과 같은 기호들로도 흔히 표현된다.\n\n* Lagrange’s notation\n  * $y', f'(x)$\n  * 어떤 변수로 미분하는지에 대해서 명시적으로 표현되지 않았음. 고등학교때까진 univariable function을 미분 했기떄문에 이 표기법이 많이 사용되었음.\n* Leibniz’s notation\n  * $\\frac{dy}{dx}=\\frac{df}{dx}=\\frac{d}{dx}f(x)$\n  * 입력 변수와 출력 변수까지 모두 명시되어있음\n* Newton’s notation: \n  * $\\dot{y}, \\ddot{y}$\n  * 최적화 논문과 financial engineering 에서 본적 있음\n* Euler’s notation\n  * $D_xy, D_xf(x)$\n  * 시계열 논문과 미분방정식 논문에서 본적 있음\n\n### Example\n\n다음의 함수를 미분의 정의를 이용하여 도함수를 계산하시오\n\n1. $f(x)=c$ where c is a constant\n1. $f(x)=\\log x$\n1. $f(x)=e^x$\n1. $f(x)=\\sin x$\n\n[Derivative Formula](https://byjus.com/calculus-formulas/)는 모두 미분의 정의를 이용해서 구할 수 있음\n\n::: {#thm-Differentiation_Rules}\n1. **The Power Rule**, if $n$ is any real number, then the power function, $x^n$ is differentiated like the following:\n$$\n\\frac{d}{dx}(x^n)=nx^{n-1}\n$$\n\n1. **The Constant Multiple Rule**, if $c$ is a constand and $f$ is a differentiable function, then\n$$\n\\frac{d}{dx}(cf'(x))=c\\frac{d}{dx}f(x)=cf'(x)\n$$\n\n1. **The Sum Rule**, if $f$ and $g$ are both differentiable, then\n$$\n\\frac{d}{dx}[f(x)+g(x)]=\\frac{d}{dx}[f(x)]+\\frac{d}{dx}[g(x)]=f'(x) +g'(x)\n$$\n\n1. **The Difference Rule**, if $f$ and $g$ are both differentiable, then\n$$\n\\frac{d}{dx}[f(x)-g(x)]=\\frac{d}{dx}[f(x)]-\\frac{d}{dx}[g(x)]=f'(x) -g'(x)\n$$\n\n1. **The Product Rule**, if $f$ and $g$ are both differentiable, then\n$$\ny=f(x)g(x), y'=f'(x)g(x)+f(x)g'(x)\n$$\n\n1. **The quotient rule**, if $f$ and $g$ are both differentiable, then\n$$\ny=\\frac{f(x)}{g(x)}, y'=\\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\n$$\n:::\n\n증명은 James Stewart의 Calculus Series 중 1개를 골라 참고하시기 바랍니다.\n\n### Example\n\n1. $S(x)=\\frac{1}{1+e^{-ax}}$ 를 $x$ 에 대해 미분해보시오.\n1. $f(x)=\\alpha_1 + \\frac{\\alpha_2-\\alpha_1}{1+e^{-\\alpha_4(x-\\alpha_3)}}$ 를 어떻게 미분할 것인지 생각해 보시오.\n1. $y=f(x)=(4x+3)^2$ 를  $x$ 에 대해 미분해보시오\n1. $y=f(x)=(4x+3)^{20}$ 를 $x$ 에 대해 어떻게 미분할 것인지 생각해보시오. (hint: composite function - Leibniz)\n\n\n### Example Answer\n\n#### sympy package example\n\n앞서와 언급한대로 파이썬 sympy package를 사용할 것인데 간단한 예를 본다. 먼저 기호의 정의를 해주고 수학 연산을 진행하면 된다.\n\n::: {#e85c15bc .cell execution_count=1}\n``` {.python .cell-code}\n# import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sympy as S\nimport matplotlib as mpl\nfrom mpl_toolkits import mplot3d\nimport matplotlib.font_manager as mfm\n```\n:::\n\n\n::: {#588cd891 .cell execution_count=2}\n``` {.python .cell-code}\n# 심볼 정의\nx=S.Symbol('x')\nh=S.Symbol('h')\nn=S.Symbol('n')\na=S.Symbol('a')\na1=S.Symbol('a1')\na2=S.Symbol('a2')\na3=S.Symbol('a3')\na4=S.Symbol('a4')\n\n## 인수분해\nS.factor(x**2+4*x+4)\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=2}\n$\\displaystyle \\left(x + 2\\right)^{2}$\n:::\n:::\n\n\n* 인수분해와 값 넣기\n\n::: {#832bf398 .cell execution_count=3}\n``` {.python .cell-code}\nf=S.factor(x**2-5*x+6)\nprint(f.subs({x:3}))\nprint(f.subs({x:5}))\nf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0\n6\n```\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=3}\n$\\displaystyle \\left(x - 3\\right) \\left(x - 2\\right)$\n:::\n:::\n\n\n* $2^x$ 미분과 값 넣기\n\n::: {#74ed8eff .cell execution_count=4}\n``` {.python .cell-code}\ndf=S.limit((2**(x+h)-2**x)/h,h,0)\nprint(df.subs({x:3}))\nprint(df.subs({x:5}))\n\nprint(df.subs({x:3}).evalf()) # 정확한 값을 원할 경우\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnan\nnan\nnan\n```\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=4}\n$\\displaystyle \\infty \\operatorname{sign}{\\left(- 2^{x} + e^{x \\log{\\left(2 \\right)}} \\right)}$\n:::\n:::\n\n\n* $x^n$ 미분\n\n::: {#fef523f7 .cell execution_count=5}\n``` {.python .cell-code}\ndf=S.limit(((x+h)**n-x**n)/h,h,0)\nprint(df.subs({x:3}))\nprint(df.subs({x:5}))\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3**(n - 1)*n\n5**(n - 1)*n\n```\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=5}\n$\\displaystyle n x^{n - 1}$\n:::\n:::\n\n\n#### Example Answers \n\n* $f(x)=c$ 의 도함수 where c is a constant. c=2로 고정\n\n::: {#e6417cdd .cell execution_count=6}\n``` {.python .cell-code}\nf=2\ndf=S.limit(((2)-2)/h,h,0)\ndf\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=6}\n$\\displaystyle 0$\n:::\n:::\n\n\n* $f(x)=\\log x$ 의 도함수\n\n::: {#f0a3c446 .cell execution_count=7}\n``` {.python .cell-code}\nf=S.log(x)\ndf=S.limit((S.log(x+h)-S.log(x))/h,h,0)\ndf\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=7}\n$\\displaystyle \\frac{1}{x}$\n:::\n:::\n\n\n* $f(x)=e^x$ 의 도함수\n\n::: {#e56839c7 .cell execution_count=8}\n``` {.python .cell-code}\nf=S.exp(x)\ndf=S.limit((S.exp(x+h)-S.exp(x))/h,h,0)\ndf\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=8}\n$\\displaystyle e^{x}$\n:::\n:::\n\n\n* $f(x)=\\sin x$ 의 도함수\n\n::: {#964f7ca8 .cell execution_count=9}\n``` {.python .cell-code}\nf=S.sin(x)\ndf=S.limit((S.sin(x+h)-S.sin(x))/h,h,0)\ndf\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=9}\n$\\displaystyle \\cos{\\left(x \\right)}$\n:::\n:::\n\n\n* $S(x)=\\frac{1}{1+e^{-ax}}$ 를 $x$ 에 대해 미분해보시오.\n\n::: {#0cba6b44 .cell execution_count=10}\n``` {.python .cell-code}\nf=1/(1+S.exp(-a*x))\ndf=S.limit((1/(1+S.exp(-a*(x+h)))-1/(1+S.exp(-a*x)))/h,h,0)\ndf\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=10}\n$\\displaystyle \\frac{a e^{a x}}{e^{2 a x} + 2 e^{a x} + 1}$\n:::\n:::\n\n\n이번 문제에서 $\\frac{ae^x}{(e^{-ax}+1)^2}$ 라는 sigmoid function의 도함수를 얻었다. 이 도함수를 간단한 수학적 조작으로 다른 표현으로 유도해보면 다음과 같다.\n$$\n\\begin{aligned}\n\\frac{d}{dx}S(x)&=\\frac{ae^{-ax}}{(e^{ax}+1)^2}\\\\\n                &=a\\frac{1}{(e^{ax}+1)}\\frac{e^{-ax}}{(e^{ax}+1)}\\\\\n                &=a\\frac{1}{(e^{ax}+1)}\\frac{1+e^{-ax}-1}{(e^{ax}+1)}\\\\\n                &=a\\frac{1}{(e^{ax}+1)}(1-\\frac{1}{(e^{ax}+1)})\\\\\n                &=aS(x)(1-S(x))\\\\\n\\end{aligned}\n$$\n\n위와 같이 $S(x)$ 의 도함수는 $aS(x)(1-S(x))$ 로 표현될 수 있다. sigmoid function은 neural network에서 activation function으로 사용되는데 forward propagation에서 이미 한 번 계산이 된다. backward propagation에서 activation function인 $S(x)$ 의 도함수를 다시 연산을 해야하는데 도함수가 $aS(x)(1-S(x))$ 것을 알면 복잡한 고차원 행렬곱 연산을 다시 수행하지 않아도 된다. 그래서 $S(x)$ 의 도함수를 $\\frac{ae^x}{(e^{-ax}+1)^2}$ 라고 코딩하는 것 보다는 $S(x)$ 의 행렬을 재활용하여 $aS(x)(1-S(x))$ 로 코딩해놓으면 연산 과정에서의 시간 복잡도를 줄일 수 있다. 이처럼 machine learning에서 수학적 통계적 지식을 잘 활용하면 좀 더 효율적인 모델링을 구현 할 수 있다.\n\n::: {#91f00de2 .cell execution_count=11}\n``` {.python .cell-code}\nx1 = np.linspace(-6, 6, 100)\nsx = 1/(1+np.exp(-x1))\nd_sx = sx*(1-sx)\n\nplt.plot(x1,sx,color='black',label='S(x)')\nplt.plot(x1,d_sx,color='red',label='dS(x)')\n\nplt.xlabel('X')\nplt.xlabel('S(x)')\nplt.title('Simgoid Curve with its Derivative')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2023-02-04_uni_derivative_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\n* $f(x;\\mathbf\\alpha)=\\alpha_1 + \\frac{\\alpha_2-\\alpha_1}{1+e^{-\\alpha_4(x-\\alpha_3)}}$ 를 어떻게 미분할 것인지 생각해 보시오.\n\n위의 식은 logistic fucntion의 genral formular 형태인데 sigmoid function이 특수한 예이다. 함수의 shpae는 parameter에 의해 결정되는데 위의 경우 $\\alpha_1$ 은 함수의 최솟값 , $\\alpha_2$ 은 함수의 최댓값, $\\alpha_3$ 은 함수의 변곡점 및 $\\alpha_4$ logistic curve가 변곡점을 지나면서 증가하는 변화율을 묘사한다. sigmoid 형태의 data를 fitting하기 위해 위의 함수를 이용한다면 error를 최소화하는 parameter를 구해야하는데 이 또한 최적화 문제로 4개의 변수에 대한 미분이 필요하다. 2개 이상의 변수에 대해서 미분은 partial derivative (편미분)라고 하는데 다음 블로그에서 다룰 것이다.\n\n\n* $y=f(x)=(4x+3)^2$ 를  $x$ 에 대해 미분해보시오.\n\n::: {#6c3aafb6 .cell execution_count=12}\n``` {.python .cell-code}\nS.limit(((4*(x+h)+3)**2-(4*x+3)**2)/h,h,0)\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=12}\n$\\displaystyle 32 x + 24$\n:::\n:::\n\n\n위의 도함수는 미분 공식 중 곱의 법칙을 사용하면 구할 수 있다.\n\n* $y=f(x)=(4x+3)^{20}$ 를 $x$ 에 대해 어떻게 미분할 것인지 생각해보시오. (hint: composite function - Leibniz)\n\n위의 문제처럼 곱의 법칙을 사용하면 20개의 인수에 대해서 차례대로 미분을 해야하므로 계산량이 엄청나게 많아진다. 이때 composite function (합성 함수)의 derivative를 구하는 chain rule을 이용하면 간단한 연산으로 도함수를 구할 수 있게 된다. deep learning 모델의 기초인 neural network는 layer nodes이 복잡하게 합성이 되는 합성 함수를 만들면서 forward propagation을 진행하고 backward propabation에서 이 복잡한 합성 함수의 미분을 수행하게 된다. 그러므로 합성 함수의 미분이 어떻게 수행되는지 아는 것은 deep learning을 수리적으로 이해하고 싶은 사람에게 있어서 중요할 수 있다. 합성 함수의 미분은 다른 블로그에서 다루도록 하겠다.\n:::\n</div>\n\n<div class=\"tab-pane fade\" id=\"English\" role=\"tabpanel\" aria-labelledby=\"English-tab\">\n\n\n\n</div>\n\n## Blog Guide Map Link\n\n* [Statistics Blog](../../statistics/guide_map/index.qmd)\n* [Engineering Blog](../../Engineering/guide_map/index.qmd)\n* [Deep Learning Blog](../../DL/guide_map/index.qmd)\n* [Machine Learning Blog](../../ML/guide_map/index.qmd)\n* [Mathematics Blog](../Mathmatics/guide_map/index.qmd)\n* [Patent Blog](../../Patent/guide_map/index.qmd)\n* [Validation Blog](../../Validation/guide_map/index.qmd)\n\n",
    "supporting": [
      "2023-02-04_uni_derivative_files"
    ],
    "filters": [],
    "includes": {}
  }
}