{"headings":["요약","nlp-모델-발전-과정","bert-이전-모델들의-한계점","기존-언어-모델의-문제점","일방향성-문제","elmo의-한계","문맥-독립적-임베딩의-한계","bert의-핵심-아이디어와-혁신","양방향-문맥의-진정한-활용","masked-language-model-mlm","mlm의-동작-원리","마스킹-전략-15-토큰-중","next-sentence-prediction-nsp","nsp의-목적과-방법","nsp의-한계와-후속-연구","bert의-구조와-아키텍처","transformer-인코더-기반-설계","bert의-전체-구조","모델-크기별-사양","입력-표현-input-representation","token-embedding","position-embedding","segment-embedding","특수-토큰의-역할","cls-토큰","sep-토큰","mask-토큰","pad-토큰","attention-mask-메커니즘","bert의-학습-과정","사전-학습-pre-training","학습-데이터","학습-설정","학습-시간과-비용","파인튜닝-fine-tuning","태스크별-아키텍처-변경","텍스트-분류-text-classification","개체명-인식-named-entity-recognition","질의응답-question-answering","문장-유사도-sentence-similarity","파인튜닝-효율성","bert의-성능과-영향","glue-벤치마크-성능","squad-질의응답-성능","한국어-bert-모델들","kobert-sktbrain-2019","koelectra-monologg-2020","klu-bert-시리즈","bert의-한계점과-해결방안","bert-vs-gpt","bert와-gpt의-학습-방식-요약","bert의-한계-masked-lm의-본질적-약점","gpt의-강점-자연스러운-생성과-학습-구조","모델-크기와-학습-데이터-규모","응용-범위와-범용성","계산-복잡도-문제","문제점","해결방안들","긴-시퀀스-처리-한계","문제점-1","해결방안들-1","생성-태스크-한계","문제점-2","해결방안들-2","bert-변형-모델들","roberta-2019-facebook","주요-개선사항","성능-향상","albert-2019-google","핵심-기술","효과","distilbert-2019-hugging-face","지식-증류-knowledge-distillation","electra-2020-google","혁신적-학습-방법","성능","bert의-현재와-미래","산업계-활용-현황","검색-엔진-개선","실제-서비스-적용","후속-모델들에-미친-영향","transformer-기반-모델들","설계-원칙의-확산","연구-동향과-발전-방향","효율성-개선","성능-향상-1","응용-분야-확장","결론","bert의-핵심-기여","후속-발전에-미친-영향","현재적-의미와-미래-전망"],"entries":[]}