{"headings":["요약","텍스트-인코딩-및-벡터화","attention-메커니즘의-등장-배경","rnn의-한계점","attention의-혁신","attention-메커니즘의-기본-원리","핵심-개념","기본-수식","직관적-이해","attention-메커니즘의-종류","참조-대상에-따른-분류","self-attention","cross-attention","처리-방식에-따른-분류","single-head-attention","multi-head-attention","방향성에-따른-분류","bidirectional-attention","causalunidirectional-attention","구조적-분류","scaled-dot-product-attention","additive-attention","sparse-attention","현대-nlp에서의-attention-활용","transformer-아키텍처","사전-학습-모델에서의-활용","bert-계열-bidirectional","gpt-계열-causal","t5-text-to-text","특화된-attention-변형","longformer","bigbird","linformer","결론","attention-메커니즘의-핵심-기여","현대-ai에-미친-영향","한계와-도전-과제","미래-전망","결론적-의미"],"entries":[]}